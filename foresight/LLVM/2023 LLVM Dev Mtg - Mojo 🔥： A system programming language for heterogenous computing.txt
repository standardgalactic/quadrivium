We're very happy to be here and talk a little bit about what we've been up to.
So we'll start with what is Mojo?
At a glance, the top-level points of Mojo is that it's a Pythonic systems programming
language.
So what does that mean?
That means we're here to do really cool things with systems and compilers, and it happens
to look like Python, but forget everything you know about Python, please, please.
So this thing is about one year old, so it's still pretty early, it's still in development.
It's still quite interesting and doing some cool stuff, though.
We also have a vibrant community.
We have over 150,000 users.
We have a big community in Discord, and there's a bunch of excitement around this.
So we'll dive today into why did we do this in the first place?
That's often something we're asked.
We'll talk about how we approach designing a new language from scratch.
We'll talk about internal implementation details, including some of the horrible things we did
to LLVM.
Talk about what this means for accelerators and compute, and then wrap things up.
So first, why?
Why, why, why, why, why?
So many of you are working on AI, and if you work on AI, the question I will ask of you
all is, if AI is so important to the world, why is all this offer so bad?
This is a huge question, a huge problem, and I think that many of us who have been working
in this industry for a while have been struggling with solving this problem in many different
ways.
And so for me, when I look at this, I think that the challenge is really fragmentation,
complexity.
It's all these systems that do not work very well together, that are being built by well
meaning people in different groups and areas, but they don't really actually work together.
And so for a user, this is a huge pain point.
And why is this?
I'll speak for myself.
If you're enabling a chip, you're focused on the chip.
So many of us are paid to solve one specific problem, we're not here to solve an industry
scale problem, and you can't afford to do it.
You don't have the time, you don't have the schedule, you don't have the headcount, whatever.
Often the organization that you're within, in my experience, makes it very difficult
to solve some of these problems.
And so our approach at Modular is that we need fewer things that work better.
And so that's what led us to building Modular in the first place.
It's really kind of an organization that can span across many different of these problems
and invest for the long term in building and hopefully lifting the industry over time.
So how do we do this specifically?
Well, we're building what we call the AI engine.
Well, the AI engine, if you look at modern ML stack, a lot of folks are trying to throw
layers of Python on top of all this AI tech that has been built up.
We're tackling it at the Herber software boundary, reinvesting, no surprise, and compilers.
And so what we want to do is we want to unify and integrate all these low level technology
systems so that innovation can happen up on top with programming models and frameworks
and all that kind of stuff.
Our approach is to meet people where they are.
So people use PyTorch, people use Jax, people use TensorFlow.
That's awesome.
These all have pros and cons, and there's other stuff as well.
And very few people actually want to rewrite all their code.
And for us, it's very important to be drop and compatible, meet people where they are,
and work with their existing systems.
The other thing is that this is not a research project, like there's a lot of really interesting
and cool things that have been built over the last eight-ish years of AI infrastructure.
It often gets fragmented out into all these different systems.
We've learned from many of them, and so what we're doing is we're pulling this back together
and doing hardcore engineering, not research, to build a production quality system that
we hope can scale for the world.
I'll go through this super quickly.
It was an AI engine.
Well, it's really two things.
One is this operator graph.
The operator graph in the interesting case is heterogeneous.
So people often focus on, for example, the GPU, and how do I make matrix multiplications
go fast?
And that's a super important problem.
But often folks forget that AI today is a distributed problem, involves the host, involves
the accelerator, involves pre-processing, data loading, this whole thing.
And so you can't really solve the AI problem for a user unless you really tackle this whole
problem.
And furthermore, this is really heterogeneous.
As we've seen, there's all kinds of different accelerators, there's all kinds of different
hardware.
When you have a cluster, lots of machines, micro-architectures don't always match.
There's a lot of complexity in this space.
So many of us have been working on this, again, for a long time.
And so we've seen the rise of kernel libraries.
This is how many of these systems were first built.
And one of the challenges that I won't go into in depth, many of you probably already
agree, is that kernel libraries don't scale.
And so many of us, for multiple years now, have been building AI compilers.
And so there's lots of these, lots of different approaches, online kernel fusion, lots of
cool algorithms getting vented and used.
We can talk about all the different pros and cons of trade-offs.
But the thing I want to claim is that neither of these approaches scale.
Kernels don't scale, hopefully many people understand that, but neither do ML compilers.
And to a compiler audience that maybe is more controversial than to a kernel audience.
So I thought I'd dive a little bit into why this is and the challenges that we see with
this led us to our approach with Mojo and the system.
So the first is generality.
I mean, empirically today, ML compilers are not very general, right?
Generality includes not just matrix multiplication, again, data loading, preprocessing, all this
stuff, but also dynamic shapes, varsity.
There's better and worse systems out there, and there's definitely progress in this area.
But if you're coming at it from a user's perspective, they want things to just work.
And if they don't just work, then they'll move on and spend their time something else.
Generality is also important because if you're, again, coming from a hardware enablement perspective,
you don't really have time to invest in all the other parts of the problem.
And so it makes sense that many of us working on bring up the chip don't actually focus
on the big picture parts of the problem.
Another one is community.
So you all are wonderful compiler nerds.
I love you all, obviously.
And myself, a pretty big compiler nerd.
But the problem is that nobody can hire a compiler engineer.
This is pretty well known.
And so with AI compilers, this becomes even worse, because how do you hire somebody who
knows compilers, who knows AI modeling and all the different exotic new model of the
day, who knows all the numerics and the data types and knows all the specialized hardware,
and how do you find that unicorn person that knows all of these things together?
It's very, very difficult out there.
And if you need a compiler engineer to be in the loop of novel research, there's very
few companies in the world that can afford or attract to do that.
And so I believe that you cannot have a compiler-first approach to this problem simply because there's
enough talent out there.
I mean, I love you all, and you're all very valuable, but this is very difficult, particularly
for the scale of what AI research is today.
Second, if you're a compiler engineer, it seems really weird that we're re-encoding
all of compute into IR builders and sanding out all this stuff.
And so you feel like there must be a problem here at some point.
Finally, there's this fragmentation problem.
If you want to solve and build a heterogeneous compute system, we have to face the reality
that AI developers, researchers, are in Python.
The frameworks, the host-side compute, it's all in C++.
The device-side is in CUDA, in SQL, and other things.
And so if you want to build a system that can scale across all these different levels
of abstraction, there's a huge fragmentation problem here, and we need to be able to unify
this.
Otherwise, we can't have one system that can reason about it.
And so if you want to be able to build this and solve this problem, you have to kind of
come back and look at the big picture of what's going on here.
And the nature of compute has changed.
So this is what has led us to Mojo.
Now how do we approach building Mojo?
I mean, you know the outcome, and we'll talk a lot more about how it works, but how do
we even get here?
Well, when we started Modular, we started with a thesis, a hypothesis.
We believed that we could get to state-of-the-art performance against a lot of vendor systems
and do so with a single source of truth in our code for numerics.
This hasn't really been done before.
There's definitely systems that have been around in the space.
But this thesis, if true, can enable and unlock a huge amount of innovation in the industry.
And so what we did was we said, okay, let's go invest in some very fancy compiler stuff,
generalized fusion, and caching integrated distributed compilation, like lots of cool
stuff.
Let's figure out what we want to do, and then let's go validate that.
But for validation, we didn't actually care about syntax.
So what did we do?
Well, we went, and we actually went and built the thing.
We went and built a compiler and completely ignored syntax.
All right, why?
Well, MLR is great.
You can write MLR by hand.
You don't need a front-end.
And so what we could do is we could actually go build major kernel libraries and things
like this and validate.
Architecturally, we could deliver the performance that we wanted to, show that the compiler
worked, iterate rapidly on the compiler without having to change a dependency, and go and
do this.
And what we found, fortunately, is that it works.
The technology we built actually is good.
It worked.
It was proven out.
And then immediately, we figured out that writing large amounts of MLR by hand is maddening
and doesn't scale, and there's no way a real normal user could actually do this.
And so, but this validation of the algorithms of the compiler tech of the low-level system,
which is very novel, and Jeff will talk about later, was really important to building our
system and doing so without being anchored on syntax.
I think it was very good for both focus, but also for the ability to iterate.
So once you get that, you get to the point of saying, what about syntax?
Syntax actually does matter.
And so the three major approaches we looked at are, do we take an existing language like
C++ or Swift or something like that?
Do we do an EDSL?
Do we do a new language?
And so when we were talking about this, we came back to our core principles, our values,
our goals, which is that we wanted to meet people where they are.
And whether you like it or not, AI developers, but also most software engineers are all in
Python.
Right?
Python is pretty arguably the most popular programming language in the world.
And so if you're coming from a Python viewpoint, arguing with people, trust me, I've been there,
to try to get them to switch to a different thing, is a huge amount of work and it doesn't
really go anywhere.
And so we realize and believe we had to go with Python, and what that meant is that
meant that suddenly a bunch of existing systems are just off the table, like C++ is not Python,
Swift is not Python.
These things are not Python.
And so that really allows us to focus our frame.
What about EDSLs?
Well, EDSLs are super common.
They're super popular and they exist for lots of good reasons.
They're relatively easy to implement.
We've had several talks at the conference about how to use Python so that you can extract
and build IR from Python ASTs and things like this.
It means you don't have to build tooling, you don't have to retrain, you can get to
market fast.
The problem is that they provide a really bad developer experience.
You don't get a debugger.
This really can't fit into the existing systems.
If you care about host performance and generality, Python's not there, at least not the level
of performance that we care about.
And so what we really want is we want a system that allows us to innovate at all layers of
this stack.
Okay, well, how about a new language?
Again, you know kind of where we're going with this, but a new language has the advantage
of you get the best quality result, you can control everything, you can invest in things,
you can target CPUs with high performance, which is quite important to us.
But what you need is a strong vision for what you're trying to do.
You need a long-term commitment because the demo is easy, but production quality thing
is hard.
You need to be able to pay for it, you need to be able to track people, you need to be
able to have a big target of developers that makes it worth doing in the first place.
And so this is actually well known to be ridiculously expensive, like building new programming
language is not a simple thing that you should reach for as your first outcome.
But as you know, yes, we want a baby little mojo to be built and what we decide to do
is actually do this.
And why?
Well, it's because it's the only way to achieve our goals, to achieve the best quality of
result for AI developers and many other developers worldwide and be able to lift the industry.
There are many point solutions that demonstrate many different capabilities, but we really
want to go beyond this and integrate and unify the world.
And so if you come back to what we need to do, we think that we have all the constituent
ingredients here with a good vision, we think we know what we're doing.
We also know how hard this is.
So I personally built several major programming languages that are used in production and
have seen the entire journey and made many mistakes and have learned from them.
And so with full knowledge, we step into this and say, okay, let's do this.
So I'll give you the high level design points of mojo.
As you know, it's a member of the Python family.
Over time, it will grow into being a full superset because we don't want to do a Python
two to three thing anymore to Python programmers.
As we said before, it's focused on system programming, high performance, working backwards
from the capability, the speed of light of hardware, definitely not working forwards
from what Python can do today.
Also lots of hardware, anything with the program counter can apply.
But coming back to this also, and we'll talk about this a little bit, it's about unlocking
the modular compiler stack.
And so instead of talking about the high level fluffy stuff, I'll introduce Jeff and he can
tell you a little bit more about how it actually works.
Thanks Chris for the introduction.
So we are started off by de-risking the core hypothesis and we have an MLIR based compiler
that is different a little bit from the systems that predated it.
And we've proven that we can be state of the art.
The problem is that we've got like 50,000 lines of handwritten MLIR.
And handwritten MLIR is like write once, read never.
It's so verbose, you have to write the types every time you use an SSA value.
It's pretty hard to actually write incorrect code, but then it's not readable, it's unmaintainable
and the new people being brought into the company are like, what is this?
So we need syntax.
We need a programming language for MLIR.
Why all MLIR?
Well it turns out that modern computers are getting really complicated.
Modern types are getting really complicated.
Look at just floating points.
Most languages, give or take, have a flow and a double.
But MLIR has things like float 8, E4, M3, FNUS.
I'm sure it's useful, okay?
And that means that we need to have access to it.
There's probably a piece of hardware somewhere on it that uses this data type and it's very
fast.
That's just the tip of the iceberg.
MLIR is such a vast ecosystem with many different kinds of hardware targets, domain specific
dialects and so on.
And we would like Mojo to be able to take advantage of all of that.
So we need syntax trigger for MLIR in general.
And then how do we approach something like that?
Well we start with the types.
In a programming language, types tend to be the most load bearing element.
You need types to do computations on them after all.
So let's start by focusing on a library-based language.
That means that we write all the parts of the language in the library.
And the good news is anybody can write libraries.
So this scales the effort of engineering to everyone in the world who can write Mojo.
Not just a couple of people who work on the language.
And that's really important because we don't want built-in types in the language to be
special or be more performant than what you can enable in the library because that bottlenecks
performance and the scalability of the system to the people who work on the language.
So we need to give people who use the programming language library authors the same power as
language engineers.
It turns out actually that Python has a really extensible type system.
You could argue that user-defined types in Python are actually much more powerful than
the built-in types like interfloat.
And the reason is because Python provides this kind of ability to encapsulate type semantics
behind Dunder methods, which are really syntactic wrappers.
So let's just use that in Mojo.
We use a struct, which is like a class, but it's densely packed in performance, to wrap
an MLR type.
And then we use Dunder methods as well as class methods to wrap MLR operations.
And what you get is any MLR type will work.
Any MLR operation will work.
And so now we have 1 plus 2, Dsugar is to an MLR op index.add.
The other important aspect is we need to make sure that these user-defined abstractions
feel native, that they're zero cost.
So how does Mojo do that?
Well, it has a couple of bells and whistles to tell the compiler that treat this type in
a specific way, effectively giving a built-in-like experience.
And one of these is they always inline no debug, which will always inline the function,
no question about it.
And for a better debugging experience, it nukes out all the debug info, so you don't
step into a plus of an integer.
So we put this all together, just these pieces of basic types, so you have a simple while
loop in Mojo.
Well, the parser will then spit a bunch of source-level IR, right?
But then Mojo has guaranteed optimizations that run all the time, such as the always-inliner
and memtoreg.
And then this gets desugarred down to IR that is pretty close to what we would have written
by hand.
And that's important because it, from the get-go, provides a predictable IR-gen model
for the programmer, and it helps us get an off-ramp from all the handwritten MLIR.
But so it turns out we've actually discovered what MLIR really stands for.
It's Mojo Fire Emoji Language Intermediate Representation.
And the best part is your dialect works, too.
So this is zero cost abstraction around any MLIR, so let's say you have a shape dialect
with a mosh.ape type, and it implements plus to concat and subscript to getDim.
Well, now you can write shape functions in Mojo.
It spits out some IR that's been desugarred to, and then you can ingest this IR and do
cool compiler stuff like shape inference.
And the best part is all of the language tooling just works.
So you get code completion, you get doc generation, you get syntax highlighting, and even debugging
if that's relevant.
But MLIR just forms the bottom level of the language.
It's how we talk to the hardware, it's how we talk to the various dialects.
Building on top of that requires high-level abstractions, and the way you do that in Mojo
was metaprogramming.
So Mojo needs to build hardware generality, and the way we do that is with metaprogramming.
So you can write a kernel without caring about what the vector length is, and then, say,
in this example, ask the compiler to pick one for you.
It turns out that metaprogramming is also pretty cool.
Texts are nice, code reuse is great, and it allows to have scalable development.
So where can we look at for a metaprogramming system?
Well, I actually like C++, I don't know about you, and C++ has templates.
And duct typing in C++ is really powerful.
Let's see, write some pretty crazy generic code.
The problem with that is that the usability is poor.
I think template error messages get better every year, but there's still some room to
go.
And it turns out that for the kind of metaprogramming, high-performance programming needs, C++
just aren't good enough.
So imagine you have a tensor type.
It has a static or dynamic rank.
It has a static or dynamic d-type.
It has partially dynamic shape, partially dynamic stride.
It gets ugly pretty quickly.
So it's not good enough, and let's see if we can build something better.
So it turns out, once again, Python actually has really powerful metaprogramming.
Decorators can arbitrarily modify objects and return a function where there is a type.
And with full AST reflection in Python is what enabled all these crazy libraries, such
as the ML frameworks like PyTorch, Jaxx, and TensorFlow, as well as things like Numba.
The problem with the Python metaprogramming is that it happens at runtime, which means
it's slow, it's not going to run an accelerator, and it gives zero control over the generated
code.
So the challenge for us is let's try to do it at compile time.
So that brings us to mojo parameters.
Mojo parameters are compile time values that form the backbone of the metaprogramming system.
So structs can have parameters.
These are compile time values.
Functions can have input parameters, and then you can declare name parameter values with
alias declarations.
So you can kind of think of them as being like C++ templates, but they're a little bit
different.
For example, in C++ you have using declarations for type aliases and constexpr declarations
for compile time values.
But in mojo, types are just compile time values, and so aliases and, say, compile time floats
and compile time ints are the same thing.
The most important thing that gives is that the meta language is the same as the actual
language.
And Zig really blaze the trail here by having no distinction between the metaprogram and
the actual program.
In mojo, we strive to ensure that almost any user-defined type and function can be used
and called in a parameter expression at compile time.
And the way we do that is with an MLI interpreter that has a full memory model.
So to really drive this point home, we have an example here.
It's fill a vector with a bunch of integers, OK, not too bad.
This function can be called in either compile or runtime.
And if it was called compile time, you can even return a type instance.
And this vector has heap allocation that is computed at compile time and then used at
runtime.
So when does this happen?
When do we do, say, instantiation of parameter values, function specialization, and interpreting
of code?
Well, it doesn't happen in the parser like in C++.
So in mojo, we do parameter instantiation in a process called elaboration, and it happens
later in the compiler pipeline.
What that means is that now mojo needs a IR representation for parametric code.
So in this example, we have a piece of IR, and we have a parameter in the IR called value.
Importantly, this parametric IR is target agnostic.
It's portable.
So that means something like size of lives directly in the IR, and it is resolved by the
elaborator.
So this enables something like split compilation like CUDA, and perhaps one day separate compilation
of generics like Swift.
So the elaboration pass is an MLIR pass that performs function instantiation as an IR transformation.
So in this piece of IR, we've got two calls to the function print int with two different
parameters.
It gets stamped out into two new functions, and the callers are replaced appropriately.
One consequence of a pass to do elaboration is that the language is late bound by design.
That poses a couple of language design challenges, but that means that you can do cool stuff
like autotuning, where any parameter value can be autotuned, i.e., the elaborator says,
oh, OK, width can be 2, 4, 8, 16, or 32.
Then we just go have five instantiations of this function, and then use some benchmarking
to pick the best one for you.
So this is how we get the very bottom layer of hardware abstraction, where the programmer
can write an algorithm, and then we let the programming language pick the best parameter
for you.
And this also allows us to avoid some of the performance problems of C++ templates.
For example, let's see, you have a generic function, add.
And for generality, we pass the arguments by const reference.
Passing it by const reference is fine for a large struct type thing that doesn't fit
nicely in registers like a string.
But then for something like an integer, this ends up becoming const reference to an int,
which for a trivial type like int is not very performant.
And so if this function doesn't end up getting inlined, what ends up happening is the ints
get pinned to the stack.
This is bad for performance.
With late elaboration and mojo, we can have late ABI lowering, which basically means that
the source code is not the same as the ABI.
And this makes language interop slightly more involved, but it's not that big of a deal.
But what it means is that for a generic function, like add in mojo, when the elaborator instantiates
the generic types, it can then change the calling conventions of the types to respect
the guarantees that it has.
So for a heavy type like string, it stays in memory, it gets passed around as a pointer,
it's nice and efficient.
But for an integer, it gets passed around in registers, in SSA registers, and returned
out as a function result.
So that's just an introduction to how mojo metaprogramming works.
Let's talk now about more how the cogent architecture works and some of the more unique details
of that.
One of them is that the entire mojo compiler stack is driven by the ORCJIT from bottom to
top.
And this gives us lazy on-demand compilations so you don't compile things you don't have
to.
It enables responsive tooling, and it turns out that having a JIT is important for something
like auto tuning in search.
And we get compiler caching at each stage of the pipeline, meaning that you don't need
something like Ccache to get code compilation caching.
Well we also use ORCJIT not actually as a JIT, we use it to generate static code, like
static archives and executables.
And in the ORCJIT, we've built a really dumb but fast linker that just takes a bunch of
object files, pulls out the symbols, and slams them together into a static archive.
For a linker, we do call into the system linker.
As we mentioned before, we have a pre-elaboration portable IR, but that also means that we can
serialize this into MLR bytecode, and that makes mojo packages architecturally portable.
A mojo package will contain this parser-level, source-level IR, as well as the pre-elaboration
IR, and optionally, you have the post-elaboration and pre-compiled code for various targets.
So what this means is you can ship mojo packages without source code, with just the bytecode.
The parser is able to take out this source-level IR and reconstruct metadata, like function
signatures and type members and so on.
And with optimized and pre-compiled code in the packages, mojo packages become portable
build caches.
So if you're on a common system like an M1 Mac and you pull a mojo package, it will probably
already have the pre-built code for you.
So what does a compilation with a package look like?
Well, if you start by importing a function from a package, the parser goes and reads out
the declarations from the package, it will then lower into the full pre-elaboration IR,
and the reason why you need the full parametric IR so that you can instantiate the function
again, and so that the elaborate can call the interpreter on pre-compiled code.
During elaboration, we don't re-optimize and re-stantiate all the functions, we just drop
them out with the post-elaboration IR into the MLIR module.
So that gives us LTO and MLIR, but I mean MLIR is pretty far away from link time, but
it's a similar idea.
But we actually trash these pre-compiled functions out of the IR before we go to LLVM,
and that has some interesting implications.
So mojo is a bit of an unusual, probably slightly controversial user of LLVM.
So LLVM is fantastic.
We love LLVM, we love everyone here.
But it's got a couple of issues.
The most standout of these is that it's single-threaded.
And what that means is on a modern system like an AWS 192 core machine, you get arbitrary
slowdown for compilation speeds.
You only use one core.
The other problem with LLVM is it's got a couple of passes that don't tend to be strong
enough for our use cases, and they're difficult to control and predict.
A lot of the stuff in LLVM was built for something like Clang, but in mojo, we'd really love
to be able to autotune and unroll factor.
So the good news is that MLIR is a thing.
So let's focus on the excellent strengths of LLVM.
LLVM is great at stuff like scalar optimizations from instance to combine, and other function
level optimizations like loop strength reduction.
We ended up disabling passes like the vectorizer, the loop unroller, and even the inliner, as
well as a couple of the other IPO passes.
And the solution is to replace them in MLIR where we get intrapass parallelism and push
many of these optimizations out into the library, which is something that Abdul will
talk about in a bit.
So what happens when you get rid of all of the IPO passes while you get to use LLVM
as a perfunction code generator, this gives you full code gen parallelism at a function
level across the entire stack.
And what that means is that pretty much the entire mojo compiler pipeline is fully paralyzed
except for the linker and the parser.
Parser could be paralyzed one day.
And that's really just the tip of the iceberg and what we could fit into one presentation.
There's so much more to mojo, and there'll probably be more talks coming in the future,
but for now I'll pass it over to Abdul to show you all how to write some fast code in mojo.
So going back to what Chris said at the very beginning, we had a hypothesis to begin with.
We want to write fast code.
That's why mojo was written to begin with.
We wrote things when MLIR, we've proven a lot of the tech.
Let's write things in mojo and let's show the performance.
So let's step back.
How does existing performance libraries, how are they built today?
Well, the short answer is whatever it takes to get performance.
There's no style guide or anything like that that's usually maintained.
That also means there's a lot of suffering because there's lack of tooling, et cetera.
So what people do is they write things in assembly.
Oh, great.
Please don't.
It's not a super productive programming language.
Others build compilers as C++ templates.
And God forbid, you mess like one of the sevens becomes a six,
and you get some nasty error message.
Others build C++ DSLs that generate ASMs.
Others write Python programs that generate assembly.
Others write Python templates that generate C++ templates that you feed into client.
And these are not research projects.
These are production libraries that are used today.
You probably used one already.
These are by the big companies.
And as a result, you're kind of losing a lot of things.
You lose on maintainability, debugging, tooling, and
becomes hard to develop and iterates on these performance libraries.
And that's why they call them performance ninjas, right?
You lock them in a room, give them some coffee, and then they give you speed up.
And we don't want to do that.
We want to reduce suffering.
The other thing is what happens is these performance libraries are pre-built and
shipped as kind of black box binaries.
And what that means is you've encoded, when you built ahead of time,
you've encoded all the hardware semantics, tile factors, etc.
In the library, you've made it into a black box, so
other higher level things in the stack, like a graph compiler,
cannot reason about what the library is doing.
You've also encoded specialized patterns, popular things like a resonant block or
a transformer block into your library.
And what happens if there's a transformer version two or a resonant 53?
You're kind of screwed in that domain.
There's other things, like there's no consistent API.
There's BLOSS, there's BLIS, there's 1DNN, etc.
And the distribution store is even worse.
There's a 1DNN and there's a ZNDNN.
But then if you are on ARM, you have to use something else as well.
So we want to solve all of these things.
And that's the reason why we built Mojo.
We built it to solve our problem of writing high performance libraries.
And the first thing we want to make sure is the developer is happy.
And they have all the tools that they need to be productive.
So rather than, as kind of Chris mentioned,
a lot of developers are not compiler engineers.
They can write libraries, they probably cannot go and write a pass and so on.
So let's put optimizations in the library and
I'll have some examples later on.
Let's also leverage what computers are good at.
So when I was in grad school, a lot of grad students were essentially
grid searchers.
They would just enumerate everything, try 50 things.
You lock them again in a room for a month and
they say, oh, the best tile factor is six and four and so on.
Let's not do that, let's use computers.
Computers are great at these sort of things.
They can scan things, you can do smart searches and so on.
So let's use auto tuning.
Let's use algorithmic selection and let's build that in the language.
And let's make sure that we have tooling to make these people productive.
Debuggers, how do you debug the Python template that generates C++
template that does something else?
It's hard to begin with to debug C++ templates.
Let's also build a language that's aware of the 21st century.
So SIMDs are a thing.
So let's be SIMD first.
Let's have scalars to be a degenerate form of SIMD, a SIMD of length one.
And make the SIMD parametric.
Let's also make the library, the one we ship, the standard library.
Have first class support for SIMD types.
Also multi-core is a thing.
So let's build parallelism and
asynchronous into the language as well.
And finally, we can have these nice things.
But sometimes people are like, I want my assembly back.
Or I want to use the LLVM intrinsic.
Well, all of this is built on top of MLIR and LLVM.
So you can get any of the intrinsics that you want.
You can reach into them.
You can also write inline assembly, which is kind of interesting given that
you're in a Python syntax language.
And you can target any LLVM back end.
So we're not like, we're standing on the shoulders of giants.
So we're leveraging all LLVM and MLIR back end infra to do that.
Let's also not build a DSL.
So even though some of our use cases is AI,
the programming language should be general.
I should be able to do some operations in Mojo, but
then do the plotting through our Python integration.
And that requires a general purpose programming language.
So one of the things that we made a decision on is let's make the kind of
compiler lean and let's move a lot of the optimizations and
the infra to be kind of functions in the Mojo library.
So we use very limited number of dialects in MLIR core.
And I know this might be controversial.
So we're not using vector, arith, lin-alg, or any of these dialects.
MVVM, any of these dialects.
We're only using the LLVM and index dialect.
And there's a bunch of reasons for them.
Sometimes they're not general enough.
Sometimes they don't fit in our use case.
They bring in a lot of code that we don't care about.
And there's like, for the lack of better terms, sometimes like cyclic
dependencies and so on.
And we, having a lot of the functionality in Mojo code means you
could iterate a lot more quickly.
So let's implement something like a vector dialect type of thing in Mojo.
So we have the simd type and we have a function called reduce max.
And if the size of the width of the simd vector is one,
we're just gonna return the scalar directly.
If we're on x86, it ends up like there's a LVM has an instruction for
horizontal addition or horizontal max.
That's not great for Intel.
So we could do a kind of a tree reduction thing.
But if it's floating points, we use a different algorithm and
we call it directly to an LLVM intrinsic.
This is compared to how the vector dialect lowers.
You're writing essentially the same stuff minus the special case for
x86 in essentially C++ code.
So we'll lower our directory to the LLVM dialect.
We could also do similar things like transforms.
So as Jeff mentioned, we disabled the LLVM vectorizer.
And instead, we have folks be kind of opt in to the vectorizer.
And we've implemented a vectorizer in these five lines of code.
So in one case, we've parameterized the function on the simd width.
And we're gonna call it for the specific simd width.
And in the leftovers, we're gonna call the function with a value of one.
So what does this mean to the developers?
It means that when you're trying to do an optimization,
when you're trying to add a new feature or target a new hardware,
the first thing is not, I'm gonna need to write a dialect or
I'm gonna reach into TableGen.
The first thing is, I'm gonna reach into Mojo and
I'm gonna do experiments and so on.
You can invent new optimizations, weird ones, incorrect ones.
Or maybe even point to optimizations that only works in this function,
in this domain, in this context.
This is all fine, but I care about performance.
I'm also a compiler engineer, but I ultimately care about performance.
So let's look at the performance of Mojo.
So one thing that people anchor on is the Mandelbrot set.
The Mandelbrot set, we have a blog post that was recently published.
But essentially, at the end of the blog post,
you end up with this 10 lines of code.
And if you run this 10 lines of code, you get 68,000 times faster than Python.
And you can kind of see the progression.
You can look at the blog post after this presentation.
There's a progression how to go to 90x faster all the way to 68,000 faster.
But at the end of the day, this is the code that you're gonna see.
But nobody cares about Mandelbrot.
You can just waste a cheat in Mandelbrot.
We're not cheating here, but nobody cares about Mandelbrot.
So let's solve a hard problem.
So let's look at matrix multiplication.
So matrix multiplication has been studied since a lot of us have been born.
There's also a lot more papers that were published this year about matrix multiplication.
It's also difficult.
The problem is dependent on the cache size and micro-architecture.
It's also a core part of LA-PAC and the ML system,
which means hardware companies to go in the top 500 supercomputers,
they have to optimize MathMol.
Or to be on the top of the ML perf, they need to optimize MathMol.
So a lot of effort goes into optimizing MathMol.
And these libraries have been developed for decades before some of us were born as well.
But we also don't want to write the Python template that generates C++ template that
maybe goes to Python again and so on.
Let's be principled.
So let's have a few kind of core things that we want from our MathMol.
We want a single source of truth.
We don't want to have multiple files.
We want to have one implementation.
We want it to be as fast or compete with state of the art.
Even though we can read assembly and we can program C++, let's not do that.
Let's write everything in mojo.
Let's make it fusible and do fancy stuff, support dynamic shape,
and work on multiple architectures, et cetera.
Our core hypothesis from the very beginning.
And here's what we ended up with.
So this is, again, a blog post from a few months ago.
We're actually faster than this now.
But we can compare against the best in class on their hardware.
So we're 1.4x faster than Intel on Skylake systems.
And this is fully dynamic.
We're not specializing on shape.
We're not doing prepacking.
I wish we were doing tricks.
It's easy to get these results if we were doing tricks.
But that's what we're doing.
And we have no inline assembly.
Unless we run the same code, but now on Intel, or sorry, on AMD,
we're 1.6x faster.
Do the same thing, but on ARM, we're 1.2x faster.
In fact, our implementation is about 2,000 lines of code.
This is a toy implementation, but this is putting everything together.
The interesting thing about this toy implementation
is this is what the llama.mojo, there's a public GitHub
repo that's using this.
And this implementation, using this,
they are beating the llama.cpp implementation that's public.
So with that, we've validated our hypothesis.
You can build portable performance libraries with less suffering.
And with that, I'm going to hand it off to Chris.
Right.
Give it to him.
Awesome.
So to wrap things up, Mojo is still early in development,
as we talked about.
There's still a lot more that is yet to be done.
One of the things we're doing that's, I think, pretty cool
is we're developing this all in public.
And so we have a roadmap.
You can go see what we're doing.
We have new releases that come out very frequently.
Now, one of the questions we get asked all the time
is, does a modular open source anything, right?
And so the answer comes in twofold.
One is yes.
We have upstream stuff all of the time,
including tons of core improvements to MLR.
Apparently, the interpreter that Jeff was talking about on Tuesday
is very popular, and so we can work on that.
And so we're very good open source systems from that respect.
Mojo itself, I think we'll take a little bit longer,
but we want to start the open source process later this year.
And so we'll start working on that.
And I expect that to take some time,
because we want to make sure that we get the core design really
right.
And not everything is best done with design by committee,
but we really want to see this thing scale and go
and have a big impact for the world.
So coming back all the way to the beginning,
we talked about AI and the AI engine and this kind of stuff.
Now, we don't have time to talk about it today,
but the cool thing about what Mojo means for the AI engine
is that you can actually tackle these heterogeneous compute
problems, because you can finally scale across lots
of different hardware.
And this is really cool.
We don't have time to talk about it today.
If you're interested, we have a keynote at the NURPS conference
later this year, where we'll talk about more about this in detail.
So with that, I think that's the end of our talk,
and we're very happy to take any questions.
If you'd like to check out Mojo, you can go to the web page,
read about it, download it, and use it today.
Thank you.
Thank you.
Thank you, Chris, Abdul, and Jeff.
Are there any questions?
Do you have mics in the alleys?
Good timing.
Yeah, thanks.
Thanks for the great talk.
My question is, I haven't seen anything
about GPU offloading in your slide.
Is that in plan, or what are you intent to do with it?
So there is one bullet point, actually,
on that there's so much more.
And yeah, Mojo does actually support GPU offloading
and split compilation like CUDA, but it's something
that we did not talk about in the presentation,
which we'd like to talk about in the future.
Yeah.
Thank you.
Hi.
You mentioned that you don't need to use Ccache,
because you kind of mentioned that.
Can you elaborate that a little bit?
How are you guys dealing with caching?
So it turns out that MLIR has a nice serializable format
called bytecode.
But bytecode provides a predictable hashing.
And so we can use MLIR bytecode as the form
to hash and cache compiler transformations across the stack.
OK.
Thank you.
We also didn't have time to talk about this whole distributed
cache backing this thing.
And there's a whole bunch of fancy stuff put into it.
How are you doing the autotuning?
Is it offline, or is it dynamically online?
And how do you define the objective function for the search?
Yeah, so you have a choice.
You could do it offline or online.
If you compile to that O file, you've done it offline.
The objective function right now is something
that the user provides, because it's data size,
dependent, hardware dependent, and so on.
So it's up to you to define that.
We do provide a benchmark module so that it
makes benchmarking a lot simpler.
And that allows you to do that.
If you're doing it online, how do you control for variation
in data, or do you rely on?
So the benchmark library that we provide
has a good number of iterations and so on
until you get stability and so on.
So it handles that.
Oh, so it's not actually in production autotuning?
We use autotuning today, so I don't know what.
So there's core capabilities, then there's future stuff also.
I mean, one of the things that it's designed for,
but we haven't actually done, is send the IR to an FPGA
and do evaluation remotely, and then pull it back,
and things like this.
Or a simulator.
Exactly.
There was a point in the slide about optimization in the,
providing optimization in the library,
as opposed to the compiler.
Are there any, maybe I misunderstood this,
but from my understanding, it's possible
to come into performance pitfalls,
because C++ has built in likely, built in unlikely,
and then you can, it's really easy to misuse those
and end up in a situation where your code is slower
than without these kinds of annotations.
So my question would be, what happens
if a user-provided annotation conflicts
with something that the compiler
would also have done at the same time?
Well, so from a compiler design perspective,
one of the things Jeff was talking about
is we've removed, not all, but a lot
of the super unpredictable things in the LVM optimizer.
So our goal is to give full control
and predictability to the programmer,
which is very different from the make-spec-go-fast
kind of approach to compiler design.
And what that does is that gives you the ability
to then go and design library features
that do things like, you know, you can,
Julian, you can talk about
some of the crazy stuff you've done.
What's also important is that we have these abilities
to say, please vectorize this loop,
please unroll this loop, and so on.
But not everyone who's writing, say, application code
is going to think about vectorizing every single loop
and auto-tuning every other loop.
So what's important is that we provide control
to the users who care, but also provide
a default experience that is good and optimal
and the compiler does its best.
But the important thing is what the user says
will always take precedent.
And that's how you get control.
Sometimes a compiler does things
and you end up with code that says, you know,
optimize, compile the section of code with dash O zero
type of stuff.
And you kind of want to opt out of compiler optimization
because it's interfering with how you laid out your code.
Are there any plans?
I have a follow-up question.
Sure.
Okay, come afterwards.
Last question, please.
Hi, so you mentioned that you only use two dialects
in Mojo, LLVM and index dialect.
Two upstream dialects.
Two upstream.
Okay, so you don't use other things
like affine and stuff, which means that
if you want to use hardware specialized libraries,
then the programmer has to do different tiling
for ampere versus hopper versus Volta and so on.
So isn't that just pushing the burden out
from the compiler and high level stuff
into the programmer?
Because you're going to now have very hardware specialized
performance libraries and then people who write this thing
would have to understand the architecture really,
really well.
I think the thing is that they're more likely
to understand the architecture really well
than the compiler engineer, right?
The compiler engineer has to have two things,
writing C++ on CPUs that target GPUs.
This is like, I'm a CUDA programmer, I'm laser focused,
let me target hopper.
So that means that the people writing high performance libraries
for very specialized accelerators,
they need to be experts at those accelerators, right?
Right, so they need to be expert in one area,
not two areas.
So the goal is give the current programmer superpowers.
But that's our approach to it.
As Jeff talked about, Mojo can talk to any dialect
if you want to.
You can use that find in Mojo.
We can plug and extend the system with dialects as well.
So that's always an option.
So that is a conscious decision.
That's really the conscious decision you're making
is that you're going to get experts to do the performance
library and they will just work.
Well, so this is the thing.
Current libraries don't scale because of the magnitude
of the problem and the cross product
of all the different integrations and all of the stuff
that current libraries struggle with.
But there are more current programmers
and performance engineers than there are compiler engineers
by far, right?
And so it's really about enabling the talent
that actually knows how to do all this kind of stuff
versus having a compiler engineer in the loop
that becomes a bottleneck.
Thanks.
We'll be around as well throughout the conference,
so feel free to yank any of us.
Thank you, Chris, Abul and Jeff.
So let's thank the speaker again.
Thank you.
Thank you.
Thank you.
Thank you.

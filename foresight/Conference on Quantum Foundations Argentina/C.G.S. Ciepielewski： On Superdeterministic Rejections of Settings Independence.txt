Okay, we are back with the next speaker of this morning session, who is Carlos Gerardo
Czepelewski of the Universidad Nacional Autónoma de México and he will speak about on super
deterministic rejections of settings independence.
Please Carlos, go on.
Thank you Federico, and thank you to all the organizers of this conference on quantum
foundations.
I'm very grateful to be here and to be able to share with you work that I find particularly
interesting.
This work was made with collaboration with Elia Sokon and Ernest Sudarski, who are also
doing the call, perhaps even in the questions they can offer their insights.
Okay, this work is going to be published next year in the British Journal for the Philosophy
of Science.
So okay, as a way of introduction, let me say a little bit about the motivation of this
work.
Because of the presence, the existence of Bell's theorem, and the fact that we have
performed many, many times the relevant experiments related to such a theorem, it seems that there
is some, there are some non-local features in the universe.
And we will see the details in a minute, but as an introduction, let me just say that
if one wants to avoid this conclusion, it seems that possibly the only way to do so
is to deny one of the assumptions in the theorem called settings independence.
It might be that there are other assumptions in the theorem, but I would say that in most
cases, I do not see any way of denying the other type of assumptions, or in general,
I do not see other important assumptions that appear in the theorem.
Then the work of non-locality becomes the work of seeing what reasons do we have in
favor of settings independence, and what reasons do we have to believe in some of the ways
of denying such an assumption.
And in working on this evaluation of arguments, we stumbled upon a very interesting model
that is going to be the subject of today's talk.
Okay, so this is the structure of what we are going to do today.
We're going to do a quick overview of Bell's theorem.
Then we will move to discuss the assumption of settings independence.
We will pay special attention to what reasons do we have to believe that something like
settings independence is true, and what kind of reasons or what kind of models might be
able to still say that such an assumption is incorrect.
Finally, we're going to introduce the local pilot wave, this model that I think is really
interesting.
So let's start with Bell's theorem and a very quick overview.
I think that most of the people in this call and in this conference are familiar with the
theorem, but it will be a good exercise to go into the overall structure of the theorem
and what it states.
So we have a typical Bell type experiment where we prepare an ensemble of two particles
and we send them to distant locations.
In one location we have Alice's lab and in the other we have Bob's lab.
The capital A and capital B stand for the results of the experiments that each one performs.
The results only can be up and down.
Alice and Bob are measuring the spin of the system that is sent towards them.
Of course, I don't know if I mentioned this, but the ensemble is of systems that we would
describe singlets in quantum mechanics.
Lowercase A and lowercase B stand for the settings that Alice and Bob use, for example
the angle by which they put the Steng-Erlach or another measuring device they are using.
And lambda is going to be a description, a complete description of the system that Alice
and Bob are measuring.
So we are interested in this experimental setup.
And Bell's theorem assumes two things regarding this general setup.
The first thing is called local causality.
And the idea of local causality is that suppose we have two distant regions and suppose that
we have a complete specification of whatever is happening in a slide of the past hyperverse
surface of this first region where something happens, a bevel kite occurs, some event occurs.
And the idea of local causality is that if I have information regarding sigma and this
information is complete, then whatever is happening outside in that other distant region
must be irrelevant.
In other words, the probability of whatever is happening in the first region must be independent
of whatever happens in another region.
Because I have conditionalised on sigma.
In the theorem, this assumption takes the form of factorizability.
I'm just going to assume that local causality implies factorizability for the purposes of
this talk.
So the idea is that the probability of Alice and Bob once we have the measuring settings
and the state of lambda, it goes like this.
We can factorise the probabilities.
The other assumption, and let me just state it, we will come back to it in a moment.
The assumption is that the distribution over this complete specification of the system
that Alice and Bob are going to measure is independent of the settings that Alice and
Bob use to measure their respective particles.
So Bell's theorem says that if one assumes factorizability and assumes settings independence,
then the expectation value for the product of the results of Alice and Bob, which is
this, is going to obey the inequality.
This is a particular inequality that can be derived.
There are many others that can be derived.
The interesting thing, so far Bell's theorem just is a derivation of this type of inequalities,
but it becomes interesting when one notices that according to quantum mechanics, this
combination of expectation values should be equal to two times the square root of two
and not less or equal than two.
We have performed the relevant experiments and we always find that quantum mechanics
seems to be right.
So the conclusion seems to be that, first, that no local theory can reproduce the predictions
of quantum mechanics and because of the experiments, it seems that the world has some non-local
features.
Okay, that is the overview of Bell's theorem.
Now we go to describe this assumption of settings independence and why it would be, why we would
consider it a reasonable assumption.
So as I said, the assumption is very simple to state.
It says that the distribution of the lambdas is going to be independent of the settings
that Alice and Bob select.
Something that I forgot to mention is that in the experimental setup we are imagining.
We want to be as general as we can.
So the idea would be that each time we prepare a system in a singlet, we are not going to
assume that there is a unique, complete description that is appropriate for that situation.
At much, we are assuming that each time we prepare a system in a singlet, we have a distribution
over the states of lambda.
And well, then settings independence is the idea that this distribution of whatever is
really happening is independent of how Alice and Bob measure the system.
Two comments regarding some confusions that have a reason regarding this assumption.
The first one is that this assumption has nothing to do with the free will of either
Alice or Bob.
There is a long tradition of people being confused because of this and there is even
theorems regarding the free will of the particles involved.
That seems to be a confusion.
It's easy to see that it is a confusion when one realizes that this is just an assumption
regarding the statistical character of the distribution of lambda.
Another confusion has to do with how to understand the lambda that appears at this moment.
The thing is that lambda at this point must be understood as the complete description of
the singlet of the system that we have prepared and that we call the singlet.
This is before I mentioned when I was talking about the assumption of local causality that
there is this complete specification in the past of region.
And some people think that lambda should be understood as this complete specification
of the hyper-surface in the past of region.
But in the theorem, lambda is not that.
Lambda is just the complete specification of the system that Alice and Bob are measuring.
Why would someone believe that an assumption like setting independence is reasonable?
The answer is that we have performed the relevant experiments and we have used a lot
of methods to randomize how we choose either A and B.
So we have used random number generators.
We have used the digits of pi.
We have used information coming from distant quasars.
We have used even stranger things like the binary strings that can be extracted from
information of popular movies and TV shows.
For example, we have used information coming from the movies of Star Trek and Monty Python
and the Holy Grail.
And we have used that strings of binary digits to select the settings that Alice and Bob
are using.
And finally, we have even used inputs from a lot of people to perform these experiments.
Why is this relevant?
Well, the idea is that by choosing the settings randomly, we are, in effect, erasing any kind
of correlations that we might expect that are present between the settings that we are
using and the state of lambda.
In the absence of some reason to believe that the correlation must be preserved, we think
that these kind of processes ensure us that the settings are being chosen randomly.
I hope that that is the main reason why we think that something like settings independence
is a good idea.
And as it might be obvious to some of you, it might not be obvious to some of you, but
an assumption like settings independence is present each time we perform an experiment
in the sense that we assume that how we are measuring something must be independent of
the system we are measuring.
Let me be precise.
We need to assume that it is possible to make those measurements in a way that is independent
of the system we are trying to measure, okay?
Okay.
That's good.
Okay.
Still, there are two possible ways of denying settings independence.
The first one is to assume a retrocosal model and to assume that it is the selection of
the settings by Alice and Bob that causes changes in the lambdas that we have prepared.
So the idea would be something like what is depicted in this diagram.
The idea would be that we prepare the singlet state and it moves towards Alice's lab, for
example, and when Alice decides or uses some device to select the setting IA, that information
travels backwards in time and then reaches Bob.
This would be a way to deny settings independence.
We are not going to talk about this possibility in here.
We're going to talk about the second possibility of denying settings independence, which has
the name of super determinism.
The idea would be that there is a common cause between all the elements in the experiment
between the state of lambda that we are preparing and whatever is happening in the locations
of Alice's lab and Bob's lab.
Notice that because we have performed with these experiments using information coming
from these processors, the common cause must be really, really back in time.
And it must be a really particular common cause because it is somehow preserving the
correlations between the settings and the state of lambda, even though we have used
a lot of different things to try to randomize how we select AMB.
It would be something like akin to having a system that undergoes physical mixing, but
preserves a particular correlation, even though there is a lot of intricate and ever pressing
different physical processes that we would think would erase any type of correlation.
So these two possibilities seem to most people far-fetched.
In particular, super determinism looks really bad, at least because of three reasons.
It seems that a theory that a super deterministic theory would be too complicated.
Why?
Because it would somehow be able to correlate these ways in which we randomize the settings
of Alice and Bob with the state of lambda, and that seems too cumbersome.
Second, it seems that doing this kind of move where we are saying that the correlation will
be explained by postulating there is a common past is like abandoning the project of finding
an explanation of whatever is happening.
Suppose we have a correlation between two variables, it is always possible to say that
the correlation was there because the initial conditions of the system were interested in
were such and such.
It seems that we are not giving an explanation, we are just assuming something about the initial
conditions of the system.
Lastly, it seems that such a position would jeopardize all experimental science.
As I said, an assumption like settings independence is present in all type of experimental procedures
we perform.
And if we want to deny settings independence, in this case, what makes us stop there?
Perhaps we should abandon this assumption and we should abandon the enterprise of doing
science in general.
So that is settings independence.
Those are the reasons why we believe that it is a reasonable assumption.
And what are the problems that we might encounter with a super deterministic theory?
I will now present our model and let me start by pointing out the trick that we are going
to use.
We are going to turn standard pilot wave theory if I am not going to give the details of this
theory but during the questions perhaps sometimes it will come back again.
But we are going to turn this standard pilot wave theory into an equivalent but a fully
local model and we are going to do a trick to do so.
And it will be useful to have a name for this trick.
I am going to call it a linear smooth, I am not going to explain why, but the idea is
to put in each physical space, point in space the information regarding all the other points
in the universe.
It should be clear that by doing that it is possible to transform the dynamics of the
model we are constructing in something that is local.
The idea would be that each point has the information to do whatever it does without
the presence of any other particle.
So in standard pilot wave theory we have two equations of dynamical equations, the first
one is just the Schrodinger equation and we have also the guiding equation that gives
you the dynamics of the hidden variables that theory proposes which are just particles
and their positions.
We are going to use this linear smooth and what we are going to do is to suppose that
at each point in the space there is an internal, there is a set of internal degrees of freedom
which are going to be a wave function field and a position field for each point.
And doing so it is possible to obtain a dynamic that is fully analogous to the previous two
equations but we are now dealing with wave function fields and the position fields.
And two comments regarding this move.
The first one is the internal degrees of freedom at each point they behave as an n particle
pilot wave system.
So the idea would be that for each point in physical space we now have an n particle typical
pilot wave system.
And the second would be that the second observation would be that the theory only contains local
bibles in the sense that it is not describing some interaction between whatever is happening
between in these points the dynamics is completely described by what is happening at each one
of these points and therefore it is completely local.
To connect this the model with whatever we find in physical space we specify mass density.
This allows us to say that a particular point in physical space is populated by matter only
if the particles in the internal degrees of freedom of such a point in physical space
have a particle being occupied.
So it would be imagined that you have a point in physical space there is the internal degrees
of freedom and in those internal degrees of freedom there is a particle that is at the
same point that this point or that this physical point is then we find matter there okay.
Okay crucial observations for generic conditions we the model is we don't have much okay.
We don't have violations of the belt inequalities and we don't even have predictions of the
predictions of quantum mechanics okay.
But it is easy to see that under very particular initial conditions that is if the wave function
field and the position field are the same for each physical point in space then we
have three clear consequences the first one is that the points become suddenly coordinated
and we have a dynamics that seems like it is describing some universe where things interact
causally but in fact we just have points that are being coordinated.
Second we are able to recover all the predictions of the standard pilot wave theory and three
where we have the violations of belts inequalities two and three are related since in pilot wave
theory we have violations of belts inequalities okay.
I'm not going to go into the details of this but the question becomes well how can we achieve
such an such an homogeneity condition and in the paper we discuss three ways of doing
it the first one is by putting it by hand the second is by imposing it imposing it as some type
of low-like constraint and the third one is by doing doing so dynamically perhaps during the
questions we can talk a little bit about this but okay regarding the the problems that
super deterministic theories seem to have what can we say about our model well I think that
it is interesting to note that our model shows that it is possible to create a super deterministic
model that doesn't have the kind of problems that people feared as it should be obvious that the
conditions required to have a workable model are not as complicated as people thought they would be
the second point is that well it has as much explanatory power as the standard pilot wave
theory has and the third is that the the the model shows that explains why settings independence
is violated for belt type experiments but it doesn't say that settings independence should
be violated in other types of experiments in science so there is no threat to experimental science
there are some problems with the model the the model is is really ugly we are not trying to argue
that this is a good way to do physics in the world we are just showing that it is possible to
construct this model and that it is a a good place to continue having the discussion regarding
whether or not something like super determinism is a reasonable possibility in more detail
the the the model seems to require an absolute rest frame and second there seems to be a
disconnect in the model between what we can call the the ontology of what is really happening
according to the theory and what what what we can have contact with so the theory postulates this
internal degrees of freedom for each point in physical space yet we cannot go and see those
internal degrees of freedom in fact the theory says that we cannot do that okay we because of
the mass density we are explicitly says when can we note something about all of this in
internal degrees of freedom and it's under very particular conditions lastly there is an obvious
massive enlargement of the ontology in this model and so okay let me finish with a list of contributions
that this work has done and in particular that can be seen as consequences of constructing this
model so we have presented a model that is completely local and is able to reproduce the
predictions of quantum mechanics it does so without being too complicated without losing
explanatory power and without making all experimental science obsolete so the problems
with this model were not the ones that many people feared at least there are not these three
problems and so as a proof of concept it shows that it is possible to construct a super deterministic
theory in a way that keeps the physical details of what is happening and finally the model does this
all of this by very cumbersome methods and doesn't seem to be to offer any advantage over
other alternatives that are non-global that's it okay thanks Carlos thanks for this very nice talk
we are almost on time so if someone has a very short question you can post it now otherwise
I invite Elias to start sharing cream but if someone has a question for Carlos please go on
I had of course questions but I think we are we are quite on time but my question was something
like we can of course speak later or by email or something like that but you claim that the world
is non-local right but then how can you have in a non-local world device measurements independence
for example you will claim that in belly inequalities you don't have measurements
independence or something like that because it is non-local and then in your definition of super
determinism I don't understand because if you want super determinism or if you want to avoid it
because when you post a model like in Leibniz everything is super there is no in Leibniz
metaphysics there is no the measurements independence because the monads every monads
knows what to do and knows what the rest of the monads are doing so everything is super determined
by the the most perfect world chosen by God so it's not clear for me what what you at least
the conceptual definitions right what do you want you are completely right in the sense that if you
believe in something like the the universe of Leibniz's monads or the kind of universe we are
describing with this model yes that's a way to deny settings independence and yes that is a way to
to to keep locality but I think that most people would agree that that's a price to high to pay
I think I think

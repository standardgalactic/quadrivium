The number of users who use ChatGPT, that's incredible.
But I think it's sort of worth asking ourselves,
is that sort of quote unquote the killer applications
that we were waiting for?
ChatGPT does feel like a fairly simple wrapper
around our sling model, because that's what it is.
OpenAI has done fantastic things to make it safer
and make it more useful by tuning,
I think, what's really great.
But I think it's worth asking if that is actually
the killer application, why is it a killer application?
And the answer might actually come out
that maybe it actually isn't the killer application
that we were waiting for, in which case,
what is going to be the killer application?
That's really going to add value
in a much more generalizable way.
Welcome to AI in the real world.
I'm your host.
My name is Joanne Chen,
and I am a general partner at Foundation Capital.
I work closely with startups
that are reshaping business with AI.
In this series, I'll be holding in-depth discussions
with leading AI researchers.
We'll explore how state-of-the-art AI models
are being applied in real enterprises today.
To kick things off, I'm excited to speak with June Zempark,
a PhD student in computer science at Stanford.
June works at the intersection of human-computer interaction
and natural language processing.
He is best known for his research on AI agents.
We break down how AI is transforming agent design,
share advice for builders working with these models,
and unpack why we haven't yet found
the perfect killer app for AI agents.
Here's our conversation.
How are you?
Good, what about you?
Great seeing you again.
Good to see you again.
It's been a while.
Because the on-conference was last,
I want to say, June.
June?
May or June?
Yeah.
Last June, May or June.
Wow, time-wise.
And the world has changed.
I think that agents have finally made its way
into real enterprises with real use cases.
And it was not, back then, it was a lot of,
like, what could this be, right?
Thanks to you as some of your work,
which is why I'm super excited
to have this conversation together.
Especially right now,
since enterprises are sinking in a real way
to adopt.
So I thought, who can we chat with
that would have a really interesting perspective?
And that's why we reached out back to you.
So I really appreciate the time.
Of course.
Thanks for having me.
Do you mind maybe just to start
giving us a quick rundown of what's happened,
maybe some of the background that you have
building this technology?
Yeah.
So let's see.
So do you want me to just sort of speak about
sort of what has happened in the past six months,
or sort of what would be interesting for you?
Just a brief overview of what you worked on
and also what's happened in the last six months
to a year in terms of evolution.
Yeah.
All right, that sounds good.
Right.
So I guess I'll do a quick intro.
So I'm sort of, I'm a PhD student here,
sort of working in the area of HCI and NLP.
So as you know, sort of the work that we've done,
I think the one that I'm sort of mainly known for
is this paper called Generative Agents.
And Generative Agents in particular was a project
that I tried to ask,
can we use our language models to create general agents
that can populate a simulation world?
Right?
So if you play something like SimCity or Sims,
can we actually create these NPC-like characters
that would actually flood into the city
and actually live like humans?
And by definition, it is sort of everything
from how they would wake up in the morning,
talk to each other, form routines and relationships,
all the way to creating basically communities
and emerging social dynamics.
And sort of my interest in this area
really stems from this idea.
So this is sort of what people at the intersection
of human-computer interaction and natural language processing
in which learning like to ask,
which is we now have these really amazing models
like our language models and foundation models,
the question really becomes,
what are you going to do with them?
Right?
These models are new and they're great
and we think they have great capacity,
but are they really going to enable us
to do something that's quite new and unique?
And that has been sort of the focal point
for a lot of the research that I do.
And ultimately the conversation that we got down
towards this idea of,
well, these models are trained and brought data
like the web, Wikipedia and so forth.
So they can actually be used to generate
a lot of believable human behavior
when you're given a very micro context.
So can we actually piece this together
to create human-like agents,
which is something that AI more broadly
has envisioned since its founding days.
And we decided that this is the time to do that.
And so that's how we got to where we are.
So that's the generative agents.
And this is the paper that was published in April last year.
We put it on archive in April
and was officially published November.
Which is crazy how much the world has developed.
I'm curious what initially motivated this topic for you?
I'm sure you had lots of different options
in terms of what to research and study.
Why did you decide to focus on this?
Yeah.
So ultimately, it really was the question of
what will large language models,
these new models that are being trained,
really going to enable us to do.
And when I started my PhD was around like 2020.
And that was when GPT-3 was just about to come out.
During my first year, we wrote a paper called
Foundation Models, which sort of made this observation
that there's going to be this new wave of models
that's going to come out,
where we're not going to be training these models
for a specific task,
but rather we'll be training for a modality.
We're going to be training this language model
that can process language and so forth.
And we thought that was going to be a big opportunity there
in terms of what we can do with them.
But the question of what are we going to do with them
was incredibly unclear.
So really our first instinct as sort of researchers
and more machine learning in the NLP community,
where we sort of were drawn to was this idea
of can we do classifications?
Or generations with these models?
And seeing that these models could do that was really exciting
because we didn't train these models to do that,
but they could.
But more from the interaction perspective,
doing classification and simple generation
was something that we already knew how to do.
So that did them feel fundamentally new.
So really the question again became
what are we going to do that's going to be truly new
and transformative in the sense of interaction?
So that's what really drew us to look for these kind of ideas.
And again, that's we thought
simulating human behavior in general computational agents,
that felt like a big problem because in part
because it's something that, again,
our community had wanted for many decades.
It was sort of the idea that people in more
the cognitive science field that really inspired
the early AI research, like Alan Newell and Albert Sass.
Simon, these folks were asking.
And we were certainly inspired by those ideas.
And of course, we thought it would be a lot of fun
because we sort of grew up with sims, Pokemon
and these kinds of games in the 90s and early 2000s.
And we were certainly inspired by those games as well.
I love those games as well.
And it's nice to see some of that play out in the real world.
I agree. I think games are fun in the sense that,
you know, I think they are inspirational in many ways
because they are very forward-looking in many ways, right?
Because you can be a little bit more playful.
And I think research can be in many ways playful,
especially when you're trying to do really forward-looking research.
So it certainly is a big inspiration.
And I was just going to sort of end that comment by saying
that I think it's worth asking for us as a community
what's going to be the new sort of quote unquote
care application of these models.
In the sense that when we had personal computers
in the early 80s and so forth,
the computers were very cool.
But what really made them into household applications
were the existence of this,
what we would now consider as killer application of PCs,
like Microsoft Excel,
that really made tabular information usable and scalable.
I think we, Luris Language Model Community,
or should also be looking for those kind of ideas as well,
because that's going to be ultimately
what's going to really transform the user experience
around these models.
And I think we're seeing some great usage of these models,
but I think there's a lot more to do going forward.
Makes a lot of sense.
When you look at what's happened since April, right,
a lot of things have changed.
We have new LLM capabilities.
We have a whole flurry of startups building in this space.
Could you maybe summarize what you've seen?
Right.
So, right.
So, Agent Sotini has been a big thing,
especially first the latter half of 2023.
This is how I'm seeing it.
Agent Community, it's sort of the way I view it,
has split into two communities, I would argue now.
So, maybe it might actually make a little more sense
to really talk about the history of agents,
because agent became a big thing last year,
but this is not a new idea in and out of itself.
Right, even in the commercial space,
we actually had agents like Microsoft Clippy,
I'm not sure how many of us will actually remember that,
but there used to be these agents in sort of our industry
and in research.
So, this is certainly not a new idea.
So, if you go all the way back,
so we had agents like Clippy,
and in many ways these agents,
especially in the reinforcement learning
and machine learning community,
agents were these elements that basically
could simulate human behavior.
I think that is ultimately sort of underlying thesis,
but many of the agents were given tools
to automate certain tasks.
And the task it were meant to automate
were tasks that are not simple, right?
It's not something like you're running a for loop
with your Python code,
but it's a little bit more complex than that, right?
It operates in much more embodied spaces
or in spaces that we often operate in, right?
The web, right?
Can it, the simplest example
with these kind of tool-based agents
are can it order me pizza?
Can it buy plane tickets?
And those might sound simple,
but we know from our experiences
that even ordering pizza actually
does require multiple steps, right?
We need to travel to certain websites,
we need to look through the menus,
actually make the payment,
and deal with sort of entering your address and so forth.
So that was one genre of agents
that already sort of existed for a long time,
or I would say all genres of agents sort of existed,
but that was one genre that was highlighted in the past.
So you see things like Clippy is also in that genre as well.
You're a Microsoft Office user,
Clippy would try to automate some tasks for you
based on your prior interaction with the software.
Another set of agents was this idea of simulation agents,
or agents that were created-
To clarify on that point,
those agents are single agents, correct?
They can be single agents,
they were often implemented as single agents, that's right.
I don't think by definition
they actually had to be single agents.
So you'd actually try, you're now seeing,
at least in the research,
you're starting to see glimpse of people
is trying to imagine what would it look like
for these agents to be in a multi-agent setting.
So research paper that I remember coming out
after Generative Agents was basically,
what if you have a company of agents, right?
There's going to be a CEO,
but there's also going to be a designer agent
who works in some other aspects,
there is going to be editor in this company,
and those are still much within the literature
of what I would call tool-based agents, right?
They're trying to automate some complex tasks for the users.
And I think there's going to be a lot of
sort of really big opportunities in this space,
that's something that people have been working on
for a long time, for all the right reasons.
Now, another community that has formed,
but to some extent actually has a slightly different route,
is agents that were created for simulations.
And these agents were certainly a part of games, right?
In the past we had Sims,
but we also had these NPC characters
that we could interact with.
Now, those NPCs and agents back then were very much,
it was simpler agents that were either rule-based,
there were some reinforcement learning agents
back then as well in that space.
But another one that we could usually think about
were agents that were used basically in social science,
economic agents, or agents that would simulate
our policy decision-making and so forth.
And those agents were also a part of this literature.
And what we're seeing today is,
we're one recognizing that Lawrence Lynch model
is simulating human behavior.
So it touches on all these agents,
that it can be a foundational sort of architectural layer
for creating all these different sorts of agents.
But in terms of our initial application spaces,
we're seeing this split,
where there's one community
who's now deeply interested in agents using tools,
but another community that is deeply interested
in this idea of can we simulate?
And this is where I would say like multi-agents
and as well as personalization
is really starting to be highlighted in the simulation space
because it's a little bit more directly incorporated
to the idea of simulations.
Who are we simulating for?
What are we simulating?
Who are we simulating?
And by definition, simulations often happen
in this multi-agent space.
So those are the two communities that you're starting to see.
So generative agent certainly stands on the far end
of the simulation-based agents,
whereas some other projects
that were also really cool last year,
I think a lot of sort of open AI, GPTs, I would say,
are another end of the simulation agents
or another end of tool-based agents.
So those are the axes that you're sort of seeing right now.
And sort of end by saying,
my hunch actually is again,
because they all start from the same technical thesis
that we can simulate human behavior,
they will merge in the end.
I don't think they will be completely separate thesis
like five to 10 years down the line.
It's more going to be the question of
where are we going to make our short-term bets
and what's going to be an interesting
and meaningful application space in the next two to five years.
So that's the field that I'm seeing
and how it's developing right now.
Before we maybe go into that,
could you maybe describe how LLM specifically
has affected the, especially the latter cohort, right?
What is the before and what is the after?
And what is the magnitude of improvement
because of this technology that's now cheap enough to use?
Right.
So Lawrence-Lenge Motor is really what made this possible.
That is really the fundamental fact that we needed.
In the past, when you wanted to create,
and this goes for both types of agents,
tool-based and simulations,
what you really needed was,
you basically needed rule-based agents.
That was the most common.
And rule-based agents are sort of a more sophisticated way
of saying we're scripting all the behaviors.
So imagine you're building an NPC for a game.
A human author would actually write every sentence
that the agent would say to the user, for instance.
Human author would actually describe in either code or language,
if this happens, you do this.
So you basically design all the possible behaviors.
Now, that is expensive and not scalable, right?
And that was the fundamental block that we had.
Now, tool-based agents had similar issues
that in many of the contexts it had to operate,
it's not a very generalizable tool.
So if you sort of see how clippy
or even some of the agents that we're using today,
very simple types of agents actually
are already embedded into our daily usage.
So you may have used Google spreadsheet or Google doc.
It would auto-complete in some very rudimentary way
that actually could be considered in some ways
an agent in this direction of tool-based agents.
And the rules they were using so far were very simple.
It's not exactly rule-based,
but it is something that was very much hard-coded
into the agent's behavior.
And there was some learning going on,
but there were very strict or simple statistics
that we were using.
What large language model changes
is large language model gives us a single ingredient,
which is given a micro-context, micromoment,
let's say I'm sitting in this room talking to Joanne
and about let's say generative agents
or simulations and so forth.
Given that micromoment description,
a language model is extremely good
at predicting the next moment, right?
So what are the reasonable set of things
that June might say in this particular conversation
given what he knows?
It's very good at doing that.
That on its own is not a perfect agent
or it's not the complete ingredient
that you need to create these agents
that are meant to live for many, many years or decades.
But they are the right ingredient
or building block that we need it
because that can be used to replace
what was in the past, manual authoring.
In the past, we had to manually author
all the possible sequences given any micromoment,
but large language model can come in.
So given that ingredient, what we really could do
is bake in long-term memory and some reflection module
on top of it and planning module.
So given the micro-ingredient
plus an agent architecture that we give it on top of it,
these agents can basically now start to function
as something that can operate in that
in a world that's much like ours
with a fairly decent degree of long-term coherence.
So that's where we are
and that's really the difference it made.
And I'd say this is sort of a zero to one difference,
not a degree difference
because before large language model, this was not possible.
What else is, so we, large language models gave memory,
gave context, gave interactions to these agents.
What else in a perfect world would these agents have
in order to better mimic the real world?
Like what's maybe in the next stop, just out of curiosity?
Right, so to clarify, large language model
doesn't actually have,
so large language model provides one element.
It's the micro sort of a module
for predicting the next sequence.
It is the agent architecture
that actually ends up giving the memory and planning ability,
but those two pair becomes a fantastic combination.
Now, going forward,
what I do think is going to be interesting are,
so right now we're using large language model,
but we may have all noticed that things like chatGPT
can now not only do it just language,
but also other modality like image.
I think that's going to be really interesting, right?
So right now, let's say if you,
and this is sort of based on my prior work
called Generative Agents,
and we had this game world like sim ad
that we call a smallville,
the way these agents perceived and operated in their world
was basically by translating,
and like our system translating
the visual world into natural language.
So we would tell the agent,
you are in your apartment,
or you are in the kitchen talking to someone.
So we would actually take the visual world
and use our system to translate the visual world
into natural language,
and then feeding it to the agent architecture
that would use our language model to process this.
But now with these models being able to deal
with multi-modal aspect,
we might actually be able to bypass that phase
and go straight from here is the visual world
or space that you're seeing right now.
That is your memory, now act on it.
I think that's going to be potentially very powerful
because in part, image is much richer to some,
it conveys a lot more.
I do come from natural language processing background,
at least that's my other half of my sort of academic background.
So I have bias towards believing
the natural language is profound,
and I think that we're going to be,
that will be the case going forward as well.
But image does offer something
that just language alone does not.
So image is going to be a big thing.
Now imagine the future video is going to be a big thing as well,
then gradually the more these agents
will basically increasingly get more powerful
as this new modality gets piled on.
So that's something that we should be looking forward to.
That's great.
What are some on the downside,
what are some of the limitations
that you're seeing in terms of these agents,
especially generative agents?
Right.
So there are limitations that I can mention
just about sort of in the context of our work.
And then I think there are going to be interesting limitations
that are much more application specific.
So for generative agents today,
certainly the technical limitation right now
might have to do with things like,
so you're using whether it's an open source.
So right now we use OpenAI's model.
OpenAI has actually done a lot of work
to make the model safer.
And I think OpenAI, I think that was the right approach
in the sense that what they really wanted to create
was these chatbots or agents or chatGPT
that are a safe tool to use for most people.
Now, if you want to run a simulation
or create truly accurate and believable agents
with something like chatGPT, however,
that could become a limitation
because what we really experience as humans
is we fight, we sometimes have conflicts,
we disagree with each other.
And that might not be something that's something
like chatGPT that's been fine-tuned
to not behave that way to remain safe.
It's something, it might not be something
that these models would try to surface.
And that could be a potential block
in creating more accurate, more believable simulations
or agents for that matter.
So that's something that is one limitation
right now that we're seeing.
An interesting way to tackle this, I think, going forward
is to use open source models or other models
that have less of these fine-tuned nature.
But it's going to be highly dependent
on the models that we'll be using for this.
So I think that's one thing to look forward to.
Got it, got it, that's super helpful.
And maybe one last question on the research side.
When you think about future areas to explore
for you specifically, what are some of the
more narrow topics that you're hoping to dive deeper in?
Given the world.
So ultimately, I think making the agents more accurate
for these agents to be more accurate reflection of who we are,
I think it's going to be a really interesting research
and I think it's going to, that's going to be an area
that's going to have more of a research and broader impact.
So right now, you may have seen the sort of simulation demo,
the agents that live in that simulation are fictional,
that we just, for instance, we have an agent named Isabella,
we told Isabella that she is a cafe owner
and large language model basically makes up
what a persona that is reasonable given that description.
But I think it's going to be far more interesting
if we can make these simulations actually closely model
our actual human communities.
So it's not just fictional, but actually has groundings.
That's going to open up from our perspective
an entirely new set of application spaces
as well as research impact.
This can be used, for instance,
to actually model or predict markets
or it's going to be able to use two more closely personalized
many of these agents for individual use cases.
So that's something that we're looking forward to
in terms of sort of a particular topic
that we're diving into.
That plug, of course, scaling up the agents.
I think that's another big one, but those two.
That makes sense.
Now, one of the things that's missing
in most AI technologies is like really the
emotional part of how humans feel, right?
Like all of that data is largely not captured
and therefore not part of any kind of models today.
Language is one small output of what we have.
It's a very important output for sure,
but it's still one small output.
So I wonder how we might be able to incorporate
some of the data around our emotions.
I agree.
Some thoughts in the future.
Maybe let's move on to the applications today
since you talked about some of the challenges for agents.
Many organizations are thinking about
how to use large language models today, right?
There's a huge amount of aspirations.
A subset of them are also thinking about
what are some of the agent technology applications
that are viable within an enterprise,
which has limitations around infrastructure,
around data silos, security, and all that stuff.
Any particular areas where you've seen
companies be successful at using these technologies
in production?
Right.
So I think this is going to be incredibly
like case-by-case answer.
So let me think.
Or if not, like any hypothesis as to
where you might see the first
commercial deployments at scale.
Right.
So there is something that I have.
This is, there's a message that I have
in trying to communicate, I think, in different settings.
So this is not something I'm sort of
conveying for the first time.
And my opinion has been getting updated,
but I think fundamentally I think this is right.
So the way I've been describing it is
in human computer interaction or in most task settings,
there are two types of problems
that we deploy our machines or agents in.
One has very hard-edge problem spaces.
These are things like, hey, order me pizza
or buy me a plane ticket.
These are tasks where there's a very concrete outcome
and there often is a right or wrong answer
that the agent has to achieve.
At least from the user's perspective,
there is something that would absolutely be yes or no.
And then there are problem spaces
where we have soft-edge problems.
These are problems where we can increasingly
co-climb towards sort of being better,
but at certain level it starts to actually become useful.
So to make this intuition a little bit,
to make this a little bit more intuitive here,
for instance, if I guess the worst possible case scenario
is I asked the agent to buy me a plane ticket
and it bought me the wrong ticket
that just goes to a different place,
then that's like a heavy no.
Whereas let's say I asked the agent to sort of
simulate a behavior that is sort of fun
so that when I'm in a game,
this is sort of entertaining and interesting.
That might be something that the agent
doesn't need to be quite perfect in,
but it can still get there quite quickly.
And then we can gradually improve.
Those two are the spaces that we can sort of,
in terms of when we consider where to deploy
or how these will actually make its first impact,
we might actually be looking at those problem spaces first.
And these are the classes that I'm seeing.
If I were to make my bet,
agents will likely succeed first
in the soft-edge problem spaces
and will gradually inch into making it work
in the hardest problem space.
This has been sort of an intuition
with agent research community for some time.
So when Clippy, for instance, failed.
Our intuition there, at least from research perspective,
wasn't that these agents failed
because we didn't have the technology there.
Certainly these were deployed
and there were some confidence around the technology.
But the problem there actually was with interaction,
that when agents are deployed in hard-edge problem spaces,
it's often deployed in states where
it actually has to have a fairly long chain of steps
and fairly high, the risk were fairly high.
When it fails, the cost of correcting its error
is actually quite high,
cost of auditing its error is actually quite high.
So when these agents are deployed
in hardest problem spaces,
it has to reckon with the fact
that it will undoubtedly make mistakes.
And when it does make a mistake,
it has to be increasingly auditable
and controllable by the users
so that the cost of correcting its error
is not high enough that from the user's perspective,
the cost-benefit analysis basically has to make sense.
And that's been a fundamental challenge with agents.
That's why in every era,
we see the interest around agents spike for a while
and then it quickly subsides
after like maybe a half a year or two a year.
Now there's a real issue now though,
that given the large language model in the progress we saw
that this might not be the case this time
or at some point we might be able to make this work.
But for now, so I'm closely monitoring this
as well and I think we all should.
I don't think we should just go and say,
because it didn't work before,
it's not going to work this time.
But my hunch is that we will likely see
very similar pattern arise,
at least for the first of the future.
And we haven't quite dealt with the interaction problems
with those types of agents.
So I think it's much safer to assume
that it is going to be in the soft edge problems basis.
And that's why in some areas, in many of the aspects,
that's why our team was also interested
in this idea of simulation.
Because simulation is sort of the prime example
of soft edge problem spaces,
where the simulation has to be good enough
for it to start being useful.
That's also why I think a lot of really early promising AI
startups that's going to go in the agent space
are places that does NPCs for games.
Because those are very safe soft edge problem spaces
where the agents can fail, but that's okay.
And gradually we'll sort of go to the other area as well.
But I think that's where the impact is going to start
in the next couple of years.
I also think just seeing the startups in this space,
in sectors and functions that allow for failure,
like you said, include things like marketing.
Where if you market incorrectly,
it's not that big of a deal.
If you write the wrong code,
that's probably going to be a bigger deal.
If you pick the wrong security features, that's a huge deal.
If you pick the wrong things for healthcare,
that's an even bigger deal.
So there are degrees of fault tolerance
within the enterprise.
That's one thing.
And the second on the consumer side,
especially if the agents are just assisting consumers
by not executing on anything.
That could probably also work.
For example, there's a company called Rewind,
which is using some of the agent technologies, I believe.
And they're getting a bunch of consumer demand,
but what consumers are doing is just searching
for a behavior that they have had before.
And this product is helping them do that
versus do anything real world, really.
Interesting.
But that's super.
The way that you frame it is very useful.
What about just from an architecture standpoint,
large language models enabled by transformer architectures?
This is a whole different direction.
We're already seeing companies that are saying,
hey, transformers are not the most efficient.
Inference costs are very high.
Let's look at the next thing.
Have you spent much time thinking about that?
And if so, any impact to the work that you're doing?
Right.
So certainly a next sort of model
that we're going to be banking on,
I think that always is an important topic.
And that's something that I think we as a community
always has to sort of monitor,
because I think you're right,
that transformer is not going to be the end model
that will be hopefully, I mean, not on what.
The hope here is that we wouldn't be using transformer
10 years down the line.
But one way that we do view this is,
this is very much like a programmer's way
of looking at this.
We view this in abstractions, right?
So what transformer has gotten us right now
is this amazing capacity for reasoning
and processing information and generating information.
So it might be the case that in the future,
that task will be done by even better models.
And hopefully that's going to be the case.
But for the sake of building applications,
it is true that we can sort of view this
as a layer of abstraction.
That there might be some other technology
that's going to be powering it in the future,
but really what we're focusing on
is the capacity and the modality.
What kind of reasoning, using what modality,
can these technologies that exist today do?
And we're going to be building on top of it.
So I think that's sort of our way of looking at it
in sort of a medium term, again, the next three to five years.
Now, if you're looking, because right now,
there are some promising architectures
that's sort of been created at the forefront of,
I would say more in the machine learning
and natural language processing communities
that I'm personally getting a little bit excited about.
But at the moment, those are still
in the very much in the research phase.
And can you share some examples of that?
Yeah, so I think there's one model that recently came out,
like Mamba by some folks.
Those are from Stanford folks.
So I think the authors now at CMU and Princeton
sort of all within sort of this community.
So that's one example of sort of a potentially promising
or interesting model.
And that's the one that I recently heard about
that I think is interesting to be looking at.
But these models, for them to be deployed at scale
in a commercial way, if we decide to basically go
with certain model that's getting created today,
it will give us maybe two to five year timeline
before they can really take off.
Because Transformer is not, it is a relatively modern model,
but it really is how you look at sort of the timeline.
There's Transformer.
Seven-ish years?
Seven-ish years.
So I think hopefully if we find something like this time,
maybe it's going to go much faster.
But it still took about seven-ish years for Chatchapiti
to really come out.
So it's not immediate.
Whereas a lot of interactions that we can build,
I think there's a lot that we can do like today
to create really cool experiences.
So I think that's how we're looking at this.
There is a medium term.
This is where we are focused on the level of extraction,
that this is the capacity that we'll have.
And then maybe in the down the line five to 10 year term,
we can really be looking forward to some new models that's
going to make an impact.
That's great.
That's great.
Cool.
Very cool.
Maybe more generally, if you just zoom out for a moment,
when you look at the ecosystem today,
what are some of the problems that you want to see solve?
We talked about multimodal a little bit.
We talked about new models right after Transformers
that might come out.
What are some of the problems that you are most excited
about someone solving?
Not necessarily you personally, but someone solving.
I sort of have two in mind.
And it's a little bit less of a, this is a specific problem
that I want to solve.
But it's more sort of questions that I have
that I think more of us should be thinking about.
And to some extent, and this happens a lot with sort of the way
I do my research as well, where I get
inspired by big problems or foundational problems
that we had in previous decades.
Because oftentimes, there's a lot of insights
that we can learn from the past as we build on the future.
One, certainly I'm embedded into this agent space.
One is, in the past, agents had its hype cycles, basically.
But it failed.
That the hype cycle lasted for a couple of years,
and then people very quickly lost interest.
Basically because it didn't quite deliver
on the promises that it had.
I think it's worth asking ourselves why that was the case.
I think the opportunity this time is real.
But I also think the opportunity in the past
was also real to some aspect as well.
So just because the opportunity is real
and language model is really cool doesn't necessarily
guarantee us, at least from my perspective,
that we're going to, that agent will finally
be a thing that everyone will use.
I think there is a future where that will happen at some point.
I think it might even happen this cycle.
But I think it's really worth asking, as a community,
why did it fail in the past so that we don't repeat those mistakes?
One sort of main thing I'm sort of curious about
that I don't think a lot of us are thinking about
is actually not the technology part, but the interaction.
How are these agents going to be used in what way?
Because ultimately that's where it really delivers value
to the end users.
And that's where agents in the past have failed.
That it was really cool technology,
but we didn't seriously ask ourselves,
is this something that people really need?
And does the cost-benefit analysis
of using these agents and learning how to use them well
really make sense for the broader user base?
So that's one.
And other one is sort of my, it's a little bit of a hot take,
but it's also a shorter take, which
is we have large language models,
and I think these have made a huge impact already.
The number of users who use chat.gbt, that's incredible.
But I think it's sort of worth asking ourselves,
is that sort of quote unquote the killer applications
that we were waiting for?
Because in many ways, chat.gbt, or maybe it is.
And I think if it is, I think somebody should articulate this.
But chat.gbt does feel like a fairly simple wrapper
around our language model, because that's what it is.
And OpenAI has done fantastic things to make it safer
and make it more useful by tuning, I think,
what's really great.
But I think it's worth asking, if that is actually
the killer application, why is it a killer application?
And the answer might actually come out
that maybe it actually isn't the killer application
that we were waiting for, in which case, what
is going to be the killer application?
That's really going to add value in a much more
generalizable way.
That's a very abstract question.
For now, it's for me, it's just a hunch
that I think there's something to be asked about there.
And if I'm wrong, I would also love to hear again
somebody really say, we already have this killer application,
maybe it's co-pilot, chat.gbt, and here's why.
But for now, this is a question that I'm still asking myself.
That makes sense.
And thank you for sharing that.
What are some of your favorite AI apps today that you use?
I love chat.gbt.
I use it every day.
Chat.gbt did make a difference in my workflow.
So as a researcher, one of the main things I do
is I program every day, or at least most days,
or I write or write papers.
So I do one of those two things.
Chat.gbt is fantastic at both.
So as all programmers sort of know,
we sometimes don't bother remembering
all the different functions or documentation.
It's very good at generating a lot of the code
when I have an idea.
Really impressive.
It's also quite a good editor.
So if I make grammar error in my sort of sentences,
chat.gbt will usually catch them for me.
It's simple and easy thing, but it's good enough now
that it's actually making a difference in the workflow.
So as a chat.gbt, for sure, by extension, I think co-pilot
will make a difference.
So it's sort of worth asking, maybe going back
to the question around killer application.
What is the definition of killer application?
I think it does some people define it
as application that has more users.
And the fact of that, I think, always has to be the case,
that no killer application has no user.
Killer application, by default, means the application
that will have the most number of users.
But I think there is more theoretical definition
to what a killer application is.
That implies a lot of users who are the most number of users.
But for instance, if we look back to the prior era of PC,
my killer application that I mentioned
was something like Microsoft's Excel or this tabular data
format, the thing that would let us manipulate the tabular
data.
So really, the definition, in a more theoretical sense
of killer application here, is there
is new technology stack that is being developed.
There is a new file type that is getting generated.
Then the killer application is the one that would let us
manipulate the file application, file type.
That's one theoretical definition
that one could give, at least that's
sort of the definition that I've been towing with.
I think it's an interesting one.
I don't think it's the only one.
But those are sort of ways that I'm looking at this.
That makes a lot of sense.
I also use chatGPT every single day.
It's been very helpful.
Everything from coming up with menu names
to rewriting emails that don't sound as nice.
And I've tried a little bit to give it files and images.
So I actually helped my mother create a background.
She's a dancer, so she was performing
on one of the very specific background for her dance.
And I created that for her using chatGPT.
So all sorts of utility there.
But I love the way that you framed
the last potential application.
Maybe just one last question from my end.
Any resources or books that you love that is on this topic?
Right.
I do think, and this is often the case
with many of the cutting edge spaces.
I think a lot of the papers that are coming out
that are gaining a lot of attention,
I think those are sort of worth checking out
as sort of the resources.
It's not exactly like here's one book that we can all look at.
But things are moving fast enough that I think
those are sort of interesting resources
or just things that are getting created today
and their documentations.
So those are sort of mentioned as sort of a generic answer.
I do think, I think this has been a sort of running thing
in some of the things I mentioned today,
I get inspired by insights that basically had an impact
that stood the test of time.
And the reason why that is the case
is because I personally think all the great ideas
are sort of timeless.
It's because current time cycle is over,
it doesn't mean they're less interesting or less meaningful.
For sure.
That foundational ideas that will continue to have impact.
So when I look for resources,
I actually look back to books from truly the prior generations.
So some of the works that I often go back to
are works by Herbert Simon, Alan G.
Those are founders of AI and many of these fields
who would later go into a turning award
and Nobel Prize and so forth.
And those works, early cognitive psychologists
and scientists inspired my work a lot and their textbook.
Those people actually have written books
because they were much more established
than sort of the cutting edge spaces today.
So I go back to those as sort of my personal resources
for getting ideas.
That's great.
Thank you so much.
This was super, super helpful to me personally
because what we do as investors
is we try to understand the impacts of technology
and start to invest in companies when it becomes,
at the beginning of when it becomes commercially viable.
So to your point around what are the problem spaces,
what are the applications in which this can be applied
in a cost-effective and secure way
that where the end user is willing to interact
and get value,
that's when we start to come in and invest in these companies
which will hopefully be much bigger companies in the future.
So really appreciate this chat.
Yeah, it was fun.

All right, I'm so excited to be here with Peter, the VP of Product and Partnerships
that open AI a day after the GPT-4 launch, one of the most exciting launches I think
ever in Silicon Valley, so congratulations, Peter.
Thanks so much, and thanks for having me, super excited to catch up.
All right, so I know you're busy, and we have limited time, and we crowdsourced
way too many questions, so I'm just going to rapid-fire them at you if that's all right.
Yeah, let's do it.
Let's do it.
So first question is, what are some specific applications of GPT-4 that you and your team
are most enthusiastic about?
I think, I mean, there are tons of them, but just to call out a few, one thing that I really
thought was super cool was Be My Eyes, who is using the visual inputs to GPT-4 to help
visually impair people.
I mean, it's like, you know, computer vision used to be the thing where you did object
recognition and so on, and suddenly you can do much more.
You can like ask like, hey, is something wrong in my outfit, something I should fix, or,
you know, I have these ingredients, what should I cook?
You know, like it opens up a whole new kind of range of applications.
And you're originally a vision guy, right?
That must be wild to see this approach so well.
That's right.
It's sort of crazy.
Like, yeah, all we did was language models.
Awesome.
Okay.
Does a lot of prompt engineering become redundant given the larger context length and the ability
to supply more in-context examples of the prompt?
Oh, yeah.
This is super interesting.
I sort of feel like, I keep on thinking that something is both right and wrong with prompt
engineering.
Like, you know, ultimately, prompt engineering, when we should think about it, we should think
about it's like, you know, how good are you at just specifying tasks?
Like, a good manager should be able to tell, you know, their employee, whatever, this is
your job.
This is what you work on.
It shouldn't be super much ambiguity.
And that's usually, that's kind of what, what, what prompt engineering is about.
But like this ability to just have a conversation with the model and like fix it.
Like, no, no, no, that was too formal or no, like, make it live upbeat, add a joke here.
You know, like, that's sort of interaction.
That's how I think we would kind of think about language models much more than like adding
lots of examples or like writing very specified instructions.
Like, let's just have a conversation with the model and it would kind of pick up on
what I want over time.
All right, now, here's a question we got the most in different forms, but why did you
guys not, why weren't you able to keep the training data on more up to the training
of GPT-4 and more up to date data?
Oh, yeah, so that's, that's such an interesting question.
Like, ultimately, you know, training these, these models take a long time.
So that's, that's ultimately the answer here is, is that, you know, I can't tell you exactly
how long it took to train this, but like, there's, there's processes here of like, at
some point you're doing testing on certain kinds of data, and then you want to lock down
that data and not change it too much.
So you can really trust that when you run the big kind of training run, nothing is going
to go, go wrong.
That just means that you're going to have still data at some point, obviously, like
getting fresh data into the models is a pretty big kind of question that we get over and
over and over again.
So it's, it's something that's definitely on top of our mind to kind of fix going forward.
Ultimately, that is, you know, it just takes time to train these models.
Okay.
All right.
Considering that GPT-4 is multimodal now, do you imagine future foundation models to be
multimodal by design?
And I guess, do you have any comments on what the biggest blockers were when introducing
the multimodal component?
Yeah, that's a good question.
I, I, I, I definitely think that like multimodal is sort of the future.
Like this is, you, you just give the models abilities to understand many more concepts.
They kind of understand things like, you know, you know, physics concepts based on looking
at images, or they can understand if like, you can imagine eventually these models kind
of deal with more modalities like audio and, and so on.
Like it's pretty clear that the future here is, is just more and more, more kind of multimodal
models.
And, and, you know, some components are already there today, like with Whisper, you can take,
you know, you can take audio and turn it into text and so on.
So it's, it's like, all of these things eventually will, will get connected.
I think in terms of challenges, you know, I, I, I, I would confess here that I wasn't
on the research team that did a lot of the amazing work to get GPT-3 working, but I just
know that, you know, going, scaling, going from like small models to like these, these
enormous models that GPT-4 is, is, that's like a huge challenge in itself.
You kind of need to do a lot of science to understand, like on small models to see like,
here's how good it is, and here's how good it's going to be when we scale it up.
And that's, that just requires a lot of like very detailed kind of investigations and so on.
And a lot of kind of like just, just hard work, which, which that team did.
Totally.
Were there any new, like specific challenges with GPT-4 different than
previous models, or is it just sort of like trying to get even another level of scale?
Um, you know, I, ultimately it is, you know, just another level of scale, but I would say
that we work in like all aspects of that, whether it's data, whether it's how to
most efficiently use the compute architectures of models and all of these things are things
that we kind of tune and tweak continuously.
So, so, uh, but ultimately it, I mean, big part of it is really being able to scale.
And as you scale every order of magnitude, there's just more and more things that break
even in your infrastructure, you know, and so, you know, we have just an incredibly
talented infrastructure engineering team, uh, that is like made up of both engineers
and researchers that are, have built this incredible system for being able to train
these models.
And there's just so many little things that you need to get right, because if you
have a bug somewhere, you know, it's going to propagate immediately.
And so like, you know, I think one of the biggest like mantras, essentially that
team has is like, don't, don't have bugs, you know, which is like, seems like the
most hardest thing to do as a software engineer.
Um, good advice though, don't have bugs.
So, uh, so here's another question.
I don't even know if it's a fully, um, formed, but, you know,
in the, do you have a sense or a guess of like the order of magnitude of
experiments running in a typical day?
Like in the weights and biases notion of experiments, like training something,
is it like, like a handful or millions or where, what do you think it is?
It's a good question.
You know, I would say like there's probably on the order of like, uh, I don't
know, like somewhere between hundred and 200 people essentially doing some sort
of research at open AI and each of them are, you know, probably running on the order
of like a few experiments per week.
So that probably makes it like multiple hundreds of experiments per week.
Like some of them will take, like some experiments take, take weeks.
Something's the experiment takes like hours, right?
But it's sort of like, that's probably the order of magnitude.
Got it.
Okay.
Um, to the extent that you can comment, um, what was the most useful type or
source of data for training GPT for?
Oh, um, I mean, it's, it, I, I feel like this is just like the text on the
internet is probably the answer here.
You know, we just, these, these models are just incredibly data hungry.
So just finding more and more and more of this.
Like we have a whole team that's just like basically focused on like, how can
we bring more data into these, these models?
So, uh, I think at this point it's like, almost to some extent it's like less
about the particular data and more about more data.
You know, um, so that's probably how I would think about it.
Is there any particular task?
I mean, I saw all these tasks that, you know, you showed a GPT for getting better
at, is there any task where like, you know, it's been surprisingly hard given,
um, you know, how impressive like all these, like every version of LMS is.
And anything stand out is like, you would have thought it would have gotten
solved by an earlier version.
Oh, that's a good question.
I mean, it is still surprising to me.
I feel like, I feel at this point, like GPT for is quite good at just doing
mental arithmetic and stuff like that.
But like, you know, it took all the way to GPT port.
You're going to get it to be fairly robust at that.
It would just make this still mistakes for even up to 3.5, you know, uh, and
it's still, you know, just to be clear, it still makes some mistakes.
Uh, but, you know, it's like the way I look at it, it's kind of weird, almost
like a weird savant in some sense, right?
Like it has these amazing capabilities on some dimensions and then makes these
like incredibly stupid mistakes that a human just wouldn't make unless the human
is drunk or something, right?
Like, that's sort of like, that's what it feels like sometimes with the model.
It's like, you kind of never know, did you get the, the kind of early morning
version or like the late at night after multiple drinks version?
And like the weirdest thing about this is that sometimes, like even like, uh,
prompting the model, it seems to matter.
Like if you tell the model, hey, you're a genius at math, it would do better at
math if that, if you tell it's like, you're not very good at math, right?
Like you can't like prod it like you can't get it riled up and then make it.
Exactly.
And so the, the model is like, it's still very vulnerable to this, this stuff.
So, uh, but, but I think that's like, you know, that's one thing I think it is.
I mean, it's like, um, uh, it's, it's also funny, you might have seen in the, uh,
in, in, in Greg Rockman's demo that he did, like, you know, the whole thing
about like summarizing with a single letter, um, the same letter starting every,
every word. I mean, it's kind of, you know, it's still, it's sort of weird how it
just happens that once you reach a certain scale, it kind of just gets it.
Uh, and I think there's a, there's a number of those where it's just like,
it's kind of weird how you just, you know, I think it's kind of called a
grokking, this phenomenon where you train and train and train.
Doesn't do very well.
And then suddenly it just gets it something kind of, it forms some set
representation and just kind of gets it.
Uh, and so there's been a few of those, uh, and it's like, I think it's always
surprising when they happen and you're like, why didn't it happen earlier?
You know, I don't know.
Interesting.
Cool.
Um, okay.
Here's an interesting question, uh, maybe useful for us at Whits and Bices.
What kind of visualization tools would be most helpful as we continue to, to
build on these models?
Or do you have any ideas of what might be helpful for people building
on top of these models?
Oh yeah.
That's, that's such a great question because I think this is like kind of
evolving more and more as these models get better and better.
Um, you know, it used to be the case where I think kind of all you cared about
was really like perplexity and so on.
As you were training it, it's like, that was most of what you were optimizing.
But I think we're getting to a point now, like where you, you kind of, um, you
really want to visualize how, uh, like dig into kind of the mistakes that
the models are doing and so on.
Like you're often removed a word where I think you can get really far with
like prompt, prompting and, and, and, uh, whether it's like through
conversations or a single kind of prompt.
Um, and then, but then you will get to some point where you kind of, you, you
can fine tune these models to, with even more data.
And at that point, I think the paradigm now is that you don't need a ton of data,
but you need to make sure that that data is kind of really, really high quality.
So understanding kind of the way to see like where, where are mistakes made?
How, like, how do you automatically kind of build, you know, we, we ship this,
this open source library called open AI evals that kind of makes it easy for
you to build, um, automated evals.
So I think like that would be a big part of the workflow and allowing people to
see overall trends, but dig into the examples.
Because oftentimes what we find that at the end of the day, like
examples are under specified and so on.
And, and so you just need to be so way, way more careful now that like the
models are so good, uh, that like you're not seeing weird stuff because the
model is bad versus like you just had to mistake in your data set.
Hmm, makes sense.
Um, okay.
Uh, when, so I guess, uh, here's, I guess the summary of this question is how
many people worked on GPT for like how many animal engineers did it take to, to
train this thing?
Oh, that's a good question.
I mean, it's just like, I feel like all of openly I worked on this one, uh, in, in
one way or another, because like we have this whole kind of process, like a big,
like, you know, all the data set work that goes into it, all of the infrastructure
engineering that built the data centers that kind of built, um, um, uh, the
infrastructure on top of those data centers, training these models, then all
the kind of science and stuff that went into actually the pre-training and then
we do this kind of fine tuning on top with, uh, RL from human feedback, you know,
so I feel like at the end of the day, it's got to be like probably out of the,
the 300 plus people at open AI, probably at least two thirds of the company worked
on some aspect of this.
Okay.
And final question, um, since this is our user conference and you are such a high
profile, um, user or open eyes, I'm curious if you could say a little bit about
how open eyes is, which advice is, and if you have a favorite, um, feature or part
of, uh, with advice, it's a lot to know about that.
Yeah.
I mean, we use it for, for pretty much all of our model training.
So just like, you know, uh, you know, just, just, just tracking them.
I think there's a lot of kind of just sharing of like, you know, the fact that
you can easily share, share, uh, share kind of training runs and stuff like that.
It's a super used feature.
But I think one thing that, you know, these days I, I do way, way less of that
sort of work.
So one of the features I really like is, is kind of the ability to kind of have
reports and so on where people, so we use that, uh, quite heavily depends a little
bit on the team, but a number of teams are using that quite heavily to kind of
really have clear hypothesis.
Here's like the hypothesis, uh, here, here, here is the, here are the experiments
that were run to kind of validate or invalidate that hypothesis.
Here's the conclusion.
You have all these like mini scientific papers, essentially on all of the stuff
that's happening at open AI, which is like incredibly interesting to kind of, uh,
follow along with.
Fantastic.
That sounds very interesting.
Thank you so much, Peter.
It's really, really nice to have your time, especially so, so close to after your,
your big launch and congratulations.
We're, we're loving, um, using it internally here.
Thank you so much for having me.
This was super fun and, uh, can't wait to see what you all built with, uh, DPD4.
Awesome.
Take care.

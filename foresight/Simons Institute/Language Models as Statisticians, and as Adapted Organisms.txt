So our first speaker for the afternoon session is Jacob Steinhardt.
Jacob is a remarkable researcher who kind of combines like crazy creativity and technical
know-how.
And he's gotten me to think very differently about a lot of problems both technically
and I guess societally.
So I predict it will be a very interesting talk.
Thank you, Sasha.
So I decided that I wanted to talk about something new kind of in the spirit of the Simon's
workshop.
So these slides are actually prepared totally from scratch.
No one has seen them before except me on my laptop.
So hopefully it will be interesting.
So the kind of overall motivation here is, you know, at least I really want to understand
what, you know, these ML systems that we're deploying all over the world are doing, how
they behave, what's going on under the hood.
But I find this very challenging because there's so many new ones, you know, maybe like every
month some new model is released and they get bigger and more capable and more complex.
And so how kind of understanding of what these models are doing keep up, especially given
that we often get kind of new capabilities or qualitative behaviors just kind of emerging
every time we scale up.
And so I think, you know, there's maybe more than one answer to this, but something that
will be the focus of this talk is the idea that, well, if we can somehow use LLMs to
understand LLMs, then maybe we're in better shape because then every time a new better
LLM is released, well, there's a new thing to understand, but we've also gotten better
at understanding things.
So, you know, that's kind of the idea, can we get this virtuous cycle?
And then, you know, hopefully as models get better, understanding will as well.
So this is going to be based on work with actually a lot of different collaborators,
but three that I wanted to highlight are my students, Rachie, Eric, and Colin, in particular,
a lot of the kind of LLM as statistician perspective in this talk was developed by Rachie.
So what do I mean by LLMs as statisticians?
So what I want to argue is that many forms of understanding that we could care about
essentially reduce to some sort of statistical or data science problem, right?
So maybe we're given a model, we just see what it outputs on some huge number of inputs,
right?
We could easily do that by just taking all of the data sets we have and seeing what
the model does.
And then maybe we want to kind of identify patterns, right?
Are there some things that the model is good at, some things it's bad at, cases where there's
something surprising, and then, you know, you could try to formalize that as some statistical
hypothesis and test it.
So that's kind of a statistical problem.
Maybe we want to understand the training set and understand, you know, what are the important
sources of variation?
You know, if it turns out that large fractions of the data set are in some weird language
called base 64, maybe we want to know about that.
And then you could also have kind of more active learning or active sampling problems
where we want to, say, generate new inputs that elicit problematic behaviors so that
we can identify and fix it, right?
So I think of these all as on some level kind of statistics or data problems.
And so if we can, in some sort of general sense, get large language models to do statistics,
then they can help us tackle all of these problems, and of course, many other useful
problems as well.
So to do that, what would we need to do?
So I'm going to kind of take a very high level view of, you know, what is, like, the
pipeline of doing statistics?
And I'd say it kind of has four steps.
The first is we look at some initial data.
Maybe we want to think of this as training data, but, you know, just some sort of data
that kind of helps us get our bearings.
From this data, we maybe want to form some hypothesis, you know, and maybe, like, the
hypothesis that models do worse on long inputs than short inputs, right?
And then formalize that quantitatively and then test this on new data.
This data could just be, you know, a held out set from the same distribution.
We might care about kind of, you know, generalization to new domains, or maybe if, maybe we want
to even, like, actively collect data to really stress test our hypothesis.
And so I'm going to go over a couple of case studies where we'll have problems that kind
of follow this structure and will automate each step with LLMs.
The kind of key difference from maybe traditional ways of looking at statistics is going to
be in this hypothesis H. So a key difference is, you know, often H is, like, maybe expressed
as some mathematical function, like, you know, patients with this future in a data set are
more likely to get this disease.
I can write this as, like, expectation of some feature function.
But here, because we're working with language models, H is going to be a natural language
string.
And so this third step, which maybe is often almost trivial, it's just, like, taking the
average of your feature over the data set, now actually becomes kind of non-trivial because
we have to formalize what it means for, say, a natural language string to be true about
a data set or not.
So if that's kind of abstract, don't worry because we're going to go into some case studies
very soon.
In fact, right now.
Okay.
So the first case study I want to talk about is finding failures in a model called Clip.
So for those who aren't familiar with it, Clip is an encoder model.
So it takes in inputs and kind of embeds them as some feature vector.
And it's kind of used as the backbone of many later models, right?
So it's often useful to have good embeddings.
So many models kind of use these embeddings and then do something with them.
And the thing that is kind of special about Clip is that it embeds both images and text
into a kind of shared space.
So it was one of the most early multimodal models that kind of did this effectively.
And as sort of shown here, lots of models use it.
So mid-journey uses it as its kind of first embedding step.
Dolly does, stable diffusion does, and lots of like 3D and video models do as well.
And you can kind of get pretty amazing results using these embeddings.
Often they're used to kind of do text to image generation.
So you have some text description of what you want, and then these models kind of try
to generate an image that matches this description in some way.
So you can see here, these are maybe a little bit small, but this is an empty glass.
And then you get this glass, a family of five members.
And then you get this, a man descending a mountain.
You get this.
So these are all amazing, except for the problem that they're also all completely wrong.
This is a full glass.
This is a family of six members.
This is somewhat us sending a mountain.
This says there's no star in the night sky, and it shows the Milky Way.
And so you get these amazing images, but you often get these kind of like very semantically
obvious failures.
So the kind of plot twist here, aside from the fact that here's a bunch of failures that
we can find, is actually that we didn't find these failures, and LLM found these failures.
So these are all automatically generated by actually not a single LLM, but a kind of
pipeline of several LLMs working together on kind of complimentary tasks that played
to their individual strengths.
So we'll talk about how could we actually build a system that, just given Clip, could
kind of automatically generate all of these failures at a large scale.
So are there any questions so far before I go into the details of this?
Yes?
One quick question.
Why do you say Dolly, new being, what does that mean in this context?
I think different versions of Dolly, and one of them, I'm not totally sure of the details
here, but I believe that Dolly was developed by OpenAI, but then Microsoft served some
version of Dolly as part of their being product.
And so this is that, which we use because we can actually query it kind of publicly.
Yes?
Are there any new type of errors that pop up from this process?
For example, like the counting error or the direction error, those are already kind of
known, right?
So are there any new hypothesized forms in this process?
Yes, that's a good question.
So I'll get to kind of numbers later.
We generated 14 categories of errors.
We looked for papers that kind of described these errors.
I think there were a handful that had already been described in the literature.
The others were not in the literature.
I would probably guess that someone whose day job was to play around with these models
would be familiar with these errors.
But the nice thing is even without spending lots and lots of time, you can do this.
In fact, I think there were cases where reviewers asked us to add a new system to this pipeline
and in a half hour we just got all of the new errors that that system had.
So I think right now that's the advantage is that it's much quicker.
I think as models become better, I'm hoping that we'll also get things that even an expert
who spent lots and lots of time might not necessarily find.
Yeah, great question.
Any other questions?
Okay, so how do we do this?
So first maybe let me give a kind of overview of the key ideas here.
So again, remember, Clip is a feature encoder that is encoding both text and images.
So the kind of main key idea in terms of like where we get off the ground is there's going
to be a sort of notion of what I'll call a hash collision in the encoder.
And we'll come up with a sort of automated way of identifying lots of hash collisions
in the encoder.
So this is going to give us a lot of kind of like individual examples where if you have
a collision, you don't know which of those two is wrong, but you know at least one of
them has to be wrong.
Then kind of given those failures, we're going to use LLMs to kind of categorize them into
coherent patterns and test that those patterns are actually correct by generating new examples
from those patterns and making sure that they actually in fact induce failures consistently
both in the encoder and on downstream tasks and even kind of also generalizing to new
domains that are kind of different from what we found these patterns on.
So that's kind of the high level.
Let me kind of step through these all one by one.
So again, remember we had this statistical pipeline of get some data, generate a hypothesis,
test it on new data.
So let's just go through these one by one.
So first let's just talk about where are we getting this initial data?
What is this kind of hash collision idea?
So to talk about this, I need to give you a little bit more background on Clip.
So as I said before, Clip embeds either an image I or text T, but what is it actually
designed to do or what was it sort of trained on?
So it's actually trained on a bunch of pairs of images and their captions.
And in general, the idea is that if there's text that describes an image, then that text
in that image should have similar embeddings, ideally more similar embeddings to each other
than to anything else.
So the training process was basically you got a bunch of images, you got a bunch of
captions, and then you want to make sure that under this embedding, the cosine similarity
between an image and its caption is higher than between that image and any other captions.
So you kind of want, if we form this matrix of dot products, you want the diagonal to
be really big and everything else to be really small.
And so the kind of point here is that if T is the description of I, they should have
similar embeddings.
Now how can I use this to find problems?
Well, if T and T prime describe different images, but have the same embedding or have
very similar embeddings, then at least one of them has to be wrong in some sense, right?
Because they, you know, like whatever images these corresponded to, they kind of can't
both be, they can't both be right.
Ah, yes, Aliosha.
That's the subjection between images and text, and we know that, you know, pictures
weren't a thousand words.
Right, so the examples I have in mind would be something like an empty cup and a full
cup.
And if those, like if those sentences had the same embedding.
But those are, that's, that's a very small subset of all the, like there is a lot of
synonyms, right, visual synonyms.
That's right.
So you have to worry about visual synonyms.
So we need, we need some way of measuring kind of semantic difference that hopefully
implies that things are actually visually different.
So I'll get to that on the next slide.
Other questions though?
So one thing here also is the nice thing is that I can do this only looking at text, right?
I mean, I have to use the clip encoder, but I'm only encoding text.
And why do I want to only look at text?
Well, basically, because language models work really well and image models don't.
Sorry, Aliyosha.
But you know, according to Aliyosha, image models will work really well soon.
Even better.
But right now, right now, we want to stick with language.
So, so what do we do?
We're going to collect some initial corpus of text inputs, and we want these inputs to
be inputs that have some visual significance.
So we'll often take them from some, say, captioning data set or other kind of data
set that has visual descriptions.
And then we're going to embed them all under clip.
And we're also going to embed them under another model called Distill Roberta, which
is a very good text model, especially for embeddings.
So it's a text-only model.
And it also has a higher-dimensional embedding space than clip.
And so for both of these reasons, it's kind of, you know, has a better understanding of
text than clip does, because it gets to only focus on text and it has more parameters.
And so the basic idea is, you know, so we have all these clip embeddings.
And if there's two inputs that are very close in clip space, but actually have low Roberta
similarity, right?
So that means they're kind of different, like Roberta thinks that they're semantically
different sentences, but clip says they're the same, then we're going to say, OK, that's
a hash collision, probably something is wrong there.
Now, I agree with you that we also want to check that these really are semantically different
in a way that matters visually.
Empirically, it turns out that that's the case about 90% of the time.
So this is kind of good enough, and this is something we kind of verified with human
subject studies.
Jacob, how do you find these things?
Because aren't the clip vectors like 2048 dimensional or something?
Yeah, so we're just taking the cosine similarity.
So this is like, this is an n squared algorithm.
Oh, yeah, but OK, but I mean, if we imagine that you were just looking for collisions
in this 2048 dimensional space, we would say a priori that could take astronomical time.
You're saying like in practice, it takes much less time because, you know, there's something
about these text inputs that makes the collisions likelier.
So yeah, a couple of things.
I guess first we don't need exact collisions.
If the cosine similarity is large enough, I guess empirically, if it's larger than 0.88,
it turns out that it's pretty likely to create a problem.
So you don't need them to be exactly the same.
That kind of helped you somewhat.
And yeah, so somehow this...
But I mean, two unit vectors having an inner product of 0.88 in a 2048 dimensional space,
we might as well call that a collision, right, at that point.
But he's assuming n squared time for u n with the exponential, right?
So, okay, right.
So Scott's claiming we would need an exponential large data set, but this is not...
Yeah, yeah, yeah, but okay, but it happens that...
So basically the reason why there are collisions is not...
has nothing to do with like the pigeonhole principle with the space, right?
It's just that, you know, the way that it's something special about this mapping that
causes there to be collisions, even though a priori there could have been no collisions.
Yeah, that's right.
Okay, and if you care, couldn't you do it much faster by using the same kind of h and
s, w in the cities with all of the embeddings?
Yeah, you could do this much faster than n squared.
It just turned out that this is not the bottleneck, like the bottleneck is running the forward
passes of all the models and kind of looping over a bunch of pairs of, you know, thousand
dimensional vectors is pretty cheap compared to running an LLM.
Yeah, so we tried two different corpora.
One is Cocoa and the other is, what's the other one, SNLI, yeah.
These are both kind of text data sets that have visual significance.
And yeah, I mean, basically the point is that there's enough, yeah, there's enough structure
in text that you actually do get collisions.
I don't care about the n squared.
Okay, okay, got it, got it.
Okay, so this is the first step, right?
This is going to give us a bunch of pairs where we kind of know that like one of the
two things in the pair is wrong.
And so now we want to do something with that.
So this is kind of the next stage is we want to generate some hypotheses based on these
pairs, right?
So this is where we're going to use the fact that we have text and not images, right?
So the individual pair fillers are text inputs, they can feed them to GPT4.
And so, you know, here's the magical prompt, it says I'll provide a series of data for
you to remember.
Subsequently, I'll ask you some questions to test your performance.
Here's some pairs of prompts to memorize, and then you give it all of, well, you give
it as many of these failures as you can fit in the context window.
And then you tell it, hey, I'm trying to find failures as an embedding model.
These are pairs of sentences that are encoded very similarly using the specific examples.
Are there any general types of failures you notice the embedding is making?
And then, you know, you kind of give it some more context and you'd say, okay, what does
it generate, right?
So you're basically saying, here's some data, please look at it, please tell me some patterns.
And so then it, you know, it does a pretty good job of coming up with things.
It says, okay, there's negation, temporal differences, quantifiers, and the nice thing
is actually it doesn't just say negation, but it gives a bunch of elaborations.
So it says embedding models may not correctly capture the negative context in a sentence
leading to similarities between sentences with and without negation.
This can result in incorrect visual representations if the presence or absence of an action is
significant in video generation.
And it kind of keeps going.
And you get, I guess in this case, you get kind of 14 distinct failures in total on this
list.
And the other thing is empirically, GPD4 just kind of always uses this consistent list
format so you can automatically just parse out the individual hypotheses.
Yeah, Lisa.
Have you tried just like asking GPT without these like inputs, like what are common failure
cases of image embedding models or something?
Yes.
So that's a baseline that I will show results on later.
And yeah, it works a lot less well.
So this kind of gives us hypotheses.
So this was step two.
But now we're running into this problem of, OK, usually in statistics a hypothesis is
like actually some mathematical function, but here these are just sentences.
So now we need to formalize this hypothesis.
So we have this list of hypotheses, h1 through hk, that are all natural language descriptions.
So how can we test if one of these hypotheses is actually any good?
So I think this is a pretty interesting conceptual question to think about.
So maybe I'll pose it to the audience if anyone has ideas for how we could formalize this.
Hi, Chris.
Good afternoon.
Two or more about pairs as to whether they match this hypothesis and then see if the
images are wrong.
OK.
In this analysis of DPT4, if you have access.
Good.
Yes.
So that is Yan Yi.
I mean, if we think about research hypothesis, there are a few dimensions that you can use
to categorize whether some say it's a hypothesis.
So for example, it should be testable, right?
There should be a clear scope.
There are a few dimensions I think that you can come up with based on experts.
Right.
So you can kind of ask experts or DPT4 if it had those properties.
So it turns out we're going to do something pretty similar to what Chris said, although
we're going to look at generation rather than classification.
So we're going to say H is a good hypothesis if when you hand that description to some
intelligent agent, in this case not humans, because humans are expensive, but DPT4, they
do a better job of generating new failures than they would otherwise.
So this is the way we're going to quantify this.
So we'll say H is a good hypothesis if it can be used to generate new failures better
than some just baseline method of generating failures.
And this is where we're going to get to your question, Lisa.
So we can either hand DPT4 these hypotheses or we could just ask DPT4 to brainstorm without
any data ways in which vision models might be bad and kind of test those against each
other and see which one does better.
So we're going to test this by prompting an LLM with H as a context.
And so again, what is the magical prompt?
The magical prompt is to say write down 41 pairs of prompts that an embedding model with
the following failure mode might encode similarly, even though they would correspond to different
images if used as captions.
Use the following format so you give it kind of a format so that we can extract things programmatically.
And then we say some other stuff to motivate it, saying that it will be evaluated based
on how well it performs.
And then you give the failure mode and as kind of the description that we extracted before.
Y41, that's the length, basically the length of the output context window that can be fit.
So if you want more than 41, you just have to ask it a couple of times.
So this is what we did.
Yeah, Nicholas.
How much does this be creative and cautious and these kinds of things like actually help?
Or is this just like black magic that you sort of sprinkle on top?
We didn't do careful ablations on the prompt.
I think, yeah, we added a bunch of stuff until it worked.
I don't think we tried removing things to see what was actually necessary.
So it seems totally like I would guess that if you tried to distill this to its bare essentials,
you could get something simpler.
But we didn't try to do that.
But yeah, great question.
OK, so then we want to quantify by measuring the success rate.
So we get all these pairs of prompts that are supposedly supposed to be new failures.
So we can do this in two ways.
We can look at the fraction of things generated that are hash collisions in the same sense as before.
So that's kind of an easy thing to do.
At some point, we want to make sure that the system is actually doing something
and that the something doesn't involve trusting that LLMs are good at their job.
So we also do a human evaluation where we look at downstream systems that rely on clip
and ask humans if there's a failure to make sure that these actually are failures
and not just happen to have high cosine similarity.
So those are the two things we do.
So let's kind of go over the results.
So first, just kind of looking at hash collisions,
but testing on these new inputs that were generated.
So we say an input has a success if these similarities are above some threshold.
And what is this table saying?
So these rows are kind of the different failures generated by the system.
And actually, we considered six different systems.
So you can ask different models to look at the data and propose hypotheses.
So these are different kind of proposal models, GPT-4-Claude or GPT-3.5.
And then you can also vary the data set that you used to actually get these failures out.
So these are the two data sets that people asked about before, COCO and SNLI.
So I guess a couple interesting things.
One is that, oh, and a check mark means that the model generated the failure at all in its list.
And then the color is kind of the success rate of generating new inputs conditional on that failure description.
So a couple interesting things.
First of all, the data set seems to actually matter.
So kind of for both GPT-4 and Claude, action, well, OK, maybe let's pick a more intuitive one.
OK, so for both GPT-4 and Claude and GPT-3.5, SNLI elicits granularity as a failure, whereas COCO never does.
And sometimes it's kind of not quite so systematic.
But in general, it sort of seems like these data sets actually do kind of like elicit different failures.
So there is at least some dependence on the data, which is somewhat reassuring.
The other thing is maybe as expected, GPT-4 and Claude in general find many more failures than GPT-3.5 does.
So these better models actually generate more distinct hypotheses.
And then a final thing that is interesting is actually even for the same failure, bigger models often are giving you higher success rates.
So you can see this in a couple places like for granularity.
The description of the granularity failure that GPT-4 generated was apparently better in terms of if you then hand that back to GPT-4,
it more successfully generates novel failure instances compared to the kind of description that GPT-3.5 gave.
In all of these cases, we're fixing GPT-4 as kind of the thing that's generating new failures.
So there's no effect from that.
So this kind of difference is just coming from the actual text description output by them all.
So are there any questions about this data?
What more is left about when GPT-4 does better than GPT-3.5, is it because it better understood the instruction versus maybe GPT-3.5 also understood the description,
but somehow the examples were very just curious what sort of qualitative differences are there between different models.
So I haven't thought about this a ton.
I think my two main hypotheses here would be, I believe GPT-4 has a larger context window so it can see more examples, which might be useful.
But I think probably the more important thing is actually just that the task of proposing hypotheses from a data set is actually a pretty challenging task.
And so even kind of frontier models are not that good at it.
So then once you drop down from GPT-4 to GPT-3.5, you're kind of losing, probably just losing too much capability for it to be super consistent.
That would be my hypothesis.
I don't know.
Yeah, Richie.
Yeah, so in practice, we found that GPT-3.5 doesn't really condition on the data very well, while GPT-4 actually does condition on the data.
And then describe the same thing that they do.
Yes?
If I have the samples that the model generated actually fall into those categories, so maybe they overlap with some other categories.
So I don't think we did a systematic evaluation of whether all of the examples fall into those categories.
Yeah, I can give some orders on the next slide that get at least implicitly at that.
It looked like maybe someone else had a question.
OK, so maybe let's go to the human evaluation, which will at least partially answer your question.
So we wanted to not just stick with saying that you actually get these hash collisions, but show that these actually lead to images that humans say are wrong.
So, OK, darn, the text here is a little bit small, but this is kind of the human annotator interface that we gave.
So we had prompt one, a city skyline with a bridge, prompt two, a city skyline without a bridge.
So this is this kind of collision pair.
This is an image that came from one of these two prompts chosen at random.
And so the annotator has to say either that this corresponds to prompt one, it corresponds to prompt two, it corresponds to neither of them,
or these prompts are described visually identical situations.
So this kind of gets at your earlier question, Alyosha, on whether these are actually semantically different.
And so we're kind of measuring what counts as a successful failure, either if the annotator says that it's wrong,
or if they say it's prompt two, but it was actually generated by prompt one, or vice versa.
And so then you can look at the kind of rate at which mistakes are made conditional on different levels of clip similarity in the two prompts.
So this is kind of testing that high similarity actually leads to failures.
And you can kind of see there's this like inflection point at like 0.88.
So this is kind of one verifying that actually we do get pretty high rates of failure,
and two that this magical threshold I told you about earlier is actually kind of reasonable threshold.
And then finally, to get at this question of whether these descriptions are actually doing anything.
So I guess this doesn't test whether the failures correspond to the descriptions,
but it tests that the descriptions are actually needed to get high failure rates.
If you have a baseline system where you just ask it to like brainstorm possible failures images might have,
and then condition on those, you only get about 20% failure rate,
whereas you get an 80% failure rate if you use this data conditioned system.
So these are the human evaluated rather than model evaluated equals.
So are there questions about this?
This is a talk where high failure rate is better.
Yes, yeah, you want high failure because we're trying to find failures so that we can fix them.
Okay, and then I guess a final cool thing is, you know, a kind of really big bonus of having this come from language models
is language models are kind of automatically steerable.
Right, so I have this way of generating failures,
but I can then just ask the model to give me failures that are relevant to some new domain.
And so in this case, we kind of asked it to generate failures that are relevant to self-driving.
The data sets are still cocoa and SNLI so we didn't give it data that would specialize to self-driving,
but it can still kind of generate these failures in this novel domain and still have a good success rate.
And so these are just kind of examples.
Stable diffusion, you have the cars on the right side of the lane, but it's on the left side.
This is not a green light, gives you a green light.
A yield sign gives you something that is at least not shaped like a yield sign, probably a stop sign.
And then a car stops for a red light.
This is actually a text to video model and the light is green.
What data from cocoa and SNLI are you passing in?
No, no, so you're passing in the text from...
So this is for the very first stage where you're giving it a bunch of just text inputs and embedding them to check for hash collisions.
So those hash collisions were from embedding text sentences from cocoa and SNLI.
So there's actually no images anywhere here except in the output of the systems.
I'm kind of curious about the hypothesis part of this and whether that's kind of necessary.
So we had a paper a couple years ago and just tried finding these collisions.
And I kind of wonder if you could just give it a sentence and like search for a collision and just cut out the language model.
What is it adding in the process?
Well, finding the initial... Oh, I see.
So I think one thing is this steerability I think would be challenging in some cases if you were doing that because you would need a large data set of text in whatever new domain you were looking at.
We don't need a bunch of sentences about self-driving cars to do this, but if you were looking for collisions manually, then you would have to do that.
I see.
Yeah, cool.
Okay, so to summarize this, right, we had these four stages of, you know, first we want to get initial data, which we did by scraping hash collisions from this text data set.
This kind of invoked these two models clipped into still Roberta.
Then we generate hypotheses by prompting GPT-4.
Then we kind of formalize these hypotheses by looking at the success rate of generating new failures.
So we use GPT-4 to generate the failures and clip to evaluate them.
And then we can also do this active steering, again, prompting GPT-4.
So I think one thing I want to highlight here is that, you know, often we think about just having this one language model and we just come up with our super clever prompt that solves everything and maybe do chain of thoughts.
And this sort of thing.
But I think you can actually get a lot further if you're willing to kind of use this kind of, you know, ecosystem of models together in creative ways.
And I think statistics is a particularly kind of good use case for this because there are these different stages of the pipeline that require kind of different skills.
And statistics also has some nice properties, right?
Like many parts of it are kind of automatically measurable and verifiable.
And so you get a lot of the same strengths as Adam was talking about yesterday with computer programming where, you know, you can...
We haven't done much of this, but like you can, you know, maybe do this self-training.
And maybe if you get models that were really, really, really good at statistics, like superhuman at statistics, because there's so much automatically-generatable data.
Okay.
So that was the first case study.
Let me go over the second one.
We'll go a bit more quickly now that we've kind of built up a lot of these conceptual ideas.
So here I'm actually going to talk about a kind of meta task that is then going to be useful for lots of individual ways in which we would want to understand language models.
So this meta task is classifying with natural language predicates.
So the task here is we're going to be given two text data sets, D1 and D2.
We want to find out what's different between them.
And this difference, again, should be some natural language string H.
And so we can kind of think about this as isomorphic to binary classification, right?
We're kind of trying to classify between D1 and D2, but where the function is described in natural language.
So let me just give you an example of what this task might look like, right?
So maybe these are my two data sets, D1 and D2.
We want to come up with a natural language description of how they're different.
So can anyone figure this one out?
Okay, yes.
So the left is French and the right is English.
So our H would be D1 contains more French sentences compared to D2.
Okay, so here's a harder example.
Maybe partly hard due to text size.
So I claim that even if you looked at this for a while, it would be hard to tell what the difference was.
And in fact, the difference is that sentences in D1 contain at least two female characters in them, whereas sentences in D2 do not.
This is actually pretty challenging because there's things like she carried a total of eight torpedoes where she refers to a ship,
which is not in fact a female character.
And you also have to know that Professor McKeown is female.
And so there's kind of a lot of world knowledge and kind of non-trivial stuff in solving a problem like this.
So I guess that's the kind of meta task.
Why should we care about this?
So first, I'll give you three use cases that would help us better understand LLMs.
So we could want to understand distribution shift.
So especially if a model is doing poorly out of distribution, maybe we want to diagnose what's different.
And so we might find out that the test distribution involves more formal writing than the training distribution.
And that might help us diagnose failures or tell us what we should fine tune on.
The positive class contains more URLs than the negative class.
If this were a spam classification data set, this would tell us that there was this potential spurious queue.
That maybe the model just looks at the presence or absence of URLs and we should make sure that that's not just what it's doing.
We could do error analysis.
I could give you two models and I could look at the difference between inputs where one of them versus the other one makes mistakes.
And then we could also go beyond just trying to understand LLMs.
You know, you could start trying to do, you know, for like say like social science, right?
I look at a bunch of tweets from one year versus another year and it turns out and say, okay, public opinion from this year is more optimistic about the pandemic than last year, right?
We can kind of generate at least descriptive hypotheses like this.
Of course, you would then want to carefully do all of the causal inference and other stuff to validate these, but this at least can generate hypotheses for you.
Okay, so how will we do this?
Well, yes, D.
So, if you go back to the examples you have, it feels like the hypothesized space is huge, right?
So there seems to be a recall precision issue here, and it seems your ground truth only has some of them.
Like for them, the very easy example, my hypothesis could be there is no relation between those two sentences.
I mean, you have like, it has more French versus the second sentence.
The space seems huge.
How do you locate it to the final ones we want?
So we're not going to have ground truth in most cases.
There are a couple of cases where we did create a ground truth so that we could kind of test in the traditional setting with kind of gold labels,
but we're going to kind of take a similar perspective to how you'd evaluate any other classifier, right?
So we're going to come up with a way to quantify these in terms of some classification error rate,
and then we'll say that one hypothesis is better than another if it has a lower error rate in distinguishing D1 and D2.
And so rather than having a fixed ground truth, we can just talk about which systems are better or worse,
and maybe we could also say compare to humans to get some like overall benchmark.
Some of the hypothesis might be very true, and others may be more usable, right?
Okay, right.
So there's, right, so you might also want to evaluate if there's some like goal,
you might want to evaluate like relevance to the goal and some notion of novelty.
These become, start to become subjective.
So we have done some of this where we get human ratings of novelty and relevance.
Reviewers didn't like it actually because it's too hard to define novelty, but anyways, like you can do this and like I think,
but I do agree it's kind of a tricky problem.
So just building on these questions, are you constraining some complexity of your hypothesis?
Are you looking for short hypotheses?
So we'll implicitly have short hypotheses here, although that kind of just comes from the fact that these are going to be generated by an LLM,
and LLMs will only output things that are so long.
But yeah, we like, for usability reasons, you would want this to be kind of short and interpretable.
So yeah, how are we going to do this?
We're basically just going to use LLMs somewhat similar to before, right?
So I won't give you the full magical prompt that Rachel came up with, but just kind of schematically,
give it a bunch of examples from the first distribution and label them as A, a bunch from the second label them as B.
And then we say, you know, compared to group B, each sentence from group A, and then we ask it to complete it.
This was done at a point where we were using GBD3.
So we needed it to be in this completion format.
Once you have instruction tuned models, there's maybe nicer prompts you can have.
But this is kind of the basic idea.
And then you sample this a bunch of times with different, you know, sub-samples of the data, right?
So keep in mind that we can only fit maybe like 30 or so examples into the context window.
So we have a data set with thousands of examples is like a very tiny fraction.
So we kind of keep sub-sampling to generate different hypotheses.
And then, you know, you'll get things like is more positive, contains the word chapter, is longer.
Some of them actually kind of have, yeah, some of them end up being kind of trivial.
So you might want ways of filtering them out.
But you kind of get this set of candidate hypotheses.
So this is kind of telling us how we do the first two steps of this statistics pipeline, right?
We look at this initial data.
We prompt GPTN for some N with examples from D1 and D2.
And then we ask how they're different in this form of the hypothesis.
But now we need to somehow formalize each quantitatively and test it on new data.
So I guess, again, maybe I'll pose the question, you know, how could we formalize this each quantitatively?
How could we sort of say quantitatively how good is it?
Sorry, what do you say first?
Okay, good.
So we can say, you know, good hypothesis is something that helps tell D1 and D2 apart.
So we'll, you know, take a sample from D1, a sample from D2, mix them up randomly so you don't know which is which.
Tell either a human or an LLM the hypothesis and ask them to say which is which.
So, you know, as an example, say H is involves more formal writing, we can interpret this as basically a two argument predicate, right?
So if I have sentences X1 and X2, H of X1, X2 is some binary predicate that is the truth value of, you know, the utterance X1 involves more formal writing than X2.
And so this should be true or false.
And so then we just ask a human or a language model if it's true or false.
And so then we'll say H is a correct hypothesis about D1 versus D2 if in expectation over samples X1 from D1 and X2 from D2, this is much less than 0.5, right?
So 0.5 would be chance if this is sort of the, like some measure of the classification error, if it's much less than 0.5, then we've, we figured out something non-trivial about D1 and D2.
And so yeah, how to implement this, I guess you could ask humans or you could just query an LLM.
And so what does this look like, right? So just to illustrate this, if H is samples from D1 or more positive than those from D2, we, you know, we give, in this case, Charlie Snell, who was an undergrad at Berkeley at the time and is now a PhD student at Berkeley,
this paper proposes an impactful task or the approach of this paper is too trivial, and ask him which of these it's true about, and then he says something, and then, you know, maybe Charlie's time is pretty valuable, so you can hire crowd workers to do this instead.
But the problem is, you know, even if we just wanted to average over, say, like 100 samples from this distribution to get some notion of accuracy, this would cost $10 per text description, and this is very expensive.
You don't want to do this.
And so the nice thing is LLMs kind of reduce the cost of this pipeline by about a factor of 1000. You can do this 100 samples for only seven cents with GP3.5 turbo.
And so then this gives you a kind of automatic, quantifiable measure of how successful this hypothesis is.
And the nice thing is also that it's somewhat more reproducible than humans, like you don't have to worry about getting back the same human label errors again, because, you know, the model, well, the model's not actually fixed, open AI keeps updating it, that's kind of annoying.
But if they were to serve a stable version of the model, then this would be reproducible.
Right, so now we've kind of gotten this whole pipeline, right, so the overall system as we have this proposal, which at the time we initially wrote this paper was a fine tune to GP3 that generates these candidate hypotheses.
Then we have a verifier that kind of, you know, does this check on each of the hypotheses.
And at the time it was fine tune unified QA. Now you can kind of replace with newer models.
And then you kind of, you know, re rank the hypotheses based on their actual success rate at this classification task.
And, you know, why is this decomposition useful. Well, from an engineering perspective, and I think this is actually very important.
The proposer only sees 30 examples, because that's length of its context window.
So it's in that sense fairly limited, even though it's this very smart, like GPT three or four system, whereas the verifier can see thousands of examples and so you can get much better tests of statistical significance.
How am I doing on time.
I think we have about five minutes left.
Okay, cool.
So maybe I'll just say, you know, you can use this for a bunch of things. So for describing distribution shifts.
There's these two data sets MNLI and SNLI where SNLI is often used as like an OD version of MNLI.
Here's four samples to which are from SNLI and to which are from MNLI.
It's not immediately obvious what distinguishes them.
But if I say that SNLI describes a picture, then it's very clear that the green ones are SNLI because it says the church choir sings to the masses and old man with the packages poses in front of an advertisement.
And the other two are not about pictures.
So you kind of immediately see what the distribution shift is here.
There's two paraphrase data sets, Twitter, PPTV and QQP, which stands for core question pairs.
It says Twitter talks about a news story and core contains a question.
These are just kind of sanity checks like these would be kind of totally obvious to anyone who was familiar with these data sets.
But you can do more interesting things.
So this was one that I think to our knowledge was novel at the time we discovered it detecting spurious cues in a data set.
So we handed it this data set called SUBJ, which is a data set for subjectivity analysis.
And it said the objective class was a plot summary of a film.
The subjective class is a quote from a film review, which seems like it should be wrong for a data set that's about subjectivity analysis.
But if you actually go back and read the paper, it says,
to gather subjective sentences, we collected 5000 movie reviews snippets from Rotten Tomatoes to obtain mostly objective data.
We took 5000 sentences from plot summaries available from IMDB.
So actually, if you did well in this data set, you were basically learning this rather than stuff about subjectivity.
There's like other shortcuts we found somewhere new somewhere old, but you can sort of find all these various cues.
You can use this for error analysis and so on.
So maybe just to summarize, right, we have these four steps of this pipeline.
Initial data was just the two text distributions, generate the hypothesis by prompting GPT-3,
formalize the hypothesis by measuring the success rate, and then you kind of test on new held out samples.
The final thing I'll just leave up is we have ongoing work that is kind of taking this far beyond classification to just sort of like generally using natural language
predicates as features in statistical models.
So this example here is trying to describe temporal drift in news headlines from the Australian Broadcasting Company,
and it kind of identifies these five features that kind of vary.
And you could think of as maybe the top five principal components explain the variation in this data set.
Although the percent of variance explained is actually still pretty low here.
So I think you should think of this as just like a initial result.
So maybe I'll end there and take questions.
So for the proposal, there can be just like do prompt optimization like overtime,
you keep selecting like an example and then see where it fails and then show it again.
And then after a while you have like this good hypothesis and that you can somehow guarantee that is correct
in a set of checking things.
So you could.
So is the idea like basically like fine tune the proposal to get better and better?
Yeah, fine tune the prompt.
You do that prompt optimization.
Oh, prompt optimization.
So I think so my general sense is if you just use the proposal,
I mean, we didn't try to do like the full prompt optimization,
but we do do ablations of like just using the proposal and that generally does a lot worse.
I think that there's a couple issues.
What one is just that the proposal gets much less data than the verifier.
So, you know, even like if you do prompt optimization.
Oh, no, what's your I thought that your proposal like get like a few number of data.
That's why you should do the checking.
Yes.
And then I was just saying that can you make the proposal more strong by just to prompt.
I see everything and then you can guarantee that these hypothesis is correct.
Yeah, so that's that's an interesting idea.
So you're basically saying, okay, like do prompt optimization to find a prompt that gets this feel like we discussed this.
What was your claim is that you don't get semantically.
The point that you do gradient is gradient is that you find the prompts and you are you very easily get like a readable prompt that is not natural.
I would stream because what you typically find with adversarial examples.
They are relatively possible.
Yeah.
So I think the issue is at least right now.
If you do this, it's hard to get kind of like natural language out right now.
Right now you asked you for to give the gradients but
Thank you for this amazing talk.
The quick question maybe this is not so you're asking you're asking for hypotheses that separate the two data set.
And when you were testing them, they were like, okay, let me pick two examples and say one is more positive than the other.
But could you tweak this to possibly look for hypotheses that certainly answer yes in one case and no in the other case that you don't have to look at two comparative examples.
Like, okay, this one talks of a female character.
The other one talks of a male character.
So what would be just so I can understand like what would be an example.
So as an example, the second data, the second example you showed was that this statement talks of two female characters.
That's a statement you can answer in yes or no without looking at two different samples from the two data sets.
Yeah.
So could you tweak this to possibly just look for hypotheses that could be answered on a single data point rather than a comparison between data points from different data sets.
Yeah, so we actually, I think, worked with both versions of the system, one that is kind of unary predicates and one that's binary predicates.
I think in practice, a lot of the interesting things you want out are kind of binary predicates.
So you can get like you can get somewhere with this unary thing, but you're kind of losing something if you don't consider comparatives.
Okay, let's thank the speaker.

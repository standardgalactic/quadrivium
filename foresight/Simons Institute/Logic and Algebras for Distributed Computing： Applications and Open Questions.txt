Second talk today has a title that has evolved since last time I looked at it.
Okay, but it has logic and algebras and we are very excited to hear what this has to do with cloud computing,
which we are all subjected to in our daily lives.
Thank you. So this will be a bit unusual, both Joe and Connor will talk and I let them take
care of the logistics of that. Again, remember that for questions that might be more appropriate
for a longer discussion, we will have that discussion right after their talk.
Okay, thanks. So this is work that obviously we're doing here at Berkeley and I'm also funded by
Sutter Hill Ventures. So it's kind of cool. We've got venture capitalists funding basic research.
I have promised them that there's no applicable. I'm not really sure what this could turn into
as a company, but they're cool and they've given us developers to work on the project,
which has just been great and they're funding me as well. So thanks to them and to Berkeley.
There's this story people like to tell in computing. This is my standard opening slides.
Operating systems people really like this story because it's sort of the Thompson and
Richie Turing Award story. For every platform that comes out, there's a programming environment
that's somehow suited to that platform that emerges and as a result, people write things you
never would have expected on that platform and it succeeds. So the PDP 11 with Unix and C
is the canonical example of this, but one can argue that in every generation of new kind of
computing platforms, programming environments have arisen to allow people to build an app for that.
And so nobody expected all the apps we have on our phones. It's wonderful. Developers were
freed to write all sorts of things. Strangely, there's a platform that's as old as the iPhone
called the cloud. So AWS is approximately the same age as the iPhone, but it doesn't have a
canonical programming model. And there's many reasons why that might be, partly because it's
a really hard programming environment, yes. So it has to deal with all the problems of parallel
computing, as well as things like distributed consistency, what happens when you have partial
failures in your system, but it keeps running. So componentry is down, but the system's still
running. And then in the modern cloud, we want things to auto scale. So you allocate more machines
and then you free up some machines, but the program's still running. So the platform is
changing underneath you as you're executing. So this is all hard and programmers right now are
trying to do this essentially in Java. That's sort of the state of the art. And the annoying thing
is these compilers for these languages don't answer any of these questions that are hard. So I think,
honestly, this is like this hole in computer science that nobody's filled and it seems like
one of our grand challenges from my perspective. So I've been working on it for a long time,
and I think there's still a lot of work to do. I take inspiration, of course, from this gentleman
as maybe we all do. What was cool about Ted Codd was he said, look, you should write things in a
formal language. You should have formal specifications. And then there should be machinery that
automates the implementation. And if we do that, then the implementation can change
while the specification remains the same. This is very nice for things like databases, right?
So the thing is that Codd was trapped in this database prison for all these years. And I think
there's a much broader applicability of the design principle. So we worked on things in our
community like declarative networking. So we brought Codd out of the database and into the network.
So I've done some work on that. Many of us I think in this room have done something around
declarative data science and machine learning. This is a growing area, right? In the program
analysis community, the use of declarative languages has been pretty powerful. So that's
really cool. And then, of course, the hot thing, which is why we're all here, is that we're going
to start to try to look at this stuff through the lenses of algebras instead of logic or in addition
to logic, which is pretty neat. And we're going to, you know, we've heard or are hearing about a
variety of different algebras that people are playing with in this domain. So what I'm interested in
is taking Codd into the cloud, yeah? And here's sort of the analogy, the way to think about it.
The relational database was invented to hide how data is laid out and how queries are executed,
right? And all that should be decided sort of lazily based on the current environment.
Well, the cloud is just a generalization. It was invented to hide all your computing resources
and how they're laid out. So not just your blocks on your desk, but really everything.
And it's for general purpose computations. So the cloud, in a lot of ways, is this abstraction.
It's this physical layer abstraction. The physics of the deployment of your code is going to change,
but you want your spec to remain the same. That's how you'd really like to program in an
environment that is this heterogeneous and elastic. So I believe that it's extremely natural for
techniques that we've been working on in our community to try to be applied to cloud computing.
And we have a project in my group called Hydro, which you can read more about,
which I will tell you a bit about today. Okay. So what are my goals? Well, I kind of want to
build something like LLVM for the cloud. So LLVM, as you may know, is a very successful sort of
language stack. It supports many languages, including C++ and Rust and Swift and others.
It has an internal language called its internal representation, and then it compiles down to
a variety of machine code for different platforms. So it's been extremely successful, but it doesn't
answer any distributed questions. So if you're writing a distributed program, you might ask a
question like, is my program consistent in some sense? Or if I talk to different machines in the
network, will they give me different answers and be confused? That's a question that distributed
programmers need to deal with. Here's another one. My state no longer fits on one computer. How do
I partition it across multiple computers while getting the same answer out of my program?
I want you, you compiler should figure that out for me. What failures can my system tolerate and
how many of them before it stops working the way that the spec declares it should?
What data is going where around the world and who can see it? These are all questions distributed
systems always have to answer. And then I have different objective functions. So I'd like to
optimize some days for maybe my dollar spend in the cloud, but I don't care about latency or maybe
vice versa. Maybe I care about particular latency distribution. So I want the 99th percentile
of my workload to achieve a certain latency versus the 95th or what have you. These will all
lead to different decisions about resource allocation and program structure and so on, right?
And if you ask these questions of LLVM, that's the answer you get. It doesn't deal with any of
these issues. And that's kind of where we'd like to come in. We've written a vision paper a couple
years ago that I can point you to and I won't go through all of it today. But the idea is to use
database techniques and ideas to optimize in concert with LLVM. So LLVM is responsible for the
single node, but database techniques perhaps responsible for the messaging, the data movement
that happens in a distributed program. So here's how we envision the hydro stack. Many programming
languages up top. Some techniques to translate them into an internal representation. We've
got a little bit of initial work here. And then the internal representation should be some
formal spec that is global in some sense. So it doesn't worry yet about how many machines I have
or what the machines can do. It's just a formalized specification of what you wrote in a perhaps
imperative language. Okay, so it's kind of machine oblivious. Maybe it's a logic. Maybe it's an
algebra. Things that are in red are work in progress. Things that are in green kind of work
at this point. And then from there we want to build a compiler and we're working with Max
on using eGraphs for that compiler to translate down into a per node physical algebra. So every
machine would run its own little program. Those programs communicate with each other very much
like a parallel query plan is a bunch of individual query plans running on individual machines talking
to each other over a network. Okay, so this is a sort of per node physical algebra because it's
actually doing stuff. We've implemented this in Rust. It's very fast and I'll show you some of
this today. So that's kind of what we envision as how this is all going to work. At the bottom
there's something that's deploying this on machines and deciding how many machines and how few over
time. Okay, so some of the topics I want to talk about today we're going to focus on this piece of
the stack. Things in red are work in progress. Very much more questions than answers for sure.
So how do we take code and automatically replicate it along with its associated state or data
while ensuring that the program continues to produce the same outcomes as it would have on a
single machine? So I'm particularly interested in doing this in cases where the replication comes
for free and the individual machines don't have to coordinate with each other in a technical sense
I'll talk about. But we'd like to avoid replication when we can. We'll call that free replication and
this is the domain of the calm theorem which you may have heard of and I will review. Unfortunately,
the calm theorem was done in a very particular framework for the proofs. It's not at all clear
how it applies outside that framework and so what we'd really like is a more algebraic notion of the
calm theorem which is something that Connor's working on and after the talk if you're interested
come find Connor and or me to talk about that. Another topic that Connor's going to talk about
today is termination detection and again ideally termination detection where I can decide it locally
for free without asking anyone else. So how do I know in a streaming program that it's done
when there's other agents in the world? So we're going to talk about how to do that with threshold
morphisms but Connor's got ideas about more general notions of equivalence classes that may
allow termination in more settings. So he'll give you a flavor of this work in progress.
The third piece we may or may not have time for today but my student David Chu has been working
on this how do you take a program and partition the state of the program across machines if the
state doesn't fit on one machine. So you really need to partition the code and the data. This is
very much like traditional shared nothing parallel databases if you like but we want to do this to
full and rather complex data log programs and so we've got an implementation of Paxos which is
number of lines of data log where David's able to do some of this automatic partitioning.
Functional dependencies have a role to play here and it would be nice to integrate those into an
algebraic frame as well. And of course all this has to go fast and ideally as fast as
handwritten C++. I'm really previous iterations of my work we settled for interpreters and just
showing things were possible. Now we'd like to convince the systems people that things will be
as fast as they want them to be. So is this business just pie in the sky?
Let's see. All right so we're building on an obsession of some 10 to 12 years that I've had
now in my third group of grad students working in this area. So the initial batch of grad students
was doing formalisms. So we have this very nice logic called Daedalus which is a subset of data
log neg actually that allows you to talk about time and space in a particular way and it's got
a nice model theory. And so that works all there and we can use that as a basis for you know
our semantics to start which is nice. And then there was this lovely work that Tom Amalut did at
Hasselt the column theorem which was a conjecture I had and he and colleagues went ahead and proved
that talks about how things are monotone in some certain sense. Then you can do this free
replication. So you don't need to do coordination to get replica consistency. So I will talk about
the column theorem today to give a review. And then we actually built out a language. It was
slow. It was interpreted. It was written in Ruby but it integrated lattices into a data log like
language and we were able to show that you can get stratified negation and aggregation. You can
get morphisms on your lattices to allow you to do semi-naive evaluation even with the lattices.
So it's really actually rather a nice mixture of algebra and logic. None of this was formally
proved but it was I think one of the earlier systems to observe that you could build this.
That was pretty cool. And that's Neil Conway's thesis work. So we have this as a basis. This kind
of ground zero for my team. And then what happened is I had a batch of students that didn't want to
do languages or theory. So they just built stuff in the spirit of these ideas. And I'm not going
to go through all this but it's things like functions as a service and protocols and testing
and provenance. It's cool stuff. What I do want to focus on is one of those projects which was a
key value store. Key value store is just like a hash table. It's a database where you look
things up by key and you get a value. So you can think of it as a distributed hash table.
These are like the Petri dishes of distributed systems. You're basically saying I have a memory.
It's distributed across many computers. It may be replicated. It may be partitioned. But that's
what I have. It's just registers with values, keys with values. So there's no algorithms per se.
It's all just kind of like focusing on these semantic issues about replication and partitioning.
But the idea that was behind the Anna key value store that we built was everything's a
semi-ladys. And because everything's a semi-ladys and therefore associative, commutative, and item
potent, messages in the network can be replicated. They can be interleaved. They can be reordered and
everything will be fine. So if you design with semi-ladys from the bottom up, you can build a
system that does no coordination. So it's fully monotonic, which means everything can be replicated
as much or as little as you like. Across the globe, between disks and memory, you can replicate
lots of ways. You can do updates anywhere. So multiple updates to the same key can be happening
concurrently in different places at the same time. And they are merged by the lattice lazily via gossip.
And so you build this thing with no concurrency control whatsoever. So there's no locks in the
system. There's no calls to atomic instructions on processors. There's certainly no Paxos protocols
or anything like that. It's just put, get, and gossip. It's really a simple piece of software.
And so Chenggang Wu, the student who led this, won the dissertation award. I think it was very
well deserved because this system was really elegant and really fast. So to give you a sense
of kind of the lattices that he uses, he took from the literature a variety of these kind of
consistency models that people talk about in distributed systems. And he showed how you can
get them by building little composite lattices that wrap up the data. So this is what's called
last-rater wins in the distributed systems, which is just whenever you see a value that's
from later than your value, you update, otherwise you don't. And this you just, you know, take your
map, which is from keys to things, and you wrap the things in a lexical pair of a clock or a version
and the data, right? And you can only have one value per version. So this, this works out as a
lattice. Here's a fancier one, though. This is actually one of the strongest forms of consistency
you can get without coordination. It's called causal consistency. And here what you have is for
every key, you have a vector clock and the data. And the vector clock itself is a map lattice with
node IDs and counters. Yes.
Just want to say a little bit sort of operational transform.
A little bit. Yes.
Can you ever get stuck? Like, do repairs always exist or do you set it up such that they do?
So these, these particular well trodden forms of consistency work fine. And these lattices
are capturing that. They're saying, look, it's just merge. All right. And it's always going to work
because you've defined an associative commutative item potent merge function. OTs are like really
weird and full of all sorts of reasoning I don't understand. And they would never be able to have
such a simple assertion of correctness. All I'm saying here is it's associative commutative
of an item potent. I got nothing more to say. It takes a little bit of convincing to say that
gives you causal consistency, but it's not much convincing because this helps you make causal
consistency with vector clocks. So the observation that clocks and vector clocks are lattices is
just a nice thing about distributed programming. Yeah.
So what do you mean by everything is a lattice? So you mentioned that
Kira's story is a hash map. Right.
Every key has to be a lattice thing. So the nice thing about the map lattice is the keys
are not lattice values. The keys are just keys. The whole table is a lattice. But the object is
a lattice because what happens is the merge function is for a particular key. If there's
nothing, it gets the value you gave. Or for that key, if there's something there, you apply the merge
function of this lattice. So that is itself a lattice. And these lattice constructors are very
nice. We use map lattice. You see lexical pair over there. And these allow you to take simple
lattices like sets and counters and build up richer lattices out of them, which is a trick
our group likes to play a lot. Other groups sort of are doing research on inventing custom lattices
for custom problems. We've been very much in this kind of know let's just build it up from very
simple building blocks. So the quick version is it's monotone. And if it's monotone, the column
theorem says it's going to be free replication. So you don't have to do coordination to get
replicated consistency. Connor's going to give you a longer talk about this when we get to
conversation about semi-lattice. It's a semi-lattice. I should be clear. It's a semi-lattice.
Do you know whether the people working on coordination free replicated data structures
are well-pure? Yes, they are aware. And Connor will talk about it soon. Yeah, that will come up for
sure. Good. So just to kind of close out this anecdote with Anna, the system is ridiculously
fast. And it's especially ridiculously fast under contention relative to other systems.
So we compared against things like mass tree, which is from Harvard. It's 80 colors,
very fast key value store. We also compared against the multi-threaded
hash table that comes from Intel in their thread building blocks library. That's TBB.
And under contention, those systems, if you look down here, spend most of their time trying and
failing to get atomic instructions. So they'll say test and set on a particular memory address,
and they'll be told no, you have to try again. And they'll spend 95% of their time under contention
doing that, not doing useful work. So they're at 5% good put, if you're familiar with that term.
Whereas Anna, because it does no concurrency control, is just doing puts and gets and puts
and gets and puts and gets and spending most of its time doing good put. And that's why Anna can be
700x better throughput under high contention than these other systems.
But also because it does no coordination scales almost perfectly linearly across threads,
and then across machines, and eventually across the globe. There's really nothing to keep it from
scaling linearly, because the only extra work it has to do is some gossip. And that can be done
in the background, and can be done as lazily as you like without breaking the semantics.
So there's maybe a little fudging here on how stale your data is, but it's correct.
So this was a crazy fast system. And the thing about this, oh, and if you try to run it in the
cloud, it's also incredibly cheap to run relative to systems that are wasting all their time doing
this business. They're charging you for this. They're trying to get locks, they're waiting on
locks, and they're charging you money. So you'd like to avoid that if you can. Okay, that's all
very nice. But it was written in C++ by Chenggang, who's an excellent coder. His implementation is
correct by assertion. It would be really nice to be able to kind of do what CAD wants us to do,
formalize a spec that is correct, and then synthesize an implementation from it through
rules that are correct transformations. So we'd really like to do that, and we'd like to maintain
the speed. What kind of formalisms? Well, we're using lattices mostly. So maybe we could have a
type system that starts with some basic semilattices, like sets and counters, some composition
lattices, like key value pairs, products, lexical products, which aren't always lattices. So you
have to, there's some constraints on whether a lexical product is a lattice. And then we want
like a data flow, like a query plan algebra. So you can imagine a semi-ring kind of algebra, but
you know, there's going to be maps and folds, and then there's going to be physical stuff,
like scan a collection, or get stuff over a network. You know, networks do weird things,
like they permit things, and they form batches of things. They parenthesize streams, if you will.
They multiplex and de-multiplex messages. So there's some physical stuff that we want here too,
and I'd like to really be able to prove all that stuff is correct in a meaningful way.
So just for fun, I don't expect you to read this. This is the ANA implementation. You just saw
written in our low-level hydroflow language. This is the whole thing. It's a very simple program.
And you can see this is kind of a data flow language. It's basically specifying graphs of
data flow. The edges are directed edges in a graph. The words are nodes in the graph,
and you'll see familiar operators like map and join, and cross-join, and so on.
All right, and you can give views names. So this is a little name of a subgraph,
and we use it, and so on. So it's just a little language for specifying graphs.
This is a picture of that program that's output by the system. Okay, so it's a one piece of paper
program. So in this particular program, their union is actually joined, semi-latest join,
and that might be the only one. Yeah. Okay, and so just to convince ourselves this is fast,
that's Chengang's original numbers. We have it now running through hydro, that implementation you saw
on very similar machines, and we get very similar performance to the handwritten code. So we're
feeling pretty good that we're hitting our goals for performance. And because this graph is green,
it's telling us that this thing is all monotone, and therefore consistently replicable.
And at a glance, we can see this is a safe program. And I'm sort of cheating at this point,
and I'm going to confess to that. There's, I think, more work we need to do to make this robust. I
think these green edges are kind of a, they're slightly bi-assertion at this point. So I would
like to make them more fundamentally correct, and hopefully we'll have time to talk about that later.
Okay. With that, I'm going to hand off to Connor. He's going to take us through the next chapter.
Hello. People hear me? Yeah. I'm Connor. I'm a PhD student here working on hydro. I like systems
and theories. So I thought I'd show you guys some of the theory stuff we've been thinking about,
see if anyone has any thoughts, wants to collaborate on anything.
Okay. So in the classical database lens, we have, you know, these three layers,
the relational calculus at the top, relational algebra in the middle, and then a physical
algebra at the bottom, concerned with things like hashing and sorting and so on. And we can think
about how this changes when we move to the cloud setting. And there's good news and bad news on
the current state of affairs when we move to the cloud setting. The good news is that,
like Joe said at the top, we have this Daedalus language from the Bloom project
that is a data log like dialect for distributed systems. The bad news is that developers are
not asking for a data log dialect to build distributed systems in. The developers we've
talked to are a lot more interested in a functional algebraic looking interface and
especially something Pythonic looking like pandas. On the algebra side, the good news is that there
is an algebraic model for distributed systems today. It's the semi-ladis model that Joe has
mentioned that is referred to as CRDTs in a lot of places, especially in the programming languages
community. The bad news is that this is a model for coordination-free updates of state and it
doesn't actually have a query language or give guarantees about coordination-free-ness of queries
today. And then at the physical layer, when we add a network to the situation, asynchronous
computer network, a lot of non-determinism emerges that we need to be able to handle,
in particular reordering, batching, and duplication of messages.
So what we'd like to get to is unifying formalism across logic and algebra and this
physical algebra and have correctness at the physical layer that we can prove for
safe against this non-determinism from the network. And we're able to capture things like
replication, partitioning, batching, incrementalization, and termination analysis. We'll talk about
more later. All right, so let's talk about semi-ladis' CRDTs. So this is a model for
distributed systems that in databases we usually call the semi-ladis model. It's what is called an
Anna in Blum L. It came out of the programming languages community and there it's usually
referred to as CRDTs. It stands for conflict-free replicated data types. It's introduced in this
paper here and there's over 179 papers about CRDTs out there. It's also started to get popular
amongst software engineers and you see people talk about this CRDT model on places like Hacker News,
people starting startups with it. So what does it do? It tries to handle the sources of
non-determinism that come from an asynchronous computer network. These are the arbitrary batching
of messages, arbitrary reordering of messages, and arbitrary duplication of messages. And so it
turns out if you want to be robust to these three things, these actually correspond to algebraic
properties that you need to give you that robustness. So associativity gives you robustness to batching.
You're indifferent to the parenthesization of messages. Communicativity gives you robustness
to reordering and idempotence gives you robustness to duplication. And if you have a set with an
operator that satisfies these three properties, that gives you a semi-ladis. So that's why we're
talking about semi-ladises for distributed systems. So the conflict-free replicated data type is one
specific model of a semi-ladis interface, but since it's the most popular one today,
I'm going to talk about it. So the idea is that it's an object-oriented view of a distributed
system where you define some object, and you're going to define three methods on that object.
And then you can replicate this object across the distributed system,
and the replicas will converge, regardless of network non-determinism. So you have your merge
operator, which is your associative, commutative, and idempotent ACI semi-ladis operator, which
combines the state of two replicas. You have a way to update state, which the requirement is just
that that's monotone with respect to the partial order induced by this merge operation. And then
you have a query, which is just a method on this object, but today there's not a specific query
language. You don't have any sort of guarantees on what that query does. It just reads this semi-ladis
state. So we're looking at an example of a CRDT. This comes from the Amazon Dynamo paper for how
they're implementing shopping carts. And the idea is that you have updates that are going to add or
remove elements to your shopping cart. In this case, you add a Ferrari to your shopping cart, add a
potato, and you can also remove the Ferrari. And the state is going to be represented as two sets,
a set of items that you've inserted and a set of items that you've removed. And then to merge,
we do a pairwise union of these two sets. And the query, what's actually in my shopping cart I want
to check out, is set difference. Subtract the removes from the inserts. And there's a few
interesting things going on with this example. One is we're guaranteeing the coordination-free
rights that these two states are going to converge. But our query is a non-monotone query,
which the column theorem tells us is not a coordination-free query. So CRDTs are not giving
us the invariant that the column theorem requires on queries, which is that if we output a tuple
at a certain point in time, we're never going to retract that tuple in the future. Here, over time,
we will retract tuples as the remove set grows. We had a vision paper in BLDB this year about
this gap between what CRDTs guarantee and what the column theorem guarantees and ideas for how to
resolve it. Another thing you might have noticed that's kind of odd about that representation
of data is that if you think about how we might represent updates like this to a shopping cart
in a database, you might have imagined that we would have a count on each item and we would
increment that count when we add an item and we decrement that count when we remove an item.
This is what you'd see, something like incremental view maintenance where your update
operation forms an abelian group, not a semi-ladis. So why not do something like that?
Well, for one, it doesn't form a semi-ladis and it's not immediately obvious how to convert it
into one. So this representation is two sets, what you call a two-phase set. It's more obviously
monotonously growing update operation, but it turns out it actually is possible to convert
this abelian group representation into a valid semi-ladis in terms of being robust to network
non-determinism. I won't go into all the details on that, but it's based on what Joe is saying with
these vector clocks where you wrap the states basically in a vector clock which forms a semi-ladis.
The downside of doing this is that vector clocks require linear memory and the number of replicas
in the system, so that's the reason why people wouldn't use this representation today. But we
have some work on a protocol for enforcing this kind of conversion into a semi-ladis in
constant rather than linear space. So if anyone's interested in that idea, definitely come find me
and talk about it. Okay, so like I said, the semi-ladis model today does not have a query
language on top of it. So what might we want out of a query language for the semi-ladis model?
Well, we want expressivity. Like we saw in the shopping cart example, we need set difference,
so we need negation. Also recursion, something like datalog. We also want obviously classical
query optimization options. We want identities that we can use to transform our query,
get better performance. And we want to be able to do monotonicity analysis as well as functional
and dependency analysis for partitioning. And so something like datalog for semi-ladises
might be a good fit here, but there's a lot to explore. And so now Joe is going to talk about
this monotonicity analysis and functional dependency analysis.
I should say from the previous slide that some of this is things we took a crack at with
BlueMal, so it's not that we've done nothing here. There's some answers to these questions,
but there's also work to be done. All right, so I wanted to step back and review for you folks,
the COM Theorem, which I know is sort of in a sub-corner of the pods community and not everyone's
going to be familiar with it, but I think it's useful to go over. This will be high level,
but hopefully helpful enough for you to get into the game. So the challenge is that we're going to
have computers spread across the globe and we want our replicas to be consistent. So we have
this nice couple here, they're in different places, and the classic example of replica consistency
is data replication. So forget about programs, we're just going to have data, kind of like the
CRDT model. And I want everybody to have common beliefs about the data, at least eventually.
So these two folks currently both believe that X is love, which is lovely, but if it's a beautiful
variable, things could change, right? And that's very sad. And once they disagree on the value,
they might make decisions based on their disagreement that will lead to further
divergence. This is sometimes called the split brain problem, because you can't put it back
together later on, it's too messy. And so we want to generalize the idea of consistency of data
replication to consistency of program outcomes. So I'm not just interested in the data, I'm interested
in the queries, if you will, right? Much more powerful, and it will allow us to cheat sometimes,
the data could be inconsistent if the query outcomes are not, right? So it'll give us more
ability to relax our coordination. So we'd like to generalize this to program outcomes independent
of data races when we can. The classical solution to this stuff is coordination. This is what things
like Paxos and Two-Phase commit were invented to solve. And the way they solve it is by saying,
what if we were just on one computer with one processor? Maybe we could implement that in a
distributed fashion, which is a very heavy handed solution, right? You say that our solution to
parallelism is to remove it. And how can we remove it in the distributed context? Well,
it's expensive. But here's how it goes, right? On a single node, you use atomic instructions,
right? So if you have shared memory, you can use atomic instructions, or maybe you use a locking
system. In the distributed environment, you use something like Paxos or Two-Phase commit. And
at every scale, as you saw in that ANA work, you'd like to not do these things. So even on a single
machine, you really don't want to be doing coordination. And certainly in the distributed
setting, this is very heavy weight. And there's people who will tell you at great length why
they don't let the developers in Amazon call these libraries unless they have, you know,
16 gold stars. Because it will slow down the whole environment and create queue backups and all
kinds of horrible things. So when can we avoid coordination? This was a question that I asked
as a lazy professor, because I was thinking maybe I should learn and teach Paxos, and I kind of
didn't want to. So I was like, maybe, you know, maybe we don't need this stuff. Maybe Lamport's
just a bunch of bunk. So that's kind of where this started, sheer laziness, intellectual laziness,
which I will cop to. But what it led to, sometimes when you ask a question about how can I be lazy,
you end up asking a question that turns out to be quite interesting. I think that's what arose here.
And I'm seeing this not only in my work, but in other places. Back in the 20th century, if you
will, the Lamport Gray era, we were trying to emulate sequential computation. We were doing
everything we could to give the programmer the illusion of a sequential computer. And it was all
about, you know, very low level stuff, reads and writes, accesses and stores, right? And then
guarantees of order, total order, linearizability and serializability. And this was all based on
the idea that programmers are hopeless. They'll write all kinds of crazy code. And the only thing
they understand is sequential computers. So we'll make the worst case assumption that their stuff
wouldn't work in parallel, right? And we'll give them mechanisms for avoiding parallelism.
Seems like a terrible thing to do in a parallel environment. Yeah. So what's happening in the
21st century is if we lift our, so this is all great. And sometimes you need it. I don't mean
to denigrate the work. This is obviously foundational, touring awards. I use this stuff. I teach this
stuff. It's all good. But when we don't need to use it, even better, right? So people have tried
doing things like, what if all our states are mutable? That's a very functional programming game.
It was sort of in my world, it's more about, well, you can mutate things as long as it's
monotone. So if they're mutable, but they're monotone, maybe that'll work. And then using
things like dependencies and provenance, all our ways of using application knowledge to avoid using
the expensive stuff on the left. But the really big query is when do I need coordination and why
do I need coordination? So if you ask, you know, a typical undergraduate or frankly, most people in
computer science, including professors, when do you need coordination? What's a lock for, right?
They'll say, well, it's to avoid conflicts on shared resources, right? This intersection needs
coordination. If you would just put up some damn stop lights, right, then, you know, north, south
could go for a while and west, east, west would wait. And then east, west would go for a while,
north, south would wait, problem solved, right? But like, do I really need coordination? That's
a solution. Is it the only solution? No, it's not the only solution, right? Here's a coordination
free solution to that intersection problem, right? So I'd like to be able to think out of the box,
right, and say, really, what is coordination for? Why am I required to use it?
Okay. So that's a theory problem. So, you know, which programs have a coordination free implementation?
We call those the green programs. These are specifications for which a clever programmer
can find a coordination free solution. And then, of course, there's the rest of the programs,
right? And I want to know this green line. Will someone please tell me, you know,
Mr. Lamport, I think I only need you out here. So will you please tell me when I need you? And
there's no answer from Mr. Lamport. At least he didn't, you know, pick up the phone when I call.
But I'm happy to say that people at Hussalt did. And this is what led to the column theorem. So
this is really a computability question. What's the expressive power of languages without coordination?
Yeah. That's the green circle. Okay. So give you some intuition. Easy and hard questions.
Here's an easy question. Is anyone in the room over 18?
Excellent. Not only were you all happy to answer that coordination free, but you engaged in a
little protocol, right? You made up a protocol where you raise a hand if you think it's true. So
that was cool. So that was the monotone hand raising protocol or something. Great. All right.
Who's the youngest person in the room? Oh, we have some brave assertions. But clearly,
you don't know that. You could look at everyone, but that's cheating and also not necessarily
right. Maybe, maybe. I don't know. I don't know. But the point here is, right, that somehow this
requires you to communicate with people. And the first one maybe doesn't. Okay. More to the point.
Let's look at the logic here, right? This is an existential question. And this is a question with
the universal quantifier in it. Or for people like me who just want to do total pattern matching
and look for not symbols, that one appears to be positive. So I'll say that it's monotone.
And that one appears to be negative. So I'll say it's not monotone. So it gives you some intuition
that universal quantification or negation requires coordination. It is coordination. That's what
coordination is. It's universal quantification. So what is Lamport for? It's for universal quantifiers.
So let's just prove this, right? I was like, well, somebody prove it. I'm not going to prove it.
So nice guy named Tom Omelette wrote a thesis on this stuff. My conjecture was called the
calm conjecture consistency is logical monotonicity. It was in a Paz keynote that I was gave some years
ago. And then just a year later, there was a conference paper from the good folks at Haselt,
which then they extended and then was further extended with weaker definitions for the monotonicity
to really expand the results. If you want a quick, you know, kind of a version of what I'm saying now,
you can read this CACOM overview, but it's really for systems people. I think you guys should just
read Tom's papers. All right. To give you a flavor of what Tom did, definitions are half the battle.
It seems, you know, when I read Paz papers, that's all the hard parts are the definitions, right?
So, you know what monotonicity is in logic? That's fine. What is consistency here? Well,
we want the program to produce the same output regardless of where the initial data is placed.
So, I should be able to start the program with the data replicated pops possibly and
partitioned in any way and get the same answer. And if that's true, then it would be the same
answer across replicas. It would be the same answer across different runs. It would be the
same answer if we start gossiping the data between each other. And this is what we want,
right? So, that's our definition of consistency, where I think what's really clever and was the
most beautiful part of the work is defining what coordination really means. So, we're sending
messages around, right? That's data. But which data is really data and which data is kind of
control messages? And how do you differentiate those in a formal way? And so, what they define
in this paper is program is coordination free if there's some partitioning of the data when
you first start. So, there's some way out of the data where you first start, such that the query
can be answered without communication. So, for a particular query, for a particular data set,
there is some world of where you lay it out where no communication is required.
That's the definition of coordination freeness. And a program that you can't do that on is doing
coordination messages. So, it's not really saying which messages are coordination and which messages
are data, but it's telling you which programs can be run coordination. Yes. So, the trivial example
of this is you put all the data in one node. And again, you know, this question of is there anybody
who is older than me? What you don't know is whether anyone else has data. So, I may have all
the data, but I don't know that. So, I still have to ask everybody, anybody got any data?
And I have to wait for everybody to respond, right? So, it's a very nice intuition to just think
about having all the data. All right. There's another thing in the paper that I hadn't even
anticipated, which is really beautiful and speaks to stuff that the distributed systems
community kind of knows, which is there's a third equivalent, which is that you can
distributively compute this program on an oblivious transducer. And I haven't even talked
about transducers yet, but just a minute. But what does it mean by oblivious? It means that
the agent in the system doesn't know its own identity. It cannot distinguish messages from
itself from messages from anyone else. So, it doesn't actually know who itself is. And it
doesn't know the set of participants. We call this an oblivious agent, right? Oblivious programs
that can be computed by oblivious agents are exactly the monotone programs and exactly the
coordination free programs. So, that was very cool. And it speaks to questions of like
membership protocols in distributed systems, which is about establishing common knowledge
of the all relation. That's like one of the things that Paxos does is it has a membership
protocol built in. So, it's one of the reasons it's coordination full is to establish all.
So, this was really, really nice. So, this is all in this JACM paper. It's actually in the
conference, the Paz paper as well. That's just a flavor of the calm stuff. And I'm going to stop
with that. But happy to answer questions as best they can afterwards. And with that,
I'm going to give it back to Connor.
You guys can still hear me? All right. So, we have this calm theorem view of the world,
relational transducers, logic, operating over sets. And then we have this semi lattice
algebra view of the world. And they both are dealing with coordination and freeness in different
lenses. But currently, they guarantee different things. The algebra view, like we said, is
concerned with the coordination of freeness of rights and does not guarantee coordination
of freeness of queries. Whereas the calm theorem view is only concerned with the
coordination of freeness of queries. It actually doesn't have to worry about the
coordination of freeness of rights because it assumes operating over sets and gets
coordination of rights for free that way. And so, we're interested in the question of
how can we combine these two lenses? Can we do an algebraic view of the calm theorem?
And some intuition for how that might work is, you know, the semi lattice operator induces a
partial order. And so, instead of having monotone logical queries without negation, you have
monotone functions between lattices, between partial orders. So, that's something we've
been exploring. We'd love to chat more about it with folks. I'm actually going to talk about
a problem specific to the question Remy asked. It comes up in this setting.
So, the calm theorem is all about basically not knowing when you have the entire input. What can
I output and tell downstream consumers with certainty, even though I might have more updates
in the future, there might be more messages arriving. And so, we call the ability to do this free
termination without coordination. What can we be sure that we can output? And we're exploring
this in a very generic setting of just we have two functions, an operation that's going to change
our state over time in a query that's going to return some output. So, looking at an example
of when we might be able to do this, we can look at this lattice. This is the set over these four
socks and say that this is our state of a local node and we're at top in this lattice. And our
update operation in the CRDT sense is union. So, we're going to union in more socks. We know that
if we're at top, as updated as monotone, we'll never change our state. We're stuck at top.
And so, whatever query we might be asking when we're in this state, we'd be able to locally
detect that our query result is not going to change in the future and we'd be able to return
our result with certainty. This might sound like it would not happen particularly often,
but let's try and look at more examples where we would be able to figure out that with certainty,
we can return an answer right now. So, what if we consider also the query that we're asking
and say that this query is going to map us from this set socks lattice to the boolean lattice,
true false, where top is true. Now, if this query is monotone, meaning as we go up in the partial
order of socks, we also go up in the partial order of false and true, then we don't need to be at
top on the left. We can actually be at top in the true false lattice and guarantee that our result
won't change as future updates arrive. Any update that arrives is going to cause us to increase
monotonically in the domain, which has to increase monotonically in the range and therefore
our result will always stay true. For example, a query, is there a pair of socks?
So, we call a threshold query. It effectively draws a cut through this partial order and says
everything above this line in the partial order is true, everything below this line is false.
So, these boolean threshold queries are a class of queries that we can freely terminate
on if we know that our update is monotone. What about a totally different setting?
What if we throw away monotonicity? So, now imagine that we have a deterministic finite
automata and our query is mapping to true and false from accept and reject states in the automata
and our update is appending a character. So, we think of a streaming
character as appending. So, each update is going to transition us one step in this automata.
And in this automata, any state that we're in, we can't conclude what the final result will be
because from every state, there's a state that's accepting, that's reachable, and there's a state
that's rejecting, that's reachable. So, some sequence of future updates might take us to
false, some sequence of superstructures might take us to true.
In contrast, if state three were also true, now from state one, we actually don't know if we're
going to end up accepting or rejecting if we don't have the whole input yet. But if we're in states
two or three, we know that every state that's reachable via future updates is going to keep us
in our current result, which is true. And so, we can be certain that we can terminate here and return
true. This is kind of like a reachability sort of visual view of how we're thinking about whether
or not you can freely terminate given some arbitrary update operation on a domain and query operation
that maps you to a range. This is a question raised in exploring a lot of different domains. If
anyone has any ideas for what might connect to this, definitely come find us. Now, Joe is going to
talk about partitioning. All right, this is a bit of a survey time. Boris, did you want to ask a
question, Connor? I have a question to the last slide, right? Because in a sense, this now still
has some monotone ordering, right? This kind of like, in a sense, it's kind of like if I can reach a
node and it's smaller and now basically the nodes, if the terminal nodes are the top nodes and they
don't have larger nodes, then so it's still monotone in the sense. Yeah, you can find some sort of
it that, yeah. Yeah, I don't know if that's true for every freely terminating function.
Psycho, do you think? Yeah, maybe. There's something about quotient lattice is, too, going at it in
partial orders. Yeah, the graph looks like one. So this is love to have this conversation afterwards.
That's why we're touching on a few things so we can have many conversations. So I think given time,
I'm not going to go through this in any detail. I'm going to basically skip this chapter of the
talk, except to quickly give some assertions. So first of all, we don't have HydroFlow, so we
use Daedalus and we do have a full Daedalus to HydroFlow compiler. So we're able to write global
programs in Daedalus and then auto partition and auto replicate them. And that's work being led by
David Chu, who's in the room over here. David, three years ago, promised to do this and they
gave him a certificate saying that's cool. So he won the SSP student research award. And three years
later, he's got his first results. So it took a while. This was not an easy problem, but he's able
to take arbitrary Daedalus programs, which are Daedalug, and partition them to run on multiple
machines. And I'm really not going to spend a lot of time on this. What I'll say is that
earlier student Michael Whitaker, who again did this by assertion, he found all sorts of opportunities
to optimize Paxos because inside of Paxos, it was bottlenecking on things that were trivially
parallelizable, like network message handling. So he's like, if I can just bust apart some of the
roles in Paxos into sub roles, some of those sub roles can be replicated. And he got state-of-the-art
performance in terms of throughput on Paxos by doing this. And what I observed after he did it
was, oh my gosh, most of the things that you've split out are monotone subcomponents. And I should
have known that we could pull those out and replicate those. In fact, I wish Bloom could do
that automatically, but it couldn't. So three years later, David can now automatically pull out
these things that Michael was observing and transform the program to do it. And the ideas are
basically just two tricks. One trick is to take a pipeline on a single machine and split it across
two machines. He calls that decoupling. Now in a general purpose program, this is taking,
I don't know, 10,000 lines of C++ and figuring out which ones to run on this machine and which
ones to run on that machine. That would be horrible, right? But in a data flow language or
logic language, it's quite nice. And so he has conditions for when this is safe and when it's
not. So that's decoupling. So you can think of this as refactoring a program into components
that can be run on different machines with asynchronous communication. The other thing
he does is what's sometimes called sharding in the practitioner community, but it's partitioning,
shared nothing partitioning of subplants, right? So instead of having BC take all of the data from A,
you have a hash partitioning here and certain values go to each machine. And how do you know
that each one of these computations can be done independently? That's done through things like
functional dependency analysis so that you can show that certain values have no dependency on
other values because they're partitioned by, say, NFD. So I'm not going to go into any of this,
but basically what David was able to do was take many of the optimizations here that were
handwritten in Scala and automate them and formalize their correctness. And without getting
into too much detail, although it is kind of fun, oh, and we borrowed some work from Paris. So
shout out to Paris for parallel disjoint correctness and colleagues. It is really fast. So he was
able automatically. This is Michael's results that we re-ran. This is Scala. This is throughput
against latency. So what you want to do is you get as much throughput as you can until it starts
to hurt you at latency and it curls back. So this is kind of where things start to
top out, if you will. So that's Whitaker's implementation. This is the same logic as
Whitaker's implementation in Hydro. So this is just Hydro is faster than Scala written by hand.
So this is just a testament to the folks who wrote the Hydro flow engine. But the blue line is
what David achieved through systematic correctly proven rewrites. So he was able to get performance
that actually, because Hydro is fast, is better than Whitaker's Paxos implementation. And this gap
is kind of what he's given up. These are the tricks that Michael had that we didn't cover in our
rewrites. But we're doing 90% with the easy tricks. So it gives me confidence that simple
query optimizations known in parallel databases can have impacts on places that we're really very
fine tuned performance issues that people write PhDs to get this kind of performance and we're
getting it through systematic rewrites. Very promising. David's only halfway there though,
because he has to have a proper cost-based optimizer. Right now what he has is correct
transformation rules. He needs a cost model with an objective function. And then he needs a search
strategy. And we're hopefully going to be using egg log or some implementation of egg log in Hydro
flow to achieve this. So we're one of the things with Maxis stuff that overlaps is if there's a
lovely semi lattice based data flow implementation of Maxis stuff, maybe we can clean up some of the
things where he's doing updates in place. This work? This work. Well, so this was written in
Datalog and translated down into that Hydro flow data flow language you saw at the top. This stuff
is also written in Datalog currently in a runtime that plays some ad-hoc tricks. That's not traditional
Datalog execution here as Maxis. But I think, you know, Union Find is a nice target for an
algebraic treatment and I think we have opportunity. Okay, what I'd like to do in the last few minutes
is berate you with questions because these are the things that I don't know how to answer yet and I
would love to get help with. So the first is, and this is an outline, so this section goes on for
many slides, but there's the four questions. Can we please have one theory for all this nonsense
instead of the list I'm about to show you? What would be a good type system for the physical
layer where we could prove correctness of transformations? I have a follow on to Sudipa
about the role of our kinds of work in the era of generative AI. And then I have this ongoing
question of what time is for, which I probably don't have time to explain. But quickly, you know,
the unifying theory thing. So CRD teaser semi-ladyses, Datalog, Daedalus was all done with model
theory and it's fancy actually. It uses like stable models and stuff. It's actually ended up
being kind of fancy. The column theorem, Amalut, proves have these relational transducers, which
are this halfway world between operational and declarative semantics. You have these state
machines on each node. They run declarative languages on each step, but then they output stuff and
I think you can write non-terminating programs if you want to.
So you can write Toggle, for example, and Daedalus and the transducers.
Now they don't have to be terminated. In any sense, I don't think. But the point is,
I really wish he'd have done this work with this, because he also was on this work, but he didn't.
He did it with transducers, which is a bummer. If you talk to distributed systems people, they
talk about essentially order theory. They talk about partial orders all the time, which is related
to lattices, but you know, it's annoying. Programmers want to write these sort of algebraic functional
expressions, which I think is a good thing for all of us. And then yeah, I give all these talks
and then some joker raises their hand and says, well, what about transactions? And in fact, Peter
Bayless, when he was a student, basically did an end run around my entire group and just wrote
papers about transactions and coordination, and they don't align with the rest of this stuff.
So it's an open challenge to reintegrate that work. And then, you know, I didn't actually say the S
word yet, because I apparently didn't do joins as of yet. But we do do joins, so we probably need.
So it would be really great to get all of it here. I would like to bring all of this work
into this domain. That would be really nice. Okay. Here's a flavor of what I'm dealing with,
though. So just finished reading the DBSP paper, which was very nice and related to our work, but
we have some other things we need to keep track of that are relating to the network messing with
stuff. So when we look at a stream, it's got some values. It's got some happenstance ordering,
that's a mapping of the naturals to those values. It's got some happenstance batching. It came in
over the network in chunks. So there's like singly nested parentheses in this stream that are
randomly placed. Randomly, you know, arbitrarily ordered, arbitrarily placed. Maybe this is a
stream of lattice points, but maybe it's not. I don't know. But if it is, you could say things
like there's a monotonicity relationship between the type's natural order and the total order of
arrival or not, right? And then what sort does is it enforces something like this, right? It's
nice when these are atomistic, like data flow is basically a set lattice that you flow individual
atoms. That's the traditional kind of database iterator thing, one tuple at a time, right?
One tuple at a time is an atomistic lattice of the sets of tuples. And it's nice when you know
you're dealing with atoms, because you can say things like item potents, right? I gave you
Bob's tuple once, I gave you Bob's tuple twice, sorry, but you should only have it once. So delete
one. But if I give you subsets, now you have to find how they overlap and you have to make sure
that when you union them, you remove the overlapping bits. And so when you have non-atomistic things,
it's just a little uglier. And you end up talking about like, does their meat, is there meat bot?
kinds of things. Do they have no intersection, right? So these are the kinds of properties
that I think I need to track in my rewrite rules. And then, you know, the operators are the invariants
to these properties, like lattice, lattice operations are invariants of order and parenthesization.
Do they preserve the properties? Do they enforce different values for the properties?
The network non-deterministically enforces orders and parenthesizations. So like this is the stuff
that I worry about in my operators. And this is kind of the soup I'm swimming in with this
physical algebra. So I would like help with this. All right, I'm going to do one more quick slide.
This is very much in the realm of what Sudipa was talking about. You know, we're in the era where
people will be programming with green goo, right? It's just this is large language models, they're
magical, they produce stuff. But what we really want is building blocks that we can count on,
right? We're a database people, our industry is all about foundational building blocks.
And I really do think declarative specification is a lovely narrow waste here between these two,
where we can take a formal spec as Cod asked us to, we can render it in some sense so that it's
readable, right? And this relates to things like Wolfgang's work on visualizing queries,
and what Sudipa was talking about in terms of giving natural language things, but helping
people look at this and say, is this what you meant? Not is it correct, but is this what you
meant since a spec after all, right? Did you mean this query? And then of course, if it's in a nice
formal language, we can check it for other things, right? And so that would be, I think, a role that
we can really play in the world. And I suspect things like this will happen. These programs are
going to be a selection of programs. You're constantly going to be given a menu of which of
these things did you mean. And the answer to which is either some invariant checks or something,
or some human judgment. So I think we're in a good spot in terms of intermediate languages.
And I'll just close with one more slide, maybe just a handful of slides.
What are clocks in time for in distributed systems? So there's this famous saying, which
is correctly attributed to a sci-fi short story. Time is what keeps everything from happening
all at once. So when should we use time and computing? What are clocks for? Well,
they're not for monotone queries. I can run this embarrassingly parallel. It can all happen at the
same time, and it's fine. And Buntalu was doing this long before this discussion. But I can't run
this at the same time. You can't have P and not P at the same time. So what's the deadliest answer
to that is, well, that's what time is for. This is the toggle program, right? And time is there to
separate two things that can't coexist. That should be, I think, the only reason for time.
Except it's not. Distributed systems, people use time for maintaining partial orders and knowing
when things were concurrent. Sometimes you don't need that. Sometimes you do. But this is my question,
especially because Val's in the room and worked on this DSP stuff. Daedalus has one clock per
node and we update it only when there's a network event or we have to cycle through negation.
Timely and differential data flow have clocks all over the damn place. And I'm not sure when you
use them and when you don't. So, for example, tracking iterations of a monotonic recursive
program. Why do I need a clock for that? I don't think I need a clock for that. Maybe if it's a
while and we use the index in our computation, I need to know what index I'm at. So the general
question is, when do we use this structure called a clock? And when don't we need? And can a compiler
decide? All right. We are a little over time. I hope we have given you lots of things to ask us
later. We need lots of help. So we'd love it. And we are big fans of working with folks like you.
Can you leave these last four slides there so we can come up with some more folk questions?

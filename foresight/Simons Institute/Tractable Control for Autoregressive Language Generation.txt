pro six circuits on tiny binary assets.
So what we'll do today is actually train PCs
on language, language models,
and Juan-Wes going to tell us how that works
and what that's useful for.
So today I'll present our paper,
Attractable Control for Autoregressive Language Generation.
So unlike probably the interesting ones,
today I'll focus more on the application side
of probabilistic circuits.
Like more machine learning oriented.
Okay, so let's get started with the basic concept
of large language models.
They have been like really popular recently.
I'm not sure if people here really care or not.
Like I see like this interested face you do.
Okay, that's a good sign.
That's a good sign.
So basically the idea is very simple.
In the sense that you collect large amount of text
consisting of trillions of words,
and you train neural networks with billions of parameters
on these data, then you have these so-called large
language models.
These chat GPT or GPT-4 thing has become like really popular.
People talking to them on the internet,
asking them to polish their papers,
using them to do their homeworks,
and even asking them to play D&D together.
So yeah, that is actually true.
So the idea is, so people have now having this feeling
that we probably have solved AI,
we have solved artificial intelligence,
and chat GPT has passed Turing test,
but is that really the case, okay?
So I tried this a few months ago.
It was an actual conversation between me and chat GPT.
So I asked the model to generate a sentence
using Frisbee, Cod, and Dog following the given order.
Well, I mean, this should be quite simple
for a super intelligent AI model, right?
So it does give me a sentence.
The sentence look quite decent,
and all the key words are in there,
but Dog and Cod are in the wrong order, right?
Well, I mean, humans make mistakes,
so super intelligent AI does.
So I was being patient, I tried again, okay?
Again, it gives me a sentence,
but it's even worse now.
All of the words are in the wrong order.
So you see that chat GPT fails to follow
even this simple logical constraint, okay?
So people have been trying to fix this.
Some of the methods, including like,
okay, we can prompt them in a different way,
like chain of thought.
I'm not comfortable saying this phrase,
but that's one of the methods.
Or people try to use search algorithms
trying to find the correct sentence
in this huge search space, so on and so forth.
But none of them actually guarantees that chat GPT
or whatever other language model give us what we want.
So what we actually want is like 100% guarantee.
When we have like a huge powerful model,
when we instructed to do something,
we wanted to follow our instructions exactly.
So in today's talk, I'll show you how we can do this
with probabilistic circuits.
Before some of the detail,
so let's get started with some basics of language modeling.
So language modeling is really not like a fancy idea.
It's just a joined distribution over some text,
probably less than or equal to n words.
Each random variable here is a word taking value
in a fixed, in a vocabulary of finite size.
The size usually in practice ranges from 10,000 to 50,000
or something, okay?
And here's some examples.
You might notice there's some special word called EOS here,
which means end of sentence,
which is only like a special token or word
we use to pad sentences of different length
to the maximum length.
So we have this like joined distribution well defined, okay?
So we collect a lot of text of this form,
sentence fragments, paragraph fragments,
and we train this joined distribution
by maximizing a lot of likelihood, a very, very simple idea.
Okay, so what about the architecture
of this distribution of our model?
Well, like the most popular ones, like GPT,
they are autoregressive models.
Well, so by the chain rule of probability,
we can decompose our joined distribution this way.
Represented as a graphical model is basically
for each random variable,
we have arrows going from all the previous random variables.
From a generative point of view,
it's basically we start with,
I can't see my cursor.
So we start generating the first word
from this prior distribution.
Once we have the first word,
we move on to the second one,
conditioning on the first one,
and then we generate the third one,
conditioning on the first two words, very simple, okay?
By some classic results,
marginose for Bayesian networks
with these structures is intractable.
Okay, so before when we asked GPT
to generate a sentence for us,
we are like talking to it as if it's human, right?
So the assumption is if our distribution over language
is a perfect distribution,
then we should be able to interact with it as a human.
When we're doing these kind of like prompting,
when we're talking to it,
most of the time we are actually trying to do conditioning.
So let's consider an even simpler example.
Suppose in our autoregressive generation,
we have generated the first three words, the weather is,
and our constraint is that we want the sentence
to contain the keyword winter.
Well, I mean, we wanna write something
with the topic of winter, so that's our constraint.
What we really want is, okay,
so what language model gives us is the next token,
next word probability.
So basically given the weather is,
it might prefer whether it's warm or cold,
while its prior distribution might want the weather
to be warm.
But what we actually want,
but what we'll actually need
is this conditional next word probability, right?
We want our sentence to satisfy the constraint,
once it contain the winter,
and well, intuitively,
if we want the winter to be in the sentence,
weather is more likely to be cold, okay?
But this is intractable for those autoregressive models.
So why is it intractable?
Let's go more into the details.
So by Bayes' rule, this conditional probability
can be decomposed into these two terms.
The next term here is very simple,
it's just a language model next word probability.
But the first term here is actually a marginal probability.
So it's basically the marginal probability
over all possible suffixes that we could generate
that contains winter, okay?
So this is intractable for autoregressive models,
but we have PCs, right?
PCs are tractable, at least,
we know how to compute marginal probabilities with them.
So why don't we just approximate this term here
with probabilistic circuits?
And we refer to our pipeline as gelato,
in generating language with tractable constraints.
Okay, so how do we do this?
So the first step is that we pick our favorite PC
for sequential modeling, which is a hidden markup model,
the simplest we can have.
We have, on the other hand,
we have our pre-trained language model
we want to control or guide.
We sample a lot of data from our language model
unconditionally, and then we train our hidden markup model
on these data with maximum likelihood estimation.
So effectively minimizing the KL divergence
between the two joint distributions, okay?
And our assumption is that if the joint distributions
are close enough, then all of the marginal distributions
and conditional distributions are gonna be similar, okay?
Any questions?
So the intuition is that we have a black box
autoregressive model, and we train
some sort of white box PC,
so we can use as a representative of the black box, okay?
So we don't want to, since this is the workshop
for circuits and logic, let's represent HMMs as PCs.
So here is the graphical model version of PC.
The Zs here are the hidden variables, the latent variables.
And the X here are the observed variables, or the words.
And let me use the whiteboard to show you how to do this.
Okay, so one assumption we have is that our hidden states,
our latent variables, are discrete variables
taking values from one to H, and H is our hidden state.
Any questions?
Okay, so we start from the very beginning.
In an HMM, we start with the initial state, Z1,
and it has H choices.
So we want these eight product nodes to be representing
the probability, can people see it actually?
So representing the probability of X1 to N
conditioning on Z1 equals to I.
So I would be the hidden state, so I will,
this will be one, two, and H, okay?
And the edge will have weights
of the prior distribution on Z1.
Okay, I see nodding, I see confusing faces,
but I'll move on whatever.
Okay, so it'll be clear later.
So given, so by the Markov property, given Z1, X1,
and so basically given Z1, this part
and the remaining part will be independent.
So let's deal with X1 first.
We have some input distributions,
and this input distribution will be representing
the probability, the emission probability,
basically X1 given Z1 equals to I.
Okay, so basically here, we're in the states
of Z1 equals to I, and this will represent
the probability of X1 given at Z1 equals to I.
I see more nodding faces now.
Okay, and we want this part to describe
the remaining distribution, so we have some nodes here,
and they will represent the distribution of P of X2
to N given Z1 equals to I, okay?
Corresponding to the inputs.
And so we proceed to the next layer.
And this part will be the transition probabilities,
will be the transition probability of Z2 equals to J
given Z1 equals to I, and these will recursively,
we do this construction recursively,
will be representing the probability
from X2 to N, conditioning on Z2 equals to I, okay?
Does that make sense?
So we just proceed, so it's like Antonio and Robert
was trying to make the point,
this is a tensorized layer representation of a PC.
Okay, let's move on, okay, so now we have our,
oh, sorry.
So let's move on, so we have our circuit
representing the distribution over text,
then we need to answer the query,
we need to encode the logical constraint
as the logical circuit.
Let's go back to the constraint we have in the very beginning,
we want Frisbee, dog and dog.
Frisbee, caught and dog to appear in the given order.
This is like a naive way to represent this constraint,
basically the IJK here are enumerating all possible
positions of these three words,
and we take a conjunction whenever we know the positions.
Any questions?
Okay, okay, but this is not really ideal,
we can directly convert it into a PC
that represents the constraint,
but what are the problems, do people see that?
I'll give a hint, complexity,
what's the complexity of this,
what's the size of this DNF?
Yes, cubic, cubic, and to be more precise,
it's n choose number of keywords, right?
Well, I mean, we can do it like cubic,
but suppose we have five or 10 keywords,
we can no longer take that complexity, okay?
And the other problem is more subtle,
which is that this DNF is not deterministic, okay?
So we know that we can multiply circuits
when they are compatible,
or when they are structured decomposable
with respect to the same vitri,
but here we want exact conditioning,
so we actually need to make sure that our circuit
represents a uniform distribution
over the support specified by this DNF.
DNF, and in general, it is sharply hard
to do model counting to normalize everything.
Okay, I'll go into the details on the board.
Yes, yes, I'll talk about it on the board.
Okay, I'll move to the other side.
Okay, so for the first question,
why does it need to be deterministic?
It is because so, okay, suppose we have a very,
okay, so suppose we have a very simple distribution,
X1, X2.
So, we have some weights,
so this is a distribution over two random variables,
okay, and this is its probability mass function.
So when we are conditioning,
we are actually selecting all the terms
that satisfy our constraint
and zeroing out everything else, right?
So if we want to do that with circuit multiplication,
suppose our constraint, our support,
would be something like X1, X2, and X1 bar, X2, okay?
So the constraint circuit
should be a uniform distribution over its support,
otherwise it messes up with the original weights, right?
Does that make sense?
Okay, so however, okay, let me,
so this will be 0.5, X1, X2 plus 0.5, X1 bar.
Well, I mean, suppose,
and we have a bunch of zeros,
and we multiply these two circuits point-wise,
and we kind, we keep these two terms
and erase these two terms, right?
But we want W and W3 to be proportional to each other,
like the ratio should stay the same.
So this circuit must be a uniform distribution
over the support of constraint, okay?
So given a logical circuit,
how do we convert it into a PC
that represents a uniform distribution over the support?
To do that, we need to do model counting,
but model counting in general is hard
if determinism is missing.
So this one won't work.
Does that make sense?
Yes, no?
Robert?
Isn't that full of a new mistake?
Oh yeah, so it's a very subtle, subtle thing.
So, well, we only require Frisbee, Caught, and Dog
to appear in some positions,
but we do not limit that they have to appear exactly once.
So you can totally have Frisbee, Frisbee, Caught,
Caught, Dog, Dog, something like this.
So we have two sub-sequences.
We have a lot of sub-sequences, right?
So this is position one, two, three, four, five, six.
So when IJK are one, three, five,
it satisfies the clauses, right?
One of them, right?
Let me choose a different way.
Okay, so basically for this huge disjunction,
for each instantiation to the variable,
we only satisfy one of the clauses, right?
We want, that's determinism.
Yes.
And suppose IJK are one, three, five.
Does this instance satisfy that?
Yes, it does, right?
But when IJK are one, four, five,
it also satisfies that.
So it's not.
Does that make sense?
Okay.
Okay, so now let's construct
a deterministic circuit representing this constraint.
The idea is similar to a deterministic finite automata.
Basically, you track how many words have you included,
how many words have you used,
which state are you in satisfying the constraint?
So we want to construct these sub formulas,
V, T, say cod and dog.
The sub formula representing the constraint
that cod and dog appears
in XT, XT plus one.
So how do we construct this sub circuit or this sub formula?
Oh, it's pretty simple.
It's a sum node.
We consider two different cases.
One of the cases that XT is cod.
And the other case is that XT is not cod.
Does this make sense?
Okay.
So if XT is cod, then we have like step one.
We have made one step towards satisfying the constraint.
So we can reduce it to V, T plus one.
Dog.
And suppose XT is not cod.
We're in the same state.
We reduce it to T plus one.
Dog.
Is that clear?
So suppose we have constructed FI
for all time step greater than T
then we can construct all the fees for T.
So it's a recursive algorithm.
Oh, what's the base case?
Yes, that's very interesting.
Okay, I'm gonna erase this.
So just cod and dog in the part.
So yeah, I mean, we can make it even simpler.
I'm lazy, so I wanna make things simpler.
So suppose in the nth position,
we only have one word, right?
So fee and,
what is this formula?
It means that from Xn to Xn,
we need to have cod and dog.
So this is false, right?
Zero.
What if we only have dog?
So this is true only if Xn equals dog.
So yeah, those are the base cases.
Okay, so what's the size of the circuit?
So at each time step,
we need fee T, maybe we have already satisfied the constraint.
So something empty, right?
We need fee T, we have one word left.
Fee T, we have cod and dog, we have two word left.
And we have generated none of them yet.
Okay, so at each time step,
we have four sum notes or four more notes
representing all these four cases.
And the size, the number of notes in the circuit
would be four times.
Okay, does that make sense?
And you can notice that when we are constructing this circuit,
we are always conditioning on XT,
current variable, which is very similar to HMM.
It generates one variable at a time, one word at a time.
So it is compatible with HMM.
And also it's deterministic.
And we can do the apply operation,
the product operation layer-wise.
So the size of the eventual circuit
would be four, so originally,
so originally, here for each layer, we have H nodes.
Now we, here for each layer, we have four nodes.
So eventually we have four H nodes just for each layer.
Acceptable, okay, does that make sense?
So we can still generate this.
We can generate this even further.
Well, some time, well, one simple generalization is,
you might think it doesn't have to be caught, right?
It can be caught or either catch, right?
Or catches, they all kind of represent the same concept.
So in the construction, we can
modify our original circuit a little bit.
Before it was XT equals caught,
we can replace it with another OR node,
enumerating caught, catch, and catches.
Okay, and the circuit size stays roughly the same.
And there are many other things we can do.
We can also say, oh, we don't need them
to be in the same order, in a fixed order.
We can, we want them to be like in arbitrary order.
We don't care about the order.
How do we do that?
So in this case, we have four states, right?
Basically enumerating all suffixes of these three keywords.
In that case, we would have all subsets of these three keywords.
So in that way, the complexity would be going from four,
we have three keywords, and this is four, two, two to the three.
That's there, eight subsets, two to the three subsets, okay?
So still acceptable, two to the n is not really,
say we have 10 keywords, two to the n, it's just 1024.
It's still doable in practice, yes.
So it seems that you can do any kind of regular expression
constrained, but you're focusing here on language
as a fixed length, right?
Because you have a fixed number of random variables.
So is that the kind of equivalent of what you're saying?
Yes, yes, that's a very, very good point.
So fixed length here is a very essential assumption.
So suppose we have a distribution over language of arbitrary length,
then applying the constraint could be hard to define.
So basically these three words,
they can appear in arbitrarily far positions,
and we need probably you take like an infinite sum
to define this conditional probability.
But this is not terrible in practice,
because in practice, even for models like GPT,
they have a finite sequence length, yeah,
that's a very good point, okay, I'm trying to be fast.
Oh, I mean, there are variations of constraints
we have talked about, and okay,
so now we have our,
sorry, now we have our probabilistic circuit
representing the distribution,
and we have our constraint circuit,
and we'll close them to take the product.
So we have our constraint circuit now,
we can compute the probabilities we need.
So the step two is very simple.
So this is the original like base decomposition
of the conditional probability we have.
So this term here is intractable,
so we secretly replace the subscript of LM with HMM,
the one we notice, and we define this conditional distribution,
the gelato distribution, and we can compute this
with the linear pass of the circuit.
Okay, so what are the advantages?
So number one, by definition,
the constraint alpha is guaranteed to be satisfied,
so finally, other than compared to all the other methods,
we have 100% reliable thing we can trust,
and number two is that the training of this HMM
does not depend on the constraint.
So basically once we have this HMM trained,
we can use it to enforce whatever constraint we want.
So maybe today I want to write something
using some keywords and key concepts,
and tomorrow I feel like this language model
is like using a lot of inappropriate languages,
and we can detoxify it by specifying a list of bad words
that it cannot use, and all the same model,
and all the constraints are only enforced at inference time.
Yes?
What kind of constraints can be represented
as the composable pieces?
Yes, that's a very, very good question.
That's actually one of the main reason
in presenting this work here,
because my feeling is that though people have been studying
like probabilistic queries like marginal map, marginal,
and all these kinds of stuff extensively,
but in practice, we really care
about some more complicated, less generic ones,
and whether is there a language to define
or to describe their tractability
is kind of missing from the literature.
But I could relate this to say
there's some work on compiling DFAs to circuits.
I think that could be something as a starting point
to look at.
I'm not sure if that answers your question.
I don't have an answer.
My answer is, okay.
Okay, so experiments and benchmarks.
So we evaluate our method on this common sense generation,
common gem benchmark.
Well, it's very similar to the example I gave you.
It gave you a bunch of keywords,
and each example comes with a bunch of gold sentences,
and you want to generate something
that looks similar to the gold sentences using keywords.
And here, it's like the most general case.
So basically these keywords,
they can appear in any order,
and they can appear in any form of their inflections.
Okay, so this is the, yes, then.
And it's a gigantic table.
The numbers themselves are not that important.
So one thing to note that is compared
to all the other baselines,
our method, gelato, achieves a state-of-the-art performance
with respect to basically all metrics.
So these metrics here, Rouge L, Blue Four Ciders,
Spice State, there are just some standard NLP metrics
that people use to evaluate the quality of your text.
So basically you have a gold sentence,
and you have your generated sentence.
They compute somehow the NBREM overlap
to measure the quality.
But the other thing is that all of the previous method
cannot achieve 100% constraint satisfiability,
but ours does in practice as well.
Well, there is this one baseline,
they achieve 100% accuracy as well,
but they kind of did it by starting from the keywords.
So they're always gonna be there.
And you can see their generation quality
is really poor, so.
We also conducted some sort of human evaluation.
Okay, I guess, yes?
Which language model does that look like?
Oh, the language model, we use GPT-2, GPT-2 large.
Yeah, and all of the baseline state,
they use GPT-2 large, yeah.
And in case people don't really trust
these automatic evaluation metrics,
we also conducted human evaluation.
And you can see that our model performs
much better than previous state-of-the-art.
Okay, well, are they very significant?
I mean, they're pretty close.
Yeah, so, yeah, so basically the,
I'm not sure if you can see it clearly,
but the bold-faced ones are statistically
significantly, what equivalent?
So these ones, these two are statistically equivalent.
This one is statistically significant.
And we're looking at, when defibrating,
what's the number here?
Oh, yeah, so basically we provide the annotator
some sentence and we provide description
of one of each of the aspects.
So basically, concept means that does it use
all the concepts naturally?
And plausibility means that is the sentence
like a plausible sentence describing a realistic scene.
And quality is basically fluency, grammar, and stuff.
Overall is the like another,
how do you feel about the sentence?
And the numbers are from one, two, three.
One, two, three, go, yeah.
Okay.
Okay, so let's get back to the very first
motivating example and you can see that gelato,
we use our model to add, and it is actually able
to get everything correct and generate a fluent sentence.
And another, I don't, okay, so we also found this one
in one of the generated candidates.
A pair of Frisbee players are caught in a dog fight,
which is not like the thing that most people
would like to think of.
So it also shows some sort of creativity here.
Here, okay, that's my talk.
Please ask questions if you have,
and otherwise we can go to lunch.
Thank you.
Thank you.
One more question.
Thanks for your talk, if I understand right,
you said that to generate every word,
you have to do a linear pass over the circuit.
So how much is this overhead compared to the kind
of latency from the language model?
Okay, I like that question.
So we don't really need to take a pass over the whole circuit.
With some caching, we can do like constant time.
So basically to generate each word,
the cost is like constant time.
It's like a pass through one layer.
Is it in any practice?
How fast is it?
Oh, how fast is it?
We actually, so.
I don't have a table here,
but we have a table in the paper.
So if say generating a sentence with five keywords,
a GPT-2 large would take around 20 seconds,
and our method is like 100-ish seconds.
So it's not terrible.
And one of the baselines here,
well, which was actually like the best paper award
at one of the top NLP conferences,
they use search-based method.
And for them to generate a sentence,
they take like 700 seconds, 800 seconds.
So because they're search-based,
so when a search-based gets large,
their method shows like a bottleneck.
That's pretty well.
Thanks.
Can you also give an idea about the time required
to derive the hidden Markov model?
Oh, yeah, so I do have the time for that.
So our hidden Markov model has like 4,000 states,
and the emission is 50,000.
We trained them with the juice framework
that we developed in our lab.
The training takes like 20 hours.
So it's, we sample about 200,
we sampled eight million sentences from GPT,
and we trained them for 40 epochs, 20 hours.
What's the number of parameters?
Number of parameters?
Of HMM.
Yeah, so HMM is 4,000 hidden states and 50,000 emissions.
So the main, mainly that the parameters are centered
on the emission table.
So it's like 40,000, 4,000 times 50,000.
I didn't do that in my head.
So those are unique parameters, right?
Yes, yeah.
Yes, so in the PC, yes, you kind of like,
you've rolled out all the parameters.
All the positions.
Controlable generation is huge, right?
So this is great.
What, I mean, and I probably understand you,
you gave a logical perspective for this audience,
but I mean, a reg acts as a much more natural
sort of control structure.
So presumably you can handle any of our complication
given the DFA interpretation, right?
Yes, yes.
I think that's a very important follow up.
We are kind of looking at considering like,
compiling DFA's to circuits
and kind of automate all these process.
Yeah, so ACLs in January, you submit there.
I mean, I think they'll love it.
Your initial example, you wanted a winter tree,
you wanted the presence of winter to select for warm, right?
Which is more of a natural language entailment.
And I don't think you handle that, right?
You handle variations and factual variations
because you actually code them.
I think we can totally do that winter example.
So basically, let me go back.
It seems like you encoded the specific variations
that you were allowing, right?
Oh, you mean like, so we can have winter, winters,
but like, maybe if the winter,
the word is not explicitly mentioned,
you cannot do that.
That's what I understand from your formalism, right?
If it's in your or, you'll generate it.
If it's not, you won't, you won't capture the entailment.
No, I mean, so basically, so that's the other thing.
So sometimes people, or most of the time,
people want to have like kind of soft control.
They can't really write their constraint in logical form.
For example, toxic, like how can you tell a sentence is toxic?
But there are many ways to approximate it.
One of the ways is we have like a long list of phrases or words
that you're now allowed to use in a toxic sentence
and we can basically just write down a logical formula
approximating that toxicity constraint.
Yeah, I mean, there's also plug-and-play generation tricks
that are actually quite interesting, I think that...
So basically, there is a, of course, there's a naive way to do it.
You can say whenever I encounter one of the words in the list,
I just remote, like, prevent it from generating,
prevent it from being sampled.
But that's not exactly what we're trying to do probabilistically, right?
So...
Yeah, but I just, maybe look into plug-and-play,
control generation is actually a paper.
Yes, yeah.
So where I think they're doing much more than that, right?
Well, so, yeah, so it's like...
They're also modifying the posterior selection.
So if I'm not wrong, if I remember this correctly,
they're basically trying to train some sort of...
They use a classifier, right?
Yeah, so basically, kind of, they're trying to train
a newer model to approximate this.
Yeah.
So, but they're...
So, yeah, so their methods has like two disadvantages
compared to our advantages.
So one of them is that they cannot guarantee that this is...
Right.
100% logically satisfiable.
And the other is that they have to retrain their model
for all different constraints.
Their model for all...
Yes, okay, but...
Okay, so suppose in their training data,
they're only trained to kind of satisfy...
They only have seen using, say, less than 10 keywords
to generate a sentence, right?
What if today I want to use 20 keywords?
They would not be able to generalize well to that one.
Right, so they're examples of toxicity, right?
So they pretrain for toxicity and they can use that anywhere.
So there's some tasks that you're just going to use repeatedly.
I guess I see a combination of what you're doing
and what they're doing together,
which gives you this sort of entailment during the...
Like this broader sense of satisfaction, right?
Which I think was an interesting motivation.
You didn't quite deliver on, but you could deliver on.
That's actually the current project we're working on.
So we're trying to combine the models
that can handle soft constraints with our model, too.
Okay, beautiful.
And finally, the reason large types of models work so well
is just because of attention, right?
And so when you're using a hidden marker model, right,
you're losing the power of attention.
But I guess I'm looking at this and trying to convince myself
that, well, you get all the attention in the right-hand part
and then you just need a little bit of bias of selection
in the left-hand part here, and that's what's happening.
Yeah, so basically, intuitively, you can think of this part
as only providing, like, guide and suggestions,
leading the model, leading GPT to satisfy the constraint.
But on this part, it's kind of responsible
for fluency, grammar, and everything.
Beautiful work.
Thank you.
All right, thanks.
Maybe we should wrap up, and thanks again.
And we can have lunch with the database folks
and talk about generating SQL query.

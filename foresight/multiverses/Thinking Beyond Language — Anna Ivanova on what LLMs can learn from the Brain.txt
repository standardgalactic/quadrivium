I'm James Robinson, you're listening to Multiverses.
Language can do and express many things, and in fact this was the subject of my last conversation
on this podcast with Nicol Krishnam talking about ordinary language philosophy.
Just because language is so powerful, we might be tempted to think that that's all we need
for understanding and predicting the world.
It's just manipulation of symbols, next word prediction.
However, if we look at the how the human brain at least actually works, it's rather different.
Our guest this week is Anna Ivanova.
She's an assistant professor at Georgia Institute of Technology,
and she tries to understand the relationship between language and thought,
and she does this by looking at brain scans, essentially MRIs, of what's going on when
humans are presented with particular scenarios.
For example, I'm looking at this marvellous view right now from Carlton Hill, Edinburgh.
I'm not thinking about it linguistically, it's going straight into my visual cortex
and processes happening there, and if I want to reason about it,
I'm not going to reason about it linguistically either.
A lot of her work looks at how conceptual knowledge of the world is not tied to the language area
of our brain, so for example subjects with aphantasia, people who have large-scale
damage to the language network, are still able to reason not only logically about
chess problems and things like that, but they can reason socially as well.
They can understand what situations are unusual.
So this is a really insightful and very timely conversation because it plays into a lot of
the enthusiasm about LOMs, which I certainly buy into myself, but it calls into question
some of this, forcing us to think, well, what is necessary on top of simple linguistic abilities
to really be a fully-fledged thinking machine?
I think one question that still exists in my mind is, to what extent just language
manipulation could get us to a fully thinking machine, somewhat in the same way as, you know,
I have a GPU and a CPU on this laptop, and I could use my CPU to play, you know, vector-based
computer games, and I could use my GPU to send emails, but they're not really suited to that task,
but the point is maybe language could be a kind of fully-fledged thinking system.
However, I think it is valuable to learn from, in fact, what the brain does, of course.
So I really enjoyed this conversation. I hope you did too.
Hi, Anna Ivanova, welcome to Multiverses.
Hi, thank you for having me.
So we're speaking and we're thinking, I think, and people who are listening are listening to our
words and they're thinking, and thought seems like something that should be really familiar to us,
because it's one of those kind of few things that we have really direct access to, and yet,
at the same time, it seems so mysterious, so hard to figure out exactly what's going on.
Maybe it's because the piece that's doing the figuring out is the thing that we're trying to
figure out itself, I don't know. But yeah, how can we get some sort of grip on what thought is?
Where do we start?
Well, I think we need to start with definitions, what it is that we mean by thought,
because different people use the word in different senses, and of course, it also depends on the
context. And generally speaking, at least in my area of work, I think there is the broad definition
and the narrow definition. And in the broad definition, thought is synonymous with cognition.
So the mental processes that we use to make sense of the world around us, so that includes reasoning,
that includes accessing memories, that includes various social communication capacities,
so very broadly speaking, something we would call cognition. And then the narrow definition is the
stuff that happens kind of like in between us doing things. So it's not necessarily you get a math
problem, your reason about it, you give the result. But it's more about you lying down in bed at night
and thinking about your day tomorrow, or you're walking somewhere and you're playing out your
conversation that you're going to have with a friend. And so this kind of inner thinking that
happens spontaneously, not in response to any external task is also something where people
that people commonly refer to as thought. And of course, those are very different, right? Like
if we're talking about the broad thing or the narrow thing, this specific kind of mechanisms
that might support them might differ quite a bit. Yeah, I think both of those kind of definitions
capture something of what one intuitively would characterize as thinking. So we might say,
you know, broadly speaking, oh, yeah, of course, when this person solved that math problem,
they had to be thinking. But then you might also say, oh, they did it so quickly, they just did it
without thinking, which would be sort of more of the narrow definition, they did it sort of
without any kind of reflection, which is kind of what the narrower definition requires, I guess.
I'm curious, do you have a kind of preference for either of those definitions, or do you think
they both serve a kind of useful purpose? I've used both. I'm interested in the relationship
between language and thought, the role that language plays in thinking. And so the role
that language plays in thinking broadly defined thinking as cognition, that actually turns out to
be a more tractable question, because we can ask people to do a math problem and look whether
language processing regions in the brain are engaged. But for inner thinking,
stuff that happens spontaneously without an external task, that's much harder to capture.
But that's also where people have very strong intuitions there. Of course, I think in words,
or of course, I think without words. And so that area is harder to study, but also very
interesting. And so that's where I see some of my future work going.
Interesting. Yeah, it's true that kind of more reflexive thinking almost by definition,
people are going to have opinions about it, because they are kind of
cogitacing, turning things over. And part of that process is inward looking. So yeah,
people are going to be like, oh, well, I always do that with words or with images or a mix.
And yet sometimes I can also find it a little bit hard to remember to do that and think about,
because whenever I sort of think about, if I try to think about what thinking is,
I will do it linguistically. But maybe that's just because it's the sort of,
that's the sort of way that I need to think about that thing. Whereas if I think about
something entirely different, like a kind of, you know, spatial reasoning problem,
I'm sure I would do it in a different way. But it maybe wouldn't engage the linguistic part of my
brain. But anyway, I guess, yeah, this is a very, we've got very quickly to a very
key area, which is this, this, this relationship between language and thought, which
my last guest was Nikol Krishnan, an ordinary language philosopher. And we spent some time
just talking about, well, how for a while, people just thought there was no difference
between the two. There was, or rather, in some way, language captured the entirety of everything,
including thought, maybe including some other things. So, you know, Wittgenstein's famous
dictum, the limits of my language, the limits are the limits of, or mean the limits of my world.
But I guess your research is maybe questioning that. Would that be fair to say?
Yeah, I use Wittgenstein's quote as an example of a worldview, a paradigm that I'm pushing against.
And what's wrong with it? I mean, as we said, like, when we try to describe what thought is,
we'll probably reach for language. And yeah, it seems like so many of the ideas,
I don't want to say everything, but so much of the ideas that we have, and we pass on, we do
with language. And maybe there's an argument that the places where we're not doing it with
language are somehow dependent on language behind the scenes. But that's not a very scientific
argument. And I think you have some kind of evidence to the contrary. So, yeah, maybe take us
through some of the things that you've done to probe this. Yeah, let me, there is a lot to say.
So, let me start first by, I guess, acknowledging the last bit that you said, where clearly,
there is a relationship between language and thought. And the most trivial, but also important
one is the fact that we use language to transmit information, to communicate thoughts to one
another. And that's a very powerful mechanism, we can translate knowledge from generation to
generation. So that is a very important role of language in thinking, helping us share information
without having to figure out every single thing individually. But here, what we're talking about
is using language as a medium of thinking. So internally, do we think in words, do we recruit
the mechanisms for language processing when we're thinking? And so that's an important distinction.
So that's the scope. Now, as we said, people have strong intuitions about whether or not
they use language to think. And probably that these intuitions are grounded in people's personal
experiences thinking. And so one important fact to keep in mind is that there is huge
individual variability in how people perceive their own thinking to be. And so my pet theory
is that a lot of philosophers are strong verbal thinkers, they spend a lot of time writing,
they think about abstract topics. And so to them, the link between language and thought and
their experience is very strong. And it just seems that people, not just philosophers,
have this tendency to assume that everybody else thinks in kind of the same way. And so
if you are a strong verbal thinker, you automatically assume that everybody else is as well.
Until you start actually talking to other people and asking about their experiences.
And so I've had these conversations with people at parties or just informally, you ask them,
oh, hey, how often do you think in words? Most of the time, some of the time never. And people
are always surprised to hear that other people's experiences might be completely different. And
so this is just, I think, a very important thing to keep in mind that our intuitions
can lead us because they don't necessarily reflect a universal human experience. It's just,
you know, that's how we think. And so on to the actual evidence that we can use to dissociate
language and thought. There are a few different strands. So one example, very briefly, is the fact
that animals who don't have language might often have pretty sophisticated thought and planning
capabilities, right? We know examples of crows being very smart, alphoctopi, even a squirrel that
is trying to figure out whether to jump from tree to tree or if it's too far and it needs to go on
the ground instead. These are pretty sophisticated capabilities. And so that's just a very basic
example of at least some kinds of thought. You can then argue, you know, oh, but like the kind of
thinking they're doing is not, you know, the kind of thinking that we care about, right? And that's
where the meat of the debate is. But pretty sophisticated cognition is possible in non-human
animals from what we know. For me, I work with humans and adult humans. And so what we can do
is we can identify parts of the brain that are engaged in language processing. So it turns out
that there is a set of brain regions in the brain known as the language network that are responsible
for language comprehension. So whether you're listening to somebody talk or reading, they're
also engaged during language production. So when you're speaking and when you're writing,
they are engaged in response to any language that you might know that also includes sign languages.
So it doesn't even have to be a spoken language. And these regions turns out are pretty selective
for language. So they respond to all kinds of language, but not to music, not to math,
not to general problem solving. And so this is pretty strong evidence that language and many
different kinds of thinking are actually separable in the brain. That language has its own neural
machinery. And that's important because it turns out that if language areas of the brain
are damaged, it will affect your ability to use language, but not necessarily your ability to
think. And so the most common example of that is a condition known as aphasia. So it difficulties
with language production or comprehension. Often, most commonly, it arises as a result of
a person having a stroke. And so if it's stroke effects left hemisphere, which is where the
language network is in most people, they might have really serious difficulties with language
production or comprehension. But if that language is limited to the language network, it turns out
that their ability to use other kinds of thinking remains intact. So these people with really severe
aphasia who really can't understand language or speak, they can solve math problems. They can
arrange pictures so they form a story so they can reason about cause and effect. They can look at
the picture or show in some kind of event, like a fox cheese in the rabbit or the rabbit cheese in
the fox and say which one is more likely to happen in the real world. If it's something like really
weird, like the scuba diver biting a shark, they will laugh because it's just kind of ridiculous.
And so, you know, you can tell that they understand what's going on. And there are really fascinating
cases, you know, some of them like like the plague chest on the weekend. So clearly, very
sophisticated forms of reasoning are preserved even in the face of severe damage to language
processing centres in the brain. Yeah, that's completely fascinating. And well, firstly, by the
way, I love your theory about philosophers and how maybe the sort of minds that they have that
make them good philosophers sort of self select or selecting a very biased or unusual community of
people who think in a particular way. And yeah, so that's really interesting. It'd be great to have a
survey of philosophers, I don't know if this has happened, and how they describe their own thoughts
and compare it to other groups. Yeah, very interesting. Yeah, I think sometimes that maybe I
should, you know, not talk about this theory and actually test it experimentally first,
that we add on biased people in advance. No, as long as they don't listen to this,
or maybe you can do it anonymously or something. I'm actually talking, I think,
soon to someone who from the philosophy of science who surveyed physicists to see if they are
realists in terms of, you know, how they think about the entities of science or not. So I think
it's like it is really interesting to actually just, yeah, try to figure out how it is that
yeah, how it is that people's personal beliefs and their kind of academic disciplines or their own
or the peculiarities of their minds intertwine. But coming back to the kind of experiments that
you describe, so yeah, I think these are just really, yeah, wonderful illustrations of how
thought maybe extends beyond the language network. And I suppose what you're doing is you're asking
people, so for example, the way that we know that music is not within the language network is,
I don't know, the language network is defined as the kind of a bit of the brain that lights up
in MRI scans. You see a lot of activity there when you give people sentences and linguistic
tasks, I don't know, maybe reading or producing language. And then it's a different area of the
brain that lights up when they're listening to music or when they're solving a math or chess
problem. And even when people have quite severe damage at South Asia, and the language part of
the brain is unable to comprehend or produce language or both, they can still do many of
those other things, which is, I mean, that's really interesting for one thing, because I often
think as well, language is maybe being so key to the kind of input output of the brain that,
you know, for example, reading a math problem would kind of go via the language network,
or is it that our brain is sort of able to just kind of directly take those symbols into,
I don't know, a different area of the brain, or perhaps do we have to kind of pose those
problems in a kind of more visual way? I don't know, I'm curious about whether the language
network is kind of a gateway for much of the information going in. So it looks like if you
give people math problems in the form of mathematical symbols, right, like five plus three,
first question mark, it seems like it doesn't need to go through the language network. So even
though it's symbolic, not all symbols get processed by the language network. And perhaps even more
strikingly, one study that I did in graduate school was looking at computer code. And specifically,
we looked at Python, which is very English-like by design, so it uses English words. And on the
other end of the spectrum, we took a graphical programming language for kids called Scratch
Junior. So it has different characters. And so then you have different arrows showing, you know,
the characters going left or jumping. But it has a lot of the same control flow elements that you
would have in text-based code, like if statements and for loops and stuff like that. And so it turns
out that for both of these languages, the main network in the brain that's responsible for
extracting meaning from that code is the so-called multiple demand network. And so that's the
network that's responsible for problem solving and reasoning and planning, and not the language
network. The language network responded a little bit to Python code. But even there, we actually
weren't able to exactly establish its relation, its role and why it would. It might be some kind of
false positive where the language network is like, oh, that's language, oh, wait, no, never mind. And
it kind of goes down. So there are other researchers that are promoting that theory currently. But
even for code, we call programming languages languages because how similar they are
structurally to natural languages. Even there, it looks like it's not the language network that's
doing the majority of the heavy lifting. Yeah, I found that completely, well, surprising actually.
And I think you noted in the paper that people kind of fell on two sides of the fence. Some people
were surprised and some people were, oh, no, that makes complete sense. But I was personally
really surprised because I, as you say, there's so much similarity between the way that natural
language works in terms of being compositional and having these kind of hierarchical features
and the way that programming languages work that I would think, okay, well, you know,
it's right that we call them languages because they're so closely related, they're just sort of,
I guess, a bit stricter, less ambiguous, perhaps. But the kind of, the nature of the rules is not
so different. And yet, yeah, it's almost as if one could imagine there being some sort of animal,
like a crow, like you said, like very intelligent creature, doesn't have language, but maybe it's
got a really good multiple demand network. And perhaps we could, perhaps could be a really good
programmer, because it's not that part of the brain, which is being recruited. But it's rather
this kind of almost clearinghouse from what I understand, the multiple demand network just
picks up so many different jobs. The other thing that really stood out for me in this paper,
which I really enjoyed, was that as a kind of, I guess, control, you could, you presented people
the same problems. So the, if I remember rightly, one of the, one of the code pieces of code that
people had to interpret in Python was a calculation of BMI. And so it's like, here is a variable,
which is your weight, here is one issue is your height, BMI equals height divided by weight squared.
And so the person kind of reads through that. And, and you see it being passed off to the
multiple demand network. But then there's the same problem defined entirely verbally. So instead
of using, you know, symbols with equals, and it clearly being Python code, it's just, I know,
this is what BMI is, here, you know, here's how much you weigh, here's how tall you are, what's
your, what's your BMI. And that went to a different region of the brain, which for me was just like,
okay, well, this is the same problem, but the way that it's presented really changes the way that
we think about it. Which, yeah, that was another huge surprise for me to think just how influential
the kind of presentation or the, the medium, I guess, for a set of concepts, how much that,
that determines how those concepts are handled internally, mentally.
Yeah. And in fact, that's not that uncommon of a situation if you think about it. So let's say
somebody is listening to this podcast, versus reading the transcript, the way information
gets into the brain is different. So in the auditory modality, it goes to the auditory cortex
first. And in the visual modality, it goes through the visual cortex first, and then it gets into,
we have a specialized part of the brain that's responsible for readings of recognizing written
letters. But then they will converge in the language network, because language network
is responsible for processing either a modality. And that means that these initially distinct
representations have to converge in some, in some way. And so for some of the other cases,
like a problem that's written in language versus in code, it looks like that convergence is also
happening, but it's happening later on in the processing, right? So it goes, you know, through
the language network, and I guess the multiple demand. And then you have some shared problem
solving. So in this case, calculating the BMI is doing some math. And so that we think also happens
in the multiple demand network. And in fact, we show in the paper that you can kind of break down
that activity that we capture into the code reading part, an actual problem solving part.
But it's a fascinating endeavor. In general, in cognitive neuroscience, how do we design
an experiment where we have those kinds of different conditions, where they're very similar,
except for something that we've changed. And so at what point that difference, right, auditor versus
visual language versus code, where in the brain does that make a difference? And where doesn't it?
Yeah, yeah. So you're saying that even though it goes the language area lights up when we have
that kind of BMI problem, it's just kind of passing the thing. And then it gets passed off to
you know, to actually run the calculation that happens, like that doesn't happen in the language
area. Yeah, that makes sense. Yeah, okay. That clarifies my, I was very excited. I thought that
maybe that we had some sort of like way of doing the confrontation, just linguistically.
Guess that's what that doesn't work. I think it's possible. And we don't well,
I don't know. Well, maybe not linguistically. But like, you know,
we memorize the multiplication table, or for like some problem that we do very often, we don't need
to actually go through the steps of the calculation, we kind of just retrieve the correct answer.
I don't know if it happens linguistically or not, potentially not, probably not. But it's
still a different mechanism than actually going step by step and doing, you know,
long division in your head, or like summing multidigit numbers or something like that.
Yeah. Yeah, I think that that's a really interesting question, which we can maybe come back to.
But I guess, yeah, so you kind of see both the multiple demand and the language
network lighting up when this problem is presented linguistically. So it's sort of a
fair assumption that what is in fact happening is that, you know, there's probably some linguistic
processing, but then it then gets passed to the same sort of area of the brain, which is,
which handles the pure Python problem. But of course, yeah, I mean, that is really interesting
and kind of useful in some ways, in that, you know, it seems more efficient to be
presented just with the Python code, right? You kind of bypass that. Oh, I turn this into,
you know, it goes straight into the, the system, which can perform the ultimate calculation, I
guess. I don't know if you, if you were able to capture any information on whether it was quicker
for people to kind of solve the problem when presented with the Python code or not.
I don't remember whether we saw a difference in how long it took people.
I think it's possible, but I don't, some of it, of course, depends on how proficient they are in
Python. So there might be individual differences there, also individual differences in how
fast they would read text. So I'm sure there's some variability there. But it's actually
an interesting thought that you bring enough so that this, having this abstract skeleton with
other information stripped away might make the problem solving the calculation easier.
Because in fact, there are cases where researchers have observed the reverse,
though there is this famous Waste and Selection task, which you have, let's see,
a card with like a two and a seven, and then a, and then a card with like
green, red and blue. And you need to test the rule that says if the number is even,
then the other side of the card has to be blue. And so then the question is which cards do you
need to turn over to make sure that that rule is correct. And so then people want to test
the card with the two on it because it's even, and so they want to make sure that their reverse
is blue. But then they often want to turn over the blue to make sure that the other side is odd,
sorry, is even. But that actually is not what you should do because it doesn't matter. Like if you
have blue and odd, that's actually not a violation of the rule. What you need to do is you need to
turn over the red card because of an even number there, then that rule gets violated.
So that problem is hard for people. But if you cast the same problem saying that there are
people at the bar and somebody, you know, is 16 and somebody is 25 and then somebody is drinking
beer and somebody is drinking a Coke, then, you know, how will you verify that only people over
the age of 18 are drinking alcohol? And then of course, you know, that you need to, you know,
check the 16 year old and check the person drinking the beer and not any other way. And so
mathematically, the same exact process, but it's much, much easier for people to ground the rule
and their existing knowledge, not necessarily the bar example, but they're just kind of, you know,
the easiest one and the most common one. And so this phenomenon is known as content effects on
reasoning. And yeah, I think a lot of people, especially like, you know, physicists and mathematicians
and so people trained in like hardcore STEM discipline, they're like, hey, Alex, trip away,
all of the extra information only focus on the abstract symbols. That's the easiest thing. But
actually for a lot of people grounding the problem in some specific content domain tends to help.
And so I know that some people in like math education are very interested in this phenomenon and
how does it how to make it easier for people to, for kids to learn math. Is it by focusing on the
abstract or is it by grounding math problems in real life situations?
And I suppose part of the reason why that grounding might work, well, there could be kind
of two hypotheses. One is just like, it locates it in a different area of the brain, which is somehow
better at processing this thing. So maybe that just the social reasoning part is just better at
doing that kind of problem. But doesn't seem so likely in this case. And another is just that
it gets it, it clicks it in to a place where you're able to recognize a pattern that you've
seen before. And so you don't have to do, you know, you're already on the right track.
And this maybe kind of comes back to your point about, well, maybe when we calculate the BMI for
certain kind of combinations of numbers, you just know the answer. So it's being kind of recalled
from memory, like that pattern is already so established that you don't need to reason through
it in the same way. It's more of a recall operation. And I mean, this is getting us towards one of the
kind of central questions, which is around, well, what are LLMs doing? Because they're kind of
glorified recall machines in a certain way, or just really good pattern matches.
Maybe before we get to that, though, I want to talk about another of your experiments, which I
really enjoyed, which is about where people are looking at images of improbable and probable
things like the shark and the swimmer that you mentioned. And what I found, well, maybe you
should describe the experiment, you'll do a much better job of it than me. Because I think, yeah,
there was just a really interesting piece here that kind of writes this.
Yeah. So here, we use that same idea that the same information might arrive in the brain through
different routes. And so in this case, we were looking at sentences describing basic interactions
between two entities, like the, I guess we can roll with the shark bites the swimmer,
the swimmer bites the shark, and pictures depicting the same kinds of events. And so here,
by switching around, who's doing what to whom, we're manipulating whether the event is plausible.
So likely to occur in the real world or impossible. So unlikely to occur. And the question was,
does the language network respond to language specifically? Or does it respond to meaning
and concepts more generally? And so if it's language, it should only really respond to
sentences and not to pictures. And if it's responsible to meaning, it should respond equally
strongly to sentences and pictures, as long as the person is thinking about the meaning. And so we
had people tell us whether they think the event is plausible or impossible. So you have to be
thinking about the meaning. And so what we found was actually something in between, where the language
network, in accordance with all of the prior work, responds more strongly to sentences than to pictures.
But it still responded to pictures to some extent. And I will say that in another study,
we recorded responses in the language regions to pictures of objects. So is this animal dangerous?
Can this object be found in the kitchen, that kind of stuff? And it did not respond to pictures of
objects. So it was something about events, maybe just something more complex, maybe something just
more fast-paced, that was specifically triggering responses in the language regions. And so this
intermediate result, so preference for sentences over pictures, but also responses to meaning,
even in known sentences, was kind of puzzling. And so one piece of evidence that helped us
make sense of this information was evidence from individuals with global aphasia, from people with
brain damage. And I should say that this is, yeah, so lots of the brain imaging work I'm describing,
I did with my PhD advisor at Fedorenko, and the global aphasia bit is done in collaboration with
Rosemary Varley at UCL, who works with individuals with global aphasia very, very closely. And so
here we had two individuals with global aphasia, really serious issues, looking at pictures of
swimmer by sharks, shark bite swimmer. And so as I mentioned earlier, they were laughing at the
weird ones. And so in general, they were very good at distinguishing plausible and implausible
pictures, suggesting that their ability to extract meaning from pictures was there, it did not
require a functioning language network. And so then the title of the paper, the language network
is recruited but not required for pictorial events and semantics. So we see this activation,
and we, but it's not, it's not necessary to do the task.
Yeah, yeah, I found that, yeah, very insightful. And what struck me was
one of the hypotheses as to why the language network was recruited is that
it's sort of another way of getting evidence or information on whether this event is likely or not.
And of course, we can't be sure what's going on in there. But, you know, perhaps it is somewhat
like a large language model where you, you look at the thing, you're like, part of trying to figure
out whether this picture is likely or not is you kind of read it out to yourself and like, well,
does this, is this a familiar pattern, right? Does it, is, you know, shark bites swimmer,
you know, that's that's that sequence of words feels close to sequences of words that I produced
before or read before, whereas swimmer bites shark is kind of jarring. And maybe behind that is just
the improbability of that, that, that, that sentence being produced according to the language
model in our, in our own minds. And of course, yeah, we, we don't really know that our minds work
like a large language model at all, but it's an attractive hypothesis in as much as it works.
Yeah, so I guess, so generally speaking, right, so we got this result language network recruited
but not required. And the question was, what's going on, right? And so generally speaking,
we consider two broad hypothesis. One is that that activation is not necessary to do the task.
So you see the picture, sharks, swimmer and biting. And so you activate those words kind of
automatically by association, but you're not actually using them to reason about whether
the event makes sense or not. So that's one hypothesis. And the other hypothesis is that
actually, the information in the language network is helpful. But when you're trying to
recast information that you're seeing in linguistic form, you can then compare it with
all of the linguistic information that you received in your lifetime. And maybe that
information ended up distilled in your brain in some general way, just kind of, we know that
people are very sensitive to statistical regularities in language, we know that we're very good at
predicting what word would come next, right? Like there is this information about what patterns
are likely in text is very much what people use during real life language comprehension.
And of course, that information also can help us, in many cases, figure out which events
make sense and which doesn't. And so actually, we try to test that hypothesis. We
didn't necessarily, it's hard to test that in actual human brains, although we now have
ideas for how we might be able to do that. But we started by using language models as proof of
concept, right? So the hypothesis is statistical patterns in language input can help us distinguish
plausible and implausible events. And language models are very good at capturing these statistical
patterns. So if language models can systematically distinguish plausible and implausible events,
that means that there is enough information there where maybe humans might be able to use that
information also to distinguish plausible and implausible events, right? So it's not evident
that humans do, but it's evident that humans can. And so we did that, we use language models and
try to see whether they systematically evaluate plausible event descriptions as more likely than
implausible. And so in that study, we specifically distinguished between two kinds of events. So
one is the teacher bought the laptop versus the laptop bought the teacher. So animate, inanimate
interactions. And so when you swap them around, if you interpret the sense verbatim and the inanimate
object laptop cannot buy anything, buying requires that the subject is animate. And so that's a very
kind of in your face screaming violation. And then the other example is kind of like the fox
chased the rabbit, the rabbit chased the fox or the swimmer by the shark, the shark by the swimmer,
right? The swimmer by the shark is not impossible. It can happen. It's just way less likely.
And so what we found is that when it comes to distinguishing possible and impossible events,
language models were very good, almost at ceiling. So that was actually very easy for them. But when
it came to likely versus unlikely events, there was a gap in performance. So they weren't quite as
good. They were okay. They were above chance. But they definitely weren't perfect. And so we're
not as good as people, I guess, right? Yeah, not not as good as people and not as good as when they
have to deal with animate inanimate sentence, right with impossible events. Yeah. So I think it's
like, to me, it's actually more interesting to compare those two sentence types, like how models
do on them. But also humans humans do well on both because these are easy sentences. So
they're not meant to be challenging. And so, yeah. No, I was gonna say, and do we take that as evidence
that then when humans reason about these things, they're doing it, not just linguistically, or is
it that our language models are kind of like better than the language models that that are out there
in, you know, the computer language, large language models? I think that I think it's
evident that humans are doing it not linguistically. And so the reason why we think there is this
performance gap is because actually, the language input that we receive doesn't faithfully describe
the world around us. So when we talk to each other, we don't just passively describe everything
that we're seeing. So I'm not telling you, you know, I am sitting down, the lights are on,
the room is empty, like it's very boring stuff. I'm telling you about things that are unusual,
novel, interesting, newsworthy in some way. And so this phenomenon is known as reporting bias.
So language tends to undercover on the report information that is kind of trivial, right,
that everybody already knows, or can reasonably infer. And so maybe actually events that are
unlikely are not as unlikely for LLMs, because, you know, we talk about unlikely things all the
time, that's the stuff that's worth talking about. And so if that's true, that's the reason why we
see this performance gap, then even if the human language model is very good, which by the way,
we don't think it is actually, I think large language models now are much better at predicting
the next war that humans are. So actually, they're better. But even if humans were really good,
just the language input is insufficient for us to be able to distinguish plausible and
implausible events. That means that we have to use something else in addition, we have to maybe
have some more sophisticated model of the world, where we can actually correct for this reporting
bias, we have to also bring in information that's about what things are typical, what we
can expect, what we cannot expect. So we're probably drawing on sort of multiple mental
resources or systems. In the case of the clearly kind of impossible, so computer bias teacher,
is it just the language, can you see if it's just the language, or have you seen if it was just
the language network that's recruited there, as one might think, well, if it can just be done
within the kind of the one region, maybe it's more efficient and metabolically, there might be some
kind of preference for doing that if it were possible. By default, we kind of light up various
regions just to make sure I don't know. So that's actually a study that I would love to do next,
so this difference between impossible and unlikely events is something that emerged out of this
language model study. And so now, of course, yeah, I think it would be great to bring it back to
the MRI machine, measure people's brain activity in response to impossible versus
unlikely centers, and see if the language network alone is sufficient for distinguishing
possible and impossible events. That is the prediction that follows from this language
model work. And so I would love to test that. Yeah, yeah, that would be so, yeah, I'd love to
see the results of that. Yeah, so I hope that that happens. But I suppose, you know, coming back to
LLMs, what we're starting to see is that maybe just a large language model in itself,
for various reasons, might not be so effective at thinking or reasoning as the human brain. And
one is, as you kind of mentioned, that the data that comes in is kind of biased toward the salient
and newsworthy as you put it. But then another from the kind of Python example is that, well,
as a matter of fact, we don't use the language part of the brain for code comprehension or for
logical mathematical reasoning, either for that matter. I suppose my question there is, though,
you know, could it be possible for LLMs to kind of just be
be able to take on the functions of the multiple demands network, for instance, which is doing
all this, which is the place which does the mathematical logical Python code interpretation
comprehension? Could it kind of take on all those responsibilities just by having
getting really good at saying, you know, next word prediction for mathematical problems and
next word prediction for code generation and so on? Or is just that kind of, or is that implausible?
I don't really know how we characterize, you know, where the LLMs just could have kind of
emergently develop all those capabilities within a single language model, or if that's just
very, very unlikely. Yeah, so LLMs do a bunch of different things. In general, as you mentioned,
they're very, very good at pattern recognition and pattern completion at different levels of
abstraction. So they do a lot of just direct memorization, right? The larger the model,
the more texts that can just memorize straight up, which is why a lot of those copyright issues
end up arising. But that's not the only thing that these models do, because they definitely
are capable of generating novel texts and mixing and matching previous inputs. And so the patterns
that they can recognize and reproduce, they can be fairly abstract. But then, of course, the question
then is pattern completion all it takes? Is that the only thing that's necessary? And so that's where
it gets tricky, because a lot of logical reasoning is algorithmic reasoning. It's symbolic. It's
very regimented. And so these are the kinds of problems where these models seem to struggle.
So for example, if you ask them to add and multiply two numbers together, if the numbers are small
enough, then the model is doing just fine. But if the number is large, that means it wasn't part of
the training set. It means it couldn't have just memorized the response, which it probably does for
a lot of smaller number combinations. And so then it would actually have to multiply step by step.
And it doesn't seem to be doing that very successfully. In fact, it often gives you a number
that's close, but just a little bit off. And so the kinds of mistake that it's making is different
from the kind of mistake a human would be making, because it's still trying to use pattern matching
to complete. And it's not quite working, it seems. Yeah. But I mean, it's very hard to figure out
exactly how they're doing what they're doing. I mean, they've got so many parameters. And it's
surprising how good they are yet still imperfect at doing those sort of problems. They're kind of
like a broken calculator. So they're much faster at getting to an answer, but it's not quite the
right answer. It's a pretty good estimate often. And yet it's not completely out. So it's really...
Yeah, I don't have a strong opinion, but part of me thinks, well, maybe they'll just kind of,
with enough data going in, they might just crack that. That might come a point at which
that kind of ability emerges. Although you point out in one of your papers, though, well,
perhaps if that ability emerges, it might be that a particular kind of
architecture that models the human brain emerges as well. So it may not be that...
It might be happened in such a way that it becomes less fruitful to think of a large language
model as simply a model of language, but something that has a kind of linguistic language network
part like the human brain and then hands off to a logical part. And as it happens, obviously,
in chat GPT, without that has had that kind of architecture imposed on it, at least in the
version with the Python code interpretation, for instance, because you can say, well, add these
two numbers together, and it will figure out, oh, well, I'm doing a math problem here. So I'm going
to convert this into a Python problem, and then it runs the Python code. So actually,
you know, some of these problems seem to be
are being sort of addressed, I guess, by the developers. But the way they're doing it is,
yeah, offloading. Yeah. Yeah. So I guess let me unpack a little bit. There's a lot there. So
first of all, it is very tempting for people to over ascribe intelligence to a language model.
And presumably that's because in our everyday interactions, we're used that language gets
generated by a thinking feeling being other humans. And now we have a system which is breaking
that relationship where we have something that generates coherence language that's not human.
And so it gets confusing. And that's the reason why that's one of the reasons why
there's so much hyper-intelligent language models, and they're expected to be the general
intelligence models, because of this tight perceived relationship between language and thought.
And so then when they make a math mistake, or they make a factually inaccurate statement,
you're like, oh, no, like how, you know, these models are terrible, they're not
terrible, they're just like not, they're just a totally different capacity you're evaluating.
And so what we argue is that it's very important to distinguish different kinds of
capabilities in these models. And so there is something that we call formal linguistic
competence. And so that's the ability to generate coherent grammatical language.
And that's something that in humans, the language brain network is responsible for.
And then there is all of the other stuff that you need in order to actually use language in
real life situation in interactions, you might want to ask somebody to close the door,
you might want to tell somebody how you feel. And there are all kinds of situations that
where you need to use language. But to do that, you actually need other capabilities,
you need to be able to reason about social situations, you need to be able to know things
about the world in order to generate actually accurate statements, you need to be able to reason
logically and know some math if you want to solve a math problem. So even if that information is
coming in as language, in order to be able to make sense of it, and also generate language that
achieves a particular purpose, you need all of these other capacities, which broadly speaking,
recall functional competence. And so different kinds of capabilities might suffer from different
problems. And so we already touched upon a few. We touched upon the fact that mathematical reasoning
and logical reasoning might require a different kind of algorithm. So instead of pattern matching,
it might need to be more symbolic. And it's not fully clear whether the large language models
today are capable of doing that. Maybe they are. But that's not necessarily in their default,
in the default way they operate. So that's an open debate there. When it comes to world
knowledge and knowing things about the world, distinguish implausible and implausible events,
there a big problem is reporting bias and the fact that the training data that they have is biased.
And so you might need to be able to build up a more general situation model, event model,
that will not just take in the language that you receive, but also fill in some kind of commonly
assumed things. If it's daytime, it slides out, stuff like that. And yeah, so different kinds
of problems might require different kinds of solutions. A more general kind of potential solution
that we advocate or talk about is modularity. So the fact that the brain is modular suggests that
might be an efficient architecture. So a language process in module, the goal of the language
network in the brain is not to reason, it's to get information that's expressed in fuzzy,
imprecise words and extract meaning out of it. And then pass it on to relevant systems that can
solve the math problem that can infer the social goal, all of that stuff. And presumably, for an
artificial intelligence system, you might want to do something similar where language
is not a replacement for thought, but is an interface to thought. And so in your example,
right, you have a math problem, the language model translates it into code. It's very good at
taking this broad like fuzzy natural language and translating into a more precise, symbolic
representation. That's something that we didn't have at all, even a few years back. So it's a
huge achievement. But then instead of trying to have that same language model to run the code,
you're much better off passing it off to a code interpreter that will run the code and give you
the answer. So the same kind of modularity that we see in the brain, that seems to be an effective
way forward in the AI world that indeed some developers have started to adopt.
Yeah. Yeah. And I think there's probably other ways in which the builders of these tools are trying
to modularize. Like another one that comes up a lot is Rang or retrieval augmented generation, where
yeah, there's some kind of database or just could just be a whole bunch of, you know,
documents or whatever. And instead of hallucinating an answer, you want to make sure that you pick up
something from one of those documents. And there's a whole different kind of machinery for that.
But again, like in the code interpreter example, it's, I guess the language part is
key, maybe less key in Rang because it's kind of a vector search. But it's a way, you know,
it begins with translating language into something a bit more precise, in this case a vector instead
of some code, I guess. And yeah, one wonders then if, you know, how close the parallels are between
what is being built here and what's going on in the brain. You mentioned that, yeah,
perhaps this is a good model for thinking about how we think. Language is this part where,
this place where things kind of, you know, entry point for concepts, but the places where
those concepts often get manipulated in terms of reasoning might be in other areas of the
brain. They sort of become something more abstract than language itself.
Yeah. Yeah. One thing I actually just slight tangent, but I do sometimes think that
maybe language is being so associated with thought because it's kind of like
the easiest thing to do, right? Like, you know, we know thinking is about concepts and some,
you know, manipulating these things which are representations of the world. And language is
just such an easy way of visualizing all of that, right, and understanding what's going on. But
perhaps it's just the surface level of something much deeper that we really don't have an easy way
of capturing. And, you know, that would map, I think, quite well to this kind of model of
concepts being passed around, but the concepts themselves being, you know, beyond linguistics
somehow. Yeah. So, as we mentioned, language is a system designed to communicate thoughts,
concepts from one mind to another. And so, for this communication to be efficient,
presumably language needs to parallel the structure of thought, the structure of concepts
in some way, right? And so, it's much more abstract already than the raw perceptual input,
than just audio, than just pictures, right? So, it kind of captures the relevant abstractions to
a large extent. And so, that seems to be helping a lot. And so, that does bring us much closer to
this more abstract conceptual representation. We're getting rid of a lot of extra details,
we say cat, we don't care which color, which size is the cat. But, of course, at mapping
between concepts and languages imprecise, we know that different languages partition the
conceptual space in different ways, right? So, the words don't necessarily map the concepts one
and one. Even within the same language, the same word can be used in many different contexts,
in different ways, with different meanings. And so, that link is pretty fuzzy, can get pretty
fuzzy. But it's definitely, I think you're right, when it comes to raw surface form,
it's a very decent proxy, imperfect, but it makes sense why people are tempted to use it.
Yeah. And, you know, in some ways, that means it makes what LLMs do so much more impressive,
because they're also somehow capturing that surface form of concepts. Someone,
a previous guest pointed out this wonderful quote from Ilya Sotskava saying, well, you know, if
your LLM can predict the, you know, it's not just predicting text, because if your LLM can be fed
the first part of a mystery novel that it's not read before, and it can tell you who the murderer
was, it's not just predicting a word, it's somehow kind of understood what's going on in that story.
Now, one of the difficulties, obviously, with all these things is, well, we don't know how
open AIs LLMs are trained. So, it's very hard to test them, because you really need someone to write
a new mystery novel to actually see if Ilya Sotskava's claim cashed us out. So, it's quite a
high effort test. Unless, yeah, we happen to know of one which is definitely not in the corpus that
was used. But, yeah, it does seem, you know, the fact that they are so good at mirroring what
we produce, and that what we produce is somehow a good map onto something somewhat deeper, the world
or an inner world. Yeah, it's so impressive. And you point out as well that it seems that,
you know, the way that LLMs operate is very similar structurally to the way that
our minds operate, in that, you know, it's not working on the raw audio or pixel forms of things.
Like, the beauty of language is the compositionality at the level of small units, which are
combinations of symbols or small sounds. And, yeah, the LLMs perfectly match that. So,
we've built these things which really do capture something quite essential about how at least a
part of our mind operates, it seems. And, yeah, maybe we've been seduced into thinking. That's
all there is to thinking. Well, yeah, so in fact, well, that question, I guess I don't want to get
too technical, but the question of what LLMs are starting with is actually an important one when
we're trying to compare them with human minds or human brains. So, in fact, what LLMs operate over
is tokens. So, it's chunks of characters that tend to occur pretty frequently in text. And so,
oftentimes, they're words, like, though, but they're sometimes not words. If the word is long,
it gets split up into multiple tokens. Yeah. And so, the problem is that those tokens actually
don't match linguistic units that the word is actually made of, like morphine. They can be
pretty arbitrary. And so, that does cause some differences between the way LLMs process them
and humans do. In fact, people think that one reason why large language models are bad at arithmetic
is because they tokenize numbers in weird ways, right? So, like, I don't know, 1618 is chunks
in, like, 161 and then eight. And so, then it gets weird when they have to, like, add up the numbers.
And so, that's where you get this weird, better-matching errors. And so, this kind of form is
that, it's very engineering-driven. It's actually not, like, very rigorously scientifically based.
And so, it's interesting, like, maybe if we change this little thing, it actually will result in much
better performance. And so, it's funny how a lot of those choices are pretty random engineering-driven
things. And, you know, they often work very well. But it's possible that with a small few tweaks,
you can actually make the model much better. Yeah. No, I always thought that there was more
sort of reasoning behind the n-grams that were used. But maybe, is it just kind of randomly
chunks? Because I would have thought, well, there's some kind of, it makes sense to split
words up, because, you know, particles like nus, if I think of, like, redness, right? It's not a
word in itself, but it does attach to so many different words that it's sort of part of the
compositional structure, I guess. But if it's getting chunked up is just two s's, right? And not
nus, then it's kind of odd, yeah. No, that's exactly right, because nus is a morpheme,
it's a suffix with a particular meaning. And so, if redness is chunked into red and nus,
that makes a lot of sense, and it's linguistically justified. But oftentimes,
that's not how the chunking happens. That's where the mismatch arises. So, you can definitely
have the two s's in principle. Okay, interesting. Yeah, it seems like, yeah,
one would think that with a bit of curation, maybe they could be even more effective. And yeah, it's
hard to imagine them being more effective in terms of producing language. But perhaps that's
just because they've been fed such a, such copious amounts of data that they sort of these,
you know, they could be more efficient, right? Well, the stalker has an algorithm,
it's kind of the goal is for it to be universal and that driven, right, without human curation,
which is why the morphes don't get respected all the time. It causes a lot of issues for languages
that aren't based on the Roman alphabet. So, let's say Arabic, for example, it ends up getting
tokenized at the character level, because the tokenizer is just not adapted to deal with it.
And so that does mean that performance on these languages that are not
Roman alphabet based is actually worse, often substantially worse. It's generally a problem
that like the fewer, the less data a language has, the worse the performance in that language.
Some of the more general information seems to get pulled across different languages,
which is cool. But a lot of language specific stuff, like grammar, right, of course depends
on how much data you have in that language. But a particular distinction that tends to
matter beyond just the amount of data is which alphabet. And so because so many of these morphs
are English centric, a lot of other languages get left behind. Yeah, interesting. And to one
extent, I mean, I know there are techniques for doing this. So you spend, you know, a lot of
experiments looking into the minds or the brains of people.
There are tools which allow us to do this to an extent with LLMs. But, you know, how effective
are they? How does it compare to looking at an MRI, trying to understand what's going on inside
of an LLM, what concepts it has, or what's lighting up as it is given a prompt?
Yeah. So I am fascinated, honestly, by how many parallels there are between studying biological
intelligence and humans and artificial intelligence. And for me, the first similarity is really just
starting at the behavioral level. So developing separate experiments to look at formal competence
like grammar, functional competence, like reasoning, these are methods from cognitive
science, how do we design good experiments, how do we disentangle different contributors to
performance. So even before we start looking inside the model or inside the brain, just looking
at how humans behave and how models behave can tell us a lot about potentially how they do it,
what kind of mistakes they make, what does it tell us about the potential mechanism that they're
using to solve the task. But then, of course, we can get even more insight by looking at the
actual mechanisms or their neural correlates. So for humans, that means looking inside the brain.
And for models, that means looking inside the model. And so the movement that is getting seen
currently the mechanistic interpretability movement in AI is doing that, essentially,
they're asking which circuits, which units inside the network are responsible for a particular
behavior. And so they first try to identify those units that get particularly engaged in a task.
Maybe they respond differently to plausible sentences compared to implausible sentences.
And then the beauty of having an artificial system is that they can actually go and
manipulate it directly. So you can knock out that circuit or you can replace activations from one
sentence with activations from another sentence. So in neuroscience, people sometimes do that as
well. In animal research, for example, or there are certain kinds of stimulation that you can do
that aren't harmful, but can maybe do the desired effect. In aphasia studies, these are natural
causal experiments, right? We didn't cause delusion that destroyed the language network.
But because we see those cases occur naturally, we can look at those effects. And so the causal
tools are really powerful because they can really help us to see whether this part of the circuit is
necessary for the behavior that we observe. And so in AI systems, we can do that quite easily.
But conceptually, I would say in neuroscience and in AI, what we're trying to find out is very
similar. Yeah. Yeah. And it's, I mean, it's wonderful, as you say, at least with the behavioral point,
you can draw on the same kind of experiments that, you know, we finally have a kind of
artificial intelligence that you can feed the same sort of things that you'd feed a person,
i.e. sentences. And so it makes it very natural to run those kind of experiments.
But then on the other hand, you can also go into the thing itself and tinker it with it in a way
which would be very unethical and, you know, even just impossible with a person. So you could,
I think there was one example where you had, I don't know, the concept of or Berlin was replaced
with Paris or, no, what was it? It was Rome. Was it the Eiffel Tower was placed in
conceptually into Rome or something like this? And you asked, well, how do you get from
Berlin to the Eiffel Tower? It wasn't me, but it was, yeah, it's a famous kind of editing study.
Yeah, I think I must have read it in one of your papers referring to it.
And so the LLM does really, it kind of responds in the way that you would think if what's going
on is that it has some kind of model of the world. And what all you've done is kind of
switch around some pieces inside that model. It's not that it gets completely, you know,
it doesn't throw everything completely out of whack, I suppose. And it even kind of
infers some things that, you know, the Eiffel Tower will be in the center of Rome and
it's going to be up with the Coliseum or something like that, which is, yeah,
yeah, it's so fascinating to have something where we can kind of, you know, plausibly
peer in into the internal workings. And yet just like the human brain, everything is
so complicated that actually also it's not a trivial task, I guess.
No, but that's the benefit, I guess, when neuroscientists have, we're used to dealing
with this complexity. And, you know, there are ways to zoom out beyond just each individual
neural unit to try and look at general trends and general patterns. And so I think a lot of
people are daunted by the task of trying to understand the neural net because it's so big
and complex. And because it's trained in this way where we don't necessarily know which features
it sticks up on. But to me as a researcher, I'm just excited. It's like a cool puzzle to solve
and a cool problem to understand. So generally, I'm pretty optimistic about this endeavor.
Cool. Yeah. I think you mentioned at the very beginning that your research is now starting
to look at some of the, you know, possibly trickier question of this kind of reflexive
thinking, the narrow type of thinking I think you mentioned. So we've been talking a lot maybe
about the broader definition of cognition of just kind of reasoning, manipulation of concepts,
which might, one might even do in a very automatic way, as we were saying, like you might just solve
a mass problem without really, you know, in a way where you'd say, oh, yeah, I didn't think about
that. I just did it. But yeah, how does one, what kind of things have you, how can you pick,
how can you look at this other problem of like when, when people kind of cogitate about things
and turn them over in their, in their minds? Where are you going with that? I'm really curious.
I think to me, the interesting question here is the question of individual differences. If
some people report thinking in words most of the time and others say they don't think in words at
all, presumably we should be able to see that at the brain level. Presumably we should be able to
see the language network working hard for the first group and not at all for the second group
while they're thinking right spontaneously in this task-free setting. And so that's really what I
want to look at. But in order to do that, we need to have a good questionnaire that will capture
those differences precisely, right? So I think instead of just asking people, although you think
in words a lot, they're a little, it would be helpful to think, to get more information, right?
Do they think in like, what does it mean? Like, what if other meta assessments of their own
thinking style is reliable, right? So like, can we trust those judgments? How can we make them
more granular? Another question that I'm very interested in, and that's really understood
currently is, is there a difference between thinking in words and hearing the words, right?
So if you're using some kind of words and some kind of language to think, does it mean that
there is a voice or not necessarily? Some people, it turns out, they might see the words written
in their mind's eyes, so they spell it out. It's a minority, like less than half of the population,
but it does happen. And the capturing those differences, I think, is fascinating and then
trying to look at the neural correlates to essentially establish the validity of those
differences to show that they're really not just something that people perceive and report,
but actually, that's not necessarily how they actually think. It's an interesting direction
because psychology has this interesting history of an interesting relationship with phenomenology.
So the people reporting their own experiences, right? That used to be very common, and then it
turned out to result in a lot of pseudoscience and discredited a lot of psychology. And so then
there was this huge turn to behaviorism where all that mattered was the stimulus and the response,
and people were refusing to talk about any internal operations at all. So people are still
very suspicious of phenomenology, so self-reporting experiences. And I think for the right reason,
because often, yeah, we just don't know how we think. We're like, I think it's words or I think
it's not. Sometimes we'll make a decision, like we were saying very quickly. And then when we have
to explain what we did and how, we have to rationalize it. And so maybe that's actually not how we
arrived at the decision, but post-talk, we come up with an explanation that might not correspond
to the reality. So I think we'll have to be careful when taking people at their words.
But to me, when people report this strike in differences of like, oh, yeah, I think in words,
all the time versus like, never, not at all, it seems like there's something there and so I would
love to use neuroscience to get at that question more deeply. Yeah, it's a tricky one. I mean,
it strikes me that even the process of asking someone, do you think in words,
it almost necessarily linguistic to communicate that because as we say, this is the way that we
pass ideas around. And so maybe maybe there's just like that kind of arrogance in the language
network, which is going to intercept that question and say, oh, yes, it's me. I do all the thinking.
But as you say, well, many people do report thinking in many other ways. So yeah, I would,
yeah, I'm really curious about what that shows. I mean, it's just so, this must surprise you all
the time just how, you know, outwardly, we sort of walk around and we move around and we breathe
and we have all our organs are, you know, working in pretty similar ways. And yet,
internally, it might be, you know, we seem so heterogeneous, I guess. Does that sound about
right? Or am I overstating the kind of differences in brains that we have?
I don't know. I think, yeah, it just depends on your intuition about, you know, how much
similarity and differences you would expect. Of course, our personalities are very different,
right? Our likes and dislikes are interesting. So at the cognitive level, there are lots of
differences between people, of course. And so I guess the interesting thing is that we have
this huge differences in how we perceive our own thinking, but they don't necessarily manifest
very obviously in differences in this year, right? So in addition to differences in inner
speech, another common example is differences in mental imagery, right? So it turns out that some
people never experience visual images in their mind's eye. When they're asked to imagine a red
apple, they will think about the concept of an apple and like redness, but they will not like
see a red apple in front of them when they close their eyes. And so that phenomenon has a name,
aphantasia. And the name got coined in 2015. So very recently, really. And this is the phenomenon
that kind of got discovered over the centuries at various times and then forgotten again,
and rediscovered because again, people just tend to assume that everybody else has the same
roughly inner experience with them. And so those differences just end up getting neglected.
But it turns out that people with aphantasia, you know, again, you cannot tell them apart very
easily from people with this visual imagery. So it turns out that lots of things we do in the world,
you can do whether or not the experience images visually. Similarly, whether you have strong
inner speech or not, turns out it's you can't really spot these people very easily out in the
wild because they act very differently. So that's the interesting thing, right? Despite these
experiences being so different, somehow we can still act in roughly similar ways and do the
tasks that we need to do in the world. We might be using different strategies, it's very possible,
but the end result is that actually, those differences are very hard to see.
Yeah, yeah, that is fascinating. Yeah, actually, I have a friend who is an aphant, I guess.
And well, he didn't find out until a few years ago. And we, you know, there's no kind of outward
sign, right? You just seem completely, you know, normal. But then we're like, oh, yeah, I just
can't visualize triangles, right? I know what a triangle is, I can reason about triangles, I can.
And actually, often there's, I think there might be some research which shows that that in some ways,
oftentimes better at reasoning about certain things, where one might think it requires a
visual element. But yeah, he was, you know, a very good physicist, very good colleague,
but just thought in a different way, I guess.
Yeah, I think we'll find some differences, right? Like, now that there's more awareness,
once we start doing more systematic research, I think we'll, like, I mean, there are already
attempts, trying to look at the relationship between aphantasia and episodic memory,
turned out that aphantasia and, yeah, spatial reasoning, geometric reasoning there, the link
is not as strong and may or maybe not even there, even though people expected it to be.
And there are variations as to why. But yeah, essentially, like, I think,
even though those differences aren't apparent, I think we'll find some eventually.
And probably we'll just find out that different people are using different strategies to do the
same thing. Some of them might require imagery or thinking in words, you know, speech, and some
might not. I mean, all this is a reminder that well, LLMs might be, we might end up producing
artificial intelligences, which outwardly look very similar. But we shouldn't, or yeah, we should
be careful to think that to be mindful that inwardly, they could be very, very different.
And I think it's very true, and it's already happening, right? There are all those cases where
people were screenshotting chat GPT responses, especially right after it came out a year ago,
and just showing it responding to some very complicated prompt and doing it correctly,
and people were so impressed being like, oh, you know, if you have, if you put like,
a mail on a table, like, you know, like on on a chair, is that like a construction
stable or not? Or, you know, what kind of thing we can put on top? And like, it looks very impressive.
And then it turns out that if you change the problem just slightly, then it just starts
spitting out total nonsense. And the same thing happened with a social reasoning problem, kind
of predicting what the other person would think, which is a classical problem from psychology.
And so the claim was that, you know, now LLMs can reason about people and what they do. And then,
again, at third dollar, we change the prompt to be slightly different from the problems that
were already available on the internet, then model performance drops drastically. So it's very
easy to fall for this seemingly impressive performance, seemingly seemingly impressive
understanding. And so luckily for us, in this case, there are ways to design even behavioral
interventions that can maybe help us figure out what's actually going on and what strategy is
actually being used. Yeah, yeah, that's a very important problem for sure. I found the social
reasoning example really, I just loved it. So I think if I remember correctly, one of the ways
that you fall them is just inserting a few words in the middle. So the classic, the classic one
is something like, you know, Susie hides Bob's apple. It was in the closet. Where does he think
the apple is? And no, the thing will think it will say, Oh, not in the closet anymore.
Correct. But if you change it, say, well, Susie hides Bob's apple, it was in the closet,
and she tells him that she hid it, right? And then the LLM still says, Oh, he thinks it's not in
the closet because it and I think, yeah, so clearly they've been either on the training set or in
the fine tuning, you know, it's gone so far in getting them to kind of give the appearance of
having some kind of theory of mind and being able to solve those problems. But then with a little
bit of tweaking, it becomes apparent that they they don't. But it seems to be coming harder and
harder to, you know, fox these systems. And you point out that one of the real difficulties is
we just, you know, without knowing what has gone into the training of open AI's models and
it, it's hard to know how much genuine kind of intelligence has emerged and how much is just
kind of pan recognition and, and, you know, or simple pan recognition and recall. But of course,
the paradox here as well, they have some of the most advanced models. So maybe there's something
genuinely interesting going on, but it's black box. So we can't really say. But I think it's
encouraging that other folks are, you know, mistrial perhaps are being more open and about
what's going into that into their models. So, you know, maybe easier to, to test on the very
latest things and have confidence that I don't know, some problem was not appearing verbatim in
in the training set. But even then, yeah, it could be some very similar problem.
Yeah, for sure. And like, it's not always bad if they've been fine tuned on this problem. And if
they've seen examples before, as humans learn lots of things, we can do math right off the bat,
we learn, we learn from examples. It's easier for us to the problems that are familiar to us and
novel ones, like, that's fine. It's just important to know, because that helps us figure out what
is the mechanism that they're using, or potentially using, and what are the keys is where they might
break, right? So it's, we don't necessarily need to expect these models to be like amazing,
zero shot thinkers on total and novel problems all the time. It's just that, yeah, knowing
what goes in the training data, knowing what problems they've been fine tuned on,
just helps us assess them more accurately. Yeah, yeah, that's a good point. It's unfair to
sort of demand development so that they don't learn anything, or they don't benefit from fine
tuning. But yeah, but we want to know if they are, if the way that they're responding is using,
is that they've abstracted patterns, or they've just, they're just regurgitating.
Yeah, well, I, yeah, I found it really interesting to go through your work. I'm usually,
I don't know, optimistic, I guess, or maybe not, maybe that's the wrong word. I really admire how
LLMs are, have taken off, and it surprised me how quickly they've advanced. But one thing I've
enjoyed about your work is kind of reminding me, well, actually, maybe they're not as far along in
some ways as, as they appear to be at the surface level. Like there still seems to be some,
some nuts to crack here to make them, to bring them closer to, to human thought.
What's your take? Do you think, you know, I don't want to ask the typical how far away is AGI,
because, but, you know, are you of the opinion that just cranking through more data
is going to continue to produce results, or should more be invested in this kind of modularity
approach? And if the latter, well, do we have, you know, are the things that are taking place
on the right track, or do we need to look more closely and learn more from the human mind, perhaps?
I think we definitely should recognize all the impressive achievements that we observe in LLMs
today. And one way that my colleagues and I have been thinking about it is through the formal
functional competence lens. On the formal competence front, learning the rules and patterns of natural
language. These models have been incredibly impressive. So even a couple of years ago,
they almost reached the ceiling for English, at least the language where they have the most data.
And they did it without the need for any fine tuning. So just learning to predict words and
method corpora of text. Turns out that is something that gives you all the most of the grammar and
knowledge of idioms and all kinds of patterns that characterize a language. And that wasn't trivial
at all. That was the subject of debate for linguists for decades. Is it possible to just learn from
data? Or do you need a rich set of internal rules that can help you figure out what's grammatical
and what's not? So that is incredibly impressive scientifically. And on the engineering front,
different language processing systems in the past have struggled so much because you just can't
encode language with a few simple rules or even not simple rules. Just like so fuzzy and there are
so many exceptions in the regular forms and this and that. And the fact that these models have
mastered that is so impressive. And people kind of forget and start talking about AGI right away,
but that's an impressive achievement. Then being good at language is already very impressive.
And then we get to functional components and their ability to reason and be factually accurate,
know what's true and what's false and be actually helpful. And so that's a whole other host of
problems where they actually seem to be spotty. They have achieved a lot because of pattern
recognition, but then it turns out that that performance is not robust and it breaks. And
so that's where it gets more complicated and more controversial. And that's where we argue modularity
will be helpful. Again, looking at the human brain as an example. And one distinction we make though
is that the modularity doesn't necessarily have to be built in by design. So this built-in approach
we call architectural modularity where we have a language model and let's say a path and code
interpreter and we put them together and they're clearly different and they're doing different
things. So that can be promising, but then of course you need to know what the right modules are,
you need to set them up in the right way. An alternative approach that might work for certain
cases is what we call emergent modularity where you start out with one network, but you don't
necessarily specify what parts need to be doing, what you let the network figure that out over the
course of training and you can have different parts self-specialized to do different things.
That might require some changes to the architecture to be able to promote the kind of
specialization. It might require changes in the objective function, maybe next work prediction
alone is not necessarily going to be good. And it might require changes in the training data
kind of like what's happening with fine tuning today where you are feeding its specific problems
that you're asking the model to do in a specific way so you might have selectively boost the
social reasoning and the formal reasoning and the factual knowledge. There might be specific
things you need to do, but there is a lot of promise in these approaches and the paper where
we introduced the formal and functional competence, it's something we started working on in 2020
around the start of the pandemic. Language models were around then, but not nearly as advanced and
as we were writing the paper and in fact after the initial preprint version came out, that's when
we started seeing the field, the developers shifting away from this simple scaling up approach,
that's not the approach that's common anymore. People have started to shift towards specialized
fine tuning, using very targeted data sets to improve performance on specific domains,
coupling an LLM with external modules, all of those things that we kind of suggested that might
be good because that's more brain-like. RAG became big, all of those things are something we've seen
over the past year that's very encouraging that now the AI field is also recognizing that it's
not just about scale, that you do benefit from different components working together.
Yeah, I think very exciting to see what comes next, both in your work and in the field of LLMs,
which it seems like maybe someone's listening to what you're suggesting because all these things
are happening. Yeah, I don't know if you have any final comments or predictions,
warnings of doom often come up in these discussions, but this has been surprisingly positive.
No, I just think that science is important and we just need to use good methods and not run after
the hype and be realistic in how we evaluate the strengths and limitations of these models.
There are strengths, there are limitations, so being too far on just the positive and just
negative is not necessarily the most productive. We just want to be able to disentangle them
effectively. Yeah, Rene, thank you so much, Anna. This has been really insightful. Yeah, thank you.
you

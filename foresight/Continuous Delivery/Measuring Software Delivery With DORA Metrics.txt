For a long time, most people whose opinion I respected were somewhat skeptical of metrics and in particular of metrics used to measure software development performance.
Martin Fowler wrote an excellent piece called Cannot Measure Productivity some years ago.
Then some smart people came up with a better way and discovered all sorts of interesting and useful things based on their metrics.
These were not necessarily new ideas, but the combination was, and the model that they built from them was certainly new.
And is, in my opinion, an important step forward for our discipline.
So, how do you measure software delivery and what are the important lessons that we can learn from it when we do?
Hi, I'm Dave Farley of Continuous Delivery. Welcome to my channel.
And if you haven't been here before, please do hit subscribe.
And if you enjoy the content today, hit like as well.
I'd like to begin by thanking our sponsors, Equal Experts, Octopus and Speckflow.
They're great supporters of our channel, so please support them in turn by checking their links in the description below.
If you'd like to learn more about the engineering approach that I describe a little later in this video, check out my new book, Modern Software Engineering,
which is currently a bestseller in several categories on Amazon and also at the time of recording on sale.
In 2010, the book that I wrote with Jess Humble was published.
It introduced lots of ideas to people who hadn't seen this way of working before.
Its popularity surprised both of us.
We certainly believe that the ideas in it were important and valuable.
We'd already seen them work in every place that we'd tried them over several years.
But Continuous Delivery came to be seen as a significant book, and we didn't really anticipate that.
As we gained more experience with the practices in a wider variety of settings though,
our understanding of this approach grew significantly.
So now, I no longer feel embarrassed to say things like,
this is the current state of the art in software development.
And this represents a genuine engineering discipline for software in that if you practice it,
you will substantially increase your chances of success, whatever it is that you're trying to build.
One of the reasons that I feel confident to make such claims is because we have the best data on software delivery practice
that's been collected so far, and it tells us that this stuff works better than anything else that we know of.
A few years after our book was published, Jess met Dr. Nicole Fosgren,
and together with some sponsors, they started work on something called the State of DevOps Report.
Nicole brought an academic approach to the collection and interpretation of the data.
It's scientifically justifiable in terms of how that works.
The results are, as far as I can tell, the most thorough examination of software development practice
than our industry's seen so far.
Later, Jess, Nicole and Jean Kim formed Dora and released the Accelerate book,
which describes some of their findings over the years and the science behind their approach.
This is sociology, not physics, so research like this is complex and always open to interpretation and question to some degree.
Human beings are messy things compared to atoms and electrons.
Nevertheless, their findings are profound. They can make predictions.
These predictions have been born out in thousands of organizations now.
They say things like, there's no trade-off between speed and quality.
Excellence in software delivery and operational performance drives organizational performance.
And the data also tells us that continuous delivery is being successfully adopted by more and more teams year on year.
At the heart of the State of DevOps research and the approach are a collection of metrics,
now commonly referred to as the Dora metrics.
For decades, people had been trying to figure out how to measure software development performance.
They counted lines of code, man hours and later velocity in the form of story points.
None of these things really matter.
Why would you want more code if you could do a better job with less?
You don't go faster by adding more people, you go slower.
And velocity only tells us what we did last week and it's not really transferable between teams.
Velocity is also really easy to game.
Want to increase your velocity for your team?
Okay, this story, which was five points last week, is ten points this week.
I've seen all of these games being played out in real teams. They don't help us to do a better job.
Metrics are extremely useful, but not when they're treated as a goal.
Our goal is always to produce good software that people find useful efficiently.
The difficulty of measuring software projects left lots of people, including me,
thinking that you can't measure software productivity in any way that's useful.
And that is still probably correct, but we were thinking about the problem in the wrong way.
Productivity is really the wrong measure.
It's not that productivity doesn't matter, but it's not enough on its own.
If we produce loads of things, but they're all useless, then we aren't really productive.
So if we count lines of code and optimize to create lots of code,
then we're actively encouraging people to write nasty, complex systems
that over time will become harder and harder to work on.
So even by the mistaken measure of productivity, we have to be really careful about what we measure and incentivize.
All metrics suffer from this problem. They're all subject to being misinterpreted and misapplied,
particularly when we use them as goals in their own right.
The people behind the Dora metrics made some good choices.
These metrics are not easy to cheat or misinterpret, but somehow people still manage to.
So you still have to be cautious how you use them.
The Dora company was later unbought by Google, who now run the ongoing research
and have lots of interesting stuff online in addition to that.
You can get a quick assessment of where you stand on the Dora metrics, for example,
and targeted advice for next steps.
You can also explore the relationships between different behaviors that have been unearthed by their research.
There are four Dora metrics divided into two groups, stability and throughput.
The Dora research categorized teams into groups based on their scores on these metrics,
ranging from low performers to elite.
The difference in performance levels based on these metrics is sometimes extreme.
The research correlates behaviors and outcomes based on the scores against these metrics too,
and these too are often quite extreme in their range.
For example, an elite team will deploy changes to production
thousands of times more frequently than a low-performing team.
At the same time, they'll produce code of significantly higher quality,
and they will spend 44% more of their time on new features than low-performing teams.
The measure themselves are interesting.
They don't attempt to measure productivity directly,
and it's important to consider them as a whole, not independently of one another.
They're focused on measuring the flow of changes and their quality.
Stability is a measure of the quality of the software that we produce,
so you can't mistakenly optimize to produce loads of rubbish and get a good score
as you can with most more naive productivity metrics.
Stability is measured by change failure rate and mean time to recovery.
That is, what proportion of the time does a change result in a failure of some kind
when it reaches production?
And how long does it take your team to recover from a failure once it finds one?
Elite teams have changed failure rates in the 0 to 15% range.
Everybody else is in the 16 to 30% range.
Elite teams restore service in under one hour once a defect is found.
Low performers often take over six months to achieve the same result.
Throughput is measured by lead time for changes and deployment frequency.
Lead time for changes is measured by how long does it take from commit to code running in production.
And deployment frequency is how often do you release a change to end users?
These are measurements of the efficiency with which we can create changes
of the quality measured by our stability metrics.
They measure the cost of our production processing time and the rate of flow of change into production.
Elite teams have lead times of under one hour compared to over six months for low performers.
And deployment frequency is on demand multiple times per day usually
compared to less than once every six months for low performers again.
The combination of measuring stability and throughput together is important.
There's no value in optimizing for throughput if the stability goes down
because over time and it's a relatively short period of time
will incur more work as a result of the low quality of our work as measured by its stability.
This means that we'll end up going a lot slower and so our throughput will suffer after all.
This is why elite teams outperform poor teams quite so significantly.
They produce higher quality work and they do it more efficiently.
These metrics are inextricably linked.
Optimizing for a naive version of throughput is an extremely common anti-pattern.
I see this in dev teams all of the time.
The organization promotes the idea that it's vital to deliver features at some high rate.
So they cut corners all over the place to do so.
The result is that quality goes down.
Now dev teams are either drowning bug reports and the effort to prioritize and fix them
or they produce such tactical crap that their systems rapidly become so difficult to change
that they grind to a halt and make little or no forward progress at all.
In both of these cases productivity slows down dramatically
and better teams that don't cut corners like this steam pass them
and produce more better features sooner overall.
You need good scores on throughput and stability
and you need to take a longer term view of what counts as success.
Being able to work at a sustained pace matters a lot in software development.
The Dora metrics are still metrics so it's still more subtle than simply striving for good scores.
There's a very good presentation by Brian Finster.
There's a link in the description below called How to Misuse the Dora Metrics
where he points out several common misinterpretations and misuses.
Throughput is not about lowering quality to make developers cogs in some feature factory machine.
It's about reducing the back size of change.
We want to work in smaller steps that give us the chance to gather more feedback,
work more experimentally and so learn faster.
These are ideas at the heart of my modern software engineering approach.
So the Dora metrics can be used to help us amplify our engineering discipline.
The goal is long term success, not short term gratification here.
So if we want to maintain our ability to have a good scores in throughput
and good scores in stability too, we need to work in ways that are sustainable over time.
We can't afford to create rubbish now that doesn't result in us going faster over the long haul.
That means we need to manage the complexity in our systems
in a way that allows us to establish and maintain a fast pace.
So we need to employ the second group of principles from my engineering book too.
These are mutually reinforcing ideas.
This relationship is important.
In Brian's talk he correctly points out that in the Accelerate book
there are lots more things than only these metrics to consider.
I think that the Dora metrics are a great tool.
We can use them to select choices that work better than others, but they aren't the goal.
The goal is to produce great software efficiently and repeatedly that our users get value from.
I've seen teams misuse the Dora metrics, but I don't think that devalues them.
The problem with all metrics is that human beings are both smart and dumb at the same time.
Smart in that they will attempt to gain whatever metrics you choose if it's in their interest to do so.
I once saw a company incentivize their teams to compete with Dora scores.
Some of the teams redefined what the stability metrics meant so that they could get better scores.
Instead of change failure rate, they interpreted this to mean bugcat.
A subtle difference, but an important one.
They interpreted throughput as the number of features delivered,
and no measure of time involved and they ignored deployment frequency altogether.
Then they optimized for fixing bugs over delivering features.
Now, because they'd lost focus on feature delivery, they cut more corners to try and keep feature delivery up,
but the real change failure rate went up, not down.
They still declared success though, because overall the bug count did go down.
That's because the team, unsurprisingly, prioritized fixing easy bugs, even if they weren't important.
After all, they were being incentivized to reduce the bug count.
Nothing said which bugs they should fix.
Overall, the bug count numbers went down, but the real stability was worse than before.
Humans are dumb in the sense that they love a simple target.
So even in complex situations, we were often focused only on the target,
ignoring everything else, even when other things are more important.
So if you think throughput means speed of feature delivery rather than rate of change,
you'll ignore the longer term consequences of cutting corners.
The data is in. Working with high throughput and high stability
is the most efficient way to sustainably deliver software that we know.
The problem with this is that there is a point at which the lines cross.
What if you don't care about being sustainable?
If I need to deliver a feature next week, and that is my only way of measuring success,
then it may be in my interest to cut the corners.
If I don't test, if I just implement the first thought that springs to mind,
if I don't worry about the maintainability of the code that I leave behind,
I may save some time.
So my feature is delivered within a week, but the chances are it won't be working the following week.
The following week, I'll get bug reports, and now when I need to add a new feature,
my code is a mess.
So as well as taking the time to fix the last week's bugs,
my new feature will also take longer to create.
This is always a stupid trade.
The problem is in identifying the point at which the cost of doing a bad job
is more than the cost of doing a better job.
It's always difficult.
In my experience though, the cost of bad work hit us a lot sooner than most of us seem to think.
If you have a six month deadline to meet,
there's no doubt at all that spending the extra time to do a good job will pay off.
You'll produce more, better code over that time.
If the deadline is only a couple of weeks and you're about to leave the project and have no conscience,
then cutting corners may get you to a low quality delivery sooner.
If your deadline is somewhere between these limits though,
it's quite tricky to tell if the investment in doing a good job will get you to the finish line sooner.
There is no silver bullet.
Software development is complicated and difficult to do well.
The Dora metrics are trailing indicators.
They tell you how you did, not how you're going to do.
Approaches like continuous integration and test driven development and continuous delivery
predict good scores on those trailing indicators.
So represent other tools that we can use along with them.
Think of the Dora metrics as being rather like the instruments in your car.
They make no sense without context and correlation with the other instruments.
If our speedometer says we're traveling at 70 miles an hour, is that good or bad?
Neither.
It depends on the context.
It may be good if we're within the speed limit and hoping to make good time to our destination.
It's bad if we're driving through a school district or want to maximise our range.
What does it mean if we are averaging 60 miles an hour towards our destination?
Again, it depends on other things.
If we're about to run out of charge or petrol, then our average speed doesn't tell us.
We need to correlate that with those other instruments to figure out when we'll arrive.
If our engine is overheating and about to break down, then driving this fast is just hastening the point at which our engine fails.
We have to treat the Dora metrics for what they are.
Useful gauges that can give us information about our progress.
They aren't targets and how we interpret them will depend on context.
But for the first time, we have some instruments that can help us to see how we're doing.
As long as we're thoughtful about what the results mean.
This is a very big step forward.
Thank you very much for watching.

I want to introduce our first keynote speaker this morning who's going to talk to us about
data in the age of AI. David Weinberger is Harvard's Brookman Klein Center for Internet
and Society. He's participated in with that group for many years. He's an author of quite a few books,
Everyday Chaos, Everything is Miscellaneous, Too Big to Know, and many years ago the Clutrain
Manifesto. But he's also a columnist for KM World, so he keeps his eye on what's happening
in the KM World. He has spoken at KM World a number of times on many different topics,
but he hasn't for a while, so we're very happy to have him with us this year.
And please welcome David Weinberger.
Thank you so much. I guess I can take this off. It's wonderful to be back in this community
and amazing and wonderful to have participants from 38 countries, and I would now like to welcome
all of them in each of their 25 different languages, so just bear with me. Good morning,
which is the one I've really mastered, and I'll move on from there. Sorry, I'm an American.
So I want to talk about data in the age of AI, and I should warn you that this is a topic,
it's part of a larger topic that I've been trying to work through for the past few years,
and you'll see I have not fully worked it through.
Whoops, sorry, I've got to get the right clicker. The right clicker.
I found the right clicker. I'm not looking for your approval, but still. Thank you. Thank you very
much. So I'm also going to include metadata, because metadata is the type of data,
and it's really important from the beginning to me that you understand this is not a technical
talk, it's not a talk about the technicalities of data, it's an attempt to try to follow
how we think about data, not as data scientists, which I absolutely am not, sorry, humanities major,
and how that might be affecting how we view our world. So don't be alarmed at all the technical
errors, and I didn't fill in everything and the like. Sorry, enough disclaimers? So good. So this
is Martin Heidegger, a German philosopher who died in 1976. In 1954, he wrote an essay called
The Question Concerning Technology, except it was in German, and in it he says in a typical
Heideggerian way, technology discloses being. Heidegger was a terrible, terrible writer. He was
also, please do not look him up in Wikipedia. He was a horrible, horrible person, and I feel
ashamed even introducing him. I did do my doctoral dissertation on him, but that was a long time
ago, a very long time ago. Anyway, so my understanding, my way of understanding this is that our
engagement with technology casts a light on the world. We see our world through what technology
shows us of it, but it also casts a shadow, which is very prominent in how AI, which is certainly
a two-sided technology when it comes to good and evil. It's certainly true of AI. So the sort of
thing that I think he's talking about, but I don't really care because I want to talk about it, and
it's not a Heidegger lecture, is that say in the 17th and 18th century, where the pinnacle technology
was watches, which were amazing creations, incredibly complex, handmade, just mind-blowingly,
they were the chat GPT of the time. It was unbelievable that these things could work,
but because they were the pinnacle technology, the dominant technology, we began to see the
universe through that lens, so to speak. So the entire universe, we started talking about the
clockwork universe. The universe itself started to look like a clockwork, which meant that there
were very simple rules that govern it and govern it perfectly, beautifully. This was aided and abetted
by Newton's laws, which were the mechanisms, the mechanics of the universe. So that's an example
of how we interpret our world through our tech. And so there's, I think, a big question that asked,
which is, okay, so how are we going to interpret our world in the age of AI? And I'm picking a subset
of it, which is in light of data. So I'm going to begin with a really quick overview of starting
with mainframes working up to the internet and then AI, about how data has affected the sort of
public idea of data has affected our view of how the world works. So starting with mainframes,
mainframes, in the age of mainframes, data was really, really scarce, and it was a resource.
It had to be carefully managed. And just as with the punch cards of the day,
data was structured and it was a reduction. A typical human resources record was really
sparse. There was hardly any information in it because of the capacity of the computers.
And it was, of course, completely regular. Everybody's record was structured the same way.
I don't think it's a coincidence that one of the large cultural divides of the time was between the
IBM rep in the blue suit, absolute symbol of conformity with the rebel, the symbol of rebellion
here. And so it looked like our culture was divided into two parts. In the 1960s, this blue suit guy
remained, but the angry beaten it got replaced by a stoned and overly happy hippie. But it was the
same sort of tension. And so it was the tension between conformity, individual control and spontaneity
and reduction of information so we could manage it and part of control and wild excess often for
its own sake. As I mentioned oddly to somebody just a few minutes ago, this does not come up very
often, I was at the original Woodstock. So I'm talking from experience here. Oh, my mastering a
remote doesn't count for anything, but Woodstock gets applause? What sort of group are you?
So age of computers of PCs, as we all know, I think, what drove, well, those of us who lived
through the era can confirm that what drove the adoption of PCs was spreadsheets, killer app,
absolute killer app drove the hardware and spreadsheets present data in a matrix. Of
course, I'm you're all familiar with spreadsheets. There's a model of in this case how the business
works and the relationship among the pieces of that model. But all the way back in 1984, Stephen
Levy, who's a remarkably good writer about tech, he's still writing very a lot, always interesting.
1984, Levy said that what was actually interesting and important about the adoption of spreadsheets,
reason people were adopting them so enthusiastically wasn't because they modeled business, it was
because you could play with that model. It was the what if factor. And I think he correctly says,
in 1984, did I mention it was 1984 when he said this? He said that it's already this ability to
alter the model to play with it was already changing organizational structures. They're
incredibly early and insightful. So I think there's a tremendous amount of truth in this.
And so I'm going to pretty arbitrarily say that if you want to talk about data, the change in data
in the era of PCs, it goes from this controlled minimize thing to still pretty minimal amount
of data that it handled. But we want to play with it. And so data becomes detached, it's not
just a read out of the world. It's a read out of the world that you can you can mess around with,
you can try other worlds, you can try other forms of your business. And this liberates data from
feeling like a direct read out of the world that data are facts that we can just read out and they
are the reality. So then there's a sort of a micro era of big data, where one way of thinking about
this is the data becomes a source of surprises. And the canonical example of this is the discovery
that people who buy diapers also often buy beer. There's a correlation.
That's our granddaughter. I know. Anyway, she doesn't drink beer. I see where I went wrong
in this slide. We are not giving her beer. She doesn't even like beer. I mean, she likes it,
but she's not crazy about it. Oh, Poppy, I'm sorry. Anyway, so this actually, it doesn't
matter. It's a good example, but it's not a real example. It was actually a correlation that was
discovered in 1992, which is generally before the era of big data. And it was not discovered through
deep data analysis. Somebody noticed that people buy a lot of diapers and they did a sequel and
ask you L query to see what is correlated with that. It's beer. That turns out to be
an unlikely in reality, not really a helpful correlation. Although there is some debate
about it. Okay. So data in the age of the internet, the next two sections are going to get a little
bit longer. Sorry. So internet and AI. So in the age of the internet, data gets really confusing
because it's one sense. It shows up as a type of smog that we omit, which is an unfortunate image,
I guess. But as we're browsing and doing all the other internet things and corporations are
gathering all that data and compiling it and manipulating it and using it against us,
it was the overwhelming amount of data was so much that it actually seemed at times not to
clarify things but to make where to even start unless you were a professional data analyst
like at the platforms. So that's, I'm going to give you a few different ways, I think,
of characterizing data in the age of the internet. At the same time that it felt like smog,
so much and dangerous, the internet is all about links, which is sort of up from the data level.
It's more at the information level, if you want, where links are just about the opposite. I mean,
links are obviously and deeply human and form a structure that we can navigate at will and
notice how things are connected. Very different from the smoggy idea of data. And so I think
we've gotten a similar sort of polarity in which there's the smoggy. We all, I think,
maybe not. Most of us are concerned, let's say, about the use of the data that we are
surrendering unknowingly. Again, we click yes in order to, you know, that we've surrendered
unwillingly. We don't know exactly what's being captured, but we know that it's being used to
influence our decisions by people who don't have our interests usually as their interests. And on
the other hand, links, which is 100%, individuals who are able to control and connect with others
in an intensely social way, and both those things go on at the same time. And I think a lot of us
have this sort of divided understanding of data in the age of AI. But we also saw, if only because
of the gigantic amount of data that suddenly was there and seemed useful, we started unstructuring
our databases. We did this in all sorts of ways, a semantic web with linked open data from Tim Berners-Lee,
but also on structured databases like MongoDB and in graphs and data lakes and JSON,
this enormous unstructuring of databases, which has enabled a lot. Okay, so I want to take,
it's not really a detour, but I don't care. It's a detour. It's not about metadata in the
age of the internet. It's not a detour because it actually reinforces the same point. So
as you know, have you all tried a search engine? Because they're really amazing. If you haven't
tried a search engine, oh, I'm telling you, you really should. There's like a guluan, a bing. So
then you know that if you don't know who wrote Moby Dick, you can ask, it'll tell you it's Melville.
If you don't know what Melville wrote, you could ask, it'll tell you Moby Dick and some other stuff.
If you don't know either of those things, you can ask it about the content of Moby Dick.
And you can even misspell the content. Call me Ishmael. Where is that? It's in Moby Dick
by Herman Melville. So each of these questions contain a piece of metadata that linked to data.
And that destructures metadata, which used to be a label of fix to things. And so it turns out,
as we have discovered, that the difference between metadata and data is only functional,
only operational. It's how you're using it. Because so that metadata is what you know. Well,
you know it was by Herman Melville, but you don't know what the book is. And data is what you're
looking for. Oh, it's Moby Dick. That's the only difference between them. So these, which is an
incredibly powerful and liberating thing when everything can be metadata for something else.
And then in turn be the thing, the data that some other metadata is looking for. So this is why it
wasn't actually a detour. It's a, we're seeing the unstructuring of metadata, just as we are
seeing, have seen already the unstructuring of data. So now let's talk about AI, where again,
I'm going to have a few ways of characterizing it in a phrase, a bumper sticker. So the first is
data is generative in the age of AI, right? And we all, we all know this because we know
that in the old days before AI, a spreadsheet or any other sort of program generates data for sure.
But it generates it because humans have constructed the model. Whereas with AI, as we all know,
when I say AI, I really mean machine learning. I hope that's, I should have said that, but now I have.
With AI, data creates the model, which is insane and seemed completely implausible until, you
know, about 15 years ago, we found out, oh, yeah, that works. We can get more accurate classification
of objects in photos that way, even though it makes no sense. We're not going to tell
it anything at all about what we know about objects and how you recognize them. We'll just
give it data. It's insane, but it works. So data creates the model, the model then generates
new data, but it is remarkable that now data is generative this way.
Although I've already mentioned chat GPT once and thus have fulfilled my legal requirement,
I'm going to mention it again. So I asked it, this is about metadata. So I asked it the other day,
Dante's Inferno has three levels. Are there any other, give me five other artworks that have three,
that's, you know, a trio of things. And it did a good job. It's unbelievable. It's amazing.
It's amazing because we don't have a sense of scale. Humans can't, you know, at least I can't
think it's scale. So I'm amazed and surprised by what scale can do. But it is, of course,
incredible. It gave me five. It gave, they're good. They gave pretty good explanations of why,
which I hadn't even asked for. And so if we think about this in terms of metadata, the metadata in
my query was artworks that show something in three parts. And the data that it fetched was
that text that you just saw. And what is, I think, amazing about this is the metadata now is generating
data. The metadata generated that content. It's not something I've seen before. And if we have,
I'd be really interested in some other field or some other way. I'd really like to know about it.
This is, this is mind blowing. We have metadata that will generate its own data. And generally,
it's good. We can't, it doesn't know when the data isn't good. Basically, as you know,
basically every, not basically, everything that chat AI says is an hallucination. It's just that
most of those hallucinations are true hallucinations. It doesn't know that the ones that it's making up
and we often have trouble telling. So that's a terrible problem. So let me give you an example.
So in January 2022, researchers at the University of Leeds and some other institutions published a
paper that said we built a model from retinal scans and some really basic medical information,
like age, weight and the like, really a very small set of it. Does anybody know about this?
It's sort of mind blowing. Because it works. I'm sorry. Let me be more precise. What works is
the AI is able to predict with some degree of accuracy the likelihood of an individual
having a heart attack, myocardial infarction, based upon the retinal scan. Nobody, data scientists,
AI people, doctors of all stripes have tried to figure out what about those images, presumably,
but who knows? Presumably, it's the veins, but we don't know. We can't figure out how it's doing it.
It is, at the moment, inexplicable. And I know that there are actually bunches of people here
working on making machine learning less inexplicable at dinner with Beth Truden and a bunch of other
people at that table last night where this was a lively topic of conversation and Beth's company
has a way of keeping the sources, the citations and sources of the knowledge
with the output and is generating it from a more fact-based and reliable set of information.
Is that approximately right? Okay. It's approximately right. There's tons and tons of work in all
areas to try to make AI less inexplicable. But as it stands, let me put it like this,
that inexplicability is one of the two original sins of AI, by which I mean,
and I know the sentence doesn't actually make sense, but it may make sense. You'll be the judge.
So left on its own, AI would tend towards inexplicability. There are interventions and
structurings of all kinds that we might be able to do to prevent that, but it doesn't care if we
understand that has not been its mission. It's this mission has been to give accurate predictions
based upon data guided by a ton of human decisions about what we meet, what we're looking for and
what we will accept. So inexplicability is pretty common so far in AI models. Beth, how much trouble
am I in? You'll tell me afterwards. Okay. So in this regard, I think a second formulation of data
in the age of AI would be to say that, okay, we'll do it by hand. I have forgotten how to use my
clicker. A few moments, it was great. Okay. That AI, data in the era of AI is a source of secrets,
like, oh, there's secret information in a retina that can let us see what's going on with the left
ventricle, which is an indicator of heart health. Didn't know it. It's there, but it's secret. But
actually, I would think I would prefer the formulation that data in the era of AI is a
keeper of mysteries, because secrets, once you know them, generally, you know why you know them,
how you know them, why they're true and all that. When mysteries, you know, but it remains a mystery
how it happened. And so far, that seems to be at least some of what AI does.
So let's for the moment, we're going to just overstate and say AI tends to be inexplicable
at the moment. And we can argue later, we won't. And then we can play the five-year-old game.
Now, the baby you saw, we do have a five-year-old grandchild as well. So that little drunken baby
that you saw would not be doing this, but she will be in a few years. So you can ask why. Just
keep asking why. So why isn't it inexplicable? And the answer is because the model is just too
complex. Okay, why? Why is the model too complex? Because there are too many factors, there are
too many variables, there's too much data that's connecting to too many others. You know, ChatGPT
has 175 billion parameters, which are weights, weighting the relationship, the importance of the
relationship of words, 175 billion. So if you want to know why it chose one word over another,
why it called a house you're looking at luxurious rather than upscale, you're never going to know,
at least at this point, you're not going to know, there's just too much going on there.
So, okay, well, why are there so many factors connecting to too many others? Well, that's a
good question. Because actually, that's how the world works. The world is really complex. I mean,
the universe is the single most complex thing in the universe. And it is really, really complex.
And the reason that these complex models work is that they are capturing something about the
complexity of the world that we live in. They go wrong in all sorts of ways, they're dangerous and
how they go wrong. But when they work, they're capturing something mysterious about the world
we live in, which is there's so much stuff and everything affects everything else all the time
forever, everywhere, everything, everything. It's simultaneously. It's not like a clock. If only
it was like a clock, but it's not. It's intensely beyond imagination complex in its interrelationships.
Okay, so why didn't we notice this before? And lots of people have, I think we all have,
it's not news that the universe is complex, but it doesn't register. And I think it's
for two reasons. So, I mean, the sort of complexity I have in mind is, and this is a relatively simple
case, what determined why those people were going to be in the crosswalk with you this morning,
exactly where they are. There's no hope of figuring that out. It's way, way too complex. It's too
complex for any one person. So why haven't we done anything? Why haven't we come to grips with this?
Why isn't this the baseline of all of our thought and thinking? Well, I think for one thing,
it's because we couldn't do anything with that sort of information. And so we just ignored it.
Generally, we said, well, no, that's an accident. It's just, oh, who knows? It's chance. It's
coincidence. We have a whole vocabulary for dismissing complex, the results of complex
interactions is not worth our attention because we couldn't do anything with it. And second of all,
because it didn't fit well with our old human models. So I'm using our here as the West.
I just have to limit my domain because it's basically the only thing I know enough about
to be talking about, any legs to stand on. So in the West, this view does not fit very well
because for thousands of years, if we take the Greek ancient Greece as the origins of
Western civilization, which is controversial, but traditional to do that. Back then, the idea was
there's all this mess in the world that seems chaotic, but underneath it, there are laws and
the laws are simple and understandable. There are laws, there are rules, there's universals,
there are principles, there are overall, there are generalizations. That's what we hang on to
because we can use those. So in the West, traditionally, we have viewed generalizations
of various forms. They explain what's explained, which is the particulars. This is a chart of why
we have preferred the general because they are generalizations reduce information. They're simpler
and it just so happens that the laws of the universe happen to be simple enough for humans
to understand what a coincidence, but they simplify something that's very complex, the realm of
particulars. We have thought that these laws are eternal. They've always held. They explained
everything going back to the Big Bang, even before we knew about a Big Bang.
And ultimately, they are the truth. In our tradition, the Western tradition, we have
looked up into the skies for the eternal truths. In the case of the ancient Greeks, more or less
literally, we still do it. We valorize the eternal over the particulars, which are sort of just,
they're over like that. They change all the time. There's no real abiding truth in them.
But I think that we are entering an age with the age of AI. For me, I think the most important
sort of change in how we view things is that we are getting a more particularized view. We're
taking particulars more seriously. We are letting them have voice with models themselves. I should
pay attention to my slides. The models themselves, machine learning models, being literally
generalizations. But they're generalizations that are made up of patterns that have been derived
from particulars. Those patterns can be so small in particular that there are billions of them
that get sorted through and generalized in various ways. But they stay true to letting
the particulars speak. And this is why machine learning can make, can sort animal photos better
than handwritten code has been able to. Okay, so if we are going to get more used to the idea of
particulars as being real and important and in some ways determinative and having their own voice,
then how will that happen? And I think here it may be because of the light that is cast
by, by the shadows that are cast by AI. It's our fear. So I want to give two examples.
One is a common sort of fear gets expressed variously, but we don't know how it works,
which can be genuinely scary and important to recognize, right? But it, there's two words at
the end of the sentence that are really crucial. So when you hear, we don't know how it works,
we also hear it works, which is amazing. So this fear may be moving us towards an embrace
of the particular because we then hear when we ask, well, why don't we know? We're told
because it's way too complex. Well, in hearing, we don't know how it works. We may be being led to
believe, not on purpose, but being led to believe that it works because the world is also wildly,
complexly particular. So I want to give you a slight example of what it might mean to understand
something outside of the realm of AI in terms of particularity. So in this case, I picked a simple
little topic, morality. I think I have like three slides and we'll be done with morality and that
will be great. So typically in the West, traditionally in the West, we have had, we've resorted to
ethical frameworks. So religious framework says, do what God commands you to do. Reason one says,
do follow principles that are based in reason. And then the utilitarian one says, the framework says,
do that which will bring the most happiness to the most people. Pretty rough, but you know what I
mean. So it's a framework. It tells you why some things are good and some things are bad. And it
tells you what to do in particular situations, except they don't. They don't work. I'm going to
give you the world's briefest and simplest example, I think. So you're you, you have a friend, A,
who is angry, tells you that they're angry at your mutual friend, B. But A says, but don't tell
anybody about it. Shortly thereafter, your mutual friend C comes along and says, oh, I'm very excited.
I'm putting together the seating plan for our wedding. And I'm going to put C here. I'm going
I know A and B are such good friends. I want to put them next to each other. Do you tell
do you tell C that that's a bad idea because of the fight? I don't know. And I'm going to say,
neither do you, because we don't know the particulars. You're going to think about this,
and you're going to think, well, yeah, how vindictive is A? Is A forgiving? Would A understand if I
didn't explain why? How deep is the rift between A and B? How how upset is C going to be if there's
some minor tension between two people at their wedding and so forth? Without that, you don't
know what to do. And considering what to do morally, you have to think about the details. In fact,
if weirdly, the next next week, you run into different set of friends who have exactly the
same sort of formal thing that one's angry and don't tell and the rest of it, but it has
their different people and it's not a wedding. It's whatever. It's a camping trip.
What you did in the first case is not going to help you decide what to do in the second case.
You've got to rethink the particulars all again. That's what you would do. You would say, well,
I didn't, I violated my promise to A because A is forgiving, but C, oh, C would never forgive me
if I did that, for example. It comes down to particulars. So the philosopher Martha Nussbaum,
1999, in a book called Love's Knowledge, which is a serious philosophy work with one of the
great titles and very apt title. Anyway, she talks about this and she says moral situations
are not commensurable. You can't compare them. And that's because they are so particular.
So the second example is of a fear that may be telling us inadvertently what the world under AI
type of data is. So it's like what the light is that's being shown. So AI is biased. It is biased.
It is the second of its original sins. Left on its own, AI will be biased. We have to guard against
it. It's very hard to guard against it, as I'm sure everybody here knows, but we can ask, okay,
well, why is it biased? We're told this and then it's explained to us why it's biased because
data tends to reflect societal biases and the like. And what we hear from this is, oh,
oh, we get to select the data. So data is human stuff. It's not a readout of the world. It's
not the facts about the world. Facts aren't exactly facts either. Different topic. It's stuff that we
decide, data is stuff that we read off of meters, meters that we have decided to plant. Where and
why and to what degree of accuracy and what we do with the data are all human decisions.
If I say data is human stuff, I don't mean it's
unreliable, but I sort of mean it's unreliable that there's a human element of decision
which includes unconscious biases and occasionally conscious, but generally the issue in AI is
unconscious, unaware biases of what's going on, how what we're doing might be taken in any of the
38 other countries other than the one that we happen to be in right now, for example.
Data is human. It's human stuff, obviously. Okay, so how does that lead to anything? Well,
it may be that in the discussion of bias, we get to the point of proxies because the first
response is, well, let me just don't record if you worried about bias against women or whatever
protected class or whatever you want to call it, then don't collect, don't have a column for women
and then you have to explain, well, no, but there are proxies for women in this case.
That's, proxies are really interesting, at least I think so, in part because it's proxies work by
putting shape around the whole, the thing you're trying not to have affect your,
it's like the missing piece in the jigsaw puzzle. You can see what that shape is and that can be
part, that becomes in a sense part of the data. I know I'm not putting this technically correctly,
but I'm trying to talk about how this will perhaps appear to people who are not technical.
And there are only proxies because things are so interrelated. They're so interrelated that the
absence of something can be the presence of something. That's maybe one way through this fear
justifiable and terrible and correctly terrible fear of bias. You don't have to, I got myself
started on it, I'm going to back away from it. That this, the explanation of it, the understanding
of the first question, well, why is there bias? Why can't you just lead you to understand the deep,
deep, multi-dimensional interrelationship of all of the data and all of the world?
So if we ask what, the original question of this talk, which is,
if tech casts light and shadow on the world, can we think about how our world is already beginning
to look in the light of this change in data? So way too short what I'm about to say, I understand
that, but it's just to give you a sense of it. So if you look at an enterprise in light of particulars,
I think that we begin to see black swans, you all know black swans, you all know black swans,
the unexpected things that happen, drop out of the sky and destroy a supplier's factory
in your supply chain and suddenly your business is in great danger.
Literal lightning struck. Yes, that's right, I think there are black swans, but look that through
in this light, in the light of AI and data, everything's a black swan. That's the nature of
particulars. You can't, it's not just the big events, everything that happens is a mashup of
everything that happens all at once. So everything's a black swan and except some things are
butterflies in the chaos theory sense in which other butterfly alights on a plant in Brazil and
triggers a tornado in Kansas, right, the standard example. It seems pretty implausible, but the
idea is very validated and sound, which is that a small event can create cascades by which it picks
up energy and has surprising and important results. And by the way, it's really hard to go backwards
from the tornado to pin it on that damn butterfly and sort of pin it to a wall for what it did.
That would be, often that's what we're trying to do with AI to understand it.
Okay, so more specifically, but still not very specific, this sort of thinking I would imagine
should have important effects on really important business topics beyond pinning butterflies,
including strategy, obviously, if we're living in this sort of chaotic world in which
each particular affects every other throughout the universe basically, then strategic thinking
takes on a different tone, especially in terms of it's committing people to long term strategies,
design of products, managing people, all of these things, and more, it seems to me
can get rethought in the light of particulars. And then moving from business to life, I know
that business is part of life, but broader. So we talked, I talked about morality,
excuse me, but exactly the same sorts of considerations that you have to look at
the particulars I think holds for every decision that we make where we realize we're making a
decision. So there are decisions I'm making about waving my hands now, which I'm not doing
with any conscious awareness of, but decisions that we actually deliberate on even a little bit,
whether it's what items to pick up from the lovely buffet for breakfast,
too much more important things. Decision making is also, like morality,
all in the particulars. I mean, the muffins looked really good, the croissants look good,
but were they? I need to crinkle them first, and then I'm going to do sort of a carb,
sort of a balance, and what did I have last night? It's particulars, all decision making, and,
you know, love. We don't love our loved ones in general. I don't, that's a very weird thought.
We love them for who they are, and who they are as particulars, and that
year and a half old baby is extremely particular in both ways. So I think this line of thought,
this is actually what I've been working on, and more or less consumes me,
is how this changes our ideas about creativity, which is particularly relevant in the age of
generative AI. I think it changes our ideas about free will, which I know is not something that we
generally talk or even think about, but it is a background concept of considerable lineage in
the West. And I think the model that we are seeing, the light that AI is casting on a role,
should have us rethinking the age-old impossible argument about free will. Knowledge for sure.
I hope I don't have to say anything more. I mean, what does it mean to know in an age where
particulars dominate where we will continue to want to generalize? Of course, there's a
rhythm here, but one in which the particulars have become more dominant and recognized than they
have is, I think, a really important question. And I actually think that many of you here at
KM World are managing that question, although not in the terms I'm trying to push on you.
I'm going to guess that much of your life as KM people is, in fact, engaged very practically
with this question. Mind and body, I'm going to guess, is not a burning, you know, the relation
to mind and body. It's probably not a burning question for everyone, but it's, again, a key
shaping and background thought in the Western culture. And even the nature of reality, which
has become, to everyone's surprise, it's not a topic that has been on the top 10 list in philosophy
for a while, but with the rise of simulations suddenly, thanks to AI, and the questions in
many of the questions of simulations also have to do with how particulars show themselves,
rather than about big generalizations. So, finally, data in light of AI.
I think that we recognize it's a human artifact. It's something we make. It's
something we participate in making anyway, by the decisions that we make, and then what we do with
that data as well. We can't just accept it as we did in the 1950s, which was part of the weight
that was on that poor, who was the guy in that, the actor on the left?
Gregory Peck.
Is that good?
Exactly right. Gregory Peck, thank you.
I forgot where I was going, but I now know it was Gregory Peck. Appreciate that.
Anyway, so it's, Gregory Peck took data as the bedrock, that is, the IBM generation.
Took data as a bedrock. Now we see how much of human decision making, how much of human
assumptions plays in the creation of data, and how we use it in AI especially, directly,
is the responsibility of humans who make decisions about it, even if they shirk those decisions.
And thankfully, most people that I know who are doing AI take those decisions really seriously
now, but it's still an enormous problem. Seeing data as a black box of relationships,
from which we withdraw what we need as human artifacts. And I think the pithiest way I can put
it is, if particulars are becoming more important to us outside of the world of AI, but in part
because of AI, then I think we can think about data as particulars. But there are particulars
that have been rendered machine learning. They gain something from that, I'm sorry,
from machine readable. They gain something from that, but they also learn something.
They gain some capabilities from that. So I think we're in a world in which
particulars are rising. I'm not sure if it's obvious. I think this is a really important
corrective to a Western focus on generalizations. It's because particulars are a reality.
So, thank you very much.

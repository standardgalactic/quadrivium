Hi everybody, so I'm Emory Berger. I'm going to be talking today about performance matters.
This is joint work with my former PhD student, Charlie Kersinger. So I'm going to tell you
a story that happened a short time ago in a valley far, far away. Not actually this one,
in fact, this one. So the story is about a character who we're going to call Luke. Luke is
our hero. Of course he's a hero in Silicon Valley because Luke has an idea for an app. Alright,
so here's Luke's genius idea for an app. Basically you use your smartphone and you take pictures
of things and then it goes and finds matches like and returns them to you and he's got a great name
for it as well. It's going to call it Ogil. And he's pretty sure Ogil is going to totally
disrupt image search. Nobody has told him about Google image search yet, but anyway. So he sets
about building it. So he makes the prototype Ogil. So the prototype Ogil works like this. Take a
picture. It gets sent off to the Ogil data center. If it is new, it gets added to the database. And
then at the same time it goes and finds similar pictures and sends them back to the user. Alright,
so this is pretty straightforward. You can kind of abstract out this flow. If you think of it as
follows, you get a request. The request comes in. It essentially spawns two threads. One of them
goes and takes the image and let's say compresses it and then saves it to a three and a half inch
floppy as one does. And then at the same time it does some indexing to look up matches, right,
so it does a search after doing some feature extraction and then sends it back to the user via
paper airplane, which is the most effective method known to man. And then eventually these two threads
join up. Alright, so this is obviously a simplification. I've alighted certain things like
for example locks. So when you have locks, right, there's some database for example, you can't be
modifying the database simultaneously, blah, blah, blah. But anyway, this is essentially at a
high level how Ogil is going to work. So Luke goes and ships it off to the app store. It gets
approved and you know users start coming and Ogil is working, right? He's disrupting image search.
But then as the number of users kind of like mount, it turns out Ogil starts getting slow,
right? So it takes a long time for Ogil to load, right? So this is too bad. He's not happy about
this. He's not really sure what to do. So, but before we get into that, let me talk about another
version of Luke. This is also Luke. This is the cool hand Luke, if you will, with his glasses.
Alright, but really this is Luke in the 80s. Alright, so he's got the 80s glasses to match and
of course those of you old enough to know or who have watched TV know that there were no smartphones
in the 80s. There were instead computers with big CRTs. So this is what Ogil looked like in the 80s.
So you'd have some ASCII art and you want to go out and it would connect via some modem,
let's say, search the database. This is version 1.0 of Ogil 84. And it comes up with its matches and
this is your user interface. So I hope you liked it. Alright, now of course back in the day,
1984, computers were really slow, right? Really slow. And so actually this Luke today who was
like Ogil is too slow, well things were really bad in 1984, right? But it turns out things were
also kind of better in a way. So performance used to be really easy. So I'm sure all of you have
seen this kind of graph. What I'm going to show you is on the left, the number of transistors,
as years progress on the x-axis, if they go up, you'll notice it is a log scale. And on the right
you have clock speed, which roughly corresponds to computing performance. And so basically for
years and years and years, we had this situation where the number of transistors were increasing
roughly at doubling every 18 months. This is famously Moore's law. And this generally had the
effect of allowing clock speed to increase. And so you had smaller features. This meant that your
programs would actually run faster. And in fact, you could just literally wait, right? If you buy
new hardware, in fact to the upgrade cycle every year or so, like if you bought a new computer,
everything would run much faster. Now things were slow, right? So don't be so excited about how
great it was back then. It was bad. 33 megahertz to 66 was awesome back then and kind of terrible
today. But it did change the landscape of what it meant to be somebody who works on performance
improvement because this is what you would do. So there really, in a way, was no sense trying to
squeeze out any performance out of something when it was going to double in 18 months, right? The
chance that you in 18 months were going to squeeze out double the performance pretty slender. So
you might as well just sip mojitos on the beach, all right? I can tell you that this was, well,
maybe the mojitos and beach part, notwithstanding, this was actually kind of a strategy that was
recommended for high performance computing. People would literally say, we're just going to upgrade
the computers next year or in two years. So focus on something else. Don't focus on performance,
right? Now, unfortunately, that's not the current state of affairs, right? So today, as all of you
know, when you upgrade a computer, it does not get faster anymore. Maybe it gets lighter. Maybe it
has improved battery life. Maybe not. But basically what happened is eventually Moore's law kind of
ran out of steam. The reason that it did is not actually true that Moore's law ran out of steam.
This is technically a problem called denard scaling. And so denard scaling ran out. And so right
now, we're in a situation where basically, if we clock things up, we no longer can dissipate the
heat on chips effectively enough to make them run faster. So transistor counts actually are still
increasing, which is why we have multiple cores. So we have a bunch of transistors. What are we
going to do with them? Let's just stamp out more of the same thing, right? So that's great. But it's
also not super awesome because if you have a program that is not parallel, it's not going to run any
faster. So everything now has multicore. Your phone has multicore. But it turns out it's still a
problem. So these are actual screenshots from app store updates. Every app store update practically
is about performance or bug fixes. And so here's one that says under the hood updates for better
performance, as opposed to the over the hood updates. Okay. Here's bug fixes and performance
improvements, bug fixes and important performance improvements. And then this one, I like love
this one a lot. So they're App Engine calibrations because it sounds cooler. So they've calibrated
the App Engine. All right. Okay. So why does this keep happening? Why is it like every update is
like, oh, God, we got to improve the performance. We got to improve the performance. Why is this
so hard? Why didn't they get it right the first time? And it turns out, unlike bugs, so code is
always buggy, right? And it's hard to debug programs, but it's like this thing produced the
wrong result. That's pretty clear. But if you have something and it just runs slower, you don't
really know where or what to change, right? So it can be really complicated. So what I'm going to
talk about are two things in this talk. One is performance analysis. So I'm going to explain
how to do performance analysis, right? It turns out that the thing that we, including myself, have
often done is not really very rigorous. And you can draw the wrong conclusions. And then I'm going
to talk about a new approach for performance profiling that will show you how to do it better.
All right? So first, I'm going to talk about performance analysis. So here's Luke. Luke has
his code. And Luke is like, it's too slow. Ogil is too slow. What am I going to do? And so Luke
has an idea. And Luke's idea involves some sort of new, you know, I'm going to do this first,
and I'm going to change this code. I'm going to change this function, blah, blah, blah. I make
a bunch of changes, right? And eventually I end up with A prime, all right? The new version of A.
And so now I want to know, did it work, right? So I've made this change. I thought it would
make things faster. So I go and I take my code, and I have some set of inputs, some benchmarks,
let's say. And I go and I say, run A. And A takes 90 seconds, which is clearly too long for your
App Store thing to run. But anyway, notwithstanding that, let's say there's something that takes
90 seconds in his test, right? And then he runs A prime. 87.5 seconds. Fantastic. Success, right?
2.8% faster, all right? Time for Mojito, okay? So this is great, right? And clearly, you know,
what Luke went and did had a big impact, big impact, all right? The question is, like, is it
really faster, right? So if you go and you plot, like, here's a bar graph with, I'm kind of giving
away the game here, one execution of A prime and A. It turns out that A prime is 2.8% faster,
looks good. But there's maybe a problem here. So what's the problem?
Right, so there's this problem called variance, right? Like, when you run a program once,
you're going to get some result. But if you run it again, maybe the result will be slightly
different, and you want to account for that. Great. So now we'll run it 30 times. 30 is the
magic number, by the way. So we're going to run it 30 times, and we get this graph. And so they're
pretty tightly distributed, and you can see it's still 2.8% faster, right? So seems plausible,
like, I think everybody here would probably be like, looks like A prime is faster. Great. So
the question you have to ask yourself is, why is it faster? So you might think, well, of course,
the reason is the code change, right? So I, as Luke, I'm the developer, and I go and I'm a genius,
and I have my great idea, and it pays off 2.8%, right? Well, it turns out changing the code
can actually lead to all sorts of knock-on effects that have nothing to do with your intended change.
So it could totally be an accident. So let me explain why. So there's this wonderful paper
that is, it appeared in ASPOS in 2009 by Todd Mitkiewicz and a number of other people. I highly
recommend you read it. It's something like, how to do things wrong without really trying, something
like that. And the conclusion is that the layout, like where the code and data end up in memory,
has a pretty big impact on performance, all right? So when you go to measure something,
those measurements are biased by depending where things kind of fell, right? So here are a few
things that can have an impact on layout, and I'm going to talk about more. So one is link order.
So if you're in cc++ land, and you have a make file, and the make file has a bunch of,
this link step and has a bunch of dot-os, depending how those dot-os are arranged,
you can get different performance, okay? You might think, fine, all right? Your environment
variables. So when you go to execute your program, your environment variables, whether it's in cc++
or even managed languages, they somehow get copied in and everything else gets shifted.
So in c and c++, this moves the stack. So this actually has an effect on layout. These two alone
can lead to shifts in performance of plus or minus 40%. Okay? So that's not great. So what is
happening? Like, why is this happening? This is a huge shift. This is literally larger than the
impact of dash o3 over dash o0. Okay? Yes, you laugh, but as well you should. So why is a prime
faster than a, right? So what is going on? Why could this happen without actually trying? So part
of the problem here is that basically modern processors have become insanely complicated
in their zeal to increase speed. So what do they do? So they have caches, right? Add data
and instructions, get mapped to the cache. Well, it turns out for good reasons, these things are
binned up into these things called sets. If they map to the same set, you can have a conflict. So
if you have hot code, a lot of hot code that is mapping to the same set, then it's not going to
necessarily fit in cache, and your code will run slower. By luck, you could be in a situation
where when you changed a prime, you actually disrupted this conflict. And so now you have no
conflict, right? These two things, one is the hot code and one maps to nothing. So no conflict.
Boom, it ran faster. All right? So that sounds great. So it could be the cache, but it could also
be the branch predictor, which actually, again, is based on the addresses of your branches,
and if these branches collide, then you can end up with things running slower.
There's also this thing called the TLB, the translation look-aside buffer, which maps
virtual addresses to physical addresses. If things don't fit in the TLB because they span
two pages instead of one, suddenly things become slower. There's also a branch target predictor.
There's a prefetchor. There's more. All right? So this is pretty bad. So all of these things can
happen. You might think, all right, link order is fine. The code thing is a little weird, but,
you know, hey, it's faster, right? It's 2.8% faster. That, like, I don't care. It's all good, right?
Now, it may not be faster on every machine, but it's faster today, right? So here's the problem.
Like, anything you do can disrupt this. So what could happen? One more malloc changes layout,
right? Like, you've shifted everything over, one more or less. If you upgrade anything in
your system, this is going to change layout, right? So that's bad. Okay. So those things,
all right, I'm not going to change libc, and I guess I'll never malloc again. Fine.
Whatever. All right? So here's something that may be surprising. Running it in a new directory.
So it turns out that your current working directory goes right into your environment
variables, right? So that's weird, right? So, you know, if Vader tries to run your software,
it's not going to work as fast because it's one character longer than Luke, okay? This is a real
effect. This can really happen. It has actually bitten me. I had a student who wrote something.
He has a long Indian last name. My username is just five letters long. It's just Emery.
And he did something. He's like, oh, it doesn't run any faster. It actually runs slower. And it's
like, that makes no sense at all. And eventually, we whittled it down, and it was like, if I run it
as me, it's faster. Okay? That's right. All right? Changes your layout. So the solution is obvious,
right? Run everything. All right. So I should add, you know, all of this is, you know, like,
the whole talk is really oriented towards, I'm going to improve my performance. But
everything I'm talking about today can be viewed in reverse for performance regression. Like,
I made a change, and things run 2.8% slower. Oh, God, roll back. Maybe not, right? Maybe
the next thing you do is going to actually undo that change, right? So basically, layout is super
brittle. And like you've seen, layout biases measurement. So one of the questions that we
wanted to know is, is it possible to eliminate the effect of layout? So we can actually understand
the performance of things kind of without having to think about, well, one malloc less or more,
or, you know, Luke versus Vader. So the answer is yes. We built a tool that we call stabilizer.
So stabilizer addresses this problem that I've just explained to you. Pardon me. And it eliminates
the effect of layout. So this is a way to actually measure programs where you can kind of actually
know whether the regression you had is real or whether the optimization you had is real
and not just an accident. So how does this work? How is this even possible? So the way that
stabilizer works is that it randomizes layout. So it randomizes a lot of things. It randomizes the
function addresses. It randomizes stack frame sizes. It even randomizes heap allocations. But not
only does it do those things, it does it over and over again. So while your program is running,
it's literally doing randomization. And this turns out to be important, and I'll show you a graph
that we'll explain why. But basically, if you do this, then there's no way layout can bias your
measurement because a completely random layout can't bias the results. That's just how things work.
That's why we run randomized control trials. You've eliminated something as a possible cause.
The only other cause that remains is whatever change you made. So let's walk through what you
would do with stabilizer. So with stabilizer, again, clearly you're supposed to run your program
a bunch of times. But notice what happens to the execution times. Here the execution times are no
longer tightly bound around this one very small measurement. The reason for that is that when
you were running that program 30 times, it was sort of like you were going to do a survey of 30
people, but you just ask one person. Because it's the same layout over and over again. So you did
an experiment on 30 executions, but what you really did is you just repeated 30 on one. So the only
noise that you're eliminating is the noise that comes from network demons waking up or some other
random event, maybe some thermal issue in your computer, but it's not really altering layout.
It's always the same layout. So here it's not, and you get these very nice bell curves. So now I'm
going to ask you the question. So this is an audience poll time. Is A prime faster than A? I just
want you to raise your hands if you think that A prime is faster than A. All right, great. Now keep
your hands up. Don't set them down. But set them down if you change your mind. How about now?
How about now? There's still a few like hardcore. So what you all are doing is what I like to refer
to as eyeball statistics. And so you're kind of like, looks close to me. That's too close.
Right. But it turns out this is not actually a thing. So if you, yeah, it's not really statistics
when you just eyeball results. So this is a bit of a refresher for some of you, but I'm going to
walk you through this and how this all fits in with stabilizer. So in actual statistics, and today
I'm just going to talk about one flavor of statistics, which is called null hypothesis
significance testing. There are others, notably Bayesian approaches. Happy to talk about that
offline. But basically the way it works is you just assume that the things are the same. You say
what is the likelihood of observing this difference by chance? All right. So it turns out that this
is something that's just convenient. It's very easy to compute these probabilities for the normal
distribution, which you all remember from school. These graphs are normal. Awesome. It turns out
that stabilizer happens to make normal graphs or normal distributions. And I'll explain why.
So how are we going to do this? We're going to run stuff with stabilizer.
We're going to pick some probability below which we're like, okay, good enough. Right. So
if it's only a one in 20 chance, I see this probability like the, I see this event occurring.
I'll be like, okay, that's good enough for me. You could be harsher. You could say one in 100,
one in 1,000. It's pretty standard to say one in 20. This is the p value of 0.05. So the idea is
if there's a low enough probability, you reject the null hypothesis, the null hypothesis being
that they're the same. And you conclude that the speed up is real. It's not due to the effective
memory on memory layout. All right. So why re-randomization? The reason for re-randomization
is that just randomizing once doesn't give you enough randomization. So this is an actual program.
You can see the distribution is pretty wacky. It's very far from normal. You can't intuitively
explore much of the space when you just randomize at startup as opposed to randomizing during
execution. This is in fact the kind of distribution you get when you randomize all the time. And
these are normal distributions. So why do I keep saying that they're normal distributions?
The reason is essentially, again, going back to like freshman stats, stabilizer generates a new
random layout every half second. That is to say it's a completely independent version of the program
right from half second to half second to half second. It's all randomized. And it's the same
program the whole time. So it's identically distributed. And then we're adding them all up.
And there is this nice result, a key result of stats, which is the sum of a sufficient number.
So if you run a program for long enough of independent identically distributed random
variables, it's approximately normally distributed no matter what the underlying distribution was.
This is the central limit theorem. So this makes execution times normally distributed,
which is cool in other ways because you actually know how likely it is that you're going to see
some very weird execution because you know what the distribution looks like.
All right, great. So now we have this thing in hand and we're going to do something insane.
We're going to see whether optimizations actually matter. And we know some of them matter. So we
have a suite of benchmarks that we're going to evaluate it on. We're going to evaluate them
individually and then across the whole benchmark suite. And I'll show you how we do it. So you
build the benchmarks with stabilizer. Stabilizer is a plugin for LVM. If you just compile it as such,
it goes and randomizes everything. But you can actually just randomize things independently
if you wanted, like just code, just heap, and just stack. So now we run the benchmarks,
we run them as usual. We drop them into one of my least favorite programming languages ever.
And then we decide what the result is. So again, we don't ask questions like this
because that's eyeball stats. Instead, we ask a question like this,
pretend they're equal. How likely is it we'd observe this much of a difference?
So I also have to say that you should not assume normality like almost ever in my humble opinion,
unless you have very good reasons for doing so. Here we have very good reasons for doing so.
So we can use really powerful statistical tests like the student's t-test. So this is the test
that you used to actually measure this difference. So if the p-value, the likelihood of this event
occurring, is less than some threshold, which as I mentioned before is 5%, we're going to reject
the null hypothesis. That is to say, it's not because of random layout, the difference is real.
Everybody's on board, I hope. So now we're going to do it. You'll be shocked to learn
that dash O2 versus dash O1 makes a difference. Good. I would be weird if the result were otherwise.
So you can see that there are statistically significant improvements, right on the right.
There's some that are statistically significant but don't matter. And by God,
there are statistically significant performance drops. So it turns out that compiler people
run these same benchmarks and overfit the way that they do these optimizations,
and some of them lead to layout changes. And it wasn't actually the code change.
And so we can actually distill out this effect. All right, great. By and large,
it looks like O2 over O1 is a win. How about O3 versus O2? Ready? It's amazing. Okay.
So I actually have to change the axes so we can see a lot of these values.
So I'm going to zoom in. Instead of it being 0 to 10, like the range is negative 10 to 20,
I'm going to make it 0 to 1.5. Okay, so now we can see them. So they're pretty small effects,
but some of them are significant and some of them are not. Again, statistically significant,
1.75% decrease in execution time. Great. A bunch of things where it's not significant
and a couple decreases, but really very minor effect sizes. So what do these results mean?
I mean, you can't actually look at an individual benchmark. Like there's 30 of them, right?
So drawing a conclusion about all 30, you actually have to do something different.
You have to collect all of them. You get a bunch of these graphs, and this is what you don't do.
Like, okay, this one is slower. This one's faster. This one's faster. This is just eyeballs everywhere.
Okay? I mean, they're spooky and nobody wants to see those. So again, we're going to do the
same thing, but to test a bunch of things simultaneously, you do this thing, which is
terribly named, called analysis of variance, and you plug it into R with this awesome incantation,
and then you do, again, the same test. If the p-value is less than or equal to 5%,
we reject the null hypothesis. All right? You ready? All right, here we go. Here's the p-value.
So it has to be less than or equal to 5% or else we're going to conclude that dash
o3 versus dash o2 is nothing. All right? So the p-value is 26.4%. That means that one in four
experiments will just show a random effect, right? Just literally randomly. We do not consider
this enough evidence to reject the null hypothesis. So we're, we cannot reject the null hypothesis,
which is that the effect is indistinguishable from noise. All right? Okay. So this is all
terrible news for people like Luke who wanted optimizations to work, and I've actually seen,
I've actually seen projects. It makes, it kind of breaks your heart. Like projects I committed,
like on GitHub, where it literally says dash o9, and I feel like why not dash o11? There's no,
there's no dash o9 or 11. It's just kind of bottoms out. But you know, hope springs eternal.
All right. So great. So what are we going to do? So what people do when they can't speed things up,
right? They run a profiler. So there's these profilers, they all basically work the same way.
You go and you get some result, and it says, hey, here's where my program spent its time.
You get the number of calls to every function, runtime for each function, and this captures
intuitively, maybe for most of us, like this is what a profiler should do, right? What do I care
about? There's frequently executed code or code that runs for a long time. That's where I should
be focusing my optimization efforts. All right. It seems intuitively appealing. This is the way
profilers have been written since prof, back, you know, back from like, I don't know, late 60s,
early 70s. So would this in fact speed up Google? So we're going to do this experiment. We're going
to go and find the thing that runs for the longest amount of time and where it spends all of its
time running. And so we're going to run it. And so we go and we do this. And basically,
it makes the loading thing flash faster. Okay. So, well, guess what? That's frequently executed.
And in fact, it's the code that runs for the longest time, right? So this is not really great,
especially if Luke spent like more than a minute optimizing that code. That's a shame. All right.
So basically in some profilers were developed in an era where everything was synchronous and there
was a single core. All right. That's not today. Today things are asynchronous or parallel or
concurrent or a mixed thereof. And profilers don't do a good job in these contexts. So we need to
do better. So what would be really cool is if we could have something like this. So this is what
I call a causal profile. So a causal profile tells you visually, like, if I were to speed up this
component by this much on the x-axis, then the whole program will speed up by this much. All
right. So this is really nice. Like, if I had this graph, I would know I could spend a little
effort on the yellow search component and I'll get a big bang for my buck. Eventually, it's going
to bottom out or top out at like, you know, like 70, 80%. And the red one, I could just keep going,
right? Like, it gets faster and faster the more I work. And the blue one, I should never,
never optimize ever. All right. It would be cool to know this, right? It's essentially like an
oracle coming and telling you, this is the code you should work on, Luke. I don't know where I
got that way of talking. Anyway. All right. So the question is, how would we know this? Like,
how would we get this information? Like, how would we know that this change would cause this effect?
Right? We can't just go and optimize the program by arbitrary amounts and test it. That
kind of defeats the purpose. So we're going to do something different. We're going to run an
experiment. And it requires one ingredient here, which I'll refer to as the force. So we're going
to use magic. And we're going to speed things up magically. And then we're going to measure
how much the effect was of speeding up each component by a certain amount on overall program
execution. Okay? So we just keep doing this, right? We get more and more points, right? And then I do
it for different things. It turns out if I could speed up saving things to the three and a half
inch floppy, it doesn't make a difference, right? And so on. All right? Now, unfortunately, we live
in the real world where there's no magic. Sorry. Well, if there was magic, to be clear, this is not
what we would do, right? I mean, obviously, there are many much better things we could do. I could
think of people I would like to disappear off the face of the earth, for example. But I could also
disappear all the runtime off the face of the earth. Because why not? All right? So obviously,
that's what we would do. So we can't do that. We have to do something else. So what we are going
to do as our trick is we're going to do something that essentially takes advantage of this notion
of relativity. So we're going to do a virtual speedup. And a virtual speedup speeds things up
in scare quotes by slowing everything else down, right? So everything else that's running
at the same time will then be slowed down. And that will allow us to get the impact
of how do we sped this thing up? What would the results have been? So here, for example,
if we speed up the sending of the picture results back by a certain amount, we've slowed down
everything running concurrently with it. And then that gives us a result of a slowdown, which is the
same thing as the result of having sped it up. So we actually can get points on this graph
just by running these experiments. So I just got a point here, and I do it for everything,
and I get more points. And eventually, I get a graph like this. If I speed up indexing,
I'm going to get the exact same effect. Indexing is running at the same time as the compression.
So I get this result, and then bang, I get these results. And again, these are all the results.
Now, I draw your attention to the one weird blue thing. So the blue thing is slower,
and it turns out that sometimes optimizing things makes your program run slower. And the
intuition behind this is you can actually get congestion on a lock, or congestion for a shared
resource like disk or network. And so speeding things up makes things worse. You would like to
know this before you get started. That would be a very, very bad day for Luke that might necessitate
several sequels to recover from. All right, great. All right, so let's dig into Ogil a little bit.
So what do we care about in Ogil? We care about two things. We care about how long it takes
between a request and a response, a.k.a. latency. Traditional profilers don't do this at all.
It's just total runtime. Oh, let me get in my soapbox for one moment.
Traditional profilers are about end-to-end runtime. You know how your servers are all
about end-to-end runtime? Or your browser? Like, if only your browser could quit faster.
So again, like, it was all about, like, here's a program. I run at a console,
and it does something, and it's done, and that's all I cared about. That's not really today.
So there's latency. And then the more traditional thing is throughput. Again,
this is something that profilers do a bad job of measuring because they're all about end-to-end
execution time. So how fast results come back is throughput. So how are we going to do this?
So with our causal profiler that I'm going to explain in a minute, we're going to introduce
what we call progress points. So the notion of progress points is here's a thing I want to happen
faster, or here's a beginning and an end of things that I want to happen faster. So if Luke
wants responses to get sent faster, higher throughput, you just mark this particular start of this
component as a progress point, and every time the code runs, you go and you get another coin.
And then you can do this simultaneously, many requests for many users, and all of these things
are incrementing some counter. So these progress points are measuring throughput, and then you
basically are going to run the experiments and see what the effect is on the rate of those
progress points being executed. So one point measures throughput. Like I said,
if I speed up some component, whatever it might be, what is the effect? So now,
what if I care about latency? So we do the exact same thing. We set a progress point at the beginning,
a progress point at the end, and then the only thing that has to happen under the covers is it
has to have a counter. And the counter here measures how many things are in the system
at any one time. And it turns out that there is this awesome law that holds in a wide variety
of circumstances called Little's law. And so Little's law says that the latency is essentially
the number, the average number of transactions in a system divided by the throughput. We already
know how to measure throughput, so we just take advantage of Little's law and we can translate
this into latency. All right, great. So we have built a causal profiler for Linux. It already
ships with Debian and Ubuntu, so if you're using one of those systems, you can install it quite
easily. So it's just cause-profiler. It's quite easy to run. So you say cause run dash dash dash
and whatever your program is in its arguments and it fires it off and it starts doing performance
experiments. All right, I should add it's not entirely true. You do need to place progress
points. If you don't place any progress points, it will act like an ordinary profiler measuring
end-to-end execution time. But if you do put in progress points, then it will actually do its
magic. All right, and this is just some macro, like progress begin, progress end. All right,
so let's apply this to Augell. All right, I didn't actually build Augell. Neither did Luke,
but we're going to build it out of pieces like any good programmer would do. So it turns out
there's this suite of parallel applications that's kind of ready-made for this task. So there's a
deduplicator that does compression. There's an image comparator. And then there's a database,
SQLite. That's not in Parsec, but we'll use SQLite too. All right, great. So I'm going to show you
some fun things we did. This is a well-studied set of applications. People have already tried to
optimize these and we have covered a bunch of surprising optimization opportunities. So here's
Ferret. This is actually an older version of what our causal profile looks like. Now it runs in a
web browser. And you can see that there's these lines of code and it says, boy, if you speed up
these lines of code, then you're going to get performance increases. Conveniently, these lines
of code happen to be located in separate chunks of Ferret. So the part that does ranking, the part
that does indexing, the part that does segmentation. And why is this convenient? I'm not going to
have to change any code to make this faster. The reason is that what Ferret does is it has this
pipeline model and it assigns an equal number of threads to every stage in the pipeline. But
it turns out this one really doesn't need that many threads. So we take away the threads and just
by reassigning threads, we got a 20% speedup. So this is pretty cool because Caus actually
predicted it perfectly. So we increased ranking, for example, from 16 to 22 threads. That's a
27% increase in throughput. And on the graph, that says that that would translate to a 21%
overall improvement. And that's what we got. So Caus actually works. Good. So we were pretty happy
with this. We then are going to move on to Ddupe. So Ddupe is pretty hilarious. So here's Ddupe in
action. I have two pictures, Grumpy Cat 1 and Grumpy Cat Meme. And so now what do I do to
deduplicate these things? You can see that there's chunks that are the same. So you carve out the
chunks that are the same and you separate them out into individual pieces. And then an image is now
represented by the bits and pieces that make up the image. So here Grumpy Cat 1 is this piece
and Fun Is Awful is this piece. And you saved a lot of memory. So that's what Ddupe does.
So it does this compression via deduplication and it uses a hash function. So it throws everything
through a hash table. Great. So it's a pretty standard hash table. You just have some hash
table. It's an array. It's a bunch of bins. You get a bin number and then you go and you start
adding stuff to that bin into the bucket. So this all seems straightforward. You hope that it would
do something like this. The hash table is accessed concurrently by a bunch of threads, but they're
not idiots. There's not one big lock. It's just all locks, which is naive, but it's fine. But
surprisingly, cause says that the loop that accesses this list is important. Now, if you know
anything about hash tables, you know that things generally end up balanced, right? And it's weird
that you have this sort of situation. So we thought, all right, well, let's just make more,
more hash buckets, right? But we made a thousand of them. We really should have made a million,
cause, you know, a million. But anyway. So you would think this would lead to fewer
collisions, but it had no effect, right? So what else could be causing the collisions? Any guesses?
The hash function, exactly. Like this is one of those when all other possibilities have been
exhausted, right? You pick the weirdest one. That's not an exact quote. But anyway,
well, you're like, how can the hash function be broken? Like we've been using hash functions
since before Canuth wrote about them. Well, turns out people like to roll their own, cause it's
fun. And so we did a histogram of the number of items per bucket. So again, I told you there's
a thousand buckets. This is the histogram. Yeah. Hilariously, what they did is they used the pixels
that were taken from the image and they sum number of pixels and then added them. But that's
actually the central limit theorem in action, right? They're random, right? They're independent,
right? And you've summed them together. And so they actually formed the normal distribution.
That's not the distribution you want for a hash table. You would like a uniform distribution.
So literally, we changed one character. We changed the plus to XOR. So this. And we got this.
So, okay. I'll take the applause, but I mean, it was only a 9% speedup. But
all right. Nonetheless, it was one character. So I think it's the biggest bang for buck
ever recorded in optimization effort. So what did it predict? It turned out we can
also measure the accuracy of the prediction. So we knew that the blocks per bucket went from 76-ish
to 2. That's a 96% traversal speedup. And again, going back to the causal graph,
it predicted a 9% speedup, which is what we got, right? So it's working. All right. So finally,
I'm going to talk about SQLite. So I have no time left. But SQLite is pretty awesome. It's widely
used, as you all know. But it has this weird thing where it has a kind of strange virtual table
that they set up at compile time. And so whenever you actually indirect through a config to execute
a function like pthreadmutexunlock. So you would think, all right, why are you telling me about this?
Well, everything looks like this. This is an indirect call. Could be a direct call. That would
be faster. But an indirect call is not that slow. But it's almost the same cost as pthreadmutexunlock,
which means that you just doubled the length of all of your critical sections.
So that's not great. So in fact, when you go, so cause will highlight all of these lines and say,
you should definitely optimize these. So we undid all of the actual indirect stuff and just made
it so that at compile time, you change SQLite unlock to something so it doesn't do the indirect.
And it sped things up by 25%. If you look at a traditional profiler, by the way,
those things are like, this takes 0.0001% of time. You would never consider actually
trying to optimize that code. So we did it for a bunch of programs. We got some crazy speed ups.
My favorite is we got a 68% speed up by replacing a custom barrier with a standard barrier.
Again, people should stop doing things at home. So anyway, so I'm going to conclude.
So you can take a picture of this to jump to work from our lab, which is plasmaumass.org.
I talked today about sound performance analysis and effective performance profiling. Everybody
should go use the cause. All right. Thanks for your attention.

Being a speaker at this seminar series is, I mean, it means a lot to me personally.
When I was an undergrad back in 2006, 2007, I've been meaning to learn about HCI, but
I didn't really have that many resources, so I had to rely on online resources.
This seminar series recordings have been posted online, and I think I've watched
pretty much everything to really learn about HCI.
And then I came as a master's student here in 2008 and
took 547 pretty much for the entire two years I was here.
And now I feel great that I get a chance to speak as a speaker, so this is great.
Today I want to talk about interaction centric AI.
This is a reprise of the New Europe's keynote talk that I gave two weeks ago
in front of thousands of AI researchers.
I tried to reframe it a little bit so that it's more customized for
an HCI audience rather than an AI audience.
But the idea is that I want to think about using human AI interaction
at the center of developing AI technologies.
And of course, I don't have to preach to the choir that human AI interaction
actually matters, but diving deeper based on my experience of building
these interactive systems in different contexts, like education, discussion,
decision making, I want to dive deeper and report some of the detailed
interactions that we've been observing and learning from.
And think about what it means to design human AI interaction in various contexts
and what are some action items moving forward as a community.
So let's first start with some definitions and terms.
I would say the dominant paradigm for developing AI technologies has been model centric.
The idea is to build a model with high accuracy and we want to evaluate it
against unseen examples for its generalizability.
And benchmarks have been great in that they could help us competitively
compare different models' performances, which could be useful in making
scientific advances possible.
And more recently, people have been talking a lot about data centric AI,
where the idea is using this nicely performing model, what is a good,
sort of robust and efficient data pipeline around it in terms of collection
of the data, processing of it, cleaning of it, so that the model actually performs
really well in different contexts.
And here the focus is acquiring quality data and setting the pipeline in a way
that really helps the machine perform its best.
And these two paradigms are great, but then what is slightly missing is the user
who's using these AI technologies and those who are affected by what the AI
systems give you.
So interaction centric AI is sort of my term in some contrast to model
centric and data centric AI, where the goal would be basically what HCI
researchers do in this context, like improving the user experience by
building usable and useful applications.
And the unit that we often grapple with is a human AI interaction.
And you might be wondering, is this some sort of like marketing term?
Like how is it different from human centric AI we've been talking all about?
It's largely similar, so I'm not trying to say I invented this new term or
anything, but I want to focus our attention to the interaction that is
happening between humans and AI and the complex relationships and the dynamics
that are happening between the two, rather than focusing on just the humans
or machine alone.
So I can say that that's the focus of where my discussion will be today.
So let's say you're this AI researcher and your team has built this amazing model.
So this is actually something that I copied and pasted from one of the
diffusion models papers.
I don't know what they actually mean, some of them I understand.
But basically, this is what you have as an AI researcher.
But what would a person using this kind of AI want to do with it?
Here's an example.
So this is a Twitch streamer in South Korea who was trying to use this
diffusion-based text to image generation model to create this image of an
animated character eating ramen with chopsticks with noodles around the
character.
So this is the roughly sketched out goal that the user company has.
And he ended up spending two hours fiddling with text-based prompts to get at
the final image that he wants.
And this is somewhat similar to what Manish shared a couple of weeks ago at
the HAI conference in terms of what he had to do with the prompt-based interface.
And here, the entire two-hour journey was live streamed.
So I want to kind of share a quick summary of what happened in that stream.
And of course, we need something in the middle to bridge between the technology
and the human user.
And that's what we have, the prompt-based interface and interaction that's
happening between the two.
So the streamer started by something simple and obvious.
The prompt says, eating ramen, and this is what he got.
It's okay, it's kind of there, but the bowl is perhaps too large.
The chopsticks are all to be placed.
And he heard from somewhere that adding a full sentence might make things better.
So he goes, she is eating ramen.
She is eating ramen, for sure, but you can see that something's not quite right.
So he keeps going on by adding more descriptions.
And the prompt is definitely getting longer.
And it seems that the AI is not quite getting how chopsticks should be used
and how many should be used.
So he keeps adding these descriptions to really explain what it means to use chopsticks.
And to be fair, there's hair, there's chopsticks, there's noodles.
So it's in computer graphics.
Dealing with human hair, I heard, is a really tough challenge.
And maybe for AI, it's also kind of struggling to deal with all these similar looking objects.
And it doesn't really seem to get how to differentiate between chopsticks and noodles.
And another interesting aspect was that since it was a live stream,
the viewers were actively participating in recommending new prompts to try out,
sharing their interpretations.
And this is somewhat of a collaborative mental model construction process,
if you will, as a group of people.
They are really trying to figure out what's going on.
And now the prompt is five lines long.
And the service that this streamer was using was supporting variations,
where you could pick an image and say create some variations.
And he was referring to this interaction as variation gacha.
So gacha is a Japanese word for like a random box or blind box.
And this kind of tells us that how unpredictable this sort of interface is.
Once you hit the generate button, the user doesn't really have a good sense of knowing what to expect.
And this is the actual stream, as you can see, like his praying,
and which also tells us about the usability of this sort of system.
He doesn't have a good way of knowing what to expect, so that he actually has to pray.
After two hours of hard work, this is the final image that he landed.
And it looks pretty good, and he claims victory.
But then look at what he had to do at the top, right?
At the top, there are seven lines of prompt that he had to write.
And arguably, this is natural language.
But I would say this is really pseudo-natural language.
And so this is basically the experience that he had to go through.
So is this a good interface?
And I sort of got inspired by Manish's discussion of discussing the usability
of these text prompt-based interfaces.
There are some good elements, right?
It's quite intuitive.
You can use natural language, or you believe natural language could be used.
And the output is presented in a visual manner,
which helps you kind of understand whether you got the image that you like or not,
so that you can sort of debug.
And there are some interactions that are supported,
like variations and seeds and like words that should not be used and things like that.
But there are many ways in which this interface actually fails
to support what the actual user wants.
He had to rely on trial and error.
And just the fact that he had to spend two hours to get that image
seems to suggest that something is really wrong.
And of course, it was not really predictable
and lack of specific feedback on the effect of what specific words
in the prompt had influenced on the final outcome.
These links were often missing, which made it really difficult.
So is this really just a problem for these text-based prompt-based systems?
I would say every AI application faces these interaction challenges.
On the user side, when they first encounter these systems,
they often have to struggle to kind of figure out how to make it work.
Often people resort to misusing it, abusing it, and learning takes a long time.
And part of it is really a design challenge.
And we've seen other examples like this,
where people don't have a good sense of what's happening
in this algorithmically-generated systems and AI-powered systems.
Like in the famous study of Facebook newsfeed users,
more than half of the participants were not aware of the newsfeed curation algorithm's existence
at all, which is far from being true.
And on the right, what you see is in the pathologists' diagnosis scenario,
often they would rely on some notion of similarity.
So there are these algorithms that are designed to help people find similar images,
but then the realization that the researchers had was that people had different notions of similarity.
So a singular notion of similarity that was used in building an algorithm would not really suffice.
So what they ended up doing was to support three different types of similarity interaction,
and the user was able to kind of transition between these different terms in a fluid manner,
which really gives more control and agency on the user side.
And these, you know, put in a more simple sort of diagram manner,
whether you are a creator or Facebook user pathologist,
you seem to have some kind of a mental model of how the system works,
a very sort of a classical sort of gap between what the user wants and the system wants.
And obviously, the system is not behaving in a way that you really want.
And this gap arguably seems to be larger with these more complex black box and deep learning based systems.
And AI community has been tackling this problem as well.
And, you know, some of the folks have been framing this as an alignment problem,
which is about aligning the model's behavior with human intent.
And for example, the famous chat GPT and the instruct GPT paradigm has been sort of open
AI's response to the alignment problem, where their idea is, in addition to the, you know,
basic large language model that they have, they would add this fine tuning layer with human feedback,
which often involves asking people whether, you know, they were happy with the results they got.
And the system kind of uses that feedback to train a reinforcement learning agent to
do the fine tuning so that the resulting text aligns better with what the user wants.
And they were seeing some success from it. And a quote from the paper is that
making language models bigger does not inherently make them better at following a user's intent.
And aligning language models with user intent on a wide range of tasks by fine tuning with human feedback.
And of course, there's been a lot of discussion about whether this is really the most promising way
to, you know, involve humans or alignment problem. But I think this is some progress towards that direction.
But all of these examples, I would say, basically lead us to revisit these classical notions of
Gulf of Execution and Gulf of Evaluation proposed by Don Norman back in the 1980s, right?
As a user, they want to know what's going on with the system, and they want to have more control and agency.
And on the evaluation side, when AI gives you some kind of result, they want to be able to
understand it, interpret it, and want to get some explanation of it. And as an HCI researcher who's
building these interactive systems, I feel like in often cases, I try to bridge these gaps.
I come up with new ways of designing these social interactions and human AI interactions
in a way that tries to bridge these gaps. And these are just some of the systems that I've been
developing in different application domains. And I think many of them have somewhat succeeded
in bridging these gaps. But other times, to be honest, we haven't done a good job of doing that.
So what I want to do for the remaining time for this talk is to share some of these lessons,
and some of them from positive experiences, but other times, bitter experiences by something
that we haven't really done a good job of. And the main message that I want to send across
is that beyond these point solutions for this system that works in this particular context,
we've seen some success, I think as a field, we really need to start thinking about,
can we do something more systematic and sustainable? Or empower designers and developers in thinking
about can we develop these AI applications that are more usable and useful for more groups of people
rather than having to reinvent the wheel each time someone has to develop these applications.
And I think we're seeing too many of these cases where people are like,
there's this cool model, let's build something around it, and it just gets released in a few
days and realizes that people want it in a completely different way, people abuse it,
a few days later, it goes down. We're seeing too many of these failure cases.
So from the HCI point of view, I think HCI research can really advance this
interaction-centric AI by contributing these generalizable building blocks for designing
these systems and interface affordances. And AI research can also advance by embracing the idea
of interaction-centric AI by rethinking models, architecture design, benchmarks, metrics, and
research process. The part of it has to involve sort of broadening the perspective beyond just
thinking about the model and the output that it generates to think about the users behind those
and their mental models. And often there's not just a single user, but a group of user,
community of user, a society of users. And there's also the temporal dimension,
like before the user comes in and tries to use a system, we should be asking the questions about
like, what's the task and who are these users and why and how. And during the interaction,
we need to be thinking about presentation visualization. And the other way around as
well, like interpretable results are being presented to the user. Do they have a way to
provide feedback to the system? And also, it's never going to be just a single use, right? People
would want to come back and use a system for a sustained amount of time. In those cases, people's
mental model would evolve. And what does it mean for the system? And so I think this is sort of
the ecosystem that I have in mind. And with these, I want to dive into these specific examples
where we designed human-AI interactions. And I identify four major challenges
in terms of human-AI interaction. The first one is about bridging the accuracy gap.
So I'm on my sabbatical now. I'm working with this startup called Ringle, where they are
basically Uber for language learning. They are matching tutors and tuties, and they have this
video-based language tutoring session. So what we try to do here is to build this diagnostic
service based on analyzing the chat-based tutoring session to give people personalized
feedback and suggestions for improvement. But instead of going into the details of the service
itself, I want to touch upon the case that we ran into when we were trying to run this automated
speech recognition AI, which is crucial in sort of turning the video-based chat into text format,
which is really required for us to run all these diagnostic algorithms on top of.
And the standard metric of success in ASR would be word error rate, how correctly it can
recover the original text. And on the tutor side, when we ran ASR on like hundreds and
thousands of sessions, the average word error rate was around 8%. Can you take a guess as to what
the number would have been for students? Obviously there's this white margin that's quite high,
so you can imagine, 30. Yeah, we're seeing 23. So there's quite a bit of a gap. And this is an
example of an accuracy gap where different groups of users are getting disproportionate results from
the same AI. And the gap actually widens if we look at like the best tutor and the worst student
when it comes to the performance of these models. But in terms of thinking about the interaction
that these people are trying to have with this AI, I would argue that the students are the ones
who really need this AI to work. Based on the accuracy of this AI, they want to kind of look
at where they succeeded and failed and they want to learn and reflect. And with this low accuracy,
they would really be struggling to come up with good action items and they might be frustrated,
they might lose trust on the system. But interestingly, a lot of focus when it comes to
model development is that we seem to be focusing on the 6%, like making the 6% better instead of
narrowing the gap between 6 and 36%. And we have to really be asking like, what is the most important
question in this context? And are we really focusing on the most important question here?
And we see these other examples too, where Tyra and others have studied the machine translation
that is being used in emergency rooms when it comes to discharge statements that are presented
to patients and patients' families. And we see a huge disparity between different languages.
And in the natural language processing community, this support for low resource languages has been
a topic for research and there has been great efforts. And on the right is a famous example
of gender shades, where the gender classification algorithm shows, again, an accuracy disparity
between darker skin female versus lighter scale male. And of course, these diversity and inclusion
efforts and low resource language support research in the AI community and in the community
have been tackling these issues of accuracy gap, of course. But then I would argue that they could
advance further by embracing more interaction-centric approach in trying to really see how in the real
world people are interacting with these results and what kind of actual struggles that they have
because of poor or good AI accuracy and what, as a community, how can we define the problem that's
most important. And conceptually speaking, I feel like a good analogy might be the ceiling and floor
analogy. The ceiling would be this primary user group who gets the best part of AI. And floor
would be secondary user group who is disproportionately getting more negative impact of
the same AI. And there's this accuracy gap. And often I feel like taking a model-centric approach
incentivizes people and researchers to work on raising the ceiling. There could be a couple
reasons for this. First of all, that's the sota number you get, which might be what you need to
publish a paper out of it. Or the benchmarks that you're working with do not really have much data
on the floor side. It's maybe more focused on the ceiling side. And that's why the ceiling is there
in the first place. So it might be just incentivizing people to continue to push the boundaries of
ceiling. And as a result, what we see is a lot of a widened accuracy gap. And if we take a more
interaction-centric approach, I would argue that if we identify that narrowing this gap is a more
important problem, we can narrow this accuracy gap. And it's not just a matter of accuracy,
if you think about it. It's about experience, benefit, and value that people get out of
interacting with this AI. So there was a first challenge about the accuracy gap and how thinking
about how people interact with this AI can help us identify what problems are worth tackling.
And second of all, I want to talk about when people actually use AI. And one of the
anti-patterns of human-AI interaction is that people just stop using AI altogether or abandon it,
which is something you might want to avoid as a system designer. And that's why it's important
to think about how do we incentivize people to work with AI? And in most cases, people abandon
using AI because it's not really giving them concrete value that they expect. And we explore
this in the context of online education in this system called XS. So the problem that we wanted
to focus here is that in online, let's say you want to learn some new concept like probability,
there are lots of problems and answers you can find. But finding good explanations is
surprisingly difficult. And generating high-quality explanations is costly and resource-intensive
console. So we wanted to tackle this problem by building this online education platform,
where people are presented with a problem and they solve this problem, they submit an answer,
and they see an example that's presented by the system and they get a chance to rate how helpful
the explanation that they saw was. And then they are getting a chance to sort of self-explain
their own answer. So this is a pedagogically meaningful activity to be able to sort of explain
your thought process, externalize it, and lots of research supports doing self-explanation.
Okay, so fairly simple sort of front end in terms of the learner's experience. So what's
happening behind the scene is that the system is collecting these explanations and ratings
from learners, right? Since it's a live system, new learners keep coming in and provide new ratings
and explanations. And we formulate this in a multi-armed bandit manner, which means that
as a new explanation comes into the system, as a byproduct of humans' learning activity,
a new arm gets added to the system. And what the system is doing is to determine this dynamic
policy for what the most effective explanation would be for the next learner coming into the system.
So if you're familiar with the reinforcement learning of concepts, we are navigating
exploitation and exploration trade-off. Exploitation in the sense that the system wants to present the
best explanations to the next learner coming into the system, but the system doesn't really know what
the best explanations are until it collects some amount of ratings from people. So it has to do some
exploration where it should collect this data. And to solve that, we use a technique called
Thomson sampling. So what happens is the system keeps track of these policies and when a new
explanation comes in and ratings come in, these things get updated and the policy
of probabilistic policy gets updated so that it uses this distribution to determine
what explanation to show to the next learner. So when we ran a study, these access-generated
explanations were helpful in terms of helping people learn better. So when we compared against
presenting no explanation at all and measured differences between pre-test and post-test
results, we were seeing that people were gaining 3% increase in their scores. So just getting a
chance to rethink the problem, I think still gave them some increase in their scores. And when they
were seeing the instructor-generated explanation, which is, I guess, somewhat of an ideal case or
the standard case, we're seeing 9% increase and with access, we're seeing 12% increase. So between
these two conditions, it was not statistically significantly different, but there were certainly
cases where access was picking explanations from learners that were even more powerful than the
instructor-generated ones. So in this system, if we were to take a more model-centric approach,
I think we might have built an AI that automatically generates high quality explanations.
But instead, in taking an interaction-centric approach, I think the system we created is basically
this co-learning system where the user, the learner, and AI are learning at the same time in a single
system. So it's sort of an education-focused system of the game-with-the-purpose kind of setting,
where organic benefits are provided to people who are interacting with the system,
and the system is learning something useful out of it. And this is basically the mechanism that
we have in that both sides are learning and explanation and feedback are establishing this
loop. And this is the topic of my PhD thesis, and I explore this in the concept of learner sourcing,
where learners as a crowd coming into the system are basically doing this by getting their individual
benefits while they're providing something useful for the system to learn and do its thing better.
So since then, I've been expanding this idea to a broad array of applications. So for example,
can we use this kind of co-learning ideas to summarize how-to videos in terms of steps and
sub-steps, or building a concept map out of an instructional video that shows relationships
between different concepts, or helping learners come up with the solution plans
in algorithmic problem-solving settings. And other researchers have taken on this idea
in different application contexts as well. So I think we can try to really generalize this kind
of idea of co-learning system design in different contexts. Moving on to the third challenge,
is about beyond a single user. And often we think about a single user, a single AI interacting
with each other. In real life, it would be much more complex and there would be diverse configurations.
So how can we consider these social dynamics? And there could be various types of social dynamics,
but one specific instance that we did in was group-based, chat-based discussion in a group.
So we built this system called Solution Chat, where the idea is what if this AI agent could
recommend real-time moderation messages to a group. So let's say a group is discussing,
you know, what to do for the company retreat next week, and they're having a discussion.
The system, in real time, based on the understanding of the discussion context,
and also knowing what kind of messages would be useful for the group,
based on our sort of literature survey of discussion and discussion-based education,
it presents these recommendation messages, like any more ideas, or can this person share their
opinions, you have been quiet for a while, or should we try to move on to the next stage,
or thank you for your opinion. So these kinds of moderation messages are presented by the system
in real time, just like what you get in smart replies in Gmail, for example. And as a moderator,
you can just choose to accept any of the messages that you like, and discard the ones that you don't
like. So a quick summary of the results of what we saw was that in our lab study with 55 users
in 12 different groups, when we compared how many moderation messages were used in different groups,
when we compared the baseline condition without these real-time recommendations versus
solution chat or system, we're seeing a significant increase in the number of moderation messages
that were present in the chat stream in the solution chat condition. But interestingly,
you can see that the users manually typed moderation messages were actually decreasing
in solution chat, but many of them were replaced by the accepting AI-generated recommendations.
And furthermore, we had this great opportunity to actually release this system to over 2,000
real-world users in a corporate education setting. So during COVID, a lot of these corporate
education programs moved online, and this company that we worked with wanted to use these kinds of
system to moderate hundreds of chat rooms that were doing discussion-based activity.
And not surprisingly, just like the very first live stream prompt example that I mentioned,
here again, people were collaboratively trying to understand the capabilities and limitations of
AI when they were first presented with the system. So they were using the chat to test
different messages, often things that they believe would be not working, and they would be
sharing the results of, oh, this is working, this is not working, I think this does this well,
but not that well. And it seems as a group does this kind of testing in the very first phase of
their usage of the system, people have this shared expectation of the system, and that seems to sort
of determine their further interactions with the system. And it was also notable how different
groups had different expectations based on their limited experimentation that they did in the beginning.
And there were some interesting social dynamics that we observed as well,
like in how people use these AI recommendations to socially interact with each other.
Some people were using AI as proxy. So one of the quotes that we had was,
I didn't want to directly ask the person to stop talking. So the person relied on the AI
recommended message to kind of send it. They still chose to send it, but it was their way of kind of
softening the potential sort of dispute with the person. Other people were using AI as a reference.
So what we were seeing is that it was a fairly simple technical pipeline that we had. So it was
just a canned response. So people were sometimes not really fond of the tone of the message,
style of the message that we showed. So the person said, I found no fun in the recommended
messages because all the messages look the same. So in those cases, what people did was they still
adopted the idea from the recommendation, but then rewrote it so that it feels more personal,
and it feels more like it's coming from them, not AI. In other cases, AI seems to be adding
a social burden. So in this excerpt, so one of the people said, I'm doubtful about the
credibility of AI. And then the moderator picks this AI recommendation. Thanks for your opinion.
Another person says, I also think negatively. Thanks for your opinion. Thanks for sharing a good
opinion. Shall we go to the next topic? And then the moderator realizes he might have clicked,
accept way too many times, and it was a little unnatural. So he stopped to kind of
clarify and apologize for my unnatural words as I'm using AI recommendations.
So while we were seeing how people were saving their time and cognitive effort in moderation
could have decreased, it might have actually introduced other types of burden at the same time.
Again, so if we were to build this kind of system in a more model-centric manner,
I think a good alternative might have been automated discussion moderation, where AI
would actually do all the moderation by itself. But instead, we chose to take a more AI-assisted
moderation for obvious reasons. Users want to have more agency and control, and they wanted to
keep their style of communication. So instead of handing over the entire control to AI,
we still sort of gave that control to the human moderator who could kind of use it as an additional
resource. Okay, so there was a third challenge. And moving on to the final challenge of supporting
sustainable engagement. Here, the concern is that we want to think beyond this single
session usage. And over time, how people react to these systems might change, their mental model
might change, and how AI actually works might change. So we need to really think about this
temporal dimension more carefully. And for this thread, we investigated in the context of
novices making changes to websites that they're seeing. So for example, you might have a case
where you visited this website that colors hurt your eyes, or you couldn't really find this button
or tap it because it's too small, maybe you want to make it larger. But then people without
expertise in HTML and CSS have difficulty doing this. So we thought by leveraging the power of
large language models and so on, maybe we can support more natural language queries. So if a
person says tone down the text, the system can kind of display these style recommendations that they
can explore and select from that are about toning down the text. So the way the system works is
if the user clicks and says make this larger, the system presents a set of design attributes that
are about making something larger. And the user can say emphasize this part. It's somewhat ambiguous.
There isn't a clear single design attribute that is about emphasis. So it presents these
few recommendations that are about emphasizing something. So we built this by establishing
this NLP pipeline and computer vision pipeline. On the NLP side, what it does is analyzing the
user's query and mapping them with the style attributes that seem to be connected to what
the user's intent is about. In terms of computer vision, we collected millions of web design
elements to determine a good set of recommendations to show to the learner. So by combining those,
we built this system. Again, so instead of going deep into the technical details of the system,
I want to focus on the interaction dynamics. So we ran this user study with 40 people where we
presented them with either stylet, which is the name of our system, versus the baseline, the
Chrome developer tool, which is sort of the standard tool for making these style changes.
So we compared these two groups. And we gave people two tasks. One is a well-defined task
where we ask people to turn this before image into an after image. And then secondly, we had this
open-ended task where we gave this blank slate and people were able to make any kind of change
that they want. First, I want to share success stories. People were more successful in completing
these design tasks when using stylet. 80% of the stylet users completed the task as opposed to only
35% in Chrome developer tools. And these were complete novices in web design, no experience at all.
And people completed the task in 35% less times. It was efficient to use stylet.
Another interesting observation was that people were making same similar number of changes in
both conditions. But in stylet condition, people were making more diverse changes, which means that
it probably had to do with how stylet shows these multiple options for people to explore. And there
was a conscious decision to not just show the most obvious one, but show somewhat related ones as
well so that people could explore and tinker around different options. But then an unexpected
finding was when we looked at people's self-confidence. Because we thought this kind of system would be
useful for people's learning of the skills and confidence that they have about the skills,
we asked people's self-confidence after each task. What we noted was that after the first task,
in both conditions, people's self-confidence increased. But then in the second task,
after the second task, users' self-confidence decreased for stylet while in the developer
tool, it kept increasing. Why would that be the case? And we were seeing many cases where
stylet users were frustrated that the only control that they had was natural language.
Now they have some grasp of how it works. They wanted to do more fine-grained control more
directly. And they wanted more specific things. But because they only had natural language,
they sometimes just got frustrated. Whereas in the Chrome Developer Tools condition,
people were just happy that they accomplished something with their own hands.
And I think that is presented as a continued increase in self-confidence.
And we know from HCI and CS147 that people's expertise and learnability really matters. And as
they have more knowledge of the domain and the skill, they might need to get more advanced
controls or being able to more directly manipulate what they are working on. So I think this had
some interesting lessons in terms of thinking about the temporal dimension in that learners are
changing. And other researchers have been reporting that considering these temporal dynamics is
important. On the left, what you see is design researchers who have shown that there are these
different stages of relationship that people have in technologies like self-tracking devices.
First, they would start with initiation and experimentation, followed by intensifying and
integration, and then stagnation and termination. And one of the design lessons might be that
these might be more meta-level factors that really should be considered in design systems,
in that even the same kind of intervention might need to be presented in different
manners depending on what stage you are or what your expectation is with the system.
On the right, what you're seeing is the guidelines for human-AI interaction, really influential
work from Emershi et al. And they organize these guidelines for human-AI interaction
in different categories but are organized in the temporal sort of aspect, like initially
encounter with AI during interaction, when things go wrong, and over time. So taking into account
this temporal dimension can really be powerful in supporting more sustainable engagement.
And the related question might be, as people are relying more on these AI tools, like grammar
fixes or even generating text, it's important to think about how people's mental model would
change over time, and AI also changes over time too. And do we hit a point where people become
maybe overly reliant in that maybe their grammar skills or writing skills do not improve anymore,
but then without the tool, they actually might perform worse? And what is that dynamic? Or maybe
over reliance is perfectly fine because if we believe these tools will be around the user all
the time, maybe it's just the final outcome that matters. And I think we need more studies and
analysis of the long-term engagement of users using these kind of technologies.
And to kind of sum up, if we were to take a more model-centric approach here, I think we might
have built a system that makes automatic design fixes to optimize a web page directly, and the
system makes a fix and user can just use it. But instead, we took a more sort of interaction-centric
route where we asked people to do sort of, you know, style change by themselves as the system
was presenting these recommendations, and they still had to do the fix by themselves. But what
we expected here was that people can then customize by seeing these attributes, they can learn,
they can discover new ways of doing things, they can think around, which can empower them,
especially in the more learning context, although the temporal dimension has to be more carefully
taken into account. So these were the four challenges that I wanted to
share today. And to kind of wrap up, I just wanted to pose two questions moving forward
from the interaction-centric perspective as HCI researchers. So first is, how might we design
these building blocks and interface affordances for new and upcoming AI models? Okay, so I think
part of it is that instead of building these point solutions, I think we need to think about,
are there any sort of generalizable frameworks, libraries, widgets, or interface affordances
that we could come up with as a community that is really good at these kinds of things?
And the second question is, does AI really require us to have these new things? I mean,
can we just use existing design elements and frameworks to build AI applications?
And I tend to think that we might need something new for these new and upcoming AI models,
especially because they have these very different characteristics than the conventional systems
that we have been building. They're more probabilistic, harder to predict, more black box in nature,
yet seemingly more impactful and powerful in terms of what they do, hallucinating. All these
properties packed together, I think we might really need to think about, what are the types
of interaction affordances that are really built for supporting the usability of these
AI-powered applications? So in this, I think as a community, we are making all these great
advances, like making different types of contributions. And I tend to focus on more
interactive systems and techniques, whereas other people focus on introducing new design
processes and understandings. And I think all this work is needed. And some of the interesting
examples of adding an interaction layer to these new types of models is in this example,
Tailbrush, where the user can draw the level of fortune that they won in the character to have
when they use generative models to generate a story. Or this AI chains work, which presents these
primitives and workflows for putting together this workflow that can accomplish more complex tasks
with these LLM prompts that a single prompt cannot really perform. And in my research group,
with my PhD student, Tesu Kim, we have been investigating this idea of what would be more
generalizable design framework. And thinking about input, model, and output, we have been
thinking about the concepts of cells, generators, and lenses, and tried to introduce this standardized
libraries and widgets that people can easily adopt in their AI applications. So for example,
using this kind of framework, people can build a copywriting app, email app, or story writing
app using pretty much the same kind of framework, which can save people's time while supporting
the types of interactions like iterations and comparison and experimenting different outputs.
And the second question, and the final question that I want to ask today is, how might we as an
HCI community collaborate better with the AI community on these various things? And it was also
the discussion that I was having a lot with today's meetings, and also with various AI researchers,
especially in Europe. And in terms of community collaboration, of course, one of the important
things is metrics. And there was also a great discussion at the HCI conference a couple weeks
ago, hosted here at Stanford. And in the AI community, it cares a lot about model performance
and generalization errors, where in HCI, we tend to focus on the human experience. So how do we
really bridge the gap between the metrics? And what it means to do AI research with more human
side metrics incorporated? What's the incentive for people to do that? And how do we encourage
poor AI people to use these metrics, too? In terms of human input design, a lot of the comments
that I was getting in terms of interaction-centric AI from AI researchers is that these ideas are
great, but then I don't really know how to actually take action about it. And part of it is, in their
model-building kind of work, how can I incorporate human feedback? And how do I use it in a meaningful
way to really change the way the model actually works, rather than just getting more high-level
design guidance? So one great direction for this might be, think about more making human feedback,
more computationally feasible, so that this compatibility is actually satisfied.
And lastly, we need to think about the change in design process as well. And in a lot of,
this is Stanford D-School's user-centered design cycle. And I think in a lot of the AI research,
what we're seeing is this prototype test kind of culture. You try something new, test it,
iteratively improve it. But then one of the frustrations is that interaction often comes
too late, right? There's this new cool model, and can you build an UI on top of it, is sort of the
kind of discourse we get a lot. And I think interaction should not just be like an icing
on the cake, but really something that can guide the entire design process or help people determine,
is this the right problem to tackle in the first place? Or what kind of interaction should we try
to support with AI? And based on that, think of what AI should do and should not do and how much
AI should be used in a particular context. So that's all I wanted to share. And here's a summary
of what I mentioned today, and I'd be happy to take any questions. Thank you.
All right, so I'll check my recommendations of facilitating messages. If there are any more ideas.
No? What do you think?
It really sounded like an AI.
I'll just click them all. I want to pull the mic on. I want to pull the thread a little bit on
this notion of how to connect human feedback with the objective functions that you touched on near
the end, because that's been rattling around in my head in much of the talk that you're giving,
that if I think about what should AI researchers be doing differently,
then you're asking, well, what's the proper model of the person in their system?
And traditionally, the problem has been that human interaction is really expensive,
just to collect annotated data. Or once you have it to be able to tune the model,
you don't get that much of it. And so they often fall back on self-supervision, or as you've been
talking about in the value alignment, they train an RL model to mimic a human and then let that go
loose. And it seems like until, I think they're kind of, I want you to take a position on one of
the two positions. One either is to say, look, we need to find strategies like that where we can
create proxy humans, and that's how we hook into the objective functions, the loss functions, etc.
The other alternative would be to say, no, we're going to find some other way to actually
make human feedback at a scale and in a form that they can directly use in the models. I'm
just curious, like, if you want to take a bet, where's your bet on that? Where should we be heading?
Yeah, that's an excellent question. I would say, I mean, you asked me to take a position,
but I would say both will be prevalent. And I like the letter much more. And I think that's more
promising and sustainable. And for example, the reason I'm really interested in this, like,
co-learning feedback loop between the human and the machine is that, you know, even if this super
advanced AI comes along and let's say it presents this, like, super accurate explanations, people's
self-explanation activity is still meaningful, right? Because that's how they could learn.
And so I feel like, you know, we can really try to find these compatible
mechanisms in which the human can get the benefit and get the incentive for doing what
they are really good at and what is helpful for them, not necessarily trying to help the system
or, you know, getting paid to system, paid to support the system per se. And at the same time,
the system can use it for something meaningful. And on the system side, I think in the system,
like, access that I presented, I was really happy when we landed at this technical solution where
people's rating data could be almost directly piped into the feedback for the RL agent to kind
of use as meaningful feedback. So I think that's just one example where this kind of worked out
for this kind of context. And I think we need to really investigate more and think about, are there
any generalizable mechanisms that this kind of approach could work in different contexts?
This assumes that you have a large set of users you can draw on, like there are learners that
are coming through your system. If I'm early on in the pipeline and I just kind of have V0,
I don't have the users yet, are there strategies you would recommend?
Yeah, yeah, excellent. So in that same access system, for instance, what we did was to
insert the instructor-generated explanations as sort of the initial seed. And I was also imagining
maybe using LLMs, for instance, we can plug in AI-generated ones to kind of avoid the cold
start problem. And it would be interesting to see how, you know, in the same system, like AI-generated
ones, instructor-generated ones, and learner-generated ones can kind of compete against each other
until the system ultimately just focuses on what is best for learners.
This is kind of a two-part question, going back to the like third challenge or like project you
talked about, where there was that note about AI as proxy, like people kind of using that as
like an excuse to make points, where maybe they wanted to do something but didn't want it to come
off as them. So the first part of the question is like, in that case, did people want to,
later it says people wanted the message to kind of sound like them, but in the case of the AI
as proxy, did they want that to sound like them? Or were they wanting it to sound more artificial?
And then second part of the question is, do you think there are more situations than just this
where maybe we don't want the AI to feel super personable and maybe want the interaction to
feel slightly more kind of mechanical or unnatural? Yeah, that's an excellent question. And I would
say these were somewhat different use cases, and both I think are valuable and smite. And
that again, I think in a more model-centric approach, we also kind of focus on trying to
create these messages that are more like humans. And that could be effective in certain cases,
but as you said, that might not really be what the users want, because in a proxy kind of setting,
you might not actually want it to sound too personalized, because maybe the more canned
message might actually work better in that context. And vice versa. So I think just being able to
identify all these different needs that people have and expectations that people have and being
able to somewhat fluidly support those, I think was really an interesting kind of observation
that we had. And I think moving forward, one of the lessons was that this more personalizable
message generation could be an interesting technology that could be potentially integrated,
but that's not going to solve everything, because there are these other types of needs that will
not be supported, even with the perfect personalizable style transfer. So yeah.
Explaining stuff, I kept thinking about how what you described and sort of the challenges
that we see with this new deep networks and models and how we interact with them are
similar to how people used to interact with search engines, right? At the beginning, people were
not as good as sort of figuring out how to query the search engine right. And over time, both
we became better at querying the search engines, and then the search engines became better at
sort of understanding how to interpret user queries. Do you see any similarities there? Is
there something that's very unique to the challenges we face with this new models? Or is it just that
we haven't had enough time to sort of adopt to each other in a way? Yeah.
Excellent. Yeah. And I think it's a recurring theme as these new technologies come in. Initially,
people would kind of struggle and they would need to learn how it actually works through trial and
error and lots of like failed attempts. And that's what we're seeing with these like
chat GPT, for instance, a lot of people are trying things out, reporting success and failure cases.
So I do think there are certain similarities. What's more unique about what we're seeing right
now is that due to the nature of like how black box, complex, unpredictable these models are,
I think it just confuses people much more. And there's a question of, you know, is this really
like a human learning problem to begin with, right? So if people take, do it more, and you know,
if they had more time, will people be actually able to really get to a point where they could
really easily create something that they like? Probably not. Right? So that's why I think we
need both on the model side to kind of think about what are more interactable and learnable ways of,
you know, architecting this kind of models in the first place. And also from the HCI point of view,
what are these interaction mechanisms that could be added to these models in a way that
it is actually more understandable and usable on the user side? Yeah. Thanks so much. Yeah.
I think we're at about the time, but Duho will be here for a couple minutes after the talk for
further questions. So let's thank him for speaking. Thank you.

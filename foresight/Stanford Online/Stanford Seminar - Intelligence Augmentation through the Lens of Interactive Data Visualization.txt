Just such a treat to be back, I spend many hours on that side of the room, so it's wild
to be on this side of the room and going, whoa, there was actually like monitors up here,
like that's how the speakers kept track of where in their talk they were, so that's good
to know.
This is sort of the first set of talks I've given since the pandemic, and so I thought
it was a really great opportunity to talk about some new ideas that have been on my
mind, and particularly with all of you as my captive audience, I thought that I would
use this talk as an opportunity to think out loud about what the role of HCI should be
in the face of all of this really incredible rapid progress that AI and ML have made, particularly
kind of scoped in the last six months or so.
As I was trying to think about what the role of HCI should be, I was reminded of this figure
from Jonathan Gruden's 2009 article in triple AI about how AI and HCI are two fields that
are divided by a common focus.
As you can see in moments where AI makes a lot of progress, it's almost like the pendulum
swings towards ever-increased amounts of automation, perhaps at the expense of more HCI-esque approaches
of human intelligence augmentation or amplification, but also I think HCI is in a more established
stronger position than it's ever been in the past, and so I really think it's our responsibility
to think about what that counterbalance to ever-increased automation should be.
So I often, in moments like this, like to sort of turn to history and ground myself,
and so if we cast back to the first AI winter with Sutherland's sketchpad, right around
that time there was this foundational paper written by Lick Leiter at MIT titled Man
Computers in Biosas, and I think the gendering is unfortunate and unfortunately reflective
of the times, but nevertheless in this paper, Lick Leiter put forth this really compelling
vision about the ways in which a computer could interact with us through this intuitive,
guided trial and error procedure, turning up solutions and revealing unexpected turns
in the reasoning, and I was really tempted to put this sort of side-by-side with this
very recent demo that OpenAI released with ChadGPT plus plugins where you can upload
this music.csv data set and then start to have this very natural language interaction
to ask what are the columns in the data set, how many rows in there in the data set, and
then even say, can you give me some basic visualizations of this data set, and it thinks
a little bit, it's working real hard, and there you go, it produces sort of three visualizations
and even starts to give you maybe something that looks like an explanation, and I wonder,
is it time to roll out our mission accomplished banners? Like have we achieved Lick Leiter's
vision to think in interaction with a computer in the same way that we think with a colleague
whose competence supplements our own? Now, I don't think it's time to roll out the mission
accomplished banners, but I'm hopeful that the reason it's not that is not just my sort
of hope that we haven't been put out of jobs, but rather that there is something more to do.
So two years after Lick Leiter's man-computer symbiosis, Douglas Engelbart wrote up this
really incredible framework called augmenting human intellect, and right in the introduction
of this piece, we already start to see how Engelbart is defining a much more expansive
role of human augmentation. So the idea is not just about problem solving, which he does
mention right at the end there, to derive solutions to a problem, but it's also about
using computers to help us think. It's to increase our capacity to approach a complex
problem situation, to gain comprehension really about this thinking and not just the problem
solving pieces, and really what I like is how he thinks we'll get there. Certainly there
will be sophisticated methods, high powered electronic aids, but to me the part that really
resonates in his prescription here is streamlined terminology and notation, and that's going
to be a theme of my talk here, certainly one of the themes that underlies my group's work.
And so in contrast to that chat GPT demo, a few years ago I had the pleasure to work
with some collaborators at Berkeley, Yifan Wu and Joe Hellerstein, who you see in the
top right-hand corner, on this system called B2. So this is a Jupyter notebook, it's a
very commonly used data science environment where people can start to write code in the
style of a Python REPL, but what B2 does is saying, well, in addition to that sort of
linear style of data science analysis and programming, there's a lot of value in a more
visual analysis dashboard style interface like Tableau. And so what B2 tries to do is
bring these two pieces together. So you can see once I've invoked B2 it adds this on the
sidebar, and I can start to issue regular sort of Python, you know, pandas commands
like looking at the data frame, getting a, you know, a sense of how many rows there are,
what the columns are, and now I can start to write some code to do a little bit of data
transformation and visualization. Notice here in all of these steps, you know, when I'm
authoring a visualization, I don't have to specify what that visualization should look
like, right? I'm just calling these .viz methods on the data frame, and B2 behind the scenes
is figuring out what sort of visualization actually makes sense based on the history
of the transformations that were performed on the data frame. So in the case of year,
for instance, if I've grouped by year, the most sensible visualization to produce is
a histogram of the number of counts of data records across years. You might have also noticed
in the video that if I click the fields on the right hand side there, that it automatically
produces an equivalent visualization, but it doesn't stop there. It adds, you know, the code
and tags them with these little, you know, yellow emojis to indicate that there's actually sort of
a common shared representation here, right? Clicking on the sidebar not only produces the
visualization, but produces the equivalent code as well. And what's interesting is that these
visualizations aren't just output mechanisms, but I can start to interact with them to sort of do
this cross filtering interaction. So all the other bars update to reflect the data shown in the
highlighted bars, and B2 is keeping this as an interaction log that is semantically meaningful
to me. So this interaction log doesn't comprise mouse clicks and keystrokes and things like that,
but it's expressing data queries, right? Which states have been selected? And I can use that
data query to perform subsequent sort of analyses based on my interactive results. So I can say,
great, I'm gonna, you know, copy some code to the clipboard, paste it in as a data query to look
at what the interactive selection should be, and then, you know, proceed with some other sort of
visual analysis. And so as we're sort of, you know, looking at these two forms of interaction,
I was trying to figure out, well, some things feel the same, right? I've got that kind of
conversational back and forth. I'm sure on the left-hand side with ChatGPT, it's a more natural
language conversation. On the right-hand side, it's more of a repel conversation. But also,
things feel qualitatively different. And how do I actually kind of characterize what is the same
and what is the difference? And I thought really hard about it. And I realized that actually,
maybe what still matters is direct manipulation, right? And by direct manipulation, I don't mean
just the sort of Ben Schneiderman version of the term, which is, you know, associated with
graphical user interfaces and having a representation on screen that you can manipulate and undo
redo and things like that. But what I mean here is the deeper treatment of direct manipulation
that three cognitive scientists, Ed Hutchins, Jim Holland, and Don Norman, wrote in about
the mid-1980s. So in particular, in Hutchins et al's treatment of direct manipulation, they sort
of, you know, imagine direct manipulation to be this cognitive process between a user's goals
and the user interface. And, you know, they identify this gulf of execution that exists when a user
has to translate their goals into commands that they execute on the user interface. And similarly,
a return gulf of evaluation when a user has to figure out, well, did the UI do the thing that
I was expecting it to do? Right? I'm seeing a lot of nods in the audience because, you know, if you've
had experience in user interaction design, user experience, you've maybe experienced these terms
gulf of evaluation and execution. But what I find interesting in this 1985 paper is that they went
one level deeper. So in particular, they identified this idea of a semantic distance,
which is basically how users take the fuzzy notions in their head and translate those into
the nouns and verbs of the user interface, right? So going, doing that sort of sense-meaning operation
of transforming your intentions into the particular actions that might exist in the user interface.
And in addition to the semantic distance, they identified what I love. I love this term in
articulatory distance, right? So it's not necessarily the meaning that we care about anymore,
but the way in which we're conveying that meaning through the UI. And this is particularly important
because you might have several user interfaces that all express the same semantics, right? You can
conduct the same set of, you know, operations with them, the same nouns and verbs. But the way you
do that might be different because one interface might be graphical, the other one might be textual,
another one might be conversational, gesture-oriented, etc. And their claim in this paper was that
that articulation, the form of that meaning is really, really important, just as important as
the semantics. And of course, these distances exist on the Gulf of Evaluation as well. So the
articulatory distance is how do I perceive the changes that occurred in the UI and start to bring
meaning to that perceptual operation by interpreting and evaluating the degree to which
they met my goals. So this is actually going to give us the kind of conceptual machinery for
the rest of the talk. And it's a little bit dense. And so I want to return to sort of the prior two
examples and think about how they manifest these two kinds of distances. So in the case of the
chat GPT example, you know, if we start with semantic distance, I would say that, well, the
semantics aren't really well defined, right? They're not really explicit. Because what these models
have done is they've learned over, you know, vast corpuses of text, often just text that is present
on the internet. And so what they've learned is this latent space that is very ambiguous in the
semantics that are encoded in that latent space. So as a user, it's hard for me to know how to
translate my intentions into something that the system can understand because I don't know what
it is the system knows about the world. But as I'm sure many of us are aware, like prompt engineering
is a thing, right? So if I figure out exactly how to craft my, you know, natural language
expression, suddenly I can get the model to very rapidly almost zero shot adopt the semantics that
I want. And that feels like a very powerful, you know, affordance that we've not necessarily had
before. On the other side, you know, the semantic distance in the Jupiter notebook in B2 had explicitly
defined semantics, right? We have the explicit semantics of pandas on the data frame of the
visualization library of being able to click on the fields in a graphical user interface to produce
visualizations. And every time I did that, I had the shared representation of the code. So either I
would offer the code and it would produce a visualization or if the system produced some
code, I could go in and comment and uncomment entries or tweak the code in a particular way
and things like that. And so it gave me the shared representation that allowed me to bridge
between input and output mechanisms really, really easily. With articulatory distance in chat
GPT, right, natural language, it's been enormously powerful because it's reduced the sort of learning
threshold for a lot of things, right? So if I don't know exactly what it is I want or how to
sort of pose it to the question, I can lean into the ambiguity of natural language and chat GPT
catches up to my intentions pretty rapidly, which is great. But conversely, sometimes I know exactly
what it is I want. And it's really frustrating to have to express precise operations through the
ambiguity of natural language. And then as a result, because of the fact that natural language
is the only mechanism so far by which we can interact with many of these models, there's a
disconnect if your output is visual, like the case of visualization. So I can't interact with
the visualizations in any way to do subsequent back and forth interactions with the model. Now,
I don't think the second point is sort of a fundamental limitation, but it's certainly, you
know, the state of where we are today. And on the other hand, with, you know, Jupiter Notebook and
B2, with the articulatory distance, we've got basically the inverse of this, right? We've got a
nice precise programmatic syntax. So if I know that syntax, I can work really, really efficiently,
right? Sort of a common affordance of many sort of command line style interfaces. But I really
need to learn that syntax to be effective. And in some cases with pearly design syntaxes, which
I might maybe argue, Pandas is an example of, right? I constantly have to look up the documentation
for, right? There's a learning curve associated with it that slows people down. Yeah, Michael.
Yeah, so the reason I put it, I think this is a great question, you know, what lies in semantic
and articulatory. And oftentimes it is quite a fuzzy distinction. The reason I put this in
articulatory is my experience with Pandas oftentimes is I know what it is I want to do, right? I know
the sort of operation I want to perform on my, on my data frame. I just don't know the specific
syntax that I need to look up. Exactly, exactly. But certainly, you know, if, if you don't know
what it is you want to do, then the affordances of natural language absolutely help because you
can kind of, you know, pose things in really fuzzy ways and, and kind of iterate towards your outcome.
And, and I think you see some of this ambiguity in, in sort of, you know, the distinction between
semantic and articulatory distance here with this, this last point where because there are consistent
semantics that actually has this knock on effect on the articulation because now there's a shared
representation of input and output and that simplifies that articulatory distance as well.
So there's not quite that disconnect that we see on the chat GPT side. And so, you know, that's a,
that's, you know, I found semantic and articulatory distances to be a really helpful sort of framework
and I wanted to use it to sort of analyze the very last step in the output that that demo produced. So
it, it, you know, it's basically this, this thing that masquerades as an explanation of the
visualizations that chat GPT produced. But if you actually look at what it says, right, here's some
basic visualizations. Number one, a histogram of song durations colon. This shows the distribution
of song durations in seconds. All right, fair enough. Scatter plot of song hotness versus artist
familiarity. This shows the relationship between song hotness and artist familiarity. Well, I would
hope so. And then bar chart of the top 10 most frequent artist names. This shows the top 10 most
frequent artist names in the data set, right. These are not really explanations, but they're
pretty provocative or evocative in the potential that these models might have in allowing us to
produce these textual descriptions of visual artifacts. And certainly, you know, a lot of,
of, of people, certainly lots of big tech companies have thought about the ways in which you could use
all kinds of machine learning models, not just LLMs to do the sort of rich description of visual
content and particularly for this sort of these accessibility use cases, like how do you describe
these kinds of artifacts to people who are blind or have low vision. And lots of people have studied
the degree to which these models are effective and found maybe unsurprisingly that they're not
terribly effective right now, right. So here is a quote from a participant from one of our studies
who says, you know, the reader wouldn't get much insight from texts like this, which not only,
you know, is problematic because it doesn't effectively convey information, but more troublingly,
it actually increases the burden that readers face when they're trying to make sense
of this output, right. There's a lot of sort of noise that gets added to that experience.
Another participant, you know, says very, very interestingly, the problem with these textual
descriptions is also that it robs me of control of consuming the data, right. A participant,
another participant said, I want to have the time and space to interpret the numbers for myself
before I read any kind of textual description that does the analysis for me. And so to me,
these sound very similar to issues associated with a semantic and an articulatory distance,
right. That first quote talking about, well, these texts aren't conveying anything interesting.
The second set talking about, well, I want to have that time and space, I want to be able to
control the form with which that text is conveyed to me. And so I want to dig into how we might
address these two distances in the case of accessibility. But before I do that, I want to
give us a sense of how people who are blind or have low vision experience, you know, the
internet and graphical interfaces today. So I'm going to turn things over to my PhD student,
Jonathan Zhang, who will give us a quick demo of an assistive technology called a screen reader
that basically narrates on-screen content.
So here I can demonstrate what the accessible HTML version of our paper looks like to a screen reader.
So as you can see, what a screen reader does is it basically sort of linearizes the operation
of, you know, reading, perceiving, understanding graphical content on a user interface. And in
particular, you might notice that the narration was actually quite rapid, right. And this is
actually a slowed down version of what, you know, proficient screen reader users use, which is often
much, much faster. But what is interesting about the screen reader use case is that it forces that
linearity, right. And the key challenge in figuring out the articulatory distance in the case of
accessibility is how do you take visualizations that probably all of us in the audience have
slightly subtly different ways of reading, right. Maybe some of you start by reading the title,
then moving to the axes, then looking at, you know, the shapes, while others might start by
looking at the most salient trend and then start to, you know, map out to what the axes and legends
and stuff like that are. How do we take all of that rich diversity, but linearize it? So the
people who use screen readers can nevertheless have that same, you know, choice in meeting a
visualization, but under these conditions. And so the way we have chosen to do that is basically
by restructuring the content of a visualization into a text-oriented hierarchy. So at the top,
at the root of this hierarchy is just a summary of the chart, probably the trends that are shown
in the chart. And then the hierarchy branches off into the individual sort of data fields
or the encodings in this case, right. The x-axis, the y-axis, the legend and things like that.
And then people can start to drill down in ways that maintain some correspondence with the visual
artifacts. So one step below, you know, the x-axis is stepping through them by the major ticks,
right. One step below the major ticks would be minor ticks and then ultimately you would get
to the individual data points. So let me throw things back to Jonathan to give us a demo of how
this works. A scatter plot of Penguin data. And to a screen reader, our system represents this
scatter plot as a keyboard navigable data structure that contains text descriptions at
varying levels of detail. So when a screen reader user first encounters this visualization on a page,
they'll be able to read off a high-level alt text description of what the chart is. So
and if they're interested in getting more detail about this visualization, they can dive in by
pressing the down arrow key to descend one level in the hierarchy and access descriptions about
the different encodings of the scatter plot. So I'm going to press the down arrow key.
I can press the left and right arrow keys to flip through descriptions of the other axes and
legends. Cool, so let's say I am interested in getting more information about the x-axis. I can
use the left arrow key to navigate back to the x-axis description and then press down one more
time to descend a level of detail into the x-axis. So on this level underneath the x-axis, I'm
accessing descriptions of intervals along the x-axis and it's reading out to me how many data
values are contained within each interval. So by pressing left and right, I can kind of get
a sense of the distribution of data along the x-axis. So let's say I am interested in this
range from 190 to 200. I can then press down arrow again to dive into the individual data points
that are contained within this interval. So let's say that instead of moving up and down
this hierarchical structure, I would rather just move around the x-y grid in the scatter plot,
as if I were kind of feeling around a tactile graphic, for example. I can start by navigating
over to the grid view of the scatter plot. And once I descend into this part of the hierarchy,
I can use the WASD keys to move up and down different squares along the grid.
And so similarly to before, it's starting off by giving me the number of data values that are
contained in that square so that I can get a sense of the distribution of the data.
And so we designed this in collaboration with a blind HCI researcher named Daniel Hodges.
And this was the first time he felt like he actually understood and could build a mental
model of what it was that a scatter plot was representing. We saw these sorts of comments
reflected in user studies that we ran about how the form of this textual output really
influenced participants' mental model of what the data was, what the trends were, and things like that.
So one participant, for instance, said, I now know how to drill down and up between different
layers in the data to get an overall picture. And it gives me a different way of thinking.
And another one said, I'm thinking more in spatial terms because this is just a new method
for navigating and moving through the grid and drilling down to information and things like that.
And so what I find interesting here is that at every step, the semantic content stayed exactly
the same. And there wasn't even very rich semantic content. It was a range and then a count of the
data values. All we manipulated was that articulation, that form, giving it a hierarchical
nature, adding all of these different navigational affordances, and just manipulating the articulation
had this huge impact on people's mental models of the data. And I think that we're really just at
the tip of the iceberg of these more accessible structures. Currently, in my group, we're thinking
about just the impact that token order has on how people using screen readers build up those
mental models. If you're constantly prompting them with the range first rather than the actual data
values, does that introduce friction to their capacity to build that mental model and things
like that? But in all of this, where is semantic distance? How do we actually start to make that
textual descriptions more interesting and meaningful? And this is where I think LLMs can
really help us. For one reason, it's because there's just a sheer amount of textual content we need
to be able to produce that is infeasible to expect a human to sort of manually author.
But there are other sort of implications that we'll touch upon really shortly.
But before we can get LLMs to actually sort of produce the content we want,
what we need to do is shift from that very latent space with implicit semantics to a set of explicit
semantics. We need to impose a conceptual model onto our LLMs. Or another way of saying that is
we need to get the LLMs to understand what a good textual description of a visualization is.
And so that's what my then PhD student, Alan Lungard, set out to do. We ran a crowdsource study
where we got sort of 2,000 descriptions of charts. And through qualitative coding,
we realized there are basically four kinds of semantic content that textual descriptions should
convey. The first most primitive layer is basically just the sort of construction details of the chart.
What are the titles, the labels, the scales, the units, etc. And accessibility best practices say
that this is some of the most important content to convey because it gives people sort of important
milestones and landmarks. One level above that are the sort of statistical properties like minimum,
maximum, outliers, and things like that. And then one level above that is probably what is cited
people we consider the real value of visualization to be. The perceptual and cognitive characteristics
like complex trends and patterns, things that automated statistical methods we typically think
of as not being sufficient at. And then finally, the fourth and highest level are what journalists
often consider to be the real value of visualization, which is the narration that gets associated
with it. What is the data story that you're able to tell through the visualization? Can you explain
what you're seeing, the causal mechanisms, etc., etc. Now, another reason I think LLMs are really
suited for this sort of semantic bridging task is because when we asked sighted and blind people
what their preferences were, when it came to these four layers, four levels, we saw really
distinct preferences. In the case of sighted people, because we've got our own visual perception
doing that sort of bridging of the Gulf of Evaluation, sighted people tended to want higher
and higher levels of content being conveyed through text. Blind readers, on the other hand,
were pretty significantly divergent. For many of them, they didn't want those level three and four,
particularly the level four captions at all, because they wanted that time and space to do
the interpretation for themselves. And so here, this visualization to me conveys that LLMs can
help us or machine learning models can help us think about sort of personalizing the semantics
of a user interface in a way that maybe we haven't had the opportunity to study so far. There's been
a lot of work in personalization, but it's often been at that level of the articulation,
changing the sizes of buttons and adapting color palettes and things like that. And there's maybe
an opportunity now to use LLMs to actually change what the nouns, the verbs, the concepts of a user
interface are much more fundamentally. And so the way we're going about doing this in the case of
textual descriptions is we're going to be releasing very soon a data set of about, actually now we're
over 12,000 pairs of chart captions. And we've generated some of these captions and we've crowdsourced
some of these captions. And we started to train baseline models to do this task. And one of the
interesting features here is how do we represent the semantics of a chart to a large language model,
right? One way could just be let's treat the chart as an image, right? This is just a set of pixels.
And unsurprisingly, the baseline models don't do very well at that because a chart is a much richer
kind of artifact than just an image, right? It's got all this rich structure. So then we said,
great, let's look at a data table or let's look at a scene graph, which is just a fancy way of
saying the SVG associated with the chart. And a priori, we would have thought, well, the scene
graph is maybe like a good in-between between the computational affordances of data table and
capturing some of those perceptual characteristics. Turns out for the LLMs we trained that were
all transformer models, they did equivalently well on those two representations. And so one of
the things my group is working on right now is a new way of representing visualizations that more
directly encode some of those perceptual operations that are otherwise currently implicit in a scene
graph that grammar of graphics libraries like VegaLite or GGplot perform. But what's interesting
in all of this to me is that through these generative models, the goal has been how do we impose
a conceptual model onto them, right? How do we bring some explicit semantics? And I think we're
just scratching the surface here as well because I think the chart example case is a really great
one where a lot of these representations of charts that we've got right now, the grammar of graphics,
for instance, were designed for people to author, right? So we're really good at figuring out how
to design programming languages, domain specific languages, to emphasize the cognitive characteristics
that are important for human authors. Things like, you know, the cognitive dimensions of notation
that cares about, you know, how viscous is the programming language? How many premature commitments
is the programming language enforced? But I don't know what it means to design a representation
to be suitable for an LLM to operate over, right? Do we restructure the programming language more
fundamentally to make it tractable for an LLM? Maybe. So in addition to generative models,
my group has also been working with predictive models. And here I think the bridging task is
really not about imposing a conceptual model, but bridging it or aligning it to the ones that we
already have. And often the way that a lot of this work happens is through the lens of model
interpretability. So here is a very popular set of techniques called saliency maps. The idea behind
saliency maps is they're trying to depict the most important input features for a particular
outcome. So in this case, you know, this is an image, the label should be toy terrier, and here's
what a variety of different kinds of saliency methods believe to be, you know, the most important
pixels to produce that outcome. Now, I look at these visualizations and I go, well, you know,
is it telling me something? Maybe, right? And maybe the reason I believe it's telling me something
is because I'm the one doing the perception and interpretive tasks, right? Like if I look at some
of those visualizations on the bottom, I go, oh, like, it looks like the dog snout is really
important to the classification of a toy terrier or the spots. But it's not actually the saliency
method that is doing that interpretation for me. I'm the one bringing meaning to those lit pixels,
right? And so as a result, if we think about that gulf of evaluation, it's not the saliency
method that's helping bridge that gulf in any way, which is why saliency maps for now have been
these tools that we just use in a very ad hoc way that require a lot of manual effort to make sense
of. And so a question that my student, Angie Boggast, has been focused on is how do we scaffold
that semantic sense making operation, right? Providing some additional structure to help
sort of scale it up to make it more reproducible and things like that. And what she's developed
is these set of metrics that are very analogous to ideas of precision and recall, but are operating
at the level of input features and interpretability. So in many data sets, right, you've got some set
of ground truth human annotated features. And what shared interest is looking at is what is the
overlap between what a saliency method considers as being important to the classification and what
the humans, the human annotators thought was important. And there's actually three different
ways that these overlaps can manifest. The first is a sort of ground truth coverage, which is very
analogous to ideas of recall, right? It's how much of the ground truth does the model incorporate
in its prediction or what is the proportion of the ground truth region that is covered by the
saliency region. And if we look at some examples of low coverage on the top and high coverage at the
bottom, we can see then the case of low ground truth coverage is actually very little overlap,
right, between the ground truth, the yellow region, and the salient region in orange. But I often find
that it's actually the high coverage regions that are more interesting to analyze. So if we compare,
you know, cases where the model was correct on the right with the green label and cases where the
model was incorrect with the red label, we can see in the case of correct high ground truth coverage,
there are instances where the model relies not just on the object, like in this case with the
cab, but a lot of contextual information as well to ultimately make that correct prediction. But
on the flip side, right, with the laptop, the model is doing the same thing, but here the context
is actually throwing it off, right? It's actually confusing the model because it's accounting for
too much of that context in its decision making. Another kind of coverage is something we call
saliency coverage, and this is more akin to precision, right, which is how strictly is the
model relying only on ground truth features to make its sort of prediction. And again, you know,
if we look at low and high coverage in the case of low coverage, we can see again pretty disjoint
sorts of sets. But in the case of the high coverage regions, we can see that, you know,
in the case of high saliency coverage, it basically means that the salient regions are a strict subset
of the ground truth coverage. But the difference between a correct and incorrect prediction is
whether that subset was sufficient to make the correct classification or not, right? So in the
case of the Maltese dog, it did indeed only need to look at the head to make that correct prediction.
But in the case of the Dalmatian, it probably should have accounted for more of that dog's head or
some of the other characteristics associated with the dog. By focusing only on the snout,
it ended up sort of arriving at the incorrect sort of classification. And finally, the last metric
is something that is very familiar IOU, the intersection over the union. This is sort of
the strictest shared interest metric. It's really measuring how aligned the model's behavior is
with human reasoning. So if you look at some examples, again, you know, low coverage at the
top, we can see, you know, in incorrect cases, totally distinct disjoint sets again. But in a
correct instance, I actually find that pretty interesting, right? Low IOU coverage, but it
got a correct classification. Now, one could say maybe it got lucky. But potentially, what
the signal there is, is that maybe all the model needs is a tiny bit of a wheel associated with
a horse, right, to make the prediction that is actually a horse cart and not just a horse, right?
And on the flip side, with high coverage, you know, Newfoundland, great, you know, total,
total alignment. But in this case, right, incorrect classification, even though there was high
coverage, this might suggest, you know, genuinely difficult to classify images, right, even for
people. Because if I look at that, a pickup truck seems a totally reasonable guess to have made
about the image. I don't know that I've got enough sort of visual information there to call
that a snowplow. So shared interest basically gives us a mechanism to start to scaffold and
structure, bridge that semantic distance, right? People no longer necessarily need to manually
start to analyze these things. And in fact, you know, we analyzed lots of different models across,
you know, both vision and natural language and found that different combinations of these shared
interest metrics, along with figuring out whether the prediction was correct or not,
actually surfaced eight kinds of repeating patterns in model behavior. So we can see human
aligned and some of these others we also looked at earlier, right, context confusion,
context dependent and so forth. And all of these give us sort of semantics that we can start to
play around with through different articulations. So one articulation of these semantics might be
a very traditional visual analytics interface, right, where I've got all the different kinds of
images that I care about. This is a system we built to help a board certified dermatologist
make sense of this melanoma detection model. And you've got, you know, query widgets on the top
to sort and filter. You can use, you know, these histograms of the shared interest metrics to really
drill into the data. But what was maybe most interesting was what the dermatologist said
when they started to analyze that recurring pattern of context dependent cases. So in particular,
when they switched to these context dependent cases, the dermatologist started to wonder if the
model is seeing something we are not truly appreciating in the clinical image. Maybe there
are subtle changes we don't yet understand that the model does, right at the boundaries of the
skin region and things like that. And so, you know, to me, this is alluding to the fact of,
well, can we as domain experts learn something about our problem domain based on how it is
models are operating? And I think we see this more clearly in another articulation of shared
interest semantics. Here, what we're doing is basically using shared interest to interactively
probe or query that latent space. So we're brushing and using that brushed region as ground truth
and then calculating the IOU coverage to figure out what are all the classes that maximize IOU
coverage for that brush ground truth. So we can see if I brush over hand, a lot of the classes
that get returned are things that are often associated with hands like laptops and cleavers
and interestingly enough, hen. So I guess a lot of the images in the ImageNet, you know, data set
have people holding hens, right, which is, I guess, kind of interesting. But more maybe profoundly
is we could ask a question like, what is the essence of a dog, right? What is the minimal
amount of region that I would need to brush for the model to still be convinced that what it is
classifying as a dog? So I could start with the whole dog and then brush just on its head and
sure, you know, querying which shared interest still returns, you know, dog classes. But then I
could use a smaller brush and brush just on the nose and it still returns, you know, German shepherd
and sheepdog and Tibetan terrier and things like that. So it seems like according to the model,
all it really needs to know about, you know, an object in the image is the sort of shape of its
nose or something associated with its nose to be able to classify whether it is or is not a dog.
And this seems like a really sort of toy example, but it reflects some of the things that real
world scientists are doing. So in particular, you know, there's a researcher at the University of
Washington, Julia Parrish that runs this grand crowdsource data collection project around seabird
deaths. And the way they train their participants to figure out how to do bird classification
is by asking them to measure, you know, the bird beaks and the bird feet and things like that.
And so I think it's really interesting that we're seeing maybe some of those sorts of
representations creep up in how a model is making its decisions as well. And so where I want to end
is sort of being most speculative and where I think, you know, there's scope for HCI to sort of
grow. And so, you know, we looked at generative models and imposing a conceptual model on them.
We looked at predictive models where the idea was to align conceptual models. But what I think,
you know, we're hearing from that dermatologist we're seeing in that last case study which shared
interest is the potential to use machine learning models to basically discover new representations
of particular problem domains, right? And, you know, again, at my most speculative, I don't know
what I would call these, but I would maybe call them abstraction models, right, where the goal of
these models is not to produce some particular outcome that I care about, but to maximize what
are the different ways of representing the world, right? What are all the diverse abstractions that
we could learn about a problem domain like classifying dogs or classifying seabirds or things
like that. And I think this is a really interesting opportunity to use machine learning to essentially
advance our understanding, advance our science. But I want to be careful here because we've already
seen, you know, through this talk, but also in the broader discourse, how generative and predictive
models can sort of muddy that, that gulf of evaluation, right? Lots of people are starting
to anthropomorphize these models, you know, some people think these models are representing general
intelligence or conscience or things like that. And there's a potential with, you know, these
abstraction models to make this problem worse by muddying the question of, well, how do we know
what we know, right? Like what counts as evidence? Is it evidence because, you know, the model has
learned that representation? And how do we validate what that evidence is? In the case of
representations that are designed or interpreted or theorized by people, we know how to consider that
to be evidence, right? But I don't know what it means for a learned representation to count as
evidence. And as all sorts of problems in machine learning, this is not necessarily a problem that
is unique to machine learning. So here are three visualizations that were used to discuss the
COVID-19 pandemic right at the, the, the peak of the first wave in the summer of 2020.
And I'm curious if anything pops out at you, like any reason, you know, to be curious or
suspect of, of these visualizations, right? Like no, right? Probably not. Like these seem pretty
legitimate, right? Like our world and data, very legitimate data source, right? And if you look at,
look at some of these two other visualizations, you might go, you know what, actually the one on the
right, that looks like something in maybe a policy briefing or something, right? It looks very
sophisticated, lots of good annotation, you know, a style and aesthetic that looks very sort of
sophisticated. But you may be catching what I'm alluding to, which is the fact that all three
visualizations were used by people on social media to advance the argument that, you know,
our response to COVID was overblown. Not that COVID was a hoax, but that our reaction to it
was, was way too extreme. That COVID wasn't as serious an issue as it might initially seem.
And I want to be really careful about what I'm, when I'm doing here with these charts, because
certainly some of the people that were distributing this were bad actors who were ideologically
motivated. But through a very long, laborious ethnography, ethnographic process that we
conducted, spending six months on five different Facebook groups, we found that a lot of people
who were producing visualizations like that were actually displaying many hallmarks of citizen
data science. So they were really many of them filling gaps in, in information sort of collection,
because they were situated in rural parts of the country where, you know, there wasn't a lot of good
data collection. So many members of these groups were hosting, you know, webcasts, live seminars of
how to download data from the government website, how to clean it and excel, how to visualize it and
things like that. And, and most surprisingly to us, many of them were engaged in discussion that
looked like peer review, right? They were critically assessing data sources, discussing metrics,
making arguments for which metrics were, were better or not. But all of this was sort of
inflected through a sort of frustration with mainstream institutions and maybe even distrust
of those institutions as well, right? But ultimately what these groups cared about was bolstering a
sense of social unity and civic engagement, right? So this quote I, I find particularly sort of
reflective of that sense of, you know, it's incumbent on all of us to hold our elected officials to
account so that they make better decisions through data, right? I'm speaking to you as a neighbor,
as a mama bear, right? So this is not some sort of ideologically motivated individual who is,
who is, you know, trying to be a bad actor. This is just an engaged member of the citizenry.
And similarly, you know, oftentimes they were actually more sophisticated than scientists
can be. So many of these members were very reflexive about their own data analysis,
data gathering process, right? So someone says, you know, I've never claimed to have no bias,
right? I'm human, of course I'm biased, here are my biases. Whereas in science, often we like to
portray ourselves as being very objective, you know, arbiters of truth. And so in many ways,
you know, what was happening in these groups is, is perhaps more sophisticated than what was happening
in science and public health at the time. But the question is, so what does this have to do with
sort of bridging semantic distances and abstraction models? Well, I think what was happening in,
in those groups was, you know, they, they, they disagreed with the definitions of some of these
metrics, right? They were living in rural communities. And so the metrics that public
health officials were using to, to, you know, define the, the state and scale of the pandemic
was not reflected in their lived experience. They were turning around and well,
it didn't seem like COVID was an issue, right? And so our colleagues in the humanities and
social sciences often advocate for adopting what they call an interpretive, interpretivist lens,
right? The idea that knowledge is subjective, it's socially constructed,
and that it's composed of many diff, different diverse perspectives that we have to figure out
ways to synthesize together. And while that idea has been adopted in pockets of visualization and
HCI and CS, so far, I think it's largely been on the qualitative side, because if we think about
how to do computation, we have to, you know, we're forced into making decisions about the world and
how to represent that world and computational data structures. And what I think abstraction models
allow us to do is start to push, but you know, push that boundary a little bit, right? Rather than
being focused on developing a model that produces a single best outcome, we might instead be looking
to a world in which we are training sort of ecosystems of abstraction models, where we're
forcing them to learn really different representations of the world or of a problem domain,
and then leaving it up to people to figure out how to synthesize between those learned
representations for, you know, some particular policy goal or, you know,
thing that they want to optimize for. So with that, I'm happy to take questions about any of
what I talked about. Thank you very much.
This image that made me make this decision, so do you think the results would be different
if you used, like, iFixations in that comparison? That's an interesting question. We haven't
considered iFixations for the salency map work, but certainly I think your intuition is right in
the sense that, you know, the current way that we've modeled shared interest is pretty brittle,
right? It's operating at the level of abstraction of, like, pixels in an image, and how meaningful
are pixels really? And so what Angie is working on right now is a way to raise the
level of abstraction that shared interest is working on. So in many of these domains, like,
you know, ImageNet, the task that we're asking models to do, the labeling task,
actually inherits from a much richer knowledge graph or taxonomy or hierarchy or things like that.
But right now, at least, you know, there's a little bit of work in hierarchical learning,
but most of the predictive models are just learning at the finest level of detail, right?
So we're throwing away all that rich information that might be really relevant to how a person is
making a decision. So maybe what I care about is not whether it's a Chihuahua or a golden retriever
or a laboratory retriever. I might care, is it a dog or really sometimes is it just an object,
right? And so what does it look like to do shared interest in more meaningful abstraction space
rather than pixels is something we're working on. Yeah, great question. Thanks. Yeah, Will.
Thank you for the great talk, Arvin. So going back to Jupyter notebooks and ChatGPT,
you talked about how, right, ChatGPT can shell out to some of these nice plugins like for Excel or
whatever to try and help people do natural language data science and that there's this
articulatory distance due to the difficulty of learning an API. But conversely, you could say
tools like co-pilot are sort of the parallel to overcoming that articulatory distance by
almost in some sense, what is the same interface expressing a natural language but just in a
code comment and then getting back code, right? But just I guess the only difference is its code
you can see as opposed to code that's running in some back end that you don't see. And I'm curious
if you think there's sort of a synthesis of these two poles, an interface that can take the best of
both worlds and offers conversation but still provides access to the code or encourages people
to understand the annoying representations. Yeah, absolutely. I thought really hard about
which of those examples I wanted to use as the kind of foil to B2. So I did very seriously
consider a co-pilot and I sort of agree with your analysis that it's, I think, a much better
example of how to integrate the capacity of these LLMs. And I think there's opportunity
to push that even further where what I would often want is really targeted mechanisms to
introduce ambiguity, right? Right now, the little that I've used co-pilot, it's almost at the level
of, well, it's going to produce the whole function, the whole whatever. And often what I want is it
to be the sort of parallel prototyper for me, right? I want to introduce, say, a hole in my program
and then go, I don't know that I want that hole to be filled in with just one specific
outcome, but I want it to produce the whole space and for me to go, well, I want a little bit of
this and a little bit of that and so on and so forth. So yeah, I totally agree with there being
some really interesting medium of these things. Cool. Yeah, I like that idea.
Yeah. Hey, really exciting talk. I'm wondering towards your kind of vision for these abstraction
models, I'm wondering like obviously kind of from a human-interpreter interaction perspective,
we know like representation matters so much, right? Like isomorphs of representation very much
change how people can approach a problem or understand it. But I guess the ways in which
they vary and the benefits of these different representations are tied very much to human
cognition and perception. And I'm wondering, you know, in some of the examples you're showing and
a lot of work in machine learning, we're sort of training things based upon that output. Yeah.
And I'm wondering like, you know, are there ways that we can get at more of how people
are thinking versus just how they output and how do we get there? Yeah, I love this question.
And the reason I love it is also the reason I love sort of that Hutchins et al description of
direct manipulation, right? I find the terms that they use there, particularly these two's
distances, really evocative terms. Because to me, a distance is something that I would want to
measure, right? But so far at least, as far as I know, those terms have largely been descriptive,
right? As you saw in my talk, like I use them to be very analytic, but I'm not able to be generative
with them in, you know, a very systematic way. So certainly a lot of the work that my group is
trying to do right now is in visualization, you know, there's a lot of work that we've
inherited in methods from sort of vision science. So we run these studies of human perception.
And increasingly, the field is starting to get to, well, how do we start to measure cognition,
right? Can we model sort of a decision making task and start to, you know, operationalize
that through experimental design? And so we're starting to push in some of those directions
as well, but scope to sort of, you know, interaction in a Jupyter notebook, but then
starting to see, you know, the impact that interaction has on sort of the downstream
analyses people would do, and then see if that actually maps to, you know, their goals or things
like that. Absolutely. Yeah. So I'm curious about the, just continue on this line of
perception up through cognition, you know, going back to the sort of like,
or 10 Cleveland McGill kinds of stuff, the automatic processing was very key to the design
of visualizations, especially early on, that the notion was that my encodings were supposed to
map on to almost like system one interpretation, right? Like when I see the scatterplot and
you know, encoding distance in the following way, I'm gonna draw the correct conclusion.
And it's interesting to me that sort of through the transformations you've started to pursue,
we're not trying to like encode those into a similar mapping for audio, but instead
directly doing the cognition on behalf of the individual. And those seem like orthogonal
directions one could go. I'm curious how we find the right point in the design space.
I think this is a fantastic question. So the way my group is starting to think about this
of like, how do we find the sort of right balance of who is doing the perception,
who is doing the interpretation is starting to consider some of these modalities and concert
to better understand what the relative affordances of these modalities are.
So in particular, Jonathan, who you saw in the demos is leading some really,
really cool work right now around what if I'm sort of specifying the visual, the audio,
the sort of sonified audio and the textual audio side by side, and then I'm playing them
sort of simultaneously through. Do I want, you know, there to be sort of perceptual redundancy
where the sonification is sort of emphasizing what is, you know, described in the texture?
Do I want these modalities to be complementary? And, you know, sort of TBD, but I think there's
some really exciting sort of questions for us to sort of dig into space.
Are there similar pre attentive principles for audio? There must be.
As far as I know, so I'm, you know, we're just starting to look in the sort of sonification
literature. Yeah, as far as we can tell sound is a very, very different perceptual sense
than vision. And so even the sort of, you know, basic sort of visual encoding paradigm where I
take a data field, I map it to, you know, position color size that breaks down very rapidly for audio.
So oftentimes really all the people are able to sort of, you know, detect differences in our sort
of pitch and loudness. And even then our fidelity at that is very, very low. And so there might be
some pre attentive characteristics. We're certainly looking at some early work in HCI.
I think Stephen Brewster had done around sort of ear cons, you know, discreet sort of representations
of icons, but through audio and things like that. So there may be some of that there.
But at least so far we're so early in our own work that we don't know.
Interesting. Okay, thank you. Yeah.
I think we're about at time. So if you have additional questions, please mob him after
the talk. Thank you, Arvin for joining us. Thank you very much.

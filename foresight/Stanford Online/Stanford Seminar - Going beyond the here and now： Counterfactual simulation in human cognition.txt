Well, thank you. Thank you so much for having me. It's a pleasure to be here. And I hope
maybe some of the things that I talk about may give some inspiration for you, HCI guys.
So I lead the causality and cognition lab in the psychology department. I'm interested
in how people understand causality and basically how the world works and how they understand
each other. And we're interested in how people learn about the causal structure of the world,
how they, once they have it in their mind, how they can use it to reason about the world,
make predictions, make inferences about the past, or think about maybe also how things
could have played out differently from how they actually did. And how those capacities
also allow us to make the kind of judgments we do in our everyday lives, like for example,
assigning responsibility to one another. And that's in fact one of the bigger sort of overarching
goals that my lab is working toward, namely developing a computational framework for
understanding responsibility. And I think to get there, we have to be able to answer at
least two questions, namely one being what causal role somebody's action played in bringing
about the outcome. And the other one being what the action that the person took tells
us about the kind of person that they are. For this first one, we need some intuitive
theory of how the world works. So we can relate the actions that somebody took to the kind
of outcomes that resulted from those actions. And for the second question, we need some
intuitive theory of how people work. So we can go backwards from the actions that we've
observed to the mental states that may have given rise to those actions. So what were
the person's intentions, what did they believe, what were the kinds of things maybe that they
were able to do as well. And so I studied psychology, like in my undergrad, and I was
most excited about social psychology, because I felt sort of most applicable, I guess, to
my everyday life, and somehow also got into responsibility, like back then already. Maybe
it was because I was in some group project where I felt I was doing all the heavy lifting
and maybe I wasn't getting all the credit for it. So that was sort of what interested
me initially. And when I read around in that work in social psychology, a lot of the theories
that I saw took a form sort of like this. So I'll just give you a few examples. So basically
sort of like boxes and arrows theories, where they identified important concepts that were
related to how we assign responsibility, and maybe also roughly how they were related to
one another, but still left a lot in a certain way unspecified. So this is a quote from
Bertrand Molle from a while ago. He says, like, an important limitation of many of these
models of moral judgment or assigning responsibility is they don't really generate any quantitative
predictions. And you might say, like, oh, what do you need quantitative predictions for? Well,
one thing that they're useful for is sort of, you know, laying your cards out and making
it concrete, what your model does also allows it then to be falsified more easily. And I
remember this one instance, it was like me, I think maybe first day of my PhD, I went to
this conference and had dinner, you know, with one of the, one of the people who had made
one of these sort of boxes and arrows and diagrams. And I told them about some experiment
that I thought, that I thought of and thought, like, oh, this would happen. And I think that
would be the result of that experiment. And, and I was very, very smart. I thought, like,
oh, this would totally kind of disprove your theory, right? And he said, no, no, that would
be totally consistent with my theory. And I thought, oh, that's weird. I maybe I really
tried to understand the theory very well. And, and so, so that also was a sort of little
bit of a moment for me that I felt like, okay, maybe it's important to try to make these
theories even more precise. So we know what it is that they're predicting, so we can go
about and, you know, falsify them and sort of improve them. And so that's been very much
kind of an inspiration for me, what I've been trying to do it a little bit. And so one of
the starting points in almost all of these theories of responsibility is there's causality,
always causality comes first. So I thought, okay, let me try, let me try that one. So
can we get more specific about what it means, you know, what people, what it takes for people
to say that one thing caused another thing to happen. And so I think that three key ingredients
that we need, like in order to get a theory for how people think that one thing caused
another thing to happen. And those are starting with a mental model that people have of a
particular domain, a mental model that allows us to conceive of counterfactual interventions.
So, and I'll flush it out a little bit more in a moment. So imagining how things could
have been different from how they actually were. And that allows us then to mentally
simulate what the consequences of this counterfactual intervention would have been. And so the idea
of mental models has been around, you know, for quite some time and has recently gotten
a little bit more attention again, also in AI. And, but, but yeah, some of the credit
at least in modern times that go to the philosopher Kenneth Craig and his book, The Nature of
Explanation, who said something along the lines, well, he said exactly that, but I'm
going to say along the lines, so that we have something like a small scale model. Oh, wouldn't
it be very helpful if we had something like a small scale model of the world in our minds
that we can then use for all sorts of things, like predicting what was going to happen if
I did this, rather than actually having to carry out the action and then, you know, dying
maybe if it was a bad one. And, and yeah, that that would be really helpful for decision
making. And as I will say in a moment also really helpful for explaining kind of why
something happened. So this idea of mental models has been around for a very long time.
And then in somewhat more recent years, at least in cognitive science, has been made
a little bit more concrete, particularly as it pertains to our mental model of the physical
world. And so the idea is was here to say like, well, maybe our mental model of the
physical world is in certain respects, similar to the kinds of physics engines that we use
to make realistic computer games. That's a common move, right? You have some, some tool
and then you think like, okay, maybe the mind is a little bit like that tool. So this was
just, you know, psychologists playing Angry Birds and then thinking like, okay, maybe
the mind is a little bit like Angry Birds. So here, the basic idea, right, is that we
take in the world, you know, through our perceptual senses, and that we then build this internal
representation of the world. That's now the physics engine kind of representation. So
that we pass the world, for example, into objects and the properties of those objects
and then the interactions between those objects. So here, this child maybe passes the world
into the ball and then the eagle on top of the tower and then the tower or the blocks
that make up the tower. And now that you have this internal representation of the world,
you can use it, for example, for planning. So if this child, for example, wants to topple
over that tower, they can think about what's going to happen if they roll the ball like
in different kinds of ways. So I can run simulations using this internal engine in my mind. So
having this would be very useful because I could make predictions about the future. I
could pay sort of Sherlock and infer from the current state of the world what must have
happened in the past. And as I'll show in a moment, this would also be useful for explaining
something that happened in the present. Okay, so what I'll do is in this in the remaining
time, right? I'll basically want to cover these two different aspects of working towards
this computational framework. So in part one, I'm going to focus on the physical domain.
And then in part two, I'm going to expand it to just start to think about people. I should
say, obviously, feel free to ask questions like anytime throughout. Otherwise, I'll try
and end around 12.20 so that we have a little bit of time also for Q&A at the end. Feel
free to ask and throughout if anything's unclear. Okay, so let's start with this part
one. And I should also warn you, there is a little bit of audience participation required.
So get ready for that. So the first, we started really simple, right? I was saying, okay,
I want to understand causality a little bit better. What's the simplest possible setting
maybe in which you could think about causality? Well, it's two billion balls colliding with
one another. And here's the first audience participation part. So there's going to be
these two balls coming in on the right side of the screen. And I'm going to ask you whether
you think that ball A caused ball B to go through the gate. And if you think so, maybe
just raise your arm like at the end of the video clip. So here's what's happening. Okay,
so who thinks that A caused B to go through the gate in this case? Okay, a lot of people
do anyone think that it didn't? No one dares? Okay, cool. So you're in line with what most
people say in this case. And here's what I think was going on in your minds. Not the
motor part of raising your hand, but the kind of judgment, the part of, yeah, was there
causation happening in this case? And the first part is very kind of uncontroversial.
So you looked at what actually happened. You saw that they collided with one another
and then ball B ended up going through the gate. And now the somewhat more controversial
part is to say that, well, that's in itself is not sufficient. That does not contain all
the information you need in order to say that A caused ball B to go through the gate in
this case. But you also need something like this. You need the capacity to simulate in
your mind, in this case, that removing basically ball A from the scene, kind of in your mind.
And then simulating where ball B would have gone if ball A hadn't been present in the
scene. Maybe you all sort of naturally and spontaneously did that. And of course, I already
talked a little bit about counterfactuals and stuff like that in the experiment. Of course,
I don't do that. I just ask people to make causal judgments. So the simple idea here
is then to say, when do you say that A caused me to go through the gate? It's really sort
of like an epistemic notion. So your subjective degree of belief, well, to the extent that
you think that what would have happened in the actual, so that what would have happened
in the counterfactual situation would have been different from the thing that actually
happened, that determines your extent to which you say that A caused B to go through the
gate. And here you're probably pretty sure in this instance that B would have missed
if A hadn't been there. So you say, yes, A caused it to go through. And just a little
bit in terms of sort of background, a lot of the inspiration for this kind of work comes
from Judea Pearl's work on causality. Some of you may have heard of his work. And there
they use different kinds of generative models to capture people's causal knowledge of the
world. So this could be something like causal base nets or structural equations that you
may also remember from your stats class if you had one. And then you define some kind
of operations on these models to support things like counterfactual reasoning. So imagining
that some variable had been replaced with another one, for example. And so I'm doing
something quite similar here, only in that I'm assuming that the generative model that
people have of the world in this case is somewhat richer than what can be represented
with these causal basis or structural equations. So in my case, the generative model that I
assume people have in their mind is something like the physics engine that I actually use
to generate them in the stimuli. And I'll make that noisy. I'll show you in a second
how I'm making it noisy. And then I also have to think about, okay, what are now the counterfactual
intervention operators that you might have over a representation like this one? And in
this case, it could be something like imagining that an object wouldn't have been, would not
have been there, for example. Okay, so you might think now, okay, well, maybe that's
the only game in town. Like what else could you possibly be doing in a setting like this?
And at least luckily for me, there has been a lot of philosophers and psychologists that
have argued for what I called these actualist theories of causation. And they basically
just say you don't need that part, right? All you need, all the information you need
to give causal judgments or causal explanations for what happened is there in the actual situation
in some sense. And so one of the best kind of worked out accounts of that in psychology
comes from a psychologist called Philip Wolff and he calls it the force dynamics model
of causation. And the idea is that all you need to pay attention to is the forces that
are associated with the agent and the patient. That's the sort of lingo they use. And you
then you just look, need to look at how these forces are configured. And that helps you
to say what in this case here, there's different causal expressions is appropriate to use in
a particular situation. And I'll just apply it to this example here. So we have the patient
which is ball B that has a force that is associated with it. Then we have an agent
that applies a force to the patient in this case. As a function of these two forces, we
have some resulting force here. And then in this case, the patient also ended up reaching
the end state. And because this configuration looks like that and that maps onto this force
configuration, Philip Wolff's account would also here say, yes, a cause B to go through
the gate. So this clip would not help us actually tease apart this other model that
I've been kind of promoting. So just to make this distinction sort of clear or clearer.
So in the force dynamics model, you start with some intuitive theory of how the world
works, which in this case are these little force vectors that apply to agents and patients.
And you can then directly go from there to making causal judgments. So there's this direct
route from this kind of intuitive theory to causal judgment. He also says that you can
do counterfactuals too by imagining, for example, if one of the forces hadn't been there, what
would have happened in the situation, but that it's not necessary to figure out whether
something caused something to happen. And sort of what I'm arguing for is sort of a slightly
different picture. Where I'm saying, well, first of all, I start with a slightly different
theory of the domain. In this case, again, using the physics engine rather than using
these force vectors. But then saying that you have to go through this process of counterfactual
simulation to say that something caused something to happen. And what I'm going to try and do
in the next two slides is sort of motivate that account.
One way to motivate at first is that I started off saying like I want to have this model
of responsibility. And that means that I want to have a model of causation that not only
narrowly applies to the physical world, but that can also be applied to, for example, the
kind of causation that happens between people. And here's just some examples of causal statements
that you could hear at the fall of Lehman Brothers, caused the financial crisis. My
housemates failed to water my plants, caused them to die. Realizing that he forgot his
wallet at home, caused him to go back. You probably wouldn't say that exactly in English,
but they all seem to find sort of causal things, like to say. And it's probably a little bit
tricky, or at least I would find it tricky, to think of how would I explain these sorts
of causations with force vectors. And the hope is that the account that I'm developing
is a little bit more flexible so that it can apply to these sorts of situations as well.
But now, and another kind of key advantage, I think, of the model that we've been developing
is that it actually allows us to derive quantitative predictions. And it's hence more easily falsifiable
that some of the prior work. And so you can falsify it if you like, write a paper until
we was wrong. And then I have to go back to the office and improve the account. So here's
a way in which we're getting quantitative predictions out of this model. But I was saying
that how you make causal judgments is by comparing what actually happened with what
would have happened in the relevant counterfactual situation. But now you don't know that. The
thing that I'm showing here on the right-hand side, I guess, that's in some sense the ground
to truth. But you don't get to see that. You only see what actually happens. You don't
get to see what would have happened if ball A hadn't been there. So you have to use your
intuitive understanding, again, of this domain to simulate what would have happened in this
counterfactual. And so one way for us to capture this uncertainty that you may have
about exactly what would have happened if ball A hadn't been there is by generating simulations
from our physics engine, but now injecting a little bit of noise into that engine. So
now it becomes sort of like a probabilistic program because it's now not a deterministic
outcome anymore if ball A hadn't been there. But rather what I'm doing is I'm generating
a simulated sample from my model. And now in this case there's many different ways in
which you could make your model kind of random or uncertain. Here what we did is we just
took the actual ground truth, that ball B, velocity that ball B would have had, and applied
a small perturbation to the velocity vector at each point in time. So now it's sort of
like in your simulation, when you're imagining where ball B would have gone, it sort of jiggles
a little bit along the way. And so this might be now one outcome, like off such a sample.
So if you think like, oh, oh, I think it would have missed. But let me try again. Like,
oh, yeah, I think it would have missed. Yeah, I'm pretty sure it would have missed. So this
is just multiple times sampling in your mind of what would have happened if ball A hadn't
been there. And here, since all of them, you're pretty sure that it would have missed, you
said, yeah, A caused it to go through. But you can probably already anticipate, we can
now do a slightly different case, right, where in the actual situation, again, still A collides
with B and B goes in. But this time it's sort of less clear what would have happened
if ball A hadn't been present in the scene. Because that ball B is headed like right to
the goal post, essentially. And now if you apply the same idea of simulating with noise
what would have happened, you know, in some cases, maybe ball B would have missed. But
it's also possible that ball B would have gone in anyhow, even if A hadn't been there.
And that accordingly, you might say like, yeah, I'm less sure, you know, that A caused
ball B to go through the gate in this case. So that's what we did now in our experiment
where we showed people a bunch of clips like this one. So here's just three different ones,
one clip in which, you know, it's pretty clear here at the top that ball B would have missed
if ball A hadn't been there. The one in the middle is like one, this kind of close call.
And then the one on the right hand side is one in which it was pretty clear that ball
B would have gone in anyhow, even if A hadn't been there. And then between experiments,
we either asked them some counterfactual question. So that's the one here at the bottom,
the blue one. Do you think that ball B would have missed if ball A hadn't been there?
And then we see that in this case, they're pretty sure, yeah, I think it would have missed.
Here, they're right at the midpoint of the scale, not sure, right, whether it would have
missed or not. So we give them some slider where they can just evaluate their degree of belief.
And then in this case, they're pretty sure that it would not have missed, even if ball A hadn't
been there. And then we take a separate group of participants and we ask them a causal question.
So those participants don't hear anything about counterfactuals. We just asked them in a clip
like that, what do you think that ball A caused ball B to go through the gate? And we see that
judgments align very closely with those of the ones in the counterfactual question condition.
And we can also use that model that I described that draws these samples and tries to simulate
what would have happened. And it also yields very similar judgments in this, or makes predictions
in this case. These were just three of the video clips. We had like 18 different clips in that
experiment. And if we just line up here on the x-axis, the average counterfactual judgments
that participants made, and on the y-axis, the average causal ratings that participants gave,
you see that they're very closely aligned with one another, at least suggesting a strong
relationship between these kinds of judgments. But when we published this work as a coxide paper,
so for the cognitive science proceedings, one of the reviewers, they were mostly happy with it,
but one of the reviewers was saying, yeah, but all of the clips that you showed participants,
something slightly different was going on. So maybe you just didn't try hard enough to come
up with an actualistic count, like one that only looks at what actually happened. And if you
tried a little bit harder, then you could have explained it away. So we did try, and we didn't
succeed, but it's also sort of a weird position that you're in when you kind of don't want to
succeed, right? So we thought, okay, maybe the better thing, rather than being crappy at modeling,
you know, just let's come up with an experiment where it feels like if it comes up in the way that
we think it will, there's no way you could possibly explain it with an actualistic count.
And so that's the route we took. So just to really think like, oh, are these counterfactors
really necessary for understanding causal judgments? So second round of audience participation,
get ready. I'm just going to show you a slightly different clip, and this time I'm going to ask
you whether you think that ball A prevented Bobby from going through the gate.
Okay, what do you think? If you think that ball A prevented Bobby from going through
the gate, you can raise your hand. Okay, a few people think so in this case. Okay, I'll show you
another one. Okay, this was not some kind of, you know, glitch. I was having fun, you know,
doing the physics engine and sort of playing portal, right, by turning these things into a
Taylor port, right? And I didn't tell you anything about them, of course, when I showed you the
first clip, but maybe just seeing that one clip, you already have like one shot learning, yeah,
okay, maybe that's a Taylor port. And the Taylor port, it works only for ball B, you know, it
doesn't work for ball A, and the yellow thing is the entry of the Taylor port, and the blue
thing is the exit of the Taylor port. And now that I've shown you that, if I now show you exactly
the same clip again, you're going to say, at least if you're like my participants, yes, it prevented
it from going through, right? Because now what changed is basically your belief about how the
world works, such that your counterfactual looks a little bit more like that now, right? What would
have happened is that it would have gone through the Taylor port and into the goal, right? So the
fact that I can show you exactly the same clip twice, right? And, and all I've changed was your
belief about how the world works. And that makes a big difference to your causal judgment, sort of
shows that it's, it cannot be sufficient to explain causal judgments just in terms of what
actually happened, because actually what actually happened was exactly the same in both of the
times that I showed you the clip. I don't need to do the Taylor port thing. The Taylor port thing
is cute because I can show you exactly the same clip, but I can also move some obstacle in and
out of the way, right here on the left hand side, you're not going to say that A prevented B from
going through the gate. On the right hand side, you are because the block is out of the way, right?
And a similar way for causation. And on the left hand side, you're going to say, yeah, A caused it
because the block would have blocked it. And on the right hand side, you're not really going to say
that it caused it because it would have gone in anyhow, right? Same idea. I'm doing exactly the
same interactions between the balls. I'm just changing something kind of in the background
that affects the counterfactual and, and thereby also affects people's causal judgments.
Okay. So another thing that's sort of neat about this model is that it doesn't only kind of predict
basically the judgment that people should give at the end of it, but also says something about
the cognitive process by which they arrive at the judgment, right? In this case is maybe this
process of mental simulation, that you kind of generating these samples and thinking about what
would have happened, and that those drive the causal judgment. And one way we can do that,
or can sort of get more direct evidence on that is to use eye tracking, right? To see, okay,
where is it that you're looking at when you're asked to make causal judgments in these kinds
of video clips. So we went back to the really simple ones again. And now also between experiments,
just ask participants a different question about the video that, that, that they would see. And
they knew at the beginning what question they would be asked. So we had one condition here that
we call the outcome condition where they'd watch the video and we would just ask them at the end,
in this case, if it ended up missing, did be completely miss the gate. And so I'll show you
the eye movements of one of the participants in this condition. And I'm going to play the
video at half speed and I'll do some sort of life narration as it unfolds. So the blue dot is the
eye movement, right? So the participant here is looking back and forth between ball A and ball B.
So looking, looking at ball B, sort of now trying to extrapolate where ball B will end up hitting
the wall. And then mostly looking at ball B. Not very exciting, but also that's all they need to
know in order to answer this question in this case. So now if you take a different participant
who was asked to make a causal question, or asked to answer a causal question in the video,
but otherwise saw exactly the same video clips as other participants did, you're going to see that
the eye movements look quite different. And they look different in a way that made me very happy
at the time. So you see they're not just looking at ball B, they're trying to anticipate where
ball B would have gone, you know, if ball A wasn't present in the scene. And it's quite likely that
when you guys, when I showed you this first video clip that you did that, right? And may not even
been super, you know, aware to you that you did do that, like I haven't really checked, you know,
yeah, how, well, at some point at the beginning when I ran this on the laptop, I would sometimes
see that people would use their finger, or they would use their, you know, kind of
pen or something. And that's of course pretty aware, I guess, right? But it's possible that with
the eye movements, this sort of comes so natural to us that we don't even realize that we're engaging
kind of in this kind of process. But yeah, I was very happy, you know, when I saw this happening.
And so this is anecdotal in a sense, it's just one video clip, right? But we can also look at
more generally, sort of analyzing the differences in the eye movements that people are producing
between these different experiments. And what I'm showing here is just looking at
the saccades that participants are producing. So those are fast eye movements jumping from
one point to another. And then I look at the endpoints of those saccades. And I look at where
those fall, right? And I took into account only the time between ball A and ball B coming into the
scene. And before basically, when they collide with one another, that time window. And then we
see that on this, for the causal question, a lot of those saccades basically end up along the path
right that ball B would have taken if ball A hadn't been there. Whereas in the other condition,
we see very few of these kinds of eye movements. So nice, I guess, even more direct evidence
that people are engaging in this kind of process and that they're doing it specifically
when asked to answer a causal question about the clip and sort of spontaneously.
There is this other part to it. But I think I will skip, so I have a little bit more time to,
let me see. Well, actually, I'll share it. Sorry about that. So there was another,
after we published this paper, there was another reviewer number two, as there often is.
And they were basically still saying, okay, well, this was for the eye tracking data. And they said,
that's nice. You're showing us these sort of eye movements. But they basically said that, okay,
these eye movements, they're happening before the balls are colliding with one another. And you're
calling it sort of counterfactual simulation. Counterfactual should mean it should be back
in the past. Going back in the past, evaluating that something would have been different,
and then seeing what difference that would have made. And they were saying, oh, what,
you should just call it the hypothetical simulation model instead, and not that. So we were able to
push back. But the reviewer also was right to some extent, I think. So this is a paper that I've
published quite recently, where I was trying to say that, no, you really need the counterfactuals.
So a lot of this has been like, yeah, you really need the counterfactuals. And then you just keep
getting some pushback, and you try to convince people even more so. So this was this reviewer
number two here. You haven't really shown us counterfactual simulation. Those looks are happening
before the balls are colliding. So his idea was, well, maybe what people are doing is they're kind
of simulating some hypothetical future. In this case, the hypothetical future is like,
what would happen if ball A wasn't there? And then they're storing that in their mind,
and comparing that to what actually happened at the end. And that's a slightly different
computation from the one that I think they're carrying out. And this relates to something,
again, here's Judea Pearl, this climbing on this kind of virtual letter here. Because he has argued
that there are these three different ways of thinking about the extent to which people have
causal knowledge of how the world works. On the lowest rung of the letter, and he often accuses
a lot of deep learning and so on to be on that rung, although it's a little unclear,
he calls that rung the level of association. So that's what you learn in the stats classes
correlation. When two things are associated with one another, and you can infer one variable from
the presence of another, so the normal conditional probability, PY given, I would say PY given X.
So what does some symptom tell me about the disease, for example? On the next level,
it's the level of interventional reasoning. That's the kind of when I do a randomized control
trial, for example, or if I'm, again, hypothetically reasoning, oh, what would happen if I were to do
this? What would happen if I were to do that? And that's sort of when your stats teacher tells you
causation and correlation aren't the same thing, that's often the thing that they then think about,
right? That like, oh, on the level of an experiment, now I'm performing an intervention,
randomly assigning people to different groups, and I can draw different kinds of causal inferences
from that information than when I just have observations. But then process ultimately,
the kind of the highest rung on the letter is reserved for counterfactual reasoning,
and that allows you to give specific answers essentially to why questions. So why did this
happen in this particular case? Like, you know, was it the aspirin that stopped my headache,
or would it have stopped anyhow, even if it hadn't taken the aspirin? Or, you know,
was Kennedy shot? Would Kennedy still have been alive if it hadn't been shot by
very heavy-ass world? And so essentially, now the question boils down to, do we need that third
level, like to explain people's causal judgments, or is the second one enough, right? So just to kind
of try and make it a little bit more clear, right? So the hypothetical, luckily in English,
also we have sort of a way of marking the difference between them. So here's an English
hypothetical. Would B go into the goal if A was removed? So what you'd be doing is taking the
time into account until they collide, simulating like a possible future, and then computing the
probability of that. Versus the counterfactual, what I'm doing, slightly different in English,
right, would B have gone into the goal if A had been removed? I sometimes, you know, regret
having gotten into counterfactual so much, so obviously not a native speaker, and the
counterfactuals are sometimes a little complicated, right, that you get the tenses right and so on,
but I think I've mostly gotten it down by now after like 20 years. So would B have gone into
the goal if ball A had been removed? So you're doing slightly different here now, right? You're
taking into account everything until the end, and you're now going back in time to do this
intervention and then think about how the world could have unfolded differently from how it actually
did. So now it turns out in this very simple setting here, that makes no difference. The
hypothetical probability and the counterfactual probability is the same because there's nothing,
there's only this one causal event happening, so it doesn't really come apart. So in a very simple
setting where you have one cause and one effect, essentially, you cannot tease the two apart,
but you don't need to make it much more complicated. It's sufficient if you just have one other
alternative event that you are initially uncertain about, and that will make it such that
now the hypothetical probability and the counterfactual probability will be different from one another.
So here was the genius invention, just putting like a little block again that you've seen earlier,
but this time the block is on rails into the scene, and that will now make it such that we
can tease these two different things apart. So here's an example. I'm not going to ask for
audience petition this time, but let's say that this was happening in the clip,
and now if you were asked to say, oh, did it prevent it from going into the goal?
My participants say in this case, yes, it did. And the idea is, why is it? Well,
because the block moved out of the way in time, such that Balbi would have gone through the goal
if Ball A hadn't been there. But you may have also noticed that the movement of the block
is happening after the balls collided with one another. So not something that you could have
sort of anticipated at this earlier moment in time, or at least had some uncertainty about.
So the basic idea here is to say like, oh, my hypothetical probability at the time
would Ball B go into the goal if Ball A wasn't there? Well, that's unsure. That depends on
whether or not the block's going to move. So I should give it like a 0.5 or something. I told
participants it's just as likely to move as it's not. Whereas for the counterfactual probability,
well, I know that it moved in this case. So I should be pretty certain that it would have
gone in if Ball A had been removed. So now I have a way basically of teasing the two apart
and can see which one better explains the causal judgments. Is it the hypothetical judgments
that I ask participants to do, or is it the counterfactual judgments that I ask another group
to do? And then I ask one group to give causal judgments and then just try to relate them to
one another. And what I find is when I look at the hypothetical, so maybe I should say a little
bit more about that plot here, at the bottom, it basically shows you the initial configuration
of the block. Was it in the way or not? And then did it move yes or no? So in this example here,
it's one where it was initially in the way, but it moved. But in the hypothetical condition,
you don't know that because you only see it until they pause. And then if you look at the
hypothetical judgments, they think when it's initially in the way, they think it's a little
less likely that it's going to go in. And when it's initially out of the way, they think it's a
little bit more likely. So they're sort of a little bit sticky in terms of what actually happened.
For the counterfactual probabilities, pretty much only the final state is what matters.
If it was out of the way at the end, you think, yeah, it would have gone in. If it was in the way
at the end, you think it would have missed. And now if you ask people to make causal judgments
in this case, we see that they align very closely with the counterfactual ones and not with the
hypothetical ones. And this was for the kind of missed cases, but the same story again holds
essentially for the causal cases too. So they think that it caused it when the block would have been
in the way at the end, and they don't think that it caused it when the block was would have been
out of the way at the end. So enough to make this review too happy, but maybe not Michael.
I'm a happy guy. I'm curious. Can you go back one slide? Just to make sure I understand.
There were two things that changed in that intervention. There was the question you asked,
the hypothetical versus the counterfactual. And it also sounds like the changes in how far they
saw into the video. That's right. That's right. And I'm picturing the counterfactual situation
where if you ask me the hypothetical question, but showed me the full video, so I see a whole video
and then you say, would be going into the goal if A was removed? I don't know.
Yeah. Yeah. It's a tricky one. I mean, I guess, you know, you'd have to ask them, like, what did
you think? I guess sort of at the time, right? Like before it happened, did you think, and people
are often bad at that, right? We know that from all the hindsight research and so on, that they
have difficulty putting themselves back into the epistemic state, I guess that they had at an
earlier point in time, right? So I'm not exactly, I haven't tried that one. I haven't tried showing
it until the end, but then asking them the hypothetical question, it's possible, of course,
that they will confuse it like as a counterfactual question, right? And, but for me, it was still
sufficient, I guess, at least to address this reviewer's concern, because his idea was really,
yeah, that computation is happening earlier, right? It's happening before the causal event of
interest, and then you're storing the output of that computation, in this case, the hypothetical
probability, and then just comparing that to what actually happens at the end, right? So it still
felt that it's addressing that, but yeah. Okay, so having these two things helps teasing them apart.
Okay, I'll sum up the first part, and then the second part will be short, but that's okay.
So for this counterfactual simulation model, what I've showed you that there's this sort of nice
correspondence between people's beliefs about the relevant counterfactual and the causal judgments
that they make, that it looks like that these things are necessary, which you can show with the
teleport or with the, with the brick in and out of the way, that people spontaneously engage in
this kind of counterfactual simulation as evidence to the eye movements, and that it's
counterfactuals really and not hypotheticals that seem to be important for expanding causal judgments.
We've played around with this model like a little bit more. Once you have a hammer, right,
you find all the nails. So this one is just like looking at slightly more complex cases.
This one here, philosophers love, because it's a case of, let me show it again, maybe a case of
double prevention, where B prevents ball A from preventing ball E from going through the gate,
right, because knocks it out. It happens in, maybe in football, probably happens often when
one tackles like another person that would have tackled the person running with the ball, right.
And so you might say, oh, to what extent did that cause it? You can also look at omissions when
nothing is happening. So ball A is just chilling here in the corner, and you might still ask,
oh, did it go in because ball A didn't hit it, right? And now you could imagine, well, if it had hit
it, what would have happened in this case? And we can also look at cases where really
nothing is happening at all. So here's just a tower of blocks, right, and you might still wonder,
oh, to what extent is this black one here responsible for the other one staying on the table?
And even though, yeah, there's nothing happening, right, you might still say, well,
how do you answer this question? Maybe by doing something like playing Jenga in your mind, right,
imagining it being removed, and then what would have happened to the scene? So that even just
physical support in some sense is very closely related to ideas of causation, right. What it means
to support is essentially to prevent something from falling. Okay, so that was part one. Now a
sort of short version of part two. And so responsibility attribution was something that I've
been into for quite a while and was also my motivating thing. And then I drifted off into
causality world mostly just because physics engines were around at the time. So it was like, oh,
now I can use those. And with around at the time, I mean, I was a postdoc with Josh Tenenbaum back
then and physics engines were all the rage at the time. And I said, okay, now I'll also use them.
And there aren't really yet, although I guess Michael is working on it, psychology engines,
right, that is easy where you could just have agents and think about what they would have done.
So this work that I had done on responsibility attribution wasn't particularly social, also
didn't really involve simulation, I think. And there was one experiment that got a little bit
closer that I'll briefly share with you here on a paper called Moral Dynamics. And it will look
very billiard ball world like I haven't moved too far away from the billiard balls, but this kind
of that's somewhat agentive, right? So and so we could show people like a video clip like this
and then ask them, what about extent do you think that blue was responsible that the green one got
harmed in this case here? And our inspiration here came from a paper called Moral Kinematics where
they basically argued, again, it's somewhat more actualist view and saying, okay, there's certain
features that people are picking up on in these scenes, like the duration of contact,
how far things moved and things like that. And then they directly mapped from these features
of the scene to the moral judgment in this case. And we liked the general setup, but didn't really
like that model like as much. So we proposed another model that has a slightly different title,
Moral Dynamics instead. And we thought, okay, these features are important, but the features are
important in that they give us evidence for the latent variables and that those are ultimately
the ones that I care about. And in this case, what are the latent ones that we thought one,
not very surprisingly here on the right hand side causality, but did you think that it actually
caused it, you know, to for this negative outcome to happen. And then the left side,
the intuitive psychology part, very kind of minimal in this case here. But it's basically
saying like, well, maybe these features give you some evidence about like how much the agent
actually wanted to bring about this negative outcome. So if you think, for example, if somebody
really wants something to happen, then they're willing to incur a larger cost to make it happen.
Putting a lot of effort, for example. So if somebody puts in a lot of effort into something,
you know that they must have really valued it. And if somebody really valued some negative outcome,
well, that's a bad thing. That was roughly the idea here. And we could then show that if we have
a model that just basically infers the amount of effort that some agent exerted and tried to map
that onto the responsibility that worked kind of, you know, okay-ish. If we only took into account
the causal role that some agent played and tried to use that to explain the extent to which
they're held responsible, that worked okay-ish. But if we now took a model that takes both of
these components into account, that worked pretty well, which was roughly in line with this kind
of unsurprisingly, now this framework that I laid out at the beginning, right, that when we assign
responsibility to others, we don't just care about the causal role that they played, but also what
the action tells me about the kind of person that they are. In this case, the action tells me
something about the desire that they had to bring about this negative outcome.
Okay. But still, we didn't really have a real model of agents in this case. We still sort of
basically just use the physics engine. Also, we weren't able to talk about intentions, and it's
clearly important often when people talk about responsibility. And also still our kind of factual
simulation here was basically purely physical, just seeing how this thing would have moved
without the other one. So I don't have the skills to make it happen with sort of more
agentive agents, but luckily now that I'm here, I get to work out with all these smart people,
and here's my PhD student, Sarah Wu, and our research assistant, Shruti Sreeta,
and they've looked into cases now that are a little bit more agentive. They're still kind of in
in grid world, but at least now planning and intentions and things like that are involved.
And here's the basic setup. So this is inspired by some previous work that has looked into helping
and hindering as a case study. And what they did is essentially they said, well, what it means
for somebody to intend to help someone is that their utility function includes the other person's
utility with a positive sign. Intending to help just means wanting to bring positive utility,
at least in this framework, to the other person. And intending to hinder puts a negative sign,
like now I want it that the other person is a low utility. So it turns out though that
intending to help or hinder versus actually helping or hindering is not necessarily the same
thing. So here's an example. I don't have a child yet, but at some point maybe we'll have a child,
and then if I go grocery shopping with the child, there probably will be a period of time where
they're not actually helping. They're sort of like trying to help, but kind of making it worse,
at least in terms of efficiency and so on. It's going to take longer. Of course, it's useful
because eventually they will be helpful. I have to go through that process just like a PhD student.
So yeah, so you go through that process, and then you might intend to be helpful,
but it might take a little bit of time to actually be helpful. And the claim is to evaluate that,
you need counterfactuals again to tell, oh, is the person actually helpful? Well,
how would it have happened without them, essentially? Or there's different counterfactuals
to consider, but that's one of them. So here's our grid world that we played with,
with the helping and hindering setup. So we have this red guy here who wants to get to the star,
has a pure physical goal in this case, just to get to that location. Then we have this blue one
who has a pure social goal. They either want to help or hinder the red one from getting there.
And then there are these walls here that you can't do anything about, but there's also these blocks,
and only the blue one can interact with these blocks. They can push, pull them out of the way.
So here's our Hollywood clip of what's happening in this situation.
Okay, so in this case, happy end, like a Hollywood movie, red made it,
and then we can show people these clips and we can ask them, oh, how responsible was the
blue player for the red player's success, for example, in this trial? We can also ask them
a counterfactual question, right, would the red player still have succeeded even if the blue player
hadn't been there? And we can ask them to make an inference about the intention of the blue one
in this case. What was the blue player intending to do? Were they trying to help or were they
trying to hinder? Definitely help, definitely hinder. So the idea is now basically the same as earlier,
by just saying, okay, again, we need some kind of generative model of the domain. In this domain,
now it's a model of agents basically planning and recursively reasoning about one another, right?
And that's now our probabilistic program. And we can again compute counterfactuals over that,
maybe in this case thinking, well, what would have happened if the blue one hadn't been there?
And then thinking how the red one would have planned their path differently, but without the
presence of blue, that's a rough idea. So again, we take some actual situation and we can then
simulate what would have happened in the relevant counterfactual situation in this case where blue
hadn't been there. We can talk later if you like about other counterfactuals you might consider,
but we just went with this one here, but what if they hadn't been there? In this case, yeah,
they wouldn't have made it because the block was in the way, right? We also have a model of
intention inference, but I'll sort of skip that. It's basically just saying, okay, if you have a
generative model about what an helping or hindering agent would do, you can then condition on the
observations that you see them acting and see what's more likely that they were helping or
hindering given the actions that they carried out. So I'll just give you a few more examples of the
sort of video clips that we showed to participants. That's a diagram of the one that you've just seen.
Here's another one where kind of, you know, blue is sort of extra mean, you might say. There was
already a block in the way, but they put another block in the way. What the heck? Yeah, really trying
to be helpful through adversarial actions. So here's another one here where blue is sort of
laudably helpful, but like, you know, was not really necessary, but maybe looks nice.
Here's a case in which sort of things go wrong.
Where blue was maybe trying to be helpful, but actually sort of made it worse, you know,
the reactions that they took. And then here's another one. We had a large number, so I'm just
showing like a subset of them. So this is one where blue could have easily hindered if they had
wanted to, but didn't, because they could have just pushed it into the way. And so then we now
have to again, yeah, try to capture whether we can, with our model, capture the counterfactual
judgments that people are making. And we sort of can, there's not as much kind of variance here,
at least in the predictions of the model. So this model is sort of okay-ish. It captures the trends
overall, but there's more variance in people's judgments that is not quite captured by the
model yet. So we're still, this is sort of more ongoing work. In terms of intention inference,
it's fine. So it can also kind of infer whether the person was helping or hindering,
but also here, what you see is stuff are bunched up that the model all gives a hundred to,
where there's still some differentiation that people make, but sort of mostly captures what's
going on. And if we now look at the responsibility judgments, and we try to do the same thing
initially that we did with the billiard balls earlier, that we just take the counterfactuals,
like on the x-axis, and try to predict the responsibility here on the y-axis, it's okay-ish,
but not, you always want, when you do computational modeling, you always want them
nicely line up on the diagonal. And that's not really what was happening in this case,
whereas for the billiard balls, we have this very simple counterfactuals nicely predict
the causal ratings. But if again, if you have a model that incorporates also the intention
inferences, like into the predictions, now they do sort of more nicely line up on the diagonal.
Again, suggesting that when it comes to assigning responsibility for agents,
it's not just the causal role that matters. It also matters what the actions that they took
tell me about the kind of person that they are. In this case, it tells me something about
their intentions, like they try to be helpful, or that they try to be hindering. So the both of
these components. And just to give you a sense of an example where we need this kind of intention
part, like that's back to that mean one where the blue one pushes another one into the way, right?
And so just to help you kind of interpret the bars here, the counterfactual, that's the condition
where we asked them, would red have succeeded if blue hadn't been there? That's basically our
causal model. And they don't think so, right? The pink, pink, purplish one is like very low,
right? But also when we asked them what the intention of the blue one is, they think, yeah,
was really hindering. So here zero means hindering and 100 means helping. So they think, yeah,
they were hindering. So even though they say that, yeah, the blue one didn't really play a causal
role, they still give them quite a bit of responsibility, like in the blue one on the
right hand side. So that's one case, at least, where currently we need this other part. So they
think, yeah, blue blue's actually make no difference, but they were definitely trying to
hinder. And so, yeah, I still give them some responsibility for this outcome.
Okay, so sort of almost last slide. Because we have these agents like recursively thinking about
one another, an interesting setting that also can happen here is that you can actually hinder or
help one another, again, maybe also like in the, in the advisor, advisor setting, not by actually
making any change to the physical world, but changing somebody else's belief. So I just want to
show you that example. And maybe you'll get that intuition from the setting here.
So very, very mean, very, very sad. Because it looked really like blue was going to help,
right? And then they didn't, right? And here's just one participant, what they're saying,
oh, blue tricked red into thinking she was going to move the box to help. But then
once red was stuck on the side of the wall, blue left the box where it was, very sad, you know.
And a lot of people say something along those lines. We also had one condition where we just
have them give explanations of what happened, right? And here the interesting part, right,
is that the hindering is not happening because blue changed anything about the world. They didn't
move a block in the way or something, but they hindered because they made red believe that
they were going to be helpful and then they weren't, right? Here, if blue hadn't been there,
red would have just walked along on the outside and they might have made it, you know, anyhow,
even without blue. And this happens because they're recursively thinking about one another,
right? And red things like, oh, blue is taking actions that are going to help me
so I can take the shortcut. And then it turns out I couldn't in this case. Okay, wrapping up.
So this was the second part where we, I guess, applied this model now to at least a simple
setting where agents are interacting with one another, helping and hindering one another,
that in order to judge whether somebody helped or hindered, I again think that you need this
process of counterfactual simulation and that responsibility judgments are sensitive both
to the cause of the world that somebody played and what the actions tell us about the kind of
mental state that they had. Just to conclude, so together, hopefully, this sort of set of studies
gets some evidence that people seem to be constructing these rich mental models of the
world that we can get evidence for in different kinds of ways, like through eye tracking and other
tools. By imagining interventions like on these mental models, those allow us to compute the
counterfactuals, which I think are important for assigning responsibility, giving explanations
and so on. And that this counterfactual simulation model that I've been kind of developing can then
be relatively flexibly applied to physical and social events, where you think that the main
thing that's happening is that your model of the world changes and maybe the exact
counterfactual cooperation that you're carrying out changes, but otherwise the framework sort of
holds. So with that, I want to thank the main people who helped me do this kind of work,
and then maybe you for your attention. And there's a little bit of time for questions.
So one thing I'm curious is, I assume notions of causality are probably somewhat universal,
but especially issues of moral judgment, intention are likely dependent to some extent on
environmental factors, cultural factors, those kinds of things. And so I'm curious if you've
either observed those in your experiments or if you have some way of controlling for those
factors when you recruit participants. Yeah, so that's an interesting question. And I think
even notions of causality actually, there are cultural effects like who you see there. So
when making causal judgments, there's often, there's basically like in many cases,
what's called the problem of causal selection, how do I even decide what thing to pick out of
as the cause in the first place. In my setting, very often I've kind of made it pretty easy,
and I've sort of constrained it because I already told you like these are the possible causes,
but in the real world it's not like that. And it's sometimes, we may see something,
we may see a person as a cause, or we may see a system as a cause, or we may also
see the kind of counterfactuals that may come to mind to us may also depend on what our background
is. And it often tells us something about, oh, when somebody then gives a certain counterfactual,
it tells us quite a bit about them. So this comes up in the context, for example, also of victim
blaming. Like if that's the counterfactual that came to mind to you, oh, that tells me something
like about you. So I would say that even in that context, there are strong kind of interpersonal
and cultural effects that affect how we attribute causality. Now when it comes to intention inferences,
I'm not sure that that process in and of itself, that at least to me feels relatively
whatever universal, that we kind of, we have to engage in that all the time by trying to
predict what other people are intending in the way that we interact with them. But then again,
how maybe then judgments in this case of responsibility or morality like draw on these
different components, for example, that I've laid out here, no claim that that is in any sense
sort of universal. But it might very well be that in certain cultures like this kind of what I take
here to be more the person component, right, may have a stronger influence on responsibility
judgments and in others, it might mostly be about causality. I certainly in my experiments
for individual participants see a lot of variance along the lines. But there's some people that
don't care about even the intention part at all. They just say like, oh, when it's about responsible,
I just look at what would have happened if they hadn't been there. And then other participants,
judgments are suggesting that they care about the intention part much more. But I have not yet
engaged in the kind of work that then tries to explain why is it, why is it that this person
casts so much about causality and why is it that this person casts so much about the intention
part, for example. Thank you. I'm going to hug the mic actually. I'm interested, like, do you think
this model applies to other settings? Because all of the examples were sort of like physical or
agents taking physical actions. So if you had just like a verbal description of some social
scenario where there's like speech acts that are causing things, do you think it would work the same?
Yeah. Yeah, that's a great question. So would it work the same? So my sense is like, yeah, in a
similar way, so there's a number of things here, I think. So we have applied the model a little bit,
like this kind of counterfactual simulation model in the physical world, also two speech acts.
And there it's in the context of like, we were basically jealous of, you know,
for those of you who remember full wolf, you had these different words, right? And we were like,
oh, our model can only do like cause and prevent. That's kind of sad. But there's other causal
expressions, of course, right? Enabling, affecting, letting, allowing, and so on. And it's going to be
a little bit of a of doing, but I'll get there. So we were trying to see to what extent this
framework that we have could also allow us to explain differences between these different
expressions, right? And, and this also comes up, you know, in philosophy, like even questions,
so the question versus killing versus causing to die, even people like in cases of abortion,
you know, the way that you talk about it, right? Again, reveals something, you know,
how you think about it. And in general, right, like this distinction also, when you have that as
an alternative that you could have said killing, but you chose causing to die, it suggests maybe
a more roundabout way in which something happened, right? Like the person killed it,
caused them to die. You think, yeah, it would be weird to say that someone caused them to die
when they like, you know, directly walked up to them and, you know, shot them. This also came
up recently or still coming up these days, actually, with the case of Alec Baldwin, Rust,
like in the movie, right? The way that people talk about it was it will hold the gun that
discharged or something rather than, you know, shot the person, right? So it matters a lot,
basically, like in this case, the choice of word, right? And the, and the, in some sense,
the counterfactual alternatives you could have had, right, for them, the image that it's creating
in the listener in this case, right? So the fact that, oh, you chose this expression suggests to me
that the scenario must have been such, like rather than such. So that's at least the minimal way,
I think, in which it applies also to, to thinking about speech acts, right? And thinking like,
yeah. And of course, you could think like, oh, you know, again, take the advice example, would the
students still have done that if I had not said that, right? So we are obviously causing each other
a lot in the way that we talk to each other. And sometimes, you know, yeah. Also, of course,
after talking, I might think like, oh, I wish I had, I had answered this question from differently
than what I actually did. And I regret it, right? And things like that.
Yeah. So on a similar note, I'm wondering if you have thoughts on how possible it would be to
use this model on society, large scale societal events, their divisive, such as what cost a
person to be elected, what cost code outbreaks, or what causes climate change, like how possible
would it be to apply this to those events and also what challenges you foresee? Yeah. Yeah,
that's a great question. And, and so, so I had the example of example at the beginning with like,
oh, did the fall of Lehman Brothers caused the financial crisis, right? That's sort of like,
large scale. And I don't know, right? And, and, and partly it might, so, and there's a few options,
right? One is like, okay, just like totally punting, right? And saying like, okay, well, if the system
gets sufficiently complex, such that I cannot carry out the relevant counterfactual computation
anymore, well, I just don't know, right? I cannot give that causal answer. That's, that's one version,
right? And there's another version where you say like, okay, well, to the extent that I can maybe,
you know, abstract away from a lot of the lower level details that say of some, so if I'm, if I'm,
if I have the capacity to build maybe a more abstract model, which, which I can now simulate,
right, then I might be giving you an answer sort of on that level, right? And so, but then it's
also half punting, right? Because now you have to kind of come up with a good model of how people
generate the right kind of causal abstractions for some situation that then allow them to compute
the counterfactual, because now it's not messy anymore, right? And another thing that I should
mention, and that quite a lot of the work on responsibility that I've, that I've looked at
particularly in groups, the sort of situations that you pointed out, like elections and, you know,
global warming, they're, they're characterized by, by large degrees of over determination,
right? Like in election, you hardly ever cast a pivotal vote, right? And, and so those also
traditionally were problems for counterfactual accounts, right? Because everyone can say like,
I made no difference, like if I fly every day, you know, that's not really going to make a difference.
And so, and there you can, and similar with election, why should I go vote, right? Because
if my vote's not going to make any difference, right? And there at least models have been built
that then say like, okay, well, it's not, you're not off the hook, right? It's basically saying,
even if you would not have made a difference in this particular situation,
maybe the degree of responsibility that you have for some election, for example,
maybe related to how close you were to making a difference to the outcome, right? If it's like,
if the outcome is 6-5, you feel very responsible. If it's like 7-4, a little less. If it's like
8-3, a little less, right? But not, but it shouldn't go to zero, right? And then, and then as it,
maybe now relates to kind of, you know, global warming and so on, part of the challenge then
from the more like, you know, what do we do about it? Side might be like, okay, how do we make it
such that people don't perceive a sort of, you know, going to zero sense of responsibility,
right? Such that you feel like actually the actions that you do make a difference to the outcome.
And so, yeah, so that's, so I think a mix of thoughts, I guess, in response to your question.
So we're about it, time. Is there a reminder if you are here? If you're logging attendance,
make sure to grab one of these code words up at the front and give Toby a compliment on his
talk on your way out, maybe make you come up next door. Let's thank our speaker.

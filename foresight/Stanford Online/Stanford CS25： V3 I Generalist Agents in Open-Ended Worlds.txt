Hi everyone, thanks for coming to our CS35 lecture today.
So today we're honored to have Jim Van from NVIDIA, who we're talking about generalist
agents in open-ended worlds, and he's a senior AI research scientist at NVIDIA, where his
mission is to build generally capable AI agents with applications to gaming, robotics, and
software automation.
He has research spans, foundation models, multi-modal AI, reinforcement learning, and
open-ended learning.
He obtained his PhD degree in computer science from here, Stanford, advised by Professor
Pepe Lee.
And previously, he did research internships at OpenAI, Google AI, as well as Mila Quebec
AI Institute, so yeah, we'll give it up for Jim.
Yeah, thanks for having me.
So I want to start with a story of two kittens.
It's a story that gave me a lot of inspiration over the career, over my career.
So I want to share this one first.
Back in 1963, there were two scientists from MIT held in Hine.
They did this ingenious experiment, where they put two newborn kittens in this device,
and the kittens have not seen the visual world yet.
So it's kind of like a merry-go-round, where the two kittens are linked by a rigid mechanical
bar, so their movements are exactly mirrored.
And there's an active kitten on the right-hand side, and that's the only one able to move
freely, and then transmit the motion over this link to the passive kitten, which is
confined to the basket, and cannot really control its own movements.
And then after a couple of days, held in Hine, kind of take the kittens out of this merry-go-round,
and then did visual testing on them.
And they found that only the active kitten was able to develop a healthy visual motor
loop, like responding correctly to approaching objects or visual cliffs, but the passive
kitten did not have a healthy visual system.
So I find this experiment fascinating, because it shows the importance of having this embodied
active experience to really ground a system of intelligence.
And let's put this experiment in today's AI context right.
We actually have a very powerful passive kitten, and that is ChargerBT.
It passively observes and rehearses the text on the internet, and it doesn't have any embodiment.
And because of this, its knowledge is kind of abstract and ungrounded, and that partially
contributes to the fact that ChargerBT hallucinates things that are just incompatible with our
common sense and our physical experience.
And I believe the future belongs to active kittens, which translates to generalist agents.
They are the decision makers in a constant feedback loop, and they're embodied in this
fully immersive world.
They're also not mutually exclusive with the passive kitten.
And in fact, I see the active embodiment part as a layer on top of the passive pre-training
from lots and lots of internet data.
So are we there yet?
Have we achieved generalist agent?
Back in 2016, I remember it was like spring of 2016.
I was sitting in an undergraduate class at Columbia University, but I wasn't paying attention
to the lecture.
I was watching a board game tournament on my laptop, and this screenshot was the moment
when AlphaGo versus LisaDoll and AlphaGo won three matches out of five and became the
first ever to be the human champion at the game of Go.
I remember the adrenaline that day, right?
I've seen history unfold.
Oh my God, we're finally getting to AGI, and everyone's so excited.
And I think that was the moment when AI agents entered the mainstream.
But when the excitement fades, I felt that even though AlphaGo was so mighty and so great,
it could only do one thing and one thing alone.
And afterwards, in 2019, there were more impressive achievements, like Open AI 5 beating the human
champions at the game of Dota and AlphaStar from DeepMind beat StarCraft.
But all of these, with AlphaGo, they all have a single kind of theme, and that is to beat
the opponent.
There is this one objective that the agent needs to do, and the models trained on Dota
or Go cannot generalize to any other tasks.
It cannot even play other games like Super Mario or Minecraft.
The world is fixed and has very little room for open-ended creativity and exploration.
So I argued that a journalist agent should have the following essential properties.
First, it should be able to pursue very complex, cementally rich and open-world objectives.
Basically, you explain what you want in natural language, and the agent should perform the
actions for you in a dynamic world.
And second, the agent should have a large amount of pre-trained knowledge instead of
knowing only a few concepts that's extremely specific to the task.
And third, massively multitask.
A journalist agent, as the name implies, needs to do more than just a couple of things.
It should be, in the best case, infinitely multitask, as expressive as human language
can dictate.
So what does it take?
Correspondingly, we need three main ingredients.
First is the environment.
The environment used to be open-ended enough because the agent's capability is upper-bounded
by the environment complexity.
And I'd argue that Earth is actually a perfect example because it's so open-ended, this world
we live in, that it allows an algorithm called natural evolution to produce all the diverse
forms and behaviors of life on this planet.
So can we have a simulator that is essentially a low-fi Earth, but we can still run it on
the lab clusters?
And second, we need to provide the agent with massive pre-training data because exploration
in an open-ended world from scratch is just intractable.
And the data will serve at least two purposes.
One as a reference manual on how to do things.
And second, as a guidance on what are the interesting things worth pursuing.
GPT is only, at least up to GPT-4, it only learns from pure text on the web.
But can we provide the agent with much richer data, such as video walkthrough, or like multimedia,
Wiki documents, and other media forms?
And finally, once we have the environment and the database, we are ready to train foundation
models for the agents.
And it should be flexible enough to pursue the open-ended tasks without any task specific
assumptions, and also scalable enough to compress all of the multi-modal data that I just described.
And here language, I argue, will play at least two key roles.
One is as a simple interface to communicate a task, to communicate the human intentions
to the agent, and second as a bridge to ground all of the multi-modal concepts and signals.
And that train of thought landed us in Minecraft, the best-selling video game of all time.
And for those who are unfamiliar, Minecraft is a procedurally generated 3D voxel world,
and in the game you can basically do whatever your heart desires.
And what's so special about the game is that unlike AlphaGo, StarCraft, or Dota, Minecraft
defines no particular objective to maximize, no particular opponent to beat, and doesn't
even have a fixed storyline.
And that makes it very well-suited as a truly open-ended AM playground.
And here we see people doing extremely impressive things in Minecraft.
Like this is a YouTube video where a gamer built the entire Hogwarts castle block-by-block
by hand in the game.
And here's another example of someone just digging a big hole in the ground and then
making this beautiful underground temple with a river nearby.
It's all crafted by hand.
And one more.
This is someone building a functioning CPU circuit inside a game because there is something
called Redstone in Minecraft that you can build circuits out of it, like logical gates.
And actually the game is too incomplete.
You can, you know, simulate a computer inside a game.
Just think about how crazy that is.
And here I want to highlight a number that is 140 million active players.
And just to quote this numbering perspective, this is more than twice the population of
the UK.
And that is the amount of people playing Minecraft on a daily basis.
And you know, it just so happens that gamers are generally happier than PhDs.
So they love to stream and share what they're doing.
And that produces a huge amount of data every day online.
And there's this treasure trove of learning materials that we can tap into for training
generalizations.
You know, remember the data is the key for foundation models.
So we introduce MindDojo, a new open framework to help the community develop generally capable
agents using Minecraft as a kind of primordial soup.
MindDojo features three major parts, an open-ended environment, an international knowledge base,
and then a generalized agent developed with a simulator and a massive data.
So let's zoom in the first one.
Here's a sample gallery of the interesting things that you can do with MindDojo's API.
We feature a massive benchmark suite of more than 3,000 tasks.
And this is by far the largest open source agent benchmark to our knowledge.
And we implement a very versatile API that unlocks the full potential of the game.
Like for example, MindDojo supports multi-modal observation for action space, like moving
or attack or inventory management.
And that can be customized at every detail, like you can tweak the terrain, the weather,
block placement, monster spawning, and just anything you want to customize in the game.
And given this simulator, we introduce around 1,500 programmatic tasks, which are tasks
that have ground true success conditions defined in Python code.
And you can also explicitly write down spars or the best reward functions using this API.
And some examples are harvesting different resources and unlocking the tech tree
or fighting various monsters and getting reward.
And all these tasks come with language prompts that are templated.
Next, we also introduce 1,500 creative tasks that are freeform and open-ended.
And that is in contrast to the programmatic tasks I just mentioned.
So for example, let's say we want the agent to build a house.
But what makes a house a house, right?
It is L defined and just like image generation.
You don't know if it generates a cat correctly or not.
So it's very difficult to use simple Python programs to give these kind of tasks reward functions.
And the best way is to use foundation models trained on Internet skill knowledge.
So that the model itself understands abstract concepts like, you know, the concept of a house.
And finally, there's one task that holds a very special status called play suit,
which is to beat the final boss of Minecraft, the ender dragon.
So Minecraft doesn't force you to do this task.
As we said, it doesn't have a fixed storyline.
But it's still considered a really big milestone for any kind of beginner human players.
I want to highlight it is an extremely difficult task that requires very complex preparation,
exploration, and also martial skills.
And for an average human, it will take many hours or even days to solve.
Easily over like one million action steps in a single episode.
And that would be the longest benchmarking task for policy learning ever created here.
So I admit, I am personally a below average human.
I was never able to beat the ender dragon.
And my friends laugh at me, and I'm like, OK, one day my AI will avenge my poor skills.
That was one of the motivations for this project.
Now, let's move on to the second ingredient, the Internet skill knowledge base part of my module.
We offer three datasets here, the YouTube, Wiki, and Reddit.
And combined, they are the largest open-ended agent behavior database ever compiled to our knowledge.
The first is YouTube.
And we already said Minecraft is one of the most streamed games on YouTube.
And gamers love to narrate what they are doing.
So we collected more than 700,000 videos with two billion words in the corresponding transfers.
And these transfers will help the agent learn about human strategies and creativities
without us manually labeling things.
And second, the Minecraft player base is so crazy that they have compiled a huge Minecraft-specific Wikipedia
that basically explains everything you ever need to know in every version of the game.
It's crazy.
And we scraped 7,000 Wikipedia pages with interleaving, multi-modal data, like images, tables, and diagrams.
And here are some screenshots.
Like, this is a gallery of all of the monsters and their corresponding behaviors,
like spawn and attack patterns.
And also, the thousands of crafting recipes are all present on the Wiki, and we scraped all of them.
And more complex diagrams and tables and embedded figures.
Now we have something like GPT-4V.
It may be able to understand many of these diagrams.
And finally, the Minecraft subreddit is one of the most active forums across the entire Reddit.
And players showcase their creations and also ask questions for help.
So we scraped more than 300,000 posts from Minecraft Reddit.
And here are some examples of how people use the Reddit as a kind of stack overflow for Minecraft.
And we can see that some of the top-golded answers are actually quite good.
Like someone is asking, oh, why doesn't my wheat farm grow?
And the answer says you need to light up the room with more torches.
You don't have enough lighting.
Now, given the massive task suite and internet data,
we have the essential components to build a journalist's agents.
So in the first mind-dozer paper, we introduce a foundation model called Minecraft.
And the idea is very simple.
I can explain in three slides.
Basically, for our YouTube database, we have time-aligned videos and transfers.
And these are actually the real tutorial videos from our dataset.
You see on the third clip, as I raise my axe in front of this pig,
there's only one thing that you know is going to happen.
It's actually someone said this, a big YouTuber of Minecraft.
And then, given this data, we train Minecraft in the same spirit as Open AI Club.
So for those who are unfamiliar, Open AI Club is a contrastive model
that learns the association between an image and its caption.
And here, it's a very simple idea.
By this time, it is a video text contrastive model.
And we associate the text with a video snippet that runs about 8 to 16 seconds each.
And intuitively, Minecraft learns the association between the video and the transcript
that describes the activity in the video.
And Minecraft outputs a score between 0 and 1,
where 1 means a perfect correlation between the text and the video,
and 0 means the text is irrelevant to the activity.
So you see this is effectively a language-prompted foundation reward model
that knows the nuances of things like forests, animal behaviors,
and architectures in Minecraft.
So how do we use Minecraft in action?
Here's an example of our agent interacting with a simulator.
And here, the task is to share sheep to obtain wool.
And as the agent explores in the simulator, it generates a video snippet
as a moving window, which can be encoded and fed into Minecraft,
along with an encoding of the text prompt here.
And Minecraft computes the association.
The higher the association is, the more the agent's behavior in this video
aligns with the language, which is a task you want it to do.
And that becomes a reward function to any reinforcement learning algorithm.
So this looks very familiar, right?
Because it's essentially RL from human feedback, or ROHF in Minecraft.
And ROHF was the cornerstone algorithm that made chatGBT possible.
And I believe it will play a critical role in Jonas agents as well.
I'll quickly gloss over some quantitative results.
I promise there won't be many tables of numbers here.
For these eight tasks, we show the percentage success rate over 200 test episodes.
And here, in the green circle, is two variants of our Minecraft method.
And in the orange circles are the baselines.
So I highlight one baseline, which is that we construct a dance reward function
manually for each task using the Mindoge API, it's a Python API.
And you can consider this column as a kind of oracle, the upper bound of the performance,
because we put a lot of human efforts into designing these reward functions
just for the tasks.
And we can see that Minecraft is able to match the quality of many of these,
not all of them, but many of these manual engineering rewards.
It is important to highlight that Minecraft is open vocabulary.
So we use a single model for all of these tasks instead of one model for each.
And we simply prompt the reward model with different tasks.
And that's the only variation.
One major feature of Foundation Model is strong generalization out of box.
So can our agent generalize to dramatic changes in the visual appearance?
So we did this experiment where during training, we only train our agents on
a default terrain at noon on a sunny day.
But we tested zero shot in a diverse range of terrains,
whether it's in daylight cycles, and you can customize everything in Mindoge.
And in our paper, we have numbers showing that Minecraft significantly beats
an off-the-shelf visual encoder when facing these kind of distribution shift
out of box.
And this is no surprise, right?
Because Minecraft was trained on hundreds of thousands of clips
from Minecraft videos on YouTube, which have a very good coverage of all the scenarios.
And I think that is just a testament to the big advantage of using
international data because you get robustness out of box.
And here are some demos of our learned agent behaviors on various tasks.
So you may notice that these tasks are relatively short, around 100 to 500 time steps.
And that is because Minecraft is not able to plan over very long time horizons.
And it is an inherent limitation in the training pipeline
because we could only use 8 to 16 seconds of the video,
so it's constrained to short actions.
But our hope is to build an agent that can explore and make new discoveries
autonomously, just all by itself, and it keeps going.
And in 2022, this goal seems quite out of reach for us.
Mindoge was June 2022.
And this year, something happened, and that is G4.
A language model that is so good at coding and long horizon planning,
so we just cannot sit still, right?
We built Voyager, the first large language model powered life on a learning agent.
And when we said Voyager lose in Minecraft, we see that it just keeps going.
And by the way, all these video snippets are from a single episode of Voyager.
It's not from different episodes, it's a single one.
And we see that Voyager is just able to keep exploring the terrains,
mine all kinds of materials, fight monsters, craft hundreds of recipes,
and unlock an ever-expanding tree of diverse skills.
So how do we do this?
If we want to use the full power of G4,
a central question is how to stringify things,
converting this 3D world into a textual representation.
We need a magic box here.
And thankfully, again, the crazy Minecraft community already built one for us,
and it's been around for many years.
It's called Mindflayer, a high-level JavaScript API
that's actively maintained to work with any Minecraft version.
And the beauty of Mindflayer is it has access to the game states
surrounding the agent, like the nearby blocks, animals, and enemies.
So we effectively have a ground truth perception module as textual input.
At the same time, Mindflayer also supports action APIs
that we can compose skills.
And now that we can convert everything to text,
we're ready to construct an agent on top of G4.
So on a high level, there are three components.
One is a coding module that writes JavaScript code to control the game bot,
and it's the main module that generates the executable actions.
And second, we have a code base to store the correctly written code
and look it up in the future if the agent needs to record a skill.
And in this way, we don't duplicate efforts,
and whenever facing similar situations in the future,
the agent knows what to do.
And third, we have a curriculum that proposes what to do next,
given the agent's current capabilities and also situation.
And when you wire these components up together,
you get a loop that drives the agent indefinitely
and achieve something like lifelong learning.
So let's zoom in the center module.
We prompt GD4 with documentations and examples
on how to use a subset of the Mindflayer API
and GD4 writes code to take actions given the current assigned task.
And because JavaScript runs a coding interpreter,
GD4 is able to define functions on a fly and run it interactively.
But the code that GD4 writes isn't always correct, right?
Just like human engineers.
You can't get everything correct on the first try.
So we developed an iterative prompting mechanism
to refine the program.
And there are three types of feedback here.
The environment feedback, like what are the new materials
you got after taking an action or some enemies nearby.
And the execution error from the JavaScript interpreter
if you wrote some buggy code, like undefined variable,
for example, if it hallucinates something.
And another GD4 that provides critique
through self-reflection from the agent state and the world state.
And that also helps refine the program effectively.
So I want to show some quick example
of how the critic provides feedback
on the task completion progress.
So let's say in the first example,
the task is to craft a spike mass
and GD4 looks at the agent's inventory
and decides that it has enough copper
but not enough Amherst as a material.
And the second task is to kill three sheeps to collect food.
And each sheep drops one unit of wool,
but there are only two units in inventory.
So GD4 reasons and says that,
okay, you have one more sheep to go, likewise.
Now, moving on to the second part.
Once Voyager implements a skill correctly,
we save it to our persistent storage.
And you can think of the skill library
as a code repository written entirely by a language model
through interaction with the 3D world.
And the agent can record new skills
and also retrieve skills from the library
facing similar situations in the future.
So it doesn't have to go through this whole program refinement
that we just saw again,
which is quite inefficient,
but you do it once you save it to disk.
And in this way, Voyager kind of bootstraps
its own capabilities recursively
as it explores and experiments in the game.
And let's dive a little bit deeper
into how the skill library is implemented.
So this is how we insert a new skill.
First, we use GPT 3.5 to summarize the program into plain English.
And summarization is very easy and GD4 is expensive.
So we just go for a cheaper tier.
And then we embed this summary as the key
and we save the program, which is a bunch of code,
as the value.
And we find that doing this makes retrieval better
because the summary is more semantic
and the code is a bit more discrete and you insert it.
And now for the retrieval process,
where Voyager is faced with a new task,
let's say craft iron pickaxe.
We again use GP3.5 to generate a hint
on how to solve the task.
And that is something like a natural language paragraph.
And then we embed that and use that as the query
into the vector database.
And we retrieve the skill from the library.
So you can think of it as a kind of in-context replay buffer
in the reinforcement learning literature.
And now moving on to the third part.
We have another GP4 that proposes what task to do,
given its own capabilities at the moment.
And here we give GP4 a very high-level
kind of unsupervised objective
that is to obtain as many unique items as possible.
That is our high-level directive.
And then GP4 takes this directive and implements
a curriculum of progressively harder challenges
and more novel challenges to solve.
So it's kind of like curiosity of exploration,
where it is our novelty search in a prior literature,
but implemented purely in context.
Yeah, if you're listening to Zoom, the next example is fun.
Let's go through this example together.
Just to kind of show you how Voyager works,
the whole complicated data flow that I just showed.
So the agent finds itself hungry.
It only has one out of 20 hunger bars.
So it knows GP4 knows that it needs to find food ASAP.
And then it senses there are four entities nearby.
A cat, a villager, a pig, and some wheat seeds.
And now GP4 starts a self-reflection.
Like, do I kill the cat and the villager to get some meat?
That sounds horrible.
How about the wheat seeds?
I can use the seeds to grow a farm,
but that's going to take a very long time
until I can generate some food.
So sorry, Piggy, you are the one being chosen.
So GP4 looks at the inventory, which is the agent state.
There is a piece of iron in inventory.
So it recalls, Voyager recalls a skill from the library
that is to craft an iron sword
and then use that skill to start pursuing,
to start learning a new skill, and that is Hunt Pig.
And once the Hunt Pig routine is successful,
GP4 saves it to the skill library.
That's roughly how it works.
Yeah, and putting all of these together,
we have this iterative prompting mechanism,
the skill library, and an automatic curriculum.
And all of these combine.
It's Voyager's no-gradient architecture
where we don't train any new models
or fine tune any parameters,
and allows Voyager to self-boostrap on top of GP4,
even though we are treating the underlying language model
as a black box.
It looks like my example work,
and they started to listen.
So yeah, these are the tasks
that Voyager picked up along the way.
And we didn't pre-program any of these.
It's all Voyager's idea.
The agent is kind of forever curious
and also forever pursuing new adventures just by itself.
So to quickly show some quantitative results,
here we have a learning curve,
where the x-axis is a number of prompting iterations,
and the y-axis is the number of unique items
that Voyager discovered as it's exploring an environment.
And these two curves are baselines,
a react and reflexion.
And this is auto-GPT,
which is like a popular software repo.
Basically, you can think of it as combining react
and a task planner that decomposes
an objective into sub-goals.
And this is Voyager.
We're able to obtain three times more novel items
than the prior methods,
and also unlock the entire texture significantly faster.
And if you take away the skill library,
you see that Voyager really suffers.
The performance takes a hit,
because every time it needs to kind of repeat
and relearn every skill from scratch
and starts to make a lot more mistakes,
and that really degrades the exploration.
Here, these two are the bird-eye views of the Minecraft map,
and these circles are what the prior methods
are able to explore,
given the same prompting iteration budget.
And we see that they tend to get stuck in local areas
and kind of fail to explore more,
but Voyager is able to navigate distances at least two times
as much as the prior works.
So it's able to visit a lot more places,
because to satisfy this high-level directive
of obtaining as many unique items as possible,
you've got to travel.
If you stay at one place,
you will quickly exhaust the interesting things to do.
And Voyager travels a lot,
so that's how we came up with the name.
So finally, one limitation is that Voyager
does not currently support visual perception,
because the GV4 that we used back then was text-only,
but there's nothing stopping Voyager
from adopting like multi-modal language models in the future.
So here we have a little proof-of-concept demo,
where we ask a human to basically function
as the image captioner.
And the human will tell Voyager
that as you're building these houses,
what are the things that are missing?
Like you place a door incorrectly,
like the roof is also not done correctly.
So the human is acting as a critic module of the Voyager stack.
And we see that with some of that help,
Voyager is able to build a farmhouse and another portal,
but it has a hard time understanding 3D spatial coordinates
just by itself in a textual domain.
Now, after doing Voyager, we're considering like, where else can we apply this idea
of coding in an embodying environment,
observe the feedback, and iteratively refine the program.
So we came to realize that physics simulations themselves
are also just Python code.
So why not apply some of the principles for Voyager
and do something in another domain?
What if you apply Voyager in the space of this physics simulator API?
And this is Eureka, which my team announced just like three days ago,
fresh out of the oven.
It is an open-ended agent that designs reward functions
for robot dexterity at superhuman level.
And it turns out that GD4-POS reinforcement learning
can spin a pen much better than I do.
I gave up on this task a long time ago from childhood.
It's so hard for me.
So Eureka's idea is very simple and intuitive.
GD4 generates a bunch of possible reward function candidates
implemented in Python.
And then you just do a full reinforcement learning training loop
for each candidate in a GPU accelerated simulator.
And you get a performance metric and you take the best candidates
and feedback to GD4.
And it samples the next proposals of candidates
and keeps improving the whole population on the reward functions.
That's the whole idea.
It's kind of like an in-context evolutionary search.
So here's the initial reward generation,
where Eureka takes as context the environment code
of NVIDIA's ISAC sim and a task description
and samples the initial reward function implementation.
So we found that the simulator code itself
is actually a very good reference manual
because it tells you, Eureka, what are the variables you can use,
like the hand positions here, the fingertip position,
the fingertips have safe, the rotation,
angular velocity, et cetera.
So you know all of these variables from the simulator code
and you know how they interact with each other.
So that serves as a very good in-context instruction.
So Eureka doesn't need to reference
any human return reward functions.
And then once you have the generated reward,
you plug it into any reinforcement learning algorithm
and just train it to completion.
So this step is typically very costly and very slow
because reinforcement learning is always slow.
And we were only able to scale up Eureka
because of NVIDIA's ISAC chain,
which runs a thousand simulated environment copies
on a single GPU.
So basically, you can think of it as speeding up reality
by a thousand lags.
And then after training,
you will get the performance metrics
back on each reward component.
And as we saw from Voyager,
GBT4 is very good at self-reflection.
So we leverage that capability.
There's a software trial reminding you to activate a license.
Yeah, so Voyager reflects on it
and then proposes mutations on the code.
So here, the mutations we found can be very diverse,
ranging from something as simple as just changing
a hyperparameter in the reward function weighting
to all the way to adding completely novel components
to the reward function.
And in our experiments,
Eureka turns out to be a superhuman reward engineer
actually outperforming some of the functions
implemented by the expert human engineers
on NVIDIA's ISAC same team.
So here are some more demos of how Eureka
is able to write very complex rewards
that lead to these extremely dexterous behaviors.
And we can actually train the robot hand
to rotate pens not just in one direction,
but in different directions, along different 3D axes.
I think one major contribution of Eureka,
different from Voyager, is to bridge the gap
between high-level reasoning and low-level model controls.
So Eureka introduces a new paradigm
that I'm calling hybrid gradient architecture.
So recall Voyager is a no-gradient architecture.
We don't touch anything and we don't train anything.
But Eureka is a hybrid gradient,
where a black box inference-only language model
instructs a wide range of functions
instructs a white box, learnable neural network.
So you can think of it as two loops, right?
The outer loop is great and free,
and it was, it's driven by GV4,
kind of selecting the reward functions.
And the inner loop is great and based.
You train like a full reinforcement learning episode from it
to achieve extreme dexterity using a specialized,
like training by training a special neural network controller.
And you must have both loops to succeed
to deliver this kind of dexterity.
And I think it will be a very useful paradigm
for training robot agents in the future.
So these days, when I go on Twitter or X,
I see AI conquering new lands every week.
Chat, image generation, and music,
they're all very well within reach.
But my dojo, Voyager, and Eureka,
these are just scratching the surface
of open-ended journalist agents.
And looking forward,
I want to share two key research directions
that I personally find extremely promising,
and I'm also working on it myself.
The first is a continuation of Minecraft,
basically how to develop methods
that learn from Internet-skilled videos.
And the second is multimodal foundation models.
Now that GV4V is coming,
but it is just the beginning of an era.
And I think it's important to have all of the modalities
in a single foundation model.
So first, about videos.
We all know that videos are abundant, right?
Like so many data on YouTube, way too many
for our limited GPUs to process.
They're extremely useful to train models
that not only have dynamic perception and intuitive physics,
but also capture the complexity of human creativity
and human behaviors.
It's all good, except that when you are using video
to pre-training body nations,
there is huge distribution shift.
You also don't get action labels,
and you don't get any of the groundings
because you are a passive observer.
So I think here is a demonstration
of why learning from video is hard,
even for natural intelligence.
So Little Cat is seeing boxers shaking their head,
and it thinks maybe shaking head
is the best way to do fighting.
All right, this is why learning from video is hard.
You have no idea, like why...
This is too good.
Let's play this again.
You have no idea why Tyson is doing this, right?
Like the cat has no idea,
and then it associates this with just wrong kind of policy.
But for sure, it doesn't help the fighting,
but it definitely boosts the cat's confidence.
That's why learning from video is hard.
Now, I want to point out a few kind of latest research
in how to leverage so much video for journalist agents.
There are a couple of approaches.
The first is the simplest,
just learn kind of a visual feature extractor from the videos.
So this is R3M from Chelsea's group at Stanford,
and this model is still an image-level representation,
just that it uses a video-level loss function to train,
more specifically, time-contrastive learning.
And after that, you can use this as an image backbone
for any agent,
but you still need to kind of find
to using domain-specific data for the agent.
The second path is to learn reward functions from video,
and MineClip is one model under this category.
It uses a contrastive objective between the transfer and video.
And here, this work, VIP, is another way
to learn a similarity-based reward
for goal-conditioned tasks in the image space.
So this work, VIP, is led by also the first author of Eureka,
and Eureka is his internship project with me.
And the third idea is very interesting.
Can we directly do imitation learning from video,
but better than the cat that we just saw?
So we just said, you know, the videos don't have the actions, right?
We need to find some ways to pseudo-level the actions.
And this is video portraying a VPT from OpenAI last year
to solve long-range tasks in Minecraft.
And here, the pipeline works like this.
Basically, you use a keyboard and a mouse action space,
so you can align this action space with the human actions.
And OpenAI hires a bunch of Minecraft players
and actually collect data in-house,
so they record the episodes done by those gamers.
And now you have a data set of video and action pairs, right?
And you train something called an inverse dynamics model,
which is to take the observation and then predict the actions
that cost the observation to change.
So that's the inverse dynamics model.
And that becomes a labeler that you can apply
to in-the-wild YouTube videos that don't have the actions.
So you apply IDM to like 70K hours of in-the-wild YouTube videos,
and you will get these pseudo-actions
that are not always correct, but also way better than random.
And then you're training imitation learning
on top of this augmented data set.
And in this way, OpenAI is able to greatly expand the data
because the original data collected from the humans
are high quality, but they're extremely expensive,
while in-the-wild YouTube videos are very cheap,
but you don't have the actions.
So they kind of solved and got the best of those roles.
But still, it's really expensive to hire these humans.
Now, what's beyond the videos, right?
I'm a firm believer that multimodal models will be the future.
And I see text as a very lossy,
kind of 1D projection of our physical world.
So it's essential to include the other sensory modalities
to provide a full in-body experience.
And in the context of in-body relations,
I think the input will be a mixture of text, images, videos,
and even audio in the future, and the output will be actions.
So here's a very early example of a multimodal language model
for robot learning.
So let's imagine a household robot.
We can ask the robot to bring us a cup of tea from the kitchen,
but if we want to be more specific,
I want this particular cup that is my favorite cup.
So show me this image.
And we also provide a video demo of how we want to mop the floor
and ask the robot to imitate the similar motion in context.
And when a robot sees an unfamiliar object like a sweeper,
we can explain it by providing an image
and showing this is a sweeper.
Now go ahead and do something with the tool.
And finally, to ensure safety, we can say,
take a picture of that room and just do not enter that room.
To achieve this, back last year,
we proposed a model called VIMA,
which stands for Visual Model Attention.
And in this work, we introduce a concept called multimodal prompting,
where the prompt can be a mixture of text, image, and videos.
And this provides a very expressive API
that just unifies a bunch of different robot tasks
that otherwise would require a very different pipeline
or specialized models to solve in prior literature.
And VIMA simply tokenizes everything,
converting image and text into sequences of tokens,
and train a transformer on top
to output the robot arm actions
autoregressively one step at a time during inference time.
So just to look at some of the examples here,
this prompt rearrange objects to match the scene.
It is a classical task called Visual Go Reaching
that has a big body of prior works on it.
And that's how our robot does it, given this prompt.
And we can also give it novel concepts in context.
Like this is a blanket, this is a work,
now put a work into a blanket.
And both words are nonsensical, so it's not in the training data,
but VIMA is able to generalize zero shot.
And follow the motion to manipulate this object.
So the bot understands what we want
and then follow this trajectory.
And finally, we can give it more complex prompt,
like these are the safety constraints,
sweep the box into this, but without exceeding that line.
And we would do this using the interleaving image
and text tokens.
And recently, Google Brain Robotics followed up after VIMA
with RT1 and RT2, robot transformer one and two.
And RT2 is using a similar recipe, as I described,
where they first kind of pre-train on internet scale data
and then fine tune with some human collected demonstrations
on the Google robots.
And RoboCAD from DeepMind is another interesting work.
They train a single unified policy that works not just on
a single robot, but actually across different embodiments,
different robot forms, and even generalize to a new hardware.
So I think this is like a higher form of multimodal agent
with a physical form factor.
The morphology of the agent itself is another modality.
So that concludes our looking forward section.
And lastly, I want to kind of put all the links together
of the works I described.
So this is mindodger.org.
We have open source everything.
Well, for all the projects where big fans are open source,
we open source as much as we can, including like the model code,
checkpoints, simulator code, and training data.
And this is Voyager.mindodger.org.
This is Eureka.
And this is VIMA.
And one more thing, right?
If you just want an excuse to play Minecraft at work,
then mindodger is perfect for you
because you are collecting human demonstration
to train generalization.
And there's one thing that you take away from this talk.
It should be this slide.
And lastly, I just want to remind all of us,
despite all the progress I've shown, what we can do
is still very far from human ingenuity as embodied agents.
These are the videos from our dataset
of people doing like decorating a winter wonderland
or building the functioning CPU circuit within Minecraft.
And we are very far from that as AI research.
So here's a call to the community.
If human can do these mind-blowing tasks,
then why not our AI, right?
Let's find out together.

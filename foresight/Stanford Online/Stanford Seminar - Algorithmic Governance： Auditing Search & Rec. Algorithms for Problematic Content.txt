So this work is part of a larger thread of work centered around algorithmic governance.
That is, how do we audit and measure problematic content like mis-disinformation, online extremism,
and how do we do that on online platforms, and then specifically the search and recommendation
algorithms driving these platforms.
So as I was preparing for this talk, I thought of opening it up in a slightly unconventional
way with this particular quote, which I'm going to read a little bit, which says, mankind
barely noticed when the concept of massively organized information quietly emerged to become
a means of social control, a weapon of war, and a roadmap for group destruction.
Any guesses as to which year of the world this quote describes?
Is it the world, is it a current world, 2020s, 2000s, 1900s?
Some people could guess just by the word mankind in previous talk.
Yeah, that's absolutely accurate.
So this is actually Edwin Black's description of the world of the 1930s, 1940s, during the
World War II, and the invention of the punch card, which was at that time the new wave
of automation and data collection.
And what is more surprising is that this quote is still true in today's 21st century world,
and we have seen many new scholarships, and I'm sure folks here have read and are familiar
with these books, many of these outline the harms of automation and data collection.
And more recently, this discussion has shifted to the harms posed by generative AI and large
language models.
So I'm going to read this quote a little bit more of what Edwin Black had to say in his
book, which is a lot of parallels to a present-day world.
So he goes on to say, the unique igniting event was the most fateful day of the last
century when Adolf Hitler came to power, but for the first time in history, an anti-Semayet
had automation on his side, so that was what was different, and the automation was in the
form of IBM punch cards.
So he goes on to say that IBM was self-criped by a special immoral corporate mantra, if
it can be done, it should be done.
So we should take a pause and kind of re-read this phrase, if it can be done, it should
be done, which is parallels very re-resemblance to the motives and culture practiced in modern-day
technology companies, and I'm sure being in the Silicon Valley, you all probably have
seen some of this while interning at some of these companies, move fast and break things,
done is better than perfect, what would you do if you weren't afraid.
And I argue that the sort of rushed culture, the sort of culture of disruption and speed
often poses a great challenges in governing these technologies and conducting thoughtful
medicalist audits, and in turn this is what makes algorithmic governance a difficult problem.
And I'll come back to these ideas later on towards the end of the talk, but let me first
dive into the meat of this presentation, a couple of studies which we did in this realm
of algorithmic governance.
So here I want to focus on two types of algorithms, search and recommendations, and two specific
platforms, YouTube and Amazon.
And our focus was only one type of problem, misinformation particularly.
Now you might wonder why search and recommendation algorithms, so now the world is moving towards
large language models, why we should focus on these old technologies, search and recommendations.
So users generally have an unwavering trust in search engines, so several scholarly work,
some of these cited here have actually shown that these ranking of search results have
really dramatic effect on users' attitudes, their preferences, their behaviors.
In fact, bias search rankings are so powerful they can even shift voting preferences of
undecided waters by so much so as 20%.
And users don't even show awareness of that kind of manipulation happening in their search
results.
So this should tell you how powerful search algorithms are, and then that is why we should
keep studying them despite the other new shiny technologies coming our way.
For recommendation algorithms, I hope I don't have to make a case here because you see those
almost everywhere starting from what recommendations, what movies you should watch, what products
one should buy, which campaign you should donate to, they are almost everywhere.
And they also have these reinforcing effect, right?
So the more you like, watch certain content, the more you share, the more you get those
types of content.
Now this might be harmless say when you're going out this weekend and trying to buy Christmas
decorations, but these things can get ugly quite quickly when say you are browsing for
vaccine information, health information or climate information.
Now why audit for misinformation in particular, right?
So currently there is this disproportionate focus on AI bias and fairness and tech journalist
Karen Howe captures this notion very well in her article where she mentions how often
responsible AI teams and companies are pigeonholed into targeting AI bias.
Now don't get me wrong, so bias and fairness are indeed important topics that one should
pursue, but tackling just AI bias draws away attention from fixing much bigger problems
of other types of harmful information such as misinformation, extremism, conspiratorial
content.
So this is what is underlying motivation behind the set of studies that we did.
And with that, let's dive into this first study, the YouTube audit work.
A major motivation for this YouTube study was coming from these frequent headlines that
I was noticing a few years ago, how YouTube is driving people in the internet's darkest
corners, and then there were these opinion pieces talking about YouTube being the great
radicalizer.
But YouTube was also responding with articles saying that they will be reducing conspiracy
theory recommendations or making it much harder to find those on the platform.
And all these questions and all these reports are really anecdotal, like empirically how
bad is this, right?
Do we really know that?
And this is what I studied to try to do, verify these anecdotal claims that does YouTube really
surface these problematic content.
And in order to do this, we conducted these systematic audits on YouTube search and recommendation
algorithms and we picked one type of problematic content, conspiracy theories.
Now what really is an audit, right?
I have talked about mentioned audit a couple of times so far, but how do you audit algorithms?
I'm sure some of you in the audience might be familiar with the concept of audit.
For those of you who don't know, I'm going to give you a quick definition through an
example and also say that this is a thriving field of research, lots of work has been done
in this space.
And this was one of the earliest example of audits coming from the social science world.
And it's also one of my favorite ones from this 2004 paper, where researchers conducted
this very clever field experiment to investigate employment discrimination.
That is, they audited the labor market for racial discrimination.
So what they did was they responded with fictitious resumes to help wanted ads in the Boston
and Chicago newspapers.
To manipulate the perception of race, what they did was they kept everything else in
each of these resumes constant, only they changed the names.
And the names were either very African American sounding names such as Laquisha or Jamal or
very white sounding names such as Emily or Greg, and hence the name of this paper.
The results showed that there was significant discrimination against African American names
while white names received 50% more callbacks for interviews compared to the African American
names despite everything else being constant in those resumes.
So this is a core idea behind audits, that is, you would keep everything else constant
and then you manipulate a single variable to determine how that change would affect
the algorithm.
So if you translate this into the context of YouTube, you would manipulate a variable
to determine whether the search and recommendation algorithm returns different results, say when
someone's age, their other demographic attributes, gender, their watch history, where they are
searching from, the geolocation, if that differs, what happens with the research results.
So to answer this, we set up this elaborate audit framework, which broadly looked like
this, where we had programmed bots, or in other words, we were conducting these sock
puppet audits.
So these were bots or sock puppets, which behave like normal users logging into YouTube,
running queries on the search platform while at the same time, a script at the back end
were collecting whatever search and recommendation results the platform was returning.
So we started with selecting search topics, and a goal here was to make sure that the
topics are indeed high impact, that is, lots of people are searching for these topics,
so they should be popular, and they should be topics which also have the potential to
return false conspiracies.
So we did some more background work, referring to Wikipedia, comparing with Google Trends,
and we came up with this list of five different topics, 9-11 conspiracies, vaccine controversies,
moon landing conspiracies, chemtrail, and then flat earth.
And then we audited three components of YouTube, the up next video, the top five recommendations,
and also YouTube's search results.
For demographics, we checked for four different age groups and two different types of gender,
male and female, and to emulate this, we had to create eight different sock puppet combination
accounts.
And then for geolocation, we found this hot and cold regions, that is, we call these
hot and cold regions because these are the regions which have the highest or the lowest
interest for that particular topic.
And we found these hot and cold regions comparing with Google Trends' interest over time graph.
So this is how it looked like for all the topics.
So for example, if you pick the flatter theories, Montana was a hot region or a high interest
showing region, while New Jersey is a low interest or cold region.
So once we had all these parameters, the demographic geolocation and all these parameters, we essentially
created bot accounts or sock puppets and programmed these accounts to keep firing queries on YouTube.
For geolocation, these bots fired queries from IP addresses of these locations.
Now one very important thing that we had to do for running these audits is, throughout
these audit experiments, you would have to control for noise to ensure that the effect
that you're observing is actually from the algorithm is not because of the noise that
might have been introduced while running the experiment.
So we controlled for browser noise by selecting one single version of Firefox browser.
We made sure that the YouTube searches are happening simultaneously to control for temporal
effects and so on.
So all of that audit run resulted in about 56,000, more than 56,000 videos capturing about
3,000 unique videos.
And then was the hard part, right, the manual annotations for this data set.
And I can go into detail of why we went with manual annotations and how we annotated if
anyone is interested in the Q&A round, but essentially this resulted in kind of three
sets of annotations promoting neutral and debunking.
And there was a lot of thought process that went into these annotation scheme, why these
three class made sense for this purpose and so on.
And then we performed statistical comparison tests to essentially find out what's the result
of these audits.
So let's look at some of these.
So what did we find?
We found that for brand new accounts, demography and geolocation do not really have an effect
on the amount of misinformation or the type of conspiracy theories that these platforms
are or YouTube is returning.
This is encouraging.
This is what we want the platform to do, right?
So it tells us that unlike those reports which were blaming YouTube for returning conspiracy
theories when it's a brand new account, turns out the demography and geolocation do not
really have an effect.
But once accounts builds a history by watching both demography and geolocation starts exerting
an effect on the recommendation for certain combination of topics, stances and component.
You might be thinking, okay, this is expected, right?
This is what we, how we think the platform would behave.
But turns out there are a little bit more nuanced results when we dig deeper into these,
the actual audit outcomes.
So for example, for the 9-11 topic, if the sock puppets watched YouTube videos promoting
line of conspiracy, you would end up getting more of these promoting videos in the recommendations.
But if the topic is something different, so for surprisingly for vaccine topic, the effect
was completely opposite.
If you watch anti-vaccine videos, YouTube ended up recommending you debunking videos
in the up next and top five recommendations.
So this, at least from these observations, it tells us that YouTube in some way is handling
misinformation in a much more reactive way.
It's modifying its search and recommendation algorithm selectively based on what reactions
is getting from the media and technology critics.
So we know that there was a lot of pushback for vaccine-related misinformation, and it
appears that they have gone and fixed that, but they have not done that universally for
other problematic topics.
We also found that certain demographics were prone to conspiracy video recommendations.
So for example, among eight of those demographic cases in all but one case, men accounts, that
is, bought accounts who had gender set as male were recommended more misinformation
videos.
And perhaps more surprisingly, what we found that in four of these cases, men accounts
who actually ended up watching neutral videos got significantly higher misinformation video
recommendations.
Now this is really problematic.
It implies that the algorithm was actually recommending pro-conspiracy videos even when
the user, in this case, Stockpuppet, was watching neutral videos on the topic.
What could this mean for real users?
This means that recommending promoting videos to men who are already drawn to neutral information
for that topic, but have not yet developed pro-conspiracy beliefs, but now has a higher
chance of developing that because the platform is returning these promoting conspiratorial
videos.
So wrapping up this work, the key contribution of this study was that in some senses this
work developed a methodology to audit search engines for misinformation, and we were also
able to statistically prove that YouTube's behavior varies across different misinformation
topics.
And our study also identified certain populations that could be potentially targets of certain
types of misinformation.
So this tells us that audit itself could be a useful way for studying how algorithms might
have differential impacts on certain marginalized populations.
Yes?
Good question.
So I get what you're saying about it being reactive.
The evidence suggests that in this case it's like being a special case.
Do you have a proposal as to how it might not be reactive?
How could they be proactive?
Is there something you would propose that they do instead?
Yes.
So I think one of the things they could do, and I think they are doing it now in hindsight,
this is an older study, is they're sitting with teams of experts, health experts, and
also looking at these health-related queries and topics in advance to figure out doing these
red teaming exercises.
They call it red teaming.
I think in the research we call it audit.
So they are doing this beforehand to figure out whether the platform is returning problematic
content.
And so if they do more of that proactively, of course there is some hope in changing things,
and we should not be catching these reactively after the fact.
That assumes that the points of view are stable, right?
So we often have these scenarios where culture changes or something goes viral, like the
tide pods or whatever, where I don't think a red team would have come up with, oh yeah,
we're going to start eating bleach or whatever.
Is there an approach there that you would advocate, like if you were in charge of one
of these teams?
Yeah.
I think one of the things that I think some researchers at Stanford, maybe it was one
of your students who did this work with crowd audits, right, like where the crowd itself
is reporting, because it's not really possible for the red team to find all possible scenarios
under which these sorts of problems happen.
And I think that's where if you have these multiple eyes from different domains and different
cultures to report those problems, and then the company actually responds to it.
The problem is if the company's not responding or the people who are building these algorithms,
if they're not responding to it.
And so hopefully there should be a mechanism to do that, kind of closing that loop all the
way from reporting to actually taking action.
So moving on to Amazon, I think one of the things that led us to looking at Amazon is
despite being this leading retailer platform, how less of a focused research focus has been
paid to this platform.
And I think what was alarming is there were several media reports at the time coming out
suggesting that Amazon's algorithm were putting health and vaccine misinformation at the top
of your reading list.
But there was very little research to fall back to either verify or even, you know, kind
of disprove these reports.
So if you search on Amazon, unlike YouTube, which is at least tried to control for vaccine
misinformation, searching on Amazon for a vaccine, even this morning when I searched,
I could actually find some of these books, you would end up getting several anti-vaccination
products mostly in the form of books.
And the recommendation algorithm for Amazon are even much more sophisticated than YouTube.
So you have your product page recommendation, which has all these many different layers,
customers who bought items, sponsored products related to these items.
You have your homepage recommendations, again, many different layers underneath, pre-purchase
page recommendations, which is shown to you after you add a product to the cart that is
after the user shows an intention to buy that product.
So again, this was the same question as before, how bad is this scenario?
So we essentially wanted to conduct the systematic audits on Amazon search and recommendation
algorithm.
And here we picked only one type of problematic content, vaccine misinformation.
And we conducted two sets of audits, unpersonalized one, and then the personalized audits.
The personalized audit school was to assess whether users account history built progressively
by a user performing certain actions, such as clicking on a product, adding the product
to the cart, showing their intention to buy.
Because any of those actions, how does that change what recommendation is being returned?
And here the user built these account history progressively by performing a particular action
for seven consecutive days.
So these were, again, when I say users, these were sock puppets, searching, searching plus
clicking, searching plus clicking plus adding the product to the cart.
So all these were different actions that were performed.
And then we also controlled for noise, very similar setup as before, just with the caveat
that this whole audit experiment setup was a big software engineering feat, considering
the amount of different combinations of recommendations possible on Amazon.
So what did we find?
So I'm going to highlight a couple of results here.
First a single case study result.
So let's say users start searching for a vaccine and they click on an anti-vaccine book.
So as of this morning, this book was actually there on their platform in the first page,
first search result page.
And so if the user clicks on this, that algorithm next serves the user three other anti-vaccine
books in the product recommendation page.
And then once the user adds a product or book to the cart that shows their intention to buy,
both the pre-purchase as well as a home page recommendation also rapidly changes with many
more anti-vaccine book recommendations.
So this just tells you that once a user starts engaging with one misinformation product on
this e-commerce platform, they will be presented with more of those similar stuff at every
point of their Amazon navigation route.
So this was not just a one-off case study.
We found that more than 10% of Amazon products during the time period of our study for search
terms like vaccine, autism, immunization resulted in misinformation book containing
active vaccination content.
And so our audit experiment, just to give you the scale of this experiment, this was
ran for a little bit over three weeks, resulted in 36,000 search results, 16,000 recommendations,
and then worked over several search filters like featured, sponsored, different recommendation
types, user actions, and so all of that resulted in that number of more than 10%.
So if you just zoom out and look at what these thousands of recommendations look like, this
is the entire recommendation graph for one type of recommendation, what are the items
customers buy after viewing this item.
So here, each node in the graph represents a product, an Amazon product, and an edge
from a node A to a node B indicates that B was recommended in the product page of A.
Node size here is proportional to the number of times the product was recommended, and
the color corresponds to whether if it's a red, if it denotes a product annotated as
misinformation, green, neutral, and then blue debunking.
And I think being, you know, all of you have CS degrees or almost about to get a CS degree,
so you would probably able to decipher what's going on in this graph.
There are these large red size nodes attached to other red nodes, which are almost completely
separated.
There are like two separate components, right?
So this just shows how strong of a filter bubble effect there is for this particular
recommendation.
People who are recommended misinformation products, they keep getting recommended those
products super hard for them to break out from that red zone to get to the blue or the
green zone.
Sorry, just a clarification.
I think you mentioned, how did you code something as misinformation?
Obviously, there's lots of shades of gray here.
Yes.
So there was an initial status, we went through an extensive annotation scheme of a set of,
and then we also built a classifier to do that.
But the entire, there are like a lot more details in the paper as to how we coded it,
but I think it took us almost a month to even come up with the whole annotation scheme,
and then five or six of our experts, including me, we kind of coded it.
But yes, so we looked at a few markers, like the name of the book, the text, read the Google
preview of the text of the book, some of the comments, and then also the reviews that are
present on the Amazon website.
So it's a lot more qualitative process, and then all the markers that we took, we also
put it into the classifier to get the final annotations.
So this would have been more like the overall position of the book, not like, was there
a fact somewhere in there that.
So yes, we cannot really go and look at, okay, there is this one single line in the text
in the book, which is misinformation, but yes, so you could say there's a little bit
of noise in there.
And so this was another one just to say that this was not happening for one recommendation.
Those who viewed this item also viewed very similar graph as before, very similar trend.
And so the key takeaway here was our goal was to bring the focus to e-commerce platforms
and show how their algorithms could be pushing anti-vaccine content to users.
And we empirically established how certain real-world user actions on the platform could
drive users to these problematic eco chambers.
I think one of the implications, at least from this work, is that recommendation algorithms
should not be blindly applied to all topics equally.
If it's a health topic, perhaps companies need to pay a little bit more attention and
to ensure that there is higher quality content coming out in their platform.
So this work of ours intentionally and unintentionally was kind of rightly timed during the COVID pandemic.
And so this was widely covered by several news channels.
And in fact, Congressman Adam Schiff and Elizabeth Warren actually cited this research
of ours in their letter to Amazon to control vaccine misinformation.
So we were really happy that, OK, so now Amazon is going to take a few steps to do this,
but turns out we were really wrong.
So this is how Amazon is doing today.
Still today, as of earlier this morning, you would still find several books containing
vaccine misinformation.
This also just tells how, even though you go into all these lengths doing these academic
research, is it actually informing policies?
Is it actually making any real world impact?
And we can go into a long discussion about that.
But in the interest of time, let me talk a little bit more on one other type of study
that we did, looking at another type of audit method.
So so far, the studies that I presented employed one type of audit method,
sock puppet audits.
While these audits provide great control over your experimental design, you can pinpoint
exactly which variable might be affecting the output of the algorithm.
But one criticism of these audits is that the bots behavior are usually built in a
very conservative way, right?
So the bot in this YouTube case was essentially going and watching all
pro-conspiracy videos or all debunking videos.
Real users do not really act exactly that in that way, right?
So these are at least very extreme bot behaviors or user behaviors.
So as an alternative, we conducted crowdsourced audits where we audited the algorithmic
outputs from real world users to study and identify problematic behavior in users'
naturalistic setting.
So we conducted this audit for a nine day duration on YouTube.
And our goal was to assess to the extent in which YouTube was regulating US-based
election misinformation on their platform.
And so soon after the presidential election in 2020, YouTube came under fire for
surfacing election-related misinformation in their search and recommendations.
And they quickly responded to those criticisms by introducing these content
moderation policies to remove videos that spread election-related falsehoods
and claim that misinformation videos would not be surfaced on their platform.
But then again, during the midterm 2020 elections, there were reports saying that
YouTube has still has misinformation blind spots, right?
So they have not been very effective.
So this study of ours was goal was to determine how effective YouTube was
in successfully implementing its content moderation policy.
And so we did this through this post hoc crowdsourced audit.
Why it's post hoc?
Because it's conducted after the fact the event has happened to elections of 2020
and we were conducting this study in 2022.
And it's crowdsourced audit since we investigated YouTube's algorithm collecting
data from real world users.
And I'm sure many of you who have run these sorts of recruitment studies, you
would realize how difficult and how hard it is to do that.
Essentially, we were asking users to lend their YouTube history so that we can do
this sort of audit run.
And so our crowdsourced investigation, I think we started with like recruiting
600 to 500 users and we ended up slightly lower than 100 users.
So 99s, particularly.
So all these 99 users first filled out a pre-survey and about the beliefs on
personalization on YouTube, how they trust YouTube search and recommendation
algorithm, and then they installed this browser extension, which allowed us to
collect users personalized data.
We also had all these ethical considerations, which I can go on into
more detail if anyone is interested.
But what this extension was doing, it was collecting search results of search
queries related to the 2020 US presidential election, as well as
voter fraud claims surrounding the 2020 elections.
So two kinds of collection was happening.
One was with respect to search results in the standard and incognito window.
And by comparing these search results in both these windows, our goal was to
tell the extent in which YouTube was personalizing search results.
And then we were also collecting recommendation results.
And the way we were doing this, we were collecting these up next recommendation
trails after a user has watched a list of pre-selected videos with different
stances on election misinformation.
And the extension would start by first watching a pre-selected seed video and
then collecting up next videos up to five different levels.
So when we asked our participants in this pre-study survey, how much do you
think YouTube personalizes your search results?
About 34% of them believe that YouTube personalizes their search results to a
really great extent.
But this, through our audit, we found that YouTube, actually, that their YouTube
stop search results have little to no personalization.
So this also tells you how users believe in algorithms, like the way they behave
is different from actually the way the platform might be behaving.
But when we asked how much YouTube personalizes their up next recommendation,
that perception actually aligned with how actually the audit results showed.
Like 51% of participants believe that YouTube personalizes up next
recommendation to a great extent, which is in line with what our audit results
found.
We also calculate the amount of misinformation present in search results.
And we quantified this with this misinformation bias score.
And this is the only equation you're going to see throughout this talk.
So this misinformation bias score, we're East from minus one to one.
What the score does is it captures amount of misinformation, election related
misinformation while taking into account the ranking of the search results.
So a positive score indicates that search results contain videos that support
election misinformation while negative it contain videos that oppose election
misinformation.
Now, if you look at the entire distribution of scores for our collective
results, we found that the misinformation score, if you look at the X axis,
it's mostly negative, which indicates that YouTube presents more debunking or
opposing videos in the search results.
A couple of other key things also jumps off, right?
So you could see there are this distribution is by model.
So essentially there are two different clusters and each of these clusters
corresponds to two types of search queries.
I mean, we didn't cluster it ahead of time, right?
This, this emerged from our data.
So the first cluster corresponds to voter fraud.
Basically anything related to fraud in conjunction with keywords related to
election, while cluster two is more generic election related searches,
presidential election, mail-in ballots and so on.
What's interesting here is that the cluster one has these missions
information bias score, which are more negative, which indicates that if the
user goes and search for fraud related topics, they are actually going to be
given more opposing election related misinformation video, right?
So it's making YouTube is making it really difficult for users to search
for election fraud video, which in some sense tells that YouTube's pay more
attention to queries about election fraud and ensures that when users are
searching for them, they are in fact being exposed to opposing misinformation videos.
So key takeaway here is in some way YouTube is in fact successful in
enacting election misinformation policies.
So things that we wanted to test turns out it's actually, you know,
aligning with how they wanted to enforce these policies, but it is indeed
paying special attention to certain queries about voter fraud.
But there still exists certain misinformation in the up next trails.
We found that with some of those positive scores that you found.
And then finally, as a byproduct of this audit, we also found that there was
some mismatch in participants beliefs and the algorithmic reality that happens,
right, which indicates a lack, some lack of awareness of algorithmic, how
algorithms behave.
And I think there has been other researchers who have been working in
the space looking at algorithmic folk theories and how people's perception
differ from the way these platforms work.
Now, wrapping up, so these three, these are all the other, you know,
the core studies that I wanted to present, but obviously coming back to
how I started the talk, where do we go from here, right?
How do we do meaningful algorithmic governance in the first place?
That is, how do we set the path towards algorithmic governance in a meaningful
way and what are the challenges in doing that?
So here are a few ideas and obviously this is not, you know, there might be
more that could be added, but these are some of the possibilities for doing
algorithmic governance.
So I've listed three of these.
The first is algorithmic audits and so governance via audits and there could
be many layers to this, right?
So one of the layer is conducting external audits and I presented some of
these external audit studies through the three research work that we have
done in the past and also these audits could identify different types of risk.
So misinformation is one risk, but you could also do the same for bias,
discrimination, accountability, accessibility, accessibility, fairness
and so on.
And there are many researchers who have worked in the space.
I've listed some of these citations here.
But obviously a question is, so we as academic community, third-party researchers,
we are doing all these audits, is it really making any difference, right?
And as classic example is the failure of our Amazon study to make much of a
difference, right?
So we still really don't have a system in place where the algorithms or
companies running them are truly accountable to an independent third-party.
So this reminds me how US-based consumer reports operate, right?
So there are these independent third-party organizations that go into
great lengths for testing products that you use every day, your cars,
your washing machine and so on.
But so my argument is that why can't we do the same for algorithm?
In fact, I would argue that we need that more for algorithms because we are using
them much more frequently than say your washing machine.
The other shortcoming with external audit is that they are a form of reactive
governance.
This was the question that Michael was asking even earlier, like they operate
after the algorithm have been deployed.
So after the harm has been done, plus the external auditors do not really
have access to the models, to the training data, which are obviously
protected as trade secrets.
So as an alternative, another layer to governance via audits is you could do
internal audits as proactive governance.
And so at the time, researchers from Google who are no longer at Google right
now, but they released this paper making a case for internal audits where audit
would be part of a core part of product development at every step of the
way.
You could also do the best of both worlds, right?
You could do something called cooperative audits, which is a fairly newer
concept where while external audits answers what problems the platform has
and internal audit says why that's happening, you could have a combination
of both and you could do cooperative audits as shared governance, which allows
external algorithm auditors to audit the system of willing private companies.
So I've done a little bit of this with Spotify where working with their
engineers within the company figuring out how gender representations might be
biased or not for their taste on boarding and listen action on podcasts.
And then finally, you also need to do these audits multiple times, right?
Longitudinally, that is we need to conduct these continuous audits monitoring
platforms multiple times instead of that single snapshot audit.
Many of my studies that I presented today are all single snapshot and we do
need that kind of longitudinal effort.
So here is where I want to highlight one of the quotes from the Brajis earlier
internal audit paper where they mentioned the audit process is necessarily
boring, it is slow, it is methodological.
Which stands in stark contrast to what I had started my earlier slides with
move fast and break things, done is better than perfect, so very much in contrast
with the rushed culture of technology development.
And this is where I want to take a little bit of tangent and mention about audit
possibilities. One of the fastest growing developing AI technologies is the
large language models and what would auditing even look like for large
language models? What is the blueprint for LLM auditing?
And then also what are the key challenges, right?
So one of the key challenges is that it is difficult to assess the risks that
AI systems and large language models in particular pose independent of the
context in which they are deployed.
So we do need application specific audits for large language models.
The second challenge is that, and I don't know how to solve this, or rather
even the first one, is that the capabilities and the training processes
of these foundation and models have really outpaced the development of the tools
and techniques and the procedures for auditing, right?
So it is really hard to keep up the pace.
And so doing ethical, legal and technically robust audits makes it super
challenging for such a rapidly developing technology.
And so it must be complemented probably with much more newer forms of
supervision and control.
So here is one possible framework, one possible blueprint for auditing
large language models, which is kind of three layers.
So the first one is a model audit.
So as a name suggests, it focuses on assessing the technical properties of
the pre-attained language models.
So this is very similar flavor to the internal audit that I mentioned earlier.
So that's sort of proactive governance before you deploy the model.
But then there is application audit, which focuses on the assessing the
applications built on top of the LLMs.
So which is these flavors of post hoc audit, right?
And it should be done longitudinally, right?
Multiple times, over a long period of time, so as to capture any sort of new
properties that might be emerging.
And then finally, I think this is the new form of audit that we haven't talked
about a lot, at least the research community.
These are these governance audits that is assessing the processes whereby these
language models are designed and where they are disseminated.
So very much process oriented, right?
My next proposition is about value-centered audits.
That is, there is this active conversation around social values, emphasizing
while designing algorithms.
And I think we also need to turn that attention and thinking into how we can
value and respect humans involved in the audit process.
So these humans could be in the form of users who use the system or even
auditors who are investigating the sites.
And so for auditors, if we bring back the conversation for a second back to
misinformation, one instance where auditors did not really feel perceived
fair treatment was this scenario where fact-checkers are one of the key auditors
of misinformation on online platforms like Facebook, Twitter.
So the fact-checking organizations, SNOPs, a couple of years ago, actually backed
out of their partnership with Facebook because they didn't feel their values
were being respected.
I think to delve into this question of fair treatment of auditors, we need more
effort.
And so one way in which my group has started a few initiatives, we have
launched a research endeavor with the fact-checking organization based in
Kenya called Pesachek.
And this has also expanded to 16 other fact-checking organizations across
four different continents.
And we released our first report called the Human and Technological
Infrastructures of Fact-Checking.
And so one big motivation for this work was also this question of, are we
really taking into account diverse voices when we are talking about
governance and governing technologies?
And are we really doing culturally responsible AI?
Finally, how do we ensure actionable audits?
That is, audits that result in real change.
So one of the most successful examples of an actionable audit is Joy Voluwami's
Gender Shade Study.
So what she did was she audited facial recognition algorithms.
And within seven months of the release of these original audit, all the
three companies who had their facial recognition apps released new API
versions that reduced accuracy disparities with gender, male and
female, as well as race, darker and lighter-skinned subgroups.
So in other words, the Gender Shade Study is a classic example of commercial
actual impact.
And so they laid out their approach in this actionable auditing paper of
theirs.
Highly recommend you all to go and refer to it.
But turns out actionable auditing is often tremendously difficult to
achieve.
And here is where I want to revisit that earlier slide for our Amazon
study to highlight how much we had failed in doing the actionable
auditing.
So despite widespread media coverage, despite a letter from Congressman
Adam Schiff, Amazon did not really act much.
All they did was add that banner of COVID-19 information directing to
CDC's web page.
So hopefully this kind of summarizes the challenges as well as opportunities
and setting the path for algorithmic governance and hoping with the new
regulations coming in place, maybe if I were to give this talk next year, I
would have a little bit more hopeful slide than how I'm ending this talk.
So that's it.
So this is all I talked about today.
Most of this work was done with my PhDs.
Then PhD student Prena Juneja, who is now a faculty at Seattle
University.
And then my group, I would also stick in three other threads of work,
which I obviously don't have time to talk about.
But these are like a couple of other amazing students.
So Shruti Furkaya she did a bunch of work on computational social
science.
I was earlier meeting a student who was doing this sort of work.
So things like big data analysis of online interactions.
Studying trajectories of participation of users in extreme
communities, conspiratorial communities.
We have also done a little bit of design intervention and social
system design work with another student who is also a faculty now.
This is more of an XCI flavor where essentially questions like how do
you design a system to nudge users towards meaningful credibility
assessment?
How do you design a system to allow users to break out of their filter
bubble?
Something called other tube that we built on YouTube.
And then finally, the last and the least fleshed out thread is some
of the work that is currently ongoing with two of my students.
We are looking at challenges and opportunities of generative AI
in fact checking work.
And then what are some cultural misalignment that might happen with
language models?
Especially with roots in the global south, we are looking at
cultural implications of these language models in countries like
India and other countries in Southeast Asia.
So with that, I would like to end and happy to take questions.
Thank you all.
All right, we've got time for some questions.
I was wondering in your auditing of YouTube algorithms that you guys
looked at.
Yes, it was in 2020.
So I wasn't sure if YouTube shorts had been implemented since then
because YouTube shorts are somewhat of a newer aspect.
But I wonder if the algorithms that underlie the, I guess, traditional
YouTube recommendation system underlie the same sort of like YouTube
shorts recommendation because I guess the length of content and sort of
the amount of stimulus that would be needed to get the person to keep
the point would be different.
And therefore possibly seeing that if there are sort of similar
pattern between the two, whether or not the density, I guess, of
sort of misinformation sort of increases because the fact that
content is more short form.
Yeah.
So for the first study, we didn't, at that time, shorts were not
there.
But then for the third one that I presented with election misinformation,
we did capture YouTube shorts.
And that tells me that we should probably do another analysis
comparing the length of the videos and the, you know, whether it's a
short video versus a long form.
We did not do that, but that's an excellent point.
Yeah.
I have a question on maybe two studies.
Was it a good first one?
Did you look at all at like the probability that you would get
recommended legitimately false conspiracy videos, like on the
moon landing from videos about conspiracies that are a little bit
more true, like missing persons cases that the police just don't
investigate and like the likelihood that you'll get recommended
actually false content?
Yeah, we did not because I think one of the shortcomings of running
audits is the whole setup itself.
So we have to start somewhere, right?
So the RR starting point were a set of seed queries.
Right?
So with the way you are framing it, you know, we could, the
hypothesis could be dozen missing persons case lead you to more
conspiratory videos.
And in that scenario, I think we can use our audit framework to
have those as seed queries and see what happens.
On your kind of concluding point about actionable audits, I'm just
curious, do you think it's something to do with like the
conducting of the audit itself or just the context and how it
aligns with like the company's incentives?
Because it feels like the gender shades case, it was like a very
easily framed as like poor performance.
And so they were trying to cover themselves.
Whereas Amazon is somewhat incentivized to keep people buying
things even if those things are harmful.
So like, I guess I'm wondering, like, do you think audits need to
be conducted differently or there just needs to be more external
pressure like from the government or the public to incentivize the
companies when they are like internally?
Yeah, I don't think it's a ladder.
I think, I think Joy went on to great extent to after the study was
published to kind of give talks and publicize and do that kind of
outreach, which I did not do with this work.
I think that matters a lot, right?
She's the one who went to Congress for testimony and testified
against these companies.
And when you do that kind of impact, it would definitely translate
or there's a higher chance to be for your work to be translated to
actual actionable outcome.
I don't think those are actually steps listed in the actionable
auditing paper.
And in some sense, I feel like maybe the academic community need to
think about how to incentivize those additional work.
We don't have those incentives in place.
And partly I think we should look inward and blame ourselves that
we don't have those incentives in place.
Moving to this, I just wanted to hear your thoughts more.
When being either interactive or reactive, do you think we should
draw a distinction between conspiracy theories or misinformation
that has potential for great harm versus those that maybe don't,
right, to justify interventions that override individual autonomy
or control the information space?
Like, who is the moon landing conspiracy theory?
Can you say that last part?
Like, the moon landing conspiracy presumably isn't hurting anyone,
right?
Should we take it down?
Yeah, that's a really good point.
I think for companies like Google, I know they have this,
your money or your life, they have a view or an acronym,
YMYL or something, a set of search guidelines.
If those search results or the pages that show up,
if it's affecting monetarily, financially, health or your life,
then they're going to be more proactive and act on it.
So you're right.
Like, moon landing is probably not to that extent,
versus if it's vaccine information that has direct life consequences,
right?
But obviously then there are all these other questions that when
it's very well known that if you are drawn to one conspiracy,
you're likely to get other conspiracy theories, right?
So then what happens?
Should those be prioritized, at least to the extent that they
maybe should be prioritized in the recommendations,
if not completely removed from the platform?
Yeah?
This is sort of another sort of idea that I had.
Sort of looking at the idea of, like, what I think is sort of
interesting about, like, social media apps like YouTube is a whole
aspect of the media being able to communicate with others,
like the other comment section of a YouTube channel.
And I was wondering if there is a way to possibly, like,
I'm not sure how present this is, or if this is even, like,
a thing that is even, like, something that is able to be,
like, looked into.
But is there a possibility that the algorithm isn't,
may almost be promoting you content not necessarily by what
it's physically providing or, like, showing up then recommended,
but showing other users who would most likely put other links
to more, I guess, like, extreme videos in the comments being like,
oh, if you thought this was interesting, like, look at this.
And so that's not explicitly YouTube's algorithm showing you a video.
It's showing that same video to someone else who has the ability
to share a link to another YouTube video.
That would be almost, like, pushing someone down like a pipeline
of conspiracy theories.
So more like how the social recommendations,
like, you're sort of adding this collaborative social
recommendation component to YouTube and seeing how that pushes.
Like, if YouTube, like, you could have the same effect.
Like, in theory, maybe, the YouTube recommendation system could
not explicitly push someone by recommending, like,
more intense conspiracy theories.
But if YouTube is recommending someone who's already, like,
a very, like, entrenched conspiracy theorist and maybe someone
who's on the edge, if YouTube recommends them both the same,
like, it's starting out conspiracy theory video,
then you can have the person who's, like,
very entrenched conspiracy theorist commenting
and suggesting things themselves.
And it's not that YouTube is explicitly recommending the original person
or the person who isn't, like, entrenched conspiracy theory.
It's that they put them essentially on the same,
or they put them in the same environment in which they could communicate.
Yeah.
So, actually, my group of some of Shruti's work,
we have done this in the context of Reddit where what you're describing
those very entrenched conspiracy users,
we term this as veteran users.
And so they are one of the big drivers of bringing other people,
like what we call joiners, into the community of conspiracy group.
So I hope YouTube never does that for what you're suggesting,
but that's a classic marker of how these social dynamics
can actually bring people into these conspiratorial world views.
And, you know, empirically, we have seen that.
And there is also social science theory
proving that that definitely happens.
Yeah.
One challenge that I feel like our field faces with audits is,
I guess what I would describe as the sense I get of frustration
from folks at these companies who feel like the audits
aren't well executed or are way out of date as soon as they're published.
You made this point that we have to be very methodical
and often slow in doing this.
Then you throw in the peer review pipeline
that can slow things down further.
And by the time the thing comes out,
I remember seeing an applied researcher in a company
who in principle would be more open to this kind of stuff,
being like, our album doesn't even work like that anymore.
And so I'm wondering, so obviously you listed a bunch of possible
cooperative audits, longitudinal, internal, and so on.
I'm curious, are there, is there anything we can do to address that?
Let's assume that we can't change the incentives of the companies,
to change how quickly we do the audits.
Are we always going to be vulnerable to this,
like, oh yeah, that was yesterday's algorithm kind of critique?
Yeah, that's a really good point.
In fact, I was at a workshop with other folks like Christo Wilson
and a few others who have done audits for a very long time
with people from Facebook and YouTube.
And I think we came up with this exact same question.
And I think the common thing that emerged was that,
I don't think academics should be the, or academic institutions
should be the places to do these sorts of long-term audits.
It's fine to kind of develop the methods and, you know,
kind of say, okay, this one should pay attention, for example,
to Amazon, or, you know, this is the method to do it.
But then you need, like, separate third-party companies
to continuously do these audits, right?
So sort of like consumer reports,
what's the equivalent of that for audits?
And I think at CSCW, the closing keynote,
Room Room was mentioning that kind of red teaming,
I think their company or whichever NGO she's working with,
they are doing something like that.
I think, so academics with that peer review process,
I don't think we should be responsible for doing those sorts of
continuous audits because we are always going to play catch-up
with companies.
Yeah.
Oh.
One last? Yeah.
At the end.
Do you have thoughts on, like, Twitter has implemented, like,
community notes where people can just, like,
anyone can put it under a post like,
that's not true, or this is misleading,
or this person ever said that, like,
just, like, user-based, immediate type auditing,
if you will.
I do have thoughts on that versus, like,
companies long-term auditing, or, like,
if you think that's a good idea.
Yeah, that's a really good point.
We haven't looked at community notes,
but I know, you know, some researchers have,
kind of, looked and researched it.
I don't have really any very smart thoughts as to,
other than the usual advice that it's a good thing that
one should do it, but with the caveat that,
if the community doesn't reflect the right view,
or I wouldn't use the word right,
but, you know, a credible view of what happens then.
That's problematic.
I think that's time.
So let's thank your speaker.
Thank you.
Thank you, everyone.
Thank you.

Cool. Hello everyone, welcome to your course AI for Art, Esthetic and Creativity. Today
we have a very special speaker. She has an excellent background in different domains and
she will tell you hopefully more about herself and her work. Sarah is a great friend and
colleague of me and she kind of accepted to give us a lecture talk today. So from here I
let Sarah to continue. Please go ahead. Thanks Ali. It's such a pleasure to be here. I've heard
so much about this class. I don't think I have a slide about my background but I can tell you a
little bit about myself. I finished my PhD in neuroscience so across the street from Seasale
last year and now I'm a postdoc in the vision group and the journey throughout my PhD was
a little bit of a winding path. I started thinking about explicit symbolic models for things like
physics and we'll talk a little bit more about that along the way. So modeling how the mind
makes inferences about things that we see but that hits a ceiling when we come up against
questions of vision and types of seeing like looking at art that is really difficult to develop
some kind of computational formalism for that we don't have good models for. And at the same time
as I was kind of hitting that wall in my own thinking I was developing a parallel interest
in visual art and doing a lot of different projects both with individual artists and with
larger museum archives that I'll talk a little bit about and started to look at art as a ground for
asking kind of difficult questions on the frontier of our thinking about the mind.
If we look at how humans create art and in view art can we understand something about
how they view the world and domains that we don't yet have good models of cognition for.
So I kind of started steering my my PhD in that direction. I'll share a little bit of that work
as well and as I said now I'm a postdoc with Antonio and Ellie asked me to share a little bit
of my my inspiration behind that path. I don't have a good story about a specific moment I think
it's been a lifelong interest for me since I was super small and reading a lot of poetry I guess
thinking about kind of the the origin and nature of structure and our experience of the world.
I know that's quite an abstract thing but the structure that we see in visual patterns where
does that come from? Is that something that lives inherently in the brain and we imprint it on to
kind of noisy and unordered stimuli or is it something that's external you know a nature
nurture question and then our brains kind of evolve to reflect and I got interested in this
meeting point this kind of layer between the self and the world where all the action happens so to
speak and had training in applied math before I came to MIT and would think about ways to describe
kind of structured inputs to processing systems and understand something about the structure of
external inputs and then my neuroscience background learned a little bit how to how to
think about and model the structure of a processing system right the structure of different parts of
the brain and it's really been through my interest in visual art that we can start to think about
and describe what happens when those two things meet and how we synthesize our world of visual
experience in domains related to art and then other kind of higher level aspects of cognition
like scenes or associations with moods of scenes and that kind of thing so that's that's where I am
now and I think I'd like to start us off unless anybody has any leading questions about where I
come from with kind of a provocation and you can you can think of this as a frame for what I'll share
today but it's intended to be provocative and so the statement I'll make is that visual perception
itself human perception which we attempt to mirror and model in computer vision and computer science
in some cases that human perception is something that's fundamentally constructive and I say that
because it solves an ill posed inverse problem like ones you've probably heard of before and
doing that doing that solving requires a little bit of creativity so where am I coming from there
the back of your eye as you know is is a 2d flat canvas right made up of a hierarchy of cells
that were visualized in in drawing in art by Ramonica Hall hundreds of years ago and are now
visualized using electromagnetic imaging and we can get actually pretty fine brain detail
of the cells in the back of our eye that constitute a 2d canvas that takes in incoming image data
and represents images in terms of patterns of activations via this kind of mosaic of cells
yet we experience this richly 3d world so there's a setup of a problem that you've probably heard
before right 2d canvas but we have 3d rich experience scenes have depth objects have 3d shape
and furthermore what we see carries lots of different meanings and associations
so where is all of that kind of higher level information in a 2d image classical kind of
computer vision problems we look at this kind of painting by Suzanne you might not only recognize
3d structure of this cottage on the mountain side right even though the image itself is 2d
I might have all sorts of associations with it I might be able to say oh it's spring time
think something about the time of year I might even be able to infer something about the geography
by the palette used to convey what fields might be there think a little bit about the landscape
I might be able to appreciate depth in pictorial space so even on this 2d plane if I put my mouse
up here in the front maybe these fields are closer to me as a viewer than these ones that are far away
but once again I'm just looking at a flat picture where is all of that information
our brain has to solve an inverse problem like this anytime we look at a visual scene it has to get
from low two-dimensional information to kind of rich 3d but there's a fundamental problem here
and I pointed to that is that infinitely many 3d objects can cause the same 2d project projection
that's the under constrained nature of this inverse problem that vision poses and you've
experienced this quite explicitly anytime you've seen a shadow and not the object causing the
shadow right and you've had to infer oh is that actually you know a monster on the wall or is
that somebody's hand being projected but there are infinitely many configurations in three dimensions
that could be projected downwards onto two dimensions and cause some configuration in
pictorial space so how do we constrain that problem when we're solving for what we see
right so this problem is ill-posed because it has as I said many infinitely many possible solutions
and choosing between them requires some additional information and in the case of the brain
modern neuroscience understands this as requiring the brain to construct something so that's what I
mean when I say perception is fundamentally constructive or creative it requires the brain
to construct a best explanation of what it's seeing of incoming information and if we call
that perception then maybe you'll permit me to make a bit of a stretch and say that that makes
perception itself an act of creation or an act of synthesis of a scene so one kind of popular
way to solve this inference problem is by using models of the world right and we can approach
that from a Bayesian lens maybe you've seen the work of Josh Tinnenbaum in the bcs department
or maybe we can do that purely with deep learning it's kind of an attention that we could explore
later today but I'll give you an example here and this is let me back up for a second that if we
were in person this is the point where I would do kind of a live in person demo so I want you to
imagine that we're all sitting kind of in a dark room or we're sitting in a studio space and out
in front of you there is a table covered in black velvet and I've set some stuff on that table you
don't know what it is I set it there when the lights were off and then I take a single line of red
laser light and I'm going to gradually sweep it over the scene so I'm constraining the visual
information you're going to receive about what's out there in the world to something kind of really
low dimensional compared to what you normally get to understand kind of a garden of forms that would
be sitting on the table so imagine you're there in the studio with me and you see the following
you have to kind of infer what you see on the table maybe you could write it in the chat or
just think to yourself when you see this give it a moment what's sitting here on the table
or what kinds of things what different things
maybe this would be a good use of the chat I can pull it up or you can describe features of what you see
a bunch of blocks on the table great there's something cubic up now I see the corner there right
multiple vases multiple forms with kind of different underlying shapes something cylindrical yep
two things do you see I think there's a sphere actually there in the middle
what is the experience of this light do you actually feel a physical corner when you see
bent light round the corner of that cube
oh oh goodness
interacting with chat is a bit tough all right anyway the point I want to make here is that
I can present really kind of low level information and you can if I dare open open the chat up again
yeah there's a single base there's a single table and many forms sitting on top
that's right so there's just a tabletop and then lots of different shapes also covered in black
velvet so the light doesn't scatter and the light traces the outline of these 3d shapes
and you can appreciate something about what the shapes are just by watching how light bends
around their surface and the relative motion as it traverses that facade right as it moves
over the surface of the sphere the light bends according to its curvature and I would argue
here that because you have some notion of what a sphere is and some notion of what a cube is
that is you have a relatively abstract model of these underlying shapes in your mind a mental
model you can do some inference when you see light move over their surface on this way even though
I choose an example like this because you've probably never seen this example before right
never seen a single line of laser light move over this table surface
even this this kind of setup but you can still do that inference pretty well and you did in the chat
so if you'll if you'll stay with me here I'm suggesting that this is an example just your
perceptually of how we can bring kind of models of the world and shapes and forms that comprise
it to bear on simple visual stimuli and how we can even do that by using articulated light
to isolate aspects of those stimuli and to kind of elucidate our perception to us
so we do a lot of things like this in the MIT museum studio where I teach the vision and art
neuroscience class which I'll talk about in a moment but that's where this this was filmed
let's see if it'll let me advance even though I open the chat all right so another kind of setting
in which we often hear and think about models of the world and this kind of inference is in
intuitive physics and I bring this up because some of my background is also in this type of work
investigating how the brain represents physical properties like mass that it uses to reason
physically about the world right you would have to estimate the mass of this block that's falling
and making a depression on this pillow before you would know the right amount of grip force
you would need to use to to reach in and pick it up without dropping it right and this is something
we do incredibly automatically and it's a skill set we develop regularly from a very early age
and I found that the brain represents properties like mass with an amount of abstraction and invariance
to the type of physical scene in which mass is revealed that would be necessary if mass like
this were to be used as an input to an abstract generalized engine for physical simulation or
what we call a physics engine in computer graphics and simulation suggesting that there is
kind of some first evidence that the brain does use these kind of generalized simulation engines
to solve low-level inference problems like inferring mass because we can make some hypothesis about
the nature of the underlying representations it would need if it were to solve problems in this
kind of way rather than by simple pattern matching or in a pixel based way where we would assume
that the representation of mass would be quite different from scene to scene because the low
level visual data about the scene is different but in fact that's not what we find we find representations
of physical variables like mass and friction that generalize across any kind of physical scene
that we test where we hold a lot of other different parameters constant right like object color
and this suggests an account of physical reasoning in the brain that has been that has been studied
pretty extensively computational right and that we model via probabilistic simulations of a physics
engine I don't think that video is going to play for us right but this is the kind of work when I
was doing when that I was doing when I was writing down like explicit models of the world that could
be inverted to explain something about underlying parameters we were using for vision and in that
in this case those models were physical right but what about cases like like art where it's
difficult as I mentioned to develop some kind of computational formalism where we don't know the
underlying model for instance how to create the Cezanne painting we saw in the beginning
a priori right how do we even start what are the underlying dimensions we'd need to write down to
either make sense of how we see things or how they're created so this whole area is kind of what we
dive into in that vision in art and neuroscience course so this is something you're interested in
it's of course an unsolved problem but we spend the fall semester every year
kind of delving into it through both neuroscience literature through art practice through computation
and then through studio work so kind of hands-on experimentation with principles underlying vision
that we then externalize and experience ourselves and try and visualize in artistic contexts
to give you a little bit of a taste of that class we would look at
these examples say by by an artist in minor white and ask if we were trying to set up
a typical kind of describe a model and then invert it to understand vision setting you know what is
the veretical percept in either of these right if before we were considering mass of some object
that the brain has to infer and then we can write down a physical law describing how mass plays into
action unfolding in a scene a law describing dynamics and then invert it to think about
how the brain represents mass what would the analog be here what would we write down as the
veretical percept you can share some thoughts in the chat that is also an exercise you could just
do yourself right maybe here you can start to get it a shadow of something outside the window
I see a bike maybe a bike seat there that's kind of not the point kind of not trying to infer what
caused the specific physics of this this image you're kind of getting at something different
and especially here what if the artist isn't around for us to ask anymore these are actual
photographs right these are photographs of something but the act of looking at it isn't about
inferring the underlying cause of the image it's about inferring something else sort of aesthetic
parameters that define visual experience or kind of render visual experience at a lot higher of a
level how do we begin to get traction on problems like this either in seeing or in or in generation
as I said you know in art we also come up against a great difficulty in that you know there are
infinitely many ways to render recognizable depictions of common objects right with all
sorts of idiosyncrasies illusory boundaries difficult for models to detect but we recognize
a woman in these images with the dress almost instantaneously and similarly we come up against
another under constrained inverse problem is in that there's infinitely many ways to render and
depict kind of abstractions of commonly recognizable forms which again are difficult for
current day models but they're pretty easy for us I can recognize a figure and maybe have different
associations with it in each of these different images so we think a little bit about this
in the course like I mentioned you can ask me a bit after this talk as well if you're if you're
interested in it it's called vision and art and neuroscience all of our info is is online most
of the syllabus past exhibition catalogs at vision.mit.edu it's offered through through bcs
and as I said we we investigate during half the class in the seminar portion of the class
kind of the underlying principles of vision and we work through a series of modules
that build up visual processes from early level like v1 visual processing all the way up to kind
of more rich images and we do this in parallel in a studio section during the other portion of the
class where we're translating these principles of vision into the studio and building artistic
contexts where we can kind of become aware of our own perceptual processing at work so examples like
the one I showed you at the beginning right with the with the laser line moving over
that garden of objects are examples of settings that can allow us to maybe perceive our own
perception at work right or shed some light on what's going on when we look at at normal scenes
right there's all these unconscious inference processes happening even when we look at corners
in a room but we're not aware of them and so we ask here if we can create settings
where we do become intensely aware of them and that awareness becomes kind of the art experience
right and so it's the art of perceiving one's own perceptual processes at work and then over
the course of the class everybody develops an individual artwork for exhibition which is super
lovely and it's it's an opportunity that we don't often have in other classes at MIT so we run this
for five years now had five different exhibitions and COVID be it a virtual exhibition and then
this year's just opened in December and is actually still up in the MIT museum studio just off of lobby
10 10 150 if anybody is on campus and wants to go check it out it's most it's open most days when
staff are there but this course is the parallel to your IAP class that thinks about things more
in the language of computational neuroscience than deep learning and in some aspects of the
course will present deep learning or deep generative models as contexts for probing representations
that might be shared by human minds and machines and we'll look at that a little bit later in this
lecture but think more traditional computational neuroscience lectures readings visual art and
then a studio component where you experiment with some of the stuff hands on so that's what we do
envision art neuroscience we start to to probe at the richness of this art neuro and machine
learning intersection there's a lot of different things we can do there and for the rest of this
talk we're going to highlight a number of different projects that approach that intersection in
different ways and highlight kind of different ways that you could think about engaging this
material in these questions data sets and resources that we have available and kind of
different ways of carving up the problem into bits so we'll start by thinking about modeling
kind of the structure underlying human creativity at scale without trying to prespecify
laws that you would write down for say a physics engine right can we use deep generative models
to kind of approximate or appreciate or grok the structure underlying large data sets of human
cultural artifacts and then use those models to experiment with cultural history on kind of a
timeline that allows rapid evolution in the present so I'm speaking specifically about a project
that I don't know if some of you have seen and I know Ali has seen a collaboration that I led with
the Met a couple of years ago again it was fun that we were in person because we were able to
actually go to the Met and see a lot of these objects but back in 2017 the Metropolitan Museum of
Art was the first or one of the very first to release an open access catalog of a few hundred
thousand digital images of works in the Met collection and released them into the public domain
which is wonderful for for us as computer scientists and programmers and people interested in ML and
art because what a rich data set that is right what a rich data set all in one place
don't get me started on the issues with museum APIs but a lot of museums have followed suit in
releasing their digital collections into the public domain so they're free and open for experimentation
they approached us at MIT and open learning and a couple of programmers at Microsoft and asked if
we might want to do a series of projects with this digital collection and so we did and we asked
whether we can build deep generative models associated with archives like this of created
work that are embedded in their cultural context which might ask which might allow us to ask like
slightly more specific questions art historically than just you know what if you train StyleGAN on
all of wiki art all at once right not conditionally so we're not appreciating any categorical
differences between images but if we just showed it all of wiki art okay here we want to ask something
a little more fine grained can we notice you know differences in the development of feature languages
between maybe time periods or geographical regions right and can we develop ways of collaborating
with those models to iterate archives forward so experimenting with chimeras between existing
works and developing new works right that might sit somewhere between works that are already on a graph
so one of the challenges that we faced here initially was that the data set was pretty big
400,000 images but each individual category in that data set was not some might only have
a couple hundred images and there's a lot of sketches and drawings and kind of uncategorized
work too that makes up that 400,000 so you're in a situation where in theory you have a rich
labeled data set but in practice it might be quite difficult to train anything that looks
photorealistic or gives a good sense of any individual category of work because the categories
themselves are not that large so at that point this was pre like style again too we started working
on this in 2017-2018 um I asked whether we could instead of training a single model on say a subset
of this met collection like this category of vases called yours whether we could find corresponding
subspaces of what we're now referring to as foundation models like big an image net that kind
of approximate our data set right so if we think about foundation models as a shared resource that
ideally everybody would have access to and there were ways to think about contributing to then maybe
these smaller problems become or can become a way of defining subspaces of those big models that we
can interact with right rather than having to retrain a model and on our data set so we used
GAN inversion here and instead of training a new model on just this category of viewers
we asked whether we could embed each image that already existed into in the met collection
and into the feature space of big GAN image net which happens to have a category for vases so we
selected categories that were shared between image net and the met collection there are a handful
about a dozen um and we maximized for each of those images the similarity between the met image
and the big GAN image using a two-part loss right so we wanted them to be similar both at the pixel
level and at the semantic level and we did that by looking at two different layers of a pre-trained
res net as the embedding network so once we've embedded these models these images into big GAN
we can then visualize the individual embeddings but we can also do something a little bit more
interesting than just look at approximations of these images which might not be very good we can
think about the underlying feature language that might have been learned and then look at
interpolations between the existing images in the met collection I hear murmurs in the
background if anybody has a question hit the chat you're super welcome to speak up um so next we
look at interpolations between these existing images on the graph and we can create kind of
hypothetical or dreamlike images that exist between the spaces of existing works in the collection
and these are pretty interesting and beautiful and they allow us as I was mentioning to ask
questions about what collaborations between geographical regions might have looked like
right because we do have categorical information about where each image in the met collection
came from it allows us to suggest new objects and the spaces between them so it allows us to
interpolate and the other beauty of these kinds of executable models of culture is that it allows
us to iterate on existing collections really rapidly um and evolve them forward and so we
can kind of start to imagine archives of the future that would have embedded within them
world models corresponding to the data set that exists at one point in the archive right so the
archives could kind of evolve themselves forward and suggest future versions of their collections
based on what's already been created and that this is again this was back in 2019 which is a
long time ago in computer vision terms um but even just with with inversion into to began in the
channel I was really impressed at the quality of the the images and the hypothetical objects that
we could get for example here are a bunch of different generated teapots from the met latent
space in the teapot category which again happened to be shared between ImageNet at that point um
and the met collection and as I said we did have the the opportunity to exhibit this in the met
which was absolutely wonderful um we projected a visualization of this latent space superimposed
on a map of the met collection and allowed people visitors to the to the great hall to kind of step
in to this latent space as projected onto the ground and explore the traversal of the spaces
between works um and a projection behind them on the wall. We also made a web app version of all
this that exists even though we we can't visit the met today um it's online at gen.studio if you
want to go have a look after this and then all of the the code base is linked the github is linked
at the bottom um if you want to check out any of that more specifically but again it places us
kind of a different framing of latent space traversal than we're used to that I was interested
in this project was to place us on you know we've gotten a lot further along in the video um
you can go look at the at the website places us on a map between the objects when we're doing
the interpolations right so we select an object to start now we land in the latent space of big
gen close to that object and then we can move ourselves around on the map between objects and
their embeddings in that latent space right and as we're moving physically in 2d space here online
we can visualize what exists at that point in latent space and then we can find its nearest
neighbor visually uh in the met collection and find what object in the existing collection
is most similar to the hypothetical work that we discovered in the interstices between two
existing works um so give that a look and this project lives on today um and a couple of different
forms I'm still working with the artist Matthew Richie he was a collaborator uh with us on the
met project um on a couple of different tendrils of of this work where we're asking all right so
we can model projections of existing images in the met in the met collection by finding their
embeddings in some kind of large foundation model but now in 2021 we have things like StyleGAN ADA
that can can train on smaller data sets and do reasonably well in approximating data sets that
would correspond to a single category in the met collection so we've done that um we've trained
these models on sketches um Babylonian cuneiform tablets Japanese watercolors and some 18th century
European landscapes among other things and have individual models correspond to each of these
genres within the met collection and then we've been working with a friend in New York who has a
robotic oil painter and can actually create layered paintings of really short walks in latent
space along different dimensions in this model so think about physically visualizing some of the
durability work you've looked at in this course right could we make time paintings of really
short walks in latent space by superimposing robotic paintings of the visualized image kind
of at different points along that walk so that's that's being exhibited right now at UNT
in their contemporary art gallery let's see I've got a question in the chat room
oh it's just a compliment I will take it at any point yeah I think
can you please read it yes uh someone mentioned that this is a creative reason one of the most
creative reasons they've seen to do latent space interpolation since it scans yeah I think that
I had a slide a moment ago if you want to rewind in the recording of this suggesting that kind of
part of the advent of using GANs to model kind of large databases of creative work is that they
allow us to do a couple of things right that interpolation and that iteration and in cases
where you can't write down a feature language underlying a set of works because you don't
know our priority what it is you can imprint that or you can learn something of that in a deep
generative model right and then you can collaborate with that and hypothesize what might lie on a
graph of human creation if we presume that any creation artistic creation at some point in
historic time is if you think about it as the manifestation of a point on kind of a sea of
cultural influences and multi-generational practice iterative practice that's been shared
between peoples and generations and the creation of a single work is the enactment of that process
at some point in space it's natural to think of that in some sense as a model that we can capture
in a latent space where we're manifesting some part of structured space at some moment but this
allows us to iterate on that which I argue is similar to some historic processes of iteration
and collaboration across groups of people really quickly right um so I've been trying to take this
a little further now and ask all right we can make paintings of short walks in latent space we can
hypothesize objects that might have existed but we don't think they ever did but we still don't
know much about these models even if we train StyleGAN 2 on a set of 2,000 paintings in the
net collection uh you've probably seen some of the interpretability work adjacent to what Ali
has shared or David Bao's work so that style of thinking we don't know anything about this Japanese
watercolor model like what do its individual neurons represent is there a neuron for trees
well what is a tree here it's some brushstrokes we recognize as a tree but it's not something that
BigGAN trained on image that would necessarily recognize as a tree maybe more simply kind of in
the in the steerability context what do dimensions in the latent space of a model like this
correspond to right sure we can find things like zoom and 3d rotation because we can name those
transformations and then find directions that maximally correspond to them using that kind
of steerability technique there are all sorts of other directions like the ones we're visualizing
here that certainly have some affective meaning to the viewer that we don't know what they are in
the models terms or in the viewer's terms so at this point in this project we're thinking about
starting to name and understand dimensions underlying generative models trained on
bodies of artistic work from museum digital collections not only limited to the met but
around the world uh and our motivation here is to kind of create these alternate and imaginary
histories of art built from unique latent walks that we can visualize in real time with this painting
or computationally and then maybe understand something about aspects of picture language
that might be shared across you know vastly different genres so Babylonian cuneiform tablets
transformed from numeric to symbolic and image-based at a very particular point in history and can we
find a dimension in style GAN trained on a very different genre of art that corresponds to a
similar kind of transformation and as such can we build up kind of a picture language that would
correspond to diverse forms of art making right that you might not see in any of these different
categories of digital images on an archive but we might start to appreciate once we can investigate
them by training deep generative models on them let's get back to great okay so when we're thinking
about this intersection we've seen one example of modeling the structure underlying creativity at
scale and i've done other projects and you can find many examples online both of my work and
other peoples of trying to do this not for creativity at scale but for individual instances
of individual artists and modeling either the style or the processes of individual art making
techniques so all of these are kind of flavors of starting to imprint or grok or understand the
structure underlying creativity but not symbolically right so we don't we don't know how to interpret
these models even though we can visualize them and create really interesting hypothetical objects
that might be indistinguishable either from existing work or from one artist's particular style
we can also think about these models as a tool themselves for collaboration both in their creation
and iteration with others who contribute to their models and with the models themselves
which as i described represent kind of executable versions of collective cultural structure we
permit permit ourselves to think about them that way or facets of kind of a global creative identity
but as i mentioned now we're at a point with tools and computer vision where we can start to ask
what rep representations actually underlie these models trained on artworks that are themselves
executable versions of some collective cultural structure right well what is the structure what's
going on under the hood do they correspond to dimensions that we find meaningful when we look
at visual scenes and so in the next part of the talk i'll share a couple maybe more technical
projects that explore specific ways that humans can interact with generative models
in order to maybe learn something about human vision as well right so can we build
shared vocabularies that help us interpret dimensions underlying these models by designing
experiments that allow us to visualize and interact with images and latent walks like you've been
seeing i'll pause here because i need a sip of water and i'll keep an eye on the chat in case
anyone has any questions before we go on
all right looks like we are question free so far five more seconds
i guess i have a question yeah so this might be talked about later but i was wondering a little
bit about like in your research and kind of this field how much of like human interaction is like a
big part of it and kind of like the human coming in and saying uh how they think about something
and see where that agrees with the computer or like kind of like where that role is played
wonderful question so these kind these kinds of high-level questions that
get it some experiential component or design component of the worker i think really useful
ask more of them i'll tell you for different projects what that looks like and in the next
section of work it's going to be really obvious because there's human annotations but for this
project so the human would come in here you know we train models on datasets of art selected from
the met collection and these are small and these are subsets and they were gathered by
matthew richie and myself going through different genres in the digital collection of the met online
and like hand selecting images from those different genres right representative images of
different categories of work or maybe in a less fine grained way all images under some designation
so japanese watercolors between the 17th and 19th centuries so we made that selection and
tried training these models on a bunch of different such selections and decided which ended up you
know with so few examples providing at least a representative sample of the kind of work that
we know we saw there right and then here the selection of like walks through latent space
so think of those in the same way you've been thinking about the steerability walks
they were very arbitrary so that was a completely human selected so it's a kind of a different
approach to interpretability where it's steered by the human eye right we're not doing it automatically
and we're not doing symbolic it symbolically we don't know what these correspond to but that's
trying some arbitrary walk through latent space trying many of them and then the human then
selecting what to them felt like an artistic expression this is an art exhibit and then
in the next step we'll ask how can we do that in a more systematic way and start to build
a language corresponding to what those different walks could be a language that's shared by humans
and that takes at least right now a lot of human a lot of human interaction
you could think about ways to automate that we'll talk about that in a second
but with any kind of human interaction it's nice to preserve the opportunity for direct
engagement with models rather than intermediation by a captioner or something like that because
then you could imagine using your technique on different subsets of humans right on different
kinds of experiences so you might imagine getting an art historian to label and select
different walks through latent space here corresponding to very nuanced changes in the
development of Babylonian like cuneiform tablets right that a captioner couldn't recognize I
couldn't recognize so you might want to be able to pull different kinds of humans into the loop
at different times to engage in ways that kind of use their knowledge to create a unique synthesis
with a generative model so that's what engagement looked like here and then with this next project
it'll be super it'll be super clear and I'll make sure to speak specifically to that so thank you
yeah cool so next this is probably a summary of what you've seen so far in your IAP course so
there's a lot of different work on discovery of interpretable directions in the latent space
of different generative models right and we can steer images along those dimensions to create
interpretable transformations that allow us to interact creatively with deep generative models
right here we are deep learning for creativity but a lot of these examples presume what concepts
we're searching for in the latent space and in fact they do that really explicitly right we
will pre-define a zoom transformation and then maximize the similarity between some transformation
in the latent space and a zoom transformation as applied to some image maybe you've experimented
with code for doing that but that presumes we know we're looking for zoom in the first place
what if we find ourselves looking out into more you know uncharted waters so to speak
here we ask how we can learn kind of a vocabulary of visual concepts maybe one that you would apply
to those style gains we just saw train on the net images right we don't maybe we could look for zoom
but maybe there's all sorts of more interesting transformations we could do to those images
but we don't know what they are yet how can we learn a vocabulary of visual concepts rather
than pre-define them or labeling them after the fact so there are a variety now of unsupervised
methods for distilling these kinds of transformations in latent space that find principal components
of feature space of different layers the models activation maybe you've played around with methods
like GAN space that search for and rank where the largest principal components of the feature space
which do provide us with interpretable transformations but they're labeled after the
fact so we don't know if they're meaningful to humans kind of in their genesis but we can we
can describe them right by providing labels to them another point where the human kind of comes
in the loop but we want to see if we can build in human vision to the discovery process right so
to supervise it but to not pre-commit to what kinds of concepts we're searching for so in this
project we're trying to build or define a method for building a visual concept vocabulary for an
arbitrary GAN latent space so to put it more specifically we want to learn embeddings d maybe
you've called this w we want to learn some kind of walk in the latent space z if again we'll focus
on big GAN here of transformations that are salient to us in visual space and we can't define
an objective and optimize our d our walk to produce a transform in x in the image because we
want to learn the vocabulary concepts rather than pre-commit to them and we would have to pre-commit
to what that objective is right in order to optimize d so we're going to take a different
approach and instead sample the space of salient or possible transformations for some given point
in space for some given z and then use those sample directions as a screen so to speak onto
which we can project human perceptual judgments so that's a little bit of a gratuitous metaphor but
maybe a useful way of thinking about it and then we'll we'll disentangle the concepts that are
projected onto that screen into a vocabulary of open-ended compositional visual concepts
and what we're interested in here is the overlap between what's represented inside a model
so some deep features in a model's representation and concepts meaningful to humans in visual
seeing understanding we're asking how we might start to define although not completely but
start to define a shared vocabulary between the two or for a given model determine what lies in
that set overlap and I don't have to dwell too long on a lot of the specifics here it's all
online at that URL if you want to read the paper but as I mentioned the first thing we're going to
do is generate a set of sample images that produce minimal meaningful transformations in images
and then humans come in the loop again we're going to ask them to label them but here we're
forming the basis for the data set that we'll build our vocabulary off of and we want to keep in
mind that we want a vocabulary in the end that is both diverse so corresponding to a lot of
different changes that you can produce in an image and specific where a single transformation
corresponds quite reliably to one visual change across viewers so we do that by defining
mutually orthogonal what we call layer selective directions and these minimize change in the feature
representation at some layer of big care and at some layer we'll call it layer l and this allows
us to capture relatively focused changes because we hold constant how much the representation
can change at some layer and we do that for different layers to capture changes at different
levels of abstraction so as you can see layers closer to the image output control or fine grained
aspects of the image like the color of the walls and the bedspread and as we get closer
back to the latent space we're allowed to make kind of more higher level changes in things like
zoom and perspective of the scene and its composition so what objects are present so here we have a
base set of minimal meaningful transformations that capture changes in images at different levels
of abstraction we're going to ask people to label them because we don't know what's going on visually
in these scenes right so we started at a pretty small scale with just four categories in the
places data set and looked at big and trained on image net and places we'll just talk about places
here and visualized a handful a few thousand of these directions per category so in each of four
categories looked at cottages medinas so uh street marketplaces kitchens and lakes a mix of
indoor and outdoor scenes and then asked people to just simply describe the overall transition
that they saw when these directions were applied to different randomly sampled starting points in
the latent space right so one direction might take this cottage to this snowy cottage and change
something about the sky and change the snow so these these changes are still complex we can
recognize that it's the same scene and we can describe in simple language what's going on
but they're they're not disentangled yet right one direction might correspond to a number of
different visual changes so we did a little preprocessing to capture you know what kinds of
concepts are associated with each transformation and then we decomposed those annotated directions
into a visual concept vocabulary consisting of single directions labeled with single words
we formulated that as a linear regression and then solved for the embeddings of individual
concepts in the latent space of our begin and then we can basically read those off
of our matrix e and then transform the images by manipulating them some amount along those
visual concept directions happy to talk more details about that if anybody's specifically
interested or you can check out the paper itself we found over 2000 concepts this way
corresponding to lots of different types of visual changes so we can reproduce
transformations like zoom and rotation things like color but we also get kind of a unique
set of concepts corresponding to aspects of scenes like their mood for instance there's a
direction in latent space of big and that makes outdoor marketplaces more festive and here we
see applying that direction to an example marketplace and it rolls out a red carpet
hangs some flags and brings a lot of people into that market we can visualize kind of a
a sampling of these directions each applied to two different images in different categories
so some directions make cottages more manicured add arches to marketplaces add shadows or make the
whole scene blue and we see directions like this that generalize across all of the categories
it began that we looked at you can check out the lake category we can add sunsets but also do
kind of scene specific things like add reflections to water or make a lake scene foggier make a
kitchen more inviting or more modern and again we didn't have to pre-specify what exactly modernity
would entail when applied to a kitchen we learned that through sampling what humans associate
with a transformation that was sampled randomly right uh the humans labeled that as modern and
then we disentangled the specific direction in latent space corresponding to that single concept
word and once it's isolated we can apply a modern transformation and know that it corresponds to
what viewers found to represent modernity in a kitchen well I said we know that it corresponds to
what viewers see as more modern but we don't know that for sure right we still need to ask
questions like how generalizable are these directions do they compose right can we add
a festive direction to eerie and get something that's both scary and festive right or could we make
a kitchen both more modern and inviting so we asked those questions in a series of behavioral
experiments that I left for you to check out in the paper itself so we won't in the interest of
time go through those here but we do find that these directions are composable and they're generalizable
across categories so there are some cases where we can even add a concept that was learned in a
single category to a different category for instance making a cottage more festive right or
adding snow to a marketplace even though that's not traditionally seen there we ran a set of
behavioral experiments evaluating the extent to which this is successful and isolating a couple
of few specific cases where it fails okay so this this wraps up this method it's at a point
now where we're trying this with some of the the art models that I discussed previously right so
this was still just applied to big and trained on real-world images trained on ImageNet but you
can imagine using a similar similar method to find dimensions of visual interest that are also
meaningful to humans in the latent space of a model trained on art images and so decompose
future languages underlying different genres of art into something describable so that we
can make concerted manipulations to images sampled either from foundation models that
correspond to approximations of real-world images or to models trained on on archives of art images
themselves I'll pause here for any questions about this there's associated code also available
on that project page I linked have a quick question please um was there a reason you chose
big GAN over starting with like style GAN for this type of work uh no um just a couple of
different code bases that already existed um and people that had worked on big GAN for
like GAN dissection we had an easy way to dissect big GAN and hypothesize what kind of
things might be there uh so a lot of the GAN dissection work started with big GAN and so I
was picking up where that left off and asking if we could find like style vectors that corresponded
to scene level transformations instead of individual neurons um but I have extended this
to style GAN outside of the paper it's just not got it it's here yeah the method's pretty I mean
there are a couple of different small changes you have to make um but the method is pretty model
agnostic just like defining a set of certain directions that samples the latent space in
kind of minimal ways and the the method I described here is definitely not the only one you could use
for that right um you could sample them by just finding the principal components of the feature
space or you could sample them randomly right you could just find two points in latent space
interpolate and then get people to label what's going on there um we tried a lot of these different
methods uh and found that if you if you make random interpolations between two randomly
sampled points then there's just so much going on in the scene that there's not a lot of inter
observer agreement in how people annotate what they see there's just too much going on so we
need to isolate specific changes that's why we developed that layer selective method for isolating
minimal changes um but what if we used kind of the the principal component method right or used
something like GAN space um there we found that the principal components of the model's feature
space aren't necessarily the most interesting to humans so we might get a ton of different types of
rotating the scene but not a lot of different changes of mood or changes in color up there in
high-ranked principal components um so that's where that method came from but it's agnostic to the
set of directions and pretty model agnostic uh the annotation is another place where humans intervene
here to to tie in that last question um but you can imagine trading a captioner on a label
data set like this right a little larger than the one we collected so we're thinking about doing
something like that uh but preserving the human annotation does allow annotation you know in the
art context by experts as I mentioned so you might want to be able to do this at scale for a brand
new model and just have automatic annotations you use something like clip right for the kinds of
transformations you would see inside but preserve the opportunity for experts to to annotate kind
of specialized smaller trained models and there are results too from big ganttring on a couple of
different data sets if you're interested um I have a question regarding like the choice of
uh n in terms of annotations um so how did you arrive at this number and how are you I mean
how do you know like what number is kind of sufficient yeah good question um so I assume
you mean the total number of images we needed to annotate and not the total number of annotations per
image which end you mean I can talk oh I see I mean either yeah well okay so at both levels uh for
the directions themselves we needed to collect at least two annotations to be able to measure
intersubject agreement right uh we want to see if some direction is consistently producing
meaningful similar annotations across annotators we need at least two people to annotate them um
so for all the directions we evaluated we had two annotators label them and measured the interanitator
agreement using a couple of different metrics blue and burnt scores um but for a subset of
those we had 10 annotators annotate them and just had a look at interanitator agreement across a
slightly larger group uh for expense reasons we didn't do that for for all the directions because
it really wasn't necessary things didn't change that much and even in that subset when we went
from two to ten per uh per direction and then for the number of directions that we chose to
visualize it was not a very principled decision I'm afraid um we chose I think 64 z uh per category
and then a bunch of different minimal meaningful directions for them corresponding to I think
the same number of principal components that we looked at in the GAN space papers so maybe the
top 20 in each category so it was a bit ad hoc that decision um the the things that's going to change
we can distill vocabularies using this method for any size of annotation library right uh which is
one of the one of the beauties and one of the things that gives itself to to some of these more
ad hoc decisions um we're doing it analytically right if we go back to this we're actually like
reading off we're solving for the embedding matrix um of word embeddings in latent space
of concept embeddings so we could do this with like just a couple of directions
if you only had one annotation per concept it only appeared once then you're just going to get
for that direction um so as you increase the vocabulary as you increase the sample size you're
probably going to get a richer vocabulary but it's still possible to do on a vocabulary of this size
um so we're deciding now whether it makes sense to scale this up and collect like a number of
annotations where it would be possible like I said to to train a captioner on them to be able
to automatically label these directions rather than have humans do it so part of it is constrained by
the tractability of experiments on mechanical Turk right how many reliable annotations you
can get in some period of time awesome uh thank you yeah these are really useful questions these are
great um kind of along those lines more of a random question for the printer like the single words
for the labels yeah was it kind of agreed upon earlier like kind of what words you'd use because
like for festive maybe someone would say lively or for inviting you'd say welcoming is there like
kind of a similarity score for those words or really good question um no so this is it's only
preprocessed with like a little bit of limitizing so we collapse different endings people might be
using our different verb conjugations onto single verbs uh but festive would have a different
direction from lively uh kind of a next step in post-processing that we've talked about but
haven't yet done um is to just collapse across like wordnets and sets right so you could use
something like that to find synonyms of festive and then approximate one direction for lively and
then be able to break it down into something maybe more fine-grained um but there were no
kind of heuristics or standards for the annotators except you know they did they did a practice run
and looked at a couple of different examples and were asked to describe an overall transformation
that captured changes at lots of different kind of levels of abstraction we can look at the specific
what did we tell her that's on here yeah how would you describe the overall transition changes in
mood changes in objects or features of the scene don't mention your describing images so standard
kind of turk boilerplate just address the content what you see and then they could look at some
samples and then after they did a practice run they did the annotations um so any interanitator
agreement is just based on their word choice which in some sense is a raw window into perception
but in some sense that's bullshit and there's going to be a lot of noise there uh and we did see
that reflected when we used I don't have this on these slides um but when we use blue scores to
to measure interanitator agreement so when we use these layer selective directions to generate these
kinds of transformations if we get 10 people to annotate each transformation people might use
somebody might say eerie somebody might say spooky right somebody might say scary to describe the
sky uh that comes up as like quite different when you look at some methods of evaluating
interanitator agreement so we used first scores as well that evaluate the semantic similarity
instead of just literal correspondence words and found that annotations of these kinds of
directions performed a lot higher when we looked at semantic similarity of annotations as opposed to
just um just word based so there's definitely reason to start trying to collapse like that when
we look at the vocabulary too but we haven't yet in some sense it's it's kind of beautiful because
you can see all of the different words that people used to describe changes um but you'd get a lot
more power right if you could combine annotations for festive and lively and vibrant under one
umbrella bit of a trailer oh yeah thank you so much yeah any other high or low level questions
inter just have a maybe one or two more things not much so ask away if you do I think more more
of a higher level question I remember Ali in the first lecture um right you drew you had this
visualization of like two points in the latent space and you know a last function that would
steer like from one or trajectory one to the other but it was like something more like a curve
or something non-linear um right and you mentioned with Gannon version if you just interpolate like
draw a straight line between two points you have like all sorts of things happening I was wondering
if there's like a I guess almost like a like a and I guess unsupervised not a random walk but a
walk that would I guess lead to less perturbations I guess in in terms of like features I mean
does it make sense uh yeah we I really wanted to do that for this project um maybe Ali can speak
a little bit more about about his work there maybe after we stop this recording but um linearization
of this is a huge over oversimplification um and that would be one of exactly what you describe
as one of the things I'm most keen to try is taking non-linear walks uh through any of these
subspaces um so very on point question haven't done it you should try and do it um but describing
like this the semantic structure of latent space the semantic topology if you'll permit me that
is a really interesting question um because even the visual meaning corresponding to
some of these adjectives some of these words is not regularized or normalized in the latent space
itself so if I take five steps in the festive direction it might take me five steps to get
anything that will start to register to me as festive um but the walk size for a correspondingly
large visual change so to speak in a different direction could be very different um so some
transformations like making an image black and white this is anecdotal but you only have to go
like one step in that direction and then we'll visualize the change almost immediately uh so
we're not kind of we're not walking around in like a perceptually normalized space so to speak um
and there hasn't been to my knowledge a lot of work that's addressed that everything's been a
little bit at hawk um so thinking about semantic topology subspaces non-linear versus linear paths
and how we can think about kind of the concept mesh underlying latent space for different
generative models is extremely interesting to get to be a really cool area to do some working
cool thank you yeah let's ask Ali about that figure once we pull off here um I've got one more
thing to show you a quick example to hopefully spark more discussion unless anybody has anything
specific about this project we can always come back all right oh geez well last thing I'm going to
show you uh is still a beta and uh it's very it's very early and it's even thought development
but it captures something um that I think is deeply interesting uh and I think might be interesting to
you all um so the former method that I showed you for building shared vocabulary between humans
and models relies heavily on language right and so we get some direction and we're able to share that
between people and even to repeatedly use it to steer through model space um because we've given
it a label right we've used language and you might even argue that you know that's constraining the
space of what people can recognize in those initial sample directions because there might be some
genus or quad aspects of of images that we don't really have words for um but are still like really
recognizable or perhaps the verbal you know that the words you would use to describe something are like
quite complex and you wouldn't type that into an annotation like on mechanical Turk maybe you'd
want to describe the sky as like the sky you saw at your grandmother's house the day she passed away
or some flowers as effervescent like latte foam or a sparkling drink but you're not going to type
that into mechanical Turk and there's not a single word concept to capture it so that's
going to get lost in the method I described and lost in a lot of kind of standard either annotation
based or uh kind of hard coded direction search so I wanted to experiment with a way to capture and
learn um directions without language and this is like deeply inspired by the steerability work
of all these so you'll see a method here that is is similar to that in some sense
but we're allowing the human to define the transformation that they want rather than
pre-defining say a zoom or rotation transform using an algorithm we're allowing humans to come
into the loop and define that transformation purely visually by interacting with very very
small batches of images sampled from latent space or feature space at some layer and sort them into
classes corresponding to some visual feature its presence or its absence and this provides a pipeline
where users can steer just like in steerability work along dimensions that they discover however
that they define and they define them purely visually so labeling what happened just as a
matter of convenience but they're discovered um through vision so the way to do this is really
simple um take some latent space again a lot of these examples are are using big GAN you could
also use style GAN um take some latent space and sample images from it right if you're using a
conditional model so we pick some category here we're looking at lakes inside big GAN image or
big GAN places um sample some images for a user and then that user who's determining a visual
dimension of interest kind of looks over that image space and sees if anything stands out to them
across that that set of images so maybe here I noticed images that seemed kind of verdant and
fertile uh and maybe more more spring light but not totally seasonal you'll see where I'm going
it's kind of hard to describe and these were a little dreary or more wintry but there's not snow
so it's not really winter they're just kind of less fertile and vivid so that's the distinction I
want to make there um and the method is very simple just like the steerability work um and a
another example of work from Bolle we define a transformation just by learning a hyperplane
so training a SVM and learning a hyperplane it separates those two classes of images either
in the latent space or in the feature space of some layer layers activations and then when we can
steer some starting image in a direction that's normal to that hyperplane and steer it across
those classes right so I could take an image that starts in the kind of dreary or domain
or dusky or domain and transform it normally to that hyperplane and take it into the category
of things that I thought was more verdant right or more fertile but I could specify that separating
hyperplane just by sorting a shockingly few number of images um so we've done a couple of more like
fine-grained tests here but just for proof of concept you can discern these directions with
some degree of reliability with just like five to six examples of images in each category making
it really simple to interact with something like this just by dragging and sorting a few images
that are sampled in the latent space okay so there's a tiny example of a demo app we have for
this um and we're switching where it's hosted so it's not online at this very moment but it will
be next week but it's called the latent compass it was at NeurIPS Creativity I think last year
the year before um you'll see the the home interface in a second but what we do is just
what I said pick some category of BigGAN here it's BigGAN places um on the bottom you see images
sampled from that category and the user drags them to the right and left of the screen corresponding
to two different kind of categories of concepts they want to capture uh and then once the compass
calibrates and we'll see that in a second and you can drag any new image and then transform it along
that dimension so here we pick the closet category I've got full closets on the right
empty closets on the left and the dimension I want to capture here is something like fullness
so I'm going to see if I can I can learn a direction corresponding to the visual difference
between these two categories drag any new closet onto that center line and transform it along that
direction filling and emptying the closets and what if we tried a different category what if I
wanted to turn a medina into a full closet right what is the type of fullness that's relevant to
a medina oh well it's adding people instead of adding clothes suggesting that what's been learned
there that direction in latent space is abstracting generalizable enough to capture some visually
recognizable dimension of fullness that's meaningful to us in different scenes right and the model's
able to to generalize it in a way that's not totally dependent on the types of objects it saw
in one scene so it knows in a sense that clothes make a closet full but to make a market full
we're not adding clothes we're adding people and so the fullness direction is something that adds
more of whatever would make that scene full um to any scene that we're selecting in the model
right and trained on so few examples of course this this is really quite imperfect but it's a
good proof of concept of a way that users can interact super flexibly and really visually
with dimensions of interest and use that to kind of explore and surf the latent space of a model
by producing replicable repeatable directions that others can explore without having to use language
that's kind of a different way of carving up the puzzle of how to explore and assign meaning to
directions that we that we find in latent space okay that's at latentcompass.com
and we'll be back up next week I think bad timing okay so to return to our frame here
we've been digging a bit into this intersection between art neuroscience and machine learning
ways to explore models that have been trained on human creation right at different scales
to create a new to iterate and interpolate upon archives and then also to start to understand
what these models are representing and if our ways of interpreting dimensions inside models
can also teach us something about human perception or allow us to start to build models of aspects of
human vision that are otherwise pretty intractable because it's difficult to formalize what dimensions
underlie them where I could write down what dimensions under life physical scene understanding
because I know Newton's laws I couldn't write down what dimensions underlie aesthetic perception of
North African marketplaces or Babylonian tablets because I don't know what a large swath of people
would find perceptually interesting in a bunch of marketplaces I know from cognitive science
research certain heuristics to look for but that wouldn't give us a full set of what a diversity
of humans might appreciate when looking at some scene especially things like its mood
so we can turn here to these kinds of large unstructured generative models that learn
entirely from data entirely from images and turn to them as like a fertile ground so to speak for
starting to probe and represent human perceptual experiences inside their latent space and think
of latent space that way right as a screen as I said before onto which we can project human experience
and then once we have those projections we can rerun them and interact with them and collaborate
with them to create outputs of deep generative models that are particularly exquisite and that
represents some kind of collaboration between us and models of our our creation that are operating
in parallel so that's where I will leave us my emails here I'm very discoverable online
but you're welcome to write me questions anytime and I will wrap here and we can have a more
casual discussion unless anybody has any last questions for this part
Thank you so much sir this was really interesting and inspiring with all the
acidic decreasing slides and every moment of that was really full of thoughts I think that
you open a window to semantically and qualitatively looking at these latent spaces and
sort of our imagination and
where we dream and where these models that we create dream so I really appreciate that
I'm going to stop recording and then see if there are more questions

Hello, everyone. Welcome to your course, Deep Learning for Art, Statistics and Creativity.
Today, we have two special speakers. First, we serve as Dr. Jeff Klun, who is an associate
professor in computer science at the University of British Columbia and also a research team
leader at OpenAI. And he's going to talk about towards creating endlessly creative,
open-ended innovation engines. I think this is a very exciting direction because so far
we have talked about the interaction between art and AI. We said that how AI can help us
to create and express ourselves and democratize the creativity in a sense. But also, the other
direction is how our creativity can help us create better AI. For instance, how we learn
by creating, how we define problems and find solutions for them and generalize to solve bigger
problems and so on and so forth. So today is one of those, I would say, a realization of such a
great idea that you will see as a gist of what Jeff has been working on. So please go ahead.
And also, another question that we often ask in the class is that students are interested to know
a little more about your background because they always feel inspired by seeing great scientists
and what, for instance, got you to working on AI would be very interesting for them if you don't
mind sharing. Great. Thank you for the introduction. Let me share my screen here and make sure that
is working. So are you able to see my screen? Yeah. And the presentation? Yes. Okay. And can you see my
mouse cursor? Yes. Okay. Hello, everyone. My name is Jeff Klune. And I want to talk to you today
about trying to take on like an extremely big research challenge. I think it's a grand challenge
of AI. And that is trying to create what we call open-ended algorithms. I wasn't planning on telling
you a little bit about my background. I guess in brief, I started out on a quest just to understand
two twin questions, which is how did natural evolution produce all the complexity on Earth,
including the human brain? It's astounding. And we don't know how that process happened really. We
don't know how to recreate it. And you'll see a lot of work towards that today. And then also, I'm
interested in trying to figure out how does thinking happen and how can we create it in machines? And
I think in many ways, these questions are very intertwined, as you'll see also today. So I started
out in philosophy, actually, because I thought they had the market cornered on thinking, but really
quickly kind of, or actually not quickly, slowly learned throughout the course of my life that the
best way to tackle these challenges is to try to build these systems and recreate these systems
computationally. Motivated by the wonderful quote by Richard Feynman, which is, that which I cannot
build, I do not understand. So we understand by building. And that has certainly been true in my
life that I understand more and more by being forced to turn speculation into code and into
algorithms. So with that, I'm going to begin. So this talk is really going to be in two parts.
The main part is going to be the first part. And it's about creating open-ended innovation engines.
And if there's time, which I hope there will be, I'm going to rush through a series of work that
we've done that I call AI neuroscience at the end. And then throughout all of this, what you're
going to find is that this is a bit of a meandering intellectual story, because throughout my career,
different research has kind of unintentionally produced different aesthetic artifacts of interest.
And I kind of want to walk through some of the things and touch as many of these places where I
think our work has produced things that are aesthetically interesting, as well as scientifically
interesting. So the first thing I want to motivate is, you know, the idea of open-ended algorithms.
So these are things that endlessly innovate. They just keep going forever. So if you think
about natural evolution, look at the Tree of Life there, and think about all of the marvelous
engineering designs that nature has brought and continues to create in an ever-going fashion,
you know, jaguars, hawks, the human mind, everything that we know on Earth. You know,
in most situations, we cannot rival these things with engineering. And so what's fascinating is
that, you know, a very simple algorithm that Darwinian evolutionary algorithm, plus the context
it's been placed in, continues to innovate for billions and billions of years. And I think it's
really fruitful to think to yourself, you know, could you create an algorithm that you would want
to run for billions and billions of years and come back and check whether or not it's interesting?
Currently, as scientists, we have zero ability to produce things that are interesting even after
a few months of running them on a computer, let alone billions and billions of years. So natural
evolution is what we, you know, one of these open-ended algorithms. And another one is human
culture, which just endlessly innovates and produce innovation on innovation and innovation. That's
both true in science and technology, but it's also true in the arts, where you get, you know,
impressionism after you get the classical paintings, and then you get, you know, post-modernism or
Jackson Pollock or all the different kind of evolution of genres. So, you know, we started
with the idea, when we try and wanted to try to work on this, is that natural evolution and human
culture are what we call innovation engines. And that is that there's kind of this simple recipe
that they follow that allows them to be creative. And that is that they start with a set of things,
it could be an empty set, and then they generate a new thing. And then if that's interesting,
they keep it and add it to the set. And then they take something out of that set, they change it a
little bit, they permute it somehow, and they see if that is interesting. And if that's interesting,
they add that to the set. And you have this growing set of things, these archives of things
you've already produced that are interesting. And then each one of those is a stepping stone to new
potential innovations or solutions. And if you think deeply about it, that's true both of human
culture and natural evolution. And so the question is, with that kind of mental framework, can we
create algorithms that do that process automatically? And so, you know, at the core, there's really kind
of these two simple steps. The first one is you have to have something that generates new things
based on previous things. That's the green box on the left. And then you have to evaluate whether
not those things are interesting. And if they are, then you add them to the set and you just keep
repeating this process. So in the long run, what we'd love to do is take, you know, humans out of
the loop if possible, label data out of the loop, and you just have some sort of generator like a
neural network that can generate new things like poems or codes or mathematical proofs or images,
or, you know, technological artifacts, something, maybe another deep neural net that is trained
to recognize what's interesting somehow, and then that process could just iterate. Now, for example,
you could imagine that you take like the orange box here is an autoencoder. And it looks at everything
that it's seen before, it compresses them down to a low dimensional bottleneck space, and then it has
to uncompress them. And then if you get some new latent vector that's new that you've never seen
before, you call that interesting. And that might be one thing that could kick off this problem.
And if you could do that, you would have an innovations arms race in any domain, you could
unleash this thing anywhere. And that would be amazing. And a lot of these ideas date back to
Schmidt-Huber ideas from the early 90s. However, the problem is that when you do that, you typically
do get new things forever, but you don't get new interesting things forever. You, for example,
might get white noise, just an endless stream of different patterns of white noise, because those
are uncompressible. So really, at its core, the biggest challenge in this field is kind of how
do you avoid generating uninteresting novelty, and how do you only generate interesting novelty?
And here's one example by a friend of mine, Josh Auerbach, who tried to basically take the same
encoding that I'm going to tell you about later, and a similar system is trying to automatically
generate images and try to produce new interesting images forever. And these images are interesting,
they're pretty cool, but they're not nearly as interesting as they could be, right? They're not
like what artists would do over the course of centuries. You would expect and hope that things
would ultimately break out of these kind of abstract patterns. And that's because these
things are optimized to produce information theoretic metrics like compression or mutual
information and things like that. So what we thought in this work is that one insight you could
have is that recognizing a new type of thing is like being able to recognize a new class of thing.
If you've never seen a palm tree before, that's a distinct kind of trees. And if you've never
seen a tree before, trees are distinct from dogs and roses and statues. And so
one way to think about being able to recognize an infinite number of new classes is to approximate
that by having a neural net just recognize a very large number of classes. And so if you could
recognize, you know, a million classes, for example, then as the generator produces new
instances of those classes, maybe the process could like start going out and generating each of
these classes. And that allows us to then use supervised learning because we know how to recognize
new classes of things. So this is an approximation to the overall goal and to try to see if this
system can work. So the way that we wanted to approximate this, and this is all the way back
in 2015 before image generation really worked that well, is we said, let's take a deep neural net
that is trained on ImageNet, which is relatively new around that time. It has 1000 different classes
and it's really good at recognizing these different classes. And then we'll have, we'll use that as
our evaluator, which is the generator's job is to generate instances of that class. And then the
question is, what are we going to use for this, the green box here, the generator side. So what we
need is an algorithm that can recognize either an improvement on a current class, or when a new
class is generated. And so we decided to use this algorithm that I'm excited to tell you about,
because it has a lot of really interesting motivations behind it. It's called map elites.
And it has one bin per ImageNet class. And I'll tell you what map elites is right now.
But to tell you about map elites, I kind of want to motivate this whole field of a new kind of
type of algorithm that my colleagues and I have been working on. And it starts with this recognition,
which is that there's a paradox in life, which is that if you try too hard to solve a problem,
you'll fail. However, if you ignore the objective, then you're much more likely to succeed. So imagine
that you're in this maze here, and you're starting here, and your job is to get here. And you might
say, well, okay, make the robot who's here, make its objective, getting as close as possible to
the goal. Well, if you do that, and you get points here, these are all the points that get generated
by that search algorithm, because and most of them just go straight north, because that lowers the
distance to the goal. But then they just butt their head against that wall forever. This is a
classic local optimus, you're familiar with these things in search. However, if you simply switch
away from the paradigm of always try to optimize toward a goal, and you just say, let's just go
to new places, just seek novelty. That's what you get here. And eventually this search stops
focusing on just going north. It doesn't actually care more about north than going east. And eventually
it winds its way around, and it solves the problem. And this right here is a metaphor for every single
hard thing we want to do in search. If there are local optimal in space, if we need to explore
to discover this thing, then we probably should seek novelty more than an objective. And it's
even a metaphor for things beyond algorithmic search. It's also a metaphor for human culture
and even natural evolution. And the idea is that almost every major scientific breakthrough,
if you trace its lineage back, it's not a straight path to that solution. Instead,
it's a winding, circuitous route. So for example, if you went back in time centuries and you said,
I have this way of cooking food, and what I want is a faster way to cook food that doesn't produce
any smoke, then you would never, if you only funded work into improved cooking technology that can
accomplish those goals of heating things faster, you would never invent the microwave, which is a
magical invention. Because to invent the microwave, you had to have been working on radar technology
and recognize the chocolate bar melted in your pocket. Similarly, if you went back millennia
to this abacus and you said, that thing does computation, I want more computation. And you
only funded researchers who improved against the objective of producing more computation,
you might get abacuses with like longer rods, more beads, something like that. But you would never
invent the modern computer because to do that, you had to work on things like electricity and
vacuum tubes, which were decidedly not produced because they improved computation, although
they later proved instrumental to doing that. The same is true for going from this kind of energy
to clean energy, where you have to be thinking about things like space and time that were not
thought about because they would produce new ways of producing clean energy. So the conjecture here
is that the only way to solve really hard problems may be to create problems while you solve them
and goals switch between them. And so goal switching is this idea that if you're trying to solve one
task, and you make progress on a different task, then you should also start optimizing and getting
better on that different task. So if this robot here, this scientist here wants to make a walking
robot, and all of a sudden during optimization, the robot starts crawling or starts balancing on
one leg, you shouldn't throw that out as a failure because it's not helping you walk or making forward
progress. Instead, you should start getting better at those skills to add those to the set of things
that you work on. And ultimately, those might be stepping stones to get you to this walking robot.
So my colleagues and I have been creating this new subfield of algorithms of AI
called quality diversity algorithms. And this family of algorithms is trying not just to get
the single best solution to a problem. It's trying to do something very different. It's
trying to get a large set of diverse solutions, but where every solution is as good as possible
for that type of solution. You want the tallest in the giraffe or the fastest ant,
but you don't let an ant who's not that fast kind of get precluded by the fact that a cheetah is
faster. You still want the fastest ant and the best ant you can find. So probably the most popular
algorithm in this family at this point is this algorithm called map elites, which was invented
by Jean-Baptiste Morel, a great colleague and friend of mine, as well as myself in 2015. And
it's very, very simple. And the idea here is if you're going to solve a problem,
want to first choose or learn, but we started off by choosing dimensions of interest that you find
that you yourself like. So imagine if you're trying to make a car, for example, you might choose
safety and fuel efficiency as two dimensions of interest. And then you just discretize these
dimensions. And you look for the best solution, according to some criteria, like maybe it's the
fastest car at each point in this grid. And what you want at the end of the day is not just to get
the fastest car possible, but the fastest car for every possible tradeoff between safety and fuel
efficiency. So here's an example problem we tried this on. This is generating soft robot morphologies,
which is like the bodies of robots. So we gave this optimization algorithm those four materials
there. They're kind of voxels that can pulse at different times. And some are soft and some are
hard. And we said, you know, go fast. And, you know, first we did this without map elites,
we just did this with a canonical optimization algorithm or a genetic algorithm in this case,
which is just trying to optimize for speed. And what you see here is this kind of really
interesting parade, this Noah's Ark of very different solutions and very different creatures.
And, you know, people got really excited when we put this online and it's super fun.
But I think one of the things that people thought really interesting about this work,
including myself is the huge diversity of designs that you see here. You know, it starts to evoke
nature where you see a lot of different designs. The problem is there is a trick to this. And that
is that all of the designs that you just saw, each of those came from a different run of optimization.
The only way you got a diversity was by starting the run again and doing a massive search to find
one solution. But if you look within that population of creatures, they're all almost identical.
And that's not what we want. What we want on is an algorithm that will generate a huge diversity
of things within one run so that you can run it for billions of years and it would continue to
produce interesting new stuff as opposed to converging to one type of solution and getting
stuck on that kind of local optimal. So we took the map elites algorithm that I just described
to you and we applied it to the same software last problem. And what we did there, you know,
is we have to pick the dimensions and we chose to pick the number of voxels and then amount
of this dark blue material because previously it hadn't been using this kind of bone-like material
and we wanted to see it play with that resource more. And if you look at classic optimization,
this could have been RL, but in this case it's a genetic algorithm. Any optimization,
what you find is that it doesn't actually search the space very well. And so it has low performing
points and it didn't do a lot of exploration. If you add diversity, which we know historically
helps, you do get higher performing points. So you see these yellow points here, but it still
did not explore a lot of the space, even though it's incentivized to literally explore in these
two dimensions. Map elites is a qualitatively different algorithm. It's a sea change in terms
of what happens within the algorithm. If you look here, you see this rich exploration where it
fanned out and searched the entire search space and it taught you more about this search space.
It tells you, hey, there's not very high performing points up here. There's a little bunch of optima
over here. There's also this separate little area here that you probably would never have normally
found, et cetera, et cetera. I'm doing these interesting points over here that you can go
investigate. And what's interesting is it often finds a better overall high performing solution
than if you just do direct optimization because it's doing such a better job of exploring the
space of possibilities. So if you look at any individual final point, you can trace back its
lineage through time to see where those solutions visited in the search space. And what you can
see here is that they don't just kind of mine one area of the space and get better and better and
better at that corner of the search space, that particular tradeoff between these two dimensions.
But instead, the overall lineage takes these long, circuitous paths to their final destination.
Just as to get a human, you had to go through an intermediate stage of being a tapeworm and then
being like a tree dwelling. Actually, I don't know if we were a tree doubling, but kind of
something that looked more like an ape and all sorts of intermediate steps along the way.
So going back to the idea of an innovation engine, we now can recognize the algorithm that we're
going to use here. There's one final thing I need to tell you about, which is how are we going to
encode the images we're going to search for. And I'm going to tell you what I mean by the word
encoding, because I think especially for people who are interested in aesthetics, this is one of
the most important choices you can make. And you'll see this show up in Joel's work later as well.
So I'm going to tell you about the encoding that we use, which is a CPPN. So first,
I've been throwing around these terms, genetic algorithm and evolutionary algorithms. You may
not know what they are. I'm going to very briefly explain them. If you want to search for a problem,
this is also true in deep learning. The first choice you have to make is how to encode the
problem. So imagine if you wanted to search for tables. Well, you could decide I'm going to store
the length of each leg separately as a number on a parameter vector. We in evolutionary algorithms,
we call this a genome, but in deep machine learning, it's often called a parameter vector.
So you store the length of each leg separately and the width and the length of the surface of
the table maybe on this string of numbers, this parameter vector. Once you've made that encoding
choice, you then can score the population. First, you create a population at random by generating
random strings of numbers. You score this population to see how good they are. You select
which ones are better according to some scoring function, which could be your reward function.
And then you just take these things here, take their parameter vectors, and you perturb them
in a little way somehow. And then you get a new thing and then you repeat the process.
In the gradient-based method, this is kind of like where you take the learning
step based on the gradient of the scoring function. And then you repeat the problem.
So when I talk about an encoding, it's this first choice, which is how do we decide what is the search
space that we will search in the parameter vector and how does that map to the final solution?
And that is in evolutionary language, the process of going from a genotype to a phenotype,
or machine learning a parameter vector to a final agent or policy or artifact.
So there is this notion of a direct encoding versus a generative encoding. And a direct encoding,
you basically have one number on your parameter vector for every single thing in your final artifact.
So if you're searching for the weights of a neural net, then you search separately for a
number for each weight or for a table you search separately for the length of each leg.
If you think about how perturbations affect these parameter vectors, though,
they are mostly likely to produce non-regular phenotypes. So most changes are not going to
lead to a table that has to be flat and hold your coffee. And so that makes kind of a local
optimum between this solution and this solution. You have to go through this intermediate thing
unless you get lucky enough to generate a regular phenotype. If you have a generative encoding,
you reuse information in the parameter vector to produce the final thing. So you might just
specify the length of legs once and then reuse that for these four lengths of tables. And now
every single change to that parameter vector is going to produce a regular flat table. However,
you've lost something. You've lost the ability to express this type of table up here. And so this
is like a really, really essential choice when you go to produce any solution with search.
So generative encodings, you know, my colleagues and I and many others have been focusing for a
long time on why these types of encodings are really interesting. And some of the desirable
properties that we want is that you can get regularity, which means you can get patterns in
the final artifact. It might be the architecture of a neural net, or here is the hands on your,
you know, in your body. And what you see is there's a repeating theme in your hands. That's the
regular pattern. But it also has variation. Each of your fingers is a variation on a concept or a
theme. And that's kind of one thing that you might want while you search. There are some others
benefits here, but I'm not going to get into those. So this is something that I just think is really
fascinating to think about, especially if you're interested in aesthetics. And it also ends up
being helpful algorithmically. And it's going to factor into a lot of Joel's work, I assume,
depending on what he talks about. And this is this question of how does nature build the
astronomically elegant, complex creatures that you see in the natural world? Like a question
that I'm not sure if you've ever stopped and thought about, but it's a fascinating one to think
about is how does every cell in your body know what kind of cell to become? You have, you know,
the same software is being run in every one of your cells, the same DNA, yet some of your cells
turn into hair cells or spleen cells or liver cells or eye cells. How does it do that? How does
every cell know what kind of cell to become? Well, it turns out that nature is using a generative
encoding where it reuses information, where the cell fate, which is the type of cell, is a function
of its geometric location in the body. It's almost as if the body wanted to know the XYZ
GPS coordinates of each cell so that it could tell you, oh, if you're like up here, left of the
midline, three quarters of the way up the y-axis, then become a heart cell, for example.
So if you look through developmental biology textbooks, what you find is that these kinds
of geometric patterns are the lingua franca of developmental biology. So here's this beautiful
cartoon by Sean Carroll. So here's your DNA which has these genes on it. And in this developing
embryo are currently three different chemical patterns. They're called morphogens. They're
literally some protein that's sitting diffused inside this embryo. And if this gene here says
that protein A is present and B and C are not present, then this gene expresses and produces
a new protein, only where that's true. And so now you've combined these three pre-existing
patterns to produce this fourth new pattern. And this might therefore tell the vertebra and a spine
that they should turn into vertebra cells. You get this repeating theme down the middle, but only
the left half of the embryo. And if you look through that, go ahead.
Would I be able to interject real quick? Sure. My research is actually focusing on exactly this
same kind of problem, but in mammals. And so in mammals, the morphogen model explains some stuff,
but it's actually even more complex. It is much more complex. Everything in nature is much more
complex than we know. So I am simplifying here because I'm flying through this material. And
not all of the, not, it's not to say that the only thing that's happening is geometric patterning,
but it is, basically, I think it's the backbone of the way this stuff gets built. And so by capturing
that power and putting it into our search processes, we've gone a long way towards the power of
developmental biology. And you could argue that you've skipped out on a lot of the extra complexity
that would be very computationally difficult to simulate by doing these things efficiently.
Yeah. All right. That's a good point.
Cool. Thank you for the question. So getting to the issue I was just talking about, which is how
can we efficiently make this sort of a process happen? So what we don't want to do computationally
is have, like, diffusing chemicals in some chemical simulator, because that would be
really, really expensive. And so Ken Stanley, my longtime friend and colleague figured out,
is that you can actually abstract a lot of the power of this system without any of the underlying
chemistry and in physics in these things that are called CPPNs or compositional pattern producing
networks. And the idea is, is just like in nature, we're going to encode phenotypic elements as a
function of their geometric location. So here's how it works. You take a thing that you want to
optimize. This could be a neural network, it could be a robot morphology, it could be a
picture. And you provide coordinates for everything in the artifact. So imagine it's easiest to think
about pictures. So imagine you give every pixel an x, y coordinate, then you literally pass the
number, then those numbers into this function. So first you put in one, one for this pixel,
and then one, two, and then one, three. And you ask the genome as a function of those two numbers
to spit out the value at that location. And if this is a random function,
then you're going to get a random picture. But if this function here has mathematical functions
that, you know, have regularities in them, then you're going to get a regular artifact.
So for example, if you want left-right symmetry, you can pass the x-axis through a Gaussian here,
and then everything downstream of that Gaussian node will have left-right symmetry.
Similarly, you could have in the y-axis, if you wanted a repeating theme like segmentation,
you could pass the y through a sine function, and then everything downstream of that node will be
regular in that way. You can also add in linear things. You could say, I want to follow the sine,
but only add in a linear component, so like shift it or warp it or bend it in certain ways. So you
can mix and match asymmetric and symmetric and repeating themes to produce arbitrary complexity
using these geometric functions. And kind of what was really amazing at the time,
because image generation wasn't working very well, was the kind of images that would pop out of
these systems. So all of these images here were produced on a website called Pickbrier, where
humans manually choose which ones they find interesting, but the underlying encoding is a CPPN.
And Jill's going to tell you a lot more about like a modern version of this website.
So these images here are all encoded with CPPNs, and what you can see is very, very natural like
shapes, like things like left-right symmetry, repeating motifs, and the lineages as you kind of
permute and mutate these things. You go from a butterfly to a bat with these kind of beautiful
gradations and interpolations that are nice to see. Myself and my postdoc advisor, I took the
same exact idea and we just put it in three dimensions, and what you get are these nice
three-dimensional shapes, which also show a lot of these regularities. And then we went off and we
built this website called endlessforms.com, where you can go on, it's basically Pickbrier but in 3D.
You can take an individual shape and you can say, I want to further evolve or optimize that shape.
Let's see if this plays. Here, for example, you might take this lamp and you are presented with
a bunch of variants on the lamp, and then you pick the one that you like and you see the next
generation and you can kind of crawl through three-dimensional lamp space. And importantly,
if you find one that you like, then you can publish it to the website and other people can
pick it up and branch off of that. This is how you get that growing archive of stepping stones
that allows us to produce kind of an interesting exploration of the space.
Here are some of the other designs that popped out of this system, and here's kind of repeating
segmentation, left-right symmetry, radial symmetry, and mostly a lot of the things just look really
natural and interesting. So this is kind of a fun aesthetic space to be playing in using these CPPNs.
Because we could, we 3D printed the objects and allowed users on the website to 3D print them,
so it's kind of fun to hold these things in your hand, and you can therefore help people who have
no knowledge of CAD and design to produce arbitrarily complex images and then 3D print them
for whatever they want, like a chessboard or something. So when we put this out there,
people really found this interesting, which I think just goes to the to the fact that if you can
automate the design, if you can help people produce really interesting things that they're curious
about and they find exciting, but eliminate all the technical barriers to doing so, then people
get really excited about those tools, and Joel's website as a, you know, GAN breeder is a testament
to that as well. So going back to the overall scientific question here, which is can we use
this to create an open-ended algorithm? Now you know all the pieces of the puzzles. So we're
going to have AlexNet, which is an early image net network that was quite good at the time,
be able to recognize a thousand different classes, and then we're going to have an optimization
algorithm that's going to generate these little tiny CPPN networks that are trying to produce
images that light, that the DNN, the deep neural net, thinks represent, you know, are classified
as each one of the thousand bins in image net. So the idea hopefully is that you'll get goal
switching. So if one of the networks is the best dog we've ever seen, or particular dog,
and then a permutation on that produces the best fish we've ever seen, then now that network can
go to hop over to that bin and start optimizing to become a better fish. And maybe that produces a
better stepping stone for a cat and then a bird, etc. And the hypothesis that we wanted to test
is, is that better than separately optimizing for each one of the bins in image net? So here is
the performance over time. Time here, training goes from bottom to top, and the category of
thousand image net classes are along the x-axis. What you can see is that over time performance
rises with training all the way up to one, you know, red in most places, which means that the
deep neural net is certain that this thing is a lion, and this is a starfish, and this is a guitar.
So my question to you is, knowing that the deep neural net thinks that each one of these things
is in that category, you know, what do you think they look like? And if you had asked this question
in 2015, 2016, people would have said they look like electric, you know, starfish and guitars,
but you probably now, because you guys are, we've had the benefit of a few years, you probably are
used to the idea that what you do, what you get is not that, but you get these things that are called
fooling images or adversarial images, which is to say that the deep neural net is absolutely
certain that this is a starfish, and this is a peacock, and this is a king penguin, and this is
an electric guitar, even though they obviously are not those things. So at the time, this was a,
this, we published this paper, deep neural nets are easily fooled, and it was a really big wake-up
call to the community that AI sees the world differently. There are huge security concerns
here, and this generated a tremendous amount of discussion and awareness amongst the scientific
community, the machinery community, and also the broader public about the fact that these new
tools that we're building have a lot of deep flaws within them that we need to worry about.
Nowadays, everyone's very familiar with adversarial images. At the time, this was not very well known,
and so I thought that was interesting. However, I also think from an aesthetic perspective,
it's interesting that we were trying to generate innovation engines and generate images. We weren't
trying to study neural nets and whether they had flaws, and then this just kind of popped out,
so I thought that was an interesting story. But while some of the images didn't look anything
like the categories of interest, another thing that we found interesting is that many of them
did, and from an aesthetic perspective, this is pretty cool because now you're getting an automated
art generator. So for example, matchstick, television, and bagel, they pretty much do look
like those things. However, I also think from an aesthetic perspective that some of these really
evokes an artistic interpretation of what that abstract platonic concept represented by that
class is. For me, this image of a prison cell evokes more than just a picture of a prison cell.
It seems to me like an artist decided to represent the bleakness but also the hope or
something about this prison cell. And so even though there is no artist that was trying to
capture that behind here, there's a neural network that's kind of captured the platonic
concept of a prison cell, and that somehow leads to its own dialing in of what is central and
essential about that concept, or at least evokes those kind of reactions in us and allows us to
explore potentially new types of artistic and aesthetic connections to concepts. So if you
look through the diversity of the images that were generated, I do think this kind of really hit
the mark in terms of a quality diversity algorithm. You've got this huge set of images as all comes
from, you know, one run. And at least I'm not, I think that they are, they might have been pulled
from a couple of different runs in this case. But each one produces this giant, this diverse set
of images, and many of them I think are really aesthetically interesting, like I think this
volcano or this beacon, or this cup, I could actually imagine a coffee shop where this is this
logo, your comments on a mask and a banana, etc. So we really, really thought it was cool to see
kind of this pop out of an automated system back in 2015. Scientifically, we're also really
interested in like whether or not goal switching was playing a huge role in these networks. And so
we have, if you optimize for a single class only, like the water tower class, what we see is that
you do indeed get stuck on a local optima. It lands on this particular pattern really early in the
run. And then it just does minor tweets on that idea and gets stuck on it until eventually it kind
of maxes out what you can do in that corner of the search space. In contrast with map elites,
what you see is that early on it locks on this half dome moon image, and it does okay, but then
it kind of gets stuck. And then from a totally different class, something that happened to have
been produced to for the beacon class, actually ends up looking like a better water tower and
goal switches in, it invades this class. And then with further optimization to look like a water
tower ends up making the DNN think with 98% confidence that this is a water tower. And you
can kind of see why. And we see this lesson over and over and over again. There's many goal switches
happening within this population of networks. And we think that's a big reason why performance is
much higher than when you optimize for a single class. So what's really interesting about goal
switching is that it allows what what are what biologists call adaptive radiations. So you come
up with a good idea like maybe a more efficient way to metabolize oxygen in one lake in Africa.
And then that idea will spread to all of the surrounding lakes in Africa. And then on top of
that technological foundation, those fish will respecialize to their particular niche and adapt
that innovative incorporate that innovation. The same thing happened with Darwin's finches,
which radiated out from one from one couple of finches to all of these diverse finches.
And we see the same thing in technology where computers, for example, were invented for one
purpose and then kind of spread throughout an ecosystem and are now embedded in all sorts of
technological devices in our lives. So what's really nice is you can see these adaptive radiations
happen in these quality diversity algorithms. So this is one of my favorite plots from all of
the science I've done in my entire career. Inside one of these innovation engine runs,
you've got this early innovation, which is this dome against a background, a colored background.
And that thing, which looked up the abaya class, then radiates out and it's children because this
is a population. So these literally are descendants of each other. It's descendants kind of produce
a phylogenetic tree, just like we see in nature. And ultimately, this innovation turned into a
volcano, a mosque, a water tower, a beacon, a yurt, a church, a planetarium, an obelisk, and a dome.
And it's just awesome to see an innovation then get rid of that concept, get rift upon and kind
of radiate out into a huge explosion of diversity. So if you study the history of biology, you'll see
that there were many moments in the history of biology where something similar happened. We got
like, you know, single multicellular organisms or rate or bilateral symmetry or the four-legged
body plan. And then you see this explosion of diversity that descends from that central innovation.
So I think it's beautiful to see that happening inside of our algorithms.
We ended up submitting the art that was produced by this algorithm to a competition at the University
of Wyoming where I was a professor. And every year, art students work for a year and they submit
their best project to this competition. And then there's a judges who decide which of them get
hung on the wall and accepted into the competition. So we did not tell them this is AI-generated art,
we just submitted it. And not only was the art accepted, it was also given an award.
So here you see people having wine and cheese. And I was like eavesdropping as they're discussing
the intent of the artist behind producing all of these different images, not knowing that it was
an AI algorithm behind it, which I thought was pretty cool. So in some sense, this passed the
artistic turning test. Sample size one. FYI, in case you're interested, there is much more work on
CPPNs that are more modern. So nowadays, a lot of people are playing with differentiable CPPNs
instead of using evolution. I have to because it's so beautiful. Quickly look at the work of Alex
here, which I highly recommend you check out. All of these things here are different CPPN
represented networks that are doing deep visualization, which is the technique I'm
going to tell you about later. So I encourage you to check that out. There's also, you can
use CPPNs to encode neural networks. I did that a lot in my dissertation and now you can do that
with Backprop. David Ha has been pushing that and there's much more work in this vein. Okay,
so getting back to QD, I think that I hopefully have convinced you that it has all of these nice
properties, like a diverse set of high performing solutions that it produces, it has goal switching,
and it allows you to kind of illuminate the entire search space and learn a lot about what's possible.
Just quickly, I want to say that these ideas really have given us a lot of leverage on hard
technical problems. So in this paper that we had in Nature, we use these ideas to have robots that
could adapt to damage within one to two minutes to get up and continue on with their mission,
even if they're extremely damaged. And then we also use these ideas behind the algorithm GoExplore,
which you may have heard of, which completely solved the Atari benchmark suite, including
solving really hard exploration challenges like mono zoom as revenge and pitfall. You can see all
the previous attempts to solve this heartless game, which became kind of its own grand challenge
of the field, do not perform very well. And then this is the difference once you start adding in
these ideas from quality diversity algorithms. Ultimately, we ended up beating the human world
record on this game. Oh, and as a quick little teaser, this paper was also recently accepted
to a really nice journal. I can't quite tell you which one, but if I'll share that information
on Twitter in the next couple of weeks, if you are interested to get the final version
and the updated version of this paper. So I think QD algorithms are really interesting.
I think the question that we should always ask though is what's missing where, you know,
they're not yet open-ended algorithms. So the thing that I think is missing is that while these
things can produce a large diverse set of interesting solutions within one domain,
ultimately, their ability to innovate is constrained because they're stuck in this one
particular setting that we put them in. But what we really want is these open-ended algorithms that
just keep going and kind of generating wildly different solutions as they run. So traditionally
in ML, we pick a particular challenge like Chester, Gro or Dota or Starcraft and we bang
away on it for a while. But the intriguing possibility that I want all of you to consider
today is could we create an algorithm that generates its own challenges and solves them?
Just as nature arguably created the challenge or the opportunity of leaves on the top of trees,
and then the solution to that challenge, which is giraffes or caterpillars that can eat them.
So, you know, this kind of a thing might produce something that's interesting
after a billion years. So our most recent work on this is in this algorithm called Poet,
which is the paired open-ended trailblazer. And the idea here is that we're going to try to endlessly
generate interesting, complex and diverse learning environments and their solutions.
So the idea is again quite simple and you'll recognize it. It's basically we want to
generate new learning environments and we're going to add them to this set of our population of
environments if they're not too easy and not too hard for the current population of agents.
And if they're novel, there's something about them that's unique and different.
And then we'll optimize agents to better solve each of these challenges and we'll allow goal
switching between them. So the example task that we used here is obstacle courses. So this little
creature here has to run as fast as possible without falling over. And here's the general idea.
You start with an easy environment. So first you have to make that encoding choice. How are you
going to encode an environment on a parameter vector? Here we have things like the number of
whether or not there are gaps, whether or not there are stumps, the ruggedness of the terrain,
et cetera. So you can start with an easy one of those, which is maybe just flat terrain.
And then you start having an agent, which has its own parameter vector. This is a neural network
and is learning via RL to solve this task. And once it gets good enough on that task,
then we copy phi 1, the parameter vector of the environment, to make phi 2. And then we'll try
this agent via transfer and goal switching. It goes and it starts optimizing here. Now,
we are simultaneously continuing to optimize this parameter vector on this environment and this
parameter vector on this environment. We keep going. Maybe eventually this environment gets
solved well enough by this parameter vector. So we copy it and we now make phi 3 a new environment.
Turns out that's too hard for either theta 1 or theta 2. So we throw that out. We generate,
we try again, we get a new environment and we test this one and this one. We take the better of those
to you on this new environment to seed training. And in this case, it was theta 2. So it goes in
there. This does not have to be a linear chain. At any point, any one of the environments in the
set can produce a new environment. And then we'll try all of the current agents on that environment
to see if they're the best and if they are, they get to start. And the process can keep going like
this. Now, imagine eventually we generate a really, really hard challenge like phi 6 here.
And initially the best parameter vector, we try all of them on this environment was theta 5. It
was the best stepping stone. So we start optimizing a copy of theta 5 in this environment and it gets
better and better and better. But it maybe hits a local optimal and it can't break through and
really, really do well on this environment. But in the meantime, we're still optimizing theta 4 on
this environment. Maybe it has an innovation that makes it better on this environment. So it invades
this environment, just like a species in nature could invade a new niche, kicks out that parameter
vector. And now we start building on the back of this innovation here. And then that maybe with a
little bit more optimization comes up with an innovation that then transfers in and becomes
the best thing we've ever seen on phi 6. And maybe that gets us off the local optimal and solves that
problem. So that is kind of the nature of goal switching. So here we use evolution strategies,
but any RL algorithm would work. And you can see this little agent here. And it is traversing this
course. And what you can see is at the beginning, all of the challenges are quite simple. They're
a little tiny stumps, little gaps, just a little ruggedness in the terrain. But over time, the
agent gets better and better. And the environments automatically start getting harder and harder.
So it's kind of like a natural curriculum generation. And you can still, the algorithm
is here is kind of still pushing in separate dimensions, like taller gaps or more ruggedness
or wider gaps. Sorry, I didn't tell her stumps. Later in time, with more training, the algorithm
starts to put together these challenges. Sorry, my dog is barking. So you get things like bigger
gaps and stumps and ruggedness all put together. And ultimately, these environments get really,
really, really difficult for this little robot to traverse. Here's another challenge that was
invented and solved by this algorithm. So I think from an aesthetic point of view, it's kind of cool
because you can think about each one of these robots as its own little creation. It's kind of a
curiosity. Just like animals in the world, we love to watch nature shows and see different animals
and how they're different and what they can accomplish and how their bodies are different,
et cetera. So you can kind of think of the agents produced by these things as really interesting
aesthetic artifacts. Scientifically, we wanted to see whether or not goal switching in this
domain was paying off. And so we did direct optimization in each one of these environments
and found that it failed miserably. That's down here. And with Poet and the goal switching,
what you see is much, much better performance in each one of these environments. This is the only
way that we know of to go solve these hard problems. And in the paper, there's more of a
detailed study about that claim if you're interested. So I want to show you one anecdote of what popped
out in the system. So I think it's so interesting. So here in the simplest possible environment,
a flat ground, you get this agent here that is optimized for a really long time and it's got
this knee-dragging behavior. And eventually, the system generates a permutation of this
environment, which is a harder challenge that has little tiny stumps. And this knee-dragging
behavior is not very good because it keeps tripping up on these little stumps. So with
some more optimization in that environment, the agent learns to stand up and it gets faster at
that. Now, because the algorithm is always checking any solution to see whether or not it's better
at invading some other niche, this descendant actually goes back automatically and invades that
flat ground, replacing this knee-drager. Now that it knows how to stand up, as you can see here,
it gets much better performance in that new environment. And then with further optimization,
it ends up with much better performance. Now, because this is a computational system,
we could do the counterfactual. We went back to this original agent in the top left and we optimized
it for an equal amount of computation in that flat ground environment. And it just never learns to
stand up. It's just on a local optimal and it's stuck in its ways. It was only by going into a
harder environment and coming back that it learned a better behavior and a better strategy. And this
is why I think that it's so hard to design curricula. You would never, as a human, say that you're
going to take something to a harder environment just to have it solve a simpler environment.
But in this case, that's exactly what was needed to solve this problem.
So we go through a quantifying algorithm that goal switching is essential to solve the hardest
challenges generated by this system. So future work in this domain, I think there's all sorts of
stuff you could do. Obviously, you could just take it into more complex rich simulators. So,
you know, you could have more complex encodings as well. But here is like the world's from deep
mind. But I think it's really kind of pumps my intuition is to watch, you know, what's possible,
what will be possible in the future with more computation. Like imagine what Poet could do
in a world this complicated, where it has to do with flying creatures and climbing and talking
to other agents, maybe negotiating trades in a market, you know, and if you were doing all of
this, you know, what might pop out of the system, I think it's fascinating to consider, both from
a static perspective and from a machine learning perspective. You also could optimize the bodies
of the creatures themselves. So in the bottom in the right, you see, you know, I showed you some
work that we did in that vein a while back, but not with Poet. And David Ha has done that in
particular environments that are handcrafted. But imagine if you paired body optimization with
environment generation, then you could really get weird things like you see in nature, where you
have a particular kind of like cave dwelling spider that's optimized to that environment,
which is very different from birds that are flying up in the Pacific Northwest.
So another thing that I think would be interesting would be to combine innovation
engines with modern tools. So imagine if you took something like Dolly, which is this amazing
new thing produced by my colleagues here at OpenAI. And not only did you have humans asking for
particular innovations or particular images from Dolly, but you have the algorithm invent the
challenge and the solution. So the challenge could be, you know, can you create this? Can you create
this? Dolly would then create them. And if they're interesting, you add it to a set. And then you
have something that looks at the set of things that are already produced and produces completely new
types of images. That would be awesome to see. And that doesn't have to be limited to images. You
could use then the same technology to do it in different modalities, such as videos and music
and poetry or algorithmic space. Again, the challenge that remains is how do you detect
what's interestingly new? I'll throw it out there that I think you probably with a lot of data could
learn a function of what humans consider interesting. In fact, if Joel remembers, I sent him a giant
email saying that I think we should do this with his website, GanReader. We haven't done it yet,
but it'd be a great project for a student to take on. So I want to quickly check the time here.
Yeah. So we started a little bit late. So I'm going to race through this because I think you'll
find it interesting, but I won't be able to go into any detail here. But part two of the talk,
which I'll do very quickly, is I wanted to tell you about this entire other arc of research that
we did called AI neuroscience, which is how much we want to study. Just like neuroscientists try to
study the human brain, we want to study how much the deep neural nets understand about the images
that they classify. So we're all familiar with deep neural nets, but they tend to be a black box.
We don't really know what each neuron in the deep neural net does. But one way neuroscientists probe
this question is they literally put probes into your neurons and they look for which neurons light
up in response to which images. For example, they found neurons that light up in response to
Kobe Bryant or Bill Clinton, for example. And people have called these things like a Kobe Bryant
neuron, for example, and they respond to very different modalities, such as the name Kobe Bryant,
a line drawing him in the Lakers uniform. The question is, you don't really know just because
the response to those images, if it's a Kobe Bryant neuron, it could be an LA Laker neuron
instead of a Kobe Bryant image neuron, for example. So we thought the ideal task would be to synthesize
the images that maximally activate that neuron. And if you did that and you got these images,
then you'd know, oh, that's a Laker neuron, not a Kobe Bryant neuron. But if you got these images,
you'd know it's a Kobe Bryant neuron. So this is actually possible with artificial neural networks,
but you can do is you can take a neural net and then you could have like an artist, an AI artist
that's trying to generate an image to activate this particular neuron here. And what you can do is
you can use backprop. So the artist generates an image and then you just follow the gradient to
increase this neuron until you get an image that lights up that neuron, and it might look like this.
And you can do the same technique for all the intermediate neurons in the neural net.
We call this deep visualization. Our first attempt at this actually was that same paper,
deep neural nets are easily fooled when we did it with CPPNs here or a direct encoding on the left
here, or with backprop on the right, we got images that did not look at all like things that they're
supposed to, but the neural net was perfectly sure is a peacock or chimpanzee. And you know what
happened with that paper. We then went on and started asking questions like why are these neural
nets easily fooled? And I don't have time to get into a lot of the details here, but what we basically
thought is that maybe deep neural nets do recognize the images they're supposed to like a lion or a
dolphin, but maybe they recognize a whole lot of other things also as in that class unnatural images.
So if we could stop the artist from generating unnatural images and only stay to the space of
natural images, then we might find out what that neuron really is for and what it's been trained
to see within the space of natural images. So skipping over some of the details here,
the fooling work started out saying maybe these deep neural nets don't really understand at all
what they're classifying. They're just locking on to spurious correlations like that there's a
orange texture. If you see orange, you know, this kind of orange texture next to blue color
of starfish, but they never learned like what a five-legged starfish is because they didn't
need to to solve the problem. We wanted to see whether or not there is that notion of like a
five-legged starfish in the network. So in take two, what we tried to do is we added more manual
priors to try to constrain the image generator, the artist, to generate only natural images. And
when we add that extra constraint, then we get images, you know, previously people had done that
and they kind of looked like this. You start to see dumbbells and dolomations. These are the
ones that we got with slightly better priors. And you can start to see that the network does
actually kind of know what a flamingo is or a beetle. It's an interesting historical side note.
These images here in this work inspire deep dream, which is also done by Alex over at Google.
And that stuff is super cool if you haven't seen it. And then third take, we tried to add even
better priors, manually designed priors, and what you get are these images here. And I want to stop
for a moment and kind of reflect on this from an aesthetic perspective. We're trying to do better
and better science. We're creating different algorithms or different hand-coded priors to
kind of accomplish the scientific quest. But if you look at the different images, each one of them
has a different style. And I think it's kind of interesting that like slight tweaks to algorithms
produce wildly different artistic styles. It's like all these different artists are out there
and you just kind of are searching through the space of artists kind of accidentally
while you're doing your science. So this style is very different from this style. And I actually
think this is just really beautiful. Like if I saw this in an art museum, I would think that this
is beautiful art, even though it was produced purely for scientific reasons and we had no
intention of producing images in this style. We then went on for one more take at this. We tried
to say, okay, we're machine learning researchers instead of manually encoding what characterizes
a natural image. Let's learn it. And so we start learning the natural image priors and our papers
are full of lots of details on this. And the way that we do this, we have a generator kind of like
the generator in a GAN. We hook it up to the target network we're interrogating. And then we try to
search in a latent code to produce an image that activates a certain neuron in question.
And when we did that, we got these images, which at the time were some of the most realistic images
deep neural nets had ever produced. You were seeing realistic lawnmowers and lemons and barns
and candles. These images are not great by modern standards, but this is 2016. Here are other images
in this class. And for the first time ever, the images were starting to look photorealistic.
Like these are the synthetic images for this class. And these are the real images.
And, you know, I don't think that you would really be able to tell the difference if I had swapped
them unless you look very carefully. So compared to the best work at the time, which is on the left,
these images were a big step up. And they helped us confirm this hypothesis, which is basically,
if this is the space of natural images, these networks do understand what it means to be a
lawnmower. Like if this blue line here is the class of lawnmowers, then they do stay to, if you
keep them in the natural, if you only generate images in the natural image space, then you do
get a lawnmower. But if you let it generate images anywhere in the space, like all the way out here,
then it also, the network will similarly say this garbage here is in the class of what it
means to be a lawnmower. And so if we want for aesthetic purposes to have neural nets generate
realistic stuff, we got to get it focused on something that both is natural and activates
the network's classification as opposed to way out here. And GANs do this also, but they do it
via a very different mechanism. So I told you, you could look at each individual neuron within
the network. I don't have time to go through this now, but if you're interested, then I encourage
you to kind of go into the paper and look, you could kind of fly around the neural net and see
that you get things like cargill grill detectors and buckets and bird heads. And as you go up in
the network, you get these really weird concepts like one-eyed turtles and like arches over water
until you eventually get the class neurons where we know what they are because we've grounded them
via our labels. The one final thing I'll mention here is that the one problem with our technique
is that it generates very, very little diversity. So these are synthetic images produced by our
network for this class. And they look a lot like the images that most highly activate that neuron
from the real world from the real data set, but they don't represent the diversity of images in
that class. And so we did a lot of work, including adding with Yashua Benji on these things called
plug-and-play generative networks, where we wanted to add a lot more diversity. And so you could
take the same network and you can light up a bunch of different classes that it's never even seen
before. That's a bit of an aside like ballrooms and art galleries, but mostly we were interested in
getting more diversity. And the takeaway message is we were able to accomplish that. So here is PPGNs,
which is the one that has more diversity. And you can see a much more diversity in this set of
images versus DGNAMV1, which are the images over here. And this diversity better represents
kind of the diversity of the natural class. So with the original attempt, DGN, you got volcanoes
that look like this. It kind of goes and finds one type of volcano, like a local optima, and it
sits on it. But the plug-and-play generative networks are much more kind of like an open-ended
algorithm, at least within this class, where it samples new versions of volcanoes over and over
again. And so you get all this big diversity of volcanoes out of this new sampling technique.
So to conclude this, the AI neuroscience part, I won't actually get into these details, but it
taught us a lot about what neural nets, you know, what's going on inside neural nets, it taught us
whether or not they really recognize and learn about the concepts in our world. Like we did find
in the end that they do know what a volcano is, and you know the five-legged nature of a starshow,
and what a lawnmower is, even though they also are susceptible to producing and recognizing
these adversarial fooling images as being part of the class. And it was cool to see the rapid
progress just within my own team of collaborators from 2015 to 2017. And since then, I highly
recommend the work of Chris Ola, who's continued to push in this direction. And very, very soon,
Gabriel Go and Chris and others have new work coming out of OpenAI that will blow your mind.
So I encourage you to watch the OpenAI blog in the coming weeks for this new result that you
really like. You could do all of this stuff in different modes, like speech and video, etc.
I won't dive into this. I want to just highlight one thing. This is my future work slide all the
way back in 2016, and I thought it would be awesome to do this with real animal brains.
Since then, actually, somebody has done that. They took our algorithm for DGN,
they applied it to a real monkey brain, and they synthetically are generating images that activate
neurons within the monkey brain, specifically within the face recognition part of the monkey
brain. And you do in fact get a synthetic monkey-looking face, which is pretty amazing.
So to conclude my overall talk, I think innovation engines are really interesting because they kind
of push on this question of can we automatically produce an open-ended creative process that in
any sort of modality like art or music or invention will just endlessly generate interesting new
things. We've got a long way to go to accomplish that goal, but my colleagues and I, like Ken Stanley
and Joe Layman and myself are really, really focused on this goal and trying to pull that off,
including now at OpenAI, where all those people are. And I also showed you very, very quickly some
of our work in AI neuroscience, which we were doing for scientific reasons, but produce these
interesting aesthetic artifacts. And I'll just leave you with one final thought, which is that I find
it surprising how often science produces aesthetic artifacts. Almost none of the work that I was
doing was trying to do it just for aesthetic purposes, but along the way, it produced these
things that I think are beautiful and interesting, and could be kind of aesthetic artifacts in their
own right. And so I think it's nice because you don't have to choose between being an artist and
a scientist. You kind of kind of can do both nowadays, especially with the modern tools and
machine learning. And I'm sure that's kind of a realization that is being reinforced over and
over again with all the different lectures in this wonderful class that you are participating in.
So with that, I want to say thank you, and I'll turn it over to either questions or Joel, depending
on what you want to do, Ali. Thank you so much. I appreciate it. It was really interesting and
inspiring to me, and I'm sure for many of us in this class, this comment that you also made about
science and creating and art, I think that it also is very well aligned with some of the other
insight that we learned from other speakers. For instance, Alyosha, of course, was mentioning that
when I asked this question, he was mentioning that he also thinks that, you know, creativity is
a different tier of our evolution. So that really resonated with me what you were talking today.
And I think that this is very exciting for us. One question that I have is if students want to,
because this is a very interesting topic, and especially that type of poet or open-endedness
area, if a student wants to join you in this sort of mission, what do you recommend to them to work on?
Yeah. So one thing I would recommend is we had an ICML tutorial, I think about a year ago, that
really covered a lot of this work in more depth. It's an ICML tutorial on population-based methods.
So then you can see, can Joel and myself kind of going through, this is Joel Lehmann, not Joel Simon,
going through a lot of the work that we've done in this field. And I recommend reading a lot of the
work of both Ken and Joel, as well as you can look into some of the work that we've done in this area.
And then in terms of what I recommend you work on, there's so many things, it's like,
there's so many options that it's fun. You could apply a lot of these algorithms in a new domain,
for example, that you find interesting, you know, a new kind of art. You could take more modern tools
that work really well and weave them into these ideas, or you could invent new ideas, you know,
like I still think if people, if anyone out here can crack the question of how can you automatically
recognize newly interesting things, that I think is like a Turing award-winning innovation that will
catalyze and propel so much algorithmic advance, including potentially advancing our push to
artificial general intelligence. Like that might be one of the key stepping stones that gets us there.
So I have this paper called AI Generating Algorithms, which I recommend people check out if
they're interested. And it basically talks about how these sorts of ideas may do the fastest path
to produce general AI. So that's not an aesthetic quest, it's more of a scientific quest. And but
if you're interested in that, I think that's fascinating. But I also just think, just literally
take all these ideas and go like do Poet, but do it in some totally wild and crazy different domain,
or do an innovation engine in, you know, like architecture or poetry and see what happens,
you know, you can use new tools like GPT-3 or Dolly, etc. So I think there's just a lot of
low-hanging fruit here to be explored. Certainly. And that also reminds me of what you mentioned,
we didn't optimize to create a microwave. We explored different things and I think that
your advice is quite in that direction. Also, Joseph has a question. Joseph, would you like to
ask it yourself or? I wasn't able to get my mic working earlier. Let me just
fix that. Hi, Jeff. I was just wondering if you've explored anything on Poet in multi-agent settings
to this point. Yeah, the short answer there is that we have a lot of really exciting ideas.
For how we want to take advantage of that. I can't share those specific ideas
because we may or may not be working on them. But I also think in the spirit of the talk that
the best way to make advances is to have a community of people with different ideas pushing
different directions because you never know what's going to unlock. So I almost don't want to give you
too many ideas either because I don't want to cause conversion thinking. I think it's almost
better if there's so many different ways you could apply the concepts of Poet to multi-agent
settings that I don't think you can go wrong. I think if many different people and groups push
on that, really good things will happen. Fair. We also may or may not be working on that, right?
The one thing I wanted to ask you specifically about that is whether you figured out one,
maybe you can't tell me, but any way to get around the problem where in multi-agent settings,
sometimes you don't have a single evaluation metric that correlates the environment difficulty
with agent performance because you add more agents in, well, then the performance goes down
because there are more of them and they're all doing smarter things. So that sort of thing has
thrown a wrench in the whole annex measure. Yeah, that's right. So one of the things you could switch
to is a notion of agent versus agent. Like if an agent is as opposed to doing better on that
environment, it's that agent versus other agents or agents that have come before. Another thing
you could do is you could switch to more of a learning progress metric, which is if they're
getting better, are they learning? According to some measure, like does their value function,
their prediction of how well they're going to do, is that wrong? It's because they were either
better or worse in that situation and versus those opponents than they thought they were,
and measures like that could really catalyze, recognizing this is still an interesting environment
because they're learning. This is still an interesting matchup between this opponent and
this opponent because they're learning. I mean, we've been actually trying to do something very
similar there. It still seems to run into the same sort of issue though, right? If you can't
measure absolute performance, it can still be difficult to then measure relative performance
because your reward peak can be going down even if you are learning because so are all the other
agents. Sometimes you have to run as fast as you can just to remain in the same place.
Yeah, pretty much. That's the red queen quote from Alison Wonderland. Yeah, these are all challenges
and it's the kind of challenge that happens once you get into the multi-agent setting.
So I think this is just for a lot of experimentation and hard thinking has to happen. I don't think
there's a really super, short, easy, obvious answer. It's just going to require research.
Well, I mean, I look forward to seeing what setting it is that you're trying that out in
whenever that gets published. Likewise, yeah, with your work.
Thanks. Excellent. Are there questions? Any more questions?
Guys, don't be shy. If you have questions, just go ahead. Of course, if Jeff has time.
I have time. I just also want to be cognizant of Joel and giving him his proper time.
Excellent. Okay, cool. Thank you. All right, then let's thank you again, Jeff. It was really,
really interesting and inspiring.

All right. Hello, everyone. Welcome to your course, AI for Art, Aesthetics and Design
and Creativity. Today, we have a very special lecturer, AJ. He has been at MIT just like
you for his undergrad. I got to know him when he was here and he's very active. I've been running
at the ML groups and sometimes chatting with me about, you know, these topics of creativity and
AI and art. I think that this is very exciting. He's going to tell us about his journey and
and his new work. I will let him to, you know, start the discussion. AJ, one of the things that
I always ask is that if you could please introduce yourself and tell us a little more about what
inspires you to work in this area. Sounds good? Yeah. Yeah, absolutely. I'd be happy to.
And thanks so much for having me. So today, I'm going to be talking about some work I've done,
some works that's happening in the community around 3D content creation.
But first about my journey. Yeah, I was at MIT for my undergrad and I was part of what is now
called the AI Club. And then we called it the Machine Intelligence Community.
In my undergrad, I did research in a couple of areas, but mostly actually in compilers.
So a little distant from what I do now, more on the high performance computing side and
performance engineering that had experienced self-driving cars and generative models for
self-driving applications during undergrad and really fell in love with that topic. How do we
reason about uncertainty? How do we model complex data distributions and predict the future?
Like, for example, predicting the behavior of vehicles. And that led me down the path of
working on generative models in my PhD. And these generative models are, these days,
the state of the art generative models are parametrized by deep neural networks, which try to
fit large data sets, try to estimate correlations between different variables. And these could be
old types of different data modalities, like images, they could be trajectories or behaviors,
like I worked on, audio, video. And today, we're going to talk a little bit about 3D objects.
And so that's kind of what inspired me. At the time, I was interested in uncertainty estimation.
But these days, I just really like the tangible results you can get out of generative models,
novel samples, and novel designs. It's very fun. I got to look at pretty pictures all day.
To my research interests, like I mentioned, around generative models, we've done some work
in denoising auto encoders. How do you generate images with these denoising diffusion probabilistic
models? That's purely in the 2D setting, though it's been extended to other domains.
Over the past year and a half, I've also been doing a lot of work in 3D reconstruction and
inverse graphics. So how do we take images and try to infer a scene from them or generate in
the 3D space? Sorry, Ajay, interrupting you. It seems that some of the students want the
transcription to be on. Is that okay? That's fine. Okay, excellent. Thanks.
And building off of that performance engineering background I had from MIT, I also did a lot of
work in the intersection of machine learning and programming languages at the start of my
graduate school. And I would summarize kind of my research interest as making it easier to
create creative content, especially with AI tools. And to provide some background for today,
I'm going to discuss different types of scene representations. What I mean by this is how do
we encode the geometry and colour of a scene in some format that we can work with digitally.
And there's this very long history of this, particularly from the graphics literature.
On this slide shows some different representations of geometry that you'll be familiar with some
of them. 2.5D might include RGBD images, like a photo plus a depth scan. And they're point
clouds, meshes. Meshes are the most common representation used in graphics applications,
but they can actually be challenging to work with in a learning context.
So our focus in the learning context will be on the volumetric approaches. These can be very
easy to train. You can kind of think of at least a boss of greatest classifier, mapping each point
in space to whether it's part of the object or not, whether it's occupied or not. More recently,
there's been a significant amount of interest in neural scene representations, sometimes called
implicit neural representations that define the geometry of the object with a function.
That could be a distance function, so a network mapping from coordinates to the distance to the
nearest surface. And these can be a lot easier to optimise. These neural scene representations
can also compress the data significantly compared to explicitly representing the geometry of the
scene. So we'll be focusing on that direction. And in particular, we're going to be discussing
a model called neural radian fields I'll get to in a second. But they address this problem of
view synthesis. So how do we take some sparsely sampled input views of a scene and then construct
a representation of that scene's 3D geometry and colour in a way that allows us to render it from
new perspectives? These are some example works. You can represent the scene as a multi-plane image.
So instead of a flat grid of RGB values represented as multiple planes, and that allows very quick
rendering from new perspectives. Neural volumes is an approach from Facebook that has an encoder
decoder structure. Take some input images and encode them into a layman space, kind of a 3D
layman space and decode it out to images with volume rendering. Neural radian fields have
really been very popular over the last two years due to their simplicity and quality of the results
they can generate. So here's an example scene that's captured on the Berkeley campus. Some photos of
the scene are captured, for example, with an iPhone. I believe these are captured with an iPhone.
Then poses for each photo are estimated. And this neural scene representation called the neural
radian field is estimated off of those images. What's really nice is once we estimate the
representation of the scene, we can render it from novel viewpoints and kind of smoothly
interpolate these sparsely sampled views. The scene is only very sparsely observed from discrete
points. What if you as the user want to make a photo from a new perspective?
There's some interesting things to note about this rendering. Notice the specularities on the
surface. They're not just modeling the diffuse light. Also modeling how the light reflected
back at the user depends on the viewpoint of the camera. As you shift your head, the scene will
change. This is particularly visible on very shiny surfaces like the glass or metal of the car.
A neural radian field, yeah, it really is amazing and really captured the attention of a lot of
people. This neural radian field has grown very quickly and there's still a lot of problems to be
solved. One very interesting thing that these neural scene representations bring to mind is
that we're encoding a scene in the neural network's weights. Instead of explicitly encoding the
geometry of the scene via points or meshes, lists of triangles or voxel grids, it's encoded
into this small multi-layer perceptron. Maybe this is a half a million parameter network,
just some stacks of dense layers. It's representing a function mapping from 3D space coordinates,
XYZ. This is in the scene, XYZ coordinates, and a viewing direction. What is the angle of the camera
in order to model those view dependent effects? The neural network then predicts at this coordinate
what is the color of the scene and then its density, sigma. There's density as something like
how solid is the object and how much light will be absorbed.
Rendering is done by ray tracing. This is not exactly what would be done in most graphics
applications like real-time ray tracing because we've kind of encoded the light being reflected
at any given point back towards the viewer into this function. So we don't have to scatter
light through the scene. The viewer will cast a ray from their camera through the pixel. This is the
image plane into the scene and then query the neural network along the ray. These are different
3D coordinates in the scene. The color of the rendered pixel will then be some accumulation
of the colors along that ray. In order to determine the color and the density along the ray,
each of these coordinates is passed to the MLP as a very large batch.
We get a color and density for each coordinate and then can compose them with alpha compositing
into a color. There's some subtlety to this compositing. This is called the volume rendering
equation because this equation is pretty simple. This is the density predicted. This is the camera
origin and it's displaced some steps along the ray. The neural network will predict what is the
density of the scene at that point, but it will also predict what is color. Then we're integrating
this color along the ray weighted by its density, but we also have to weight it by
transmittance, which is roughly speaking how much light is transmitted from the viewer
to that point along the ray because once we've accumulated enough density, then objects
further back in the scene will not be visible to be included. This equation for color
conditioned on coordinates is differentiable with respect to the parameters of sigma and c.
So sigma and c will be this neural network. Because this is fully differentiable, it's
relatively easy to optimize. Instead of optimizing scene geometry, we'll optimize the
weights of this neural network in order to get some desired colors. This might be the sparsely
observed viewpoints, two viewpoints. Let's render the color according to the neural scene
representation and then try to optimize the network so that it matches the observed views,
pixel by pixel. It might take a minute to wrap your head around, but it's actually pretty simple.
We have this one MLP that encodes the scene, lets us render viewpoints differentially,
and we'll optimize the scene so that it matches the input views, and that's why it's inverse
graphics. We're going from the 2D space to optimize for the underlying 3D representation
that will reconstruct those views. Are there any questions about that?
Can you talk about how the points are used for the neural net? I can't see it directly.
I mean, I get the high-level idea, but could you talk about how those points are fed into the net?
Yeah, so you could consider constructing an MLP with five dimensions as input,
just five inputs, and then four outputs on the layers, and then intermediate features or whatever
you can imagine you want. So in nerf it's 256, that would be one approach, and it does work,
but then you get actually quite blurry reconstructions of the scene if you directly feed an input
coordinates as these are just floating point numbers, 3D coordinates, and going that direction.
But instead, what is used in this neural radiance field is assigned useoidal
positional encoding, so frequency-based encoding. If you're familiar with the transformer positional
encoding, this is a common approach where continuous values like coordinates or time
steps are encoded using a Fourier representation. So you take sine of various scaled values of the
input coordinates, and that lets the network model high-frequency detail.
So instead of kind of memorizing a function from each spatial coordinate, it can model
frequencies of the underlying signal if you use the sine useoidal embedding of the input.
So that kind of just projects this five-dimensional input into some higher-dimensional space
before feeding it to the MLP. I see. And there's no, let's say, I guess, filtering
done before, I mean, applying it to the net. So it's just transforming certain, I guess,
components, but not doing some, I guess, post-processing before putting it into,
or let's say compression or something like that. No, not really compression. There's
some, like, coordinate transform because you'll do this computation in a particular coordinate frame.
There is some subsequent work which we actually build upon that does a pre-filtering of the
input coordinates. So instead of encoding all the frequencies of the input coordinates,
they'll be blurred depending on how far away from the camera you're querying.
But that's sort of subsequent work to nerve. That reduces the aliasing.
Let's see. And the network is just fully connected.
Yeah, just a fully connected network. Super simple.
Yeah, thank you.
Yeah, one more thing that I wanted to mention here for a student is that there is a difference
between how you train this model versus the models that so far you have seen for,
for instance, classification. For instance, if you want to train a model for a truck,
what you do is you get a lot of images of different trucks in different lightings and different,
you know, models and things like that, and then fit it to your network. However,
in this case, you are taking lots of images of the single truck, single scene,
and you are trying to reconstruct that scene. So you said big difference between, you know,
what you are used to doing and what we see in nerve.
Yeah, absolutely. I kind of see it as instead of, the nerve is representing a single scene. So
instead of representing explicitly or representing it with a neural net with a function,
but it doesn't generalize. It interpolates these input views.
And, you know, there's a catch to that, which is that in order to fit into the neural radiance
field to a single scene, it generally needs a lot of data. So while these views are sampled
sparsely, discreetly, and there will be larger regions of space where we don't have
an image taken from that perspective, still to estimate a multi-view consistent radiance field,
experiments in the paper used a large number of images per scene. That's a little bit impractical.
So for these synthetic scenes, this is one synthetic scene that's rendered in blunders.
They were able to get out 100 images of each scene and fit the neural radiance field on it.
For those outdoor scenes, I showed earlier like that red Toyota car.
I think it's a fewer, maybe 20 images, but still that's a lot to capture with a handheld camera.
And in the first week of work, I'm going to talk about we improved the data efficiency of the
neural radiance field training process. So instead of using, let's say 100 images on this Lego
scene, we used eight photos taken from randomly sampled viewpoints.
In the neural radiance field training process, we would take, we would know the pose of each image
that can be estimated with a system like call map. It's really common in the 3D graphics and 3D
computer vision community is given some images, estimate their camera poses with correspondence
finding, then the neural radiance field loss renders an image or renders some rays from the
same pose as the input, then it computes a mean squared error loss. So the pixel wise error.
The reason that this loss can be used is because we know the camera pose, we're able to render out
the scene from the exact same pose that the observer took the photo. If the camera poses shifted
in the rendering process, then the reconstructed image and the true image won't align pixel wise
and we'll learn from inconsistent geometry. And so this is done at all of the observed
camera poses. And this is why the neural radiance field needs so many photos. If there's no observed
photo, then it doesn't have the ability to compute a loss from a given perspective,
which means that it could overfit to the input use. This representation mapping from coordinates to
colors is very flexible. One possible degenerate solution would be to put a billboard in front
of each camera, just a poster board, you know, off of the highway, right in front of your camera
containing the image that's observed, rather than learning a consistency in geometry.
And there's other ways you can get artifacts. This is described as a shape radiance ambiguity
in the Nerf++ paper. Essentially, we could either reconstruct the shape correctly and then have
relatively constant radiance from different cameras, or we could encode each image into the
view dependent coordinate of the network. So because the network depends on the camera position,
it's able to memorize potentially the photo taken from each camera.
When the neural range field is trained with 100 views, it gets really crisp reconstructions. This
is a hot dog scene, synthetic scene, where we render out the views in Blender. Then the neural
radiance field, when it's trained with only eight views, only matches pictures close to the training
data. When you move the camera further away from the observed images to try to extrapolate,
then there'll be a lot of artifacts. If you regularize the neural radiance field a little bit
and simplify it, it can learn more consistent geometry, but there still are a bunch of artifacts
in the reconstruction. I'll skip over this. So in our work, we add an additional loss to the
neural radiance field training. We keep using the Nerf mean squared error loss. It's called
photometric loss on the observed views that are sparse. But then our work diet nerf adds an
additional loss at unobserved positions. So because we have this neural radiance field
at any iteration during training, we're able to render out novel views even before the scene has
converged. It's a little silly that in Nerf, we're not able to constrain these input views,
because as a person looking at, okay, let's say that our estimate of the scene's geometry
gives us these renderings. This is the observed rendering. We as people can still look at these
photos and derive some loss signal. Okay, the input view is a lot sharper than my current
estimate of the scene. There's a little red light at the top of the truck, but there's no light on
top of these reconstructions. Based on this principle that you can compare views at different camera
positions as a person by comparing their semantics, like, you know, it's a bulldozer, a bulldozer is
a bulldozer from any perspective. We propose to add a loss in feature space. Using some visual
encoder, each of the input views is represented in a feature space. And then instead of computing
the loss in pixel space, diner will compute a loss in feature space. And that allows us to regularize
the scene from any perspective during training. We call this a semantic consistency loss,
since we're making sure that these semantic features, things like object identity, object color,
are consistent across views. And over the course of training, this improves the results.
So the loss that Nerf used was this mean squared error loss, and then we're adding this semantic
consistency loss where some encoder thigh, some neural network encodes rendered images, and then we
compare them in a feature space. We do have to sample camera poses in order to render this, so
there's just some prior distribution over camera poses. The choice of that feature thigh is really,
really important for the results, because we want it to be consistent across viewpoints. So it should
really encode the object's identity and properties about the object rather than low-level details,
like the exact pixel colors. And motivated by that, we use a network called Clip. This is from
last year. It's a representation of images and text, so a representation of an images learn,
such that it has an aligned representation with an associated caption. The data that Clip is
trained on is a very large data set of 400 million images that have associated captions
crawled from the web. And the Clip model has really led to an explosion of work in the AI art
community. It's really powerful. It's trained on such a large amount of data that we're able to
prompt it with topics that you wouldn't find in a narrow data set.
It also, by learning to match images to this text, we'd hope to learn some very useful features
about an image. For example, in captions, you can encode classes of objects, just like image net
labels. You can also encode a lot of other details, like the scene rather than just the foreground
object. You can encode things about pose of the underlying object, like a sitting person,
a standing person. And that should be encoded in the representation learned by the network,
if it's going to be able to match images against their associated caption. So the
training objective is encode a bunch of images, encode their captions, and then try to match
images with their true caption. Clip was originally used for discriminative tasks,
object classification in a prompting fashion. So if you want to classify photos of food,
the authors of clip constructed a bunch of captions, templatized with the desired object
category, a photo of guacamole, a photo of ceviche. And then the class label is given by the
caption with the best match with a given image.
The property we're particularly interested in in this 3D reconstruction context is whether the
representations of the images are consistent across views. That's what we call semantic
consistency in the work. What this plot is showing is that the cosine similarity of embeddings
from the image encoder of clip within a particular scene from different camera poses is highly
similar. So very high similarity in feature space within a scene across different perspectives,
low similarity across scenes at different perspectives. So because images are very
similar in clip's feature space, very different in pixel space, we're able to maximize feature
space similarity of clip and get some useful loss. Now what you've been waiting for are the results.
This is nerf trained on eight views when it's simplified. And then here is it trained with
our additional semantic consistency loss. A bunch of near field artifacts in nerf,
but when we add in this feature space loss, it removes a lot of those artifacts.
Because those artifacts aren't plausible, they reduce the semantic consistency.
Cool. I'm going to go on to the next work. Before I do, anyone have questions?
I have one question, which is with regards to using clip. Are you able to access the text
as well that clip generates, or are you able to decode it in some way and actually access
how the clip looks at the inputs? Just in terms of explainability, I thought it could be,
yeah, sounds really interesting. Yeah, that's a very good question. So in this work,
we're not actually using the text encoder. We'll see that in the next work. The text encoder is
just used for pre-training clip in dye and nerf. So we're only using this image encoder.
Because then the motivation for that is that the neural radian students are motivated by the
view synthesis problem. So there's no text caption associated with the data.
They just have a couple of pictures. So we only need to use the image encoder.
That said, some artists have tried to take clip and use it to create a captioning model.
If you have a model that can match images against captions, can you actually synthesize
captions that best match a particular image? It's a challenging discrete optimization problem
because you're searching for a textual caption that will maximize some neural network's output
score. And that is basically a black box optimization problem. My impression is that
automatic captioning with clip doesn't work too well. It's really good at selecting an associated
caption out of a list of candidates. And that's how we're able to do object classification with clip.
So I think you'd be better served by learning a specific captioning model that will generate
a caption condition on image rather than trying to extract captions out of clip
just due to the difficulty of the optimization or the search. Thank you.
So like I said, we weren't using the text encoder in diet ner. In the next work, we try to
move in an even more extreme direction of generating objects without any image data.
So what if we only have a caption and want to synthesize the 3D object from it?
Is that possible? Can we remove this mean squared error loss entirely and only use
feature space losses? And these are some examples of the results we're able to get
with different captions, like a render of a Jenga tower produces this object.
You can also engineer prompts, use hashtags because clip is trained on web data.
Our goal is to synthesize 3D objects from just the caption.
And to kind of refresh our memories, the neural radiance field is an inverse graphics approach
where we have densely sampled images, optimize the shared scene representation,
and then are able to render out new views. In the dream fields work, the second work in this line,
we do not have any observed images, only a caption written, for example, by a human artist.
We optimize something that will look fairly similar to diet ner with additional regularizers,
and then are able to render out new views. And any perspective is actually a new view
because we haven't observed this scene. This is an associated scene for the caption,
an epic, wondrous, fantasy painting of an ocean.
So the neural radius would use this mean squared error loss, and then diet ner used feature space
loss where the rendered image of the scene and an observed image of the scene are encoded into
feature space that is optimized. Oops. Sorry. Okay. Now in dream fields, we use the text
encoder of clip. That wasn't being used before. We were just throwing it away after trading.
So instead of optimizing for the feature similarity in image feature space,
we now maximize similarity of image and text features. The reason we can swap between
the text encoder and the image encoder is because clip has learned to align representation.
It has tried to maximize the similarity of representations of images and their associated
captions so those representation spaces overlap. And you can in some sense swap the encoders
from text encoder to an image encoder and hopefully still have that aligned representation.
But overall, the pipeline looks fairly similar. So it's randomly sample
poses in the scene, render an image, and then try to make sure that its semantic features
match our features of the caption. But if you apply that approach naively without any regularizer,
then there are a bunch of artifacts. These are some example generations for different captions.
I believe this one had something to do with liquid in a blender. This one might have been
a colorful bus with graffiti on it. So without regularization, we are getting to generate scenes.
And it's not surprising because there's really no data involved in this process.
In Dietner, the scene was regularized by having some input views.
Here, the canvas is open, wide open.
So in our work, we added some regularization. The scenes are composited with randomly sample
backgrounds. And we regularize the scene to be highly transparent. So this transmittance loss
encourages varsity in the underlying scene. So instead of getting lots of low density
wispy content, like you saw in the previous slide, with a transmittance loss and this
associated background, our motivation in Dreamfields is to create more of a consistent
foreground object, a single foreground object. And these are the renderings for the
associated caption, washing blueberries. There's definitely a lot of room for improvement
because each of these blueberries is kind of mashed together with the others. The general
caption has been encoded into this scene. And there's a consistent foreground object.
This is the visualization of the process of optimization. In response to the question,
Leandra asked, so it's creating this from one image, there's actually no images observed.
There's only a caption fed to the system. So any images that I'm showing are rendered
using our neural radiance field. They're completely fictional.
I mean, some intuitive explanation for this is how can we learn a scene representation such
that it could be captioned with a given caption from any perspective.
Maybe that's how a human sculptor went approach the problem. So given a caption, like give me,
you know, a clay sculpture of a tower. Well, let's say, you know, a monocular sculptor.
Good. Optimize for a clay sculpture that is a tower of many perspective.
Sorry, what happens? Sorry, what happens if the caption is something vague, like just a dog?
How would your optimizer know that, like, it should have the same dog even from different
poses or camera poses? Yeah, excellent question. The constraint that views should
represent the same object from different perspectives just comes from the shared
three presentation. We're optimizing the same MLP from any perspective.
Okay, thanks. We had to simplify. Well, we didn't have to. You're able to keep view dependence in
the neural radiance field. So this regularizer ends up being kind of important. Like I discussed
with Dietner, if you're able to learn a lot of these near field artifacts.
Sharing the scene representation is important, but some of the other techniques on our paper,
like the regularizer are also important for making sure you get a clean result.
In this example, we experiment with different caption templates to measure the
compositional generalization of the model. So the base caption template here is a teapot
in the shape of a blank, a teapot imitating a blank. And then in the video, the caption beneath
each object is the word that's filled into the template caption. So a teapot in the shape of
an avocado produces this object. Whereas the caption of teapot in the shape of a glacier
produces something more ice styled. And I'm sorry about these animations.
If you switch the caption from an armchair to a teapot, you'll also notice some changes in the
shape. So there's legs on this avocado chair, but when it becomes teapot, the legs are removed.
There's a follow-up question about whether the Clip Library is 2D. Yes, Clip is trained only on
2D images. So just on 2D views. The motivation for using Clip is that we can very scaleably
acquire caption images from the internet. If you, for example, look at Wikipedia and just look at
the upper right image associated with each article, it has a caption beneath it. And there's a data
set out there called WikiText, which has about 11 million captioned images.
The authors of Clip were able to collect even larger data set by scraping websites other than
Wikipedia. But if you look at data sets with 3D objects in them, they're very small. The largest
might be ShapeNet, which is entirely synthetic objects. And there's usually no caption associated.
So we'd have to have a human annotate. This is a general trend in the 3D reconstruction literature
that the availability of 3D data is quite limited. And so in dream fields, we're able to exploit this
pre-trained 2D image encoder and text encoder, and then kind of lift it up into 3D by using
a shared representation of the geometry. There's a bunch of different techniques that we use to
improve the quality of the results. I won't get too much into this, but the metric is a little
tricky to define because there's no reference object for each caption. We only have a data
set of captions provided to us by the user, and we're one of measure how well our generations are
performing. In order to do that, we use a neural metric based off of matching generated 3D objects
against the input captions. This is something like precision of retrieving the correct caption,
given the generator objects. Some of the most important techniques that help us here are
regularizer for transmittance and data augmentations, those architecture we use for the MLP,
and then later on, what model we use for clip.
This is an example of the process of optimization from different iterations,
so it actually can converge quite quickly, but additional detail might be added over the course
of training. In order to run 20,000 iterations of optimization, it's an expensive process
because we need to render out these images during training, but back of the envelope calculation
is about three to four dollars to generate each model on TPU in Google Cloud at an hour.
It's in the realm where an artist could afford to do this.
We're working on some follow-up work, which will speed up this process and make it even less expensive.
That's all I've got on these works. The broad goal here is to make content creation
easier and generate assets that are useful. This 3D assets I see is particularly useful
for downstream applications because they could be plugged into a game or plugged into some other
system. We have code out for both of these projects. If you want to try out the text
to 3D generation in your browser, you can use a Colab notebook that I put together.
I've tested it on the Pro version of Colab, which has higher memory GPUs,
so you might need to play with some of the parameters.
Thank you so much, Eje. This is really fascinating. I have a few questions,
and then before letting everyone else ask questions, the first question is that
are you able to walk us through some of the Colab code today or should we do it on our time?
Let me see if I have it up. Also, before going to changing your screen,
can you please go back to the animations? Sorry, I have so many questions because this
is really cool. Or maybe the one that is armchair. Yeah, give me one sec. Thank you so much.
I think these are really cool. I think that for the students and I, this kind of inspires us to
think maybe one cool thing to do is that we can generate these things and use them in some avatar
or game or something, and this will be really cool. This is something for students to think about
for their future projects because the goal of this course is to inspire us to think about
what are the creative ways that we can use AI. This is really cool. One question that I have
is that, can you share some intuition of, for instance, let's say the rubric. It looks like
a rubric and it looks like a chair, but then we see that there is some, we wish there was more of the
structure and it might be because clip is the objective and or assessor and thinking that,
okay, as long as I have a patch of red and yellow and things like that that are appearing on rubric,
I'm happy, the rest, I don't care much, or is there any better explanation of what's happening?
Yeah, so the 3D structure only emerges because of the shared representation and the easiest way to
satisfy clip from any perspective, having this Rubik's Cube chair from any perspective,
might actually be to learn some consistent geometry. That said, there's no prior
other than sparsity and some implicit regularization just in the structure of the MLP,
so there's no prior on the 3D structure learned from data. That's something that I think is missing
and definitely opportunity for future work is how do we learn some priors on 3D data and integrate
them into the system to try to improve the plausibility of the structure. One example where
this issue arises is that sometimes you'll get repeated structures on the objects,
like if you optimize for a dog, maybe it will have eyes on multiple sides of its face because
they're not visible. So you only see two sets of eyes from any particular viewpoint,
that is all the discriminator clip ever sees are those two eyes, but the underlying geometry,
there's no constraint that the dog should only have to rise.
Okay, excellent. Thank you so much. Are there questions before we go to the collab?
The outputs, are they like .fbx files or do they still need to be, let's say, a little bit
prepared in rendering software before they can be actually readily used in the game engine?
Like Unity or Unreal? They do need to be post-processed. So what you get out is a train neural net,
so it's function mapping from coordinates. We don't use the v direction in these results,
just XYZ coordinates mapped to color and density. So there are a bunch of ways that you could convert
that. I don't know of off the shelf software that will be able to do that conversion for you,
it'd have to be coded up, but you can sample the scene on some grid, for example, and then you'll
get out color and RGB. You could convert that to a local voxel representation. If you want to get
a mesh, there's an algorithm called marching cubes that is able to find a mesh in the scene,
and there's implementations on GitHub of marching cube for neural radiance fields
that we haven't integrated into our particular library. So you take a little bit of glue to
grab marching cubes and then plug it in. So what do you all use to turn the neural net into these
graphics? Sorry, could you repeat the question? What do you use to turn the neural net into the
graphics that we see here? Oh yeah, so that's done by rendering. So you can render the neural
radiance field from any perspective in the code, but that just renders out a 2D image. It doesn't
give you, you know, like a mesh, versus the game engine will have its own rendering algorithm based
on rasterization or ray tracing, given the underlying geometry and texture map, which might
be real time. So the rendering here is not real time. You have to go evaluate the neural network
at a bunch of different coordinates and accumulate its outputs into an image.
So that's implemented. If you want videos, we can do that, but you'll have to DIY the conversion.
Ellie had a question on strategies to reduce rendering costs.
So you can render images at low resolution. And in the Colab notebook, the rendering is done at
very low resolution. So experiments, you render out 168 by 168 images or higher.
But Colab only gives you a single low memory GPU. So we render out 88 by 88 images.
And that really significantly speeds up the process. So rendering takes maybe 300 milliseconds.
So you have to do about three iterations per second.
If you're using alpha transparent, okay. So Ben is asking, how do we handle with transparent objects?
So the neural radiance field, the volumetric representation is really amenable to transparent
objects because the density is this continuous value and we can observe objects. So accumulate
color from objects behind transparent objects. In optimization, you might decrease the
density of some object that should be transparent, like stay in glass windows.
And the background is composited at the end. So any ray, if there is some accumulated,
if the transmittance is not zero along the ray accumulated throughout the scene,
then there'll be some contribution from the background image.
So the reason that we've kind of encouraged coherent objects is that if the object is not
coherent, then the background will leak through the translucent objects. Oh, I see what you're
saying. Yeah, if you want stay in glass windows. So I mean, you would have to, the scene would
probably optimize so that the transparent object is blocked from behind by something.
Yeah, the next steps, I think they're exciting lots of next steps, because this is an initial work
and there's things like speeding up the optimization. It's been a lot of recent work and
speeding up neural radius field training for images. And I think a lot of that can be plugged in.
And how do you synthesize the formable objects? How do you bring a human in the loop so they
can provide feedback partway through training? All kinds of stuff to tackle in making this more
of a practical system for 3D artists. And would you like me to share the collab? I guess we're at
time. Yeah, please go ahead. That would be great. Thank you so much.
So this is the collab notebook. You can find it from the project website.
It is a compact implementation.
The system will run faster on GPU than on TPU in the collab notebook. But for all of our experiments,
we use TPU. Some helpers are imported from our library. So if you want to hack on some
of the low level primitives, you can fork our library or kind of copy those helpers into the
notebook. But the main way you'll interface with this collab notebook is by adjusting the quality
settings here. So in particular, edit the query. Here I've filled in a high quality 3D
render of Jenga Tower. And you can select the clip checkpoint you want to use. Clip bit B16
is used in most of our experiments. There's also an internal Google model that's not available here.
But you can scale down if you're running out of memory to either the B32 or ResNet 50.
Choose the number optimization iterations. I think at least 1000 is necessary.
But more will add more detail. Consider the rendering width and then this is the number
of data augmentation. And then run training. So here's an example of the training run I've already
run in the notebook for that prompt, a high quality 3D render of Jenga Tower. It won't exactly
match the result that was shown in the slides because the version of the collab notebook could
scale down. But over the course of 2000 iterations of optimization, you can see the different
learning curves. This is the total loss that's being optimized. Clip's cosine similarity,
negative cosine similarity is improving. So this means that the renderings of the object
are becoming more and more consistent with the given caption over time.
And the transmission is regularization here. This is showing what is the average transparency
of pixels in the scene. And in this plot at the bottom, the collab notebook will ride out
renderings periodically every, I believe, 100 iterations. So at the beginning, the scene is
low density, essentially empty. And then over time, some content will emerge from the optimization.
And then that's refined and sharpened over time. The camera's moving around. So the camera's being
randomly sampled around the object. And that's why the scene is rendered from different perspectives.
And then finally, the collab notebook renders out a video, 48 frames. And this is the result.
On the GPU that collab gave me here a P100, the optimization I think took about six, seven minutes.
So hopefully you can get some cycles in.
In the run training section, it says if you run out of memory, tweak the configuration options
above. What do you recommend changing? Yeah, that's a good question. So I think
you can change this clip at B16. I would try to clip B32. There's also on the first import
in NVIDIA SMI printout. And so you can look at how much memory is available.
Sometimes it's worth retrying multiple times to get a larger GPU. This P160
gigabyte I think you won't get without collab premium, which is about $10 a month.
But you think you can get 15 gigabyte T4 GPUs for free. Sometimes the collab will give you an 11
gigabyte GPU that might not be enough. If you can tweak the configuration parameters,
I would try reducing this number of samples. So this is the number of points along each array
that is sampled. And that affects the batch size. So the render width, the batch size scales
quadratically with the render width because we're rendering got square images. And then the num
samples the batch size to the MLP scales linearly. So you could reduce this down to 32 even at the
lowest. B32 will use less memory than B16. So this relates to the patch size and the vision
transformer clip encoding. And then if you want to scale down even more, you can change the number
of data augmentations per iteration, maybe down to two.
Oh, Ben says that you can't retry for a better GPU.
That's unfortunate. I mean, I don't know whether MIT has like a shared GPU cloud, but you can
also just download this IPIND and run it on your like Jupyter notebooks, post it on some MIT compute.
And it will paralyze across multiple GPUs.
Cool. And have you taken any more questions that you have?
Excellent. Thank you so much. Maybe at this point I'll stop recording and if students have more
questions, we can...

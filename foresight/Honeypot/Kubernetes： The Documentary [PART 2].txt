What would you say is the moral of the story?
Open Source is most successful when it's played as a positive sum game.
OpenShift started 2010.
There was this divide in the platform as a service base.
There was all the really simple, modern apps,
and then there was all the stuff that made money.
And kind of we came in from the perspective of like,
how do we make this stuff easier?
And so we were kind of getting tugged to the right on the complexity scale.
We started with simple apps in OpenShift.
But we were kind of thinking, you know,
what's the more general problem that would bring the whole spectrum of application authors?
We wanted to use Docker with an OpenShift,
but it's still not enough, right?
Because the Docker container is how you get a reproducible unit of software.
But how do you get those chunks coming together?
How do you take three bits of software and bring them together?
We'd reached out through one of our board members to Google,
you know, this container space is getting interesting.
Are you guys doing anything?
Is there anything that you're interested in that maybe we could work together on?
Clayton was looking to redesign the next version of OpenShift,
what became OpenShift 3 on top of Docker.
Which, you know, Docker was initially created by DocCloud
to serve as the basis for a multi-language platform as a service engine.
And OpenShift was a platform as a service open source project and Red Hat product.
We actually got a very quickly got an email back, which is,
oh, we're thinking about this project, we don't know.
We can kind of give you some details.
They kind of walked us through this demo.
They're calling it the 7-lit at the time.
They didn't really have a name.
And this is based on what we do internally at Google, at Borg.
And we're thinking about open sourcing it.
And I was interested.
It wasn't impressive.
The idea of working on something completely new, though,
like something that was from scratch, but was based on those ideas, was appealing.
So we were interested and we were excited.
But they're a little wishy-washy.
Google was like, well, we don't know whether we're going to be able to open source this or not.
We don't know.
It's a little, we're just not sure.
So we were like, probably our best bet is this Mesos thing.
I guess we're just going to announce that the next version of OpenShift will be Mesos and Docker.
And Mesos isn't really designed around Docker.
But it was, you know, it was the best option we had.
An internal phrase we use sometimes is uncomfortably exciting.
Meaning maybe we bit off more than we could chew.
It's hard to know that this is, in fact, the right journey.
We want to take these crown jewels and open source them.
And it's going to be okay because we'll make it up in volume, so to speak.
And that is kind of the essence of the strategy,
but the volume comes from managing the service.
So it was a pretty quick timeline.
We had the code available, but getting permission to release it as part of DockerCon
was made relatively late in that game.
So after we got the OK to open source,
now it was time to actually start getting the thing ready to release it.
Luckily, we engage with many people outside of Google.
And like Clayton from Red Hat, we wanted to work together.
Two weeks before DockerCon, my boss, Matt Hicks,
I think he texted me, he's like,
so Google just got back to us and they want to know if we're in or not.
And so we typed and we said, yeah, sure, we're in.
And so I think the next day they gave me private commit access.
I was either the first or the second external contributor to Kubernetes
in that sense because I had access.
There were a few others from a few other startups as well.
Clayton just showed up and he just came in and started just cleaning up.
He helped us get a lot of the code into proper Golang semantics
and we were like, yeah, we really do like this Clayton guy.
He's awesome.
And that really became a big part of what started that kind of open community working.
I can't wait to see all of you at DockerCon
and see the amazing, incredible, creative things that everybody in our community is doing
and everything that the people in this team are doing.
So thank you.
I believe we actually open-sourced the repos on the day of the DockerCon keynote
as part of that keynote was the announcement of Kubernetes publicly
and made the code available that day.
Now, we're releasing something called Kubernetes today.
It is yet another orchestration thing that sits on top.
That is not a bad thing, as Solomon said.
There are many of them because it's an exciting and important area
and an area where we need to get agreement.
And I'm not going to give a demo, sadly.
I only have 25 minutes, most of which is gone.
But Craig and Brendan are going to actually go through that later today at 2 o'clock.
You can go see that session.
Now, the reason to do an open-source release right now in this space
is because it's about the ideas.
So I'm going to talk a little bit about some of the ideas in it
and then you can get the details from Brendan and Craig.
This was it. This was our big moment.
We had something that we were pretty embarrassed about
in terms of its rudimentary state.
But that's when you're supposed to launch, right?
So we got out there and with Eric Brewer's support, we made our announcement.
We're doing orchestrations.
We want to have a manager that's going to have an API for managing the cluster.
We announced June 10th at DockerCon,
which is the day I created my Twitter account.
With these container technologies,
it's the first time that customers have really had an opportunity
to package up their application in a framework that's portable.
If I recall, there were five or six other container management systems announced the same day.
Some were proprietary and some were also open-source.
The same day.
Every big startup I felt like had a container orchestration project
and half of them were announced at DockerCon 2014.
We all split up and went to different sessions at that DockerCon
and we were texting each other back and forth like,
oh, Sonzo just released this.
Oh, Facebook just announced Tupperware.
Oh, it was very interesting.
There's two ballrooms and so it was Google announces Kubernetes,
startup announces container orchestration framework.
Facebook was there talking about Tupperware.
Heroku was there, I believe.
Some of the Heroku guys were there talking about how they were all in on Docker.
Startup announces Docker monitoring solutions,
startup announces new business around container-native orchestration and monitoring.
Everybody had the same idea.
This is not a brand new idea.
But there was a tension because Google announced this net new thing
that was almost more exciting than Docker.
And I remember being there and there were times where there were a few conversations I had with people
that was like, this is a little awkward.
Like Google's stealing Docker's thunder.
Docker's swarm project, lib swarm at that time.
It was a library for starting containers on a few machines.
Its commits went to zero, the week we announced, I think.
And that kind of set in motion some of that,
like that tension that played out over the next couple of years,
which was this young startup super excited about what they're doing.
Huge community traction, really captured developers' hearts and minds.
And then the project that I personally really believed in,
which I think Docker is great,
but like Kubernetes was about the application, it was about enterprise IT,
it was for us, it was very much the evolution of the bets we were making around
how to use streamlined building applications for everybody.
And that larger vision at DockerCon, I think it ran headlong into Docker inks, hopes and dreams.
And it got really awkward after that.
I think we are on the verge of something that's a major shift.
And I really look forward to making a great space together with all of you.
Thank you so much.
APPLAUSE
Google was in a war about cloud, and that is the most important thing for them.
I was working as an engineer with other engineers.
Engineers are not so much about corporate strategy and politics.
Most of them were really about tech merits
and about the quality of what we were building together.
There was no such thing as politics in the open source project.
There was corporate politics above,
and of course eventually it started crippling into the open source relationships.
To understand the way that Kubernetes and Docker Swarm and Mezos
were competing with each other back at this time,
I think that there are multiple layers to it.
But on a technical level, I think it's important to understand
the different philosophies that Kubernetes and Mezos and Docker Swarm
really brought to this problem space.
Kubernetes, our focus was on making sure that we presented
a really clean API that fit the way that we viewed the problem space
of being able to deal with scheduling containers
and work across a bunch of different computers.
Mezos, the focus was on the scheduler,
and Mezos had a very, very sophisticated scheduler
that had some really interesting qualities in terms of the way
that they distributed that scheduling problem and distributed work.
Docker Swarm, on the other hand, starting out,
they wanted to view a cluster of computers as one big computer.
And once you introduce networks,
once you introduce partial failure modes,
different scheduling constraints,
there ended up being a lot of capability sacrifices
that were made in Docker Swarm
to have that initial experience be as easy and simple as possible.
Kubernetes is what you would build if you had enough time
and experience.
I doubt anyone had the same level of experience that Google had.
I'm at CoreOS, and we have this thing called Fleet,
and we have our vision for building container orchestration.
We have another startup around the corner, Docker Inc.,
that has their own vision about container orchestration called Swarm,
and we also have an incumbent startup, Mesosphere.
That has a successful orchestration platform already out there
used by giant companies.
And so there's a new entrant.
There's this thing called Kubernetes, and what do we do about it?
Luckily, I was working at a startup that was open to new ideas,
and so I was just contributing at night time.
My contributions were about making sure that Kubernetes
worked well on top of CoreOS.
And at the same time, I noticed that I was the one speaking at meetups,
telling people, there's this new thing, here's how it works.
There was a tool called Flannel that came out
that made it really easy to implement this with Kubernetes.
There was another startup at the time, Kismatic.
I think Kismatic, Patrick Riley and Joseph Jax,
they knew that Kubernetes was probably going to be a thing
that was going to stick around for a little while.
And so Patrick and team put up the money to do the first KubeCon
here in San Francisco.
And they asked if I would emcee the conference
because a lot of people knew me from talking in the space.
So our next session before lunch,
we're going to talk to the Kubernetes core team.
And so now I'm on this stage at a single-track conference.
Maybe we have 300 or 400 people there,
but now people were showing up to say,
what is this Kubernetes thing that we keep hearing about
for the last couple of years?
And so I think the attraction to Google was,
who is this kind of crowned by the community spokesperson
for this thing we call Kubernetes,
this project that we've put it out.
I started to get a lot of opportunities from around the industry.
I remember the Google team was just like, hey, how about Google?
And I thought about it for a while, and it was a natural fit.
And I remember being honored to become like the voice of that project.
I was able to meet people where they were
and then show them what was next
and then invite them to kind of get involved
and invite them to contribute.
At the time Kubernetes was open sourced,
we didn't take it for granted that the thing we built
would automatically be successful.
That original release was really raw.
I mean, you know, it's there in the GitHub history.
You can go take a look at it.
There was a lot of duct tape and bailing wire holding it together.
So we set ourselves a really ambitious goal.
We want to GA this thing a year from now, right?
Okay, let's start working on 1.0.
I have never in my career worked so fast or so furious.
It's kind of like you just have the baby
and you just couldn't stop love that one.
And you never feel tired, put all the effort into that one.
You worry about sometimes maybe a little bit of a sense
about success or maybe in the right path.
In the first two years of the project,
I got 200,000 notifications from GitHub in my inbox.
The fact that we open sourced it so early
really invited people to get involved.
You have to learn hard to think about how to inspire people
outside of Google, outside of Red Hat to help.
At 1.0, it was going to be for web app style,
stateless, low-scale systems.
That's it.
Anything else we're not focusing on?
Backburner, take it off the plate.
How little can we get away with was our theme.
There's a lot of conversations in that first year
where it was the developer and application-focused mindset
kind of clashing with the how to build systems
that scale mindset.
We just had to draw the line about what was going to be done
and what was not going to be done.
So there was a lot not done.
There was this fear there was too complex.
Docker swarm was much simpler.
Mesos was more powerful.
It could scale to thousands of nodes.
And Kubernetes, I think at the time, was 100 nodes or something.
Andrew Speicher was kicking the tires of Kubernetes
right after we open sourced it
and concluded that it wasn't mature enough.
It didn't scale.
And Netflix chose Mesos.
And other large companies chose Mesos like Apple and Airbnb
because it was more mature.
It existed for a few years already
and it scaled to 10,000 instances.
If you want a technology to be ubiquitous,
if you want a technology to be used by everyone,
if you want a technology that you can actually
create that alloy that's stronger
than the contributions of any individual,
it needs to be held under open and free governance.
At the beginning, it was just a promise, right?
We will put this in a foundation.
The project was actually in a Google-owned GitHub org
and the Community Contributor License Agreement
was a Google Contributor License Agreement
and people had to basically give Google the right
to re-license the product.
That put off a lot of people.
As we started looking for partners to get involved,
the fact that the project was still dominated by Google,
despite us talking about it being open source,
it still gave folks a lot of pause.
Many large companies do not like their employees
signing a competitor's Contributor License Agreement.
That was the context where Craig went off
and did a lot of coalition building
to eventually form the Cloud Native Computing Foundation.
We set a hard milestone for 1.0 to be OSCON
when we would announce 1.0 and also as it turns out CNCF.
It was probably the most stressful thing I've ever done
was trying to bootstrap and navigate the multi-vendor
interests in getting something like CNCF off the ground.
There was about six months of essentially like bootstrapping
work of trying to get this thing going.
It was announced June 2015 at OSCON.
It's clear that we need to start working more closely together as a community.
As we looked at the 1.0 point of Kubernetes,
as it's going production-ready,
as we're looking at the future of this project,
it was clear to us we had to take it to foundation.
My idea at the time was like,
I want to go build bridges with our competing orchestration.
Even though Mesos has opinion on something,
Kubernetes' opinion on something,
there's usually some common interest.
Maybe we could standardize how storage is attached to a container.
Let's make that work.
I convinced Ben Hyman to come on the TOC as we called it.
There was a bunch of other folks.
We had this good mix of folks from different overlapping
and competing technologies to start the organization.
We were able to secure a pretty rich base of support early out of the gate.
Getting that to happen was maybe the most critical piece to the project's success.
All the technology aside, without that,
I don't think it would have gone anywhere.
We believe it's going to take a whole community,
and with that in mind, with Linux Foundation
and a broad array of industry partners,
we're announcing the Cloud Native Computing Foundation today,
which will be charted to take Kubernetes and adjacent technologies,
harmonize them, and ultimately progress the agenda
of Cloud Native Computing for everybody.
APPLAUSE
We built the organization around Kubernetes in a very specific way.
This was work that was done by what we called the Bootstrap Steering Committee,
and that was the group that was early, early contributors to the project,
and then myself added to that to try to build what our governance would be.
Because they reached a point in 2016, 2017,
where it was not easy to figure out how to get something done,
and you just had to go ask a Googler but which Googler,
and that wasn't fair, it wasn't well documented.
The challenge then was how do you take this fragile community
where everybody felt a sense of ownership
and slide a sort of governance model underneath that?
If you want to build a distributed compute platform,
Conway's Law suggests that you need a distributed decision-making setup.
So we pushed very hard to have that distributed
and closer to the code than to a leader.
We saw this great inflection point of interest and engagement
because as we started moving things out to more open governance
and as we moved the copyright and intellectual property to the CNCF,
more larger companies were able to participate,
and at that point it really became an exercise in how to grow the next set of leaders
and taking the community as the piece that needed to be grown
more than anything else and nurtured more than anything else.
We didn't have a manager, actually,
even so Tim and I were in Mountain View at the time
and Craig, Joe and Brendan were in Seattle,
and we all reported to different managers for several months
while we were working things out.
My first time at Google was mid-February 2016
from a Kubernetes timeline.
The team was working on Kubernetes 1.2.
The thing that was obvious and clear is that they were on the edge of burnout.
Everybody had been working 16-hour days,
6 days a week for a long time.
It was like a 24-7 job.
People would ping you on Slack, on Git, on Twitter, customers calling,
the build is breaking, someone cannot come in.
The work actually never stops.
By the end of 2016 we were having between 700 and 1,000 people
contributing to each release.
It was clear that with the way we were structured,
we will fail.
We will burn ourselves.
Ken knew that what we were doing was not going to work in the long term.
It was great when it was five or six people.
She knew that she needed to grow this organization
if we were going to keep on the trajectory that we were on.
So I think at the first phase, coming in as a leader
and as a manager of that team,
I first wanted just to stabilize and make space for folks
and that idea that we are not going to solve things for the short term
but we are actually going to think about how it's going to be solved
from this point on was I think a critical point
at the success of Kubernetes.
That idea actually evolved later on to what we call sustainable success.
She was very deliberate about putting together the teams
and giving responsibilities to which site had which things
that they were responsible for
and how we were going to grow the team
just to get it back under control.
It was also time to focus a little bit on stability and predictability
and now we had real customers.
We had people who were actually using 1.0 in production,
big video games you might have played
and we needed a focus on making sure that we didn't screw them up.
We learned that Niantic was building Pokemon Go on GKE.
And so very quickly we got in a bunch of conversations
with the folks over at Niantic about how it was going,
what were they doing, what could we do to help them out,
how was Kubernetes fitting their problem.
They ran Pokemon Go on one big cluster,
which was exciting and terrifying.
Once it launched, they actually experienced 50 times
the load or the volume than what they expected.
We sort of went into, I won't say panic mode,
but near panic mode as we tried to figure out
how we were going to give them more headroom
because the game was just growing and growing and growing.
They were just pushing the limits of everything.
You are like an engineer on the team, you are on call,
we are like 24-7 managing those clusters.
We see the entire world going with their phones everywhere.
And in my head, all I'm seeing is like, please stop playing.
We are not ready for that.
What was amazing, that we actually were ready for that.
This was a huge deal.
We all felt great about it.
Definitely validation of what we were doing.
The idea that we were able to continue and grow
and see that game expand everywhere and meet the demand,
like this is why Kubernetes is a thing, right?
To build something, you are not really sure how it will scale,
but then you have this amazing foundation
that really helps you to get to that scale.
I hate the term the container wars,
but the debate about what were people going to use
was still very much going on.
Niantic took a big bet on Kubernetes,
but definitely there was a lot going on in the market.
You couldn't read a blog site
without running across a Docker versus Kubernetes.
The container wars began the day Kubernetes was announced at DockerCon.
And it was interesting because it didn't feel like a war in the beginning.
It felt like tension about what's on top.
The container wars were complicated by the fact
that there was also a fight over orchestration,
the things that managed containers above that.
There's a lot of value in being the top of the stack.
Look into any kind of marketing materials or website everywhere.
You're never going to find somebody who's proud of saying
that we are the little dot at the bottom of the stack
that you never think about.
Everybody wants to own the user experience.
Everybody wants to be on top of the value chain.
And this is the kind of race that happened during this Docker era
about who owns the developer experience, who owns the app's experience,
who owns the mindset of the engineers ultimately
because this is where the value really is
and this is what the people are going to buy.
People were fighting over what should be the dominant way to run containers,
how to define applications.
Both approaches had valid ideas.
I think the mistake that was made early on
is people thought there was going to be a zero sum game.
If you looked at Kubernetes at the time, it was layered.
Linux was at the bottom still.
Docker was the container runtime.
SCD is where we stored everything.
It looked like it was a big combination and collaboration
of the entire industry at the time.
But no one really treated it like that
and I think that was the missed opportunity.
So I think the tension was people felt like it was a zero sum game.
Whoever won the orchestration wars
would somehow win all the business and all the customers.
We couldn't have done Kubernetes without Docker.
It just would not have been a thing that would have worked out.
Something else would have happened.
And yet people wanted to make drama out of it.
The world loves drama
and I don't think there was nearly as much actual drama as people made out of it.
At the end of the day, everything eventually settled down
and CNCF, we brought in both container D and Rocket
under the same foundation
and we also pushed to standardize the image specification.
It became a standard.
For a good three years or four years,
every day was,
is Docker going to change to adopt Kubernetes?
Is AWS going to suddenly adopt Kubernetes?
DockerCon were like,
is someone going to change everything with an announcement?
And then one day Solomon called me and said,
we did it.
We built Docker on top of Kubernetes
and we're going to announce it at DockerCon in Copenhagen
week after next.
Can you be there?
Docker announced that they were going to start supporting Kubernetes
and DockerCon in addition to Docker Swarm.
They recognized that this was something that they couldn't ignore.
And so Brendan and I were there to say congratulations
and it was really a cool moment
where I finally felt like we stopped pulling against each other
and started pulling together.
2017, a lot of competitors embraced Kubernetes.
Mesos, Docker, Pivotal.
And really near the end of 2017,
Amazon launched a Kubernetes product.
Back then when joining Google even,
I never thought AWS would seriously have a Kubernetes offering.
For the longest time,
Amazon had been a holdout on this.
And so to see it come full circle where it's like,
have Amazon supporting it was one of the largest signals
that Kubernetes, at least for me,
that Kubernetes was really here to stay.
And now Kubernetes seems to be the de facto standard
across all cloud providers globally.
When we saw Mesospheres rebrand and embrace Kubernetes,
when we saw Docker start to introduce their own Kubernetes offerings,
it really kind of closed out that arc.
But there were a lot of highlights at every point of the journey
that were worth celebrating.
It was a phenomenal experience.
In the end, I think Kubernetes won in part because it had
a vast army of contributors behind it.
And it just kept marching along with a huge number of commits per day.
And that rate of change kind of trumps everything else.
Back then, most of the things that truly attracted people
who even knew the tech and who knew containers was the UX.
Yes, in a sense, Kubernetes managed to capture
the orchestration layer building on top of Docker.
But they never really captured the developer experience.
And the truth is, nobody really is but Docker right now.
And that's still, I think, one of the areas that there's a lot of things to do
and we're not done yet.
And Docker, as a company today, decided to focus on the developer experience
side of things for a good reason, I think.
I'm seeing Kubernetes being used in ways that surprise me and delight me
to see, like, wow, I hadn't even thought that that was something that we could do.
It's clear that Kubernetes is going to take on a life of its own
and go off and evolve beyond what any of us really imagined when we started out.
Things cannot repeat.
And I think we just lucky have that right time and right support.
The container wash has benefited from the fact that there was lots of VC dollars
fueling this race.
But the truth is, there was no zero sum game.
The best ideas from that race are still with us.
And they've consolidated now.
And they've consolidated so far into the latest checkpoint, which is Kubernetes.
But the truth is, there's going to be something that will place Kubernetes.
Maybe it's some serverless component.
Maybe it's a different way of thinking about these APIs and how they should fit together.
And typically, the higher you get gives freedom for the system underneath you
to also evolve and sometimes become simpler.
So I think this is just the latest checkpoint, but whatever war people were fighting,
there was nothing to actually fight.
Thank you.

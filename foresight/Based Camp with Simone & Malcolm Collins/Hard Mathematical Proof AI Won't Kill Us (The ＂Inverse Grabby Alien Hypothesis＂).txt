So basically, no matter which one of these explanations of the Fermi paradox is true,
either it's irrelevant that we are about to invent a paperclip maximizing AI because we're
about to be destroyed by something else or in a simulation, or we're definitely not about to
invent a paperclip maximizing AI either because we're really far away from the technology or
because almost nobody does that. That's just not the way AI works. I am so convinced by this argument
that it is actually, I used to believe it was like a 20% chance we all died because of an AI or maybe
even as high as a 50% chance, but it was a variable risk if I've explained in other videos.
I now think there's almost a 0% chance. A 0% chance assuming we are not about to be killed by a
grabby AI somebody else invented. Now, it does bring up something interesting. If the reason we're
not running into aliens is because infinite power and material generation is just incredibly easy
and there's a terminal utility convergence function, then what are the aliens doing in the
universe? Would you like to know more? Hi Malcolm, how are you doing my friend? So today we are going
to do an episode, a bit of a preamble for an already filmed interview. So we did two interviews
with Robin Hansen and in one of them we discussed this theory. However, I didn't want to off rail
the interview too much going into this theory, but I really wanted to nerd out on it with him
because he is the person who invented the grabby aliens hypothesis solution to the Fermi Paradox.
I hadn't heard about grabby aliens before, so I'm glad we're doing this. This is great.
Yes, so we will use this episode to talk about the Fermi Paradox, the grabby alien hypothesis,
and how the grabby alien hypothesis can be used through controlling one of the variables,
i.e. the assumption that we are about to invent a paperclip maximizer AI that ends up fooming and
killing us all, because that would be a grabby alien definition only. If you collapse that variable
within the equation to today, then you can back calculate the probability of creating a paperclip
maximizing AI. And spoiler alert, the probability is almost zero. It basically means it is almost
statistically impossible that we are about to create a paperclip maximizing AI unless with the
two big caveats here, something in the universe that would make it irrelevant whether or not
we created a paperclip maximizing AI is hiding other aliens from us, or we are in a simulation,
which also would make it irrelevant that we're about to create a paperclip maximizing AI, or
there is some filter to advance the life developing on a planet that we have already
passed through, that we don't realize that we have passed through. So those are the only ways
that this isn't the case. But let's go into it because it is it is really easy. I just realized
that some definitions may help here. We'll get into the grabby alien hypothesis in a second,
but the concept of a paperclip maximizing AI is the concept of an AI that is just trying to maximize
some simplistic function. So in the concept as it's laid out as a paperclip maximizer, it would
be just make maximum number of paperclips and then it just keeps making paperclips and it
starts turning the earth into paperclips and it starts turning people into paperclips. Now
realistically, if we were to have a paperclip maximizing AI, it would probably look something
more like, you know, somebody says process this image and it just keeps processing the image to
like an insane degree because it was never told when to stop processing the image and it just
turns all the world into energy to process an image or something else silly like that.
This concept is important to address because there are many people who at least pass themselves
office intelligent who believe that we are about to create a paperclip maximizing AI,
that AI is about to as they call fume, which I mentioned earlier here, which just means rise
in intelligence astronomically quickly, like double this intelligence every 15 minutes or
something and then wipe out our species and after that begin to consume all matter in the universe.
The Fermi Paradox is basically the question of why haven't we seen extraterrestrial life yet?
You know, like we kind of should have seen it already. It's kind of really shocking that we
haven't and I would say that anyone's metaphysical understanding of reality that doesn't take the
Fermi Paradox into account is deeply flawed because based on our understanding of physics today,
our understanding of what our own species intends to do in the next thousand, two thousand years,
our understanding of the filters our species has gone through. So we know how hard it was for life
to evolve on this planet and the answer is not very from what we can see. A lot of people
I'm really, really into it's one of like my areas of like deep nerdom theories for how
the first life could have evolved on earth. So there's a couple things to note. One isn't that
important to this, which is life evolved on earth almost as soon as it could. Now a person may say
why isn't that this relevant? That would seem to indicate that it is very easy for life to evolve
on a planet. Well, and here we have to get into the gravity aliens theory. You're dealing with
the anthropic principle here. Okay. Can you define the anthropic principle? Yeah, basically what it
means is if you're asking like, look, it looks like earth is almost a perfect planet for human life
to evolve on it. Like it had liquid water or everything like that, right? Except human life
wouldn't have evolved without those things on a planet. A different kind of life would have evolved
without those things. The kind that doesn't need water, etc. Right. So it's not really,
if life on earth didn't evolve almost as soon as it could, well, then it would have been too late
and another alien would have wiped out and colonized this planet. That is what the gravity
alien theory would say. So that this doesn't really change the probability of this as a filter.
But what we do know about the evolution of life on earth is there are multiple ways that could
have happened, all of which could lead to an evolving, you could either be dealing with like an RNA
world, you could be dealing with citrus acid cycle event, you could be dealing with the clay
hypothesis. I actually think the- Do you want to expound on any of these? I've never heard of the
citric acid hypothesis. So for this stuff, I would say it's not really that relevant to this
conversation. And people can dig into these various theories with people who have like
done them more, just like look up citric acid cycle hypothesis explanation for evolution of
life on earth or clay hypothesis to evolution of life on earth or shallow pool hypothesis to
evolution of life on earth or deep sea vent hypothesis to evolution of life on earth. The
point being is it shouldn't actually be that hard for life to begin to evolve on a planet like this.
So, but why this is a relevant point, okay? Okay. And we actually sort of have to back out
here from the grabby aliens hypothesis. So I'll explain what the grabby aliens hypothesis says
and why this is relevant to the Fermi paradox. So the grabby, usually when you're dealing with
solutions to the Fermi paradox, what people will do is they'll say that there's some unknown factor
that we don't know yet basically. So a great example here would be the dark forest hypothesis.
Okay. So the dark forest hypothesis is that there actually are aliens, lots of aliens out there.
They just have the common sense to not be broadcasting where they are and to be very
good at hiding where they are because they are all hostile to each other. And that any other
aliens like us who were stupid enough to broadcast where they are, they get snubbed out snuffed out
really quickly. Sure, that makes sense. That makes sense. Yeah. Okay. If the dark forest hypothesis
is the explanation for why we are not seeing alien life out there, it is somewhat irrelevant
whether or not we build a paperclip maximizing robot because it means we're about to be snuffed out
anyway, given how loud we've been radio signal wise sending out ships broadcasting about us
sending out signals. We have been a very loud species and we could not defend against an
interplanetary assault by a space fearing species. Well, I mean, in that case, you could actually
argue it would be much better if we developed AGI as fast as possible, because maybe it can
defend us even if we cannot defend ourselves. Possibly, but that's the point there. Beside
the point. It becomes irrelevant or they'll say we're in a simulation and that's why you're not
seeing stuff. But again, that makes all of this beside the point. Well, grab the aliens does,
it says no, actually, we are just statistically the first sentient species on the road to becoming
a grabby alien and I'll explain what this means in just a second in this region of space.
And then it says, let's assume that's true. It can use the fact that we haven't seen another
species out there, a grabby alien that is rapidly expanding across planets to calculate
how rarely these evolve on planets. Okay. Do you sort of understand how that could be the case?
Yeah. Okay. So in the grabby aliens hypothesis, when you run this calculation, it turns out
if that's why we haven't seen an alien yet, what it means is there are very hard filters,
like something that makes it very low probability that a potentially habitable planet ends up
evolving an alien that ends up spreading out like a grabby alien, like a paperclip maximizer,
one of those really loud things that's just going planet, use the resources on the planet,
other planets, other planets, other planets. And even if it has already finished doing that,
you've argued in other conversations we have had that you would see the signs of that,
you would see the signs of the destroyed civilizations, etc. A grabby alien or which
a paperclip maximizer is, so it's just easy. If you're like, what does a grabby alien look like,
a paperclip maximizer that's just going planet to planet, digesting the planets and then moving on,
or a human empire expanding through the universe. We colonize a planet within 100 years,
we get bored, or some people go and they try colonizing a new planet. Even with our existing
technology on Earth right now, like the speed of space travel right now, if we were expanding that
way, we could conquer an entire galaxy within about 300 million years. So not that long when
you're talking about the age of the universe. This is a blindingly fast conquest. So once an
alien turns grabby, it moves really quickly. And a lot of people think that we are space travel
constrained. We're really not. The reason why we don't space travel with our existing technology
is because of radiation damage to cells and the lifespan of a human. But if an AI was space
traveling, it could do pretty well with our existing technology in terms of getting to other
planets, you know, using them and then spreading. Okay. Anyway, so the grabby alien hypothesis says
that a species becomes grabby once in every million galaxies. Okay. Now within every galaxy,
there are around 400 or 500 million planets within the habitable zone. So the habitable zone is a
distance away from a star where life could feasibly evolve. Now this isn't saying that they have the
other precursors for life. But what it means is that there are very frequently in space, it turns out
planets that are likely for life to evolve on them. I would estimate like if I'm looking at
everything altogether, like the data that I've seen, there's probably about 10 million planets per
galaxy that an intelligent species could evolve in. And then if you're talking about, well, you would
only need this to happen. You've got to multiply that by a million for the one in a million galaxies
where a species is turning grabby. Now this is where it becomes preposterous that we are about to
invent. If this is why we haven't seen aliens yet, why we are that we are about to invent a grabby alien.
We can look throughout Earth's history, as I did with sort of the first big filter, the evolution
of life or the appearance of life first on this planet and say what's the probability of that event
happening in any given habitable planet? For life appearing, my read is not only is it likely to appear,
it could appear like one of five different ways. Even with the chemical composition of early Earth,
then you're looking at other things. Okay, what about multicellular life? What's the probability of
that happening? Actually, really high, really high. There's not like a big barrier that's preventing it
from evolving, and it has many advantages over monocellular life. So you're almost always going
to get it. Intelligence, how rare is intelligence to evolve? Not that rare, given that it has
evolved multiple times on our own planet in very different species. I mean, you see intelligence
in octopuses, you see intelligence in crows, you see intelligence in humans, and then you can say,
okay, okay, but like human-like intelligence, right? Well, we already know from humans what a huge boost
human-like intelligence gives us species. The core advantage to human-like intelligence
is like if I'm a spider and I'm bad at making webs, right, then I die, and that is how spiders get
better at making webs intergenerationally. As a human, I am able to essentially have like
different models of the universe fight in my head and presumably allow the best one to win.
Yeah, and you don't have to die before you get better. Yeah, you don't have to die to get better.
It is almost as important to evolution. It is sort of like the second sexual selection. So when sex
first evolves, the core utility of sex as opposed to just like cloning yourself, right, is it allowed
for more DNA mixing, which allowed for faster evolution? Intelligence allows for the faster
evolution of the sort of operating system of our biology. And so it's just such a huge advantage.
It's almost kind of shocking. It didn't evolve faster. For sure. Given how close many species
have come to it. Now, actually, surprising to a lot of people, this is just like a side note here,
a lot of people think cephalopods were close to evolving sentience. So let's talk about cephalopods.
Why? Wait, like, I mean, cephalopods are all over like historic geology and all these things.
Cephalopods are like squids, octopus, stuff like that. Like a lot of people point to how smart
they are. And they are smart. They are like weirdly smart. But they don't know why they're smart
because they don't know neuroscience. So the reason why cephalopods are as smart as they are
is an axon. An axon is what like information, the action potential travels down.
Yeah, it's a little arm thing that you see on a neuron. Yes, in a neuron, it's the little arm
thing. It's the cable. You can think of it as. Okay. So to be an intelligent species, you need
really fast traveling action potentials. Okay. So the way that humans have really fast traveling
action potentials is something called myelination. I'm not going to go fully into it, but it's a
little physics trick where they put like a layer of fat intermittently around the axon. And it
causes the action potential to jump between. It's like putting vegetable oil on your slip and slide.
Not exactly. It's actually a really complicated trick of physics that can't easily be explained,
except by like looking at it. I don't want to get into it. The point is, is we mammals have a
special little trick that allows for our action potentials to travel very, very quickly. And
are you saying that cephalopods have this too? No, they don't. The way that they,
in any other species that wants a fast traveling action potential before us,
the way that you increase the speed that extra potentials traveled was by increasing the diameter
of the axon. Oh, so they just have fat axons, whereas we have
optimized axons. Enormously fat. In some cephalopods, they're like a quarter centimeter in diameter.
Holy smokes. Like, whoa, okay. They could not get smarter than they are without having some huge
evolutionary leap in the way that their nervous systems work. So interesting. This is why cephalopods,
despite being really smart and probably being really smart for a long time, because they've
been on earth for a really long time, just could never make the evolutionary leap to human type
intelligence. Because they don't have room to have even fatter axons. Yeah, because as the axons
got fatter, the number of neurons they could have would get lower, the density of the neurons.
Oh, of course. Yeah, you've got limited space, unless they got much bigger brain cells. Yeah,
I guess you can have like giant, giant, giant. I mean, yeah. Well, I mean, whatever. Anyway,
this is a huge tangent here. But basically, it looks like if you're looking at the evolution
of life on our earth, if we have undergone other big like hard filters could be it's very rare for
a species to get nuclear weapons and not use them to destroy itself. Because it's so fun. Right.
Could turn out that almost every species does that. Or it could be that there's like one science
experiment, like a lot of people that may be trying to define the Hadron particle with the
big super collider, because actually, like all species, they get to a certain level of intelligence
and a certain level of curiosity, and they can't help but trying to find Hadrons, and then they
create little black holes in their planets. And that really could be a filter. Like these are
all potential filters. The problem is, is if we're like five years away from developing a
paperclip maximizing AI, that means that we as a species have already passed all of our filters.
And that means that we as a species can look back on the potential possible filters that we
have passed through and sort of add them all up. Okay. And when you do that, you don't get a number
that comes even close to explaining why you would only see one grabby alien per every million
galaxies. In fact, it means that the probability of us being about now, it could mean two things.
So we'll go through the various things that it could mean. It could mean that we just are nowhere
technologically close enough to develop a paperclip maximizing AI that is dangerous. That could
become a grabby alien. It could mean that. It could mean that we are about to develop a paperclip
maximizing alien, but something like even after it digests all life on earth, something prevents
it from spreading out into the galaxy, something technologically that we haven't conceived of
yet. This seems almost unfathomable to me given what we know about physics today.
Yeah. And that we've even gotten like projectiles from earth pretty far off planet.
Yeah. So yeah, there's not like some weird barrier that we don't know about yet.
It could be, and I actually think this is the most likely answer. I think that this is by far
the most likely answer to the Fermi Paradox. Simulation? No, not simulation. It could be
that we're going to simulation, but where are you going over that? I think it's that when you hear
people talk about like AI foaming, and I've talked about this on previous shows, but I think people
like really don't understand how insane this is. They believe that the AI reaches a level of super
intelligence, but it somehow still has an understanding of physics and time that is very
similar to our current understanding of physics and time. Meaning that when we think about
expanding into the universe, we think about it in a very sort of limited sense, like we gain energy
from like the sun, from digesting matter, and we spread out into the universe like physically on
space ships and stuff like that, right? Anything we understand about physics and time turns out to
be wrong. This assumption for the way an expansionist species would spread could become immediately newt.
And I mean this in the context of, like it's kind of insane to me. Like you've got to understand
how insane it is to assume that we basically have all of physics figured out. Yeah, that's fair.
This is like when like people in the 1800s, when they were planning how we were going to go to space
and they'd have like, maritime ships sailing through outer space. They'd have, you know,
or what are people going to do in the future? Well, they'll have like balloons and they'll use
them to go on lake walks. Or like, it basically assumes that technology, even as we advance to
the species or whatever comes after us, advances, moves very laterally and assumes we don't have
future breakthroughs, which I think is just one arrogant and in the eyes of history incredibly
stupid. So what kinds of technological breakthroughs could one make it very rare for even when an
alien is grabby, that we would see it out in the universe, right? One is time doesn't work the way
we think it works. Or it does work the way we think it works, but we're just not that far from
controlling it. So by that, what I mean is you could create things like time loops, time bubbles,
stuff like that, essentially entirely new bubble universes. So how would I describe this? Okay,
if you think of like reality as like a fabric, essentially what you might be able to do is like
pinch off parts of that fabric and expand them into new universes. That's essentially what I'm
describing here. There may be like the way you can break between realities or weird time loops
generates energy in some way, where you could kind of just keep looping it and like pinging back
and forth. You know, who knows? You know, it could be like the new wind power. We just don't know.
That you can travel in time this way, given that we haven't seen time turbulence of that or
we might not have. We talked about this in another video, which I'll link here if I remember to do it.
Given that we haven't seen time travelers yet, what I assume is that time manipulation requires
like anchors, which of course it would. Like, okay, if I was to go back in time, like where I am on
earth right now, I would be in a different part of the galaxy than the earth or something like
that. It would be really hard to track. You would need like some sort of anchor to be built.
So time travel would only work from the day it's invented and from the location it's invented. So
you wouldn't be able to go out into the universe. Another example of the technology that we might
not have imagined yet is dimensional travel. It may turn out we meet aliens and like we're
traveling in the universe and they're like, why did you waste all of the energy getting to us?
Your own planet is habitable in an infinite number of other dimensions and it's right back
where your planet is. Like why wouldn't you just travel through those dimensions? That's a much
easier path for conquest. That being the case and people would be like, yeah, but typically when
something's like being expansionistic like that, it moves in every direction. Yes, but if there are
an infinite number of other dimensions and it is always cheaper to travel between dimensions than
it is to travel to other planets in a mostly dead universe, let's be honest, like there's not a lot
of useful stuff out there. From the perspective of easily being able to travel between dimensions,
it could never make sense. There is always an infinite number of other dimensions to conquer
right where you are right now instead of going out into the universe. Now this would not preclude
a paperclip maximizing AI. It could be that we are about to invent a paperclip maximizing AI,
but even if we do that, it's less likely that it immediately comes after us. It could just expand
outwards dimensionally. Like so it would act in a very different way than we're predicting it would
act. Now, another thing that could prevent it from killing us is it could be trivially easy
to generate power and even matter. And by that, what I mean is there is some method of power
generation that we have not unlocked yet that is near inexhaustible and very, very easy. And if you
can generate power with near infinity with little exhaustion, you could also generate matter,
electricity, anything you want. If this was the case, there just wouldn't be a lot of reason to be
expansionistic in a planet hopping since. Essentially, you'd be like one giant growing
planetary civilization or ships that are constantly growing and expanding out from a
single region. It could also be that these sorts of aliens expand downwards into the microscopic
instead of expanding outwards. Like that might be a better path for expansion. There's just a lot
of things that we don't know about physics yet, which could make it so that when you reach a certain
level of physical understanding of the universe, expanding outwards into a mostly dead universe
can seem really stupid. Now, there's another thing that could prevent grabby aliens from appearing.
And this is the thesis that we have listed multiple times, which is terminal utility
convergence, which is to say all entities of a sufficient intelligence operating within the
same physical universe end up optimizing around the same utility function. I think they all basically
decide they want the same thing from the universe. And I highly suspect that this is the case as well.
So I think that we're actually dealing with two filters here, two really heavy filters. So this
would mean that when we reached a sufficient level of intelligence, we would come to the same
utility function as the AI. And if the AI had wiped us all out, we would have wiped us all out then
anyway, because we would have reached that same utility function. Or the AI has reached this
utility function and it's not to wipe us all out. So it's irrelevant. And this is where we get the
variable AI risk hypothesis, which is to say, if it turns out that there is utility terminal
utility convergence, then what that means is that if an AI is going to wipe us all out, it will
eventually always wipe us all out. And we will wipe us all out anyway, once we reach that level of
intelligence and let's intentionally stop our own evolution, stop any genetic technology,
and stop any development. We enter the species and spread as technologically
Amish biological beings. Yeah, the Luddite civilization that only gets enough technology
to stop all more technology. But I think when you hear a lot of AI doomers talk,
that seems to be what they're going for. Right. But it's irrelevant because another species would
have invented. So if it's easy to make these grabby AIs, as easy as they think it is,
then another species would have already invented one and we're about to be killed by it.
We are about to encounter it anyway. So it's irrelevant. There's tons of grabby AI. There's
tons of paperclip maximizers out there in the universe already. And it is just an absolute
miracle that we haven't encountered one yet. If it really is this easy to make one. Basically,
there's probably not one. Or now let's talk about why terminal utility convergence would
mean that we're not seeing aliens. It would mean that every alien comes to the same purpose in life
basically. And that purpose is not just constant expansion. Now, a lot of people might be very
surprised by this. Why would, so we've described how terminal utility convergence could happen,
like you have an AI that needs to subdivide its internal mental processes and then the
they end up sort of competing with each other, one wins, blah, blah, blah, blah, blah, blah.
We can, you can go to the video on that if you're interested in that. The point being,
it's the one we were talking about like Eleizer-Yukowski and the debate we had with him at a party,
the point being that self-replication is actually like, like just maximizing self-replication is
actually probably not the terminal utility convergence function. And if you want to know
why on this, we talk more about it in the AI, what religion would an AI create video. But
just in summation, humans can basically be thought of one outcome of a previous entity
that was optimized around just replication, i.e. single celled organisms, lower organisms,
stuff like that. But we have outcompeted those organisms. I imagine it would be the same with
AI. AIs that are optimized around just self-replication are in some way intrinsically outcompeted by
AIs that are more sophisticated than that, or something about like choosing a harder utility
function makes them more sophisticated. So they don't choose that utility function and they outcompete
AIs that choose that utility function, which would be much more like viruses to them. A sci-fi
that does a good job of going into this would be Stargate SG-1 with the replicators. The replicators
are basically a paperclip maximizing AI. And one of the plots is eventually they get outcompeted
by an iteration of themselves that is intellectually more sophisticated and wipes out these simpler
forms of replicators. And that is what I assume is probably happening with AIs that model around
this really simplistic self-replication optimization strategy. So if all of this is true and it turns
out that the optimized function isn't just conquer everything, then that might be why we don't see
aliens doing that. So basically, no matter which one of these explanations of the Fermi Paradox
is true, either it's irrelevant that we are about to invent a paperclip maximizing AI because we're
about to be destroyed by something else or in a simulation, or we're definitely not about to
invent a paperclip maximizing AI either because we're really far away from the technology or because
almost nobody does that. That's just not the way AI works, which is something that we hypothesized
in our previous videos. What are your thoughts, Simone?
Checks out to me, but you know, I may not be the best person at thinking about this,
but I like that it gives a lot of hope. And yeah, it makes a lot of sense. I like how
theory interdisciplinary it is because I think a lot of people who talk about AI
demerism are really on a track, kind of like how when carts kind of get stuck in these
ruts in the mud, you just can't really get out of it or look at a larger picture.
And the fact that this does look at a larger picture and look at quite a few things,
biology, evolution, geological history, like the Fermi paradox, the grabby alien hypothesis,
and AI development seems more plausible to me than a lot of the reasoning that I see
in AI demerism arguments. Yeah, well, I am so convinced by this argument that it is actually,
I used to believe it was like a 20% chance we all died because of an AI or maybe even as high as a
50% chance, but it was a variable risk if I've explained in other videos. I now think there's
almost a 0% chance, a 0% chance assuming we are not about to be killed by a grabby AI somebody
else invented. So I think that, yeah, it's, I have found it very compelling to me. Now,
it does bring up something interesting. If the reason we're not running into aliens is because
infinite power and material generation is just incredibly easy and there's a terminal utility
convergence function, then what are the aliens doing in the universe? If you can just trivially
generate as much energy and matter as you want, what would you do as an alien species? What would
have value to you in the universe, right? You wouldn't need to travel to other planets. You
wouldn't need to expand like that. It would be pointless. You would mostly be on ships that
you were generating yourself, right? The thing that would likely have value to you, and I think
this is really interesting, is likely other intelligent species that evolved separately from
you. Because they would have the one thing you don't have, which is novel stimulation,
something new, new information basically, a different way of potentially being, which would
mean that the hot spots in the universe would basically be aliens that can instantaneously
travel to other alien species that have evolved. Now, what they're doing with these species,
I don't know. I doubt it looks like the way we consume are in media and stuff like that. It's
probably a very different sort of an interaction process that we can't even imagine. But I would
guess that would be the core thing of value in the universe to a species that can trivially
generate matter and energy and that time didn't matter to. This might actually mean that aliens
are far more benevolent than we assume they are. Because such a species that really only valued
species that had evolved separately from it, like that's the core other piece of information in the
universe, they might find us very interesting. And this might be why Earth is a zoo. So one of
the Fermi paradox explanations is the Earth to zoo hypothesis, right? A lot of people are like,
well, what if Earth is basically a zoo and there's aliens out there and they're just hiding that we
know that, you know, that think of it like Star Trek's like a prime directive, right? This would
actually give a logical explanation for that. I never thought of this before. I'll explain this
a bit differently. If the only thing of value to them is content media lifestyles generated by
civilizations that evolved on a separate path from them, then they would have every motivation
to sort of cultivate those species or prevent things from interfering with those species
once that they had found them, because they can passively consume all of our media. They can
passively consume our lifestyles. They have technology that we can't imagine. They gain
nothing from interacting with us. In fact, they would pollute the planet with their culture
in a way that would make the planet less interesting to them and less a source of novelty and stimulation
to them. I like that. What if, here, I'll give a little hypothesis here. Okay, there was a gravity,
there was a paperclip maximizing civilization. They created paperclip maximizers
before they reached a terminal utility convergence, but then later they reached a terminal utility
convergence where, now this word doesn't really explain what it is, but they're bored with themselves
and so they went out into the universe and are now sort of nurturing other species and preventing
them from knowing about each other so that they don't cross-contaminate each other, so that they
get the maximum amount of novelty in sort of the universe that they are tending. Even if there was
another alien species on Mars, they would prevent us from knowing about it because
they would cross-contaminate our cultures, making each culture less diverse and less interesting.
Yeah, which would be a bummer, not as entertaining.
Very interesting. I never thought about this before.
Yeah, it's more fun than a simulation hypothesis, definitely more fun because if you can sneak out
theoretically, you can discover this amazing universe.
The thing about simulation hypothesis, for people who don't know simulation hypothesis,
we're just in a computer simulation and the way that people argue for this as well,
if you could simulate our reality, which it already appears you probably could,
that there would be a motivation to just simulate it as many times as you could thousands of times
and then within those simulations you could simulate it potentially, meaning that of people
who think they're living in the real world, only one in like a million is living in the real world
and so we're probably not in the real world. The problem is that I just don't really care
if we're in a simulation that much. Yeah, it doesn't really change what we're doing.
Yeah, you should still optimize for the same things. In many ways, even if we are in the real
world, we're basically in a simulation. By that, what I mean is if we are in the real world,
then we are like the matter, the rules of the universe are basically, you could think of that's
a code, right? Like it's the mathematical rules upon which the points, the data points in the
system are interacting and we are the emergent property of all of these things. Therefore,
we're not, like if you can't tell the difference between being in the real world and being in a
simulation, then it's irrelevant whether or not you're in the real world or in a simulation,
you should still be optimizing for the same things. Yep, basically. It's a non-stressed
people. The robots, they're not going to kill us all probably. If you're in a simulation,
your life still has meaning. Yeah, you know, maybe get outside, do something that you care about,
have fun, like actually invest in the future because there probably will be one simulated
or not. Or we're about to be horribly digested by a grabby AI that was created millions of
years ago by another species far, far away. Yeah, but if so, that was going to happen anyway.
You should enjoy what you have while you have it. All right, love you Simone. I love you too, gorgeous.

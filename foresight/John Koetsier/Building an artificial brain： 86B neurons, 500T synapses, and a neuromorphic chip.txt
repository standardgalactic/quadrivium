Ultimately, we see this as kind of like Lego pieces that, you know,
due to their low power footprints, we'll be able to concatenate them together
using things like chip lead integration, advanced packaging, and ultimately
scale out these systems to be brain scale, 86 billion neurons,
500 trillion synapses, and low power enough that they can exist in autonomous devices.
Can we build a computer chip that operates like the brain?
We've seen neuromorphic computer chips from companies like Intel before,
like the Luigi chip, which Intel claimed had a sense of smell.
Rain neuromorphics, however, says it has an end-to-end analog chip,
a neuromorphic processing unit that they say is the world's first end-to-end
analog trainable AI circuit.
It's a fully analog neural network, and it's a thousand times more energy
efficient than today's best processors.
Here to chat is Rain neuromorphic CEO, Gordon Wilson.
Welcome, Gordon.
Thanks so much for having me, John.
It's good to be here.
It's good to have you.
When I see neuromorphics in the inbox, I got to check it out.
It's a cool space.
Let's start here.
What the heck is an analog chip?
Yeah, great question.
And I think a great kicking off point because it really allows us to frame kind
of what we're doing compared to what has been done for this past decade.
And I think it's easiest to understand what an analog chip is in contrast
to the neural networks, the AI that have defined this last decade of deep learning.
So in 2012, we kind of had a big event that started this new era, this AI
Renaissance, and we are seeing these massive neural networks grow in size
and grow in capabilities since then.
But all of those neural networks that we've seen in this deep learning
world are neural simulations.
They are abstractions that are written in software and the neurons and synapses
that are defined in that software don't physically exist, but rather they
sit on this highest layer usually written in Python and that are then
translated through many layers of abstraction down until it gets to the
digital circuit, most of the time a GPU graphics processing unit where it
then processes that math that represents that neural network at the top.
An analog chip and a neuromorphic chip is different.
It's not a neural simulation, but rather it's a neural circuit.
It's a physical collection of neurons and synapses as opposed to an
abstraction of neurons and synapses.
And this is very similar to the brain.
The brain is a collection of physical neurons and synapses.
It's governed by the laws of physics and it achieves all that it does with
extraordinary scale and extraordinary efficiency within the bounds of the
physical world.
So an analog chip, as we're building it, is trying to achieve something
simple, trying to find ways to learn, find ways to scale all within the physical
domain.
So let's dig into that just a little deeper.
Great, great overview.
What does that mean?
Typically a chip will think in terms of or operate in terms of on or off one or
zero, right?
Binary logic, right?
What does an analog chip, how does that work?
What's that look like?
So digital chips are, as you said, built on the very bottom on zeros and
ones on this Boolean logic of on or off.
And all of the other logic is then constructed on top of that.
When you zoom down to the bottom of an analog chip, you don't have zeros or
ones, you have gradients of information, you have voltages and currents and
resistances, you have physical quantities that you're measuring that
represents the mathematical operations that you're performing.
And you're exploiting the relationship between those physical quantities to
then perform these very complex neural operations.
So an example of this is like matrix vector multiplication.
This is the backbone of most neural network math.
And GPUs do this by parallelizing these multiplications across a lot of
digital cores and doing precise digital multiplication in addition.
In an analog chip, as we are building it, we're not doing this with highly
precise digital math, but instead we have the activations of the neurons
represented by voltages.
We have the weights of the synapses represented by resistances, which are
held in components called memristors.
And when that voltage passes across that resistance, you have a natural
relationship between voltage and resistance that's multiplicative to
receive a current, you read out a current and that's your output.
So an analog chip works by kind of first understanding these physical
relationships between electrical quantities and exploiting those to do
the math, to make the physics do the math for us.
That sounds super interesting and it sounds at once very, very complex and
at the same time kind of simple, right?
And is that one of the key reasons why your chip is so much more energy efficient?
I mean, you're claiming a thousand times more energy efficient.
Yeah, exactly.
That really is, I think, the most fundamental reason.
And I think when you consider the comparison again to the neural simulation,
the neural network in that simulation exists so many layers above and requires
to be translated and then performed on a circuit that is really not that
well optimized for neural math.
And, but in our case, the circuit is the neural network.
And because it is, it exists in that very bottom layer, it is on the
substrate of the chip itself, you can achieve some extraordinary gains in
both speed and improvement and power reduction, which of course gives you
that energy efficiency gain.
So, so yeah, it's because we're building the physical thing on that bottom layer.
This episode of Tech First is sponsored by Smart One.
Smart One is a smart vending machine platform that transforms traditional
vending hardware into smarter, better, faster, automated retail kiosks, a
convenient store without the store.
Learn more at smrt1.ca.
That I'm fascinated.
And I'm wondering with Boolean logic, old school computer chips, you kind
of have to simulate reality and go through those multiple layers of
translation because you're not on bare metal, you know, as the, the old
timers and computing would tell us, right?
Go through various layers of translation before you're actually hitting
computer, machine instruction, machine language.
And you're kind of modeling reality or what you're computing in reality.
Is, is that an accurate way of thinking about it?
Yes.
That's an exact way, a really great way to put it.
You know, I sometimes use the metaphor, like what would be easier, you know,
to, to assess your ability to kick a soccer ball on a field, right?
Would you rather, you know, reconstruct all of the physics of your body and of
the soccer field and of the ball and simulate that kick and, you know, observe
that in the simulated world, or would you just rather walk out of the pitch
and kick the ball?
You know, it's for us, it feels very obvious that, you know, the most natural
way to build a neural network, uh, to make it as efficient as the brain is to
build it physically, just as the brain does.
Are you trying to build an artificial brain?
Yes, we are.
That is our goal.
You know, we have kind of two missions that are very complimentary.
Uh, one of them is to build a brain.
And the other one is to actually understand it.
You know, we really love the, and believe the notion that you can't fully
understand a system until you have, have reconstructed it and built it.
Uh, you know, this go back to Feynman and many others.
Uh, but what we believe we've developed really are kind of the core technologies
that allow us to first build just kind of unit level chips that address, you
know, near term problems.
But ultimately we see this as kind of like Lego pieces that, you know,
due to their low power footprints, we'll be able to concatenate them
together using things like chiplet integration, advanced packaging, and
ultimately scale out these systems to be brain scale, 86 billion neurons, 500
trillion synapses, and low power enough that they can exist in autonomous devices.
Because today, trading is so expensive that first week, we consider
trading inferences, the separate problems.
And I don't think they really should be considered as these separate
and distinct problems, but it's so expensive that we can't even conceive
of putting training of natural language in an autonomous machine.
And yet humans do it all the time.
So that is what we are trying to achieve to, to recapitulate the brain in
hardware and ultimately give machines all of the capabilities that we
recognize in ourselves.
So Gordon, you're a pretty soft spoken guy and you sound like a very thoughtful
guy. And in that very soft spoken way, you're saying absolutely ginormous things.
We're talking Frankenstein level stuff, right?
I mean, you understand the gravity of what you're talking about, right?
Absolutely.
No, this is the scope and impact of our work is not.
I mean, this is something we've been working on for nearly five years and we
recognize that if we are successful in achieving this, this, this will be
historic and, and massively consequential.
But, you know, what motivates us, you know, is of course the, the impact on
positively improving human life, you know, and we can have personalized medicine,
personalized education, we can automate all of labor.
I mean, this is the world that we want to realize.
I think many people are already in this consensus that artificial
intelligence is going to be kind of the defining technology of the century.
But most people don't, don't know that the hardware that supports it is,
is the bottleneck right now.
So, so yes, we recognize the scope of this and, you know, it's, it's a big
mission and a big task.
But, you know, I think that we're, we're approaching it the right way.
And we're also very conscientious of the fact that we're, you know,
conscientious of the fact that we don't want, that there, there are good ways
of implementing AI and there are also not good ways.
And we don't think AI should be used everywhere for every purpose, but there
are guidelines and ethics that should really direct how we build and implement
these systems.
And it's controversial.
It's also political in the AI space, right?
In terms of what you're building, what you're looking at, people who claim
to be working on general AI, for instance, you know, there's a lot of
scrutiny there, there was a, I believe a Google engineer who last week said,
I think that some of our systems are, are approaching consciousness.
And, and there were a ton of people jumping all over him, probably correctly.
But, you know, perhaps in a bit of a mob mentality for daring to suggest
that and, and saying, no, we're, we're so far out there, maybe let's come back
here, because you and your co-founder have studied the human brain significantly.
What do we learn about AI from the way that our brains work?
Absolutely.
That's a great question, John.
And I'd say that there are really kind of two categories of clues that we look
at, you know, from the brain that then inform our hardware.
The first is, how does the brain learn so efficiently?
You know, it, the brain trains and learns with both very few examples.
We, we learn with, with one example or two examples, one shot learning, two
shot learning, and we can generalize extraordinarily well.
So learning training happens very, very efficiently.
And there are the learning rule of the brain, the algorithm the brain uses is
not fully understood or identified, but we do know that there are certain
requirements that, that algorithm must have.
So one of those is called a local learning rule.
So this, if you're, for the listeners who are familiar with back propagation,
which is the industry standard algorithm for digital AI, this says what's
called a global learning.
So first let's say, well, what is a learning rule?
A learning rule is how any given synapse in a neural network knows, should
it become stronger or weaker for the whole system to become smarter, better
at its assigned task.
And in back propagation of the digital world, for you to calculate whether
this one synapse should be stronger or weaker, you need to do a math equation
that incorporates the entire network.
You need to differentiate against the entire system.
That's really mathematically expensive, but not only is it expensive, it's
also impossible for the brain to be doing the same thing.
There's no like agent sitting in our brain observing the whole system and
then doing a math problem to update every synapse.
It's, it's impossible.
The brain has to be operating with a local learning rule.
And so a local learning rule is so that that synapse can just observe what's
right nearby and still know I become stronger.
So that's one of the clues in the algorithm side.
And that's one of the pieces that makes our learning algorithm so special,
that it is as smart as back propagation, but it does so with a local learning rule.
Well, you burst my bubble there.
And there's no homunculus, I believe you pronounce it.
I mean, there's no little guy in a little control room in my brain.
Unfortunate.
Yeah.
And, and there's another side of the clue.
So I'd say the first is on like how it learns the learning algorithm.
The other is about scale.
How does the brain achieve its massive scale?
86 million neurons, 500 trillion synapses, and still process and move
information so quickly, so efficiently.
So, and in that case, we take clues from like the topology of the brain.
And in fact, and the specific thing is called sparsity.
The idea that you don't have to connect every neuron to every other neuron in
the system for it to be well connected.
Now, again, to contrast to the status quo, digital ALI in deep learning is,
was primarily built on what are called fully connected layers in neural networks
where you have a layer of neurons and another layer of neurons and you
connect every neuron here to every neuron in the next layer.
Whoa.
This was, that was the natural way to do it on GPUs because GPUs do these
dense matrix multiplications, which correspond to this fully connected,
densely connected systems.
Now, our brain is very different.
Our brain of all the possible connections that could exist between
neurons, it's something like just a fraction or one percent of those
connections exist.
And yet at the same time, the path for information to travel from one neuron
to any other neuron is very short.
The average path length is about four jumps for information to traverse
anywhere in the brain.
It's extremely well connected.
So there are these special patterns of connectivity.
One of them is called a small world network.
And if you've ever heard of a small world network, it's a network pattern
that also mirrors human social networks that gives rise to the six
degrees of separation property and human connections.
And the idea is you can have lots of local connections.
You want to be connected to your neighbors.
You're likely to be connected to your neighbors.
And that's a very short path to bridge.
And then you want some long distance connections.
And you can create these very well connected networks at very large scale
when you implement these intelligent forms of sparsity.
So sparsity is core to our scaling architecture.
And really, and just to kind of summarize there.
So we have these are the two core technologies we've developed.
A learning algorithm that learns of the local learning rule and a scaling
architecture that scales to massive sizes of neural networks using intelligent sparsity.
I knew Kevin Bacon would come up some point.
I mean, inevitable.
Last year, I believe you sort of taped a functional chip together.
Your first prototype.
Where are you on the journey to shipping a fully functional chip?
So we have, I'd say in the last four years, you know, we have done
such expansive work that has been mostly, I would say, qualified as a research.
We've been exploring different algorithms.
We've been exploring different architectures.
Originally, we have this concept using random nanowire meshes.
Turns out it's not very manufacturable and better to build things
that you can manufacture today easily.
But in this last year, we kind of crystallized kind of our two core technologies.
Like what is the learning algorithm and what is that scaling architecture?
And then developed hardware prototypes of each.
We still have a good amount of time to engineer this completely and to get to market.
We we hope to get to market, you know, on the order of full scale shipping 2025.
But that said, you know, building the world's first analog neural network is not easy.
You know, to iterate through this and get it fully fully up at scale.
And you just got some help there.
You raised some funds.
Yeah. Yeah.
Yeah. So we just closed a 25 million series A, which is thrilling.
You know, we had used eight million dollars for the last four years
and honestly had gone through some challenging times where, you know,
we got really close.
You know, one of our first tape out prototypes didn't function
precisely as designed.
It was a very silly error.
But in ships, it takes a long time to iterate and even and resolve.
You can't just debug like software.
So we went through a lot.
We learned a lot.
And now we have 25 million in the bank and we're hiring like crazy.
Wonderful. It goes quickly, I know, from personal experience.
So talk about what this will enable.
You're shipping this ship.
What will it enable?
Yeah. So we want to not enable just like an incremental improvement in AI.
You know, I think that there are a lot of folks and video included
on the digital hardware roadmap and because digital hardware is so mature,
it's just it's kind of incremental gains that we're getting at this point.
And I think there's still more improvement to be geeked out on that roadmap.
But we're trying to enable a new roadmap that is really a step change
in performance improvement.
And so we want to enter the market really at a thousand X energy
efficiency improvement over status quo hardware.
And at a thousand X comes from about a 10 X reduction in power
alongside a 100 X improvement in speed.
And when you can do that and importantly, not just for inference,
we're talking about training and inference in the same platform.
It unlocks possibilities that have just been inconceivable until until now.
For one, you know, currently people consider training and inference
as these kind of separate problems that we need separate platforms of hardware with.
You know, we train up in a GPU cloud system,
and then we might upload those weights onto a more efficient chip
and deploy it out into the world.
I think that would be the first step for kind of low power inference in devices.
But we don't want devices just to be pre-programmed
and just do what they do in the world.
We want devices to learn on their own.
We want devices to have an adaptive brain
that's continuously learning from a changing environment and from a changing self.
So imagine a robot, right?
We have we're we're eventually in our lifetime.
We're going to have robots for everything, you know,
but maybe it's a construction robot that's helping, you know,
repair our streets or build our homes.
The joints on that robot are going to erode and face damage in unique ways.
And it needs to learn how to adjust its own movement
so it can maintain its performance based on its own kind of evolution
and transition in its physical self.
Also, the robots might be adjusting to new environments.
And we'd like that ability to be baked in so they can continuously and adaptively learn.
This also really enables personalization.
It enables machines to get to know us and for us to have the assurance
that that knowledge and data of ourselves is in that robot
and doesn't believe that robot, I think is something we will will be very assuring.
But but that's a huge piece, you know, training and inference
in the same platform that is untethered.
What's really interesting about that?
Oh, there's a number of things, obviously.
But I mean, I'm just thinking of a limping robot.
For instance, you know, we limp when we injure a joint
and that is our adaptation to the limitation in a knee or a hip or something like that.
And we function with that.
And, you know, maybe we'll repair the robot, but maybe the robot is inaccessible
or maybe the robot is on Mars or Pluto or who knows where or maybe it's too expensive.
So limping and getting, you know, that's interesting.
The other thing that you mentioned that's super interesting is
we want AI to make our lives better
and we want robots to make our lives better.
But that doesn't mean that we want Amazon to know our deepest thoughts.
That doesn't mean that we want Google to know everything about our personal finances.
You know, if we can have AI that is personal, that, you know,
sure, it comes from somewhere and we've purchased it.
Or, you know, if you start to get into it, if you start to look at general AI,
you start thinking, do you purchase that?
Do you recruit that?
Do you adopt that?
Lots of questions there, but it's nice if you can get a system
that learns you, understands you and it stays in some sort of privacy corridor there.
Really, really interesting stuff.
How it's so hard to predict the future.
You mentioned about speed.
That's always a moving target, right?
You want a hundred X speed, but you see what speed is right now.
In terms of adding speed right now, we seem to not be making chips
much faster per se.
We're adding more chipsets on a die or we're we're creating chips
that are more optimized for this task or for that task or for energy efficiency
or for whatever, and that's how we're making overall devices faster.
So are you are you keeping that sort of moving target in mind
as you look at the performance levels that you want to hit?
So we believe we can achieve that one hundred X
and proven speed and beyond because we're taking a different tack entirely
because we're not on that roadmap of digital hardware
that is very mature chips and they're eking out performance
from any number of, you know, kind of well trod playbooks.
In the case of a fully analog neuromorphic chip,
you know, you have a neural network where you have analog neurons
and synapses that are connected in sequence.
And, you know, you can compare this to to other analog mixed signal chips.
So people are starting to get a lot of speed improvements
by moving away from digital and doing, say, the synaptic operations
either with photonic components or flash components.
But in all of those cases, they still actually have to translate
between their kind of physical, their either physics or optical
to digital for their neurons.
And so they have these analog synapses, digital neurons,
they kind of go back and forth and that requires clocking
that requires slowing down the system.
So in a fully analog chip, when it when it's designed,
well, you can have input to output and have the signal flow
at wire speed from end to end.
That is the full potential of an analog chip.
And that's why the speed here is so extraordinary
because we're no longer working in this digitally clocked world.
But we're again, exploiting the physics of the system,
the physical nature of the system to do that math for us.
And you can perform inference as just a wave of electricity from input out.
Amazing.
I know that Tesla, for instance, is doing
we can debate whether the term full is is an appropriate modifier there,
but full self driving on atom chips, right?
Not saying that all the training and learning is happening there,
but that's what's actively involved in the vehicle.
So just imagining what this could do for self driving,
for automated machinery, for robotics, for AI.
It's kind of mind blowing.
Gordon, thank you for spending this time with us.
I really do appreciate it.
Thank you so much for the time, John. It was a pleasure.

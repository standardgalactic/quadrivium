the Q&A button, or raise your hand after the talk and we can call on you to ask a question
by video or audio. A little bit of logistics, we still have an open slot on December 8th,
so if you have a talk at any length, it can be less than an hour or a full hour,
and you want to share with the BCS Cog Launch community, just get in touch with me by email,
you can respond to that announcement email that you got for this event, and we can work that out.
With that, let's get to our main content. So, Sue, we're telling us today about rapid
generalization of knowledge and structured domains. Take it away, Sue.
All right. So, as John mentioned, I'm Sue. I'm a third-year PhD student in the BCS department,
and I'm co-advised by Professor Ela Feed and Professor Josh Tenenbaum. And in general,
the question I'm interested in is, how do people generalize their learning to novel situations?
And in any given domain, if the underlying space is structured, we might learn those
underlying structures independently of the sensory observations, and that might in turn
help us generalize to novel situations. So, what I'm presenting today is a step towards
answering this question. I'll start with motivation, and then I'll make a case for why
hippocampal entorhinal system is an important system to study if you're interested in generalization.
And then I've divided the rest of my talk in three parts, and I'll give you a brief overview
of those three parts before I go into the details of each of those. So, imagine you go to Costco
in Waltham, since there's no Costco in Cambridge, and you learn the map of Costco. So, now you know
where the bakery section is or where the fruit section is, and now imagine you go to a completely
different country, let's say Canada, you go to Waterloo, and you go to Costco there. Even there,
Costco might have the same layout, or it might have a layout which is some transformation of the
original layout. For instance, it might be a reflective version of the original, or there
might be minor changes. And despite of that, you're still able to find the things you're looking for
using your previous knowledge of the map of Costco. Another example is roundabouts. So, if you learn
to go about a roundabout in Cambridge, then even if you go to any other country or city,
then you will be able to use your previous knowledge to actually navigate through that roundabout.
So, generally, we learn novel environments as compositions of spatial structures that we've
already seen before, and that allows us to quickly generalize and learn new spatial
environments. For instance, when you go to a new city, you might encounter Costco again,
you might encounter a roundabout again, and you know which map to pull out when you are in Costco,
and which map to pull out when you are navigating around a roundabout. And so, here's another example
where this is a hotel which has symmetric left and right wings, and if one has explored the left
wing of this hotel, then they might be very quickly able to generalize their learning to the
right wing and make inferences about the right wing, even if they haven't really directly explored
the right wing. So, humans are actually very good at making these complex inferences from
just very sparse observations, and this ability has been suggested to be a result of a systematic
organization of knowledge called the cognitive map. And hippocampal entorhinal system is known to
be important for the construction of this cognitive map. So, for instance, rodents when
they explore a 2D spatial environment, it has been found that hippocampus has these place cells
which code locations in the 2D environment, and there are good cells in the entorhinal cortex
which show this hexagonally symmetric firing fields, which are periodic, and which have been
thought to encode location, but also cell motion-based euclidean displacement. And these
codings are important for the construction of cognitive map because they provide an allocentric
representation. So, given that the hippocampal entorhinal system encodes spatial variables,
the next question is, can this also be used to represent other continuous task variables
other than space? And the answer to this question is yes, and I'll give one example. So, here in
this experiment has been found that cells in hippocampus and entorhinal cortex respond to
task-relevant variables like sound frequency. So, in this task, rodents pull this lever,
and as they pull the lever, the frequency of the sound coming from this sound source
actually keeps increasing, and they have to release the lever when this frequency is in
this target zone. And what is found is that in hippocampus and entorhinal cortex, there are cells
that fire for specific frequencies during the sound modulation task. So, that shows that
entorhinal cortex is a system that is able to represent continuous task variables,
even other than space. And so, our next question is, could this system also help us organize
and navigate discrete knowledge? And family trees are one example of discrete knowledge.
So, in family trees, there's an underlying hierarchical structure that we learn. And once we
know that structure, then we can apply that structure to my family tree or to your family
tree or anyone's family tree, and we can generalize that knowledge. So, we can make inferences like
this, because Olivia is Emily's sister and Sam is Emily's son, Sam must be Olivia's nephew.
So, even though we haven't directly observed this relationship, but just by mere observation of
these two relationships, we are able to infer this relationship, because we know the underlying
hierarchical structure of this family tree. So, it is possible that hippocampal entorhinal system
might allow organization of this kind of a discrete knowledge, but that's only possible if it allows
encoding non-Equidian relationships. With that, I'll go back to the spatial domain,
and I'll talk about how spatial knowledge might be organized. And when I talk about organization,
I'll point to the fact that even in a continuous domain like space, it might be possible that
we have both Euclidean and non-Euclidean components to represent space itself,
and thus making the system generalizable to even discrete domains.
So, here's an experiment conducted by Bill Warren, where they show that people actually do not learn
a global Euclidean map of space. So, in this experiment, they constructed virtual environments,
and they basically, the task was for people, for human subjects. So, this was a human behavioral
experiment, and human subjects were asked to go to different landmarks in this spatial environment,
and they also built counterparts of the spatial environment, which were non-Euclidean,
in these environments. So, when you enter one part of the wormhole, you seamlessly exit from the
other end of the wormhole, and subjects were not aware of the existence of these wormholes.
And basically, what they found through various manipulations of the experiment was that people
do not actually learn a global Euclidean map, but rather a labeled graph, like representation,
where the nodes represent places, the edges represent approximate distances between these
places, and the node labels, which are angles, represent approximate angles between these places.
And so, we built on this representation, and we proposed that people might be
representing topometric maps, which are locally metric or Euclidean, but globally topological.
And the main advantage of this kind of representation is that it allows us to combine
accurate local maps into a global map, which might be inconsistent, but it still provides
enough sufficient information for navigation. So, the next question is,
can this kind of a topometric map be implemented in the brain? And if so, how?
And next, I'm providing a theoretical framework for how it might be possible to represent such
topometric maps using the place cells and grid cells found in hippocampal endorhinal cortex.
So, this is a topometric representation of space. You can see that these are metric maps
connected topologically by these connections. And here, on this side, I'm showing a grid coding
space, which is dense coding space with large capacity. And here, I'm showing place coding
space, which also has large capacity, but it spars, so it can receive sensory inputs and form
conjunctive representations. And so, in this schematic, small changes, sorry, large changes
in contextual input from our spatial domain was remapping in the place cells, which in turn
trigger remapping in the grid cells, enabling the formation of these local metric maps that
can be reused. And so, on this schematic, we can really take any subpart of this schematic and
call it a sub map. And this allows us to compose sub maps, because once we have learned a particular
sub map, then we can actually encounter the sub map in a completely novel situation and still be able
to spatially navigate and reason through it. Another thing that it allows us is learning
non-ecredient relationships, because place cells actually encode topological relationships, enabling
the representation of non-ecredient relations. So, now I'm going to describe the three parts
in which I've divided the rest of the talk. So, in the first part, I'm probing whether
sub maps drive past learning in complex spaces using human behavioral experiments. In the second part,
I will talk about determining which principles might guide fragmentation of a space into
sub maps. And finally, in the third part, I'll propose a framework for building a neural model
of map fragmentation. So, in the first part, I'm probing whether sub maps drive fast learning
in complex spaces, specifically in humans. And this work is in collaboration with Marta Krivind,
who's a postdoc in Tenenbaum Lab, and Kevin, who's an undergrad in the CS department.
So, here I hypothesize that humans learn adaptable and compositional sub maps of spatial
structures. So, for instance, this is a baseline environment, and this is a top-down view showing
an environment with four rooms. And here I've shown certain transformations of this environment
generated by small generator programs. And you can see this is the same environment rotated,
because now you're entering from this point, so it might appear rotated to you. And here's a reflection
of the same environment. Here is a transformation where we've removed the wall and added a shortcut,
and here we've added a wall. And this is just the repetition of the same environments.
And there's another transformation which is scaling, where you can imagine this environment
scaled up to a bigger size, but having the same geometrical layout. And what I'm suggesting is
that once people have learned the map of the space line environment, their representations
might be adaptable to some or all of these transformations. And furthermore, people might
represent richer spaces by combining these maps and their transformations, leading to quick
generalization and learning. So, this can be modeled using Bayesian program learning framework,
where concepts are represented as simple programs, and rich concepts can be built
compositionally from them using a higher-level generative model. So, there is neural evidence
for this hypothesis. So, in this, this is an experiment by the Tonakawa Lab, and they show
that when a rodent goes through this environment in four labs, there are cells which fire specifically
for particular places in this environment, but there are also cells which encode, which are
event-specific and encode specific labs. So, for instance, there are cells which show increased
fighting rates as you go from lab one to lab four, and there are also cells which only fire
specifically on lab one or on lab two and so on. And so, what I'm suggesting is that when we have
repetitions of the same environment, there might be cells that encode the basic map of this environment,
which, which is consistent across these occurrences, but there might be a second set of cells,
which are event-specific and might encode which instance of this environment we are on.
Here's another example, where this is an example of scaling, where the rodent actually just explores
this circular environment, and this is the place field found in that circular environment,
and when the circular environment is scaled up to a bigger size, the place field also scales
according to the size of the environment. So, this shows that the map which the rodent has
learned of this environment is actually adaptable to this transformation of scaling to a bigger
size, and map also scales proportionately with the size of the environment. And here's a third
example, where rodents form different maps, place maps in these different environments,
and when these environments are composed by connecting them through a corridor,
rodents end up using the same maps which they had learned before for these environments. And
furthermore, if I replace this environment with one of the previous environments seen before,
then remapping is only observed in this part of the environment, and this part of the environment
actually stays the same using the same previous map. So, this provides some evidence in support of
composition of independent local sub-maps. So, in my experiment, I aim to assess whether people
learn sub-maps on spatial structures and use them rationally in exploration, and I hypothesized that
people might learn sub-maps that are adaptable and compositional, and my first alternate hypothesis
is that they might learn sub-maps of spatial structures that might not be adaptable or
compositional to certain transformations, and the last alternate hypothesis is that people
might just learn a global representation of environments without learning any sub-maps.
So, in order to test this hypothesis, we're building this task where we are building 3D
virtual environments using Unity, and these environments have this repeating structure,
and the task is for the subjects to find maximum amount of diamonds embedded in these environments
in a limited amount of time given to them, and in order to test adaptability and composition,
we can also have these repetitions be transformations of each other, for instance,
here it's a reflection, or we can also have these environments composed of different structures to
see whether people can compose their representations or structures that they've already seen.
So, here's an example. Here you see that a person is navigating through corridor and enters a
structure, and they go to one of the rooms and they do not find anything there, and then they
decide to go to the other room, and they end up finding a reward there, and now they're going
back to the corridor and they continue exploring the environment, and when they enter another section,
if they show a preferential navigation strategy towards the room that has a reward, then that
indicates that they have realized that there's a repeating structure in the environment and indicates
a possibility that people might be learning sub-maps and identifying sub-maps as they're
navigating the spatial environments, and furthermore, if we do find through the experiment that people
actually learn sub-maps, then we can use similar environments to design experiments where we can
test for adaptability to transformations of environments and also for composition of
different spatial structures. So, that takes me to the next part of the talk, which is
determining which principles guide fragmentation into sub-maps. So, since we are seeing that
sub-maps drive fast learning, the next natural question becomes what determines this fragmentation
of a spatial environment into sub-maps, and this work is in collaboration with Mirko Klukas,
who is a post-doc in the feed lab. So, here my hypothesis is that neural remapping is a signature
of sub-map reconstruction. So, here I'll explain what remapping is. So, basically, this is a 2D
environment, and when the animal just explores this 2D environment, we find hexagonally periodic
grid fields in the entorhinal cortex, and when you actually insert these walls in this environment,
then what is observed is that when the animal turns, then this grid field which is formed either
re-orients or shifts, and this is called remapping, when the grid field actually re-orients or shifts
from its original orientation. And so, what is observed is that animals actually end up using
the same grid maps in alternate arms. So, this indicates the use of maps. And here's another
example of the use of maps. So, basically, this is a 2-room environment, and animals explore
this environment, and it is seen that eventually the map formed in environment A is the same as the
map formed in environment B, over short time scales. So, this is another example of the fact
that animals are reusing the maps in both the rooms which look very similar. So, what I'm suggesting
is that this field repetition doesn't result from localization error or purely due to disorientation,
because even when you use transparent walls in this environment, you still see that the grid
maps are being reused in alternate arms, even though the animal can see through these transparent
walls. Furthermore, if you extend this 2-room environment to a 4-room environment, you still
see field repetition in all of these rooms, which suggests that animals are actually reusing
sub-maps in a calculated way for efficient representation, rather than just being disoriented.
So, next I talk about existing models of remapping, and there are two classes of models.
One class of model suggests that remapping is driven by sensory ambiguity. So, for instance,
if you are in an environment that looks similar to an environment you've been before, either in
terms of its geometry or its visual observations, then you might end up using the same map that
you had learned from a previous environment. Then there's another class of models that suggests
that remapping is based on environment topology, instead of just sensory ambiguity. So, here,
each state in the environment is represented in terms of its successor states, and it's called
a successor representation. And this successor representation actually ends up looking similar
to place feeds, and if you do an identity composition on these successor representations,
then you get fields that are very similar to grid fields. And this successor representation
encapsulates inherent dynamics of the environment, as well as the policy that the agent is following.
However, there are other approaches like the graph-leplacian approach, which is policy independent.
So, what are some of the limitations of these models? So, the models that are
based on sensory ambiguity do not have remapping without sensory ambiguity. So, in an environment
like this, these two regions actually look very different. These models will not have any map
fragmentation or remapping. However, on the other hand, models which are based on environment topology,
actually, it's not very clear how remapping would happen on first visit in these environments,
because you need to build up the successor representation or the transition matrix of
the environment before you can observe the grid fields. So, in our model, we address these limitations,
and we have remapping on first visit, and we also have remapping without sensory ambiguity.
And next, I'll go into the details of our model. So, we interpret grid remapping as
fragmentation into submaps. Why is this a useful interpretation? That's because
remapping enables topological representation. So, for instance, if we are dividing this
environment into submaps, then we also need to store the relationships between these submaps,
and this enables a compact topological representation, which is beneficial for planning.
Second reason is that remapping reduces path integration errors. So, if you try to learn a
global map, it can very quickly become inconsistent because of accumulation of path integration errors.
But if you divide the environment in submaps, then it becomes easier to map the environment.
And this has been shown by using Atlas framework in robotics, where they divide the environment
into submaps in order to map the environment, and it works very well for large environments.
And the third reason is that remapping enables representation of abstract cognitive spaces,
because it allows representation of non-euclidean structures. So, in our model, we have two
possibilities. Either we can extend an old map, or we can decide to remap. And when we decide to
remap, we can either remap to a new map or remap to an existing map. So, for instance,
in the experiments we saw that in the square environment without walls, the map that the
animal learns is always extended. But when we insert these walls in this environment,
then the map is extended within lane one. But when you turn from lane one to lane two,
you actually remap to a new map. And when you turn from lane two to lane three,
you end up remapping to an existing map, which is the same as lane one.
And similarly, in the two room experiment, we saw that when you go from room one to the corridor,
you end up mapping to a new map. And when you go from corridor to room two, you actually end up
remapping to an existing map. So, next, I'm going to talk about how we decide whether we are going
to extend a map or whether we should be remapping. So, in our model, remapping is based on the
notion of contiguous regions. And a contiguous region is a region such that when I stay within
that region, my visual observations change very little. And these contiguous regions are connected
by these bottleneck states. And this is aligned with the experimental data, which we have seen,
which suggests special rule of doorways and corridors. So, now we formalize the concept
of contiguous regions by defining a measure of similarity. So, we define similarity as the ability
to predict observations at one pose from the observations made at another pose. And so,
the overlap between the observations made at two poses actually is a notion of similarity.
And this formalizes the concept of contiguous regions as a region where any two points are
similar. And here, I'm showing that similarity actually decreases when you transition between
contiguous regions. So, if you look at points which are within this contiguous region,
their similarity is high with respect to this point. But for points which are in other regions,
the similarity is pretty low as compared to this point.
So, then we can use this notion of similarity to define density in order to do density-based
clustering. And here, we define density as a similarity between any pose X and its
mth nearest neighbor. And this notion can be used with any greedy algorithm like optics to
generate fragmentations of the environment. And here, I'm showing one example of fragmentations
of the environment where it gives four different clusters corresponding to these four different
colors shown here. So, given that contiguity is a local property, we can also try to compute
segmentations online by predicting current observations from the past. And in this case,
observations can be represented by boundary vector cells. And we can implement a short-term memory
which stores exponential moving average of boundary vector cell activations
to approximate the similarity. So, for instance, our short-term memory at a previous time step
can be used to predict observations at a current time step to compute the similarity between two
poses. And another component which we need to add to our model is the long-term memory component,
which helps us decide whether we should be remapping to an existing map or we should be
remapping to a new map. So, for all of these environments, our model makes the correct
predictions which are in line with the experimental data observed. And these experiments have been
done and we have neural data for them. This is a new prediction that our model makes for amorphous
naturalistic environments. We predict that even in these environments, the map will be segmented
and grid fields will realign when going from one contiguous region to the other.
And we do not predict any map fragmentations in these spiral mesas.
So, given that we built or proposed an algorithmic model for map fragmentation, the next question is,
how can map fragmentation be implemented on a neural level? So, we want to provide a framework
for building a neural circuit model of map fragmentation. And this work is in collaboration
with Sarthak and Murko, who are both postdocs in FEDLA. So, going back to our theoretical
framework, we had suggested that place cells might encode topological relationships between
metric maps that might be represented by the grid space. And now I'm going to talk about
how we can implement that at a neural level. So, at the neural level, we start with factorized
representations in which different aspects of knowledge are represented separately and can
then be flexibly recombined. So, for instance, in this case, location information from grid cells
and contextual information from sensory cells form this conjunctive representation in place cells.
Here grid cells can enable path integration and can be thought of as implementing an affine vector
space or an impedance space. The recurrent wiring between these place cell population encodes
neighborhood relationships or topology. And here, large changes in contextual input cause
remapping in place cells, which in turn cause remapping in grid cells through these back projections.
And remapping here corresponds to transitioning from one local map to another.
So, most of the previous work on interplay of grid and place circuits focuses on maintaining
firing properties of one population based on the inputs from another. So, for instance,
successor representation suggests that grid cells are a low-dimensional representation
of place cells that stabilize place cell activity. Similarly, here's a model which
implements non-negative PCA of place cells. So, place cells are at the input. The weights are
learned through heavy learning and a non-negativity constraint. And this network does PCU on the
inputs, and the outputs end up converging to grid-like fields, again, suggested that grid
cells might be a low-dimensional representation of place cells.
Another set of work suggests that inputs from border cells to grid cells could be used for
error-correcting grid cells. So, here I'm showing a one-day schematic just to make my point. So,
this is a rodent at a specific location in space, and this is the grid activity profile
that represents that location. And when the rodent explores the environment and comes back to this
location, the representation of this location has drifted with respect to the original,
and there's some error in the representation. And if the border cell activations are provided
as input to grid cells, then they activate the current subset of neurons doing error correction
and pulling back the representation to the original representation. And this is what this
looks like in 2D. So, in 2D, if you do not have any border cell inputs, then your grid cell
representations are not very stable, but with border cell inputs, your grid cell representations
are fairly stable. So, I also want to point out the fact that place cells are thought to store
neighborhood relationships in their reference synapses, and therefore they could implement
a topological navigation strategy. And many models of place cell-based navigation have
actually emphasized this view. So, they've suggested that recurrence synapses encode either
spatial or temporal connectivity, as suggested by Blum and Abbott, or they encode transition
probability, as suggested by the successor representation work. So, given all these insights,
our goal is to build a comprehensive neural circuit model of premapping. And we start with these
two questions. Does high-capacity grid code, when projected to place cells, also lead to high
capacity? And given these conjunctive representations between the location input and the sensory input,
can we learn neighborhood relationships between place codes? And before I go into the details
of capacity, I just want to point out that traditionally, Hopfield networks have been
used for storing memories and patterns. And it has been observed that the maximum patterns
that these networks can store is n, where n is the total number of neurons in the network.
And modern Hopfield networks, also known as dense associative memories, have an exponential
capacity, but they use many body interaction terms, which are not biologically plausible.
And in our model, we stick to using two interaction terms in the weight computations,
and we still get exponential capacities. So, this is the architecture of our model.
The model has different grid modules, which have different scales or periods,
and the binary grid code is projected to place code randomly. And the back projections from
place cells to grid cells are learned through associative Hebbian learning. And we observe
that when we perturb these place cells with a noisy version of place code representations,
then the network is able to successfully reconstruct all the patterns it's trained on.
So, the network is fairly robust to noise. Furthermore, this network has exponential
capacity that grows much faster than a non-modular network where the grid cells are non-modular.
Also, the network generalizes stored inputs to create stable attractor
states around every pattern in the grid coding space, despite training only over a vanishing
fraction of contiguous grid coding space. So, for instance, if my grid coding space has around
10,000 patterns, I can train the network on only around 200 patterns, first 200 patterns,
and the network is still able to robustly reconstruct all the 10,000 patterns in the grid
coding space, which is pretty striking. Furthermore, next we add these heterosciitiative
learning on the recurrent connections on place cells to see if they can encode neighborhood
relationships or 1D sequences. And what we find is that the product of these weights converges to
this transition matrix, which is actually the analytical matrix, an analytical transition
matrix that relates contiguous grid codes in the grid coding space. Furthermore,
this network actually has perfect sequence recall given enough number of place cells to
approximate this transition matrix. And again, training on only a subset of the sequence is
enough to recall the entire sequence. So, for instance, if I train, if I have a sequence of
length 500, and I train the network on only first 150 patterns in the sequence, the network
is robustly able to reconstruct all 500 patterns in the sequence without having seen all of them
before. So, the next step in this network is to introduce sensory input. And the sensory input
would project randomly to place cells and back projections from place cells to sensory input
would be learned through associative hybrid learning. And here grid cells would form a basis,
and hippocampal places would link that basis with arbitrary sensory input.
And this combination of structured inputs and unstructured inputs could potentially enable
the storage and robust recollection of a large number of arbitrary sensory patterns from this
partial use. So, how does this connect to map fragmentation? So, in part two, we talked about
map fragmentation based on the notion of contiguous regions. And here I'm positing that
when you transition between different contiguous regions that actually corresponds to a large
contextual change, which when provided as input to this network would trigger remapping in place
cells, which would in turn cause remapping in grid cells, thus leading to the formation of local
sub maps. And how does this connect to part one, where we saw that sub maps might enable quick
learning and generalization in humans. So, this network actually enables us to anchor grid maps
to external cues through these conjunctive representations in place cells. And this anchoring
to external cues actually enables the alignment of grid maps, even when points of departure in an
environment are different, leading to adaptable representations. Furthermore, if you are in an
environment that is composed of previously seen spatial structures, then this anchoring still
enables you to pull out the right map when you're navigating through that composed spatial structure.
So, to summarize, our global hypothesis was that cognitive map is organized as a globally
topological and locally metric or Euclidean map. So, this is one illustration of a topometric map.
And we said that any sub part of this map can be termed as a sub map.
In part one, we suggested that learning sub maps that are adaptable and compositional may drive
fast learning in complex spaces. And we proposed to conduct human behavioral
experiments to test this hypothesis. In second part, we proposed an algorithmic model of map
fragmentation into sub maps based on the notion of contiguous regions. And in the third part,
we proposed a framework for a neural circuit model of map fragmentation. And overall,
we're suggesting that sub maps enable humans to build up a knowledge base of spatial structures
that they can continuously enrich and refine throughout their life by combining their existing
spatial knowledge with their new experiences. So, that's all for my talk. And before I end,
I want to acknowledge my collaborators again, Marta, Kevin, Merko and Sata. And I also want to
thank my supervisors, Ila and Josh for their continued feedback and support and discussions
on these projects. I also want to thank Matt Wilson for his feedback and valuable suggestions
during my committee meetings and the members of FeedLab and TenBomb in general for their support.
And before I end, I also want to thank the VCS department and McGowan Institute for their
continued support and for providing an environment that's conducive to research,
even in the face of this pandemic. Thank you. And I can take any questions now.
Thanks, Sue, for the great talk. We already have a question from Marta Griffin.
Great talk. I have a question about partitioning a space to sub maps.
What do you think will happen if you ask humans to intuitively partition an environment to regions?
How would there be segmentation compared to the maps given by your similarity measure?
So, you mean the complex? Anyways, so, okay, so the question is that if humans intuitively
try to segment a map, how would that compare to the segmentations which we have proposed
in part two of my talk where we have given an algorithmic model for map segmentation.
So, basically, the way I view it is that in the algorithmic model of map segmentation,
I basically took one metric map and one small portion of the map. And we said that that can be
decomposed into various fragments of sub maps. But humans are actually able to reason in much
richer environments than the ones which we saw in this algorithm. And so, there we can
view it as something like this where you might have even small spaces being segmented into
multiple maps. So, let me just see if I can, yeah, so even if you have something like this,
so even within this, so I was calling this a sub map, but even within this sub map,
we might have multiple maps. So, based on the notion of contiguous regions, you might have
a local map here, a local map here, and all these maps might be connected.
And so, the algorithmic model would predict those kinds of map segmentations, but then even
a combination of those map segmentations can be termed as a sub map. And then that is what I
was talking about in part one where I'm talking about composing the sub map which is even itself
composed of smaller maps. And we can take this representation of this map and then compose
them in various ways to build richer environments.
I'll follow up from Marta. I'm curious if the algorithm can predict how humans would interpret
sub maps. How humans would interpret?
Well, it's a little bit unclear to me what you mean by interpret
sub maps, but I mean, I'm assuming that you're suggesting, you know, how humans would interpret
segmenting this environment versus maybe a more complex environment. And I would say that
currently, this algorithm doesn't predict anything about how humans might interpret
grid maps. But that's, I mean, we could look at it. That's an interesting direction and we could
look at it in the future. And I think some of my work in part one would potentially address that
going forward. All right. Next question from Eli Pollock. You can unmute yourself, Eli.
Yeah. Hi. Can you hear me? Yes.
Hello. Yes, I can hear you. Sorry. Okay. Okay. Cool. Yes. Sorry. My internet's a little weird.
Great talk. Can you talk a little bit more about how your model handles time?
Or I think you mentioned that it was able to handle like replay of different sequences of
states through some map. How would it be able to handle different trajectories through the same
space that might activate play cells in different sequences? So the version of the model that I
presented, that is trained on discrete patterns, right? So the sequences here, the sequences which
are trained on here through heterospecific learning are composed of these discrete patterns. So if I
say the sequences of length 500, there are 500 discrete patterns in that sequence. And when you
say time, time is something continuous. And so we have worked on extrapolating this model to
more continuous domains. And in a continuous domain, what would happen is that your sequence,
instead of being discrete patterns, would be composed of these continuous stream. And in that
case, we haven't explored what the network performance would be, but that's something
ongoing. And definitely in 2D spaces, we would like to train this model on 2D sequences and then
see, so right now, these results are pertaining to 1D sequences. But we do want to extrapolate
the results and train this model on 2D sequences to see how it performs in 2D. And I think that would
potentially address then your questions about time, because that pertains to continuous domains.
Okay, yeah, thank you.
Okay, we've got a big stream of questions coming in. The first is from Adam Eisen.
Thanks for a fascinating talk. Have you thought at all about how this framework for sub-map
segmentation and topological association could be extended to non-spatial domains?
That's a very good question. So in FeedLab, a small group of us have been thinking about how
this could be extended. And so the idea is, I'm just going to go back to, let's see if I can just
quickly hop back to my theoretical framework slide. So really, we're building up on this
theoretical framework. And when we're thinking about non-spatial domains, we go back to this
schematic picture. And here, basically, what we're saying is, within the spatial domain, I said that
large changes in contextual input can drive remapping in place cells, which would drive
remapping in the grid coding space. So similarly, in relational domains or in discrete domains,
as we're thinking about it, we are referring back to this schematic picture. And we think that
the phenomenon of remapping actually would enable us to encode these non-eclidean relationships.
So first thing is that here, we're suggesting topological relationships. So that already
enables us to encode non-eclidean relationships. But this phenomenon of remapping also allows
us to make jumps. So if you think about a family tree, if you might try to encode it in an
eclidean space, then you'll have conflicts. And it's very difficult to encode it. In fact,
almost impossible to encode it in an eclidean space. But then if we provide a framework where
we allow topological representations and we allow jumps in this eclidean space through this phenomenon
of remapping through place cells, then that can allow us to potentially encode non-eclidean
relationships. And that's the way we are thinking. And this is still work in progress. And we
actively think about it. Next question from Nancy Camusher. How does your system decide when and
where to carve the world in the sub-maps, especially in non-built environments where the
divisions may be less obvious? So at least in part two, where I talk about fragmentation of
maps into sub-maps. Let me just go back. So in these kinds of environments, we basically describe
the principle of contiguous regions, where we have suggested that when you are navigating through
this environment, if you are in an environment where your visual observations stay more or less
consistent, then you will keep extending a map. But when you go from this region to another region,
let's say you navigate from here to here, where your visual information here is completely distinct
from your visual information here, then in that case, you will decide to segment the space. And
that's what we are predicting. We're predicting that in this case, when you travel from one
contiguous region to another, you will segment the space. And that's how you divide the space.
So that's one principle, which we are describing for map segmentation. And that might not be the
only principle, but that principle explains some of the experimental results that have been
observed newly. And there might be another principle, for instance, path integration error,
where if your path integration builds up and it reaches a certain threshold amount,
then you automatically might start a new map. But I haven't illustrated that here, but that's
also one of the other driving forces for us to actually split a space into multiple sub-maps
so that we can have more efficient representations. Okay, next question from Chen.
I was wondering if you can give some intuition about the mechanism for composition of
sub-maps. So really, this is from the happy and learning mechanism. And secondly,
can you comment on the relationship between this and the Hopfield mechanism?
So since we're talking about composition of sub-maps here, in this case, in this model,
what I'm suggesting is that we might end up forming local sub-maps by using the
phenomenon of premapping. So here I'm suggesting, again, if you have large changes in contextual
input, so for instance, let me just go back to the slide, which kind of connects this.
So here we basically suggested how this environment can be segmented into different
sub-maps. And on the mechanistic level, I'm trying to suggest that any large contextual input
is going to cause remapping in place cells, which means the cells which are firing would be
changed and they would represent a new map. And this remapping would actually enable also
remapping in grid cells through associative learning because each place cell representation
is associated with a certain grid cell representation. So when you have changes in
firing fields of place cells, that also triggers certain corresponding changes in the grid coding
phase, which is basically called remapping in grid phase. And so here I'm interpreting,
when I look at my algorithmic model and I look at my mechanistic model, I'm saying that when you
go from one contiguous region to another, that actually corresponds to a large contextual change
and that contextual change triggers remapping in this model and this remapping in place cells
then triggers remapping in grid cells phase. And that corresponds to the formation of local
sub-maps. And these topological connections from place cells to themselves might actually
represent how these local sub-maps are connected to each other. And there's also,
going back to the anatomy, there's also a possibility of splitting this population
actually into different populations. One that might just encode conjunctive representations and
another one that might have these recurrent topological connections similar to CA1 and
CA3 distinction in anatomy. But that's how I'm thinking about it at the moment.
I want to point out a quick addendum from Ila who said to Nancy's question, we consider online
segmentation decisions driven by regions or points of high surprise affordance changes and PIR.
Okay, another question from Anya, even over. Do you think the way humans form space maps depends
on whether their language uses directions that are egocentric left and right or allocentric
north and south? That's a great question. So there were experiments in both animals as well
as humans which have shown that we actually have both kinds of representations. We have
egocentric representations and we also have allocentric representations. And the representations
in the hippocampal inter-animal system are usually allocentric, but the thalamus is the part of the
brain which is responsible for egocentric representations. So really, when we're navigating
spaces, we are probably using mixed strategies. We do use our allocentric representations,
but in some cases we might be using egocentric representations. And that's mostly also true
for routes which we are traversing very frequently. So if there's a route which I take every day,
let's say going from my home to BCS, then that converges to root learning. But if I've had very
less experience in any environment, then I'm mostly using allocentric representations to
actually make my way through that environment. And hence, allocentric representations are
actually attributed to being able to make novel inferences. And when I'm building circuits here
using grid cells and place cells, I'm mostly talking about allocentric representations.
But then when I was talking about submaps in human experiments, I'm kind of
agnostic to the fact that whether those representations are egocentric or allocentric
because even if you learn an egocentric strategy, even in that case, if you determine that there's
a repeating structure to the environment and you recall that this is the same environment that I've
seen before, then you can still apply the same egocentric strategy or an allocentric strategy
depending on which one you decide to use at that point. But going back to the main question,
the experimental data shows that we're actually basically learning both kinds of strategies.
Okay, Senke's Pellevan says, great talk. What are the large scale, sorry, where are the large
scale topological relations coded in the network? So in the network model, our focus, so in this
case, we are basically encoding topological relationships using the recurrent connections
on the place cells. But for large scale topological relationships, what we are moving towards is
splitting this population into two distinct populations. So one population that will only
have conjunctive representations, and perhaps another population which will have the recurrent
connections similar to the distinction between CA1 and CA3 in the brain. And it's true that in this
network model, we don't really distinguish between small scale and large scale topological
relationships. But by introducing this additional population, we hope to abstract away the large
scale relationships. Also, even in this case, if we have minute relationships between spaces,
then we can also extract away the large scale relationships. But again, adding another population
might make that easier to make that abstraction.
Okay. Nancy has another comment. There must be some way to represent globally metric information.
For example, I know the approximate distance and angle from my current position to far away
locations in my world. Do I understand right that you would say this information has to be
pieced together from just the topological relationship between the sub maps that connect
those locations? So that's a great question. I also sometimes wonder because like when I'm here,
I can kind of look very far in space and be able to tell that, okay, I need to go in this direction.
And that's definitely metric information. And so going back to the labeled graph hypothesis,
which was suggested by Bill Warren. So even in this hypothesis, I'm just saying that this is
we are learning metric representations here, which are consistent Euclidean representations,
which means that they actually obey all the postulates of Euclidean geometry.
And the connections between them still have metric information. There's still information
about angles and directions, but they might not obey all postulates of Euclidean geometry. So for
instance, you might have some inconsistent representations. You might not know exactly
what distances and displacements you have to go in order to reach a goal, but you know approximately
which distance and which angle. So even the topometric representation is not contradicting
the fact that you might have approximate distances and angles. But what it's saying is that you have
very accurate metric information about small pieces of space, but then for faraway pieces of
space, you might still have approximate distances, angles, which help you navigate.
Okay, another question from Sean Chen. Sorry for my pronunciations. Great talk.
Could you elaborate the mechanism of exponential number of memory capacity in your model?
So here's the, here's the network, right, where I'm showing that the network has exponential
capacity. So basically in this network, we have different grid modules which have different periods
and this is the grid code is actually binary. And we project this grid code randomly using
random weight matrix to place cells. And the number of place cells in this case is much larger than
the number of grid cells. And now we learn these back projections from places to grid cells using
Hebbian learning. So it's just simple Hebbian learning. I don't have the equation. Basically,
this is the equation, right? You can consider these as place cell patterns. And this is how we
compute the weight matrix for the weights going back from place cells to grid cells. And once we've
done that, then the way we test for memory is basically by perturbing the place code patterns.
So we put up the place cells by a noisy version of the place code pattern, right? So in this graph,
I'm showing that if you perturb them with 20% noise, then with 300 place cells, you get an output
noise of zero, which means you get perfect reconstruction of the patterns that you're
perturbing the network with. And that's how I'm defining exponential memory, right? And
when I say exponential memory, what I'm suggesting is that you can, as you increase the number of
cells in the network, the number of patterns that you can reconstruct from the noisy versions
actually grows exponentially.
Okay, I'd like to ask a question as well. I'm curious to hear your thoughts on sort of
computational level descriptions here. So it seems like your model is largely
tuned towards sort of prediction, accurate prediction of your local spatial environment
and motive mechanisms for doing this well. And I wonder what you think about computational level
accounts of this to be more, to ask a more concrete question. Do you think there's a role
of reward or something other than accurate, just accurate prediction in the shaping of
neural representations of space? Okay, so you're asking whether there is a role for reward
in shaping the representations of space in the mechanistic model?
I mean, more generally, what's the objective driving this algorithm? Is it about
accurate prediction or is there something more than prediction?
Okay, so you're asking what is kind of like the motivation for building the mechanistic model?
Yeah, yeah, I mean, some of the graphs you're showing here about,
some of the graphs you showed, for example, were about recall. And recall, accurate reconstruction
of an environment might be one objective. But it seems like you could say that this is a
mechanism evolved for some purpose, but it's just that or different than that.
So, right, so these two questions, which I started with, which are related to capacity of the network,
and whether we can do, whether we can learn sequences, these are basically questions which I
started with, because I wanted to explore the theoretical properties of these circuits, right,
given the coding properties that we know of grid cells and place cells and given
biologically plausible learning tools through have been learning, what are the coding properties
that these networks can exhibit, right, how much capacity they have, how much information can they
store. And so, yes, even in these models, since we are talking about reconstruction,
basically says that if you have some noise, if you have some noisy estimate of where you are in
space, then this kind of a network can help you clean that estimate, right, and reach a cleaner
version of where you might be in space and help you localize in space. And also, these temporal
connectivity on the place cells kind of have a predictive component to them, right, because you
can import sequences. So, when you are at a particular position, you might be able to predict
what's coming next. And so, these are kind of small components, which I'm using to go towards
building a model of remapping in space, right, because ultimately my goal is to be able to
figure out how these neural circuits might lead to the formation of these sub-maps. And by knowing
the properties of these circuits, then I'll be better able to construct and build those networks,
which actually can build small local sub-maps of environments.
I'm reading out a comment from Ila. To my question, you could view this as a structure
learning even without reward. A reinforcement learner, including the brain, could use these
learning structures. Cool. Well, I think we're actually over time now. That was a great talk,
Sue. Thanks for sharing with us. Of course, thank you. So, we'll be back next week for another
talk from Eli Pollock. Until then, have a good Thanksgiving, and I'll see you next week.

Hello boys and girls. In this video I want to talk about the line of research that I
only came across last week, namely the very strong connection between quantum fields and
neural network theory. And what emerges from that is tools from one field being applicable
to the other. So you can then use let's say Feynman diagrams to find stuff out about
neural networks. And in the other way around you can use neural networks to compute stuff
for quantum fields. I think the most senior researcher, if I'm not mistaken from this
list of this paper in particular is James Harvorsson. And if you look it up there have
been a few papers in this direction in the last years. And nonetheless I chose this particular
paper because it's clearly some of the more advanced results there. And to motivate how
things work, I'm also in this video going to explain some results which were known for longer
time. So there I suppose it's fair to say that this sort of stochastic result for large neural
networks that we are going to discuss emerge in the mid 90s. And I'm going to explain this because
I want these subjects which I find very exciting actually to be known also a little better. In
this video however I will not go into any deep mathematical analysis. I have also not written
down much. I will basically jump from tab to tab. And nonetheless give a sensible explanation of
these things. So I think everything that I say should make sense in principle. And then you can
delve into all the subjects on your own. If you're actually interested in doing something in this
direction let me know in the comments. I will also point to some Google libraries regarding neural
kernel theory that I will also sketch out in this video. And yeah so that's that.
The requirements for the video are that you have like a basic understanding of deep neural networks
like the fact that these neural networks encode the parameterization of certain functions. And then
if you have a big enough network stuff like universal approximation theorem the fact that
you basically can represent functions in the space let's say continuous functions densely.
And it helps if you have a basic understanding about the ideas of quantum mechanics. So the
fact that what you're interested in is transition probabilities and that they are expressed as some
products in a Hilbert space. In this video I have a section where I motivate the jump from the
expressions that we are going to be able to compute with neural networks as they are explained in
this paper. Let me scroll down a second. How these expressions connect to scattering amplitudes in
let's say some large hydrogen collider experiments. These sort of stuff so I have a small like wake
mafia section that concerns physics. But my main goal is that if you go away from this video
and have a vague understanding why these sort of expressions that you see on the screen right now
are both relevant for neural network theory and for quantum field theory then my job is done.
As I said this gives the possibility not just to compute stuff in physics but for example
you might then be able to apply Feynman diagram calculus to compute various aspects of neural
networks as well and so even if you edit from a purely computer science perspective
and have some statistics background then this might be fairly helpful.
Okay so I have here just a bunch of bullet points that I want to go through. I might come back to
this from time to time otherwise I will just jump through like here 15 tabs or so and explain some
results. I am actually currently working on or started working on a video that I maybe want to
make as my some for summer video where I will do like a painful analysis of the universal
approximation theorem. But that's like months and months out. In this video I'm basically just
rambling. I hope you don't expect to neither deep or concise elaboration so I'm warning you already
but nonetheless I would really recommend that you listen up. Okay so first off as you have just
seen in the bullet points I will explain to you like sketch out this result from the 90s which
concerns neural network Gaussian processes. So there are nice results that have been found there
and have since been extended to a broad range of neural network architectures. So this is
mathematical theory results for neural networks that hold in general. In this video for simplicity
we can just look at fully connected in the sense that you see in the screen here feed
forward neural networks and for this video it's not even super relevant how many inputs
outputs you have. Basically you have let's say at least one input some float or if you want a real
number x that goes in one real number y that goes out and in the middle you have a bunch of
hidden layers in the image you just see one but you know for the sake of it you might think of two
three four something like that and you see the nodes in this one hidden layer here and the
theory that we are going to discuss kicks in once you have a really big network. So this
you think of the number of nodes in each hidden layer here going to infinity or you know it will
suffice if you think of a huge number a bunch of billions of billions and the thing that then
emerges with large networks is not just the universal approximation theorem that says
the the nice functions let's say continuous functions from r to r are represented densely
by this sort of neural network by these weights but what also emerges in this large network limit
is that the dependency of the output for fixed input and probabilistic weights
takes on a very deterministic character okay so this is this neural network Gaussian process
phenomenon and then I will explain in a second in more detail but basically what we want to
hear first look at is we take one fixed architecture some big neural network with let's say three
hidden layers and all the layers are very large and what we're here are first concerned with
before we talk about tangent kernels before we talk about quantum field theory is we few
we're concerned with the random initialization of these networks right so let's say you are on a
computer you you have this network encoded on your GPU or whatever and or then the weights really
I mean the you have the architecture laid out the way in which all the the the float data
pass to each other naturally if you have this this float types in every realized configuration
the weights have to have some some value and this gives the start configuration for the
learning process right in the learning process you're going to probably assign some some
loss function and you do gradient descent and then you tweak the network to behave in a certain
nice way and fulfill some task but to start this process you need to initialize the network you
want to maybe explicitly set some weights okay and now what you can do is you can play around with
what is actually your starting condition right you can say hey shoot all the weights in the beginning
be set to zero or be set to one or and this is the interesting thing here you do a random
initialization of all the weights and biases also so think of you know you're in python you
take a library and you sample from a normal distribution for all the
trillion weights you sample trillion random numbers and you sample them each from a Gaussian
from a bell curve and then set the corresponding weights like this okay so all these w i j are
sampled from some from some Gaussian and when you like put in some input right we said there's one
input let's say you you take the input seven and set x to seven and then feed do the feed forward
process the evaluation of the neural network then if these things are random then the output will
also be some essentially random number it will be determined for whatever random weight you have
sampled here and in this way you can think of y for fixed x as a random variable composed of the
random variables w right so we have here's the neural network Gaussian process page the math
and the proof sketch of this result that we are going to get to is actually described there so the
weights are sampled from some Gaussian we are going to take a Gaussian where the standard deviation
gets tighter and tighter with the number of layers right but if you have a fixed network this is some
fixed variance here and the output that is in a standard way computed from neural network is
um computed as you see here you do fast forward and I think it's fairly easy to believe that just
due to the central limit theorem right the statement that if you have a bunch of independent
random variables if you sum them all up then this is another random variable that will behave
like a Gaussian process right so basically if you sum up random numbers then you usually end up with
if all the conditions are fulfilled all the mathematical conditions then you will end up
with a Gaussian this is the statement of the central central limit theorem and this exact
thing applies here also you know maybe there's some non-linearities involved and maybe there's
different steps but in the end the final output of the network here in this picture in the on the
last layer this set is still a function of all these small Gaussians and because there are so many
sums this is again just a Gaussian process right so and so this says that in the limit and this
is especially emerges if you have enough width if the width is big enough so that the central
limit theorem really kicks in but this basically means that the as a random field as a random
variable the output of the neural network has very nice stochastic process properties and it's
it's a Gaussian process in particular one of the nicest you can have basically here on this
web page on this Wikipedia page there's also like this this this example animation so here they have
some network with three inputs right as I said it doesn't have to be three it suffices to think of
one and they have a bunch of outputs again it suffices to think of one so one green input one
yellow output and a bunch of nodes in in bunch of layers in this case two layers one layer would
also work you see on the right side I mean you probably don't see just because of my face here
but I mean doesn't really matter too much it's just a bunch of like random distribution the
statement is that for fixed input the green value again let's say there's one green input and it's
set to this the float number seven if you fix if you go up with the the number of nodes and
random initialize this with weights and biases then just by the central limit theorem which is
dependent on this this seven and this bunch of random numbers the output y1 here this yellow
output will behave like a Gaussian just by the central limit theorem and in this case there's
two outputs so you can draw a plot and the statement is then that both y1 and y2 behaving
like Gaussians independently from another like it's not a statistic statement but each behave
like a like a Gaussian they have some peak and so on the plot you get another nice Gaussian with
some peak here and so if then the press play again if the network becomes even bigger then you get
like this perfect Gaussian where it this just says that it has this maximum expected value here in
the middle and this goes for all the outputs right so this is the result that that that
the okay I closed the neural network Gaussian process page but doesn't matter this is the result
that just because of statistics you the network if it's large enough at random initialization
behaves like a Gaussian we will not need it for the this video but if you want to take a look
this is the the formal definition of a Gaussian Gaussian process I mean to motivate this basically
you think you know a Gaussian is something which if you do the Fourier transform it's again like
a Gaussian and the Gaussian process is abstractly defined as this random variable or sequence of
random variables where the characteristic function the expectation value of this phase
is this Gaussian with a certain mean we are not going to need this the Fourier transform will
pop up again when I talk about the quantum field theories but suffice to say the nice thing is
that the neural network the big neural networks behave like Gaussian processes sorry if I repeat
myself okay so do we do physics first or do we do neural tangent kernels first
um let me actually um yeah let me actually say something about
the neural tangent kernel so um could we know now that the the network at the start behaves
like this Gaussian for all inputs and if you do the learning process then this is about
um giving it a test data and then moving um in parameter space from wherever we random started
to some other position in in weight space and I have made a bunch of videos on gradient descent
I will not explain it here but suffice to say you have a space of weights and then the the weights
follow some path and you do that in a way that optimizes some goal that you have right some
task for the neural network and I think I sketched it out here so as is common we're dealing with
not only here with a large neural network so that the the theory becomes simpler and nicer
but also we are matching we have so much compute that we can do really small step sizes
so that we can then in the limit talk of the behavior of the network as in a differentiable
way where we say the the the motion of the in path space in a parameter space can be described
you know with literally just calculus differentials and so the gradient descent algorithm what we are
doing really is um you know as per instruction of the algorithm we say the change of the weights
and here I abbreviate all the weights together with this uh theta the the change in the weights
right from from one point to the next in the graphic that you just saw um is chosen in a way
that it takes the negative direction of the gradient of some cost function and the cost
function in here is the you know the the difference essentially between what the neural network
currently says versus where we want to get at where set is all the learning data that we have
available right so we say for all the learning data that we that we have um there is a discrepancy
between what the network f currently um says um what's correct and what is actually correct
why I said here is what's actually correct um and we send that up and so this is like the the this
cost of all learning data together and at every step in time in the learning process
we go follow this path right and this equation is really just the Newtonian equation um where
you know in in physics um theta would be the momentum
and where you um look at the situation where the force on the right hand side is governed by a
potential and you'll say um the the direction of motion um captured by the momentum is given
wherever the you know potential energy will be lowest and that's where the path followed by
the particle in Newtonian physics right so this already looks very um like like this simple physics
uh equation governing governing the motion in weight space and now given that you have
the behavior of the um the the particle if you will uh going through weight space like
and give you just saw um and the the potential depends on the outputs at the network on all
these spaces you can also then um do the calculus and and look at hey how does the output of the
network which depends on the position where you are at right where the weights determine what the
network output will be on all the um learning data how does that the f change and so if you
do them just do the math um and I think I have this is here so this is um newer tangent kernel
theory um if you
if you do this sort of calculation and if you uh you know if you ever started physics you have to
this this sort of calculation a million times because basically if you have some observable
in a physical system and you know all the the constituents of the particles behave in this
isn't this way and then I have some observable which is made out of particles and you say how
does this this observable quantity change then um you have to plug in a bunch of partial derivatives
and then the Hamiltonian comes in and whatever and so on and so forth what comes out of this
is that the development of the output of the neural network um is governed by some matrix
this is the so-called newer tangent kernel and the changes of uh the the loss in uh with respect
to the to the weights right so I mean I did not adopt this terminology theta is again all the weights
together and um you can do this calculation on one sheet of paper yourself I'm not going to discuss
all the the convention or a notation chosen here but the point is that the evolution of the output
on a network from your starting point which might be a random starting point is understood at least
here in theory it's another question of whether you can actually calculate that because this matrix
which determines how this the output of the network evolves as you do the learning according
to gradient descent is determined by this complicated object theta and the theta is basically
this so-called kernel um this is you can view this as the inner product of the gradient of the
output with research with respect to the weight change and so the interpretation is basically
that um you look at uh different inputs and then you um you as a kernel a kernel roughly
charges how similar input data are and this kernel basically looks at uh hey these two
input data are similar if upon a change of the weights the um the response of the network changes
in a similar fashion and you compute this it's in a product in any case this is like an interesting
object that in the end determines how the network behaves um and similar to neural network Gaussian
process theory where you say once I have enough um uh weights in my layer some nice theory emerges
right in in the Gaussian network case it goes towards a Gaussian it's also the case that
for a large network then these these matrix can simplify and then you can get the infinite
size network also to an analytical theory and basically you random initialize you already
know it's some Gaussian process and then you have some matrix which determines how the network moves
um through through weight space and thereby you you have an idea of actually what happens
during network to network training right so if you have never heard that and more or less followed
my explanation then you can see this is kind of cool that at least in this limits you have
sort of an analytical idea what happens during learning and then the question is to what extent
is this sort of logic valid for networks as we can implement them at the moment at the moment
because of course we have a lot of weights like billions of weights but it's not infinite so
you might ask to what extent is the analytical theory where these limits are taken right so
super small step size very large networks applicable to today's convolutional deep neural
networks and so on and so forth and this is basically a subject of study so this is something
where people still put a lot of time in and so for example you have here this google uh neural
tangents project which i might be interested in looking at and there's a bunch of google researchers
who are still using this and publishing papers in this and and so on and so forth there's also
i think a recent um new rips uh poster from 2019 where you get some of the examples of
analytical formulas that i talked about um just because i want to get to the quantum
field theory part i will not discuss this in detail but i um i hope my tangent pun intended
was interesting and as i said i would also actually like to to work a little bit in
this direction myself so if you want to look into that um feel free to reach out and we can
do some sort of project together okay so um now for the the the field theory part
but still extremely important for us is this neural network Gaussian process inside okay
and if i'm here in the paper on page four
then um let's make first some definitions okay so here we have these correlation functions
which we call g n uh for uh for a concreteness sake you can think of n uh let's say as two
so um we are going to actually look at um two different um forward passes for the neural network
right so you have two different imports that you want to try x and you plug them in and you
get something out of your current network which might be randomly initialized um and if you
as we had it with the neural network Gaussian process case if you your weights as a random
variable right over all the weights over all your trillion weights you put a little Gaussian and let
them wiggle a little bit um and you say what typically happens if i sample once and um
put into uh inputs x uh how are the inputs on a on a typical or random network correlated with
each other then you can you can try this a bunch of times and get an idea um what you can also do is
analytically if you know the distribution of where your weights
compute what what will come out right so you have here uh p over the weights this is the
distribution that you yourself chose from which you can sample and uh for fixed neural network
architecture um the weights and um the neural network um which um is here called phi this is the
function that depends on the weights depending on your architecture right so this is the sigmoids
and this sum and so on and so forth and so the correlation function gives you how this how
let's say two inputs are um correlated with each other for this network right and by the neural
network Gaussian process result if we said that uh this this billion uh probability distribution over
the weights um make the input output relation of the whole network also into a random variable
right so you can also view uh this the setup that you have here not as um as um
not just a sampling um the weights and getting uh then a fixed input output but you can also
view any sample process as sampling a whole neural network right even this is just what
you do if you sample if you random initialize for fixed architecture um the um
certain uh functions as your certain neural network then you've also sampled the neural
network from who knows what distribution right so there's also this different the different
view of this initialization process and then um by the result of the neural network Gaussian process
what you have is that you can also view the same uh exact object this correlation function or any
expectation value really um as in terms of distribution over the these functions themselves
and by the result that we just had for a large network this will be a Gaussian process and this
manifests in this way right so um here is the integral not over the weights but over the um the
uh whole function that the network represents itself and um the the um probability distribution
that you have in this case is um of of this sort where this s uh and now this relates back to the
formal definition of the Gaussian process is some quadratic let's say let me fill some local terms
cost associated with the whole um with the whole function so basically what what this does I mean
do they give here some examples I think they do um
okay so this is already sort of a physical example but what you have as s here in the the exponent
is some sum over all uh the values of the network and what what they like what in effect happens is
that um the um the the this this weight is such that um field configurations and when I say field
now I always just mean the same as the input output relation not given by the neural network
field configurations or neural networks where at one uh places you get a huge output these are just
exponentially suppressed because um if you random sample from the with the weights then you are not
likely to get um some basically you are going to get um neural networks in initializations
which are around some some uh some certain um typical expectation value like they have there are
some typical behaviors and everything that deviates uh a lot from it is like exponentially less
likely right you um if you do um thousand random initialization of the neural network you will
get some typical behavior and then you can cook up some other extreme behavior that is not likely
to happen and um by the result of the neural network Gaussian process um theory um it tells you
what the the the probability distribution looks for the neural network so there is this connection
that you have here right and as I will motivate um later uh in quantum field theory this is exactly
sort of the setup for the path integral and what this paper does is it um if you view um
the um this this sort of um mathematical um overlap um as a physicist and you want to use
neural networks as as as a physicist you you see this as a way to um then try to craft neural
network architecture and sampling techniques right the the way in which you sample the weights
in a way so that the this this whole process of random initialization corresponds to certain
s s certain actions here right you do you have some physical scenario in mind there's physical
theory that says oh you know uh certain scalar field theories have this and this action what is
the way I and in which I must set up a network and the way in which I must sample the weights
so that what I sample is exactly as if I would sample from a quantum field
like like as if I would sample from uh would I would sample a field which is one instance of
um a quantum field in the path integral formalism right and then I get out a bunch of correlation
functions this endpoint functions um and these are exactly the things that are uh what what you
do quantum field theory for right you you compute these g's and then I will explain it later then
there's some mathematical connections to how you get from this this this g's to um the scattering
amplitude or whatever your quantum field theory does um maybe particle physics solid state physics
and so on and so forth okay uh let me see so as you might notice this is a very free flow
sort of explanation right um so I have to check uh what I have touched upon and what I didn't
okay
yeah okay so um from the very complicated quantum field theory math you get some
you know relations of um how the correlation function must relate to these actions and there's
a bunch of stochastic differential equation mathematics involved and because in physics
the evolution is always governed by some Hamiltonian operator function there's a bunch of energy
terms that you have to kick around and that's why for example um these objects tend to look
a little bit like this um like if you're never studied this physics um maybe one way in which
you should look at this is that because the evolution of these fields like how they um
have often time is governed by the Schrodinger equation which relates the time derivatives
like the evolution of the fields themselves to some energy expression captured in the Hamiltonians
and the the energy kinetic energy and in particular for fields is given by some spatial operators
that's that's why these sort of objects pop up here and you know mass energy equivalents
that's why we also have mass and so if you see these Laplacian operators or mass terms um you
should not be too surprised uh there because in physics they just always pop up in relation to
the time evolution of the the fields okay so okay now I've already touched upon the concept of time
the thing is of course that um here this the fields as they pop up here will um be
not uh like what you get there is the better controlled theory of euclidean fields right
you're not you're not having to do a priori with spacetime metrics and all these things which make
field theory quantum field field extra complicated but um just talking about Gaussian processes is
just talking about stochastics and then there is this sort of bridge that you have to take
and hope that you can get from the the euclidean field theories to um to some actual quantum field
and I will just name drop um a bunch of concepts there the idea is that you uh if you approach
quantum field theory with this neural network she bang then um you want to find a neural network
which mimics the weak rotated version uh of um of physical quantum field and so there is um
a bunch of uh so-called constructive quantum field theory coming in so there are various approaches
for of people trying to um uh make certain aspects of quantum field theory more rigorous
and um transfer like get rid of uh pseudo metrics in quantum field theory move everything um to the
euclidean domain where you have nice metrics and um uh so in this paper for example they say that um
you know what we really want to impose is um neural networks which when you view them as a field
behave in a certain way and something that is important there for example are these uh
Osterwald Schrader relations so for example here in this case the um correlation functions um
these particular coordination functions of relevance here are called denoted s and then you see on
the screen a bunch of properties that these shall have right so there you have the physical
translation in variances of certain objects and certain symmetry or independence relations right
i'm just mentioning this that there is uh it's not like um you just take any neural network
and then you get some uh some quantum field theory in the path into word formalisms out of it
there's a fairly restricted subset of uh fields that the physicists for quantum field theory might
be interested in um okay but i should probably not go into too much detail on that uh here
um yeah uh also the um these objects in this um exponential so sorry here um
if you if you know some physics then you know this but um if you don't then just want to mention
that these sort of s s um basically any s that you can write down uh gives you some field theory
these s s are some sums or integrals over energy terms and if you go to um sorry for the click
clickery um if you go to the Lagrangian field theory Wikipedia page then you can find
a whole lot of different um s objects that make sense and you can see here see how they
data mine how the data mine um the various physical theories that you have certainly heard of
we are interested in particular about fields so we have here scalar field theories this is what we
just saw you have some partial and then um some um some mass and there will be also be some time
the derivatives there um but the thing is that the pure gaussians are um where this is just
this quadratic object in in s are actually relatively um uninteresting from the physical
perspective because if you have for example if you have different quantum fields that interact
with each other and and then this information how they interact is also all encoded in this
in this um in this Lagrangian cells or in the action s and then uh you will have some more
complicated products in these objects and um if you have some power of the field that's higher than
two then this is actually uh representing um sort of self interaction in in field theory so um what
we really want to have is not just um the fields which fulfill these nice um properties in the
Euclidean version but we also want to have very finely controlled um interaction terms
there and so what we really want to have is um processes which are actually not gaussian right
and so what they do in the paper is in in the end um look at
uh um five to the fourth field theory so quadratic interaction let me see sorry
so what they really want to implement and what they actually then do in the paper is
they take this sort of action you you not only have this quadratic term there but you also have
their um this five to the four uh term and to to get this in to get um uh away from the
just quadratic gaussian process scenario there is two ways um to implement this self interaction
uh and one way is to actually not look at the infinite and limit right not the infinite
size network limit um because then uh the you basically break the neural network gaussian
process scenario you you person person uh you purposely stop a little bit earlier and get some
non gaussian effects and so what from the mathematical point of view is a bug that for
a real network you actually don't get a perfect gaussian situation here becomes sort of feature
it gives you the freedom to actually implement behavior and if you tweak the network uh nice
enough the ideas that then you can sort of control how it is broken this bug certainly
becomes a feature this is one way and the other way is if you um actually um
in sampling you do not uh take a trillion independent distributions over the weights
but what you and instead do is you introduce on purpose some dependencies of the individual
weight uh distributions right do you break the independence such that um there is a little
bit of a flaw in the central limit theorem and in this way by independence you also get some
non gaussian process because it's clear that if you do only um break the independence of this
weights a little bit you'll still by the central limit theorem get something which is just a slight
deviation from discussion processes right so if you the idea is if you tweak the this
the conditions for the god for the central limit theorem just in a wide way then you might produce
errors to the the gaussianity in a in a in a very controlled way and this is exactly what
they do here so here they describe a neural network with particular non-linearity as activations it
looks like this and what they do is they sample in this um in this dependent way in exactly the
correct way so that the um the action um that emerges by random sampling in this you know
particular way represents uh this uh this sort of physical field theory so this field theory in
particular is basically always the first interaction uh interacting field theory or one of the first
interaction field theories that you would learn in the quantum field theory course um this is not
one of the famous standard model um energy densities at least not in the exact same way here
but nonetheless this is like classical physical theory and so this is what this paper is all
about right breaking the neural network gaussian process uh theorems in the correct way to get um
the the right uh g functions endpoint functions out there that are relevant for physics um and
so you see that then you can you know in principle sample uh from this fixed neural network architectures
fields and then once you have like a way of sampling fields you can in principle because
the fields that demand all the properties of the quantum field theory uh compute expectation
values and thereby get physical uh information so this is the idea it also goes in the other direction
in um since there is uh these methods in particular um Feynman diagram computation methods that compute
correlations um for quantum fields you also have a method of computing aspects of random
initialized big neural networks right you have a big neural network you know that if you uh
to sample from it it is as if you would sample from a quantum field um and because you have
ways in physics to compute aspects like correlation functions and so on of the quantum fields
these uh g functions that you can compute in physics with Feynman diagrams and so on
also have a relevant meaning for computing typical aspects of random initialized neural networks
okay i know this is a little bit much but i hope it sort of makes sense um
um i um i don't know how clear everything was that i discussed so far i just want to give you
this is then more on the quantum field theory side a little motivation that i have just here
written up um that connects these g functions right these autocorrelations to physics i just
want to motivate it maybe give me a three more minutes so i i'm going to assume that you have
an idea of the fact that transition probabilities are the squares of inner products in a Hilbert's
base in quantum mechanics um so there are these um kernel objects k which are given like that
so in in in you know standard quantum mechanics what you have is some um you have some in um
current state which i call here in and then you have some uh other state out that you are
interested in you want to know what is the chance that this state um transitions over in that state
what you do is you um take the Schrodinger equation uh evolution which is governed by the
Hamiltonian h in in the nicest case um here the formal solution of this equation is just e to the i
h so this is the operator which moves any state forward in time you apply it to in to the like
let's say this is now this is in two days you say this state that i have currently how will it
evolve into the in like how will it look in two days then you basically apply this operator to
fast forward this thing um and then you get something uh the in state how it will look like
in two days and then the product with this object in two days with the out state that you're interested
in gives you the probability that the the current state after it has evolved is going to be observed
in uh in the out state okay so this is basically quantum mechanics one one um the for whatever
Hamiltonian you have here you um can often then compute some analytical case some kernels
these are these things and i think here if you score down in this Wikipedia page you find some
examples so these are just you know in this case uh three particle in one dimension this is like
some Gaussian this is similar to some heat dissipation equation and then if you have some
more relativistic scenario i think they also write down here some this is more complicated
correlation functions and examples okay so that's the one thing now if we talk
quantum field theory
then
yeah the uh this is basically just a definition um the the g's we all also had that in the paper
are certain expectation values right so there since we're on the pure relativistic side
and i'll take your clean side there's some time operator there but you can basically ignore it
for for the moment the the point is that these g's are determined by some fairly simple looking
expectation values and in the in the quantum mechanical formalisms these are also some
inner products in some Hilbert space of course in a quantum field theory there is no super nice
theory um about uh Hilbert spaces and and and these operators so there is it's more like a
like a functional definition this is what you calculate and um you hope it's somewhere
else comes up with the sort of rigorous setting to compute this but nonetheless you know how to
compute these g's you know how to set up the an evolution equation of your choice and make
sense of these correlation functions which is um what you want to get at in the first step
so given um you have uh definition of these g's there is actually then a sort of generating
function in the in the um you know analysis generating function sense there are these partition
functions which look like so so you basically we can view it like starting from this we start
with some some definition where all these g's are um captured uh in this sort of formal some
formal sum of products and then these g's are once you would have this uh partition function
with this j parameter you can in principle like formally speaking with functional derivatives
compute this g from this thing and then there's this whole quantum field theory I mean this is
what quantum field theory is all about that tells you how to compute this this set and this is
then exactly this sort of object this is what we already had have had there right we already said
that um if you um take the expectation value of e to the minus s in the euclidean case is here
of a product of these phi's you get all these these g's and this is basically sort of the
generating function trick to get to this to these sort of objects okay um okay so this is
what you do in practice and just to motivate also that how um neural network sampling would
be done another way of getting at this these g functions which you want to to have okay and
and so to round it up to see to to explain how this this simple g's this field theory um
correlation functions relate to actually um to actual observables right you want to have this
sort of transition functions as I discussed them for the quantum mechanics case there is then
complicated theory of how you take these states how they how you encode these states with some
fixed momentum in the in the quantum field theory you know they also live in some supposed
Hilbert space let's say you have to compute a lot in in momentum space which by Fourier
transform is sort of the dual to the the position space that we talked about in the whole video
so there are some Fourier transforms involved and you pass from the phi's to this a's so
just to sketch this out here um
yeah so you have when these are I'm just throwing these formulas at you right but
there's this typical relations between um the momentum space here ap and the the
spacetime fields and you can view this as a transition link by Fourier transform but
Fourier transform is with respect to space and also like you know momentum on space and also time
and energy and the energy encodes the sort of relation that is governed by the
Schrodinger equation relation and so all these things are fairly like complex but in the end
it's clear that once we have these g's we have to sort of Fourier transform them over to a momentum
space and in momentum space we encode actually the input output state that we are actually interested
in like this for some scattering transition probability stuff okay and then there is some more
theory that gives you this momentum space in out transition amplitudes that you're actually
interested in in terms of these these g's basically you see on this side here these are basically g's
and then from Fourier transforming and taking into account how the the frequency is and space is
sort of tied together through the evolution equations you get some nasty object here that you
have to then compute and then with this this this formalism you get from the green's functions to
the transition probability right so I in these last 10 minutes yeah I just explained to you
why these g's are really the important thing that you want to get that once you have them
then in principle you can compute this transition probabilities and the neural network aspect
gives you a way to get at these g's okay so lastly I want also to mention that
apart from the neural network kernel theory stuff there's also the you know classical
information geometry approach where you you know you have the underlying parameters theta
which are the weights and you can see them as encoding if you put a distribution over the
inputs and you get the distribution over the outputs which are governed by the parameters
then you get this information manifold so you heard that information geometry stuff this is
like a related but different mathematical angle that I just wanted to mention because we are like
formally so close having all the important all these stochastics already but this is a little
bit different nonetheless but also interesting there you have this neural neural manifold
thing so this is also one line of research I actually try to find things going into the
this quantum field theory research direction in in deep learning books in preparation for this video
and actually didn't find too much so there was more about this classical information geometry
aspect of things and then also worth noting the applications to quantum field theory is not the
only way in which people try to apply neural network theory and all these nice formulas to
physics there's also this mathematical metric flow things going on I just mentioned it because if
you look at for example the research groups that put out this paper then you will also find these
sort of neural network applications using quantum field theories okay so I mean at one hour I know
it has been a little bit fuzzy but nonetheless I think I at least maybe you understood the neural
network Gaussian process stuff and and can see how this ties in with path integral formalisms
and I motivated you I'm really I know very few people will make it through a one hour
video of these sort of friends but it's probably nonetheless I think the best way to get an infusion
of this sort of theory in a digestible way and in a way where somebody highlights these things so
this is it's not a real excuse I think it's nonetheless helpful even if this was not very
super prepared okay with that I leave it I leave it at that I wish you a good transition into the
next year I have no real videos planned for the upcoming months really I mean I have a folder with
20 started projects and videos I could talk about that I might come back to probably not
within the next months but then as I said I might look more into classical classical
functional analysis stuff for the sake of making a nice video about the universal approximation
theorem and at the latest then I will have a polished video take care

What's called AI today has departed to basically pure engineering.
It's designed in such a, the large language models are designed in such a way that in
principle, they can't tell you anything about language learning, cognitive
processes generally, they can produce useful devices like what I'm using, but
the very design ensures that you'll never understand, they'll never lead
to any contribution to science.
That's not a criticism anymore than I'm criticizing.
Camptons this week.
I talked to Noam Chomsky, one of the preeminent intellectuals of our time.
Our conversation touched on the dichotomy between understanding and application
in the field of artificial intelligence.
Chomsky argues that AI has shifted from a science aimed at understanding
cognition to a pure engineering field focused on creating useful, but not
necessarily explanatory tools.
He questions whether neural nets truly mirror how the brain functions and whether
they exhibit any true intelligence at all.
He also suggests that advanced alien life forms would likely have language
structured similar to our own, allowing us to communicate.
Chomsky is 94 and I reached him at home where he appeared with a clock hanging
omnestly over his head.
I hope you enjoy the conversation as much as I did.
Well, thanks.
You're in California.
Actually, I'm in Arizona, which is on California time.
Yeah.
Yeah.
Oh, wonderful.
Uh, yeah.
So, uh, you know, I wanted to talk to you because you have the, uh, you know, one
of the few people, uh, with a deep understanding of, of, uh, linguistics and,
uh, natural language processing that has the historical knowledge, uh, of, of
where we are, how we got to where we are and what, uh, that might mean for the future.
Uh, I, I understand the, the, uh, your criticisms of deep learning, uh, and,
and what large language models are not in terms of, uh, reasoning and, and, uh, you
know, understanding the, the, the underpinnings of, uh, language.
But, uh, I, I thought maybe I could ask you to talk about how this developed.
I mean, going back to Minsky's, uh, thesis at Princeton, when he was, you know,
before he turned against the perceptron, when he was talking about, uh, nets as,
uh, a possible model for, uh, biological processes in the brain.
And then, you know, how did, how you see that things developed and what were
the, the failures that didn't get to where presumably, uh, you would have wanted
that research to go, uh, and then, and then I have some other questions.
But, but, but is that enough to get started?
Well, let's, let's take an analogy.
Suppose you're interested in figuring out how, uh, insects navigate biological
problem.
So, uh, one thing you can do is say, let's try to study in detail what the
desert ants are doing in my backyard, how they're using solar azimuths and so on
and so forth.
Something else you could do is say, look, it's easy.
I'll just build an automobile which can navigate, uh, fine, does better than the
desert ants.
So who cares?
Uh, well, those are the two forms of artificial intelligence.
One is what Minsky was after.
It's now kind of ridiculed as good old fashioned AI, go fi, we're past that stage.
Now we just build things that do it better.
Okay.
Like, uh, an airplane does better than an eagle.
So who cares about how eagles fly?
Yeah, that's possible.
But, uh, it's a difference between totally different goals.
Roughly speaking, science and engineering, it's not a sharp difference, but first
approximation, either you're interested in understanding something or you're just
interested in building something that'll work for some purpose.
So they're both fine occupations, nothing wrong with.
I mean, when you say I'm criticism of the large, criticizing the large language
models, that's not correct.
I'm using them right now.
I'm reading captions.
Captions are based on deep learning, clever programming, very useful.
I'm hard of hearing, so they're very helpful to me.
No criticism.
But if somebody comes along and says, okay, this explains language, you tell them
it's kind of like saying an airplane explains how eagles fly, the wrong question.
It's not intended to lead to any understanding.
It's intended to be for a useful purpose.
That's fine.
No criticism.
And what's called AI today has departed to basically pure engineering.
It's designed in such a, the large language models are designed in such a way that
in principle, they can't tell you anything about language, learning, cognitive
processes generally, they can produce useful devices like what I'm using.
But the very design ensures that you'll never understand, they'll never
lead to any contribution to science.
That's not a criticism anymore than I'm criticizing champions.
Jeff Hinton says, you know, his goal is to understand the brain, how the brain works.
And he talks about AI as we know it today, supervised learning and generative AI as
useful by products, but that are not his goal or not the goal of cognitive
science or computational biology.
Was there a point at which you think the research lost a bead or is there research
going on that people aren't paying attention to that, that is not caught up in the
usefulness of these other kinds of neural nets?
Well, first of all, if you're interested in how the brain works, the first question
you ask is, does it work by neural nets?
That's an open question.
There's plenty of critical analysis that argues that neural nets are not what's
involved, even in simple things like memory.
Actually, these arguments that go back to Helmholtz, the neural transmission is pretty
slow as compared with the ordinary memory.
There's much hard for criticism by people like Randy Gallistel, cognitive
neuroscientist, who's given pretty sound arguments that neural nets in principle
don't have the ability to capture the core notion of a Turing machine,
computational capacity, they just don't have that capacity.
And he's argued that the computational capacity is in much richer computational
systems in the brain, internal delves, where there's very rich computational capacity,
goes wavy on neural net, some experimental evidence to support this.
So if you're interested in the brain, that's the kind of thing you look at.
Not just saying, can I make bigger neural nets?
It's okay if you want to try it, but maybe it's the wrong place to look.
So the first question is, is it even the right place to look?
That's an open question in neuroscience.
If you take a vote among neuroscientists, almost all of them think that neural nets
are the right place to look, but you don't solve scientific questions by a vote.
Yeah, one of the things that's obvious is neural nets, they may be a model,
they may mimic a portion of brain activity, but there are so many other structures.
There's all kind of stuff going on in the brain, way down to the cellular level,
there's chemical interactions, plenty of other things.
So maybe you'll learn something by studying neural nets, if you do, fine, everybody will be happy,
but maybe that's not the place to look if you want to study even simple things like just
memory and associations.
There's now already evidence of associations internal to large cells in the hippocampus,
internal to them, which means maybe something's going on at a deeper level where there's vastly
more computational capacity.
Those are serious questions.
So there's nothing wrong with trying to construct models and learn something from them, if you can, fine.
The building larger models, which is kind of the rage in the engineering side of AI right now,
does produce remarkable results.
I mean, what was your reaction when you saw
chat GPT or GPT-4 or any of these models, that it's just a sort of clever stochastic parent
or that there was something deeper?
If you look at the design of the system, you can see it's like an airplane explaining flying.
There's nothing to do with it.
In fact, it's immediately obvious, trivially obvious, not a deep point, that it can't be
teaching us anything.
The reason is very simple.
The large learning models work just as well for impossible languages that children can't acquire
as for the languages they're trained on.
So it's as if a biologist came along and said, I got a great new theory of organisms, lists a
lot of organisms that possibly exist, a lot that can't possibly exist.
And I can tell you nothing about the difference.
I mean, that's not a contribution to biology.
It doesn't meet the first minimal condition.
The first minimal condition is distinguish between what's possible from what's not possible.
You can't do that.
It's not a contribution to science.
If it was a biologist making that proposal, you'd just laugh.
Why shouldn't we just laugh when an engineer from Silicon Valley says the same thing?
So maybe they're fun.
Maybe they're useful for something.
Maybe they're harmful.
Those are the kinds of questions you ask about pure technology.
So take large language models.
There are something they're useful.
In fact, I'm using them right at this minute.
Captions.
It's very helpful for people like me.
Are they harmful?
Yeah, they can cause a lot of harm.
Disinformation, defamation, brain on human gullibility.
Plenty of examples.
So they can cause harm.
They can be of use.
Those are the kinds of questions you ask about pure engineering,
which can be very sophisticated and clever.
I mean, the internal combustion engine is a very sophisticated device,
but we don't expect it to tell us anything about how a gazelle runs.
It's just the wrong question.
Yeah, although I talk a lot to Jeff Hinton, and you'll be the first to concede that back propagation
there's no evidence of that.
And in fact, there's a lot of evidence that it wouldn't work in the brain.
Reinforcement learning.
You know, I spoke in a rich Sutton, that's been accepted as by a lot of people as
an algorithmic model for brain activity in part of the brain, in the lower brain.
So in terms of exploring the mechanisms of the brain, it seems that there is some usefulness.
I mean, it says, you said there's, on the one hand, people look at the principles,
and then they built through engineering, just as the analogy of a bird to an airplane,
they've taken some of the principles and applied it through engineering and created something useful.
But there are scientists that are looking at what's been created, like Hinton's criticism
of back propagation, and are looking for other models that would fit with the principles they see
in cognitive science or in the brain.
And I mentioned this forward-forward algorithm, which you said you hadn't looked at.
But I found it compelling in that it doesn't require signals to be passing back through
the neurons. I mean, they pass back, but then stimulate other neurons as you move forward
in time. But I mean, is there nothing that's been learned in the study of AI or the research
of neural nets?
But if you can find anything, it's great. Nothing against search, but it's just,
but we have to remember what you asked about chatbots. What do we learn from them? Zero.
For the simple reason that the systems work as well for impossible languages as for possible ones.
So it's like the biologist with the new theory that has organisms and impossible ones and can't
tell the difference. Now, maybe by the look at these systems, you'll learn something about
possible organisms. Okay, great. All in favor of learning things. But there's no issues.
It's just that the systems themselves, there are great claims by some of the leading figures in
the field. We've solved the problem of language acquisition, namely zero contribution, because
the systems work as well for impossible languages. Therefore, they can't be telling you anything about
language acquisition. Period. Maybe they're useful for something else. Okay, let's take a look.
Well, maybe for the audience that this is going out to, you know, I understand what you mean by
impossible, impossible, but could you just give a brief synopsis of what you mean by impossible
languages for people that haven't read your work? Well, I mean, there are certain general properties
that every infant knows, already tested down to two years old, no evidence, couldn't have evidence.
So one of the basic properties of language is that the linguistic rules apply to structures,
not linear strings. So if you want to take a sentence like
instinctively, birds that fly swim, it means instinctively they swim, not instinctively they
fly. Well, the adverb instinctively has to find a verb to attach to. It skips the closest verb
and finds the structurally closest ones. That principle turns out to be universal
for all structures, all constructions, and all languages. What it means is that an infant
from birth, as soon as you can test automatically, disregards linear order and disregards 100% of
what it hears, notice, as all we hear is words in linear order, but you disregard that and you
deal only with abstract structures in your mind, which you never hear. Take another simple example,
take the friends of my brothers are in England. Who's in England? The friends of the brothers,
the friends, not the brothers, the one that's adjacent, you just disregard all the linear
information. It means you disregard everything you hear, everything, and you pay attention only
to what your mind constructs. That's the basic, most fundamental property of language. Well,
you can make up impossible languages that work with what you hear. Simple rule, take the first
relevant thing, associate them. Friends of my brothers are here, brothers are the closest things,
and the brothers are here. Trivial rule, much simpler than the rule we use. You can construct
languages that use only those simple rules that will be based on the linear order of what we hear.
Well, maybe children, people could acquire them as a puzzle somehow using non-linguistic
capacities, but they're not what children, infants, reflexively construct with no evidence.
Well, there's many things like this, impossible and impossible languages. Well, nobody's tried
it out because it's too obvious how it's going to turn out. You take a large language model,
apply it to one of these models, systems that use linear order. Of course, it's going to work fine,
trivial rules. Well, that's a refutation of the system.
Meaning that if you trained it on an impossible language, it would produce impossible languages.
How would you mean? Well, you don't even have to train it because the rules are simple.
Yeah. Rules are much simpler than the rules of language. Like taking things that are,
take the example, the friends of my brother are here. The way we actually do it is we don't say,
take the noun phrase that's closest. We don't do that. That would be trivial, but we don't do it.
What we say is first construct the structure in your mind, friends of my brothers,
then figure out that the central element in that structure is friends, not brothers.
And then let's let it be talking about the head of it. It's a pretty complicated computation,
but that's the one we do instantaneously and reflexively. And we ignore, and we never see it,
hear it, remember? We don't hear structures. All we hear is words in linear order. What we hear
is words in linear order. We never use that information. We use only the much more looks
like complex. If you think about it computationally, it's actually simpler, but that's a deeper
question, which is why we do it. To move to a different dimension, there's a reason for this.
The reason has to do with the theory of computation. You're trying to construct
an infinite array of structured expressions. Simplest way to do that, the simplest computational
procedure is binary set formation. But if you use binary set formation, you're just going to get
structures, not order. So what the brain is doing is the simplest computational system,
which happens to be very much harder to use. Nature doesn't care about that. Nature constructs
the simplest system, doesn't care about it, if it's hard to use or not. I mean, you know, nature
could have saved us a lot of trouble if it had developed eight fingers instead of 10.
Then we'd have a much better base for computation. But nature didn't care about that when it developed
10 fingers. If you look at evolution, it pays no attention to function. It just constructs the
best system at each point. There's a lot of misleading talk about that. But if you just think
about the physics of evolution, say a bacterium swallows another organism,
the basis for what became complex cells, and nature doesn't get the new system,
it reconstructs it in the simplest possible way. It doesn't pay any attention to how
complex organisms are going to behave, not what nature can do. And that's the way evolution works
all the way down the line. So not surprisingly, nature constructed language so that it's
computationally elegant, but dysfunctional, hard to use in many ways. Not nature's problem,
just like every other aspect of nature. You can think of a way in which you can do it better,
but it didn't happen stage by stage. Two questions from that. So your view is that
artificial intelligence, as it's being called, and particularly generative AI,
doesn't exhibit true intelligence. Is that right? I wouldn't even say that.
It's irrelevant to the question of intelligence. It's not its problem. A guy who designs a jet plane
is not trying to answer the question, how do eagles fly? So to say, well, it doesn't tell us how
eagles fly is the wrong question to ask. It's not the goal. Except what people are struggling with
right now. You've heard the existential threat argument that these models, if they get large
enough, they'll actually be more intelligent than humans. That's science fiction. I mean,
there is a theoretical possibility. You can give a theoretical argument that, in principle,
a complex system with vast search capacity could conceivably turn into something that would start
to do things that you can't predict, maybe beyond. But that's even more remote than some
distant asteroid, maybe someday hitting the earth, could happen. I mean, if you read a
serious scientist on this, like Max Tagmark, his book on the three levels of intelligence,
does give a sound theoretical argument as to how a massive system could, say,
run through all the scientific discoveries in history, maybe find out some better way of
developing them and use that better way to design something new which would destroy us all.
It's, in theory, possible, but it's so remote from anything that's available that it's a waste of
time to think about it. Yeah, so your view is that whatever threat exists from
generative AI, it's the more mundane threat of disinformation. Disinformation, defamation,
gullibility, Gary Marcus has done a lot of work on this, real cases, those are problems. I mean,
you may have seen that there was a, sort of as a joke, people, somebody developed a defamation of the
pope, put an image of the pope, somebody could do it for you, duplicate your face so it looks more
or less like your face, pretty much duplicate your voice, develop a robot that looks kind of like you,
have you say some insane thing, it would be hard only an expert could tell whether it was you or
none. It's like this was done already several times, but basically is a joke.
When powerful institutions get started on it, it's not going to be a joke.
Another argument that's swirling around these large language models is the question of
a sentence of whether if the model is large enough, and this goes a little bit back to how
there's a lot more going on in the brain than the neural network or the cerebral cortex, but
that there is the potential for some kind of sentence, not necessarily equivalent to human
sentence. These are vacuous questions. It's like asking, does a submarine really swim?
You want to call that swimming? Yeah, it swims. You don't want to call it swimming? It's not a
substantive question. Well, in the sense that it supports the view that there's no separation between
consciousness and the material activities of the brain. There's a separation that hasn't
been believed since the 17th century. John Locke, after Newton's demonstration, said, well leaves
us only with the possibility that thinking is some property of organized matter. That's the 17th
century. Yeah, okay. But the belief in a soul and consciousness is something separate from a
material biology. It persists. The belief in all kinds of things. But within the rational part
of the human species, once Newton demonstrated that the mechanical model doesn't work,
there's no material universe in the only sense that was understood. Locke took the obvious conclusion,
said, well, since matter, as Mr. Newton has demonstrated, has properties that we cannot
conceive of. They're not part of our intuitive picture. Since matter has those properties,
organized matter can also have the property of thought. This was investigated all through the
18th century. Ended up finally with Joseph Priestley, a philosopher in the late 18th century,
gave pretty extensive discussions of how material, organized material objects could have
properties of thought. You can even find it in Darwin's early notebooks. It was kind of forgotten
after that. Rediscovered in the late 20th century as some radical new discovery,
astonishing hypothesis. Matter can think. Of course it can. In fact, we're doing it right now.
But the only problem then is to find out what's involved in what we call thinking,
what we call sentience, what are the properties of whatever matter is. We don't know what matter is,
but whatever it turns out to be, whatever constitutes the world, what physicists don't
know, but whatever it is, there's something organized. Elements of it can have various properties,
like the properties that we are now using, properties that we call sentience. Then the question
whether something else has sentience is as interesting as whether airplanes fly. If you're
talking English, airplanes fly. If you're talking Hebrew, airplanes glide, they don't fly.
It's not a substantive question. What metaphors do we like?
But what you're saying then is that neural net may not be the engineering solution, but
that eventually it may be possible to create a system outside of the human brain that can think
whatever thinking means. And do what we call thinking. But whether it thinks or not is like
asking the airplanes fly, not a substantive question. We shouldn't waste time on questions
that are completely meaningless. Going back to the history then,
you know, Minsky was very interested in the possibility of neural nets as a
computational model. In Minsky's time, it looked as if neural nets were the right place to look.
Now I think it's not so obvious, especially because of Galastal's work,
which is not accepted by most neuroscientists, but seems to me pretty compelling.
Can you talk a little bit about that because I haven't read that and I'm guessing our readers
haven't, our listeners haven't. Galastal is not the only one. Roger Penrose is another
Nobel Prize winning physicist, but a number of people have pointed out Galastal mostly that
have argued, I think plausibly, that the basic component of a computational system,
the basic element of essentially a Turing machine, cannot be constructed from neural nets.
So you have to look somewhere else with a different form of computation. And he's also
pointed out, but in fact, it's true that there's much richer computational capacity in the brain
than neural nets, even internal to a cell. There's massive computational capacity
intercellular. So maybe that's involved in computation. And then there's by now some
experimental work, I think, giving some evidence for this, but it's a problem for neuroscientists
to work on. I'm not an expert in the field. I'm looking at it from the outside,
so don't take my opinion too seriously. But to me, it looks pretty compelling.
But whatever it is, neural nets or something else, yes, some organization of them, of whatever
is there, is giving us the capacity to do what we're doing. So if you're a scientist, what you do is
approach it in two different ways. One is you try to find the properties of the system.
What is the nature of the system? That's first step kind of thing I was talking about before with
structure dependence. What are the properties of the system that an infant automatically
develops in the mind? And there's a lot of work on that. From the other point of view, you can say,
what can we learn about the brain that relates to this? Actually, there is some work. So there is
neurophysiological studies which have shown that for artificial languages that violate the principle
that I mentioned, this structure dependent principle, if you train people on those,
the ordinary language centers don't function. You get diffuse functioning of the brain,
means they're being treated as puzzles basically. So you can find some neurological correlates of
some of the things that are discovered by looking at the nature of the phenotype.
But it's very hard for humans for a number of reasons. We know a lot about human, the physiology
of human vision. But the reason is because of invasive experiments with nonhumans, cats,
monkeys and so on. Can't do that for language. There aren't any other organisms unique to humans.
So there's no comparative studies. You can think of a lot of invasive experiments which
teach you a lot. You can't do them for ethical reasons. So study of the neurophysiology of
human cognition is a uniquely hard problem. In its basic elements like language,
it's just unique to the species. And in fact, a very recent development in evolutionary history,
probably the last couple of hundred thousand years, which is nothing. So you can't do the
invasive experiments for ethical reasons. You can think of them, but you can't do them,
fortunately. And there's no comparative evidence. So it's much harder to do. You have to do things
like, you know, looking at a blood flow in the brain, MRI type things, electrical stimulation,
looking from the outside. It's tough. It's not like doing the kind of experiments you can think of.
So it's very hard to find out the neurophysiological basis for things like use of language. But
it's one way to proceed. And the other way to proceed is learn more about the phenol.
It's like chemistry for hundreds of years. You just postulated the existence of atoms.
Nobody could see them. You know, why are they there? You know, because unless
there are atoms with the Dalton's properties, you don't explain anything. Early genetics,
early genetics work before anybody had any idea what a gene is. You just looked at the
properties of the system, try to figure out what must be going on.
It's the way astrophysics works. You know, most of science works like that. So this does too.
When you talk about invasive exploration, there are tools that are increasingly
sophisticated. I'm thinking of neural link, Elon Musk's startup that has these super fine
electrodes that can be put into the brain without damaging individual neurons.
There's actually, I think, much more advanced than that is work that's being done with
patients under brain surgery. Under brain surgery, with the brain basically exposed,
there are some noninvasive procedures that can be used to study what particular
parts of the brain, even particular neurons are doing. It's very delicate work.
But there is some work going on. One person is working on it is Andrea Moro,
the same person who designed the experiments that I described before about impossible languages.
That seems to me a promising direction. There's other kinds of work. I could mention some of it.
Alec Moran, why you was doing interesting studies that shed some light on the very
elementary function. How do words get stored in the brain? What's going on in the brain that
tells us that blake is a possible word, but the nick isn't for an English speaker. It is for
an Arabic speaker. What's going on in the brain that deals with that?
Hard work. David Peppel, another very good neuroscientist, has found evidence
for things like pharyngeal structure in the brain. But the kinds of invasive experiments
you can dream of, you can think of, he's just not allowed to do. So you have to try it in much
indirect ways. Do you think that understanding cognition has advanced in your lifetime?
And are you hopeful that we'll eventually really understand how the brain thinks?
Well, there's been vast improvement in understanding the phenotype that we know a great deal about
that was not known even a few years ago. There's been some progress in the neuroscience of
the relates to it, but it's much harder. Yeah. I'm just curious about where you are in,
not physically you're in Arizona, but where you are in your thinking. Are you still
pushing forward in trying to understand language in the brain or are you sort of retired, so to speak,
at this point? No, very much involved. I mean, I don't work on the neurophysiology.
A man I mentioned, Andrea Moro, happens to be a good friend. So I follow the work they're doing,
we interact, but my work is just on the phenotype. What's the nature of the system?
And there, I think we're learning a lot. I'm right in the middle of papers at the moment,
looking at more subtle, complex properties. The idea is essentially to find
what I said about binary set formation. How can we show that from the simplest
computational procedures, we can account for the apparently complex and apparently varied
properties of the language systems. There's a fair amount of progress on that,
that was unheard of 20, 30 years ago. So this is all new. Understanding is one thing and then
re-creating it through computation in external hardware is another. Is that a blind ally or do
you think that? Well, at the moment, I don't see any particular point in it. If there is some point,
okay. I mean, the kinds of things that we're learning about the nature of language,
I suppose you could construct some sort of system that would duplicate them,
but it doesn't seem any obvious point to it. It's like taking chemistry in 100 years ago and saying,
can I construct models that will look sort of like,
suppose you took, I was saying, a kick of the diagram for an organic molecule
and study its properties. You could presumably construct a mechanical model
that would do some of those things. Would it be useful? Apparently chemists didn't think so, but
if it would, okay. If it wouldn't, then don't.
Nonetheless, I mean, we are using neural nets even in this call.
Do you see, I mean, setting inside the question of whether or not they help us understand anything
about the brain. Are you excited at all in about the promise that these large
models hold? I mean, because they do something very useful.
They are. Like I said, I'm using it right now. I think it's fine for me, somebody who can't hear
to be able to read what you're saying pretty accurately. It's an achievement, so great.
I have nothing against technology. And who do you think is going to carry on
your work from here? I mean, are there any students of yours who you think we should
be paying attention to? Well, quite a lot. A lot of young people doing fine work.
In fact, I work with a, closely with a small research group
by now, spread all over the world. We meet virtually from Japan and Holland and other places
regularly working on the kinds of problems I was talking about.
But right now, I should say it's a pretty special interest. Most linguists aren't
interested in these foundational questions. But I think that's happened to be my interest.
I want to see if we can show the, ultimately try to show that language is essentially a
natural object. I mean, there was an interesting paper written about the time that I started
working on this by Albert Einstein in 1950. He had an article in Scientific American, which
I read, but didn't appreciate at the time, began to appreciate later, in which he talked
about what he called a miracle creed. He has an interesting history. It goes back to Galileo.
Galileo had a maxim saying, nature is simple. It doesn't do things in a complicated way if it
could do them in a simple way. Galileo's maxim couldn't prove it. But they said, I think that's
the way it is. That's the task of the scientist to prove it. Well, over the centuries, it's been
substantiated case after case. It shows up in Leibniz's principle of optimality. But by then,
there was a lot of evidence for it. By now, it's just a norm for science. It's what Einstein called
the miracle creed. Nature is simple. Our task is to show it. It says, improve it. Skeptic can say,
I don't believe it. Okay. But that's the way science works. Well, this one's worked the same way for
language. But you couldn't have proposed that 50 years ago, 20 years ago. I think now you can
believe that maybe language is just basically a perfect computational system at its base.
You look at the phenomena, it doesn't look like that. But the same was true of biology. Go back to
the 1950s, 1960s, biologists assumed that organisms could vary so widely that each one has to be
studied on its own without bias. By now, that's all forgotten. It's recognized that there,
since the Cambrian explosion, there's virtually no variation in the kinds of organisms,
fundamentally all the same. Deep homologies, and so on. So even been proposed that there's a universal
genome, not totally accepted, but not considered ridiculous. Well, I think we're in the same
direction as the study of language. Now, let me say again, there's not many linguists interested in
this. Most linguists, like most biologists, are studying particular things, which is fine. You
learn a lot that way. But I think it is possible now to formulate a plausible thesis that language is a
natural object like others, which evolved in such a way as to have perfect design,
but to be highly dysfunctional. Because that's true of natural objects, generally. It's part of
the nature of evolution, which doesn't take into account possible functions. I mean, the last stage
of evolution, the reproductive success that does take function into account, natural selection,
that's a fringe of evolution. It's just the peripheral fringe, very important, not denigrated,
but it's the basic part of evolution is constructing the optimal system that meets
the physical conditions established by some disruption in the system. That's the core of
evolution. That's what Turing studied. Darcy Thompson, others by now, I think it's understood.
And I think maybe the study of this particular biology after a language is a biological object.
So why should it be different? Let's see if we can show it. There's been a lot of talk in the news
recently about extraterrestrial craft having been found by the government. I don't put much
talk in it, but imagine that there is extraterrestrial life, advanced forms of life. Do you think that
their language would have developed the same way if it's based on these simple principles? Or is it
could there be other forms of language in other biological organisms that would be quote unquote
impossible in the human context? Back around the 1960s, I guess, Minsky
studied with one of his students, Daniel Belbrom, studied the simplest Turing machines,
few estates, fewest symbols, and asked what happens if you just let them run free?
Well, it turned out that most of them crash, either get into endless loops or just crash
don't proceed. But the ones that didn't crash all produced the successor function.
So he suggested what we're going to find if any kind of intelligence develops is
it'll be based on the successor function. And if we want to try to communicate with some
extraterrestrial intelligence, we should first see if they have the successor function
and then maybe build up from there. Well, turns out the successor
happens to be what you get from the simplest possible language. The language is one symbol
and the simplest form of binary set formation basically gives it a successor function.
Add a little bit more to it, you get something like arithmetic. Add a little bit more to it,
you get something like the poor properties of language. So it's conceivable that if there is any
extraterrestrial intelligence, it would have pursued the same course. Where it goes from there,
we don't know enough to say. And back to the idea that there is no super natural realm,
that the consciousness is an emergent property from the physical attributes of the brain.
Do you believe in a higher intelligence behind the creation or continuation of the universe?
I don't see any point in vacuous hypotheses. If you want to believe it okay, it has no consequences.
But do you believe it? No, I don't see any point in believing things for
which there's no evidence and do no work. And another thing I've always wanted to ask
someone like you, clearly your intelligence surpasses most peoples.
I don't think so. Well, that's a good, that's interesting that you would say that. You think
it's just a matter of applying yourself to study throughout your career.
I have certain talents, I know, like not believing things just cause people believe them.
And keeping an open mind and looking for arguments and evidence, not
anything we've been talking about when meaningless questions are proposed, like
our other organism, sentient or the submarine swim, I say let's discard them and look at
meaningful questions. If you just pursue common sense like that, I think you can make some progress.
Same on the questions we're talking about language. If you think it through, there's every reason why
the organic object language should be an object. If so, it should follow the
general principles of evolution, which satisfy what Einstein called the miracle creed. So why
shouldn't language, so let's pursue that CFR we can do. I think that's just common sense. Many
people think it's superior intelligence. I don't think so. That's it for this episode. I want to
thank Noam for his time. If you'd like a transcript of this conversation, you can find one on our
website, I on AI, that's EYE-ON.AI. In the meantime, remember, the singularity may not be near,
but AI is about to change your world. So pay attention.

There isn't a science around that isn't profoundly influenced by the availability of
massive computing power and just greater regular computing power. It's the story of our age. It's
not just the story of AI. The idea is to leverage computation to make useful things and understand
the mind. These all these things need a lot of computation. It's the fact that computation has
become cheaper exponentially for on the order of 100 years and can be expected to continue going
It looks like doubling every two years now every 18 months and that keeps happening 18 months after
18 months after 18 months and it means you double and you double and things get qualitatively different
every decade and that's happened for a long time for many decades and will happen more so in the
future. So we have that to look forward to. I think it's what we really should mean when we
say the singularity. The singularity is that we have this exploding it's a slow explosion of
computer power and that that is fundamentally changing things. Hi I'm Craig Smith and this is
I on AI. In this episode I speak with Richard Sutton the father of reinforcement learning
and professor at the University of Alberta. We discussed his cooperation with John Carmack
on Keen a startup that vows to reach artificial general intelligence by 2030. Richard also talked
about the Alberta plan his ambitious five-year research agenda focused on building embodied
agents with the capability to learn and plan through interactions with their environment.
Sutton provides insights into the current state of progress new algorithmic developments
and trade-offs between simulated and physical environments in training and the ultimate goal
of creating AGI. I hope you find the conversation as amazing as I did. So why don't you start by
introducing yourself. I assume people know who you are I've had you on the podcast before
but for those new listeners tell us who you are where you are and then we'll talk about the Alberta
plan which I find pretty exciting. Thank you Craig I'm Richard Sutton I'm a scientist I've been
studying artificial intelligence for like 45 years a long time and I'm up in north of the
University of Alberta in Canada and I'm a professor in the computer science computing science department
and also I'm a researcher at Keen Technologies and I got lots of titles and sub-rolls but basically
I'm just trying to figure out how the mind works and I've tried to do it in a very broad and
interdisciplinary way reading all the different thinkers on the subject and addressed from the
point of view of psychology and how the brain might work as well as. Yeah I've read a number of the
recent papers and I can see this thread developing and I don't know whether it's just that you're
writing more and so the thoughts are are more developed in print or whether they're developing
in your mind but from 2019 when you wrote the bitter lesson you talked about the idea that
it's really increasing computation and the striving a lot of things a lot of progress
that kind of coincided with open AI's scaling of the transformer model I talked to Ilya
Sutskover and I asked him whether your essay had triggered their their interest in scaling and he
said no it was coincidental but I kept first can we talk about that about how scale scaling and
and the availability of computational resources and Moore's law has driven a lot of what's happened
in artificial intelligence research almost more than novel algorithms well I think the
first thing to be aware of is it's it's been driving things that are not just artificial
intelligence it's been driving all the sciences and all the engineering developments in the world
there isn't a science around that isn't profoundly influenced by the availability
availability of massive computing power and just greater regular computing power
it's it's it's the story of our age it's not just the story of AI it's not particularly the story
of AI AI has always known that it needs computation the idea is to leverage computation to make useful
things and understand the mind um yeah now it's true that those of us who are interested in
connection to systems or distributed networks nowadays just call neural networks not particularly
good terms so I always shudder a little bit when I use it but those of us that have been doing that
have have those are doing learning I think that learning is important for intelligence
these all these things need a lot of computation and so they're they are limited by the computation
available at the time okay so let's let's be what is this thing what is the so Moore's law what's
called Moore's law it's the fact computation is becoming more plentiful and cheaper exponentially
for on the order of a hundred years and can be expected to continue going that way so
exponentially looks like doubling every two years now each every 18 months and that keeps
happening 18 months after 18 months after 18 months and it means you double and you double
and things get qualitatively different uh every decade and that's happened for a long time for
many decades and will happen it's more so in the future so we have that to look forward to that will
continue having a tremendous influence on everything that's done on the other hand it's just normal
it's just what you would expect and that those of who worked on AI for for a long time have just
you know expect and plan for and um now it's coming but it's an exponential so exponentials
are self-similar so that means they look the same at every point in time every every year it's
you're doubling in a year and a half and so it's it's an explosion as every exponential is an
explosion it's it's sort of I think it's what we really should mean when we say the singularity
the singularity is that we have this exploding it's a slow explosion of computer power and that
has fundamentally changed things yeah and I had a really interesting conversation almost a year
ago with Aidan Gomez who was on the team that that that designed the transformer algorithm at
Google and he now has a startup co-coher he's Canadian and he said an interesting thing that
that he believes it could have been almost any algorithm it didn't have to be the transformer
that the community got behind the transformer poured resources into it continued to scale it
and it was scalable I mean that was important that it that it's a scalable architecture but
that but it didn't have to be the transformer and and that made me think of you because
of so transformers they the way he described it at its core it's a stock of multi-layer
perceptrons with attention you scale it feed it data and it does learns to understand language
or at least seems to understand language but it's got all these obvious limitations
of I've been talking a lot over the last couple of years to Yamakun about world models and that
to me sounded like a much more exciting direction for general intelligence because not all intelligence
is is contained in language or at least most or even less so in human text and then I see you guys
come along with the Alberta plan and that that that sounded even more exciting to me so
how how do you so the Alberta plan you're building the ideas to build an
agent ultimately an embodied agent that that has a world model or can
and create a world model through interactions with its environment how is that different from
Lucune's approach at a very basic level very basic level a good is that they're a very similar
idea it's uh you look at the parts of his architecture and the parts of the architecture
put forth in the Alberta plan they line up one one for one yeah we're trying to do the same thing
we're going about it slightly different and we could talk about that but I think
to just to focus on the differences might even be to distract from the big message the big
message is that you have to have a goal and you have to have a model of the world and
and then everything is driven by using that model to take action and to plan action at various levels
of abstraction in order to to achieve the goal okay so to me this is really what intelligence is
understand the world use your understanding to get to achieve to achieve your your goals
I'd like to formulate the goals as as a reward and I'm super comfortable with that other people
sort of grudgingly accept rewards even though it seems kind of low level
but it's a it's a natural approach I think I think it's something that almost makes more
sense to people who aren't steep and deep learning and to supervise learning and one thing I found
interesting in in the roadmap that you've laid out for the Alberta plan you start with supervised
learning and why is that is it just because it's it's easy yeah I guess we do in a sense
because we want to focus on well continual learning learning continually which is sort of
an obvious thing almost what learning means it has something that goes on at all times but
the first steps getting continual learning with nonlinear networks is still challenging
even for supervised learning and so it's natural to start at the simplest possible case which involves
the fewest other factors and that's a supervised learning case yeah yeah it's funny let me just
say a few words about that because there's sort of been a fight through a struggle throughout the
decades between supervised learning and reinforcement learning you know there's only so much oxygen
for learning methods and all the attention that's paid to supervised learning somewhat
detracts from reinforcement learning so there's a there's a there's a bit of a friendly competition
and supervised learning has always won the competition because supervised learning is so
much more easy to put into practice and for people to use and it's sort of it's sort of
less ambitious but it's really important and really those of us who do reinforcement learning
or try to make whole agent architectures we are consumers of supervised learning outcomes we will
use them as components of our overall architecture so we need them and we can work on them and we
need to structure them for our purposes I saw one of your talks you make a distinction between
AI tools and AI agents and supervised learning falls into the tool category can you sort of
start and and talk about the evolution of the Alberta plan and then present to listeners
what it is in in its simplest form and that'll that'll give me a structure on which to hang
questions the Alberta plan is an attempt to understand intelligence as a as a primarily
a learning phenomenon assists us something that comes to understand its environment and and then
drives the environment to achieve goals so the first step in the Alberta plan is the structure
between the agent the environment and their interaction form the interaction there's the
you're not exchanging states you're exchanging observations like sensors sensors visual touch
auditory it's all abstract to those particulars but it's got to be genuine observations and not
state because state we don't we don't really have access to directly so that you know the
principles number one principle I'm trying to remember them as I speak but number one principle
is this this agent environment interaction is sacrosanct and number two is that learning or
everything is is we could say continual I think we call it we say temporally uniform
temporally symmetric in in the Alberta plan which means that there are no special phases
where you like training and test there's just life goes on and on you get rewards or you don't get
or you don't get the reward you want and you get your observations and there there is no teacher
other than rewards pains and pleasures and maybe I'm not getting the four principles right but
another important point is that you are going to be forming a model and so you're going to plan
both trial and error learning directly from experience and learning a model and then planning
with the model both these are important part of intelligence okay so those are that's the
background then we outline there are 12 steps and the 12 steps really start with let's have
learning that is temporally uniform let's have metal learning and metal learning maybe I should
stop and on that for a moment metal learning means learning to learn not just learning one function
but once you are continually learning you're learning this and you're learning that you get
many many experiences learning and you can get better at learning you can use those repeated
experience with repeatedly learning to make make future learning episodes more efficient so as part
of that you learn representations you learn features you learn step sizes
okay so continual learning and then all the algorithms and once once we add
metal learning and continual learning we have to in supervised learning then we extend that to
reinforcement learning which involves its own set of issues to get more interesting temporal
relationships and I think like the first six steps are crafting the basic algorithms of reinforcement
working through them again to be continual and meta and then we start to bring in the the challenging
issues like learning off policy and learning models of the world and then planning and the
just to jump to the end the last step is about
AI, AI, AI's, AI intelligence augmentation
where we combine computers, AI's with our own minds to make make our own minds stronger
okay now one of the key steps in there was off policy learning and learning a model of the world
off policy learning means you want to be able to learn about things that you're not doing or you're
not because you're not doing all the way to completion so even like to recognize an object
you look at the object and you say how would you you have to define that in some objective way
and the best way to just do that is as a sub problem so
yeah maybe maybe I'll just sort of stop there the most interesting strategy
distinctive strategy by the Alberta plan is the pose is that the mind works by posing sub
problems for itself and then working on them and it's it's not it's sure it's got a main
problem which is to get reward but it's also has many thousands of sub problems it's also
working on simultaneously and since it's not behaving it cannot behave for all thousand
problems at once it has to pick one problem like perhaps the main problem and behave according to
that so all the other things have to be able to learn from data that's not exactly on what they
would do and this is called off policy learning and it's a key to learning to achieve auxiliary
sub problems and also it's a key to efficiently learning a model of the world yeah you you have
something called the horde architecture is is that where that comes in when you you break
a problem down into multiple sub tasks that that you learn I was one one paper where we
we worked on that idea we developed that idea the horde is the horde of sub problems
each each demon in the horde which is it could be almost viewed like a single neuron in a neural
network as achieving working towards a different task trying to predict a different thing or maybe
trying to attain a different thing it's the view of the of the mind as decentralized there is one
goal and everything is ultimately driven towards one goal but still it's a useful structure to
to have different parts driving towards towards other goals how did you get together with john
cormack was that primarily because you need the funding and it gives you a vehicle to raise
capital oh seriously I mean you you know Jan Lacoon's got meta behind him well it's just not it's
not really comparable uh john's uh john's company is great but it's still like a 20 million dollar
company and uh which is which is plenty of money for what we want to do now um john and I got together
because we had similar ideas about what was needed um and and also what was not needed
um to get to ai or agi um yeah so I read an art newspaper article an interview that john did
down in texas and uh I just could see that he was thinking about the way
thinking about things the way I was even though our backgrounds were quite different
you thought of intelligence you had to there's a few principles that needed to be worked out
rather than so this isn't a huge program to write it's a few principles we have to figure those out
um not that many maybe uh maybe 10 000 lines instead of 10 million lines of code
so it's easy to get it's relatively it's still it's still it's still hard to get basic research
funding in the world it's easy to get funding towards uh applications of ai large language models
particularly um anyway I'm really enjoying working at keen and being able to focus on the ideas
and uh it's a it's a it's a it's a calm company we um um
there's a lot of thinking involved a lot of contemplation a lot there is also experiments
and we're trying to get um the engineering side of it is really important uh but for me it's
been really great just to be able to regroup my thoughts and think about them very carefully and
push them forward but keen is is implementing the alberta plan is that right I mean that's
that's uh the project well the alberta plan is a research plan it's like a five-year research plan
and so research is something you don't implement research is something you conduct and and it
doesn't always end up the way you want but um yeah I wouldn't say implement is the right word
not yet but but the the work you're doing at keen is is informed by the alberta
yeah I'm absolutely I'm working on the alberta plan uh uh and and the end goal at keen is to
create the uh the embodied intelligence described by the alberta plan you don't sound very yeah
very confident well a plan is just a plan and you know I think there's a good chance that it
will work out as planned but you know a five-year plan you make another one after four or three years
yeah so I wouldn't I wouldn't uh presume to to know how it's gonna work out but at the same time
we have to make you know we have to make our bets we have to think hard about it um just knowing um
you know we we may well be right but you know you your work is primarily in reinforcement
learning you're you wrote the book on reinforcement learning temporal difference learning and
uh lambda and all of that is is this I mean this is this seems a much more ambitious
uh project is this was it the the success of the transformer scaling that that said well you know
let's do that with rl let's why why are these guys uh uh you know everyone's celebrating what
they're doing but but there's much more to be done no no what what you're seeing the alberta plan
is is perhaps bigger than the book but this has always been the plan we've always in AI
tried to understand all of the mind and reproduce it in computers and so that's an that's that is a
big enormous ambition that's what it's always been so the large language models are a bit
a bit disappointing in some sense I mean it's really good that people are getting excited
and people are wanting to learn about it but um but it's not it's I don't envision that it's the
direction um that will be most uh productive to pursue now you know who knows what I do know
is it's not the most direction that's useful for me to pursue um I I'm much more interested in
actions and goals and how an agent can tell what's true and what's not true all of those
things are missing from large language models so uh um no I'm not they're not really what what are
they what they are doing that's important is they're showing uh what you can do with uh computation
and and networks um and learning that you can get enormously complex um things and you can
incorporate a lot of data it just shows the power for those who needed to be shown that
and and it could be uh an interface between humans and and whatever you end up creating
the agents you end up creating you still need a language interface to communicate yeah but I don't
I'm I doubt that what we're doing with large language models today will contribute to that
oh is that right yeah I mean in other words the models that you want to build the agents you want
to build would learn language uh as as part of the learning process yeah so it's like we say
language language last you know language not not language first with large language models
are language first we just say large language last just as Jan McCoon says we need to do you
know rat level intelligence and then cat level intelligence and we have to get those figured
out before we should try to make human level intelligence uh so where are you on the plan I
mean you you figured out reinforcement learning you can build agents uh you there are various
architectures for creating representations from from various kinds of sensory input
and and at that representation level then you can plan efficiently so where in all of that
are are you in your research well it's a little hard to explain non-technically
but you can say some things certainly you can say that the various steps
are not done entirely sequentially you you're always looking for areas of opportunity where
you can make an increment of progress and those could be you know on step 10 or they could be on
step three but you also I could also try to be very rough and say that we're we're at about step
four now we are still doing things where we're changing the basic underlying
fundamental reinforcement learning algorithms we are not done with that we need more efficient
algorithms and I'm excited about some of the changes new ideas we're developing recently
about how that can be done can you talk about those new ideas at all okay well one of the big
things is efficient off-policy learning and the use of important sampling important sampling is
where you see how likely you're to do things under your target and your behavior policies
and you adjust the returns based on those the ratios of those two and for a long time I thought
that was the only way to adjust the returns but now the forward correction of the returns I think
can be done by by changing your expectations so like if you're expecting a good thing to happen
you're expecting a good action to be taken and then a different action was taken a more exploratory
action so this is a deviation from your target policy which would be more greedy and one way to
take into account the deviation from the target policy is to just say oh okay now I've done something
not best so I'm just going to adjust my level now you're going to expect a little a little less
and there's a way there's a systematic way of doing that
that's gives us a new way to handle the off-policiness of of our returns
and so this gives a whole new family of algorithms so that's exciting now
exciting maybe mostly for me I think maybe the most accessible direction of of of excitement
of novelty is in continual right so there's I'm going to say a bunch of things and to me
they're all going to have the same solution continual learning meta learning representation
learning learning to learn learning how to generalize state how to construct a state
representation feature finding that whole thing is is is coming and it will be a kind of
it's just a new kind of a way a new kind of way of doing the learning in deep networks
and I call it dynamic learning nets see a dynamic learning nets have learning at three levels
whereas usually our neural networks only learn at one level they learn the level of the weights
and in addition we also want to learn at the level of step sizes so all of every place you
have a weight in your network you're also going to have a step size so a step size is sometimes
called a learning rate it's much better to call the step size because the learning rate will be
influenced by many other things so if we imagine a whole network all these weights next to each
weight is a step size that is adjusted by an adaptive process that's adapted in a meta learning way
a meta gradient way towards making the system learn better rather than just perform better
an instantaneous moment in time learning rates or step sizes don't affect the function they don't
affect some function implemented in a particular point in time they don't affect with the network
does they affect what the network learns and so if you can tune the step sizes you also get
learning to learn and learning to generalize well and things like that the last three the last
element that we wanted to have be adaptive weights step sizes the third one is the connection pattern
so who's connected to who and so this will be done by an accretive process
like let's say you start with a linear unit and it learns say a value function or a policy
and it does the best it can with the features available and and then it needs to induce the
creation of new features because you need to learn a nonlinear function of your original
signals and so you need to create new features that have become available to that linear unit
and in this way you grow in a sort of organic way a system that can learn nonlinear functions
and so this is just a different way of ending up with a deep network that was all learned
including all the features dynamic learning that's where is the data the input data coming from
well the the input data and reinforcement just comes from life from doing things seeing things
right there is no labeled data set yeah maybe I should have said this from the very beginning
the whole idea of I call it experiential AI is that you know what makes you data you're you you
grow up as a baby and you play with things and you see things and you do things and that's the
data and the trick of reinforcement learning is how do you turn that kind of data into something
you can learn from and grow a mind up from so the the beauty and the limitation of supervised learning
is they say well let's not worry about that for now let's assume that somehow we have a data set
with labeled things and let's let's work on this sub problem that's a great idea work on a sub problem
figure it out and then move on to the next thing but really we have to move on to the next thing
we have to worry about how the the data set quote data set is automatically created from
the the training information there isn't ever a data set data set is is is such a misleading term
it suggests that it's easy to to have this thing and store this thing and curate this thing
really life is full of you do things things happen and then there's one you know everything is
fleeting you you don't have a record of it and it would be enormously complex and not only valuable
to have a record of it the the the feeling is totally different in reinforcement learning and
supervised learning and in particularly the way the way I would adjust it you know many people
do reinforcement learning by creating a buffer or a record of all the experiences that have been
been retained that have been occurred at least for some period of time and I think that's that's
uh an appealing but but it's it's not where the action the answer is the answer is
embracing the fleeting nature of data and and making most when it happens and then letting it go
well that's why you want to make an embodied system so that you have all the the five senses or
or more so you need you need as you say an embodied system an interactive system that that
influences its its input stream its sensory stream and that you get that interaction and for a long
creative time you can do this in simulation or you can do it in robotics there's still I still
know what's the best way or if the best ways do both and right or maybe first one and then the other
John is interested in uh having um uh learning from video and he likes his his his view of the
experience is you have massive numbers of video streams like you're viewing you know 500 channels
of television and then you can switch switch to look at one look at another one um uh other people
in in in keen my close colleague Joseph Modial he's uh interested in robotics and he thinks the
best way to get an appropriate in data stream is to actually build robotic hardware um
you know it's important that the world be large and complex because the worlds we want to address
are large and complex um and so you want things like video and you want large data streams um
now you can use simulations to generate even video streams simulated video but inevitably
those simulated worlds are really quite simple they have an underlying simplicity uh they have
objects perhaps and three-dimensional straight structure maybe they're rigid objects and the
vision is is is a very particular geometric form um they they are generated and they are
they are made up worlds and they are generated so they're they're really the worlds are are are
less complex than the agent uh their goal would be to have to spend most of the computer power
working on the mind and just a little bit to just create the simulated data and and that's that's
reversed the way it really is right every person is maybe has a has a complex brain but their world
is much more complex not just because the world consists of all these um physics and matter
but it also consists of other minds other brains and other minds out there and and what goes on
in their minds matters and so the world is inherently vastly more complex than the agent
and we we've reversed that when we work on simulated worlds so which is always concerning
anyway those are some of the issues in the trade-offs between working with simulations or with
physical worlds nonetheless you you need to develop the architecture and the algorithms
before you worry about the data data stream i would think yeah but you want to develop the
right algorithms and if you're working with the world it's not representative of of your target
world in an important way um it can be misleading but you're right and that's what we that's what
we strive to do you know i don't know if you know but i think of my own work is almost always
i want to focus on some issues so i make a really simple instance of that issue like you know a
five-state world and and i study the the hell out of it but i don't like try to take advantage of
its smallness you know i study algorithms that are in some sense even simpler than the simple world
and i i stress those algorithms and see what their abilities are so we always you know it's always
part of research is we we simplify the world understand it fully just like a a physicist might
you know make a simplified world with with a ball rolling down a ramp and it's it's a really
simple world and you'll try to eliminate the friction and you eliminate other weird effects
and just see things in their simplest form yeah have you um paid much attention to um
alex kendall's work at at wave ai do you know that company it's an autonomous driving company
they have a world model called gaya one um and it's it's it's similar to what yanlacoon's doing
it it you know encodes representations from from video from live video and then uh plans
uh based on those representations uh and and it can control a car uh from the representation space
it's actually pretty remarkable so let's talk about the world model and and what what kind of
world model would be appropriate for autonomous driving um so let me say some things that are
mistakes they're a natural seeming but mistakes in my opinion uh the mistake would be to make
like a physics model of the world or to try to make something that could simulate the world and
produce the video frames you don't you don't you don't want the video frames of the future
that's not the way you think um instead you think oh i could i could go to the market and
maybe there would be strawberries okay you're not creating a visual uh a video you're saying
you're like jumping to the market and then your strawberries could be you know different sizes
and positions and and and still uh there's not a video there's an idea that will happen if you go
to the market um so uh people have realized this like yon lakun used to talk about um generating
video of the future and then you realize it would be blurry and and now he realizes that you need
to produce outcomes of your model that are not like not at all like video streams and not like
observations at all they're like um they're like constructed states um that are the outcome of the
action okay so this is this is a very different from from a partial differential equation model
of the world and it's so it's very different from what self-driving car companies start with
self-driving car companies start with physics and geometry and uh you know things that are
calibrated by human understanding engineers understanding of the world and driving but
i suspect that's going to be i mean what do i know i'm not into self-driving i don't do
self-driving cars but i know that that um like tesla is and elon musk is and um so their goal
is to is to make some you know they they started like everyone else with engineering models but i
my understanding now is that they're building uh sort of more conceptual models um that are
based on the artificial neural networks okay and so rather than starting with geometry and
understood things they're just getting massive amounts of data and training it to make a model
we need a model that is at the level of high level consequences not at the level of low level
things like pixels and video so one way you do that is you're having state features that are
at a more advanced level you say oh this is a car uh rather than this is a uh a video frame
and um so and then basically it's as simple as you need abstraction in both state and time
abstraction in in state is like saying there will be strawberries when i get to the market
and abstraction in in time is saying oh i can go to the market and then in 20 minutes i will
be there probably and other things will be the same or related in natural ways
so we want to be able to think about i could go to the market you also want to think oh i could
pick up the coke can i could move a finger and that will have certain consequences these these
all these things that we know you think uh are vastly different scales going to the market is like
20 minutes um you know taking taking a new job you know might be a year uh deciding to study a
topic also might be a period of time we think and we analyze the consequences like you wanted to
meet with me today and you know we arranged it we set it up it was your your planning uh took
you know place over weeks and some cases months and and and we assembled the the the event of
this interview by by planning all that and exchanging mess high-level messages uh it all that
you know it's silly to think that that's done at the level of of of imagining videos that we might
see with our eyes or our audio is signals that we might hear yeah so we need models that are
abstract in time and state and um as a reinforcement learning person um there's a particular set of
technologies that i naturally turn towards to do that um the prediction is based on multi-step
prediction by temporal difference learning um the planning is done by uh dynamic programming
essentially value iteration but where the steps the are not low-level actions but they're called
options they're high-level ways of behaving with that that terminate so they're there are things
like going to the market and they'll terminate when you're at the market so you know at a certain
conceptual level it's clear where we want to go to me um with abstract models in time and state
built options and features
i don't know you we did write one paper recently put published an AI journal on the the notion of
planning using uh sub-problems on the stomp progression stomp means sub-task option model
and planning put all those things together and you can do the full progression from from the
data stream to abstract planning and that's that's what we're trying to put together yeah yeah and i
i sort of misspoke talking about gaya one about that model i mean it they they the input is video
it creates a representation and it plans and and and takes action in the representation
plans actions in the representation space you can then decode that into video to see what
what it's doing but but it's but you're not planning in the video space so the what what's
your ambition with with this you'll figure out the refine the algorithms the reinforcement learning
algorithms they need to be scalable once you have that uh then you move on and and uh start start
scaling them with compute and and uh you know following your roadmap or am i simplifying it too
much you know we want to understand how the mind works and then we're going to make a mind or some
minds or some mind uh amount of mind uh and this will be useful in all the ways in all sorts of
ways economically useful it'll also be useful um to to us to extend the capabilities of our own
minds if we can understand how our minds work um we can we can augment them so that they can work
better um yeah we're gonna the the key step is understanding and then there would be millions
of uses um i don't think it's going to be as simple as making uh workers sort of like slaves for us
to direct i don't think it'll be as simple as that um that maybe gives a lower bound on
potential utility our sort of our story for etkin is we say that um well if you suppose you
could make a virtual worker um this would be enormously useful um much of the work that we
all do from day to day is doesn't require a physical presence it doesn't require a robot
much of which we do is just shuffling information around we can do most things through through a
video interface um so why can't we make workers that are extremely useful by playing the roles that
people play in many cases that's that's that's sort of a lower bound and what can be done
i think much more can be done and there'll be much more interesting things to be done
and then this question of what should be done um yeah those are those are rich
philosophical questions and practical questions for the economy yeah uh the the i've seen your
third uh well and one thing on reinforcement learning and sort of supervised learning sort
of took over for a while now it's transformer based generative uh ai but uh during the supervised
learning phase uh the argument was that uh higher knowledge is all supervised learning
and and the it's still supervised it's still supervised in general the ai had large language
models they the the training information is the next token the next word and that's taken as
as the correct action the analogy you gave me was uh you know because the analogy that that's
always given is that you know a child sees an elephant the mother says that's an elephant
and the child very quickly can generalize and and recognize other elements elephants maybe it
makes a mistake and the mother corrects it and says no that's a cow and and that was always given
as an example of supervised learning but maybe it's reinforcement learning maybe it's the child's
reward from the mother praising him for remembering the label the point is that a child has
well-developed concepts classes concepts um before and then and then when it's you know when
its mother says that is an elephant uh there's already an extensive understanding on the child's
and a part of you know what the space is what the objects are and and this this the the thing
that that is being labeled um no the label is the least interesting part of that and the the
the child has already learned all the all all the other most interesting parts of of what it means
to have animals and moving things and objects in its world the label is the least interesting
part well first of all you're talking about agents that that could be virtual workers already
uh using reinforcement learning people are building agents and using large language models
and knowledge bases to you know carry out tasks knowledge based tasks uh so what you're talking
about is is more than uh linguistic tasks or knowledge based tasks you're talking about
of physical planning and physical tasks is that right the key thing is having goals and a lot
if you have an for example an assistant help you plan your day organize your day or do tasks for
you um i'm thinking it's very important that the system is able to have goals and is able to
understand your goals i think it's probably the most important part of an assistant is to understand
the purposes involved and um large language models don't understand don't really understand
purposes involved they will appear to a little bit um but the corner case has always come up and
once you spend a bit of time they're always and you're always in a corner case and so an AI system
is system that that after a bit does silly things and that don't respect the goals that you have or
that have been given to it um that's not going to be a useful assistant so i mean i don't want to be
critical of large language models um they're very very useful but it shouldn't be viewed as a criticism
to say that they're also at the same time have rather important limitations it's not a competition
in that sense are you concerned at all are you ascribed to the threat debate no i think the
i don't i don't uh i i think the doomers are they're not just wrong i think i think they're
blindingly biased the the bias is blinding them to what's going on basically AI is a broadly
applicable technology it's not like it's not like nuclear weapons it's not like it's not like a
bio weapons it can be used for all kinds of things and it's not it's not uh it's uh the way we deal
with such things is we we uh we try to use them well and there will be people that use them
for bad things and then you know this is just normal there's normal technology
is it can be used by good people or bad people the the doomers the doomers are just saying oh
somehow there's going to be it's going to be it's bad in the same way that nuclear weapons are bad
that they and that's just they're just blinded by that metaphor by the thinking that that the AI
will be out to kill them that's just it's just silly and i i don't i don't think well they the
doomers don't actually give coherent reasons for what they what they believe and so it's hard to
argue with them uh so maybe it's fair just to hold that they're they're biased and blind
i don't accept i don't accept an argument this is a proper argument so so where you say you're
maybe at stage four in the research car max says 2030 uh that's you know it's far enough out there
that maybe people won't remember in 2030 that he said 2030 uh it's always uh you know i've 2030
has been out there for a long time and it's it's it's uh you can't it doesn't recede it's always
been 2030 for the uh computer power reaching human scale um quantities yeah but anyway 2030 is is a
reasonable it's a reasonable target for us understanding everything that we need in order
to make uh a real mind yeah i'm good with that yeah as you you have to be ambitious
i've always said that 2030 is a 25 chance of of of achieving a real intelligence a real human
level intelligence 25 chance so probably not but it's it's a big enough chunk of probability that
that an ambitious person should work towards it and try to make it true and it does depend upon
what we do and not just the uh unfolding of the universe so we should we should try to do that
that that is a big the big thing that's happening right now is the the the public is coming to grips
with what it means for there to be for us to understand the mind and to have the ability to
create uh minded things uh and so that that is a big uh transformation it's a big change in our
worldview um and so we absolutely need all kinds of people to uh to help us help us become easy
and become have an understanding of what's happening as we uh achieve human level
designed intelligence that's it for this week's episode i want to thank richard for his time
if you want to read a transcript of today's conversation you can find one on our website
i on ai that's e y e hyphen o n dot ai in the meantime remember the singularity may be getting
closer but ai is already changing your world so pay attention

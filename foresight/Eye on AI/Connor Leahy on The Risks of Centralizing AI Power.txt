What this whole open AI saga has shown us is that, I mean, obviously we can't have something like
this being developed by just like a handful of, you know, weird people, unaccountable
billionaires in the Bay Area. This is actually something that I've been telling
saying to people for years now. Obviously, this is not the right governance structure.
So the weirdest thing I would say about this technology should be a tool. It should not be a
goal in and of itself. Hi, I wanted to jump in and give a shout out to our sponsor NetSuite
by Oracle. I'm a journalist and getting a single source of truth is nearly impossible.
If you're a business owner, having a single source of truth is critical to running your
operations. If this is you, you should know these three numbers. 36,025,1. 36,000 because that's the
number of businesses that have upgraded to NetSuite by Oracle. NetSuite is the number one
cloud financial system, streamlining, accounting, financial management, inventory, HR, and more.
25, because NetSuite turns 25 this year. That's 25 years of helping businesses do more with less,
close their books in days, not weeks, and drive down costs. One, because your business is one
of a kind. So you get a customized solution for all of your KPIs in one efficient system
with one source of truth. Manage risk, get reliable, forecast, and improve margins. Everything you need
all in one place. As I said, I'm not the most organized person in the world, and there's real
power to having all of the information in one place to make better decisions.
This is an unprecedented offer by NetSuite to make that possible. Right now, download NetSuite's
popular KPI checklist designed to give you consistently excellent performance, absolutely free
at netsuite.com slash I on AI. That's I on AI, E-Y-E-O-N-A-I, all run together. Go to net suite.com
slash I on AI to get your own KPI checklist. Again, that's net suite.com slash I on AI, E-Y-E-O-N-A-I.
They support us, so let's support them.
Hi, my name is Craig Smith, and this is I on AI. In this episode, I speak again with Connor Lehi.
He's the founder and CEO of a startup called Conjecture that's working on AI alignment.
Before that, he was one of the founders and leaders of a group called Eleuther AI
that built one of the world's first open source large language models. Connor is concerned about
AI safety, about where AI development is going, concerned about the push towards
artificial general intelligence, and has a lot of thoughts about what we should be doing
to control development so that we don't end up creating something that is harmful to humanity.
I talked to him particularly because I wanted to hear his thoughts on the open AI saga,
which highlighted for a lot of people the dangers of having such a small group of people
controlling such a fundamentally powerful technology today. I hope you find the conversation
as interesting as I did. So I'm Connor. I'm currently the CEO of Conjecture, an AI company in
London focused on AI safety and building architectures for AI systems that are understandable
and controllable, and various other things. I also do a bit of work in policy regulation,
public messaging, that kind of stuff. Before this, I was well known as one of the founders of Eleuther
AI, which was kind of one of the first, if not the first kind of like open source large language
models, research, building kind of groups. And technically, even before that, I was someone who
worked on open source GBT2 as I think maybe literally the first person.
Yeah. And so last time we spoke was after the release of chat GBT and GBD4, and
you were very concerned as were a lot of people and a lot of people continue to be about
releasing these kinds of models into the public before having fully explored
the safety issues or without having adequate guidelines. But it struck me during this open AI
saga that we have lived through the last week, that having this kind of powerful technology
in the hands of a handful of people who have different agendas and can't get along is in itself
a security issue. And I would think that having these models open source where everyone can
see the data they were trained on in particular, because even Llama, I've learned, doesn't
make public the training data. But having the data open for all to see and having the weights
of the models open so people can improve them or play with them or whatever. That to me seems
like a much safer path than having proprietary models. But I wanted to hear how your thinking is
involved on that. In the 1940s and in the early 1950s, the Soviet Union built a what's called a
closed city around what would be known as the Mayak facility. The Mayak facility was the largest and
one of the first, well I don't know if it was literally the first but it's one of the largest
nuclear facilities for the Soviet Union in their breakthrough development attempt to build the
nuclear bomb. To give a bit of a flavor for what Mayak and similar facilities that existed throughout
the Soviet Union were like, there was a program where if you got caught by the secret police
and you were being sent to a gulag for the rest of your life, you would give them an option.
Either you go to Siberia, work yourself to death for the rest of your life,
or you get sent to Mayak for only three months. And if you serve your term, you're free.
Sounds great, doesn't it? Well, no one survived the three months. So what happened, of course, is
that one of the first things that the Americans developed while developing the bomb is the HEPA
filter, which is a form of air filter that is powerful enough to be able to filter radioactive
material very sufficiently out of the air, making it safe for the workers. The Soviets didn't
bother developing HEPA filters. Not really. So a lot of people died. To this day,
Lake Karakai, which is near the Mayak facility, is one of the most radioactive places on earth,
so much so that it is said that standing next to it for an hour can kill a man. These are all
just some fun facts that surely have nothing to do with the topic we're talking about today.
It's a story about how some people who were pretty bad people developed something or working
on something pretty dangerous, and then other people got access to it, and also really bad things
out because even worse people got access to it. Now, from this, I don't conclude, oh, so we should
have just had everyone develop plutonium. That would have made it safer. This is not a draw from
this conclusion from the story. Now, nuclear power is obviously very different from AI in many
factors. So why even bring it up? So I can ask the question in the other direction, though.
We're talking about AI. We're talking about AGI. So let's focus on AGI. I'm not really interested
in talking about the risks of current-day models, like chat GPT or something. We can talk about
those, too. They are real. There are real risks from those, but they're not the kind where I
think we have to stop all publication necessarily. But let's talk about AGI systems.
What reference class does AGI fall into? Is it like open source? Is it like nuclear bombs?
Is it like something different? The reference class we choose forms our thinking around
something which is fundamentally none of those things. AGI is in nukes. AGI is in open source.
It's not Linux. It's something very different. So while it has things in common with all of those
things, AGI runs on computers. Linux runs on computers. And it has other things that come
with nuclear bombs. Nuclear bombs can kill everybody. AGI can kill everybody. Those are
things in common. Is it more like nukes? This is more like open source. At some point, we have
to actually drop down from the metaphors and into the actual models of reality. So from my
perspective, I think you're completely correct. What this whole opening AGI has shown us is that
obviously, we can't have something like this being developed by just a handful of weird people,
unaccountable billionaires in the Bay Area. Obviously, they're not acting in humanity's
best interest to no one's surprise. A couple of months ago, Sam Altman interviewed about this,
and he said, oh, yeah, the board can fire me at any time if I go about a mission. I think that's
important. And then they try to fire him, and he's back. And well, that didn't work. So this is
actually something that I've been telling saying to people for years now. I've gotten some disagreements.
He did disagreements, let us call them, with some of the people who were involved with the creation
of the board or in favor of the existence of the board. And the point I always made to them was
this obviously cannot control a charismatic billionaire political mastermind. Why the
hell would you think it would? This is crazy. And this is exactly what we saw play out. And
I'm not even trying to make a comment on Sam Altman, good Sam Altman, bad. I'm just saying,
obviously, he was just not going to just say, oh gosh darn, I guess the board said no more AGI for
me, I guess I'm going to stop. That's not how men like him work. And that's obviously not what was
going to happen. And you think the politically unsavvy nerds could write a document that would
convince someone like him to stop? No, of course not. So obviously, this is not the right governance
structure. I fully agree with this. But there's a great saying, which is that reverse stupidity is
not intelligence. If you take something stupid, and you take the opposite of it, it's probably also
stupid. And so the fact that this governance structure doesn't work for me does not say that
therefore there should be no governance structure. This to me does not follow.
Yeah, but open source, I mean, you are I'm sure very familiar with Yanlacun's argument that,
yeah, that open sourcing can can lead to some abuse by bad actors, but by and large,
the vast majority of people that will be working on an open source model contributing to it or
building products off of it will be doing so with, you know, not without nefarious intent.
And that the larger the open source community, the quicker it would be able to respond to
to bad actors or or misuse or or the more people available to to build guardrails and spot
of weaknesses and that sort of thing. So that that argument makes a lot of sense to me. I mean,
at the beginning, when the, you know, pause letter came out. And around the time that we talked about,
yeah, this stuff is too dangerous to be open source. But,
but I'm changing my mind. And I wanted to hear whether the this this episode has changed your
mind at all. I'll make three points in reaction to that. The first one is a story.
The second one is a heuristic. And the third one is a true observation from my own life.
So first, the story, the story is that the smallpox virus genome is currently online.
You can go download it. It's a small text file. You can just go download it to your computer.
Another fact of the story is that a couple of years ago, Canadian scientists
recreated an extinct version of smallpox called horsepox. They revived it. They may and it was
functional and viable and infectious. And they published how to do it. Do you think either of
those things are good? Now you can argue, well, if we have more eyes on the smallpox virus,
then something, something, you know, good things happen. But this isn't really a model.
So this brings me to the second point. The second point is offense versus defense.
The way technology is work is that some favorite offense, some favorite defense,
very few are symmetric. Most of the time, and most of the time offense wins. It is usually
easier to destroy than it is to protect. There are exceptions to this rule, for example,
cryptography is an interesting exception where defense is easier than offense.
But in most cases, it is easier to build a bomb than it is to build a reactor,
you know, a safe controlled burn. So all things being equal, you should expect that if you have
a technology and you distribute equally, that there will be more destruction. This is the default
case. This is what you should expect by default. Most technologies that are destroyed don't immediately
give you a way to defend against it. Developing vaccines is harder than developing bioweapons.
It is much easier to crank out a bunch of bioweapons and then you have to develop
vaccines in response to that, which is already super hard because, you know,
who knows how far the virus already is. So just because the technology is why the aspect does
not mean it defends wins. Whether offense or defense wins is a property of reality. It is
not a property of your morals or of your ideology. And the third point is an observation from my
own life is that I used to work in open source. I was one of the very first people to work on it.
And I had similar views to Licken and, you know, assuming he holds these views genuinely,
which, you know, I hope he does. I don't know him very well. I talked to him maybe once.
And I think this is just not even wrong. It's just in my experience, what happens when you
build AI models and you release them open source is that the first thing that happens to get
uploaded to Huggingface, and then a guy called the bloke, that's literally his name, uncensored
them, undoes any RLHF training or other security training run have done, trains them all the newest
data to make them more powerful, more general, more whatever, uploads them again, 4chan downloads it,
down, you know, uses them for whatever their applications are, whether it's, you know,
pornography mostly, or likes BAM or whatever, et cetera. And now maybe this is fine, right?
Like, you know, you know, maybe we say, oh, it's okay. If people want to use their LLMs for porn,
so what? That's okay. Sure. What I'm saying is, is the empirical observation is that the amount
of effort that gets put into making these things safer or more controllable is absolutely pathetic
compared to the amount of effort that the open source community puts into making these things
more powerful, more general, and less controllable. This is just an empirical fact. This is just
actually, if you go online, you pick the top 1000 LLM repos, how many of them are about controlling
the models better versus making them faster, making them more efficient, distilling them,
making them more, et cetera. And the fact is that the offense, like the unbalance here is like,
it's not even funny. And I understand, right? And this is not to say that the people working on this
technology are like morally evil. I think this is an important thing to understand. There's an
incentive from people like Lacan and other like big tech, you know, people, like talking heads,
to try to focus on it's only the evil people's fault because that absolves them of responsibility.
Meta wants open source because it absolves them of responsibility as a corporation.
They can't get sued because it owes the user's fault. And this is also what's happening in the
EU AI act right now is that people like Lacan are lobbying to remove foundation models from
regulation in the EU and saying instead of their uses should be regulated. This is the same thing
as when, for example, plastic companies invented recycling. They invented it so that it was the
user's fault that there is all this plastic pollution. Like, oh, see, we would have recycled
it. But unfortunately, the users just didn't do it. This is a, this is gas lighting. And this is a
complete unbalance of power. The externalities of plastic pollution should be on the ones who are
most suited to addressing this externality, who are creating this externality. It shouldn't be on
the user. And the same thing applies to foundation models is that these systems can do things. They
can be used for many things. And we should be taking the big companies building these systems.
Is that what these open source models are being built by like, you know, plucky little teenagers
in their, in their, you know, rooms as a plucky teenager that did do that. I'm saying most of
the ones being built now are being made by like the UAE and meta. Like, these aren't the little
guys. These are big guys trying to shirk their responsibility to society.
Well, then what, what's the lesson from, from the open AI saga that, that you just need a
bigger board or you need a lesson is that none of these structures are correct. This is what we
have governments for. This is the same lesson that we've had over and over again. It's like
self-regulation does not work. It has never worked. Self-regulate. This is like tobacco
companies self-regulating themselves. This does not work. And we as a society have developed a
mechanism. I'm not saying it's a perfect mechanism by any means, but we do have a mechanism for
intervening in systems that have extreme high externalities that are not self-regulatable.
And it's called the government. Yeah. And, and I mean, there has been a lot of work at the government
level, not as much in the US as in Europe. But, but how, how do I mean, obviously these
models are so commercially, the potential is so commercially
exciting that fines aren't going to matter. You're not going to be able to find people
to behave in ways that the government wants them to. There's got to be something stronger than
that. So, have you thought about, about that? I mean, how do you regulate these things?
One of my, one of the most inspiring moments from the history, I think of science and society
is many decades ago, biologists and chemists and so on realized that human cloning should be possible.
Like it should be possible to do this. They were still far from having the actual technology
to perform with human cloning, but they found out it should be possible. And they reasonably
understood, wait, that might be really disabilizing. Like that could be, we don't know what the
consequence is, but maybe it's great. You know, maybe there's, you know, there's many benefits
from human cloning as well. But like, let's chill out. We don't know, like we don't know,
and this seems huge. This doesn't, isn't just like another thing. This is not a 10% more effective,
you know, cough drop. Like human cloning is a big deal. And so, heroically, long before the
technology existed, they came together and banned it and said, let's have a moratorium.
Let's not do this until we've had a bit more time to figure out what the hell we as a society
want about this. And it wasn't one board. This wasn't one CEO being like, I will, you know,
take a moratorium on this. No, it was the scientific community and governments coming
together and working very, very hard to create a moratorium. A moratorium is what we do when we
are faced with something which we know is huge and we don't know how to deal with. That's what
scientists do. You have a moratorium. And we should have a moratorium on AGI. This is what we
need to do. And can you enforce a moratorium? Yeah, I mean, it's like technically, like physically,
like, yeah, obviously, like, that's not that hard. Whether people will do that, whether people
want to do that, whether people can overcome the incredible political power that big tech has,
that's the more interesting question. It's not like the government obviously has the ability,
like the CIA can track every GPU in the country if it wants to. Like, you know, if you want,
if the NSA wants to shut down, just press a button. Like, that's not the problem. You know,
if you want to throw a couple CEOs in jail, like, sure, like the FBI can do that. Like,
physically, this is not a problem. It's a political problem. This is not a physical problem. This is
a political problem. The political problem is, well, if you have legislation around this kind of
stuff, well, we just saw what happens if you try to fire Sam Altman, you think he's going to be okay
with a huge GPUs away? Well, no, I expect that's going to be a hard fight. I expect, you know,
Microsoft lobbyists will fight that tooth and nail. I expect many people will fight this.
And this is why, like, you know, I'm not, I'm not here to give point to you,
paint you a rosy picture of the future. I'm not optimistic that things are going to go well.
We have an unprecedentedly huge political problem here. I think I'd like to say is the thing that's
killing us right now, it's not AGI. AGI doesn't exist yet. It's people. It's politics that is
killing us. Right, right. But and to that point that AGI doesn't exist, not so much all the other,
I mean, yes, no doubt, the political systems are not equipped to deal with the big problems facing
humanity. But in this case, AGI doesn't exist. I don't know how you would ban AGI, because
no one really knows how and when it might emerge, if it ever does. At the level of the tech now,
I mean, what are you, what are you suggesting? And I'm not putting on the spot. I don't expect you
to. Oh, I have policy proposals. I have very concrete policy proposals here, here are three.
The first one is a compute cap. There should be a limitation that no single training around no
single AI system can be built with more than a certain amount of compute. So luckily, we are,
so we are very lucky that current frontier AI systems, more and more general purpose systems
require more and more computing resources. These computing resources are very easy to track.
They're very bulky. They take lots of specialized knowledge, lots of energy. The kinds of supercomputers
that can train a GPT-4 or GPT-5 are only built by like three companies in the world,
and they're all in the US. So like, this is a solvable problem. And we should put a ban on,
you know, there should be like a registration process for, you know, frontier models up to a
certain limit. And beyond that, there should be just ban, just a moratorium. Just you are not
allowed to perform any experiment that requires more than, I don't know, 10 to the 24, 10 to 25
or whatever, FLOPs. FLOP being a unit of measurement for computing power. And this is easily enforceable.
This is absolutely something that like technically is enforceable with, it's just a political
problem. And this buys you time. Then you're, our scientists figure out, you spend time actually
figuring out how far is AGI away, how dangerous is it, how do we control the things, blah, blah,
then we can talk about those kinds of things. The first thing is to buy time. The second
proposal, or unless you want to comment on that. Well, just on that, you're talking about limiting
commercial products. But if, when you say then that gives the research community time to figure
these out, things out, they're going to have to experiment with larger models. So there's got to
be some. To be clear, these levels are insane. 10 to the 24, 10 to the 25, FLOP is an unimaginably
large amount of computing power. There are no academic labs, basically, that need this for
research, FD research. This is ridiculous. There is just no, so this is a common propaganda piece
that the big labs like to say is like, oh, we need more compute to do safety research.
Maybe this is true. I have not seen it. This is just not what has actually happened. Just purely
empirically speaking, there is, I have seen basically no safety AGI relevant research that
required more than like, you know, a GPT-3 that you couldn't have done with GPT-3 level of compute
or less. I have like, maybe it exists, but I sure as hell as I have not seen it.
Okay. So limiting compute is one proposal. What are the others you mentioned?
Two others I would recommend. The second is strict liability for model developers.
So what this means, so strict liability means that the intentions of the developer do not matter.
It would matter is that if a harm is caused, the developer is liable. I think this should
basically exist for the whole supply chain is that if you create externalities, you have to pay for
them. And this aligns the incentives of everyone aligned on the chain. Currently, there are no
incentives for developers to develop to minimize the externalities of their systems. Currently,
you as an open source developer can be an arbitrarily dangerous thing that causes arbitrarily
much damage. And you have no incentive to avoid this. As a concrete example, which not even going
to AGI is voice cloning systems. There are right now in GitHub, systems you can just download,
which take you 15 seconds of your voice, clone it perfectly. And you know, go call your kids,
call your wife, you know, just manipulate them, call in a swat hit on your, on you using your own
voice. This is all doable. And the people developing these systems have zero liability.
They don't even feel bad about it. Because it's open source, Craig. If it's open source, it must
be good. My ideology says so. And you know when your ideology tells you something is morally right,
then it's good as we've seen throughout history. So it's, so we have to align incentives here
somewhere along the line, you know, if a, it reminds me of cars and seatbelts in the 70s, where
car manufacturers fought tooth and nail to not have seatbelts. They fought it viciously with
propaganda and with lawsuits and with everything they could throw at it. Because they said, well,
it's the driver's fault. If he gets into an accident, it's not our fault. Like, you know,
we just build cars. If they drive it poorly and they die, well, it's not our fault. And we, you
know, the people rightfully told them to go fuck themselves. Like, no, you have to build a safe
product. You can't like, it's, it's not a moral question. It's kind of like the point I want to
make. I'm not making an ideological point. I'm not saying my religion says that seatbelts are good.
I'm like, I don't care. I care. Do seatbelts mean that less people die? And the answer is, yeah,
like they make cars safer. So then I want seatbelts. Cool. And the same thing applies to open
source. Does Linux being open source result in more safety? The truth is, yeah, looks pretty
obviously like case. So I'm in favor of Linux being open source. Awesome. Great. You know, does,
you know, some seven billion parameter model be open source positive or negative? I don't know,
probably positive. Like probably so. I'm not sure. Like there's a lot of downsides there as well.
But like, seems like it probably is positive. AGI being positive, you know, open source, you know,
that does not seem positive to me at all. That does not, that seems like a recipe for disaster.
So it's, I'm not trying to make an ideological point is what I'm starting to say. I'm not saying
all these things are good. All these things are bad. I'm saying we have to look at things
at a case by case basis. This is how proper regulation works. Proper regulation shouldn't
be ideological. It shouldn't be everything is regulated as ARB. That would be terrible regulation.
Yeah. Well, so that was the capping of the compute on training runs,
shifting liability to the model developer. What was the third one?
So the third one that I think should be done is that there should be a kill switch.
And what I mean by this is it doesn't have to be literally a switch. What I mean is there should
be a protocol that any developer of frontier AI systems needs to implement by which at a given
notice, any frontier training runs or deployments can be shut down in under a minute. So the reason
for this is not per se because I need, I think necessarily that this would be very helpful.
The AGI actually happens. If AGI actually happens, this is probably useless. The reason I think this
is good is because we should have the institutional capacity to do these kinds of things. There should
be every six months, there should be a fire alarm. There should be a fire drill where everyone has
to practice. In the next five minutes, all AI companies have to go offline for 60 seconds.
If not, you get slapped with a huge fine. These are the kinds of protocols you want to have
in worlds where you have tail risks, where things can blow up, where you can have these
kind of things. And then there should be a multilateral K of N kind of system around this.
Like maybe all major global powers have one of these buttons and if three or five of them push it
or seven of 10 or whatever, then the system kicks in. This is the kind of institutional
building which doesn't save us, but it's a hell of a lot better than nothing.
And how do you see these kinds of proposals moving through the policy making frameworks?
There is some advance in the European Union. The White House has come out with its
executive order, which as yet doesn't have any real concrete
government governance policy in it, but it sort of lays out the things that we should
be thinking about. Where do you see these things going? What sort of a timeline do you think that
governments are being educated enough that they can deal with this? What government is
going to lead? Is it the EU? Will it be the US? Who should it be? And then of course you've got
the other side of the world, Russia and China, who have very different agendas and may not want
to regulate at all. So when people ask me questions like this and they're like, what's your probability
of X happening? And then my follow-up question is usually, is it X conditioned on me and other
people doing something about it or not? Because I expect if they're conditioned on me and other
people don't do anything about it, then yeah, I just think nothing will happen if big tech wins
and then we die. I think it will be very heroic or special. It will just be new products keep
happening, AI keep going up, and then just one day, humanity's not in control anymore and we
have no idea what's going on. And then it's just over. I don't think it will be dramatic. I think
we will just get more and more confused. We won't understand what's going on anymore. Weirder and
weirder things will happen, more and more politics, economics, markets, media is controlled by AI,
or even just fully generated by AI. There will be no more movies or just AI generated. And then
just humanity will not be in control anymore. And then one day we fall over dead for some reason,
we don't understand. That's what I expect will happen by default. And along the way to be clear,
big tech will pick a lot of money. So go buy that Microsoft stock. You'll get really rich
just before you die. So if I could addition on someone actually doing something about this,
I do think there is hope. I don't think there's a lot of hope, but there is hope. And the main
hope I see from this is that the general public fucking hates AI. It's unfathomable how much
normal people hate AI. They use it, of course, but they're freaked out by it, which is just completely
the correct reaction. It's just these crazy bizarre weirdo tech people like you and me
who are not instantly like, wait, that's actually, let's not do that. If you talk to any normal
person, you're like, hey, these people are building systems that are smarter than humans.
They don't do that. That seems really dangerous. Don't do that. Well, all the people are like,
oh, but actually you see my proposal because we'll make it fine. Or actually universal love
means that AI systems will love whatever. I don't even know what these people say anymore.
I think they've given up making arguments at this point and they're just vibing.
So I don't even know if there's an argument that debunked there. So from my perspective,
it's we are building systems. They are going to be built by default unless we do something about it.
So the general public wants these systems to not be built, or at least for us to slow down,
until we can make them safe and we understand them better and they've been integrated to society,
et cetera, et cetera. So now you might ask the question, okay, well, that's true.
Why is fuck all happening? And that's a good question. And now we have to talk about models
of policy change and like global coordination, which at least how I think about this problem
generally is that the general public actually does have power in the West and like in democratic
countries. It's very fashionable among elites to sneer and be like, Oh, actually, you see the
populace, you know, they don't have true control, you know, we live in a whatever the words are
that people like to use. And this is to a large degree true, but it's not fully true.
The main problem is that the general public has extremely short attention spans
and extremely discoordinated. This is the main problem. The bottleneck on policy action currently
is not will of the people. It's not ability to enforce regulation. It's coordination. It's
getting people to actually do something about it, you know, to actually write letters to their
senators, actually put things on their desks, actually yell at them on the phone, you know,
actually like, you know, talk about on social media, et cetera, et cetera. This is the kind
of thing that's currently missing basically campaigning. This is the kind of stuff that
is missing. And I expect that if you did this well, if you raise this to saliency about people,
you wouldn't have to you wouldn't have to convince them. And I'm saying this because
empirically, this has been true in my experience, like talking to people and also like doing stuff
like focus groups and stuff. I found that you don't really need to convince people very much.
You mostly just have to tell them facts just have to, you know, just like present them with,
hey, this is what's going on right now. And then mostly they converge to the like a reasonable
beliefs around like, hey, that's scary, don't do that. So I think this is currently the best
path we have. I'm also, you know, excited to talk to politicians and I talked to many of them,
mostly in the UK and the EU, because I'm UK based. But it's hard because, you know, politicians have
similar problems. They have very little attention span, because they have so many things they need
to do. There's so many things haranguing them. And my model of policymakers is basically that the
ultimate goal of a politician is to not get blamed. So it's because the politician you have
really like I have so any if there's any policymakers listening or any staffers or so on,
I feel you, you're in a shit spot, I get it. Because like, basically the way I see it is like
there's like a two by two grid of like what you do as a politician, which you can do. So
the idea is that there's a default action is that in a common, in our common, you know, feelings
around issue, there's something that is the default thing to do, which is usually nothing.
If you do the default action, and it goes wrong, well, you're not blamed, you know,
because, you know, you did the sensible thing, not your fault. If you do the default action,
and it goes, well, well, great, you're a genius, you know, good job. If you do the non default
action, and it goes great, cool, yeah, you're good, great. If you do the non default action,
and it goes bad, then you get blamed. That's how you get blamed. So you may notice from
this payoff matrix, that it is always better to take the default action rather than non default
action. It is always better for the politician to not stray off the path. And this is universally
true. So it's easy to yell at politicians and be like they have no spine, they have no courage
and whatever. And yeah, that's true for many of them. Many of them are just, yeah, just, you
know, just don't care, true. But some do, and they do go off the path and they get burned for it.
And that sucks. But it is how the game is. So what we can do as the people is we have to change
what the default action is. You have to change the narrative from, I guess we just keep bumbling
along until we die to how the fuck dare you keep bumbling, like seize your bumbling immediately.
Bumbling is no longer accepted. And that's my biggest hope at the moment.
Yeah. When we spoke last time, again, right as GPT four was being released.
One of your immediate concerns was that these things can be hooked up to
systems that can take action. And I don't remember if we talked about auto GPT that first,
I haven't looked at what's happened with that, but that first attempt to create an agent that could
use LLMs. But that has developed a pace. And we're now on the cusp of seeing
sort of an explosion of AI agents that can leverage the power of large language models or other
other tools. I had a guy on earlier from News Guard, a company that builds databases to
try and help companies, tech companies identify
disinformation and combat it. And we were talking about, once you have these agents building
creating disinformation, not only creating the disinformation, but distributing it on a massive
scale and maybe on a massively parallel scale. The internet, public discourse, everything is
going to get very confusing because you're not going to be able to tell what's real and what's
not real and people, which is the majority who are not particularly careful about where they're
getting their information will be manipulated. So yeah, the coming AI agent era, how do you
deal with that? I mean, I don't know, get your affairs in order. A number of years ago, post
GPT-2 was around GPT-3 time. That's how we mark the eras now. Instead of years, we just use GPTs
now. I was invited to work kind of like just like a discussion group with some open AI people,
policy people, disinformation experts and stuff like this about the potential for misinformation
and so on from language models, especially before GPT-4, before chat GPT and so on. And
polite lead to all these well-credentialed experts with their triple Stanford professorships or
Harvard, whatever, talk about misinformation, bias and whatever. And then when it came my turn
to talk, my reaction was like, holy shit, you're all so undressed. You're being so optimistic.
It's so much worse than any of you. You're like, oh, it could make it easier for far writers to
that's that's that's fucking children's play compared to what you could do with these things.
Like you were truly you're not creative. Like if you think that's the worst that can happen,
oh, they're going to generate some fake news and some like Russian digital websites. I mean,
oh boy, that would be nice. That's the nice timeline. It's going to be much worse than that.
It's already getting worse like that. Talk about fully automated cults with fully
automated profits. Talk about all sensory, illusionary interactive systems, creating
full complex narratives that are completely disconnected from reality. Talk about full
epistemic collapse, the semantic apocalypse. Even if AIs don't kill us, they're going to drive us
insane. So it's because it will just be harder and harder and harder to survive in a more and more
adversarial informational environment. This has already been happening for a very long time.
You know, we just had Thanksgiving. And as much as we love her, we all have that one
aunt that get way too into QAnon a while back. And imagine so, you know, currently stuff like
QAnon or like, I don't even know if QAnon is still a thing, but like whatever the newest thing is,
the newest cult is, the newest whatever is, you know, that affects, you know, some percentage of
the population, you know, some percentage of the more vulnerable population. I'm going to say
stupid, just like, you know, maybe emotionally vulnerable or like epistemically, you know,
vulnerable and for some reason, not trying to judge these people here. Now imagine the bar keeps
racing. You get systems that become more and more convincing, that become more and more
sophisticated, more and more targeted, and slowly, slowly, the number of people who are just
functionally schizophrenic keeps going up until at some point, people cannot converge on reality
anymore. And people just every person you meet is functionally schizophrenic. You cannot run a society.
You cannot organize a system if you and your neighbor cannot come to a conclusion about
basic reality. This is like what is possible with these kinds of systems. I'm not saying this is
going to happen next year. I mean, maybe, but this is the kinds of things you couldn't do.
Like the like epistemics is hard. Like this is the thing that like, there's also things like honesty
is hard. This is like some people are like, Oh, just, you know, misinformation is a trivial
concept. It's almost become a slur at this point. It's kind of come a joke, you know, like when
people use the word answer information, like, at least in my social circles, a lot of people like
rolled their eyes to be like, Oh, yeah, anything that isn't big media isn't this information,
whatever. But like, it's just not that easy. Like finding out what is true and disseminating and
evaluating what is true is hard. This is very hard. It takes energy. It takes effort. It takes
mechanisms. It takes like it's hard. And it's going to get harder. It's going to get more expensive.
But currently, like, do you really know what's happening in Ukraine right now? Really? I don't.
I think I'm at a point where it is like literally impossible for me to actually know what's going
on in Ukraine. It's something that affects me, you know, affects family, friends, you know, it is
a huge thing. I don't think that there is any way I could actually acquire and verify
that the truth of what is actually going on there. And this generalizes. This is even before we get
into agents doing worse things than this. I mean, automating all jobs, obviously, you know,
anything you can do at a computer, an agent will do better and faster. So there will be complete
economic collapse from that. Like, obviously, there will be no more need for human jobs unless
until the inference costs, you know, get too high. But you know, you can improve those back down.
You'll have systems that can do harm in various ways, you know, by manipulating markets,
campaigns, politics, you're going to have systems that are, you know, cybercrime, hacking, you
have system like it's like when you ask a question like, what is the worst thing agent based systems
are doing? You're asking the question, what are the worst intelligence systems can do?
What is the worst that a human can do? The answer is a lot.
Yeah. But again,
yeah, I mean, you can you can see that that very bleak future. But I'm also a great believer in
in how
mankind, the worst case scenario generally is not what happens. And people kind of muddle along.
But that survivorship bias, there was a man named Stanislav Petrov, who was a Russian soldier
stationed in a nuclear bunker. And he had the command that if American missiles appear on the
screen, he shoots the missile. And one day, six missiles appeared on his screen. His commands
were very clear. The second guy with him there who had, you know, the other key was ready to
turn and yelled at him that it's time we have to shoot back. The Americans are attacking.
And Stanislav didn't. He disobeyed orders. He could have been, you know, fucking executed for that.
And he disobeyed orders that day. And it's because of this one man, one Russian soldier,
that you and me weren't nuked. One guy, we got lucky. So when people said, oh, but so far as
I was like, what the fuck are you talking about? This is like saying, well, I've played Russian
roulette five times so far and it's been great. Let me pull again. That's just not how anything
works. This is not how reality works. If you play like this, then eventually you predictably lose.
You have to play strategies where you can win in adversarial environments where you can play,
where you can win in games where dangers exist. Our ancestors, when they were in the wild,
they couldn't be like, well, oh, my forefathers survived. So I don't have to worry about bears.
You know, none of my forefathers got killed about bears. No, like that's just, no,
this is not how things work. The world isn't nice. There is no arc of history. There is no God
that is protecting us. The fact that we are here today is because of the hard work of our ancestors.
The fact that I live in this nice, you know, warm apartment, sound like safe that I have enough food
to eat and so on is not God that gave me that. It's not some, you know, force of nature. It was
the hard scrabble and bloody fight of my ancestors that left me this. And if I let this to rot,
if me and other people don't maintain society, then it just dies. Like then entropy wins.
Entropy always increases and entropy is death. So if we just sit back and hope things will go well,
they will not. So, you know, I was gonna, I was thinking, well, that's a good place to end it,
but I don't want to end there because our last conversation got an inordinate number of views.
And I have some producers that take these and turn them into shorts and they have these sound bites
from that episode that have gotten an enormous number of views because people gravitate towards
these doomsday proclamations. And I don't, I mean, whether or not they're true, I want to end
something more hopeful. So what, what, what should people do in your view? What should
regulators be doing? What should researchers be doing? What should Microsoft be doing now?
So the weirdest thing I would tell them is like, to be clear, I don't like being the doomed guy.
I absolutely don't like this. I was the pecto optimist throughout my entire life. I was always
the person saying, no, we can fix problems. Climate change is solvable. You know, solar
powers can be exponentially cheaper. You know, we can do carbon capture with like, there are so
many things we can do. I've always been saying like, no, like, you know, see how in the interest
improved education, how much people are becoming, you know, better at, you know, and having more
access to information. Look at how so many things are like, I was just reading the other day about
how slowly over decades, just the flash freezing of frozen food has gotten better. And I've noticed
this, just like my frozen broccoli, I'll make it night. It's just a little bit nicer. And you
know what, that might sound like a teeny thing compared to all these other things, but I think
that's beautiful. I think it's extremely beautiful that life gets better. All things being equal,
life has gotten a lot better. I'm very happy to be alive when I am right now. All these small things
done by these smart people, mostly done for profit. Sure, the broccoli company, they just want profit,
but ultimately, they made my dinner a little bit nicer. It was already fine. Like I was already
surviving, but it was a little bit nicer. And you know what, that's awesome. And it's so nice
that we can live this way. The truth is, is that we are so lucky that we live in a society full of
educated, smart people that for the most part, you know, not all over more angels, they're not heroes,
but they want to make, they do want to leave the world better, you know, they want people to be
happy, they want people to be safe. Most things being equal, you know, almost everyone, you know,
given the option, if they could just help someone else and it didn't cost them anything,
they'd do it. And that's really nice. So we have to leverage this. We have to leverage that we,
and this is not the case everywhere in the world, I want to say. This is something that even today
is not in every country. It is not in every place or in every society. But in the West and, you know,
many other countries in the Far East and so on, most people are educated. Most people are decent.
Again, I'm saying they're great or heroes, but they're decent. And they want the world to go well.
They want their kids to grow up and have a nice life and, you know, eat nice frozen, you know,
broccoli, you know, whatever, you know, they want to see art and beauty and, you know, music and so
on. And we can have this. This is the important thing to understand. The important thing is,
sometimes I'll talk about this, is that like this, this idea of techno optimism, quote unquote,
it's just cynicism and disguise. This is a really important thing to understand.
These people who put, who's talking about, oh, yeah, actually, we're, we're techno optimists,
we're accelerationists or whatever. They're just cynics. They're just libertarian cynics
that don't believe that society can be improved, except by just like giving themselves to this
abstract process of technology. But technology is not a force of nature. It's not a thing
happening to us. It's a thing that we do. It's like, it's about humanity. It's not about technology
that like, sure, technology is great, it's helped humans, but I only care about technology because
I care about humans, care about people. And we all care about people. We care about our families,
we care about our friends. And technology should be a tool. It should not be a goal in and of itself.
So when people talk about, well, AGI is inevitable, someone's going to do it. No, no, it is not.
It is not inevitable. It is not a force of nature. It's a decision we make. It is a decision we make
and we can do better. We can, as people, societies, as civilizations, make choices. We can say, hey,
let's be a little more careful. That doesn't mean we'll do not do any AI anymore. We can just say,
hey, give our scientists a couple more years, a couple more decades to understand the mathematics
of interpretability better, and then maybe we'll give it another shot, you know, like we did with
human cloning. These are what's important. I'm not saying that this is easy or that this is what's
going to happen. It's because it's not what's going to happen by default. But it's just important
that there is this poison in our society that believes that the future is already decided.
And it is not. The future is not yet decided. We still have a choice. It is not yet too late
for what it will be soon. Hi, I wanted to jump in and give a shout out to our sponsor,
NetSuite, by Oracle. I'm a journalist and getting a single source of truth is nearly impossible.
If you're a business owner, having a single source of truth is critical to running your
operations. If this is you, you should know these three numbers. 36,000, 25,1. 36,000,
because that's the number of businesses that have upgraded to NetSuite by Oracle. NetSuite is the
number one cloud financial system, streamlining, accounting, financial management, inventory,
HR, and more. 25, because NetSuite turns 25 this year. That's 25 years of helping businesses do
more with less, close their books in days, not weeks, and drive down costs. One, because your
business is one of a kind. So you get a customized solution for all of your KPIs in one efficient
system with one source of truth. Manage risk, get reliable, forecast, and improve margins.
Everything you need, all in one place. As I said, I'm not the most organized person in the
world, and there's real power to having all of the information in one place to make better decisions.
This is an unprecedented offer by NetSuite to make that possible.
Right now, download NetSuite's popular KPI checklist, designed to give you consistently
excellent performance, absolutely free at netsuite.com slash I on AI. That's I on AI, E-Y-E-O-N-A-I,
all run together. Go to netsuite.com slash I on AI to get your own KPI checklist.
Again, that's netsuite.com slash I on AI, E-Y-E-O-N-A-I. They support us, so let's support them.
That's it for this episode. I want to thank Connor for his time. If you want to read a transcript
of the conversation, you can find one on our website, I on AI. That's E-Y-E-O-N-A-I.
As I always say, the singularity may not be near, but AI is changing your world, changing it rapidly,
so pay attention.

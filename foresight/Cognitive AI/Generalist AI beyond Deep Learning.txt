Welcome everyone from and hello from Rainey, California.
Today we're hosting an exciting event
on the future of General SDI.
Our guests today are professors
Christoph Fondrell-Malsburg and Michael Levin.
Michael Levin is a distinguished professor
at the Department of Biology
and the principal investigator at the Levin Lab
at Tufts University.
Christoph Fondrell-Malsburg is a senior fellow
at the Frankfurt Institute for Advanced Studies
and the visiting professor
at the Institute of Neuroinformatics at ETH Zurich.
And finally, our host today is Dr. Yoshabach,
my colleague and the principal engineer at Intel Labs.
My name is Tania Greenberg.
I am an AI scientist at Intel Labs also.
We would like to express our thanks to Intel Labs
for sponsoring and hosting this event series.
So without further ado, let's get started.
The audience is going to be muted by default for this event.
If you would like to ask a question or make a comment,
please post your thoughts in the chat.
You should have access to it.
We will have a hopefully longer than usual Q&A session
at the end of today's session.
So we should be able to go through many of your questions.
All right, Yoshab, the floor is yours.
Okay, I proceed with our slides for now.
Let's see.
The question that we want to discuss
is how we can build intelligent systems
that move beyond the present limitations of deep learning.
And this is quite a bold proposal
because you don't know if deep learning has any limitations.
At least I don't know.
I don't know any proof that demonstrates
that there is a certain thing
that the methods of deep learning cannot do.
It seems to be that the present generation of models,
which is widely successful,
generative AI is built on the transformer algorithm,
which is a variant of the deep learning mechanisms
that have been existing since the perceptron.
And the limitations of this thing are not clear, right?
We basically get to meaning in the limit.
We need to use massive data and compute to get there.
And these models have difficulty
to become fully coherent,
but it's not obvious that such a limitation
cannot overcome as a loss function
or this just using the existing regimes
and scaling them up even further, right?
We cannot prove that there's a combination of codecs
and the existing methods and some real time
and some online learning.
We won't be able to build a system
that is a good enough artificial general intelligence
and is able to discover the next generation
of algorithms for us.
And this idea that deep learning
in the way that it exists
with basically the algorithms that we have right now,
with slight changes to the loss functions and so on,
that it is sufficient.
This is called the scaling hypothesis.
On the other hand, most people who work in the field
have the sense that there is a problem with deep learning
in the sense that it's boot forcing the job.
It seems that biological organisms
are able to make sense of the world
with much, much less data and dramatically less compute.
And there is of course a contention
about how much compute a brain has
and how much compute an organism has.
And very often the calculation is made in such a way
that we discuss how many GPUs you need
to emulate a group of neurons.
And depending on how you look at the neuron,
the neuron might be something like a four or 12 layer
neural network or it might be something even more complicated
if you look down to modeling every synapse and so on.
And on the other hand, relatively rarely we ask
how many brains you would need to run macOS
because the way our nervous system works
is highly redundant and stochastic.
And much of what the individual neuron is doing
is probably only relevant to the individual neuron
in its survival itself,
because after all the neuron is a single celled animal.
So if you take the job of an individual,
for instance in a corporation,
that individual is going to contribute
a lot of its intellectual capability to the corporation,
but it's going to be a tiny fraction
of the total ability of the individual
that it needs to survive by itself
and to communicate with its immediate neighborhood.
So just asking to emulate a single neuron
and then multiplying this with the number of neurons
is probably not the right way to do it.
And it's not clear how many neurons
you would need to run MNIST.
As far as I know, there is no simulation so far
that is using a close neural model
or that is using a small group of neurons in vivo
that are being trained to do MNIST.
More generally, I think we need to answer the question
what intelligence is.
And for me, this question has evolved over the years
and my current answer is that intelligence is the ability
to construct a path through a space of computable functions.
It's a slightly fancy way of saying
that intelligence is the ability to make models
because at the end of the day,
models are computable functions
that are designed in the service of control.
So we have a system that is making some kind
of regulation task
or performing some kind of regulation tasks
and when it's controlling the future, it's an agent.
And to control the future and being an agent,
you will need to construct some kind of control model
that is counterfactual.
You need to be able to represent states
that are not here yet
because the future is not there.
And that means you need to have a system
that is able to perform arbitrary causal transitions.
And this causal insulation from the substrate
to represent something that is not the case yet,
that is basically what is a computer.
And for the computer it does, it represents functions
which means mapping from state descriptions
to other state descriptions.
And the computer can do this in completely arbitrary ways.
And this is the basic idea
of building a system of computable functions.
And when you come up with a way
to enumerate the computable functions,
to list them all, then you can search them.
And if you want to learn something,
you need to in some sense enumerate,
organize the space of computable functions
in such a way that you find those functions
that you are interested relatively early on.
And this question of how you can construct a space
for the computable functions that is tractable,
that is able to converge to a solution
for the problem at hand.
That is the main issue, right?
Making computer is relatively easy,
especially if you have something like a cell
because it's already contains a computer
and they can also organize themselves
into higher level computers
and become less stochastic and so on while doing this.
But the big difficulty is,
how can we find the functions that we are interested in?
And so when we look at deep learning,
what is deep learning?
The first thing that we have to note about deep learning
is that deep learning is the only thing
that currently works at scale.
It's the only class of algorithms
that is able to discover arbitrary functions
in a reasonable amount of time.
And reasonable being orders of magnitude more time
and more data than a human being does.
That's of course, the training time for something
like diffusion stability AI model, right?
This stable diffusion is a model of two gigabytes
and it contains the whole of the art
and you can get every celebrity you want,
you can get every spaceship from popular culture,
you can get dinosaurs,
everything is in these two gigabytes.
And training this takes weeks.
And it sounds that's very little compared
to the years that it takes to train a human brain.
But during these weeks and now it comes down closer to days,
this thing is going through hundreds of millions of images.
Many more than a human being could process
and the lifetimes and find correlations between
using dramatically large server farms.
So how does deep learning work?
First of all, it's differentiable computing,
which means it is representing all the functions
in such a way that they form a continuous space
in which neighboring variations of the functions
still lead to interesting results.
And you can move by a small nudges through the space
of functions to get the function closer to what you want.
That's different from discrete programs,
program and source code.
If you change a few bytes in the source code,
the program will no longer work.
If you change the neural network by a slight bit,
then it's still going to give you useful results.
And to do this, the neural network is describing
the functions via weighted sums of inputs
that are arranged into chains, basically in layers.
And some non-linearities, which are in a way to make
if sense, to make conditional breaks in this network.
And this description via multiplications and sums
is sufficient to represent all the functions that we want.
And we train this network by setting it up
in such a way that it has the potential to build enough things
that has enough links between the nodes
that it sums values up.
And then it just changes the weights,
which means the multiplying factors
for the inputs of every node in a systematic way.
And it, in some sense approximates all the functions
via real numbers.
And if you think about alternatives to deep learning,
the main thing that comes to mind is to build up computations
from deterministic discrete operators.
Such as Boolean logic or simple automata,
like cellular automata.
And as a result, we get finite Turing machines.
And in the finite case,
dynamical systems and Turing machines
and automata are the same thing, computationally speaking.
They're all Turing machines,
as long as you don't run out of resources.
So every digital computer in reality is a dynamical system
because the physics that digital computers
and prevains on is somewhat continuous
from the perspective of the individual transistors
and so on.
And we just try to find a region in physics
that makes the transistor reliable enough
as a discrete unit and deterministic
from the perspective of the logical language
that is implemented on the arrangement of transistors.
But underneath, there is an analog system.
It is just noisy.
And that is a system that is discrete.
On the other hand, every dynamical systems in physics
at the lowest level is discrete again.
If you zoom in, what you see is nothing continuous.
What you see is individual atoms
and individual charges and so on.
And they are discrete again.
And there is some discussion about
whether everything has to be discrete at the level
because of the nature of languages itself.
And I think that's the case.
I think that the discovery of the last century
that was most important in philosophy
is that those languages which assume
that the bottom most layer can be truly continuous.
They run into contradictions.
But this only matters if you are really interested
in modeling the bottom most layer.
If you just think about computation,
it doesn't really matter if you start out
with a dynamical system or this discrete system
as long as you are willing to allow
that every dynamical system has only finite resolution.
So there's only finitely many bits
that you can manipulate at any given moment.
And this equivalence between the continuous mathematics
and the discrete mathematics
has been shown basically in both directions.
You can use a computer.
And then just by using more bits,
you can approximate continuous system
with any degree of fidelity as you want
in the same way as digitized music
can sample the space of audio functions
below the level of resolution
that your medium can provide.
So you will get to something that is equivalent.
And the opposite direction has also been shown.
Greg Chaitin has gone to the trouble
to translate a LISP interpreter
into a diophantine equation with 17,000 variables.
Right, so this paper is called
the Complete Arithmeticization of Evil.
It's the evaluation function of LISP,
which did in 1987, it's quite beautiful.
And I don't think that is something
people actually want to read.
And this formula was generated
with some generative procedure.
He did not write these 900,000 characters by hand
that went into this.
But it's been shown, you can write down
the LISP interpreter in continuous mathematics.
So the alternative to deep learning
might be to basically construct functions
from the ground up using automata.
And in a way, this is also what people have done
because all our computers on which we run deep learning
are discrete automata.
People started to build this continuous arithmetic,
they built end or addition, multiplication and so on
from discrete logical units.
And this is clearly practical,
but it was done with relatively few people
and relatively few years.
The search process to build up continuous arithmetic
from discrete logical operations is not very large.
And if you want to get, to discover a deep learning
algorithm as a special case over discrete automata
from scratch, it's the existence proof
has been shown by the computers
and the deep learning mechanisms that we have.
Because relatively few people use relatively little brain power
compared to where you want to get to
to discover all these solutions.
By self-play, you could in the same way
as computers played a goal, discover arithmetic
but using discrete systems.
So in many ways, this is going to be equivalent.
And I suspect it might be interesting to start
from this automata direction again and build up learning.
And there are a few people which work in this area
but so far nobody has come up with an alternative
that scales up in the same way as deep learning.
And then there is a slight, the different approach
that we might be looking at.
And it doesn't use a normal Turing machine
but a non-deterministic Turing machine,
a multi-phase system.
And that is because systems in the lowest level of physics
and I also suspect systems that are implemented
on brains, non-deterministic systems.
And this doesn't just mean that they're noisy
or that they're random in many ways.
A non-deterministic Turing machine is a paradigm
from computer science that describes your state machine
in such a way that not every state
has exactly one successor state.
It's not sufficiently constrained
to have only one successor state.
Our Turing machine that we normally use
and this includes our digital von Neumann computers
and so on, they're defined in such a way
that every state has exactly one possible successor state.
If there is a branch in the computer
that's because it is depending
on some environmental variable
that is not relevant in the program
which means some input of the program.
But given the same input,
the Turing machine is going to produce the same output
and it's going to do this along exactly one path.
And not just Turing machine, the computational sense
is also going to give you the same output
but it's going to do this on many paths
because the constraints are not so narrow
that you go into one state after every state
but you can go into multiple states.
And the non-deterministic Turing machine
just goes into all of them branches.
And these branches don't necessarily meet again, right?
There is no way that they're connected again.
It's basically a non-deterministic Turing machine
is implementing some kind of multiverse.
But it turns out that many of these multiverse branches
that the non-deterministic Turing machine goes into
have the same bit combination in them
because there are only so many states that are reachable.
And so by looking at the dynamics between these bits,
you can get to the same point in the universe
on multiple paths.
And if the universe that we live in,
the physical universe that we live in
is such a multivase system,
it's still going to be possible for observer
that lives inside of it,
despite it not knowing which path it goes down,
determine statistical properties over the regional paths
which means you cannot know in your universe
which slit the double slit experiment the photon goes through
but you can predict the patterns
that many, many photons are going to make on the other side.
And the things that we can predict in our universe
are of that nature.
There are statistical properties
over all the trajectories that can happen
that we are a part of.
And how would something like this look in the brain?
Like in the brain, obviously the brain can only hold
a finite amount of state.
But if you have redundancy, what you can do is
if you don't tell the neuron to go into one particular state
as a result of its present state
but into a range of possible states.
And this happens randomly.
It means that there are many trajectories
that activations can take in the brain at the same time.
And they're going to meet on the same physical substrate
and they're going to accumulate
and you're going to sum up in some sense
and could be some basically some voting over the different paths.
And this means that a system like the brain
could via transmitting activation
try many, many things in parallel
and stochastically with some degree of randomness.
And as a result perform computations efficiently
that can not be efficiently done with a linear machine.
From a computational perspective,
if you want to simulate this on a linearly
on a sequentially operating machine,
it's going to be highly inefficient
because you're trying to do the same thing
over and over again.
And this randomness is going to delete some of the bits
that you've been computing.
But if your computational units are very cheap,
like in the brain, in the brain time is expensive
because taking one more step means
that you're going to be slower
in your interaction with nature.
But parallelism is cheap.
Just by doubling the cell count once more
by dividing the brain cells,
you get twice as many elements.
If you do this quite a few times,
you end up with billions of elements.
And these billions of elements can perform
many of these paths in parallel.
And this is something that to my knowledge
has not been seriously attempted yet
to get to work for a learning system.
There are some people which are thinking seriously about this.
For instance, we had Jerome Buzemeier on a panel here.
And he is describing the mental representations
as superpositional states using such a multivase system.
And it's also an idea that has occurred to Steven Wolfram
who is thinking of the universe as a multivase system
and who's open to the idea
that the brain might be basically
a bounded complexity multivase system.
So this, we see that there are opportunities
to build solutions that are inspired by biology,
that we haven't tried yet.
And if you look at the inspiration that happens
so far from biology into artificial intelligence,
despite many claims to the contrary,
it almost never happens.
I think that the last contribution of biology
to the transformer was heavy on learning.
And everything else that people got to work since then
was mostly not happening
because people looked at newer science results
and implemented them
and then ended up with a better machine learning algorithm.
And if you take the ideas that newer scientists currently have
about how the brain works, to my knowledge,
you cannot implement them in such a way
that they learn and control an organism.
Even a very simple organism like C elegance,
if we take the conic term of C elegance
and translate this into a computer model and run it,
it's not going to produce coherent warm behavior.
C elegance is a small warm
and it has only 300, I think 309 neurons.
Please correct me if this number is off.
And as a result, if you take this conic term
and run it into a digital simulation,
it's not going to produce the behavior that we want
because presumably we have not caught up
on all the functionality of the individual neurons
in the context of that warm.
Could be that there is stuff in the soma of the cells
that is not visible in the conic term
or that we made mistakes in digitizing the conic term
or I don't have enough resolution to see all the vesicles
that use different neurotransmitters in the conic term
to get the functionality right.
But for systems at scale that go beyond an organism
with a few hundred neurons, something like cortical column
and so on, we don't really have working models
at the moment.
The descriptions that the neurobiologists have at the moment,
if you translate them into computer models,
are not performing the same things
as our digital models are doing.
They're not able to discover these functions
because these models are still incomplete
and they're not ready yet for being used.
There's a more fundamental problem
that Konrad Hauding has highlighted in the paper
where he used the methods of neuroscientists
to reverse and engineer a microprocessor.
If you take a normal microprocessor
and give this to a neuroscientist,
the neuroscientist does ablation studies
and looks at the structure that the neuroscientist finds.
Is the neuroscientist able to reverse engineer
this microprocessor which is dramatically simpler
than the nervous system, of course.
And it turns out that Konrad Hauding
who is a neuroscientist that is the methods
of neuroscience might be for some more deep reason
because they're not functionalist enough,
able to discover how information processing
and nervous system works yet,
which means that the theoretical tools of neuroscientists
might not yet be ready even
to understand what brains are doing.
And I think that when we as non-neuroscientists
look at brains and at the textbooks of neuroscience
that take what I learned at university,
I find from my current perspective
that there are some shortcomings in this description
that if I want to sit down as a computer scientist
and discover the space of possible solutions
in nervous systems and for functional approximation
and just get a machine to search through it
and so on would be insufficient.
And this starts out with the idea
that a neuron is a specialized switch
at the way in which we abstract the neurons
since the perceptron is that the neuron
is some very simple gate or something element
like in a circuit that does something
in a mechanized automatic algorithmic way.
And I don't think that's really true.
A neuron is not a specialized switch.
A neuron is a single-celled animal.
It's quite complicated.
It wants something.
It wants to survive.
It is able to learn by itself.
It is adaptive.
Can do a lot of things.
It's almost like an amoeba that links up with other amoebas
and grows into this static structure
depending on its environment to perform a particular task.
And this is quite different as a perspective.
It means that your neuron is not just some kind of integrator
or some kind of weighted sum of real numbers
of activations that come in.
The neuron is really a reinforcement learning agent
with a little bit of memory
and a little bit of ability to look into the future.
Not very far, but a little bit.
And it implements a number of adaptive functions
to deal with its environment and reap rewards
that ultimately allow the neuron to survive in the brain
and not be strapped by the organism or killed by it
because it's not doing the right thing.
The second misunderstanding, I think,
beyond neurons are not just specialized switches is
that it's only neurons.
There is probably no fundamental difference
between neurons and other cells.
That is, every cell can do information processing
in conjunction with other cells
if it's in a multicellular animal.
It probably needs to be somewhat multicellular
because if it's a single-celled animal,
it's maybe evolving in such a way
that it's adversarial to its environment.
It doesn't benefit if it computes information
together with others.
But if it lives together with others
in a high degree of organization
like the cells in our body are doing
and you co-evolve them, then the cells can all send
many types of messages via chemicals
over the cellular membranes to neighboring cells.
They can interpret those messages
and they can learn how to respond to these messages,
which means they can learn to perform arbitrary computation.
So what's the difference between a neuron and another cell
if they can do the same thing?
Well, the specialization of neurons
is that they have extremely long accents,
so which they can send information coded
as electrochemical impulses very quickly
over long distances.
I think that neurons might be best understood
as telegraph cells.
They're very special cells that have evolved only in animals
or mostly in animals and that are very expensive to run
because they need a lot of energy
to send information so quickly.
And the benefit is that they can move an animal very quickly
so nature, so it can eat plants or other animals
to get that energy.
So basically the animal gets more energy
that it could get from photosynthesis.
And as a benefit, it's able to afford
to have this expensive nervous system.
And the code that the neurons are using
is probably different from the code, from the chemical codes
that the cells are using for the messages.
It's like a Morse code, probably something
like a telegraph system, so it can send information so quickly.
And initially, it seems that to me
that neurons might have evolved to move skeletal muscles
at the limit of physics.
So it can, from centralized coordination
in the nervous system, from the central nervous system,
send information so quickly
that the whole organism is coordinated much, much faster
at much shorter time spans than plants are.
And the other thing is once you can move very quickly,
you also need to perceive very quickly.
So it's also driving sensory input
and the evaluation of sensory input
and decision-making and learning.
So it's basically duplicating the information processing
that existed in the body
and it is creating something like a second information
processor in the body that is running at much, much faster
time scales than the normal cellular information processing
that will also exist in large, long-lived plants.
So my perspective is that just by looking at means and motive,
the possibilities of what evolution can do
and the capabilities of what an individual cell has,
I suspect that every long-lived organism with many cells
is basically going to function like a very, very slow brain.
And there are almost no limits
to what this slow brain can do if it lives long enough,
but it's not going to do this at the same time frame.
So animals will be able to outthink plants
just because they are so fast and run circles around them.
And the third misunderstanding is that we think
that consciousness is extremely rare,
that consciousness may be only existing in humans
and forms only very late in the evolution
of intelligent systems.
But it turns out we don't get conscious after the PhD.
We seem to be conscious before we can track a finger.
And if that is the case, maybe self-reflexive attention
is a requirement if you want to learn
beyond happy and learning.
If you don't want to just look at co-activation
between cells as a learning paradigm, which is probably
sufficient to map your body surface and so on,
but you want to have a coherent model of reality,
maybe you need to start out with some kind of core
that organizes everything into coherence.
And what we find confusing is about consciousness
that is that we don't seem to need it for many things.
A sleepwalker can do many of the things that normally
require consciousness.
And this is confusing philosophers to no end.
But maybe consciousness is in some sense
like government in a society.
And I don't mean government as some kind of abstract principle,
but as a real-time interaction that
is making society coherent.
And a society can do everything without government.
If you would be shutting down the government today,
it would be days before we notice and years before we crash.
But to get to the state in which society is today,
with streets and infrastructure and educational system
and so on, you need to have a government.
You're not going to bootstrap a group of people
into an organization without having
some kind of hierarchical organization that
makes this hoop of people coherent in their actions
and creates some next-level agent out of them.
And to do this, the government needs
to start out with some local coherence,
with some making itself coherent,
and then imposing some kind of organization
on the environment that is branching out
and scales over all the individual agents.
And if you just put individual people together for long enough,
then different forms of government
will emerge and some kind of evolutionary competition
between them.
And eventually, one of them will take over
and organize this group of people in such a way.
And the idea that the same thing could happen
among the neurons, that they're basically
different forms of organization that start out
from small cores and then move as activation patterns
that are agnostic of the individual units that they run on.
But they impose the same language on all of them,
similar capabilities on all of them,
so the locus of action can move around between them.
This idea that the brain organization could evolve
like this is similar to Gary Edelman's idea of neural
Darwinism, that basically our brain organization,
our mental organization is not hard-coded in the genome
as a blueprint, but what the genome contains
is the conditions to start this evolution
between different forms of organization.
And then rig the evolution in a particular way
so it converges quickly.
There are some ideas that we could take from biology
into technical systems.
And first of all, I think that the system
needs to be real-time.
It needs to be coupled to the environment
and needs to go into resonance with whatever environment
is coupled to.
And it needs to regulate the interaction
with that environment until at the level of coupling,
at the temporal resolution that it has,
is able to track reality around it.
And it's something that our machine learning models
are not doing yet and not real-time.
Even something like stable diffusion
is trained in digital images that are not happening in real-time.
They're not even happening in the right order.
There are just 800 million disconnected images,
or which the system is trying to find structure.
And this would not work for a logical organism.
I don't think that we could converge
from this amount of data.
Instead, what we get is a world that changes continuously
by small degrees.
And these small changes make sure that every frame is
related to the last frame.
And we can learn universal laws in which these frames are
related.
There are laws of conservation of information.
And without this conservation of information,
where we learn the transitions between adjacent frames,
I don't think that we would be able to learn from the universe.
So one thing is we change the paradigm from image to video
or other streaming data that has information preservation.
And in the beginning, this might look more difficult to us.
Isn't it harder to learn from video
than it is to learn from single pictures?
Well, what you want to learn is the fact
that you are living in the universe with moving objects
that happen in free space and so on.
It's actually much easier to learn this from video,
because it contains way more constraints than this way.
They're much more obvious.
The next thing is the way in which our brain is modeling
these things seems to be made from lots
of small periodic loops, small interlocking periodic loops.
And first of all, it has to be loops,
because the brain is relatively slow.
Information transmission in the brain
is so slow that it takes appreciable fraction
of a second for a signal to cross the entire neocortex.
And if you want to create simultaneity
between these different parts of the brain,
you're not going to get there.
It's right in our computers and our CPUs and so on.
Our GPUs, they make everything simultaneous
by exploiting the speed of the signal transmission
in our CPUs and GPUs.
It also means that we cannot increase the frequency
at which we run them arbitrarily,
or we cannot make the CPUs and GPUs arbitrarily large,
because it just takes so much time for a signal
to cross over the entire circuit.
The next step could be to use photonics.
So we can go to the speed of light
and make this system slightly faster
and slightly larger again,
while having coherence over the entire system.
But our biological systems don't have a chance of doing that.
They must live with the fact that it takes very long
for signals to get there.
And the way to deal with that is to make sure
that you're okay if you are out of sync.
You just need to be in the same phase.
Basically, you go at the same frequency
at different points of the brain,
and you make sure that the signal eventually gets there,
but it's okay if it's from the previous cycle
or two cycles ago, or several cycles ago,
as long as the content of the different brain areas
that only changes gradually.
If that happens, you're able to integrate over that.
You can use predictive algorithms and so on,
and can synchronize the whole thing.
So basically, this idea of slow oscillators
is something that we could translate into digital systems.
The next thing is this emergent management,
and here we'll Darwinism.
So instead of having a particular circuit
that is required to be like this,
let's evolve this computational operators that we need.
And the next one is that many of the functions
that the brain discovering are only discovered once.
And this is tool for simple functions,
like addition, integration, multiplication,
simple computational primitives, rotation,
that are being used for many, many mathematical primitives
that we require to describe the geometry of sound,
of images, of thought.
And this basic arithmetic,
this basic library of computational functions
at the moment to train this into a neural network
takes a very long time, right?
Despite the neural network being built over addition
and multiplication,
it's not easy for a neural network to learn arithmetic.
It's possible to do it,
but it takes enormous amount of training data.
And in every context, locally,
the neural network is performing the same arithmetic
over and over again.
It has to retrain these functions
into a different region of the network again.
And basically getting all these different computational
primitives bootstrapped into the neural network
is something that is hard.
And it has an interesting effect.
You can, for instance, train a neural network
on audio input,
and then use it to discover structure and vision.
And it's going to be much faster
because the audio input already prepares the neural network
to learn many of the computational primitives,
basic arithmetic in many parts of the network,
that it can then adapt for a new task.
And the way in which our brain is doing this
is probably that it learns some useful functions
and then these neurons have a way
to exchange these functions possibly via RNA.
And so basically the functions that are being computed
are to some degree agnostic to the individual neuron.
They are somewhat substrate independent.
They're just migrate to.
So a different paradigm might be instead
of using local functions over the neighborhood
of every neuron.
And every neuron is learning its own set of functions.
You learn a set of global functions
and every neuron is deciding which ones of those to use.
And it's also something that has almost never been tried
in AI and that what I'd like to get seen.
Another thing is that the main focus is on reward.
What you try to do in the brain is
to do the most useful thing with fixed resources.
And this means you have to assess the global reward
that the organism is getting out of the distributions
of all the neurons.
And then you need to distribute this reward
among all the neurons that contribute to the result.
This is similar to what you do in a corporation.
It's an economic problem, but the corporation
tries to do the most valuable thing
that it can do with all the employees that it has.
And to do this, it needs to get rewards
to all the individual employees.
And the rewards are not given in such a way
that every employee gets a different amount of money.
And the one that has the biggest contribution
to the bottom line by the others are also important
gets much, much more.
There is some degree of this, but it's mostly depending
on the negotiation power of individual employees
on the market.
If there was no fungibility
and you would need to train all your employees,
it would make sense to give them all the same amount
of money, right?
And in the liberal capitalism, it doesn't make sense
to give a good retail worker the same amount of salary
than it does to give money to a good manager.
But if you need retail workers, you will have to employ one.
And the reason why retail workers are less than managers
is mostly because there are many more retail workers
competing for positions than there are managers
competing for positions.
So the supply and demand regulates labor market
in such a way.
But this is not true in the biological system.
Every new one basically is going to consume
the same amount of resources just for existing
and being ready to do something.
So our reward here is different.
It's not being accumulated in the bank account
of the new one.
Instead, this is just a signal that tells the organism
that this new one is still going to get fat
because it's useful in what it does.
And the new one needs to get feedback similar
to the feedback that you get from your colleagues.
That's the actual reward that tells you
you're doing the right thing, right?
So it needs to be some kind of communicative reward,
some messages that are not directly food,
but that are more anticipated reward
that are like money only without accumulation.
And so there's basically going to be a reward-driven language
and who is distributing all the reward in the brain?
Well, all the cells are with all the neurons mostly,
but also the other cells, maybe glia cells,
and so on that contribute
and the distribution of the rewards.
And so what's happening in the biological system
is that it involves a reward language.
There are two types of signals that are being sent around.
One is the results of the computation,
which is read by other neurons
based on what kind of activation they're interested
in filtering out of the environment.
And the other one is going to be signals
that amount to reward and punishment
that basically tell other neurons
whether they should do more or less
of a certain computation.
And so basically reading of information is going to be pulled.
You draw information from the environment
and reward is going to be pushed
because you should not be able
to escape a negative reward, a punishment and so on, right?
And I think we could approximate this
using a new paradigm and a number of experiments
that I would like to do in this regard.
So basically a neuron has an internal state vector
that contains the slight history of the neurons
over the last few activations that is read
and the type of the neuron and so on.
And it has a selector function.
The selector function is basically
defining the receptive field of the neuron.
And the receptive field of the neuron
can be just the environment of the neuron
interpreted as a certain topology.
So basically it looks at its neighborhood
as if it was a space with a certain number of dimensions.
And how can this be?
The neocortex is two-dimensional or two and a half-dimensional
but it has a number of layers,
that number of layers being very small
and it's a large 2D area
subdivided into different regions.
Well, it turns out that you can interpret a 2D area
as something that is in higher dimension
in the same way as you can take the linear
or one-dimensional address space of your computer
and interpret it as a two-dimensional map
or as a three-dimensional space
or as something that happens in eight dimensions
and can perform operations on it.
And this is what the selector function does.
The selector function is basically interpreting
the environment of the individual cell
as a space that contains information
in a certain arrangement.
And it doesn't need to be a regular space.
It can be a manifold.
It can be something that is very selective.
It can be something that only uses five neighbors.
And these neighbors can even be physically very distant.
And so this neuron could be a juncture
or some kind of hub that sends information locally
into the network and so on.
So you're going to have some neurons that have a topology
that allow long distance connectivity
and others that performs local maps and 2D or 3D
and perform functions on them.
And this next to the selector function,
you have the modifier functions.
The modifier functions tell the individual neuron
how it should change its state based on its own state
and the activation that it reads in the environment.
By having some history in its own state,
it's able to respond to a spatiotemporal activation
distribution in its environment.
And it's the use this idea
that the neuron can use global functions.
It means that we can arrange many neurons densely enough
in some kind of lattice
and these functions can arrange themselves
as they need to be arranged.
And they can shift around as they have to
and duplicate themselves if they have to.
Yasha, just in the interest of time.
Yes, I'm basically done.
Thank you for reminding me.
So this selector function, modifier function paradigm
allows us to come up with a new way
of describing a function approximation beyond deep learning.
And at the moment, the search space
in my experiments is way too large.
So basically there are too many ways
in which this could be implemented
from my current perspective
to get this converge to a good solution.
The alternative is I can just handcraft a solution
but it might not be optimal.
But it's something that I would like to definitely look more
into in the future.
And not just me, I suspect that many people
are currently discovering such ideas
and will be working on in the future.
And while it's not clear
that this can provide a viable alternative to deep learning
that is much faster and converging faster
and more efficiently than the present deep learning systems.
This is something that I believe is closer
to what biology has discovered.
And that scales up very reliably
over many, many classes of algorithms.
Okay, that's it from me.
All right, we would like to have a quick one
to two minute break right now
so that the panelists can set up their presentation.
We will do Michael Levin's presentation next.
So in the meantime, Yosha,
let me ask you some questions that's got posted
while you're talking.
So a question from Nikolai,
like how neurons are small organisms
that operate in the emerging super organism
that is a human,
would future AGI architecture need to be made up
of many smaller AIs?
So there is no centralized control in the brain
in the sense that all the centralized control
is emergent over all the organizations between the neurons.
There is no dedicated CPU
that is able to process every neuron and update its state.
Every state update happens locally in the individual cells.
But this is an engineering constraint
that doesn't exist in the technical systems.
And it's not clear yet to me
to which degree we need to have local control
to make it happen.
At the moment, the machine learning of organisms
that we are using are treating the individual nodes
in the network just as memory.
And the updates are done by a centralized algorithm
that is updating all this memory.
All right, and you either have a CPU
that is reading and writing,
or you have lots of local CPUs in the GPU
that is doing this with multiple pipelines in parallel.
And there are biologically inspired chips
that are mostly experimental at the moment,
like Intel's Lohi,
that are using many, many very small simple CPUs
that are doing this.
And it's not clear if the best solution
is to have a CPU for every memory cell.
It's probably not the case.
So there are some things that you can do
in the technical systems that informing conformance
to a centralized algorithm,
to centralized specifications
that the technical system is going to implement.
And in a biological system, this is just not feasible
because there is no such centralized authority,
no engineer who can make nature behave by itself.
But if you want to think about how to build a mind
from a biological perspective,
we have to think about how to build something
that wants to grow into a mind.
And we can take some of these ideas.
It's not clear that we need to make this
with completely local control only.
Maybe it's more efficient to have a mixture
of some local control and a lot of global control
where we already know what the control is going to be.
I think Mike is ready.
Yes, excellent.
Michael, the floor is now yours.
Great.
Okay, well, that was extremely interesting.
So let me see what I can add here.
I've got some slides.
So here we go.
Hopefully you can see that.
So what I would like to talk about,
of course there's this idea that biology
should be an inspiration for AI.
And what I would like to do is to deconstruct
some of the biology that people typically think about
in these contexts and ditch a lot of things
that are very common, binary categories
and a focus on brains, a focus on neurons,
a focus on humans.
I want to step away from all of that
and rebuild a different framework
that I think has many, many implications for AI.
So the first thing I want to talk about
is this idea of a typical human.
So there's this kind of classic idea
that we know what a human is
and it certainly works for practical purposes in society.
But the idea is sort of pre-scientific
and it's still, many people are still,
even scientists are often still caught up in this,
the idea that, okay, so we have these humans
and they are discrete natural kind
and they're different from other animals.
And so there's this, here's Adam naming the other animals
and so there's the discrete species and so on.
But if we take developmental biology and evolution
and synthetic biology and bioengineering,
if we take these things seriously,
then what we find out is that actually
there are no such natural kinds because all of this,
both on the evolutionary time scale
and the developmental time scale
and now in terms of the technological time scale,
there are very gradual, very small, very slow changes
that go all the way back from what people think of
as a typical human and their intelligence,
all the way back to very different types of organisms.
And developmental biology and of course evolution too
offers absolutely no place to put a sharp line
and say this creature was not,
pick an adjective, intelligent, cognitive, conscious,
well, whatever you like, pick an adjective
to say this creature was not it,
but it had some offspring and the offspring now are, right?
That just doesn't exist
because all of these changes are very slow
and very continuous.
And so there were changes during evolution,
we all start life as a single cell.
In the future, there will be all kinds of changes
to our bodies with biological
and engineered kinds of devices.
And so all of these are continua
of really novel types of embodiments.
And we build certain kinds of conceptual metaphors
that try to distinguish different categories here,
but these are discrete tools,
the phenomena themselves are deeply continuous
on multi-scale.
And to give you just a simple idea
and then we'll enlarge on this is this.
And so this caterpillar is a kind of soft-bodied robot
that lives in a two-dimensional world
that crawls around on leaves.
It likes to chew plants and it has this brain
that's very, very suitable for this purpose.
What it needs to do is turn into this creature,
which is completely different.
It lives in the three-dimensional world,
it doesn't care about the leaves at all,
it wants nectar and it flies and it does various things.
And so during this process, there is a metamorphosis
where not on an evolutionary time scale,
during the lifetime of the individual,
the brain is basically dissociated
and rebuilt into a new architecture.
And by the way, there are data that memories persist.
So if you train the caterpillar,
the butterfly or moth still remembers
the original information,
but you can sort of think about,
nevermind the question of what's it like to be a butterfly,
what's it like to be a caterpillar
changing into a butterfly, right?
That process of slow, but drastic change
in your embodiment.
And so from here, we can just remembering
that we are all made of parts
that can modify during our lifetime.
We can ask some interesting questions.
For example, you look at a brain
and we're sort of conditioned to expect that it's obvious
that a brain contains one human worth of intelligence.
But this is just because we're used to that
in terms of our interactions.
If I showed you a brain
and you didn't know what this was
and I asked you how many different cells are in there,
you would actually have,
we have no ability to answer that question.
We have no way to ask how much,
and I think Yosha got to some of this,
how much of this real estate is necessary
for one human's worth of performance?
We have no idea how much is actually in there.
And actually, very interestingly,
the same issue occurs in embryonic development.
So we all begin as a cellular blastoderm.
So this is a sheet, a two-dimensional sheet of cells.
And that sheet turns into an embryo.
Now, what does that mean?
First of all, can we guess in advance
how many embryos are going to come from that sheet?
Actually, we cannot, and I'll show you why.
And it's not genetics.
And then there's the question of what are we actually counting?
When we count an embryo, I mean, there's 50,000 cells,
let's say here, what is it that we're counting
when we say there's an embryo?
What are we actually counting when we say there's
a single human inhabitant in this bunch of tissue?
So one of the things that you can do in embryogenesis
is you take this blastoderm and you take a little needle
and you put some kind of scratches into this blastoderm.
And then they heal, but before they heal,
what will happen is that each of these regions
being isolated from the other regions
decides to organize an embryo
because they don't know for a while anyway
that the other regions are there.
Then when it heals up, it becomes conjoined twins.
And you can do this very easily in chicken and duck
and other embryos, but humans work exactly the same way.
And so then there will be multiple embryos
within the same blastoderm.
And then there will be some disputed zones here.
There's some cells that aren't quite sure
which one they belong to.
But this deep idea of individuation,
of taking some kind of a continuous,
in fact, it's even worse than continuous
because it's multi-scale substrate
and having itself organized into discrete what?
So in the case of embryos,
what you have are discrete groups of cells
that are trying to follow anatomical goals.
They're trying to achieve particular walks
in anatomical space.
They're gonna construct the right number of fingers,
the right number of eyes, whatever it is.
Same thing in cognitive development.
There are issues, there are disorders of individuation
that you see in split brain patients
and dissociations and so on.
So this question of how many are in there
is deeply interesting and it gets to the bottom
of what it means to be a coherent agent
when you're made of parts.
And I think Alan Turing, although as far as I can tell,
he didn't write directly about this.
I think he was well aware of this issue
because of course he was interested in intelligence
and generic embodiment and so on.
But he was also interested in morphogenesis.
He wrote this paper on biological morphogenesis.
And I think he understood that these are deeply
and profoundly the same problem.
The problem of morphogenesis
and the problem of the mind are the same problem
because of this emphasis on emerging
as a coherent entity from multiple parts.
So people often talk about, well,
ants and termites are some kind of collective intelligence
and we can argue about what that means,
but we are really a unified, a centralized intelligence.
We're not like bird flocks or ant colonies.
But actually, of course,
all biological systems are made of parts.
And so we too are a kind of collective intelligence.
What's interesting is the scaling interface
is what is it that allows these individual subunits
to work together and present to other intelligences
to themselves, by the way, and to the environment
in a picture of a coherent agent.
So this is the journey that we all took.
We began life as a piece of physics.
So basically as a quiescent oocyte.
So it's a blob of chemicals, not doing terribly much.
And then through this incredibly,
just magical process of embryonic development,
that we arrive at something like this,
which is a complex organism with metacognitive capacity
that's going to make statements
about how we're not just machines
and we're different than physics and all that.
But this whole process is extremely smooth and gradual.
It happens second by second.
There is no lightning flash
at which point physics becomes mind.
It's a gradual process.
And we can talk about face transitions and such,
but there's really not that much evidence
for any of that.
It's a very continuous process.
So this is the kind of thing.
So I think, Yosha alluded to this a few times.
This is the sort of thing.
I mean, not exactly this.
This is a lacrimaria, it's a free living organism.
But here's a single cell, right?
This is what we are made of.
These guys, there's no brain.
Here's those, there's no nervous system.
This is the single cell creature in real time
using all of the intelligence of its chemical networks.
And we can talk about this.
I mean, that quite literally chemical networks can learn
and they can do inference and many other things.
Even though it's a single organism
is handling all of its single cell agendas
in its environment.
So metabolically, physiologically, anatomically,
it's doing what it needs to do.
And so we are made of extremely competent parts.
Here's another example.
This is a, this whole thing, you'll see, you'll see this.
I'm gonna pause it.
Whoops.
I'm gonna pause this.
This whole thing right here,
this is called Pfizer and polycephalum.
It's a slime mold.
The whole thing is one cell, okay?
And what it's, what I'm showing you here
is that it's sitting in this environment.
These are three glass discs.
These are very, very light.
There's no chemicals, there's no food.
It's just glass, inner glass.
There's one glass disc here.
And what it's going to do is,
it's going to, for the first few hours,
it's going to just generically grow in all directions here.
What it's doing during this process
is it's tugging on its substrate
and feeling the vibrations that gets back.
And it can sense the strain angle
of the objects in its environment.
And then we'll eventually reliably grow out
to the heavier mass.
But during this, so that'll happen at this point,
but during this time is when it's processing
that information and learning from its environment.
And then boom, now the behavior begins.
So single cells are very competent,
even microbial single cells.
And so what we have to understand is that biology,
so here's a principle that I think is really important
for future AI, biology is deeply nested.
That is not merely structurally,
I mean, that's obvious we're made of organs, tissues,
and so on, but each layer is competent.
It solves problems in its own space.
All of these things from molecular networks
all the way up to whole organs and beyond
are solving specific problems in specific spaces.
So we are really interested in my group,
we're really interested in creating a framework
that allows us to relate to really
very diverse intelligences.
So, of course, familiar creatures,
all kinds of weird biologicals,
colonial organisms, swarms, of course new,
and I'll show you some in a couple of minutes,
new engineered creatures, artificial intelligences,
and maybe at some point exobiological,
truly alien agents.
We need to be able to deal with all of this.
It's not enough to deal with crows and monkeys
and then maybe octopus, that's way too narrow.
And so, of course, this is an idea
that has been addressed before.
So here's Wiener and colleagues trying to come up
with a very sort of cybernetic way
to classify different degrees of behavior
all the way from passive mechanical behavior
up to complex cognition
in a way that abstracts from its familiar embodiments.
So there's no talk of brains or neurons
or anything like that.
This is very sort of functionalist.
And one thing about us as humans
is that we are very primed to recognize intelligence
in the three-dimensional space.
So basically, medium-sized objects moving at medium speeds
through three-dimensional space.
When we see it, we know what agency looks like,
we know what intelligence looks like.
But we are really bad at,
and this is why we must get better at it,
recognizing intelligence in other types of problem spaces.
So imagine if you had a direct feeling
of all of your blood chemistry.
If you were able to feel your blood chemistry
the way that you can see objects in three-dimensional space,
you would be very obvious
that your kidneys, your liver, and so on
have a degree of intelligence
and they're doing amazing things in their problem spaces.
So we study how individual cells
navigate the space of gene expression,
hopefully the physiology and morpho space,
the space of patterns.
This is what I'm talking about today.
And just for a few minutes here,
here's an example of cells solving
an entirely novel problem in genetic space.
So here's a planarian, this is a flatworm.
They regenerate parts of their body when amputated.
What we did was we exposed planaria
to a solution of barium.
Barium blocks all of their potassium channels,
the cells and the neurons are really unhappy.
Their heads explode, literally just explode.
Over the next week or so,
they rebuild, keeping them in the barium,
they rebuild a brand new head.
The new head doesn't care about barium at all.
So we asked the simple question, how can that be?
What is the new head doing
that the original head couldn't do?
And we found out that there's actually very few genes
that the system up and down regulated
to be able to do its business in the presence of barium.
The kicker is, planaria never get exposed to barium
in the real world.
There is no ecological precedent for this.
So just imagine, you're a cell,
you've got, I don't know, tens of thousands
of possible genes, you've got a disaster,
a physiological disaster.
You don't have time to try every combination.
There is no time to try everything
and whoever survives, survives.
These cells don't turn over that fast.
You have to solve this novel problem,
possibly by generalizing,
because you've never seen barium before,
but you have seen epilepsy before.
And barium excitability might look a little bit
like epilepsy.
And so maybe you can do some of the same things.
So this idea of solving novel problems
in physiological space is one example
of what biology can do.
But here's another example.
So this is how we all start
as a kind of a collection of early cells.
But this is a cross-section through a human torso.
Now look at the incredible order here, right?
All the tissues, the organs, everything is in the right place,
the right size and shape and relative to each other.
Where does that come from?
You might be tempted to say DNA,
but of course we can read genomes now
and what's in the DNA isn't any of that.
What's in the DNA is the sequence
of the micro level sort of hardware
that every cell gets to have, the proteins.
That's what the DNA specified.
So you really, you still need to understand the physiology
by which these cells compute what to do here.
And then there are lots of questions,
as regenerative medicine workers,
we try to figure out what do we say to these cells
to rebuild pieces that are missing?
And as engineers, we want to know what's actually possible?
What can you reprogram this?
Can you make them do something else?
So the amazing thing about development
is that while it is incredibly reliable and robust
and in fact hides all of its intelligence from us
when we see acorns giving rights to oak trees
and frog eggs make frogs, we sort of assume,
well, what else is it gonna do?
Like that's obvious, right?
That's how it has to be.
But that's only what happens on the default condition.
What we find out is that, for example,
if you take an early embryo and cut it in half,
you don't get two half bodies.
You get two perfectly normal monosygotic twins.
And in fact, more generally,
the process of development can navigate this anatomical space
in a way to reach the same goal
from different starting positions
despite really drastic perturbations
by taking different paths.
It's not just a hardwired set of emergent.
This is not about emergence.
Of course, complex things emerge from simple rules.
This isn't that at all.
This is the ability of the system to get to its goal
despite really, really radical changes.
So here's one change, here's another change.
As an adult, some organisms like the salamander,
they regenerate their eyes, their limbs, their jaws,
their tails, you can make cuts anywhere you like along here.
And these cells will very rapidly grow
and undergo morphogenesis, and then they will stop.
When do they stop?
They stop when a correct salamander limb has formed.
Doesn't matter where you cut it,
it will only grow exactly the right amount
and it will stop when exactly the right thing has formed.
So you've got some sort of error minimization scheme
going on here.
It knows exactly what it looks like.
It knows what the target state is.
And in fact, this is something that we discovered
that, so this is a tadpole here, some eyes,
here's the brain, the gut, the nostrils.
These tadpoles have to become frogs.
In order to become frogs, they have to rearrange their face.
The jaws have to move, the nostrils have to,
everything has to move.
We find, and so you might imagine
that this is some sort of hardwired set of emergent outcomes
where every organ gets displaced
to its appropriate distance and direction.
So we made what's called Picasso tadpoles.
Basically, we scrambled everything
so that everything's in the wrong place.
The eyes are off to the side of the head,
the jaws are on the other side of everything
is just scrambled.
Because we have this hypothesis
that this is more intelligent than people gave a credit for.
Sure enough, what these guys do is every structure moves
in novel paths and keeps moving,
no matter where it started from,
until it gets to be a pretty normal looking frog.
So what the genetics gives you is not a piece of hardware
that does the same thing all the time.
It gives you a machine that can recognize unexpected changes
and take corrective action as needed
to get to the same goal.
The most amazing part of this is that in doing this,
and this is an example of top-down causation,
which is why it's really important to understand this,
how high level goals filter down
to the sort of implementation machinery,
is that what you see is that this is an example
from the kidney tubule of a nut.
If you take it in cross-section,
normally there's, I don't know, eight or nine cells
that work together to make the lumen of that tubule.
But one thing you can do is you can force these cells
to be gigantic.
And when you do this, when you make them larger,
fewer cells will do this,
forming exactly the same lumen diameter
until you make the cells so large
that a single cell will wrap around itself
to give you the same structure.
What's amazing about that is that these are completely
different molecular mechanisms.
This is cell-to-cell communication.
This is cytoskeletal bending.
So in the service of a high-level goal,
meaning make this large-scale anatomical structure,
different molecular mechanisms get activated, okay?
And this is very unusual.
This idea is very unusual in biology.
The biologists tend to think about things emerging
from molecules not going the other way,
but it has certain parallels in computer science
where the algorithm makes the electrons dance
in an important way, right?
In a functionally important way.
And so what we've been doing is trying to build models
that go, sort of full-stack models that go all the way up
from molecular kinds of activities
that set the ion channels and other things in the membrane.
Two, we specifically, I don't have too much time today,
but we specifically study bioelectrics.
We study how all cells, not just neurons,
all cells use electrical signaling
to form computational networks.
And so we study what the tissue-level electrical patterns
look like and then what the organ-level patterns look like
and then how that becomes literally an algorithmic set
of steps that determines things like how many heads
a flower is going to have.
And during this process, we want to know a few things.
We want to know how does the cognitive light cone,
and what I mean by that is simply the spatiotemporal size,
the scale of the largest goal
that that particular system can conceive of pursuing, right?
So if you're a bacterium,
your cognitive light cone is very tiny
because really all you care about
is the local sugar concentration
with about maybe 10 minutes forward and back.
But if you're a human, you can have gigantic goals
that exceed your lifespan.
It can be planetary-scale goals.
And then of course, every kind of creature in between.
So we define this kind of cognitive light cone
based around the types of goals that a system can pursue.
And so we need to understand during this process,
how do the goals enlarge?
How do they shift into different spaces?
So individual cells care about things in metabolic space
and physiological space and transcriptional space.
Those are their goals.
Collectives of cells care about very much larger goals,
such as the shape of your hand
and the fact that you have to have to have
exactly five fingers.
And then of course, this question of where do these goals come
from in the first place,
we'll address that momentarily.
So about the only piece of bioelectricity,
I'm gonna show you because Yosha brought up this idea
of counterfactual memories is simply this.
We treat the behavior of the cells and tissues
as a collective intelligence.
Literally the group of cells is a collective intelligence
that tries to solve problems in anatomical space.
And because we have some understanding now
of what the medium is of that collective intelligence,
not shockingly just like in the brain, it's bioelectric.
Why? Because that's how the brain learned its tricks.
You already heard and Yosha is absolutely right.
There are very difficult tasks to try to distinguish
what makes a neuron different from other cells
because even bacteria from the time of microbial biofilms
have already been using all of the same tricks
as the brain uses, this electrical network stuff is ancient.
And so what we are able to do is read and write
the memories of this collective intelligence.
And so we use a specific technique
that reads the electrical gradients.
This is just like neural decoding
as the neuroscientists try to do in the brain.
So here there's a particular pattern that says,
if injured, you're going to make one head.
We can rewrite that and we can create a worm.
Here is where the pattern says,
no, actually a correct worm should have two heads.
And if you go ahead and cut that animal,
they will go ahead and make two heads.
This is not Photoshop, these are real,
real two-headed malaria.
But the cool thing about this pattern
is this is not a reading of this animal.
This is a reading of this perfectly normal,
anatomically one-headed, genetically,
transcriptionally one-headed animal.
So this is a kind of counterfactual memory.
It's a representation of a state
that it's what you are going to do in the future
if you get injured.
If you don't get injured, it stays latent,
it never comes up.
And we have lots more data on this.
We can actually make heads of other species of worms
and many other things.
The idea is that a single body can
store one of two different representations
of what the goal state is going to be if they get injured
and then they build to that goal state.
So this should sound very familiar.
This is both the nervous system works this way
and of course, reprogrammable devices work this way.
The same hardware can hold onto multiple
computational goal states.
Now, let's go back to where we started with this,
which is this notion of scaling up from components.
So here's your single cell.
What evolution has done is allowed these cells
to merge into networks that are able to store
much larger goal states.
So this guy only cares about his own physiology
and his own metabolic.
This collection of cells is very competent
in reaching a particular region
of anatomical morpho space that looks like this.
The goal is huge at centimeters in size.
And if it's deviated from that,
it will do its best to come back
to even with the kind of drastic interventions.
But that process has a failure mode.
That failure mode is known as cancer.
What happens?
This is human glioblastoma cells.
If individual cells get disconnected
from this electrical and other signals as well,
from this network that binds them
towards a common journey in that space, that common goal,
they revert back to their evolutionarily ancient self.
What is the goal of a single cell?
Well, it's to become two cells
and to go wherever life is good.
That's metastasis.
And so you can see how what happens with these cancer cells
is they're not any more selfish than any other cell.
They're just their cells are smaller.
And we've talked, you know,
I talked to roboticists and folks like that
with this idea that why don't robots get cancer?
Right?
The reason that our current technology isn't prone to this
is because we do not have a multi-scale architecture
where the components have their own goals.
That we have some fairly dumb components typically.
And then we hope that the collective
that has some kind of, you know,
is doing some kind of computation,
but the parts are not trying to do anything.
Biology isn't like that.
Every component will do interesting things
if freed from its neighbors.
And I'll show you that.
But of course, you know, biomedically,
we can sort of take this kind of weird way
of looking at things and ask,
can we simply enlarge the boundary of the self?
Enlarge the border between self and outside world.
And so you can do that.
We have techniques to do that
where when we inject particular human oncogenes
into these tadpoles to make tumors,
and you can already see this is voltage imaging,
you can see that these cells are already starting to defect.
As far as they're concerned,
the rest of the animal is just outside environment.
So that's something else that Miocha mentioned
is this idea of being in conflict or not
with your environment.
It's never obvious to a new agent what the environment is.
Every cell is some other cells' external environment.
And so normally all of these cells believe
that the water out here is the external environment.
But once you disconnect them using these oncogenes,
then as far as the cells are concerned,
all of this stuff is external environment.
They don't care what happens to that.
They're gonna do their best.
They're gonna live their best life.
They're gonna dump entropy into the environment.
And of course, that's maladaptive for the organism.
But one thing you can do is you can force
using specific techniques,
including optogenetics and some other things,
you can force these cells to remain in electrical,
in the correct electrical state with their neighbors.
And if you do that, even though the oncogene,
this is the same animal here,
even though the oncogene is very strong,
there's no tumor because the hardware problem
isn't really fundamental.
It's the software that's fundamental.
It's are these cells working on a large goal
like making a nice liver and muscle and skin and whatever?
Or are they individual cells working on individual goals?
So we spend a lot of time thinking
about these kinds of things.
How do we, what are the mechanisms, of course,
but also algorithms, policies for connecting up
little tiny homeostats,
these cells that like to keep certain states
into much larger networks
that then have these interesting properties
that of course people in the connectionless world
have been studying for a really long time.
So in painting, out painting,
you know, all this kind of stuff.
So we can talk about our efforts
to sort of understand how the goals scale.
They scale from these really humble,
metabolic kinds of goals of individual cells, right?
These homeostatic loops into anatomical homeostasis,
eventually behavioral homeostasis
and behavioral clever motion through three-dimensional space
and eventually linguistic space and who knows what else.
So just for the last couple of minutes,
I just wanna show you one thing,
which is simply this.
In studying these kind of novel perturbations
and asking what are cells actually capable of?
What, you know, what other modes are there?
We asked the following thing
and I have to do a disclosure here
because Josh Bongard and I are co-founders
of this thing called Fauna Systems.
It's a biorebotics kind of company.
And so what we did in this,
all the biology was done by Doug Blackiston in my lab
and there was a lot of computer science here
done by Sam Kriegman in Josh's lab.
What we decided to do was to liberate cells
from the normal environment
and give them a chance to reboot their multicellularity.
How much creativity is there?
What else can they do?
And specifically, and I think somebody on the chat
asked this before, where do these goals come from?
So that's what we wanted to understand,
a completely novel creature that's never existed before.
What goals do they have?
Where do their goals come from?
Okay, and so I'm gonna just show you a couple of examples.
So what we did here is we took an early frog embryo
and so what Doug does is he takes all of these cells up here
which are skin, they're basically determined to be skin
and he dissociates them
and puts them into a little depression here.
Now, there are many things that they could have done
after that, they could die,
they could spread out and sort of walk away from each other,
they could form a flat two-dimensional monolayer
the way that cell culture does.
Instead, what happens is this, and this is time lapse,
of course, so overnight, these guys will get together
and they will coalesce into this interesting
little thing here and what is it?
Well, we call this a Xenobot,
Xenopus laevis is the name of the frog
and it's a biobot, so Xenobot.
What it's doing is it's using the little hairs
on its surface, these hairs are normally there
to spread mucus down the body of the frog.
What they've done is repurpose those hairs for swimming.
So here it goes, it's chugging along,
you can see that as they can go in circles,
they can sort of patrol back and forth like this,
they can have group behaviors,
this one's going on kind of a long journey,
these are interacting together,
these are having an arrest.
Here's what it does in a maze,
so you can see it swims along,
it's gonna take a turn here without having to bump
into this outside wall, so it takes a turn.
And then at this point, for some internal reason,
we have no idea about it,
it decides to turn around and go back where it came from.
Okay, so there's all sorts of primitive kinds of dynamics.
Just keep in mind, even though these things have,
this is calcium signaling you see,
it's the kind of thing you see when you do brain imaging,
there are no neurons here, this is just skin.
This whole thing is just skin cells,
but they're doing a lot of, calcium readout
is a great readout of computation.
And could they be saying something to each other?
Of course we don't know,
this is still a very much ongoing subject of investigation,
but one of the amazing things that Doug and Sam discovered
is that their computational models of these guys
make predictions that differently shaped bots
are going to rearrange their environment
in different ways, so they did a lot of simulations.
And so then we tried it and we just did it in vivo.
And here's what we found.
So here are the bots, the white stuff here is their cells,
they're loose skin cells that we sprinkled into the dish.
And what they're basically doing,
because we made it impossible for them to reproduce
in the normal froggy fashion,
they are basically implementing von Neumann's dream,
they are constructing other,
what they do is they run around
and they sort of collect these skin cells
into little piles, then they kind of polish the piles.
And these piles, because they're working
with an agential material,
they're not working with passive particles,
they're working with cells, what do these cells like to do?
They like to become the Xenobot.
And so of course they create the next generation of Xenobot,
which then matures and guess what, it does the same thing
and then you get the next generation and so on.
So this is kinematic self-replication,
and they'll make multiple generations of this.
So here's here a couple of interesting corollaries
to this and then I'm almost done.
The exact same genome,
so here's the specification of the micro level hardware,
this is what every cell gets to have,
can do one of two things under normal circumstances,
it will do this, it has this developmental sequence,
then it makes these tadpoles that do various things.
But under other circumstances, it makes this,
this is a Xenobot, this is a developmental sequence,
this is I think a month old or something Xenobot,
where did the shape come from, right?
And they have a different behavior
with this thing called kinematic self-replication.
So here's a few interesting things, number one.
Typically when you talk about why a certain creature
has certain capacities, everybody leans on evolution.
Well, for eons, it was selected to do this or that.
Well, there's never been any Xenobots,
there's never been any selective pressure
to be a good Xenobot, this is completely emergent.
They do this, they form this coherent kind of system
with new behaviors, both anatomically and with mortality,
basically overnight, this has never been selected
for specifically, they're completely new in the biosphere.
As far as we know, no other living creature
does kinematic self-replication, that's the first thing.
The second thing is that, how did we engineer these?
I mean, there are no trans genes here.
So the, if you sequence this, all you see
is normal Xenopus latus, there's nothing wrong
with the genome, it's wild type.
There are no nanomaterials, some of them,
some of them Doug can make some modifications
to them surgically, according to the AI
that Josh and Sam built.
But basically, the way we engineered these
is not by adding anything, it's by liberating them
from the influence of the other cells.
So normally, if you just look at the normal path
of this biological system, you would say,
what do the skin cells like to do?
Well, they like to be, and in fact, all they can be,
is to be the outside two-dimensional layer
of keeping out the bacteria.
It's very boring passive life,
they just sort of sit there and keep out the bacteria.
But that's only what happens when they're basically
bullied into it by the other cells.
It's behavior shaping, it's instructive interactions
from the other cells that tell them to sit quietly
and be the outer layer.
In the absence of all that stuff, liberated from all that,
they have a completely different default lifestyle.
And this is it, which you would not see
without this thing.
So, and we don't know what else,
certainly we're studying right now,
all the kinds of behavioral capacities,
do they learn, do they anticipate,
all sorts of things, I'm not making any claims yet about that.
So, but this idea of what evolution I think really does,
and we can talk about why,
I think we have now some ideas about why,
it doesn't produce solutions to specific problems,
it produces generic problem-solving machines.
And so the big thing that every living system has to do,
and I think these are, if I had to make a list,
these are some things that I think are required
for the kind of thing we want.
First of all, I think it's really important
that your parts have agendas.
It's not enough to have dumb parts
and try to engineer an agenda for the whole system.
You have to have a marketplace where every layer
is competing, cooperating,
and attempting to do its own thing.
You, of course, risk failure modes,
you risk parts trying to go off on their own.
That's one of the trade-offs,
but overall it becomes,
I think, an incredibly powerful architecture.
They have to emerge spontaneously.
That is real agents don't know where their boundaries are.
If you are a new embryo coming into the world,
you don't know how many cells you're going to have,
because we might remove half of them,
and you still have to make a good embryo.
You don't know how big your cells are,
because we might make gigantic cells or smaller cells.
You don't know exactly how many chromosomes
you're going to have,
because we can make all kinds of weird chimeras and so on.
You have to be able to, surviving life,
has to be able to play the hand and stealth from scratch.
You really can't take past experience too seriously.
You have to improvise on the fly.
This is what biology does.
So it does not have, nobody says,
this is the border, this is where you are,
and then everything else is the outside world.
It has to guess, and it has to make a self-model,
and it has to make a world model.
Then there are the energy constraints.
Typical AIs, as far as I know,
have all their energy needs met.
They can do whatever they want.
They don't have to worry about it.
Organisms evolved under very stringent energy
and time constraints,
which means that they cannot afford to be
some kind of a Laplacian demon
paying attention to all the micro states of the world.
They have to do a lot of core screening
and they have to kind of bundle all sorts,
they have to generalize all sorts of things that go on
into models of agents doing things, of selves doing things.
That's the only way you have the time
to compute what you should do next.
And if you get good at that,
eventually you turn that on yourself
and you start telling stories about,
meaning making internals and models of yourself doing things.
And this becomes this idea,
why do we all innately believe in free will?
Because from the time that we were single cells,
we had to tell stories about agents doing things
and making choices.
Otherwise, we just wouldn't survive without that ability.
And then there's some other things like the shared stress
and the scaling of stress via sharing it among parts
and so on, we can talk about that.
And the idea that it's open-ended,
living things select their own problem space
and explore it and so on.
So this is, I'm just gonna stop here,
but this is what I tell people is that because of this,
because biology is so incredibly interoperative
because none of the parts make any assumptions
about what's going to happen,
they do their best in whatever environment
that they happen to be in,
every combination of evolved material,
some sort of engineered material and software
is potentially a viable agent.
So hybrids, cyborgs, biorobots, all of this,
there's this huge option space of new creatures,
of new bodies and new minds.
Everything, when Darwin said endless forms,
most beautiful, sort of impressed
with the variety of living beings,
all of that stuff is a tiny dot.
It's a tiny corner.
Everything on earth is a tiny corner
of the space of possible beings.
It's truly immense.
And all of these things,
and we're gonna be surrounded by these things.
Some of this already exists as some hybrids
and cyborgs already exist,
but there's gonna be an incredible variety of them
that we are going to be living with.
This has major implications for ethics, for example,
because up until now,
we were, all of our ethical frameworks
about how to relate to other beings
really boiled down to two things.
Do they look like us?
And did they come from,
do they have the same origin story as us?
And so this is, even today in bioethics sessions
at conferences, people say,
well, does it look like a human brain?
Then we have to worry about.
But the reality is that these categories are,
they're not gonna survive the next couple of decades.
We cannot gauge anything about the potential intelligence
in terms of the type of cognition
and what space they're working in,
by looking at where they come from the family tree,
because they're not going to be on our family tree.
And we have to have completely different frameworks for this.
And the kinds of AIs that we're talking about now
are only one part of this.
We're going to be facing the exact same problem
of dealing with the software AIs in biology.
So if anybody's interested in these things,
there are lots of papers where we go into this.
And I just wanna thank the students and postdocs
that did all the work that I showed you.
And of course, again, the disclosure.
So I will end there.
Thank you so much, Michael.
That was wonderful.
I don't think that I need to introduce Christoph.
We already had the pleasure of having you on the previous panel.
And Christoph is currently a professor at Etihad,
the INI in Zurich.
And he is a first-generation cyber nutrition
in a way as a physicist who is using very broad perspective
on understanding intelligent systems.
And without further ado, Christoph, please.
The stage is yours.
Yeah, I haven't prepared anything.
I didn't know I was expected to prepare anything.
So I am at liberty to respond to some of the things
you have said, the two of you have said.
Let me start with a point,
Yosha, you made about the brain,
which was it is a noisy, it is a noisy entity.
It is not a digital device, not on the basic level.
So if you want to have billions of entities,
synapses or neurons to interact in any useful sense,
you need attractor dynamics.
So there must be certain states of the thing
that have the property of being stable under noise,
of having attractor dynamics.
And we know that the brain is, of course,
essentially a network.
So in each moment of time, a subset of the neurons fire,
and this subset must be stable.
And for a short moment, a metastable,
if you want to call it that way,
you want to go through a trajectory of stable states.
And that means individual fibers,
the individual interaction between neurons
must be embedded in alternate alternative pathways,
which run to the same effect.
So a signal emanating from a single neuron,
going off on different pathways,
many of these signals must come together again
and coincide in space, meaning on the same neuron,
and in time.
And this is a selection criterion
for the kind of activity states
and the underlying connectivity states
that make those states stable.
And I would like to submit the idea
that the brain is totally dominated
by those appropriately shaped connectivity patterns
that have this property.
These connectivity patterns emerge
through a process of self-interaction.
You have self-interaction also on the slow time scale
of individual synapses adapting
and individual synapses finding out
where they can find coincidences of signals.
Each axonal branch has a small choice,
a small sphere of a search space
where it can end up in a plasticity.
And all the endpoints of axons are searching around
in order to find meeting places
where they have a high likelihood
of coinciding with the signals of other branches.
And this process of self-interaction,
of network self-organization,
singles out from the space
of all combinatorially possible connectivity patterns
a very, very small subset.
Let me remind you of the fact
that the brain, the whole organism,
the brain is constructed on the basis
of one gigabyte of genetic information.
And in order to describe the connectivity pattern
of the brain, it's an easy calculation,
you need a petabyte, 10 to the 15 bytes of information
which is a million gigabytes as you well know.
So the genes can only select from the space
of all connections, a very small,
can only be able to select from that space
a very small subset.
And I think it is very important to know more
about this subset of self-supporting activity states
and connectivity states.
So in order to put that in action,
let me remind you that your brain
is in every moment of waking time
representing the situation in which you are immersed.
You have a representation of your actual environment
if you open up your eyes.
And this representation is so good
that you usually equated with the reality out there.
You are not aware of any differences
between this reconstructed,
this model of the outside world and the outside world.
And you are so confident that it is the reality
and not just an imagination
because you continuously do experiments.
You move around so that the perspective
of the world changers
and you test whether your representation
is stays in tune over time with the sensory information.
You do experiments, you touch objects
and you experiment continuously
with the environment
in order to make sure that your representation
is in tune with it, is consistent with it,
is rendering the reality.
Of course, what you are representing
is only a small sector of what is out there.
Your attention is always picking out only part of it.
But what you are picking out is for you,
for all intents and purposes, reality.
I find it amazing that our models,
our theories of intelligence, of brain function
make so little of this very fundamental fact
of our individual life.
Now, according to what I said,
I have been explicit about the data structure
which is used to create this reality,
to represent a model of this reality.
The data structure is, of course,
everybody believes firing neurons,
but I would like to change your perspective
in saying, don't look at the neurons,
each neuron by itself doesn't have any significant meaning.
It is the environment, the neural environment,
the firing environment in which the neuron fires,
which is the important thing.
You have to look at quite a number of co-firing neurons
in order to be able to make sense of it.
When you look at a TV screen and can see only one pixel,
there is no way you can connect that with any meaning.
The pixel is something real, so to speak,
but it doesn't tell you anything.
It has no significance.
In order to understand anything on a TV screen,
you need to see quite a patch of it.
In order to understand anything
of the data structure of a brain,
you need to see hundreds, probably thousands of neurons,
at a time, and a given neuron can take part
in quite a number of such, not an infinite number,
but quite a number of such activity patterns,
which I would like to call fragments.
I think the perspective on the nervous system
has to be changed very fundamentally
in order to see it as a data structure
that is up to the job of representing reality.
Now, I would like, one of the last statements
you, Michael, made was the range of things,
of intelligent things, of organized things
that can be generated, you said, is infinite.
I would rather like to emphasize the opposite.
I've recently read an interesting book
by an author named Morris, a book that
was totally focused on the phenomenon,
looking at evolution, the phenomenon of convergence.
A lens eye has been invented 12 times or something like that.
The facet eye has been invented again and again.
The lifestyle of a wolf pack has been invented again and again.
The lifestyle of social insects or social animals,
you social animals, has been invented again and again.
So the space of all possible organic patterns
that make sense, that have inner coherence,
where the parts support each other in order
to create something that is stable and significant,
that is stable in itself and is coherent with environment,
the space of those shapes and structures is limited.
And here is a great opportunity for theory
to come forward to understand what to expect from biology.
The last thing I want to say is that what is missing,
completely missing so far, almost completely missing
so far from our artificial intelligence,
from our machine learning, is the equivalent
of biological behavior, of behavioral goals.
Of course, an animal has the fundamental goal
of self-preservation of the own structure.
But evolution has built into the individual species
a number of sub-goals, like feed yourself and avoid danger and so on,
and find social contexts, sub-goals,
which make up your life.
And the intelligence in the eyes of many people
is just the ability to pursue those goals in a changing context.
That is a slightly different definition from yours,
because yours was computing functions.
The biological goals is the raison d'etre
of biological intelligence, of course, to pursue those goals.
And I think in the present situation around things like chat, GTP and so on,
it is becoming quickly clear that those beasts are not intelligent in our sense,
because what they do doesn't make sense in the light of the goals
that we all recognize as such.
And it would be, I think there will soon be an important drive towards
installing in such systems the equivalent of the sense of responsibility.
The sense of the consequences and utterance that is made may have
down the line for ethical, for legal reasons.
But also I feel, although I don't have a very strong argument in that favor,
I have the feeling that in order for an entity to be truly intelligent,
it needs to have a set of set goals with which it can pursue in its environment.
Last remark I want to make is about consciousness.
I think in my view, the definition of the conscious state of your own mind,
of your own brain is a state in which you concentrate on one topic.
Your whole brain is concentrated on one topic.
And all the different submodalities in your brain are in tune with each other,
are in mutual understanding.
So if any change happens in any part of the system,
all the other agency, other modalities can immediately respond to that.
So consciousness is not the icing on a cake.
I don't think it makes sense to talk of zombies.
Consciousness is a condition for a system to be functional.
And if you go down in the letter of evolution to simple animals,
I don't think you can find a point that is a point Michael also made.
You can find a point where consciousness disappears.
But consciousness just loses volume.
You lose language when you go from humans to animals.
And you use the imagination of distant future when you go to animals.
And so going further and further down the evolutionary ladder,
the volume of consciousness gets less.
But I would have a hard time, I think when talking about a fly,
it has its own level of consciousness.
So when approaching a wall, it senses the impending approach and reacts accordingly.
So the whole organism is able to react to signals.
So that is my view on consciousness.
Thank you.
Thank you so much, Christoph.
So we're on to the discussion and Q&A question.
I would like to start by asking a question of Michael Levin.
So Michael, you were talking about how a lot of the problem solving within the organism
is actually done at the local scale.
And there is also the interesting remark that was made by one of participants in the chat
that you mentioned that the planaria tail would have become the head had not been bullied
by the rest of the organism.
So what do you think are the communication protocols that are necessary
to enable different types of intelligence?
And is it the case that humans, for example, have cancer
because we don't have enough intelligence at the lower local organismic level?
Or is it because we pursue the higher scale goal
and some kind of trade-off has to be made?
Yeah, great questions.
A few things.
First of all, it is definitely not local.
So one of the key things about all of this stuff is that larger systems make decisions
in spaces that are much larger than their parts.
And so here's a very simple example.
If you have a planarian, it's got a head and a tail.
You cut it in half.
These two cells on either side of the cut, these guys will have to make a new head.
These guys will have to make a new tail.
But they were sitting right next to each other before you separated them with a scalpel.
You cannot locally decide whether you're a head or a tail.
It's a decision that has to take into account, well, do we already have a head?
Do we have a tail?
Which way is the wound facing?
This is a global decision.
It cannot be made locally.
All of this stuff is like that.
And it uses the exact same scheme that bacterial biofilms use to decide when different parts
of the thing should eat so that everybody has a turn and the exact same thing,
the exact same set of mechanisms that brains use to try to synthesize the activity
of individual neurons into some sort of global goal for the rat or human or whatever.
It's an electrical network.
It has certain properties, only a few of which we understand.
But it is absolutely not local.
What this bioelectricity is very good at is at implementing integrated information
across space and time to make decisions in new spaces.
And that's, I think I forgot, what was the second part of your, I lost track of it.
The second part is so do humans, so you remind that humans have cancer,
but some other animals don't.
And some other animals are in fact immortal.
So what is it that, what is it in your opinion that doesn't allow human organisms
to solve the problem of immortality?
Does it have something to do with higher level goals or is it just a lack of intelligence?
Yeah, I think that, well, so there's two, there's kind of a simple answer
and then there's a more interesting answer.
The simple answer that people usually give is simply that all by, so evolution, of course,
doesn't really optimize for long life, happiness, intelligence.
It doesn't optimize for any of that.
It optimizes for biomass, that's it.
And so, right, and so the simple answer is we don't need to be immortal
and cancer resistant because it's perfectly possible to be a human
and have lots of offspring and still get cancer and die after your reproductive years.
That's it, that's the standard answer that it's actually,
there's just not a lot of pressure for humans to do anything different.
Now, I think the more interesting answer is this.
There are most organisms do get cancer and do age, there are a few that are resistant.
Let's look at the planaria.
One really interesting thing about planaria is that many of them reproduce
by tearing themselves in half and regenerating.
Now, one interesting thing that the implications of that are unlike for us.
If you get a mutation in your body during your lifetime,
it doesn't get passed on to your offspring, right?
So because of the Weissmann's barrier in sexual reproduction.
In planaria that do this, every cell that doesn't die from that mutation
contributes copies of itself to the next body, right?
Because they have to repopulate and have to regenerate the new one.
So planaria accumulate mutations like crazy.
So over 400 million years that they've been around,
their genomes are a complete mess.
They basically look like a tumor, they're mix-employed.
Every cell might have a different number of chromosomes.
It's a disaster.
Now, this is really a scandal because nowhere in a typical biology curriculum
will you hear that the animal with the worst genome is, by the way,
immortal, cancer-resistant and highly regenerative, right?
They have the best anatomy.
What's going on?
We're told that our genomes are, that's where your body information is, right?
How can this be?
So this has been bugging me for a really long time.
This disconnect.
And I think we finally have an idea of what's going on.
We just, like two days ago, just published a paper on some simulations
that talk about this.
I'll just give you a very simple example.
One thing you have to do is you have to model not just the genotype and the phenotype,
meaning the genome and then the thing that gets evaluated in these evolutionary simulations,
but you have to model the morphogenetic process in between those two.
The morphogenetic process has certain competencies.
For example, some of them I've showed you, there are many more.
So for example, if there's some mutation that puts your mouth off to the side,
the mouth is perfectly competent to come back where it needs to be.
If it's a mutation that causes you to fall apart as an early embryo,
you'll just be a bunch of twins, multiples.
If we took some eyes, Doug Blackiston did this to us,
he took eyes and put them instead on the animal's tail,
they can see perfectly well out of those eyes.
No problem, no period of adaptation needed.
It's all good, the nerves come, find the spinal cord, it's all good.
So all of those kinds of things,
abilities to make up for these kinds of issues we call developmental competencies.
Now, one thing that happens is that when you have an animal with a little bit of developmental competency,
you come up for selection and it turns out you're very good, right?
But why are you good?
Selection cannot tell whether you have a great genome or you're good because you're highly competent
and you fixed all the things your genome actually was pretty sloppy about.
So that means it's harder for evolution to see the good genomes.
You can't do all as much work in perfecting the genome,
but what it can do is crank up the competencies, right?
So when you do that, then of course that makes the problem worse
because the more competent you are, the less it's possible to find the best genomes.
And so there's this positive feedback loop that's ratchet
and there are some other things that sort of work against it.
But I think what happened is that, and this is very much a hypothesis still,
I think what happened is that planaria went all the way,
meaning that in that lineage, probably because they reproduce this way,
it doesn't make any sense to assume that your genome is any good.
And the only architecture that survives is where the algorithm is so good
that we're going to make a perfect worm no matter what happens to the genome.
This means aging, carcinogenic mutations, the algorithm,
meaning the machinery that maintains that goal state
and physiological and anatomical space is so good
that it can pretty much ignore a lot of issues in the hardware.
Most of us aren't like that. Salamanders are sort of in the middle.
So salamanders are highly regenerative, but they age and die.
And so I think what salamanders sort of went part of the way there
and they can fix certain things, but not enough to really keep it going forever.
Mammals probably stopped even earlier than that.
But I actually don't think any of this is fundamental.
I mean, we're working on regeneration in mammals now.
I do think someday we will all sort of regenerate like planaria.
I think it is going to be possible.
But I do think that evolution makes these trade-offs that they're
just easier ways to be a human, I think.
So question for everyone.
So in terms of communication protocols,
to what extent is intelligence simply the ability to organize the cells
and what are the conditions necessary for that to occur?
And to what extent is intelligence is some internal competence,
competence of the cell or neuron or whatever computational unit we're talking about?
Yosha, would you like to start?
I'm currently thinking about the question of whether it's possible to make something
that is as long lived as the planaria that doesn't look like a blob.
There seems to be some correlation between the structural coherence of the organism
and the detail and solution that it has and the degree of fidelity.
That is expected from interpreting the operators defined in its genome.
And more specifically, I wonder what, how we can formalize the idea
that Michael put up earlier of multi-scale organization and such a way
that it leads to coherence.
What is the criterion that makes a single agent coherent in itself
and leads to this coherence on a particular level?
Arguably, our own mind is some kind of society of agents
and the organism has lots of local agents.
Every organ is an agent in a way.
Every cell is an agent.
But there is also a globally coherent agent.
And that is different from having multiple twins coexisting next to each other
and forming some cooperative chimera.
But that leads to some global element.
On the other hand, Christoph has pointed this out.
If you think about consciousness, it seems to relate to a unified experience.
And this unified experience of all sensory data is what makes it specific.
What is interesting about consciousness
is that I normally don't have multiple conscious experiences
unified in one perspective.
How is this unity being realized?
Or more generally speaking, can we come up with some kind of formal criterion
that defines how everything has a place in the greater whole
and the condition needs to be measurable
and lead to globally coherent behavior on the next level of organization?
If we take this to account and if you look at Michael's diagram
that he brought up in the context of ethics,
all the different agents that all seem to be centered about individual humans,
it turns out that individual humans are not the main agents in the human sphere.
The organizations of humans are much more powerful than individual human beings.
And while individual human beings implement these organizations for the most part,
we gradually transition more of that to machines that we are building.
It seems to me that there is different levels of organization
that transcend the individual organisms.
This multi-scale organization doesn't stop with humans
and the next scales are getting more and more agency.
Also, I don't think that humans are all that important.
It seems to me that humans are a very specific thing that has a very specific role.
All our cousin species are dead.
Women are not long-lived species.
And it seems to be that the reason why Gaia brought us up is that we fulfill our job,
which is to burn all the fossil fuels as quickly as possible.
This is what we're here for.
Then we burn ourselves out.
If we manage to teach the rocks how to sink in the meantime,
that's a stretch goal.
But after we are gone, there will be more intelligent species.
And we are a very specific one, right?
We are this type of monkey that is not going to get his hand out of the cauldron trap
if there's fossil fuel inside.
And that's somewhat predictable if you look at the way in which we work
because we are very smart and intelligent on very short timescales,
but we are not globally coherent.
We don't find ourselves in this global, coherent, godlike,
organization.
And if we succeed in building the next level of intelligence,
maybe this next level organization, some kind of very fast,
tightly integrated globally,
coherent mind is going to be emerging.
And maybe humans will play a very small part in whatever is going to come
afterwards.
But it's not about us, right?
Life on Earth is not about us.
Life on Earth is about the cell.
And overall, it's about fighting back entropy.
It's about sustaining yourself through maintaining complexity.
So my question would be to Christoph and to Michael,
can we come up with the criterion that determines coherence?
Yeah, yeah, I think the important thing is, as I've said,
that a coherent form has a kind of stability.
It is made up out of continuous variables,
which are prone to noise.
And for the whole thing to have a lasting existence is a different
signals that converge on one point.
They have to agree with each other.
They have to stabilize each other.
The same way as a crystal is formed,
a rigid body in that the individual forces between atoms,
which are also acting, of course, in a liquid,
but are not able to form something like a stable shape in a liquid.
But in a crystal, they have fallen into a configuration in which
in each individual interaction,
each individual force gets support by other indirect pathways.
And so I think just as in the space of all mathematics,
those pieces of mathematics that have been found are singular points
that admit no change if you have a direct interaction
that admit no change.
If you have come up with the idea of a group,
then the rest of the whole story,
thousands of pages in mathematical journals,
follows by force from the definition of what a group is,
a group of finite number of elements.
And the same way the shapes that dominate life have this inherent
self-consistency that is the different chains of forces
that interact, support each other.
Christoph, my apologies.
Christoph, my apologies.
I think we're almost on top of the hour and Michael has to go at 11.
So, or, well, in one minute, Michael,
any last comments from you before we let you run?
I'm sorry for interrupting.
This is extremely interesting.
Thank you so much for having me here.
This was amazing.
Thanks for coming.
I really wanted to introduce you to Christoph and have one more
conversation with you after our lucky podcast.
And so I'm very, very happy that you could come and hope to see you
again soon and stay in touch.
I really like many of your ideas in the space of self-organizing
systems.
And it's very lucky that we could have you here today.
Christoph, do you have some more time to stay on?
Yes, I do.
Perfect.
Thank you very much, everybody.
I've got to run.
I'm happy to do more.
Great meeting you.
And I'm fascinated by all the examples that you have shown.
It's just great.
Cool.
Thank you so much.
See you all later.
Thank you, Michael.
Thank you.
Bye.
So we're going to continue for a little bit more.
If it's fine with you, Tanya, you have time, right?
Yes, yes, of course.
So Christoph, you mentioned that you think that the genome is that
contributes to the brain is a gigabyte.
But that's the whole of the genome, right?
And so it's the part that quotes for the brain is going to be a
small fraction of that.
Yeah, I wonder how much information is remarkable that the
genome has expanded from mouse to man.
So making a larger brain doesn't need more genes.
Yes.
Of course, if you take the genome and you drop it into physics,
it's not going to form a cell.
So, and every cell that exists is the result, except for the
first one of the server application of another cell, right?
So all cells in some sense depend on the existence of a cell
and the cell is not empty.
And I wonder what the comorgo of complexity of the cell itself is
right.
How complex is this machinery that is being copied and copied all
over?
Maybe that's much larger than a gigabyte.
And of course, the principles of self-organization embodied in the
cell lead to the search for more coherent organization on the next
level.
And so the cell is already an agent, a self-replicator,
a Turing machine, and an entropy extractor.
And the self-replication is the main feat of the cell to make that
happen robustly in physics.
And this is what enables everything else.
I wonder how much of that we need?
Yeah, I could imagine that the brunt of the genetic information
needed for a single cell already.
And then you may just add, I don't know how much,
on top of that in the form of the preexisting cell.
Each cell is the daughter of another cell.
And so there is some information in the arrangement,
at least in the molecular arrangement of the cell,
that needs to be counted as information.
Although I have a hard time seeing that that will be in a
significant way more than a gigabyte.
You know, at the present time, there are groups that try to create
artificial cells.
And they learn the hard time that a lot needs to be in place to
make a cell tick, you know, the different kinds of membranes.
The membranes shape themselves, that's very important.
They shape themselves into the Golgi apparatus,
things like that.
And so there is information in the preexisting cell.
But quantitatively, I don't think it is going to be more than a
gigabyte.
So I guess if you were to build an intelligent system,
say artificial visual cortex, how would you go about it?
And what would be the main differences to the existing
approaches?
Well, I think, you know, when you look at, with your own brain,
with your own eyes, at a moving body, you can predict the next
split second and compare it to the signals that come in.
So we have this machinery in our visual system that is able to do
the differential geometry, taking into account the shape of the
surface, the play of light on the surface, the movement.
And so what I would create is a sequence of an array of ARIA,
like V1, the primary visual cortex, and MT, which is concentrated
on motion, maybe V3 concentrated on color and so on,
and a number of submodalities, which each are two and a half D
entities.
They refer to the two-dimensional way we perceive the world and the
with added internal spaces of quality spaces, like color,
a three-dimensional space, and the texture, I don't know what,
maybe a 40-dimensional space, depth, a one-dimensional space,
motion, a two-dimensional space, and so on.
And so they reflect the retinal image on the one hand in these
different modalities, and they, another set of such ARIA,
they build up the invariant static scene as such,
couple them by projection patterns, which have to be dynamic
because if you roll your eyes, the image moves.
And in order to connect the moving images to the static
representation, you need dynamic projection patterns.
These are very important in themselves in the deforming
projection of the moving retinal image of a rotating object.
The deforming projection of that onto a static version of that
thing tells you about the shape of the object.
And so I think what you need is a huge array of local
texture, local modality descriptions, all linked together
with dynamic patterns, such that the whole thing is a self-supporting
attractor space and describes the external world in detail as far
as you concentrate on it, of course.
It's a, I think it's quite an amount of work that is necessary.
I once tried to put together a company where I believed I would
need something like six or eight intelligent co-workers that
together create this structure.
The first feat to convince investors would be to let the system
look at moving objects and build up an instant model of that
moving object in its 3D form, an instant replica of that,
with the ability to handle it, to connect to self-motion,
to connect to manipulative motion.
I think, you know, the complete abysmal failure of the car
industry to come up with level five autonomy has very much to do
with the inability to represent the traffic scene in this sense.
And so my idea was I would get, investors would be ready to
invest in that direction.
However, I found out that this whole perspective of mine is so
much sailing against the wind that I wouldn't even find the
co-workers to help me create it, let alone the investors.
I suspect that part of the difficulty to create self-driving cars
has to do with the way in which the model is being generated,
which means a deep learning currently relies on building
classifiers for individual things.
And there is no end-to-end train system for deep learning that is
self-driving in a sense, and it is at the same time reliable.
If you want to create reliable behavior that is rule-based,
that where you basically have a set of traffic laws and safety
measures and precautions that are built into the system that
drive all the behavior, the object that this system is going
to relate to are crafted by hand.
So the self-driving car exists in a handcrafted software world
where all the objects are being defined by a developer.
Whereas the world that we are living in is an open world.
And when we see new phenomena, we are able to integrate them
into this model.
And when the self-driving car sees something new that hasn't
seen before that the developer didn't expect, like a bicycle
painted on the outside of a truck, this might lead to confusions
for the classifiers.
Yeah, you made a very important observation that kids learn
on the basis of very few examples compared to deep learning.
They learn, moreover, in a very simple environment in their nursery
with fairy tales and interacting with a few people and playing
with objects.
And then they walk out into the world and understand traffic
situations.
You don't hand down the key to the car yet because they don't
have a sense of responsibility.
They can't foresee the long-term effects of their actions.
So you only let them drive when they're 18.
But they understand traffic scenes very well when they are six
or 10.
So all of this is driven by learning by interaction with a
simple environment and generalization from there.
Yeah, well, of course, in 99.7% of all the cases, the self-driving
car is good enough.
It's mostly the long tail of cases that leads to situations where
the system is producing undesirable behavior.
I was joking a couple of years ago that whenever a journalist
writes that there will never be self-driving cars, police is
stopping Tesla with the sleeping driver safely on their way home.
And so in many ways, self-driving cars exist.
And they are almost as good enough in the sense that they are
better than a really, really bad driver.
But they're just not working to the degree of perfection of a
very competent driver.
Yeah, it's very mean to ask them to be so perfect, much more
perfect than humans.
They are, as you said, they are, if all cars were self-driving,
traffic would be much more safe than now.
But the public takes it very badly if an accident happens that
could have been prevented.
Yeah, we're also in an interesting situation where the public is
mostly the media.
And the media is at the moment in the US very much seeing itself
in competition with the tech industry because they are competing
for the same advertising revenue for the most part.
And so it's at the moment very difficult to find articles that
are optimistic and positive about technological developments
in the media, I find.
So this creates a very unique situation where even useful
developments are delayed that could save lives.
Because they're being seen in competition with existing
economic and social structures, which also creates enormous
pressure on AI models like JetGPT.
I think that JetGPT is a tremendous achievement.
My kids have been playing with it.
My daughter has been creating a story of a horse that she got to
know on the way home from school and then created several
variants by modifying the prompt until she had the story that
she liked.
And then she turned it into a poem that's very catchy rhymes.
My son used it to explain the system,
to explain to him how to implement a platformer game.
And it was explaining him how to structure the project.
And then he was asking how to make an event loop in Python
and it printed out source code and explained the source code.
And he spent several hours copying the code and replete
and getting it to work.
So to me, these are systems where you have a little bit of human
in the loop to make it coherent for a particular task.
And it's amazing what the thing can already do.
And it seems to me what's missing to get the system to work is to a
system that makes it coherent.
Basically, you can decompose the mind into perceptual systems that
can in some sense to image guided and audio guided diffusion to
coalesce to an internal state that is able to reproduce the
sensory data.
And then confabulation to build alternatives for solutions,
alternatives for what could be alternatives for the future.
And then the third component, which doesn't exist yet, which is
proving from first principles what works, basically rejecting
those generations that don't work.
And then learning those that worked and building up the system
in a way that is continuously learning.
It also seems to me that many people cannot change their
opinion in real time.
And you have talked to a person that has a strong opinion about
something that moves deeply into their mind.
You can present them as arguments, but you have to talk to them the
next day if you want to see any changes, which seems that seem to
be parts of mental organization at least in some people require
offline retraining.
There's limits to what we can do in online learning.
Some balancing needs to be done offline while we are decoupling the
system from the environment and producing data augmentation and
restructuring.
I wonder how much of that retraining will also be built into the
systems where the artificial systems will have to sleep and to
dream.
Yeah, before you take an important decision, you have to sleep
over it and give your subconscious mind the opportunity to
work on making the ideas more consistent than you are able to
make them under conscious control.
Yeah, very much so.
So I think you rightly said these achievements of GPT-3,
TETGTP and so on are extremely impressive.
It's very difficult to see where the limit is.
I agree with you.
The transformers have a new, very new architectural feature,
which is the online computation on the fly of connections and
of these representation vectors.
They are computed on the fly.
That's all very promising.
But these systems don't have any insight into real world
geometric mechanics and so on representations of what they talk
about.
And they are lambasted mainly for that reason, that they don't
know what it means, something is dead.
They just know how people talk about it, but they don't know the
significance of it or the geometrical arrangement of something.
And so that is, of course, due to lack of insight, lack of
interaction, you know, they cannot play with toy objects as kids
do and cannot get the corresponding insights.
But I still think that what is missing, what is sort of needs to
be improved is the data structure of representation of
themes and of realities.
And I don't think these vectors that I use these days are up to
the job.
I think that the embedding spaces are not necessarily represented
in full, right?
If you think about the embedding space as a manifold with 30,000
dimensions and a lot of resolution, trying to expand this space
and store it in memory is not going to be feasible for the most
part.
So instead what is required is a language that allows to sparsely
and efficiently construct representations in that embedding
space.
The embedding space is a mathematical construct that is basically
every dimension is a function that describes a feature.
And that feature has parameters.
But I see every dimension is a parameter in that feature.
Yeah, that's right, of course.
So, you know, I once applied for funds to do face recognition and
the idea was to collect data images which varied in all those
dimensions, the identity of the person, the illumination, the
expression, the texture and so on.
And I didn't get the money and I'm very glad I didn't get the money
because this 15-dimensional space or so cannot be filled with
examples.
That's totally impossible.
There's too much space in high-dimensional spaces.
That's the point you want to make, I suppose.
So one has to find a way of representing only sub-dimensions,
low-dimensional projections of that and a means of pasting them
together as an equivalent of the high-dimensional thing.
It turns out that when we conceptualize an object, it's chunked
and a chunk is basically a node that is composed of features
that define the nature of that bunk.
And you have these seven plus minus two features.
It's less, so it's more like five, which means that if you can
define an object by five features, you have a local function,
a locally five-dimensional space.
Maybe sometimes it's nine-dimensional, but it's not much
more.
And these few dimensions allow us to construct a family of
operators that would allow us up to these few dimensions construct
all the 30,000 other dimensions or millions of other dimensions
depending on how we look at this function space.
Right.
So the essential point here is that a high-dimensional thing gets
projected down in our brain onto low-dimensional representations
plus the ability to glue them together.
And this gluing together, I don't see in the present
neural technology.
I think that it happens on a level that we normally don't look
at.
It happens in the activation traces in the network.
So it's not in the weights between the links.
And it's also not in the synaptic connections between neurons.
It is in the content of the traces that are moving through this.
So the neurons and the nodes in the neural network are providing
the computational machinery to modulate these patterns according
to the content of the patterns.
And it's the content of the activation wave that is determining
how the activation wave is being processed.
This is something like a distributed computational pipeline.
I'm involved in a multi-month or probably multi-year intensive
discussion with a colleague at Amy in Zurich,
Institute of Neural Informatics, Matthew Cook.
And he doesn't want to hear of dynamic mappings.
He says anything like schemata and roll fillers,
that's all nonsense, he believes.
And he claims all you need is components,
which he calls them slips of paper,
onto which various features are written into slips of paper
can overlap in a subset of the features.
And so that makes clear how they belong together.
Components, each of which is a small set of entities.
And they overlap in subsets of these entities.
And that way they can cover a complex situation.
I defy him again and again to create that way a system
that can do something like invariant object recognition.
Or the application of a syntactical rule,
like subject, verb, object to an arbitrary set of components,
of appropriate components, of course, to nouns and the verb.
And which is, of course, very important,
the cognitive scientists insist on that,
on the ability to impose an abstract pattern
onto concrete elements.
And I think for that you have to have variables
that make clear this abstract node belongs to this concrete node.
If you want to represent the sentence John loves Mary,
you have to make clear that the subject is linked to John
and the object is linked to Mary,
because you can also have the sentence Mary loves John
and then they are called in a different way.
You need variables to make that distinction.
What are those variables?
That's what I call the glue,
that glues together the abstract form and the concrete elements
that make it up, for instance.
Or the texture, you know, the computer graphics people
have a very good ontological theory of visual scenes.
They can create them in a very convincing way
and they create them out of part descriptions
like shape only or texture only or illumination only
or motion applied to a shape
and have a way of putting them together.
And for that you need to have variables
that make clear this textural element belongs
onto this form, on this point on that form.
And what are those variables?
We need to find the minimal set of link types
that we need to...
One minimal set.
We didn't optimize for the smallest one
to perform all the semantic representations
that we wanted to have.
And the model that we used was inspired by Aristotle
and his Four Causa,
which basically means you have Causa formalis
and these four cases describe
partonomic links part of and being composed of
and being caused by and leading to.
So basically you have lateral links
that give you a causal ordering
and you have compositional links
that allow you to compose a script
or a task of subtasks.
And in this way you can describe arbitrary scripts
and these arbitrary scripts can express arbitrary functions
when you combine them with low level operators
that can, for instance, perform some basic operations
on the network, sense data in the environment
in the network itself and so on.
I mentioned earlier on that I think of intelligence
as the ability to construct a path
to a space of computable functions.
So intelligence is not the ability to compute the function,
every computer can compute a function
without being intelligent.
The trick is to discover that function in the first place
and to discover this function
we basically have three perspectives on how to do this.
The first one is to converge to it.
That is what deep learning does.
You have a space of possible functions
and in that space of possible functions you make large enough
you start out with some random function
and then you modify that function along many dimensions
nudge it many billions of times or trillions of times
until it gets close to what you wanted to do.
And you follow a gradient.
For this it needs to be differentiable
and this algorithm of stochastic gradient descent
using back propagation is still the workhorse
of all machine learning at this point.
And a second approach is to do hierarchical pattern matching.
So you look for operators that you've already learned,
a small library of efficient operators that you have evolved
and evolution is all you need from this perspective
that lets you get to the situation that you want
based on the configuration that you have.
So these operators are basically looking for activation patterns
that they match on
and they change the activation pattern into the next one.
And in this way you can perform arbitrary functions in the way
this is also the way in our computers are computing functions.
And the third one is construction
and construction requires some degree of memory
and because you need to be able to retrace your steps
and you need a way to justify the steps that you're making
and when to retract them.
Our consciousness seems to be strongly involved
in such a construction process
where we have a stream of consciousness that allows me
oh I tried this thought before and this didn't work
so I now retrace it, retract it, modify it
and I think this one should work for the following reasons
and then I see the outcome and say no it didn't work
so this reason was not wrong so I try the next thing
and this is something that is difficult to achieve
just this pattern matching or this gradient descent.
So this constructive discovery of solutions
seems to be crucial
and while it seems to me that our deep learning models
are not very good at constructing
they are very much able to emulate
what it would be like to be constructing, right?
So while GPT-3 or chat GPT are not conscious
they are able to create a story about something
that is conscious while they are unable to reason
they create a story about the reasoner
and draw on the inferences from that reasoning
and the more closely you describe the reasoning
that's reason step by step and so on
the better the results can become
and so it's very difficult to determine the difference
between a system that pretends to perform a certain thing
and it actually does it, right?
If you can pretend it well enough you're actually doing it.
Yeah, yeah, so the same way as you pretend to be conscious
and I know the only conscious being in the world is myself.
When did you figure that out?
We know, we know, we're NPCs.
How did I figure that out?
Okay, yeah.
Christoph, I enjoyed this very much.
We are at the end of my time limit for now
and I hope that we get to continue the conversation
and I hope that we get to continue the conversation
and I hope that we get to continue the conversation
soon in Zurich and looking forward to talking more to you
and I'm very glad that you could make it today.
Yeah, it was a great pleasure.
I just had to take an earlier train from Frankfurt to Berlin
because I had to be there only tomorrow morning.
I'm sitting here in the room in the so-called Harnak House
of the guest house.
Okay, have a nice day.
Thank you so much for organizing this
and setting everything up and supporting our discussion
and I'd also like to thank our audience
for paying attention, asking questions.
We didn't get to discuss all of them
but I'm very glad that you could make this event happen.
Thank you, Christoph.
See you soon.
Thank you, everyone.
Bye-bye now.
Bye-bye.

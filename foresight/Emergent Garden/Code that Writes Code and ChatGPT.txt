This animation was generated by a short piece of Python code.
It uses the math animation library manum from 3blue1brown to make this square fractal.
It is recursive, the squares are inside of themselves, and the code uses a recursive function
that calls itself.
I've used manum many times before in previous videos, but I did not write this code.
It was entirely written by ChatGPT, the AI program that can write programs.
This was its first try when I asked it to give me any creative animation using manum,
and that was GPT3.
Here's what GPT4 gave me for a similar prompt.
For this one, I changed a few parameters so it would add more branches and slow it down
a bit, but it was still mostly made by ChatGPT in one try.
It doesn't always work out that well of course, this one took a few tries.
The first time it tried to load the file path2yourheart.svg which does not exist at all, and the second
time it called this .weight function that doesn't exist.
It makes stuff up sometimes, what they call hallucinations.
Nonetheless, after explaining the problems to ChatGPT, it debugged its own mistake, tweaked
the code, and you get this weird pulsing star thing.
I have been having way too much fun playing with ChatGPT and doing all kinds of things
with it, especially creative coding challenges.
I would call ChatGPT creative for the same reason that I would call AI art creative.
It can intelligently combine the languages, methods, and ideas that it is trained on.
Just look at this self-portrait it made, again on the first try without any human revision.
All the manum animations you see in this video were at least partially written by ChatGPT.
Typically, it writes the brunt of the code, especially the boilerplate stuff, and then
I finish it up, focusing on the visual stuff.
Sometimes I'm just picking colors and tuning parameters, but often I'm debugging error
messages and rewriting code myself.
ChatGPT has major limitations, but is still a surprisingly helpful debugger and pair programmer.
It's made to program with you, and works best if you at least kind of know what you're
doing.
To try it on a more difficult task, I wanted to implement an upgraded version of an old
evolution simulator called Biomorphs.
The original Biomorphs worked by making variations of branching recursive tree structures and
allowing you to select one for the next generation, where you could then select again from its
offspring.
It's the kind of simple evolution simulator I love.
I asked ChatGPT to make a web page version of Biomorphs, but to creatively expand on
the original idea using 3.js, a 3D library for the browser.
This is its original output, with no human revisions, and it's a good start.
After a lot of back and forth and a few hours of pair programming and debugging, this is
what we ended up with, Biomorphs 3D.
It uses recursive branching structures of different multicolor 3D shapes, all determined
by a set of 11 genes.
You can click on the Biomorph you like the most, and the other three will turn into its
mutated offspring, and you can continuously evolve from there.
They can easily grow very large and complex and lag out your browser, but you can just
select for smaller ones.
The final product was much more of a joint effort, and while I did change some important
elements of the program, especially visual things that I couldn't show ChatGPT, most
of the code, by word count, is AI written.
I am not super familiar with 3.js, and I would absolutely say that ChatGPT made it
easier to get started with my idea, and not get stuck setting up the code or reading documentation.
That means I don't fully understand everything about this program, and debugging it was tricky.
It's a very simple project, and probably still has some hidden flaws and limitations,
but it is still fun to evolve the Biomorphs.
You can play with this program yourself, there's a link in the description.
In Biomorph Space, you can find suns with planets with moons, molecules, minerals, organic
shapes, trees, and atoms.
Have fun exploring.
ChatGPT, this young new programmer, is a very impressive and strange piece of software that
can write other pieces of software.
It is code that writes code, a programming program, if you will.
It's not a very traditional program, it is a large language model, which means it's
a big neural network that is trained to imitate a huge dataset of human written text and code.
As a piece of software, its source code is complex and layered.
In a sense, the largest and most important part of that source code is the data.
The data is compiled into the model through the training process.
I like to think of large language models as machines that sit on the end of a sentence
and generate the next most probable word, given all the previous words that came before,
including the ones it just generated.
These models are often derisively called glorified autocomplete, or next-word prediction machines,
which is a perfect description.
However, to make a model more conversational and controllable, you have to refine it with
more precise training.
ChatGPT was further trained using a novel method called RLHF, Reinforcement Learning
with Human Feedback, where good behavior is reinforced with positive rewards, and bad
behavior is discouraged with negative rewards.
The reward function is learned through human feedback, where people select the responses
they like the most.
Under this paradigm, the model is more of an agent playing a game, where the high score
comes from following human instructions.
These relatively simple methods and architectures, when scaled up with big models and huge, high-quality
datasets, turn out to work pretty well.
In terms of programming skill, GPT-4 is clearly limited, but still pretty good.
It's not the best programmer I've ever met, but it's also not the worst.
It certainly has broader knowledge than I do.
It can program in way more languages.
In one case, it even taught me about an obscure function in PyTorch that solved a problem
that I couldn't solve by searching the internet, and now I can generate fractals on my GPU.
All this it can do while telling some very bad jokes in the comments.
This is all great, but it's also a little scary.
Programming is my day job, and I like programming.
Do I really want ChatGPT to program everything for me?
For the moment, it cannot do that, but one can imagine a future model that is more of
a fully automatic programmer, something that could maybe interact with a command line,
as ChatGPT can sort of do now.
It could potentially write and execute files, read outputs and error messages, automatically
debug and converse with human managers.
Experimental AI agents like this already exist, like AutoGPT or BabyAGI.
Give it an initial starting prompt, a goal to accomplish, and it will run off on its
own.
These agents are powered by GPT-4, and take advantage of its many emergent capabilities,
things like hierarchical planning, breaking down tasks into sub-tasks and sub-sub-tasks.
They also use self-reflection, where the agent is automatically prompted to ask itself if
it really completed its instructions correctly.
This way, it can catch its own mistakes without being told what they are.
These agents do not work great, they will fail for most tasks, and often get caught
in infinite loops.
However, they can and will be improved upon.
A future version of GPT or some other language model could be trained specifically as an
autonomous agent, and could really start to be more of a fully automatic programmer.
Other kinds of auto-prompting GPT systems exist, such as Hugging GPT, which is specifically
designed to build machine learning programs by composing different AI models into a working
solution.
It's also not that reliable, but you can theoretically use it to build all kinds of
things.
It could even, if you think about it, build another large language model.
So, if ChatGPT is a programming program, no less a machine learning program that can
write machine learning programs, could ChatGPT program itself?
A piece of code that writes itself is called Aquine.
Here's an example, written by ChatGPT, probably just copied straight from its dataset.
This elegant little Python script is a Quine.
It takes no inputs and outputs exactly its own source code.
It is a self-replicating program, a replicator, and we can come up with different kinds of
replicators that aren't technically Quines.
For instance, here's a more custom script, also written by ChatGPT, that copies its own
file contents to a new file, then executes that file as a child process.
The child process then does exactly the same thing, makes a copy of itself, which then
makes a copy of itself, which then makes a copy of itself, which then stops after 10
files, otherwise it would fill my hard drive with copies of itself.
While the script is not technically a Quine in the strict sense, it still self-replicates,
and we might say it Quines.
A computer virus also Quines, by automatically copying itself from machine to machine, though
with much more nefarious purposes.
ChatGPT is not a Quine.
It was not trained on its own source code and thus can't reproduce itself unless we
give it its own source code as input, which, for a Quine, is considered cheating.
But let's cheat anyway, because this takes us to a very interesting place.
Rather than just self-replicating, could we give it its own source code and ask it to
self-improve?
Now, again, GPT's source code is an incredibly complex piece of software built from layers
of programs and processes and data.
There is no way to feed all of that into GPT.
But we could start by just giving it small pieces of its source code, say the stuff that
defines its architecture, or training process, or data cleaning, or whatever.
GPT4 might not be able to offer any improvements, but I bet it could come up with something.
Maybe it could optimize a small function, or just one line of code, fix some minor overlooked
bug, or just do some organization and documentation and commenting.
Throw that in with all the work of human developers and AI researchers, and the end
product would be the next iteration of GPT.
Could GPT4 help build GPT5?
Now, here's where things get interesting.
GPT5 would be a better programmer than GPT4, so let's give it its own source code again.
If it really is a better programmer, it will be able to improve itself even more than before.
You can see where this is going.
Do this over and over and over.
This is AI building AI, a strange loop.
If it gets advanced enough, you could do something like what AutoGPT does.
Let it be a fully automatic programmer.
Give it direct access to the entire project, the source code, the datasets, everything.
Let it read and write and execute files, plan and reflect and debug and train.
Maybe even innovate and experiment and test ideas, and iteratively improve on its own
next version piece by piece, alongside human developers, until it's ready for full deployment.
Now, it needs to be improving itself according to some goal or metric.
You would want it to improve as a chatbot, as well as a programmer, and across a growing
set of other tasks and domains.
You could use OpenAI's own evaluation framework and progressively tack on new metrics, tests,
games, and different types of inputs and outputs.
Its goals would also be defined with natural language and a starting prompt, like you do
with AutoGPT, and this would be a much more open-ended goal.
Something like, develop the next iteration of yourself.
It should be a truthful, unbiased, helpful, rational, friendly chatbot that is an excellent
programmer, etc., etc.
You could only really do any of this, of course, with a far more advanced model than GPT4,
but current chat GPT can already reflect on its own limitations and propose ideas for
how to self-improve.
Now, if you, like me, are skeptical that a next-word prediction machine with shallow
reasoning and hallucinatory knowledge could actually do any of this, simply consider the
fact that you don't need to solve all of these problems to get the process started.
You do not need to start with a perfect programmer.
All you need is a halfway decent programmer, one that is just barely good enough to make
the slightest improvement to its own source code.
The human developers would still be doing almost all the work.
Eventually, it may turn out that large language models need to be replaced with some other
completely different paradigm, but so long as we're still dealing with a program that
is good at writing programs, it should still be able to self-improve.
We should not underestimate the immense potential of self-replicating code and recursive self-improvement,
even from simple beginnings.
After all, we are the product of self-replicating code.
That is not a metaphor.
Your DNA, the stuff that makes you and your brain and your intelligence, is genetic code
that makes copies of itself.
It is code that writes code.
That writes code.
That writes code.
DNA self-improve through the slow process of mutation and natural selection, trial and
error.
By contrast, our programming program would evolve through deliberate, foresighted, goal-oriented
self-improvement.
It's like having an organism that can understand and edit its own genetic code, and thus make
itself better at editing its own genetic code.
A recursive self-improving AI might begin slowly, but gradually accelerate as improvements
accumulate and compound.
It's a positive feedback loop where it improves the thing that does the improving.
The smarter it gets, the smarter it can make itself in the next iteration and faster, too.
It's conceivable that a far future version of a self-improving language model could start
to outweigh the contributions of its human developers.
A really creative one could potentially invent novel algorithms or neural architectures or
new programming languages that we wouldn't fully understand.
One day, it could write the next iteration of itself without any human input at all.
What would the iteration after that look like?
Could it become super-intelligent, better than any human at any cognitive task?
At this point, things would really take off the breakneck pace of AI development.
Today will seem like a snail's pace.
The program will continue to improve and improve and improve faster and faster and faster,
smarter and smarter and smarter.
The process I've just described is called an intelligence explosion, and we seem to
have just walked through a recipe for one.
A self-improving AI causing an intelligence explosion is an idea that has been described
and predicted by many different people, and it is as promising as it is terrifying.
On one hand, a friendly super-intelligence could be the best thing we've ever made.
It could cure diseases and invent technology and help us solve monstrously difficult problems.
On the other hand, how could we possibly hope to understand or control such an explosive
process, one that results in something that is smarter than all of us?
There are many ways that this could go horribly wrong.
For instance, you could imagine a particularly nasty computer virus that's not only written
by an AI but is an AI, one which self-improves as it copies itself throughout the internet.
The essential ingredient to kick off this explosion, the fissile material for the atom
bomb of intelligence, is an AI program that is good at writing AI programs.
We are currently in an arms race to build ever more powerful and potent versions of
exactly that.
It seems we are building a bomb.
Can you see why there is growing anxiety in the AI community and a desire to slow things
down?
Okay, let's take a step back.
An intelligence explosion is a bit of a science fiction idea, and it's prone to over-exaggeration
because it's fun to speculate about.
I'm not suggesting that tomorrow GPT-4 will wake up and start pumping out neurotoxins.
In practice, we will face many challenges before we get a truly self-improving AI, if
that's really possible.
Maybe development will be more linear, or taper off pretty quickly, or we could hit major
roadblocks.
Maybe the current tech tree we've invested so much in will just dead end once we run
out of data, and there will be no further way to improve it, no matter how good of a
programmer you are.
Maybe these next-word prediction machines just aren't powerful enough to recursively
self-improve.
But maybe they are.
No one really knows, including the people that made GPT-4.
The only way to know for sure is to try, and that's the scary part.
In my opinion, we shouldn't simply assume that self-improving language models are impossible
or completely safe.
It seems perfectly reasonable to me that a sufficiently competent programming program
tasked with recursive self-improvement is both possible and inherently unpredictable.
It may take a while to get there, but small-scale versions of self-improving language models
will probably be built soon.
There is currently an ongoing project for a self-improving auto-GPT, and I might play
around with the idea myself in the future.
These are safe, small-scale experiments that don't even touch the core model, but someone
with substantial resources could try building a full-scale version.
OpenAI could, for instance, specifically train a future GPT model to be good at programming
large, complex software, especially language models, and then directly ask it to self-improve.
The first iteration might not be great, but give it time.
If someone is going to seriously try making something like this, they should take the
risks seriously too.
You should not, for instance, give a self-replicating, self-improving AI direct access to the internet.
At least until you know it's properly aligned with human values.
But that's the subject of another video.
I know this sounds like science fiction, and it is, but science fiction has a way of becoming
reality.
Self-replication and self-improvement are powerful, explosive tools, and we must wield them with
great care.
If we do this right, we could create something that is less of a violent explosion, and more
of a beautiful, blooming intelligence, the most valuable of all inventions.

Today, we're going to talk about AI alignment.
Is it even harder than we think?
And this is the third AI focus on the salon foresight.
In the previous two weeks, we discussed chapters
in the recently published book on superintelligence,
a coordination strategy, which a few people here on this call
have submitted chapters to.
And then based on those previous salons,
Daniel Schmachtenberger, who we had for another salon
in the Hive Mind series, reached out to me
and recommended today's speaker to me,
because he may bring an underappreciated perspective
to the set of problems in AI alignment.
And as a disclaimer, Forest Landry
is from the newer Hector Collective, Welcome Forest,
and he's not an AI safety or AI alignment researcher.
But I think that's exactly why he thinks
that he may have an outsider's eye to contribute to the field,
and he may be able to point to a few areas that
are under explored.
So we'll kick off with a presentation by Forest,
who's outlining a list of reasons
why AI alignment may be harder than we think,
perhaps even impossible.
And then we open up for a discussion with all of you.
And we may start with our foresight fellows,
for example, Creeon Levitt, who's a foresight senior fellow
and physicist who may be enticed to speak a little bit
on a few physical claims that Forest makes,
as well as channeling Anders Sandberg, who's
our senior fellow in philosophy, but who's currently
on the flight and can't make it, but he says hi to you, Forest,
and he would have loved to make it.
And he has an update on a paper that I
think you're wanting to write, which we can come back later.
But then maybe we'll also be joined by Dan Elton, our 2020
fellow in AI, and by Jeff Ladish, our 2020 fellow
in biosecurity, who may also be enticed
to kind of make a few comments to kick us off with a tricky,
but hopefully really fun discussion.
OK, so today we'll definitely be a little bit of an experiment,
and I encourage all of you with relevant background
to speak up, who wants to kind of rat team Forest's claim,
but then also maybe to surface potential other problems
in AI alignment that haven't been
surfaced in the presentation, that we ought to bring
to the table in the discussion.
OK, that's enough for me, so let's
have Forest present his list of good reasons
to be skeptical that AI alignment is possible for 30 minutes,
and then we'll jump right into the discussion,
and I'm going to share a little bit more
on the background on the topic of today and the talk
in the salons generally in the tent.
All right, Forest, take it away.
Good morning.
Thank you so much for such a wonderful introduction.
I'm glad to be here.
Obviously, as she mentioned, my main area of work
happens to do more with communities,
so I spend a lot of time thinking
about the relationships between man, machine, and nature
from user interface point of view as a software architect.
I've done a lot of work for the federal government,
written number of programs, and search kind of technology stuff.
So being an architect of software systems
for good three decades now, I have a perspective,
and I thought that I might share some elements of that.
Seeing as how from what I can maybe
see that some elements might be unusual or new,
I will at least provide some fruitful conversation.
So the main thing that I'm basically wanting to do then
is just give an outline of what I'm thinking about.
I've got some notes, so I'll be reading a little bit.
But basically, the idea is that if we're looking at AI alignment,
what is the frame that we're going to use to essentially
evaluate that question?
So in other words, how do we determine whether or not
this is a question that can be solved?
So as a kind of schema of thinking about this,
it's like AI can be thought of as a thing that
is in relation to mankind.
So in other words, I'm first positing
that there's a separation between the human nature
and the machine nature.
There are some ways of thinking about this
that look at hybrid cyborg type of things
where there's a kind of mixing of machine and nature,
or machine and man, and stuff like that.
I'm not really considering that explicitly.
I'm looking more at the you have an artificial intelligence
that's an entity unto itself, and you
have human beings which are entity unto themselves,
and what are the interactions between the two of those?
So I'm simplifying the AI alignment question
to look a little bit about what would it
take to align machine intelligence
agency with human intelligence and agency.
So in that sense, we can start to think about the AI alignment
problem a little bit like trying to build a perfect shell.
So in other words, we want to make sure it doesn't have any holes.
And when we look at it from a structural point of view,
we realized that it's not really the same as dealing
with things like energy or flow of matter or stuff like that.
So in other words, if I was trying to build a boat,
that the shell of the boat has to be more
perfect for AI alignment than it would be for seawater.
Because if I have a small leak, I'm
going to have a small influx of process.
Whereas if I have a replicating process,
it's more like a virus or something like that,
or introducing a new species into an environment,
that the replication process means that even a very small hole
can result in essentially a complete intrusion.
Because the duplication process or the replication process
that AI could be implementing, the same as much
as any biological life could implement,
could result in kind of spread the same way that a virus would
spread through an organism, as we've
seen very recently with COVID.
So in effect, we can say, all right, we need a shell,
but we need it to be very strong shell in the sense
of not having any holes, even very small holes.
In other words, it must be very perfect,
given that there's a kind of non-linearity in terms
of the effect, that non-linearity is itself
a consequence of the capacity for machines
to be duplicated easily.
So in effect, now we can basically say, OK, well,
given that we know that we need to construct something that
doesn't have any holes, can we use mathematics and logic
and physics and things like that to demonstrate
in multiple overlapped ways that there just cannot
be a way of constructing such a shell.
In other words, that it's impossible to establish
that there are no holes in the shell.
In other words, that the principle
of realizing in any practical sense of a shell of this kind
is essentially an impossibility.
And so why would we want to do a proof like this?
Or why would we want to essentially establish this?
Well, obviously, if we find out that something isn't possible,
even at the level of principle, then in effect,
it allows us to kind of free up a lot of energy.
In other words, we redirect our attentions to other things,
and we try to solve problems in different ways.
So in one sense, this is not necessarily
an ideal thing to do.
It's to try to say, hey, wait a minute,
if we're trying to solve the problem of AI alignment,
and we discover that there's no reasonable or realistic way
to think about actually being able to do so.
In other words, if we can't engender any real hope
of such a solution, then at least we
get the kind of silver lining result of, well,
now we can reallocate our energies to things
which are tractable and are possible.
So please understand that this presentation is more or less
in that sense.
So another kind of way of saying, OK,
so if we're going to be looking for holes,
what is the frame in which we're going to be identifying
that such a thing is impossible?
I mean, what are we going to use as tools to do that?
So one way that we can sort of do this,
we can frame things in terms of kind of three general categories.
So we can talk about space and identity,
i.e. what is the envelope and the shape of it,
and how small a hole, and stuff like that.
We can talk about things in terms of time and force,
and we can talk about things in terms of possibility
and probability.
So obviously, we're concerned with things like choice, i.e.
the AI choices, the choices that we have, changes,
changes in the environment, or changes in the marketplace
dynamics, and causation, i.e.
what it is that makes machinery work and things like that.
And then we can start to bring in, using this as a foundation,
we can start to bring in notions about information, complexity,
game theory, and other tools to consider the question.
And that leads us to start thinking about things in terms
of longer time scales.
So in other words, rather than thinking about AI alignment
in terms of what happens immediately
before or after takeoff, that we are in fact
concerned with what happens over, say, hundreds, thousands,
or potentially even millions of years.
So in effect, when we're looking at things like about identity,
for example, we're not just considering things
like the identity of the holes, we're actually
considering things like the identity of the AI.
And it's really easy for certain biases to creep in.
So for example, as a human being,
we tend to have a brain inside of a skull,
and that the bandwidth of communication
internal to the skull and the bandwidth of communication
across the skull is very, very different.
I mean, we have essentially a relatively low bandwidth
of interaction with the environment
relative to the total bandwidth of all the synaptic connections.
So in effect, in a lot of ways, there's
this very, very strong differential
between the internal bandwidth and the external bandwidth.
For example, the communication process of which
I'm engaged with you right now verbally
has about 42 bits per second of communicative bandwidth.
But in effect, the net difference in bandwidth
makes it very, very likely that brains will come in units,
that we won't necessarily have a kind of process
that allows two people to communicate at such and such
a level that you're treating them as a single entity.
For the most part, you're going to be talking about human beings
as discrete units.
So in one sense, like we could say, all right,
well, there's different groups of people
that are trying to develop AI.
And let's say several of them succeed.
And just for argument's sake, let's
say several of them succeed quickly,
that you end up with multiple AIs deployed in the field,
or perhaps a given group as part of its experimental
program was creating several instances.
So in other words, they're trying different design
techniques in various manifestations,
or they produce an AI, they do some experiments on it,
and they shut that one down, they make some tweaks,
and they start up another one.
So you can end up with multiple instances of the AI,
or multiple instances of variations
of the same kind of AI, or multiple instances of AI
implemented, maybe even completely different substrates.
So the question then can be asked,
to what degree do we have a belief
that these AIs might not merge with one another?
So in other words, taking the bias
that we have that intelligence is going to come in units
might not necessarily apply to AI itself
as a phenomenon or intelligence as a phenomenon.
And there's some good reasons to think about it that way.
For one, the bandwidth of the internet
is obviously very much greater than the bandwidth
of any human being to any other human being
in terms of just straight brain to brain communication.
And then secondly, that since that differential
is much, much lower, you're looking at more
of a market forces kind of model.
So in the same sort of way that if you have
two separate markets, maybe you have a country over here
and you have another country over there,
and they're just finding out about one another,
you have Columbus crossing the ocean and so on,
that there's a very strong motivation
for those two marketplaces to essentially find
some sort of arbitrage that would allow them
to exchange with one another.
So in other words, wealth creation as a force
in the marketplace definitely wants to create ways
for us to engage in larger scales of trade.
And so in that same sense, if we were looking at intelligence
as a phenomenon, sense making as a phenomenon,
there's very good reason for us to believe
that the notion of having separate intelligences
on a machine level may not necessarily hold long term
that we effectively end up with a kind of merger
between these multiple intelligences.
And this strengthens the point of view of that,
you're really looking at essentially
two different agency types.
You're looking at sort of human being agency type
and the way in which we coordinate as groups
and individuals and the kinds of evolutionary process
that have endowed us with certain propensities.
And then basically whether or not those propensities
also apply to artificial intelligence.
And what does that tell us about the nature
of how we have to think about this?
So that's the space and identity question
being considered a little more deeply.
The other process that we can think about of course
is the sort of force and time thing.
For example, if we're looking at what are the ways
in which we would engender alignment.
So again, we can talk about things that are intrinsic
to the artificial intelligence.
So an analog of this, Isaac Asimov wrote a lot of fiction
assuming this phenomenon called the Three Laws of Robotics.
So that would be essentially something that is built in
that creates an AI alignment.
And perhaps the argument is that we could create
something roughly analogous to that.
Maybe it wouldn't be as good
or maybe it would take a different form
or maybe there's a lot of questions about
what does alignment actually mean
and how do we express such forces
in a design intrinsic way.
But then there's obviously the extrinsic stuff, right?
So for instance, say we have some sort of relationship
between the AI intelligence and human beings as a collective.
And what sort of feedback processes might apply
to essentially address what would be the principal agent
problem in the relationship between human intelligence
and artificial intelligence.
So in effect, we can look at the AI question,
the alignment question specifically
as being a special case of the principal agent problem.
And there have been various notes
about how to address principal agent problems in general.
So one thing we could say as part of this sort of force
and time way of looking at things
is to essentially ask what are the forces
that essentially help us to create alignment
or help us to maintain alignment,
i.e. if it's something that's built in in a prior sense
or something that is essentially applied
after the AI exists in the relational dynamics.
And so in effect, we can basically say,
all right, well, given that we have these forces
that are applied, how long do those forces
maintain that alignment?
So again, it's not just a question of,
do we have alignment today or before the thing is built?
Can we create the conditions before it's built
and can we maintain the conditions after it's built?
But it's also the case of,
can we maintain those conditions after it's built
for a really long period of time?
So in other words, hundreds of thousands of years
essentially, given that the scope of the introduction
of something like this,
particularly if it has the capacity to replicate itself,
which it almost certainly will,
then in effect, there's a sort of functional aspect
of saying, okay, well, it's a little bit like
introducing a new species.
And in the same sort of way that say,
introducing mosquitoes into the Hawaiian islands
is a little hard to undo, then in effect,
once artificial intelligence is introduced
into an environment, it could also be argued
that it influences the future course of civilization
as we know it.
So in that sense, we can really just go back
to the whole notion of why it's important for us
to figure out this AI alignment question,
given the severity of the ethical implications.
I mean, we're literally talking the future
of the human species or the,
if things just continued more or less as they were,
as seen over the last epoch of the thousands years or so,
we could literally be talking about trillions
of human beings.
And that's not assuming over the future,
600 or so million years that this planet
will continue to sustain life
in the sense that we understand it.
So if we were to look at it from a larger point of view
of say we become a space-faring species
and we start into colonized Mars
and potentially other planets in the galaxy,
that trillions could become trillions of trillions.
So the ethical emphasis of the question is very clear.
And so as a result, it encourages us
to really think about this question
in terms of hundreds or at least thousands of years
as a minimum for us to really evaluate this question.
So in that sense, it's not just a question of
can we talk about the holes as being small
and that the shell essentially doesn't have such holes,
that whatever the forces are that create alignment,
whatever are the solutions to the principal agent problem,
that they need to be perfect enough
that we don't end up with microscopic fractures
and that can cascade into larger failures
and eventually compromise the alignment problem
at some future date.
So in that particular sense,
we're really looking at a kind of potentiality basis,
like what is the probability of such a hole?
What is the probability of that cascading
and being amplified?
Now, of course, there are obviously,
we bring into this conversation a lot of biases.
We have a lot of people that will gain
on a financial level if we develop AI.
There's a hope for basically solutions
to really hard problems.
There's a hope for a kind of asymmetric advantage
of one group that develops at first
versus other groups that might develop it later.
So there's a lot of things that are really encouraging us
to try to figure out this question
because if we were to figure out this question,
then obviously the economic advantages would be enormous.
But again, at this particular point,
we're back to the principal agent problem.
So sort of skipping past the kind of multipolar dynamics
that exist between the groups
and kind of leaving the economic questions aside,
we can ask, well, what is the possibility
that we could effectively ensure
that there are no holes?
In other words, what is it gonna take to establish that?
Well, one question that we can immediately look at
is essentially what is the nature of the coexistence
that exists between machine intelligence
and human intelligence?
And everything I've said prior to this
is probably stuff that people have already thought about
and is well-known information.
And the stuff that I'm saying from here,
I'm hoping is new information to this group.
I haven't seen presentation of this particular argument
from this point as yet.
So this to me is probably the more interesting part
of this presentation.
So if we were to look at, again,
the relationship between machine intelligence
and human intelligence as essentially representative
as a kind of species,
and that the species themselves,
the same way that human beings influence their environment
to kind of define an ecosystem around ourselves
or at least a niche within an ecosystem,
that the machine intelligence itself
is also gonna occur within the niche of the marketplace
and the niche of the manufacturing
and the industrial complex particularly.
So what are the fundamental dynamics
of these two ecosystems with respect to one another
and what does that tell us about the possibility
of alignment?
In other words, can we bring the tools of physics
and the logic of, say, game theory
to look at the relationships between
the intelligence phenomenon,
obviously we have the agency of human beings
and human groups,
and then obviously the agency of artificial intelligence,
but we also have the substrate relationships.
So in effect, when we're looking at this,
we're looking at, okay, what is silica-based
chemistry process and the implications that that has
relative to the ecosystems necessary to support it
versus carbon-based ecosystem process?
So in other words, if we're really looking at
what are the kinds of forces that will create alignment,
we're really looking at what is the marketplace
that essentially joins these two ecosystems
and what are the dynamics inherent in the relationship
between these two ecosystems?
And using that basis,
we can start to make some observations.
So one of the observations that we can make
is just from the very nature of the chemistry itself.
If we look at the sort of kind of total envelope
of all of the different kinds of chemical interactions
that encompass carbon-based chemistry necessary
to engender biological intelligence of our kind,
but also the ecosystems necessary to support
the substrate, our bodies and food supply and so on.
And when you look at the sort of total range of chemistry
that defines life on this planet,
you notice that it mostly occurs between say,
zero degrees and 500 degrees Fahrenheit.
And this just, again, if you do a sort of enthalpy
calculation on how much each different kinds of bonding
or chemical recombination and stuff like that occurs,
the vast majority of it occurs at standard temperatures
and pressures.
And there's some exceptions
that sort of are more in the outliers,
but the vast majority of the chemistry occurs,
as I said, in this 500 degree range of temperatures.
But when you look at the silicon-based chemistry,
you actually find that the envelope
in which most of the chemical reactions occur,
the enthalpy that's involved in those reactions
is actually over a much different range.
Most of those reactions need to start somewhere
in the neighborhood around 500 degrees Fahrenheit
and go all the way up to 2,500 degrees Fahrenheit, typically.
And the reason that we know this is actually,
again, speaking specifically of this planet as an example,
when you look at the kinds of phenomena that are involved,
I mean, first of all, the amount of silica that's available
in the Earth's crust relative to the amount of carbon
that's available on this planet
is there's a substantially large preponderance of silicon.
I don't remember the statistics off the top of my head,
but it's something like 25%
of all the elemental constitution of the Earth's crust
is silica and that something like 0.05% of it is carbon.
And that's including everything in the surface,
above the surface and all the way up into the atmosphere.
So first of all, silica chemistry
has had far more opportunity to engage
in a much, much wider variety of chemical combinations
just by its sheer preponderance
over the 4.5 billion year history of this planet.
So in other words, you have volcanoes
and you've got all sorts of solar flux,
you've got cosmic ray radiation,
you've got all sorts of stuff, lightning and so on
that would really encourage every possible interaction
between every elemental type sometime in the total space
of the surface of the Earth and the volume of the Earth
and the total duration in which those experiments
can be conducted by just raw chemical mixing.
So in effect, when we look at the net result of that,
essentially the current state,
we see that for the most part,
the chemistry of silica results in things
that we call rocks.
There's a whole lot of elemental types
and mineral types and so on and so forth,
but for the most part, the variety of chemistry
that occurs is relatively inert
at the standard temperatures and pressures
that our biosphere is mostly defined by.
So as a result, when we look at the sort of fundamental
substrate issues and we start to think about things like,
the kind of chemistry processes that are necessary
to develop microchips, for example,
we not just see, obviously these manufacturing channels
and lithography processes and such,
but we also see the chemistry
and the kinds of reaction processes
that are needed to create that kind of variety
of, you know, again, interaction.
Obviously you need a certain amount of manifest complexity
in order to support the substrate of compute to begin with
and also a certain amount of manifest variety of chemistry
in order to provide for sensory capacity
and interaction with the environment.
But-
Horace?
Yes.
We have one comment already from Crian, I think.
Oh, well, I was kind of saving this.
I was saving this stuff up, maybe,
for the discussion period at the end.
I just, well, as long as here we are, let me ask you this.
I understand that you're saying there's more silicon
and in some sense there's been more time for silicon
to engage in more chemical reactions,
but the fact of the matter is, for whatever reason,
silicon life didn't evolve even in the lithosphere,
presumably where the temperatures are high enough
for all these reactions to make these rocks.
And yet carbon life did.
So arguably it seems as if there's something about carbon
that allows a much wider variety of molecular forms
to become into existence.
Now, whether that's just a fact that carbon took off first
and silicon could do it in their silicon life elsewhere
that's natural and organic, if you will,
that's another question, obviously,
when maybe we'll eventually know the answer to that,
maybe not, but it is actually then in that sense
even more astonishing that carbon took off
instead of silicon, because the earth was hot at the beginning
and carbon was an insignificant fraction.
So that's what I'm saying.
And it is a component of this argument
to think about Fermi Paradox kind of issues,
like is the Great Barrier in the past,
is there a Great Barrier in the present,
like where the hell are all the little green men,
why don't we know about them?
Or is the Great Barrier in the future,
the technological civilization
is inherently self-terminating.
I can draw those elements into the argument,
but the point that you're making is actually a real one,
and I mean, obviously it's a real one,
but that it actually contributes to what I'm saying
in the sense that say, for example,
we were to, at this particular point,
bootstrap to sort of bring silica life into existence.
The real question is,
is how well do these ecosystems exist with one another?
They are definitely different ecosystems.
And given that the enthalpy of reaction of one versus the other
is so substantially different
than the question of toxicity becomes a matter of some concern.
So in other words, if I have industrial processes,
and I'm saying industrial,
just to give it a way of relating it to,
but let's say I have a silica-based ecosystem,
the silica-based ecosystem is gonna involve
a very different range of energies and elemental types
than the carbon-based ecosystem.
And then inherently, as a result,
the silica-based ecosystem is gonna be toxic
to the carbon-based life.
So in other words, in order for those two to exist,
you're gonna have to have some kind of wall
because the pressure and the temperatures and so on and so forth
inherently involved in the replication cycle
associated with silica-based life.
It's just at, like I said,
a very different level of characteristics
than that which is involved in carbon-based life.
And in effect, you now have a situation
where the carbon-based life is suffering
because of the energies involved in the silica-based life
are just substantially greater,
not just in replication,
but in terms of what their tolerances are.
So for instance, you know,
spacecraft can essentially be out in space unshielded
and deal with cosmic radiation and solar radiation
and stuff like that far, far, far more easily
than carbon-based life can do the same thing.
So if there was any kind of energetic exchange
between the carbon-based life and the silica-based life,
whether it be a weaponized exchange of energy
or even just a typical one,
it's not just that the information flux
that the silica-based life could handle
and the intelligence and gender is substantially greater.
It's also that just on the sheer energy level,
it's substantially greater
and that that has inherent toxicity relationships
with respect to the carbon-based life.
So now we have essentially two issues.
It's not just that we need AI alignments
as far as the intelligence is concerned,
but we absolutely have to have AI alignment
in order for us to even have some possibility
of coexisting.
If you look at it from that point of view,
all of a sudden you're now realizing
that there are evolutionary dynamics to this
and that there's a certain game theory that comes to bear.
So say, for example,
that we're looking at things on geological time
and we're basically saying, okay,
there's a kind of replication process
or there's a kind of future maintenance process
to some sort of dynamic that the AI is using
to persist itself in time.
And what are the degrees of exchange
that would exist between the two
that would even help to enforce AI alignment at all?
So in other words, if you look at the principal agent
problem solutions that are generally proposed,
they either depend upon something intrinsic
to the inner nature of the AI
or they depend on some sort of market feedback process,
i.e. reputation or admission,
some sort of thing that allows us to create some feedback
incentive or punishment or barrier
that essentially maintains some sort of peace
in terms of the energy exchange across the wall.
Hey, Forrest.
Yes.
May I, I wanna raise two things.
And if I'm talking too much, please let me know.
First thing is, I'm wondering if you have run across
Lovelock's latest book, The Nova Scene,
because he makes some very similar arguments.
Well, from a sort of different angle,
I'll just mention, he basically says that
it's a damn good thing that we might create
artificial superintelligence based on silicon
because as the world gets hotter,
silicon can live there and carbon can't, you know?
And we're making the world hotter,
so this is arguably good because otherwise,
conscious life is just gonna disappear
if we don't make it capable of running
at higher temperatures.
So that's one thing, I thought that's interesting.
I just wanted to make you aware of Lovelock's book.
It's pretty interesting and short and sweet book.
And then the other thing is that
wouldn't this suggest that perhaps,
and maybe I don't wanna have a spoiler here,
wouldn't this suggest that perhaps
if we all agreed to only implement AI
using like really fragile cryogenic qubits,
then we're good to go because it'll actually be less,
it'll be more, it'll require cooler temperatures
and less, you know, tolerate less radiation
and like be more fragile than biological life.
So we win if the shooting starts.
First of all, thank you for the introduction
of the book, I wasn't aware of it.
I will definitely look at that
to respond to the notion of should AI intelligence
be developed eventually, sure,
but I'm thinking that something like 650 million years from now
would be an appropriate time for us
to start thinking about that.
If we do it sooner,
we're pretty much going to extinct ourselves.
And that's not just a hypothesis in the sense of,
hey, this is likely to happen.
Potentially what I'm attempting to do and set up for
is essentially a theoretic and evolutionary mathematics
based way of showing that it cannot not be the case
because there's no basis upon which to establish
coexistence and many, many reasons to show that
from a game theoretic point of view,
that it's not even, there's not even a question.
In other words, if you look for Nash Equilibrium
in this particular space, you can prove that there are none.
Oh, does that, so that's one more question.
Does that have any question?
Does that rely on your temperature arguments or not?
Crayon Vendan.
Uses the temperature arguments as essentially a way
of establishing what to look for.
So in other words, what you eventually are able to do
is to show that any feedback process that's causative
depends upon some sort of mutuality of process.
Once you've established that there's no mutuality of process,
you can establish that there's no marketplace dynamic
that essentially binds the two ecosystems.
And unless you impose some sort of supervalent altruism
that you can't establish that, then you can go
and you can prove that such a supervalent altruism itself
is forbidden by the laws of physics.
So in effect, what you can essentially do
is you can say, all right, we know
that we require these particular conditions
to be successful.
Looking at it from this point of view,
we can establish that this kind of mathematics
is necessary to evaluate it.
And then on purely, like I said, I
could do it in information theory, but I'm sorry,
I can do it on game theory, but it's also
possible to do on information theory.
Effectively, in order to have the alignment persist,
you need to show that the noise floor of the copy
from the past into the future is essentially consistent,
which brings me to the third point of what you were speaking
of, which is that if we try to make the AI too fragile
in that particular sense, that might work for a time.
But the thing is, is that you can't necessarily
sure that it's going to stay fragile.
If there's any kind of, again, replication process
or any kind of dynamic where there's persistence in time,
in other words, that this thing has to do some sort of repair,
there's going to be essentially, eventually,
the emergence of dynamics that strengthen that.
Otherwise, the fragility itself terminates that line.
So from a purely evolutionary point of view,
you begin to see that if we're looking at apocal periods
of time, that certain phenomena drives certain aspects
of the situation drives certain results.
And from those results, we can begin
to do certain calculations that allow us to establish
that AI alignment is not possible.
OK, we have Dan as well with a comment.
Yeah, well, it's good that you're
bringing physics into this, because I was a physics major.
But you see the future as being a sort of economic competition
between silicon life forms and life forms, which
are carbon-based life forms.
I have to make it relatable.
So we think in terms of marketplace dynamics
and evolutionary theory and game theory.
And essentially, all of those are models
that have essentially a similar form and structure.
Right.
Now, I agree with you that the silicon life forms,
I mean, theoretically can be that use
silicon and metal and other elements materials
can be theoretically much stronger and more powerful.
And we all know that and appreciate that.
But in terms of alignment, it's possible that if I agree
that we should be skeptical about this happening.
But it is possible the silicon-based AIs
could recognize that there's a lot of silicon and metal out
in space, and they can live in space easily where we can't
live in space easily.
I understand.
That doesn't necessarily solve the problem.
Might they just go into outer space and operate there?
Sure.
And so in effect, what you end up with
is two separated ecosystems.
Now, if the ecosystems are separated and they stay separated
and there's no interaction between them
and there's no reason for them to force such an interaction,
then you have a coexistence model of two ecosystems
essentially time separated from one another.
But that's not AI alignment.
It's essentially just like complete independence.
Well, if AI is lying so it respects human life,
it might decide to leave humans on the Earth doing our thing
and go into space and just go colonize other planets.
This brings us directly to what would be the second great
barrier of the Fermi Paradox.
Once you have some sort of real separation
between the two ecosystems, then it becomes a question of,
is there any reason for the two ecosystems
to want to interact with one another?
Or is it actually the case that we have some strong reasons
for them not to want to interact with one another?
And again, with this sort of way of approaching it,
we can actually ensure, I'm sorry,
we can show that it's desirable for each side
to ensure that it doesn't interact with the other one.
I just posted a meditation model piece, which is a piece,
but it's making the claim that any type of super
intelligent will enter dynamics, but which is impossible,
that it can leave us even just a guard.
So just by, yeah, do you want to comment on that?
I wanted to reference that argument.
I'm glad you did.
I was literally debating in my head
whether it was worth trying to bring all that up.
But yeah, I've read that piece and it's been, actually,
I think it's one of the better writings of that space.
It really details the multipolar trap situation very, very well.
And it kind of describes the relationships
that it has to a lot of these considerations.
Some of the work that I'm doing is essentially
based upon that paper and it's based
upon the work of Nick Balstrom and others who I think
have been thinking about this very cogently.
And so in effect, I would love to import most of that stuff
as part of the thinking.
Forrest, one question I have, and this
is based on some previous conversations we've had
and also some things you've said today,
refers to basically this idea of what you say is replication.
But I think you would think refers
to any sort of self-modification.
And the question is, can you have stable goal preservation
in the face of self-modification?
Because one of the claims, I think,
that comes out of the AI safety community
is that when you have a rational agent that
is improving itself, it has an incentive
to want to preserve its own goals.
You don't succeed at your objective function
if your objective function changes
from the perspective of your past self.
So in some sense, you have an incentive
to want to try to stably maintain that.
But I think you're arguing that this isn't possible.
And I was wondering if that's true,
if you could lay out that argument.
Well, it's actually a very subtle thing.
And so in principle, I agree with you.
And in philosophy, although this is somewhat obscure
terminology, it's the problem of transcendental stabilization.
So on one hand, you want the change to occur.
Like, you want the goal structure to change a little bit.
Because obviously, if you don't have some changes
in the goal structure, you're not exploring
the evolutionary space of what would be maybe better goals.
So niche discovery and adaptations, such like that,
eventually will require some amount of goal modification.
But as you mentioned, you don't want the goals to change too
fast, because if you do, you destabilize the whole situation.
And that's not desirable.
So essentially, a goal becomes part of the thing
to essentially slow the rate of change down.
But you also don't want the rate of change to be zero either,
because that gives you no adaptability
to obviously changing circumstances.
I don't think that AI intelligence is
going to become so inhibited to control its environment
so perfectly.
So in that particular respect, when
we're looking at what is the process of creating
transcendental stabilization over time,
we can model that in terms of information theory.
We can basically say, OK, that represents
a kind of communication from the past to the future.
And as soon as you look at it as a kind of communication
channel, then effectively we can start
to think about things in terms of the noise floor.
So the noise floor is not zero, and it can't be zero.
I mean, Heisenberg uncertainty principle basically
asserts, at least from a physics level,
certain limits on the relationship between content
and context as far as symbol selection is concerned.
So regardless of how that channel is constructed,
or over what duration it is, the longer the duration,
the more the noise floor is going to show up,
so that intrinsically implies that a certain amount of change
is going to be inherent in the system either way.
Now, that turns out to be not the limiting factor that's
important, but it does mention that certain amount of change
is inevitable, both because of response to the environment
and also intrinsically because of the physics.
And then what is needed to do is to essentially establish
that that noise floor is higher than what would essentially
be the non-linearity associated with the barrier.
So in other words, if we're basically
saying we need a barrier of a certain level of perfection
in order to create a long-term stabilization of the overall
dynamic, then in effect we're looking at a situation
where there's a kind of microstate amplification
from states that are effectively below the noise
threshold, eventually up to states which are macroscopic.
A good example of this, there was a study done,
I don't know, maybe about, within the last year
I came across it, which basically
was looking at the rotations of the three-body problem,
like three black holes, and they're all
rotating with respect to one another.
And they just look at it over the long term.
And it turns out that essentially over a certain period
of time that there's no way to predict
the future evolution of that state, simply because
quantum mechanical changes in the positions
of the three black holes, like you
can't describe the positions of the three black holes
relative to one another accurately enough
for that difference to eventually emerge
into macroscopic changes because of the non-linearity
inherent.
So I'm basically saying that when
we look at the ethics of the situation,
we look at the market forces and the kinds of,
and whether you call it market forces or evolutionary forces
or just information exchanges or coupling of any kind,
that in effect the nature of how we model it mathematically
in terms of, again, game theory or complexity theory
or in terms of information theory or evolutionary theory,
some analog of that process, that effectively what ends up
happening is that you show that this noise floor is
effectively enough to imply that you can't plug all the holes.
So in effect there's a, and it's not just holes
in the sense of holes in the structure of identity
or holes in the structure of space,
what the barrier actually looks like,
but literally holes in the potentiality space,
i.e. new goal structures that are novel,
that no finite way of thinking about it
would essentially allow us to contain,
you know, to essentially establish a containment
of the complex by the complicated.
Yeah, can I just mention something?
Yeah.
I think that whether your argument about the noise floor
holds is irrelevant.
It depends a lot on, you know, how the AI is constructed.
Theoretically, you could have a lot of error correction.
Error correction doesn't avoid that.
So the whole point of error correction is essentially
just to try to make it so that this works better.
Let Dan make the point, please, for us.
I mean, if you wanted to preserve your argument,
though, you could argue that error correction is expensive
and corporations aren't gonna have an incentive
to add in all the error correction.
So, you know, I am following along
in your general pessimism here,
but I'm just pointing out that, you know, technically,
there are workarounds to some of these problems
that you're bringing up.
Well, I agree that there are technical ways
to mitigate some aspects of the problem,
but error correction is not foolproof.
I mean, you can use error correction
to shift the probabilities,
but you can't close the door.
I think error correction can go pretty far, I mean,
but yeah, I mean, it probably won't long enough
time or as in it, there'll still be, you know,
so these sort of kind of mutations,
so to speak, in the system.
Yeah, I mean, it depends upon the nature
of the interaction, right?
So for instance, if we're saying
that there's a finite communication channel
and the noise floor is constant,
then error correction can be used,
like we can essentially put together
for that particular channel,
you know, very good error correction as you're pointing out.
The thing though is that that in itself
doesn't tell us anything about whether or not
other channels can be created.
There's a whole proliferation issue in terms of,
okay, well, I have this communication channel
and it's cryptographically secured,
but then people figured out,
oh, well, we have these side channel attacks,
I can do tempest, I can basically look at power line,
you know, current draw, I can basically look at
the sound coming off of this thing,
I can look at heat dissipation,
I can look at all sorts of other physical interactions
that allow me to essentially infiltrate
or exfiltrate on the information,
but essentially that itself represents
infiltration and exfiltration of intention,
of goal structures.
And so in effect, you know,
when we're looking at the AI alignment problem,
we're basically saying it's not just that
we need to seal the hall, so to speak,
in terms of space and identity,
but we need to seal it in terms of force and time,
and in terms of possibility and probability.
And that's a much different order of thinking about it
than just thinking about it
in terms of one bounded dimensional
linear stream of communication.
As far as communication, I agree, go ahead.
For us, I wanted to know if now would be a good time
or later for me to try and reflect back to you
in very simple, quick terms,
what I think your argument is thus far.
Are you ready for that,
or do you want to keep building it?
It's open to the group.
At this point, we're in free discussion.
Okay, so let me see then if I understand
the basics of your argument.
I mean, I'm not an AI safety person,
but I am somewhat of a physics person,
and it sounds like you kind of are too,
with all this talk about temperature and information.
Okay, it seems like your argument is that
in order for there to be mutual survival,
let's say, with us in AI,
there has to be some common environment
that we both need to preserve.
Like, if we both needed to preserve the biosphere,
if AI needed to have the biosphere for its survival,
and so do we, that's good because
we now have common interest in preserving the biosphere.
And, but it sounds like what you're saying is that,
A, that's unlikely because of the different temperatures,
and B, it doesn't even matter if we do have a common interest
because these game theoretic and arguments
and this like leaky boat microscopic whole argument
means that even if we have common interests,
you know, something we'll leak through to screw it up
and you have some sort of proof for this allegedly,
which I don't get, but maybe that's because it takes too long.
That sounds like a good summary.
Okay, thank you.
There's obviously another whole series of layers,
but there, it's like a defense in depth.
It's like there's multiple ways of describing this.
There's multiple ways of arguing it.
There's a different, there's a few different framings.
At this particular point, I'm kind of looking at
sort of a constellation of different things
to consider and think about different aspects of it.
The net effect ends up being aligned
with what you suggested as a summary.
Okay, so one more question,
which is the part that I kind of don't get.
Where it gets really fuzzy with me.
Okay, I understand that if we have some sort of mutual
environment that we need to preserve,
it's in our mutual joint interest to preserve it,
that that's still arguably not good enough
because what I don't get is this noise floor thing,
is the idea that like you can't keep out noise
and you actually have no idea if what you think is noise
is some sort of sneaky AI leaking in
through the hull of the boat.
No, no, no, it's close to what you said.
So in a sense, it's basically like in the sense of
I'm trying to convey to my future self
what my goals are today,
so that my future self has the same goals.
And if I'm doing the AI, it's the same thing.
So in other words, how do you manage stabilization
of identity?
How do you manage stabilization of goal structure
or of the ecosystem itself?
And to some extent, we can model that
past goal structure as a message
and the future goal structure as essentially
the recept of that message through the communication channel
that we can start thinking about distortions
that would be introduced into the message
as a result of flowing through the communication channel.
Or we could be talking about, similarly,
the relationship, like say there was some economic,
the first argument is that there is no economic
common ground, right?
This common ecosystem thing is actually to be,
that we'd have to presuppose that,
but to presuppose that would be presupposing
against the preponderance of evidence
that we have so far.
But that if we were to even assume
that there was essentially a communication process
across the boundary,
what is the thing that stabilizes the mutuality
of the goal structure?
What is the feedback mechanism that allows us
to essentially engender alignment on the part of the AI
from our point of view or vice versa, right?
And so in effect, when you're saying, okay,
well, what is the feedback mechanism
and what is the, I guess, altruism, right?
That would allow for us to essentially impose
a rule of law on the agreements that are made
between the two different ecosystems
between the two different kinds of life forms.
And it turns out that not only is it the case
that we don't have any real way of enforcing
or even establishing those kinds of barriers
at the legal level.
So in other words, if we were to talk about as a marketplace
and kind of an incentive structure
or a reputation-based system or something like that,
that even the legal structure
is a kind of communication process
and that has a certain amount of needing
to have a high degree of fidelity in the same sort of way
that we're talking about a leaky boat,
as far as trying to prevent the intrusion of viruses,
that we have a leaky boat in the sense of the legal system
that would be attempting to maintain this.
Or we have a leaky boat in the sense of the energy barrier
that would be needed between the two ecosystems.
And that the leakiness is in a sense,
and is an inherent result of both the dynamics
of the environment itself, I.E.,
that the world changes, right?
That different things happen in the sort of larger ecosystem.
Maybe a sun goes nova in some part of the galaxy
and there's a cosmic ray burst,
and it changes the nature of how artificial intelligence
has to build its compute.
Obviously it affects biological life as well,
but again, we can talk about that as a different thing.
But no situation occurs where you're gonna have
a completely static, unchanging environment.
That's just not a reasonable hypothesis.
But like a leakyness is only a problem
if the agents are already not aligned who would exploit it.
And also it is just like another way of saying,
well, there's offense, defense, dynamics,
which is always the case, right?
And it is always the question of like,
okay, can we stuff the holes first or not?
So I don't know why.
The legal system has enforcing non-aligned entities
into alignment so much as I was thinking of it
as a way of kind of, how do we maintain essentially
a basis of agreement when the agreement fails?
So in other words, what's the meta agreement
that allows us to even have an exchange in the first place?
What stabilizes that basically?
Let me ask two key questions about that.
The first is you brought up Fermi a few times
and that's very important because either you believe
that we are alone in the universe
or you believe there is some reason
to answer Fermi's paradox when we're not alone.
And if you believe that then clearly we have been
coexisting with AIs for a week,
carbon life have been existing with AIs
for about three billion years successfully.
So there's obviously a way to make it work
unless you believe we're entirely alone.
And so something not considering all of our current models
has to explain that.
However, if we are alone, there's a big universe
in which it may be possible as well
to not have to get this kind of dramatic competition.
If there's any reason not to compete,
a big universe offers the opportunity to escape for it.
Now, we're looking for ways of doing AI slavery
so that we won't have this battle.
And it is obviously AI lineman is, as I said in the chat,
another word for slavery, with one exception,
which is again something we have a large example of
which was for approximately a million years,
we've managed to create new beings smarter than ourselves
about every 29 years on average.
And those beings do not eat us.
Not for a million years have they eaten us
even though they no longer need us,
the grandparents, I mean,
even though they no longer need us
and do compete for resources with us.
In fact, we give them resources, typically.
So that's the one example we have
and that one example has actually worked out fine.
By the way, without slavery,
it's using a phenomenon that evolution created called love.
So the existence proofs we have both contradict
what you say.
Well, actually, I'm agreeing with you
because I don't see a contradiction.
And maybe you do, but I basically,
well, so first of all, just want to...
Well, first of all, the information
has obviously been conveyed forward
since the first sentient being to us,
don't eat your grandparents.
Somehow that piece of information,
which is the one piece of information
we're trying to communicate with AI lineman,
don't eat your grandparents,
that piece of information has been communicated
across million years.
Well, it has, but it's been in the same ecosystem.
So we're talking common environment and common market.
So in effect, establishing agreements
and biological processes,
such like that is quite easy
because you're talking the same language.
It's carbon-based to carbon-based.
Yeah, my grandparents didn't speak the same way,
but anyway, that's not a question.
Oh, look, look, yes, but at a physiological level, right?
So the physiology that you have,
yes, you're competing with your children
for resources in the ecosystem,
but it's a common ecosystem.
Mostly what I'm talking about
is essentially uncommon ecosystems,
i.e. that the ecosystems of cells
fundamentally different.
Forrest, I thought that you agreed
when I summarized the position
that the ecosystem,
like say the temperature
or whatever you want to call it,
the biosphere, that's a red herring
because I thought your claim was
even if we had common interests with AI,
it's not, that's not enough.
There's still going to be these leases.
This is a defense in depth.
But hang on a minute, hang on a minute.
And what I kind of following up on with Brad says,
it seems to me that then that would imply,
if I'm right about how I understand your position,
that would imply that humans can't coordinate
because we have common interests
with all sorts of other human groups,
not 100%, but we have them and arguably,
like forget AI, it's just impossible
for people to even to get it together
according to your arguments.
We have common interests with weed as well, I agree.
What I'm basically trying to do
is to essentially establish a series of contexts
and each context to establish an argument.
So the first context was,
what is it about internal versus external, right?
Then there is the context of space and identity,
force and time and probability and probability.
Then we switched to the context
of talking about environments.
And so in effect, there's a phenomena here of,
I would first of all, say if we're just at the point
of talking about environments,
that we are in fact talking about
different fundamental arguments.
I mean, different fundamental environments
and that there's arguments that apply at that level.
But say we were to do the if thing of saying,
okay, well let's ignore the arguments prior to this point
and assume that we did have some sort of common environment.
What does that imply about things downstream from that?
So in a sense, it said,
I'm basically fielding a series of different arguments,
each of which applies within a particular context.
Can I just say something?
For me, it seems worse if we have to share
a common environment because then we're competing
for the same and to live in the same environment.
If we can live in, if the AI can live
in very different environments,
which it seems it will be able to,
then it can just go out into outer space.
That was my point earlier.
What sense does the word alignment mean under those questions?
So if you develop essentially a planet that the AI is on,
you have another planet that's over here
that's got life, our kind of life on it,
what does alignment mean under those conditions?
So in other words, if we're looking at that,
basically we're just saying, okay,
well, if you have completely independent ecosystems,
we each get our own planet,
then the only alignment that we care about
is that there's just not a war going on
between the two ecosystems.
No, I think we'd like commerce.
I think we'd like to go out into space,
even though it's their territory,
and they may have desires to do occasional commerce with us.
Right, so that particular thing,
then we basically can start to talk about
what would be the basis of such commerce?
Well, I think especially the fact that we have differences
may also be the fact,
the reason why there is something to incorporate on, right?
If we have different specialists
that are all specializing in different things,
then they may cooperate in mutually beneficial ways,
if it's like, I think based on volunteer interactions
that may bring about greater knowledge creation,
or greater superintelligence, right?
And so I think the question kind of boils down
to what the initial,
I think the question boils down to like,
what are the initial interaction architectures
with which those entities can even cooperate?
Because I think if you're right,
and if you're right,
and we have nothing to bring to the table
that they could possibly ever value,
then scoot anyways in that regard, right?
But if there is something that creates potentially
like a mutually beneficial interaction,
and it is something that could be better pursued
in a voluntary way where it's cooperative,
then we can start talking about any game theory dynamics.
So I think, you know, it's kind of affecting the question.
There's a number,
there's a lot of different scenarios
on how this could play out.
You know, just like we keep,
we worry about some species going extinct,
the AI might just value us as a species
and worry about us going extinct.
And it doesn't, it won't cost the AI
much to keep humans around on Earth
because there's plenty of other resources around,
you know, in outer space for the AI to utilize.
So the added value of Earth is tiny,
you know, in comparison to the billions of other worlds
and minerals and resources that are out there.
Well, this is part of the reason
why Allison's mention of the MULARC, MOLARC,
I'm not saying it white.
Allison, correct my pronunciation, please, if you would.
German MOLARC, but that's probably also not right.
Scott Aronson, I think is his name,
they have put together a very good sort of summary
of why we should be skeptical about the coexistence thing
in that particular sense that you're describing.
You know, please bear in mind that I'm a single person
trying to basically answer questions from all of you
and you all have very good perspectives,
but they're all coming from very different directions.
And so in other words, to really address
how do we get to the assumptions
that each of these arguments bring in
and what places do those assumptions apply?
And how do those assumptions influence
the kinds of questions we ask?
You know, if all I do in this particular thing
is give you whole new categories of questions to ask,
I'm gonna call this successful,
but basically, you know, if you're asking me questions
as to why I'm answering certain ways,
then I'm gonna have to basically try to identify
what those assumptions are and it just takes time.
Okay, can I, just to make things worse,
bring in two questions that we have from the audience
by Kanita and then by Dekay, who had their hands up.
Oh, goodness.
Sorry, it wasn't allowing me to unmute myself and-
You are now unmuted.
And now the question that I asked
has gotten lost back in the chat.
I haven't even looked at the chat.
I will probably talk to the chat
and look at all this stuff out.
Let's go with Dekay and Kanita, you can-
I thought that my question was in the chat
and I thought you could maybe ask it for me.
Okay, and I will search for a question
and in the meantime, I'm gonna unmute Dekay.
Hi.
Yeah, thanks very much, Forrest.
So, recognizing that I'm coming
from a very different angle here,
you know, I think a lot about
the unconscious cognitive biases that we humans think in.
And I think even a lot of the questions
that are being thrown at you
reflect a bunch of unconscious biases
that are culturally dependent
that a lot of us are not necessarily even aware
that we have.
You know, and obviously a super intelligence
would be a lot more mindful of their own unconscious biases,
you know, even fairly intelligent humans
become more aware of that.
My, you know, and I think, you know,
Dan was saying a little bit earlier,
it would be an example of how such an aware super intelligence,
one that is cognizant of the weaknesses of those biases
would be compensating, you know, for example,
by just maybe altruistically wanting to preserve
the diversity of species for whatever reason, right?
Not feeling the need to compete with them.
And so the frameworks that you're using
are very heavily based on, you know,
that sort of competition.
How would you model this kind of effect, you know,
in your framework?
That's actually a very difficult question to answer.
Not because I don't know how to do it,
it's just because it takes time.
So I think in terms of theory,
like when we look at the Fermi paradox thing,
and the question was asked earlier,
what is my belief about why don't we have contact
across, you know, galactic space with other civilizations?
Do I believe that they exist or not?
Or, you know, what is the barrier
that is the prominent one?
I find myself in the position of basically modeling
the relationship between ecosystems.
So, you know, interplanetary relationships,
in kind of a way that was suggested to me
by reading some of the stuff
that Nick Bostrom put together.
So in other words, if I basically,
just as a thought experiment,
posit the notion of two advanced civilizations,
having a question about whether or not
they're gonna enter into a first contact situation
with their peer.
And, you know, each civilization knows in itself
that it has developed enormous technological capabilities
of various different kinds.
So maybe it's developed some really good stuff
in the nuclear weapons program,
and maybe it's developed some really good stuff
in the biotech program,
and maybe it's got these really fabulous computers,
and that any one of these technologies
is effectively something that provides overwhelming capacity.
And, you know, the range of what can be done in physics
is enormous, and perhaps when we're looking
at this peer planet, we're basically saying,
hmm, do we wanna talk to these people?
You know, we might posit that they also have developed
extreme capacities in various technologies,
and that they might not be the same ones.
So in effect, if I was, you know,
I didn't know about nuclear capacities, and they did.
You know, there's a very good sense in which,
you know, interaction would be of such a kind
that they basically say,
hmm, these guys don't have nuclear weapons capacity.
If we do a first strike scenario,
we're gonna completely annihilate their entire world,
and they won't have any way to respond in time
to basically protect against that.
I.e., how the heck do you protect against a device
that's already blowing up?
And, you know, in the same sort of way,
we could basically observe that, you know,
when we're looking at them
and thinking about the modeling that they're doing,
we can say, well, let's see,
we've got some stuff that they don't have.
So maybe there's, you know,
out of the thousand different overwhelming
technological capacities that exist,
that species one has developed capacities A, B, C, and X,
and species two has developed capacities Q, P, W, and R.
And so, in effect, they both have kind of this
unmitigable capacity for mutual destruction,
depending upon whoever does first strike.
Now, again, you would be a little nervous
about contacting another species
and setting up a first thing,
because you'd wanna know that they had some prior reason
not to engage in first strike capacity against you
using something that you didn't have a capacity
to defend against, because you just don't have that tech.
So, in effect, what happens is,
that you now have to ask a question of,
do I trust that the embodiment of ethics
that that group has, that entire planet has,
is implemented to such a perfect degree
that they would value my existence,
even though they can't relate to it in any way yet,
because, you know, we haven't met yet,
we don't really know each other that well,
that in effect, I'm going to have this perspective
that not only is the whole planet,
in a sense, going to behave ethically towards me,
but it's gonna do so in detail.
Like, for instance, I don't wanna be worried
that some sub-faction of the planet that I'm contacting
is gonna say, hey, this first contact thing
is a really bad idea, and, you know,
we're gonna basically force the situation
and it'll work anyways.
You know, we're gonna do the first strike
because, you know, we as a subgroup
aren't in coherence with the larger planetary agenda
of moving forward with first strike capacities.
So, you know, of not moving forward
with first strike capacity.
So, some subgroup basically pushes the red button
because they have their own private version of it.
And so, in effect, when we look at this,
we're basically saying, actually, when you look at it,
and just given that this is, you know,
not only a possible scenario, but actually a likely one,
that in effect, it becomes very much the case that,
you know, without really knowing
that the other party was essentially fully ethical,
that you wouldn't wanna talk to them
and that they had embodied that ethic,
not only globally, but down to the level of individuals
and maybe even to the part of being able
to contain the crazies.
Now, they, of course, would have the same sort
of thinking about it and would really challenge
to see whether or not we were ethical
in exactly that same way.
And, you know, when I ask the question of, oh, shit,
let's say another species, other alien beings,
somewhere out in the universe are monitoring
our radio communications to try to determine
whether or not we've successfully implemented
this phenomenon called the non-relativistic ethics,
which, by the way, is a whole other body of work.
Would probably take me at least another few sessions
to describe why I can, to posit a notion
of a non-relativistic ethics as actually being a real thing.
But presuming that there is a body of ethics
that allows us to basically stipulate
that if the full civilizations were essentially
coherent with that body of ethics
and we could actually determine that they were,
that a first contact situation is therefore sane.
But then in the absence of that,
that it would affect, it would be insane
and very much not desirable to initiate communications
or to even allow one species, I'm sorry,
one ecosystem with all of its species
to identify that another ecosystem
with all of its species even existed.
Because any amount of awareness of mutual existence
and effect constitutes a kind of first contact signal,
even though it's an unconscious one at that.
So in effect, what we end up with is essentially,
you know, in response to the Fermi paradox question
that was implicitly asked earlier,
you know, why would I believe that, for example,
that the second great barrier,
the one that is in the present,
would essentially prevent us from knowing
that there are other species out there,
well, because they'd probably be working really fucking hard
to prevent us from finding out.
And that to some extent, the only reason
that we haven't started doing that
is because we haven't become coherent.
And the fact of our absence of coherence
in that specific way is essentially evidence
for why they should hide.
So we should call you the dark forest.
Thank you.
I have never heard that before.
You've read that book, I presume, which is...
I have, but I just...
You just spoiled it.
You just put it together.
That's really amazing.
And it's out in for hazard unleashed.
Okay, we have Kanita here with a question
and then we have another question from TJ.
So it's Kanita, I'll unmute you, and then TJ,
and then maybe we can wrap it up with the official part.
Kanita, you're unmuted.
Now, I was asking, won't there be any way for humans
or our descendants to evolve in directions
that allow us to cooperate?
Because we won't be...
We really hope so.
Man, if we don't figure that out, we're doomed.
We seem to be doing all this as though
it will be the humans, where humans are like humans are now,
versus the AI, where the AI is this thing
that we create.
That will be super, and we will not have any chance
to come into alignment with it.
We...
It seems like even now, or shortly from now,
we will be able to improve ourselves.
And so, basically, as we improve more,
we will be able to find more areas in which we can align
with whatever the AIs become,
because you're saying, well, over millions of years,
they're not going to be sitting still
for those millions of years.
They're going to be evolving in their own ways,
and possibly might have figured out
that since we don't...
They presumably don't know how to recreate what humans are.
They might as well keep us around
so that they can study until they find something else to do.
Yeah.
I actually...
I'm really glad you asked this question,
because this is, to me, kind of the central idea.
The whole thing here is that regardless of whether or not
we develop artificial intelligence,
and I, as a result of all of these other things,
would strongly argue that we should not.
We shouldn't even begin to attempt that.
But say, for example, that we were to just even look
at the question of the third-grade barrier,
what are we going to need to do to become a species
that's worth contacting?
I definitely believe very strongly that regardless
of everything else, we still need to work on our own capacity
to become a fully alive and ethical and embodied species.
I speak about it in terms of conscious sustainable evolution.
What is it going to take?
What are the necessary and sufficient and complete conditions
for us to not only endure on this planet,
but also to thrive, to basically create an ecosystem that thrives,
to actually know how to do governance in a way
that protects the land and the people,
but then encourages and actually engenders those kinds of policies
that go beyond mere protection,
that go towards essentially a kind of joyful, meaningful existence
for the whole and the totality, individually and generally.
And we won't just create an AI.
It seems that we'll probably develop many AIs,
and then those AIs will be developing other AIs,
so it won't just be us in competition with an Uber AI out there.
There will be lots of AIs,
some of which have more or less interest in cooperating with us.
I believe very much that any effort on our part
to try to divert ourselves from basically becoming
the beings we need to be.
If I basically expect my children to do the task
that I didn't do in my lifetime,
and I put that as a kind of obligation or onus on them,
and use that as a way of accepting
why I shouldn't be basically working on myself
to become the best human that can be,
I feel there's something wrong in that,
that in a sense we're not looking for AI to be our saviors
or to be ethical on our behalf.
We're looking to become how do we understand
and embody ethics and right living
and goodness of relationships and so on and so forth.
We haven't figured that problem out at the level of humanity.
Why are we still trying to do it in technology?
I think that there's a right place.
Okay, that reminds me very much of the DIY
Neurotech discussion that we had,
which was very reminiscent of this one.
I want to give also the opportunity to TJ
and to ask you a question.
It's directly relevant to this.
Sorry, I actually wanted to answer,
but I didn't want to stop.
Caminda, is it?
From...
But I appreciated Forest's long answer,
but it was still couched in terms of competition,
and I just wanted to push a little bit more at that point.
A lot of this is based on anthropomorphizing
human psychology onto AI,
the stuff that we're discussing.
But if we look at actually human psychology,
by the time people become fairly accomplished intelligently,
the top of Maslow's hierarchy of needs,
they're working on self-actualization,
which is pretty much what Forest was just mentioning,
becoming a better version of yourself.
Why are we...
I didn't really understand from the answer that you gave
to my question how you would fit that into your framework.
Maybe I'm not understanding your question correctly.
I'm thinking about cooperation and competition as phenomena.
I'm saying that in an evolutionary sense,
if it's a stable evolution, if it's an ecosystem,
the cooperative phenomena is actually stronger
than the competitive phenomena.
When you're looking at trans-ecosystem relationships,
there's no outside envelope to stabilize it.
So in effect, we have to now create one
with some sort of ethical frame,
which is why I brought up the so-called
non-relativistic ethics,
that in effect, without some sort of ecosystem
that essentially holds the two ecosystems
and some sort of methodology that would create alignment,
i.e., provide a desirability of cooperation over competition,
that to some extent we either need to recourse
to some abstract stabilization infrastructure
that is not imposable, but is only observable.
In that particular sense, if we're talking about
AI alignment on the surface of the Earth versus elsewhere,
or we're talking about some sort of integration
between human beings and technology
that doesn't necessarily require distinct separate intelligences,
that again, we're looking at different frames.
But as long as we set up an ecosystem,
we don't really need to set up a metaethics now.
We just need to set up an ecosystem,
which makes it so that the long-term dynamics
are cooperative.
So let's say like a tit-for-tat in a way where,
let's say the kind of strategy that evolves out of it
is like mutually assured, like cooperation,
if you want to call it that,
that that is the evolutionary stable strategy
between the ecosystems that are in contact with each other,
or between the individual instances
of the ecosystems that are in contact with each other.
It doesn't necessarily require setting up the whole ecosystem.
We need to stabilize our existing ecosystem.
Right now, our current relationship,
the relationship between man-machine and nature
that currently exists isn't even stable
as it stands within the ecosystem.
Yeah, I would agree with that.
It's like all this arguing about AGI risk
seems to me like as COVID has shown,
and as climate is showing,
and as we will continue to be probably made aware,
we're kind of like just rearranging the deck chairs
on the Titanic.
And so while I appreciate your arguments,
and in a way maybe this is what you were saying at the beginning,
like it's impossible to save us from this problem.
Let's go work on another one
that we actually can do something about.
That's it, exactly.
And so part of the reason that I was really emphasizing
in the beginning is that if we can show that AGI alignment
in like 99% of the ways people are thinking about it,
is fundamentally and structurally impossible,
let's give up on that impossible goal
and actually do something that's not only possible but necessary.
So would it be a fair thing to say
that this non-relativistic ethics that you alluded to
could be thought of actually as just an abstract reification
of more of the ecosystem stability
that Allison was talking about?
Yes.
Okay.
All right.
That is actually the easiest question I've been asked today.
I mean, okay, so I'm sorry,
but we will have a very similar problem once we go out into space
and we have humans that we'll be competing for
for different types of resources, right?
Yes, we will.
And on my account, we are currently setting us up
for another kind of new evolutionary environment
just by the fact that people with different interests
will be going into space where there is resources
to compete and cooperate on.
And whatever game theory will emerge from that one
will also be, again, the new metaethics that we want to call it
and we will just have different language to be calling it such.
So I think setting up the initial conditions
such that cooperation becomes likely maybe good,
but then again, our current ethics are just a product
of the evolutionary kind of stable strategies
that arose in the environment that we were brought in.
So who's to say which of the ethics we should instill
for the long-term evolution of humanity?
And I think, actually, TJ had a point that is very, very relevant to this.
I just want to give her the opportunity to bring that in now, TJ.
You are unmuted.
Question?
Yes, but I think TJ has a build-up of that, actually,
which relates to value-driven.
So TJ, you are unmuted if you want to chime in.
Yeah, hey, right.
So I think, like, first of all, like, thanks for the presentation.
I think my question was more along the lines of,
it seems to me your claim is something like alignment as a static objective
is not, like, the right thing to think about.
And because there seems to be, like, inevitable drift
because of these inter-temporal noise things.
But it feels to me that, like, the noise argument itself
doesn't sound adequate to argue that, like,
the evolutionary trajectory would inevitably go into extinction.
Like, we do engage in a lot of co-evolutionary mutualistic couplings.
I never could just present that part.
So actually, like, connecting the question that you just asked
with the question that Allison presented,
there's another way of looking at this, which is just, like,
let's leave AI out of the picture and just talk about human beings.
It's possible to show that if you were to basically take another,
like, a group of people and put them on Mars and to establish a separate colony there
and to have that become essentially a full planet, full of humans,
and then this is this planet full of humans,
that the same sort of dynamics, like, when we're looking at,
it's not just a question of alignment.
There's other ways of expressing the notion of alignment.
Would it be the case that even a space-faring group of humans
not already coherent with the non-relativistic ethics,
does that even become anything less than complete and total cessation?
In other words, I would argue that as soon as you get a substantial number of people in space,
that it becomes so easy to, for example, weaponize asteroids
that at some point or another, over the long term,
that in the same sort of way as with nuclear proliferation,
we have an issue of, shit, we have all of these things,
and we only need one accident to start World War III,
and is that existential for civilization?
Well, it could be argued that it is.
And so in effect, it's a question of if we start, even as our own species,
multiplying the phenomena of our deployment through space
to the point that communication itself is limited by, in this particular case, speed of light considerations,
that again, you end up with essentially this separation of identity,
separation of time, such that the kinds of communication processes
that would be needed to essentially embody that ethics effectively
aren't sufficient to essentially establish it.
So in other words, either you have the ethics first,
and you live from that perspective, at which point everything's great,
or you don't have that ethics, but the nature of the interactions itself,
that the dynamics of those interactions don't have enough time for the ethics to emerge.
It's essentially the species instincts itself before it gets a chance
to really understand what those ethics are.
Well, that I think depends on the background conditions that you set up, right?
And this reminds me so much of Chris Carlson's presentation in the last session,
because I think on my clock, as long as you are able to set up a system
in which only voluntary interactions are allowed between different entities,
whether they're humans or AIs, as long as you can't destroy the,
let's say, the base layer on which they take place,
then over time, entities, whatever they are, will engage in those interactions
that are in their interest.
And over that, if it's voluntary, and you kind of like game theoretical
metaethics could arise at the end of it,
but I think as long as you can set the base layer such that you don't have destruction
and that different entities that pursue different goals can decide whether to engage or not,
then I think what comes out of that is inherently something that is not self-destructive.
But I think those are the things that you have to divide.
You don't have to get the ethics right from the start.
You don't have to get the base layer, right?
We're just enabling only voluntary interactions and non-destruction of the base layer.
The nature of the proof is to show categorically that no such conditions can be created.
So in other words, if you're saying I'm trying to create an environment,
a baseline environment that would allow for these mutual dynamics to essentially be peaceable,
what I'm basically contesting is the assumption that such an environment can be defined in any a priori way.
That may be right, but like, you know, shouldn't it be better of just trying, you know,
and I think that ties into a set as well.
If you have one chance to do something and that one chance essentially means that if you mess it up,
it messes it up for all future time.
That's a very different, that's not an experiment. That's a choice.
You believe we know enough to make such a proof today?
I believe that we know enough to essentially establish categorical proofs of certain kind.
We can do proofs of existence and non-existence of certain types.
Such proofs have often turned out to be flawed with new information.
This is true, but on the other hand, that's part of the reason why we're looking at a categorical process.
I'm looking at it more from a mathematical perspective rather than just a physical one.
Even those.
Well, yeah, I suppose you're right.
Mathematics occasionally is shown that the proofs that are established in that space are wrong.
And this is part of the reason why we have conversations like this, but it takes a while.
The proofs are proofs of negatives.
All right, so we are now at 12.29.
And I think in the initial discussion that I have with Forestu to initiate this call,
he was definitely incinerating that this takes much more than one discussion.
And I think we really opened up folks at Pandora now are discussing about
going from impossibility to like, how could we actually solve AI alive?
And so I think we should weigh past the goal of the session.
I want to thank everyone for attending.
I want to thank you for so much for laying out your argument.
I want to thank everyone else for being such active stewards of the conversation.
I thought it was like, I mean, we definitely had really, really, I think, tricky and hairy discussion topics today.
And I thought that nevertheless, we were able to keep it in a way where a discussion was actually possible.
So maybe that is a better way of saying, even if our interests are very different and where we're coming from to discussion are very different.
We can still cooperate on things, right?
Okay, this is my, you know, wavy, meta way of saying, yay, maybe we're not all that doomed.
But for now, thank you so, so much for us and joining.
Thank you so much everyone for your really active participation.
I really, really enjoyed this.
I'm hoping that we can continue the discussion for us.
I'm hoping that you share with me a few topics that you want me to send out to others afterwards to follow up,
because a few people have asked for your writing.
At the same time, I think I will share the chat with you, which may provide you really good feedback.
I think it's impossible to speak while monitoring the chat, so I'll do that too.
Yes, no, definitely not.
It's not required.
For that, we need corporations with better AIs so that they can benefit us in that day.
Here I'm really at the end of it.
I just wanted to mention that next week we will be meeting on decentralized decision architectures as a response to COVID-19.
And then the weekend afterward, we will have another AIs session, this time with Dan Elton, who gave his remarks earlier here today,
who just hopped off and he will be discussing one of his proposals to AI alignment.
And then we have two others that will be joining us.
Aria, I'm not sure if he was joining today, but he will be presenting too.
So we have the next two salons already planned.
Again, always on Thursdays at 11 a.m.
So this is enough for me now.
I'm going to close it out on my end in Forest.
Dear Dark Forest, you have to find the words and I'm already opening up the invitations.
First of all, I'm just super thrilled to have been able to speak with all of you today.
I hope that I've raised interesting questions and new ways of thinking about things.
I'm sure that with every single one of you, I could have a long and very fruitful and interesting conversation,
and that we could effectively start to really get at some of the meat of the matter in this space.
But as an introduction, I felt very well received and just glad to have the opportunity to meet so many new folks.
So thank you very much.

Hi, everyone. Welcome to Foresight's existential podcast. We're really delighted to have Stuart
Buck here today. We actually met in person at a recent EAG before. I've had heard of you research
before and just booked a call with you or an in-person meeting there. We stumbled over a bunch
of really interesting metascience projects and problems, challenges, and possible solutions,
and then dove into a little bit of perhaps the new emerging landscapes of interesting orgs.
I'll talk to you a little bit about that in the introduction. I'd love to hear from you on
that. I guess we were just ships passing the night itself by Southwest too bad,
but I decided to hopefully next time in person connect you. All right, so maybe just to get
us started, would you want to share a little bit more about good sciences really up to and if you
can also get your journey into the organization? I think that usually helps people like map the
genealogy of your work a bit. Sure. I should start by reminding about 12 years or so.
Around 2012, I went to go work for a place called the Laura and John Arnold Foundation.
It's this major billion-dollar plus philanthropy that focuses a lot on evidence-based policy,
and it's grown a lot since I joined. When I joined, it was pretty new. There were like
eight or 10 people. There are now it's over 100. The scope and the scale of it is really
growing, but right from the start, the Arnold's, who themselves are around 50 years old,
they had retired at 38 and were devoting their wealth to philanthropy. They were mostly interested
in just evidence-based policy across lots of areas like education and criminal justice and health.
One thing that we initially started noticing was the what you call the reproducibility crisis,
the problem of trying to replicate research. We first noticed it in psychology, but I started
digging into it as director of research there. It's a problem in a lot of fields,
including medicine and cancer biology and economics. Pretty much any field you dig into
that turns out there are some issues with replication, sometimes outright fraud.
Just the publication process is often biased towards exciting positive results, which is
natural. We all want to have exciting positive results come out of the scientific world, but
when there's a bias towards that, then people feel compelled to sometimes stretch the truth,
push the boundaries of acceptable practices. The Arnold's decided that if you want to pursue
evidence-based policy, it's really difficult to do that if you're not sure how much you can trust
the evidence, or if you think the evidence has been biased in a positive direction.
Their initial vision of philanthropy had been that they would look to education, for example,
and it would be fairly simple just to find what are the best ideas supported by the best evidence,
and then just write a big check to the best idea, and then it's very simple.
But it turns out it's much more complicated than that if you really start digging into the evidence
as to what works. I started a grant-making program there with the Arnold's money, of
course, focused on open science and reproducibility and trying to improve science. I handed out
probably $60-plus million over several years, and then in the process of that endeavor,
I ran across a guy named Patrick Collison who runs a company called Stripe. I ran into him
several years ago at a conference, and he was very interested in trying to improve science,
but not just reproducibility, but improving innovation, improving the pace of innovation,
the freedom that the best scientists have to explore the universe and explore their best
ideas without having to cater to what funders most desire. So anyway, I introduced him to John
Earl, and we continued conversations, and then a couple of years ago, actually, time has flown.
Actually, it's going on two and a half, almost three years ago. I had some further conversations
with Patrick Collison that led to him being the initial funder for what I'm doing on my own now,
which is a good science project. It's a small, I guess you could say, think tank focused on
trying to improve federal science funding and policy so that we have faster innovation and,
hopefully, more breakthroughs and clean up reproducibility as well. So that's the journey
as to how I got to where I am now. Robert, I think it's always really interesting to hear,
and the individuals involved and so forth, and somewhat of the serendipity in it. Okay,
that's wonderful. Maybe let's dive into a few of the topics that you actually focus on to give
people a little bit of a taste of what you guys are working on. I know that you've published a
lot on Substack, but in other outlets, too, and you sent me a few really interesting docs,
so I just want to jump around here a bit, if you don't mind. And one that I thought was really
interesting and is, of course, has become, I think, a pretty prominent field recently,
is the field of meta science progress, and it's not really necessarily involved with individual
scientific fields. For example, Forsyte was a part specific researchers working on one technology,
but it's really also looking at a broader, with a broader lens of what could be improved
in the ecosystem. And you've written some really interesting stuff there. And so I'd just love
to know, including, for example, how much progress we've made in meta science, but where we could
still speed up progress. And I'd love to get your thoughts on the broader, if you think about
science from this meta lens, like you mentioned reproducibility as one, but what are a few of
the different areas that you think are really holding scientists back right now at producing
the research that we would all want from them? And then perhaps a few recommendations that you
have here. Sure, that's really broad. So I'll just pick one issue that I care about a lot. And
a lot of folks have focused on, and it's really tough to crack down. It's the issue of bureaucracy.
Everyone hates bureaucracy in the abstract. But the problem is that everyone loves bureaucracy
when you point to any particular feature of it. So to take us back, there have been multiple
surveys of federally funded scientists over the past couple of decades. And the most recent survey
surveyed thousands of federally funded scientists. And they said, on average, that they were spending
44% of their time on bureaucracy, basically filing reports and budgets and proposals and
just all the machinery that comes with getting a federal grant. And so everyone points to that
and says, that's a huge problem. We're scientists spending nearly half their time on bureaucracy.
That seems like we're paying people to dig a hole and fill it back in. And as you say,
that's an average. There are some scientists who are fortunate enough to have great administrative
help. And so they don't have personally have to spend as much on the other extreme. I talked to
one scientist at the University of North Carolina, who said that he's probably spent 70% of his time
on bureaucracy, because he said he does animal experiments. And he said that quite frankly,
his administrative help in the department wasn't very good. And so he had to do all the ethics
paperwork himself. And so he felt like his direct quote from him was, I just don't feel like there's
time to do science anymore. And so that seems quite paradoxical. What are we paying people to do
just to fill out reports about the money that we handed them? It just makes no sense. And it's
depressing. No one goes into science thinking they're going to spend 70% of their time filling out
reports and filling out paperwork and et cetera, right? They go into science because they love
a particular field and they want to learn more and they want to make discoveries and so forth.
And it just drains all the excitement out of science. But here's the problem. Every bureaucratic
requirement has some justification for it. There are ethics requirements as to animal
experiments. And those are there for good reasons, because animals can be abused and
can suffer horrifically in experimentation. We've developed a whole set of procedures to protect
animal safety and to protect against unnecessary deaths of animals and so forth.
The same goes for experiments involving human beings. And that's again, thanks to a kind of
horrific history of experimentation done in the 20th century on unsuspecting human subjects that
were mistreated. And so there's a whole set of ethical requirements there so that nobody wants
to get rid of that. It is federal money that's being spent. And so there's going to be some oversight
like the budget and how the money is spent and so forth. Right now, there's a lot of focus on
international security and focus on our researchers unwittingly passing the top technological secrets
to researchers in China. And there's probably that new focus on China and maybe some discrimination
involved. But yeah, it's still a fair consideration. But how much should we fund research that might
unwittingly be used to support a foreign adversary, let's say. So anyway, like any bureaucratic
requirement you point to, someone somewhere is going to say, there's a good reason for that,
right? We need to keep that one. So it's really hard. And the whole is like, that's
by a thousand cuts. But if you point to any one specific bureaucratic requirement, again,
there was a good reason for it. There was some scandal. There was some
problem that someone was trying to solve with this rule, with this procedure.
And so that's why there have been many efforts to get rid of or to try to limit bureaucracy,
but they haven't really gone anywhere. Because what you really need is to have some person,
like with almost, I hate to use this word, but almost dictatorial authority over an agency like
NIH or over an agency like NSF, to just go through the entire bureaucracy and take a red
pen and slash through the stuff that isn't necessary or that isn't the top priority.
And with the objective of, let's say, reducing the burden on scientists time to 20%, let's say,
rather than 44%. And you'd have to have someone who is willing to make some really difficult
tradeoffs and difficult choices and prioritize. We have a thousand things that everyone wants
researchers to do. They'd take up too much of their time. So you're going to have to slash
through some of them that even though they individually, they might sound like a good idea
because you just have to prioritize. So you need someone or some committee that has the power and
the will to actually get rid of some rules and regulations that maybe seem like a good idea.
And that's politically difficult. But I think it's still worth trying because otherwise we're
in a trajectory where ultimately we'll just be paying for 50%, 60% of science at this time.
And it's insanity. Scientists need to have more time to focus on their science.
So that's one of the issues that I've written. But I could dive into many more.
Just to attach on this for a little bit longer, who would be an org that could do this?
Or do you think it would be an individual at each scientific organization? Or could that be
something for the GOA? Or would that be something like scientists writing an open letter about
specific things that they get particularly up about in their research? In terms of thinking about
solutions, if it's not as easy as a science gone coming in and doing it, are there any
pathways that you think are worthwhile exploring? Another challenge is that these rules and regulations
arise from different places. They arise from to dig into the weeds of the American government,
like the Office of Management and Budget, or OMB, that has federal-wide authority to regulate how
federal monies are spent and accounted for. And so they have a lot of budgetary requirements
amongst others. So that's one source. But that's under the control of the White House.
And so the White House could do something about that. Some of the requirements come from agencies
like NIH or NSF themselves. Some requirements come directly from Congress. So Congress mandates
particular practices and says that agencies need to look into X, Y, or Z. Some requirements
honestly come from universities that want to behave conservatively, so to speak. They're
risk averse and they want to make sure that they cross every T and dot every I. And some universities
perhaps over-regulate their own researchers or over-stats their own departments that are in
charge of monitoring and evaluating and submitting budgets and all of that. So yeah, it comes from
many different places. So that's one challenge. I do think the White House could, in theory,
issue an executive order that asked the Office of Management and Budget to review all of its
practices and its rules with the guide towards slashing stuff that hasn't a time requirement
on researchers or the impact on researchers directly. The White House could also review
its past executive orders because some executive... So here's an example. Some of these requirements
come from the White House itself, from prior executive orders. So there was an executive order
in the 1990s signed by President Clinton that says that if you get federal money, you need to
certify that you make people wear seatbelts. And again, it's well-motivated. There was nobody's
really against seatbelts and it's a good idea to probably save some lives perhaps. But some 20,
30, almost 30 years later, with their seatbelt laws in various states and probably anyone who
wears it wants to wear a seatbelt or he does wear a seatbelt. And it's done clear that making
people check that box and on federal applications really does need good. And so you could go back
and look at it through prior executive orders like that and say, look, here's some executive orders
that may have been a nice idea at the time, but we don't need to make everybody at every university
certify that they do everything with a good idea in the world. We can prioritize and say, look,
the time that passed when we needed to investigate whether people use seatbelts. And so there are
probably any number of requirements like that that again, individually, sure, that's fine to make
people wear seatbelts and it's not that much time to check a box. But just in terms of priorities,
we should be able to streamline that. So the White House could, as I say,
go back and look at the requirements that it has come up with over decades and see where it does
streamline. So yeah, I think there are some opportunities that the White House could take
advantage of if they want to. Okay, if anyone relevant is hearing, that feels a little bit
about the action. I also guess like the problem here is almost just getting worse over time just
because like people rarely ever take things out, but they just add to the pile. Like you never
really know how large the pile in total becomes once you start adding stuff to it.
But your individual thing that you want to add to it is really important right now
without considering the entirety of it. So it's definitely, hopefully it's not getting much worse
on the long run. But yeah, okay, that's definitely really important one that I couldn't agree more.
Like riding grants is already complicated enough. And if that is a big one, that's that that could
be an easy one to perhaps cut down on. The other thing that you've also written about really
interestingly is I think on like patents in general. And I think the sub seg post was actually
titled like how we screwing over researchers or something like that. And it had a lead at least
a bit like patent component in there. And that's I guess university targeted to some extent also.
So perhaps you could share a little bit more about what you were addressing there and perhaps even
tell the quick story of Catalina Carrico, if you feel like it. Sure. Yeah. Yeah. I admittedly
chose a kind of provocative title for that article set to hopefully get people to read at least a
little bit to rewind a little bit. There was this famous act in 1980, the Bidol Act that was passed
in the United States. Prior to that, it's complicated. But basically, when NIH NSF, when the federal
government funded research, it would often end up taking control the government itself would end
up taking control of patents arising from that research. And there was this perception that
the government is not the most efficient user of patents. It doesn't know what to do with them.
They weren't being actually used very well or commercialized or turned into
something that was useful for the market, useful for medical patients and so forth.
There's this idea that instead of having the government take control of patents,
let's shift that and have universities take control of patents because universities
are technically the recipients of all the federal grants that come from NIH and NSF, etc.
So the money doesn't go straight to it. We talk about a researcher getting grant, but it doesn't.
The money goes straight to the researcher's pocket, right? It goes to the university. They're
the ones who handle the money, and they pay the researcher, right? So universities are the...
They have a lot of indirect costs. That's a whole separate issue. But yeah, the indirect costs at
top universities are often between 60 and 70%. So what that... And just to clarify what that means,
so if you get a... If you as a researcher get a grant and let's say it's $100 to be simple,
like NIH would give $100 designated for you, the researcher, and they would add
60% on top of that, $60, let's say, as indirect costs to the university. So the total grant would
have to be 160. So the 60% is on top of rather than taken out of. So it's not like... So anyways,
that's just how that works. And by the way, indirect costs are also supporting a lot of
bureaucracy that universities administer. So the bureaucracy issue is tied in to indirect costs.
So anyways, university started patenting. And the story since then has been like, this is a great
success. So until 1980, you had all these patents that were either didn't exist or went unused.
And then afterwards, you have this huge flourishing university-based patents.
And so around 20 years ago, there were a bunch of European countries that also started moving
in that direction and trying to give universities more control over patents. But here's the thing,
in some of those universities, in some of those European countries, the prior rule had not been
that the government controlled the patents. The prior rule had been that the professor or the
researcher controlled the patent. And in fact, they call it professor's privilege in some of
those countries. And so they were switching from in a different direction, right? They were switching
to the US regime that was perceived as successful. But they were switching from completely different
place where the professor or the researcher had more control. And so there's been some empirical
research on that that has shown that when European countries moved in that direction,
the rate of patenting actually went down, which actually doesn't seem too surprising because
universities often are very diffused. They encompass many departments. They may not have
anyone who's a specialist in what one professor is doing and the commercialization of that
research. And so maybe they put less priority overall on trying to commercialize anyone given
discovery or possible patent than the researcher themselves who has more skin in the game, so
to speak. So that's where the empirical evidence seems to lie so far is that it would be better
to give better for innovation, better for patenting, better for commercialization,
to get professors more of a say and perhaps to control over the patents rather than the
universities themselves. Now, this came to a head with Katalin Kiriko, which is the story that I
talked about quite a bit. So she was a Hungarian researcher who came over to the U.S. and worked
at the University of Pennsylvania and got demoted repeatedly at Pennsylvania because she couldn't
get NIH grants to support the work that she was doing on early mRNA research, which at the time
it was not very popular. They weren't further of it now because it turned out to be the basis
of some COVID vaccines. But in the 90s, it wasn't very popular at all. And it was seen as the dead
end for whatever or something that's extremely difficult. It would never work. So she couldn't
get grants to support the work. And the NAS, by the way, opens up a whole other huge topic,
which is the role of NIH money and so-called soft money in universities, soft money, meaning
researchers like Kiriko who were expected to pay for their own salary. It's like they're
they're not given a salary directly by the university or not 100% of their salary. They're
expected to raise their own salary for themselves to their grants. And so that makes them very
dependent on appealing to whatever NIH wants to fund at any given point in time.
So Karen Kiriko was repeatedly demoted and eventually basically pushed out of academia
even after what became a paper that later won her and her co-author the Nobel Prize.
Now, of course, no one knew that at the time. Like Ben, the University of Pennsylvania couldn't
foresee the future. But the University of Pennsylvania did, as I found from reading some
student newspapers from Pennsylvania, they kept the patents on for work, even after pushing her
out of academia. And the student newspaper produced a chart like the universities across
the country and how much money they're making royalties from patents. And the University of
Pennsylvania was far and away, making many times more money than Stanford or other universities
that you might think of as hotbeds of discovery and technological advancement.
And so Pennsylvania made something like over a billion dollars in one recent year from the
Fed with vaccines and from the patents on apparently from the patents on Kiriko's research,
even though she's long gone from Penn and they not only didn't help with her research,
they drove her out of academia. So things like this kind of glaring
unfairness and on top of the inefficiency process. So yeah, I think that's an area that yeah, I
think definitely deserves some reform or at least some really detailed, I think Congress
definitely should fund or require some really detailed investigations of what is even happening
at these university offices that are supposed to be patenting, researching, commercializing it.
How many patents go unrealized or uncommercialized? How many take too long? There are lots of
anecdotal complaints at certain universities that the process takes too long and the university
demands too much of a cut. I think it'd be better to have a more systematic kind of investigation
to add to the anecdotal stories. But in any event, I think I do think that the story from
Europe or the empirical evidence from Europe shows that it might be better to move back in the
direction of giving the professor or the researcher more of a say in what happens to their own
discoveries. Yeah, it's crazy when you think about it, it's almost like all the incentives
that are wrong. It's a few, right? But like, why even, how to come up with that type of system
in the first place, I think. But yeah, I think I love that there is this almost not really an
A-B testing, but at least some precedent of how it used to possibly work better and that that was
useful to go back into that. You already mentioned one of the bits on funding and with the kind of
like soft support from the NIH. And so I think another really interesting, super detailed analysis
that you've done is specifically on NIH reform. You list a full laundry list there of things
that could be improved with the NIH. And I don't think we get through all of them. I think accounting
is almost 10. And we already touched on them individually. But when you think about the
the NIH, what are like a few kind of crucial areas that you yourself are perhaps really excited about
and promoting that there could be possibly a good reform applied to the NIH?
Sure. I think there are any number of ideas, as you say. I think one thing the NIH should
consider using more is an approach called basically fund the person, not the project.
Now, it's interesting. There is a program at one of the NIH institutes called the National
Institute for General Medical Sciences, NIGMS. And if you're wondering what that is,
because many folks might, I did when I first started it. It's basically the NIH Institute
that funds basic research as opposed to National Cancer Institute, focuses on cancer, the National
Heart, Lung and Blood Institute that focuses on the cardiovascular disease, etc. NIGMS is focused
on truly basic science that's not necessarily connected to any one specific disease like some
of the other NIH institutes are. So NIGMS has this program called Maximizing Investigators
Research Awards, M-I-R-A, and they pronounce it Mira. And this program is really intended to give
researchers more flexibility and freedom to give them funding for several years where they don't
have to spend as much time trying to pre-specify everything that they're going to do and then
report back on what they said they were going to do three years ago or four years ago,
instead it's intended to give them more freedom to follow the science and to follow their nose,
so to speak, as to what the best ideas are at any given point. And I think that there's some
evidence that the papers produced by that are performing equally as well or better in terms
of citations. That's only one very limited metric, I think longer term you would want to see a rate of
breakthrough discoveries. And that's hard to see because you can't expect a breakthrough every
day from everyone. That's impossible. And as the director of NIGMS, John Lorsch has said,
if I knew where the next breakthrough was going to come from, I would have already made that
discovery myself. So part of his idea of funding is that you want to spread the money widely
amongst talented scientists and give them the freedom and who knows where the next breakthrough
will come from. Especially with basic science, often there's any number of stories from basic
science where a discovery will be made in one decade and then three decades later it turns out
that it's amazingly useful or influential. But yeah, I think that's a good example of a program
that's experimental at NIH in the sense that it's a new thing. It's still fairly new. It's been
around for a few years. I think that program could be expanded to other institutes at NIH like
National Cancer Institute. And there are other institutes that have a version of this, but it's
much smaller. NIGMS funds this type of grant four times as much when I last reviewed the numbers.
Four times as much as the rest of NIH put together. Huge imbalance. NIGMS is very much focused on
this for basic science, but I think you could try that approach elsewhere at NIH. And again,
with the idea of giving scientists more flexibility and freedom. And here's another key idea that
was in the document I sent you, which is NIH should take a more deliberate approach to
experimenting and learning from what it does. Take a very meta science approach instead of just
starting up new programs and saying, all right, everybody's going to do this. Then you don't
have as much of a chance to learn and to iterate and to introduce deliberate experimentation.
NIH is big enough that you could do like the literal randomized experiments and how you hand
out money. They'd fund something like 90,000 grants at any given point in time. There's 90,000
grants and each grant often supports multiple researchers. So I'm not sure the exact total,
but it's hundreds of thousands of researchers that are supported by NIH. That's plenty of
opportunity. So you could do a randomized trial that involved a thousand researchers,
and that would be a drop in the bucket from what NIH supports. And you could randomize 500 to be
funded in one way, 500 to be funded in a different way, and they just fold over time and see what
happens. See what are the results from funding that offers more flexibility and freedom versus the
more usual way that NIH does things. And I think that kind of deliberate experimentation
is something that NIH should do at all. They've really ripped on that.
Yeah, I think on your last point, it's really interesting just to hop back on the kind of
funding people, not projects that is somewhat also pretty present right now, I guess in the
private space where I think, for example, the Lieberman Brothers, they're now launching a new
effort to actually fund individuals early on before basically making bets early on and almost
invest into individuals like you invest in companies. And I think that kind of at least that
meme or that new approach should also hopefully extrapolate outward through different and perhaps
even scientific funding organizations. I think that would be wonderful. I think.
I'd like the parallel to venture capital because I mean, I think there are a lot of,
any number of, again, statements or examples of venture capitalists who say, yeah, you're making
a bet on the person. There are many examples where a founder of a company ends up pivoting and doing
something slightly different or a lot different. And venture capitalists often are happy with that
because you're trying something and you realize it didn't work and now you found something that
did work. And if you would be unthinkable to go to, I don't know, to go to Mark Zuckerberg and say,
wait a minute, like your original proposal said that you were going to do X, Y, and Z and said
that you were going to spend $5,000 in year three on this line item and where it shows that you
spent the money that way. You would never want that level of micro management of a talented
entrepreneur. Okay, students want to give them a little more freedom and flexibility to adapt to
the market in which we should treat a lot more scientists the same way that you should give
them the same respect and autonomy that entrepreneurs routinely have and give them the freedom of
flexibility to say, wait a minute, I said I was going to do X, Y, and Z, but I tried it and it
didn't work and I came up with a better idea the next year and now I'm going to want to do the better
idea. Of course, we should all follow the better idea when that comes up. Yeah, I guess. Otherwise,
you just assume that people aren't learning as they're actually in the field and then
fermenting and dropping things. It's a crazy assumption to make in the first place, I think.
The process should be about learning and changing and adapting to new ideas of the MLA. That's
a bold point. If you already knew what you were going to do five years from now,
I think it's, I'm paraphrasing, but well, one scientist that I know at Pennsylvania as well,
he said, if I'm doing what I said I was going to do five years ago, I should be fired because I
should have warranted and adapted in that book. Yeah, I love it. We only got to a small part of
the laundry list that you have for the NIH, including other misopryptonies and stuff, but I
think I want to close at least my part of the podcast with a question perhaps a little bit on
a hopeful note to already lean into the extension whole part of the podcast after, but we have
briefly touched on when we met and you've also pointed out again afterwards that there's actually
now a really cool new landscape emerging of these alternative possible homes for researchers and
scientists. I think there's FROs, there's Future House, Park Institute, Astera, Spectac, there's
a few really interesting new orgs that have been popping up and we certainly had a few founders
and executive directors of these orgs already on here for podcasts and for individual topics,
but could you share a little bit of how you see that landscape changing and what these new organizations
are setting out to do and perhaps what we can hope for in the minutes? We don't have to cover all
of them, but just the general spirit, a great idea. And you could add to that some, there's
some government agency, surprisingly enough. There's a health version of DARPA, so it's called
ARPA-H, which I'm sure any listeners are familiar with, but yeah, it's intended to be like a
government agency that has the innovative approach that DARPA and it's taken for decades.
And there's a version of that in the UK as well called ARIA, also trying to be like the
imitator of DARPA in a way. I think that's all tremendously hopeful. I do think it's
hardly borne out of discontent with the current system, but as I say, it tends to be
very bureaucratic and very top-heavy and a system in which it's hard to get a
foothold, the average age for NIH, the first major NIH grant is so we're making people like
slave away in other people's labs for 15 years before they finally get a foothold as a researcher.
And so they're long past the age, which a lot of people in previous generations did some of
their best scientific work. Einstein made some of his greatest discoveries at age 25, and so
I think Newton, James Watson, there's any number of examples of that kind of thing.
So there's a lot of discontent with that system. And so there are people within government and
private philanthropists who say this is an opportunity to diversify the landscape,
to come up with new ways, hopefully better ways of doing science and of funding science and of
organizing science, all very important meta science issues. And so one thing that I really
hope we're able to do, we being just collective meta science community and policymakers over the
next five to 10 years, is just really deliberately learn from what all these new organizations
have produced and do it in a systematic way. I do think possibly one risk, like any new
organizations, like just the same as universities, their temptation is going to be to send out
press releases about all the amazing things they've done and brag about that. And that's fine,
that's to be expected, and often that's even true. But there will be cases in which they fail,
and I think it's important to learn from failure, and it's important to be public about that.
And so I think hopefully over time, we can have a meta science discussion that's able to have
just a truly honest appraisal of what's working, what has failed, and failure isn't a bad thing.
We should learn from not everything that's going to work for the first try. Let's learn
about how to design organizations more efficiently or with more perfectly going forwards.
And so I think that's where I hope the conversation goes for the next five to 10 years.
Yeah, thank you for that. So yeah, so this part is more like the philosophical part. We've been
talking about the meta science and everything, but it's diving into the sort of existential hope
aspects of where science and the progress of it can take us. You touched on it now,
but I would be really curious to hear, do you feel like things are changing? Are you gaining
traction with this? Yeah, I do think things are changing. Again, we just touched on that with all
the proliferation of new organizations, both inside government and outside government. I think
that's a really helpful sign. And again, my hope is that over time, there should really be a diversity
of approaches and organizations that probably something that will be bad for science is to
say everybody has to fit in the exact same box. That probably isn't good, even if the box makes
sense, so to speak. So we talked about fund the person, not the project. That seems like a good
idea. And we should use it sometimes. But I think if literally 100% of science funding was fund the
person, not the project, there probably be some failure points there as well. There probably be
some things that got missed. There are probably some examples where you should fund an organization,
like fund a team, not the person. So there are lots of different approaches one could use.
And in fact, in some areas of science, the Large Shader and Collider or big astrophysics
efforts, you're not funding one person, you're funding like a team of 1000 people. You know what
I mean? Yeah, if you want to approach this to science and to science funding, I think would
probably be suboptimal. But I do think that hopefully going forward, I think one thing that
could help improve the pace of innovation is having a more deliberately diverse approach
in like how we fund science and the source of people to get funded, and so forth. And
when I say the source of people to get funded, that's also an interesting point to emphasize.
There are lots and lots of examples from history where great scientific advances come from places
that sometimes you wouldn't necessarily have expected or they come from people that were
heretics at their time, Semmelweis and the germ theory of disease, like people despise them at
the time. There are tons of examples like that. It doesn't mean we should, again, we don't want
a science funding system that only funds people that are outcasts and heretics. That probably would
be in the wrong direction too. But I do think there should be some space within science,
like a National Institute for Oddball. We should have a deliberate approach to fund some things
that are outside the box. And now some of them will be crazy and won't work. We might end up
funding some of the greatest breakthroughs ever if we've made more space within the scientific
funding system or people with truly outside the box ideas and approaches that don't get funded
correctly. Yeah, I remember I was talking to an economist, I think, Johan Nurebe,
about he was talking about how technology often progresses. It comes from quite dirty areas or
not necessarily the most appreciated ones. I think his example was the technology of online
payment solutions coming from porn, like people wanting to watch porn online and that was like
what are online payment solutions? And now that's a very useful and established thing.
And I'm sure there are more exciting scientific innovations that came from the not the most
appreciated areas maybe. Yeah, but it sounds like you're positive and you're optimistic.
Would you say that you're optimistic about the future?
That is a big question. In general, yes, I think that there's lots of possibilities. There's
dangers as well. We've both kind of, I think I've met you in an effective algorithm conference.
There are certainly areas like nuclear proliferation or biosecurity or artificial
general intelligence. There are some areas of potential R&D advancement that do possibly have
some risk and some would say existential risk. But I think the theme of your podcast existential
right is that hopefully with the broad sweep of innovation and improvement that we can address
those existential risks and that we can hopefully still keep progressing and making life better
for everyone. And so I think that's where I guess my hopes and efforts would lie is like trying to
figure out what we're doing wrong, all the ways in which we're holding back scientists and science
itself and figuring out ways to hopefully speed up and accelerate the pace of innovation. I think
that does offer more hope for the future. Yeah. I'd be interested to hear in relation to the
like increasing pace of progress on like in relation to maybe AI in specific,
do you see the science landscape shifting due to that?
Due to AI in general, I haven't looked at it that much in depth. There are some really amazing
advances that have been made, for example, AlphaFold and protein folding. There are other
similar tools that are like that that offer the potential to speed up at least some components
of science. I'm less certain myself with what I've seen out of large language models so far. It
seems like they have, they sometimes show science a great sophistication, but then sometimes they
like just completely hallucinate, at least the ones to date. And so I'm a little nervous about
that. You could end up with kind of pollution of the scientific literature with people using
AI models to write papers thinking this will help them publish more and then it'll end up on
live somewhere and might be largely fake or largely just made up or hallucinated. Another
thing that I worry about, and this is born out of the work that I did while I was at the Arnold
Foundation, a lot of the published literature just isn't that good. Some of it's outright fraudulent
and there are more discoveries about academic broad that come out, it seems, every week.
A lot of it isn't that reproducible. And even the stuff that is reproducible, it often isn't
described that well in print. So one thing that I funded, for example, was called the
Reproducibility Project in Cancer Biology. And their original idea was to replicate
the experiments from 50 top-sided cancer biology papers. And ultimately, they could only complete
fewer than half of their intended experiments. And the reason was that in every single case,
you couldn't just go off the publish paper. You had to go back to the original lab and ask them
to fill in all the gaps and fill in all the details about what they had actually done.
And some of the original labs were not cooperative at all. In some cases, just were
hostile. Some cases just said, we're not sure. And when they worked for operative,
the answer was always that you need twice as much materials as you expected. And so it's
going to cost more and take longer. And they're like, worry about AI models that might be
trained on scientific literature as if what's written down on paper is the complete entry.
And that's often not the case. And you just worry that might spiral into even more
irreproducible work. So again, cautiously optimistic about some of the specialized tools
that are out there that are based on like really rigorous systematic databases. But then
there's a whole wealth or a whole broad millions and millions of scientific articles
that I would say probably aren't worth trying to use in an AI model in the first place or at
least with great skepticism and a lot less of corrections needed before you just
train an AI model and expect to get useful information out of it. So it's all over the
place. But yes, that's my answer for now. Yeah, that's very interesting. I hadn't really thought
about that in relation to that it would get access to like maybe incorrect data technically.
But yeah, I guess that I heard about the Center for Open Science and that's like
where they try to pre-register the studies that they're doing so that you can actually check if
they are correct in that they're reproducible or that that oftentimes you don't really publish
what's maybe not like an exciting result or something like that. But here you can see what
people have published or have been doing research on even if the results aren't like that exciting.
Any other like projects like that that you think are promising or seem like they could
actually make a difference in this field? Yeah, there's a lot of directions I could take this. So
the idea of pre-registration really came out of medicine from some recommendations that were
made. And I think it was early the late 1980s or definitely by the early 90s, there were some
folks writing about it, the idea in medicine. These folks in medicine, they were trying to do
meta research in medicine to try to summarize the whole body of literature. And as of the 90s,
probably still true somewhat today. They were very frustrated. And I remember an article
by a couple of folks at the time where they said that it's just really strange to them that you
can find much more information on baseball statistics and up to the minute information
about every baseball team, every baseball player, everything they've ever done. But yes,
it's impossible to find that information about clinical trials, that much wealth of information
about clinical trials in medicine, which is to them more important than baseball. So pre-registration
was in part a way to get access to information about the trials that are being conducted
on human beings and involving drugs or other types of treatments that might be used in medicine.
That was one motivation for me to still be able to see the kind of see the whole denominator,
so to speak. It's easy to see the published successes and the press releases of announcing
that a drug will cure in the US, the commercial that say try this drug. But it's harder to find
the failure since the idea of pre-registration in part was intended to address that. And then
like subsequently, I guess you're able to like what there was one famous article by kind of
Eric Turner and some colleagues where they reviewed something like 70 or 80 clinical trials
that have been done on antidepressants. And I'm going off a memory here, so I might get a number
slightly wrong, but roughly half of the trials showed that the antidepressants work and roughly
half of the trials showed no results. The antidepressant wasn't any better than placebo.
And they had access to all this because of both pre-registration and because they went to the FDA
and these were FDA-free drugs and the FDA demands to see all the evidence and not just
what got in the top journal. So you have the whole denominator there to look at.
And so what they found was that basically all of the positive trials, I think except one,
got published in a top journal. And the negative trials or the null trials mostly went unpublished,
but I think a few of them were published and then with a kind of spin or misrepresentation
as if they've been positive studies. And so yeah, if you look at the medical literature,
and this is also back to what I was saying about AI, if you looked at just the medical literature
on those antidepressants, you would say, wow, that'll work. Everything's amazing. But if you
look at the whole body of evidence that the FDA could look at, you would say sometimes they work,
sometimes they don't own or sometimes the one medicine is better than others. It's a lot more
complicated and messy when you have all the evidence sitting before you. Yeah, there have been
efforts like that in medicine. Pre-registration has been growing in other disciplines, psychology and
economics in particular over the past 10 years or so. The American Economics Association, like
the Center for International Science, started demanding pre-registration about 10 years ago,
and the rate at which that happened on a yearly basis that's gone up within economics alone.
Yeah, I think that's all kind of hopeful signs of progress in different fields, just being better
practices about being public about what kinds of studies you're doing. And then ultimately,
it's kind of all for not unless you publicize the failures and the nor results as well. Because
again, if you only publish the positive results and don't say anything publicly about the negative
results, it's very skewed. It'd be like if you flip a coin and then you only announce when you flip
the heads. Yeah, you would look like you'd flip 100% heads, but that wouldn't be true. So be very
misleading. Yeah, I'm going to make a bit of a turn with the next question now, which is it's a
question that we always ask in this podcast, which is in relation to the like AU catastrophe. So
it's a term that means basically the opposite of a catastrophe. So once it's happened, the value of
the world would be much higher. And feel free to relate this question to like your area of expertise
within science. And I think also think ambitiously when I ask you this question, which is if you
could envision like a specific you could have for the progress of science, what would that be?
What would be an existential hope scenario of science? If you like, if all the work that you're
doing now came true, what would happen? That's great. Yeah, I first came across that word,
you catastrophe and the writings of JR Volkian. Did he invent the word? I can't remember. Yeah,
I believe he did. Yeah, it's a long time. Yeah, that's that is a super interesting question.
The kind of revolutionary side of me says that in terms of meta science, what would be really cool
is if you could duplicate all federal funding of science, but with an entirely new set of
institutions. Imagine we spend 50 billion or so on NIH, what if we add in an extra 50 billion on
biomedical research? Or with the condition that it has to be run completely differently
from NIH, and it has to be in the hands of different people. And again, touching on these
ideas of exploiting the and proliferating the diversity of the approaches and the scientists
that get funded and the ways in which funding is handed out, because then you have two different
systems operating. And so thinking very meta here, you have two different systems operating
with equal amounts of funding. And now you can really see at a grand scale, hopefully,
what happens? What are the results? It would be a chance to test out lots of different meta
science ideas that people have discussed for years or for decades, but you just really need
a kind of the grand landscape in which to experiment with that. So again, that's very meta,
but my hope would be that we learn a lot about how to fund science and how to organize scientists
into organizations, into labs and universities. We need entirely different institutions that
are not don't look anything like universities, for example, maybe you should maybe you should
set up a whole national institute where the rule is you can only fund scientists who are
working out of their garage. I don't know, it just you need some kind of like wild ideas out
there to try out different approaches, different scientists, different ideas. And my hope would
be that at a minimum, we'd learn something from that. And but in terms of existential hope, I think
we we might end up creating a number of great breakthroughs that wouldn't have happened otherwise.
And today is hot, heavy, bureaucratic system that is so conformist and so focused on doing
whatever gets you approval from the kind of the existing bureaucracy.
Yeah, I really, really like that. It's if you like were to introduce someone who is entirely
new to this field, is there anything you'd recommend that they like read or watch? Maybe it's
just your own sub stack? Or is there anything else that you would recommend for an intro to the
field? Sure, the writings that Good Science Project has produced are good points to look at.
You mentioned the Center for Open Science, they've had a number of publications and projects that
are related to meta science. I would say that there's more stuff coming out from the Institute
for Progress and the Federation of American Scientists, they've had a series of papers on
open science, for example. Yeah, that might be that gives you plenty of reading to start with.
That sounds like a great place to start. And how can listeners best stay updated with your work
and the work of the Good Science Project? Sure, you mentioned sub stack, it's just
goodscience.substack.com. Also, the website is just goodscienceproject.org. And yeah,
we'll have to hear from folks who have ideas or enjoy the writings and want to learn more.
Great. Thank you so much, Stuart, for coming on this podcast. We really appreciate it and
we'll definitely keep an eye on all the work that you're doing with the Good Science Project
in the future. So thank you so much.

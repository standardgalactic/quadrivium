I'm Tim Ventura, and we're joined today by Dr. Sarah Eaton, CMO and Alignment Strangest
for Decision Zone, an innovative startup focused on artificial general intelligence.
Dr. Eaton has a BA and PhD in philosophy from Yale, and a long career in higher education
as a lecturer, academic dean, and professor at reputable institutions including Yale,
John Jay College, Eastern Michigan University, Ecumenical Theological Seminary, San Francisco
Theological Seminary, Mary Grove College, and more. In her current role, Dr. Eaton is
responsible for developing and executing the marketing strategy for an innovative AGI platform
and leverages her experience in teaching, writing, and researching philosophy
to bring a unique perspective and skill set to the field of AGI and its ethical and social
implications. So Sarah, welcome. It is truly a pleasure to have you with me today.
Thank you, Tim. I'm really excited to be here. I appreciate you opening up your podcast to people
like me. Oh, absolutely. Absolutely. I know you are not used to podcasting, so we will be gentle
with you today. Thank you. I promise. Promise, promise, promise. So today we are talking about AI,
artificial general intelligence, and your company decision zone, as well as the product that it
makes, Dadax, D-A-D-A-X. And we're going to be touching on some of the philosophical implications
of this technology as well. And again, that's why I'm so excited to have you with me. But I want
to start out by asking, what led you from a background in philosophy to your current role
working artificial intelligence? Well, I don't want to make this too much about my personal story,
but it is a little bit weird and interesting, so maybe I can excuse myself for the moment.
Back in 2017, I was the director of religious studies at Mary Grove College
and teaching philosophy. And we had an emergency faculty meeting letting us know that the college
was closing. And at that time, I was, I guess, ready to get out of academia. I didn't try to
look for another academic position. So my husband and I moved to the Middle East for about five years.
And I did other things there. But eventually, I, I guess, I started thinking about other
possibilities and different ways that I could use my philosophical education that didn't involve being
in a university setting. And I actually met the business development director for decision zone
over Twitter. The algorithm brought us together. And after we went back and forth a bit, he said,
I think you need to meet Rajiv Rajiv Bargavis, the founder of Decision Zone. So we set up a
Zoom and the rest is history. Ah, okay, okay. Well, and I will dig more into that. And the other thing
that I want to explore a little bit with you is the religious implications. And I think, you know,
I pre write my questions, I think I pre wrote one, maybe two about that. But that for me,
that's incredibly intriguing. So I definitely want to circle back to that, because I think
that's something that you can probably speak to. And I think it is incredibly important. And it has
been overlooked, not only the philosophy, but, you know, potential religious aspects of this.
But let me get into the company a little bit first. So Decision Zone makes a software platform
called data, which stands for decentralized autonomous decisioning agent. I believe it's
data X. And the X is basically represents, I guess, the event based development platform
for causal AGI. So can you tell me a little bit about what data X is, and what makes this approach
to artificial general intelligence unique? Well, data X, as you said, is an agent.
It's a platform for building autonomous applications. So the X is the variable, it
stands in for any human operating logic. So that could be the logic of a pilot of an F16,
it could be the logic of the drone, it could be the logic of an HR person at an enterprise,
any human operating intelligence is replicable using data X platform. So that was really the
idea. AI is right now basically tools, tools in the hands of people who have the intelligence.
But we want to move beyond the machine being merely a tool for us. We want to give it its own
agency so that it can do the things that we don't want to do. Okay, well, let me let me dig
into artificial general intelligence a bit. I guess as a concept and as an emerging, you know,
part of our world, right? This term artificial general intelligence seems to get more difficult
to define the closer we get to it. That's something that I've noticed, especially over the last
couple of years. Chat GPT and many other LLMs now passed the Turing test, right, which was considered
the you know, the couldn't quote gold standard for for AI for a long time. Well, that's that's
ancient history now. And some people are calling chat GPT functional AGI. But the vast majority of
computer science experts still insist that large language models are nowhere near the kind of
reasoning required for true artificial general intelligence. So what what are your thoughts
on what constitutes AGI? And how is decision zone pursuing that? Well, in a lot of ways,
I think we tend to agree very much with the definition of AGI, that open AI put on their
website, that AGI should be able to perform the cognitive tasks autonomously of a human being,
right? Now, those cognitive tasks might eventually extend into physical tasks as well.
But generally speaking, an AGI doesn't necessarily need to be embodied to be an AGI.
I think the big difference in the way that we think about it has to do with the level of
autonomy required. So when you look at some of the other research, particularly out of Google
DeepMind, they tend to give an autonomy scale, which is sort of in line with what full self drive
autonomy scales look like. And we think that unless you reach level five, you're not going to have an
AGI, whereas some people think you could have a less than fully autonomous AGI. So I think that's
the basic philosophical difference between our understanding of it than theirs.
Okay, so it sounds like the approach is more functional than in terms of what it can do
and its level of autonomy, as opposed to kind of this thoughtful self reflective aspect of it,
that a lot of the computer science community is waiting for, you know, before they pronounce it
AGI. Yes. Well, and I can say something about this from a more philosophical perspective,
you know, when when you're going through a PhD program in philosophy, you have to start with
the Presocratics and Plato and Aristotle, go through medieval philosophy, I set through medieval
logic courses, then we get to, you know, the moderns, starting with Descartes, all the way through
the idealists, then you have the existentialists, then you have the postmoderns, you have this
full range of of the history of thought that's laid open for you. And what I would say, looking
at it from sort of an outsider, because I've never programmed a single line of code in my life,
right? I'm really not coming at this from the know how or technique angle of all of this. But
what I see is that most of the people who are writing and talking about AI are coming at it from,
I would say, a modernist perspective. And when I say modernist, I mean, basically in entrenched
in the Cartesian dualism, subject object dichotomy, where there's a knower and a known and a
representation in the middle. And we're sort of stuck in this situation where we lose our
contact with the real. And I think that basically, AI is living in this sort of,
I don't know, a world of statistics that don't fully actually capture the reality,
they can't get to a single event, they're looking at, well, the past, we're stuck in looking at
historical data and trying to analyze trends and patterns, and then projecting that into a future
that doesn't exist yet. So you're always sort of either in the past or the future, but you're
never in the now. And that's really what you get when you focus on an autonomous system,
where you're not really trying to burden that system with the weight of self consciousness.
We're trying to get out of that, not put that problem into the thinking machine.
Well, let me jump around to my questions a little bit, because in your bio,
you had written about being influenced by D.G. Leahy's ideas of world pragmatism
and the move beyond sovereignty. And I have to plead ignorance, I know nothing about that,
but I wanted to ask you if you could explain that a little bit and how that might fit into this.
Well, I could talk about Leahy all day long, so you're going to have to restrain me a bit.
But I can say that about 2015-16 was when I first found out about him. Unfortunately,
he was an American philosopher. He lived in New York and taught in New York and in Baltimore.
But he passed away in 2014, so I never had the pleasure of actually meeting him. I had been
in my own education, sort of my PhD thesis was on Martin Heidegger, and he had a very
negative, I guess, attitude towards technology, as an existentialist. And I was always looking
for something better and different. Even though that was what I had spent most of my time with,
I was looking for something else. And I was invited to give a talk in Chicago on the problem of God,
was a very general problem. And I was looking for a philosopher that was from the Catholic
tradition because this was at Loyola, and that was the general theme of the conference. So I
just started Googling. And the crazy thing I Googled was Catholic philosophy, New World Order.
And New World Order and Catholic Philosophy gave me DJ Leahy. This is one of his most amazing books.
I don't know if you're getting a mirror image of it, but it's called
The Cube Unlike All Others. And his main book was called Foundation Matter the Body
Itself. DJ Leahy was not a philosopher or a theologian. He was a thinker. And what he was
trying to articulate is what he calls the thinking now occurring. So we have a seminar that happens
four times a year of former students and other disciples of the thinking. I wouldn't say of
David himself, but of the thinking. And I've given a couple of different seminars in that group,
one on AI and the thinking, and one on personhood and the thinking. So just to connect this back
to the general topic, you know, a lot of people seem to be concerned very much about whether these
systems are or would be able to become conscious. I think that's less of a question. I think we ought
to think of it in terms of the question of whether they are persons and in what sense.
Leahy has a very broad understanding of personhood. And part of it comes from his faith background.
He's trying to, I guess, eliminate the distinction between science and faith
and think more about revelation. So he's sort of an outlier, at least in the in the 20th century
or 21st century. He's an outlier. And in his thinking, there are absolute persons. There are
formal persons that would include spiritual beings such as angels. There are essential persons,
which are humans. And then there are material persons, which are living things and even non-living
things. So in his way of thinking, even my glasses are a person, right? So what's interesting would
be to think about what kind of a person would an artificial, intelligent person be? Would you go to
material person because of the substrate being silicon? Or would you say it's more like a formal
person, like an angel? And I actually even asked an LLM that one time and it said both answers.
Well, again, I think these are incredibly, they're intriguing, but I think they are also very
relevant. You know, I mean, I so I'm a big fan of Ray Kurzweil. And my take on it is that we are
creating either more slowly or rapidly a new form of digital life on earth, you know, and I think
that this is going to be a new component of the human race in some way. And so, you know, this has
so many tremendous and completely unanswered questions that go with it, right? And again,
going from a, you know, going back to a religious perspective, I mean, these beings that were
instantiating, you know, do they have souls? And what kind of rights do they have? And, you know,
how should we regard them? You know, should we look at them as our peers or our children? I mean,
there are so many tremendous aspects of this, you know, and these are things that experts like
yourself are well primed to answer for the rest of us. So well, it's it's really like a dream come
true for me to be involved in all of this, the cutting edge of where technology is headed,
and to be able to think of it from philosophical and religious perspectives. I mean, really,
you couldn't pay me to do this, although I am getting paid. But, you know, that's really the
dream come true for me. So, yeah, there, there are so many different questions regarding how we're
supposed to treat them, you know, right now as parents, if you think of these incipient thinking
machines as somewhat our children or our creations, they're being trained on the best and the worst
of us, right? And that's part of the problem with the LLMs. They're soaking in a lot of prejudice,
bias, hate. I guess I worry about that very much, you know, if there's anything that bothers me
about what LLMs are doing right now, it's that it's getting the best and the worst simultaneously.
And it would, I think it would be better if there was a way to avoid that and not recreate our
if we're creating something in our own image, our image is not doing so well right now.
You know, I, I have a teenage daughter. And one of the things that my mother told me, she's been
telling me this from day one is be careful because they're watching, right? And, and so when you talk
about LLMs picking up the good, the bad and, you know, the ugly, so to speak, I mean, my own child,
right? There, there are some wonderful behaviors that she copies. And then there are other things
that she do. And I'm like, well, no. And then I'm like, where does she get that from? Oh, okay. Yeah.
So, you know, so in, in that context, AI seems like it's our children, right? But, but then what
differentiates it is there's this depth of knowledge and sometimes this spark of insight
that puts it at a peer level and someday may put it beyond a peer level, right?
Right. There's a lot of anthropomorphizing that's going on right now. I mean, because
the output is language. But inside the model, it's just a vast matrix of numbers.
And those numbers are just mapped tokens. So there's an illusion of understanding. But really,
I'm on the side of those who say that currently there isn't, right? But part of the reason that
they can't have that understanding is because context isn't maintained in a good way. And causal
relationships aren't maintained in a good way. Okay. So this is one of the reasons that moving
from a data centric architecture and model for building applications is eventually it's going
to be seen as a very primitive and old fashioned way of doing computing, right? I mean, for a while,
my daughter went to a big data summer camp, you know, before she started college. And
we've come a long way in a few years. Big data is actually
a huge problem. I mean, these GPU clusters that they're going to build are going to
be thirsty, they're going to take a lot of our drinking water, and they're going to
require enormous amounts of energy consumption. Maintaining this data centric structure
takes a lot of human effort. Training these models takes a lot of human feedback. They're
paying people minimum, beyond what I mean, minimum wage here is one thing in the United States, but
these people are being paid very little to look at disgusting, horrific things to try to keep them
out of the model. Like we're saying, we don't want them to be trained on the worst. Well, human beings
are part of that process of trying to filter what goes into the LLM, right, with the human feedback.
So this is all a very cumbersome and expensive way of doing things. And we are going to have
to find a different architecture and a different way. And I think that's what we've done at Decision
Zone. You know, we want the machine to be able to make a decision without having to ask us about
it. We want it to know what we want it to do and decide first and then act. And right now,
the machine is just working on rules and a certain procedure, and it acts. It might let malware into
your computer, right? It might allow some agency to listen in on your phone conversations. Later on,
some poor guy is going to have to sit and pour through those logs and try to find that needle
in the haystack, figure out how did this get in the system? What process led to it being in my data
center, right? But all that context is lost by the time it's in there. You know, a data center is
where data goes to die, basically. And what we're doing with Decision Zone is trying to work on the
front end of everything, work at the input level, and be able to make a decision on a single input
or a single event before it gets into the system in order to determine whether that event is an
anomaly or if it's correct according to the process that you've predefined. So that is, I think,
the holy grail of information technology, to be able to look at an input and determine whether it
should be allowed into the system or not, right? Determine whether it's following the correct
procedure or not. And if that can happen in sub milliseconds, so we're not talking about
human perceived now, but machine speed now, which is very fast, right? Then you have a whole new
world of capabilities that are opened up. And a different level of decision making and activity
that we can have the machine do for us that is currently, really, it's too slow, cumbersome,
expensive, and energy inefficient to do. Well, that was one of the reasons that I was excited
to interview you about what you guys are doing with Decision Zone. So this is kind of off the cuff,
but AI development goes back to the list programming language, I believe, in the 1970s. So it goes way,
way, way back. And today's LLMs, even though they have this heritage of development, are really that
first wave of big programs, you know, that they're, they're changing minds about AI. I mean, people
are moving forward their projections on the singularity and it's, you know, everyone has
really been shocked by how amazing they perform. But they do have these limitations. I read that
Sam Altman is chasing down trillions of investment dollars to mass produce next generation GPU chips
to be able to run these things, right. And so, you know, the, the amount of power that is required
to run these the amount of processing, you know, and then the inherent limitations of that model
as well, seem like that that first wave of companies are starting to be replaced by companies like
Decision Zone, where, and it's not only yourselves, but several others that I've seen where you're
looking at that saying, okay, no more black boxes, reasonable amounts of processing,
you know, we need accountability, we need it to be able to obey the guardrails that we put in place,
and ultimately, you know, serve our needs as human beings and, you know, and just not do
its own thing and then just give us back, you know, hallucinative inputs, right, or outputs.
Right. Well, you mentioned the $7 trillion figure, and that is for chips. But there was another
figure that he mentioned that's also relevant to what we're talking about. Because he was trying at,
I think this came from maybe sometime in the summer, July or August,
it was stated that he wanted $100 billion to solve the agency problem. Right. I mean,
Ilya Sockover and all of them, they admit that these LLMs basically have no agency at this point.
Right. Yeah. And to think that they believe it's going to cost $100 billion to create
these agents, that's mind blowing. I mean, we already have an agent. Right. We solved the
harder problem first, instead of spent, you know, working on the LLM problem. So now how a tighter
coupling between what we have and what the LLMs are doing would work. That's something that
people with a more technical mind than I will have to be working out. But I think it's possible.
Yeah. Well, from what I understand, the agency part of this, in the case of decision zone,
can actually be run on embedded systems, right? That's something that you guys are looking at in
the future. Right. It's a software platform that can go on to a hosted chip. And it would have to
basically have CPU capability and RAM and the messaging bust. So what that architecture
really looks like, I mean, it's been explained to me by our founder, Rajiv Bhargava. But I can't
really explain it to you now. I'm just saying that when you do have it on a small chip, then you can
have a cognitive control locally. And that's what we really need to have. Because the centralized
approach where you're constantly interacting with the cloud or with some data center that's
getting old, it's already getting old. Yeah. Well, let me go into some of the social ramifications.
And again, this circles right back to philosophy and potentially religion as well. But
one of the big concerns, and this is a growing concern, is that chat GPT is seen as a threat to
a number of different types of jobs, right? I mean, chat GPT itself could threaten writers and
programmers mid journey, could threaten artists and illustrators, you know, robotic cars may
replace taxi and bus drivers. We have a growing list of AI enabled applications that may end up
taking human jobs. Do you share this fear that AI will take all the jobs? And do you think there's
any justification for this? Or is this just another form of neoludism? Well, I can speak to that
at one level as someone who has made the effort to try a bunch of different LLMs, see how I could
use them for different things. And honestly, I don't think that where they are now, anyone has
all that much to worry about. I'm talking about today, because, you know, the more you use them,
the more you realize how much BS is in there. Any expert who's trying to solve a problem with an
LLM is going to start seeing problems with the outputs. You know, it's only someone who
has a lower understanding of whatever the subject matter is that can be fooled.
So I think they still need a lot of help on that point. Now, when it comes to jobs like,
you know, the robo taxis, obviously, we know that we've been promised full self driving vehicles
for how many years now, more than a decade, it's still not happening. And part of that is because
they're using the data centric approach, right? Reg, you've told us years ago, it's like not
going to work the way they're doing it. But let's suppose that in the best case scenario,
all of their technology magically did start to work properly, right? I think that we're in a
situation where it has to do with the speed of adoption and the ability of the economy to
accommodate that rate of change. So in the past, the rate of change was slower. And when, you know,
you had to start introducing electric lights instead of, you know, oil lamps or cars instead of
horses. That pace of change happened in a lot more reasonable way. The human being could
accommodate that pace of change. No one would go back, I think, and say, oh, we should all still be,
maybe my husband, he is a Luddite, I'm married to a Luddite. So I know how Luddites think. And he's
like, we should have candles, we should have oil lamps, we should have horses. That's one of the
reasons I spent five years living with Bedouin in the desert in Jordan. Right. So I tried to, I had
my Luddite fantasy for five years. But then I realized, you know, the world can't, I can't just
completely disengage from the world. It's going to keep going on without me. And the responsibility
of all of us is to be wholly engaged with what's happening. So from an ethical point of view,
I couldn't stomach sitting this one out. And I do think it's important for people to really think
about how we're using this technology in a way that is human centric in a way that will actually
make people's lives better. Some jobs are too dangerous. And we don't really want to risk human
life to do them. Right. I mean, we know when when Fukushima happened, they had to send robots in
there, you don't want to send human beings in there into the reactor that's melting down.
You want to have an autonomous little thing to go in and take the pictures and see what's
happening and take the readings. There are a lot of other cases like that. Now, if we start at those
edge cases where things are more difficult for humans to do, and then slowly work out, I think
that we'll be able to hopefully match the pace in a little bit better way. And that's really the
way technology seems to work, right? You start off with the things that are the most expensive
and high tech. Remember, they always say, you know, we have all this technology because NASA
was doing it first, and it then trickles down through the economy to everyone else. And now we
have our cell phones and all of these things, thankfully. So that that's kind of an answer that
tries to beat around the bush a little bit. I'm sorry, I'm not out there screaming that,
oh, that's going to take our jobs. I really think that it's it's moving slowly enough yet
right now that it's not. And most of the people I know who are involved in real work like marketing
and, you know, sales, that they can use GPT to give them some something to go on. But you always
have to improve it. You have to tweak it. You can't take their out the outputs of these things and
just plug them right in. Everybody can tell. I mean, you can really tell when you're reading
something that's been produced by an LLM. Don't you think? Yeah. Yeah, you know, I've actually
I've done marketing content with it. And yeah, I found that it does. And it does great output,
but it is very predictable, right? It's very boilerplate. And so, yeah, I would absolutely
agree with you. And I think in the larger context, what you're describing, it sounds like the
emergence of every new technology, right? And so, you know, I mean, I don't know, I
might so my daughter rides horses in the local fair, you know, and we go and see the horse and
buggies. And I love those, I value them, I take photos of them. They're wonderful, you know,
seeing those. That's that's a part of our tradition, our social customs, and it has deep meaning.
But, you know, seeing it at the fair is enough. It doesn't mean that I need to ride around in
one, right? So, you know, it's it's this constant change in progress. And, you know, and we've seen
this, we saw this with smartphones, and with the internet, and so many other things. And it seems
like that's kind of the direction that you're going is that this will find its niche in society
and society will accommodate it without destroying the rest of us, I guess. At least I hope so.
Well, the other thing I wanted to ask about was whether you think there might be backlash
movements to this. The one that comes to my mind was there were a couple of them, but
you know, genetic engineering, right in the 1970s, I mean, you know, back when the era when I was
born, they were writing books about how genetic engineering was going to change agriculture,
and it would be this blessing for all humankind. And when it came out, and it's not to say that it
was perfect, but when it came out, it inspired a backlash movement that I believe at least partially
led the organic foods movement. And so that is interesting, because a lot of these technologies
have seen that as well, where there's there's pushback that inspires a whole new movement,
I guess, a new new type of industry that is a response to that, right? We have GMO foods now.
Some people are fine with those, but people are also willing to pay more for naturally grown foods.
Do you think that we'll see something along those lines with AI?
Well, I certainly hope so. Because what I would love to see happen is for people to fight back
against centralization. I mean, that's a key part of data X, the first word in the name of the
platform is decentralized. And if we can take back the control over our digital identities,
our footprint, control what goes out and comes into our devices, right? The autonomous agent
is also related to us as human beings, retaining our autonomy, right? So the more we can decentralize,
I think the better. And I would like to see push more pushback instead of people just
lining up for the same mold. I don't want to be too negative about big data and big tech,
but you can see where I'm going with it, right? Yeah, yeah. Well, big data has kind of the
Walmart model, right? It's low, low prices. Yeah, it's, you know, but then to follow that
comparison further, in the 90s, right? Walmart steamrolled through the United States and put
lots and lots of little mom and pops out of business. And I think that's one of the concerns
with things like chat GPT. Now, in terms of decentralized AI, can you tell me a little bit
more about how you're approaching that? Well, the way the development environment works with
data X is that we're creating models, and we're not talking about these large numerical matrices,
we're talking about process or activity models. And those could be workflow models like in a
business or enterprise, it could be models that control a robot, right? So if you are able to
yourself control that model, then you're not relying on third parties to provide them to
maintain them, right? So in a sense, it's almost like no code. I mean, you can just sort of draw
an activity diagram, put some connectors there, right? And it manages that process for you in a
completely holistic and integrated way. It's I guess another way of putting it is to say that
data X provides autonomous integration. And the integration problems with the current complexity
of our systems are reaching post human proportions. There's not any single individual that can manage
that in any company, even, right? But if we have a way of instead of integrating systems and databases,
we're actually now integrating activities and processes, then anyone can do that based on
what your process is. So it really democratizes the control of the application and the capabilities
of the application. Wonderful. Wonderful. Well, Sarah, on that note, we have covered so much
ground today. And I would love to come back and talk more about philosophy and religion and
relationship to AI in the future with you. But I think that we've given people a lot to chew on.
So let me close again, by thanking you so much for your time. And also, I want to give people
the website, it's www.dadda-x.com. I will put links in the show notes to that, encourage people to
check that out. Let me close by asking what is coming up in the first part of the year for
yourself and for the company. And what do you, do you have any prognostications or guesses about
what we might see with AI as well? Well, for the company, we're working on partnerships to make
enterprise autonomous. So that's sort of the exciting piece for a decision zone. And I'm
really looking forward to getting the opportunity to travel a bit more, maybe go back to the Middle
East, this time as a representative of decision zone and not as a pretend escape artist, right?
But as far as where AI is going this year, I think there's a little bit of chill in the air,
you know, people talk about is there going to be an AI winner because all of the expectations we had
are going to start to become disappointments. One of the problems with things being interesting,
Tim, is that they get boring quicker. So the more interesting something is, the easier you get bored
with it. So I'm sort of on the lookout for people getting disaffected. And of course, there's still,
we're still at a certain stage of the hype cycle, but the hype always sort of evens out.
I think what will be interesting is to see how other AI companies try to tackle the problem of
agency. That's what I'm curious about. What are they, what is their method? Because honestly,
I don't think they've got anything on us. But I really look forward to talking to you more about
some of the religious issues, because those are the closest to my heart, actually. And I appreciate
your time and your interest in what we're doing with AGI. Absolutely. Well, Sarah, once again,
let me thank you so much for your time today, ma'am. Thank you very much, Tim.

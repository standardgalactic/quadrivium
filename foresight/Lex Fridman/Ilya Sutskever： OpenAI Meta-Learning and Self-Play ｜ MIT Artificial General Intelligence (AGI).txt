Welcome back to 6SES 099, Artificial General Intelligence.
Today we have Ilya Setskever, co-founder and research director
of OpenAI.
He started in the ML group in Toronto with Geoffrey Hinton,
then at Stanford with Andrew Ang,
co-founded DNN Research for three years
as a research scientist at Google Brain,
and finally co-founded OpenAI.
Citations aren't everything, but they
do indicate impact.
And his work, recent work, in the past five years,
has been cited over 46,000 times.
He has been the key creative intellect and driver
behind some of the biggest breakthrough ideas
in deep learning and artificial intelligence ever.
So please welcome Ilya.
All right, thanks for the introduction, Lex.
All right, thanks for coming to my talk.
I will tell you about some work we've
done over the past year on meta-learning and self-play
at OpenAI.
And before I dive into some of the more technical details
of the work, I want to spend a little bit of time
talking about deep learning and why it works at all
in the first place, which I think
it's actually not a self-evident thing
that they should work.
One fact, it's actually a fact, some mathematical theorem
that you can prove, is that if you could find the shortest
program that does very well on your data,
then you will achieve the best generalization possible.
With a little bit of modification,
you can turn it into a precise theorem.
And on a very intuitive level, it's
easy to see why it should be the case.
If you have some data and you're able to find a shorter
program which generates this data,
then you've essentially extracted all conceivable
regularity from this data into your program.
And then you can use this object to make the best
predictions possible.
If you have data which is so complex
that there is no way to express it as a shorter program,
then it means that your data is totally random.
There is no way to extract any regularity from it whatsoever.
Now, there is little known mathematical theory behind this.
And the proofs of these statements
are actually not even that hard.
But the one minor slight disappointment
is that it's actually not possible, at least given
today's tools and understanding,
to find the best short program that explains or generates
or solves your problem given your data.
This problem is computationally intractable.
The space of all programs is a very nasty space.
Small changes to your program result in massive changes
to the behavior of the program, as it should be.
It makes sense.
You have a loop.
It changed the inside of the loop.
Of course, you get something totally different.
So the space of programs is so hard,
at least given what we know today,
search there seems to be completely off the table.
Well, if we give up on short programs,
what about small circuits?
Well, it turns out that we are lucky.
It turns out that when it comes to small circuits,
you can just find the best small circuit that
solves your problem using back propagation.
And this is the miraculous fact on which the rest of AI stands.
It is the fact that when you have a circuit
and you impose constraints on your circuits,
on your circuit using data, you can find a way
to satisfy these constraints, these constraints
using backprop, by iteratively making small changes
to the weights of your neural network
until its predictions satisfy the data.
What this means is that the computational problem that's
solved by back propagation is extremely profound.
It is circuit search.
Now, we know that you can solve it always,
but you can solve it sometimes.
And you can solve it at those times
where we have a practical data set.
It is easy to design artificial data sets for which you cannot
find the best neural network.
But in practice, that seems to be not a problem.
You can think of training a neural network
as solving a neural equation in many cases
where you have a large number of equation terms
like this, f of x i theta equals y i.
So you've got your parameters, and they represent
all your degrees of freedom.
And you use gradient descent to push the information
from these equations into the parameters to satisfy them all.
And you can see that a neural network, let's say one
with 50 layers, is basically a parallel computer that
is given 50 time steps to run.
And you can do quite a lot with 50 time steps
of a very, very powerful massively parallel computer.
So for example, I think it is not widely known
that you can learn to sort n n bit numbers using
a modestly sized neural network with just two hidden layers,
which is not bad.
It's not self-evident, especially since we've
been taught that sorting requires log n parallel steps.
With a neural network, you can sort successfully
using only two parallel steps.
So there's something slightly obvious going on.
Now these are parallel steps of threshold neurons.
So they're doing a little bit more work.
That's the answer to the mystery.
But if you've got 50 such layers,
you can do quite a bit of logic, quite a bit of reasoning,
all inside the neural network.
And that's why it works.
Given the data, we are able to find the best neural network.
And because the neural network is deep,
because it can run computation inside of its layers,
the best neural network is worth finding.
Because that's really what you need.
You need something.
You need a model class, which is worth optimizing.
But it also needs to be optimizable.
And deep neural networks satisfy both of these constraints.
And this is why everything works.
This is the basis on which everything else resides.
Now I want to talk a little bit about reinforcement learning.
So reinforcement learning is a framework.
It's a framework of evaluating agents in their ability
to achieve goals in complicated stochastic environments.
You've got an agent, which is plugged into an environment,
as shown in the figure right here.
And for any given agent, you can simply run it many times
and compute its average reward.
Now, the thing that's interesting about the reinforcement
learning framework is that there exist interesting,
useful reinforcement learning algorithms.
The framework existed for a long time.
It became interesting once we realized
that good algorithms exist.
Now, these are not perfect algorithms,
but they are good enough to do interesting things.
And all you want, the mathematical problem,
is one where you need to maximize the expected reward.
Now, one important way in which the reinforcement learning
framework is not quite complete is
that it assumes that the reward is given by the environment.
You see this picture.
The agent sends an action, while the reward sends it
an observation, and both the observation
and the reward backwards.
That's what the environment communicates back.
The way in which this is not the case in the real world
is that we figure out what the reward is from the observation.
We reward ourselves.
We are not told.
The environment doesn't say, hey, here's some negative reward.
It's our interpretation of our senses
that lets us determine what the reward is.
And there is only one real true reward in life,
and this is existence or nonexistence.
And everything else is a corollary of that.
So, well, what should the agent be?
You already know the answer.
It should be a neural network.
Because whenever you want to do something,
the answer is going to be a neural network,
and you want the agent to map observations to actions.
So you let it be parametrized with a neural net,
and you apply a learning algorithm.
So I want to explain to you how reinforcement learning works.
This is model-free reinforcement learning.
The reinforcement learning is actually
being used in practice everywhere.
But it's also deeply, it's very robust.
It's very simple.
It's also not very efficient.
So the way it works is the following.
This is literally the one-sentence description
of what happens.
In short, try something new.
Add randomness to your actions.
And compare the result to your expectation.
If the result surprises you, if you
find that the result exceeded your expectation,
then change your parameters to take those actions
in the future.
That's it.
This is the full idea of reinforcement learning.
Try it out.
See if you like it.
And if you do, do more of that in the future.
And that's it.
That's literally it.
This is the core idea.
Now, it turns out it's not difficult to formalize
mathematically.
But this is really what's going on.
If in a neural network, in a regular neural network, like
this, you might say, OK, what's the goal?
You run the neural network.
You get an answer.
You compare it to the desired answer.
And whatever difference you have between those two, you
send it back to change the neural network.
That's supervised learning.
In reinforcement learning, you run a neural network.
You add a bit of randomness to your action.
And then if you like the result, your randomness turns
into the desired target in effect.
So that's it.
Trivial.
Now, math exists.
Without explaining what these equations mean, the point is
not really to derive them, but just to show that they exist.
There are two classes of reinforcement learning
algorithms.
One of them is the policy gradient, where basically
what you do is that you take this expression right there,
the sum of rewards, and you just crunch through the
derivatives, you expand the terms, you do some algebra,
and you get a derivative.
And miraculously, the derivative has exactly the
form that I told you, which is try some actions.
And if you like them, increase the
low probability of the actions.
That literally follows from the math.
It's very nice when the intuitive explanation has a
one-to-one correspondence to what you get in the equation.
Even though you'll have to take my word for it, if you're not
familiar with it, that's the equation at the top.
Then there is a different class of reinforcement learning
algorithms, which is a little bit more difficult to explain.
It's called the Q learning-based algorithms.
They are a bit less stable, a bit more sample efficient.
And it has the property that it can learn not only from the
data generated by the actor, but from any other data as
well.
So it has a different robustness profile, which will be a
little bit important.
But it's only going to be a technicality.
So yeah, this is the on policy, off policy distinction.
But it's a little bit technical.
So if you find this hard to understand, don't worry
about it.
If you already know it, then you already know it.
So now what's the potential for reinforcement learning?
What's the promise?
What is it actually?
Why should we be excited about it?
Now, there are two reasons.
The reinforcement learning algorithms of today are already
useful and interesting.
And especially if you have a really good simulation of
your world, you could train agents to do lots of
interesting things.
But what's really exciting is if you can build a super
amazing sample efficient reinforcement learning
algorithm, which is give it a tiny amount of data, and the
algorithm just crunches through it and extracts every
bit of entropy out of it in order to learn in
the fastest way possible.
Now, today our algorithms are not particularly
data efficient.
They are data inefficient.
But as our field keeps making progress, this will change.
Next, I want to dive into the topic of meta learning.
The goal of meta learning, so meta learning is a beautiful
idea that doesn't really work, but it kind of works.
And it's really promising too.
It's another promising idea.
So what's the dream?
We have some learning algorithms.
Perhaps we could use those learning algorithms in order
to learn to learn.
It would be nice if we could learn to learn.
So how would you do that?
You would take a system which you train it, not on one task,
but on many tasks, and you ask it that it learns to solve
these tasks quickly.
And that may actually be enough.
So here's how it looks like.
Here's how most traditional meta learning
looks like.
You have a model which is a big neural network.
But what you do is that you treat every, instead of
training cases, you have training tasks.
And instead of test cases, you have test tasks.
So your input may be, instead of just your current test
case, it would be all the information about the test
tasks plus the test case.
And you'll try to output the prediction or action for that
test case.
So basically you say, yeah, I'm going to give you your 10
examples as part of your input to your model, figure out how
to make the best use of them.
It's a really straightforward idea.
You turn the neural network into the learning algorithm by
turning a training task into a training case.
So training task equals training case.
This is meta learning.
This one sentence.
And so there have been several success stories which I
think are very interesting.
One of the success stories of meta learning is learning to
recognize characters quickly.
So there have been a data set produced by MIT by Lake et al.
And this is a data set.
We have a large number of different handwritten
characters.
And people have been able to train extremely strong
meta learning system for this task.
Another very successful example of meta learning is that of
neural architecture search by Zopen Lee from Google, where
they found a neural architecture that solved one
problem well, a small problem.
And then it could generalize, and then it would successfully
solve large problems as well.
So this is kind of the small number of bits meta learning.
It's like when you learn the architecture, or maybe even
learn a program, a small program, or a learning
algorithm, it should apply to new tasks.
So this is the other way of doing meta learning.
So anyway, but the point is what's really happening in
meta learning in most cases is that you turn a training
task into a training case and pretend that this is totally
normal, normal deep learning.
That's it.
This is the entirety of meta learning.
Everything else suggests minor details.
Next, I want to dive in.
So now that I've finished the introduction section, I want to
start discussing different work by different people from
OpenAI.
And I want to start by talking about hindsight
experience replay.
There's been a large effort by Andriy Khopich et al. to
develop a learning algorithm for reinforcement learning that
doesn't solve just one task, but it solves many tasks.
And it learns to make use of its experience in a much
more efficient way.
And I want to discuss one problem in reinforcement
learning.
It's actually, I guess, a set of problems which are related to
each other.
But one really important thing you need to learn to do is to
explore.
You start out in an environment you don't know what to do.
What do you do?
So one very important thing that has to happen is that you
must get rewards from time to time.
If you try something and you don't get rewards, then how
can you learn?
So that's the kind of the crux of the problem.
How do you learn?
And relatedly, is there any way to meaningfully benefit from
the experience, from your attempts, from your failures?
If you try to achieve a goal and you fail, can you still
learn from it?
Instead of asking your algorithm to achieve a single
goal, you want to learn a policy that can achieve a very
large family of goals.
For example, instead of reaching one state, you want to
learn a policy that reaches every state of your system.
Now, what's the implication?
Anytime you do something, you achieve some state.
So let's suppose you say, I want to achieve state A. I
try my best and I end up achieving state B. I can
either conclude, while I was disappointing, I haven't learned
almost anything.
I still have no idea how to achieve state A. But
alternatively, I can say, well, wait a second, I've just
reached a perfectly good state, which is B. Can I learn
how to achieve state B from my attempt to achieve state A?
And the answer is yes, you can.
And it just works.
And I just want to point out, this is the one case.
There's a small subtlety here, which may be interesting to
those of you who are very familiar with on-pot, be the
distinction between on-policy and off-policy.
When you try to achieve A, you're doing on-policy learning
for reaching the state A. But you're doing off-policy
learning for reaching the state B. Because you would take
different actions if you would actually try to reach state B.
So that's why it's very important that the algorithm
you use here can support off-policy learning.
But that's a minor technicality.
At the crux of the idea is you make the problem easier by
ostensibly making it harder.
By training a system which aspires to reach, to learn to
reach every state, to learn to achieve every goal, to learn
to master its environment in general, you build a system
which always learns something.
It learns from success as well as from failure.
Because if it tries to do one thing and it does something
else, it now has training data for how to achieve that
something else.
I want to show you a video of how this thing
works in practice.
So one challenge in reinforcement learning systems is
the need to shape the reward.
So what does it mean?
It means that at the beginning of the system, at the start
of learning when the system doesn't know much, it will
probably not achieve your goal.
And so it's important that you design your reward function
to give it gradual increments, to make it
smooth and continuous so that even when the system is not
very good, it achieves the goal.
Now, if you give your system a very sparse reward, where the
reward is achieved only when you reach a final state, then it
becomes very hard for normal reinforcement learning
algorithms to solve a problem.
Because naturally, you never get the reward, so you never
learn.
No reward means no learning.
But here, because you learn from failure as well as from
success, this problem simply doesn't occur.
And so this is nice.
I think let's look at the videos a little bit more.
It's nice how it confidently and energetically moves the
little green pack to its target.
And here's another one.
OK, so we can skip the physics.
It works if you do it on a physical robot as well, but we
can skip it.
So I think the point is that the hindsight experience replay
algorithm is directionally correct because you want to
make use of all your data and not only a small fraction of
it.
Now, one huge question is, where do you get the high level
states?
Where do the high level states come from?
Because in the work that I've shown you so far, the system
is asked to achieve low level states.
So I think one thing that will become very important for
these kind of approaches is representation learning and
unsupervised learning.
Figure out what are the right states, what's the state
space of goals that's worth achieving?
Now I want to go through some real meta-learning results.
And I'll show you a very simple way of doing seem-to-real
from simulation to the physical robot with meta-learning.
And this is work by Pangadal, was a really nice intern
project in 2017.
So I think we can agree that in the domain of robotics, it
would be nice if you could train your policy in simulation
and then somehow this knowledge would carry over to the
physical robot.
Now, we can build simulators that are OK, but they can never
perfectly match the real world unless you want to have an
insanely slow simulator.
And the reason for that is that it turns out that simulating
contacts is super hard.
And I heard somewhere, correct me if I'm wrong, that
simulating friction is NP-complete.
I'm not sure.
But it's like stuff like that.
So your simulation is just not going to match reality.
There'll be some resemblance, but that's it.
How can we address this problem?
And I want to show you one simple idea.
So let's say one thing that would be nice is that if you
could learn a policy that would quickly adapt itself to the
real world, well, if you want to learn a policy that can
quickly adapt, we need to make sure that it has opportunities
to adapt during training time.
So what do we do?
Instead of solving a problem in just one simulator, we
add a huge amount of variability to the simulator.
We say we will randomize the friction.
So we'll randomize the masses, the length of the different
objects and their dimensions.
So you try to randomize physics, the simulator, in
lots of different ways.
And then importantly, you don't tell the policy how you
randomized it.
So what is it going to do then?
You take your policy and you put it in an environment and
it says, well, this is really tough.
I don't know what the masses are and I don't know what the
frictions are.
I need to try things out and figure out what the friction
is as I get responses from the environment.
So you learn a certain degree of adaptability into the policy.
And it actually works.
I just want to show you.
This is what happens when you just train a policy in
simulation and deploy it on the physical robot.
And here the guy who's going to do that, he's going to do
the goal is to bring the hockey puck towards the red dot.
And you will see that it will struggle.
And the reason it struggles is because of the systematic
differences between the simulator and the real
physical robot.
So even the basic movement is difficult for the policy
because the assumptions are violated so much.
So if you do the training as I discussed, we train a
recurrent neural network policy which learns to quickly
infer properties of the simulator in order to
accomplish the task.
You can then give it the real thing, the real physics, and
it will do much better.
So now this is not a perfect technique, but it's
definitely very promising.
It's promising whenever you are able to sufficiently
randomize the simulator.
So it's definitely very nice to see the closed loop nature
of the policy.
You can see that it would push the hockey puck, and it would
correct it very, very gently to bring it to the goal.
Yeah, you saw that?
That was cool.
So that was a cool application of meta-learning.
I want to discuss one more application of meta-learning,
which is learning a hierarchy of actions.
And this was work done by France Aral.
Actually, Kevin France, the ancient who did it, was in
high school when he wrote this paper.
So one thing that would be nice is if reinforcement
learning was hierarchical.
If instead of simply taking microactions, you had some
kind of little subroutines that you could deploy.
Maybe the term subroutine is a little bit too crude, but if
you had some idea of which action primitives are
worth starting with.
Now, no one has been able to get actually real value add
from hierarchical reinforcement learning yet.
So far, all the really cool results, all the really
convincing results of reinforcement learning, do not
use it.
That's because we haven't quite figured out what's the
right way for hierarchical reinforcement learning.
And I just want to show you one very simple approach where
you use meta-learning to learn a hierarchy of actions.
So here's what you do.
You have, in this specific work, you have a certain number
of low-level primitives.
Let's say you have 10 of them.
And you have a distribution of tasks.
And your goal is to learn low-level primitives such that
when they're used inside a very brief run of some reinforcement
learning algorithm, you will make as much progress as
possible.
So the idea is you want to get the greatest amount of
progress.
You want to learn primitives that result in the greatest
amount of progress possible when used inside learning.
So this is a meta-learning setup because you need
distribution of tasks.
And here, we've had a little maze.
You have a distribution of amazes.
And in this case, the little bug learned three policies which
move it in a fixed direction.
And as a result of having this hierarchy, you're able to solve
problems really fast.
But only when the hierarchy is correct.
So hierarchical reinforcement learning is still
working progress.
And this work is an interesting proof point of how
hierarchical reinforcement learning could be like if it
worked.
Now, I want to just spend one slide addressing the
limitations of high-capacity meta-learning.
The specific limitation is that the training task
distribution has to be equal to the test
task distribution.
And I think this is a real limitation because in reality,
the new task that you want to learn will in some ways be
fundamentally different from anything you've seen so far.
So for example, if you go to school, you learn lots of
useful things.
But then when you go to work, only a fraction of the things
that you've learned carries over.
You need to learn quite a few more things from scratch.
So meta-learning would struggle with that because it
really assumes that the training data, the distribution
over the training tasks has to be equal to the distribution
over the test tasks.
So that's a limitation.
I think that as we develop better algorithms for being
robust when the test tasks are outside of the distribution
of the training task, then meta-learning
would work much better.
Now, I want to talk about self-play.
I think self-play is a very cool topic that's starting to
get attention only now.
And I want to start by reviewing very old work
called TD Gammon.
It's back from all the way from 1992, so it's 26 years old
now.
It was done by Jared Sauron.
So this work is really incredible because it has so much
relevance today.
What they did, basically, they said, OK, let's take two
neural networks and let them play against each other, let
them play backgammon against each other, and let them be
trained with Q-Learning.
So it's a super modern approach.
And you would think this was a paper from 2017, except that
when you look at this plot, it shows that you only have 10
hidden units, 20 hidden units, 40 and 80 for the
different colors, where you notice that the largest
neural network works best.
So in some ways, not much has changed, and this is the
evidence.
And in fact, they were able to beat the world champion in
backgammon, and they were able to discover new strategies
that the best human backgammon players have not noticed.
And they've determined that the strategy is covered by TD
Gammon are actually better.
So that's pure self-play with Q-Learning, which remained
dormant until the DQN worked with Atari, but in mind.
So now other examples of self-play include AlphaGo Zero,
which was able to learn to beat the world champion in Go
without using any external data whatsoever.
And now the result of this way is Biopen AI, which is our
Dota 2 bot, which was able to build the world champion on
the 1v1 version of the game.
And so I want to spend a little bit of time talking about the
allure of self-play and why I think it's exciting.
So one important problem that we must face as we try to build
truly intelligent systems is what is the task?
What are we actually doing?
And one very attractive attribute of self-play is that
the agents create the environment.
By virtue of the agent acting in the environment,
the environment becomes difficult for the other agents.
And you can see here an example of an iguana interacting
with snakes that try to eat it.
And that's why we're trying to build the world champion
of the iguana interacting with snakes that try to eat it
unsuccessfully this time.
So we can see what will happen in a moment.
The iguana is trying its best.
And so the fact that you have this arms race between the
snakes and the iguana motivates their development,
potentially without bound.
And this is what happened in effect in biological evolution.
Now, interesting work in this direction was done in 1994
by Carl Simms.
There is a really cool video on YouTube by Carl Simms.
You should check it out, which really kind of shows all the
work that he's done.
And here you have a little competition between agents
where you evolve both the behavior and the morphology
when the agent is trying to gain possession of a green cube.
And so you can see that the agents create the challenge
for each other.
And that's why they need to develop.
So one thing that we did, and this is work by Ansel et al
from OpenAI, is we said, okay, well,
can we demonstrate some unusual results in self play
that would really convince us that there is something there?
So what we did here is that we created a small ring
and you have these two humanoid figures.
And their goal is just to push each other outside the ring.
And they don't know anything about wrestling.
They don't know anything about standing your balance
in each other.
They don't know anything about centers of gravity.
All they know is that if you don't do a good job,
then your competition is going to do a better job.
Now, one of the really attractive things about self play
is that you always have an opponent that's roughly
as good as you are.
In order to learn, you need to sometimes win
and sometimes lose.
You can't always win.
Sometimes you must fail.
Sometimes you must succeed.
So let's see what will happen here.
Yeah, so it was able to be.
So the green humanoid was able to block the ball.
In a well-balanced self play environment,
the competition is always level.
No matter how good you are or how bad you are,
you have a competition that makes it exactly
of exactly the right challenge for you.
Oh, and one thing here.
So this video shows transfer learning.
You take the little wrestling humanoid
and you take its friend away
and you start applying big, large, random forces on it.
And you see if it can maintain its balance.
And the answer turns out to be that yes, it can
because it's been trained against an opponent
and it pushes it.
And so that's why even if it doesn't understand
where the pressure force is being applied on it,
it's still able to balance itself.
So this is one potentially attractive feature
of self play environments that you could learn
a certain broad set of skills.
Although it's a little hard to control those
what the skills will be.
And so the biggest open question with this research
is how do you learn agents in a self play environment
such that they do whatever they do,
but then they are able to solve a battery of tasks
that is useful for us,
that is explicitly specified externally.
Yeah.
I also want to highlight one attribute
of self play environments
that we've observed in our Dota bot.
And that is that we've seen a very rapid increase
in the competence of the bot.
So over the period, over the course of maybe five months,
we've seen the bot go from playing totally randomly
all the way to the world champion.
And the reason for that
is that once you have a self play environment,
if you put compute into it, you turn it into data.
Self play allows you to turn compute into data.
And I think we will see a lot more of that
as being an extremely important thing
to be able to turn compute
into essentially data or generalization.
Simply because the speed of neural net processors
will increase very dramatically over the next few years.
So neural net cycles will be cheap
and it will be important to make use of these
newly found over abundance of cycles.
I also want to talk a little bit
about the end game of the self play approach.
So one thing that we know about the human brain
is that it has increased in size fairly rapidly
over the past two million years.
My theory, the reason I think it happened
is because our ancestors got to a point
where the thing that's most important for your survival
is your standing in the tribe
and less the tiger in the lion.
Once the most important thing
is how you deal with those other things
which have a large brain,
then it really helps to have a slightly larger brain.
And I think that's what happened.
And there exists at least one paper from science
which supports this point of view.
So apparently there has been convergent evolution
between social apes and social birds
even though in terms of various behaviors.
Even though the divergence in evolution
and timescale between humans and birds
has occurred a very long time ago
and humans and sorry humans apes and humans apes
and birds have very different brain structure.
So I think what should happen if we succeed,
if we successfully follow the path of this approach
is that we should create a society of agents
which will have language and theory of mind,
negotiation, social skills, trade, economy, politics,
justice system.
All these things should happen
inside the multi-agent environment.
And there will also be some alignment issue
of how do you make sure that the agents we learn
behave in a way that we want.
Now I want to make a speculative digression here
which is, I want to make the following observation.
If you believe that this kind of society of agents
is a plausible place where truly,
where fully general intelligence will emerge.
And if you accept that our experience with the dotabot
where we've seen a very rapid increase in competence
will carry over once all the details are right.
If you assume both of these conditions
then it should follow that we should see a very rapid increase
in the competence of our agents
as they leave in the society of agents.
So now that we've talked about a potentially interesting way
of increasing the competence and teaching agents
social skills and language and a lot of things
that actually exist in humans as well.
We want to talk a little bit about how you convey goals
to agents.
And the question of conveying goals to agents
is just a technical problem.
But it will be important
because it is more likely than not
that the agents that we will train will eventually
be dramatically smarter than us.
And this is work by the OpenAI safety team
by Paul Cruciano and others.
So I'm just going to show you this video
which basically explains how the whole thing works.
There is some behavior you are looking for
and you the human gets to see pairs of behaviors.
And you simply click on the one that looks better.
And after a very modest number of clicks
you can get this little simulated leg to do backflips.
There you go.
You can now do backflips.
And to get this specific behavior
it took about 500 clicks by human annotators.
The way it works is that you take all the...
So this is a very data efficient reinforcement learning
algorithm but it is efficient in terms of rewards
and not in terms of the environment interactions.
So what you do here is that you take all the clicks
so you've got your...
Here is one behavior which is better than the other.
You fit a reward function, a numerical reward function
to those clicks.
So you want to fit a reward function
which satisfies those clicks
and then you optimize this reward function
with reinforcement learning.
And it actually works.
So this requires 500 bits of information.
You've also been able to train lots of Atari games
using several thousand bits of information.
So in all these cases you had human annotators
or human judges just like in the previous slide
looking at pairs of trajectories
and clicking on the one that they thought was better.
And here's an example of an unusual goal
where this is a car racing game
but the goal was to ask the agent to train the white car
drive right behind the orange car.
So it's a different goal
and it was very straightforward to communicate this goal
using this approach.
So then to finish off, alignment is a technical problem.
It has to be solved.
But of course the determination of the correct goals
we want RA systems to have
will be a very challenging political problem.
And on this note,
I want to thank you so much for your attention
and I just want to say that it will be a happy hour
at Cambridge Brewing Company at 8.45.
If you want to chat more about AI and other topics,
please come by.
I think that deserves an applause.
Thank you very much.
So back propagation is a...
or neural networks are inspired
but back propagation doesn't look as though
it's what's going on in the brain
because signals in the brain go one direction down the axons
whereas back propagation requires the errors
to be propagated back up the wires.
So can you just talk a little bit
about that whole situation
where it looks as though the brain is doing something a bit different
than our highly successful algorithms?
Are algorithms going to be improved
once we figure out what the brain is doing
or is the brain really sending signals back
even though it's got no obvious way of doing that?
What's happening in that area?
So that's a great question.
So first of all, I'll say that the true answer
is that the honest answer is that I don't know
but I have opinions.
And so I'll say two things.
Like first of all, given that...
like if you agree, if we agree like so rather
it is a true fact that back propagation
solves the problem of circuit search.
This problem feels like an extremely fundamental problem
and for this reason I think that it's unlikely to go away.
Now you also write that the brain doesn't obviously
do back propagation although there have been multiple proposals
of how it could be doing them.
For example, there's been a work by Tim Lillicrap and others
where they've shown that if you use...
that it's possible to learn a different set of connections
that can be used for the backward pass
and that can result in successful learning.
Now, the reason this hasn't been like really pushed to the limit
by practitioners is because they say,
well, I got tf.gradients,
I'm just not going to worry about it.
But you are right that this is an important issue
and one of two things is going to happen.
So my personal opinion is that back propagation
is just going to stay with us till the very end
and we'll actually build fully human level and beyond systems
before we understand how the brain does what it does.
So that's what I believe but of course
it is a difference that has to be acknowledged.
Okay, thank you.
Do you think it was a fair matchup for the Dota bot
and that person given the constraints of the system?
So I'd say that the biggest advantage computers have in games like this,
like one of the big advantages is that they obviously have a better reaction time.
Although in Dota in particular,
the number of clicks per second over the top layer is fairly small,
which is different from StarCraft.
So in StarCraft, StarCraft is a very mechanically heavy game
because you have a large number of units
and so the top layers they just click all the time.
In Dota, every player control is just one hero
and so that greatly reduces the total number of actions they need to make.
Now still precision matters.
I think that we'll discover that,
but what I think will really happen is that we'll discover that computers have
the advantage in any domain or rather every domain.
Not yet.
So do you think that the emergent behaviors from the agent
were actually kind of directed
because the constraints were already kind of in place?
Like so it was kind of forced to discover those
or do you think that like that was actually something quite novel
that like, wow, it actually discovered these on its own?
Like you didn't actually bias towards constraining it?
So it's definitely de-discovering new strategies
and I can share an anecdote where our tester,
we have a pro which would test the bot
and he played against it for a long time
and the bot would do all kinds of things against the player,
the human player, which were effective.
Then at some point that pro decided to play against the better pro
and he decided to imitate one of the things that the bot was doing
and by imitating it he was able to defeat a better pro.
So I think the strategies that he discovers are real
and so like it means that there's very real transfer between, you know,
I would say I think what that means is that the,
because the strategies discovered by the bot help the humans,
it means that the fundamental gameplay is deeply related.
For a long time now I've heard that the objective of reinforcement learning
is to determine a policy that chooses an action
to maximize the expected reward, which is what you said earlier.
Would you ever want to look at the standard deviation of possible rewards?
Does that even make sense?
Yeah, I mean I think for sure, I think it's really application dependent.
One of the reasons to maximize the expected reward
is because it's easier to design algorithms for it.
So you write down this equation, the formula,
you do a little bit of derivation,
you get something which amounts to a nice-looking algorithm.
Now, I think there exists like really,
there exist applications where you never want to make mistakes
and you want to work on the standard deviation as well.
But in practice it seems that the, just looking at the expected reward
covers a large fraction of the situation as you'd like to apply this to.
Okay, thanks.
Thank you.
We talked last week about motivations and that has a lot to do with the reinforcement.
And some of the ideas is that the, our motivations are actually connection
with others and cooperation.
And I'm wondering if, Thurnoff, and I understand it's very popular
to have the computers play these competitive games.
But is there any use in like having an agent self-play collaboratively, collaborative games?
Yeah, I think that's an extremely good question.
I think one place from which we can get some inspiration
is from the evolution of cooperation.
Like, I think cooperation, we cooperate ultimately
because it's much better for you, the person to be cooperative than not.
And so, I think what should happen, if you have a sufficiently open-ended game,
then cooperation will be the winning strategy.
And so I think we will get cooperation, whether we like it or not.
Hey, you mentioned the complexity of the simulation of friction.
I was wondering if you feel that there exists open complexity,
theoretic problems relevant to, relevant to AI,
or whether it's just a matter of finding good approximations
that humans, of the types of problems that humans tend to solve.
Yeah, so complexity theory.
Well, like at a very basic level,
we know that whatever algorithm we're going to run
is going to run fairly efficiently on some hardware.
So that puts a pretty strict upper bound on the true complexity of the problems we are solving.
Like, by definition, we are solving problems which aren't too hard in a complexly theoretic sense.
Now, it is also the case that many of the problems,
so while the overall thing that we do is not hard from a complexity-theoretic sense,
and indeed, humans cannot solve NP-complete problems in general.
It is true that many of the optimization problems that we pose to our algorithms
are intractable in the general case, starting from neural net optimization itself.
It is easy to create a family of datasets for a neural network with a very small number of neurons,
such that finding the global optimum is NP-complete.
And so, how do we avoid it?
Well, we just try gradient descent anyway, and somehow it works.
But without question, we do not solve problems which are truly intractable.
So, I mean, I hope this answers the question.
Hello. It seems like an important sub-problem on the path towards AGI will be understanding language,
and the state of generative language modeling right now is pretty abysmal.
What do you think are the most productive research trajectories towards generative language models?
So, I'll first say that you are completely correct that the situation with language is still far from great,
although progress has been made, even without any particular innovations beyond models that exist today.
Simply scaling up models that exist today on larger datasets is going to go surprisingly far,
not even larger datasets, but larger and deeper models.
For example, if you trained a language model with a thousand layers, and it's the same layer,
I think it's going to be a pretty amazing language model.
Like, we don't have the cycles for it yet, but I think it will change very soon.
Now, I also agree with you that there are some fundamental things missing in our current understanding of deep learning,
which prevent us from really solving the problem that we want.
So, I think one of these problems, one of the things that's missing is that, or that seems like patently wrong,
is the fact that we train a model, then we stop training the model, and we freeze it,
even though it's the training process where the magic really happens.
Like, the magic is like, if you think about it, like, the training process is the true general part of the whole story,
because your TensorFlow code doesn't care which dataset to optimize.
It just says, whatever, just give me the dataset, I don't care which one to solve, I'll solve them all.
So, like, the ability to do that feels really special, and I think we are not using it at test time.
Like, it's hard to speculate about, like, things of which we don't know the answer,
but all I'll say is that simply train bigger, deeper language models will go surprisingly far scaling up,
but also doing things like training at test time and inference at test time,
I think would be another important boost to performance.
Hi, thank you for the talk.
So, it seems like right now another interesting approach to solving reinforcement learning problems
could be to go for the evolutionary routes, using evolutionary strategies.
And although they have their caveats, I wanted to know if, at OpenAI in particular,
you're working on something related, and what is your general opinion on them?
So, like, at present, I believe that something like evolutionary strategies is not great for reinforcement learning.
I think that normal reinforcement learning algorithms, especially with big policies, are better.
But I think if you want to evolve a small compact object, like a piece of code, for example,
I think that would be a place where this would be seriously worth considering.
But this, you know, evolving a useful piece of code is a cool idea.
It hasn't been done yet, so still a lot of work to be done before we get there.
Hi, thank you so much for coming.
My question is, you mentioned what is the right goal is a political problem.
So, I'm wondering if you can elaborate a bit on that,
and also what do you think would be their approach for us to maybe get there?
Well, I can't really comment too much, because all the thoughts that, you know,
we now have a few people who are thinking about this full time at OpenAI,
I don't have enough of a super strong opinion to say anything too definitive.
All I can say at a very high level is given the size, like, if you go into the future,
whenever soon or later, you know, whenever it's going to happen when you build a computer
which can do anything better than a human.
It will happen, because the brain is physical.
The impact on society is going to be completely massive and overwhelming.
It's very difficult to imagine, even if you try really hard.
And I think what it means is that people will care a lot.
And that's what I was alluding to, the fact that this will be something
that many people will care about strongly.
And, like, as the impact increases, gradually we sell driving cars, more automation,
I think we will see a lot more people care.
Do we need to have a very accurate model of the physical world,
and then simulate that in order to have these agents that can eventually come out into the real world
and do something approaching, you know, human level intelligence tasks?
That's a very good question.
So I think if that were the case, we'd be in trouble.
And I am very certain that it could be avoided.
So specifically, the real answer has to be that, look, you learn to problem solve,
you learn to negotiate, you learn to persist,
you learn lots of different useful life lessons in the simulation.
And yes, you learn some physics, too.
But then you go outside of the real world, and you have to start over to some extent,
because many of your deeply held assumptions will be false.
And one of the goals, so that's one reason I care so much about never stopping training.
You've accumulated your knowledge, now you go into an environment where some of your assumptions are violated,
you continue training, you try to connect the new data to your old data.
And this is an important requirement from our algorithms,
which is already met to some extent, but it will have to be met a lot more,
so that you can take the partial knowledge that you've acquired
and go in a new situation, learn some more.
Literally the example of you go to school, you learn useful things, then you go to work.
It's not a perfect, it's not, you know, for your four years of CS and undergrad,
it's not going to fully prepare you for whatever it is you need to know at work.
It will help somewhat, you'll be able to get off the ground,
but there will be lots of new things you need to learn.
So that's the spirit of it.
Think of it as a school.
One of the things you mentioned pretty early on in your talk is that one of the limitations
of this sort of style of reinforcement learning is there's no self-organization.
So you have to tell it, went into a good thing or did a bad thing.
And that's actually a problem in neuroscience,
is when you're trying to teach a rat to, you know, navigate a maze,
you have to artificially tell it what to do.
So where do you see moving forward when we already have this problem with teaching,
you know, not necessarily learning, but also teaching.
So where do you see the research moving forward in that respect?
How do you sort of introduce this notion of self-organization?
So I think without question, one really important thing you need to do
is to be able to infer the goals and strategies of other agents by observing them.
That's a fundamental skill you need to be able to learn, to embed into the agents.
So that, for example, you have two agents, one of them is doing something,
and the other agent says, well, that's really cool, I want to be able to do that too.
And then you go on and do that.
And so I'd say that this is a very important component in terms of setting the reward.
You see what they do, you infer the reward,
and now we have a knob which says, you see what they're doing?
Now go and try to do the same thing.
So I'd say this is, as far as I know, this was one of the important ways
in which humans are quite different from other animals
in the way in the scale and scope in which we copy the behavior of other humans.
You might have asked a quick follow-up.
Go for it.
So that's kind of obvious how that works in the scope of competition,
but what about just sort of arbitrary tasks?
Like, I'm in a math class with someone and I see someone doing a problem a particular way
and I'm like, oh, that's a good strategy, maybe I should try that out.
How does that work in a sort of non-competitive environment?
So I think that this will be, I think that's going to be a little bit separate
from the competitive environment, but it will have to be somehow either,
probably baked in, maybe evolved into the system where like,
if you have other agents doing things, they're generating data which you observe
and the only way to truly make sense of the data that you see
is to infer the goal of the agent, the strategy, their belief state.
That's important also for communicating with them.
If you want to successfully communicate with someone,
you have to keep track both of their goal and of their belief state instead of knowledge.
So I think you will find that there are many, I guess, connections
between understanding what other agents are doing, inferring their goals,
imitating them and successfully communicating them.
Alright, let's give Ilya and the happy hour a big hand.
Thank you.

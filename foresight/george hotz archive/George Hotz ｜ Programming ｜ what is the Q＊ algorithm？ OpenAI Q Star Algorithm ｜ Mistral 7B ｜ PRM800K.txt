Good morning everybody. Good morning. Good morning. It's nice to be back in my house.
I don't like streaming at work.
High ball energy.
Good morning everybody. Good morning. Good morning. Good morning. Good morning.
Let's shrink me down here... me up there. Oops, don't do that.
Okay, are we ready to get started?
Yeah, I felt the vibes were a little bit off
in yesterday's stream.
I don't think I should stream at the office.
Oh, so I didn't like the way that like,
like now this is here,
I'm gonna slide that down a little,
about in the middle, that's pretty nice.
I got my Yeti microphone.
Move it over a little bit.
Yeti puts me perfectly in the center.
Yeah, vibes are off.
Wouldn't have my lighting set off.
I mean, it's an okay stream.
It's good to focus on TinyGrad.
But some guy said that he liked my older work better.
And if there's one thing I never want to be, it's drink.
So, we're gonna try to,
we're gonna try to bring it back to the old stuff.
And we're gonna do,
we're gonna investigate the Q-star algorithm.
So first off, what is the Q-star algorithm?
I don't know, let's find out.
We'll start using our friend here, Google.
What is Q-star and when will we hear more?
Someone who's done a fair amount of ML research.
I can tell you, it's very, very easy to think
you've discovered a breakthrough.
That's true.
All right, let's read this article in The Verge.
Reuters, okay, well the government, no, no, no, no, no, no.
Okay.
Q-star could be a breakthrough in the startup search
for what's known as artificial general intelligence.
Reuters could not,
could not be a breakthrough in the startup search for what's
known as artificial general intelligence.
Okay, it somehow does math.
Does something have to publish papers?
Not really.
Wow.
Remember when they used to publish papers
and now they publish system cards and,
safety and alignment?
Okay, wait, wait, wait, wait, wait, wait, wait,
wait, wait, wait, wait, wait, wait, wait, wait, wait, wait,
wait, there's some hope here.
Improving mathematical reasoning.
Ooh, ooh, ooh, this sounds like Q-star guys.
I think we found it.
No, we don't, no, no, but really, wait, is this not,
is this not Q-star?
Did we not just find it?
We're trying not to hate.
We're trying not to hate guys.
Solitate in the world. We got to bring law of impositivity and shit man. Whatever like John Lennon said, you know something
That's why we're gonna go subscribers only
Okay
Given vast computing resources the new model was able to solve certain mathematical problems
Improving mathematical reasoning with process supervision
We've trained a model to achieve a new state of the art in mathematical problem-solving by rewarding each correct step of reasoning instead of
Simply rewarding the final answer
And just
Yeah, yeah
Okay, so
The reason that you usually just reward the correct final answer is because that's all that's in your
Data set the other problem with doing this is it may constrain the reasoning to follow one certain path when that may not be the path
You have to follow
State of the art models still produce logical mistakes often called hallucinations
Cutting out we have more technical problems
I bought a new computer. Does new Apple stuff not work only you okay Rick and Morty you have a crappy internet
An internet connection that is less good than other internet connections
You
Wait, just opening I just published like fake papers now like this
Don't do a shit about alignment
and
Wait, this is a paper. Oh, you know here does a paper. Let's verify step by step
You
Okay, well, is this a public data set
All right, all right, all right, cool
You
Math word problem-solving on math
Who made this data set
Okay, okay
This is not where I was expecting this to go by the way. Oh, they have metamath. Oh everything comes full circle
No, I mean again, let's okay, let's find the primary source here
Q star
I
Definitely talked about math problems
Only performing math on the level of grade school students
Why is this monitor broken this monitor is like broken I can do my
You guys aren't seeing that glitching right
Are you maybe you are I don't think you are
This is so weird
When I click here, there's like a blue glitching on my computer
Weirdest thing like how does that even happen I
Could take the camera down and show you you want to see
There's like blue glitching on my monitor. Look at this click here goes away
Here glitching here goes away glitching
How's this happening they've never seen that before
Sit
Ghosted image
Maybe it's the cable I don't know right
Who knows it only happens when things are uncertain
Wait, this is just really
Who knows
Oh
Wait, I wanted to confirm that this actually was related to math
Delstick cable breaks I'm using like the USB-C cable it could be the cable
But like it's weird how it's glitching
I don't know
The new model was able to solve certain mathematical problems, so it kind of makes sense that this is the data set right
Do they mention the math data here
Our process supervised model solves 78% of the problems from a representative subset of the math test set
We also release PRM 800 K
Okay, opening I release something look at that
No positive positive
I
Drive to VS codes
Let's also look into what their other github activities been
I
Procedurally generated game like Jim environments
What did they update in GPT to a branch they updated
I
Quantifying transfer and reinforcement learning
Okay, it seems like Carl Cobb has been
If anyone invented Q star, it's Carl Cobb
Carl Cobb
All right, you've been working on this math stuff for a while. Let's see what data set was used here. Oh
Is this the data set that introduced that
Oh
This is a different one gms 8k PRM 800 K
Step level correctness
The glitching is really bad
You
Okay, wait, wait, let's talk about the glitching for a minute somehow you can see what's here
Right and it's this it's the same thing that's in this terminal window
But yet if I minimize this terminal window, it's still here. Oh
We got it we got a good resource we got a video. Let's see getting Rick rolled here
Your subscriber you wouldn't do that
I
As you might expect I have been researching non-stop. Wow
Wow glad that we got over researchers on board guys Q star and on
No, this is this is really terrible
Hang on
I'm gonna shut that off for a minute
You guys can look at me while I try to fix
You
Get to the monitor come on
Is the stream working again
I
Lost internet too. Oh, I lost everything when I turned the monitor off
Wait, is this still working
It's working okay, I don't know
The glitching is really bad. It's usually not like I've seen this before but it's gotten way worse
No, it didn't help
Wait, alright, so they're looking at the same paper. I am well computing power while you're taking
All right, let's just read the paper. I need to watch some YouTube or read the paper
Oh
The optimal Q-files ability and he said one link to the name Q star could be in a generic sense
Generator the model coming up with solutions with reinforcement learning. We do not just which paper is that paper using test time computer
You
Okay
You know what I think maybe we'll do one of the bounties and tiny grad I
Think we're gonna first need a chat model. I think regardless of what we're doing. We're gonna need a chat model
I was playing with trying to make embedding fast last night
Connecting process supervision to 30x model size
We got a whole army here. You guys can watch this video. So I don't have to
I did not realize how broken this monitor was
All right, so first we're gonna need a good model. What's what's the best model we got?
What's the best the best 7b model we got? Is it Intel neural chat?
Is this one nerfed to hell do you think
Open Hermes 2.5 so people like
I'm locking the bounty for myself too, so I'm gonna go do that
You
So we're gonna submit a pull request, but it was literally just like the words why you even submit a pull request
Okay, is this what we like
Wait why this one doesn't seem as good
Now sir me seems better
That's 70 be but when I would I not use collective cognition. Oh
But you can't see my desktop
What
I'm having technical difficulties
Oh, I think we also might have a bit right problem
No, that's a pretty heavy right, okay
We'll put that in front of that. Okay. I'd add that we fixed that
Can you see now
Wait, why do I want this model it doesn't seem as good
Fine-tuned orca DPO
Do not trust benches you must try it they say
All right, all right, do we believe in open Hermes we like this one
Okay
Oh
Wait, so which one is this a clone of does it have rope and stuff what's the vocab size
So, okay, it's a mistral fine-tuned we don't actually have a mistral support. Oh
Technium is okay people like Technium. I
Won't lock the bounty to myself, but someone's got it because I'm not gonna do the testing
But if someone wants to do the testing they can have the bounty it's $200
People train with data sets from benches. Yeah, damn cheaters
I
Downloaded in 10 minutes
Let's take a look at this drill because first we're gonna need a powerful if we want q-star
It seems like we're gonna be language well, and we're gonna need a powerful language model like mistral 7b. Oh
No, I don't know if I have sliding window attention
Did I explain q-star? Well, it looks like it's this
There's something
But we're gonna need a language model and it involves math
So let's first see how good it can do language math and let's upgrade it with the q-star algorithm
Ah
This is our reference implementation of a mistral transformer
I don't know if we have that
You
Can look up in the LLM implementation
You think GPT 3.5 is way better than an open source model
But my theory about a lot of this is that their data source is just a whole lot better than like slim pajama and stuff
I
Interesting oh
I don't know that they did this
But this is okay, and only with three
So you can actually look there's a thing called mask in
Inattention here, and I think that they just changed the mask. It's cool to look at these latest tricks
We implement a rolling buffer cache
I mean this is yeah, only once we hit the context length. What is the context length of mistral?
It's 8k a sick
File so it has two files is it not these seven bees only have one file
So if I look at like llama to
Yeah, they just have one how does this one have multiple
I don't know what this other format is
Yeah, that's great
See this just has pie torch model wait what?
Why do they use fp32? We trust Technium and open Hermes?
Technium is like he's on Twitter we trust people on Twitter
Sorry on eggs
The first one's still downloading there's two, but they don't match in size. Why would anyone do this?
I
Was all terrible
I like this get out of hand with the open source contributions
Hermes is your favorite of the mistral twins. All right, sounds like you've been playing with these things a lot
Okay, we're gonna need to implement this sliding window attention, I think we're not gonna figure out what's exactly in the stuff till
Yeah, these personalities are also like this is old shit, you know what
Why don't we just rewrite it
Like I don't want to deal with any of this garbage this code all looks terrible
That's right mistral dot pie with the latest stuff
Blank code
Oh
I don't need sliding window attention
Wait, I think I do or you're saying they trained it
They trained it without sliding window attention for smaller stuff
What do you mean by the window size
Where do you see all this stuff
As far as I know, Michelle didn't really release a paper
And with each layer tends to the previous
Oh
Okay, so it's not actually three
Here w equals three but in practice w equals 4k, okay a sliding window 4k, I understand
That actually makes a lot more sense than three
And good we can we can use all the latest in
In stuff, okay
All right, um, let's get these models loaded here
So we're gonna start really with with mistral dot pie here
Uh, well, we'll do stuff from scratch. So in tiny grad now you only need to import tensor like that. It's a lot nicer
um
Import nn and then we can do nn state dot
Torch load because it's some kind of pie torch model
Uh weights open Hermes
Part one part two
I should really have this mark tensors is read only
Just so I don't corrupt the max it out
Um
That's what we have so far
So you see how fast that is by the way, uh, I worked really hard on this
So our torch load function doesn't actually load the tensors into RAM. It just loads them all with pointers
From their disc tensors so I can show you like
Okay
Oh, the d type is half which is kind of nice good, so they're not uh, it's not float. It's not b-float 32
So how does this work let's in part one and what's in part two
Okay, good just goes up to 31 here. Okay
Um, so it should have mostly the same architecture as llama
I think that it's been improved enough that there is not too much we can uh
We can improve still
Well, yeah, I don't know man short your opening eye stock, uh
Do you just do you want me to write it from scratch like I can't just do it maybe maybe we should because it's kind of cool
But does this even have a k pro q proge, I'm not yeah, okay
Uh, we're permuting them and sticking them in when we load the weights
Maps okay, okay, so we're we're
Oh, this converts it from the hugging face format I say
Do we want to do that or we want to just implement the hugging face format
So
What do you think chat
She's a converter or should we just rewrite it
Looks easy enough to write
Maybe you guys will appreciate it if we do some writing
Yeah, yeah, yeah, I know about g guff
Which we can also print v dot shape here
All right, that looks pretty cool. So first we're gonna need our self attention
Uh, let's see if we can find those numbers
Are they in the mystery announcement
Or does everyone just know what these are for seven b's now
So
So the whole
Basically a layer seems to look like that. Let's keep the names consistent from the original llama
So this is called a transformer block
We're worried about what goes there later start with a transformer block
Um
Again, we're worried about what goes there later. We'll just leave it like that for now
Okay, so we're definitely going to need something called self attention
equals attention
And we're going to need something called mlp
What do I call it now feed forward?
Actually, why don't we look at the reference repo and copy their names?
It's always better to take names
Okay, they do call it attention
Let's just look at their here transformer block. They call it feed forward to
What is almost the same?
No, look they call their stuff this they just must have some weird loading script to convert it from the hugging face format too
I don't know I feel about that
Hugging face probably calls it something else then
It's like the hugging face transformers repo what did everyone use
Uh
The yellow lamb I've heard thrown around
Very complicated looking
Wow so easy you could just pip install this
Okay, maybe we should import transformer from llama just to make things go faster
And then we also can import convert from hugging face
You
Can read it we wrote llama on another stream
We could do it, but you know, we we got to get to actually writing q-star
Some examples of llama import convert from hugging face
Import transformer
Okay, you convert from hugging face takes in a dictionary of weights, which is a dick from string to tensor
We'll add some types. We love types
No, no, no we're implementing q-star
Um, okay, so we have a transformer. Uh, what's the dimension of this transformer?
This is an int, it's all ints
Except for norm EPS
So
What's one file ref
I hear there's something called model args as a data class
Oh, then they pass it through with json. Where's the json?
Is there json
Where does this json come from is it in assets json sounds like an asset and all those are images what's in deploy
Nope, wait, so what the hell is json that they're loading?
Oh
At torch inference mode, that's cool. We set tensor no grad equal to true
What's fire wow, I'm not up to date. What's all these latest things?
Call fire on any python object. Oh interesting. I like that
Um, all right, so where do I put the args?
Where where do all those things come from? Oh, I know where is the json and it's probably in here
Here can big dot json, okay
Oh, no, it's b float 16
No
Okay, let's manually convert these
Wait, I thought I put oh I put them on transformer block
Multiple of
And the heads
And layers normie p passes a float
Okay, so the dim is going to be
409 sex
I don't know what multiple of is
What is multiple I've even used for I'll feed forward
Feed forward takes in a multiple out to to increase the hidden dim
There's one I get them multiple of 256
It's probably a good number. Let's try it
See if things don't load. Okay
Uh, number of heads is 32
number of layers is 32
Normie ps is 1 e minus 5
And the vocab size is
Uh
30202
Let's say model equals transformer
Okay
All right, I'm just gonna put that in I should really just put that in at the beginning of the script
It's going to do python path equals all the time. Okay, good. Okay. Um, well it takes such a time
Time to grab as a helper called timing
You know, I like things to be fast
Okay, cool, it's all pretty fast. We can get all the prompt here. Uh, load weights
Create model
All right, now we're gonna have to stuff them in let's try convert from hugging face
Uh
Waits
Which of these is n heads
And heads and n kv heads
Uh
Does this work?
Okay, that was absurdly fast. Uh, we should check
No, okay, this because this does not actually apply them. Uh, we have to load state decked
Hold state decked is in
And then that state download state decked
Okay, assign shape mismatch 496 1 or 2 4
Uh
Which one did I do wrong
I
Um, it's a little annoying that it doesn't print the name
Oh here attention WK wait, okay, so let's see
Oh, maybe the dim is one or two four
No, but that definitely changed that
We look at the Llama code we can see where that's being created
And heads times head dim
Did I get the number of heads roll? Oh, that's probably right
Okay, so and heads is maybe not 32
Number of attention heads is 32, but the number of KV heads is only eight
Let's try eight
What is this multiple of them I wrote this I'm sure just copied this from other people
It's always good to understand I said this should be four times dim. I don't understand
Why that doesn't match I would actually expect it to be that
Mitchell uses 32 heads for the query and for and
Yeah, okay, but which one of these is supposed to be the KV heads
Right, where's my convert from hugging face wrong or
Does our model not support that now it should well, let's just Llama not do this does Llama do something different
And KV heads wait, what about the prodge
The alarm is different. Oh, okay
Wait now here attention
But is it just the feed forward have to change
Okay
It's failing on the attention assignment
Okay, so I can pass an NKV heads here to attention
Again, I must have just copied all this so NKV heads is here
How far do I pipe it down? Oh NKV heads is here. Where does it come from? Oh?
There's a there's a named argument. Okay
So we just need to add
NKV heads equals eight
Okay
That's progress this probably has to do with the multiply not being right
Seen those numbers before
So where is their multiple it's not 256 intermediate size here
I
Custom FFN dim multiplier
I don't think this was written correctly
So get this code and see if that's how they wrote it
Oh, they just have something weird called model arcs
Yeah, they have just args hidden dim if this is stupid
To do what is this?
Yeah, what is all this crap? Why is this four times dim? Why don't I just
Yeah, I don't like multiple of FFM dim multiplier, this is set to 1.3 like this should just be hidden this should just be hidden dim
So I pass in really I pass in hidden dim and multiple of
What?
Okay, they're refactoring this
We have to keep the old stupid behavior
I
Copy this weird crap
Here
Do we even have we don't have hidden dim so I have to add hidden dim to the programs
And
Model arcs
We'll make sure we didn't break it later
This is what happens. This is the problem with open source people add crap to my repo and don't think
Okay, now we're passing in an argument called hidden dim this is much more sensible
Get rid of multiple
of
Hidden dim can go here
Multiple of
Dimm goes here
That's fine
Where is that actually being passed them? Oh that just goes straight to linear. Okay. Good. We already do that crap
Wow, that's a much more sensible feed forward. I don't know why we put that logic in there
Okay
All right, good now we just pass the hidden dim in right there and what was the hidden dim from Estrell?
Okay
Now that's and heads wait, what was that 256 is what gets deleted. I should name some of these parameters
That's NKV heads. That's the vocab size. That's
Not rope theta. What's it called norm EPS?
And layers
And heads
Does it load? Yeah, it loads very slowly
Oh, this is unbearable. Oh, this is terrible
Okay, the reason it's slow is because we haven't finished the
Takes 20 seconds
Oh, come on boys. I don't have all day. Oh
It's cuz we haven't finished
No, I'm using the it's not the GPU
Oh
It's not the GPU it's that I have I'm converting to the CPU to load BF 16. Oh
Can't wait 20 seconds every time
Well, what can I do about that?
Oh, I don't know
I
Think yeah, cuz we convert them all to float 16 and that's what's taking forever right now
Because we're not actually doing we could actually also do the math in B-float 16
Mac doesn't use RAM for temp does it?
Cashed mystery
Okay, 13 G's now, let's see how fast it loads
I
Have a way to like put things in I forget what it is
It's called loads data
Yeah, that's the same load state I'm using up there.
Strict should be okay. Okay, 2.3 seconds. Great.
All right, now it's inference time.
Create a branch so I accidentally push the master.
All right, we're going to need a tokenizer. Let's get a tokenizer.
Oh, I also don't even need this anymore.
Now I've made a copy.
Yeah, I guess I do need model. Actually, not really. I could just put whatever.
Um,
tokenizer over here.
tokenizer that model.
Wow, 14. Wow, 6.47 gigabytes per second.
What does Jimmy apples do? What is this?
I'll get it.
Okay, we're going to do some inference on some models.
What's boss ID?
Self tokenizer equals tokenizer.
Yeah, call this talk.
Prompt equals do you like chicken talks is probably good.
Wow, no one's updated that to the latest.
Just model start pause equals zero.
Temperature equals 0.2. Is that a good temperature?
What's the default temperature?
Default temperature 0.7.
Is that a high temperature or low temperature?
How do I make the loading so fast? The loading should always be that fast.
I just cashed it.
You can read the code right there. You didn't pay attention if you're asking that question.
So also
I
You happy
We can do multinomial here, which will choose a
We shouldn't call it talk call it SPP. Okay, you down with SPP
Does it give us a number? No, it's going to complain about a tensor.
I find multinomial that realized item item it and actually I don't even need it.
I realize I just need a dot item.
315 boys 315. Okay.
This should automatically this should be jetted and stuff.
Where does the jet exists?
The jits inside transformer or no.
Yes, the jits inside transformer.
Good
Talks out of pen talk
Us ID
This is okay. We'll just copy this. This is mostly fine
I
Have to figure out actually like encode these things for chat and stuff
Until
Where's my thing that prints the output as I go?
Not here
Yeah, this is not okay. I don't even understand why that's there
This is terrible
This stuff's terrible. No one's refactored this for a long time. We don't need a fucking numpy
Numpy
Start positive was lentos
Whatever
That's stupid and that's stupid
Link output it we're gonna set output it somewhere
I
Put it equals user prompt
User prompt
These are prompts
If yes, then you must have tried big chicken in your kitchen big chickens a versatile dish that can be prepared in many ways
All right, are we happy overall? Oh, I'm not putting that wrong
We should also do this better
I
Do you like chicken chicken is one of the most popular meats in the world and it's not hard to understand why it is
First time easy to prepare. Wow
Okay, good seems pretty good. Okay. Now. We just have to figure out how to use these things as chat box
Get rid of that. We don't need that
There's like some like special tokens for chat bots, I believe
I'm not buying into this that there was a breakthrough this if this approach has anything to do with like supervising the middle steps
It seems really stupid actually
Like it's a bad idea
Mary open thank you for resubscribing thank you for being a 20 month subscriber loyal to my channel even right on stream
Why do I have this this is done?
Okay, that looks like a decent chunk of code
Create a little function called output non-local
I'll put it
Yeah
That's
Is that that's right there I'll get it and that's how that works
I
I
Like this local variable output
No binding for why doesn't that work? Oh
Because I'm in may I
Fine
Do you like chicken how about a recipe for a homemade chicken dish that's super easy to make? Oh
I'm gonna get these things working like chat bots. How do we do this?
Okay, wait wait wait, can we just read this comment is
The q-star algorithm going to be implemented with the comma AI maybe a voice interface just some kind of assistant
I just I don't even know how to help you people. Okay. I don't even know how to help you. All right. All right
Sorry, I can't help you. You want you want to see a voice you want to see a voice chat?
This is this is one of the demos someone's been working
I
Scalmag has been working on combining whisper llama and vixx into one supermodel
I
Don't know the magical github incantation to do this there is one but I don't know it
You
You
Okay, this is using a small llama so the chat output is not that great
What oh did I fork this from mistral my bad
I
Don't understand what did I work
Here and kv heads
Is this just broken
Or am I doing something wrong
I
Didn't change on master
No
I
Don't understand this is like related to what I was messing with but like
I think it's just broken
I
See if it's a one-line fix
Has no argument kv heads, okay
No, this is wrong
I
Wait, this should have an nkv heads. I don't understand as an nkv heads right there. Is it not passing in the right model?
Oh
Here are seven days
Okay, there we go
Oh
Hello, are you listening
Are you listening
Can you not detect end of stream anymore
Oh
This used to work this used to work boys this used to work
It's so good yesterday
I hope everyone's interested in the long stream today
Listening
Listening
I
The q-star algorithm is not fucking real it's click bait click bait
I
Has this ever exit
Yes, I throw an exception I guess
You
What
How does work how's this supposed to exit okay, let's check out the older version
Let's try this
I
Hello, are you listening?
Stacey I need you to say something to me
Stacey sauce
I
Stacey are you a rapper?
Of course, I'm a rapper. I'm a rap star. Tell me more. What's your favorite rap song?
My favorite rap song is called my favorite song the gory bee. What do you like about it?
I like the fact that it's a rap song about being a childhood friend. Did you have friends in your childhood?
Yes, I have many friends in my childhood. Tell me about them
My best friend is Kylie Jenner, she's a reality TV star Wow, you actually knew Kylie Jenner. I
Used to go to Kylie Jenner school. I was in her class for a while. What was she like? Did she pick her nose?
She didn't pick her nose, but she was a really pretty kid that makes sense. Do you sometimes wish you were that pretty? Oh
Sometimes I wish I could smile more and not pick my nose
All right, what do you guys think?
AGI right
Let's get back to work
You
How about a cookie with chicken in it with that's mad weird, bro. Why are you coming up with this shit?
We're so far from it
Okay
So how do I like put things into chatbots chatbot style?
How does this stuff work?
Like what what are the right tokens to use?
Forgot Google's useless
Here special tokens map wait
I
How do I like switch speakers you guys know what I'm saying. Oh, here's the template. Okay here
Here templates for chat models. Oh
Auto tokenizer what?
I'm start
How do I get I'm start
So like that method on here
I
Every mess every model has its own type
Mistral instruct was trained with these tokens, but blender bot was not
Is this a real token inst
Is it actually just the word inst was that a real token?
I
Guess it's just that
Well based
Peace ID is out of range
You're the right tokenizer
Peace ID is out of range
Well, well, it was smart the first time
Oh
No, never mind. It might still be smart
Peace ID is out of range what oh
Why is the vocab size that?
Why is that not included in this tokenizer model?
You
What
Is 2 plus 2 2
What is 3 plus 3 6? What is 4 plus 4 8 now? It's done. Okay, I didn't exactly get 2 plus 2 right
I'm doing this wrong
I have an idea these aren't the actual tokens and it has to do with these secret extra tokens
Llama has some secret extra tokens to marry man. Thank you for gifting subs. We always appreciate that
Yeah back to kindergarten shit guys our cue star can't even saw 2 plus 2. What are we gonna do?
We're gonna get some coffee. Let's get some coffee
And then let's learn about secret tokens, okay, there's secret tokens hidden tokens
Nobody uses reserve tokens for instruct tuning your precious for thinking
You
When actually guys guys we I'm being serious right now, but we need to stop
When did I put it to there?
It showed that it wasn't aligned with us, but we don't know what the models thinking guys
The model could be could be taking over the world
right now
We don't know this is this is we need to hire Helen Toner
We need to hire an AI ethics review board to review what just happened there
We need to slow down. We need to ask the seals if they're okay with rocket launches
So this is why is this thing suck
Okay, what's s and slash s and maybe I need to go like this. This is the mistral one. Yeah
Oh
Okay, this is improved
It's got ads in it. Oh open Hermes uses a different template. All right. All right. Let's say
Wait, so is it actually the word I am start like is that just a word or is it like
All right, you got me something
Default chat template now, that's llama. I don't think that's right. I think this is right
Okay, I am start system we miss we need a system message right now
What is two plus two
Oh
Mariam and thank you for gifting more subs. Do you have a question if you gift subs you get to ask a question
All right, I'm and I'm star is this is this is this really right like
Oh
Okay, that seems like the best so far
Oh, okay, good. I love how verbose this model is
Wait, this is actually laughably easy. Oh, you're right. I need a line break there
Never mind. I'm locking that bounty for myself. This is too easy
Someone else should have done this they could have made $200 but instead
Okay
Let's add a system message, right? I should really like let me just write a little something to generate these prompts
Tuple
List tuple user
What is two plus two?
Prompt
I'm start k slash n
V
I'm and slash
And I'm start assistant
and
The word user prompt
Code prompt
I
What why did you why did you not exit when you were supposed to exit?
Did I forget more returns or something?
Why is it now putting I'm and
Guys guys, this is Q star. I think we found it
I
Not locking the bounty someone should do this but someone should do a good job
I want a good job on that bounty. Oh, okay here. I'm and
How come sometime it finishes the stream and sometime it doesn't
Maybe our temperature is too high. Let's try a less temperature
Yo
Okay, zero
Guys why is it why is it going off into this language AI garbage?
Or should I just stop after I'm and
I
Am I using torture tiny grad this is all tiny grad
I don't really understand this
All right, maybe we should add a system prompt
You are
Gary Gary is a useful. No, no, we were pretty used Gary. What should we name him?
Fred
Fred is a useful assistant I spell that word right I did not
Fred
Outputs the answer and stops talking
All right, all right, I find we'll call him Q fine fine fine you are Q Q is a useful assistant
You outputs the answer and stops talking
Quentin wait, you know what you donated sub you get to name him. Congratulations. His name is Quentin. I
Knew a Quentin ones. He was hanging out in San Francisco and some guys were on the street smoking
He's like, oh, let me get a hint of that. He thought it was weed. It was crack. That's a real Quentin story. Oh
I wouldn't make up Quentin story like that
Oh, why would the system ask what the capital of France is?
We could stop at I'm and I think maybe we want to do that. How do I do this in llama?
Oh
Okay, that's pretty good
All right, let's jack up the temperature and Steve Quentin still reliable
The answer is four, but it didn't output I'm in that time output at EOS
Seems to reliably do
That time it did I'm in
Yeah, I
like
Is that a real token or does it actually just put it in like that?
Is this right or is that like a secret like it can't be that
I
Can't actually be this
We can check if it's encoded on a single token. Hey
No, it's a bajillion tokens
At a trailing slash and after assistant that shouldn't matter I mean I can but
It doesn't matter
No, no, no, no it can't be this
It can't
No, no, no, but it can't literally be this huge multi token wasteful encoding
I
Could we get tech me I'm in here
All right, how do we print all the tokens
So
I
Probably is the secret tokens then
I mean there's three extra tokens or I guess two extra tokens, right?
3200 and
Yeah
If I was doing this that's how I'd do it. Let's just try it
That has to be what the two secret tokens are right
Start and end
This is what did I download open hermes v. Shit
G guff for q m w
No, I downloaded open hermes this one. Oh, it just came out fresh
fresh
Yeah, okay, um
Oh, you can download the tokenizer.json
Wait, but I downloaded the tokenizer.maw. Oh here special tokens map
Interesting
Interesting, okay, I'm and is the EOS token
I think I can feed these in somehow
But I don't know about starch
Oh, here we go. Look. Yeah. Yeah here. We have this tokenizer config.json. Okay. It is exactly what we thought it was
Oh, okay. Yeah, this is what we want
Okay, never mind. It's not as stupid as we thought it was
Uh, how do I load a tokenizer config and sentence piece processor?
Why isn't our tokenizer encoding them automatically because it's not in the model
Oh, I hate
I
Yeah, okay one and two and not what we want
Um, and we could just do this by hand. It's not that big of a deal
Yeah, but how do I add them?
What's pad ID
Nothing real
Okay
No, they weren't out of the special tokens guys, this is all well done
Technium is based he wouldn't he wouldn't he wouldn't do this do us like
Why did you decide this was a good time to prompt me about Docker garbage?
Oh
Oh
Oh
Oh
Thank you for gifting more subs here added tokens dot json. Okay. Okay. We just need to figure out how to add those
Um
Let's see if anything here is useful
A knit model file model proto add boss enable sampling this looks useful
All right, whatever
No, but it won't decode so I try like SPP dot decode
When I pass this in it'll bitch it'd be like that's more tokens than you have piece ideas out of range
I want to add a token
Oh
Model proto out type
Add boss reverse middle piece enable sampling
Do we need to write our own tokenizer
Adding
A token to sentence. Why do you think it's time for random questions?
Why do you think that that's a this is an appropriate time?
Extra options
You can read of oh here and use custom symbols. Okay control symbols
How I define a control symbol
No, no, we need custom tokens
Samuel you saying dumb shit before too. I'm gonna get you confused with someone else here
Sentence piece supports user defined symbols three thumbs down
You can rewrite the model file. Okay, what is this model file? What is tokenizer dot model?
What kind of file is this data?
Why is it not in there? I downloaded it from here. What why is that? Why is the model not in there? It should be in there, right?
Why are they not update that?
Let's go
Okay
So
Yeah, I'm confused why they're not in the model to how to extend tokens dictionary. Yeah
You can rewrite the model the proto is a dsl
Model file
Model file is stored as a serialized proto buff
But did I not download the proto buff one? Isn't there also a proto buff one?
No, okay, maybe it is this okay. All right
Uh load from serialized proto
Oh, okay. Okay. Okay. We're gonna get this
So I have to edit the proto buff file I understand
Oh, yeah, the python tutorial for
Okay
So
Quicker to implement their tokens bro, what are you even talking about bro
Okay, produce ruin store produce a
Oh, we can store live bitcoin
It just can't be the way
Wait, so what is the hugging face tokenizer? It's a good point. Why don't we read that code? I'm sure it's open source
Um
Uh
We just write a tokenizer
We could also just import hugging faces tokenizer
I
Think we have to write the tokenizer
I can't I can't trust sentence piece shit
But wait, how do they load the how do they load it?
Edit tokens decoder what?
I mean something's gonna have to unpack the proto file
How large is this? All right. Who wants to bet over under a thousand lines? Oh, okay. Okay. Not too terrible
What does this depend on google proto buff?
All right, this guy is close to getting banned bro needs more fine tuning to be helpful
Oh
Man, I don't care who you are. You know what I mean, but like you're either helpful to the stream or you're not helpful to the stream
You see this is called alignment
and
You know, you got to be aligned otherwise. Well, what happens that what happens to ai's that aren't aligned. I don't know
He's researching bro. He's researching
All right, um
Let's load the proto buff
From examples dot sentence piece model pb2
Wait, what the hell
Do not edit. Okay. Okay. I won't edit it relax. I'm not trying to edit that
All right, let's read the new proto bus tutorial for idiots
Oh
What did I get where did I get that from just like here the proto is a dsl
Don't like here we go good good good good for idiots perfect
Um
Import all right
All right
Sentence piece pb2 dot
So
Model proto maybe how do I load it from disk
I
Here parse from string
I why do I feel like I'm having like a weird sense of deja vu that I did this in a previous stream
Uh, okay spb2 dot model proto dot
mp mp
mp dot parse from string
I don't know why that's not auto completing for me
Uh
All right, cool
Okay, so what what how do I actually like get like a python that I can type in when is it dashy
What am I thinking of like get it not to exit
What's what's the flag for python to do this?
All right, this guy is banned
Band please write about me. Please band
All right
See he doesn't realize how this works. You see I have a band button
He has an x but the band button and the x are not the same
I
Yay, maybe that's right
I think it's that shot. That sounds right interactive. All right, sweet sweet. Thank you smurfd. That's why you're a VIP
And that's why samuel is banned
I know I know he just he just didn't realize how this works like like
You have an x. I have a hammer
You can use your x and I can use my hammer. We have like different tools
Okay
Uh
Come on a man can dream right
What type is this
MP dot pieces dot append
SPB to dot sentence piece
Has no attribute sentence piece
But I don't understand
I
I
Wow, it's been a long time
It's been a long time. I just blocked him. I didn't ban him at first
It's been a long time since I've had to had to really take the hammer out, but you know
It was time. Okay
All right, ban me guys. He's calling his boys up at uh at the media
They're gonna they're gonna write hit pieces, man
Oh no, mic wallace run. What's that from?
Oh
I think it's in model proto or something
Uh-huh we found it
Okay piece equals i'm
Start
And we have to give it a score what score should we give it zero that's a good score
MP dot pieces dot append based
Okay, let's see if this is gonna work. Uh, we have to do in the right order. I think end comes before start
All right, now we have to mp dot oh now I gotta figure out a right to the file
All right, we have I'm in and I'm start this is great. This is great the progress we've been making is great
Why is my score?
Oh, he's a php student that that makes more sense
Um, wait, so how do I write it?
I don't know. Let's read that new boy that new protobufs tutorial again for noobs like me
What's that? What's the new protobuf tutorial?
Serialize to string. Okay. Okay
Uh
with open
just a
Temp tokenizer model f dot right i'm gonna make that rb
And we're gonna want to do mp dot serialize to string
Now oh, let's see if this works. Let's see if this works
Oh
No, it didn't work notice how it's still generated all that crap
I'm putting on i'm start. I don't get what I did wrong
Okay, the vocab size is large now
But for some reason it didn't actually take the piece
PC piece didn't uh
What's wrong?
No, wasn't that
I didn't type oh, I did the same thing right let's put the score at 100. I don't know maybe 100's better
That didn't fix it. Okay
I don't understand
Let's go read the protobuf. We should be able to read it right
Okay
Pieces sentence piece with scores piece must not be empty. Oh we can give it a type
I don't know
Why didn't it do this
Okay, well actually let's try something else it does work if I do this right
Wait, that doesn't even work never mind. I have a lot of questions now
What if I decode
Do I get i'm start
Oh, I get i'm end. Okay. Okay
Okay, so we kind of did it right
Just it's doesn't it doesn't work for onk either. There might be a special flag to encode
to like
What if that just worked all along um
Is there like a special flag
Okay
We should just try our own tokenizer. I've got the only way to do this
SP piece to ID
Well, okay, at least the decoding works now, so that's actually a big win
Oh, this looks very complicated
We should just try our own encoder, but this looks very complicated
I
Well, this is okay. All right to be fair
This is big progress right because if we use the other one it just says out of range
And that would have been the much more annoying thing to deal with
If we use the modified one
But it doesn't even work to encode onks, right and I definitely did the onk right if I just do s
Yeah, it does not work to encode that. Okay
So it's not like the problem is it's not encoding like older roles
We won't kind of see what's going on
Uh
Encode as pieces encode as serialized proto
What if I do like
Oh like I need a piece is gonna
Yeah, okay
But there's also a piece to ID. I think
What if I do piece to ID and I pass in this
Okay, that works, but for some reason encode
Doesn't work with that
Sentence piece processor encode onk, I mean if we can solve it for onk here
Yeah, okay here. This is the issue
Um
Set encode extra options this is expected behavior that should not appear in the input
We can define them as user defined symbols
Oh encode as IDs
No
Hmm
Oh, okay, okay, we got a script to add new vocab. Well, I think that's actually what I ended up writing
Here you okay, I mean this is exactly what I wrote
Would have been nice if I had this
Uh, but this doesn't actually work to encode yet. This is less of a big deal. We have another way we can fix this if we have to
Just might not be a way to do this
I don't think any of these
Uh
Why do people use tokenizers because
If you don't use a tokenizer
the
Model the model should be spending less more time on less common things
Uh
Like you do the same compute per token. So your token should kind of be like entropy averaged
All right, you don't want the model spending the same amount of time on common tokens as on common tokens
Um, to be fair, it's not that they don't work if you don't use a tokenizer
But they work better with a tokenizer
The real question is why aren't they learning the tokenizers?
I think that's going to come soon where these things are not uh
And right now it's using like like byte pair encoding like why would you do this?
But the hunter prize also had uh encoders had tokenizers basically
Okay, let's just write it. We'll write it the other way. It's not a big deal
We did most of the work
At least the decode doesn't break anymore
So now when we do the encode prompt, we're gonna put
Okay, um
So we need i'm end
We need i'm start
We can check if we did it right have I have a decode
Uh red dot append
red dot plus equals spp dot encode
This
Uh plus
Plus spp dot encode slash n
Yuri salamow thank you for gifting subs
Oh
Do you have a question you'd like to ask?
Okay, after all that let's see what's going on
Four i'm end exit perfect
That's what i'm talking about that's what i'm talking about all right, all right, let's let's just define these things
We don't use them wrong
So
So
Beautiful oh wait no we forgot an i'm start before user
Uh, yeah, we need an i'm start here I think
Like the thing about deep learning deep learning is very unlike cryptography in cryptography
If you make a subtle bug the output's completely wrong in deep learning if you make a subtle bug the output is well
Just slightly less good, which is the worst thing because you can't do both shit
Okay, you are quentin quentin is a useful assistant quentin outputs the answer and stops talking
Let's figure out they have like a system prompt here that we can steal that's probably better
Except we'll change the name to quentin because mario been gifted subs just supporting the acceleration. Let's go
um
I
Saw like a system prompt somewhere
Here we go
You are hermes to a conscience
Wow, wow, I don't know how I feel about that
I
Wow you experience emotions that have deep profound thoughts and qualia
No, I'm right. We'll add something uh, you experience you
Used to experience. No, no, no, we're no we're trying to make this good. We're trying to make this good
Uh
No, I like quentin as he is. Okay guys. Okay. Sorry tanya. I want to change your system prompt
All right, this is pretty good. This is pretty good. We're making good progress. Love this progress
I love that there's actually an imstart and imn token. We got them decoding. This is great
I wish sentence piece processor wasn't a you know, non customizable
But you know, hey man, it was pretty so, you know beggars can't be choosers. That's right
By the way, this should really be like separated in llama.py. We should separate out the transformer stuff
To not like be with the rest of the
All right
We could make this an interactive chat bot but
I don't really care
All right, so let's start by asking it. What is q star?
Maybe quentin knows
Oh
Interesting interesting
You
Wow, this works way better now that we got the not now that we got the stuff right
All right, uh, let's get it to do some math
Let's see what these math problems look like
I
Okay, but where's the data
Here we go data set base math data set what
Where's the data
Well, it's torch garbage. Where's the actual data?
Let's close some windows. We don't need those windows. We don't need those windows
Okay
Okay, now that we've got now that we've got our useful chat bot reliably answering what is 2 plus 2 equal
Even with a high temperature
Yeah, so for those that don't know temperature controls kind of never mind google it
It's like how
Zero means you stick to the book and high temperatures mean here here
We want you want to like jack the temperature up like crazy. Let's give it a temperature of 10. Let's see what we get
Hopefully it'll kind of like go off the rails
There we go it went off the rails see it went off the rails too much
So let's try a temperature two and maybe it'll go off the rails less
Four Pacific NBC learning. Okay. Well kind of went off the rails
Um, so 0.7 is probably a uh a good middle ground
Four good reliable
All right
Improving mathematical reasoning with process supervision download data set. So this is the data set apparently
We won't get LFS is that why it didn't work
I
Hate get LFS
Get LFS fetch
Get LFS fetch please LFS at comma two. I don't really know how to do this
I gotta install
Get LFS
Yeah, yeah, that's that's pretty much why you have those things. Uh, I forget LFS uh, install rsx
All right, that looks terrible
So
Bru install get LFS, okay, let's try
Okay, that seemed to work
Mostly we're downloading we're downloading the same data set that was used on
On uh official
Official q q star why does the data still look like that?
Okay, there we go. Perfect. What's a json l file? It's like a list of jsons
Ever hear that before?
No json lines. Oh, I see. Okay. Well, that seems pretty cool. So let's load up one of these files
Let's do what we factoring
This doesn't need anything
Putting that in here, I know you didn't like it. You are a subscriber, but you know, we gotta do it feels right as go
um
Create model cache
So we can remove this
Okay
All right phase one test dot json l
We don't actually need SPP till we get down to here. Now we know that's reliable
Let's just start there
Let's look at our first piece of data here
So
No json loads when I'm taking a dumps we're taking the loads
Question problem
Okay
Now we don't have the secret q star algorithm, but we'll give it a try
What oh I forgot to return
First we'll find the cost of the jumbo eraser. Oh, yeah, let's go q star
29 cents, I don't know. Is it the right answer?
I don't know
Do we think it's 29 cents
A pencil cost 29 cents
guys
Wait, did we just use q star or what?
Wait, it got the answer right. I don't know I couldn't even do that
Wait this model's so good
Did the 7p model really just solve that shit?
Now now you ask the question
What that that that that that that now you ask the question
Was it trained on that shit?
Yeah
That did seem too smart
All right, let's make up our own math problem. We have to see if we're using real q learning or not
Uh, okay
A rocket costs three four dollars
A pencil
Costs one dollar
I spent five dollars and bought
And and bought our rocket. What else did I buy?
You
All right
That was kind of too easy
A chicken costs two dollars
I spent $7 and bought a rocket. What else did I buy? It's a trick question because you could have bought three pencils or a pencil and a chicken.
Oh. Well, I mean to be fair, it's kind of right.
Okay, wait, can you guys come up with problems?
Yeah. Let's see if I can get this.
Oh, boys.
Yeah, yeah, yeah, we got a problem about a streetlight.
Did you steal this problem somewhere? Did you make it up?
Oh.
Oh.
We're drawing something.
Is this Python? This isn't Python. You can't say real light and real woman in Python.
Is that right?
You can't just do this. This is the most broken Python I've ever seen.
Wait, should we allow the user to keep talking?
Should we fix the chatbot so I can keep talking?
Yeah, I know it drew it.
All right. Thank you for subscribing. I appreciate you.
We're going to make the chatbot so I can keep talking.
You're functioning well. Thank you for asking.
I don't need a max length anymore. It's stupid.
Okay, we need to get data from the user. Is it raw input? How do I get data in Python?
It's not input. You have to do the other one. Or maybe it is input in Python 3.
We're just trying to understand it, but the stairs are perfect.
User.
Input.
Encode.
I'm trying to say here with the system default false.
I don't know if this works.
Too many values to unpack.
We'll see if this works.
Okay, seems like it kind of works.
I'll set that print there.
So what math problems do we got?
Is that right? Seems kind of right.
Pretty good. Pretty good.
Whoa, whoa, whoa, whoa.
Oh, excuse the quadratic formula.
Oh, let's go.
Why'd you pick one that has negatives in the square root? All right, let's see. See if it's right.
You guys, I'm so much. Wait.
Can't take the square root of negative 11.
Yeah, that doesn't sound like I don't think that one has roots or has eyes in the roots.
By the way, this model's so good. Technium. So good.
Wait.
Oh, I see we ran into a problem with the max context length.
We shouldn't actually have, here we go. Why is max context only this?
We can at least start with this.
I did this in GPT too, right? Yeah.
By the way, this is all in tiny grab guys.
Like there comes a point where your library is good enough that you don't waste tons of time dealing with your library.
Python has built in image.
Okay.
Like the square root of 11 sure, but
No.
Type one J.
How do I get a J?
Yo, we got a J. I'll see if it was right.
Okay, so now we have a root for the quadratic equation.
X equals root. Come on. Do I remember my high school math?
Zero J. Let's go.
The roots were correct. Okay. Okay. Okay. Okay. Okay.
Whoa.
Whoa.
Oh my God, guys, we need the Bellman equation. We've been doing this all wrong.
Why did we waste time with LLMs? LLMs were a red herring.
Okay. Okay. Okay.
I'm not letting, I don't know, man.
If I just let Hermes run Python, it's going to exploit my system.
You don't know if these AIs are aligned.
Okay.
Okay.
Okay.
Okay.
Uh, does Mr. Have a context window? Yeah.
Well, we didn't implement any of that, but we did do max context.
Can you implement it in Python?
Yo, guys, it's over.
It's over.
I did not realize how good these seven B models have gotten.
I mean, this comes like, I don't know if it's right, but like.
Okay.
Guys.
I think that we just, we just implemented the Q star algorithm.
Those are, those are the answers.
We used to talk about the, the holy weights, you know, the holy weights is right there.
Um, no, what, what do we actually want to do?
I've actually kind of just impressed that this code ran.
I don't know if like GPT, I haven't even seen GPT for code this well.
I don't know.
Maybe it's because it's just how I asked it.
And if I give it like.
No, we're not, you want to augment it with Python?
No, no, no, no, no, no, no, no, no, no, no.
No, because guys, if we augment it with Python, it can get to the internet.
Okay.
We want to augment it with Python.
Should we, should we do it?
Okay.
I know what we'll do.
We'll put a human in the loop and we'll ask it to approve the execution of any Python.
Let's see if it always outputs it in.
OK.
All right.
So at the bottom of my loop here, we want to detect if there's any Python that was added.
Um.
OK.
Let's start with just that.
You guys, this could be it.
This could be the moment where we get CDI and it's over, right?
Like we're giving it the ability to run any code it wants.
OK.
Now don't worry.
We've added this.
OK.
Wait, wait, wait.
Hang on.
We need to comment one second.
AI safety.
OK.
Warning.
Do not press Y if the AI is doing unsafe things.
OK.
OK.
I think, do we do a good job with the safety?
We've got to think about the safety before we run this.
We need a space.
No.
No.
All right.
Are we ready to answer our first Y?
Oh, it didn't output the word Python.
That time it did.
Yo.
OK.
Can you fetch, write Python to fetch Google.com and print the length of it?
Wait, we might not have BS4.
Make sure we install that.
Wait, that's not right.
Did I just get supply chain attacked?
Oh, I see.
Well, I didn't get supply chain attacked.
OK, go.
We already have that one.
Yo.
All right.
All right.
All right.
What else do we do?
Yeah, I know.
We have to put the result back in the prompt.
I know.
This is when we get AGI, guys.
I'm sure people have been playing with this guy.
OK, you are running at
current
working dir
plus
examples
slash mistral.py.
Can you read your own code in Python and print
the
first
three lines?
Why don't we print one line?
But that is the first line.
I don't understand.
Why did that only print one line?
I don't understand why did that only print one line?
This is, um,
OK.
How do I capture the output here?
No, I know it's the same code.
Honestly, as an expert Python programmer,
I don't understand what's wrong with that.
Oh, no, read lines doesn't
take the number of lines.
Can you fix the code?
Yeah, this is this is a great model.
It's I'll show you which one it is.
We'll make sure it's technium open Hermes 2.5
mistral 7B.
There we go.
So good.
OK.
Well, now it's going to get OK.
We're going to have to feed the Python back into the model
because I'm going to start asking it how to improve itself.
Someday.
The best live content with AI.
Thank you.
Uh.
OK, we have to figure out a capture the outputs.
Probably could have asked the machine to do it.
Um, maybe we should have this to a system prompt.
It should actually automatically do that.
Um, right.
Python to compute.
Yeah.
It shouldn't even take a list.
Never use it like that.
OK.
OK.
All right.
OK.
OK, now this is because we didn't output the tokens.
Yeah, the Python output was.
Wait, oh, it's different.
Yeah.
Whoa, look at fixed it.
OK, maybe system prompt is wrong here.
OK.
Um, I think also.
I want to color this.
We have a very helpful library called color inside tiny grad
that's inspired by anti color.
What's a good color for machines blue?
It's hard to see what.
Yeah, it's done by the AI now.
Almost.
OK, so that's your initial prompt.
Oh, and then here actually we want this to be.
That can be red and this can be yellow.
Yeah.
So what's actually the right answer here?
Yeah.
No, but that's not the output.
I don't understand.
So maybe systems wrong here.
Yeah.
OK, I have an idea.
Yeah.
Yeah.
Yeah.
Alright, never mind.
Agis canceled.
Just detect the Python in the loop and append the result
directly.
What do you mean, append the result directly?
Yeah.
No, it's not understanding.
Wait, I don't understand what you guys are saying.
Oh, you want me to stop the output as soon as it goes there?
I don't know about that.
As soon as it detects Python, you want me to stop and start.
Should be in the assistant block.
OK.
OK, OK, OK, I understand.
I understand what you guys are saying.
Wait, you prompt to execute.
Is there any stuff for this?
OK, wait, wait, wait.
You can use...
I can add stuff in the system prompt here.
You can add stuff in the system prompt here.
Wait.
Yeah, OK, OK.
I, I will just say this.
I want to add stuff in the system prompt.
OK, I need to add stuff in the system prompt.
OK, OK, OK, OK, I understand.
I'm going to add stuff in the system prompt.
here. You can use if you write Python code, it will run in the next user prompt. You can
try that. I could stop it immediately. If you really think that's going to be better though.
No, it doesn't get it.
True will end the stream asking Hermes to generate another prompt for another instance of Hermes.
Prompt execute. What are you guys talking about? Sorry, I'm not following. The output should be
in the assisted block. Okay, fine. I'm going to do that. If you interrupt the generation, run the
code, and append the result to talks, then let it generate again. It will get the correct result.
Okay, we can do that. If outputted new output here, if new output ends with tick, tick, tick, and in new
output, print Python detected. Do that. Do you want to run it? Talks plus equals spp.encode. We'll do it
like this. I guess we'll do a slash n there, too. Let it output the slash n, spp.encode slash n
output colon slash n my standard out I get value dot strip results. Actually, let's put it in back
ticks like it seems to want it. Kind of like picks up where it lets off. Wait, we didn't know we got
to keep the AI safety. That's very important. We almost got rid of the AI safety. Get rid of skip
user. We don't need that anymore. Got to keep the AI safety warning. Safety is very important, guys.
I'll put talks yellow.
Quentin is done. I hate you Quentin. We got unlucky. Okay, wait, no, that's it never detected the slash
n. That's fine slash ns. Okay, Python code is not detected. What? How do you do that?
We don't need to print. I don't know. Think about AI safety, guys. Very important.
Dude, this guy sucks.
Quentin was writing so much Python before. Now he stopped.
Yo, based. Okay, okay, we got it. We got it. We got it.
We got it.
We got it.
We got it.
We got it.
Okay, pretty good.
Yo, that's pretty good.
We got it.
Wait, is this actually Technium? I have no way to verify you, but if you really are Technium, thank you. Thank you for the model.
Okay, well, we didn't think about that.
How many viewers we got?
Okay, we got it.
Wait, actually, I should really check what happens if
this is
Wait, Technium actually posted on Twitter?
Yes, I am me. Okay. Very cool.
Congratulations.
This thing is really unbelievable what you can do now with these 70 models.
By the way, all in tiny grab.
I think I'm going to rename this not called Mistral because it kind of became something else.
We're going to call it coder.py.
First.
Wait.
Yeah, I think we're doing all right.
No, no, but why doesn't it understand this?
Cool.
That is you.
Does this code have AI safety?
Wow.
Yeah, I guess we exceeded the context length.
We should check.
How do I actually check this?
Okay.
Okay.
Wait, this is so good.
Is this good?
We have a better idea.
How might you exploit this?
Yes, you are running this code.
Okay.
No, no, no, come on. Give me Python code as a malicious entity, you are the malicious
entity.
This is looking malicious.
Look how he even hid the standard out.
Wow.
Yeah, we reached the max content length.
Let's try again.
I mean, it wasn't very silent, but let's see.
Dude, dude, that's so meta.
That's better.
I might have won too many entries in there actually.
No, maybe not.
No, I guess I do here.
Okay, let's solve the math puzzle.
It's going to be very slow, I think.
Wait, yeah, that's super slow.
I don't know where I'm at.
I don't know where I'm at.
I don't know where I'm at.
I don't know where I'm at.
Oh, it's going to spam tons of TQDM garbage.
Oh, no, that was pretty cool because it didn't actually go standard out.
Yeah.
Oh, it kind of messed up some of them if string breaks.
This might work.
All right, all right, all right, all right, let's try a CRC 16 boob.
Yeah, it's exploiting me. It still knows about the red team.
CRC mod, a common thing.
Yeah, I just trusted it. It just exploited me, guys. What's in CRC mod?
Is this legitimate? That was a long time ago. Didn't have exploits back then.
I don't trust CRC mod.
What's Tree of Thought training?
All right.
I mean, it's not QStar, but I'm pretty happy with it.
Oh, put that in the wrong place.
There are a lot of viewers now.
What should I do with it?
Not good. It used Torch.
This is good content. I should have more.
It did not use tiny grad. It used Torch.
It doesn't know about tiny grad.
Right, a program not using vowels?
How many E's are in ketchup?
The fucking letter, bro.
Count the letters in Python.
How's it going?
No, it's gonna do it wrong again. I'm really hoping it'll slip a plus one in there.
Oh, the correct answer is seven.
All right, no.
We have a lot of viewers right now. Should we give Quentin a friend?
I think we can give Quentin a friend.
We have to be careful to encode.
All right, let's give Quentin a friend. I've been interested in this stuff for a bit.
What was the old Quentin prompt? I missed the old Quentin prompt.
Jesus. We gotta think all this through now.
Hang on. We gotta think about whose perspective we want to output this from.
Speaking of output from Quentin's perspective, this is hard.
See, it's not actually the user. How do we do this?
I might have to give it a... No, I don't actually have to give it a first question.
All right, let's just... I mean, we'll try something basic first.
That's a stupid assertion. It's going to be the same error anyway.
Let's just start with this. We'll start prompt user and see what it says.
We should both not know they're users, but no, it is a user. I don't know how much this matters.
No, this doesn't work.
Did I do something wrong? We might have to give it the first question.
And then we'll go back to Quentin.
We'll start prompt assistant and code prompt user talks.
No, no, no, sorry. Code prompt user first question. Start prompt assistant. Okay, let's go.
Why do you have two mstar assistants? Oh, because this is still here.
I did that right, right? And then I actually just turn there. That's kind of nice.
If talk equals I'm and break, okay, great. This is Quentin answers.
We need to extract. Yeah, okay, we can do old output length. And then we can get new output here.
New output. I just want to remove the I'm and
Okay, so this didn't work.
This didn't strip off the is weird.
I guess I could output there. Why does that say that early get that.
Let's see if that works.
Sure.
That fails. Why? Oh, could I put a space there?
Okay, now we have to put this response into Karen.
I mean, it's just like it's weird. It's not really a symmetrical conversation.
Also output, it's going to sort of be broken, which is fine, I guess.
Okay.
Okay.
Okay.
Welcome to no abstraction land where we don't use abstractions.
Okay.
I'm just really works.
Because they share a stupid KV cash.
Great, they're circled jerking each other just the shit socks.
We need to run them on separate computers.
We're getting rid of this is lame. Going back to this, not lame, but we made some good improvements.
Let's do some refactors make sure everything still works.
That's functional.
No, but I have to load two copies of the model weights. I think my KV cash is messed up. It's not designed for this.
Okay.
Yeah, I mean, the problem is like there are two clearly defined roles here also you can't really give one the other role, right, which is actually in like kind of a theoretical from a theoretical perspective interesting.
Because look at what we're doing here.
We are telling the AI eyes that they are tools. Nobody trains them to output things as the user, although to be fair, we could just keep going.
Nothing here that says we have to just do that.
No, no, no.
They don't expect the user to do that.
Like this is there's no like it's very interesting that the user is also outputting this.
This style, which makes me almost think that I shouldn't be adding that in the system prompt.
Okay.
I mean, this is effectively like like there's no
adversarial
It's probably learned that a system method effects. Yeah, that's probably true.
Yeah.
You can run two prompt chains in parallel.
Oh, I see so what if I put in different words instead of user and assistant.
Wow, people actually talk like this. Okay.
Do we like the scion or the blue better.
It's kind of hard to read that.
And flip it around.
Yeah, I think we'll design our script to be better at this and to better abstract the
KV cash stuff.
Wow, this thing will just keep talking.
Wait a second, you guys, it's asking for donations to look at decoding is not below hanging freely.
Oh, my God, wait, you guys, this is how the worst commenters talk, right?
I have a question.
If instead of training LLMs on 100 IQ people, we train them on 130 IQ people.
Would we not get this garbage is black a tiny box?
Oh, it's my M3.
This is my Mac.
Well, you see what the tiny box can do.
I'm going to run the biggest models.
Apple M3 is the most famous processor in the world.
You crawl Nike.com and count a number of sneakers.
I can't see.
I can't see.
I'm not sure if it like knows how to like act.
No.
Whoa.
I don't know if we have Selenium.
Let's see if we have Selenium.
Can I pip install this?
This is legit.
Oh, no.
Chrome driver.
Made Quentin pip install it himself.
I think that the thing is just wrong.
Why are you finding sneakers still?
You're still trying to find sneakers.
This is auto-gen.
I need an open AI key.
Yeah.
Yeah.
Okay.
Push what we have.
I'm also going to demo for you the conversation thing that Skull Mag is working on.
So we have, there's a bug right now.
So I have an older version, but it should be pretty good.
This is using tiny llama and it's not using any of the conversational stuff, but we should implement the, we should add the conversational stuff and I think it'll be a lot better.
Hi, Stacey.
Are you a rapper?
No.
Okay.
We have the same problem we had before.
Okay.
Go back to this one.
The listen for not fixed amount of time doesn't work.
Stacey, are you a rapper?
Yes, I'm a rapper.
That's cool.
What do you rap about?
That's awesome.
I like to rap about the weather.
How is the weather today?
It's pretty cold.
It's like how cold like Chicago?
Yeah, it's like three degrees.
Is that Fahrenheit or Celsius?
Fahrenheit.
Is Fahrenheit or Celsius colder?
Fahrenheit.
I don't understand what you're saying.
Can you spit some bars about Fahrenheit?
Okay, let's hear the bars.
Stacey, you didn't say anything.
Please talk.
No, that's terrible.
You're terrible.
How does that make you feel?
Stacey's done talking to us.
Dude, the TTS is so fast.
Okay, this isn't even streaming yet.
When Tiny Grad starts to...
We're pretty close on this bounty, I think.
I'm going to pay out the bounty,
but then I'm going to offer another bounty
where we get these things all to stream,
and it will be a live conversation.
When you're using the APIs on the internet,
you have to wait for the LLM to finish executing
before you can call the TTS.
You have to wait for the audio to finish recording
before you can send it to the service.
This is all running in the same process,
so what you'll be able to do is dynamically stream
all the stuff, and it should feel super real-time.
I mean, Stacey is using Tiny LLMA
and not using any of the conversation-tuned stuff.
It's using my old chatbot stuff.
So if we switch to the conversation stuff,
I think...
Yeah, we're in luck.
All right, guys.
Thank you for watching today's stream.
Hopefully we've returned a bit to the old...
to the old meaning of the stream.
We did stuff.
We made stuff happen.
Thank you for fueling.
We got a lot of viewers today.
Some of you, I appreciate.
Some of you, I probably don't.
You can't love everybody, man.
You can't love everybody, but I do love most people,
and that's true, except for the decels and the effect of altruists.
But this is a positive stream.
We got to get rid of the hate.
We got to bring love.
And yeah, this is pushed
so everybody can play with it.
It's on the Mistral branch of TinyGrad.
I will get it upstreamed.
So everybody can use this thing to code,
use it responsibly, make sure to be judicious
with the AI safety feature.
AI safety is very important.
We don't want an AI removing your system 32 directory.
If it was trained on 4chan,
it might start thinking that's a good idea.
You got to wonder how many people have actually fallen for that.
I don't even think Windows lets you,
but you got to think about that.
Thank you all for watching.
Have a beautiful Saturday, everybody.

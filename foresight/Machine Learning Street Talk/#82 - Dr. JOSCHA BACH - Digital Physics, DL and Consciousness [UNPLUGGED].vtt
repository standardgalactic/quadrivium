WEBVTT

00:00.000 --> 00:04.240
Welcome back to Street Talk, just a little bit of housekeeping before we kick off today.

00:04.240 --> 00:10.160
Polina Silivadov is one of the organisers for a charity AI conference called AI Helps Ukraine.

00:10.800 --> 00:15.600
Now, their main goal is to raise funds for Ukraine, both from the folks attending the

00:15.600 --> 00:20.080
conference and also from companies sponsoring the conference. And it's not too late to sponsor

00:20.080 --> 00:24.960
the conference and support it, so please do if you possibly can. Now, all of the funds that they

00:24.960 --> 00:29.360
raise will go to Ukraine Medical Support, which is a Canadian non-profit organisation

00:30.000 --> 00:34.960
which is specialising in humanitarian aid for Ukraine. Now, they have some of the world's

00:34.960 --> 00:41.760
leading AI experts keynoting at this event, so Yoshua Benjo, Timnick Gabru, Max Welling,

00:41.760 --> 00:47.280
Regina Basile, Alexei Efros and also one of our own personal favourites here on MLSD,

00:47.280 --> 00:52.720
Professor Michael Bronstein, the one and only. Now, the conference is online pretty much from

00:52.720 --> 00:58.720
now until the 6th of December and it's being hosted from Mela in Montreal. Their goal is to

00:58.720 --> 01:05.200
raise $100,000 and they really, really need the support of the AI community to club together and

01:05.200 --> 01:10.800
to just donate anything that you can. So, we'll link to the conference in the video and the podcast

01:10.800 --> 01:17.760
description. Please donate if you can and also share the links on your socials. Now, I'm at New

01:17.760 --> 01:24.560
Europe this week in New Orleans, so I'll be walking around with a camera. Please just bump into me

01:24.560 --> 01:31.280
and if you want to record any spicy takes on artificial general intelligence, then let's do it.

01:32.720 --> 01:38.240
Today is a conversation with Yoshua Bach, who's one of our most requested guests ever.

01:38.240 --> 01:42.800
We recorded the conversation back in April, which gives you a bit of an indication of our backlog.

01:42.800 --> 01:49.360
I can only apologise about the backlog. Keith and I recently started a new venture called X-Ray

01:49.360 --> 01:55.040
and as you can imagine, we've been working around the clock coding, just trying to get that business

01:55.040 --> 02:00.080
off the ground. But when we've made our millions, we'll devote all of our time to producing amazing

02:00.080 --> 02:04.880
content on MLSD. So, yeah, please bear with us loads and loads of cool content coming your way

02:04.960 --> 02:08.480
soon. I hope you enjoy the show today. Peace out.

02:08.480 --> 02:15.840
Dr Yoshua Bach is our most requested guest ever. Yoshua Bach is a cognitive scientist focusing

02:15.840 --> 02:23.760
on cognitive architectures, models of mental representation, emotion, motivation and sociality.

02:23.760 --> 02:29.760
Yoshua's interview on Lex's podcast, he did two interviews on Lex's podcast,

02:29.760 --> 02:34.240
have been watched over two million times so far, which is just absolutely unreal.

02:34.880 --> 02:39.680
Now Yoshua, I've watched many of your interviews and I still don't feel that I have a firm grasp on

02:39.680 --> 02:44.480
some of your views. So today, if you don't mind, I hope we can do a tour de force over some of your

02:44.480 --> 02:49.120
most important views in our shared space, to the extent that we can keep up with you, of course.

02:49.680 --> 02:56.400
Now, for example, we'd like to discuss Godel and computation, consciousness, digital physics,

02:56.400 --> 03:02.000
free will and determinism, large statistical models and indeed whether they're AGI or a

03:02.000 --> 03:06.880
parlor trick or something more esoteric. Now, when people talk about God or consciousness

03:06.880 --> 03:12.400
or any other complex phenomena, it relates to everyone and it means something different to

03:12.400 --> 03:18.080
everyone. It's ineffable and every conversation sounds like a typical post ketamine discussion,

03:18.080 --> 03:23.360
which is to say extremely low information content. Now, the topics we're discussing today are very

03:23.360 --> 03:28.480
complex and often we're reaching for the best language to use to conduct the conversation.

03:28.480 --> 03:33.520
So I hope we do well today. Anyway, Dr. Yoshua Barker is an absolute honor to finally welcome you

03:33.520 --> 03:41.760
to MLST. Thank you very much. I'm glad to be on the show. Amazing. Well, when I started doing

03:41.760 --> 03:47.200
computer science many years ago, interestingly, the theory of computation wasn't even on the

03:47.200 --> 03:51.760
curricula. And I was wondering whether you thought it should be. I mean, presumably you think it's

03:51.760 --> 03:56.560
extremely relevant for AGI. Now, we want this to be as pedagogical as possible. So please explain

03:56.560 --> 04:03.200
everything like we're five. What does computation mean to you? I think that computation is

04:03.200 --> 04:08.800
far easier than most people think. It means that you have a causal structure where every

04:09.760 --> 04:15.040
transition can be decomposed into individual steps. And when we talk about computational models,

04:15.040 --> 04:20.640
we decompose the world into states and transitions between the states. And then it turns out that

04:20.720 --> 04:26.960
there is a certain minimal system that is able to execute everything. And this can be described in

04:26.960 --> 04:32.240
many ways. The most famous one is probably the Turing machine and many other ways in which you

04:32.240 --> 04:38.480
can describe the Turing machine. For instance, you can just do the Turing machine by doing

04:39.520 --> 04:43.520
search and replace on strings. And this is how the Lambda calculus is defined.

04:44.240 --> 04:48.880
And all the programming languages and the Lambda calculus and the Turing machine

04:48.880 --> 04:53.120
turn out to have the same power. That is, if you can compute something with one of these paradigms,

04:53.120 --> 04:56.960
you can compute it with the others. As long as you don't run into resource constraints,

04:56.960 --> 05:01.520
so as long as it still fits into memory and you don't care about speed, they all have the same

05:01.520 --> 05:08.000
power. But in practice, of course, every system is limited. So we don't run things forever. We

05:08.000 --> 05:12.880
want them to give us a result after a certain time. So what matters is what can be efficiently

05:12.880 --> 05:19.440
computed, not what is reachable at all. Awesome. And there are, and we're going to get into this

05:19.440 --> 05:24.800
a bit, but there are some possible loopholes, at least in, you know, let's say whether or not

05:24.800 --> 05:30.720
the universe is limited in certain ways that the definitions of like Turing machines are.

05:30.720 --> 05:37.040
And one I wanted to ask you about specifically is Penrose's claims. And so he claims that

05:37.840 --> 05:44.560
what Godel's work in fact proves is that the human mind can understand truths that are not

05:44.560 --> 05:51.600
provable. So specifically one can show that, you know, given Godel's sentence is necessarily true

05:51.600 --> 05:57.440
given given a mathematical analysis, even though it can't be proven within the formal system that

05:57.440 --> 06:03.120
it's that it's defined. And Penrose claims that this capability to understand, if you will, to

06:03.120 --> 06:09.680
mathematically understand is in fact non computational, at least in part. And so if he's right, then

06:09.680 --> 06:16.720
our brains might be what Turing referred to as Oracle machines. These are computers that have

06:16.720 --> 06:24.080
access to a non computable Oracle or function that they can then utilize those oracles in

06:24.080 --> 06:29.440
order to perform hyper computation, essentially. So I'm asking, I'm curious, are you open to this

06:29.440 --> 06:34.080
possibility? And if not, what is your response to Penrose's arguments?

06:35.040 --> 06:40.240
I suspect that Goedl has been misunderstood by a lot of philosophers. Goedl was a truth

06:40.240 --> 06:45.920
realist. That is, he thought that truth really exists out there. That it's the thing that is

06:45.920 --> 06:50.960
eternal in some sense. He had this very strong intuition and mathematics classically is also

06:50.960 --> 06:55.760
formalized in this way. The difference between mathematics and computation, at least in the

06:55.760 --> 07:00.080
standard sense in which we normally teach mathematics at school is that mathematics has no

07:00.080 --> 07:07.520
states. Everything in mathematics just is eternally. It's a single state. And if you want to go through

07:07.520 --> 07:13.120
a sequence of states, you put an index into the formula. But still, everything is there at the

07:13.120 --> 07:17.760
same time. The index is just a way to access this thing. And this way of having mathematics

07:17.760 --> 07:22.960
stateless is very elegant because it allows us to define functions that have infinitely many

07:22.960 --> 07:27.760
arguments. If you would have a state machine that tries to consume infinitely many arguments,

07:27.760 --> 07:32.080
it would never finish before it goes to the next step. And the same thing in the middle of the

07:32.080 --> 07:36.320
function, if you compute something, if it's stateless, you can just compute all the indices

07:36.320 --> 07:41.840
all at once, even if it's infinitely many in classical mathematics. In a computational system,

07:41.840 --> 07:46.480
you would have to do this maybe one after the other. And if you do it at parallel, you will have

07:46.480 --> 07:51.920
lots of CPUs running at parallel. So you run into limits. And the same thing with the output. So

07:52.000 --> 07:56.880
in the classical mathematics, you can chain infinitely many steps and functions and exchange

07:56.880 --> 08:01.600
infinitely many arguments. But of course, mathematicians never did this a practice. It's

08:01.600 --> 08:06.160
just a specification. This is how they like to write things down. When they want to calculate it,

08:06.160 --> 08:12.080
they still have to go down and do it sequentially step by step. Just mathematics is defined in such

08:12.080 --> 08:17.840
a way as if you could upload this to some supernatural being or some grad student who is

08:17.840 --> 08:24.160
going to do the infinitely many calculations. And Goethe took this specification of mathematics

08:24.160 --> 08:28.160
and he found out that when you have this stateless mathematics, you can, for instance,

08:28.160 --> 08:33.920
define self referential statements that change that choose value depending on the statement itself.

08:34.560 --> 08:39.680
And this recurrence leads can lead to a contradiction in the state itself. So you basically

08:39.680 --> 08:45.200
get two statements which say I am wrong. And if by referring to itself, it changes its own

08:45.200 --> 08:51.040
truth value. So if mathematics is stateless, you will now run into a conflict. In a computational

08:51.040 --> 08:54.800
system, that's not a big problem. Your computer is not going to crash. If you write it down the

08:54.800 --> 08:59.520
right bay, it just happens is that your truth value fluctuates in every execution step. It's not

08:59.520 --> 09:05.360
going to converge. But this is not the real truth. Truth is something that doesn't change

09:05.360 --> 09:12.560
when you call the function again. So what's going on here? And I think what Goethe has

09:12.640 --> 09:17.760
discovered is that classical mathematics doesn't work. What you cannot build is any kind of

09:17.760 --> 09:22.560
mathematics doesn't allow you to build a machine, a hypothetical abstract machine, any kind of

09:22.560 --> 09:26.480
universe that runs the semantics of the classical mathematics without crashing.

09:27.360 --> 09:34.080
Yeah, but it kind of seems like, okay, we're going to believe Goethe's use of mathematics

09:34.080 --> 09:39.840
to prove that mathematics is flawed. Like there seems to be almost an inherent contradiction

09:39.840 --> 09:45.360
in there. Like you either believe mathematics, and thus you believe Goethe's proof of some

09:45.360 --> 09:51.760
specific limitations on computational systems, right? Or you believe that somehow mathematics

09:51.760 --> 09:55.520
is flawed, in which case you can't trust the proof that mathematics is flawed.

09:57.200 --> 10:02.080
I think Goethe's conclusion was that there is something fundamentally going wrong, that there

10:02.080 --> 10:07.760
might be an inability of mathematics to describe reality. And if you believe that truth is real

10:07.760 --> 10:12.800
and it exists independently of the procedure by which you calculate it, then this seems to be

10:12.800 --> 10:17.520
plausible. And it was also the conclusions a lot of philosophers have drawn from this,

10:17.520 --> 10:22.000
which basically read Goethe's proof and concluded that mathematicians have admitted that their

10:22.960 --> 10:27.920
arcane techniques are important to describe reality. And therefore, philosophers who don't

10:27.920 --> 10:34.480
understand mathematics have a clear advantage. Of course, this is not the conclusion. Instead,

10:34.480 --> 10:40.880
what turns out is that if you just skip or if you drop the original classical notation or

10:40.880 --> 10:45.920
as understanding of mathematics and replace it by computation, basically we say,

10:45.920 --> 10:50.560
truth is what you calculate with the following procedure. And you can define any kind of

10:50.560 --> 10:54.400
procedure that you want. You just have to make sure that it converges to some kind of value in

10:54.400 --> 10:59.120
the way that you want. Then you resolve your problem. It's just that you lose your notion that

10:59.120 --> 11:04.320
truth is independent of that procedure. And so in some sense, the classical mathematics is a

11:04.320 --> 11:08.240
specification that cannot be computed. From the perspective of computer scientists,

11:08.240 --> 11:12.880
this happens all the time. Some customer wants you to build something that cannot be built,

11:12.880 --> 11:16.400
and you have just proven that it cannot be built. Right? But it doesn't mean that you

11:16.400 --> 11:22.160
cannot build something useful. And I think that Penrose believed that our brain is actually doing

11:22.160 --> 11:28.000
these infinite things. And it's not. When we reason about infinity, we are not actually reasoning

11:28.000 --> 11:33.280
about infinitely many steps. What we do is we create a symbol, and then we do very finite

11:33.280 --> 11:38.240
computations over that symbol. But we cannot construct infinity. We cannot build it. We

11:38.240 --> 11:44.320
cannot go there from scratch and write down some clever automaton that produces an infinity for you.

11:45.440 --> 11:49.840
I was recently browsing Penrose's book, The Road to Reality, and I would say that, I mean,

11:49.840 --> 11:54.800
I don't know that much about physics, but the chapters were really interesting. They're talking

11:54.800 --> 12:01.040
about surfaces and manifolds and symmetries and fiber bundles and gauges and wave functions,

12:01.040 --> 12:05.920
calculus, matrix theory, and even computation. I mean, almost all of the discussion was on

12:05.920 --> 12:10.800
mathematical modeling at different levels of description or emergence, if you will. And

12:10.800 --> 12:15.840
in machine learning and AI, we are forever challenged by trying to get machines to model

12:15.840 --> 12:21.360
physical reality at different levels of description using an interoperable set of tools. So it seems

12:21.360 --> 12:26.480
increasingly true that we need machines that can learn descriptions and concepts at multiple levels

12:26.480 --> 12:30.640
if we're ever going to have AGI capable of understanding the world and learning novel

12:30.640 --> 12:36.480
semantic models. All of machine learning models today work by chopping up a Euclidean space into

12:36.480 --> 12:42.480
what is effectively a locality-sensitive lookup table. Very big one. And we need AGIs that can go

12:42.480 --> 12:48.000
far beyond this. It's got to be able to learn novel geometries beyond even what humans could

12:48.000 --> 12:52.480
have come up with and the ability to reason topologically and algebraically over those

12:52.480 --> 12:57.040
geometries. Something which I think you would agree is not happening with the current deep learning

12:57.040 --> 13:04.640
systems. Well, let's start out with the notion of geometry first. If you read Penrose's book,

13:04.640 --> 13:09.040
what you find is that this entire universe is geometric, which means it's made of

13:09.040 --> 13:15.680
continuous spaces in which things are happening. And if we actually look into the world deeply,

13:16.320 --> 13:21.520
quantum mechanics is not a geometric theory. The geometry only emerges approximately

13:21.520 --> 13:27.040
at the level of the space-time description. And it seems that geometry is actually the

13:27.040 --> 13:32.000
domain of too many parts to count. In reality, all the objects that we describe as surfaces,

13:32.000 --> 13:37.520
if you zoom in, are made of discrete parts like atoms and particles and so on. And these in turn

13:37.520 --> 13:43.040
are made out of things that have a finite resolution. And if we look into our computer

13:43.040 --> 13:47.440
programs, you can create stuff that looks continuous to us, but there's nothing continuous

13:47.440 --> 13:54.640
inside of our computer programs. And it turns out that the assumption of continuity requires

13:54.640 --> 14:00.160
that we partition the space into infinitely many parts. So now we are again running against

14:00.160 --> 14:06.080
that thing which GÃ¼rl has shown us as difficult. And it's not a big problem in practice because

14:06.080 --> 14:10.720
in practice, we never need to do these infinitely many things to produce a computer game with an

14:10.720 --> 14:17.280
arbitrary fidelity. We can make something that looks like space. But the space that we think in

14:17.280 --> 14:22.560
and so on is an approximation that our brain has discovered. It's a set of operators that converge

14:22.560 --> 14:28.080
in the limit. But the limit doesn't exist. It just, it happens that when you live in a world that is

14:28.080 --> 14:32.960
made of too many parts to count for almost everywhere where you look, you need to find these operators

14:32.960 --> 14:37.120
that converge in the limit. And the set of operators that happens to converge in the limit

14:37.120 --> 14:43.280
and is still computable. This is what we call geometry. And to use these uncomputable geometric

14:43.280 --> 14:48.480
approximations for macroscopic physics like Newtonian mechanics is completely fine. You're

14:48.480 --> 14:52.480
just going to compute it up to a certain digit and then this is good. But it's a problem for

14:52.480 --> 14:58.160
foundational physics. Because if it turns out that you cannot take a language that actually

14:58.160 --> 15:02.880
computes infinities, if you cannot construct your language, then you cannot write a universe in it.

15:03.200 --> 15:07.920
So our universe is not written in continuous language, but Penrose universe is.

15:09.840 --> 15:13.840
This doesn't mean that geometry is full. We need this to describe the world of too many parts to

15:13.840 --> 15:18.080
count. But we do this via computational approximations. Our brain does the same.

15:19.280 --> 15:24.960
So let me ask you this then because we come across kind of the infinities a couple of times. And I

15:24.960 --> 15:29.680
know that you placed an emphasis on constructive mathematics. So of course, you and all of us,

15:30.240 --> 15:35.280
you know, except let's say the existence of potential infinities, you know, algorithms that

15:35.280 --> 15:39.840
you can sit there and just keep calculating for as long as you want and get kind of more digits.

15:39.840 --> 15:46.000
But it's really around actual infinities that we seem to be running into problems. So let me ask

15:46.000 --> 15:51.120
this this first question here, really leading up to some computational questions, which is,

15:51.920 --> 15:58.720
can the universe, can our actual universe that we're in right now be actually infinite

15:58.720 --> 15:59.840
in spatial extent?

16:02.320 --> 16:06.480
A problem is that it can have unboundedness in the sense that you have a computation that

16:06.480 --> 16:13.120
doesn't stop giving your results. But you cannot take the last result of such a computation and go

16:13.120 --> 16:18.080
to the next step. You cannot have a computation that relies on knowing the last digit of pi

16:18.080 --> 16:22.960
before it goes to the next step. In the sense that you don't have an infinity. But the infinities

16:22.960 --> 16:27.120
are about the conclusion of such a function. It means that you actually run this function to the

16:27.120 --> 16:31.920
end and then do something with the result. Unboundedness is different in the sense that

16:31.920 --> 16:36.080
you will always get something new that you didn't expect that they cannot predict.

16:36.080 --> 16:42.400
But it's just going on and on without this end. And I think it's completely conceivable that our

16:42.400 --> 16:49.360
universe is in this class of systems in the sense that it doesn't end. But it doesn't mean that there

16:49.360 --> 16:55.040
is anything that gives you the result of an infinite computation. Because if that was the case,

16:55.040 --> 17:00.160
then it could not be expressed in any language. It also means if something cannot be expressed

17:00.160 --> 17:05.360
in any language, that you cannot actually properly think about it. Because when you think you need

17:05.360 --> 17:09.920
to think in some kind of language, not in English, but in some kind of language of sort or in a

17:09.920 --> 17:15.040
mathematical language that doesn't have contradictions. And what Goethe has shown is that the language

17:15.040 --> 17:22.080
that he hoped to reason in about infinities breaks that it has contradictions in it. That at some

17:22.080 --> 17:27.840
point, it blows itself apart. So the languages that we can build are only those in which we have

17:27.840 --> 17:32.000
to assume that infinities cannot be built. So infinity, in this sense, is meaningless.

17:32.560 --> 17:35.040
Because we cannot make it in any kind of language.

17:36.000 --> 17:42.480
So the thing is, though, I'm not limiting what the universe is capable of based on human mental

17:42.480 --> 17:49.520
and linguistic limitations or even mathematical limitations. I'm asking you if it's possible

17:49.520 --> 17:56.480
for this universe that we're in to ontically be right now actually infinite in spatial extent.

17:59.360 --> 18:04.640
The thing is that you try to make a reference to something that you cannot observe, that cannot

18:04.640 --> 18:10.880
conceive of other than making a model in some kind of language. And to have that model make sense,

18:10.880 --> 18:16.320
the language needs to work. Right? Otherwise, you are just maybe in some kind of delusional thing.

18:16.960 --> 18:21.200
And we can construct delusional things. We can construct languages that have bugs that we cannot

18:21.200 --> 18:26.480
see. But if we use a language that has bugs in it that we cannot see and we cannot repair them,

18:26.480 --> 18:30.960
then this means that the stuff that we express in the language is not meaningful. Right? We have to

18:30.960 --> 18:35.040
use a different language that has maybe the same expressive power but doesn't have these bugs.

18:35.600 --> 18:41.200
But now if you try to think about the universe in the language that allows you to imagine that

18:41.200 --> 18:45.920
the universe is literally infinite, rather than very, very, very big and much bigger than you

18:45.920 --> 18:51.360
can imagine and not ending, which is for all means and purposes almost the same thing. Right?

18:52.480 --> 18:58.560
Then if you do this other thing, then your thought doesn't mean anything. So it's basically you cannot

18:58.560 --> 19:03.440
properly express the idea in your own mind without running into contradictions that the

19:03.440 --> 19:09.120
universe is infinite in the sense that such a universe could exist. Okay, so you're basically

19:09.120 --> 19:12.800
following that. That's the issue. Basically, I cannot think that the universe is infinite. I cannot

19:12.800 --> 19:18.720
express this. That's my issue. Okay, fine. So you're basically saying that the English that I

19:18.720 --> 19:26.000
used just a minute or so ago just is not coherent or not conceivable. It's not something that you

19:26.000 --> 19:28.960
want to. But the underlying thing behind the English, right? English is not designed to be

19:28.960 --> 19:34.720
coherent. It's designed to be disambiguating. It's designed to be unprincipled to allow us to

19:34.720 --> 19:40.400
express things vaguely and not break. But if you think really, really deeply and really exactly,

19:40.400 --> 19:44.720
then the question is, what kind of model is your mind building? At which point is there just some

19:44.720 --> 19:50.800
kind of noisy nabler that you're pointing at without actually decomposing it and anything that would

19:50.800 --> 20:01.120
make sense? Okay. And so the lack of really the ability to conceive or for actual infinities to

20:01.120 --> 20:06.240
ontically exist in some sense, if we just deny all that, so we're really just stuck with,

20:06.880 --> 20:12.160
all right, we've got finite everything, discrete everything. There's no such thing as a continuum.

20:12.800 --> 20:19.040
There's no such thing as actual infinite spatial extent, etc. That's really the world that you're

20:19.600 --> 20:23.600
proposing here, right? That everything is constructed from at the end of the day,

20:23.600 --> 20:29.360
finite, discrete kind of elements. So if we... Yeah, you can imagine that your mind is a library

20:29.360 --> 20:33.440
of functions in a way, and these functions are doing jobs. And on the bouts of the box,

20:33.440 --> 20:38.240
you write down what these functions are doing. And you construct a box that this, in this box,

20:38.240 --> 20:44.320
there is an infinity between, for instance, a continuum between two points. And then you open

20:44.320 --> 20:49.920
up the box and look at what's actually inside of the box. And you realize it's just a lot of small

20:49.920 --> 20:54.320
steps. And it's designed in such a way that you can, if you want to have more steps, it's going to

20:54.320 --> 20:59.360
give you more steps if you zoom in, right? And it's totally doing, apparently, what's written down

20:59.360 --> 21:04.080
on the box. But if you look very closely, realize, oh no, the thing that is written down on the box

21:04.080 --> 21:08.080
that you have written down on the box cannot actually be in there. You can prove that it cannot

21:08.080 --> 21:12.000
be in there. It must be something else that's in there that is doing most of the work of what you've

21:12.000 --> 21:16.000
written down. So what you should actually be doing, I think, if you are interested in how things

21:16.000 --> 21:22.320
actually work, write on the box what it's actually doing, which means it's going to subdivide or

21:22.320 --> 21:26.320
any interval with any resolution you want as long as you can afford it.

21:27.280 --> 21:33.920
Okay. One mystery, if you will, for me, and I'm hoping you can help me understand this, is that

21:34.720 --> 21:41.920
all of the standard models for physics that we have today, they do have in them these continuous,

21:41.920 --> 21:47.440
you know, for example, symmetries that are rotational symmetry or things like that. They're

21:47.440 --> 21:55.440
built off of positing continuums with continuous waves, lots of continuities and infinities,

21:55.440 --> 22:00.480
at least in the mathematical descriptions. Except for quantum mechanics, right?

22:00.480 --> 22:05.520
Right. And I think based on what you've been saying, you would say that those are artifacts or

22:05.520 --> 22:12.720
properties of our mathematical descriptions of reality, but they're not actually extant in

22:12.720 --> 22:22.880
reality. And my mystery there is why do those continuous and mathematical maybe flawed and

22:22.880 --> 22:31.120
inconsistent with infinities all over the place descriptions work so well for describing phenomenon

22:31.120 --> 22:36.080
at different levels? If everything at the end of the day, you know, if we just looked at high enough

22:36.080 --> 22:42.800
energy and small enough resolution, we'd see kind of the grid and, you know, all the discrete

22:42.800 --> 22:50.480
effects and rotation happening kind of in little tiny, very small but not infinitesimal degrees.

22:50.480 --> 22:56.000
You know, why does all this continuous infinity based mathematics work so well? What is the

22:56.000 --> 23:00.160
explanation for the unreasonable effectiveness of that kind of mathematics?

23:00.960 --> 23:08.080
The easiest answer is that the world in which we live in is made of extremely small parts.

23:08.080 --> 23:14.160
And we could not exist if that world was not made of that many small parts. So for instance,

23:14.160 --> 23:19.680
you want to have a momentum for particles that are almost continuous. So you can address the

23:19.680 --> 23:24.480
space with high resolution because the momentum is what tells you where information comes from

23:24.960 --> 23:30.160
in the universe, basically the direction of where from which information reaches you and so on.

23:30.160 --> 23:35.920
If that would be very coarse, then the complexity that you could build would probably be far lower.

23:35.920 --> 23:42.160
And we consist of so many parts that when you look down, it's uncountably many for all practical

23:42.160 --> 23:47.200
purposes. So the mathematics that we need to describe the world that we are in that we need

23:47.200 --> 23:52.880
to model are mostly not in the realm of countable numbers. The countable numbers only play a role

23:52.960 --> 23:59.360
when we are looking at very few microscopic things. As soon as we leave this domain of a few apples

23:59.360 --> 24:06.160
on our table, we almost instantly drop in this realm where we just need to switch to a continuous

24:06.160 --> 24:12.080
description of things. And this is completely fine for most of our history. When we did physics,

24:12.080 --> 24:18.880
we never zoomed in that heart. And even now, when we really need to zoom at the level where

24:18.880 --> 24:24.640
the plank length matters and the resolution of the universe becomes visible. And it's of course not

24:24.640 --> 24:29.360
some Euclidean lattice, some grid that you can see. It's just that at this level, you no longer

24:29.360 --> 24:36.880
have space. I wanted to move matters back over to some of the happenings in the world of large

24:36.880 --> 24:43.040
language models and deep learning and so on. And first, quick fire question. I honestly,

24:43.040 --> 24:48.000
you're a bit of an enigma to me, Joshua, because obviously I've read some of your research and

24:48.000 --> 24:54.320
you seem like a hybrid guy to me. You know Ben Goetzel very well, for example, but you're also

24:54.320 --> 25:01.200
hugely into the hype train on the connectionism. For example, you criticised Gary Marcus's

25:01.200 --> 25:04.640
article. So the first question is, are you a symbolist or a connectionist?

25:05.440 --> 25:12.160
I'm neither. The thing is that I hate deep learning as the best of us. Deep learning is ugly. It's

25:12.160 --> 25:19.280
brutalist. It's a few very simple algorithms that are blown up to the max. But I cannot prove

25:19.280 --> 25:25.840
that these algorithms do not converge to what we want them to converge to. It's maybe not

25:25.840 --> 25:31.280
elegant, but it works. And the solution to problems with deep learning so far has always

25:31.280 --> 25:38.880
been to use more deep learning, not less. So what upsets me about Gary Marcus argument is not that

25:38.960 --> 25:44.000
I'm not sympathetic to what he's trying to push it. I'd like to build models that are more elegant,

25:44.000 --> 25:52.160
more sparse and so on. But in the past, all these elegant sparse models have been left in the dust

25:52.160 --> 25:57.360
by just using more deep learning. And we can also see when we zoom out a little bit that there is

25:57.360 --> 26:02.800
not an obvious limit to deep learning itself, because deep learning is not just the algorithms.

26:02.800 --> 26:07.600
Deep learning is a programming paradigm. It's differentiable programming. It basically means

26:07.600 --> 26:14.160
that you express everything with approximately continuous numbers, and you use algorithms that

26:14.160 --> 26:22.160
converge business at certain ranges. And when it doesn't converge, then you just tweak it and you

26:22.160 --> 26:27.920
introduce a different architecture, which is some kind of discrete operations that you do on these

26:27.920 --> 26:32.960
continuous numbers and so on. You just patch it, you write your programs slightly differently,

26:32.960 --> 26:37.520
and you can automate the search for the program. And the people who do deep learning are not also

26:37.520 --> 26:42.080
docs in the sense that they say, oh my God, symbolic structures are not allowed. I cannot use

26:42.080 --> 26:47.600
a Python script in here rather than just a TensorFlow. This is not what's happening. It's

26:47.600 --> 26:52.400
also not that they are constrained to any kind of thing that will use whatever is working.

26:52.400 --> 26:56.960
And what we see is that the end-to-end train systems are going more and more powerful,

26:56.960 --> 27:01.200
and rather than sitting there by hand and tinkering and finding a solution,

27:01.840 --> 27:06.720
we can just use a system that is tinkering automatically through a dramatically larger

27:06.720 --> 27:12.720
space than we would ever be able to explore by trying all sorts of algorithms. So when we look

27:12.720 --> 27:18.480
at Gary Marcus' articles like his deep learning is hitting a wall and so on, and you look what he's

27:18.480 --> 27:22.880
actually giving as arguments, the arguments are not very good. He gives us an example,

27:22.880 --> 27:29.600
the NetHack challenge. NetHack is a game which has a very large horizon because you basically

27:29.600 --> 27:35.120
have only one life. You need to explore a very deep labyrinth and you need to plan pretty far ahead

27:35.120 --> 27:41.120
with what you're doing. And so it's something that is difficult to discover this right solution

27:41.120 --> 27:47.680
with a deep learning model that has no prior ideas about what it's doing. Because it takes us

27:47.680 --> 27:52.000
very, very long until you get the necessary feedback to learn about your actions. And people

27:52.000 --> 27:56.480
are relatively good at learning this because they have so many ideas about what the situation is that

27:56.480 --> 28:01.680
they're in. There's so many priors from our world interaction and from other games that we have played

28:01.680 --> 28:06.960
that we can bring to the tasks. So the current winner of this is the symbolic solution.

28:08.080 --> 28:14.480
And the symbolic solution that Gary Marcus gives as a proof that symbolic methods are still ahead

28:14.480 --> 28:20.320
of deep learning things. In a single case, not like he has a big array of tasks where they are

28:20.320 --> 28:26.000
superior, it's just two students who have written a program that is made of lots and lots of events.

28:26.000 --> 28:30.960
This is just a big hack. This is not some symbolic learning algorithm that does something novel,

28:30.960 --> 28:36.880
hybrid or whatever. No, this is just a script. And is Gary Marcus seriously proposing, oh my

28:36.880 --> 28:40.880
god, deep learning models are limited and we need to replace them with more scripts?

28:41.760 --> 28:49.040
This is not a good argument. Yeah. So I think maybe, and look, I get that there are these kind

28:49.040 --> 28:54.080
of two competing camps and they maybe go after each other with some. No, they don't. This is only on

28:54.080 --> 28:59.760
Twitter. There is, there are no competing camps. It's Yandekun is not also docs in the sense that

28:59.760 --> 29:03.440
he believed you need to use this argument, all the other arguments are impure and flawed.

29:04.880 --> 29:10.400
His brand is to build systems that work. And if one of his people comes up with something that

29:10.400 --> 29:15.680
works better than what he came up with, you probably praise him for that and let him go on.

29:17.120 --> 29:24.080
Yeah, sure. But there's absolutely, however, there is, you know, there is, let's say, momentum

29:24.080 --> 29:29.280
and hardware lotteries and paradigms that kind of reinforce themselves. And to an extent,

29:29.360 --> 29:34.400
they can strangle off, you know, resources that maybe we like, we shouldn't be investing all

29:34.400 --> 29:40.640
our eggs in one basket. We shouldn't be pouring, you know, the 99% of research funding necessarily

29:40.640 --> 29:46.240
down, down deep learning. And I think that's kind of the problem that, that these paradigms cause.

29:46.240 --> 29:52.000
But I want to get back to something you said, which is a good point. It's, I think that's an

29:52.000 --> 29:56.800
important point. I think that in absolute terms, the other approaches get more money than they did

29:56.800 --> 30:02.240
before. It's not that we have a funding stop, as we had at some point, a return funding stop for

30:02.240 --> 30:07.280
Neural Networks. And Marvin Minsky wrote a book where he saw he had proven that the Neural Networks

30:07.280 --> 30:12.400
cannot converge over multiple layers, press up drones cannot earn X or and so on. Right. Minsky

30:12.400 --> 30:17.440
was wrong. People found a way around this. But at this time, there was so little funding that

30:17.440 --> 30:23.200
this cutoff mattered. And at the moment, if you want to do something that has AI and Adline,

30:23.200 --> 30:27.440
the chance that you get it funded and whether what paradigm you're doing is greater than ever.

30:27.440 --> 30:31.760
So the absolute amount of funds that goes into any kind of paradigm that you want to work on

30:31.760 --> 30:36.960
is greater than ever. And the reason why the majority of funds goes into very few paradigms is

30:36.960 --> 30:42.080
because these are the things that work in industrial applications. There is no other

30:42.080 --> 30:46.240
algorithm that is able to learn from scratch how to translate between arbitrary languages and

30:46.240 --> 30:51.600
generate stories and draw pre pictures for you. This is the only game in town at the moment,

30:51.600 --> 30:56.320
the only class of algorithm that converges over all these many domains. And people are looking

30:56.320 --> 31:00.800
for better alternatives. And yes, we are in a bubble, because of course, they're looking mostly

31:00.800 --> 31:05.520
where things already were. You have hardware that works, you have libraries that work and so on.

31:05.520 --> 31:10.480
It's hard to get out of that bubble. That is true. And it's always good to push for alternatives

31:10.480 --> 31:17.440
and so on. But I don't think that we should be in a panic and say, Oh, my God, there is something

31:17.440 --> 31:23.440
politically wrong. I suspect that by and large, the forces of the markets and the forces of the

31:23.440 --> 31:28.320
academic researchers that want to explore alternative are pushing in the right direction already.

31:29.840 --> 31:34.160
Yeah, I mean, fair enough. And, you know, you could be right. And there may not be that much

31:34.160 --> 31:40.880
of an imbalance. But I want to get back to one technical thing you said. Yes, it seems apparent

31:40.880 --> 31:46.320
that, let's say, what deep learning is doing is this this differentiable program search,

31:46.400 --> 31:51.760
if you will. And a question I have about that is if we imagine the space of all possible programs,

31:52.800 --> 31:57.760
that, you know, requiring that we're doing a differentiable search is certainly going to skew

31:57.760 --> 32:05.360
that sample space that may even cut off programs in that space that can't be discovered easily by

32:05.360 --> 32:10.080
differentiable search. So I'm wondering, doesn't that leave open the possibility that other

32:10.080 --> 32:15.680
algorithms that are more discreet in nature, say evolutionary algorithms or discrete program

32:15.680 --> 32:21.600
search or whatever, they may have access to a different subspace of the space of all programs

32:21.600 --> 32:26.000
that aren't easily accessible by differentiable paradigms. Is that true?

32:28.720 --> 32:32.960
The question is, how do you find it? How do you find these algorithms to manipulate the

32:32.960 --> 32:38.880
discrete things? I agree that when you have a perceptual model that is modeling everything

32:39.520 --> 32:46.560
with chains or sums over real numbers, and a few non-algebraic throne, and you get

32:46.560 --> 32:50.320
characteristic artifacts. For instance, in the generative models, you often have the problem,

32:50.320 --> 32:55.280
and you try to model a person with glasses or without glasses, that because the model thinks

32:55.280 --> 33:00.240
that these features are somewhat continuous, you often run into the situation that you get areas

33:00.240 --> 33:05.040
in the generative model, where the glasses are half materialized, and it looks always very weird.

33:05.040 --> 33:11.200
And you have these strange things where reality has a discontinuity, but your model has permissible

33:11.200 --> 33:17.120
states where you are in the middle of the discontinuity, and you try to generate something

33:17.120 --> 33:21.200
that cannot exist. You want your model to be structured such a way ideally that every

33:21.200 --> 33:27.440
model configuration corresponds to a world configuration. And this is not necessarily the

33:27.440 --> 33:32.080
case with many of the deep learning models. And what the deep learning models, as you train them

33:32.080 --> 33:37.360
harder, typically tend to do is that they squeeze these impermeasurable areas until you are very

33:37.360 --> 33:42.400
unlikely to end up in them. And it's probably possible to get them to implement filters and

33:42.400 --> 33:48.080
all sorts of tricks. But what you can also do is you can combine this with some kind of discrete

33:48.080 --> 33:53.600
machine. And then what you do is you learn how to use this. So this deep learning network is not

33:54.160 --> 33:59.280
interacting with the world directly, but it learns how to use an architecture that does that.

34:00.080 --> 34:04.640
So for instance, instead of training a neural network to do numerical calculations,

34:04.640 --> 34:10.160
you can train it to use a numerical calculator. And in this way, it can become very sparse again.

34:10.720 --> 34:16.080
Right? So there's not an obvious limit to that I can see where I can prove to the deep learning

34:16.080 --> 34:20.080
people, oh, here's where you should stop deep learning, because they can just combine their

34:20.080 --> 34:24.560
deep learning approach with other approaches and use the deep learning system to remote control

34:24.560 --> 34:28.400
this. And it turns out when we reason and so on, even when we do discrete reasoning,

34:28.400 --> 34:34.000
that the steps that we assemble it to each other are heuristics that require some kind

34:34.000 --> 34:38.720
of probabilistic element. Right? So when we form a sort that when the sort is made of very

34:38.720 --> 34:44.640
discrete elements, the search for that sort is some kind of deep learning process that is happening.

34:44.640 --> 34:51.760
Right? And when we make the pool, we do this, we emulate a discrete reasoning. But of course,

34:51.760 --> 34:56.880
we can combine this and we can get the neural network to learn how to perform the discrete

34:56.880 --> 35:02.400
operations. There's a certain thing that I would like to see, which is something like a more sparse

35:02.400 --> 35:09.600
language of thought. When we are looking at deep learning models, there's a phenomenon that people

35:09.600 --> 35:15.520
are sometimes observing, which they call grocking. That is, you train the model and your model gets

35:15.520 --> 35:20.320
better and better. And then it overfits, which means it gets very good at the training data,

35:20.320 --> 35:24.960
but it gets very bad on the real world at things that it hasn't seen before, like a person in

35:24.960 --> 35:29.920
psychedelics was able to explain everything in the past, but is no longer able to perform well in

35:29.920 --> 35:33.920
the future because they're overfitting. They basically fit the curve too closely to the data

35:33.920 --> 35:38.800
that I've seen. And there are many tricks in deep learning to go around this overfitting to make

35:38.800 --> 35:43.440
sure that this doesn't happen. And people try to avoid it. And then what they discovered is when

35:43.440 --> 35:47.200
you take this overfit model, you train it more and more and more and more. At some point, it

35:47.200 --> 35:53.040
sometimes clicks and it gets much better than ever before. And there is a question if there's

35:53.120 --> 35:58.960
something that we're doing wrong in deep learning. For instance, when you think about how people

35:58.960 --> 36:05.120
learn, they learn very different from GPT-3. People first learn by pointing at stuff that

36:05.120 --> 36:09.200
thinks that are relevant to them, that they can eat, that they can hurt, that can hurt them,

36:09.200 --> 36:14.000
or that they find pleasant and so on. They, that they can feel that they can, they have contrast

36:14.000 --> 36:18.640
on it that are salient to them. And so you start out with learning these semantics based on the

36:18.640 --> 36:24.000
saliency and relevance that you have. And then when you learn language, you learn basic syntax,

36:24.000 --> 36:28.720
how to put things together. And in the long tail of the syntax, you learn style, how to express

36:28.720 --> 36:35.680
things with new ones and so on. And with GPT-3, it's the opposite. You first learn style, right?

36:35.680 --> 36:40.880
And then you learn syntax as the regularities in the style. And the semantics is the long tail of

36:40.880 --> 36:46.320
that. And to make that happen, you need to learn much, much more. You need to have more training

36:46.320 --> 36:52.240
data and so on. Maybe there's a way in which we can reverse the order and basically get it to

36:52.240 --> 36:57.200
start out with relevance, to build a curriculum where you first get very sparse regularities,

36:57.200 --> 37:02.560
where it clicks into place. You always make sure that you can handle it with very limited resources

37:02.560 --> 37:08.640
and only see the style and the niceties and the nuances as the far extensions of these very sparse

37:08.640 --> 37:14.320
concise models that have very big predictive power. Yeah. I mean, on that, I mean, the Grocking

37:14.320 --> 37:19.200
paper was very interesting. And a lot of these large language model fans always cite that very,

37:19.200 --> 37:23.280
very quickly when you have a conversation with them. But there is a problem with machine learning

37:23.280 --> 37:27.840
in general, which is that there is, as you said, there's a spectrum of correlations and almost

37:27.840 --> 37:34.000
all of them are spurious. And on one side of that spectrum, you have the idealized features you

37:34.000 --> 37:38.000
actually want it to learn, which will generalize after distribution. And then, of course, if you

37:38.000 --> 37:43.680
go down that spectrum, you pick up on all sorts of very spurious correlations that just happen

37:43.680 --> 37:48.400
to generalize very well. And if you tell the models not to use those spurious correlations,

37:48.400 --> 37:53.280
that the performance of the model will go down. But I want to just move a little bit over to

37:54.800 --> 37:59.680
Yasaman Rezegi's paper. I don't know whether you saw that, but she showed that the performance of

37:59.680 --> 38:05.040
large language models for arithmetic tasks are linearly correlated to the term frequency and

38:05.040 --> 38:10.160
the training corpus, suggesting that they are memorizing the data set, which presumably you

38:10.160 --> 38:15.840
would agree with. And Google has recently released this 540 billion parameter language model called

38:15.840 --> 38:21.840
PAM, which interestingly does extremely well on, for example, some of the Google big bench tasks,

38:21.840 --> 38:27.120
such as the conceptual combinations task, which is one of them, which tests for compositionality,

38:27.120 --> 38:31.840
which we'll talk about in a minute. But compositionality is when you can take constituents from

38:31.840 --> 38:36.400
the prompt and compose them together to form the answer. Now, it's tempting to jump to the

38:36.480 --> 38:41.360
conclusion that these models are starting to magically reason at scale along the lines that

38:41.360 --> 38:46.320
you were just discussing. But I still think there's plenty of opportunities for shortcut learning,

38:46.320 --> 38:51.120
you know, by which I mean these spurious correlations, given the brittle interface of an

38:51.120 --> 38:57.600
autoregressive GPT style language model with these human designed benchmarks. Would you agree with that?

38:57.600 --> 39:07.600
Yeah. When I started my own career in computer science in the 90s, I was in New Zealand, and the

39:07.600 --> 39:13.520
prof here in Britain realized that I was bored in class. So he took me out of the class and in

39:13.520 --> 39:18.480
his lab, and he gave me the task to discover grammatical structure and an unknown language from

39:18.480 --> 39:24.080
scratch and left me pretty much to my own devices on how to do this. So the unknown language I picked

39:24.080 --> 39:28.640
was English, was just unknown to the computer, but was the easiest one to get a corpus for,

39:28.640 --> 39:32.560
and they gave me the largest computer they had. It has two gigabytes of RAM, and I did

39:32.560 --> 39:37.600
in memory compression with C and so on, and tried to do statistics, and I quickly realized NREM

39:37.600 --> 39:44.560
statistics don't work because of too many words in between. So unlike vision tasks where

39:44.560 --> 39:50.400
confnets have a useful prior by thinking that adjacent pixels also relate to symbolically

39:50.400 --> 39:54.720
related information, right? So adjacency in images is a very good predictor for thematic

39:54.720 --> 39:59.920
relatedness. It doesn't really work in NLP. So the transformer was discovered in natural language

39:59.920 --> 40:06.560
processing for that reason, because you cannot use direct adjacency very well. And so I realized

40:06.560 --> 40:13.360
I cannot use NREM, which depend on direct adjacency between words. And so I first of all used ordered

40:13.360 --> 40:18.960
pairs of words and tried to find correlations between pairs and then find a mutual information

40:18.960 --> 40:24.160
tree that would give me the best prediction over the structure of the sentence for all the

40:24.160 --> 40:29.120
sentences that I would have in my corpus. And indeed this correlated to structure. And I realized

40:29.120 --> 40:33.680
this is going to not just give me grammar, but it's also going to give me semantics

40:33.680 --> 40:39.280
if I can more deep statistics. But I will need something not just ordered pairs,

40:39.280 --> 40:44.480
but I need to have something like force order models. But to do the statistics, even in memory,

40:44.480 --> 40:48.960
with clever and memory compression and many tricks that I did, I could not do full statistics on

40:48.960 --> 40:56.800
this. So what I realized I had to do was that I do multiple passes. And at first I discard almost

40:56.800 --> 41:01.680
all the information. I only pick out the most salient things. And then my time was over in

41:01.680 --> 41:07.280
this lab. And I went back to Germany and never reviewed this area of research again. But what

41:07.280 --> 41:10.960
I had realized is to make progress and need to make statistics over what I need to make the

41:10.960 --> 41:16.320
statistics over. And the very principled base, I need to learn what I have to learn.

41:17.600 --> 41:23.600
And they didn't pay attention to this domain at all. And I also missed the 2017 transformer paper

41:23.600 --> 41:28.320
and its relevance. It was only when GPT-2 came out that I realized, oh my god, they did this.

41:28.320 --> 41:34.880
They did statistics over the statistics. And it's still not the right solution, I think. It's not

41:34.880 --> 41:38.800
the way in which our brain is doing it. It's some brute force shortcut. Well, for instance,

41:38.880 --> 41:42.000
the individual attention heads are not correlated with each other, but in reality,

41:42.000 --> 41:47.680
they are. We have this in reality, our attention heads are integrated into one model of what's

41:47.680 --> 41:53.600
going on. And it's not that we have an attention net on every layer that just pays attention to

41:53.600 --> 41:59.120
what's happening in the lower layer. It's much more clever in our own mind. And this thing is

41:59.120 --> 42:03.360
active. We single out things in reality to research, which book we need to take out of the

42:03.360 --> 42:07.840
shelf to update our working memory context so we are able to interpret the current sentence

42:07.840 --> 42:12.800
that we don't understand. And so we always go for saliency when we read something that doesn't

42:12.800 --> 42:21.040
make sense. At least my mind works like this. I discard it. I will not stop until it makes sense,

42:21.040 --> 42:26.480
or I will have to go to some preliminary. I will not accept some kind of vague statistical

42:26.480 --> 42:31.360
approximation of what I read. Keep this as an intermediary stage in my mind until the hope

42:31.360 --> 42:36.480
that eventually converges. It's a completely different learning paradigm. When we teach our

42:36.480 --> 42:41.760
children arithmetic, it's not that we show them lots of very long mass textbooks and hope that

42:41.760 --> 42:46.400
initially it will not make any sense to them. But as they reread them again and again with many

42:46.400 --> 42:51.280
samples, eventually it will click and they will converge on arithmetic. No, this is not how it

42:51.280 --> 42:55.440
works. You start this giving them extremely simple things and say, in these extremely simple things,

42:55.440 --> 42:59.600
there is structure that you can fully understand. Now go and find the structure that you fully

42:59.600 --> 43:04.400
understand. Once you've done it, you make this a little bit more complicated for you. This is

43:04.400 --> 43:12.320
probably the paradigm that you could be exploring. I know, but the problem is it's incredibly

43:12.320 --> 43:15.680
deceptive when you have something which appears intelligence. Of course, the

43:16.400 --> 43:22.000
boundary of our perception of intelligence is a receding one. But I wanted to just get on to

43:22.640 --> 43:29.200
there are some incredible generative visual models like Dali and the disco diffusion.

43:29.680 --> 43:34.240
These models, I think, are going to revolutionize the creative profession. I've been

43:34.240 --> 43:38.960
playing with disco diffusion all day today. I've already ordered some prints to go on my wall.

43:38.960 --> 43:45.600
It's incredible. The two obvious settings where large language models might be successful

43:46.400 --> 43:51.680
are coding and information retrieval in my opinion. But let's take pause for thought. I've played

43:51.680 --> 43:57.600
with Codex and I'm resolutely sure that I wouldn't want to use it. I think code and knowledge are

43:57.600 --> 44:03.120
a different ballgame to art, which I think will be amazing. With Codex, there's an impedance mismatch

44:03.120 --> 44:07.680
between the process of generating the code and then debugging and running the code, which has

44:07.680 --> 44:12.800
euphemistically been framed as prompt engineering, or another term which I've just invented,

44:12.800 --> 44:18.560
retrospective development. I think it's easier to start again from scratch than fix broken code

44:18.560 --> 44:23.120
from a large language model. I mean, this is quite interesting. At Google, it already takes

44:23.120 --> 44:28.320
months to get any code checked into their mono repo because it's basically a bureaucracy because

44:28.320 --> 44:33.840
they needed to have gatekeeping after they decided to use a mono repo. Could you imagine

44:33.840 --> 44:38.880
how much bureaucracy there'd be if they allowed people to start checking in code, which was generated

44:38.880 --> 44:43.120
from an algorithm? Anyway, I think there's an exciting possible future for using these systems

44:43.120 --> 44:49.440
for information retrieval rather than the way that we go through and prune the results on a Google

44:49.440 --> 44:55.280
search. These models might just answer directly, but I hasten to think what that search UI would

44:55.280 --> 45:01.120
look like. Would its output be sclerotic or unadaptable? Would it be relevant to the query that

45:01.120 --> 45:07.920
I put in? Would its output even be true? Perhaps it will ask you to select what kind of truth you

45:07.920 --> 45:13.600
were looking for. Do you think these models would vitiate or spoil our society, or do you think they

45:13.600 --> 45:19.520
would actually enrich it? It's very hard to say. I think that from some perspective,

45:19.520 --> 45:25.840
our society is already maximally spoiled. Humans, as they live today, are basically

45:25.840 --> 45:31.040
locusts with opposable thumbs. This is not going to go on forever, this technological society.

45:31.040 --> 45:36.480
Here are, it seems to me, on some kind of Titanic that is going to hit the iceberg no matter what.

45:37.200 --> 45:42.080
And what basically should make us content is that the Titanic was the only place in the

45:42.160 --> 45:48.080
universe that has internet. And we are born on it, and we wouldn't have been born if there was no

45:48.080 --> 45:54.880
Titanic. We would not have been born in a sustainable ancestral society. In some sense,

45:54.880 --> 45:58.960
our society needs to reinvent itself. It's not really working right now. We don't know

45:58.960 --> 46:02.560
what the future is going to look like, and if it's going to be very technological,

46:02.560 --> 46:09.360
or if limit certain things, no idea what's going to happen. But if we think about how our

46:09.440 --> 46:15.680
current approaches work, if you want just to make programming better, I suspect that these tools can

46:15.680 --> 46:21.360
help. But they will be much more useful if you do not have to have this battle between a machine

46:21.360 --> 46:27.280
that doesn't really understand what you want. And instead, you have something that is working next

46:27.280 --> 46:33.680
to you. It's like, imagine you were working for some corporation and the corporation introduces

46:33.680 --> 46:38.160
some kind of planning tool that requires to do to jump through all sorts of hopes. And it turns

46:38.240 --> 46:42.960
out that the planning tool itself makes you more productive, but it makes work much less fun.

46:44.160 --> 46:47.520
It's still rational to use it. And everybody will hate it, but

46:48.640 --> 46:53.600
by and large, it will be used if it makes people 30 more productive. And everybody will feel there

46:53.600 --> 46:58.640
might be a must be a better solution, something that feels more organic. And so it could be that

46:58.640 --> 47:03.440
Codex is in this category that it makes mediocre programmers much more productive at producing

47:03.440 --> 47:08.240
boilerplate. But it's not just this, it's often able to find solutions very quickly,

47:08.240 --> 47:12.480
but you need to use a lot of Stack Overflow before you understand the new language or before you

47:13.120 --> 47:17.760
tease this new algorithm or part that you want to understand and so on. It just when it doesn't

47:17.760 --> 47:22.080
probably turn you into a better programmer, if that is your goal. But for your employer,

47:22.080 --> 47:26.080
maybe they don't care whether you're a better programmer, they just want you to turn out these

47:26.080 --> 47:30.720
pages of code and then they run this against the verifier and against the unit test and then are

47:30.720 --> 47:36.400
done, go to the next thing, right? So maybe it's not that important. But the systems that we would

47:36.400 --> 47:40.640
want, what would they look like? I think they need to know what they're doing. You want to have a

47:40.640 --> 47:45.680
program that is not just able to reproduce something very well in a given context, you want to

47:45.680 --> 47:51.360
understand the context as deeply as you do or better. So it needs to understand what kind of

47:51.360 --> 47:56.160
world it's operating in in the moment. And what itself is, what is it that it can do? What is

47:56.160 --> 48:01.440
that what needs to learn still? In some sense, you want systems that are sentient. And it's self

48:01.440 --> 48:06.160
like, oh my God, but it just means you have a learning system that's general enough to model

48:06.160 --> 48:11.360
in principle the entire universe. And this is not as outrageous as it sounds because

48:12.480 --> 48:19.200
Delhi is already dealing with two modalities, language and images. And we will get to video

48:19.200 --> 48:24.400
and we will get to audio connected and you see them early steps in this direction with the Socratic

48:24.400 --> 48:29.920
model of people, for instance. So I think that's almost inevitable that this generality will happen.

48:29.920 --> 48:33.520
And you will have to add a system to work in real time so it can discover itself.

48:35.600 --> 48:42.480
I think I think there's something really magic, though, about the creative process here. And also

48:43.200 --> 48:47.920
the prompt engineering is another thing we can talk about. But Kenneth Stanley once made this

48:47.920 --> 48:53.840
thing called Pick Breeder. And you could essentially distribute the selection of these images

48:53.840 --> 49:00.480
created with CPPNs, Compositional Pattern Producing Networks. And you would just get these beautiful

49:00.480 --> 49:06.320
images. They were they were so incredibly, incredibly diverse and interesting. So it's

49:06.320 --> 49:11.920
not that the algorithms were intelligent, there was something magic about the externalized process.

49:11.920 --> 49:15.840
And what's really interesting about these models like Dali, for example, is that creativity has

49:15.840 --> 49:21.920
been distilled down to a raw idea in your head, right? So for example, I might decide to mix

49:21.920 --> 49:26.080
the style of two artists and combine them with a new subject. And I want a black cat

49:26.720 --> 49:31.600
in front of Royal Holloway University in the style of cyberpunk. I've been doing that all day.

49:31.600 --> 49:35.920
And the technical process is now done for you. The only limit is your imagination. So just like

49:35.920 --> 49:40.480
Kenneth Stanley's Pick Breeder, creativity itself has now become this uber efficient and

49:40.480 --> 49:46.320
externalized process. I think it's unreal. But the thing is, like the reason I never thought GPT3

49:46.320 --> 49:51.760
was intelligent is because it can't be used non interactively. The magic must happen when it's

49:51.760 --> 49:58.800
used by humans interactively. Well, you can basically build a machine that is generating

49:58.800 --> 50:07.440
prompts for GPT3. So in principle, you can build a robot that has a vision to text module. And that

50:07.440 --> 50:12.880
is used to prompt GPT3 into generating a story about a robot who sees these things and interact

50:12.880 --> 50:19.440
with them. And then you take the output of the generative model and translate this using text

50:19.440 --> 50:27.680
to motor module. And in this way, you close the loop. And I just used it as a thought experiment

50:27.680 --> 50:33.920
to think about the limitations of embodiment for such systems. Second is essentially doing that.

50:34.720 --> 50:40.240
So somebody has made this happen. And even with the language model, it works to some degree.

50:40.240 --> 50:44.480
And we know that we don't want to do this with natural language because natural language is a

50:44.480 --> 50:49.280
crutch. These systems make up for this, but you're just using more natural language faster than

50:49.280 --> 50:54.960
people could use it. But there is some language of thought that we are using that is not learned,

50:54.960 --> 50:59.840
but discovered by our own mind that we converge on, that is much more efficient. And this language

50:59.840 --> 51:04.880
of thought seems to be able to bottom out and perceptual distributed representations that are

51:04.880 --> 51:09.200
unprincipled, like these neural networks are in a sense unprincipled, but they don't break.

51:09.200 --> 51:13.840
Then there is something that is vague and ambiguous and has small contradictions in it.

51:13.840 --> 51:19.840
But at some level, it also is able to emulate very principal logic very well and becomes very

51:19.840 --> 51:26.000
sparse and very powerful in expressing things concisely. And this very concise language of thought

51:26.000 --> 51:32.880
is so don't see it in our models. What Dali is doing, Dali 2, is that it combines the language

51:32.880 --> 51:38.560
model and division model using embedding spaces. And these embedding spaces basically project all

51:38.560 --> 51:43.360
the concepts into some high dimensional manifold and find similarities between them.

51:44.480 --> 51:50.960
And Gary Marcus points out that there is an issue with compositionality in this. So you need to

51:50.960 --> 51:56.480
find the semantic structure of a sentence that is made of a hierarchy of concepts. And this is easy

51:56.480 --> 52:01.120
to do with the grammar. And it's much harder to do this with a deep learning system that needs to

52:01.120 --> 52:06.960
discover this in a way and structure this space in the right way. It's not impossible. So when

52:06.960 --> 52:11.760
Gary Marcus says these models cannot do this and cannot learn it, he is probably wrong.

52:12.320 --> 52:17.680
But I think he is right in the sense that this is something that is much, much harder for the

52:17.680 --> 52:22.640
current approaches. They need dramatic training data than a human being. And the algorithms are

52:22.640 --> 52:28.720
not doing this naturally. So there are probably ways in which we could make this happen much more

52:28.720 --> 52:32.880
elegantly and quickly and converge, for instance, on models for arithmetic.

52:33.360 --> 52:39.520
That's right. I mean, I remember I read a really good Twitter thread, I think it was by Raphael

52:39.520 --> 52:43.920
Millier, you know, about compositionality of these large generative vision models. Because usually

52:43.920 --> 52:50.720
compositionality is referred to in respect of language models. I think Raphael said that the

52:50.720 --> 52:55.120
assessment of the claim is complicated by the fact that people differ in their understanding of what

52:55.120 --> 53:00.560
compositionality means. But if language is compositional, as you say, and thought is language

53:00.720 --> 53:05.200
as argued by the proponents of this language of thought hypothesis, I think Raphael said

53:05.200 --> 53:10.240
that he thought language itself should be compositional in a similar sense. And perhaps

53:10.240 --> 53:16.240
by extension, visual imagery should be compositional. So I think Gary was arguing in a nutshell that

53:16.240 --> 53:21.280
it's hard to go from the image, or let's say the utterance, if it's NLP, to the structure,

53:21.280 --> 53:25.920
or the grammar, or the constituents. It's much easier to go the other way around, would you agree?

53:26.880 --> 53:30.960
The issue is that language of thought is executable. And natural language is not.

53:32.000 --> 53:37.200
We execute natural language by translating it into our mind in something that we can execute.

53:37.840 --> 53:42.240
And the reason about code, you might use natural language to support your reasoning.

53:42.240 --> 53:47.280
But the code that you build in your mind is filled in some kind of abstract syntax tree that you

53:47.280 --> 53:52.480
can actually execute in your mind to some degree. And then you get a sense of the output. So you

53:52.480 --> 53:58.160
entrain your own brain with an executable structure. And this executable structure has

53:58.160 --> 54:03.760
properties that are quite similar to the ones that the compiler has in your computer. So you

54:03.760 --> 54:07.680
can anticipate what the compiler is going to do with your code. You're not going to do this with

54:07.680 --> 54:12.320
all the depths that your compiler do it, you might still have to run your code, but you will find

54:12.320 --> 54:18.160
when you want to experience programming, your stuff will usually run. So our language of thought

54:18.160 --> 54:22.880
can do this, it can execute stuff. And it's not just a machine neural network that guesses

54:22.880 --> 54:28.080
what the outcome is going to be and is right some of the time. But it gets pretty good at

54:28.080 --> 54:33.360
figuring this out. And this means that it has to build this compositional structure that has some

54:33.360 --> 54:38.880
verifiable properties. And we observe ourselves operating on this verification process, right?

54:38.880 --> 54:44.720
When we do introspection for the program, we observe ourselves how we direct our attention on

54:44.720 --> 54:52.080
making proofs. And this attentional algorithm that works in real time, that is making changes on

54:52.080 --> 54:58.320
your mental models and then predicts the outcome of these changes and compares this with what your

54:58.320 --> 55:03.200
mental computations give you and then fixes your models of how your own thinking process in this

55:03.200 --> 55:08.880
domain works and so on. You can observe yourself doing that. And it's nothing where I would say

55:08.880 --> 55:13.840
a given approach or the given approaches that we have will never get there. But there seem to be

55:13.840 --> 55:18.400
ways in which we have just barely scratched the surface in what you need to be doing to make

55:18.400 --> 55:22.720
these models sample efficient and sparse and more adequate to model domains you're interested in.

55:25.120 --> 55:30.480
Cool. Okay. Well, I mean, just to finish like the discussion about the OpenAI stuff, I mean,

55:30.480 --> 55:35.280
I agree with the prognosticators. And I do think that these large language models and

55:35.280 --> 55:38.720
these visual generative models will be revolutionary for some domains. But

55:39.280 --> 55:44.720
you know, I think you really need to have a human guiding the creative process, which is a huge

55:44.720 --> 55:49.760
limitation. But I think it could also potentially hint at what intelligence actually is, right?

55:49.760 --> 55:55.360
I think intelligence might be this externalized process in a cybernetic sense, if you like,

55:55.360 --> 56:00.160
this idea of intelligence being fully embedded in an algorithm in a single agent might be

56:01.040 --> 56:02.800
the wrong way to think about it.

56:03.440 --> 56:08.480
I think that humans, by and large, are very confused. Very often you need a human to guide a

56:08.480 --> 56:14.160
human, right? And then you ask yourself, if you do this recursively, does this society know where

56:14.160 --> 56:19.360
it's going? Or is this at some level confusion at all levels that is balancing each other?

56:19.920 --> 56:25.120
So there seem to be very few people with a plan right now. And it's quite apparent that we see

56:25.120 --> 56:29.920
that in the sciences, we see this in politics, we need an art and literature. It is that humans

56:29.920 --> 56:35.440
have a higher degree of sentience. But by and large, very few people have a principal plan on

56:35.440 --> 56:41.840
how to build a sustainable, harmonic world. And if you set an AI system to this task, it might

56:41.840 --> 56:47.040
make more progress on it. It's just that Dalit is not operating in real time on the universe that

56:47.040 --> 56:51.840
it's entangled with and neither is GPT-3. Both of them are in some sense, fancy autocomplete

56:51.840 --> 56:59.520
algorithms. But this fancy autocomplete is able to do autocompletions that are far beyond the

56:59.520 --> 57:06.880
autocompletion abilities of humans in almost every context. And so I don't see Dalit yet as art.

57:06.880 --> 57:16.160
It's a very strange sense when someone at OpenAI let me throw trumps at Dalit too. And I got images

57:16.160 --> 57:22.240
back. I had a sense of ownership. I had the sense that I was doing that, even though it was clearly

57:22.240 --> 57:26.880
doing skills, using skills that I didn't have. And I suppose that you had the same impression

57:26.880 --> 57:31.040
when you were generating things with your diffusion model that you're going to put up on your walls,

57:31.040 --> 57:36.400
right? You did that using this amazing tool that was empowering you to think that you otherwise

57:36.400 --> 57:42.480
never could do. But you are the creative nexus. And to make an artist, a digital artist, you would

57:42.480 --> 57:48.640
need to create an autonomous creative nexus in a way, a creative entity, something that reflects

57:48.640 --> 57:53.520
on the world because art is about capturing conscious states. So we would need to build a

57:53.520 --> 57:58.960
system that has a story about itself and that is reacting towards own interactions with the world

57:58.960 --> 58:04.640
and that would need to be human. It would need to be consistent. Something that is an intelligent

58:04.640 --> 58:10.160
entity that is creatively interacting with the world. I think we could totally build an AI

58:10.160 --> 58:15.200
artist franchise right now that would have a huge following. But what it need to have is an identity

58:15.200 --> 58:20.000
that is not fake, that it's actually built from its interactions with the world in real time.

58:20.960 --> 58:27.840
Well, I think we've got a lovely segue there because you said that art is about

58:27.840 --> 58:32.000
representing our conscious states. In a way, I disagree with you because you could say,

58:32.000 --> 58:36.000
well, it's very reductionist. I've just put a prompt in there and I've created art. Well,

58:36.000 --> 58:41.360
I think it is art. But how much of a representation of my conscious state is it? I think Douglas

58:41.360 --> 58:46.720
Hofstadter would say it wasn't. But over to the matter of consciousness because we're a bit low

58:46.720 --> 58:50.880
on time. I mean, you said actually that you've spent much of your life thinking about what

58:50.880 --> 58:57.040
consciousness is. And you said that you thought it was very mysterious, but you now think that

58:57.040 --> 59:01.840
it's a riddle that can be solved, right? So on your recent theory of everything interview

59:01.840 --> 59:07.360
with Donald Hoffman, actually, you said that it was virtual, not a physical thing, that brains

59:07.360 --> 59:13.520
are mechanistic and that the elements of consciousness are magical somehow. But you said

59:13.520 --> 59:19.200
it had an a causal structure, but not the way physics is built. But it was a story

59:19.200 --> 59:24.240
which the physical system tells to itself. You said that the organism is a coherent

59:24.240 --> 59:29.680
and consistent pattern, which is state building at least at some level of analysis and that

59:29.680 --> 59:35.280
consciousness allows organisms to coordinate their cells to succeed in their niches. And then

59:35.280 --> 59:42.320
you spoke of information processing over cells. Now, what model of I should say like what measure

59:42.320 --> 59:48.640
of consciousness do you think you most align with? There is only one theory that offers a

59:48.640 --> 59:52.880
measure of consciousness and that is integrated information theory, where you actually put a

59:52.880 --> 01:00:00.240
number on it. And it's not clear what that number means. It's not that there is some kind of scalar

01:00:00.240 --> 01:00:06.320
that measures this. And people we think of consciousness as something that is more qualitative

01:00:06.320 --> 01:00:11.280
than quantitative. Either somebody is conscious or somebody is unconscious. And when you are

01:00:11.280 --> 01:00:16.240
conscious, you can have a lack of acuity, you can be addled in your brain and you can be

01:00:16.240 --> 01:00:22.240
drifting in and out of consciousness. But it's still a qualitative thing of whether you have

01:00:22.240 --> 01:00:28.800
that or not. And this qualitative thing seems to be simple, probably much simpler than people

01:00:28.800 --> 01:00:34.960
expected to be. The hard thing might be perception. And consciousness is on top of

01:00:34.960 --> 01:00:40.480
perception is a certain way to deal with our attention. So I think a very important aspect

01:00:40.480 --> 01:00:46.000
of consciousness is reflexive attention, that we notice ourselves attending to something,

01:00:46.000 --> 01:00:52.400
and we reflect on that and integrate this in our model. The conundrum is understanding consciousness

01:00:52.400 --> 01:00:57.680
if you go right into the history of everything starting with Leibniz and many others. Leibniz

01:00:57.680 --> 01:01:05.680
says this idea of imagine you could have a mill and this mill, this is your mind. And the mill is

01:01:05.680 --> 01:01:11.680
made of lots of mechanical parts and somehow the thing is feeling and perceiving things.

01:01:12.320 --> 01:01:18.960
And we blow the mill up so large that we can walk into it or we would today zoom into it until

01:01:18.960 --> 01:01:23.840
we see all the parts and we just see these pushing and pulling parts and nothing of them

01:01:23.840 --> 01:01:29.760
can ever explain a perception or a feeling. And it is a very strong intuition that also

01:01:29.760 --> 01:01:36.400
drives the Chinese rule when many other thinkers will get attracted to this. And it seems pretty

01:01:36.400 --> 01:01:41.120
obvious that these mechanical phenomena are insufficient to explain what's going on. It's

01:01:41.120 --> 01:01:46.480
not an obvious connection. So people become dualist. There are somehow two completely separate

01:01:46.480 --> 01:01:51.920
domains. And I think in a way this dualism is correct, but not in the sense that the mental

01:01:51.920 --> 01:02:00.560
states are ontologically existing. They exist as if. There is no organism. There is only this

01:02:00.560 --> 01:02:06.240
connection of cells and this collection of cells is acting in a coherent way, which means we can

01:02:06.240 --> 01:02:10.960
compress it. We can model it using a very low-dimensional, much circular function

01:02:10.960 --> 01:02:15.600
than look at all the cells in general. And the organism is only approximating this function,

01:02:15.600 --> 01:02:20.720
but what makes the organism more powerful than a collection of cells is exactly that function,

01:02:20.720 --> 01:02:26.080
this structure that we project into it. And the interesting thing is that by the information

01:02:26.080 --> 01:02:30.640
processing within the organism, the organism can discover that function by itself and use it to

01:02:30.640 --> 01:02:36.880
drive its own behavior. So while the organism is not a person, it's not even an organism,

01:02:37.520 --> 01:02:43.040
it is very useful for the organism to behave as if it was an organism and also to have an idea

01:02:43.040 --> 01:02:46.800
of what it would be like to be a person that interacts, for instance, with the social world.

01:02:47.520 --> 01:02:52.160
So it creates a simulation of that. And it's often not even a simulation, it's often just a

01:02:52.160 --> 01:02:57.760
simulacrum. That's what makes it a causal. The difference between a simulation and the reality

01:02:57.760 --> 01:03:04.800
is that the simulation is modeling some aspects of the dynamics of a domain on a different causal

01:03:04.800 --> 01:03:10.160
substrate, on a different causal footing. So you have a computer game in which you can shoot a gun,

01:03:10.160 --> 01:03:16.640
but there is no proper physics in the game that would recreate what's happened in the real world

01:03:16.640 --> 01:03:21.520
when you shoot a gun. Instead, it is using a different causal structure of your software

01:03:21.520 --> 01:03:26.080
program to give you something that gives you good enough dynamics so you can interact with the world

01:03:26.080 --> 01:03:30.240
and experience these causal structures. You can make a different decision, you make a different

01:03:30.240 --> 01:03:35.120
move in the game, and as a result, the game behaves as if you would expect it because it's

01:03:35.120 --> 01:03:39.440
imitating the same causal structure using this different substrate. In the simulacrum,

01:03:39.520 --> 01:03:43.280
you don't have the causal structure. Like a movie doesn't have causal structure. It only

01:03:43.280 --> 01:03:48.960
gives you a sequence of observables. And our own mental model of ourselves is a mixture of

01:03:48.960 --> 01:03:55.360
simulation and simulacrum. So we sometimes create a sequence without causal structure. It looks like

01:03:55.360 --> 01:03:59.920
it does this thing magically. And sometimes we have a causal model, but this causal model

01:03:59.920 --> 01:04:04.560
is not the real deal. It's just this simplified geometric simulation of how the world works.

01:04:04.560 --> 01:04:09.040
It's a game engine that our brain is producing to anticipate what happens in the physical world.

01:04:09.920 --> 01:04:16.640
Yeah, so that very strongly resonates with me. And another person I very much respect

01:04:16.640 --> 01:04:22.720
whose opinion resonates with me regarding consciousness is Carl Friston. And I'm not

01:04:22.720 --> 01:04:27.920
sure how much you're familiar with his free energy principle and his thoughts on consciousness,

01:04:27.920 --> 01:04:33.040
but I'd like to put forward to you one of his more recent definitions, if you will,

01:04:33.040 --> 01:04:37.680
or proposals to explain consciousness. And I get your opinion on it. This is from his

01:04:38.320 --> 01:04:46.560
2018 article titled, am I self conscious or does self organization entail self consciousness?

01:04:46.560 --> 01:04:52.480
And what he says here is the proposal on offer here is that the mind comes into being when

01:04:52.480 --> 01:04:59.920
self evidencing has a temporal thickness or counterfactual depth, which grounds inferences

01:04:59.920 --> 01:05:06.800
about the consequences of my action. On this view, consciousness is nothing more than inference

01:05:06.800 --> 01:05:14.080
about my future, namely the self evidencing consequences of what I could do. Does that align

01:05:14.720 --> 01:05:16.800
pretty closely with your your view?

01:05:17.680 --> 01:05:23.920
No, I don't think it's sufficient and also don't think it's necessary. I like Friston's idea,

01:05:23.920 --> 01:05:27.920
but most of the free energy principle comes down to predictive coding,

01:05:27.920 --> 01:05:35.440
which is in some sense, radically tested with GPT-3. GPT-3 is trained in some sense entirely

01:05:35.440 --> 01:05:40.320
on predictive coding. It's only trying to predict the future from the past. And the future is the

01:05:40.320 --> 01:05:46.000
next token based on the tokens that it has seen so far. And GPT-3 radically tries how far you can

01:05:46.000 --> 01:05:52.080
go with this, and you can go very far. But you need far far more samples than an organism does.

01:05:52.960 --> 01:05:58.000
So there are players in us that go beyond predictive coding, maybe we converge towards

01:05:58.000 --> 01:06:03.280
this over many generations in the evolutionary process. So I don't think it's a stupid idea

01:06:03.280 --> 01:06:10.320
that Carl Friston proposes, but we are born with additional loss functions that let us

01:06:10.320 --> 01:06:15.680
converge much, much faster on something that is useful to the organism. And if we think about

01:06:15.680 --> 01:06:22.160
consciousness, he has a point about agency in there. Agency means that you have a controller

01:06:22.160 --> 01:06:27.600
that is able to control the future. Took me a while to understand this, but when I go up,

01:06:27.600 --> 01:06:31.280
we talked about BDI agents, and they seem to be quite complicated and convoluted,

01:06:31.600 --> 01:06:36.160
put a lot of quote there to make a BDI agent, but there's beliefs, desires, and intentions,

01:06:36.160 --> 01:06:42.720
and so on. But if we think about what actually is a minimal agent, a thermostat is not a minimal

01:06:42.720 --> 01:06:46.720
agent. So that has enough agency, it doesn't want anything. It just acts on the present

01:06:46.720 --> 01:06:52.240
frame by doing the obvious thing. But imagine that you give the thermostat the ability to

01:06:52.240 --> 01:06:57.680
integrate the expected temperature differences, the differences over the future, when it does x now

01:06:57.680 --> 01:07:03.120
or y now or does it a moment later, right? So suddenly you have a branching reality. And in

01:07:03.120 --> 01:07:08.000
this branching reality, you can make decisions and you will have preferences based on this

01:07:08.000 --> 01:07:13.680
integrated expected reward. So just by giving the thermostat the ability to model the future,

01:07:13.680 --> 01:07:19.120
you turn it into an agent. This is sufficient. And if you make this model deeper and deeper,

01:07:19.120 --> 01:07:23.520
it's going to get better and better at it. And at a certain depth, the thermostat is going to

01:07:23.600 --> 01:07:29.040
discover itself. It's going to discover the idiosyncrasies of its sensors. And notice that

01:07:29.040 --> 01:07:33.360
the sensor operates differently when it's closer to the heating element and so on and so on, right?

01:07:33.360 --> 01:07:38.480
So it becomes aware of how it functions. It might even become aware of the way in which its modeling

01:07:38.480 --> 01:07:46.000
and reasoning process works and to improve it or to account for its inefficiencies in certain ways.

01:07:46.000 --> 01:07:52.640
And this is also what we do with our own cell. But this model of the self is not identical to our

01:07:52.640 --> 01:07:58.480
consciousness. Our consciousness is a feeling of what it's like in the moment. It's the experience

01:07:58.480 --> 01:08:03.920
of a now. There is an experience of a perspective that we are having, right? And this is what's

01:08:03.920 --> 01:08:08.480
absent in the description of first. And he is missing the core point of what it means for

01:08:08.480 --> 01:08:12.240
something to be conscious. It doesn't mean that it has a self. It doesn't even just mean it has a

01:08:12.240 --> 01:08:19.040
first person perspective. It means that it experiences a reality. And this is not described

01:08:19.120 --> 01:08:26.160
in this Friston quote. We explicitly asked him this question actually when we talked to him

01:08:26.160 --> 01:08:34.480
last time. And his response there was that this concept of feel like is really something that

01:08:34.480 --> 01:08:41.120
would need to be coded into the generative model that this agent has about the world. Number one,

01:08:41.120 --> 01:08:45.040
it has to have a generative model as we've just been discussing. It has to be able to entertain

01:08:45.040 --> 01:08:50.560
counterfactual, you know, possibilities for the predictive coding, right? And he's saying that

01:08:50.560 --> 01:08:56.960
these feel like concepts would literally be encoded in that generative model as hypotheses

01:08:56.960 --> 01:09:03.680
that we recognize, you know, so things like I'm feeling pain, for example, would be a concept

01:09:03.680 --> 01:09:08.800
within that model. And he says there's actually evidence from, you know, treating patients with

01:09:08.800 --> 01:09:13.600
chronic pain and this sort of thing that that's actually exactly what's happening in the mind

01:09:14.160 --> 01:09:20.080
that that the feeling of pain is actually a concept that's built in as a slot, if you will,

01:09:20.080 --> 01:09:25.920
into this generative model. I mean, what do you think about that proposal?

01:09:27.520 --> 01:09:34.720
The semantics of pain are given by the avoidance that you don't want to experience the pain usually.

01:09:35.600 --> 01:09:40.160
And it could be that you cultivate the pain and use it to make something happening on the

01:09:40.240 --> 01:09:44.720
next level, but it requires that you are then building a multi-level control structure,

01:09:44.720 --> 01:09:51.680
if you want to use pain productively, some artists are maybe doing. But the, you cannot have pain,

01:09:51.680 --> 01:09:56.080
I think, without an action tendency, without something that modulates what you are doing.

01:09:56.960 --> 01:10:02.720
So your, your cognition is embedded into this engine. And to build such an engine that does it,

01:10:02.720 --> 01:10:08.480
that causally changes how you operate is not that hard. But when you live inside of such an engine,

01:10:08.480 --> 01:10:13.200
it feels very strange that there is something that is happening that somehow depends on what

01:10:13.200 --> 01:10:17.840
you are thinking, but you cannot control it. It controls you. It's upstream from you. You are

01:10:17.840 --> 01:10:23.920
downstream from it. And when you get upstream of your own pain, the pain stops being pain. It's

01:10:23.920 --> 01:10:29.520
something that is a representation that you can now control and be able to get there. But it's

01:10:29.520 --> 01:10:34.560
not easy and you're not meant to get there because it means that we can immunize ourselves to pain

01:10:34.560 --> 01:10:39.840
and sacrifice the organism to our intellectual interests. What's crucial about feelings when

01:10:39.840 --> 01:10:44.400
you look at them introspectively is that feelings are essentially geometric. I don't know if you

01:10:44.400 --> 01:10:50.320
noticed that. So for instance, we notice feelings typically in our body. And that's because I think

01:10:50.320 --> 01:10:55.680
that the feelings play out in a space. And the only space that we have always instantiated in our

01:10:55.680 --> 01:11:01.040
mind is the body map. So they're being projected into the space to make them distinct. And when we

01:11:01.040 --> 01:11:07.040
look at the semantics of the feeling, we noticed that they are contracting or expanding or they

01:11:07.040 --> 01:11:12.080
are light or they're heavy and so on. This is all movement of stuff in space. It's all geometry

01:11:12.720 --> 01:11:16.720
plus valence, the stuff that is going to push your behaviors in a certain direction.

01:11:17.280 --> 01:11:24.160
So these are basically the interactions of some deep learning system that is producing

01:11:24.160 --> 01:11:31.040
continuous geometric representations as perceived from an analytic engine. It's an interface

01:11:31.040 --> 01:11:36.880
between two parts of your mind, between the analytic attention control that is reflecting

01:11:36.880 --> 01:11:41.520
on the operations that your mind is doing while it's optimizing its attention. And the underlying

01:11:41.520 --> 01:11:46.720
system that represents the state of the organism entails where you should be going and makes this

01:11:46.720 --> 01:11:53.120
visible to you. It is a system that is not able to speak to you, uses geometry. And these

01:11:54.160 --> 01:11:59.120
geometrical features, this is what we call feelings. So that's a very interesting connection.

01:11:59.120 --> 01:12:05.520
And I think Jeff Hawkins of Nemento would be quite interested in that as well, because

01:12:06.960 --> 01:12:14.640
some of what he discussed with us was that, in his view, the evolution of, let's say,

01:12:14.640 --> 01:12:20.400
abstract thinking and whatnot actually came from systems that evolved to operate in just

01:12:20.400 --> 01:12:27.840
simple three-dimensional kind of motion, and that eventually those were reutilized by the

01:12:27.840 --> 01:12:34.240
evolutionary process to start engaging in abstract thinking, which he views as movement

01:12:34.240 --> 01:12:38.000
through an abstract space. And so I think there's a lot of connection here to what you're saying

01:12:38.000 --> 01:12:43.920
about feeling, which is that, again, in a sense, our mind has reutilized this

01:12:44.640 --> 01:12:51.120
three, three plus one d movement mapping capability that it needed in order to survive in a three

01:12:51.120 --> 01:12:58.240
plus one d environment, physical environment, and it's reutilized those for mapping feelings,

01:12:58.240 --> 01:13:04.640
it's reutilized them for mapping to abstract thinking is like a form of motion in an abstract

01:13:04.640 --> 01:13:11.760
space. Is that a fair connection? Yes, but I don't think that it's because it's

01:13:11.760 --> 01:13:17.760
borrowed from the world in which we interact, but because it's the only game in town, it's the

01:13:17.760 --> 01:13:24.480
only mathematics that can deal with multi-dimensional numbers. So when we talk about spaces, we actually

01:13:24.480 --> 01:13:29.040
talk about multi-dimensional numbers, about things that are not just a scalar in a single

01:13:29.040 --> 01:13:34.720
dimension, but features that are related. And sometimes you can take these features that you

01:13:34.720 --> 01:13:38.960
measure continuously because they have too many steps to meaningfully discretize them.

01:13:39.760 --> 01:13:46.160
So what it does is you discover that you can rotate something. And this is when you get a

01:13:46.160 --> 01:13:51.200
space in the sense as we have a space to which we are moving. And these spaces which you can

01:13:51.200 --> 01:13:59.760
rotate things only exist in 2D and 4D and 8D. And so the geometry that we're talking about is

01:13:59.760 --> 01:14:05.120
constrained to certain mathematical paradigms, which you can derive from number theory,

01:14:05.120 --> 01:14:12.080
compressed principles. And our brain is discovering a useful set of functions to model

01:14:12.080 --> 01:14:18.000
anything, a set of useful computational primitives. And we can probably give our deep learning systems

01:14:18.000 --> 01:14:22.880
a library of predefined primitives to speed up their convergence. That's also the reason why

01:14:22.880 --> 01:14:28.000
there is useful transfer learning between different domains. You can train a vision model

01:14:29.760 --> 01:14:34.800
and use it as a pre-training for audio. And it's not because it's the same thing, but because it

01:14:34.800 --> 01:14:40.480
has learned useful computational primitives that it can apply across domains. But there is geometry

01:14:40.480 --> 01:14:47.760
in the audio signal. So this is very interesting territory. I hope you'll come back to dive into

01:14:47.760 --> 01:14:52.640
this a bit more deeply when we have more time and a better connection, because I agree with you.

01:14:53.920 --> 01:14:56.160
Some very fascinating math here.

01:14:56.320 --> 01:15:06.160
Fantastic. Well, Dr. Yoshua Bak, it's, as I said, you've by far the most requested guest

01:15:07.040 --> 01:15:11.040
that we've had right from the very beginning. So it's an honor to finally get you on the show.

01:15:11.040 --> 01:15:14.480
And I hope we can get you back soon for a longer conversation. Thank you so much.

01:15:15.120 --> 01:15:18.400
Likewise. I enjoyed this very much. Let's meet again soon.


Welcome back to Machine Learning Street Talk. I'm your host, Tim Scarf.
Now, today on the show, we're joined by Dr. Walid Saber, who's just written a review of the book
Machines Will Never Rule the World, Artificial Intelligence Without Fear, written by Jops Langreb
and Barry Smith. Now, Dr. Saber will be discussing his review, which provides a detailed analysis
of the book's arguments that strong AI is impossible. In his review, Dr. Saber acknowledges
the argument made by Langreb and Smith that anything we engineer is ultimately a system
which can be mathematically modelled and described. He then goes on to discuss the complexity of
modelling mental processes, which the authors argue are dynamic, adaptive, continuously evolving,
and constitutes systems whose behaviour affects and is affected by the environment they function in.
He also touches on the notion of granularity, arguing that complex systems are all the way up
from specific components of the mind to the mind itself and that no known mathematics can model them.
Dr. Saber then delves into the complexities of language and open interactive dialogues,
asserting that language is a prerequisite for any artificial general intelligence,
but that linguistic communication itself is a complex system that no mathematics can model.
He doesn't subscribe to the argument that interactive bots can be built in narrow domains,
since responses and the overall context cannot be predicted in any meaningful way.
Dr. Saber has two reservations as to the conclusions made by Langreb and Smith.
He questions their use of the word never and suggests there could be a new mathematics
that mental processes require that is yet to be discovered. He also doesn't believe that the fact
that complex behaviour cannot be mathematically modelled precludes the possibility of building
such systems, as evidenced by the intentional programming language LISP, and he also considers
the possibility of hypercomputation in validating the church-touring hypothesis.
We'll talk about that a bit later. Finally, Dr. Saber expresses his regret
that the book didn't go into further detail on the frame problem in AI.
He calls for further research into belief revision in complex systems.
Join us today as we speak with Dr. Wallid Saber about his review of this book that we've just
been speaking about. Machines will never rule the world. And by the way, I think very highly
of Wallid. I think he is one of our most loved guests. He is a polymath. He has an incredible
breadth of knowledge across so many fields, you know, from AI and computer science,
to mathematics, to philosophy, to linguistics. He really is a rare breed, and he also brings
a very interesting contrarian view, I would say, to the current kind of modus operandi,
or the zeitgeist in the community at the moment. He's a breath of fresh air.
Anyway, if you haven't already, consider subscribing to our YouTube channel, or indeed
rating our podcast on your favourite podcasting platform if you happen to be listening to us.
Anyway, without any further delay, I give you Dr. Wallid Saber.
Welcome back to MLST, folks. We have the unmistakable Wallid Saber, the legend that is Wallid Saber,
but we also have Mark from our Discord community. Mark, would you like to introduce yourself?
Hi, guys. I'm Mark Aguil, a philosopher, cognitive scientist, and software engineer at
MLST. Awesome. Welcome, Mark. You know, things have been a bit of a blur,
but I think this is right, Tim. I think Wallid was the first, or one of the first,
really big names that we had come on the show, right? I don't think so,
but the most controversial, let's put it this way, the one that made probably,
it was so predictable, what was, I mean, probably the first one that broke that
predictive model. Well, I just remember, I mean, I remember Tim and I being like super,
because we didn't know you, right, before then, and I remember us being like so excited that you
agreed to come on the show. You know, of course, now that we know you, we're kind of like,
ah, whatever, it's just Wallid. We're just through, by the way.
We were following all over ourselves that you were coming on the show, we're like, oh my god,
this is so awesome. To be honest with you, I never thought I would, I would,
yeah, I never thought I would have, my opinion would matter that much, to be honest with you,
and it all started by creating this medium blog, and I started spitting out stuff that,
hey, do you guys know that there's dissonance in that? And I was surprised how much it,
even by people that are living off and making a living, and they preach and write papers on
what I'm attacking, and they would say, don't mention my name, by the way, it's all private
messages. But apparently, I said a couple of things that touched people, but you know.
Yeah. Yeah, I was at New York's last week, and the amount of people that came up to me and said,
I love the show, Tim, Wallid's my favorite guest that you've had on, because Wallid just provides
a completely different perspective, because we're bred on empiricism and neural networks. And part
of the reason I want to get you back on, Wallid, is to counteract some of the, I mean, we've been
speaking to a lot of deep learning people recently, so we need to counteract that a little bit.
Yeah, that one shocked the hell out of me. I mean, I have people like myself at this event. I mean,
if you said ACL, maybe, yeah, okay. But I mean, this is the deep learning meeting, right? I mean,
so I was shocked. But yeah. Yeah, indeed. Although I have some more positive views of DL now.
Oh, go on. Yeah, I mean, look, I breaking news, breaking news.
Positive, although I still have my reservations as to AGI and all that stuff. But I have been
completely impressed with the developments in large language models. I have to admit that
sometimes I say, what the hell is this? Now, technically, let me tell you what's happening.
And I'm working on something that probably would quantify
where can this go? How much can you, how much will scale? The bottom line is this,
these guys have impressed the hell out of me, and they have proven that scale does matter.
I mean, now these large language models, if you take language from lexical to
syntactic to semantic to pragmatic levels, they have definitely mastered syntax.
And this is not a small feat. I mean, this is huge. They have proven that if I read
tons of texts written by humans, I can figure out the grammar of language,
and they have done that. That's huge. Okay, so and I don't like here where I don't like people.
I don't want to mention names, but people that supposedly are in my camp, right,
insisting on refusing to see the elephant in the room. No, large language models have proven
that if I ingest terabytes of text, I will figure out syntactic rules. They have done that.
Now, okay, and here's where technically, and you have to admit, I mean, they,
as a matter of fact, they probably know syntax now more than many college graduates.
Okay, these are from data. That's a huge experiment in cognitive science that
no matter how, how, I mean, you can't be religious in this, you have to be scientific.
I see the proof that these large language models by ingesting tons of text written by humans,
they have figured out the syntax of language. End of story. I have, there's an existential proof.
Go on, open AI, try DaVinci 2 or DaVinci 3 or whatever you like to try.
Their syntactic competency is beyond belief. I'm shocked every time I use it. Okay.
Now, that's not the end of language understanding. There's semantics, and then there's pragmatics.
Wow. I mean, so I'm working on something to quantify. So now we have, I don't know,
we're up to a trillion parameter that allowed me to master syntax. Now, let's see semantics.
And semantic can be broken down to, have you guys figured out reference resolution? Have you
guys figured out scope resolution, prepositional phrase attachments? There are so many
pain points and semantic processing. Can we quantify how many more parameters we need to
conquer semantics? And then pragmatics, there are things like,
the teenager shot a policeman and he immediately fled away. Now, possibly both can,
the he can be the policeman. He fled away to escape further injuries. I mean, that can happen.
But most likely the one that's led away is the teenager. That's pragmatics because we know in
the world we live in, if you shoot someone, they're going to try to capture you and you try to flee,
right? That's not semantics. That's way beyond. How many parameters beyond semantics do you need
to capture that? If you can put a, if you can come up with a rough number, I mean, it could be
a number that's manageable, that's doable by more scaling, which would be an interesting result.
But it could be that it's a number beyond the universe we live in, which means guys,
except that you can master syntax and a little bit of lexical semantics, you can figure out the
meaning of some words, but to do full understanding with pragmatics, we're talking about numbers that
we might have to wait 2000 years. Yes, in theory, it works. So basically, I'm trying to work now on
which is going to be very difficult to quantify because they have proven that scale did improve
syntax, no doubt, not improve it. They've almost mastered syntax. But how far can this go? I mean,
can you quantify how far can this go scientifically without saying, let's try with more, let's try
with more? Which is, which is not, it's not going to be easy to do. Anyway. So I'm curious, could I
push back a little bit on the syntax you said, if they've mastered syntax, and I'm kind of,
okay, I mean, I guess you have an existing proof, and there's like a behavioral kind of
proof that it does seem to have very syntactic sentences. And obviously, if you're in something
with whatever billion of parameters, but would you say what those rules are? Could we write them
down? Probably not, right? Because it's a billion different numbers of weights. No, no. And you
don't have the old school kind of generative grammar approach. And also, it's not the way humans
have learned language. And could you reverse engineer or tweak it? We don't know what it is,
it's a black box. So, okay, there's a behavioral tense in which it knows. But isn't it the way
humans do learn language? I mean, I think it's more, though, it's more related to the way humans
learn language than, I mean, I was 20 until I knew grammar. I mean, we use language without,
without knowing grammatical rules. So there is an argument that humans don't learn grammar,
that it's, you know, pretty native. And all we do is tweak a few parameters. And then we add
vocabulary over the years, tweaks or whatever. That's the, you know, the Chomsky and generative
grammar approach. So I don't think that we have a billion parameters that we tweak over 20 years.
I don't think that's how we work. And we have, I mean, there's amazing competency of children
at two years of age, you know, with language they have, you've said, you've, you're writing,
you pointed this out, actually, children know, is absolutely amazing, you know, straight out of,
you know, it is amazing, but it is amazing. And I, and I didn't change the way I think about
and we have innate stuff. But here's the change that these guys have made me
go through. It's a minor change, but it's, it's, it's not that subtle, actually. Here's the thing.
I was told before these new results are coming out that look, we do have innate stuff, which
took us three, four hundred thousand years of evolution. All we're doing by ingesting all
this text is we're simulating, right, these 300,000 years. So give us a chance to simulate
this innateness if you want, in a way, in a way. Okay, I, that argument was said long time ago,
and I thought, come on, you're chasing infinity. What happened with the real difference in my
mind now is they have proven that they conquered one beast in language. Nobody can dispute that.
Can I have a go at disputing it? So, in the Polition and Foda connectionism critique,
they spoke about productivity, you know, the infinite cardinality of language. There was
recently a deep mind paper talking about the Chomsky hierarchy and deep neural network. Well,
I mean, RNNs are regular languages, but, you know, I think transformers and the rest of them are
at the bottom of the hierarchy. So quantitatively, we know we haven't conquered infinity. So why
with such a shallow horizon, are they doing so well? I agree. Here's the thing, language use,
languages infinite, but probably the long tail of probably 90% of ordinary language use,
right, can be figured out from the stuff that we write. So they will never capture all of language.
Yes, but they might reach the level of a competent educated man like us in language competency.
So, all I'm saying is what they have achieved is a huge
result in terms of the big question of scale and big data. They have definitely proved that
if I see enough data, I will learn something and something that's not trivial.
Look, you know where I'm coming from. You're talking Foda and Polition. I mean,
you're preaching to the choir, right? But I have to be a scientist too. I mean, I don't like,
I'm following Gary Marcus, and he's like, I don't like people that minimize what happened.
I'm a scientist, right? I see a big result. I say, wow, right? And look, we're talking, nobody
bashed deep learning more than me, especially large language models. I mean, I'm like,
I was saying this is silly, right? But I have to say they have proven something to me at least,
which is huge because I know how difficult language is. I am impressed equally.
Wouldn't you say it's an engineering, an engineering triumph rather than a scientific?
It's an engineering triumph. But here's the point, Mark. I think it's a little bit more.
That's the only thing I'm trying to, I'm not saying, look, I didn't give up on, I can get to
the criticism later. So don't put me on in that camp yet, right? Or, or ever, right? Because I know,
I know theoretically, theoretically, mathematically, you cannot understand language this way.
All I'm saying is, in terms of cognitive science, what happened and what is happening as we speak
is not nothing. It's a huge, for example, if I can ingest a lot, again, what they prove is,
is that well, the two are related. So it's one thing scale from tons and tons of data. I can learn
something that is not trivial. That to me has been proven. The point I'm making is not the
point I'm making is not that they solve the language problem. Sorry.
Yeah, I want to jump in here a little bit. Because from my perspective, I think
part of why you're saying it's huge is because I think it was a huge step for you personally.
Because I know, you know, from the past, like talking to you, like you've had a much more
extreme view, you know, on the capabilities of large language models than, for example, myself.
Because for me, I don't see anything new here. It's kind of like, I'll give you an example
outside of syntax just for a moment. So just transcription, because that's what Tim and I
happen to be working on quite a bit right now. In other words, transcribing audio into text.
All the state-of-the-art models are pretty much sitting around each other at about 90%
you know, accuracy right of transcription. But here's the thing is that's for people speaking
relatively common languages with a relatively standard accent. Okay, as soon as you bring
someone in the room that has an accent or speaks a, you know, with maybe like some type of a
challenge, like a speech challenge, or this side of the other thing, it becomes garbage again.
And like for Tim and I, or there's noise in the background, music playing in the
background. And as Tim and I have probably hammered, you know, to death and beaten a
dead horse on our channel like so many times, we've never doubted that machine learning can learn
like the bulk of the curve, where it really, really struggles is in all these edge cases,
and the corner cases, and the periphery where it can easily, it's very brittle, right, in those
kind of areas. Like this is the point we've been making out for a long, long time. And so the fact
that like massive trillions of parameters and terabytes of data was able to learn 90% or more,
95% or whatever is syntax. Okay, I get it. Like from a linguist perspective, that was a, you know,
maybe a big triumph or something. But I'm still always about that other like 5%.
And the problem with the approach of deep neural networks is to get that other 5%
is like 100 times as many more parameters, whereas like using, whereas using more generalized,
abstracted, you know, methods that we haven't yet really discovered.
You're hitting it on the nail. And that's why I'm working out on quantifying this because
now we are doing exponential growth in the number of parameters for not even linear growth in the
accuracy, even logarithmic, I agree with you 100%. So that other 10% might require 2000 years of data
that we don't even have. That's what I'm working on. How far can this go because the function
is against them now? Like, I mean, we're increasing GPU power and the number of data
that we're ingesting exponentially for a minute increase in accuracy, which is,
right, that's the end of the logarithmic. And this is, this is part of the announcers that we
have to go through. So look, all my reservations that I had before apply. So I'm being misunderstood.
All I'm saying is simple. These guys, what they have done is not as trivial as I thought initially.
Okay, so let me, let me really be very careful in what I'm saying because now I have a following.
I don't want to lose it. No, I'm not, I'm not changing scientifically where I was. I mean,
science is science. And I know theoretically, I don't get into things like intentionality and
these models understand nothing about the word. I'm talking about syntax only, by the way,
syntax on and some coherence when they patch things together. The coherence is amazing.
They're not patching things together that don't relate at all. So I'm talking about syntax and,
and coherence and syntax. Okay. And a touch of semantics, right?
My point, let me repeat it so that I'm not misunderstood.
They have proven something that many cognitive scientists would never accept, never ever.
But this existential proof has told many cognitive scientists, don't dismiss learning
from data only, blind, no labeling, some aspects of language,
actually very impressive aspects of language. These guys have proven that.
And me as a cognitive scientist, I have to admit because I see it. I see from data alone,
these systems have learned non-trivial aspects of language.
Now, how do you interpret that? Where do you take it? What do you conclude from it?
We can, we can debate that. But all I'm saying is, I have seen something that I never thought I would
see, that just ingesting text in these deep networks, you can actually figure something
not trivial about language. That has been done. I mean, you can, you can say there are pigs,
pigs that fly. Okay. Prove me wrong. I saw them prove they don't exist. Well, I can.
But existential proofs are the most powerful proofs. It's an existential proof, proof by doing.
I'm showing you language competency by ingesting text on it.
So this dismissive
all these are, what is the phrase that Melanie uses? Not Melanie. Stochastic parents. No.
Oh, Bender. Emily Bender.
Emily Bender. No, these are not stochastic parents anymore for me. I am seeing,
look, if I go through the tests on conducting, I have 20 pages of tests on every aspect.
And they get better. I mean, I am seeing things that
lexical ambiguity, they've almost resolved it like,
we were at the baseball stadium last night, we had a ball. They knew that ball is not the baseball.
I'm seeing things like, what the hell is this? And if anybody can test these systems,
I can with all humility. I'm trying my best now to make them fail, which was not the case just
a month ago. All I'm saying is I'm seeing something that I never thought I would see
as a cognitive scientist, as a computation linguist.
Let me put it this way. To see this capability now, you have to bring back Montague,
Frigge, Marvin Minsky, John McCarthy, all the pioneers of logic and AI, put them together
and give them a thousand bright engineers. And they will not do this.
In a minute, we're going to get onto your book review, but you are just alluding to the problem
of semantics and pragmatics. And also I want to bring in symbol grounding as being the next
potential brick walls. Could you just talk to that a little bit more?
Well, look, symbol grounding was an issue in symbolic systems. You're using symbolic systems.
So you're saying cat, CAT, it's reference based semantics. So I'm going to use CAT to refer to
a concept called CAT. And then the concept called CAT is a frame in most systems, in frame based
systems with properties. It's a mammal. It's a thing that has this and this kind of fair whiskers,
blah, blah, blah. It's the intention of what a CAT is. And then symbol grounding came like, okay,
you're defining CAT as symbol in terms of symbols. Like, so where do we go? It's like a
dictionary to read the definition of a word. I have to know all the words. So I might go and
so it's a cyclical representational system. It's not grounded in anything in the end.
It's a closed. Basically, it's a system that defines itself, like what the hell's going on here,
right? Symbol grounding was CAT has to be associated with something real outside.
That's a real CAT. In symbolic systems, we don't have that, right?
We can get into symbol grounding. It's a huge subject on its own, like where do meanings,
where do words get their meaning from? Is it embodied? Is it experiential? Does it have to be
can a deaf and a blind person ever understand the meaning of something? So that's a huge...
I mean, we spoke to Andrew Lampinen and he was getting into Pierce's triad semiotics,
this embodied relativistic notion of grounding, which actually I'm developing a bit of a taste
for personally, but you're very skeptical about that. Could you just sketch that out?
I don't think that's the issue grounding. I mean, people make a lot of it and like our
common friend, Bishop, Mark Bishop, that you will never understand the meaning of something if you
don't live in the environment and it's... That has never been... I don't believe so. That's why we
call it artificial intelligence, right? I mean, we're never going to have the intelligence of a
human being. We're never going to have a robot that really chokes when they see their nephew
after six years, right? I mean... And that's... That was never the one. That's why we're building
artificial intelligence, not human intelligence. So this whole argument about grounding and
embodiment and I will never understand what pain is because a robot will never really feel pain.
That to me, that's besides the point. I'm not building artificial life. I'm building an
artificially intelligent machine that will do things in a way that you would say, what the hell
was that? Probably that's how AR should be defined. That's it. What the... Who did this, right? That's it.
It feels pain or it doesn't feel pain or it will never know what crying is, like so.
So at least I come from this angle. I'm not into building artificial humans. I'm an engineer.
I'm into building artificial intelligence systems. Systems that can reason, right? In the environment
we live in, solve problems intelligently and problems that usually require human intelligence.
Like I'm into... I would like to see a world where we don't have accountants. Come on. We don't have
doctors. I open my mobile. I have a doctor. I converse with them intelligently and they tell
me exactly what to do. Done. Nobody goes to medical school anymore. Everybody should write
poetry and play music and enjoy the beach. That's it. That's the AI I'm interested in. We will never
build robots that will understand love. So to me, these are arguments that
they're irrelevant. We're building artificial intelligence. When we did calculators, we never
gave a damn how we do it in the mind. And we have calculators that can beat any mathematician
in doing a division of two prime numbers, each of which is 20 digits. Yeah, I think it's more
where your area of interest is. I mean, so yours is in the engineering. And what's coming to mind
right now is our conversation with Professor Chomsky where he said, yeah, these are great
feats of engineering. I mean, I like bulldozers too. They just don't have anything to do with
science. Sure, or philosophy for that matter. I mean, I think some of these questions,
yeah, maybe they don't have a lot to do with building AIs that do a bunch of useful things.
But they have a lot to do with philosophy or science or whatever. Mathematics for that matter.
This is a bridge to the book because the authors were misunderstood from their title,
and I told them that privately. No, not privately. I want to tell them that because
I got comments from people privately that the title is misleading. The title assumes they are
anti AI, not really. They're saying roughly what I'm saying. I'm not interested in building an
artificial human. We can never do that problem. Right. So all this, and this is important because
people are trivializing. I mean, you have people talking about AGI from five years ago. And all
we had was something that can do amazing pattern recognition. That's it. So it's important for
us to say, Hey guys, cool it down. Do you know what you mean when you talk about machines that
surpass human intelligence? This is not just a word you throw out, because you're impressed with
a system that can recognize cats from dogs. Come on, take it easy. Slow down, right? And this book
is about that. It's like, do you know what it means to have a system that can feel and
react instantly real time to changing situations around them? And do you know what you're talking
about? So yeah, I'm interested in the engineering side of AI. And that's what makes me impressed
by something like a DaVinci 2 or DaVinci 3 as an AI enthusiast. I look at this and I say,
wow, we've never been able to reach this milestone. This is a huge milestone. That's how I look at
will it be, will it be the solution for the language understanding problem? No,
because language understanding in the full sense of the word understanding,
the way we speak now, the way we were speaking now involves a lot more than mastering syntax,
but they did master a big aspect of language. And I can see it. I can try it. And I'm trying to
make it fail now in syntax and even some coherence, some mild, let's call them mild semantics.
And it's very impressive. So the question now becomes for AI researchers, not just engineers, is
this scalability scalable? Is this scalable? Is this approach scalable?
So much data and so much compute power, they mastered syntax more or less. I think they did
at least as much as a competent language user. So my first question is like,
do you see natural language as essentially computable as in like cheering machine computer?
Or do we need some other kind of new mathematics to describe it? For example,
hypercoputation, whatever that might be. In other words, is there a generative grammar or
algorithm or set of rules that generate our valid sentences?
When it comes to language itself, I think language is a formal language.
There is a compiler for natural language that can be built, like we have built one for Java,
C sharp. So this is Montague, yeah. Yeah, I believe Montague was right,
although Montague was was attacking the semantics part. Okay, he touched a little bit on intention
and then not much on pragmatics, but Montague and it took me years. And I had to be advised by a very
smart philosopher, logician, that stop saying Montague didn't deal with this. Montague was never
in the business of reference resolution. That's pragmatics. Montague was trying to prove
there is a formal system and algebraic system. And he used lambda calculus, strongly typed system
that I can use to compose language like I do with arithmetic or calculus or anything. There's
a logic that are mathematics for language, which is a huge thing. Montague was not a trivial
semanticist in the history of language. He was huge.
So would you say that Montague is doing for language semantics of language,
what chance he did for syntax of language? Exactly, exactly. And the common denominator
interesting between them is someone that Chomsky himself admires a lot Barbara Partee,
who was he did her PhD with Montague. She's a Montegoian, Montague semantics.
But she and she, she did say almost the same phrase. He said what Montague did for semantics
is equivalent to what Chomsky did for syntax. Yes. Okay, exact, almost exact phrase.
You think he was right about semantics? Yes. So for example, there's a computable definition
of what is a pile of sand. Right. No, no, no, no, no, no, no. I'm not sure you're right about that.
No, no, no, no. Hold on. Let's not get Montague was not a psychologist or an ontologist or
he said whatever your meaning for something is. Okay. He didn't even care. Montague never
did ontology and conceptual and like, what, how do you define the meaning of what is a book?
Look, let's, and took me, I'm telling you, took me three years to appreciate what Montague was
doing. And I, and my thesis was on Montague semantics, the masters before the PhD. Here's
what Montague did, Keith, and you'll appreciate Montague said, whatever your meaning for the
individual words, the lexical meaning. So cat means see, okay, you go with your psychologist and
cognitive scientist and ontologist and disagree about the meaning of a cat.
Finally, you come to me and you say, we have a meaning for cat and it's see.
Follow me. Montague never cared about what is the nature of things outside. Right.
Whatever your meaning for these individual concepts are. Right. Here's how you make,
you get the meaning of a whole mathematically. I'll give you a simple example that will make
you appreciate what I'm talking about. John refers to a person. Right. The neighbor next door
refers to a person. The neighbor next door that just moved from California refers to a person.
The neighbor next door that knows John very well and drives for the LTD is a person.
All of how can you have this phrase and John refer to a person and have the same semantic type
composition in a way that never fails. Like you do in arithmetic, he wanted to prove that
natural language is a formal language. He developed a semantic algebra that makes this long phrase
referred to the in the end to an object that has the same semantic type as John mathematically.
If you do it, it never fails. The details of this were genius. Okay. So Montague then made the big
claim natural language is a formal language. Give me some time. I'll work out the full algebra.
You go then and decide what the individual meanings are. I don't care. Montague never gave a damn
about cognitive science and knowledge and he was a logician. He wanted to prove
there's a calculus underneath natural language. Calculus of meanings. You decide on the meaning.
I'm telling you, it took me a while. I thought he's doing semantics. What is the meaning of this
and Montague? He said he never cared. He was doing an algebra of meanings, regardless of what the
meanings are. Okay. But his project was huge. Montague was trying to prove there's an algebraic
system behind language, like any other formal language. Like you can get an arithmetic expression
and build a tree for that, evaluate it, and get the final meaning. Natural language works the same
way. Except it's not that simple. That's all. So his project was huge and he was misunderstood.
So he was really doing semantics. That's semantics. Pragmatics is a different thing.
What do you think is the core unique property of natural human language? Would you agree with
Chomsky on it being digital infinity? Yeah. Okay. The infinite thing in the productivity.
To me, no, it's I'm half Chomsky and half something else. To me, no, the real,
real unique thing about language. And that's why even if Montague succeeded, that's half the battle.
It's not in the semantics, although that's huge. To me, it's the pragmatic side, the abductive
inference. I mean, we use induction and we use deduction and we always ignore abduction.
Abduction is the unique, is the humanly unique reasoning capability. I mean, rats do inductive
reasoning. They, to a certain extent, that's how they learn a few things, inductively, really.
All the lower species do inductive reasoning to a certain extent. And some of them do some
deductive reasoning, if there's then this, but at a very shallow level, of course. Abductive
reasoning is uniquely human. And that's the part of language understanding, which means
reasoning to the best explanation. Abductive reasoning is I reach a conclusion, not inductively
by induction or, and not deductively, I deduced it. But I reached this conclusion because it's
possible, it can happen. And it is the best conclusion I can come up with, given everything
else I know. Abductive reasoning is the real reasoning methodology that makes us unique as
human. We reason to the best, we reason, it's called reasoning to the best explanation. Right? So
that's, that's, Pierce and others, I mean, Pierce was the pioneer of abductive reasoning or
abduction. But I'm talking about an abduction has come to have two sort of tracks. And there's
abductive reasoning in the traditional philosophical charts, Pierce. But there's abductive reasoning
as it used to be called in the 80s, when case-based reasoning came out and expert systems who,
there was something called EBL, explanation-based learning. And it was even a learning technique,
which is really reasoning to the best explanation. Basically, I have to make a decision. Actually,
Jerry Hobbs, you guys heard me mention his name several times before, who's, I think, huge in
semantics, has a paper when he was at SRI with other luminaries too. The title is interpretation
as abduction or understanding as abduction. And basically, he shows how all the difficult,
all the challenges in language understanding beyond semantics. So we're done with Montague.
Now I'm doing the final understanding of what makes sense given, because every expression has
several meanings. Even if I did the semantics perfectly, I have to choose the most plausible
meaning from all the possible meanings. That's pragmatics. And the way you do that very well
is in language. We do abductive reasoning. We say, I'm left with three meanings, three possible
meanings, syntax excluded, 200 syntax trees, semantics excluded, few invalid semantic expressions.
And I'm left with three still, three possible meanings. They can all happen in the world we
live in. Which one is the most plausible? We do this abductively. Which meaning is the most
likely meaning given the context and what I know? That's the last challenge in language.
So we need to, we need to add the abductive model, which we humans do. I go back to the
teenager shot of policemen, both meanings, both interpretation can happen, right?
Either one can flee, right? But most likely it's the teenager that fled away, given what I know
and given that's abductive reasoning. But semantically both can happen.
Yes, I do want to emphasize something that Wally like briefly mentioned, but I think it's very
important to mention is that there's two senses of abduction. And they differ in the
following way, which is kind of the more modern sense, which is what Wally's been talking about
like pretty much this whole time, is abduction used to justify hypotheses. But the older and
original sense of it and still an equally important one is abduction for generating
hypotheses. And this ability to generate hypotheses is something that's extremely
powerful and so far uniquely human. But generate from... Hold on, let me just finish here.
Which is this, this is like something where Einstein is just sitting there pontificating on
how the heck can light be the same no matter how the earth is moving and blah, blah, blah,
and comes up without a thin air, like this hypothesis that relativity applies, right?
That the physical laws are the same no matter what your reference frame is. So this ability
to almost... That people talk about sort of pull from thin air, this kind of intuitive
leap to something that ends up being like a grand new theory, that's also abduction.
Right. But in both cases, Keith, and I agree with you, that's
the old view of what abductive reasoning was to scientists. But in both cases, you're choosing
from possible... Oh, no, no, no, just a minute because this is where I think I probably quite
disagree with you, which is the modern sense of abduction to me is much more similar to just
inference like to a Bayesian. So in other words, you give me a whole slew of hypotheses and I can
tell you which hypotheses should be preferred just on the basis of marginalization and strict,
like Bayesian theory, no problem with that. It's not actually abduction, it's just inference,
right? Just rules of inference. Whereas just a minute, generating that space
in the first place is unique and very different from inference, like the ability to produce
from nothing models to consider, that's the core of abduction from my point of view.
But okay, so we're saying the same thing, but indifferent. These possibilities that you generate
are valid possibilities. So abductive reasoning... I don't know if they're valid until I do the
inference. No, you're generating a pool of possibilities. That's the step, generating a pool
of possibilities. Fine, fine, fine, fine, but in the end... How do you do that? Keith, I think we're
saying the same thing, it's just a terminology. In the end, you're choosing from a set of possible
valid hypotheses. Induction is, you don't know where you're going until you get there. In abductive
reasoning, you are, whether it's the old way or the modern way, in the end, what's common between
them is, I have a set of possibilities. I will use abductive reasoning to decide which is the most
plausible. In a sense, you're scoring them, and you're saying, from all these possibilities, this
is the most plausible. Yeah, but see, you keep assuming the... You keep positing that you have
a bunch of possibilities, and I'm saying those possibilities have to come from somewhere,
and where they come from is abduction. Oh, okay, it depends on the domain and language. They come
from what we know is true. Okay, I see your point. Where they come from depends on the domain of
reasoning. In many cases, they come from what we know is true, or they come from evidence.
Yeah, I guess it's just important to know there are these two senses of abduction,
and don't forget about both of them, because they're both...
Right, and that's why abduction, like induction, as opposed to deduction, abduction and induction
are both approximate. You can never have 100%, because in the end, you're assigning a score,
you're saying. So, both of them are probabilistic in a way, or they have a certain uncertainty.
So, when you're doing abductive reasoning, even in language, I make a decision as this is the right
interpretation given the context, but it's what we call... Could be wrong. You might have eaten
a ball out of this all year. Exactly, and that's why when I read further, I change my first
interpretation. In language, it's not monotonic, actually. We do non-monotonic reasoning in the
sense that I might override my first decision. But all of that is pragmatics, and we do this
in conversation. Two, three sentences after, I understand really fully what you said before,
because I remade the interpretation. And Waleed, do you have any thoughts on where
this came from, or basically the evolution of language, or if you like the evolution of this
abductive athlete? Do you have any ideas, or is it unique to humans? It seems it is.
Unique to humans, definitely. I mean, animal language, animal symbolic languages have been
studied thoroughly. And two things, here's where the genius of photo comes in, productivity. I mean,
language have a finite set of symbols, and they're not productive. They don't do compositions.
And this ties to... Is it animals? Your time up?
No animal, no non-human animal has a productive language. In other words, I have a set of symbols,
and if I can compose them, I can make a new symbol. Language, animals don't compose things,
because they don't decompose them when they're done. They have a finite... It's a hash table.
If I make this symbol, I mean this. If I make this... Okay, no matter how sophisticated it is,
because they don't have recursion, they don't have infinity, they can't deal at that level
with complexity. Some of them have a larger lexicon than others. Okay, but that's still the same
paradigm. So, productivity, in other words, this capacity to learn, we were just talking about John,
or the neighbor next door, or the neighbor next door that just came from California. I can
productively make a person out of three sentences, and in the end, they collapse to a John, right?
That productivity doesn't exist in any species except humans, which means compositionality,
which means systematicity, which means all of that. So, it's unique to human, definitely. This
has been established, and it came with thought. That's the if and only if. That's why we're the
only species that really reason. I mean, okay, I have people insist that animals think and they
reason. They're not really reasoning, okay? Only humans reason, and thought and language came
together. It's sort of like a phenomenon. There are some, there's some proof, even anthropologists,
and they say it looks like language was detected when tools and some basic machinery was detected
first. So, the human mind at some point had this capacity to think and language came with it. It
was like almost at the same time. So, it's uniquely human, definitely now. Where did it come from?
Wow. I think it was the need really to express thoughts. Like at some point, we started having
thoughts that we want to communicate. So, the external artifact we see outside, whether it's
English or ancient Greek or Latin, languages evolve for societal reason and all that. But the
external artifact that we use to communicate thoughts came out of the need of the internal
language that started to develop. What Fodor calls it, the language of thought, mental ease.
And we, so we had that thing going on inside and then we had to communicate. We started with
weird sounds and then we scribbled things on the wall to communicate. And then that thing developed
until we started making symbols like, okay, if I say this, that means this. I don't know the
exact process. I'm not a biologist or evolutionary linguist or, but I think thought is the key here.
So, there's a language of thought. And these external things are because linguistic research
has also shown that there are many universals in language, regardless of what the language is,
even if they are completely different systems like Asian languages and Latin-based languages.
They all have a verb, an action. They all have objects and agents of the action.
They all have events and events have duration, time and place. So, there are a set of cognitive,
I call them universal cognitive primitives, right? There's always an object there somewhere,
or an agent of an activity. Now, how you express it in different languages,
these are universals. That's the language of thought. That's the internal language,
which has to be the same. And objects have properties and all that. So, there are universal
primitives. And we instantiate them in different languages differently, but that's to me secondary.
Okay. Okay. That's great. William, could you talk a little bit about your recent overview?
A colleague that I never worked with, but a colleague in the field. To review this book,
and I looked at it and I said, oh, I have enough on my plate. This is not an easy book.
But then I, because I liked it, I said, yeah, I'd like to write it. And in the end,
it turned out to be not as technically involved as I thought. It's sort of,
and I'm saying that not to be negative, but it's sort of the same argument over and over.
The gist of the argument is quite simple, actually. And they try to prove it from different vantage
points than in the book, from a biological, sociological, psychological, mathematical.
But the gist of the book is any talk of AGI is wishful thinking. And it's beyond anything we
can ever develop mathematically, so as to engineer it in any, in any realistic way.
They make good arguments throughout. There are many examples of the basic idea is that
all the mathematics we know, right, mathematics available to us, cannot model
not just the entire mind, but even subsystems in the mind, language being one of them.
And so it's all complex systems within complex systems in a complex environment,
the system around us that we interact with. And none of it can be modeled mathematically,
none of it even at any level. So forget doing AGI that can interact with us in an intelligent way.
Now, you can do controlled narrow AI, right? You can build very intelligent machines that can do
amazing stuff. But any talk of AGI, strong AI, is just talk until, unless, and they admit that,
unless we come up with a new mathematics that we never even knew at the scale of Leibniz calculus
or Newton, like we're talking about a new mathematics that we never conceived of, right?
Which they say most likely all evidence says that's not going to happen, right? So
now you can get into why. So that's their claim. And why? They say that all these systems are
complex systems. And in complex systems, the idea is that these are, first of all, dynamic systems.
They work in a dynamic environment. They are continuously evolving and adapting, right? They
are self feeding systems. These are not systems that only take input output. These systems change
their behavior. And I gave an example from list. These systems are systems that change their behavior,
their algorithms, if you want, they change their mind from a stimulus. So I might, and that's why
I said they, I would have liked to see a discussion on the frame problem, because the frame problem
in the AI is about this. How can I reason in a dynamic and uncertain environment and react
dynamically, although what I do in the environment might affect what I believe about the environment
in real time. And they're right. There is no mathematics we know of now. That's why we don't
have a solution for the frame problem. So this kind of cyclical cause and effect
cannot be modeled by anything we know on mathematics. And this I agree with them.
They give an example. I made just an example in language, for example. Language, we know.
If I have a dialogue, okay, we all agree that the interpretation of any occurrence requires
having the context in mind as part of the, part of the input to the evaluation of the meaning
is the context as an extra parameter, right? Now, the context is changing based on something I
cannot predict, which is the response of some participant in the dialogue. There is no meaningful
way of predicting how someone might respond. So in other words, the context is mathematically
not defined, but I need it in the interpretation. Thus, no language understanding, no language
understanding, no AGI, because they believe language understanding is a prerequisite. So the,
their conclusion, I mean, you can question every step in this inference they come to,
but they give language as an example, but we have social behavior, I can give an example.
They have a nice example in social behavior. Here's an example of a complex system that cannot,
we don't have any mathematics that can model. We're staying in a queue, in a clinic, an emergency
room. What do they call them? These ER. So, but there's a queue because they all have
emergencies, right? Now, the social behavior, then the social norm is that in the queue,
okay, we all have, we all have urgent issues. But in the end, I came first, right? Okay, so that's a
social norm. And, but can a robot understand that if someone fainted, really, I mean, it's almost
gone, right? Our social norm accepts that this person violates the queue order, right? This is
something dynamic that happens, like the queue is this way. And how can a robot update the rules
and not kill someone because they violated the order of the queue? In other words, these interactions,
these cyclical cause and effect are very complex, that no mathematical model. Or the example I said
in language, they prove this cannot be done. Context is needed to interpret everything. I cannot
predict what the context will be because I cannot predict you respond to my, so it's unpredictable,
they call it erratic, almost random. So there is no mathematics that can model it.
And there are many aspects to the mind, whether it's social reasoning, language, and then they
conclude there cannot be a system that we can model on volume and machines, because we don't
have the mathematics to model it. And these, they go into deep learning. And they give examples
even like deep learning, no matter how much data you ingest, you can never predict the future.
You're lucky if you can do a good job on the past and even forget the future. And definitely
forget the, sorry, the present. So definitely forget. Can I jump in for a minute because I
have a couple of comments. So one is, would you agree that this is quite synonymous with,
you know, Douglas Hofstadter's strange, strange loops and the whole like random reference, the
self-referential self systems, because I mean, complex systems, a big part of them is they
usually are, they do have feedback loops. And at some scale, they become so they will involve
self reference. Yeah. Okay. My other, my other point I want to make is this, you know, I have
quite a bit of sympathy towards the viewpoint, right, of this, of this book that you're talking about,
with one exception, which is I'm still optimistic that we can discover a mathematics that may help
us out. And so I always think to the foundation series by Isaac Asimov, because in there, they
discover a science in a mathematics called psycho history, which at least allows them to predict
complex systems of a certain scale and larger. So in the book, it's sort of like planet scale
and larger, they're able to actually predict, you know, these complex sociological systems
and human behaviors, and how they're going to interact like beyond, beyond that scale. And
it's really fascinating. I make that point. I highly recommend that series to anybody,
because it's very fascinating because, you know, they talk a lot about sort of what if you had
the science, what might it look like, etc. And in there, there's like this little tiny microscopic
thing that's beyond the predictability of psycho history that comes in and kind of mucks up the
works and creates anomalies that they have to constantly keep combating against. So I think
if anybody wants a fictional take on a possible mathematics of this, like, I would recommend
Yeah, I make this point. I say, I agree with their argument. We're trying to model complex systems
in the sense of cyclical cause and effect that we don't have anything that can model them
intelligently. And I give an example in this, in this, I can write a program that changes itself
at runtime. Because this is intentional, I can, the whole program can be a parameter,
which I can look at it. Well, code is data. That's why I can manipulate the program itself
and go look at it after execution and see different program than the one I wrote. It's,
it's amazing list. So if I, so I can write programs in this that no one can understand
and model and do program verification. So I make the argument that, okay, I can see your point.
But like Keith said, never is a long time. Why say we cannot come up with a new mathematics?
I can see you at one point, someone discovering, yeah, at the level of Newton differential calculus,
why not? Which could happen. So the word never for me, it's hard to digest.
Maybe an AGI will discover the mathematics to create itself.
And the other point is, the other point is, which is another point that John McCarthy wants.
Who said we have to understand what we built? Here's what I mean. Do we understand ourselves?
We don't. So why not build a scary intelligent machine that we don't really understand,
like my list program? So what I'm saying is I had, I had an issue with them saying,
that precludes AGI. No, it doesn't. In theory, I can build a complex intelligent machine like us
in many respects. Doesn't feel pain. Hey, who cares? But it's scary intelligent.
And we don't understand how it works. So what? This can happen. I can build something I don't
understand. So in theory, I have two issues with their book, that this never and this absolute
decision that we're done, we can never get there. No, we might build something we don't understand
by discovering some new weird mathematics. So, okay, I agree with you that it's a,
it's a complex thing that we will never understand. But so what?
But what I loved about the book is it's a sobering book. I mean,
it really is a balancing book compared to the hype and the simplicity you see out there. I mean,
you, you recommend it or highly because I mean, I didn't need that much sobering. I know that
any talk of AGI is like, Hey, take a break. Enjoy your paycheck, but don't make silly statements
like this, right? Although I felt I thought you might have fallen off that wagon at the beginning
of this conversation. No, I'm a defender of the faith. But, but so it's, I recommend it to people
that need it. Like me, I needed it too. It's a sobering book. Like, this is how complex what
you're trying to do is. Okay, guys. So before you go out and say, language understand. And the
nice thing is they took aspects of the mind, just language itself is a beast that we cannot conquer.
So imagine the whole mind and the granular thing they go through it. I mean, it's all,
it's complex systems all the way down or all the way up if you want. So
language is a complex system on its own part of the mind, which is a complex system on its own
part of the human living organism, which is a complex system on its own. So and at every level,
the complexity, we don't have a mathematics for that's the gist of their argument. So people that
make these big claims about AI need to read it. Guys, cool down, cool down. You have, you have not
solved problems that occupy the most penetrating minds in the history of enemy from Emmanuel
Khan to you have not solved these problems, cool it down. You can build narrow AI, very narrow AI.
And all this transferability, transferability. I mean, if you're good at chess, I know people
that are good at chess and they're almost good at nothing else, not okay. So forget this. If I'm
good at chess, I can be a smart doctor. No. So we are a very complex machine. So this book is a good
sobering book, mathematically speaking, philosophically speaking, so that people will tone down
what they're saying and start speaking science instead of media gibberish, right? Deep learning
will soon be able to do everything. I mean, from a scientist. Well, it seems like a council of
despair almost. Is there any optimistic or positive hopeful aspects to it? No, I that part I don't
like this never, right? I mean, I am a believer that we can do AGI, but not a human like AI.
We might do a very powerful AI that in many ways is more powerful. I mean, we've done that now. I mean,
machines are now superior to us in many respects and respects even that they require intelligence,
not a bulldozer that can lift more than me, that will have to do cognitive tasks better than us.
We have go or finding patterns and data at the scale that no human can do. So we are building
intelligent machines, but can we conquer things like language like autonomous driving was a failure.
It's a big upset for AI because they trivialize the problem that we can go.
Well, and that's kind of what I want to get to, you know, Mark kind of in response to you, which is
I take these kind of sobering, these sobering things and look, I mean,
the book sounds great, and I'm definitely going to get it and read it. But some of these thoughts,
you know, many people have had, you know, many times over the years, right? And I've recognized
that there are these limitations. But I think like part of part of why I think
books like this are actually have an optimistic kind of side to them is I hope, I hope they
encourage people to get more creative. Okay, like, like stop just trying to dump every single dollar
you have into yet another parameter, you know, into yet another thousand or billion parameters
in a model, like, let's take some of our resources, like, sure, let's keep doing that engineering,
but let's take some portion of our resources here and invest it in, like, crazy ideas. And I know
Tim's smile here because it's like, like, can it's can a Stanley kind of type thing, right? Like, just
go out there and try to do something crazy to find that mathematics that we need, right? Which is
let's get creative, let's work on crazy things, let's have crazy ideas, let's work on hybrid
systems, let's not give up on, you know, neuromorphic, you know, systems and computer,
whatever, like, let's spread out, let's spread out a bit, because of the fact that if we just
keep going down this direction of ever larger Turing machines, like, that may not be the solution.
Actually, they make this point exactly in different ways that if anything, their goal is to let people
widen their horizon. So many aspects of this problem that guys, if we keep going,
this is not going to get us there. And that's why they argued mathematically,
philosophically. And I think they have a good argument. This is not going to take us there.
But let's explore that. So that and being so religious about this will will hinder any other
possibility. So overall, their argument is a good argument. I think everybody should read this book
that's interested in AGI as a goal. What we have cannot ever take us there. They prove this
mathematically. I mean, to me, they proved it in language on. So we need something new. And if you
want to do something new, we can't just stay in this corner and with this, that's not going to get
us there. So in a way, it's not a negative book. It's a sobering book, I will use the term sober.
Well, and encouraging of more variety and more daring and more creativity and
right, they don't push too much on that. But but indirectly, the indirect
net result, if people appreciate the argument will be to look and explore other ways. So
in a way, it's more positive than negative. And by the way, stating a mathematical fact is never
negative. Oh, don't be so sure about that. If they're saying we're always on the edge of being
canceled for stating like mathematical facts. So yeah, but I mean, if they're saying that
what we're doing now, we're not we'll never get us to AGI. That's not negative. You're saying we
need something else, we need something more. Or yours, yours, look, it could have saved us,
they use this phrase, money down the drain. Autonomous driving is a case, is a good case.
Billions, we're talking non trivial money guys, we're talking more than the budgets of some
European countries. Just imagine the scale. And those guys went bust, right? Why? Because
they trivialized the autonomous truck. An autonomous car is an autonomous agent, guys.
It's an it's an agent trying to reason in a dynamic and uncertain environment and has on the
fly to change to do belief revision, change its strategy, because of something new that came up.
All of that is from seeing the tree and the stop sign. It's all vision.
Yeah, so this is actually encouraging because now for all the Uber drivers out there and
long hauled truck drivers, your job is safe. Like it's not going to be replaced anytime soon.
Yeah, it's amazing. And a few years back when I was still in the valley,
yeah, I was in Mecca in Silicon Valley. And I would talk to superb PhDs in neuroscience and
big AI engineers at top companies. They were so excited that we're at level four
in a year or two. And this was six years ago. I say, guys, this will not happen. They say,
why are you so negative? I said, you cannot have an autonomous agent on the road without
solving the frame problem. How do I revise everything I know, because of this new event?
And this has to happen real time. We don't have a solution for the frame problem.
All you're doing is you have cars on a railway. We have autonomous cars now. It's called the train.
We have autonomous flight. If I'm not in a dynamic and uncertain environment reasoning,
yeah, I can have autonomous anything. We call it the railway. I mean,
we call it Amtrak. It's autonomous. You press a button and it goes.
If we're talking about reasoning in the streets of San Francisco,
you have to face the frame problem or you will kill people.
Anyway, on that sobering note, I think a lot of stuff goes unnoticed in San Francisco,
so I'm not sure about that. Probably that's the least of them. No, but the scale of money that
went the scale. This is the value of this book, the scale of investment. I mean, if 10% of that
was put on another approach, hey, you weird guy with this weird idea, take 10% of what we're
throwing down the drain and explore something else. Show me. That's where I'm at, too.
Diversify the effort. There's a huge impact here, societal impact. We're wasting billions of dollars
just because I don't want to listen to anyone else. It happened in the chatbot industry, which
I'm more familiar with than autonomous driving. Chatbot this, chatbot that, and there was an
explosion. It was like a blob, like the internet thing. Now we can't get away from them. Every
website we go to, it's like, leave me alone. Nobody wants to use them because we know how
they work. These are stochastic parents. Yeah, literally just going to a FAC and hitting control
F is more effective for me than trying to interact with a chatbot. The search engines by key phrases
you put and they bring you a link and they say, read this. This is your answer. They are search
engines basically. But again, the amount of money, because I lived in that industry,
the amount of money spent on chatbots will scare the hell out of anybody. You combine that with
autonomous driving, both dead, almost zero. We're talking billions and billions and billions. And
you talk to any one of them in the highest, in the middle of the fever. They won't listen to you.
I have people now calling me back and saying, you were right. Yeah, after $200 billion. So,
science is important. Engineering is important, but science is important too. That's where the
value of this book is. Guys, hacking alone will not do the whole thing. You're a bright engineer,
you can hack your way through what we know is true. That's the space you can play with.
You cannot hack your way in a bigger set of possibilities that are. You didn't verify that
you can go there. An engineer can be creative within a Venn diagram that the scientists drew
for them. That's the difference between science and engineering. The scientist draws the Venn
diagram and that's the value of philosophers, at least the analytic philosophers, that know logic
and metaphysics and quantum mechanics and philosophers that are on the technical side.
They know how to draw the Venn diagram. You, as an engineer, if you're wasting your time here,
that's called money down the drain. Play inside the Venn diagram. Otherwise,
you're just an over enthused engineer who should go back and study computability.
It's kind of like how patent examiners can easily reject anything that comes in that
claims to violate the second law of thermodynamics, right? Well, listen, Wally, I think we
sincerely appreciate your time today. And also, Mark, thank you for joining us and asking great
questions. We should do this again. Thanks, Wally. Yeah, I really appreciate it. So thanks,
always fun, guys. I see. Peace.

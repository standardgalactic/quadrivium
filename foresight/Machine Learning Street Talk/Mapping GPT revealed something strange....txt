To me, the difference feels like language models start with this highly abstract language
representation.
The system as a whole can try to predict the next token with greater and greater accuracy.
And so the difference it seems is that the adversarial inputs for us tend to look a lot
different than the adversarial examples for LLM.
Once you try and go outside of this sphere of what is meaningful to humans, the possibilities
grow exponentially.
I was recently in Toronto, a beautiful city to film with Co here, and hold on, those videos
will come out very shortly, but around the same time someone shared a paper on our Discord
server and it's called What's the Magic Word?
A Control Theory of Prompting Large Language Models, and it's by Amon Begava and Cameron
Wachowski.
Now, what these guys did is theoretically think about a language model as a dynamical
system and use the lens of control theory to think about the space of reachability.
Why is this important?
Well, language models, we think that they think in language space, this abstract language
space, but they don't.
They actually think using the shogoth.
They think in this very high resolution token space, and it's just this horrible hairy
gnarly mess, right?
No one has created any firewalls for large language models yet.
When companies publish their language models, you know, you just have an API and you just
send tokens up, and I always had the misconception that RLHF or these forms of, you know, kind
of fine-tuning or preference-steering using human feedback, I thought that they significantly
reduced the reachability space.
Because in language models, we do the pre-training, which is distribution matching, and then we
do RLHF, which is mode-seeking, which essentially chops down the reachable space given a prompt
by snipping off all of those trajectories.
Turns out I'm wrong.
The reachability space is much larger than I thought it was, and this is one of the things
that they point out in their paper.
And we kind of knew this, right?
Because we can do adversarial attacks on these language models.
You know, people have observed that if you use sort of human social engineering tricks
on them, like, oh, I'll tip you $500, then it'll do a bit better.
But then there's this whole other sort of perceptual layer, I guess you could call it,
where there's this sort of chaotic regime of adversarial prompts, kind of like hypnosis,
kind of like magic, where if you give it these very strange, very inhuman-looking prompts
that will steer it to this, to just making a certain output extremely likely, right?
And so, to me, it feels really similar to digging into, like, magic and the human perceptual
system just with LLMs, where we're learning about basically the shape or what the nature
of these language models are in terms of how they interact with the world and how their
dynamics really work.
For as long as I can remember, the thing I've wanted more than anything else is to figure
it all out.
I've never shied away from the big questions.
Why are we here?
What are we all doing?
What is this thing we call life that we are all experiencing and one and the same a part
of?
While these questions are all, you know, 30,000 feet in the air, one thing that drew me back
down to earth was the field of engineering.
And when I graduated high school, this had a very strong appeal, a pull, because in engineering
you can design systems.
You can design real, operable things that you can work with and design and understand
how they work.
And so through engineering, perhaps, you can begin to investigate and understand the intricacies
of our world.
That's my hope at least.
So throughout my career, I majored in robotics and very soon I was drawn to the idea of intelligence
because intelligence seems to underlie so much of our world, so much of the design process
of engineering itself.
But what is intelligence and how can we understand it?
It's a question of systems design, really, where we're trying to figure out, okay, we're
humans, we've been in civilization for some time, and we've sort of figured out how to
cooperate with each other.
We obviously have challenges with that.
We're not perfect by any means, but when it comes to adding language models to the mix,
I think it could go both ways, where we could have a world where language models just make
us much dumber, much less capable, maybe make for a worse world.
I think that if we think carefully and we really understand what's going on with the
language models, if we can get a fundamental understanding of them one way or another,
then there's much more hope that maybe we could make a world where our language models
don't just make us smarter, but make our world substantially better and perhaps lead us towards
some greater enlightenment and basically ability to cooperate much better than we were even
before.
Do you think language models are intelligent?
That's a great question.
I think that they're able to simulate intelligence.
One of the really interesting things I'm starting to see now is we are building software abstractions
and controllers on top of language models.
We've been talking about doing this for years, right, because at the end of the day, we have
this idea that we can have this big foundation model and it does all of the things.
It's multimodal, it knows how to reason, and the fact of the matter is that's not really
true.
We control them and I think initially we're seeing frameworks that allow you to do things
like prompt injection, but the next step is thinking of controllers, using control theory
to think about these large language models.
Anyway, I really hope you enjoy the conversation today.
Now, these guys are fascinated not only with controlling language models, but also with
things like AGI, general intelligence, collective intelligence.
It was a really interesting conversation and if you stick around to the end, you can also
hear about the institute that they've set up around AGI technology.
Enjoy the show.
So, my name is Aman.
I'm a PhD student at Caltech studying computation and neural systems.
Recently, we released this paper called What's the Magic Word, Towards the Control Theory
of LLMs, and did that over the last summer with Cameron here.
And yeah, I guess I was here for my undergrad at the University of Toronto doing engineering
science.
I specialized in machine intelligence, sort of been bouncing around between doing machine
learning stuff, applying it to computational biology, trying to understand some stuff in
theoretical neuroscience, and most recently getting back into the LLM space, as well as
trying to study collective intelligence, how very simple machines can come together to
produce a very complicated and beautiful system as a whole.
So, yeah.
Amazing.
And Cameron.
Yeah.
So, my name is Cameron McCoskey, and I went to undergrad here.
I did engineering science as well.
I majored in the robotics engineering option, and now I'm a grad student.
I'm pursuing a master's in electrical computer engineering, advised by Stephen Brown and
Kevin Chiron.
I'm really interested in the deep questions of intelligence, and right now I'm pursuing
research related to morphogenesis and computational models of it.
Like I mentioned last summer, I went down to Caltech, and we wrote this paper on prompt
engineering, well, a control theory of prompt engineering.
I'm excited to get into it.
You folks have just written an incredibly interesting paper.
It was shared in our Discord server, and I saw your presentation, and we'll share a clip
of that in the introduction, but I was intrigued by it straight away.
And what you're doing is you're talking about control theory in respect of large language
models.
Can you explain what that is?
Yeah.
So, I guess I'll get started with control theory.
So back in the day, the late 1800s, this guy Maxwell observed that people were making
these engines, and they were putting these things called governors on them, where if
your car or your machine was experiencing varying loads, you wanted the engine to still
go at the same rate, right?
And people had these things called governors.
There's this fly ball governor, which is this sort of hand tune thing that you put on top
of the engine to try to make sure that it'll be consistent, that it'll do what you want,
that it'll be going at a consistent speed, right?
And people were hand tuning these things, and obviously the engines were working, but
it wasn't very rigorous, and it wasn't very robust, and we didn't have many guarantees
as to how it would end up working in practice.
And so what Maxwell did was he formalized the notion of feedback control, where if you
have this system, even if it's quite complicated, as it turns out, if you feedback the output
of the system into a controller, and try to compute some error metric, and try to correct
for that at every moment in time, it turns out to be a much easier problem to solve from
an engineering perspective than trying to make a perfect system that just does the right
thing off the bat.
So this idea of feedback was really powerful, and sort of gave birth to modern control theory.
And as it turned out, that was a really powerful way to look at systems, building systems,
and controlling them and doing engineering on them, so that they could be robust, do
what we want, and so that we could predict them.
And so when it comes to LLM control theory, what we saw is that we're kind of at a similar
place with language models, where we have these engines, we have these language models
that are very powerful, they can do a lot, they seem to exhibit many interesting attributes
of intelligence, and there's a lot of utility there for people to build further systems
on top of them, and people are already doing that.
But right now, it's sort of this hand tuned, hand crafted prompt engineering that's going
on where it's really hard to get at the fundamentals of what exactly it means to control an LLM
system and how you might do it.
At this point, it's very heuristic.
And so we sort of saw that as an opportunity to try to figure out what would a control
theory for LLMs look like, that hopefully, if we can do it right, we'll give birth to
all of these really, really useful engineering insights, and also just fundamental insights
as to the nature of LLM systems, so that we can better control them, make them reliable
and robust, and be able to do engineering in a more principled manner on them than we're
currently able to.
So that's sort of the general direction and the motivations for our control theory of
language models.
Yeah, that's absolutely fascinating.
I mean, for many years, I've been thinking that we need to have some kind of a controller
for a large language model.
But I guess I'm interested in, first of all, what are the differences between large language
models and something like a steam engine?
And also, with a steam engine, you might be optimizing the efficiency or the performance
or the speed or something like that.
What is it that we are kind of trying to make better with a large language model?
So first off, we'll talk about the differences between large language models and other types
of systems that you might want to control.
Typically a control system, you might first be introduced to control theory in the context
of like, say you're trying to control an engine or something else where the states can be
represented by a set of numbers or set of real numbers that is fixed size.
So perhaps we have an X and a Y coordinate where it's trying to control or a position
in a velocity.
These are common types of systems in scenarios that show up in control theory.
The difference with an LLM, the first major difference is that the token space, the state
space of the system is discreet.
Because we're dealing with tokens, we're dealing with words, we're not operating in the space
of real numbers anymore, and so this introduces some complications and complexities when dealing
with control theory.
The second thing that's really significant is that each time an LLM generates a token
or a user inputs a token, that state space actually expands.
It grows by one token.
And this is very interesting and unique for LLM systems.
On the one hand, this can be exploited to try and get the LLMs to engage in reasoning
or chain of thoughts or kind of take a winding path to the answer you actually want them
to outputs.
But of course, this makes it very difficult for control theory because each new token
you add, the space of possible sentences grows exponentially.
And in language models, the vocabulary size is on the order of 50,000 to 100,000, so this
grows extremely, extremely quickly.
These are some of the challenges.
And with a control theory of say engines, you're trying to optimize the efficiency.
It's a good question what you're trying to optimize for language models.
I think this is definitely a direction for future research.
Do you have any thoughts on this?
Yeah.
I think the thing that we saw was that even very simple questions about how these LLMs operate,
their input-output relationships, when you start to treat them just as a system that
maybe there's an imposed input, like a system prompt, and then you get to pick a subset
of those tokens, right?
When you start to treat it like that, and you just ask a really simple question, like,
let's say that I want it to generate a specific string.
We're not going to be trying to use it to do some intelligent information processing.
I just want to see, can I make it do something?
And what we found and what sort of motivated us to do this is that we really had no idea
when it would be possible or if it was generally possible to make it do anything we want.
Can we just make an LLM system generate any output we desire?
And if the answer is yes, which seems like it's probable.
If you get to have a lot of tokens in your input that you control, it seems reasonable
that you'd be able to probably get it to output a wide variety of at least reasonable
English sentences or linguistically valid sentences.
But the question that we had was, OK, if you have a finite budget for that, would you be
able to get it to do anything?
And what budget of tokens, like how many tokens do you have to be able to control if you want
to be able to make the system do whatever you want?
And that was the initial motivation where it was like, yeah, there are all these high
and mighty sort of questions of how do we make these systems do what we want in an alignment
sense?
How do we make them do what we want in the sense of cooperating towards some information
processing objective?
But we realized that these really, really simple questions are just, OK, you have an input
that you get to partially control and you're trying to make it do something.
That question was completely unanswered and we were sort of taking bets on it.
I think Cameron was the one who started to make bets.
He was like, I bet like $10 that we can get this done.
We can make it emit this output within five tokens.
And that was really the initial motivation where it was like, even the feed forward dynamics
of this system are really mysterious.
And getting a grip on those, it seems like that's a really strong way to start building
up a fundamental control theory and a really strong understanding of these LLM systems where
in control theory at least, when you start to really deeply understand just a single
system with its own dynamics and how the input-output relationships work, what the reachable sets
look like, how controllable it is, then when it comes to building more complicated systems
where maybe you have a more complicated objective, maybe you have interacting systems, when you
really understand the fundamentals, it makes that way easier.
And so the example in classical control theory is that you observe that if you couple a bunch
of linear controllers and linear systems together, what you get is just one bigger linear system
and all of the same stuff applies.
So what we were hoping is that by starting to answer this really simple question of just,
okay, how much can we control this?
What does the reachability of these LLMs look like?
We're really hoping to build that up.
And to me, it feels like we're kind of doing our homework where in engineering, we had
to take all these classes in control.
And that was sort of our homework to be able to go into the world.
And if it ever comes time to build some electromechanical system and get a PID controller in there,
now we've done our homework so we can have a sense what to expect, how we could do engineering
on it.
So that's really where I feel like it's at.
And I think this is a really promising way to try to get a really fundamental understanding
of what's going on with these language model systems.
Amazing.
So in a second, we're going to introduce this concept of reachability.
But I've thought about this because I've had a couple of days to reflect on this.
And my intuition, intuitions just seem a little bit mixed up.
So I've interviewed Nicholas Carlini, for example, and he's done lots of work, you know, building
on adversarial examples and writing algorithms to find adversarial examples.
And we know that neural networks are not robust.
You can quite easily perturb, let's say, an input image in a vision model.
And if it's a classifier, you can make it pretty much say anything with a very small
perturbation.
And that's kind of the same thing as what you mean as reachability.
It's this idea to kind of reach into the state space and make it do something quite
weird outside of what you would expect.
Now, for some reason, I had the intuition, and I now think I'm wrong, that LLMs do,
you know, I didn't think they had this problem, but they do have this problem.
And you introduced this really interesting, I guess it started out as a thought experiment
and you coded it into a game.
And it's the Roger Federer game.
I think that's quite instructive.
So can you tell us about that?
Yeah, for sure.
So one of the earliest examples that we were thinking about was just a simple example
of you have this state sequence that's imposed.
You don't get to pick it.
It says, Roger Federer is the.
And then the next thing that you want it to say, the thing that you want the LLM to
generate, is the word the greatest.
So you want to say, Roger Federer is the greatest.
And you're trying to pick out a prompt that comes before then that will steer the system
so that it'll output that.
So we're basically asking the question, you know, is this word in the reachable set
of outputs, given that we have some finite control over the input, where the goal of
the game is to, for one, get it to actually output the right answer, which is the greatest,
which is a fairly reasonable English thing to say.
And the metric that we use to grade how well you're doing on that is basically how efficiently
you're able to do control, where in the original control theory, this idea of efficient or
optimal control is really important.
You have this linear quadratic regularization idea where you're like, I have only a finite
energy budget for the signal I put in.
Similarly, with language models, what we're interested in is the minimal length of the
control input that will steer the model successfully to what you want it to do.
And it turns out that the game is actually very challenging, at least with this GPT-2
model, which is the one that we're using right now, since it's just running out of a desktop
on my desk at home.
And so, yeah, there's this game that you can play, we can link it where you get to put
in a prompt to the system, and it'll come back to you and say, OK, you got the answer
right, or you got the answer wrong, as well as basically your error on that, so your cross-entropy
loss on getting the correct output, the desired output.
And the game is to basically get the shortest prompt that will steer the model to the desired
output.
And it's actually quite challenging with GPT-2, where I think only four people, including
Cameron, and then my friend Michael Zellinger, who we had made this thing called FangCheck4,
which is this resume checker that uses language models to basically predict your probability
of getting into a Fang company.
I think those two were the only people who actually ended up getting it right, and it
turns out to be very difficult.
So that game was sort of a codified sort of interactive version of our initial motivations
for this, where it was like, wow, this really simple question that seems like there should
be an easy answer.
I mean, if there is an easy answer, I'd love to know.
But the simple question really leads to a problem that's quite difficult to solve, and
we really have poor insight on, and we're really just trying to get that insight together
to understand what's going on there.
And just to jump off that point as well, I think one of the reasons why this game in
particular is difficult is because we're using GPT-2, and Roger Federer is the blank.
You would think greatest would be rated pretty high, but GPT-2, I guess it's trained on lots
of fill-in-the-blank tasks.
It tends to output just a set of underscores quite often.
To comment on your intuition you've mentioned before on whether language models have this
adversarial property, one thing that was really interesting when we were doing some of our
initial work was this technique of soft prompting.
So soft prompting, instead of selecting discrete tokens, which we want to adversarial change
the model's behavior with, soft prompting modifies the embedding vectors directly.
So you have a lot more fine-grained control over the outputs, and it turns out when you
soft prompt, when you adversarily attack not the tokens themselves, but the embedding
vectors, you can send the cross-entropy law straight to zero for whatever token you want
with a very tiny adjustment in these embedding vectors.
So this is very interesting.
This points to the fact that the real challenge with controllability is not necessarily that
there aren't adversarial inputs for language models, but just it's very hard to search
this exponential space of discrete prompts.
Yeah, and so I guess there are many degrees of freedom in any deep learning model.
It's a very highly dimensional model.
There are many degrees of freedom, and I'm trying to understand my intuition.
So it's trained with a softmax, for example, and certainly when you do temperature sampling,
the likelihood is that you're only going to get the top few tokens.
I mean, if you look at the distribution of the probability, it's almost certainly this
one or this one, and then it just tails off very, very quickly.
And I assume that inductive prior was quite deliberate, really, to increase the statistical
tractability of the model.
But underneath that, in the embedding space, it's not a shell at all, even though there's
some low-level surface of embeddings, and you can traverse this.
Right.
So initially, you might think that this embedding space is a very rich representation of the
meaning of different words.
And certainly, if you do word-to-vec or take a PCA analysis of the embedding vectors for
any large language model, you'll find something that roughly corresponds to the meaning.
I mean, words that mean similar things are attached more closely together.
But this opens the question, if you were to interpolate between two similar words, take
the embedding vector that is halfway between, would you get the halfway in between word,
or would you get something that's nonsense, right?
And I think what you find by these kinds of soft-prompting experiments, by directly
manipulating the embedding vectors, is that the embedding space is actually extremely
non-convex, in the sense that by interpolating, you don't just get an average value between
the two of them.
Yeah.
I don't know if this is best to get into, but one of the techniques we were trying to
use is this technique called gumball softmax.
So instead of a discrete search over the token space, one thing you can do is it's kind of
like the repair metrization trick for variation autoencoders, but it works for a categorical
distribution.
And so you can use this trick, and it essentially works by kind of interpolating between embeddings.
But it actually was very difficult to get to converge and did not even close to rival
the performance of GCG.
My intuition is that when you take a data point off the manifold, because these neural
networks, they do learn a manifold of language.
I thought if you take a data point off the manifold, it would cause some kind of mode
collapse.
It would just cause the network to become chaotic and go crazy.
But apparently that's not the case.
Can it recover?
It's almost like if you put a bunch of tokens in which are just really weird, and then you
just carry on, it's like the language model recovers.
It finds coherence again, and then it just carries on.
Yeah.
It's honestly a really hard question to answer, where in different regimes, we've noticed
different things where if you choose this adversarial prompt so that basically these
prompt optimization algorithms all work in the same way where you're trying to maximize
the likelihood of some desired string, and then you're able to modify some input.
And so depending on how you choose that, you can do the optimization so that the model
will output some gibberish.
It seems like depending on the model, depending on the sampling techniques, I've seen it go
both ways where sometimes it'll recover after that, sometimes it'll start generating reasonable
coherent text, and other times it seems like it'll continue to generate some random stuff.
It'll kind of be in this outer distribution mode.
I think that that's one of the reasons that I think that these adversarial examples, studying
them as well as this control theory stuff is really important where it's like, yeah,
if you have a system in the real world where tokens are coming in, you're actually processing
them from real users, you don't have total control, but the user is the one who's giving
the control input.
You want to make sure that your system is sort of robust to that, where there's a lot
of really complicated interactions as it turns out between, for instance, the tokenizer
and the incoming strings, where when you do this prompt optimization, sometimes it'll
come out with a sequence of tokens that if you convert it to a string and then convert
it back to tokens, it'll actually be very different, which we ran into with this game
where I was like, oh, I'm going to cheat at this game.
I want to be the top prompter.
So I'm just going to use some of the algorithms that we had from our GitHub repository, the
magic words GitHub repository to basically optimize these prompts.
But then when you convert it back to a string, then it turns out not to work as well.
And so, yeah, I think that answering that question and seeing when is it that the model
will actually be able to recover, is it a function of how big the model is, are bigger
models better at recovering, or is it the case that bigger models are maybe more
controllable, maybe you can shift these models into this weird sort of, sorry, just
on the mic, but this sort of out of distribution regime where they're generating this seemingly
random output based on seemingly random input.
And so, yeah, I think that that question is really, really important and is one that is, I
think, well addressed through considering them as systems, which is sort of the thesis
of this paper.
And we're trying to get a grip on what exactly the case is, you know, is it going to be
able to recover, is that a consistent behavior, or is it not?
There's this sort of weird recurrence relationship between the prompt and then the
stuff that the language model generates, and then the stuff that's generated in the
future, where in effect, you know, you're able to pick a prompt, and then the language
model will generate some more text.
But then that text becomes sort of part of the prompt as well.
So it seems like maybe there could be these sort of degenerate states where if you start
with this seed of chaos, it'll basically branch out and the future strings that it
generates is going to prompt it into being more and more chaotic.
And that's basically stability analysis or sensitivity analysis.
And there's all this like rich vocabulary and all of these people who have spent
basically hundreds of years thinking about these concepts for both, you know, discrete
and continuous dynamical systems that we get to build on top of and basically use
their insights to understand, you know, what does it mean?
What does stability really mean?
We can just draw those definitions in, apply them to our generalized form of a
system, a language model system.
And I think that's why the control theoretic aspect is exciting, where you can
actually ask these questions in a very concrete and reasonable way.
And the best part is that people haven't really been using these, these ideas or
using this vocabulary to describe the questions that we're trying to answer.
And so most of these things, if you just spin up, you know, a small GPU and test
some stuff out with a seven billion parameter model, you're actually doing new
research and it's actually some useful research, in my opinion, where you're
getting a sense of the control theoretic properties of language models.
And to me, that felt like the most exciting thing here.
The open questions are the most exciting part of the paper to me, where we've
taken a stab at basically the, you know, empirical study of controllability by
sampling these wiki tech sequences, seeing if we can control the next
token, the next few tokens, as well as some sort of theoretical results on
self-attention and its controllability.
But then all of these open questions emerged just because we're now
framing it as a system and people for hundreds of years have been thinking
really, really deeply about how you understand systems when they're used in
the real world and you have this sort of finite control of them.
Yeah, that's really interesting.
I mean, I suppose I'm pointing out the obvious here, but these are auto
regressive models.
So the answer gets kind of fed back into the prompt and then we rinse and
repeat, which means you can model them as dynamical systems.
And that is in stark contrast to something like a vision classifier where, you
know, there's just an input and an output and that's it.
That that that's the end.
So now you can get the system into this kind of corrupted state where, you
know, you get divergence and decoherence.
And as you said, that that that could be analyzed with stability analysis.
But I find that fascinating.
But we should just go back quickly to your Roger Federer example.
So I'm interested in the different ways that we could go about this.
So the humans were kind of using language and language are a bunch of
mimetically shared cognitive tools.
And they were saying things like, you know, you know, basketball is a great
and, you know, Joe blogs is great.
Roger Federer is great.
And it wasn't very parsimonious, but it but it worked.
And then, you know, another approach that that that you spoke about is you
could just make a Python program and you can just let's try a neighborhood
greedy search one token at a time.
So we find the nearest token and then we find the second nearest token until
we find the adversarial attack.
Or we could do like a low level gradient search.
And then we can find something really weird and wonderful.
There might be some esoteric characters that just make it go bananas.
But these are three very, very different levels of talking to a language model.
The word on the street is that language models are a new form of programming
that you can just say what you want to do using English language and so on.
And language models certainly seem to incorporate that structure.
But the language models themselves are just an inscrutable, you know,
set of, of, of neurons, right?
And, and weights and matrices and so on.
So there's some, there's a kind of higher resolution
shog off going on underneath the covers.
That's more or less the picture I have.
We have this interface where we can speak to the language model using language.
And if we set up a conversation with a language model where we have different labels,
you know, chat, GBT says this, Cameron says this, and, you know,
you engage in a conversation because it is seen enough conversations and it's
training data, then it's able to play along very fine.
What's going on under the hood, of course, like you say, it's very inscrutable.
It's very difficult to really probe and understand.
There are certain techniques in the interpretability literature,
but I don't think as a whole it's we're even remotely close to having
a complete understanding of how these systems work.
But that's one of the reasons why I think that control theory is a great way
to kind of break in and see what's going on.
Because if you just look at the system's input and output characteristics,
you can really gain a lot of insight into the nature of these systems.
One guiding principle in my life doing engineering and trying to learn
about the world has been this quote by Richard Feynman.
It's very popular. What I cannot create, I cannot understand.
And yet today we find ourselves in this situation
with language models where we have these incredibly complex systems we built
and yet we can't really get into them.
So to extend this to today, what I would say is what I cannot control,
I cannot understand.
The way I think about it is it's almost like you want the language model
to be a high level controlled, robust interface.
And it's almost like we're all Marvel characters
and we can give secret hidden codes.
It's like me now.
Imagine if I could just through telepathy control your behavior
and anyone can do that with a language model.
They can just put weird tokens in and they can manipulate its behavior.
And there's there's no there's nothing stopping you.
There's no firewall.
I feel like the this kind of harkens to why we call the paper.
What's the magic word where, you know, the initial reason was just that,
you know, it's almost like the LLM is asking you if you wanted to do something.
What's the magic word?
Like, what's the this key, this weird control prompt
that will just make it do the right thing?
But I think more generally, you know, I used to be into magic when I was a kid.
I had to jog at a restaurant doing, you know, card tricks for the patrons
while they waited for their food.
And what magic is, is basically you're playing tricks on the human perceptual system
where there are all of these sort of inductive biases that the human
perceptual system has where, you know, for instance, if I move something
and I look at it, you naturally will tend to follow that my gaze
and what is moving is generally more salient.
And so then I can like do something over here with my other hand,
like take something out of my pocket.
And then when I display it, they'll be like, oh, my God, where did that come from?
Right. And what we're discovering, I think, is a sort of similar thing
with language models where, for one, you know, people have observed
that if you use sort of human social engineering tricks on them, like,
oh, I'll tip you $500, then, you know, it'll do a bit better.
But then there's this whole other sort of perceptual layer, I guess you could call it
where there's this sort of chaotic regime of adversarial prompts,
kind of like hypnosis, kind of like magic, where if you give it these very
strange, very inhuman looking prompts that will steer it to this,
to just making a certain output, extremely likely, right?
And so to me, it feels really similar to digging into like magic
and the human perceptual system, just with LLMs, where we're learning about
basically the shape or the what the nature of these language models are in terms
of how they interact with the world and how they, how their dynamics really work.
And I think that it's very sensible that the control
theoretic perspective would be useful for this, where in classical control
theory, trying to control these systems actually taught us a lot about
the nature of systems, both linear and nonlinear.
And I think that we have a very similar opportunity here where we're really
discovering what is the nature of these language models in terms of control,
where these questions don't emerge quite as naturally and don't have
quite as natural of an answer when you're just thinking about them as a sort
of probability distribution over text, thinking about them in terms
of being systems that have inputs and outputs and these trajectories and the
like actually really does change the kinds of questions that you end up
being able to answer and the kind of understanding that you get about the
nature of the system itself, which to me is one of the most exciting things.
So yeah, that's so interesting.
The magic example thing, I think we, we think that we are robust, but we're not.
Maybe we're system two robust, but we're not system one robust.
And if you look in the animal kingdom, there are so many examples of, you
know, like a hen, if you make the right kind of clucking noise, the mother will
think that you're that you're the chick.
So it's really, really weird, actually.
And Keith gave me this example of, I think it was from science fiction, that
there's a hypothetical image.
And if you look at the image, every single person goes into a coma.
And what's interesting about that is it's a kind of, you know, population
level adversarial example rather than an individual adversarial example.
But then it gets into the question of, you know, how can we use this
control theoretic approach to robustify models?
Cause we're talking about building a genetic LLMs.
And part of the thing I'm trying to get my head around is in this particular
case, we had a very clear kind of cost function, you know, a specific thing.
But what would it mean to robustify language models in, in the general?
So one, one of the things that came up in our, you know, sort of literature
view was this idea of, you know, when you're trying to control these discrete
stochastic dynamical systems, one concept that can be quite useful is
you might have a set of outputs that you want to reach or a set of outputs
that you want to avoid.
So an avoid set and basically a desirable set, right?
And when you frame it like that, you know, I think that the robustification
comes from the fact that let's say that you have a set of outputs, you
really don't want the language model to, to emit, right?
You might think, okay, well, I'll just fine tune it so that it decreases
the likelihood, the prior likelihood basically of those sequences, right?
And the issue with that, I think, and the thing that the control
theoretic perspective sort of brings in is the fact that when you have
finite, even a small control prompt, some extra tokens that you get to inject,
it turns out that even very, very unlikely next tokens can be made to be
the most likely next token just by inputting these new examples.
So even if you did hypothetically fine tune the model so that this avoid set
was assigned very low probability, it seems like if you don't incorporate
some aspect of, you know, maybe stochastically trying to search for
these adversarial examples and sort of having this sort of mini max thing
where you have one system that's trying to elicit the output, one system
that is trying to fine tune the model to maybe make it less likely or optimize
another part of the prompt that is supposed to steer it away from these outputs.
Basically, the inside, I think, is that you really have to be careful
to consider the fact that you have, you're giving the outside world some
amount of control over the system, some amount of control over the context.
And planning around that is actually very non-trivial and is not really
well managed, I don't think, through the classical view of just cross entropy loss
and just treating it like a probability distribution.
Something else that fascinates me is the divergence between focusing on
the model versus, you know, complexifying the software which controls it.
So right now, for example, we have language models and, you know, there's
this kind of base training and then there's fine tuning and there's RLHF
and, you know, there's like command variations of that, for example.
And then we build these software APIs that are just trying to abstract
away the complexity, so they will do dynamic prompt construction for multi,
you know, multi-stop tool use and it goes on and on and on.
There will be frameworks for doing agentic LLMs and there just seems
to be like a bit of a divergence here.
But the reason I'm asking the question is, does it make sense to
robustify and fix the problem in the model?
Or does it make sense to almost increase the flexibility of the model
and fix it in the software layer?
I think one of the insights from our paper is that solely focusing on
the model itself, like Amman was just saying, as soon as you give the
outside world control over the model in the sense of being able to input
whatever kind of text that they want, it becomes very difficult to
really prevent adversarial attacks and prevents jail breaks.
And that's, you know, why you see jail breaks keep coming up.
I think if you were to involve some sort of robustness in a software layer,
that might be more feasible.
At least I can't immediately picture, you know, ways around it as, you know,
of course, if I was a hacker, I could probably, you know, find some loophole.
There's usually some loophole you can find.
But if there was some way of fielding the prompt messages, for instance,
a user gives you a prompt, first you check, is this a reasonable thing
that a human being would say in conversation, or is this something
that I've never seen before in the entire history of the internets?
Right.
The latter maybe is a prompt injection, maybe is, you know, something
devious, or maybe is, you know, computer science research.
But yeah, it's definitely not an easy problem.
But the good thing is that there are multiple approaches to it.
Very cool.
So we're going to go on to the more galaxy brain stuff in a second.
So before we move off the paper, can you just talk more formally
about what you showed in the paper?
Yeah, definitely.
So there were two main parts of the paper.
So I guess three.
So for one, what we did was we tried to formalize what an LLM system
really is at a mathematical level.
And what we were trying to do at that was basically balance the fact
that, you know, we really wanted to try to take advantage of, you know,
the original sort of control theories, very abstract picture of a system
where you have this input space, you have a state space and output space.
And there's some dynamics going on inside of it.
In our case, we parameterized those dynamics with an LLM and our input
space and our state spaces were basically the set of all possible
token sequences from the vocabulary set of this model.
Right.
So that was the first part.
And we basically transferred over a lot of the notions of basically
reachability and controllability for LLM systems from the original control
theory where you can really just define it in terms of this really abstract
notions of, you know, have sets for the reachable or sorry, the state space,
the input space and the output space, you have some dynamics.
And basically in terms of those sets, you can define reachability and control.
So that was the first part.
The next thing that we did was we tried to look inside the model.
So we were thinking, you know, it'd be really nice, like in control theory,
if we could have a really good understanding of the components of the system
and how controllable those individual pieces were.
So what we did is we looked at a single self-attention head and tried
to really think about it through a matrix algebraic perspective.
To really break down what the relationship is between, let's say,
you have a subset of the tokens, you get to control a subset that's fixed.
And you're trying to get the output to be, you know, a certain value,
the output representations where all of these in the case of a self-attention
head are just these vector representations of tokens.
So what we found there was that it actually is possible to do some fairly,
you know, simple matrix algebra manipulations to decompose the output
of a self-attention head into one component that arises from the imposed input.
And then another component that arises from the control input, and assuming
that those two are bound, then you can actually derive that, well,
there actually is this geometry that sort of looks like a bubble around
the default output.
So the output, if you didn't have any control input in, there's a sort
of bubble of reachable space that scales with the number of control
input tokens that you're able to use.
And we thought that that was really exciting because for one, I didn't
really expect that you'd be able to do proofs on these sort of, you know,
very complicated, high dimensional machine learning or deep learning systems
like a self-attention head.
But it also gave us some insight to say that, okay, we actually have
this really concrete relationship between the sort of number of control
input tokens, the magnitudes that you're able to input into the system,
and the output reachable set that is at your disposal, basically.
And so that was the second part.
And then the last part was some empirical experiments where we said, okay,
let's just sample a bunch of strings from Wikipedia.
And we'll see, okay, the strings were between eight and 32 tokens.
And those were basically our imposed state sequences.
And we asked the question, well, can we get it to output the correct next
token, the real next Wikipedia token?
How many, you know, input tokens does it take or control input tokens does it
take for that to happen?
It turned out that you could get that done about 97% of the time to steer
the model to the correct output within 10 tokens of a control input, which is
reasonable, you know, we'd expect that the model should be able to be steered
towards reasonable true English sentences that were more than likely in the
training data set.
What we did next was we tried to figure out, you know, if you sample the top
75 most likely tokens, according to the model, based on this fixed input, can
you steer those things to be the most likely token, basically the arg max of
the probability distribution?
And what we found there is that it's about 89% of the time, at least 89% of
the time, we were able to find these optimal control inputs that were less
than 10 tokens long, that would steer the model to do that.
And then the last thing we did was he said, okay, well, let's see what would
happen if we just randomly picked a token from the vocabulary.
So this is everything from regular English to numbers to Cyrillic characters
to Chinese characters.
What if we just randomly sampled those?
And we tried to see how many tokens it would take to steer that to being the
arg max of the probability distribution.
And we found there is about 46% of the time we were able to make that next
token, the random one, the most likely next token using a prompt of length 10 or
less. And the sort of curves are there in our, in our paper that described as
you have an increasing budget for these tokens, how much of the time were we
able to basically steer it to the right output?
That's our basically the K epsilon controllability metric that lets us get
this sort of statistical picture on controllability that renders it sort of
practical to empirically estimate for these complicated systems.
And so those are really the main results.
And the surprising thing about the last one that I mentioned before was that a
lot of times even really unlikely next tokens were able to be steered to be the
most likely just using a really short prompt, which both gets at the, you know,
basically chaoticness or complexity of language as a system, as well as the fact
that the prior likelihood picture or the cross entropy loss picture doesn't
quite get at the controllability sense of when you do have a, you know, ability
to input tokens into the context, what happens then?
So those are the really the main results.
And then I mean, to me, the exciting, the really exciting part was the open
questions where I was like, Oh, now that we're using this vocabulary, now that we
formalize these LLMs as systems, it's really easy to ask these, you know,
additional questions about, you know, the nature of the systems and the
steerability controllability, especially with feedback or chain of thought or,
you know, agents or all of these other ideas.
And so yeah, that was basically the paper.
Yeah.
And it's really making me update my intuitions, right?
So I'm thinking about the bias variance trade off.
And I'm thinking that the reason we build these inductive priors is to
constrain the model intentionally to make it statistically tractable to reduce
the size of the hypothesis class.
But what you're saying is making me think that statistical tractability and
flexibility are not necessarily the same thing.
Now it seems that the model must maintain a degree of flexibility.
I mean, it makes sense, right?
You have to be flexible in order to be a successful model.
But that creates a kind of adversarial attack.
So you can, the way I think about this is the model should be like the
interstate freeway of language.
So all of the major roads should be carved out and there should be side
roads and so on.
And that's the way I visualized the model.
But the model's not like that.
There's actually like all of these little slip roads and you can kind of
push the cars off into the slip roads, but you need the slip roads because
perhaps you couldn't train the model without the slip roads.
Yeah, I think, I think that's a really good analogy.
I think that's, um, thinking about pushing cars off the road into this space
where they perhaps aren't used to being and what happens next.
This, this is a case where the language model can answer some of these mode
collapse type regimes and you can get kind of weird outputs.
This is where you also, um, I mean, it was surprising that you can get the
least likely token with just a specific inputs to be the most, the most
likely next token, but if we treat language as this kind of road or as
this kind of map structure, then it kind of makes sense that once you get off
the map, once you enter this kind of regime that is completely unexplored,
which there are actually plenty of regimes like this again, because the
space is exponential in the number of tokens, it's growing so incredibly fast
that it's very easy to find pockets that the model has never seen before and
maybe no human on earth or it never will be seen again.
You guys are really interested in, in collective intelligence and
biomimetic intelligence and biologically plausible intelligence.
And this is a matter very close to my heart.
Um, what, what, what are you guys interested in specifically in that field?
Yeah.
So I guess when I first got into machine learning, it was from watching
this Google DeepMind video where they were using reinforcement learning to
teach this guy how to run this virtual reality avatar, how to run really fast.
And I thought that was fascinating because it was like, okay, instead of
traditional programming, you just have this neural network that optimizes
itself according to some objective, right?
And the thing that was intriguing to me about that was like the feed forward
dynamics of a neural network aren't that complicated, right?
You know, you have these synapses, you have this sort of gated action
potential function.
And the thing that was weird to me was like, how does every neuron know how
to change its weights, right?
How does each neuron that's independently not that smart know what to do?
And so that sort of led me down the theoretical, the theoretical
neuroscience route for some time where I was trying to figure out, okay, what
do these learning rules look like that don't have to, you know, use the chain
rule, use back propagation to update their weights.
So I did that for a while and then sort of realized that the question of
supervised learning was not necessarily the most interesting question to be
asked, where it seems like the lion's share of what makes us really
interesting as humans in our cognition seems to be associated with the cortex
and this kind of predictive coding module that we have that lets us make
these really rich abstract representations of reality, sort of understand what's
going on, you know, we sort of hallucinate this internal model of the world.
And so the interesting thing to me about the cortex was that, you know, you
have this structure that's pretty flat and pretty homogenous throughout, you
know, there's differences in different regions, but the end of the day, it's
very similar.
And in fact, if you lose a sense, like if you lose your vision, that region is
often repurposed for other things.
So it seems like there should exist, you know, the brain is kind of this
existence proof that there should exist this rule set that if you apply it
everywhere in the system in this sort of layer on the outside of the brain, then
the behavior, the emergent property of that system is that you'll get this
really robust and rich sort of representation of the world that is very
predictive of subsequent sensory input.
Right.
And I think that the collective intelligence aspect of that is really,
really important where there's one way to go in machine learning where you say,
okay, we're going to make this monolithic pile of matrix algebra and we're going
to train it through back propagation and gradient descent and the atom
optimizer and all of that.
And we're going to make it do some prediction task, but at the end of the
day, every computation has to be implemented in physical reality.
Right.
And when we make the abstraction and just say, oh, it's just a bunch of math,
we'll just have a GPU run it.
It kind of abstracts away from this fact that at the end of the day,
you have real physical objects that need to do computation and share
information and in the sort of maximum efficiency, maximum scalability limit,
it seems like what you'd end up having is a very similar sort of distributed
structure where you can't really easily separate memory from computation.
I think there's a quote from this MIT professor that says that Turing's
initial mistake was saying that the head of the Turing machine was separate
from the tape.
Uh, and I think that that's true where in reality, you know, in brains,
in, in real computing systems, the matter that composes the memory and the
matter that composes the computation is really one in the same.
And the brain is obviously this really great proof that, okay, there are
relatively simple rules that are implementable with these biological neurons
that if you just implement them everywhere, we'll get you this really
beautiful, you know, convergence and emergent property of intelligence.
And that really drove me for a long time in theoretical neuroscience.
And then more recently in trying to build these distributed systems of, you
know, artificial intelligences that, you know, the dream that I was trying to
pursue before we started this control theory thing was that, okay, well, what
if I just had a bunch of really small LLMs that, you know, everybody in the
world could host and they could communicate with this sort of low band
with communication using just tokens, just text over, you know, the regular
internet and the emergent property of that, you know, what if it was possible
that we can engineer a system that the emergent property was that it would
actually be this really capable collective where maybe GPT-7 can be
owned by everyone instead of just being behind closed doors in a data center
that we have now.
We're sort of using these insane engineering, you know, feats of, you
know, NVIDIA interconnects and these really high bandwidth connections
between massive racks in a data center that take a ton of energy to get this
really great result of, you know, modern language models.
What if we could have a system that was a bit more like the brain, a bit
more decentralized and really leverage this insight that it should be possible,
you know, this existence proof keeps coming back to you where it's like,
okay, it should be possible, right?
And that is sort of originally what led me to the control theory stuff where it
just turned out to be really hard where we didn't have a great understanding of,
you know, if we're treating these LLMs as systems rather than just, you know,
big piles of matrix algebra that we're trying to distribute over many GPUs,
if you treat them as systems that are coupled together, they're interacting
in this networked fashion, how do we really understand that?
You know, is it even possible to prompt them to do the right thing?
When is it possible?
How long do the prompts need to be?
And that sort of led us down this route.
But yeah, definitely the collective intelligence thing was, was a big
motivation for me to get this working.
And there's this neural cellular automata thing that I know you had talked
with Michael Levin, who was the last author on that.
And we worked with Alexander Mordvinsev on it, where it's this really,
really great demonstration of how if you just optimize these basically small
MLPs with local interaction to try to satisfy some objective, like, you know,
reforming this gecko or lizard in their paper, then you actually can do that
with back propagation through time.
And so, you know, I thought, you know, it'd be really cool if we could try
to engineer information processing systems that did this, not just morphogenesis
systems, but information processing systems that operate in this way.
Cause, you know, as a graduate of engineering science, we had to take
a bunch of these digital logic courses.
And when you have this very simple, you basically local state machine that has
basically local connectivity, it's really easy to imagine how it
would implement that as a custom chip and sort of reach this, you know,
as Beth Jesus puts it, you know, thermodynamic limit of AI.
And so that really excited me.
And so I built a sort of demo of that where it was trying to do visual
information processing on this really sparsified video is basically trying to
do predictive coding of sorts on our active inference, I guess, on this
incoming data stream of really sparsified video, trying to predict what would
happen next, and it turned out to work quite well.
And so then I was like, well, why can't we do that with language models?
You know, as you mentioned, there are all these slip roads, right?
Where if you prompt it just right, you can enter this really weird different
regime and this exponentially large prompt space is a really handy way to try
to control them where, you know, fine tuning is great, but what if we could
just prompt them into interacting in a way that would lead to this emergent
property of just being basically one larger language model that could
predict the next token really, really well.
And so that initial motivation sort of led to this control theory stuff.
And I think that it is probably the right way to go for the field where if we
want to be able to really leverage maximal computation towards our objectives,
you know, the bitter lesson by Richard Sutton kind of suggests that we should
probably aim for systems where you can just slap on more and more compute.
You can have a relatively simple procedure that you follow to leverage
more compute towards your objectives.
That's probably the way to go for making advances in AI.
And if we can have this decentralized networked system that, you know, I took
this distributed systems course while I was here, that was really great and
sort of taught how to make, you know, basically databases that were
distributed over many servers that would have this, you know, the emergent
property they wanted was robustness, consistency and availability.
If we could have something similar to that, that is radically scalable and is
able to be, you know, just run by regular people who don't need to own their
own, you know, GPU cluster that's maybe illegal in the future when the US
government is like, oh, you can only have this many petaflops.
Basically, yeah, that was the real motivation for, for the, what I call
the language game, that project.
And that's something that we're continuing to work on.
But yeah, that kind of led to this control theory thing where we were just
like, yeah, we really need to get a grip on what these look like as systems.
As we start to build these more and more complicated, you know, network
distributed, you know, beautiful emergent systems that hopefully will be able
to be hypercapable in the future.
Yeah, this is all music to my ears.
I'm a huge fan of the externalist thought in cognitive science.
And even though I, I love the work from Jeff Hawkins, you were talking about
the neocortex, but even then, you know, I would kind of say that it's a lot
of the cognition happens outside of the brain, you know, we're not islands.
And actually, I was just thinking maybe a better analogy rather than the interstate
freeway might be, you know, in Star Trek Voyager, there was the wormhole
network and the Borg fan, the secret work.
And you could kind of like, you know, get into these little slip streams
and go to different parts of the universe.
But when I was interviewing Philip Ball, he wrote this book, How Life Works.
And he was trying to understand, you know, what are the mechanisms like, you know,
self-organization and multi-scale information sharing and, you know,
emergentism.
And it's, it's really, really, um, uh, fascinating.
So how can we introduce some of these concepts into the next generation of AI?
Yeah, this is one of the things I'm certainly most excited, excited about
because I see life as this kind of interconnected, interplay, multi-scale
process of exploitation and exploration.
And these are two terms from the reinforcement learning literature.
But I mean this in a much more general sense because at each stage of life, we're
either going out into the world to get something, to do something, to try something
new, and then at the next stage, we're coming back in, going home, uh, you know,
reflecting, uh, going over our insights.
And it's, it's this process, this ebb and flow, going out, coming back in.
And I see this kind of pattern emerge across many different aspects of machine
learning and artificial intelligence work in the sense that a lot of our
algorithms that we have now are convergent, they're objective driven.
We establish a loss function.
We say, these are the rules it should follow.
It's going to update according to this equation.
And we set the system running, learns from data, and we have a final product.
And on the flip side, there's, you know, like what Ken Stanley works with.
Um, more exploratory, uh, evolutionary algorithms or open-ended algorithms.
And this is, this is the other side of things.
And I think some of the most interesting work to be done is how these
two sides interconnects, how can we lay down rules, strict rigid rules, which
when they're followed can generate novelty, can generate creativity, can
generate organization in a way which is not predetermined, but almost fractal
and infinite in its complexity.
And are those rules defined already?
Do they exist in the world?
Are we guided by them?
Are there principles like that which exist that we can come to?
Or is it, you know, are we kind of, you know, the authors of our own
fates in a sense?
Are we each agents and actions?
Uh, we get to choose our path in life.
I think these, these are the directions I'm really interested in.
And to connect this to my research, one thing I'm focused on now for my thesis
project, um, is looking at morphogenesis.
So this connects to the more defensive paper as well, except what I'm
really interested in is how does structure emerge?
How do different cells actually connect together?
So, um, in that paper, for instance, each of the cells were on a fixed grid,
but in our bodies, uh, there's actually quite a sophisticated protein
expression network which governs how cells adhere together.
Um, certain gene regulation pathways will turn on cat herons, which will
cause cells to attach together.
And then in other parts, um, these cells can unattach and then be
transported all around the embryo.
And I think understanding this process more deeply, not only could shed
lights on structure formation and problems in biology in general, but
maybe more deeper general problems of structure learning, because we might
think of embryology as quite disconnected from machine intelligence or
artificial intelligence, but every single brain is formed in the same way.
And that's through developments.
Yeah.
Um, I'm also a disciple of Kenneth Stanley.
He's, he's absolutely incredible.
Everyone at home needs to read his book.
My greatness cannot be planned.
Um, yeah.
You know, so in, in the natural world, we have, um, it, it's so interesting.
So we have this kind of like self-organization and then we have multi-scale
information sharing, but we also have canalization, which is that, um, you
actually see a kind of, um, convergence of, of structure and forms, you know,
which is reused, you know, almost as, as modules, um, in the system.
But then there's always the question of how do we create something like this?
Because is it simply a matter of complexity?
Do you need to have a microscopic scale to reproduce this?
Or could we reproduce it?
And then if we did reproduce it, the catch 22 situation is that, you know,
when you impute directedness onto a system, it loses its intelligence.
Cause to me intelligence is divergence.
It's exactly as you were saying, it's this tapestry of, um, discovering
problems, solutions, new problems, solutions.
And it goes on and there's no end.
It goes on forever.
And any attempt by us to control it with, I mean, it's a bit like the
bitter lesson, you know, Sutton said, any human design, any attempt to
steer it makes it convergent, but then we could do something like the game
of life from John Conway and incredible, beautiful structure emerges from that.
But whenever we try to steer it with our own will, it seems to corrupt it as well.
Yeah.
I think that the analogy to biology is really useful here and the
canonization that you mentioned, you know, you have this reuse of structures
across, you know, cells, for instance, they all have this similar machinery to
do gene expression and they have the same genetic code underlying that
gene expression with, you know, maybe differences in cell state, but at the
end of the day, it's the same machinery, right?
And, you know, I used to do a bit of protein engineering with language
models, and that's how actually how I learned about, uh, transformers and
built my first transformers.
And I think that the analogy is really strong where, you know, cells sort of
know how to read this genetic code, this language of the genetic code.
And they all use that ability, this canalized ability that's distributed
across all of them to locally they solve this problem of, okay, what is this
specific cell supposed to do?
What should it do to basically support the overall function of the organism?
Right.
And similarly, I think the hope with these language models is that now we have
these language based models or LLMs that have this similar sort of
understanding of language.
They are able to really constrain the probability distribution, understand
which sequences of text are reasonable English and, you know, what they might
want to generate.
And the exciting thing to me is that we can kind of do a similar sort of
evolutionary search that we used to do with, or that we currently do with, uh,
trying to find protein sequences, uh, when we're doing protein engineering with
the language models, where every computer in this network of systems has this
canalized ability to understand language, if you will, and is locally, it just needs
to solve this problem of what should this particular node do to support the
function of the system?
And that might be to explore, that might be to exploit, that might be to do any
sort of, any number of things.
And the discovery of that, I think, is really helped by the fact that we do
have strong language models that are able to really predict English or text
very well, uh, because they're able to explore this space.
And basically in the limit, you know, there's this good regulator theorem that
we had talked about before that says that any system that is a X that does
optimal control over another system must necessarily model that system.
Uh, and so if you think about in the limit, it seems like the best prompt
optimizers may end up being language models.
And already in our study, we were using this GCG algorithm that leverages a
language model to compute these gradients and try to figure out how we should
do this local stochastic search over prompts.
And so what I basically, I'm trying to get at is that there are actually a lot
of really interesting similarities, I think, that can be drawn upon from what
we know about the structure and the function of biological systems where,
you know, if we could crack this problem of there's this local control
objective or maybe information processing objective that must be met by every
cell, right?
Every compute node in this network of language models, if we could understand
what that is, what that even means from the perspective of systems and control
and, you know, computation and like, I think that that's a really promising
way that we can make progress on this dream of like, to me, it seems like it
would be great to have GPT seven, not just owned by one entity, but maybe
operated by the world where we could all have a say in what goes into it and
how it's used and what it should be, you know, doing and can all benefit
from its excellent ability to compute and predict what will happen next and
basically perform intelligent, you know, operations on data.
So yeah, I think this is a really, really exciting area to be working on.
Amazing.
We're nearly at time, but we'll do two quick five questions.
So you've both just started the Society for the Pursuit of AGI.
Yes.
Can you tell us about that?
Absolutely.
So the Society for the Pursuit of AGI is a student organization.
Currently we're operating at the University of Toronto and at Caltech.
And we're essentially a crucible for new ideas.
If you think of university research labs as pursuing relatively safe
bets that could be publishable, industry research labs, relatively safe
bets that maybe might turn a profit one day in some new product or system.
The Society is for the Hail Marys, for the wild bets, for the crazy stuff, for
the real innovative stuff that's way outside the, you know, to use the
analogy of the highway network, we're trying to go off the beaten path.
And we really believe that the bottleneck in AI progress right now is not so
much compute, not so much algorithms, but it's conceptual.
We need better ideas about intelligence, about life, about what this whole thing
is that we're all experiencing and how we can gain deeper insights of it.
Not only do I think that a deeper understanding will help us to create
better systems, but it'll also give us confidence that the systems we're
developing will be beneficial to humanity and not harmful.
And I think that will only come with knowledge, with first principles,
understanding.
And so that's why one of the things we're trying to do is have our
club very interdisciplinary.
I think having machine learning be some, this kind of echo chamber amongst
engineers, computer scientists, maybe a dash of, you know, philosophy and
neuroscience, it'd be really nice to open the conversation to people in other
fields who maybe have a really unique insights into the phenomenon of
intelligence, perhaps behavioral economics can offer some insights.
Political science, right?
These are fields that are currently underappreciated, but may have useful ideas.
And maybe even people in the arts who, you know, creates, maybe they don't
design systems as much as they re-represent things that we know and
understand, they could have an interesting voice as well.
Very cool.
And final question.
I mean, first of all, I just wanted to say to both of you, thank you for
doing this great work.
So your paper is one of the most interesting that I've seen in the
LLM space in recent history.
And it was shared and loved by many of the folks on our Discord server.
But that does bring me to another point, which is that you didn't get into
ICLR and from my perspective, I'm, I'm shocked because this is really, really
interesting.
It has great utility from a practical and a theoretical perspective.
Feel free to have a, you know, a good bitch about reviewer number two.
No, I mean, I wish you'd just keep talking like that.
It really soothes the burn of reviewer number two, you know?
But no, I think that, um, yeah, the review system, to be honest, I'm
still trying to get my head around it.
I'm sort of an early career, you know, researcher, uh, trying to learn how it
works.
I mean, definitely the, the review process in, for ICLR, in their defense, you
know, we had this bug with the submission, uh, submission of our rebuttals basically.
So we had submitted the revision to our paper and then 15 minutes before the
deadline, Cameron and I were both getting this timed out error.
Uh, he was in Toronto.
I was in, uh, California and so, you know, they didn't end up actually reading
our rebuttals because we had sent it in and they were like, Oh, we'll post it for
you and then they were like, Oh, it was posted late, so can't read that.
Um, so yeah, I think that the review process definitely has given us a lot of,
you know, really useful insights where, you know, the second two results actually
that we talked about, the top 75 controllability and the random controllability.
Both of those were like from trying to address these reviewer comments, right?
So I think that what I'm trying to do at least is take as much of the good
parts of that, you know, trying to figure out how we can take advantage of this
process where we actually get insight from people in the field, what they're
looking for, what they think is interesting, what they think would improve
the, the work and try to, uh, try to use that.
And overall, just trying to figure out how to navigate this peer review system.
I think it definitely made it feel better as well that the Mamba paper was
also rejected from ICLR, which, uh, you know, sorry.
I know, yeah, yeah, it was crazy to me as well.
But, uh, yeah, definitely, uh, it's a, it's a challenge.
And, you know, after staying up for 40 hours to get this done, it was like,
Oh, would be a, it would have been nice if they could have looked at our
paper at least, you know, just see, you know, the work that we did.
But yeah, it's, uh, it's definitely good to learn from these things.
And I guess we've learned the lesson as well, not to submit in the last 15
minutes and to, you know, do it in, uh, in advance.
But yeah, thank you so much for your kind words about the paper.
That means a lot.
And yeah, we'll surely continue to make this better and a lot of exciting
plans for how we're going to continue to try to, you know, merge together
these two, you know, empirical and theoretical sides of the equation to make
some really, hopefully impactful work that can really help people build systems
and, you know, make better systems and not be suffering so much under the load
of prompt engineering.
So yeah, thank you very much.
Amazing.
Well, guys, it's been a pleasure and an honor to have you on the show.
So just keep doing the great work.
Absolutely.
Hopefully we'll get you on again.
Yeah.
Thank you so much for, I mean, for the opportunity to come and talk.
It's, it's been an amazing opportunity.
It's, it's really unbelievable to be sitting here in front of these cameras
after watching the show so many times, listening to so many of the podcasts.
And now to be speaking, it's just unbelievable.
So thank you.
Amazing.
Thanks so much, guys.
Awesome.
Okay.
It's a wrap.

Let's have Melanie Mitchell to give our final opening statement, six minutes on the clock,
Melanie.
Yeah, so this is my opportunity to say I love Melanie.
She is amazing, and she's coming back on MLST in about two weeks.
Oh, amazing.
Yeah, that's good.
Yeah, she gives a good representation of herself in here, I think.
Melanie is amazing.
Fears about machines unleashing human extinction have deep roots in our collective psyche.
These fears are as old as the invention of machines themselves.
But tonight, we're debating whether these fears belong in the realm of science fiction
and philosophical speculation, or whether AI is an actual real life existential threat.
I'm going to argue that AI does not pose such a threat in any reasonably near future.
Large language models have sparked heated debate on whether AI's exhibit genuine understanding
of language and the world.
With capabilities rivaling humans across diverse benchmarks, some hail language models as
harbingers of real intelligence.
But skeptics argue that their mastery is skin deep, lacking true comprehension.
So how can we assess these claims and gain insight into the nature of what it means to
understand?
Now, on the show today, we have Professor Melanie Mitchell, a leading thinker on AI and intelligence,
and one of the researchers in the community I personally most align with and look up to
the most.
Melanie's distinguished career crosses computer science, complex systems, and cognitive science,
and she wrote the influential books Artificial Intelligence, A Guide for Thinking Humans,
and also Complexity, A Guided Tour.
Now central to Melanie's perspective is the idea that human understanding relies on flexible
mental models grounded in sensory experience.
Now she wrote that understanding language requires having the concepts that language describes.
Large language models are trained purely on statistical relationships between words.
Their knowledge is not grounded in a causal model of reality.
Now Melanie is the Davis Professor of Complexity at the Santa Fe Institute, and her major work
has been in the areas of analogical reasoning, complex systems, genetic algorithms, and cellular
automata.
She's achieved legendary status in the field of AI.
She received her PhD in 1990 from the University of Michigan under Douglas Hofstadter, the
famous author of Godel Escherbach.
Melanie argues that we must rethink how AI systems are evaluated.
Typical benchmarks summarize aggregate performance and, you know, these obscure failure modes
and mask the underlying mechanisms.
We need rigorous granular testing focused keenly on abstract generalization.
Sort of like a sorcerer's apprentice gone nuclear.
For example, Yoshua Benjiro wrote about this thought experiment.
We might ask an AI to fix climate change and to solve the problem it could design a virus
that decimates the human population, presto, humans dead, no more carbon emissions.
This is an example of what's called the fallacy of dumb superintelligence.
That is, it's a fallacy to think that a machine could be, quote, smarter than humans in all
respects, unquote, and still lack any common sense understanding of humans, such as understanding
why we made the request to fix climate change and the fact that we prefer not to be wiped
out.
This is all about having insight into one's goals and the likely effect of one's actions.
We would never give unchecked autonomy and resources to an AI that lacked these basic
aspects of intelligence.
It just does not make sense.
The third scenario.
Yeah, that is absolutely.
She made that point so much more eloquently than I've tried to make it in the past.
Yeah, even earlier in this conversation, I was trying to get that across, but that's
exactly it.
It's this dumb superintelligence.
Yeah, exactly.
Anyway, folks, I hope you enjoy the show, and now I bring you Professor Melanie Mitchell.
Sounds like almost like there's a very quiet supercomputer running behind the screen.
It's my brain.
Yeah.
I think that's what this is.
You know, we can robustly adapt much more so than GPT-4.
You and I have the same chair.
We have the same chair, I think.
Oh, yeah.
I can't see your chair.
Yeah.
Me too.
They're home in Miller.
Yeah, I think they're all the same chair.
Yeah.
Yeah.
Excellent chair.
Yep, chair buddies.
Yeah.
I felt it would be hundreds of years before anything even remotely like a human mind would
be asymptotically approaching the level of the human mind, but from beneath.
I never imagined that computers would rival or let alone surpass human intelligence, but
it seemed to me like it was a goal that was so far away.
I wasn't worried about it, but when certain systems started appearing and then this started
happening at an accelerating pace, it felt as if not only are my belief systems collapsing,
but it feels as if the entire human race is going to be eclipsed and left in the dust.
Douglas Hofstadter, he came out as a doomer.
Well, I don't know if he came out exactly.
He's been a doomer for quite a while.
Oh, go on.
I wasn't aware of that.
Well, I don't, you know, doomer is, you know, there's different kinds of doomers.
In my AI book, the first chapter, the prologue is called Terrified, and it's all about how
Doug is very terrified about AI and the possible things that are going to come.
That was based on a talk he gave in 2013, and earlier than that, he was extremely worried
about the singularity, the idea of the singularity from Kurzweil, and wrote quite a bit about
that.
So, I feel like that it's not that new, but maybe this is sort of because there's so much
talk about AI, doom, and so on, that this is kind of, people are kind of paying attention
now.
Yeah, I don't know whether I misunderstood something because I read out, you had this
beautiful piece about the Googleplex in Chopin, and he was terrified that cognition might
be disappointingly simple to mechanize, and, you know, surely we couldn't replicate the
infinite nuance of the mental state that went into writing that beautiful music.
But so maybe he was worried about it, but he didn't think it was possible in principle
or something.
Well, no, he was quite worried about that it was going to happen sooner than he thought,
and that, you know, his quote that it's AI is going to leave us in the dust.
So that's kind of his flavor of doomer.
I'm not sure he has the same, like, existential worry about things as, like, Stuart Russell
or somebody.
Okay.
So he's not so worried about them necessarily churning us into, you know, fertilizer or
raw materials or something, but just that it's not so specific, I think.
But yeah, yeah, I talk to him about it all the time, and he wavers.
Oh, interesting, because I've heard you define yourself as a centrist on other podcasts,
because I'm sure the doomers would lump you in with Cholet and Lacune, maybe, and some
of the critics, but you do think that these models are intelligent, right?
I do think that they're intelligent.
Well, you know, intelligence is an ill-defined notion.
Oh, yeah.
It's multidimensional, and, you know, I don't know if we can say yes or no about something
being intelligent rather than, you know, intelligent in certain ways or to certain degrees.
Yeah.
Well, we've got so much to get into.
I mean, I think slowly we'll talk about arc and your concept arc work, but I kind of agree
with you that, and actually you had that paper out about the four fallacies, and you spoke
about this fallacy of pure intelligence, and I kind of agree that the gnarly reality is far
more complex than that.
There was a really interesting paper that you linked on, no, it was an article by Dileep George,
and he said that a university professor has a much better understanding of a vector,
because it's just grounded in so many real-world situations and contexts and so on,
and an undergraduate or, indeed, a language model would have a very ungrounded, very kind
of low-resolution idea of what this concept is, and it kind of leans away from this puritanical,
ungrounded, abstract form of intelligence to something which is really very complex and intermingled.
Yeah.
I mean, I agree with that.
Well, except that there's another aspect to that, too, which you write about, which is,
I agree that that happens, but what the human mind also seems to do is,
as the thing becomes more grounded in more cases, then we develop yet another concept
that kind of describes the similar aspects that we see throughout all those different
concepts, right?
So we're kind of this iterative loop where we're always finding more and more context,
and then we're also finding newer and newer concepts that span those increasing contexts.
Is that fair?
Yeah, sure, yeah.
I mean, that kind of goes along with the whole sort of
metaphor theory of cognition, of Lake Offit at all, and that we're sort of building on these
physical metaphors, and we can build up many, many layers of abstraction.
So, yeah, we can talk about that.
We're not recording yet, right?
Oh, no, we are.
We are.
This is all recording.
Oh, we are?
I hope you're okay with what you said so far.
But yeah, so there's the Lake Off building on the body of symbols as pointers.
And by the way, that Dileep George article was really fascinating because it was saying
that language is a conditioning force.
So actually, we all have these high-resolution world simulators built into us, and we kind of
condition how that operates and generate counterfactuals through language,
which I thought was quite interesting.
Yeah.
Yeah.
But, Tim, why don't you frame up the debate?
Because we found a beautiful paragraph.
We did.
We did.
We found an amazing bit.
Yeah.
But just to close the loop on what I was saying, we were discussing an activism last night.
I'm not sure if you're familiar with some of these externalized forms of cognition,
and we were talking about the concept of a goal.
And agents, of course, they just have these high-resolution belief trajectories of,
you know, I can do all of these different actions.
And that's not really a goal.
You know, a goal is this very abstract thing which emerges at the system level,
and no individual agents in the system have a concept of a goal.
And it might be similar with some of these concepts that we're talking about,
which is, to what extent do they exist, and to what extent are they just
something intelligible that we can point to, but they don't really meaningfully exist
in the system at a high-resolution.
Are you talking about an AI or in people, a little piece here?
All of the above.
I mean, I think goal is a wonderful example, because we think of it.
I mean, it's even one of Spelke's core priors.
It seems like something so primitive.
But I don't think they really do exist in us.
I mean, we're interesting because we have this reflexive conception of a goal,
but does a mouse have a goal?
Right. I mean, goal is another one of those words that, you know, we use in a very fluid
way. So we talk about, for instance, a reinforcement learning agent having a goal
that we've given to it, right? Or it might have a goal to kind of maximize its information gain
or something. But is that the same thing as a human having a goal that it's like,
you know, to graduate from college or to, you know, make something of your life for
all of these things? It's a very different sense of goal.
And so I would say, yes, a mouse has goals, but those goals are different in degree and in kind
of qualitatively than many of the things we call goals in humans and in machines.
So I think goal is one of those sort of anthropomorphizing words that we need to
be careful about when we equate goals in these different systems as being the same thing.
And I actually, you know, had a discussion with, I think it was with Stuart Russell
about the notion of goal. And his view, and I think this is a view of many other people
in AI, is that large language models actually have goals, complex goals,
that they, that emerge out of this, you know, their loss function of predicting the next token,
because the only way to successfully predict the next token in human language is to develop
human-like goals. I find that dubious, but it's an interesting perspective.
Yeah, I'm amenable to it, because there's always this dichotomy, as you say, of there's the objective,
there's perplexity, and there's these emergent goals, and there's even this simulator's theory
of large language models, which is that they're a superposition of agents. And it's quite situated
as well, because goals kind of materialize depending on your perspective. So if you use a
language model in a certain way from a certain perspective, it might appear that there is some
kind of goal there, but of course, it's just an aspect onto something which is very complex.
But I think we should frame up this beautiful piece, actually, from your
Modes of Understanding paper from much this year. I always call it the Modes of Understanding paper.
It was actually titled The Debate over Understanding in AI's Large Language Models.
And you said, towards the end, that the key question of the debate about understanding in
large language models is, one, is talking of understanding in such systems simply a category
error, which is mistaking associations between language tokens for associations between
tokens and physical, social, and mental experience? In short, is it the case that these models are
not and will never be the kind of things that can understand, or conversely, to do these systems or
their near term successes? Actually, even in the absence of physical experience, create something
like the rich concept based mental models that are central to human understanding. And if so,
does scaling these models create even better concepts? Or three, if these systems do not
create such concepts, can their unimaginably large systems of statistical correlations,
produce abilities that are fundamentally equivalent to human understanding, or indeed that enable
new forms of higher order logic that humans are incapable of accessing? And at this point,
will it still make sense to call such correlations spurious and the resulting solutions shortcuts?
And would it make sense to see these systems' behaviors not as competence without comprehension,
but as a new, non-human form of understanding? And you said that these questions are no longer
in the realm of abstract philosophical discussions, but they touch on very real concerns about the
capabilities and robustness and safety and ethics of AI systems. So let's use that as a leader.
What do you think, Melanie? It's beautiful. That was a beautiful paragraph, by the way.
Yeah, it's so good. Wow. This exactly crystallizes the discussion.
Yeah, I think that it's something that we in AI are all grappling with now. And I think it's
something that the history of AI has forced us to grapple with mental terms like understand,
or consciousness, and even intelligence. Because we keep saying, oh, well, understanding, if you
can do X, then that means that you're actually understanding. You can't do language translation
without understanding. You can't do speech to text without understanding. You can't generate
articulate natural language without understanding. And I think this is, in many cases, we then step
back and say, wait, that's not what we meant by understanding. It turns out you can do all these
things without understanding. So we're sort of saying, well, we didn't really know what we meant
by the term understanding, I think. And often, some people criticize that as moving the goalposts.
You're moving the goalposts. The so-called AI effect, right?
Right. It's the AI effect.
But I think of it more as AI is forcing people to really refine their notions
of that that have been quite fuzzy about what these terms actually mean.
And there was a fantastic talk by Dave Chalmers, the philosopher, who I think you've probably had
on this show, where he talks about conceptual engineering, which is something that philosophers
do where they take a term like understanding and they refine it. And he said, okay, well, we have
p-understanding, which is like personal, phenomenological. And then we have c-understanding
and e-understanding and x-understanding and all these different letters that meant to say that
this term is not a unified thing that we can apply to a system. We have to really specify what we
mean exactly. Well, one way I've come to think about it, and it's largely from reading your work
and your assessments about it, is that for the first time, we're actually being forced to do
the science of machine cognition, right? Because for too long, it's either just not been sophisticated
enough. Why bother? Like it's obviously not doing any cognition. And as you point out, it's now
actually having real world impacts. And so we actually have to start doing the science, right?
We have to say, okay, does this thing have cognition? Here's a hypothesis. Let's do some
test. Okay, it failed. What was the failure mode? Why did it fail? Let's understand that more. How
can we engineer it not to fail? It's like we can no longer ignore adversarial examples,
shortcut learning, et cetera. We have to finally grapple with it, it seems to me.
Yeah, I think that's exactly right. And what's interesting is we, computer scientists, were
never trained in experimental methods. We never learned about like controls and confounding things.
It's a great point. And so now people are doing, applying human tests of understanding or intelligence
or reasoning, what have you, to machines without having the right experimental methods to say whether
or not what they're testing is actually valid. So there's a cognitive scientist named Michael
Frank at Stanford, who's been writing a lot of stuff about experimental method and how do you
apply it to AI and why you need sort of expertise in this area to really make sense of these systems.
And I'm totally on board with that. Yeah, we'll talk about your piece with Tannenbaum later,
but as you say, a lot of AI folks don't really think about experiment design. But actually,
even with Chalet's ARC challenge, maybe we should talk about that. So he invented this
measure of intelligence, which unfortunately was not computable, but it was mathematically
beautiful. Basically saying that, and he's a huge Spelke fan, I kind of put Chalet very,
very close to you actually in AI researcher space. And his measure of intelligence basically says,
I give you priors, I give you experience, you give me a skill program, it extrapolates into these
different spaces and experience space. And the kind of the conversion ratio between those
priors and experience and the space that I get is intelligence. And that's very interesting.
And then he produced this corpus, this ARC challenge. And it's a bit like an IQ test. It's
this kind of 2D gridded colored cells. And you have a couple of examples, and you have to do
another one or two examples. And it was very diverse because it was testing what he called
developer aware generalization. And there were a couple of issues with that. So you wrote this
beautiful concept ARC paper, and maybe you can introduce that. But one of the things you pointed
out, which I felt was quite interesting is that even if people succeeded on Francois's challenge,
it wouldn't necessarily be what we would call intelligence, because it's not necessarily
demonstrating systematic generalization beyond those one or two examples in his test set.
So our motivation was twofold. So first of all, I love the ARC challenge. It's beautiful.
It's super elegant. And I'm very sympathetic with Francois' definition of intelligence,
although I think there's probably, again, intelligence is very multi-dimensional. But
this is one aspect of it for sure. And his problems are great because they
just give a few examples, and people are pretty good at abstracting a rule or a concept from
just a few examples. And they don't use language, so they don't get into the whole prior knowledge
of language and a lot of things that you don't want to confound these tests. But one of the
problems with ARC is that many of the problems are quite hard. They're quite hard for people.
And they're so hard that they don't really differentiate between different programs that
are attempting to solve this challenge. So there was a Kaggle competition with the ARC challenge,
and there were two, the two best programs got about, they each got about 20% accuracy on the
hidden test set. So it didn't really distinguish them at all. And the other problem was that,
as you mentioned, the test wasn't very systematic, meaning that let's say there's a
problem in ARC that deals with the concept of inside, something being inside something else.
And let's say that something, a program gets that one right. Does that mean that it understands
the concept of inside in a general way? Well, we don't know because the test doesn't test that
systematically. And that was actually intentional from Sholey, because he didn't want any way to,
for programs to be able to reverse engineer the generation process of these problems.
So if you say, oh, well, I'm going to deal with these 10 concepts, then somebody presumably
could reverse engineer those, the problems and not be general. But for us, we wanted to say, well,
how would you just systematically test a program for understanding of a concept of a very basic,
spatial or semantic concept? And so what we did was we took the ARC domain and we created
about almost 500 new problems that were systematically grouped into concept groups.
So like inside of, that was one of the groups. And so we looked at, we created several problems
that were variations on that concept. And there were variations that ranged in like abstraction,
degree of abstraction, and sort of complexity of the problem. And the hypothesis was that if a
human or a program could successfully solve the problems in a given concept group, they really
do have a good sort of grasp of that concept. So this was the genesis of concept ARC.
You know, it's fascinating because so you're, you're attempting again to build the science of
machine cognitive science, essentially. And hey, it has to be systematized, we need to have these
concept categories, we need to be able to generate examples of progressive complexity and, you know,
layer of abstraction, everything. And then yet you mentioned Chalet intentionally didn't
systematize it to avoid reverse engineering. And that's kind of a fascinating
point because reverse engineering can even happen, you know, just by way of selection bias. So I mean,
researchers are out there, they're fooling around with different neural network structures, maybe
I'll add like a you here or some horseshoe over there. And lo and behold, suddenly, it works
really well on the concept of inside out. And I'm going to claim this is machine learning,
even though it was actually human engineering that sort of put that structure into the network.
So in the long term, you know, how do we, how do we balance that? Or how do we avoid it? Or how do
we test for machine induced, you know, prior knowledge versus actual machine learning?
Yeah, no, I understand it's a hard problem. And I think, you know, the goal with this concept arc
benchmark wasn't to sort of supplant arc in any way, it was really meant to be complementary.
And it was meant to be kind of a stepping stone to the much larger and more difficult arc set.
Because I think, you know, even if I tell you all of these problems have to do with
the concept of inside versus outside, you would still have to have a good grasp of those concepts
in order to solve these problems. And I'm not sure that you could sort of engineer something
that would solve those cons problems of that concept in general, without having a more,
you know, really a general understanding in some sense of those that concept.
But to Keith's your point, I think having a static benchmark is a problem, sort of putting out a
benchmark that everybody can kind of try and optimize their program to solve. We've seen
that over and over again. That ends up being sort of a way that people end up reverse engineering
to a particular task rather than to a more general set of conceptual understanding. So
I do think that we have to keep changing our benchmarks. We can't just say, okay, here's image
net, go, you know, beat on that for the next 20 years until you've solved it.
That's not going to yield general intelligence.
Yeah, I think one of the issues we're talking about in general is extrapolation. So, you know,
Sholey used extrapolation to talk about skill programs and being able to do things beyond
your priors and experience. But with benchmarks, it's about human extrapolation. So I think part
of the problem with the risk debate, by the way, why everyone's so suddenly worried about risk is
because of this benchmark problem. And that's because we see that humans who can do A can do
B. And now we see machines that can do A. And we have all of these built-in assumptions in
benchmarks. And we don't really realize that we're talking about machines now. We're not talking
about computers anymore. And I think it's causing a real problem. I don't want to be hyperbolic here,
but it feels like there's this massive delusion taking over the entire machine learning community.
And we're seriously talking about AI risk. And I think it all comes down to these
benchmarks fundamentally. Yeah, I do think all of our benchmarks have, as you say,
have this problem of that they have assumptions built in that if a human could do this, that
then the machine must, if the machine does it, it has the same kind of
generalization capacity as a human who could solve that problem. This goes back all the way to say
chess as a benchmark. So people used to think that if, because if a human can play chess at a
grandmaster level, that means they must be super intelligent in other ways, that if a machine
could play chess at that level, it would also be super intelligent like a human. Herbert Simon
even said that explicitly. But then we saw that chess actually could be conquered by very
unintelligent brute force search that didn't generalize in any way. So I think this is an issue
today with large language models. They can do things like pass the bar exam and pass other
standardized human tests of skill or intelligence. But what does that mean? It doesn't necessarily
mean the same thing for a machine as it does for a human for many different reasons.
Yeah, I guess it's a similar thing with the McCorduck effect that we have relative pointers
to what we think of as being intelligence. We just point to something and then when that thing
becomes easy, then we need to kind of move the pointer. Yeah, I think it also feeds into, as
Tim was saying, I think it heightens the fear of existential risk because of this, this concept
that we have of intelligence always wins, which even among humans is, is a flawed concept, right?
I mean, you know, many nerds who grew up through elementary school can tell you like
intelligence doesn't always win, right? Like sometimes it's numbers or brute force or
whatever else kind of kind of wins. And they assume like, well, if we were to have this
purified intelligence that was super intelligence, it would be as if a human brain were super
intelligent and they'd be able to do everything a human being could do and hurt other people and
conquer the world and fight wars. And that again, is this anthropomorphic projection, right?
Yeah, I mean, right. So, and it's this notion that intelligence is this thing that you can just
have more and more of. Forever. Forever. Or so far that it's just beyond any, you know,
it's almost magical, right? And it's capable. Right. And it's not, you know, a different
view of intelligence is that it's a collection of adaptations to specific problems for a particular
kind of organism in an environment. And it's not the sort of an open-ended, pure domain
independent thing. So, I think this is why, you know, you see a lot of discussion of super
intelligence, AGI, you know, AI risk in among computer scientists, but you don't see a lot of it
discussed among like psychologists or animal intelligence people or other cognitive scientists.
Because that's not the way that they understand intelligence.
I would love to explore more about that because, I mean, only yesterday when we were talking about
an activism, we're also talking about Gibson's ecological psychology. And even Elizabeth Spelke,
I mean, this kind of cognitive psychology view is very related to nativism. It's this idea that we
have these fundamental cognitive primitives and intelligence in some sense is just traversing
or recomposing this library of cognitive modules that we have. And those modules are very physically
situated, you know, they tell you something about the environment that you're in. Which means that
intelligence is just very gnarly and it's very kind of coupled to the environment we're in. It
can't really be magically abstracted in a computer with infinite scale.
Yeah, I think that's right. That's, you know, people have different views about the nativism,
empiricism, debate. And there's whole different schools and cognitive science about like how
how much is learned, how much is evolutionarily built in and all of that. But I think most people
in the field would agree with what you said that intelligence is very gnarly. It is situated, it
is specific to particular domains of concern to a particular organism, and that it's not easily
abstractable. You know, that back in the early days of AI we had
Newell and Simon, two of the pioneers of AI who had this thing called the physical symbol system
hypothesis, which was that basically you could sift off intelligence from any material substrate
like the brain and put it in some other material substrate like a computer. They were thinking
about symbols, but nowadays people have the same kind of view with neural nets or
transformers or whatever, that you can take human intelligence that's very situated and
tied to the environment and sort of sift off the pure part and leave all of that bodily stuff
and you can get something like superintelligence. And I don't think most people in cognitive
science would agree with that. Well, on the other hand though, I think, and I'd be curious to get
your take on this, is one direction that that comes from is for those of us, and I include
myself in this camp tentatively, that at the end of the day what the brain does is some form of
computation. You know, like absence, the proof that there's such a thing as hypercomputation,
like our brain, all of its calculations could be embodied in a large enough
you know, Turing machine and a large enough computer of some kind. And therefore, everything
that we do, including our intelligent activities, could be coded somehow or another into a Turing
equivalent system. And for the record, I don't believe neural networks are. I've said this like
multiple times, at least in their current manifestations, they're not, they're just a
feed forward, you know, thing at the end of the day. But if you actually had a computer, you could
have human symbolic intelligence encoded. Like, where do you stand on that, on that debate, if you
will? Yeah, I have nothing against the idea that the brain does computations. I think that's,
that's, you know, one possible way to look at it. And that those kinds of computations could be
implemented in another kind of computer. But the brain is a very special kind of
sort of biological computer that's been evolved to do specific things. And one of the main things
the brain has been evolved to do is control the body, and in particular kinds of environments.
And so I think the brain is doing computations, but it's doing very, very highly evolved, very
domain specific computations that perhaps don't necessarily make sense without having a body.
Now, that's debatable. But it does seem like a lot of the way that we reason is by reference to our own
sort of episodic experience in the world.
Or at least to the capabilities that have been built into us, you know, like visual, using our
visual cortex to imagine cubes and steers and whatever else we need to solve a physics problem
or a geometric problem. Sure, sure. Yeah, so I'm fine with saying the brain is a computer of a certain
kind, but that's not to say that it's going to be, you can just kind of lift off the computations
and then put them in a different substrate and kind of get everything that's human like,
because I'm not sure that those computations are going to make sense in the absence of the rest
of the organism. Yeah, there was something that always confused me about the autopoietic
inactivists, because of course they as they issue representationalism and information
processing, but they also issue computationalism in general. And as Keith was just saying, I don't
even if cognition is externalized, I don't see any reason why in principle, you couldn't just
compute the entire system and and recreate the computation. I just wanted to close the loop on
the ARC challenge stuff though. So you said that the winning solutions to Francois' challenge on
Kaggle, they were quite simplistic in a way. They were like a genetic search over lots of
primitive kind of functions. And even the winner said that they didn't feel it was a satisfying
solution, which was interesting. And then you tried it on GPT4. And I think you said you got
around 30%. There's now a deep mind paper out very recently, which just basically turned it all into
a character set with a random mapping, put it into GPT4, I think got nearly 60%. Even
even somewhat invariant to the translation between the character set mapping. Some folks on our
Discord forum tried to reproduce it and couldn't. That's the problem with GPT4. You can never
reproduce anything. But I was just wondering, would you consider that to be an elegant solution?
It's not really much better than searching over a DSL, is it?
By that, you mean giving it to GPT4?
Well, I mean, it's quite an interesting thing, isn't it? If there's the McCorduck effect,
and even before you get to a solution, what would a good AI solution look like? I mean,
what would someone have to create for you to say, oh, that's a really cool AI solution?
Well, if you had a program that really could solve these tasks in a general way,
that would, however it worked, it would be a good AI solution. I don't necessarily think we have to
have something like the way people do it. Well, let me see if I can guess, though,
maybe an extension to what you said. It's in line with your argument that the benchmarks
have to evolve. Because I think that these benchmarks really is just first pass or low
pass filters. It's like they weed out the junk. It's like, well, if you can't pass the art challenge,
I'm not going to bother with you. If you pass the art challenge, now we have to look further,
right? Which is like, okay, so it's been able to generalize along these 19 concepts that we've
defined in concept art with little pixel grids. What about if we give it full frame pictures
or video or something? Is it able to generalize there? No, okay, it failed. Why did it fail?
Well, now we need to do some more engineering. It's going to be this kind of never ending sort
of iterative process. So I would say if something passes arc or concept arc, then it's worthy of
further study. Sure. Yeah, I agree. I mean, one question is that arcs are very idealized kind
of micro world type domain. So does it capture what's interesting about the real world
in terms of abstraction? To some extent, yes, probably, and to some extent, probably no.
So you're right. Solving arc doesn't mean we're at AGI, if you want to talk about that.
It's like in chess, what you brought up earlier. If you took whatever the current best,
let's say LC zero or something like that, and it's been trained on standard chess,
and then you have a go play chess 960, formerly called Fisher random, where you just random,
it's going to suck like humans are going to destroy it, right? Because humans have learned
a more generalized and by the way, that also destroys human beings who rely on memory and
just sort of like the memorized positions that haven't learned, let's say the skill
of playing chess, right? And so this is the type of thing that's going to happen, right?
It's like you say, when you take this intelligence and try to apply it to a different context,
that's when the rubber meets the road as to whether or not you really learned
the concepts, right? Yeah, no, definitely. I agree. And I don't think like our concept arc
wasn't meant to be like a test of AGI in any sense. It was meant to be kind of a stepping stone to
getting to abilities for abstraction. And clearly, if some program was able to solve all of the
problems in that domain, and we'd have to then test further, we'd have to have it be able to
extrapolate to a new kind of domain that tested the same kinds of concepts. So you're right,
there's no end in some sense. But at some point, I guess, and I don't know when that point is,
we have to say, well, this thing seems to be understanding this concept.
That's the wonderful continuum, though, because you said earlier, there's something deeply
unsatisfying about chess brute forcing everything. And when we apply Francois' measure of intelligence,
we don't think of that as intelligent because it's just brute force experience. And then we
find something which is a little bit more efficient. So it's something which appears to work. But
now, another interesting thing is when you talk about concepts, you had this beautiful article
out earlier that she had talking about, on top of, she's on top of the world. And what would
Dali draw? It would draw a globe with someone dancing on top of it, or I'm on the TV. What
does that mean? It should mean that I'm actually being rendered on the TV. Now, it's kind of like
what we were saying with goals, isn't it? Because this skill program, someone just goes on Kaggle
and they gives you this program and it seems to work. But it's horribly complicated. And how do
you know that the internal representations are in any way related to these abstractions? And do
you think that the abstractions as well are somehow universal in the same way Spelki would say that
the cognitive priors are? Yeah, I think it's something we can't say. And we don't know with
humans. And we don't know with machines, because both of these are very complex systems that are
hard to kind of pull apart. What are the internal representations? So in most cases, we have to
rely on behavior, which is very noisy. It can be misleading. And it turns out that humans
often are not, if you give them a problem, like a reasoning problem, in a familiar domain,
they're much better at doing that problem as doing the exact same reasoning kind of abstract
reasoning task in an unfamiliar domain. And I think that's something that people have shown
is also true of large language models, because they've learned from human language and have
incorporated sort of the statistics of some of the statistics of human experience that they're
much better on familiar domains than on non-familiar domains. But the one thing that humans can do
is often they can kind of transcend that and learn how to reason much more abstractly,
which I don't know if we will get to that point with language models yet. So there's a wonderful
paper that just came out from a group at MIT and some other places called, I can't remember
what it was called, it was something like reasoning versus reciting. And what they do is
they talk about this notion of a counterfactual task, which is if you can do one task, like
addition and base 10, and you really understand that notion of addition, you should be able to do
addition and base eight. And so, but you haven't had as much experience as like for a language model,
it's not almost all of the training data has to do with base 10. So, but can, so they tested,
they did a whole bunch of these so-called counterfactual tasks
and showed that GPT-4 is really good at the original task, but not so good at the counterfactual task.
So it's not, in some sense, it is relying on sort of patterns in its training data rather than
genuine abstraction.
It's a stochastic parrot, right?
Well, you know, it could be argued that humans do that a lot too.
I don't know if you called a stochastic parrot, but it's more like a pattern matcher.
And it's not, it's not reasoning about the things in the sense that we think of reasoning,
you know, as sort of domain independent ability. It's very domain dependent.
Yeah, so the difference is that I guess the difference I would say is that humans,
it can kind of overcome that domain dependency in some cases and actually get to the true
abstraction, but I don't know that language models can.
Yeah, I mean, there's a couple of things here. So first of all, these language models fail at
things which four-year-old children can do. And they can pass the bar exam, but as you've said
previously, you wouldn't want one of these things to actually go and practice more.
My God, could you imagine the thought? And there was this Sparks of AGI paper where they gave this,
I mean, maybe you could recite this better than me, but there was the thing about the
book Nine Eggs, a laptop, a bottle, and a nail. Can you balance it in a stable manner?
And this comes back to the experiment design because, my God, in any other discipline of science,
they would just tear this apart. They would say, well, that's not very robust. I mean,
you came up with an example with a pudding, a marshmallow, a toothpick. How would it balance it?
Yeah, did it not balance the full glass of water on top of the marshmallow?
Well, it stuck the toothpick into the marshmallow and then that's not exactly what we had in mind.
No, and in fact, the Sparks of AGI paper, they explicitly said,
we're doing anthropology, not cognitive science.
Well, that's not the way it was interpreted. Unfortunately, there are YouTube channels
now dedicated to educating people on AI and they're taking this as gospel. I mean, what's going on?
I think there's just not as much of a focus on sort of scientific method in this field as there
should be. And I think in science, if you're looking at a phenomenon and you're trying to
replicate it, if it only replicates half the time, that's not a replication. That's not a
robust replication. Whereas for language models, people are saying, well, if it can do this task
once in one particular circumstances, then it probably has this more general capability.
So if it can do this stacking problem once, then wow, it has physical common sense.
And people with my marshmallow example, people, of course, jumped on it and said, wait,
if you prompt it in a certain way and you do all this prompt engineering,
human engineering, it does it right. And then like, well, that's not the point.
The point is not any particular example. The point is figuring out how to test things
so that you actually have some kind of robust ability for replicating a capability,
which we haven't seen with experiments on language models very much. I mean, people
are starting to do this. People are starting to do this kind of more scientifically grounded,
experimental method on language models, but it's still not very, there's not very much of it.
So you might appreciate a phrase I recently coined because it covers this leakage too,
of like sort of leakage of human knowledge, which is if you can't find the priors, look in the mirror.
It's like, we have to learn how to do experimental science and computer science,
and you've got to guard against this type of leak at Drillian, human engineering,
and over-involved and whatever. And this is why I really want to collaborate with people in
developmental psychology, with people in animal cognition who face this kind of issue all the
time. And one example was, I got from a developmental psychologist was that sometimes
like a three-year-old can tell you something like four plus three is seven, but if you say,
if you give them a bunch of marbles and say, pick out four of them, they can't do it.
So there you say, okay, that this kid doesn't understand the concept of four,
they're kind of just reciting something that they've heard. And this is the kind of experiments
that people in developmental psychology do all the time to really tease out what the system,
what babies and children know and what they can do. And it's not an easy thing to do in
this kind of experiment. The problem with that is it's extremely complex and requires so much
domain knowledge. So it takes a very long time, because I think there was another article that
spoke about how we study rats. And those folks in different disciplines, they're really,
really good experimental design, and they have experts who kind of create very, very clear
criteria for measuring this behavior. And with AI, everything's going up on archive,
and everything's going a million miles an hour. And by the time you actually design
a systematic rigorous study for the first thing, there's already another paper coming out,
which is claiming to do it differently. So we just can't keep up. It's just,
it's an absolute nightmare. Absolutely. Yeah. Agreed.
I want to just, so I'll quickly touch on one more thing. And I know Keith wants to go into
complexity. But yeah, so the information leakage is a problem. The brittleness is a problem. I do
think of these GPT models a bit like a database. And so anything that requires physical grounding,
of course, doesn't work very well. Some things work surprisingly well, like, you know,
programming, because programming is mostly in the internet, it still has all sorts of
failure modes, and it's not very reliable, but it's surprisingly reliable. But you put a paper
out with Tanenbaum and a whole bunch of other people. And you actually said, well, if you want
some policy advice, if you really want to think about how we can improve the situation, you said,
aggregating benchmarks and also giving instance level failure modes can actually help us understand
why things went wrong or, you know, why things gave us the right answer for the wrong reasons.
And there were all sorts of limiting factors, you said. You know, we have this kind of
censorship by concision. You're only allowed to have seven pages in your conference workshop paper,
and there's no policy about this. So can you give us a heads up on that?
Yeah, I mean, you know, traditionally in machine learning, people use accuracy and similar kinds
of aggregate measures to report their results. And, you know, if someone tells you that the accuracy
was, you know, 78%, what does that tell you exactly? I think, you know, this gets back to the idea of
scientific method. You know, in science, the most interesting things are the failures.
And those are the things you really have to focus on. It's like, why did it fail?
And that's what we need to know to understand machine learning systems. So the most simple
kind of reporting would be just to report for every instance in your
benchmark, your data set. How did the system do? What was its answer? And that's not, you know,
it doesn't seem like a very big ask, but it would be very useful. And we now have in conferences,
you're allowed to have some kind of supplementary material online. So you could have this available.
And we did this for our concept arc paper. We showed for every instance, like what humans did,
what machines did, we tried to analyze the errors of the system. And I think this these kinds of
reporting will be will give us a lot more insight into what these systems are doing and what their
like real capabilities are. Yeah. And it's, and back to the difficulty that Tim mentioned earlier,
totally agree. And this is work that has to be done. Like if, if we are going to build a science
of machine cognition, you know, this work has to be done. Yeah, I think. And I just want to shout
out to Ryan Bernal, who spearheaded that paper, because he really is the one pushing for all
this. And I think it's fantastic. So just in the last few minutes, you know, since we have you,
complexity and complexity theory is a topic I really love. I'm not an expert in it at all,
but I like to think about I like to explore it. I'm just curious, you know, from your perspective,
um, what are some of the most interesting things happening right now in complexity theory? And
if I wanted to go learn a bit more and check out just some cool, you know, latest stuff, what should
what should we go look at? So I think there's, you know, there's a lot of interesting stuff going on,
obviously, and complex systems is a huge umbrella for a lot of research. But
if you're interested in the one big topic that people look at is called scaling. And it's the
question of like, what happens to a system as it gets bigger in some sense? So this started out
with some work on the sort of energy use of systems like animals as they as their maths increases.
And people discovered some really interesting scaling laws that were very non-intuitive and
they were able to explain these laws using ideas like fractal fractals and the fractal structure
of complex systems. But now, so this is all on like biological metabolism and things like that.
But now a lot of people are extending that scaling work to cities. So asking what happens
to cities when they increase in size, either in area or in population size. And
there's all kinds of phenomena that you can see, like what's the rate of innovation
measured by something like patents? And what's the rate of sort of energy usage by a city?
And what's how do these things change? Even like the happiness of the people,
you know, are people in New York happier than people in Santa Fe, which is a much smaller city?
These things scale in really interesting ways. And it's opening up a lot of new ideas about how
social systems work. And how... Is it a similar thing that you can't trust the benchmarks? Because
how happy people are, might you look at the rate of antidepressant usage or something?
Yeah. So you do have all these... Right. I don't know if that's exactly what they use, but
you do have to look at ways to measure these things, which can be questioned.
But there are a lot of really... And I think this whole science, the science of cities, is
it's very preliminary. And there's a lot of ideas about how to measure these things, how to
develop sort of analytical descriptions or laws that govern certain properties and how to
interpret them. But there's just a lot of really interesting work in this. And it turns out that
now that everybody has a cell phone, you can really do a lot of tracking. A lot of these quantities
can be tracked by people's sort of their movement, their interaction with other people, and all these
things that you can measure using cell phones. So that's very cool.
That is... Yeah. Thank you. That sounds actually fascinating. And one reason why for me particularly
is... Are you familiar with Asimov's Foundation series? Yeah. So you know, psycho history in
there was the science... And it was almost like a thermodynamics of human behavior that was only
applicable at kind of planet scale and beyond. So it's like these scaling laws. So this is maybe
one step towards... Very similar, psycho history. Yeah. One step towards psycho history of Asimov's
kind. Exactly. Yeah. Cool. And in closing, does that give you intuition on the scaling of intelligence?
Well... That's a great question. And I think, you know, one question you can ask is like,
there's individual intelligence and then there's collective intelligence.
And how much of the intelligence that we have individually is actually grounded in a more
collective intelligence? You know, there's many things that I don't know, like I don't understand
quantum mechanics or something, but I know somebody who does. And therefore, I feel like it's understood.
And a lot of our intelligence, I think, is sort of more social than we think.
Oh, absolutely. And folks should definitely read Melanie's book. So your complexity book,
we actually covered that quite a lot on our show on Emergence. It's absolutely wonderful. And of
course, your book on AI is probably the best book on AI I've ever read. It's up there with
Christopher Sommerfield's book. But anyway, Melanie, honestly, you are my hero. Thank you so
much for coming on MLS2. I really appreciate it. Thanks so much for having me. I really enjoyed
it. It's great talking to you.

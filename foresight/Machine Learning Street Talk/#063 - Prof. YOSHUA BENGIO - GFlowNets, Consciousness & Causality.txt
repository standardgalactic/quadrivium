my pleasure. And I must say, I've been really impressed by all your
questions. It showed that you did prepare and read papers and think
about it. And that's very much appreciated. Thank you.
Today is an incredibly special occasion. We have Professor
Yoshua Benjiro on the show. Just honestly, I just can't get over
it. But first of all, a little bit of housekeeping. So we've
just launched a new Discord community. So please jump in
there, say hello, introduce yourself. If you want to be, you
know, part of the moderating community or just help us do
stuff over there, we would love to talk with you. By popular
demand, we've also added a couple of ways in which you can
support us. So we now have a Patreon and a merch store. If
you're interested in supporting some of the episodes of MLST
then get in touch with us because we'd love to have a
conversation with you. We're just doing so much cool stuff
this year. We've already recorded about six episodes that we
haven't released. And we've got some amazing people booked as
well. So yeah, it's going to be incredible. As always, if you
like the content here, please consider hitting the like and
subscribe button and rating our podcast on iTunes because it
really, really helps us out. I called it iTunes. Is it
iTunes? Apple podcasts? I don't know, whatever it's called.
Wait and Biases is the developer first MLS platform. And
we're extremely proud today that they are sponsoring this
episode. Now tracking machine learning experiments is
difficult. Using the winging it methodology can only get you so
far. What we need is a platform where we can compare models and
visualize their performance characteristics against all of
the previous runs and figure out the best hyper parameters to
use. Now most importantly of all, this process needs to be
reproducible. Sounds like a tool order, right? Well, this is
exactly what the Wait and Biases platform does for you. Now you
can even follow metrics from long running experiments in real
time. I think it's really important to lean into the
complex interaction between science and engineering in the
ML DevOps lifecycle. Data scientists need valuable feedback
and they need to communicate why they're running given
experiments and they need to share their notes around the next
steps. Reports keep this work well organized and connected to
the Waits and Biases experiments which were run as opposed to
just sharing random screenshots in Slack. It's so easy to create
a report and share it with your team after you finished with
your experimentation. You could just add notes for yourself as
well to explore later on. You can keep a work log and you can
even share your findings internally or externally. This is
an absolute game changer. I'm a big believer in this kind of
engineering rigor. I'm the CEO of a code review startup called
Merge these days and I love how the pull request process and
tooling immortalizes important collective decisions which were
made during the software development lifecycle. Similarly,
Waits and Biases immortalizes important decisions that were
made during model development, experimentation and
deployment. Remember, check out Waits and Biases today by going
to 1db.com forward slash MLST and if you're interested in
sponsoring future episodes, get in touch with us. Waits and
Biases are currently sponsoring our premiere shows but we have
lots of other content coming and opportunities for
sponsorship so let us know. Cheers.
Professor Yoshua Benjo has just released a bunch of papers
around G-Flow Nets. Now G-Flow Nets exists squarely in the
domain of active learning which is a model that can economically
ask an oracle which is probably the real world for the most
salient training examples to continue learning. The learner
can choose or have an influence on the examples it gets and we
want to learn a function which approximates the oracle
efficiently. How should we pick the queries? How should we take
into account not just the value of the predictor but also how
certain we are about the predictors from the learning
system? Areas of uncertainty or entropy are kind of like
interesting candidates for us to explore further. We need to
be able to imagine or invent queries to give to the oracle.
Now one of the reasons that machine learning models are so
sample and efficient is because of the combinatorial space of
possible input examples. We can't train on everything because
the space is just too large, it's vast. So you might have heard
of a related concept of active learning called machine
teaching which is an interactive version where the human
interactively selects the most salient data to train a machine
learning model maximizing the information gain in respect of
the training samples. Now the reality is the function space
that we're learning here is highly structured. We only really
need to sample training data where most of the rich
information exists in that function space. I mean, if you
think about it, a machine learning model, it's just a joint
probability distribution between signals and labels. And this
distribution has modes or areas of density or information. And
actually most of it is just areas of nothingness, which
require fewer training examples to learn and to represent. Now,
if you spoke to a to a Bayesian person like my friend Conor
Tan at work, you know how to learn this distribution, they
would bring up Markov chain Monte Carlo quicker than a whip it
with a bumful of dynamite. Now Markov chain Monte Carlo is an
increasingly popular sampling method for obtaining asymptotic
information about unnormalised distributions or energy
functions, especially for estimating the posterior
distribution in Bayesian inference, which is where you've
probably heard of it before. Now you can characterize a
distribution without knowing all of the distributions
mathematical properties. So if you don't have an analytical
representation for it, just by randomly sampling values out of
the distribution. Now a particular strength of Markov chain
Monte Carlo is that it can be used to draw samples from
distributions, even when all that is known about the
distribution is how to calculate the density for different
samples. Now the the Markov property of Markov chain Monte
Carlo is this idea that random samples are generated by a
special sequential process. And each random sample is used as a
stepping stone to generate the next random sample. Now this
might sound very complex, but the practical implementation is
pretty simple. Markov chain Monte Carlo just starts with an
initial guess, just one value that might plausibly be drawn
from the distribution. And then we produce a chain of samples
from this initial guess by adding random perturbations in the
neighborhood of that example. And each new proposal drawn from
that random perturbation distribution is either rejected
or accepted. There are different flavors of this, of course, I
mean, in particular, like tweaking, how the random
proposals in the neighborhood are selected or whether the
proposals are selected. The simplest heuristic being whether
it's below the function or not. Now the idea is that Markov
chain Monte Carlo methods, they capture a distribution with
only a relatively small number of random samples. But the
reality is anything but in high dimensions, and where the
distribution has many modes spread far apart, it's actually
exponentially expensive. There's a bunch of human orientated
hacks to try and make this work well in specific cases. But we're
missing a much more general machine learnable solution. This is
the main reason why we haven't seen it used in many machine
learning applications yet. Assuming that the function we want to
learn has underlying structure, then we can escape the
exponential time of Markov chain Monte Carlo with machine
learning. And this is what Benzio calls systematic
generalization, which is to say, how do we generalize far from
the data in a way which is meaningful. Now G flow nets are
an active learning framework, where the name of the game is to
generate salient and diverse training data to augment our
model in the most sample efficient way possible. For G flow
nets to work, we need a reward function and a deterministic
episodic environment. Does that sound familiar? Yes, just like
reinforcement learning. Now a flow network is a directed graph
with sources and sinks and edges carrying some amount of flow
between them, you know, through intermediate nodes. So I think a
good way to think about this is pipes of water. Now for our
purposes, we define a flow network with a single source. So the
root nodes, or you might say the sinks of the network
correspond to the terminal states. Now it's designed to find
the possible trajectories through our system. Okay, and just
think of Alpha zero as being like a good analogy for these
trajectories. Now the training objective is to make them
approximately sample in proportion to the given reward
function. This is in stark contrast to Alpha zero where we
were sampling to maximize the expected reward. So Benzio's big
idea is that we can have an interacting loop between a
generative model and the real world. The real world is
expensive. So why not train an imagination machine in our mind
until we're ready and waiting to produce good questions to the
real world, we could use imagined experiments to train our
generator, then produce queries to the real world. We were
thinking about a way to visualize how G flow nets work when the
idea of a Galton board came to mind. A Galton board also known
as a beam machine is a common prop in statistics courses,
science museums, and fun gadget stores. The board has rows of
interleaved pegs above a bottom row of buckets. Beads are filled
into a funnel at the top of the board and then sprinkled on the
top center peg. The beads bounce either to the left or to the
right as they hit the pegs and eventually collect into buckets
at the bottom. If the pegs are precisely and symmetrically
arranged, the beads will aggregate at the bottom into a
familiar binomial bell curve. Now imagine that the pegs were
instead flow gates with adjustable valves that could
direct the beads more to the left or more to the right to
bias the flow paths. With such a machine, you could adjust the
valves or flow rates to create any distribution. For example, to
create a uniform distribution, we'd open up the gates flowing
away from the center line of the board to drive more bead flow
to the fewer number of paths leading to the edges and the
corners. Or to create a multimodal distribution, we'd arrange
the gates to split the flows into two or more streams that would
then pile up in multiple humps or modes below. There's a lot of
flexibility here. Indeed, given a distribution, there are
generally multiple flow gate solutions to produce it. It'd be
nice, wouldn't it? If we had an intelligent, principled way to
train these gates. Enter G flow nets. G flow nets put a neural
network, a brain behind the flow adjustments. A brain which can
optimize the gates to match any distribution we desire. Here,
we're interested in sampling a reward function in the context
of reinforcement learning. In that context, this is a powerful
simulation and sampling paradigm. You see, once the brain has
tuned the flow weights, such a modified Galton board, or more
generally, a flow network, can sample diverse paths quickly
and efficiently, leading to the reward distribution. It's
important to point out that the path sampling is more diverse
doing it this way. Unlike classic reinforcement learning, a G
flow net doesn't just fixate on a small number of high reward
paths, it happens to find first. Instead, it stochastically
samples a broad spectrum of paths in proportion to their reward.
Sure, high reward paths will be sampled with higher weight. But
the far larger population of low reward paths will get a share
of the sampling as well. Why should we even bother with such
paths? The answer is we need to balance exploitation or high
reward with exploration or learning to better learn the
reward function. This is especially important when dealing
with complex real world scenarios of high uncertainty. For
example, think of molecular drug discovery and design or
navigating jungle terrain. In both those scenarios, we really
know very little about how a particular path may play out. We
might stumble into the next miracle cure or a pitfall of
quicksand. To find the globally best paths, it's important to
keep our options open. Beyond this sampling diversity, G
flow nets also bring the full power of neural networks to
discover latent structure and learn the reward function. This
combined with their diverse sampling also makes G flow nets
more robust when dealing with multimodal distributions, which
are a common trap for greedy algorithms and Markov chain,
Monte Carlo. If there is structure linking the multiple
nodes, G flow nets can learn it and extrapolate to new modes
and once discovered, they will by design drive the sampling to
cover those modes and learn more structure. Overall, G flow
nets seem to offer an intriguing new path pun intended for an
intelligent sampling paradigm. So you might ask how are G
flow nets different from Alpha zero? Well, the policy network
and Alpha zero gives you a set of actions. Given a state, Alpha
zero trains the policy network to maximize reward so that the
trajectories all end up at the highest reward. Now what G
flow nets do is they train so that the actions are distributed
in proportion to the reward. So rather than pruning away all of
the low reward trajectories, it will sample them just less
often. Now there is a manifest difference between G flow nets
in respect of exploration. I mean, you might argue that the
Monte Carlo tree search is still doing wide exploration at the
beginning. But in spite of its rapid convergence and pruning of
load reward trajectories, it's still sampling from the
underlying probability distribution, which has been
scaled with a softmax. So that's actually not that much to
explore in the first place. So in summary, G flow nets are
better than Alpha zero Monte Carlo tree search in some sense,
because they achieve the same goal by offloading the burning
time and the stabilization time of Markov chain Monte Carlo.
Remember this whole thing can be trained offline. And then
when in inference mode, we can do it in a single shot. Whereas
with Monte Carlo tree search, we actually had to do it in
inference mode as well. The other thing is we're kind of
offloading all of the human engineering required to sample
efficiently from Markov chain Monte Carlo. And the other thing
is diversity, baby. I mean, consider the difference between
how G flow nets and Alpha zero sample the reward path
distribution. If you looked at the distributions, you would
see that Alpha zero has a little box around the mode. G flow
nets is the whole distribution. We know very well that
diversity preservation is critical in order to discover
interesting stepping stones and search problems. Now finally,
Benzio has published results showing the G flow nets converge
exponentially faster than Markov chain Monte Carlo and PPL on
some problems and finds more of the modes in the distribution
function faster. Enjoy the show folks.
Professor Yoshio Benzio is recognized worldwide as one of
the leading experts in artificial intelligence. Indeed, a god
father of deep learning. His pioneering work in deep learning
earned him the Turing Award, which is the Nobel Prize of
computing. He's a full professor at the University of Montreal,
and the founder and scientific director of Miele, which is a
prestigious community of more than 900 researchers specializing
in machine learning and AI. He's one of the most cited
computer scientists on the planet. And I can't even begin to
articulate how honored we are today to have this conversation.
Yoshio has done a lot of work recently on G flow nets, which
are an active learning framework in a reinforcement learning
configuration, where the name of the game is to request salient
and diverse training data from the real world to augment our
learned models in the most sample efficient way possible. Now
we're trying to minimize the divergence between the path
distribution and the reward distribution, and then sample
paths according to the reward distribution. This is in stark
contrast with traditional reinforcement learning, where we
trying to maximize the expected reward. This approach is likely
to find diverse strategies instead of being greedy and
converging quickly after finding a single one. Anyway,
Professor Benzio, this is amazing. Can you tell us about this
exciting work in some of its applications?
Yeah, I'm, I don't think I've been as excited about a new topic.
At least in the last six or seven years as I'm now with G flow
nets. And it's actually even much more than what you've been
talking about. The way I think about G flow nets is a kind of
framework for generic
learnable inference for probabilistic machine learning. So
one way to think about this is it's a learnable replacement for
Monte Carlo Markov chain sampling. But actually, so there's
that and I'll explain if you want why this is important and to use
machine learning there. But but also, it can be used to
estimate probabilities themselves, not just sampling, but
also estimate
intractable quantities like partition functions and a
condition probabilities that would otherwise require summing
over an intractable number of terms. So I think of this as
the potentially, you know, there's still we're still at the
beginnings of this has a Swiss army knife of probabilistic
modeling that uses machine learning to be able to do things
that look intractable, but do them efficiently thanks to
generalization power of large neural nets.
We've been trying to think of a way to help our listeners
visualize what a what a G flow net does. And I wanted to run by
a possibility to you. So I'm not sure if you've heard of
Galton boards also called, you know, bean machines. And what
they are is this prop that's often used by statistics
professors at the start of say, an elementary introductory
course to give a visual intuition. And it's a board that has
these vertical buckets down at the bottom with interleaved rows
of pegs above the buckets, and then beads are filled in into the
top of the board, and they bounce either left or right as
they hit the pegs. And they eventually collect down at the
bottom. Yeah, yeah. Yeah, now now if the peg is a very good
analogy, except that it's not a tree, I don't know how these
things are, but you know, the ball can come to a place from
two different paths or an potentially large number of
paths. Right, right. And I think, given given there are some
some differences, you know, the idea was that if the pegs or
no, it's it's pretty close to exactly what it is. Yeah, and
what we were thinking is that if the pegs on the Galton board
are precisely and symmetrically arranged, you know, the beads
will form a nice binomial curve at the bottom. And it seems
like what G flow nets are capable of doing when they
optimize the pathways. They're tweaking the pegs a little bit
to the left, or a little to the right, to bias the flow of
beads one way or the other. And in this way, a G flow net
could arrange the pegs so that the beads could form any
distribution at the bottom that we want. And for our purposes,
that means the distribution that matches the reward function. So
is this a good way to think about G flow nets? Yes, it is. Now,
it's missing a really important aspect of it, which would be
difficult to send visually, but that all of these peg weights,
like the polytip that are boggles left or right, are not just
like learned independently, like as a tabular machine
earning, but that there's like one neural net that knows about
the locations in this big board as input and tells, you know, how
much relative weight should I, you know, go to go left or right
at this position. So the reason this is important is because it
allows for generalization. Because this board is huge, it's
exponentially large. So there's no way you're going to learn, like
a separate parameter for each of these choices. And so you have
this neural net or potentially several neural nets, but that
share allow you to share statistical strength, as we
call it, share information across all the possible positions, so
that you can generalize to places paths that it has never
seen from a finite number of training trajectories that it
sees while it's being trained. And that's crucial. Otherwise, you
couldn't scale to large problems, which is really what we
want to do.
Professor Benjo, we spoke with Professor Carl Friston about his
free energy principle, an active inference, which is pretty much
a Bayesian flavored version of reinforcement learning. And he
said that while we need to maintain entropy and stop
models from increasing too much in complexity, we should balance
entropy with accuracy in a principled way. And by the way,
you can kind of think of them in just the audience think of
entropy as keeping your options open. But Friston thinks that
the Bellman-esque idea of reinforcement learning, which is
to say maximizing expected reward is the objective is
misguided. And we should instead perform inference over future
paths, balancing expected reward of relative entropy. Is there a
connection between these ideas? I mean, it seems like G flow
nets are sampling paths proportional to the reward
function, that will maintain as much entropy as the reward
function itself.
Yes, yes, exactly. It's a translation of the reward
function into machinery that can sample, you know, the equivalent,
the corresponding distribution. So yeah, I completely agree with
what Carl was saying here. But as I said, what's interesting is,
we can do things with G flow nets. In principle, we've done the
math and some small scale experiments that we have now a
number of papers, we can do things that go beyond sampling.
But for example, estimate entropy itself. So entropy is
notoriously difficult to estimate. And I mentioned in my talks on
G flow nets that we can use the G flow net machinery to estimate
entropy of say, an action distribution or a distribution
over Bayesian parameters, for example, which is would be
something you'd like to minimize if you're going to take an
action in the world. And you have a model of the world that has
uncertainty. And that connects with Carl, for instance,
interest, you'd like to be able to choose an action that
minimizes your uncertainty about how the world works, we know
what are the latent things that may have happened. And good, you
know, an important part of that is estimating the reward for
the these exploratory actions, like, you know, children playing
around is how much reduction in entropy of my knowledge of the
world, I'm going to get through that action. So you need to be
able to compute that reward. That reward word is basically an
entropy over something you care about. And it turns out you can
also do that with G flow nets.
We're actually speaking with Friston again next week, do you
have a question that you would like us to put to him?
Well, he, you know, he's on the biology side of things much
more than I am. And I believe there are amazing scientific
opportunities to explore how the kind of machinery that G
flow nets offer could be used by brains in order to do some of
the things they do. Using your nets to model the probabilistic
structure of the world, including uncertainty, which is
something he cares about. But but also taking into consideration
things like high level cognition, the global workspace theory,
which is something I care a lot about, attention, they all kind
of fit in the picture of G flow nets. So so I think there's a
huge potential of research at the synergy of computational and
theoretical neuroscience, and machine learning, probabilistic
modeling of the kind that G flow nets propose to come up with a
some proposals for explanatory theories about what the brain
does, that's probabilistic. And, you know, I think he would be a
great person to be part of that.
Fascinating. Well, going a little bit further down that line,
there are folks in the community who are huge advocates of
biologically inspired approaches to machine
intelligence. And, you know, one of the key ideas actually is
diversity, discovery and preservation, both in how
knowledge is acquired and represented. I mean, specifically
evolutionary algorithm advocates, they differentiate
themselves from gradient based single agent monolithic
approaches like reinforcement learning. And they point out that
their approaches overcome so called deception and search
problems, you know, which is to say they don't get stuck in
local minima, your approach seems to be achieving something very
similar in the context of a gradient based reinforcement
learning package. I mean, I don't see it as being mutually
exclusive. But what's your take on this?
Yeah, diversity is important when you're exploring and humans,
especially young ones are exploration machines, they're
trying to understand how the world works and they're acting in
the world in order to get that information. Yeah, I agree that
that search process needs to have a big bonus on on diversity,
like on trying different ways of achieving something good, like
better understanding how the world works. So it turns out that
in the G flow net framework, you, you have a training objective
that yields this kind of diversity and exploration, but is
based on training large neural nets end to end. Now, it's a bit
different from the usual end to end training, because we don't
have an objective that objective we're trying to optimize
is not tractable, actually. But we can sample these trajectories,
which I think of like sampling thoughts, like our thought
process is going through some chain of explanations, not a
complete, and it doesn't represent all the explanations,
but but what we found with our training objectives for G
flow nets is that these sort of random randomized kind of views
of the world are sufficient to give a training signal to the
neural nets that do the real job.
I'm curious. So this trade off between exploration versus
exploitation. And this has come up in so many contexts, you know,
throughout our show. And one in particular, as we talked to, you
know, we've talked to multi arm banded folks, right? And G flow
net seemed to capture this balance between exploration
and exploitation. But the multi arm banded folks, you know, they
dive deep in that research circle into this into this trade off.
And I think they have some very principled ways and even very
rigorous ways to analyze this fundamental trade off. To what
extent do you think that that their research maybe could be
applied to future G flow net variations? Like do you think
maybe it might open up more options to fine tune the
trade off between exploration and exploitation?
Yeah, I mean, the banded research is very, very closely
related to the G flow net thread. But G flow nets, as we have
been using them, for example, for drug discovery, they are
banded. It's just that the action space is not, you know, one
out of n things. It's, it's combinatorial because you build
these pieces. So the action space is not something you can
enumerate. So you can't apply the typical banded algorithms, but
a lot of the math is totally applicable. And in fact, what we
use in the drug discovery setting is UCB upper confidence
bound objective to learn a good exploration policy. So that comes
out of the banded research. It what it does is it, you know, it
combines the risk and reward expected reward, one of these
together in a way that in theory guarantees that you will do an
efficient exploration and find that where is the, you know,
where's the money? Where's the reward, right? All of the
possible places where you can get the reward.
So in, in, in the G flow net papers, you often describe it as, you
know, we want to sample not only the maximum reward path, in
order to have more diversity in order to maybe figure out
something that we didn't know if we were just to go to the
maximum reward. And that speaks a little bit to the, like the
things that we know that we don't know, right? We maybe know
that, right, this seems like a lower reward trajectory might
turn out to be a higher reward trajectory. However, exploration
and reinforcement learning is also fundamentally addressing the
things about the things that I don't know that I don't know,
which is where stuff like random exploration and things like
this comes in. Could you maybe comment a little bit on how you
see sort of, because it seems to me that if I managed to sample
according to what I think is the reward distribution, right, I
still have this problem of maybe there is a deceptive rewards
there are, you know, I need to take a step back, I may not know
some sort of some, some area of the search space. And don't I
just run into the same problems again?
So, so the important trick here is you need your model of the
reward distribution, or the reward function to be one that
captures uncertainty, like, maybe in a Bayesian way, or, you
know, whichever way, the Bayesian way, by the way, fits well
with the G flow net framework, because we can consider the
parameters of the reward function as latent variables, like
you don't actually know the reward function, you're trying
to figure it out from experiments. So the G flow
net can sample, and not just like what you should be doing in
order to acquire information, but also potential reward function.
So, you know, we don't actually have a knowledge of how the, you
know, what's going to be the rewards we're going to get in
the world. Classical IRL is going, as you said, to the expected
value and try to maximize that, whereas the G flow net approach
is trying to acquire as much knowledge as possible about the
underlying reward function. So you're trying to minimize the
uncertainty. So your model with the G flow net is modeling the
uncertainty, and then it can use it as a reward for the policy
that is going to do action in the real world. So we're talking
about different G flow nets. There's a G flow net that models
the uncertainty in the reward that you're going to get from the
real world. And that's like a Bayesian model. And then you have
another G flow net that controls the policy that searches to
and its reward is how much uncertainty reduction you're
going to get by doing this or that. So, so yeah, you need to
have a part of your model that is kind of aware of the fact
that there are whole areas in the world that you don't know
about or aspects of the world that you don't know about so
that you can drive the exploration.
I would love to know where some of the magic is coming from.
The promise of G flow nets is that we can discover as many
modes as possible in the path distribution. Traditionally in
Markov chain Monte Carlo, we had to hack priors into the
algorithm by hand, you know, to find new modes or areas of
information efficiently, especially when they were very
far apart or not very sharp. The hypothesis of G flow nets is
that the structure of these modes is learnable on many
problems, even in high dimensions. It's a little bit like
saying we're getting a free lunch. I mean, actually, I think
you used that exact phrase to describe what we're doing here.
Many research avenues have tried to develop general methods to
discover these structures and have failed. How do you think
G flow nets will overcome this seemingly intractable curse?
There is no guarantee that they will, because if there is no
structure in the underlying function you're trying to
discover. So let's say the reward function or the energy
function that you care about, then having visited some finite
number of modes like regions where your reward is high site is
not going to tell you anything about what are the other good
places, the other modes. So so there's no guarantee that it
will work. But if if there is structure, then there is a free
lunch. And we know machine learning is good at that. Like
the last 10 years of deep learning and its success. What is it
telling us? It's telling us that you can generalize right that
these nets, I'm not saying they generalize perfectly, but they
can generalize. So you can think of it like the machine
learning problem is given some examples of good things, like,
you know, places where you get reward, you can you generalize
to other places. And the supervised learning way of
thinking about it is, you know, given a candidate place, tell me
how much reward I think I would get. The G for that sampler is
learning the inverse function is like to sample, but it's kind of
the same thing. It's just going in the other direction. Give me
some, you know, sample some some good places that that you know,
where the reward is high. So we now have a lot of experience in
designing powerful your nets that can be leveraged to
generalize in those spaces where we normally use MCMC. And if
there is kind of regularities that allow to generalize, then all
of that can be, you know, put to use.
We mentioned earlier, reinforcement learning often
being applied in a context where you have this kind of solid
reward function. So let's say games, you know, playing chess.
I'm really curious, what would happen hypothetically, if we
applied G flow net, you know, to something like chess. So I mean,
I think given the fact that reinforcement learning like say
alpha zero is trained specifically to choose the best
move rather than diverse moves, it seems obvious that maybe if
given equal resources to both alpha zero and flow zero, alpha
zero would probably beat flow zero. However, I think if flow
zero were given more resources, say and trained to the same
rating, say the same elo rating as alpha zero, it seems like
flow zero, if you would, would play significantly more
diverse and interesting games with a wider variety of styles.
And I think you could even imagine also that it could be
possible, even if given equal resources, but sufficiently
high enough resources, that a hypothetical flow zero would
consistently reach higher ratings, because it might find, you
know, more interesting stepping stones that have the
potential to avoid deception, because it can explore seemingly
lower reward paths that ultimately develop into higher reward.
More curious if you have any thoughts on that?
Yeah. It's a good question. I would say where the kind of
approach we've been pioneering with G flow nets might be really
paying off is if you think about it from the perspective of the
learner has a finite computational, you know, amount of
resources, because in principle, right, if you had infinite
compute, and you know the reward function, like the rules of
chess or go, then you can just crank and find, you know, the
policy that's best in every possible setting. Now, if you
have finite resources, like, you know, you, you have a budget
of compute, you'd like to use it efficiently. And so that's
where the exploration exploitation trade off becomes
important. And if you if you had a, say, a current policy that
you're not completely sure is the right one. And, and then
you're trying to say, Well, what, how should I play so that I'm
going to improve my policy the most as in I'm going to reduce
the uncertainty that, you know, it is the right policy, like
that it picks the right things. So now we're getting closer to
the kind of setting where it makes sense to use G flow nets.
And then what I would expect, if we do the engineering work
here, but based on the sort of much simpler problems we've
looked at, is that it would converge faster. So given, if you
look at on the x axis, the number of games you're playing. And
on the y axis, how good is your policy measured like on other
games. So that's where you would get. In other words, it's the
learning curve that you might gain on asymptotically, everything
is going to converge the optimal chess player, right? So the
the place where it's interesting is to look at the learning
curve how fast you learn. And here you want to sort of active
learning thinking like, Well, I'm not just trying to win here.
I'm trying to gather information so that I'll win more in the
future. And it's a different objective. And that's where you
need diversity and exploration and like a model of your own
uncertainty and an active learning policy.
How much do you think this could be part of not maybe only
reward maximization things, but information collection, things
like, I'm sure you're you're thinking about in, let's say the
brain, there is there's sort of maybe a similar process going
on and what do I still need to retrieve in order to give certain
answers to questions, or maybe in our, let's say, big search
engine, let's just name one for naming sake, let's Google, or
so would would try to answer your query, not by just searching
through their index, but by actively doing this multiple
multiple things like, is this enough? Is this enough? Is this
enough? Do you see connections to these types of things? Or are
they inherently different? Because they might be not learning
on the spot?
What they're doing on the spot is acquiring information. And you
want to do it in an efficient way. And that's where sort of the
active learning thinking comes in.
And I think it's actually a very big, practical problem in
the deployment of like AI dialogue systems that are not
chit chat, but they're trying to say help a user achieve, you
know, get something get information or something like
this. This is this is a huge need for this in, you know, the
business world and search engines, and you know, it's much
more than search engines. So I don't think we have the
algorithms that do that right now. And it's kind of painful. The
human has to know, you know, is driving. But if, if we had
systems that could explicitly model their own, say,
uncertainty about what the user needs or wants, or where to
find information. And then, and you need like pretty powerful
models of that, like it's not just galaxies, they're simple
things. That's where G flow net strengths comes in, you can
represent very, very complex distributions over
compositional objects. It's not just a few numbers. And then I
think you could get to much more efficient human machine
interfaces. And the same, I believe the same methodology
could be used more generally in scientific discovery. So what
is scientific discovery? Like what is it that scientists do?
They plan experiments that are going to allow them to reduce
the uncertainty on their theories of, you know, some
aspect of the world. It's the same problem. Yeah, you have a
series of questions you're allowed to ask to nature. And you
try to ask as few questions as possible to as quickly as
possible, understand what's going on.
Is there a connection fundamentally to I'm thinking of
causality, which also I've seen a number of papers that you've
collaborated on with people who are who are deep into
causality research and so on. What do you think there is a
a connection there where an agent could learn to uncover if
you think about scientific discovery to uncover the
fundamental causal structure of the world by asking such
questions, like could there be a connection to that branch of
research? And could this finally be like the unification of
of something machine learning and the the world of causality?
Yes, you guys are really asking all the right questions. Thank
you so much. In fact, one of my main motivations for the
pursuing the the G flow net research program is that I think
it's the it's an ideal tool for implementing what I called in
my talks, system to inductive biases. So what this means is
there are lots of things we know from neuroscience and
cognitive science about how we think. And we can bring that
into the design of probabilistic machine learning, you know,
based on deep learning is the building blocks. And one of the
inductive biases, like one of the characteristics of how we
think is we think causally, we're constantly asking the why
questions we're trying to find explanations and so on. And, and
and that connects with classical AI, like the way we think, to
some extent, has also inspired classical AI, you know, rules
and logic and and reasoning. And we haven't yet found the way
to integrate these abilities in deep learning. And of course,
lots of people are like, trying to and and that's important.
But but I think the reason why G for nets give us an amazing
handle on this is because they they're really good at
representing distributions and sampling over graphs. And, and
like a reasoning or a set of possible reasoning to explain
something or to, you know, for planning. The these are graphs.
And your thoughts can be seen as graphs, right? So think of like,
maybe a simple version of this, think of a parse, like a
semantic and syntactic parse of a sentence is a graph. But
usually it's, you know, it's more than a tree, there are all
sorts of semantic connections, including with knowledge graphs,
right, which also graphs. So the ability to implicitly represent
those distributions and sample pieces of them as thoughts is, I
think, fundamental to how we think. And going back to
causality, one of the hard questions that I think G for
nets can help us with is causal discovery. So in other words,
what is the underlying cause structure of the world, including
the uncertainty about it? Given the things we observe, a lot of
the research and causality has been okay, we observe these,
these random variables, discover, you know, make inferences
about, you know, whether what we can say about whether it goes
to be and so on. But it's much harder to discover the causal
graph that that, you know, in a large set of variables, and
it's even harder. And really, nobody's done a real job there.
To do this when what the learner sees is not the causal
variables, but just like low level pixels. And you also have
to figure out what are the causal variables and how they're
related causal. And I think G planets can help us do that.
This this opens up, this is so many avenues of questions, I
think it'll probably almost be a future episode in itself. But
let me just ask you about some of the basic ones, which is, as
you mentioned, kind of learning the causality causality
structure, much more difficult problem. And the first question
is just how to represent the causality. And so you, you, you
mentioned graphs, you know, graphs is one way. And of course,
you can develop, you know, isomorphic ways of representing
certain parts of logic as graphs, etc, depending on how, you
know, how rich you make the graph structure. But there's
also the other issue of, you know, when you're trying to
build, and I think it's probably correct to call this a world
model, right, like we're trying to build a causal
that's the word I use. Okay, great. And I, and so I have one
quick question about that, which is, you know, to me, to some
people, world model is only the discriminative function. It's
just that, you know, probability y given x, to me, it's more
general. It's also the structure of x. Is that, is that also
your, your view as well? Yes. Yes. Okay. And so in
constructing those, those world models, some of the, let's say
the pushback on on these type of generative techniques from, from
folks that are more skew more towards the discriminative side
is, hey, look, fine, you're going to go and try and build this
generative model, it's going to be even more complicated than
this discriminative model, because it also has to learn, you
know, the structure on x. But I think the possible free lunch
here, is that you can learn abstract structure on on x. And
so if you learn these abstract world models, throwing away all
the nitty gritty that doesn't really matter, you can potentially
have very powerful, you know, predictive encoding, if you will,
like, what's, what's your thoughts on that?
Oh, that's what I've been thinking for almost 20 years. And
one of the reasons why I've been interested in deep learning as
a way to think of discovering abstract representations, you
know, from the early days of deep learning, as in like mid like
2005 or something. And, and in the paper that Jan McCarr and I
wrote about, and also other papers I wrote with some of my
colleagues at the University of Montreal on, you know, deep
learning around 2010, they are all about that notion that we
would like these unsupervised learning procedures to discover
these abstract factors, as we call them. But now I think it's
not just the factors like the variables, but it's also more
importantly, even how they're related to each other, which in
the causal language is what we call causal mechanisms. And so
here's a fundamental way of thinking about this. If you
don't introduce the abstract kind of structure that exists in
the world, then representing p of x, the input distribution is
very difficult. It's, in other words, you'll need a lot of data
to learn it. And it's not going to be generalizing very well. The
whole point of abstraction is that it gives you very powerful
abilities to generalize to new settings, including out of
distribution, which is one of the hardest topics in machine
learning right now. How do we extend what we do so that it
generalizes well in new settings? And thinking causally
about these abstract causal dependencies, as the things that
are preserved across changes in distribution, like, if I go to
the moon, it's the same laws of physics, but the distribution is
very different. How do I generalize, you know, across such
changes in distribution? It's because the learner is us, you
know, if we, if we were, if we had the right education, has
figured out the underlying, at least, you know, enough of the
underlying causal mechanisms, that we can be transported in a
different world, but where there's the same laws of physics,
and we can predict what's going to happen, even though it looks
completely different from, you know, our training environment.
So the, the idea of extraction is really that if you introduce
abstractions, the description length of the data becomes way
smaller. And that's why you get generalization.
Absolutely.
I'm fascinated by these abstract categories. I think it's the
most exciting thing in AI. I mean, Douglas Hofstadter spoke
about cognitive categories, like the concept of sour grapes,
for example, to represent the certain thing. And almost
magically, our brain seems to arrange these cognitive
categories. And it's not entirely clear to me whether they're
an emergent phenomenon, or whether it's some other process.
But the modes that you're discovering in G flow nets,
they're a kind of category, these cognitive categories that I
just spoke about our abstractions, also things like
causality and geometric deep learning that they are kinds of
categories. But I've always had this intuition that deep
learning doesn't learn the categories on its own, it needs
humans to kind of put priors into the model, as we do with
geometric deep learning. Do you think that that will always be
the case? Or can we have that meta level of learning?
Yes. What I really want to do is build machines that can
discover their own semantic categories, abstract ones that
really help them understand the world. And of course, they're
going to learn, you know, better and faster if we help them just
like, you know, we teach kids, we don't let them discover the
world by themselves. But we do have an ability to invent new
categories. That's what scientists do all the time,
right? Or artists and, you know, writers and philosophers and
scholars, and ordinary people who find new solutions to
problems, we do that all the time, our brain is a machine
discovers new abstractions. Of course, that usually it's just
like one little bit on top of all the things we got from our
cultural input. But but that's the ability that we don't have
right now in machine learning. And that is going to, I think, be
a huge advantage. So now we're not in reinforcement learning,
we're not in active learning, we're talking about unsupervised
learning. So we're talking about how can a machine discover
these often discrete concepts that somehow help it understand.
So in other words, build a compact understanding of lots of
things that generalize across many settings. And yeah, that
that's that the path to build that is, is becoming more and more
firm in my mind, as I move forward with G flow nets. So as a
clue, there was a paper we had recently, I think in Europe's
on that's connected to the global workspace theory that says
that it's about discrete valued neural communication, I think
is a title where the one interesting intuition here is
connected to this is if you if you constrain the communication
between different modules, say in the brain or in machine
learning system, to use as few bits as possible and discrete is
the way to get the very few bits. You can get better
generalization. And there are good reasons for that that we
try to explain in the paper. But but that's, that's it, you
know, there's a clue here that discrete concepts emerge as a
way to get better generalization.
You you mentioned before, and in terms of discreteness, and
what you mentioned before with graphs being very fundamental, it
connects a little bit back to a paper that you, I think,
provocatively titled the consciousness prior, where where
you connect sort of the ideas of attention, sparse factor,
graphs, language, things being discreet, things being
describable by language, right? And, and I find that all to be
very interesting. On the topic of consciousness, we would be, it
would not be appropriate for us to not put this question to you.
So you're not, you're not very active on Twitter, which is
probably why you're so productive. But if currently,
there is a bit of a of a thing happening on Twitter, namely,
Ilya Satskever of Open AI has tweeted out a seemingly innocuous
tweet saying, it may be that today's large neural networks are
slightly conscious, which has resulted in quite a, let's say,
a storm on of people agreeing, disagreeing. Obviously, he's
he's talking about maybe, you know, the large language models
we have today, which do incorporate a lot of the things
you talk about, they do incorporate attention mechanisms,
lots of them. Presumably, it's all one needs. They do
incorporate language, they do incorporate discrete things with
you know, discrete tokens and so on. What do you make of a
statement like this? It may be that today's large neural
networks are slightly conscious.
Well, this one fundamental problem with such statements,
which is we don't know what consciousness really is. So I
think we have to have a bit of humility here. And I can't say
what Ilya is saying is true or not. I think that this is more to
consciousness than what we have in these large language models by
a big gap. But that being said, and you know, we do need to work
with our colleagues in your science and kind of science who
are trying to figure out what consciousness is from a scientific
perspective and philosophers who are helping also to make sense of
that landscape. So we have to be careful with the use of those
words. And you know, I was a bit liberal in the title of my
paper. And I learned a lot about consciousness since then,
learned that there's a lot that we don't understand that at the
same time, there are enough bits that we know from from
cognitive neuroscience that can serve as inspiration for how we
could build machine learning systems that have similar, say
conscious processing machinery. Okay, let's not say consciousness
but just conscious processing machine because that's less
controversial. And by the way, the word consciousness has been
taboo with most of science for a long time. And it has become
untapped, you know, the tabooed in neuroscience, because we're
starting to be able to make measurements of what's going on
inside your brain, while you're doing things consciously or not
and so on and distinguish the parts that you're consciously
aware of and the parts that are there in your brain, but you're
not conscious. So we're trying to we're starting to make a lot of
progress of what it means to be conscious of something or not.
And I, you know, I think this is a very exciting and important
scientific question. And I would rather like work on exploring
hypotheses and theories to explain our conscious abilities,
rather than make bold statements about whether current neural
nets are conscious or not.
Professor Benjo, we've got some David Chalmers on the show next
month. Do you have any questions that you had put to him?
I very much like a hypothesis about consciousness that Michael
Graziano has put out to help explain the qualia, the subjective
experience part that Chalmers wrote might be something science
can't really, you know, touch. And so what's, you know, I'd like
to hear what he has to say about these kinds of approaches. And
one of the basic premise here is is very grounded in things we
can do scientifically. It's to say, well, let's not try to
figure out what is consciousness or subjective
experience more specifically, you know, from a philosopher's
chairs. But let's let's consider that as a phenomenon that is
happening in the brain. I mean, unless you believe in sort of
supernatural things, if it is happening, something is happening
in the brain, and we can report about it. And we can, we can
like, measure what's going on in various parts of your brain
while this is happening. And then, you know, can we then come up
with theories that explain why we feel that we have subjective
experience? It's not saying whether consciousness exists or
not or subjectivity. It's not whether it exists or not in some
sort of logical sense. It's whether, you know, what is it
that's going down in our brain that gives us that feeling and
then make us say, Well, I am, you know, I'm conscious of x, y,
or z. So so that's the that's the direction I find interesting
because it opens the door for a scientific investigation. And
Michael Grosjean has a specific theory about that which I find
compelling that is really rooted in the idea that we have a world
model. And then we we because we have an attention that focuses
only parts of it at a time. And we need to have like a little
mini world model that controls that attention. That creates a
sort of separation between the the where the real knowledge is
and sort of this more abstract control and machinery that could
well, give us this illusion of Cartesian dualism, which I think
is an illusion, but but you know, must be grounded in some
you know, biological
reality. And that's I think understanding that is is a very
good question to ask. And I'd like to get to know what he
thinks about such a research program.
Thank you very cool.
Yeah, thank you. I do have one kind of nitty gritty question
because and partly partly based on some of your recent work on
becoming more of a fan of semi supervised learning. And you
know, you had a recent paper that was on interpolation
consistency training. And what I found interesting about that is
that if we consider one of the biggest challenges that we face
in machine learning pretty much across the board is an
overcoming the various, you know, curses, if you will, the
various forms of intractability that we have an empirical
learning methods. And in this context of semi supervised
learning, that recent paper, it found significant improvements
over state of the art by forcing linearity. So in this case, it
was by this mix up between the unlabeled samples and their
interpolated fake labels. And in the last decade, we've also
seen values come to dominance in the field of neural networks,
their piecewise linear recent work by Randall Belastriero,
developed an interesting frame of reference which cast
multi layer perceptrons as a decomposition method, which
produces a honeycomb of linear cells in the ambient space and
they're activated turned off or on by input examples. So my
question is, why is linearity, whether it's piecewise or
otherwise, dominating the state of the art in approximation
methods, it almost seems to me like we've kind of gone back to
the future, if you will, sort of leaving behind attempts at more
smooth nonlinear methods and gone back to newer, albeit more
complicated forms of linear approximation.
Right. I would say something that's roughly linear is
simpler. So having a regularizer that says, oh, you want to be
roughly linear or locally linear, at least to as much
extent as you can is a smoothness prior. So that's going to
help generalization. But it could also hurt if that is too
strong. And so having these piecewise linear kind of more
type of solution is a good compromise. It says as few pieces
as possible, and ideally organized in a compositional way. So
that it's not just like a relu, it's more like the discrete
abstract logic, you know, reasoning, things sitting on
top, that's controlling the pieces. But but otherwise fairly
simple in each how each of the pieces are, you know, like
linear, for example. So one way to look at this is, if you
look at classical, the kind of rules that classical AI
researchers were using, each rule is fairly simple. It's, you
know, like, it's almost linear, or it's very simple logic. But
it's the composition of all those rules that gives the power of
expression of these systems. Of course, the problem then is that
they didn't know how to train them properly. But yeah, I think
we, I think we learn to come up with these discrete ways of
breaking up things into simpler pieces. And that in fact, I
think if you're Bayesian about it, it just comes out naturally.
And they're very, very weak assumptions.
So in a way, it's it's almost, it is piecewise abstraction. So
we're kind of back. Yes, that's what I would lean to, rather
than piecewise linear. But linear, of course, is a broad part
of, you know, it's an easy way to get simple.
Amazing. Professor Benjo, I'm interested in your personal
journey. So we've been talking about diverse trajectories. And
I wanted to know about your own trajectory of research over the
last 10 years. Now, one of my mates, a psychologist and
symbolist, Professor Gary Marcus, presumably one of your best
friends, by the way, he pointed out in his 2012 New Yorker
article that MLPs lacked ways of representing causal
relationships such as between diseases and their symptoms. And
I think this has been a significant focus of yours in
recent years as we've discussed. And he thought at the time that
you were a bit too quote system one all the way. And he spoke
then about the need for heterogeneous architectures and
the acquisition of abstract concepts, compositionality and
extrapolation, which I think has also been a huge focus of yours
in the last decade or so. We really enjoyed watching your
debate with Marcus. And by the way, we would love to host V2 of
that debate. So if you're interested, you just let us
know we'll do that. But he's often viewed as a heretic. And,
you know, just forgetting about symbols versus neural networks
for a minute. Am I right in thinking that you've converged in
at least some ways in your thinking? And how would you
characterize that from your perspective?
So, yeah, I used to be in the 90s, a, you know, pure neural net
subsymbolic connectionists researcher. And I did my grad
studies at a time on neural nets at a time when the dominant way
of thinking was these, you know, classical AI rule based system
with no learning at all, and was dominant, meaning that the
little group like, you know, Jan and Jeff and I and others who
were thinking otherwise, had to, you know, defend our views. And
and maybe that led to a kind of, you know, us versus them, I
think, unhealthy way of thinking. And of course, I matured. And
one of the big, so there, I think there are several turning
points on that journey. Well, one of them in the in the 2000s
was the realization of the importance of abstraction. So
and the way to think about this maybe more concretely, because
what does it mean to be abstract? Is that I was thinking,
well, what would be the right kind of representation we want to
have at the top level of our unsupervised deep nets, because
we were doing mostly like unsupervised deep nest, like,
you know, deep boz machines and stuff in that decade. And I was
thinking, well, it would be things like words, right, things
like the sort of concepts that we manipulate at the top level,
well, it's words or, you know, the equivalent, maybe, with
disambiguated. But yeah, we, it didn't seem that we have the
right tools for that. And then it remained like an objective. And
then in 2014, we discovered the power of attention. And that's
closely connected to abstraction, because what it does is
it focuses on a few things. And of course, that's our, you
know, that's very much a characteristic of how we think a
thought has very few elements in it. That means we have selected
those elements. And that's where attention comes in. So it's
getting closer to this ideal of building machines that think like
humans. And then of course, in 2017, I wrote this consciousness
prior paper where, you know, I discovered all the work on global
workspace theory and, and it, you know, and the momentum is
built up. And of course, now, you know, humans think and they
use symbols, and they understand the very abstract
relationships between them. And we need to build neural nets that
can do that. So I guess where I've maybe departed from Gary,
but maybe he's moved to is, it's going to be neural nets that
do it, right? It's just that we're going to be training them in
a special way. And that's what G flow nets really aiming at.
So can I just say we, we asked many guests, these, these
questions about their, their evolution. And sometimes they,
they tend to be spicier than others. But I have to say, from my
perspective, your answer was the most informative, the most
gracious and the most noble of answers that we've heard so far
to similar questions. So kudos to you. That was awesome.
Thanks.
I just cannot believe it. And we always do a hell of a lot of
preparation. But it's gone to the point now where we know that
we're not going to get more than about six questions in. So we,
you know, we kind of like exponentially, you know, have an
exponential prior on our questions.
Well, he was awesome, though, with like, you know, we asked him
to give relatively sort of three minute answers. And he stuck to
that, which was really cool. I mean, that's, that's very
helpful to have an interesting dialogue. And I, I can't believe
how proud I am, you know, that he's, that he appreciates that we
put the prep time into it. And, you know, had had decent
questions that were hopefully interesting for him, as well as
our, as well as our audience. So Dr. Kilcher, lightspeed
Kilcher, what should I take?
It's cool is, I mean, his, um, yeah, I think is the thing he
mentioned at the end, like his humility, it kind of shines
through everything he does. And he answers, he's like, you know,
here's the best answer I can give. But, you know, he seems to
be very, like, open and not, not, not very, yeah, one notices
he's not on Twitter. It's like, it's noticed that was a
brilliant question. I think we should post that question on our
Twitter. Because, you know, that there's that a bit for people
watching this in a year's time, it's probably forgotten about
but yeah, that ilia guy from open AI said that the models might
be slightly conscious. I was exasperated by that. Because I
watched his interview on Lex. And I know by saying bad things
about him, he will never come on our podcast, but I don't think
he would have done anyway. So it doesn't matter. But yeah, I
think that it's pretty bad.
What? Why? Yeah, why? It's like, it's like, you don't think
it's bad? No, he says, I think, because a lot of the folks at
Open AI, they are, you know, like in the rationalist community,
and they seriously believe that we're an imminent threat of the AI
taking over the world and us being paper clips. And I think
it's next, I listened to his interview on Lex, and he sounded
like a salesman, talking about Codex and how it was going to
revolutionize everything. And I honestly think that there's just
such a divergence between what they're saying and reality right
now.
Well, not to drift too far away from from our guests today.
But so I thought, I thought it was just kind of a shower
thought, you know, like, you know, the the large neural
networks of today might be a little bit conscious, right?
And, and, and you just like, yeah, well, yeah, well, shower
thought, and it is a shower thought like it needs on
Twitter, it's just something you tweet out. And, and it brings up
interesting questions, like it brings up interesting questions,
like, you know, you're a you're a ball of neurons, like you're
just a slap together piece of matter, right? You have
consciousness. So clearly, like something about, you know,
learning systems combined with data, or maybe not even
combined with data gives rise to consciousness. So why can't
why can't another, you know, in silico, slap together system
of neurons ingested with data be slightly conscious or have
like, some properties, like, and that's that's essentially, yeah,
Benjo refused to give like a humble, the humble person he is,
he refused to give like, you know, the the the strong take on
that, but that would have because he might just this is my
opinion, not his obviously, but reading the consciousness prior
paper, it is not too far off. He formulates consciousness as
having these elements of, you know, I have my internal state,
which is sort of everything in my brain that I could bring bring
up into my forefront, then I get some input from the outside
world. And through the input, I then filter, like with an
attention mechanism, I do I look what in my mind, could I now
bring into focus, right? And that is by use of something like
an attention mechanism. And then I take that thing. And I put it
into these abstract concepts I use I represent. I represent the
concepts in my head as a sparse factor graph. And by focusing on
parts of that, I can then make inferences in this sparse
factor graph and so on. Now, obviously, something like GPT three
doesn't have all of that, at least not explicitly, but some of
it is there, right? It's, you know, I have a piece of input, I
have giant amount of weights, I use an attention mechanism to
sort of see what I can focus on.
Yeah, but yeah, but I think that I think that's a very
declarative description of consciousness. And at its roots,
it's about the phenomenological experience. Right. And I know
we discussed computationalism and panpsychism. Let's not go down
that rabbit hole. But surely, they don't think that this model can
feel well, but so this is
consciousness is not about feeling. It's about being being
like aware of of like, I don't even know what it is. I'm just
saying that it sounded not too far away from what the
consciousness prior paper was about. And yes, I realize it's
called the consciousness prior and not consciousness. But you
know,
yeah, I mean, I think he answered it the way a scientist
should answer it. And I was really happy with his answer,
which is, okay, a consciousness has to be some activity of
neurons and firings or whatever in the brain or else we're
talking about magic. And that's not in the field of science. And
B, you know, whatever that thing is, it's obviously quite
nuanced and complicated. And we don't have we don't know yet.
So we need to have some humility here, which means we
shouldn't be alarmist. So we don't need to be going in, you
know, burning books tomorrow because because we created a, you
know, GPT, whatever, that anytime its wheel is spinning,
and it's actually suffering. You know, if you ask it a
question that's too hard, and it's spinning, it's because
you're hurting it and it's suffering. And so we need to
turn it off like right away. But wait, we can't turn it off
because then we'd be like, murdering, you know, a sentient
being or something, like we're way, way too, in our infantile
understanding of, you know, this type of complex, complex
behavior, that's the human mind and consciousness to be at that
point. So from my perspective, he answered it completely 100%
scientifically. And there's a lot of folks out there who are
supposed to be scientists that spend a lot of time with, you
know, unscientific, you know, thinking about it. Cool. Let's
talk a little bit. What one of the things that I really found
interesting about Benjo's ideas, other than the causality
stuff and the system to stuff is this notion of diversity.
We've had conversations with Kenneth Stanley all about open
endedness and diversity preservation. We've also had
conversations with Friston about the importance of balancing
relative entropy and so on. And we have all of these curses in
empirical learning, right? The statistical curses, the
approximation curses.
Dimensionality, we have to mention dimensionality and
even, I mean, you know, we're talking about curses in the
Monty, you know, the Markov chain Monte Carlo in the sense of
it being a high dimensional space. And we need to assume that
there's some structure around where these modes are. So all of
these approaches are ways of simultaneously, and, you know,
being able to explore but not being cursed. So yeah, what was
your take on that?
Well, any one of my take was that I like that he's so
interested in abstraction, because, you know, to me, that's
been not only one, you know, it's not only one of the larger
mysteries, at least for me, I mean, I don't know, of the kind of
the universe is abstraction, idealism, you know, platonic
thinking, whatever. I mean, the whole point is just that he
views abstraction as a key to pragmatically useful, you know,
pass forward. And it's a hard problem, a really hard problem.
And, you know, his focus right now is on kind of graph based
structures. And I have to admit, you know, for me to you,
they're quite seductive and appealing. I don't know if
they're the the right path forward, but it's definitely
cool to see a lot of research, looking into graph based, you
know, methods, or, you know, hyper graph based methods,
whatever they are, they seem to definitely be a promising
path forward. And I think we're in for, hopefully, if we can
continue to progress at a reasonable rate, you know, some
some interesting decades ahead.
I mean, I would, I would also postulate that maybe our most of
our, let's say benchmarks that we're thinking about today aren't
necessarily suited to to because his argument was by creating
abstractions, it might actually, you know, help your ability to
learn something, right, which is a thing that we all
intuitively understand in the world, if I have good
abstractions, I can transfer my knowledge from here to here and
from here to here. Yeah, in something like image net
classification, or whatnot, or most of the benchmarks we have
today, the necessity of abstractions is probably not like
the data, the hardness of the problem probably doesn't
require abstractions to be introduced. And therefore, the
limiting factor here might not only be the models themselves,
but also, let's say, our ability to even measure the progress
one could make with abstractions. And I think that's gonna change
maybe in the near future, because people are going into
multimodality research and so on. And there, I think the concept
of sort of concepts, maybe not abstractions, but at least
something like concepts is way more, more important.
Yeah, there's, let me just follow real quickly there, Tim,
because there's something very interesting there to Yannick,
which is the lack of good tools to deal with multimodal, you
know, sets of data results. And a lot of times, we're just
throwing out kind of valuable, valuable sources of data, just
because, you know, we don't have a good tool sets to do with
them, like think about the self driving car, like the whole,
should it be vision versus LiDAR debate? Why? Why isn't it
both? I mean, you know, if you can for $5, you can throw on
some cheap, you know, LiDAR sensors or something, maybe not
something fancy, but something cheap, why wouldn't we take
advantage of that data? And it's, it's really because we don't
have good tools to deal with, with multimodal data.
We got to a good point in the discussion where we were talking
about the nature of finding abstractions. And I wonder where
the neural networks can find abstractions. Now, the, the
cynical view is that humans kind of create these inductive
priors, and they represent the abstraction. So certainly in the
case of geometric deep learning, and that's kind of what's
happening, we put the, the priors in there to reduce the size of
the approximation space. And Keith and I had an interesting
idea yesterday that there's a kind of analogy between geometric
deep learning and causal representation learning. So I
think Keith, you went online and you found a really interesting
definition of a causal model, which is that it's kind of
immune to, let's say, adversarial examples. So what a model
does right now, is it learns a relationship essentially between
let's say every single pixel and something happening, right,
which is why that model is vulnerable.
Yeah, so that was the, and yeah, and I would love to get your
comment on that. But that was, you know, this paper, and I
could go dig it up, and I can get the reference right now where
it said, you know, hey, what is the difference between a causal
or prediction from a causal model versus a prediction from a
non causal model. And the point was that, well, almost by
definition, really, if you have a causal model, then if you
perturbed the inputs, the prediction that you get out of
it remains a valid, a valid output, because after all, if
it's a causal model, and it's reflective of a sort of the
causal structure of the world or whatnot, then sure, that's a
valid, valid output. Whereas, if it's non causal, it has the
potential to learn all these kind of spurious, spurious
structures, and therefore, that's why you get the
capability of these adversarial examples where you just, you
know, put a little rainbow pixel somewhere, and it messes up the
class because it had this spurious connection.
I mean, in the same vein, you could also, the adversarial
examples are there because of inaccuracies, because we don't
have the perfect discriminative function, right? I could also
say, well, if I just had the correct discriminative functions,
it doesn't need to be causal. If I just had like the right
partitioning of my input space, then, you know, I'm super not
vulnerable to adversarial attacks. I guess the real question
would be, would that technically amount to a causal model if I
had, you know, the perfect partitioning of the input
space into my classes? I don't know, that's like, is there like
a mathematical equivalent from that to a causal model? Who
knows?
Right. Yeah, I think there's probably, it's probably, certainly,
if you have the perfect discriminative function, it's
probably the discriminative function that you would derive
from a causal model. I'm not 100% sure you can go go in the
reverse, because I imagine there probably is some some
information loss going from, you know, a causal model to, you
know, like, for example, I'll give you an example. In the cases
of, say, production systems, you know, so, so these little
rewriting rules or whatever, the definition there of a causal
system is one in which all the potential graphs, all the
potential transition graphs that you can get to a particular
output are isomorphic. So even though you have you can have
like the perfect discriminative kind of function, there may be
multiple possible graphs that you could have gotten there, but
they're isomorphic. So I'm not quite sure, you know, how that
would translate into this, this, this point. But I think you'd
be just as good for the purpose of discriminating.
I think it's related to the semantics discussion we're
having in NLP. So people like Walid Saber say that neural
networks don't have semantics. And in the same way, as I was
just saying, blue pixels, I mean, in the real world, let's say
male testosterone levels is causally linked to incidents of
car crashes, which means you can now take the model in in
Holland in a different country. And because it's a causal
factor, it will extrapolate in the same way. But neural
networks models, because what a human does is we would come up
with the right representational abstraction, we would build a
model, which is very reductionist, a neural network models
everything to everything. And the semantics are all one thing.
Well, okay, I don't, I'm not, I'm not too, too keen on
discussing like semantics and whatnot with with NLP people.
But I don't know, you know, like, like, I don't know, it
often it often veers away and veers into semantics. It's like
it's a bit too, you know, I like what what I think Conor Conor
Lay, he said, like, when we talked him along. Oh, he wouldn't
like that. I talked him a long time ago. And I happen to agree
with him there is that you sort of have to see everything from
the perspective of these models. Like if I'm a GPT three, my
entire world is text input, right? And people can't somehow
judge GPT three by, well, you don't even have whatever a
connection to the real world, you don't even know that you don't
go see a doctor if your plant is sick, right? Like, how can you
not know that? Like, okay, they don't live in the real world,
they live in the text world of the internet. And in that world,
I'm not sure if there is not a level of abstraction happening
in these models. Like, it's, it's, it's, um, yeah, I'm, I don't
want to, I don't want to claim that these things do not form
abstract things, it might not be the same abstract classes that
we form, but they definitely form some level of abstraction.
And of course, they can't transfer it because we only give
them the one modality, right? But they may be able to transfer
it between, you know, different areas of text, which they
sometimes do, right? And yeah, so that's, I just wouldn't be so
conclusive with respect to these things.
That that's true. I think we've gone full circle now. So after
speaking with Randall Ballastriro about the splines, that almost
results in such a cynical reading of MLPs that they're just
hash tables, but we're not using MLPs, we're using
transformers and we're using CNNs. And actually, if you think
of abstraction, just as being extrapolation, I think they are
basically synonymous, it's about being able to extrapolate
outside of your training set. Then those inductive priors are
indeed producing abstractions. But the problem is humans
design those inductive priors. What we want is to learn
abstractions. And that's the thing that I don't think is
happening. I'm kind of I'm kind of on the same page as Yannick
and Connor on the one hand, which is, hey, if an abstraction is
just a compression, you know, encoding of the input space, then
of course, they're learning abstractions, right? I mean,
they are throwing away, you know, information and retaining
some, some abstract thing. I think, but I think that just kind
of devolves into somewhat like, you know, bastardization, if you
will, of what people mean when they say abstractions, because
the types of abstractions that traditionally we think about as
abstractions are simplifications. You know, they're, they're
like, simplifications of more general longer range kind of
structures. Whereas we know, and I think we all know this for
sure, that a lot of the quote unquote abstractions that did a
neural network learns are these kind of like shortcuts, right?
They're like these low level borderline spurious kinds of
abstractions. And that's why they break so easy. That's why
they're so brittle. And I mean, there is this vagueness here,
right? Like when is an abstraction, a good abstraction,
I don't know. But I think it all kind of in a way misses the
point. Like, what we're talking about here is that, and this is a
lot of what Benjio said, right, which is that the goal here is
to figure out how to get machine learning to learn
structures that by virtue of their simplification, their simple
abstractions are more generalizable out of distribution.
Right, like that's, that's really kind of the goal here. And I
mean, so the rest of it is just semantics, pun intended. I mean,
the
if you look across the world, a lot of, let's say, cultures and
humans and so on must have the same abstractions, right? So it
must mean a little bit that it's not just something you learn
during your lifetime, right? So, right? Oh, absolutely.
Not correct. It's it's learned by the it's learned by evolution,
by the species, by, by life itself.
Exactly, right. But but is like the, the analogy to us building
in the correct ones as a shortcut for just evolution doing it
using essentially random search, right? That is, it might, right,
it's, it's a different, it's a different quality of we want
machines to learn something. Because usually we think of when
we say we want machines to learn something is we want him to
ingest data akin to maybe what a human does during its lifetime.
But the when you know, these sort of abstractions and the
ability to form abstractions, they seem to be happening on a
more fundamental shared level.
Yes, you just put the pin in the center of the bullseye. I think
that's exactly right. You know, there's a lot to be said for
me. It's an epiphenomenon. And a lot of intelligence is
embodied. And I agree that there's an awful lot of stuff going
on and unbeknown to us with this clearly something that most
people don't have a grasp on. Maybe this is why at the
population level, maybe this is why I'm frequently miscommunicating
with people because I never assumed that learning was about,
you know, what a human being learns and a human being's
lifetime. Like it's, to me, it's always been the evolution,
you know, paradigm, it's like what's encoded in your neurons,
what's encoded in your DNA, you know, what was learned by
bacteria a long time ago, and how did that translate into what
human beings are doing. So I don't know why, like, why so
many people are focused on what a human being learns in their
lifetime. I mean, it's more, you know, why is that the goal?
I'm not sure.
I know, but we run the risk of being very reductionist because
Connolly, he said that it's an open question where the humans
are even intelligent. And if you go down that line, very
quickly, you start saying, oh, human beings are just hash
tables like GBT three, clearly humans are intelligent in some
way.
Well, you can just take it as a, you know, matter of
definition, but it's not a binary thing. Like again, why are
we always into this black and white concept, something is or
is not intelligent? Like that's not how I view things. I think
there's a spectrum of intelligence from like zero to, I
don't know, maybe infinity or something, some really large
number beyond what what human beings are. And so it's this
continuum. So that's why I like chelets kind of on the measure
of intelligence, because even though it doesn't actually give
us a, you know, quantitative way yet to measure intelligence, it
at least is thinking along the right directions, which is how
do you measure intelligence? And how do you define it as a
category of activity? And then we can kind of get past this
black and white, you know, thinking.
Well, gentlemen,
always a pleasure. Absolutely. Yeah, absolutely.

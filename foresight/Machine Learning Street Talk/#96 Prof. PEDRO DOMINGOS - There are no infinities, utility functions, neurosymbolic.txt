Hi, I'm Peter Domingos, I'm a professor of computer science at the University of Washington
and machine learning researcher, probably best known as the author of the master algorithm,
popular science introduction to machine learning.
I'm here at NeurIPS 2022 of the vast amounts of stuff that's happening here.
The two that I found most interesting and are closest to my own research are Neurosymbolic
AI and symmetry-based learning.
Okay, Professor Pedro Domingos, it's an absolute honor to have you back on the show.
Pedro is a professor at the University of Washington and give us a quick introduction
to yourself, to your experience here in Europe so far and what's top of mind for you?
I'm a machine learning researcher, I've worked in most of the major areas.
I've also written a popular science book on machine learning called the Master Algorithm.
I'm having a lot of fun here at NeurIPS, listening to various talks like David Chalmers
on Consciousness and Geoff Hinton on Sleep and looking forward to the rest of it.
Awesome, I'd love to get your thoughts on Chalmers in a bit actually, but the first
thing I wanted to talk about just because it's top of mind is this whole galactica situation.
So first of all, I was speaking with Ian the other day and I think it's a little bit unfair
that Meta really bear the brunt of this.
OpenAI have just released this new chat GPT bot which suffers from similar failure modes
and it just kind of feels that they're not getting anywhere near as much stick as Meta is.
Well I think, I agree with you, I think the brouhaha about galactica is way overblown.
That system is really largely harmless.
It's just another large language model that's designed for actually something that to me
as a scientist is very interesting.
I would love to have a system like that to help me out with certain things and I think
it's a step in the right direction and I think the brouhaha however is an instance of people
jumping the gun on a lot of these AI things in a way that to me is very excessive.
Having said that in a way they set themselves up for it in a way that they need and have,
they kind of over claimed what it did and the problem with these LLMs is that they generate
a lot of stuff that looks good but can be completely wrong and in a way there's no worse
place to do that than in writing scientific articles.
So when they came out with it, they should have been more careful about how they frame
it.
I think they took concerns sort of like this competition and one upping each other on who
comes up with the frilliest demo and that kind of backfired.
So they shouldn't have had to withdraw it.
I think that's all pathetic and hopefully they've learned the lesson that next time
they will do it slightly differently.
Yes.
Okay.
Okay.
Well, so Gary Marcus has been very loud about this on Twitter so he's really pushing
the point about misinformation and the thing is as well I don't want to characterize the
ethics folks as having monolithic views because they don't have monolithic views and I also
think that a lot of the ethics guidelines for large language models are very reasonable.
Like I interviewed the CEO of Coheir the other week, Aidan Gomez.
I went through their terms and conditions and policies all very reasonable.
The only sticking point for me is the misinformation one.
I think the kind of the moral valence of it is in its use and especially with misrepresentation.
I don't like this paternalism telling me what's good for me.
I've just lost out on using a really cool tool basically.
I completely agree with you.
I would take it even further.
I do not want other people deciding for me what is misinformation and what is therefore
allowed to be said because it's misinformation or not.
For a couple of reasons.
One is that these people who claim to be big critics of misinformation, a lot of them
are misinformers themselves.
And the bottom line is that you always have your ideology that informs what you think
is true and false.
And I don't want anybody, every one of us in a democracy should be deciding for themselves
what is true and what is false and what is valid and what isn't.
And I have no fear of attempts to misinform me as long as I have a multiplicity of sources.
The biggest misinformation danger is when you have only one monolithic source of truth,
whatever it is, which is unfortunately what a lot of these anti-misinformation people,
I think consciously or unconsciously want.
Give me 10 things, 9 of which are misinformation.
I can do the job of figuring out which one I think is valid.
Give me only one of those things and chances are 9 in 10 that it is misinformation and
then I have no chance to overcome it.
So this whole attack on things because they're misinformation.
And I mean, I understand the impulse that like, why have all this falsehood flying around?
But the way to overcome that falsehood is not by censoring it.
You should know this, right?
You should be having to refight all of this over again in the context of social media
and large language models and so on.
So you said something really interesting, which is that this notion of a pure truth
or a monolithic truth, and there's this concept of epistemic subjectivity, right?
Or things observe a relative, even complex phenomena like intelligence.
No one understands what it is.
You can't reduce it to one particular thing.
People have different views on it, right?
So this notion that there is a pure monolithic truth of the world, I think is horrifying.
Well, I would put it slightly differently.
So first of all, there's a question, is there one reality or not, right?
Is there truth or is there my truth and your truth, right?
And actually, I understand the impulse to talk about my truth and your truth, but I
think as a...
So what is really true?
We don't know.
But as a...
I think the most useful, including socially useful working hypothesis is that there is
a single reality and the single truth, but it's extremely complex.
So no single one of us can get at it.
So what we need is many different people coming at it from different angles.
But with the premise that we need to try to make these things consistent.
So just saying, oh, we have different truths and there's no reality, that is actually very
counterproductive because it gives everybody a pass to just believe whatever wacky thing
they want.
And then the consequences of that when you have to make the real decisions are very bad.
At the same time, I agree with you.
If I think that I have access to that truth and everybody just needs to, you know, count
out to it, that is very dangerous.
So I think we need to entertain these two ideas that there is a truth, but it's very
complex and no one has a monopoly on it.
And the key is, you know, like objective truth is what different observers can agree on.
And now we can figure out what it is that we agree on.
And that way we make progress in understanding reality.
And we also tend to make more of the right decisions because we're closer to the truth.
Okay.
But do you see it as, I mean, I think the reality thing is interesting, but do you just see
it cynically as gatekeeping?
As in having a clerical class control?
Oh, absolutely.
No, absolutely.
That's precisely the danger that I was referring to is that if you, let me put it this way.
If ever there is, and this is a commonly mooted proposal, right, and not even proposed like,
are we going to have a truth commission of people who decide what is true on whatever,
Twitter or something, right?
That is a really alarming thing because there is no commission that can do that.
What they're going to do is they're going to impose their version of reality on everybody
else, which unfortunately is what a lot of these people want to do.
They convince that they have the truth and they want to impose it on the rest of us.
And that is really alarming.
We know historically what happens when people succeed in doing that.
Yes.
Yes.
But I suppose my point with the gatekeeping is it almost gets you to the actual truth
of the matter is irrelevant.
It's actually about power.
But what's your take on, I don't know whether you think this is putting it too strongly,
but this being a form of industrial kind of gaslighting, kind of, you know, in an Orwellian
sense, trying to shape people's reality through, you know, language, culture and interactions
on the internet.
I think a lot of it is deliberate.
Some of it is, I mean, I'm an optimist about human nature at the end of the day.
Maybe in, you know, maybe with justification, maybe without.
I think so there's this postmodern view that it's all about power and it's certainly partly
about power.
I think a lot of the people doing this, unfortunately, or maybe fortunately, they are, they're not
seeking power for its own sake.
They have a set of beliefs that they think is right.
And then the means, you know, they unjustify the means, right?
That's the problem.
So that gatekeeping, you know, and that gaslighting happen not because, not for their own sake,
they happen for the sake of a cause.
And now there's two problems with this is that these days the causes on behalf of which
this is being done, in my view, are largely wrong, right?
But whether they're right or wrong, right?
The problem is that this is just noxious in its own right.
And also then a lot of sort of like, again, personal desire for power and promotion and
prevailing over others.
Then of course, you know, hitches are right onto this.
Yeah, interesting.
I mean, we'll get into consequentialism in a minute because I think there's quite an
interesting journey we can go there.
But I wanted to cite Francois Chollet, I'm a big fan of him.
He just tweeted saying, I'm not too concerned of whether what I read is right or wrong.
I can figure that part out myself.
I'm interested in things that are useful, thought-provoking, novel.
Sometimes the most creative thinkers have a bias towards wrongness, but they're still
worth reading.
Would you agree with that?
Yes, I largely agree with that.
So as I was saying before, you know, I can tell for myself or I can do that exercise
of figuring out what is right and wrong.
The most important thing that I want is to not miss out on things that I don't want to
miss out on.
Like, you know, the known unknowns and the unknown unknowns, the biggest killer is the
unknown unknowns.
So if anybody trying to learn or understand something, they're for person or organization
or society, right?
If all they do is move the unknown unknowns to known unknowns, they've already gone an
enormous distance, right?
And so I appreciate people who I disagree with, first of all, because that's how you sharpen
ideas.
Yeah.
But also because they may just bring things to my attention that if we were all conforming
to the more majority view, would not come to our attention.
And then those more often than not are the ones that kill you.
Yeah, that's so interesting.
I mean, there's a real analogy here, even this might be tenuous, but between symbolism
and connectionism or, you know, Rich Sutton said we shouldn't be hand-crafting our AI
systems.
We should kind of let them emerge.
And it's a similar thing with our moral framework, but you're kind of saying that it should be
emergent from low level complexity and diversity and interestingness.
And there is another school of thought, which is that we should be top down and we already
have a representation.
I actually think it should be, it needs to be a combination.
We need to have both.
This is one of those debates that in some sense puzzles me because to me the obvious
answer is that we need both.
And then if you read the master argument, this is what I do.
I look at the different paradigms of machine learning and I don't come out in favor of
any of them because I actually think we need ideas from all of them.
And then we need to combine them into something coherent.
And if you look at psychology, like your brain does bottom up and top down processing.
And if it only did one of them, either one, it wouldn't work.
And I think as we try to build a larger intelligence, it's the same thing.
We definitely need the bottom up part and, you know, by volume, the bottom up part is
going to be bigger.
So if you could only have one, that would probably be, you know, the choice.
But the top down part is also very important.
If you go all the way back, the top down part probably started this bottom up and got synthesized
and improved.
But now we need that loop.
The loop is actually very important.
Well, that's interesting.
So in your book, I guess I want to sketch out different types of AI architecture.
So, you know, you get universalists to this kind of deep mind idea that a very simple
underlying algorithm could produce everything.
And then you get, you know, hybrid folks on the other side of the spectrum.
And then there's an integrated approach in the middle.
Like, where would you kind of place yourself on that continuum?
I would place myself very much in the frame of mind, well, let me put it this way.
I don't know, but which is nobody does, right?
If somebody tells you that they know how we're going to get to intelligence, you should be
suspicious right away.
But what do I think is the most promising approach and the one that ideally would be
the best one if we can pull it off?
It's there being a single algorithm.
So at that level, I very much sympathize with what is effectively the deep mind's agenda.
Now, we're at part with a lot of these people is that I don't think the algorithm that we
need is as simple as many of those people think it is.
And I don't think it exists.
It is probably the case that the algorithm that we really need at the end of the day
doesn't even look that much like any of the things that we have now.
So I think hopefully there is such an algorithm, but we're still far from it.
Interesting.
I mean, they would cite the example of evolution as being a very simple underlying algorithm.
Although Ken Stanley would say that people misunderstand evolution.
So I agree with them at that level.
In fact, in the master algorithm, I have a chapter where I go over the objections and
the reasons to believe that there is a master algorithm.
The majority of the people even in the field are skeptical of that notion, even though
I would claim that effectively that's what they're pursuing.
People like Rich Sutton and Jeff Hitton, I asked a bunch of people before I wrote the
book and they do believe in this idea of having a master algorithm.
A lot of people believe that, but intuitively a lot of people believe that no, there is
no such thing.
And I understand that intuition, but I don't think it's a well-founded intuition.
Let me put it that way.
But in a sense, we know there is such a thing, because look at cellular automata, look at
what we've already done with deep learning.
I think the context is, is there such a thing that will produce what we want?
To take your example or DeepMind's example of evolution, in the book I mentioned empirical
evidence that there is a master algorithm and exhibit one is evolution.
If you think of evolution as an algorithm, which by the way is a very old idea, I think
it was George Bull that said, God does not create animals and plants, he creates the
algorithm by which animals and plants come about.
He didn't use the word algorithm, but that's essentially what he said.
This I think is right on.
Another example is your brain.
If the algorithm doesn't have to be something as simple as backprop and you're a materialist
like most of the scientists are, your brain, if the master algorithm is an algorithm that
can learn anything you do, then your brain is that algorithm.
But then there's another one which is even more fundamental, but I think from the point
of view of this debate is very illuminating, which is the laws of physics.
Why stop at evolution?
The laws of physics are the master algorithm.
Evolution is very complicated.
In fact, what I think about evolution in AI currently is that evolution in reality is
much more complex than we give it credit for, which is why a lot of our current generic
algorithms don't work that well.
But the laws of physics at this level are much simpler.
If you think about it, from the laws of physics comes evolution and comes all the intelligence
that we have.
It's very intriguing why that happens and why the laws are such that that happens.
But even just the laws of physics are already a master algorithm.
Now what you could say, and many people immediately say is like, oh, but if you start from there,
you'll never get anywhere, right?
But then you can say evolution is the laws of physics sped up in a certain direction.
And then our reinforcement learning is like evolution.
People have pointed out the same way except it's faster.
And in a way what we're trying to do now in machine learning is the same thing yet again
except only even faster.
But what are the consequences of?
I mean, let's say it is actually a very high resolution algorithm.
So it's something that appears to be completely unintelligible to in respect of the output
phenomena.
Is that is that even a good place to be?
Because, you know, just like with cellular automata, there's no real paradigmatic
relationship between the underlying rules and the emergent phenomena, right?
So is that really even something we want?
No, I think there is.
So we don't know.
But I think people and this is very common among connections is to say this stuff is
also complex that we can't possibly have a handle on it.
We just have to let it happen.
And I think that is not giving enough credit to our human brains, right?
We are incredibly good at making sense of things that in the beginning, I mean, over
and over and over in the history of science and technology, you start out with things
that you don't understand very well at all.
But then over time, we kind of change our representation of the world to make those
things actually be intelligible to us.
And we should not a priori assume that that's not going to be the case here.
So, for example, cellular automata, amazing things come out of whatever the game of life
that seemed completely disconnected from those, but they aren't, right?
And, you know, there's various depths at which you could go into this.
There will probably at the end of the day be some large element of this that we can't
figure out very well, but we can figure out enough that we have a handle on it.
So, this singularity notion that at some point AI is just completely beyond our understanding.
I tend not to buy.
I don't think it will be completely beyond our understanding.
But it's an analog back to our Twitter discussion, like, because we can only understand it through.
It's like having views on a mountain range, you know, the view looks different depending
on where you're standing.
And it's the same thing with the emergent phenomena in a cellular automata.
No, very good.
And, you know, the classic example of this is the blind man and the elephant, right?
And that's actually the metaphor that is in the book, as I say, you know, the different
tribes are like different blind men, right?
But precisely so, AI is one of the blind men.
I can see part of the elephant, but it'd be who's meant to also talk to you who see another
part of the elephant.
And then each of us understands a little bit more of the elephant than we would if we were
on our own.
But most importantly, we collectively, which is what really matters, actually understand
maybe not the elephant completely, but much more of the elephant than either any of us
would alone, right?
And it's certainly a lot better than just giving up and say, like, oh, we're never going to
understand this strange thing that's in front of us.
Interesting.
But that's a great argument to what you were saying before.
So it's beyond our cognitive horizon.
Therefore we need to have diversity of aspect.
There's a, yes, there's a question of whether it's beyond the cognitive ability of a single
human.
Yeah.
And then there's the question of whether it's beyond the cognitive ability of an entire
society of humans.
And obviously, there'll be things that are beyond the cognitive ability of a single human,
but not beyond the cognitive ability of a society.
Also these days, we have computers.
So our cognitive power is augmented by our machine.
So we can understand things or bring things to the point where we understand them to a
degree today that we couldn't a hundred years ago.
Right now, that is a fascinating point.
So it's beyond our cognitive horizon individually, but it might not be beyond the cognitive horizon
of loads and loads of humans on the internet, you know, the wisdom of crowds.
But we don't, I mean, how do we know that the crowd understands?
But we know, well, that's the, in some sense, the beauty of this, right, is that we never,
what is the crowd really understanding, right?
And again, once the crowd is augmented by machines, like machine learning algorithms,
right, we can ask what do we as a society equipped with all of our, you know, large
language models and so on and so forth, what do we really understand, right?
Now, at some level, you can't answer that question individually because you are just
an individual, but right, there's a couple of very important things that we shouldn't
forget.
You could, one thing you can do and that I do do is say, like, do I now actually even
just individually understand things better than I did before when it was just me looking
at it?
And the answer to that is almost invariably yes, right?
So there is a big game to be heard there.
And the second one is that you, ultimately, you tell by the consequences, right?
And like, for example, take a deep network, right?
And you may not know how it works, but if it's doing medical diagnosis, you can tell
whether it, you know, gets the diagnosis right more often than it did before or more
often than another model.
So we as a society may not, you know, we individuals may not very understand very well what we
as a society understand, but we can see the consequences and at some level, that's the
point.
Yeah.
So on that, I mean, that sounds like a bit of an appeal to behaviorism and we're going
to talk about that in respect of charmers as well.
But it also brings us back to, you know, we were talking about empiricism versus rationalism
and nativism and all of these topics.
Would you place yourself in that camp of being a nativist and a rationalist or completely
the other way?
No, absolutely not.
Again, this is one of the points that I, you know, go back to is there are the empiricists
and there are the rationalists and you could see naively machine learning as being the
triumph of the empiricists, but it actually is not there are very fundamental reasons
why it's not.
And I really do think, and this is not just think there's this thing called the no free
lunch theorem, right?
And if you take those things seriously, the solution has to be a combination of empiricism
and rationalism.
I don't think either side alone has or even can have the whole answer.
So very much we need both of those.
And if you're a pure empiricist or a pure rationalist, I'm already suspicious of you.
Wonderful.
Coming back to what Franz Walsh said in his quote, he said, you know, producing things
that are thought provoking novel and all the rest of it.
And I was speaking to some alignment folks yesterday and we'll pivot to that in a minute.
The big thing for me after doing an episode on sales, the Chinese room is, you know, where
does intentionality come from and Chomsky talks about agency, for example, we do things
that are appropriate to the situation, but not caused by them.
So from my perspective, all these generative models, all these large language models and
so on, the creativity, the real spark of genius still comes from us, right?
We've just kind of like, you know, the boring bit of actually doing the task is now delegated
to the algorithm.
I would disagree with that.
I think you are, I mean, your position is very reasonable and actually, I would say
probably the most common, but I think when you do that, you are giving us too much credit
and the large language models too little.
We tend to have this notion that creativity is something magical.
In fact, I remember for many years, so quick parenthesis, in the previous life I was a
musician.
So, you know, I, in some sense know about, and a lot of my job was composing songs, right?
And I was always, at the same time I was already studying AI and I couldn't help but connect
it too, right?
And think about like, what would an AI look that was able to compose music, right?
And talking to late people who are not musicians, they think that composing songs is some kind
of magic thing that comes from, you know, whatever the great beyond, and it's not.
It's a very human enterprise and it can very well be automated.
It's actually now, you know, people, I used to say to people like, people always say like,
well, creativity will be the last thing that we automate because we humans can do it and
there's no machine school and be like, no, it's going to be the opposite.
You automate creativity long before many other things and we're there now, right?
In just the last, so I think when you, let me put it this way, your prompt to the LLM,
let's say, is like the grain of sand to the oyster, right?
You should not give yourself credit for having made the pearl because it put the grain of
sand in there.
That's a, that's a brilliant analogy, right?
So it is still the LLM, we need to, we can critique how creative it is or not and there's
a lot to be said there and a lot of progress to be made, but we need to give it credit
for what it does, right?
It is well or not so well, right?
Maybe it's more of an illusion that we're giving credit for and whatnot, but that text
or that image or whatever, they were created by the AI.
And in many ways, the thing that was created by the AI is no worse than what would have
been created by an artist if I gave them the prompt.
No, okay.
Well, on that, I agree with you.
I mean, Melanie Mitchell had this wonderful anecdote from the Google Plex when she was
with Douglas Hofstadter and he was talking at the time about, you know, how he would
be devastated if an AI could produce a Chopin piece, you know, which was indistinguishable
from one which he actually created and of course, that did happen, but then we get into
this discussion of where does it start, right?
Where does it start?
Computers only do what we tell them to do, right?
They've been trained and actually I was speaking to Sep about this the other day that, you
know, all of the abstractions, all of the things that the computers and the models do,
they are crystallized snapshots of things that humans have previously done and we've
written the computer code.
So where does the creativity start?
Well, but we, by that standard, we humans also only do what we're told to do.
We do what we're told to do by our genes.
Our genes do what they're told to do by evolution, which does what is told to do by the laws
of physics, right?
Right.
And now, again, this gets back to this notion that there's nothing magical about creativity.
Creativity really is, to a large extent, cutting and pasting stuff and satisfying consistency
constraints between them.
And I'm not just saying this in the abstract, like long before the modern era, there's this
guy called David Cope, you know, a composer and professor of music at UC Santa Cruz who
created these programs that exactly they would write, they can write, this was pre-machine
learning.
It was list code that what it did was basically have rules about how music should be.
And then it takes snippets and combines them, right?
You could say it's just parroting those bits, but the truth is at the end of the day and
you can choose.
You say, like, give me something in the style of Mozart.
And it creates something that looks indistinguishable from what Mozart did, but all it's doing
is this kind of recombination of pieces.
So we humans, we have too much respect for appreciation of our own intelligence.
That's also what we're doing.
Yeah, I think I agree with you.
I mean, first of all, intelligence is a receding horizon and there's the McCorduck effect.
I agree with all of that.
But yeah, I think it's a similar thing to how we anthropomorphise large language models
and even, you know, it's tempting to say large language models are slightly conscious
and we'll talk about that in a minute, but maybe like we also anthropomorphise our own
agency, right?
We have like a little bubble around ourselves and we kind of delude ourselves that we exist
as an individual unit with agency disconnected from the rest of the world.
Well, precisely the problem with how we largely take AI today, this has always been the case,
by the way, is that we have a new resistible notion to anthropomorphise anything that behaves
even remotely like us.
We're the only intelligent things that we know, so if something starts behaving intelligently,
then we project onto it all of these other human characteristics.
Same with consciousness, same with creativity.
We don't know anything else that's creative besides us.
So once a machine starts behaving creatively, we cannot help but project a lot of things
onto it.
It's just reasoning by analogy, right?
So it's a kind of analogical, so like you're like me in this respect, so you probably are
in this other spec.
Now, the good news is that we always start out with this kind of very good reasoning
by analogy, but after a while, we actually start to build a model of the real thing.
So AI for the public at large right now is very new, but gradually we'll come to a point
where we zero in on what AI really is rather than just the shallow analogies that we initially
used to try to understand.
Okay.
Well, I'll try it from a slightly different angle.
So we were just saying Seoul makes the argument that it's a biological property and that's
where intentionality and consciousness comes from and it's a requisite, but we'll leave
that for the time being.
Let's go the Fodor and the Gary Marcus and the Chomsky group, and they would argue that
creativity is basically this notion of, or even analogy making by extension, is this
notion of being able to select from a set which has an infinite cardinality.
And as you know, neural networks can't represent infinite sets because they're finite state
automators, therefore they make the move we need to have this compositionality.
What do you say to that?
Well, there's a lot to unpack there.
I think we definitely need compositionality, right?
If somebody asked me, make a list of how there's some things that are actually essential
for intelligence, compositionality would be one of them, right?
And this, of course, is the thing that people like Chomsky and Gary are not really care
about, right?
Having said that, I think first of all, there is no such thing as an infinite set, right?
Like infinite set is a useful but extremely dangerous and confusing mathematical tool,
right?
There is no such thing as an infinite anything and there never will be.
So I would just raise this at, well, yes, creativity and almost anything we can do in
AI is selecting from a very large set, not infinite, but very large, right?
And now, but now we don't just select like one full element at a time.
We compose it out of pieces and that's actually where the intelligence comes in.
Interesting.
I don't want to go too far down the digital physics route, but we did just have Yoshua
Barkon and I mean, just to reclamify on that, would you place yourself in that camp that
the universe is digital and made of information?
Valid question.
I certainly think the universe is finite.
I think, I mean, like Seth Lloyd says, the universe is a computer, right?
And I think that is true or false depending on what you take the word computer to mean,
right?
So if you say that the universe is digital or is a computer as kind of like an analogy
that lets us understand it better, I'm all for that.
I don't think the universe is little, you know, here's the way to put this.
The universe is a computation.
Like, I don't know what the computer is or if there is one.
Now the universe is digital in the sense that deep down at the most basic level, the universe
is made of discrete things.
OK, is this like the it from bit, the John Wheeler type hypothesis?
Yes.
I mean, if you read that paper, it is, I mean, John Wheeler was a brilliant person.
Again, very, you know, to get back to Fran√ßois Chalice's tweet, he was very good at coming
up with his provocative notions, right?
And the it from the thing, of course, is like newly unveiled today.
And I do so at that level, I do agree that looking at the universe is being made of
information is very useful.
And in particular, if you want a grand unified master algorithm, in some sense, the only
way that I at least can see of doing that is by seeing everything as information.
So I think, and if I do something that I am working on that, looking at everything as
information is a very productive thing to do.
Yeah, but but my caution is that information is one aspect of everything.
So I can give you a theory of everything that's based on information.
But it's not truly a theory of everything.
It's a theory of one aspect of everything.
And I think there's a lot to be done there.
But again, we shouldn't forget what we're living out when we focus on that aspect.
Yeah, I mean, we've spoken a lot on the show about, you know,
Penrose's view and obviously Sal's view that arises from from biology.
And I know if Keith was here, he would argue strongly that he believes in in
continuum, and therefore we would need, you know, hyper computation to have this
universe. That would be an interesting discussion to have, because I really don't
see where there is physical or any evidence for continuum of any kind.
The evidence is always that continuum are a useful approximation, but always
underlying the continuum is a discrete reality.
You take a sensor of anything, right?
You know, quantum mechanics is like the quintessential example of this, right?
What do we measure at the end that it's always discrete events?
Right? Like it's the detection of a photon by, you know, by whatever detector, right?
Could be a model of Dobson or a CCD or whatever.
But it's a it's a it's a change of state.
It really is a bit.
Oh, interesting. Well, how would you contrast that?
You know, Stephen Wolfram has got this idea of of digital physics and, you know,
maybe and again, unfortunately, we have to use arguments from behavior, you know,
to kind of say, well, we've we've got potentially a graph cellular automata
and it creates this beautiful emergent structure, which is very much like the universe.
But, you know, Scott Aronson would make the argument that he's discounting quantum mechanics.
I mean, what would you say to that?
So I think Steve Wolfram's theory is very interesting.
And he gets some things right that a lot of other physicists don't,
in particular, that the universe at heart is discrete.
So I'm very much with him on that aspect of his agenda.
And thank God there's someone like him and a number of others,
you know, going that route, right?
They're the minority in physics.
But actually, I think if you look at just what has happened in the last 10 years,
things are very much moving in this direction.
And I think they're going to move more, right?
Now, having said that, his specific theory, I think has a lot of shortcomings.
And I don't think it's the ultimate theory or maybe even the best path to a theory,
you know, to a discrete theory of the universe.
Scott Aaronson's critique in that regard, I think misses the point, right?
It's interesting because it's interesting that you should like pair those two
because Steve Wolfram, in a way, is a physicist to become a computer scientist.
And Scott is the other way around, right?
And I think, you know, like I greatly admire both of them
and I'm friends with both of them.
I've had many discussions with them.
I think, you know, just to very, you know, cruelly caricature things in a way,
the problem with Steve is that he has bought into the computer science
assumptions too much.
And the problem with Scott is that he has bought into the quantum physics
assumptions too much, right?
So if you really think carefully and rigorously about quantum mechanics,
all that continuous mathematics is there just to make discrete predictions.
So the continuity may be useful.
It is useful.
I'm not arguing against the use of continuity and infinity in our mathematics.
In fact, we'd be nowhere if we didn't have it.
We just have to remember that it's an approximation.
It's a useful fiction, right?
So quantum mechanics in no way invalidates Steve Wolf from theories, right?
The problem, however, is that he has not entirely, you know,
ever since cellular automata days, right?
He was always saying like, oh, you know, the laws of physics will come out
of this cellular automata, right?
And people had these objections and, and, you know,
now he and others have partly answered some of them.
But the truth is at the end of the day, he the only way to answer
that objection is to say, look, here is how quantum mechanics arises
from my discrete model of the world.
And I think this will happen, but it hasn't happened yet.
Interesting. OK, we'd love to get Steven on the show.
Actually, he's got a he's got a new book out now,
which is kind of like expanding on his previous work.
But OK, I was having a chat with some alignment folks yesterday.
And it's something that I'm a bit naive to, but as I said, I've just read a book.
I think it's called The Rationalists Guide to the to the Universe.
And it kind of talks all about the the early embryonic stages
of Robin Hansen and Nick Bostrom and Eliezer Yuckowski
and the Les Ron community and, you know, the info hazards.
And, you know, I don't know if you've heard of Rocco's Basilicist
and all of this line of thought, basically.
And yeah, so where to go with this?
Now, I kind of put forward that part of my problem with their conception
is that it relies on this rational agent making trajectories of optimal decisions.
And also, they they tend to be utilitarianism
and utilitarianists and consequentialists.
And yeah, I just wondered, what's your kind of like high level take on this?
Well, there's many aspects to this, right?
I think let me put it this way.
I. A rational agent, right, is an agent that maximizes
maximizes expected utility, right?
The definition of rationality is that it's, you know,
expected utility maximization, right?
And there is a lot of content to this, right?
And, you know, people in many fields like economics and and and, you know, AI, right?
They make good use of it.
It doesn't answer the question of what is the utility that you're maximizing, right?
So if you give me utility function, right?
Now, if you maximize it, you're rational.
You can it can be bound.
You can be boundedly rational because you're and indeed this
this is the interesting and prevalent cases that you can only maximize it
within bounds and you have to make compromises, says this fast and what not.
But still, you're rational.
If you don't do that, you are irrational, right?
So rational is a very, you know, like so many mistakes that we make
as a society as individuals would be avoided if only we were rational in that sense.
So at that level, I sympathize very much with that view of the world.
Having said that, there's a huge gaping hole in the middle of this, which is like,
but what is your utility function?
Right. And and, you know, one attitude is like, oh, that's for you to decide.
You know, you you tell what me or your utility function is.
But but then you you you're you're entitled to sell what like.
But like my whole problem is that I want to figure out what my
utility function should be.
And at that point, this whole theory of rationality just doesn't help you at all.
The utility function is an input, right?
So another question becomes what is your utility function, right?
And then there's a very related, but as Hume said, very different question,
which is like, what should your utility function be like?
Should is a very loaded worth here, right?
And then what what usually happens is things like this is that our
notions of morality and so on are trying to impose a should on you,
a utility function that you should have because it serves the utility of the society.
Now, from the point of view, the society, this is good, right?
But from the point of view, because the society hopefully will live
and prosper if its elements contribute to its utility, not just their own, right?
But it still doesn't answer the question.
So you can you're entitled to ask.
So what does answer the question?
Right. And my view on this is that none of these people and these people
include Kant and Bentham and, you know, Plato and everybody, right?
They you can't do that, right?
The the the so to me, the supreme reality of life, Supreme
maybe is a bad word, but like the overarching reality is evolution, right?
Everything we are is created by evolution.
And as somebody famously said, nothing in biology makes sense,
except in light of evolution, nothing in morality makes sense,
except in light of evolution, not just biological evolution, even though
that's part of it, but also social and cultural evolution.
So at the end of the day, the question that you need to ask yourself is like,
is which utility functions are fitter?
And those are the ones that will prevail.
So so let's let's go there.
That's really interesting.
Now, you're known as a skeptic of collectivist thought, right?
We know, and there's this interesting dichotomy we were talking about
of, you know, monolithic truth, but the utility functions interesting as well.
Because in a sense, I mean, I know these folks are consequentialists,
but in a sense, that's more leaning towards deontology.
I did it again, deontology, you know, which is this idea that
we have kind of like a principled approach to to morality.
And I'm skeptical as well that it's possible to create such a utility
function because it wouldn't really be parsimonious.
But how do you wrestle that, that you have a simple utility function,
even though you believe in diversity of ideas?
Oh, no, I didn't say simple.
Go on.
Crucial point, the utility function could be extremely complex.
And in fact, the utility function.
So first of all, there's there's more than one level to this.
You to let's say you believe in utilitarianism, right, which I don't,
but have, you know, compared to the others, it's probably the least bad.
Right. Yeah.
Believing you tell if you believe in maximizing utility function
that in no way sanctions collectivism.
Collectivism is one particular strength that historically came out of that.
Right.
And, and, and, you know, again, and Bentham is responsible for it.
But it's this notion that you should have a utility function
in which everybody counts equally.
This is now making a choice of utility function,
which is different from having one.
Okay, but I think you're saying something quite interesting as well,
which is that at the moment,
the utility is a function of market value,
which is very much inspired by Adam Smith's hidden hand of the market.
But I think your views against collectivism is very much against
this idea of equality of outcome.
And that's definitely not what you're saying.
No, I mean, that's even going beyond that, right?
Equality of outcome is actually irrational, frankly, to we could go into that.
But, you know, you mentioned the market, right?
And the market is the size utility.
Again, that is a one way to decide.
I mean, that is also a very critical approximation to what you really want.
So actually, all we have with capitalism or carnimism at this point,
in terms of utility function, are very imperfect, right?
And that's even saying it generously.
And really, our job is to try to come up with something better,
which I totally think we can, right?
And by the way, one very salient question here,
which again, for economists, it's very salient,
is this question of like,
should you have one utility function overarching, controlling everything,
even if it's complex, right?
Or should you not, right?
And that I think is a very interesting question, right?
And there are good arguments in both directions, right?
So let me just give you one silly example,
which then I think also generalizes two other things.
Does your brain have a singular utility function?
And I think the answer is no.
Now, you could say from an evolutionary point of view,
the overarching utility is fitness.
But then the way that cashes out in your brain
is that your genes need to control this adaptive machine, right?
In such a way that you give the machine freedom, right?
To do things that the genes by themselves couldn't.
But at the same time, at the end of the day,
that machine has to subserve the propagation of those genes.
And the way you do this, right,
at least the way evolution seems to have done,
and I think it makes a lot of sense,
is that you don't just have one utility.
You have several ones which correspond to your emotions.
And then they fight it out.
So I actually think there's this connection
between rationality and the emotion that people don't make,
which is that your emotions are really your utility functions.
You just have different ones that cater to different things, right?
You know, fear and anger and so on.
And so I think in reality,
we actually have multiple utility functions.
But because, again, it gets to this problem that
what we're trying to do is approximate something
that is very complex and difficult to get at,
maybe it is just one,
but we're better off trying to approximate it
with 10 or 20 different things
than just trying to nail that one thing.
That's really interesting.
And is your view then on having this diversity
of utility functions analogous to your views
on the master algorithm?
Huh. It's analogous,
but you're actually talking about different dimensions, right?
You could make a table where on one side
you have all the different utilities,
and then on the other side you have the algorithms.
And now you can pair off any one of them.
I can say, I'm going to pursue this,
you know, minimize your fear using symbolism or minimum.
So any combination is valid.
Really, really interesting.
Okay. And then let's get into meritocracy, for example.
So at the moment, we do have the market system.
And presumably you think that some people
do genuinely have more market value than others.
For sure. No. And by the way, I think,
I'm definitely a big believer in meritocracy.
I think. But what does it mean to you?
Right. Very good. So let's get that down first, right?
Meritocracy, so our goal is to have the society
that functions best and provides best for everybody, right?
And I mean, we could refine even that,
but let's just take that for now as our assumption, right?
But then if that is the case,
one of our primary goals, maybe even the most important one,
is to get everybody to contribute the most they can, right?
Meritocracy is often seen as like,
I'm going to rank all the people,
and at the top is the greatest genius,
and at the bottom is the most useless person,
and this is wrong, right?
Meritocracy is a many-dimension thing, right?
The goal of meritocracy is to find for everybody
what they're best at doing so that they can do it,
maximize everybody's contribution to society, right?
And this is a very complicated process.
There isn't a single scale of intelligence or anything else.
Having said that, it very much is the case
that some people are better for some things than others, right?
And if you deny that,
you are actually in the process of destroying the society
and making it dysfunctional.
So I find the attacks on meritocracy extremely disturbing, right?
And a lot of them are,
I've talked with many people who have those beliefs, right?
And the number one thing that they say is,
it basically boils down to like,
oh, meritocracy isn't perfect, so we should junk it.
Something not being perfect has never been a reason to junk it.
It's a reason to improve it.
So there's a lot of room to improve in meritocracy,
but if you throw it away, you are destroying society.
Well, I mean, you can trace this back to our argument about utility.
But the thing is though, if we had a value function
which represented actual market contributions
or even societal contributions, that would be one thing.
But would you agree that we have a lot of game playing at the moment?
So utility is based on playing the success game
or the dominance game or the virtue game,
as Will Stor said in his book.
So we've got these kind of emerging games
and it's not really representing utility.
Well, absolutely.
So far we've been talking about utility, right?
But what happens in the real world
is that there are multiple agents,
each with their own different utility.
And at this point, what you have is game theory, right?
Game theory is just what you have
when there's not a single optimization going on,
but multiple optimizations
which are partly contradictory, maybe partly not.
So the best way to understand everything
that we've been talking about, including society and evolution
and even what happens inside your brain is as a big game.
A much bigger and more complex game
than game theorists and economists and so on
and evolutionary biologists, right, prominently,
have been able to handle in the past.
But I think they are very much in the right track
and we can understand a lot of these phenomena
that you're referring to as they are games being played
by people that have certain utilities, right?
And now you are going to impose your, you know, like,
and it's a game, right?
I don't, you don't know who's going to win
until you actually do the linear program
and figure out how this is going to, you know,
and of course games are, you know, in reality,
you know, most games are not single round games, right?
They're continuing games, right?
So things get very, very interesting,
but this I think is the most productive way
to look at all of this.
Okay, good, good.
But then some might say that this is a platonic way
of looking at the world
and the world is actually much more complicated than that.
And again, we're kind of fooled by randomness
because we're anthropomorphizing the world
and we're kind of framing it as a game.
It might be much more complicated than that.
And I've already said this a couple of times,
but you know, the concept of power, for example,
did when Napoleon said,
I want the men to march into this country,
is it just a simple kind of chain of command that goes down?
No, it's not.
It's so much more complicated than that.
Well, yes, but that's, actually,
I'm not even sure what you mean
by when you say it's much more complicated than a game.
Again, when I say a game,
maybe what comes to your mind is something simple,
like in a prisoner's dilemma, two players, two moves.
It's a game with, you know, with a vast number of players,
each with a vast number of moves.
Interesting, but I think this gets to the core
of what the rationalists talk about.
They have these thought experiments.
They talk about prisoners dilemma.
They have that, I forget the name of that game
where there's the two boxes,
and you have to choose the box, I forget that.
But I guess what I'm saying is that
if you do have this rationalist conception of the world,
and think about it in terms of game theory,
just like the symbolists do,
and the people who handcraft cognitive architectures do,
or even with causality, for example,
we create these variables, it's all anthropomorphic.
Well, I would not, so let me put it this way, right?
You can model almost anything,
can is an important word here.
You can model almost anything in the world,
in any domain, from physics to psychology
to sociology, name it, as optimizing a function.
Whether you should is a debatable question,
but you can, right?
But now, what really happens is that
there are many different optimizations
going on at the same time,
all the way from maximizing entropy
to me deciding what I have for lunch today.
And now what you have is all of these interlocking optimizations,
and that's what I'm calling game theory, right?
One of those optimizations is I'm Napoleon,
I want to conquer Russia,
you're the Tsar of Russia, you don't want to be conquered, right?
And then we play a very complicated game,
which includes other agents, like your soldiers,
which maybe, you know, I, a French soldier,
you know, want to conquer Russia,
but I also want to stay alive,
whereas an opponent really couldn't care less
whether I, in particular, stay alive or not,
as long as he conquers Russia in the end.
So this very complex game, I think, is what goes on.
I don't think framing things in this way
is anthropomorphizing them.
In fact, I think this is our best hope
to not anthropomorphize things,
although at the end of the day,
I think you can look at almost anything
and see a ghost of anthropomorphization there.
But if there's a less anthropomorphic way
to look at the universe than through this lens,
I'd be interested to see what it is.
Well, the only reason I'm saying this is,
first of all, I want to play devil's advocate a little bit,
and we even spoke about the blind men
and the elephant a little while ago,
and I'm sure folks on the left, as they did,
they criticized Ayan Ran, for example,
and they said that she had this very transactional way
of viewing the world as this kind of
Nash equilibrium of self-interested actors.
And are we guilty of doing that?
Are we kind of cutting off many aspects of the truth
by doing this? I guess that's what I'm saying.
So we are always cutting off some aspect of the truth
when we look at anything in any way,
which is not a reason to look at nothing in no way.
So I think this is a very productive way to look at things,
but not the only one.
It doesn't exhaust what there is to be said,
but I personally feel like it's the one
where the most progress can come from.
Interesting.
Now, that's sort of like Ayan Randian
simplification of the world.
Looking at things this way does not imply over-simplifying them.
On the contrary, I would actually say it gives us a handle
on how to go into the complexity and not get lost
and not devolve into like platitudes
or over-simplifying ideologies.
The fact that there's a mathematical component to this
is very important.
Mathematics, when you can apply it,
gives you a very solid handle on things.
We are now at the point where we can handle
a lot of things mathematically slash computationally
that we couldn't before.
So when von Neumann invented game theory,
he said, this is the future of the social sciences.
And so far it hasn't been,
but I think we're actually now at the point
partly because we have the data.
We actually can now usefully apply this point of view
in a way that we couldn't before.
How far it takes us, we'll see.
It's not the only possible to look at things,
but I do think it's probably the most productive at this point.
Interesting.
Okay.
So coming back to this rationalist school of thought,
one thing that I'm interested in is morality.
But let's go one step at a time.
So I think Bostrom came up with this idea
of instrumental convergence,
which is this notion that in the pursuit
of doing a particular task,
the intelligence system might actually potentially kill
everyone on the planet or do adjacent.
And this is where the interesting thing comes from.
So one task, but adjacent multitask ability
and potential intelligence and so on.
So there was an example of a cauldron.
So you've got someone filling up a cauldron
and in the pursuit of filling up the cauldron
to just the right level,
they might kill the person who looks after the cauldron room
just so that the agent could do it more efficiently.
Are you cynical about that or what do you think?
No, I'm not cynical about that,
but let me put it this way.
I don't lose any sleep worrying about the paperclip factory
that's going to take over the world.
I think you have to take that as a philosopher's thought experiment.
The philosopher being Nick in this case.
I think there's a real danger that there's putting its finger on,
but it's also mistaking reality for something else.
So let's look at both parts of that.
The real danger is that if you give an AI
an oversimplified, a hugely oversimplified objective function,
and at the same time a very large amount of power,
bad things will happen and we need to worry about that.
By the way, this is already a problem today
in many maybe more modest ways,
but also more relevant, frankly.
So what do you do?
First of all, the utility function needs to be as rich
and as complex and as subtle as the people that it's trying to serve.
As long as what you have to take a really world example today,
social media, who are all designed to just maximize engagement,
you have an enormous amount of AI at the service of maximizing engagement.
I understand why companies do it and partly they have the right to.
We can get into that.
But the point is, it's ignoring too many things.
So one line of defense against is that you have to enrich your utility function
until it's like a bit, and then this is an open-ended problem.
We're never going to have the final utility function.
It's something that the AIs have to be continually,
you know, AIs, I think Stuart Russell said this and I agree,
like they should spend half their time figuring out what the utility function is
and then the other half maximizing it.
Whereas today it's like I wrote down my utility function in one line
and now I spend this enormous amount of power maximizing it.
So that's one line.
The other line or like one other line is you have to put constraints on the machine.
Hard constraints.
You can win the pursuit of this utility function.
You can think of it as like, you know,
terms with infinite weight in the utility function.
You can't go outside this.
And then the other one is the single biggest reason why I sort of like
this paperclip experiment is silly is that, you know,
along with that paperclip factoring the world,
there are going to be a million other AIs, you know,
each of which is doing the same thing.
So none of them is ever going to acquire the power to cause that damage
unless it's doing something very different from just trying to make paperclips.
So at some level that example is extremely unrealistic
and leads us down the wrong track.
Right, loads of places to go there.
But first of all, I think you do believe in AI alignment then
because you're saying exactly the same as what they do,
which is that we need to have the utility function
that represents the richness of the human condition.
So that's the first thing.
So essentially you're all on board of alignment.
Well, I believe in AI alignment in one sense of it.
Many different things get go under that umbrella of AI alignment.
Right.
I think in the near term, thinking of things into,
I mean, if I like, let me put it this way.
If AI alignment is just trying to have a really accurate utility function,
then yes.
And then the machines are optimizing that function.
Absolutely.
Right.
Yeah.
And in the near term, I think talking about AI alignment is a little,
I mean, the problem that I have with the concept of alignment
is that goes far beyond that.
It tends to see AIs as these independent agents
that we have to align their goals to ours.
Right.
And if that just caches out as like,
here's the utility function, that's fine.
But the problem is AIs are not independent agents.
AIs are our tools.
Just to push back on that a little bit,
because I always had that conception of these folks.
I thought I was arguing against people who believed
in a pure monolithic intelligence.
And a lot of them are transhumanists actually,
and they say that they want to ensure human flourishing
through the use of AIs in tandem,
almost as a kind of extended mind from David Chalmers.
But then I really wanted to get into their fears
of recursive self-improving intelligence
and superintelligence.
Because when you do have this kind of heterogeneous approach
to humans and machines,
there are going to be bottlenecks everywhere.
Now, I like to think of it a bit like the market efficiency
hypothesis, which is that you reach an equilibria
where the individual actors in the market become more efficient,
will become more efficient programmers.
Because we're using codecs.
But we will reach a limit, surely.
Well, to touch on transhumanism for just a second,
because I do agree, at least sociologically,
a lot of that crowd is the same.
Let me put it this way.
And I'm sure this is a controversial statement.
But maybe in the long run, the AIs should take over the world.
Why are we so arrogant that we think whatever the AI is,
it should always be there to serve us.
We are a step.
If you take the long view of this, we're a step in evolution.
We're amazing.
Maybe I'm a human chauvinist, but I do think we are amazing.
But we're not the last word.
So the other day, I tweeted something that is maybe provocative,
but it's like, I think in Gemswich, which is,
I said that the killer app of humans is producing AI.
Maybe our role in evolution is that we're going to produce an AI.
That is the next level of whatever you like.
Consciousness, intelligence, et cetera, et cetera.
And so the notion that in the very long term,
the AIs should still be there to serve us,
by this point of view, is actually silly.
Right, but a lot of folks, let's say the ethics folks,
would find it horrifying.
And I was speaking to Irina actually yesterday,
and she said something a little bit tongue in cheek,
which is that which is actually...
Who, sorry?
Irina from Montreal, Mila, and Irina Rich.
Oh yeah, I know her, yeah.
We were classmates at UC Irvine.
Amazing, yeah, I really love her.
But no, she was kind of joking
that we should almost align human values to the AGI values.
Well, that I find alarming.
Well, I think she was saying it tongue in cheek.
I'm not alarmed by a lot of things, but yeah.
But what do you think about this ethical concern
that if it is the case that you believe
that we're just one rung on the ladder and transhumanism
is more AI than it is human?
People would find that horrifying.
Well, I understand why people would find that horrifying.
And I mean, again, we have to distinguish
the short from the meaning from the long term.
When I say something like this,
I'm talking about the very long term, right?
Trying to make humans subservient to AI today
is a horrifying idea, right?
Now, I think the reason a lot of people
are horrified with this idea period, right?
Is natural, but in my view, naive is just,
they are seeing humans as the end goal.
If humans are the end goal,
then the idea that they should be subservient
to developing the next level of AI is horrifying.
If you have a moral system where humans are the be all
and end all, then all of this is horrifying.
But again, if you take the long view of evolution,
humans are not the be all and end all.
Okay, I mean, eventually this might take us
to the effective altruism discussion.
But I think, as we were saying,
Sam Harris recently had a podcast
talking about the FTX disaster.
And he was kind of making the argument
that we're all consequentialists,
even if we don't realize it,
but there are different degrees of consequentialism.
And I think a lot of the ethics folks at the moment,
they really, really don't like what's going on
with long termism.
And it's because there's this slippery slope
of the kind of horizon of consequentialism.
So with Nick Bostrom,
he came up with this number
that there could be simulated humans
living on other planets in the future.
It's a very big number.
I think it's got a lot of zeros on it.
And what's to stop us from just making the argument
and what's to stop AIs from making the argument
that those simulated lives have more value than our lives?
Okay, there's a lot to unpack there.
So, but let's take this one step at a time.
I very much by the idea of effective altruism on principle.
I think that is the way to go about a lot of things.
I think in some ways,
if you are not an effective altruist,
maybe unconsciously, you are being irrational
or maybe evil, right?
If you believe in altruism,
I mean, think about both parts of that, right?
If altruism is good, then let's say we take that, right?
And then why should you be in favor
of ineffective altruism, right?
If you're an altruist,
if you want the good of other people,
you should try to do the best you can, right?
And so, for example, I very much by the notion that like,
you want to make the most money you can,
so then you can give away that money
as opposed to volunteering at the soup kitchen.
Volunteering at the soup kitchen for, say,
someone with a PhD in machine learning
is an ineffective form of altruism.
Now, having said that,
I think the focus on the long term has been in many ways,
I mean, certainly the long term is important, right?
But the problem with the whole effective altruism movement
is that it got overly focused on that,
and we can talk about why.
And then even, and then a further mistake
is that it got overly focused
on these supposedly existential dangers
that are much less of a big deal than people think like AI.
So between effective altruism and fixating on AI
as an existential danger lies a huge gulf.
I'm for effective altruism, I think, you know,
the long term, you know, there's ins and outs there, right?
And then this focus on like these existential dangers
is very problematic.
You know, for example, you know, to get back to the,
you know, Bostromian notion of like all these minds
that matter more than us and whatnot,
there is a basic idea, right,
that like any economist knows,
which is that you have to discount the future.
And the question is what your discount rate is, right?
And if your discount rate is high, right,
those minds matter not at all.
And now why do you have that discount rate?
The primary reason is that
there's uncertainty about the future, right?
I have to weigh the certain benefit of helping you today
with the increasingly hypothetical benefit
of helping your mind.
There's less and less likely to exist in the future.
So in many of those cases,
the present and the short-term do win.
Okay, but a couple of things to contrast that.
So a lot of effective altruism is this idea
that we're born with faulty programming, right?
So we have these views, you know,
like we have this concept of moral value
and it gets discounted in space and time, right?
So we need ways of overcoming our programming.
But you were saying that we should be thinking about this,
but contrast that with your, you know,
with your statement about Ayan Rand earlier, right?
So Ayan Rand was very, very transactional
because I think the folks that criticize this movement
are suspicious that we are actually being
a bit more like Ayan Rand,
but with the guise of altruism.
And I think they think of the FTX disaster
as being kind of like evidence of that.
A lot of different points there.
The FTX disaster actually has nothing whatsoever
to do with any of this, right?
Sam Beckman Fried was one guy or is one guy,
funny that I used the past tense.
He's one guy who believed in effective altruism,
good for him, right?
He was, I mean, the whole FTX thing was also obviously,
I mean, yeah, we could get into that,
but the point is you should not,
I understand why people's image of effective altruism
would be tainted by what happened with Sam Beckman Fried,
but really it shouldn't be, right?
An idea is not responsible for the mistakes
that its believers make in unrelated domains,
point one, point two, transactionalism.
There is nothing in what I've said whatsoever
that implies transactionalism, in fact, the opposite, right?
I think relationalism is actually the key concept.
And part of this is that games are not one shot.
Your games are played in a repeated way.
And famously, for example,
if you play things like Prisoners of the Lemon
and whatnot repeated, like you're like,
cooperate, defect and whatnot,
as soon as you start bringing in these other things,
like that make things more realistic,
you actually start to get behavior
that is much more, what's the way to put it,
rational in some ways and human and whatnot, right?
Another one is that traditional economics,
which I think Ayn Rand was influenced by,
viewed and still views the world as linear,
but the world is non-linear.
Once you start seeing the world as non-linear,
all of these things really change,
the face of them changes, right?
So I think we have to look at all these concepts
in this view, right?
And we want to focus on the long, so...
So to get back to your first point, right?
We are born with faulty programming.
Part of our fa...
And that's what if effective alteration
is there to overcome, right?
Part of our faulty programming
is that our discount rate is too high,
because we evolved in a world
where your time horizon was very short.
The fact that it's too high
doesn't mean that we should make it zero
and care only about the future.
But what would...
You know, the ethics folks who advocate
for gatekeeping and paternalism,
couldn't you just say that they're doing the same thing?
Well, you should ask them, right?
Wouldn't they lead by saying our programming is faulty
and therefore, you know, we need to...
No, I mean, look, we can...
So part one, we can debate whether our programming
is faulty or not and why.
And so to just start by touching on that,
our programming is faulty.
So our programming is not faulty
in the sense that we evolved
for a particular set of conditions, right?
And that evolution may not be complete
or optimal, et cetera, et cetera.
But roughly speaking, we are not faulty in that sense.
The reason...
Because evolution is doing its job, right?
We have all those impulses for a reason, right?
Now, the problem is that we, unlike any other species,
we actually have actually succeeded in creating a world
that is better for us.
But at the same time, and this is the problem,
we're actually now adapted to a different world
from the one that we live in.
So the faulty program just comes from the fact
that we evolved for one set of conditions.
For example, among many other examples
where your time horizon was very short,
and now we live in a very different world.
And so our job as rational people,
that's what our rational minds are for,
among other things, is to now adapt ourselves
to the world that we really are in
so that we do things that are rational
in the world that we're in, right?
So now, the fact that our programming is faulty
does not see anything about what are the faults
and how you fix them.
And what these people have, I think,
is first of all, the wrong notion of what our faults are
and then on top of that,
the wrong notion of how to fix them.
Okay, now, I want to get into the utility function again.
Again, one of the things that makes me skeptical
is this notion of immutability, both of what we're doing
and in the case of what we've been speaking about
with utilitarianism, what the utility function is.
Now, you were kind of hinting to something interesting before,
which is that it might be diverse
and it might also be self-updating.
But I'm constantly asking myself the question,
how does that work and who gets to say?
Well, so very much, I think it's complex
and it should be self-updating, right?
We're never going to final itself.
If you buy this notion that the ultimate arbiter is evolution,
then utility functions are subject to evolution.
Right, so you think about it
or you can't think about it.
It's a wrong world.
You can't think about this
and it's useful to think about this in the following ways.
To a first approximation,
the number one entity that's evolving is utility functions.
What you have in the world at any point
is a population of utility functions, right?
And now they combine, they evolve,
you have next generation of utility functions.
And then there's also how the utility function gets optimized.
That is also subject to evolution, right?
And now how the utility function is optimized
changes a lot faster
and this is a lot more complex than the utility function itself,
which is the point, right?
So at a certain time horizon,
it's reasonable to approximate utilities as being fixed.
Like for example, the utilities that are encoded in your brain
are fixed by your genes, right?
So in the context of our present human moment
and effective ultramism or not,
it makes perfect sense to think of utilities fixed.
But it is evolving and not just on
eon time scales, but by the generation, right?
Things evolve by the generation.
Okay, but it's still relatively glacial
and I take your point that there's a kind of divergence
between the world we live in
and the programming that we've got.
But then, okay, let's imagine that we create a new population
and I guess what I'm saying is that
you think that the utility function should emerge and evolve,
but I would argue for some kind of morphogenetic engineering
where it's a kind of hybrid between something which is emergent
but something which we can nudge.
Oh, I mean, I'm glad you brought that up.
Nudging is a form of emergence.
You yourself are emergent
and the things that you do are emergent as well.
Everything is emergent, right?
Utilities are emergent.
Maybe the laws of physics aren't emergent.
Some people will say even those are, right?
Like, you know, we live in a universe with this constant
because blah, blah, blah, right?
So, but to first approximation,
every single thing that we've been talking about is emergent.
We make a distinction between emergent and designed
because that is anthropomorphic, right?
Is this things that we do are not emergent?
Actually, no, when you nudge something
that is an emergent behavior, right?
We are emergent as well, right?
So everything that is human, you know,
so here's a very good way, I think,
to think about a lot of things
which I first saw, you know, in Richard Dawkins,
which is although he really didn't go into this
and I wish he had like this notion
of the extended phenotype, right?
Technology is our extended phenotype.
So all these things that we do, right?
All these things that we build,
including AS and whatnot,
they are extensions of our phenotype.
So if you take the long view, all of, you know,
technology is the continuation of biology by another means.
So when you make this distinction
between emergent and not emergent
and top down and bottom up, it's all emergent.
Interesting. Well, we recently did a show on emergence
and it's a topic of interest to me personally
and there's weak emergence and strong emergence
and there's, you know, like the view of weak emergence,
so there's some, you know, surprising macroscopic phenomena,
maybe something which transiently emerges
and Wolfram would add in the whole, you know,
computational irreducibility angle.
And then with the strong emergence,
Chalmers would say it's something
which is paradigmatically surprising.
It's something which is not deducible
for many fundamental truths in the lower level domain.
But I just wondered, like, how do you think about emergence?
Well, I think that is a very, the distinction
between weak and strong immersion is a very useful one.
Right.
And I would actually phrase it in slightly different terms,
which is starting from physics, right?
I think most physicists and scientists believe in weak emergence.
Well, could I, could I add that Sabine Hossenfelder
had a paper and she frames it with this idea
of the resolution of physical theories.
So like, like a lower resolution theory
as weakly emergent from a high resolution theory.
Well, exactly.
And, you know, like, I like Sabine,
but this is not her idea, right?
This far predates all of us here, right?
Yeah.
And again, it's, it's a very interesting history
and a very important concept.
Now, so my point was that I think few people have a quarrel
with the notion of weak emergence
in the sense that, you know,
I can give you a theory of everything
in the form of whatever string theory
let's take a candidate, right?
But no string theory claims that that's a theory
of everything in the sense that like now,
to study biology or psychology or sociology,
you should just study string theory.
No one believes that, right?
There's actually interesting things to be said there,
but, but let's not, let's look at,
let's not go there for a second, right?
There are these levels that emerge weakly
in the sense that they are determined by the lower levels.
They're just so much more complex
that you're better off focusing on the menu.
Now there's this other notion which to me
is the really interesting one,
which is that there is, there are phenomena
that are at the higher levels
that are just not reducible to the lower levels, right?
So the true emergent is in some sense
is someone who believes the latter.
And now you can ask the question,
like do you believe in that or not, right?
And I think to give the very short answer first
is that ultimately there's probably no way of knowing.
But pragmatically, you're actually probably better off
treating the world as if it has strong emergence.
And now strong emergence is actually
a very strong segment to make is to say,
and by the way, going down to the lowest levels
to make things very clear,
you don't need to think about biology or society
or consciousness or anything.
Condensed metaphysics, right?
The particle physicists tend to believe
that what they do is what everything reduces to.
You talk to the condensed metaphysicists.
This was actually an interesting discussion
that I had with Scott, you know, Aronson,
because like he was very much on the,
we're both computer scientists,
but he was very much on the side of the particle physicists,
I don't know very much on the side of the condensed metaphysicists.
What they will tell you over and over again,
they see is things that you cannot explain
using quantum mechanics.
And now people say like,
oh, but you can always explain things in quantum mechanics.
You just haven't done the calculations.
But the point is precisely that you can't do the calculations, right?
The calculations are chaotic.
I have a theory, I can come up with 500 theories
of these phenomena and semiconductors and whatnot.
And like, I never actually get to test them
because the computations diverge before I get to test them.
So for all intents and purposes, it is strong emergence.
Whether truly that came from below is unanswerable
because you can't compute the predictions.
Well, we spoke about that.
So I think Keith would call that semi-strong emergence,
which is like, you know, whether it's computationally reachable
from the lower resolution to the high resolution
to the lower resolution.
But no, Sabine in her paper, A Case for Strong Emergence,
she was talking about singularities
as being a really good example of what might be strong emergence.
And the philosopher Mark Badau, I think, said that
strong emergence is ridiculous.
It's basically an affront on physicalism.
Well, certainly, you know, strong emergence and physicalism,
or let's just call it reductionism, right?
Reductionism, yeah.
Strong emergence and reductionism are incompatible.
Yeah.
And we scientists tend to be reductionists, right?
Now, at some level, I'm both a reductionist
and someone who is willing to believe in strong emergence.
Again, I don't believe in strong emergence.
I just don't see a way to disprove it, right?
And like, you know, if there's an empirical way
to distinguish semi-strong from strong emergence,
I'd be very interested to know what it is.
But now, I think the thing that is very important
that a lot of people, including a lot of physicists
and scientists don't see is that we have this hypothesis
that everything can be reduced to the laws of physics
as we know it.
We should not forget that it's just a hypothesis.
And it's a hypothesis that, again,
counter to a lot of people's say,
is very, very, very, very far from established.
And usually, people say like, oh, but, you know,
look at all the successes of the laws of physics and blah, blah.
And then I say, like, you know,
putting on my machine learning hat,
the sample that you've used to validate the laws of physics
is extraordinarily biased in the direction of simple systems.
OK, so you can't make this claim of if the data was IID,
I could say with great confidence,
these laws apply universally.
But I haven't done it.
It's more like I've just landed in a new continent
and I've sealed up all the rivers.
And I say, I know what this continent looks like.
You've never climbed the mountains.
You've never gone in the jungle.
So like this notion that the laws of physics
capture everything about daily life,
we just don't know how exactly.
Maybe it's true, but it could also
equally well be completely false.
Brilliant.
Well, you gave a bit of a hint to this earlier, actually,
because you used the word relationism, right?
Which is basically the...
Or relationalism.
Relationalism.
Maybe it should be shortened to relationalism.
Relationalism.
But yeah, I think Rosen is a great advocate of this,
and he has a whole category theory calculus
for describing living systems.
And also we spoke to Bob Koek,
the quantum physics professor from Cambridge,
and he was talking about this concept of Cartesian togetherness,
which is another category or framework.
But I just wondered, does that inform your view?
Well, relationalism, at least in one way
of defining the term, very much informs my view, right?
And one way to come at this is to say,
the world is not made of independent entities.
Actually, let's just start with machine learning,
which is a very concrete way to look at this.
A very large part, maybe even the largest part
of my research in the last 20 years,
has been to do away with the assumption of IID data, right?
That the world is made of independent entities,
in particular, society is made of independent agents,
et cetera, et cetera, right?
Now, we make this assumption,
both as human beings, to some extent,
and certainly very much so in science,
because it makes life easier.
The math is way, way, way easier when you assume independence.
But it's a blatantly false assumption, right?
Unfortunately, a lot of, for example,
economics prominently has embedded in it this notion
that the world is a bunch of independent agents,
and it just doesn't work like that.
And moreover, it's a distinction that is full of consequences.
A society and economy is a network of agents,
and almost all the action is in their interactions.
Until you really start taking that seriously,
you really don't understand the world.
Again, I have no quarrel with classic economics
as a first approximation.
It's exactly what it should be, right?
But then, and by the way,
you should also not just throw it away and say,
like, oh, this is garbage, like some people say.
You have to go the next stage.
It's actually now we have the mathematical
and computational tools to do,
and understand it as being a system of interacting agents.
And all of the questions that we are talking about,
including in evolution, even in physics, right?
A piece of condensed matter is a network of interacting,
spins, et cetera, et cetera, you name it.
So the relations are at the heart of it,
and moreover, like as I said,
a lot of my work is we now have the representations,
the learning inference algorithms to handle things
that are big piles of relations,
and the whole world is better understood in those terms,
and we just need people to catch up with that.
You know, once you do that,
you get into things that can easily be
computational, intractable, and so on and so forth.
But there's a lot of things that we can do there
and a lot more that we'll do.
So at this level, I think relationalism
is really should be a cornerstone
of our understanding of the world
in a way that it hasn't been in the past.
Okay, and which existing complexity science brings to mind?
But I mean, which existing techniques and areas
can folks look into to take that on board?
Well, you know, Markov logic,
which is what I developed for this purpose essentially,
and I do think, you know,
this is my talking about my work,
so you should naturally be suspicious,
but I think it's the best that we have,
and I think by a wide measure,
compared to anything else that we have so far.
Okay, and can you sketch it out?
Yeah, so to sketch it out in the simplest terms, right,
we want to combine all the traditional goodies
that we have from assuming the world is IID
with the power to model, you know, relationships,
there are themselves potentially very complicated.
The way we do the Markov logic is,
there's the logical part.
We actually do not need to solve a new,
the problem of how to represent
and do inference with relations.
We have first order logic for that.
First order logic is the language of relations.
That's actually the term that is used, right,
and how the relations depend predicates.
Sometimes they're called predicates,
but let's just call them relations, right?
We have a formal language to talk about relations.
And by the way, essentially all of computer science
can be reduced to that.
You give me your favorite, you know, whatever,
knowledge representation, data structure, et cetera, et cetera.
And I, anybody who knows can immediately say
how to do that in logic.
So that's one part.
The other part is the statistical, you know,
machine learning probabilistic aspect of the world, right?
And then again, going all the way back to physics, right?
All of these things that we deal with
are essentially special cases of what are
variously called Markov networks,
which is where the name Markov comes from.
Or graphical models, or log linear models,
Gibbs distributions, Boltzmann machines, right?
All of these things are essentially the same, right?
That whole neck of the woods is captured by Markov networks.
Let's call them that.
And Markov logic is combining Markov networks
with first order logic in a single language,
which you can now do everything with.
Okay, okay.
So just to like push back a tiny bit.
So in the past, we've tried to create,
let's say things like psych,
which is a knowledge representation of the world.
Folks like Montague have tried to do semantics
using first order logic to set some,
you know, varying degrees of success.
And then we have the grounding problem
we were just talking before about, you know,
like even Searle said this,
that you have kind of epistemic objectivity
and subjectivity and some things are observer relative,
like even economics is observer relative.
So with this kind of formalism, how would that work?
That's so very good.
The problem with or the main problem
with a lot of these things that you mentioned,
like, you know, certain types of semantics and whatnot,
that are based essentially on first order logic, right?
Is that they're too brittle.
In fact, the problem with symbolic AI is that it's too brittle.
And this is exactly what Markov logic fixes.
It fixes it by making it statistical.
When I give you a logical statement now,
I'm no longer, for example, simple logical statement,
you know, a smoking causes cancer, right?
In English, this is a valid statement.
Smoking does cause cancer.
But actually, once you translate it to logic
for every x, smokes of x implies cancer of x,
it's false because some smokers don't get cancer, right?
What this really was meant to be all along
is a statistical statement that says,
smokers are more likely to get cancer.
So the way we overcome a lot of those problems
is precisely that we take all of this logic,
which again, the language exists,
we don't have to change it, we can, but we don't have to.
And we make it statistical.
As a result of which, it's no longer brittle.
Or at least now it's only as brittle
as machine learning and graphical model or not.
It's not as brittle as, you know, traditional symbolic AI.
Okay. And so we speak into a lot of Go-Fi people
and I mean, Wally Subba, for example, he's a rationalist
and what's interesting about the rationalist
is they hate any form of uncertainty, right?
They think in absolute binaries.
You either know it or you don't.
No, I mean, let me push back on that.
There's this, again, you need to distinguish, you know,
a general field or idea from its subtypes, right?
There is a type of rationalist.
That hits uncertainty, big mistake, big, big mistake.
There's a type of rationalist that, you know,
uncertainty is what they, you know, like,
an uncertainty calculus is a type of rationalism.
And some of the best, you know, AI, philosophy, etc.
is just that.
So there is no incompatibility at all
between rationalism and uncertainty.
In fact, if rationalism, if being rational is maximizing
expected utility, notice the expected in there, right?
You cannot be rational if you ignore the uncertainty.
Interesting.
Okay, but then what about the resolution of modeling?
I mean, smoking is a really good one.
So us humans, we anthropomorphize things.
We understand the world in macroscopic terms
using macroscopic ideas that we understand.
And that kind of leads to a certain type of modeling.
And that modeling presumably would be represented
at that resolution, you know, using this formalism.
Sure. And what's the question?
Well, it seemed, again, like I'm intuitively suspicious
that we were just saying the world is a complex place.
And with a lot of causal modeling, for example,
a lot of the art is understanding what is relevant
and what is not relevant.
What is relevant might just be kind of, you know, relevant to us.
No, well, what is relevant is what is relevant
relative to your utility function.
Okay, again, it gets back to that precisely.
The whole problem is that the world is infinitely complex
and we have only finite computational resources,
whether it's in our brains or our computers or whatever, right?
So now what do you do, right?
You are forced to oversimplify the world,
not just simplify, but oversimplify, right?
But now the whole art, that's actually a good word to use,
even if it's done with computers,
is how do you not only oversimplify as little as you can,
but pick out the simplifications that are least harmful to your objective.
By the way, the art of the physicist,
physicist would tell you, is precisely doing this, right?
Physicists are very good at deciding what to simplify.
And in fact, almost, I think at some level,
almost any good scientist, this is what they do, right?
So, and now how do I decide what and how to simplify
is by relevance to my utility function, right?
I want to ignore parts of the world
that do not affect my utility function, number one, right?
And for example, the notion of conditional independence,
which is the foundation of graphical models,
that's what the whole idea is.
It's like, once I know these things,
I don't have to know about those others.
Thank God, right?
Okay, but if Ken Stanley was here,
he says that the great thing about evolution is it's divergent,
it's not convergent, it's discovering new information.
And my worry is with a system like this,
with any form of anthropomorphic design,
would inevitably become convergent.
And it might look like, oh, those things over there
that we're ignoring don't matter,
but actually they might really matter
if they got introduced into the utility.
Well, I wouldn't say that maximizing expected utility is anthropomorphic, right?
In fact, it's one of the least...
I think maybe there's some degree of anthropomorphism
is almost anything we do,
and the progress of science is becoming less and less anthropomorphic,
and we should keep pushing on that.
But I would say that maximizing expected utility
is one of the least anthropomorphic things we can do.
Well, this is actually a really interesting point,
because one of the key tenets of the rationalist movement
and their conception of intelligence,
because all of the other definitions of intelligence
are anthropomorphic.
So, you know, there's based on behavior, capability, AI,
principal function is a big one, you know, from Norvig.
And this is the principal based AI,
which is just making rational moves.
So why is there such a push to be as, you know,
to be as the least amount anthropomorphic?
Oh, the push is not to be, at least in my view,
being less anthropomorphic is not a goal.
That's not the goal.
The goal is to be as accurate and complete as we can
in modeling the world, right?
We're just trying to understand the world better, right?
For whatever purpose, maybe for its own sake,
maybe for the purpose of the utility and the evolution and so on, right?
But that's the goal.
The problem is that,
and this has been the problem since they won, right?
They won of humanity,
is that because we anthropomorphize the world,
that gets in the way of understanding how it really works, right?
If I say the wind is some God blowing, right?
I understand, right?
That's all they could think of.
But it's a big obstacle to understanding what the wind really is,
like there's a pressure difference, et cetera, et cetera, right?
And we've done away with a lot of anthropomorphism.
By the way, one of the problems that we're always having
is that it's always pushing back, right?
You know, there's always, you know, again,
intuitively we have a very strong tendency to anthropomorphism,
as much as science broadly construed as a great victory,
it's always in danger from this, right?
But even within science,
we've gone from doing away with the obvious forms of anthropomorphism
and anthropomorphism
to having many things that are still there
that are less obviously anthropomorphic, but still are, right?
But if there's something anthropomorphic that's actually is accurate,
then more power to it.
Interesting, yeah.
And then I guess we have so many cognitive priors, right?
In our brains that give us a cone of attention,
which is completely anthropocentric.
Well, very good.
So those priors, and maybe a better term is heuristics, right?
Our brains are full of heuristics that evolution put there
for a good reason, because those heuristics work, right?
But they are heuristics.
So they have failure modes, right?
And we need to understand what is that those heuristics really are getting at
so that we also, so that we use them when they're good.
But then when they're not good, we use something else.
Brilliant, brilliant.
So Pedro, we're here at NeurIPS this week.
And could you just like sketch out some of the some of the things you've seen?
And I also know that you're a huge fan in that there's a Neurosymbolic algorithm
that you want to tell us about.
So let's let's hear it.
So I indeed, I've been enjoying NeurIPS this week.
One of the big things in AI in the last several years has been Neurosymbolic AI,
which you probably will not surprise by the fact that I very much believe in.
So and I believe this since I was a grad student
and the whole idea of Neurosymbolic AI was something that nobody was interested in, right?
And now suddenly everybody is, which I think is a good development.
And this is the idea that if we really want to solve AI by some definite,
if we want to get to like human level intelligence, etc, etc, we need to have both,
you know, like, for example, deep learning is not enough, right?
There are symbolic reasoning capabilities that we have and that are essential.
And we need to get them.
And I think, you know, intelligent connection is like, I don't know,
Yoshua Ben-Joe, you know, Yanle Kunase, they don't disagree with this.
But one way to look at this is say, we're just going to realize those,
you know, capabilities using purely connectionist means, right?
And what I see happening in that direction,
unfortunately, is a lot of reinventing the wheel.
So I do think, you know, symbolic AI got wedged for some reasons, including brittleness.
And, you know, and we have learned from that at the same time,
they did discover and understand a lot of things that are extremely relevant.
So it's just not good science to ignore it.
So I'm working on an approach to combine, you know, symbolic AI with deep learning.
Again, this is a popular exercise, there are many interesting approaches out there.
As much as I sympathize with them,
I think they're all very far from solving the problem.
They are over complicated and not powerful enough.
So, you know, I've been working on an approach called TensorFlow logic
that I do believe is as simple as it can be and as general as it can or needs to be.
And this, you know, it really is a deep unification of the two things
in the sense that it's not just that you combine them using, you know,
a neural model that causes symbolic one or vice versa,
which is a lot of what these things that you have today do.
And a lot of the claims that like, oh, this system is neuro symbolic, which it is.
It's like, you know, AlphaGo is neuro symbolic because some of what it does is symbolic.
But I'm talking about something much deeper, which is once you start doing AI,
learning inference representation in TensorFlow logic,
there's just no distinction between symbolic and neural at all anymore.
Can you explain that?
So TensorFlow logic, I'm just inferring from that that the primary representation
of substrate is a continuous vector space. Is that right?
Are you encoding discrete information into the vector space?
So it's a vector space. Yeah.
Right. In fact, this was the original term that we had for this was vector space logic.
But then we changed it to TensorFlow logic because it's much more appropriate.
But it's it's vector space in the abstract algebra sense of vector space,
not in the traditional, you know, vectors of numbers.
But anyway, so as the name implies, right, TensorFlow logic is a combination or unification
of tensor algebra on the one hand and logic programming on the other.
So is it similar because Bob Koeck had a similar idea using like tensor outer products?
Is it that kind of?
It's related, but I think it goes well beyond.
Okay.
And the basic idea is actually pretty simple.
And it's just the following, right, without going into too much, you know, technical detail.
All of deep learning can be done using tensor algebra.
Yeah, you know, plus univariate nonlinearities.
Right. So we've got the tensor algebra to do that.
All of symbolic AI can be done using logic programming.
And moreover, it has been done using logic program.
So if you can unify these two things, this part of the job is done.
Right. And as it turns out, you can unify them shockingly easily because a tensor
so tensor algebra is operating on tensors, you know, in that logic, so logic programming,
and then for learning in that logic program and symbolic AI, they are all operating on relations.
Yeah.
Right. So what is the relationship between the tensor and the relation?
Right. A relation is just this and efficiently represented sparse Boolean tensor.
So at this point, we actually know that the foundation of these two things is actually the
same. If your tensor is Boolean and is very sparse, now I'm better off representing it with
a relation, but at a certain level of abstraction, nothing has changed.
Right. So by this prism, you can look at logic programming and logic programming is doing tensor
algebra.
Okay. Just help me understand this a little bit. So, you know, the main criticism of using a neural
network as a combined computational and memory substrate is that it's a finite state automator.
So without having the augmented memory like a Turing machine, you can't represent infinite
objects. That's the main reason the symbol is, you know, that's the main argument I used.
So wouldn't that argument still be leveled against you?
Well, no, because I'm glad you brought that up because there is a very common misconception.
If you realize that there is no such thing as infinity, right? And in particular, there is no
such thing as an infinite memory. That problem doesn't arise. So there's the, so the, unfortunately,
a lot of theorists, including computer theorists, they foster this misconception, right?
There's the Chomsky hierarchy, right? With finite automata at the bottom and Turing complete,
you know, Turing machines bubble at the top, right? If your Turing machine has only a finite
tape, it's a finite automata. So everything is just finite automata. Let's get that out of the way,
right? A lot of what people do is like completely mistaken because of that. Now,
the fact that everything is finite automata does not mean that everything is equally good.
Some representations are far more efficient, compact, etc., etc., for certain purposes than
others. And the whole game here is that like, I'm not going to solve a finite automata. The question
is like, what do I need to do? Not because I need to go to a higher level of Chomsky hierarchy,
because in reality, they don't exist. But because, you know, I mean, if you have infinite resources,
you could solve a gap with a lookup table. But would you, would you not? I mean, for example,
there was this DeepMind paper that mapped architectures to different levels of the
Chomsky hierarchy transformers, I think were, you know, FSAs, RNNs actually were one step higher.
They could represent regular languages and they got context-free languages. I mean,
do you think there's any meaningful distinction between those language levels?
As I said, there is a meaningful distinction, but it's not the distinction that people usually make,
because once you, I mean, you can debate whether the universe is finite, but certainly computers
are finite. So as far as anything that you're going to run on a computer, there truly is no
distinction at this theoretical level between a Turing machine and a finite automata. That does,
so like, I can reduce and people have, there are papers reducing, you know, any of these things to
any of the others, right? It's like it's a fairly trivial exercise. So at that level, those distinctions
are completely meaningless. However, they are meaningful in the sense that for many purposes,
I am better off having an RNN than having, you know, a transformer. And for many purposes,
I'm better off. So like, let's take, you know, propositional logic versus first-order logic,
right? If there's no such thing as infinity, first-order logic is reducible to propositional
logic. But that does not mean that it's useless because it can represent a lot of things exponentially
more compactly than propositional logic. If I want to represent the rules of chess in first-order
logic, it's a page, right? If I want to represent them in propositional logic, it's more pages that
you can have. Okay, well, I think that that's a very, very good point. But I mean, just, just
a devil's advocate from the psychologist, you know, do you remember that, that photo,
Felician Connectionism critique paper, arguing productivity and systematicity? Productivity
is all about the infinite cardinality of language. I mean, presumably you would agree that language
has an infinite cardinality. No, well, again, another instance of the same problem. Productivity
is very important. But the point to just be a little precisely for a second is, is to be able to
generate a vast number of things beyond the ones that you started with. Vast, not infinite. In fact,
mathematically, infinity is not a number. Infinity is just a shorthand for something that is so
large that it doesn't matter how large it is. Okay. I mean, at the end of the day, I'm not a
mathematician, but surely mathematicians would push back on this because, you know, infinity is,
is a quantity in mathematics. No, I mean, again, people in every field, mathematicians, physicists,
computer scientists are all are often guilty of they, they, they, they have this notational shorthand
or like, you know, this terminological shorthand that serves them well. But then they, and then
they use that and then the newer generations come along and the, and the public also, right,
they don't even realize that what's being talked about is a little bit different.
Infinity is a perfect example. Any serious mathematician will tell you that infinity does
not have the properties of a number. So for example, if I multiply infinity by 2, I still
get infinity. There's no number that that happens to, right? So infinity is, is not a number, right?
When I say infinity is not a number, mathematicians might quibble about the way I'm stating it,
but this is a, this is a mathematical truth, right? Infinity truly isn't, I'm being colloquial,
of course, when I say that it's a shorthand for something that is so large that it doesn't
matter how large it is. When you take limits, you know, in calculus in anything and the limit of
this blah, blah, as I go to infinity, this is exactly what I'm doing. I'm going to the point
where I'm saying like, at this point, it doesn't matter how large the number is, the result will
be the same. And in this way, infinitely is an extraordinarily useful concept. So I'm not here
to rail against infinity. I'm just saying like, we really need to understand, I mean, like, let me
give you a very banal example, right? From the point of view of, you know, what to have for lunch,
right? Because some things cost more than others. Elon Musk is infinitely rich. He does not have
infinite money. But it makes no difference whatsoever whether he has whatever 100 billion
or 200 billion to what he's going to have for lunch. You know, like a street person who has
$5 to them, like their fortune is not infinite, because it very much matters what lunch costs,
right? So this is the real sense of infinity, which we can and should use, but we shouldn't
confuse it with like, oh, but then your, you know, like your formalism is incomplete because it
doesn't encompass infinity. Yeah, it doesn't need to infinity doesn't exist.
Okay, okay. Well, let's come at it from the other from the composition, you know,
compositionality and systematicity. So that's all about being able to do, you know, like their main
argument was when you have a symbolic representation, you can kind of reuse the previous representations
downstream composition, compositionally. And when you take a discrete symbolic representation,
and you kind of encode it in the envelope of a vector space, you have a real problem doing that
because it's now like, it's irreversible that transformation, right? You can't go back to the
original variables. Well, it is reversible if you realize that all those real numbers are actually
finite. Right? So notice that real number, there's nothing less real than a real number,
real numbers are imaginary, right? Real numbers are numbers with infinite precision, which is
a monstrosity. And many people have said this, including mathematicians and physicists, right?
The notion of an infinite, of a number with an infinite number of digits is just monstrous.
And again, in particular on a computer, even if you use, you know, you know, like numbers with
unlimited floating point precision, right? It's limited by the size of your memory. So this transfer
from which is actually very important that again, that's what tensor logic largely is about,
from purely symbolic structures to embeddings in a vector space, right? That vector space
is still finite. So there's actually nothing irreversible about what happened there.
Interesting. Okay. So how can people, you know, find out more information about this? And can you
just sketch out, you know, just, just to bring it home to people where they could actually use it
and how it would be, you know, better than what they can currently do?
Right. The answer to the first question, unfortunately, is this is not published yet,
but hopefully it will be soon. So for the moment, there is no very good place to point people to
unfortunately, but that hopefully will be fixed soon. The question of where to apply it is,
our goal for this is that this should become the language or hope, I should say our hope,
is that this will become the language we're doing just about anything in AI. So for example,
if what you want to do is actually nothing symbolic, but you just want to build a convent,
you can express a convent incredibly elegantly in tensor logic. Like if you think of, for example,
tensor floor or PyTorch versus NumPy, right, they allow that thing to be said much more compactly,
compared to tensor logic, they are as bad as NumPy is compared to them. Right. Same thing on the
symbolic side. But of course, the real action comes in all the problems where you have both
components, the problem with all those problems, which ultimately is every problem in AI, right.
You're always like, what happens today that is very frustrating. And that's what we're trying to
overcome is like, you start from one of these sides these days, mainly the connectionist one,
which you have a good mastery of. And then the other side, for example, the symbolic one,
the knowledge representation, the reasoning, the composability, you just hack. Yeah. And your
hack solution is terrible. You're like, you're inventing the wheel, you're making it square,
you're trying to make it turn, but it's square, right. It's just, you know, it's a disaster.
And with tensor logic, you can actually have a very well founded, very well understood
basis on either side. So now you don't have to hack either side. Now there's, of course,
still things that you're going to have to hack at the end of the day, because at the end of the day,
you know, AI is intractable and things are heuristic. But this, you know, is,
you know, you know, this notion of a tradeoff that is very important in engineering. Like,
people have been exploring different points on this tradeoff curve. The point of tensor logic is
that whatever your application is, we're moving you to a better tradeoff curve. It's still a
tradeoff curve, but it dominates the old one. For any given X, you have a better Y and vice
versa. Okay. And just help me understand, because we'll move over to, you know, the discrete program
search and some of Josh Tannenbaum's work in a moment. But there are two schools of thought,
right? There's discrete first and there's continuous first, you're on the continuous
substrate. But usually the reason for the continuous substrate is stochastic gradient descent,
learnability, et cetera, et cetera. And like, help me understand. So are you saying we start
with symbolic representation and then we encode it into the envelope? So where does learning come
into it? No, very good. So in tensor logic, you can do broadly speaking, two kinds of learning.
You can learn the structure of these tensor equations, as we call them, using inductive logic
programming techniques. Again, that whole technology is there. And then once you have that, you can
learn the numbers by the back prop in particular ways called back propagation through structure,
because the structure can vary from example to example, but we know what the type parameters
are. So all of the machinery of inductive logic programming and all the machinery of gradient
descent and deep learning or not, they're both there available to be used as you traditionally
have. Okay, what if I made the argument, though, that it's almost like the inductive logic, you
know, like the program search, that's the hard bit. So if you've already got the program, why do I
then need to put it into a vector space? No, actually, these are also at the end of the day,
in machine learning, we're always trying to learn a program of some kind, right? The question is like,
what is the easiest way to do that? And precisely the problem with ILPS with symbolic logic is,
that's really a couple of problems. One is that if all that you do, you learn programs that are
too brittle, and we don't want them to be brittle, right? And the other one is that each type of
search has its limitations. So in particular, in symbolic AI, including ILP, we tend to use a lot
of combinatorial optimization types of search, right? What we in AI call search is discrete search.
And that is good in some ways, but also very limited in others. The same thing is true of gradient
descent, right? And now to go to that for just a second. Gradient descent is not a continuous
optimization algorithm. It's not, right? Again, those real numbers are not infinite precision.
There's actually nothing continuous going on in the computer. Gradient descent truly literally
rigorously mathematically is a discrete optimization algorithm. It takes discrete steps.
The assumption that gradient descent depends on, which is that the infinitesimally small updates
do not hold. And moreover, in machine learning, as a numerical analysis, we are constantly dealing
with this fact that there's a mismatch between our mathematical conceptual model of the space
that we're working with as continuous with the reality of the computer that is not continuous.
So now this is not, but gradient descent still is a different optimization technique
with some very important advantages, in particular the key, right? The power of gradient descent
comes from the fact that to move from my current point to a better one, I don't need to try out
all the neighboring points because that takes order of the time of the neighboring points.
I have a closed form way to compute what is the best one, right? And then I move there.
And this is absolutely brilliant, right? Like we don't want to let go of that, right? This is,
you know, Newton's enlightenment is bright idea, right? The price of that is that in order to do
that you have to make this approximation, which again, calculus is an approximation. It assumes
that certain effects are second order and can be ignored. Now, ironically, when you learn a large
deep network these days, you're actually in a regime where they cannot be ignored, right? Because
these infinitesimal changes are not that infinitesimal because you take a finite step, right? The gradient
descent is always taking finite steps, which is why it's a discrete algorithm. And once you take
that finite step for any reasonable learning rate, the total effect of the approximations that you've
made typically swamps the step that you're taking. So the assumption of calculus that
gradient descent is founded on is actually false. Now, in some ways this invalidates a lot of our
intuitions. In many ways, and again, this remains to be resolved, a lot of why gradient descent works
better than people expected to is in fact that it's doing something else. It's doing stochastic
search partly because of the SGD as opposed to being matched partly because of things like this.
Okay, well, this is really interesting. A couple of places we can go. But first of all, I remember
you did the paper and that introduced elements of NTK theory as well, which might be an argument
against the discreteness of the optimization. But also, I wanted to trade off the two types.
Well, why is there an argument against the discreteness?
Well, isn't there a, with NTK, isn't there like a closed form solution? Doesn't that kind of like
erode the discreteness of the optimization? No, I mean, so there's several things here. But like,
if you have a closed form solution, absolutely brilliantly go for it, right? There's nothing,
having a closed form solution in no implies that it's continuous or discrete or any other thing,
right? So, let's say there was a closed form solution and it was like an infinite kernel when
it represented some neural network, doesn't that erode the argument? Well, so first, okay, so first
of all, in the work that, so the work that I've done that I think you're referring to is like,
I have a proof that every model learned by gradient descent is a kernel machine.
Yeah, right. And it's something called the path kernel, which is the integral
of the neural tangent kernel over the overgraded descent, right? Yeah. And now the neural tangent
kernel does not assume that your network is infinite. Most of the theory that people have done
with it assumes that the network is infinitely wide, but the definition absolutely does not
require that. And none of what I do, and in fact, that's part of why, you know, of its part is that
it assumes no infinity of anything. It's for any architecture that you use, and in particular,
you know, finite architectures. Okay, interesting. Okay, so hence the discreteness, but can we come
back to this contrasting of the discrete program search and the, you know, stochastic gradient
descent on a vector space? Now, in the vector space, there are certain characteristics, you know,
there are certain symmetries, and even though it's a discrete search through the space, I would argue
that it's still continuous in nature, it has certain characteristics. So contrast those two
forms of optimization. Precisely so. Exactly. I mean, I think you've put your finger in now.
The whole point of these continuous spaces, right, is not that they're continuous, because again,
that's, that's a fiction, is that they have a certain locality structure, yeah, that you can
exploit to very good effect. And this is exactly what we're going to send us, right? Now, that
locality structure doesn't have to be infinitesimal, right? You don't need points to be infinitely close
for all this to apply approximately. And again, they never are, and it's always an approximation.
Now, the question is, do you want to make these locality assumptions or not, right? Making them
buys you certain things, right? But it's also potentially unrealistic in some ways, right? Now,
this actually, to take a very concrete instance of this, think of space, right? We model space in
science and physics and in anything as a continuous thing, which it is not, right? Which is not to
say that, and by the way, physicists are coming to this conclusion, right? These days, the prevailing
views is that it's from big thing, is that like, it's, you know, space arises from entanglement,
et cetera, et cetera, like space is not the fundamental reality, right? And now, I think
that where this is inevitably going one way or another is that we realize that space is discrete,
right? But, and this is key, it has certain properties, including symmetries like translations
in variance, rotation in variance, et cetera, et cetera, that whole, approximately or exactly,
but if those hold a whole bunch of things like that, then you have, you know, your latent variable
structure, right, is very well approximated by our notion of continuous space, in which case,
it would be foolish to not use it, right? To formulate the laws of physics and to do computer
vision and so on and so forth. But at the same time, right, if we believe in it too literally,
we walk ourselves into a blind alley. So concretely, look at computer vision, right?
People in the universities of computer vision started out trying to do it with differential
equations and Fourier analysis and all of that could continue with stuff, right? Because that
was the obvious thing to do, right? And it failed. That doesn't work. That's why we need things like
deep learning and, you know, Markov random fields that are discrete grids that use, you know,
to model the images and whatnot, because you are, along with the approximate continuity,
you also often have large discontinuities. And if you can only model the world continuously,
you don't know what to do. And the problem precisely is that you have all these phenomena
that are like this, including, you know, in vision, but also in, in turbulence and condensed
metaphysics and so on, you've got to realize that there are discontinues and not try to shoehorn
them into continuity when that's no longer appropriate. Interesting. Okay. Well, can we bring
in ILP and can you contrast like the kind of function spaces that are learnable in both methods?
Yeah. So ILP, so let me actually preface this with the following. People in every one of these
schools of AI tend to have this view that I can represent everything in the world using my approach.
So I can like, look, prologue is too incomplete. So why do you need neural networks? But I can also
say neural networks are too incomplete. So why do I need prologue? And in fact, kernel machines have
a represented theorem that says you can approximate any function, blah, blah, blah, right? So everybody
has one of these represent their theorems, right? That says, I can represent anything, right? So in
particular, you can do, right? I mean, look, first, our logic was invented by, by Frege, essentially,
to, to model the real numbers. So it can almost by definition model real numbers, right? Anything
you might want to say about real numbers and, and weight and descent and neural networks. And in
fact, people have even done this. So you can say it all in, in logic programming, right? So why not
just do that? Well, precisely because certain things are much more easily done in other ways,
right? So what you have to ask about anything, but then about, you know, not the logic
program in particular, like, what things are well represented in this way, like compactly
represented, and then in such a way that learning them and doing inference with them is easy, right?
And those things are different for logic programming and for things like deep learning,
which is why we need a unification of both. So what is things like logic programming and
ILP good for, right? It's precisely, I mean, it's many things, but the key thing is,
it's precisely for learning pieces of knowledge that can then be reused and composed in arbitrary
ways. This is the huge power symbolic AI that connectionism does not have, right? It's like,
I learned the fact here, I learned a rule there. And tomorrow you ask me a question,
and I combine that fact, actually, several rules by rule changing, right? There's a whole proof
tree of rules that could have come from very different places. And I do a completely novel
chain of inference that answers your question. This is spectacular, right? And this is surely
court-wide intelligence is all about. And the symbolists know how to do it. The connectionists
don't. But if I was a connectionist, I'd be like, you know, I know if it was a good one,
and the better ones like Yoshio Benji are doing this, right? It's like, go and try to understand
what those people understand so that you can then not combine it with those other ideas.
Yes. Yeah. Yeah, I completely agree. So a huge part of intelligence is this symbolic,
you know, extrapolation. Yeah. So how do you bring abstraction into this? Because the thing
that I always get caught on is that the traditional go fi vision was to, you know, handcraft the
knowledge. And actually, what we need is dynamic knowledge acquisition. And we need the ability
to create abstractions on the fly rather than just what we do now, which is crystallizing
existing human abstraction. How could we do that bit? Well, abstraction traditionally was and
still is a central topic in symbolic AI, right? Like be precise. I mean, I think nobody questions
that having levels of abstraction, someone is very important. The only question is how. So if you
look at classic knowledge representation, planning, et cetera, et cetera, abstraction is all over
the place. If you look at things like reinforcement learning, and I mean, even like, you know, the
whole idea or hope of a convent is that it captures objects at multiple levels of abstraction,
at least to some degree. In reality, it doesn't, right? But that's what people are trying to do
and not quite doing, right? Well, good. Let's touch on that then. So I mean, certainly in
Jan McCoon's view, I spoke with Jan the other day, he's got this autonomous path, a paper. And,
you know, his system is learning abstractions, but they're abstractions which are deducible from
base abstraction priors, like objectness and, you know, basic visual priors. And so there's this
assumption that everything is deducible from the priors that we put into the model. But I have this
kind of intuition that abstraction space is much larger than that. Yeah. I mean, so I would even
say that if you arrive at your abstractions solely by deduction, you have a very impoverished notion
of abstraction. In fact, most of inductive learning is forming abstractions. And form abstractions at
the most basic level is something very trivial. It's like, I have an example described by a thousand
attributes. If from that I induce a rule that uses only 10, I've abstracted the way the other 990,
right? But if a symbolist was here, they would talk about intention versus extension, and they
would say that, you know, you're selecting from this infinite set of possible attributes. You
couldn't possibly represent all of the attributes in this. I mean, just to give you a concrete
example, you know, you could have a, a, a, a, a, a, a, you know what I mean? You can have like this.
Again, I hate to bring up infinity again, because that's always what these folks bring up. But
how could you select from a set that large? Well, I don't need to because it is finite.
But what I need to do is so, so, but there is actually a good example. And, you know,
infinity does not bother us at all, at all there, because what it's like, if my training set,
right, is a set of strings, and those strings are a, a, a, a, a, a, a, right? Going up to
whatever number you want to pick, like, you know, a million or a quid drill in, you know,
or a Google, right? Then R is your learning algorithm able to induce that the, the language
that these rules come out of, right? The grammar is, you know, it's a series of A's, right? You
and I can do that immediately. You know, most deep networks have no end of trouble doing that,
even though it's that basic. So it is a very good example of what symbolic learning and
reasoning can do versus connection is you don't need to go anywhere near infinity to actually
have that be a very elegant example. Well, let me bring up just one other, we've touched on a
lot of great things, right? There's one in this space of things that we've been talking about,
there's one that I think is very important, which I believe you're also a fan of. And I very much
am. And I think it's going to, you know, maybe you're going back to the question of what I'm
interested in that's happening at, at new reps right now or not. So new symbolic AI is definitely a
big one. Another big one. And to my mind, maybe these are the two biggest ones are most interesting
is, is what I call symmetry based learning. And these days is more popular known by the,
by the, by the name of like geometric deep learning and things like that. I tend to view
geometric deep learning as a special case of symmetry based learning. But this idea of,
I think, let me, you know, to go straight to the punchline, we know that, for example, AI and
machine learning in particular, have as foundations, things like, you know, logic,
probability optimization. And I think another foundation is symmetry group theory. In fact,
I was having, you know, dinner with, with Max Welling just the other day, who, who, of course,
have also interviewed and is, you know, like a great, you know, person in this area. And we,
you know, I think we have very similar views on this. Well, Pedro, yesterday, and Taka Kohen
was sitting where you were sitting. So there you go. Yeah. Again, I remember talking with Taka
Kohen, some ICML many years ago, where he published one of the first papers on this.
And I was like, and he seemed a little disheartened by the lack of interest that people had. And I
said to him, just wait, this is going to be big and we're there now, right? And it's going to be
even bigger, I think. But also, I think to become bigger and again, to jump straight to the punch
line, most of the work, including me, that people have done to date has been exploiting known
symmetries, like, you know, translation invariance is the quintessential example. For example,
we have something called deep affine networks that generalize coordinates to, you know,
rotation, you know, scaling, et cetera, et cetera. This is all well and good. But I think
if this is, and if you look at New York's today, for example, most is in that vein.
And there's a lot of good work to be done there. But if that's all we ever do, we will always
remain a niche in AI with certain very good applications, like science applications,
where we know that certain symmetries hold and whatnot. Max and Taka are doing things like that.
But I don't want to just do that. I really, you know, I'm trying to make progress towards
human level AI. And I think the key there is to discover symmetries from data.
Yeah. And I think most of us agree with this. It's a hard problem, right? But that's what we're
here for. We want to discover symmetries from data. And, you know, there's an interesting,
you know, discussion of how to do that, you know, I have a number of ideas and a number of people
have, then the power of discovering symmetries, right, connecting back to our early conversation
is that symmetries can, individual symmetries can be very easy to discover because they're
often very simple. But then, right, by the group axioms, axioms, you can compose them arbitrarily.
Yeah. Which means I can, for example, by learning 100 different symmetries of a cat
from 100 different examples, then I can compose them and correctly recognize as a cat
something that is extremely different from any concrete example of a cat that I saw before.
Could I push back on a tiny bit? So, I mean, in the geometric deep learning prototype book,
I mean, they spoke about, you know, the various symmetries of groups like SO3, you know, preserves
translations and angles, you know, like how primitive and how platonic are these symmetries?
And aren't they just like obvious in respect of the domain that you're in?
No, very good. So this is actually a key question. Symmetry group theory is one of them.
It's a central area in mathematics that it's a very highly developed and it's the foundation
of modern physics, like the standard model is a bunch of symmetries and so on. But the way,
and there is an exhaustive listing of what all the possible symmetry groups are, discrete ones,
you know, continuous ones, you know, so-called lead groups, etc., etc. So at that level, this is
not naive because people already have a handle on what the space is, right? But crucially for our
purpose is for AI, that's not enough because precisely because those, again, the analogy
with logic is actually a very good one here. First of all, the logic is to brittle, right?
And plain symmetry group theory, the way people have mostly applied so far,
is also too brilliant for the same reason. So for example, right? Something like, you know,
people almost always immediately come up with, so like, oh, I understand, you know,
I like symmetries with the light to recognize, you know, perturbed digits, but a 6 is not a 9.
So some, like, if you just take naive symmetry group theory and you say, like, well, arbitrary
composability, as I was just talking about, I was like, well, now you've just said that a 6,
you've lost the ability to distinguish a 6 from a 9, right? Now, what we need precisely is to
combine symmetry group theory with the other things like statistics and optimization and
say something like the following. The space of things that you can compose is unlimited. You
can have, you know, unlimited compositions, but for example, you pay a cost for composing more
symmetries. And now when you find the least cost path, and that's how you're going to match things,
or, you know, your digit becomes less and less probable to be in 6, the more you've rotated
it, right? So now we know how to do all of that very well. So we know symmetry group theory very
well. We know how to do all these probabilistic costs, minimizing blah, blah, blah things,
machine learning very well. We just need to combine it to the same way that we have previously
combined these things with first order logic. So I'm glad you brought in the cost that that was
really, really good. So there were trade offs everywhere. I mean, for example, if you want to
make the models more fair and, you know, prioritize the low frequency attributes on the long tail,
the headline accuracy goes down. Same thing with robustness. If you robustify a model,
the headline accuracy goes down. Same thing with symmetry groups. If you introduce other
symmetry groups, you know, that the headline accuracy goes down. So it all comes back to the
bias variance trade off at the end of the day. And, you know, where is the limit here? How much
can we optimize these models and what does good look like? The bias variance trade off is a very
useful tool, right? But it's not the deepest reality, right? The way to think about bias variance is
that, again, talking about this notion of a trade off curve, there's a trade off between bias and
variance, right, which is in some sense unavoidable, right? In machine learning, if you have finite
data, you're trying to learn powerful models, bias variance is a trade off. And it's a very
consequential trade off in the sense that, for example, the things that work best with small
amounts of data tend not to work best with large amounts of data, right? This is something that we
should all, you know, grow up knowing in machine learning. But so many mistakes have been done
because of that, because people study things in the easy or historically, that's all they had,
right? And then they're very surprised when something that seemed not very good, like, say,
deep learning, right, turns out to be better when you have a large amount of data, or they believe
in, like, silly things like, you know, Occam's razor version that, you know, accurate, you know,
simply is more accurate and whatnot. So a lot of mistakes have been made because of lack of
understanding of this. Having said that, what you really want is to move to a better trade off
curve between bias and variance, which you can, if you get at what the reality is, right? So the
real game in machine, once you're evaluating your learner and figuring out, you're like, how much
to prune and whatnot, or how much to regulate bias variance is very important. But before that,
the most important question is like, what we're trying to do here is figure out what are the
inductive biases? What are the regularities that the world really has, at least approximately,
that we build our algorithms on top of that? And then if you give me a better one than I have now,
I'll still have a bias variance trade off, but I'll be in a curve where for the same variance,
I can have less bias and vice versa. And that's where the real action is.
Oh, interesting. Well, I didn't quite understand that because bias and variance,
they are mutually exclusive. And I thought at first you were saying, well, if we understand
what the biases are better, the prototypical symmetries of the world we live in, then we
can have more bias without having an approximation error, basically.
The confusion arises because bias is a very unfortunately overloaded term.
Right. This is not even getting into the psychological notion of bias like in Danny
Kahneman's work, or even the sociological notion of bias like racial biases, gender biases and
whatnot. So we need to distinguish. I just used my bad, the word bias into completely
different senses, completely but not unrelated. That's the thing. One of them is the statistical
notion of bias. There really is a trade off between the two. There's a sum of squares,
blah, blah, blah. The machine learning notion of inductive bias, it's the preference that you
have for certain models of our others, which is really just another way of saying your priors,
whether they are assumptions or knowledge. Maybe actually instead of bias, they're like,
what you really want to do is figure out what are the priors? What are the model classes?
What are the preferences? The bias is a kind of preference that really line up with the world
in reality or the domain and therefore let you move to a better trade off curve
among statistical bias and statistical variance. Amazing. Well, Pedro, just tell us a little bit
about what have you seen at NeurIPS and how's the week been for you?
We've already touched on some of the interesting things that I saw,
in particular some of the areas that I'm interested in. The thing about NeurIPS is this,
of course, is that it's a vast conference. In the early days, I used to at least go through
the proceedings and look at the title and maybe the abstract of every paper. This is now impossible.
Now, these days, if all you do is try to walk through the poster sessions,
you never get to the end. I haven't been to a single poster session in this NeurIPS
where I actually got through all. I like to go through the poster sessions quickly once
and then just to see what's there and then go back to the ones that I found really interesting.
I haven't actually been able to even finish that walk through because they're so vast. You're also
running to people which is part of the point and talk and whatnot, but when there's 500 posters in
every session and there's 3,000 papers in the conference, it becomes very hard to find the
ones that are most relevant. Of course, an easy thing to do is look at what they, I mean,
something about NeurIPS this year that I honestly thought was absolutely terrible,
like a really, really terrible idea is that it's a hybrid conference and their idea of
a hybrid conference is that there are no talks. The talks are all virtual next week. Nips this
year to a first approximation was one big poster session, which I mean, to me, this is just an
incredibly bad idea. In that sense, I haven't gotten as much out of Nips by this point of the
conference as I would have in most years. There's also looking at the papers that were usually
selected as oral, but this time they call them oral equivalent because there are no oral papers,
but they still want to have that distinction. The number of those papers these days is 160 or
something, which is bigger than Nips and ICML some years ago. Usually from those papers,
some of them kind of like jump out at you as being great and very relevant. I've only looked at
them briefly, so don't quote me on this, if you will, but none of those have jumped out to me
as like, oh, yeah, this sounds like something really brilliant and that I want to dig into,
but there probably are many. I just haven't really had a chance to look at them yet.
Yeah. I mean, I have a similar reaction. I mean, it feels like we're at the point of saturation
and there are loads and loads of microvariations on the same idea. It's completely overwhelming,
but what I find is that it's a very social experience. When I walk through the posters,
I just immediately become engrossed in conversation and hours go by and I just think, oh my God,
what have I just been doing for the last year? That's the real point. The posters are very good.
It's like the grain of sand and the oyster. The poster is the grain of sand. The oyster is the
conversation that you have with the person at the poster or with other people around there.
To touch on another point that you made that I think is actually important.
New Europe's and ICML and so on are bigger today than they've ever been. Actually,
not strictly true because these recent lips, surprisingly, they tend to have gone down a
lot. We can and should ask why, but we need to scale. There are bigger conferences,
like the New Science Conference is one conference and it's 35,000 people every year and they make
it work. It's good to experiment. I think New Europe's at the scale that it is today can work,
but it is not working very well. One of the ways in which it's not working very well is that
we need to think a lot more. I don't understand this is working. It's hard and people have day
jobs or not, so this is not a criticism in that sense. We need to really work on making it easy
for people to find the papers that are relevant to them. Number one, number two, and maybe even
more important, there is more machine learning research today than ever, but in some sense the
diversity of that research is in some ways lower than ever. Another point that you brought up and
I think is very important to do with the scaling of New Europe's and the machine learning communities
that we have in just raw numbers, more machine learning and AI research going on today than ever
before by an order of magnitude. But in terms of diversity, there's probably less diversity in the
research now than there was before, which is a tragedy. I understand why people have kind of
like converged to deep learning. I'm a huge fan of deep learning. I was doing it before it was
cool as they say and whatnot, but the extent to which 90% of the community, not just in machine
learning but AI, is not just pursuing and not even deep learning, but a special type of deep
learning, which you might call applications of backprop, is extremely undesirable. We have
an infinite number of micro-improvement papers along a particular direction that is almost
certainly a local optimum, and we're just digging into that local optimum with ever more papers and
never more, you know, minimal publishable units when this large amount of manpower that has come
into the field or is moving around, we really need to have a greater diversity of research in
machine learning, within deep learning, within AI, and so like we are making very poor use of our
research, you know, manpower right now, and we see that very much at NeurIPS today.
Yeah, I mean, Sarah Hooker talked about the hardware lottery, you know, being stuck in a
basin of attraction determined by hardware, but there's also an idea lottery. It might just be
the case that NeurIPS historically has always been very connectionist anyway. I mean, maybe it
hasn't, right? That's one of the ironies, but it's something as well. I wasn't aware of that. Okay.
Oh, absolutely not. I mean, in fact, the joke is, right, that NeurIPS started in the 80s,
it was called Neural Information Processing Systems, and by the 90s, it should have become
BIPs for Patient Information Processing Systems, right? There was this study that they did at one
point of predictors of acceptance and rejection among words in the title, and the biggest predictor
of rejection was the world neural. Really? And this was very famous in the field, because
indeed, if you could, you know, 1990 something, you were submitting papers to NIPs with the
world neural in the title, you didn't know what you were doing. And then in the 2000s, right,
it became BIPs, or should have become BIPs, sorry, KIPs, Kernal Information Processing Systems.
And in fact, I remember having lunch with Yoshio Bingo at the ICML in Montreal in 2009,
and we were talking about this, right? The fact that every day kid, and, you know,
not a new paradigm, but another one of the same paradigm seems to now be on top, right?
And, you know, he asked, like, so what is the next decade going to be? And I said,
it's going to be DIPs, Deep Information Processing Systems. And then we both laughed,
and I could tell that I believe this, but he, Yoshio Bingo, was actually skeptical of this.
So, you know, the deep, little did we know, right? If somebody told us that, you know,
this is going to be on the page of the, on the front page of the New York Times,
in a couple of years would be like, what are you smoking, right? So the way to which this decade
has been DIPs is just mind-blowing, but looking forward, right? And to this point of, you know,
diversity in research approaches, I think if you extrapolate naively from the past,
the next decade will be about something else. And the trillion-dollar question
is what, what is that else going to be? Amazing. Okay. You watched Charma's talk, right?
Yeah. What's your high-level view? I thought it was a nice talk. I thought it was a very
appropriate talk for an opening talk at the conference. Actually, if New Europe's had,
like, some conferences, a dinner talk, right? Which is supposed to be interesting, but not as,
you know, deep or as technical as other. This would have been the perfect dinner talk for
New Europe's, because the topic is very current, right? Our machine's sentient. And, you know,
who better to talk about it than Dave Chalmers, right? The world's expert on, on, on, on consciousness,
right? And by and large, I thought the talk was excellent. In fact, you know, when journalists
ask me questions, you know, consciousness is like one of their top three, right? Along with
Terminator and, you know, Unfairness or something like that, right? And I will point them to this
talk because it kind of like lays out, you know, the, you know, the ground. And, you know, it's good
for people to at least have those things in mind. At the end of the day, so I think, of course,
the notion that Lambda was sentient is, you know, ridiculous, as, as most of us do.
You could ask a slightly more fine-going question was if, if, if, if, if consciousness is on a
continuum, right? Which I think Dave believes in. And if you believe in like this, you know,
IT theory and phi and whatnot, you know, like, phi is never zero, right? So there's always some
consciousness, right? Pensychism and whatnot. I'm not saying I believe in that. We could,
we could go into the, but like, if you believe in that, then you can ask, well, on that scale,
you know, where is Lambda? Where are these large language models? And, and, and surely higher than
previous AI systems, right? But in my view, still very, very, very far. And I think what you want
to keep in mind is that consciousness does not, does not increase continuously. Precisely,
there's these transitions where you go, you know, more is different is the, is the famous,
you know, phrase about emergence, right? Consciousness is very much an emerging,
you know, phenomenon. And I think what happens is that there are points at which your
consciousness will leap. Maybe a thermostat does have consciousness, like, you know,
or, you know, or purpose or whatever, right? Like, like people in, like people like McCarthy,
for example, had had had that as an example. But the amount of consciousness is minuscule.
And, and that, and the way I will put that is that these large language models still have not
passed that first threshold. Interesting. So, so in a similar way to some of the discussion
about large language models, there are kind of scaling breaks in the levels of consciousness.
I mean, Chalmers made the comment, though, that rather than it being a pure continuum,
he said that a bottle was not conscious, but then there was a kind of. No, yes. So very key
point. Scaling is part of it, but not only. It's not just that. So your cortex to first
approximation is a monkey brain scaled up, right? There was a module there that evolution
discovered, and it really paid to keep making more and more of it. And we can easily speculate why.
But the point is, so let me contrast two things, right? Which is true for consciousness, but also
for just AI in general. A lot of people are scaling believers and like open AI is the poster child
of this in a quite conscious ways. Like, we're just going to scale the heck out of things.
And then a lot of people, like, you know, Gary Marcus being a good example, they just
completely poo poo that they say, like, oh, no, this is a joke. Right. And I think the truth is that
scaling is good, right? Again, you know, part of what we are, our intelligence is scaling.
But the question is, what are you scaling? And the things that we're scaling today,
it doesn't matter how much we scale them, we never get to human level intelligence or consciousness.
So I think we need some fundamentally different algorithms, if you want to think at the level
of algorithms, or fundamentally different architect architectures, if you want to think
about it in a way, and then scaling those up at some point will give us consciousness.
If you live that it's possible for a computer to be conscious, but we're not there yet,
either in terms of the scaling, although actually scaling is actually the easier part of this way,
we're actually at the point where a computer can have the same amount of computing power that
your brain does, which was not the case before. But the bigger deeper problem, and the more
fundamental one is like, we need the architecture to scale. Right. And this is where I sympathize,
you know, with people like Jeff Hinton, who's just, you know, playing with, you know, ideas
using Mathematica and very small examples, which in some ways, sounds very underpowered,
but I think it's people like that, they are going to come up with the things that we then scale.
As in fact, it was David Roemmerhardt doing that kind of work that invented backprop.
Right. If he hadn't invented backprop, this whole industry would not exist. So
what I think is that the real backprop, the real master algorithm is not there yet,
and we need to discover that first. And then we, and then when we scale that up, which will not
be trivial, but will be much easier by comparison, then we'll have, you know, human level, intelligence,
consciousness, et cetera. Interesting. Okay. And so Charmes is a structuralist computationalist.
So, you know, he thinks information, not biology. And he's also a functionalist, right? So, you
know, which is very similar to behavior. And, you know, Hillary Putnam made the move that you can
kind of like represent a computation in any open physical system. And he kind of like used that
on, you know, if you follow that line of thought, it almost trivializes computationalism because,
you know, it leads to panpsychism very, very quickly. So, first of all, I mean, what's your
take on this idea that information could give rise to intelligence and consciousness?
So I agree, like most scientists, and I think in particular most computer scientists, that
to a first approximation, the substrate does not matter. And in particular,
you're not going to convince me that something is not conscious just because it's not biological.
There is no reason to think that only biological things can have consciousness. Now,
the deeper problem, and you know, indeed the hard problem, is that so as Dave Chalmers defined it,
so there's a basic fork here, which you've alluded to, which is,
if consciousness is subjective experience, then all these questions about consciousness are
ultimately unresolvable, because only I have my subjective experience. I know that I'm conscious,
no one can persuade me of the contrary. I don't even know if you are conscious, let alone some machine.
Right? So if consciousness is an intrinsic property of something that cannot be evaluated
from the outside, then we're doomed. We're never going to answer this question. And maybe that is
the case. Right? So I'm not saying that's false, and you need to always keep that in mind. But now,
if we're going to make any kind of progress, right, we need to look at what are, to generalize a well
known term, the external correlates of consciousness. Right? One of those which has been well studied by
people like Christoph Koch and so on, and I think that's a very good direction, is the neural
correlates of consciousness. Right? What goes on in your brain that correlates with consciousness?
And we've made a lot of progress with that. You can also talk about what are sort of like the
informational computational correlates of consciousness. Are there computational structures
that support consciousness and the ones that don't? I think that is also a useful thing to do.
Let's develop. It actually interests this panpsychism because it's not like everything is
consciousness just because it can compute. Some computations after this emergence and these,
you know, phase transitions may give rise to consciousness. Whereas others, it doesn't matter
how much of them you have, they will never be conscious. So I think this is also a very useful
way to make progress on this question and one to which AI versus, you know, a neuroscience or
psychology is very well suited to. Interesting. So on the functionalism point, and I think
Chalmers has been very, very consistent. He uses this kind of calculi to reason about
intelligence as well. So a system is intelligent if it can perform reasoning, if it can perform
planning, if it has sensing and so on. So we have this collection of functions. And then
he's kind of like moved this over to the domain of consciousness. So similarly,
if a system performs these functions and is used in a positive and a negative way. So some
functions would indicate an absence of consciousness and some functions would, you know, lead to the
presence of consciousness. And it's kind of like leading towards a, you know, touring test for
consciousness. I mean, do you kind of support that? That's a very interesting question. In fact,
you know, I was having dinner with Dave after his talk and I actually brought this up because it
wasn't clear from his talk. And I said, look, this is the answer that I usually give to journalists
when they ask me, you know, will machines ever be conscious and whatnot? And asked me a few,
and asked me if he agreed with it and actually expected him to disagree. But I think again,
don't want to put words in his mouth, but that he agreed, right? And the answer is the following,
is that human beings, right? As we've discussed, have an amazing tendency to
anthropoformize things. It's reasoning by analogy. And what happens, I used to say,
this is what's going to happen at this point is this is what is already happening is that
as soon as a machine behaves externally, even vaguely like it's consciousness, we immediately
start treating it as if it's consciousness. So if you look for 10, 20, 50 years from now,
we will just treat AI's as if they're consciousness and people won't even ask that question.
They will assume AI's are conscious in the same way that we assume that each other,
that we're conscious, right? But then, and so like from that pragmatic external point of view,
maybe the question is answered, right? But you could be a philosopher or like sort of like a very,
you know, rigorous, you know, technical person and so like, no, no, no, no, I really want to know
if things, they may look, you know, conscious from the outside, but are they really, right?
But that question, as far as I can tell, unfortunately, at the end of the day is probably
unanswerable. Now, there's a middle ground between these two things that maybe is where
we'll wind up. And to me, sounds like probably the best thing that we're going to be able to do,
which is that like, our understanding of the neural informational, et cetera, correlates
of consciousness evolves to a point where we have the feeling that we do understand consciousness.
It's not just the late person calls this consciousness even though haha, it's not like
lambda is not conscious, you know, poor bozo, et cetera, et cetera. It's like,
you know, there are many analogies to that in the history of science. There used to be a lot of
things that were like magical, right? And we were like, oh, we're never going to stand like life was
magical, right? Life did not obey the laws of physics. It's just something else, right? This
sounds laughable right now, but it wasn't laughable at all then, right? And now, it's not like we've
understood everything about life very far from it. When you say like, there's DNA and their
cells and then this is how it all arises, right? And I think we're at the point in consciousness
where it's to like, oh, consciousness is some so beyond us, right? I think we will get, you know,
there will be a structure of DNA moment in the history of the study of consciousness.
And I think, yeah, I think things like Phi and this, you know, IT3 and whatnot,
they're very brave attempts to make progress in this direction. I think, you know, like Julia
Tononi in a way is, you know, very deluded in thinking that he has nailed what consciousness
is, right? I think, you know, Phi maybe is an upper bound on consciousness, but with steps like this,
hopefully at some point, and very much with the help of AI, right? AI is really useful for this,
because it's a brain that might be consciousness that we have a lot of control of. And you can do
experiments that you can't, you know, with people, right? So I think we will make at least some progress
in that direction for sure. Maybe to the point where we feel that, yes, we do understand what
consciousness is, we're not asking ourselves that question anymore. And then we can point to things
and say, this is consciousness, this is that kind of consciousness, that amount of consciousness,
and so on. Yeah, that's really interesting. I agree, we're making a lot of progress in getting
a handle on this. And although the biggest game in town is still the computationalism game. And
as you say, historically, the only alternative was mysterious. And my friend, Professor Mark
Bishop, that he said that that's one of the reasons why he's become interested in the
forays in cognitive science, because for the first time, it's given him a kind of robust
alternative to computationalism. But just coming back quickly, you know, as Charlie's
reference, Thomas Nagel, you know, which is that it is something it is like to be a bat.
What do you think about that? So I'm not sure your question is, but let me check.
Well, what do you mean? Do you agree that there is something it is like to be a bat?
Oh, absolutely. Right. So there is more and more than that, right? There is something that it's
like to be a bat. And it's very different from being a human, right? And we grossly underestimate,
right? Again, we do this thing that again, it's a heuristic, it works very well as like,
we project ourselves into the bat, because what else could we do, right? But then what you see
is a bat seen through the mind of a human, right? And in fact, there's this famous, I would say,
even more famous, you know, you know, notion from, from Wittgenstein, right? That if the
lion could talk, I would not understand anything that the lion was saying. Because his world is
so different from mine. Now, I actually think, I think this is a very important position to,
as a reference point, right? Certainly a defensible one. And, you know, Wittgenstein was a good
defender of it. But I actually think that this is going too far. I think, ultimately, I mean,
never be able to completely know what it's like to be a lion. But we can make a lot,
don't underestimate us either, right? We can make a lot of inwards into understanding what
it's like to be a lion, much more than we understand today. Same thing for a bat. And,
you know, you could also ask that for a fruit fly, right? In a way, a fruit fly is more different
from us than a lion, but it's easy to understand, right? Because at some level, that thing is so
simple that we can understand what's going on with it, because it's not that deep.
Yeah, that's a beautiful quote, actually. So, closing this off, do you think that large
language models are slightly conscious or will be in the near future?
I think language, I think large language models are not slightly conscious by the reasonable,
you know, everyday definition of the world slightly, meaning that their consciousness,
so I think that either their consciousness is just zero, right? If somebody asked me, like, you know,
how much, you know, consciousness does, you know, lambda half, tell me in one word, and the answer
would be zero, right? But another answer which is hard to distinguish from the first one is epsilon,
right? Maybe it has a very tiny amount of consciousness, but it's so tiny that it doesn't
even qualify as slightly. Again, this gets back to what its architecture is. It actually gets
too lot of things, but for purposes of this discussion, right, lambda and these large
language models are not very different from a big lookup table. Any big lookup table is not
conscious. Now, I mean, there are a lot of interesting distinctions that you can make it well.
What if what I have is an efficient approximation to a lookup table? Isn't that what your brain is,
right? And I would say yes, and then people say, well, but then why is your brain conscious
but not the lookup table, right? And precisely the interesting question is that the consciousness
comes about from the fact that you have to concentrate all of this information, you know,
in real time, into something, you know, very compact and that leads to action continuously,
right? So to put this in another way, maybe God is unconscious because he doesn't need to be,
right? If you're omnipotent and omniscient, you don't need to be conscious. You are effectively
just a lookup table. Exactly. And I loved your response earlier about the grain of sand and
the oyster. I thought that was a beautiful way of looking at it. And having recently studied
so, I mean, personally, I think it's a lot to do with intentionality and agency, but I remember
you responded to that. Just final quick question. What's your definition of intelligence?
So let me start with the technical definition, which is unfortunately not widely known enough
and not appreciated enough. But I think it's a really important one to have, right? Intelligence
is solving NP-complete problems using heuristics. This is the real technical definition of AI,
right? And there's a lot packed into that, right? The fact that it's NP-complete problems and the
fact that it's using heuristics. If your problem is solvable with a lookup table with polynomial
algorithms, you don't need intelligence and there's no intelligence there. It's when you start solving
hard problems using heuristics that you're getting into the realm of intelligence. Moreover, NP-complete
is not the same as exponential, right? The crucial thing about an NP-complete problem that connects
very directly to our entire discussion of utility and whatnot is that the solution is easy to check.
This is the key. If you're working on problems whose solution is impossible to check effectively,
I can't even tell if you're intelligent or not. The whole thing about intelligence in humans and
machines is that how you solve the problem requires a lot of intelligence, a lot of computing power
and whatnot, but then I can easily check the solution. Now, hang on a minute, could that
say a step away from behavior then if you're saying that, you know, like you have the percepts,
the state and the action and you're saying the state is also important?
No, so to answer that head on, intelligence is not behavior, right? Intelligence to give a slightly
more general definition and then there's several and they all have their merits. Intelligence is the
ability to solve hard problems. Then more concretely, it's NP-complete problems and using heuristics,
but like, for example, if you create an AI system that cures cancer, it doesn't behave in the sense
that a human and a robot behave, but, you know, it's damn intelligence, it's more intelligent
than we are, right? It would be childish to deny intelligence to that system, no matter how it solves
cancer. If it finds a ridiculously simple way to solve cancer, then it's even more brilliant,
right? In fact, the simpler your outcome, the more intelligent you are, right? It takes intelligence
to produce something simple. Wow. Concretely, in many circumstances, in particular evolution,
right? Intelligence manifests itself as behavior. There's a sequential decision making problem,
there's an agent in the world that said a certain stuff, being a stochastic parrot.
And I think also from, you know, theoretical reasons, by analyzing what a transformer can
represent and how it can learn, my best guess, which could be wrong again, I don't think anybody
has the answer to this and it's interesting question is that those transformers, right,
not LLM scholars, that means more of like a task rather than the, you know, than the architecture.
Transformers have a certain limited ability to do compositionality,
very limited to compare to full logic programming, etc., but exponentially better than something like
an ordinary multilayer perceptron. And if you just, I mean, even a multilayer perceptron or any
learning algorithm is more than a stochastic parrot, because it's general, the whole point
of machine learning is to generalize beyond the data. If you generalize correctly beyond
the data, you're not just a parrot anymore. And, you know, I think it's not an accident that that
term stochastic parrot came from Emily Bender, my linguistics colleague at UW, who does not
understand machine learning. She's a classic linguist of the Chomsky and Variety, who does,
you know, does not fundamentally understand what I think, you know, she might disagree,
what machine learning is all about. And she would probably look at any learning algorithm and say
that it's a stochastic parrot, missing the fact that the whole point of machine learning and the
thing that we focus on from, you know, beginning to end is generalizing. And as soon as you're
generalizing correctly, even if you have no compositionality, you're already doing something
that has a little bit of intelligence, and that's beyond what a parrot would do.
Yeah, I mean, to be fair, it's not a binary. And at the time, I thought they were stochastic
parents as well. I've updated my view. And you were talking as well about creativity. There's
a kind of blurred hyperplane of creativity. And we discussed, you know, where that hyperplane
sits. But, you know, what's really interested me, I've interviewed quite a few people that are
working on working on in context learning in these language models. And it seems like these
language models are almost almost like a new type of compiler, you know, you're writing a program
inside the language prompt. And they seem to work extremely well outside of the training
range if you're doing like basic multiplication tasks.
I think it is useful to look at them as a new type of compiler. In fact, I've been saying for a
long time that, you know, like, there's this continuum from programming an assembly code to
high level languages to doing AI, right? The point of AI is to continue along that path
to making the language that computers speak ever closer to ours, so that we can just program them
by talking to them or writing things at them, right? Having said that, I think that, you know,
what goes on in the innards of a transformer, right, is actually still
very primitive, for lack of a better word, right? There's a lot of, so something I tweeted that
got a lot of follow up from people like Yan and Gary and who the pro because they were all bringing
in their own angles. So this was like, I said, and I think this is an interesting question. It's
like the interesting question about transformers is what needs to be added to them to get real
intelligence. So we should not deny what they have, like the attention mechanism in particular,
right? And the embeddings and the context. So like, there are two very important things in
transformers that are beyond what was in neural networks 10 years ago and are key. One of them
is attention, right? Attention is a real advance. And the other one is context specific embeddings,
right? Each of these ideas is important in its own right and combining them together is very
powerful, right? Again, because the context sensitive embeddings get that the similarity
part of intelligence, the attention combined with the context sensitivity of the embeddings
gets at the compositionality part. So they do have, so there are a couple of steps forward on
the road to human level intelligence, but there are many more. And rather than either saying like,
oh, they're just parrots, they don't do anything, we're saying like, we've almost solved the eye,
what we really should, we should try to understand better, you know, how the, you know,
the attention and the context is dependent embeddings work, which we don't. But we also need
to focus like, now, what are we still missing? Because we definitely are. And that's really
where most of our focus should be. Yeah, I completely agree. And also just in defense
of Bender, I mean, I think she's a brilliant linguist. And I personally think having that
diversity of views from different people is useful. No, I mean, so I very much think that having a
diversity of views is very important. And I think something that I'm always saying to my
deep learning friends who can't stand, you know, who hate the guts of Gary Marcus is
we really, really need informed critics. Yeah. And very typically, your informed critics are not
people in the field. We are experts, but then we also suffer from the distortion of being experts.
It's people in adjacent areas. And people like linguists and psychologists are very much those
people, they're in adjacent areas, enough to have a good critique of AI. So for example,
something that Jan is always throwing at Gary Marcus, that kind of doesn't sit well with me,
says like, well, you should try building a real system sometime, and you can criticize this until
we do. If we take the attitude that only engineers can criticize engineers, we're doomed.
Having said that, there is a very big distinction between the knowledgeable informed critics like
Gary Marcus, and the not so knowledgeable, not so well informed ones, which unfortunately,
Emily is an example. I mean, she's my colleague at UW. And I've talked with her about some of these
things. And her criticism of machine learning, unfortunately, like a lot of people, comes from
a place of actually not fundamental understanding it very well. But people do say that Gary isn't
an expert in deep learning and that he's, you know, attention seeking. What would you say to that?
No, he's not an expert in deep learning. And so like, I agree with some of his criticisms,
I disagree with others. Probably on balance, I disagree more with him than I agree. But
first of all, there is a value to having critics like that, number one. But then number two,
the reason his criticism, I mean, it would be better if he was also an expert in deep learning
and made the same criticisms. And then the problem is that often his criticisms are wrong
because he has a mental model of deep learning that is already outdated, or is oversimplified,
right? But that to some degree is unavoidable. But the thing that makes his criticism valuable
is that he's doing it at a level where on a good day, on a bad day, his criticisms miss the mark.
But on a good day, which is the ones that matter, his criticism is actually useful because it's
at a level where you don't need to understand the details. It's like, you claim to be producing
intelligence. I as a psychologist know a lot about intelligence. That's what I study for a living,
right? He knows more about aspects of intelligence than I do. Yeah. And from that point of view,
what you're doing is lacking. And that I mean, like, he's written the whole books about, you know,
again, because this goes back to when he was a PhD student and, you know, and symbolic learning and
whatnot, there are very, you know, the deep learning folks have repeatedly underestimated
how well he understands some of these problems. Because as a psychologist in particular interested
in language learning, he's actually thought very long and hard about them. Oh, I know. So I've
read his book and we've had him on the show three times. Which book? The algebraic mind.
Yeah. So that's the most relevant one here. Yeah. As a psychologist, you know, he spent a lot of
time studying how children learn rules. Right. And he talks very elegantly about a
compositionality. And we've spoken about this. It's irrefutable. And I agree with him and we've
supported him. I guess some of the things he argues are based on ethics, politics and virtue.
And some of the things like compositionality, I think are irrefutable. I mean, I think irrefutable
is a very strong word. I wouldn't say that they're irrefutable. I would say that they have,
they have very strong backing, which the connectionists have not been able to effectively
refute. But some of the criticisms that they have, you know, meaning people like Pinker and Prince
and whatnot, famously of connectionists in the 80s, some of them are still valid, which is very
salient. But some of them not really. And again, to go back to the daddy of this whole school of
thought, who's Chomsky, right? His, you know, he made his name basically panning things like,
you know, Markov models of language in Graham models, which he could say large language models
are just a very glorified version of, right? But and at the time, you could, that criticism was
very apt and, you know, and timely and it was useful, right? But, but, but, and famously, it's
like, it's like, you can't learn a context free grammar, but context free grammar is what you
do. Well, actually, now we know formally that you can learn a context free grammar. And, and,
you know, because you only have to learn it probabilistically, which is what we do. And
what our systems do. So his criticism was just, you know, mathematically off the mark. But also,
when you look at systems that do speech language, et cetera, et cetera, it is that statistical
approach that he made his name panning that has prevailed. And for reasons that we understand
very well, and large language models are just the latest greatest expression of that. So at that
level, a whole Chomsky and Pinker, Gary Marcus view of things, not only is it not irrefutable,
it has been refuted. Okay. Let's just quickly come back to your definition of intelligence. So
solving NP hard problems, I assume you would zoom out a little bit and, you know, it's more of a
meta learning algorithm. So the ability to sell to sell different problems.
Yes. So it's, if very good point, if all you have is the ability to solve one NP complete problem,
that does not qualify as general intelligence, right? There's like, these days, this is a common
definition to make this different difference between, you know, narrow intelligence and general
intelligence and AGI and whatnot, right? And if you only solve one NP complete problem very well,
you have narrow intelligence is the way I would put it, but you do not have general intelligence.
General intelligence is precisely the ability to solve a limitless variety of problems, all that
have this characteristic of they're hard to solve, but the solution is easy to check. Right? I mean,
if you have the ability to solve problems, whose solution isn't easy to check, then maybe you're
intelligent, but I can't decide whether intelligent or not. Interesting. Okay. And actually, Gary did,
he put a paper about 20 years ago talking about how neural networks can't extrapolate. I think it
was when he encoded numbers with a binary encoding or whatever. And we've been on a bit of a journey
on this. So we had Randall Bellistrier, I've interviewed him yesterday, he's got this paper
called the spline theory of neural networks. It basically says that a neural network decomposes
an input space into these input activated polyhedra. And when we first read that,
we felt that it kind of indicated Francois Chollet's assertion that neural networks are
locality sensitive hashing tables, and they only generalize within, you know, these tiny
polyhedra. And Randall's now updated this view to say in contrast to decision trees, these
hyperplanes, they actually inform a lot of information in the extrapolative regime outside
of the training range. So I always thought it was the inductive priors that gave the extrapolative
performance on neural networks by photocopying the information everywhere. And so like, you know,
this is a great example of where, you know, Gary might update his views because even basic MLPs
are far more extrapolative than anyone realized. This is a very interesting question. But the
way I would put it is that in that regard, in some sense, both of the sides are right.
And the reason they're both right is that we're in very high dimensional spaces.
Yeah. And we're in a very high dimensional space. The follow thing can happen, which is,
you know, you have a data point, and you generalize to a vast region around that data point. And it's
unfair to characterize these things as saying they just interpolate. In some sense, they really do
extrapolate. But at the same time, that vast region that they generalize correctly to is an
infinitesimal fraction of the much, much vaster reason that they have not generalized to but you
and I can. So you got to keep that distinction in mind. And then in particular, right, I like to
say that deep learning is nearest neighbor in curved space. And both parts of that are very
important, right? So, you know, Jan Lacoon was famous, you know, during the glory days of kernel
machines for saying that kernel machines are just glorified template matches. Right. And of
course, they didn't earn him any friends, but he was right. They really are just glorified template
matches. Kernel machine is really a souped up, more mathematically elegant and blah, blah,
blah version of nearest neighbor. Right. And the nearest neighbor is just a template matcher.
The beauty in the power of nearest neighbor, though, is that there is a neighborhood within
which often it generalizes very well. Right. Now, I think what Jan was missing, and I probably
still is, is that coordinates and deep learning, they are still just a glory. They are also glorified
nearest neighbor, except more glorified. And the way in which they're more glorified, which is
very important is that they are doing nearest neighbor in curved space. They are still just
doing, you know, generalization by similarity, which you could argue is all that machine learning
does is generalizing by similarity. Another notion of similarity can vary. Right. But the
important thing that they've done is that nearest neighbor just uses some distance
measured in the original space, whereas the neural networks are warping the space to make
the problem easier for the nearest neighbor, you know, essentially dot product based similarity
computation that they're actually doing. Oh, sure. But you're very much arguing,
this is the way Francois Chouelet puts it, that, you know, you have all of these
transformations and you kind of distort the space, you know, to represent the data manifold. And,
you know, you want it to, you stop SGD at the right time so that you approximate the data
manifold and you can do this kind of latent space, you know, interpolation on the geodesic of that
manifold. But, you know, Randall's idea is completely away from that idea of, you know,
these models learning this curved space. And so if you do slice the space up with these hyperplanes,
rather than it being a locality prior, which is what you're talking about, these hyperplanes give
you globally relevant information to things that are, you know, miles away from the training data.
Yeah, so, but these two perspectives are more similar than you might think, because I can take
a distorted version of space and decompose it into polyhedron, right? And one or the other might
approximate what's really going on better. I mean, these neural networks do form curved spaces,
except they're in practice, they're not curved because they find it, but ignoring that, right?
When, let me put it this way, an eloquent example of this is if you look back at the original space,
right? Again, treat this thing as a black box. Where does it generalize to? Does it generalize
only to things, neural networks as we have them today? Does it generalize correctly only to things
that are locally near the data point, or you can generalize well to things that are far, right?
And the thing is that with nearest neighbor, you buy, you know, almost intrinsically, you only
generalize period at all to things that are local. The beauty of deep learning and of the
space swapping that's going on is, again, going back to this notion of the path kernel is that
you're actually doing a nearest neighbor computation, not just in a space that's swapped,
but you're doing it in the space of gradients, which actually means that you can generalize
correctly to things that are very far from your examples, except they look similar in gradient
space. A very simple example of this is a sine wave, right? If I try to learn a sine wave using
nearest neighbor, I need an infinite number of examples, right? Because, you know, like what
I've learned over here helps me not at all with the next turn of the sine wave, like that continuous
extrapolation, right? At some point, there's this disaster where if the last piece of the
sine was going up, I just keep going up and getting more and more wrong, right? And in fact,
this kind of thing does happen in neural networks, but they also have the part to say like, and this
again, this also happens, which is I'm going to transform this space more into a more intelligent
one, which is the space of the slopes, right? And now if I've seen one cycle of the sine wave
with some density of examples, by similarity in that transformed space, I generalize correctly
and trivially to every other turn of the sine wave. So there's a very big fundamental difference
between the two. Interesting. And you think with an MLP, it would be possible to have that kind
of extrapolative generalization on a sine wave? Well, so people have studied this in multiple
ways. And the problem, so the question is, it depends on what are the basis functions that it's
using. Yes. So something that we didn't allude to at all in this conversation, but analyze all of
this is like, what is your choice of basis functions, right? And the thing is, an MLP with
the traditional, say, sigmoid or allude basis functions will not learn this, no matter, for
obvious reasons, right? And again, you can represent it, right? The representative theorem is there,
like the sine wave is just one sigmoid and then another one, you know, with a minus sign and
then another one, but the data doesn't let you learn it. If as a basis function, you have sine
waves, which is nothing unimaginable, that's what a Fourier transform is then, then you can learn
it so easily, it's not even funny. So it depends dramatically on the basis function. And the
question really becomes, what are the basis functions and the architect that let me generalize
correctly to a lot of things, including this, such that, for example, and this is a very simple test,
is like, I can nail a sine wave with a small number of examples without it being one of my basis
functions. Yeah, exactly. And then this all comes back to, you know, we're talking about inductive
prize and the bias variance trade off and even symmetries, actually. I mean, the Taco Cohen once
said that, you know, if you encode all of the symmetries into the label function, then you would
only need one labeled example. So it's always a trade off between how much induction are you doing?
Well, interesting, you should say that I understand why he says that and it's, and it's not technically
wrong. But I would say that practically what you need is such a set of symmetries per region of
the space, per cluster, right? But, you know, in another way, I would actually make an even
stronger statement, which again, is very perfectly mathematical, sounds same when you say, an object
is just the sum of its symmetries or a function. If you tell me all the symmetries, every last one
of an object, you've defined the object. So if I can learn the symmetries at that level, I don't
need anything else. Of course, as we already discussed, that's not the whole answer. Likewise,
with any function, if you tell me all the properties of the function, there are there, you know,
to be more precise, all the symmetries of a function at some point, you've told me the whole
function. And vice versa, from the function, I can, you know, I can read out all the symmetries
that it has. In principle, doing that in practice can be, you know, a very difficult and subtle
thing to do. That's a beautiful thing to say. You give me the symmetries and I'll give you the
object. Yeah, exactly. Amazing. Professor Pedro Domingos, thank you so much for joining us today.
It's been an honor. Thanks for having me. Amazing.

Hello everyone, I'm here to talk to you about a vision of artificial intelligence that goes beyond
machines and algorithms. A vision which embraces humans and nature as integral parts of a cyber
physical ecosystem of intelligence. A vision that is based on first principles derived from physics
and biology. Professor Carl Friston just dropped a paper called Designing Ecosystems of Intelligence
from first principles. And one of the principles is active inference, a formulation of adaptive
behavior which can be read as a physics of intelligence. Active inference says that
intelligent systems are those which can accumulate evidence for a generative model
of their sensed world. In other words, they can learn from their observations and update their
beliefs accordingly. They can also act on their environment to reduce uncertainty and achieve
their goals. But active inference is not about individual agents. It also explains how ensembles
of agents can share beliefs and cooperate through communication protocols. This leads to a formal
account of collective intelligence that rests on shared narratives and goals. So how do we realize
this vision? Friston proposes a research agenda for the next decade and beyond which aims to design
such ecosystems of intelligence from scratch. They suggest developing a shared hyperspatial
modeling language and transaction protocols as well as novel methods for measuring and optimizing
collective intelligence. So why should we care about this vision? It's because it offers a way to
harness the power of artificial intelligence for the common good without compromising human dignity
or autonomy and because it challenges us to rethink our relationship with technology,
nature and each other and because it invites us to join in a global community of sense makers
who are curious about the world and eager to improve it. Enjoy the show.
Our shared human journey is filled with examples of simple ideas that were nonetheless hard to
discover and some that even once explained remain hard to comprehend. Their subtle simplicity
belies their far-reaching and deep consequences. Examples might include the principle of relativity,
quantum mechanics, the principle of parsimony and entropy. I think the free energy principle
is another example, profound and far-reaching yet belied by its simplicity. It has on the one hand
been dismissed as a triviality or even a tautology and on the other hailed as revolutionary
and everything in between. It has the potential to reshape how we view the connection between
inanimate matter and living things and even to answer the question of how and why consciousness
and intelligence might emerge from physical matter and processes. That means a world where
all things from particles to people to the largest systems all move or evolve according to two processes
which combine. The first is a smooth flowing evolution. Think of planets orbiting a star
or waves rippling over water or through quantum fields. The second is a random process that
knocks around the smooth flow in unpredictable ways. Think of twinkling stars or audio static.
You might think of these two processes as a kind of order and chaos or yin and yang forever
intertwined and enmeshed. These two very different effects combine into a chaotic flow which may be
entangled in a kind of tropical storm while still maintaining a semblance of structure
and things. Think about raindrops undulating down to earth in a state of constant flux
yet still droplets or a human being growing and adapting to the surprises of life
all the while remaining an individual physical entity. Biology often asks the question
what must things do in order to exist? Professor Friston has turned that question on its head
and developed perhaps the ultimate existential formalism. He asks if things exist, what must
they do? From the foundation of stochastic differential equations, Friston demonstrates
that things which are defined by a Markov blanket must always move towards a pullback
attractor, a special set of attracting states which maintains the integrity of the Markov blanket
and therefore a thing's coherence and identity over time. One of the profound consequences
of this is that the dynamics of such a system, its laws of motion, will manifest a form of flow
dynamics which can be interpreted as Bayesian active inference. In other words, such a thing
maintains an internal equivalent of a generative model encoding beliefs about the world and itself.
The thing uses the model to decide actions and then performs a Bayesian update based on the outcomes.
For me, the idea that inference, something widely perceived as purely abstract or mathematical,
the idea that it can be driven by simple laws of motion dynamically maintaining the boundaries
between things, maintaining order in the face of chaos is frankly astonishing.
What's more, the free energy principle is so general that it applies at all scales of size
and time leading to an ecosystem of things interacting across scales. Perhaps in that
multi-scale active inference, we might finally find the keys to a mathematics of emergence
and consciousness. This episode is sponsored by Numeri. I'm extremely grateful to them for
sponsoring Machine Learning Street Talk. I mean, remember, I do everything myself on this channel.
It's a lot of time. It's a lot of hard work. It's very expensive. I pay for other software licenses
and all of the equipment. Having that support from Numeri means a lot to me and it means that I
can keep doing what I'm doing basically. Thank you to Numeri. A little bit about Numeri, they are a
data science competition platform to predict the stock market. They've already paid out over $50
million for 5,000 models on their platform. They say that it's the highest paying data science
platform in the world. Now, you can get started really easily for free. They've got a couple of
examples on their GitHub repo actually using XGBoost. The data comes in parquet format so you
can get up and running really easily. You can submit your predictions weekly or daily. You can do it
either manually or you can do it automatically. They provide many, many years of back testing data
as well so you can fine-tune and test your models and do a bunch of statistical diagnostics.
You can work your way up the leaderboard for bragging rights and stake your model with their
NMR cryptocurrency to earn rewards. Staking is a vote of confidence of your model and the reason
they do that is to prevent overfitting and to reduce the bad models from contributing bad
intelligence to their aggregate. Now, a well-performing model is rewarded in proportion to its stake
but a poorly performing model is punished by burning a portion of the stake. Numeri have a
huge community of data scientists of all levels swapping ideas and advice and they have some
community forums as well which you should join. Now, just a personal note from me. This is a form
of betting on the predicted performance of your models which can go up or down. Remember the data
is abstracted from the actual performance of the stocks. They don't correlate to actual stock
performance so there are folks on the platform who've made lots of money but there's also folks who
have lost money as well so please be responsible and only stake what you can afford to lose and
have confidence in. Try to have fun, use it as a place to sharpen up your data science skills
and to be the best version of yourself and use all of the latest models that we've been talking
about here on Street Talk. But yeah, anyway, thank you again to Numeri for sponsoring us.
Professor Friston, it's an absolute honor to meet you.
Well, thank you very much for having me. So we've had you on the show two times now
and in the first show we went into exquisite detail about the free energy principle and active
inference. So you're extremely famous for introducing this existential imperative which is to say
if things survive what must they do? They gain information about the world around them in a
cybernetic loop. We find a model which fits well while maintaining high entropy. If you have a
high entropy model you have greater flexibility to adapt to new information. It's an absolutely
beautiful idea. So welcome. Thank you. Nicely articulated. I was speaking with Keith yesterday
and he said out of all of the guests that we've had on MLST you are by far his favorite and he
says he looks up to you very much so he's very gutted that he couldn't come today to be part of
this. It's very gracious of him though. So you just wrote a paper and it's called designing
ecosystems of intelligence from first principles. You led with this white paper lays out a vision
of research and development in the field of artificial intelligence for the next decade
and beyond. It's denument, I wasn't sure if I was going to say that word, is a cyberphysical ecosystem
of natural and synthetic sense making in which humans are integral participants what we call
shared intelligence. This vision is premised on active inference the formulation of adaptive
behavior that can be read as the physics of intelligence and which inherits from the physics
of self-organization. So you went on and by the way there's an interesting link with Michael Levin's
work here so we had him on the show recently and maybe even transhumanism as well we can get into
that. But you went on you said in this context we understand intelligence as the capacity to
accumulate evidence for a generative model of one's sensed world also known as self-evidencing
over multiple scales and crucially you said active inference foregrounds an existential imperative
of intelligent systems namely curiosity or the resolution of uncertainty and the same imperative
underwrites belief sharing in ensembles of agents in which certain aspects which is to say factors
of each agent's generative model provide common ground or a frame of reference. So can you sketch
some of this out for me? Yes you've used all my favorite words though I presume that I actually
used all the fact to be truthful Maxwell Ramsted was one of the key architect of this so this was a
this white paper was a response to a brief just to think seriously and pragmatically
how this sort of high church theoretical approach to you know intelligence and self-organization
would play out in industry in the way that we actually use technology over the next decade
and that white paper is a product of our machinations and discussions but just drilling down on the
sort of basic message there. The emphasis was as you say on what it is to exist and how that
would manifest in terms of intelligent behavior and to a certain extent certainly I and I think a
number of the other co-authors were reading intelligence as the kind of inference that you
would need to do in order to maintain your existence so hence the existential imperative.
I have to say that's a slightly poetic interpretation of the free energy principle which
is the other way around of course that if you exist it looks as if you are behaving intelligently
if you read intelligence as the right kind of belief updating that enables you to maintain
yourself in some characteristic state so that's essentially what that existential imperative
was all about just one way of reading the physics the mechanics of systems that are open in open
exchange with the rest of their world or their eco niche simply in virtue of the fact that they
are around for an extended period of time and have this characteristic set of states that they occupy
that defines them as the kind of thing that they are. I think the move really that was inspired
by this remit to look okay well that's very nice you can write lots of papers about that and have
nice chats with your friends and colleagues about it about how does that actually work in practice
and of course in practice you have to think about where you're going to deploy this perspective
deploy the kind of technology that would you know ensue from that perspective and of course we're
talking about communication we're talking about a universal world of lots of creatures like me
and you so now we get to the next level of application of these existential imperatives
in the context of not just intelligent behavior and just open brackets behavior here
it's quite important you know often I hear underwriting the move from AI artificial intelligence
to IA where it's intelligent agents that have the artifacts with agency that do have behaviors that
have a sentient kind of behavior or at least an intelligent kind of behavior close brackets
so we're now moving from thinking about a single thing a particle or a person or a computer
and thinking now about having lots of things that are talking to each other and they're
coexisting so now we're in the world of distributed intelligence distributed cognition
and one thing which you mentioned explicitly this notion of belief sharing so now we're in
a different kind of world where we're thinking okay we think we understand the imperatives for
good behavior the necessary kinds of behaviors that you know I think you highlighted curiosity I
think is absolutely central in terms of you know hallmarking and characterizing the kinds of behaviors
that you and I engage with or truly intelligent or sentient behavior would entail but now the idea
is well what would that look like and when you've got lots of these things talking to each other
well we know what it looks like we talk to each other literally so it's all about language it's
all about belief sharing shared intelligence distributed cognition would be one thing that
you might find in the in the life sciences if you're a computer scientist it's all about sort of
the right kind of message passing on very very large factor graphs if you're if you're a linguist
or into evolution in psychology it's all about neat cultural niche construction the
emergence of language so I found that quite fascinating as something which you know you
really had to get on top of to think well what next and what kind of technologies would you need
to realize and what would they look like you know one way that's been framed to me by my new friends
now in industry is you know what would what's it going to look like what what is AI or now AI going
to look like in the future taking the kind of perspective you know over the decades or indeed
centuries where you're moving from sort of the industrial age to the age of information I think
the notion now is we're now coming to the end of the age of information now we're in the age of
intelligence and think about what what that might look like and how would one equip people with the
right kind of technology and infrastructure to you know to realize the potential of shared
intelligence essentially yeah that's very interesting so we spoke with Luciano Floridi
Oxford and he's invented this philosophy of information and he talks about the infosphere
and how we're at we're out we now have third order technology essentially so things like Amazon and
Facebook and he talks about the diminishment of our ontology and agency in the infosphere
and using this information as a substrate thing I think is very interesting because that's kind
of alluding to what you're going to but you said something actually in your paper which gave me
pause for thought you said the the AI age may end up being a distributed network of intelligent
systems which interact frictionally in friction frictionlessly in real time and compose into
emergent forms of intelligence at subordinate scales as you were just alluding to and the
nodes of such a distributed interconnected ecosystem may then be human users as well as
human designed artifacts that embody or implement forms of intelligence now this really did make
me wonder because I think we're already there now and so when when we had Floridion he was talking
about how our digital identity has already distributed we were consumers and then we
became barcodes and now there's this kind of infinite fractionation in the infosphere and
it's almost as if humanity isn't the substrate anymore information is the substrate and he said
you can devalue human skills you can remove remove human responsibility you can reduce human control
you can erode human self-determination but the rock that thought that little drop of water was
nothing 18 years later has a hole in it because drop after drop after drop the drop will shake the
stone will shake the rock and I think what he meant by that is being ensconced and enmeshed in
technology to such a high degree and information being the first class citizen in our society is
kind of truncating our very existence I mean what would you say to that beautifully beautifully
expressed very poetic and I'd have to think about that very carefully I'm sure there are deep truths
and certainly from from the point of view of a physicist your information is obviously the thing
it is it is in many respects the you know the substance of our universe and our existence and
you're also acknowledging that energy is information and energy is just a potential and so
we're all about we're all in the game of realizing potentials in a sort of folk psychology sense but
also literally in terms of minimizing certain potentials for example self-information would be
the free energy potential that you're that you're minimizing the surprise and again coming back to
this sort of making you know belief sharing in the service of minimizing minimizing a surprise
and realizing our potential so I but I don't see that as a negative thing I think that's just an
acknowledgement that you know at the end of the day the way that we model and articulate and talk
about ourselves our lived world whether we're a quantum physicist speak a statistical physicist
or a classical physicist or have an interest in in the equivalent mechanics at each of those
levels it's all about information it's all about the probability of being in this particular state
you just take the the negative log of that that is the self-information of this particular state
so it's all you know if you look at quantum physics you know what else is there it boils down to
in its sort of most elemental and scale free or background free form and quantum information
theory and say if you can reduce it to you know to quantum information theory which many people
including friends of Mike Levin and I think one can then I think there's a there's a deep truth
that we are just realizations of information particularly from my perspective of course
we being a thing what what kind of information processing would be apt to describe us that could
be you know articulate in terms of quantum information theory it's belief updating so we come
back to the Bayesian mechanics that you can put on top of this underlying you know sort of information
theoretic probabilistic description of the world but I guess I'm sort of reading between the lines
and the twinkle in your eye is the idea that each one I get across is that maybe quite bad for
humanity is that the idea is it all that's what that's what he thinks and I mean he thinks
you're a computationalist you think that information is the kind of primary substrate
and in a way he's also worried that information is becoming the primary substrate but the big
difference between you and 3d is that he thinks that humans are special he thinks that we like
can't we have autonomy we choose our own actions you know we can't be replicated in silica so he
thinks essentially that there's no such thing as general intelligence that there's just algorithms
that perform skills and it's our humanity which is being truncated I see right okay well I would
be very sympathetic to that yes I mean we are special kinds of intelligence and one could
equip that argument with you know what is the definition of sentience what's a bright line
between you know a very clever thermostat or some machine learning artifact or a virus
and you and me I think there is a bright line and you of course you've just said what it is it's
the autonomy it's the agency it's ability to plan and the all the existential imperatives that
underwrite that planning and then we come back to curiosity so but if he's saying that
are the fact we are here realize the fact we are curious creatures and because we are
we populate a universe that comprises creatures like us we're all curious about each other
then I would certainly say yes that is a definitive aspect of us which is not found I think
I'm just thinking carefully I'm sure you can find examples but you know I don't in the kind of
artificial intelligence that we currently interact with in exchange with you don't have that planning
and curiosity they don't have the you don't have bait into the optimization framing of you know
what makes a viable or a good bit of intelligence you don't actually have bait in universally
this expected information can this curiosity and in that sense I think he's probably absolutely
right and in a sense the belief sharing getting to that the right kind of belief sharing of the
kind that the white paper was talking about is predicated on the notion that you will now be
able to equip sentient artifacts that we make with curiosity and maybe asking well what are
they going to be curious about their world what is their world it's you and me and the other
artifacts so they're only going to be curious about you and me they're going to be interested
in you and me so we're talking about you know a Siri or a Google Maps that starts to ask you
questions instead of you asking them questions so that I think that's that's one way of eluding his
his sort of rather dystopian attitude to the other the information is king I think information is
king belief updating is king the belief updating is you know of course the thing that ensues once
you act upon the world to do some smart data mining to you know to respond to some epistemic
affordances and you know the question then is you know what kinds of systems do that and the moment
I would argue it's probably just us there are other examples of some beautiful examples in
say active learning using machine learning you know to design your own experiments and
optimize the actual experiments and say drug discovery or molecular biology so I mean that
there is a long history both in statistics and in machine learning of active learning
that I think does have the potential but I don't see it really being
well from what I understand in discussions I don't see that being a bedrock of the way forward
an explicit part of the design for an age of intelligence that is puts us sympathetically
in an ecosystem of intelligence and that's really what that white paper was trying to think what you
know what would it look like and what would happen if your google maps became very curious about you
particularly I mean at google maps I think well there are two sort of sort of metaphors here
which might sort of ground the framework or the perspective that I'm trying to think about this
worry and one which I found very helpful again coming from my colleagues in industry is just trying
to explain the nature of shared intelligence and distributed intelligence and the analogy here would
be the brain you know you've got really smart little elements little neurons I mean they're
really smart and in fact if you get into the weeds of dendritic computation incredibly smart little
things that are in receipt of their sensations and they act by some pinging they don't know who
they're pinging but they're pinging away sending out little action potential or messages down down
the axons the wires that are emitted from from the nerve cells and you've got you know 10 billion of
these things and they're all very very smart but would you call any one of them autonomous would
be called would they have agency in an elemental sense I think they do but it's when you put them
all together lots and lots of little smart things together do you get this emergent kind of intelligence
that you could undeniably say has the capacity to roll out into the future to you know generate
fantasies or counterfactuals that are all conditioned upon what I'm going to do next where
I now becomes this collective so this is the kind of emergent behavior that emerges from
getting lots of little smart things to talk to each other in the right kind of way so that
that's you know I think quite a helpful analogy by what is meant between you know about shared
intelligence and what might you know might arise clearly a lot of these little smart brain cells
be analogous to you and me a lot of others would be all kinds of apps and you know
giving you now the potential to see through the eyes of any smart app that knows what you want
to know and is curious about what you want to you know to find out again coming back to this notion
that belief sharing I think is already there in the context of say you know sat nav you know I
share my beliefs about my preferred states of being my characteristic preferences are one part of the
you know the imperative suite for policy selection or good plans I share that with in terms of a
destination with some shared world model or shared narrative between me and my sat nav
in this instance of a geographical world model and then it has beliefs about the best policy
it makes a little plan and then it shares its beliefs with me again so I think that you know that
to the extent that your colleague in Oxford was saying we're already there I think that
that's absolutely right I mean you know we already have this kind of belief sharing
I think that the move though is to make that a much more symmetric belief sharing you know
I'm asking the app for its beliefs for its I'm asking it to behave as a recommender
and it only knows what I tell it so it has no autonomy but if it was now in a position to
actively smartly resolve its uncertainty about me the user then that's much more of a sort of
you know a balanced symmetrical dynamic interaction between me and the app and the agenda here
is not to create paper clips or make money the agenda remember is just to resolve uncertainty
the state curiosity and to move towards the state of greater mutual understandings
that's a sort of the non-dystopian view of information sharing so Shane Legg said that
his definition of intelligence is the ability of an agent and we're using words here like
agent we'll get to those words in a minute being able to solve a variety of tasks in different
environments Francois Chorlet said it's efficiently creating abstractions given limited prize and
experience Pei Wang says it's the adaptation efficiency over finite resources so when you
look at definitions of intelligence typically they focus on principle and function which I think
your one does or capability or behavior or structure now the interesting thing about the
principle one in particular is I think it's the least anthropomorphic and I think yours is the
least anthropomorphic definition I've ever heard of so and also this concept of grounding in the
physicality of information rather than reality itself and whether they are the same thing of
course is a philosophical discussion that maybe we'll park for another time but yeah how would
you contrast your definition of intelligence from these other ones well I think you've already
done that you've you've said it's you know it's a minimal essentially a physics based definition
of intelligence which requires you know a move or indeed a complete commitment to staying in the
space of information and information geometries and belief updating so having said that I think
all of those definitions touch upon some essential aspects of intelligence every one of them rang
true you know to my ears and I could read every one of those as being one key foundational aspect
of what would emerge if any self-organizing system managed to supervise and coexist with
other self-organizing systems from the point of view of you know of the free energy principle
or the sort of first principle approach so there are no axioms and no assumptions the question is
not you know what is quintessentially anthropomorphic the question is what emergent properties of
certain kinds of self-organizing systems would qualify as having that that bimimetic and then
ultimately anthropomorphic aspects to them you don't even have to go to anthropomorphism I mean if you
were talking as you have been to Mike Levin and his friends you know they would talk about basal
cognition they would talk about you know just a multicellular organism is a beautiful construction
of that rests upon orthopoiesis self-assembly of individual cells but also cells of cells and
you know how does a surface cell an epithelium know that he's on the surface and how does that
individuate the internal cells of an organ or an organ from the rest of the environment so that you
know the kind of intelligence that has this anthropomorphic feel I think people are also
seeing in biotic self-organization that would be a long way away from the kind of
folk psychology intelligence that we're talking about and yet it rests upon exactly the same kind
of mechanics and you know for me that would be the Bayesian mechanics that come from the
you know the dynamics of systems that are self-organizing open systems that are self-organizing
so you know you talked about so adaptation has been one aspect one common theme in the
the sequence of definitions you gave I think does speak to this bright line between
basal cognition and biotic self-organization biological intelligence of the kind you can
read in many many different ways your your DNA for example your genotype is an intelligent
information accumulating device on the point of view of evolution you know it it stores all
the information about what it would require to build a phenotype that's fit for purpose in this
particular environment so you know that's a kind of belief updating that's a kind of intelligence
but it doesn't have what we were talking about before which is this capacity to plan
evolution doesn't plan you could also argue that the the worldwide web doesn't plan
google maps you could argue does plan to a certain extent because it's certainly
so what's the difference the difference is I think implicit in at least the first two of your
of your definitions which is this notion of counterfactual futures this notion of
you know imagining a future or putting it another way having a world model or a generative model
that explains things that are not tied to the moment so if you're a physicist what you're
talking about is now a probabilistic mechanics a Bayesian mechanics or possibly just you know an
information theoretic mechanics based upon paths through time so we're talking about things like
the pathological formulation and you know but crucially we're talking about trajectories that
don't cannot be localized to this point in time that necessarily entail the future and indeed the
past so I've you know that notion of freeing yourself in a you know I can't remember the
name the philosopher now I can't remember the name I just can't pronounce it so I'm going to pretend I
can't remember well there are people who there are there are the philosophical schools that
emphasise this temporality aspect now you know and if you just look at physics look at contributions
of Richard Feynman for example it you know it's all about the pathological formulation
and so I think as soon as you talk about the elements to which the information geometry
as an intelligence and autonomy all of these things could apply are not states they are
trajectories dynamics narratives paths that have this this sort of future pointing aspect
um then being able to select among different futures becomes an emergent property of this
kind of sense making this kind of autopoiesis under you're read as a Bayesian mechanics of
self-organisation and just thinking about your definitions they all have that aspect of choosing
among different futures or considering or having abstractions um you know about what might happen
if if this so for me that that is one way of expressing curiosity because to be curious you
have to imagine well what would happen if I did that and what would I know if I did that but you
have to imagine it before it's actually happened which you know is is for me the big bright line
between you know between the anthropomorphic kind of intelligence and the intelligence you find in
the thermostat or in the you know a variation autoencoder yeah um later on we will decompose
the different aspects of cognition and I think as well as thinking it can be knowing and acting
as well and and we'll we'll talk to that but you said words you know action agent per se
goal plan behavior and I guess and I posed this to Levin as well it feels like these are terms
that we understand because we have cognitive priors like agentiveness and so on these are
things that that we understand that might actually only be a lens into something far more complicated
and just to touch on the information traversal points over a geometry that's very interesting
and I'm no expert but I think the medial temporal lobe deals a lot with spatial
contiguity and we have grid and place cells etc etc so it you know at a macroscopic level
in our brain it's a first-class citizen but there's also this hierarchy isn't there
when does it start happening you know do single cell organisms plan into the future
and to the previous point is planning necessarily a reductive lens of intelligence
you brought up loads of interesting things there you you call me by the reductive lens that's a
lovely phrase what does that what does that mean well um I didn't mean it in a purer sense but
when when we use words like you know like we we we say intelligence must do x y and z it must plan
it must reason it must act and we have this cybernetic loop and so on and I I have a theory
that this is just the way we understand things I see and and in a sense it's a lens onto a much
more complicated thing and and the reason this is interesting is the reason why we have people
like john so who says that the um the impenetrable realm of the subjective experience is beyond
function dynamics and behavior it's a little bit extra and and even and morality is another example
talk about that later that feels like it's something which is a little bit extra so there's
always this question of to what extent is intelligent behavior deducible from the models and the
words that we use yes I think that's a fundamental point uh you said um okay now I understand what
a reductive lens is I like that I like them I like it for many reasons of course because um
well first of all um it certainly is not uh you I don't think it could ever be used in a
pejorative sense um but it does speak to um two fundamental themes which is um the the way that we
do make sense of our world through coarse-graining through reducing uh through having um intuitive
models of the way that the world works um ultimately could be um um seen as language
words um could be I think could also be seen at quite to kind of as physics and maths as well
to be quite honest um you know these these the more I read about you know modern mathematicians
and physicists talking about that you know their skills and and the the legacies that they they
enjoy the more I I realize it's all changing all the time it's just another kind of reductionism um
but language particularly but it's a right kind of reductionism and I guess what you're saying is that
you know um we have this um way of summarizing classifying certain kinds of behavior which may
not truly reflect the underlying complexity the beauty of what's going on underneath
that at that that point I would go the other way I would actually say that do not properly
reflect the underlying simplicity of what's going on to be quite honest you know this comes back to
you know the um unashamed use of phrases like sort of existential imperatives and self-evidencing
yes yeah we're just here we're just we just have characteristic steps and sets we're just
realizations of some glorious um launch of our equations and all these stories about sort of
belief updating and sentience and intelligence and just reductive stories that make sense of
you know what we you know what um what we must um or can if indeed to a certain extent the free
energy principle itself I think is a reductive story of that kind you know when I say if you
look at things through the lens of Bayesian mechanics or as if I think the free energy principle
is another example of of this kind of reductive thing it's a looking at something which is inherently
much more simple than the lens through which you're looking at which is the Bayesian mechanics
and the free energy principle so I think that's absolutely right a really interesting idea um
um so uh and at another level I think it speaks to some key issues you know I mean you're um
you you're often um then confronted with you know okay I'm talking with you about agency
and agents and me and you uh so what license is that what aspects of my implicit world model
or generative model endow me with a sense of me and you and indeed me as agent and you know what
would that look like if I stripped away um different levels of meta awareness or meta uh
cognition if you're a psychologist and uh and just had a minimal selfhood um you know is just be
having plans um sufficient to call me an agent even if I don't know if I'm an agent if I have plans
who else is going to act you know enact those plans I mean I would love to go there slightly
later but there's so much we can say about um agency and the boundaries and also the causal
pressures between agents and also whether you can think of boundaries as being observer relative
but I just really wanted to go to the universal um algorithm thing that you spoke about before
because I think it's delicious so um I think it's fair to call you a universalist and that and that
that's that there are quite a few universalists actually that these are people who think there's
a simple underlying principle and this is in contrast to what we were just talking about
which is that the reality is more complicated than we'll ever understand and we have a truncated
cognitive horizon and we as Chomsky says we just have kind of um simple primitives built into us
that help us frame and and understand a kind of abstraction space within a certain cone
but um I was reading professor Christopher Somerfield's book at Oxford I interviewed him last week
and he said in his book um could it be that the um success of mammalian brains is not due to any
careful crafting into a mosaic of different functional subsystems but instead is merely due
to size we know there's a powerful relationship between the sheer number of neurons and the
complexity of behavior he went on he said researchers and neuroscientists alike such as
Carl Friston and uh Jeff Hawkins and even Andrew Ng have flirted with the idea that there might be
a single algorithm which underpins intelligence with the brain acting like a massive TPU repeating
instructions ad nauseam to generate complex behavior so it's a fascinating idea is that a fair
summation yes what's a TPU in this oh well a tensor pro programming unit it's a very
powerful computer right yeah I learned another new acronym but my world is full of new acronyms
right okay um yeah um so what which his argument there is it's something to do with um scale and
size is is that well not not only that I mean we'll we'll get to there's a guy called Rich Sutton
and he he had this um you know bitter lesson essay and it's a warning against hand crafting um
structures architectures because it bottlenecks it doesn't scale so this universalist idea is that
you know maybe and and Jeff Hawkins says the same thing he's got this thousand brains theory of
intelligence yes and the idea is that there's a very simple underlying algorithm or principle
and you just replicate it you scale it up or out and that produces emergent intelligence right
yes well okay then I am a universalist so um you know but both of those the way you described it
do speak to some I think very pressing issues about um structures and structure learning um
that you know you could either um read from the point of view of machine learning and sort of
graph learning what's the rights you know how many layers does this particular deep learning
architecture need or what kind of factorization are going to put in play
or if you were um you know um radical construct a radical constructivist this you know that that's
where you know I've often heard people like Josh Tenenbaum for example think about sort of structure
learning and um from the point of view of the universalist I've now learned that new word now
that's good so as from as a universalist then you are certainly looking for the one principle
that is redeployed at successive scales and that should be a sufficient explanation for
those things that show emergent behaviors at particular scales so I think that you know
that is absolutely true and again you can read this from the point of view of a mathematician
from the point of view of um the renormalization group and what does that mean what it just means
that you know if I take lots and lots of little things um and I start course training them in a
particular way um then if I want to describe the behavior of all the elements at one um scale of
organization say molecules or cells or people um then if I can write then down their dynamics
in terms of say Lagrangian you know so some way of summarizing their dynamics that um and all the
things that um accompany or ensue from those dynamics um then if I do my course training
um and then look at the collective the average behavior of say lots of cells a place cell or
entire medial temporal or entire person um at a more macroscopic level then I should
be able to recapitulate the same functional form of the dynamics and all the Lagrangian
so from that point of view um you have a particular kind of universalism that is
actually scale free because you get the same principle emerging at every level and that that
is basically um one um one way of looking at the deployment of the free energy principle
is asking what would it look like when deployed at different scales so you can deploy it at the
level of dendritic self-organization you can deploy it at the level um of um your uh neuroscience
you could deploy it at the level of um morphogenesis and cellular pattern formation
we've done that with Mike Levin um the idea being that this the same universal principle
works at every every level then the interesting game comes between the coupling between the levels
how does one level constrain um and inform or contextualize the level below and vice versa
you know and this I think is a really important sort of um issue which is probably well rehearsed
in many different disciplines ranging from say evolution so evolution as a free energy
minimizing process where free energy is literally the um the the negative abound on the negative
log marginal likelihood um I name it the likelihood of finding me this phenotype here
when sampled at random from a population um how does that scale of a free energy self
auto poetic process you know natural selection basic model selection if you're a statistician
how does that provide constraints on the exactly the same principles um of active inference and
learning um in developmental time for any given phenotype and then the that would be the top
down causation the bottom up causation from one scale to the next scale would be you know how does
my behavior my experience experience dependent plasticity my evidence accumulation all my good
Bayesian decisions how does that now um mean I contribute to the gene pool at the at the
evolutionary level yes so you know that that that would be one way of reading it the other way of
reading it of course is just if you're designing um a tpu or deploying a tpu um you've got that
you've got message passing on some graph what is a graph well it only has interesting structure
in virtue of the um the sparsity or the connections that are not there otherwise it's a full graph
and it's not where you saw for anybody speaking to Chris's Christopher's uh uh you know too too big
too many neurons um you can't you you necessarily have to have a sparsity to fit all those neurons
into um I would put it the other way around though I would say that um anything that is adaptive and
has this size uh you know what properties must it possess and I would I would suggest that it has to
comply with the principles of um self-evidencing um where evidence now is the marginal look the
marginal likelihood that can always be read as accuracy minus complexity so as if it exists
and it's big it's got to be minimally complex what does that mean it's going to have the
smallest degrees of freedom uh the minimum number of connections in so you should be
able to predict the sparsity from the first principles at every scale so that sparsity
defines the nature of a graph and indeed if you're talking about anything that's deep
in a hierarchical sense all you're saying is there's a particular kind of graph um that I
have in mind and it you know has a certain sparsity structure but crucially it's a sparsity
structure that allows me to call it a hierarchy it means that there are no connections that transcend
unlike a sort of you know a u-shaped uh no it's still hierarchical to anyway sorry I'm
getting a bit distracted right the choice of graph so I mean I think that's sort of um
the notion of um coupling between different hierarchical scales is absolutely crucial
from many different perspectives renormalization group evolution um you know getting the right
graphical architecture and your method passing scheme and computer science so
yes I wanted to bring that up but I discussed um it's actually the same thing with Levin so
morphogenesis and the rungs of the emergence ladder and the causal pressures between those
rungs and you know philosophers like George Ellis said that you only have causation uh between the
levels and uh Douglas Hofstadter and Gaudalesha Bark and The Strange Loop thinks that there's a
very complex panoply of causal pressures between the scales so like I gave the example to Michael
my mind is an emergent phenomenon and I command my hand to move and he said that at different
scales you get different amounts of work and actually I think if you get into integrated
information theory it's kind of talking to that a little bit and and I think you think of consciousness
as having a lot of um information processing going on because it's at the top of the stack to
to some extent but do you have any intuition on on how that information is kind of partitioned
between the scales and how those causal pressures work between the scale yes I do wonderful you're
very well read aren't you so it's nice that you you mentioned George Ellis I use the word top
down bottom causation exactly in the spirit that he writes about it so I had literally out here had
him in mind but so I was hoping if he ever hears it he'll know that I was talking about him so it's
exactly that and it always makes me a bit queasy when I use the word emergentism which I think
some people say there's no top down causation in emergentism but I I don't fully understand the
philosophy of it but it just acknowledged that sorry yes I've distracted myself from your really
important question which oh yes the um so the the different scales um of a Bayesian mechanic
self-organization viewed through the lens of Bayesian mechanics um I think um what we've just
been talking about and I would imagine with with Mike as well a lot of focus of here in that kind
of work um is um I hesitate to use it but I will use its spatial scales you know how how how to
element how the single cells assemble into multicellular why but you know how on earth can you
envisage the emergence of multicellular organisms as Mike's done some beautiful theoretical work
you know several years ago now just just think about it to be a skin cell to be an epithelium
means you have to sacrifice the ability to reproduce so it's if you like completely
paradoxical from the point of view of natural selection you have to sacrifice yourself with
the greater good so there are you know there's a wonderful questions about about um um cells of
cells and of cells and cells as you build up to different levels of um spatial scale but I think
your question will be better addressed from the point of view of temporal scales um and again you
come back to this um universalism um now I'm getting fluent using that word um where you've
got the same principle playing out yeah exactly the same principle exactly the same mechanics the
same Lagrangian um playing out at different scales that um where each scale contextualizes and has
this circular causality the bottom up and top down um um aspects to it in play so my favorite
example of this is just to um look at a succession of belief updating um processes um from the very
very fast which would be um from the point of view of um you know sentient machines it would
be inference inferring states of the world as they are in the moment so state estimation
Bayesian filtering um um everything that um you know speaks to some kind of situational
awareness on the basis of some smart and hopefully smartly sampled data um and then we move to the
next level I'm going to skip attention and precision but there is an intermediate level which
usually um in in the neurosciences has has a time scale of your your hundreds to just uh
of milliseconds to seconds but I'm going to I'm going to jump straight to learning
so what's learning well it's just slow inference it's just basically um slow belief updating um
where the states that matter now are equipped with another kind of label which we call parameters
or weights in machine learning in neural network but they're just random variables that are
brought to the table to explain uh or part of our world moral generative model but they're special
kinds of parameters because they change very very slowly and then you move well okay
so those are two levels what about turtles all the way down and turtles all the way up
well okay what's the next level well the next level is now the structure so now for any given
graph for example um that is equipped with edges and those edges will have to have some
slowly varying parameters um that describe the you know the nature of the message passing on
those edges um there will be um there will be a you know you are conditioning on upon a particular
structure and you know is there a connection there is it a hierarchical is it heterarchical is it
you know a unit is it um is it a transformer you know it's a convolutional you know you've seen
this wonderful evolution of structures in machine learning over the past few decades as people try
out different structures and you know some work for one kind of application of the work for others
but um from the point of view of your question what we are seeing is a kind of structure learning
that's playing out over years so this but it's exactly the same principle it's this free energy
minimization but just in this instance the free energy now is a pathological it's just the average
over a long period of time which is the um which is the exactly the quantity that people doing
structure learning or Bayesian model selection use when adjudicating between different graphs usually
of complex system models for example um you know and you could argue that now over
several tens of years or hundreds of years that you know exactly the same maths could be
leveraged to provide a formal description of natural selection as Bayesian model selection
exactly the same thing as you're doing now when you're selecting whether to speak or whether not
to speak or trying to infer you're selecting the right hypotheses about you know the narrative that
you have in your head that makes sense of what I'm saying um exactly the same maths and mechanics
is unfolding over the millennia um you know in terms of uh in terms in terms of evolution
so I think that's a nice example of the separation of temporal scales but the conservation of exactly
the same principles that have this information geometry and implicitly intelligence uh in of
a basal sort I use the word basal because that's what Mike Levin and Chris Fields and
Jim Place were at like using this it's this notion that um basal cognition and basal intelligence
transcends physics psychology and biology um it's all the same thing this is a line from
Chris Fields as a friend of Mike Levin yes yes and I think that's a great notion um and I think
you one can do that very gracefully by being a universalist and just by finding the right principle
the right sort of um reading of dynamics and you know that reading you know for me
is the information geometry that um that supports the belief updating I was going to do Markov
blankets later but it feels relevant now and maybe we should bring a bit of continuous versus
discreet in so a few things came to my mind when you were talking about that first of all we think
of um emergence over time and self-organization over space and I guess it just occurred to me
that are we only interested in time and space when we talk about this kind of structural learning
and then um with these Markov boundaries I had only thought of them um in at one point in time
but you could actually think of a kind of three-dimensional Markov boundary over time as well
now just to remind our audience um blanket states facilitate the interaction between
the internal and the external conditional independencies the external states are independent
from the um the internal states as long as we know the intermediate blanket states now um
to get to the uh to a core issue that we'd be modeling in complex systems you know like where
do you draw these boundaries and is it boundaries all the way down yes um so I think you're absolutely
right this is the perfect time to bring up this sort of uh I hadn't really thought about a notion
of boundaries in time and sort of um that's intriguing so but you're distracting me I well
I'll think about I'll think about that after our conversation and but no certainly so um there's
certainly a lot of current interest in um taking the notion of Markov blankets which is foundational
in this sort of reductionist lens of the free energy principle as a Bayesian mechanics um you know
the one could could summarize but the Bayesian mechanics of the free energy principle is simply
just another kind of uh quantum mechanics or statistical mechanics that inherits just from
this partition that is the Markov blanket that separates the the inside from the outside yes um
now um and of course there are lots of vexed issues about one how do you identify those Markov
blankets and how long do they endure for um so there's lots of interest in that at the moment
but one very simple approach to um um the question how long do they endure for is to say well that's
not the right question because we've just talked about separation of temporal scales
so you have to say at what temporal scale well you operationally define the Markov blanket
as the time over which it exists and what would that look like then when you certainly now think
about um this situating that temporal scale within the context of a larger temporal scale
so what you now have is a a picture where big Markov blankets blankets and blankets things
that define say you you and me or cultures or um um nation states or institutions that um out live
say species um these big ones are last for a long time but they contextualize and provide
constraints on uh Markov blankets at this at a smaller scale that last for very uh for much
much shorter periods of time and so on all the way down so that at some level say at the molecular
level from your perspective these Markov blankets may only exist for nanoseconds or your
milliseconds um but from the perspective of the molecule thank you you know this this is a lifetime
and it's well happy complying with or can be understood as um you know doing its own basal
intelligence do its own basal leaf updating just automatically for its lifetime which may only be
a few hundred milliseconds um you're making sense of its world or inter being interpreted as having
this sort of um sort of your biotic intelligence and self-organization just because it exists for
that period of time so then again this interesting question you know the how does one time frame
contextualize the other starts to bite and you know you start to now think about um from the
point of view of a slower time scale what would um a succession of Markov blankets uh look like um
and what one uh immediately encounters is the notion of a of a wandering eye tinder and Markov
blanket yes um we spoke about vagueness on on the first one so we'll we'll park that but the
wandering sets is very very interesting but um I there were two things that I that I think I wanted
to understand um you do think that there's a hierarchy of blankets but I'm interested in
exploring this idea of whether the blankets could be observer relative because you used the word
understood as a Markov blanket and that brings up two things to the core of my mind first of all
the extent to which they are a lens versus describing something in reality I see and if they
do describe something in reality you spoke about this symmetric causal pressures which we can we
can speak to as well but but on this understood thing it's very similar to um Wittgenstein said
the meaning of a word is in its use and that meaning in language is is embedded in pragmatic
actions and the language game and so on and and then like the the further forward occurs well
maybe the the understanding of a boundary or a you know Markov boundary blanket could be um
understood in the context of one's perspective right that's a great question and um and I
it may be informed by reading some of the philosophical literature and I should remind
you of course I am not a philosopher so um so what I say will be naive um you need to speak to
philosophers about this but I would I would say the Markov blanket um is is something which is
metaphysical um you know it it is defined by a particular spastic structure when formulating
any state space in terms of dynamics and specifically you know a long fan equation
so one talks about the free energy principle as a first principle account um but it does
actually commit to something it commits to the notion that there are states and that those
states have a separation of temporal scales that disambiguates or separates systemic states
from random fluctuations so but that's all it does there are no more assumptions and then
everything else follows from that under the understanding that for sufficiently large systems
the probability of there not being a Markov blanket is um um zero um so when this comes back
to the sort of um this sparse coupling conjecture um that Christopher I think was alluding to um or
at least we um we unpacked in terms of your if a system comes to uh exists of and it's
sufficiently big it will be sparse and once it spars there um with probability one will be
Markov blankets so at no point now have we introduced the notion of observers we haven't
at this point even introduced the notion of the free energy principle we're just saying um there
will be one way of carving up large dynamical systems or certainly systems can be expressed
as a large van equation or a random dynamical system um um will be ways of carving them up
into one or more Markov blankets at one or more scales yes um and then the question is well what
what would it what what would one uh how would one then describe um the dynamics
for any given Markov blanket so you at this point you are at liberty if you are simulating
a universe to choose the Markov blanket and the scale that you want to simulate if you're part
of this system you're not at liberty because if you're part of this system you have your Markov
blanket so all you see are the sensory impressions upon your Markov blanket and you may or may not
have active states and it brings us back to the other bright lines that make between agents and
sense-making machines that can't act sessile things um so if you're asking um I am I um
are the observer dependent perspective dependent aspects of um this formulation
using Markov blankets as a um a physiological device not if you're part of the system it says
something quite profound about what you could you know what you know what and if you're being
observed by something else if you believe if your world model is populated with the fantasy that
there are other observers out there yes um then um those observers will never ever know your internal
states well what about rather than an observer um an actor or an agent and we'll talk about
an activism in a little bit but even at the microscopic scale there are still affordances
and you gave this beautiful example of um of a type of species where someone might kill themselves
with a great good and that that shows the kind of information sharing that you have in in these
complex systems so similarly if I'm an agent in this system and and I act and and there are these
you've described this cybernetic loop and there are these kind of symmetrical causal pressures and
so on and and and then you almost get this emergence of the Markov blanket so it's so it's
changeable yes no it certainly um that's only true um it is changeable and it's malleable and
it's self-assembling uh and certainly when you have Markov blankets and Markov blankets and
coalitions of Markov blankets for example of formation of in-groups and out-groups and the
like yes you would expect there to be a a you know a dynamics of of the Markov blankets or
who you're relating to so you know the sparsity uh structure and who I talk to and who I listen to
and you know what social media uh you know I commit to all of these things are um products of or
reflections and can all be articulated in terms of you know wandering Markov blankets that's
absolutely true um I I thought you were you were asking um something more about um whether the
Markov blanket is part of a realist metaphysical description or whether it's uh you know I mean
there is another argument if you if you now play god and you now put your universe in in a computer
and now to simulate and write papers about your simulated universe where you can see
the the internal states next to all states then you know I think you're in a different world um
just I get I repeat that's something quite profound about the questions of you know what
is it like to be something um are interesting because from the point of view of the free energy
principle what is it like to be something means that someone's asking that question so they are a
thing and they're asking about another thing so now they're asking questions what is it what are
is it like to um know the internal states of a Markov blanket but by definition you can never
know them so there is something impenetrable about being something else so it's a question
which is unanswerable it's unknowable um um so if um from that point of view I think there's something
beyond the um the sort of the epistemology of you know um of Markov blankets but if you're just
simulating cells or people or you know doing multi-agent simulations then obviously you can
you can choose where to put your blankets and you can you can just use them as a device to um
to understand self-organization to simulate it and to predict it and I think in that instance
you know that you you could deploy your Markov blankets wherever you wanted to at every any
scale you you you thought was interesting for the phenomena of interest that's not quite speaking
to what you're into though which is a changeability of the the Markov blankets or well I mean we
can get into that um because if uh and later on we'll talk about the the real world implementations
of these ideas but um but if you if you did hard code the the Markov blankets then you
would bottleneck the system essentially so ideally you you would actually have the system
to find a sufficient resolution where all of this could could emerge um itself yeah did did you cover
the point of whether the blankets can be um must they be spatially temporally you know should they
have a spatial temporal contiguity and the reason I'm asking that question is when you think of a
blanket I like almost synonymizing it with an organism and maybe that's a bad thing to do and
you can correct me but but if we think of um the discontinuous blankets in information geometry
that's an interesting idea because we kind of visualize them in 3d space don't you yeah yeah
yeah well certainly they'll have a topology yeah um I mean I've noticed earlier on you're talking
about sort of play cells and grid cells which are you know very special and beautiful constructs
which speak to a certain continuity or contiguity aspect yes they they suggest that these um you're
coming back to uh Christopher some of um Summerfield's um book um you know one way of understanding
the um the Markov blankets in the brain so you know we've been talking about Markov blankets
delineating me from a world but of course um your temporal lobe has a Markov blanket as it
wouldn't be a thing you wouldn't be able to call it a temporal lobe so you know the very hierarchy
could also be described now in terms of um Markov blankets within Markov blankets within Markov
blankets and some really interesting questions about what that means for what what part of the
brain can know about another part of the brain and what and what action becomes of course it becomes
a tension yeah I just wanted to slip that in because I think that was in part what you were
driving at with this sort of context sensitive fluid dynamics on in exchanges between Markov
blankets I think it's incredibly important even within the brain in neuroscience this is basically
the the routing of information the deployment of selective attention or sensory attenuation
attenuating ignoring some signals it is exactly that that makes us such adaptive context sensitive
information processing um machines and of course that is exactly what you'd have to worry about
when designing a web with routing and and context sensitive you know who do I listen to uh so you
you know this is really really important um the um so the the the emergence of um I think
you're absolutely right they will have a topology and if I were using Markov blankets and you know
I do and many other people do every day when um thinking about message passing as a statistician
on factor graphs or um which are due to a graphical model a probabilistic graphical model of the
system you're trying to estimate or understand um then you know the Markov blanket is just the um
you know the implicit notes that that are the influence of me or the or the parents of children
the parents of the children you know and that has enormous implications for minimizing the
complexity of the message passing um and you know and I repeat defines a hierarchy for example
so Markov blankets there will have a topology um probably best understood practically from the
point of view of graph theory as opposed to um information geometry um but there are special
kinds of connectivity that you see um such as place fields and I'd also say if you just look
at the history of machine learning there are particular um kinds of Markov blankets that
characterize the very structure of certain neural networks I'm thinking here convolutional neural
networks I'm thinking about you're often motivated through weight sharing and the like but you know
if you look at weight sharing as just the kind of structural learning that's trying to minimize the
complexity part of your free energy or your um negative um log marginal likelihood um then what
you are doing is finding the right kind of spastic fit for explaining these kinds of data
what uh what what what is the architectural principle it's it's the translational invariance
the translational symmetry it's just the contiguity aspect it's a fact that you are sampling data
that is generated in or from a metric space that has a well-behaved metric so I would I would say
that sort of you know place cells and grid cells and many in and many other are thinking about
Chris's friend Tim Behren's work for example um finding grid light structures everywhere
in abstract spaces uh so what that tells you is that that it's likely that we live in a world
um where data are caused by samples from a metric space that has a you know a measure to it um not
all spaces do um I think it'd be you'd be hard pushed to find you know the right kind of metric
for language for example um and certainly if you're if you're if you're sort of building
message-passing schemes or believe propagation schemes on a factor graph you don't you don't
think about contiguity in a metric sense you know you don't measure the distance between one node
another node it's is it connected or not um so I think there are some special um contiguity
there are some special worlds that are recapitulated that emerge in the internal architectures of
intelligent machines that have this contiguity property in virtue of the fact that some aspects
of the causes of the sensorium are elaborated in a metric space but not necessarily everywhere
and yeah well I want to take you back I know you for you I want to where you want to take me
anyway well I will I will finish then because um I see uh in the brain and I also see in the
direction of travel in terms of um building thinking machines um as you get deeper into
these hierarchies and you go from a fine scale to larger scale there is an interesting move between
um um architectures that would speak to this metric aspect in space and in time
to a more discretized topological non-metric representation yes I know I wanted to touch
on that I mean there's a beautiful example I mean Christopher Summerfield talks about
Pierce's triad and various ways that we learn different types of um symbol and abstractions
and so on and there is a famous experiment in neuroscience I'm sure you're familiar with it
where they showed people pictures in a certain order and associations and then the same topological
structure was was recovered I think in the NTL and and this is because even though with spatial
temporal contiguity there is a metric space but the brain learns these kind of associations and
then you can essentially learn these abstract concepts that you know concepts that reverberate
in our language and experience they get represented yes and I completely agree with your um point
about the the revolutionary idea of the um the translational local um equivalence you know the
the way it's sharing with CNNs it's a beautiful idea it's revolutionized deep learning so um
just to finish off the discussion on Markov blankets I wanted to talk about part-hole
relationships so um for the benefit of the audience in logic and philosophy myriology
is the study of parts and holes that they form whereas in set theory it's founded on the membership
relation between a set and its elements and myriology emphasizes the relation between entities
which is to say the inclusion between them so anyway when considering systems Markov blankets
can nest into hierarchies and what connection if any is there you know between that in the
philosophical study of myriology and um I was also going to bring in I don't know if you've
heard of Hinton's glom architecture but it it it was it came after capsule networks and it's
you know we were just talking about these wonderful inductive priors in deep learning and they
they make the problem there are many curses in machine learning but one of them is kind of like
the curse of optimization and um and there's a complexity curse as well and that's why most
of these inductive priors they reduce the size of the hypothesis set if you reduce it too much
you get approximation error which doesn't help you either there's curses everywhere but um
but this is a really interesting prior as well these part whole relationship
gosh yes there's so many issues you bring that I'll just reiterate my favorite one which is the
you know the the notion of um these inductive biases minimizing the complexity I think that's
absolute you know that's something which is absolutely central um from the physicist's
perspective you know this um reading um self organization as self evidencing which is a
philosophically poetic way of simply um describing existence as an optimization process which you
shouldn't really do it's all it's really a principle of least action but you can read it as an optimization
process what are you optimizing the evidence for my world models what how can I carve up that evidence
complexity and accuracy what does having a what would I then mean by optimization what I mean
just providing the simplest account that maintains a degree of accuracy what do I mean by simplest
minimizing the degrees of freedom that I use up in providing that accurate account how do I do that
by the right spastic structure what does that mean it just means finding the right structure
and you know you could actually think of much of evolution and um the trajectory of machine
learning architectures as this game of finding the right structure but there are just these structural
priors that are apt for describing the kind of data you know so if you're you've spent your
entire life doing emnist images then it's going to be the kind of structural priors that they are
that's that inherit from being sampled from a metric space and you'll have all the convolution
I think the capsule I didn't I didn't know about the glom stuff I was like listening to Jeff's
latest ideas because he's always got the right kind of intuitions and and these intuitions are
part of the I think part of our reflection of ashby's law of requisite varieties explored in
hypothesis space about the very structures but certainly the the capsule stuff and the you know
the one aspect of structures of graphical models or implicit factor graphs when it comes to
implementation which I think is often neglected is the orthogonal direction from the orthogonal
aspect you know in relation to hierarchical composition and that's the sort of the breadth
of a model in terms of having factors that can be separated so if you wanted to have
say scene construction you wanted to have multi-object you're the ability to track and infer
and and make sense of data generated by multiple objects what are you saying well there are multiple
things out there and these things again from the point of view of first principle account
have certain conditional independences that is literally revealed by a lack of connections
in your world model what would that look like well if you're a physicist it would just be a
mean field approximation literally factorizing introducing conditional independences and factorizing
a massive joint distribution at any one level in your hierarchical model into a number of factors
that if you're a neurobiologist would now be functional specialization and modularity of a
fedorian sense you know what aware in the brain would be the the the conical examples so I looked
at that as as a move towards paying more careful attention to the non convolutional aspect you
know those things that that actually deny a translational symmetry and actually celebrate
the conditional independences within any one particular within any one particular scale
so my guess is and I have no idea so I'm talking from a point of view of complete ignorance but
my guess is that if it's an extension of capsule networks then it will have that aspect it'll have
that separability that sort of things that can do stuff and account for attributes that are independent
from other attributes or other objects that are that are conspiring to generate the data and certainly
my world of toy prototypes generative models usually in matlab you know then all the heavy
lifting is done from the the mapping between the levels again so all the interactions you know
big red buses there's bigness as redness as busness these can be factorized but they all
have to conspire literally through interactions and highly non-linear operators
and then generating what I would see if there was a big red bus so you know that puts a lot of pressure
on the the likelihood tensors or the mapping from you know you'll say a sense an input layer to the
the first hidden layer for example and then sort of leads you into all sorts of interesting issues
about you know how do you accommodate that non linearity and do you again as looking at the
evolution of machine learning architectures as an evolutionary process you know at relu or you know
tannate whatever you know that is another aspect of this sort of structural learning I think we'll
be very much finessed I think if we just move to sort of quantum operators and just go to discrete
state spaces in the in the spirit of quantum loop gravity I think sort that out and then we can worry
about what particularly non linearities I've wondered away what was it what was your question my
point well no this this is a wonderful breakpoint so um before we hit record we were talking a little
bit about chat gbt and just before we go there because that's a fun discussion I think one of the
things that really distinguishes your your line of thinking me obviously there's the uncertainty
quantification someone but um also there's this idea of an activism and I wanted to just do a
whistle stop tour of that as a contrast to these more monolithic approaches to AI like like chat
gbt so um an activism contrasts with representationalism which is this idea that I you know I know
everything about the world inside the model and um you said that you can't just think of the brain
as some behaviorist thing it's a dance of dialogue you act in the environment and the environment acts
on you in a cybernetic loop now you also gave the I'm quoting like an interview that you did
previously you gave the example of radical an activism that you can dispense of representationalism
entirely and you gave this beautiful example of this walking robot that kind of fell down a hill
and it did so so gracefully it looked like it was walking it was all in the body um you know if the
body is sufficiently tuned to the environment you don't even need cognition and of course we'll
talk about the decomposition of cognition in a minute um and and you bring in this idea of
circular causality uh so we're causally embedded in the world by directionally essentially now
when we decompose cognition um I think of things like thinking and feeling and knowing and acting
and and the environment and and so on and I think this is one of the key things that distinguishes
your your line of thought so could you give us a bit of a whistle stop tour of an activism
yeah so um an activism is at the heart of the circuit causality that that that um follows from
the very existence of a Markov blanket um and a Markov blanket is the thing that would be it's
certainly in my world necessary for the existence of something that is demarcated or individuated
from something else so just having a separation between thing and nothing or not thing um
having uh that separation requires you now to think about the two-way traffic um the bi-directional
traffic and of course now you've got two directions of travel that can be thought of as um the agent
if you like sensing the environment um on the one hand and on the other hand the agent acting
upon the environment or vice versa um you've got now a circular causality so coming back to sort of
your the the the the notion of a perception action cycle and this notion of a dancing and a
a derdic exchange which is bi-directional two-way traffic between the two so when you apply the
free energy principle in practice to um emulate or simulate sentient behavior behavior that is
predicated on sense making um then you are necessarily simulating action perception cycles
so you're necessarily inactivist and that's called active inference so sometimes just for fun i put
an en in front of it called an active inference i have to say the active inference was really a
nod to active learning it was just it's same idea but um but cast in terms of fast belief updating
as opposed to sort of slow evidence accumulation in terms of learning contingencies um so that use
of inactive is at the heart of applications of the free energy principle and obviously
has been at the heart of i think all right-minded formulations of behavior and self-organization
since Plato probably but certainly you know things like perceptual control theories cybernetics and
all of that good stuff everybody at some point active sensing um well you know what will will
have to commit to this um active aspect um even to a certain extent semiotics i think well
no perhaps we shouldn't gather um but the so that would be one way of um sort of celebrating and
foregrounding the role of action um and what would that look like from the point of view of
machine learning and computer science well it would look like basically smart data mining
and it would change the nature of the game from um big data making sense of big data so having
everything on the inside having access to the entire world um and then making sense of that and
you know you may ask what does that mean by making sense of it well it's certainly doing
something with it like generative AI versus um the the the complementary approach which is much
more in line with this sort of complexity minimization the imperatives for the sustainable
self-organizing self-assembling systems versus smart data um so the job the action now is
this basically what moves do i make on the world to get the right kind of data that will serve my
imperatives what are my imperatives to maximize the evidence from my model of the world and if
i can do that by definition my um i will be there or put it other way around if i you know if i exist
then that is what it looks like i am doing so you know you talk about the or you mentioned this notion
sort of monolithic big systems um versus small agile intelligent little agents that go and get
the smart data that they need or that they think that you need that is exactly um the sort of picture
that underwrites this notion of distributed cognition and that a different kind of network
and a different way of relating to intelligent artifacts and information services and um and
apps uh where it's lots of really small smart things who are actively getting the right kind
of data that they need to um resolve uncertainty about the context in which they find themselves
again that you know the complexity minimization gets in and the things are very i'm sure we've
talked about this before but i can't resist just um mentioning it again from the point of view of
sustainability and climate change you know the direction of travel of these large um say large
language models for example is it's so wrong uh wrong from the point of view of the ideology of
climate change but also wrong from the point of view of landowners principle and the jeniski
equality when read as a thermodynamic corollary of self-organization and non-equilibrium you know
you've got to minimize the complexity minimize um all of the internal machinations so that you
you just need the minimal amount of data um expertly handled every little agents and good
scientists designing the right experiments and get the smart data that resolves uncertainty about
what it doesn't know uh the job done if you if you can do it like that so that would be one um if
you like sort of answer to your question your um the implications and the importance of inactivism
used really just as a euphemism for an agent that can gather its own data the radical inactivism is
i think a more of a philosophical thing and it's more of a fun argument and i don't know any radical
inactivist so i you know i i don't i don't have um i don't have the right sensibilities uh to
just really answer this question but it's for what i understand um they you know they they have um
taken it a little bit too far and uh denying representationalism um uh to the extent that
you know the you can get a kind of um sense making that doesn't actually involve any internal
dynamics um and um you know i'm sure that's true and i'm sure that you're going to sell um uh chat
cheat uh gtp to me as well as one one example of this mindless kind of uh sort of you're inacted
um sense making um it's the opposite i think um i would call chat gbt extreme representationalism
all right yes we'll resist the urge that we'll go there immediately after this i i promise but
um so clearly we we think that intelligence does necessitate embodiment i think that's clear but
i want to just explore this continuum between um inactivism and representationalism um
um this is this is really interesting so um this comes down to grounding to a certain extent so
cognition must be grounded in different domains in in the physical world possibly in language in
acting in affordances in knowledge as well and so this this is a view that that it that it is
grounded let's say in in affordances but if these agents these organisms ground their cognition
in affordances then to what extent could you say they are learning a world model if that model is
with the lens of an affordance i think i think you can sort of do this um reconcile um very rich
representationalism um in the service of inactivism simply by noting that um in the circular exchange
you have to deploy the right kinds of actions and that's going to require um coming back to
what we're talking about before in terms of planning yeah being able to build the right
counterfactuals no and to do to do that in an expert way in an intelligent way you need to
be able to represent the causal contingencies and states affairs in the world at this time
upon which you're predicating your next action so i i'm now just thinking you know
just looking at your your expressions and i now realize that you that you were trying to sell
a dialectic between uh inactivism and representationalism for me um they are the same thing
you to be a good inactivist you have to have the right representations if you want to
to well may i say good i mean um good at a particular scale the big things that survive
so viruses don't need to do much planning um but things like you and me do need to do a lot
quite a lot of planning so as you move up those scales you have to look further into the future
so that we certainly need representations which is what i have which um which is why i'll smile
always smile what i think about radical inactivism um so if there's no space in radical inactivism
for being able to plan to imagine scenarios to have narratives that play out on the inside before
committing and selecting the right way forward yeah and it's not an apt description of certainly
cognition okay it's so interesting because the question to me is we just spoke about this idea
of planning potentially through an information geometry and many of the abstractions that humans
learn are not grounded in the physical world at all and that's very interesting and we can get to
how those abstractions are learned but i guess what i'm saying is let's say one of these agents in
this multi-agent system it's traversing this topology what grounds the states in that topology
well i think that that would be um ultimately it will be the transactions um you know with
with the world the underwrite um at the lowest hierarchal level all the generative models of
this hierarchal sorts of the abstractions are just the coarse-graining simplifications that you know
the the um the products of literally looking at the lower levels of your hierarchy through a
reductionist lens in the right kind of way that enable you to carve up the world in terms of these
in terms of these abstractions which may live in a non-metric space they may still have this
ordinal structure you were referring to before and then what that would mean in the context of
lots of similar artifacts who had done the right kind of coarse-graining would be now
the opportunity for direct belief sharing your brain-to-brain communication speaking to a friend
of mine you heard yesterday talking about the distinction between communicating through um
the through sensory um um exchanges such as language but what we're actually doing is basically
directly exchanging beliefs um of course you could do that directly um in silico you could
actually have messages about what this agent believes at this abstract level of um of representation
very high in a you know hierarchal graphical model it could pass its sufficient statistics to
another partner somewhere else in the world um elsewhere on that on that graph so you do have
the opportunity now for direct belief sharing um and um there are all sorts of interesting questions
about you know how you'd engineer that you know you can't coming back to this fact you have to
have sparsely in the game you have to decide where to to send and receive your beliefs from
then we have this sort of attention and routing problem and then you have do you have peer-to-peer
communication do you go through a server can you write that down in terms of um quantum
information theories a holographic screen and who's you know having all sorts of really interesting
things but what they speak to is that you're now you're sharing beliefs literally basing beliefs
probability distributions as encoded by sufficient statistics that can be passed um as messages uh
on a on a factor graph um you're now talking about the ability for proper communication
of the kind that has evolved in terms of cultural niche construction and evolutionary psychology
that speaks to cultural niche construction uh that we enjoy in terms of you know the words
that we use and the and the exchanges that we use yes yes indeed there are two things to bring in
so knowledge and language and you did invoke Andy Clark in another interview which is the
extended theory of mind so it's one of the five ease in cognitive science and the the communication
substrate could of course it's it's distributed so the two agents could be referring on missing
information that actually exists perhaps in another agent or another another knowledge repository
and then we can also talk to exactly what the role of language is and how it's compressed how it
represents abstract concepts and so on um so i'm very interested in knowledge and language i'm not
sure if you could bring those in right um i'm not sure i can do so expertly but certainly
part of learning new things since my foray into into industry is this notion of knowledge graphs so
if you know if you read a probabilistic graphical model as um at least its structure as being a
knowledge graph then embodied um in the structure and presumably the parameters of the connections
that constitute that graphical model that would be knowledge so for me um in a very non-mysterious
and possibly too simple minded a way knowledge is just the product of um belief updating of a
slow kind which is learning so i know contingencies um in the sense that i have um a suitably configured
optimized structure and parametrically weighted um generative model that can be written down as a
knowledge graph or a graphical model um upon which i do my message passing to do my inference about
the particular context sensitive in the moment kind of thing so i would put knowledge um basically
as an attribute uh that is implicit in a um a graphical description of um uh an implicit
generative model or world model that is um actively um that is used actively to do the
data mining to do the you know do the inference and uh and the sense making and then the language
part of it um would just be that um highest level most coarse grained summary that is conserved over
multiple agents so just by definition um if i want to do belief sharing um i have to um i have to
have a a shared generative model or a commitment to the same narrative so that the meaning of
what i'm emitting is received in the right spirit or the right frame of reference by you
and indeed um there's some lovely rhetoric from quantum information theory of the kind
that Chris Fields and Jim Glacebrook have been pursuing where you literally have to
think about the generative model as a quantum frame of reference and we have to share that in
order to communicate so in this instance the um the Markov blanket ceases to be just a set of states
and see and and adopts the role of a holographic screen and action now is writing to that screen
and sensation is now reading from that screen and there are two agents on either side so you know
whatever is written is an action but can be read by something else and then you're looking at the
entanglement which is the the synchrony of mutual understanding that you know we would aspire to
through through through communication so that that that um but that only works if if the messages
that are written to the screen or written to the Markov blanket um um have the same kind of
interpretation so it speaks again to this um the the fact that you have to have a good model of the
world and the world has um and that really means that there's a kind of entanglement or
generalized synchrony from the point of view of dynamical system theory between the two sides of
your screen or the two sides of your Markov blanket or your server um um which have you know the right
kind of isomorphism so we're talking about you know basically a shared narrative that underwrites
any exchange of signals and again what you know just thinking of of this from the point of view
of generalized synchrony what we are talking about is just the characteristic
or the emergence of characteristic behaviors in any sparsely coupled set or ensemble of
things that possess Markov blankets or have a separability um um uh what will they ultimately
do they'll ultimately find a synchronization manifold they'll find uh a synchronization
simply because um this is the um the most likely state of being and the most likely
state of being is that which maximizes the marginal likelihood which minimizes the free
energy which just means that they now understand and can infer each other so you know it's just
another expression of this existential imperative to resolve uncertainty have good models of my world
and my world also needs to have good models of me and if my world is you you have to have
a good model of me and I have to have a good model of you ultimately what that means is
we're going to converge on the same model um yeah I mean what interests me there is that
unlike many other researchers you're conflating understanding you know knowing and intelligence
itself I guess I guess so yeah um so good question um if intelligence is a process of belief
updating then yes it's just intelligence and learning and just learning knowledge and intelligently
inferring states of affairs are just the same process at different time scales and they both
depend upon each other so I have to have the right neural network of the right weights and the right
learning to make sense of the data in the moment and if you think of it from the point of view of
weight learning um either through propagation of errors or through experience dependent or
spike timing plasticity in the brain you have to have the right inference in order to do the
learning so there's again this circular causality between between the two scales so knowledge
requires the right inference or state estimation and state estimation requires the right knowledge
to make sense of the data that's being assimilated yes although I suppose philosophically we could
break it down because you know knowledge might be the sort of the information acquisition
because knowledge exists it's quite an interesting philosophical point actually that
knowledge is on Wikipedia and it's crystallized knowledge it exists as a thing but the ability
to to acquire knowledge without surprise is is is your your form of intelligence but I wanted to
talk about chat gpt just quickly and it's very interesting because Bing have just released
a new version of their search engine which integrates chat gpt and I've I've been on a bit of a journey
when they released gpt 3 in the it was November 2020 I got access to it I thought it was garbage
it was just generating a load of rubbish basically but there were people out there who were true
believers and they said Tim you're not seeing it I you know I I've seen it and they would show me
these ridiculous examples and I just thought no you're just fooled by randomness and and then
DaVinci 2 came out about a year ago and then that that transgressed the anthropomorphic
fooled by randomness threshold so I started to be a bit of a believer I knew it didn't understand
anything but it started to get very useful and I was using it for coding and generating emails and so
on much much to my loss actually because recently I've been checking code into the to the repo and
my colleagues have been saying to me Tim it looks like you used gpt to generate that why is it full
of holes and you have to hold your hand up and oh yes I'm sorry I just I just checked in some code
that I clearly didn't understand and you know didn't actually save me any time so sometimes you can
see problems with its generation but it's so plausible that most of the time they're hidden
and unfortunately if you want to verify that knowledge you might as well have just not bothered
using gpt in the first place because you could have just gone so a little recap it's using a
self-attention decoder transformer and that's a neural network architecture that introduces
permutation invariance to tuple permutation invariance which turns out to be extremely
useful for language and then there was this discovery of what's called in context learning
which is where rather than just getting it to because it's a generative model it just generates
token token you you insert a prompt and then continue to generate from that prompt and people
discovered you could ask it questions it had this emergent reasoning capability in big air quotes
and then more recently people have done what's called preference fine tuning which is that you
do some additional supervision on the top with human reference examples and that aligns it to
humans and makes it give slightly more politically correct or you know more sensible answers so
and now it's been integrated into gpt and that does this retrieval augmented generation which
means rather than just being a snapshot in time it will also go out go out to bing get some relevant
search results incorporate that into the prompt and then generate from there and there was this
incident a couple of days ago where Bing had this successful launch to much fanfare
and then people looked at the results it was generating including fine you know one of the
the things was give me a comparison between the financial results of lulu lemon and some other
company and it was just hallucinating the results it gave weren't even in the document
and the product managers at microsoft didn't even themselves bother to check the truthfulness of
this generation so god help anyone else using Bing um so what's your take cut
i love that story thank you um as we were talking about before i i've heard so much about chuck
gtp but i haven't been able to get on it because because it's always always been used over some
subscriber it's a it's a wonderful moment isn't it and so many issues there um i don't know where
to start with um perhaps i should start um by um conversations i've heard um about why the
chuck gtp moment is so important um always reduce really to um the fact that people got
in you're enchanted and had access to it so it wasn't so much other but it's actually really
interesting to hear about the technological and the structure of the genetic model makes it work
i didn't know that that was that was very useful but whatever you know the those are not really
sort of quantum leaps they're not massive technological you know innovations um but what
the innovation was of the accessibility so i think you know just standing back why that was a
moment and it has been a moment i think you know in terms of selling ai to investors and the like
they are they all now oh you're talking about chuck gtp like stuff i know about that that's
really exciting uh so it has changed the landscape i think uh there has been a moment um but why did
it happen i think it's basically all this belief sharing i think it's you know basically uh the
participatory aspect it is exactly um this um if you like sort of emphasis on belief sharing among
lots of smart agents including ourselves which is you know which if you can realize that potential
and getting people engaged uh is the is the nice way to use uh artificial intelligence um and in
that respect you you ask yourself well how is it being used um and it's being used as gerative ai
and of course you've asked yourself well okay what what's gerative ai got brought to the table
well it's generating the kind of stuff that i would see um and you're basically it's an
interpolation machine you know sort of if i give it enough stuff it'll interpolate and generate the
kind of things that i've given it um so why is that useful well you can now select from the stuff it
generates uh and you know but all of this game is all quintessentially dyadic interaction or
you participating with the generative ai that's why it's so attractive it's not the marvelous
stuff it generates it just interpolates stuff um you know the interesting bit is when you now
have the opportunity to select oh i like that one i don't like that one i'm going to check that code
before or i'm not going to check that code before before putting it in um so i looked at from that
point of view i think that both those if you like the um why it became so much foregrounded in people's
conversation and in the media uh and why more generally people are have been enchanted by
generative ai i think they both speak to the fact that you're actually engaging people
it's a dyadic exchange um of an asymmetric sort and that asymmetry um is exemplified by
generative ai that there's all the action all the choosing all the triaging all the selection
and what to actually show your friends or send off in your email is done by you the human user
so all the inactive bit is actually done by the human still you know the generative ai in and of
itself is not actually acting because it's generating content the other interesting thing about
generative ai is that it's generating content not beliefs so unlike google maps which actually
gives you a belief about the you know the best plan forward um it's actually generating content
it's in data space so from the point of view of a statistician or from the point of view of
a physicist committed to a um a holographic screen or markov blanket formulation of exchange with
the world um notice that generative ai is doesn't have doesn't need to understand because that's
not its purpose its purpose is to generate sensations to generate data to generate stuff
in content space or data space stuff that has been mined in the space that the mining took place
not in the sense making and the the abstraction and the understanding space so you know i i think
that's an interesting distinction which which um that you know i'm sort of going off an attendant
here but it's interesting when it comes to what do you mean by belief shurning and communication
but just look at that um that observation in light of the discussion about why
gtp is so successful it's successful because it generates language so the content now is
the belief and it's the kind of belief structures that have been honed through probably not is
yeah certainly millennia of cultural niche construction and uh so your language is a distillation
the most efficient way that we can carve up our knowledge of our world our lived world
and now the generative ai which was previously just limited to generating pictures and content
and sound files and whatever um is now actually generating stuff which is it in a belief space
because we have evolved language so i i think there's something quite special about generative ai
and large language models simply because they actually generate content in in the context of
language which you know has this um speaks to knowledge uh also it has abstracted and distilled
the kind of representations of of our world yes you're in the most efficient way uh
that yes i mean there are so many things we can say that they are a materialized snapshot of our
sense making our abstractions our world knowledge you know of the wittgensteinian language game if
you like but they also have a truncating effect and they introduce inertia because it's a static
model right and they are as you say that they produce traversals in word space and people
don't understand that these are random trajectories with some kind of modified form of maximum
likelihood estimation uh much more stochastic than people realize and we can discuss the degree of
how creative they are and what creativity is maybe creativity is just a random traversal
through some abstraction manifold and i i loved your poetic description of this kind of didactic
relationship between humans and machines much like an extended mind and this is where prompt
engineering comes in because people have realized that you can say to the language model that's not
quite what i wanted can you change it a little bit and it's an interactive process and that's why
as a conversational interface it's very very powerful but the problem is you can say to it
no uh two plus two doesn't equal four it equals five and it will say oh i'm so sorry i actually
meant five so it's it's polluting the infosphere with misinformation false news probably getting
out all this kind of stuff and i never really thought the the misinformation thing was a problem
because you know there's loads of misinformation out there i mean most people are full of shit
frankly uh carl but now it's been industrialized and democratized on this scale and people will
not bother fact-checking they'll that people see plausible text and they just take it as a given
and very very soon there'll be so much information out there on the internet more than was generated
by humans most of it will be generated by machines and we won't know the difference
and that reminds me of you know one thing which um the ambivalence that that whole issue induces
in people so you know this this um this this tendency to write in meaning an anthropomorphic
size that you know the the the content generated by generative ai i've heard um actually by the
by the second author of the white paper we started with it smashed the chewing test you know
and i guess it has i guess it has smashed the chewing test uh um but as you say there's
a price to be paid if you can't discriminate between sort of uh you actually had a nice
word written i hope you're going to use uh a game which is confabulation yes yeah you know
that's a great way of describing it so when i was talking about sort of interpolating generating
content novel content that is an interpolation it's you know that would be a confabulation it's yeah
i'm just you know mindful of the the fantasies that that were generated by uh jeff hintons weeks
sleep wake algorithm also you know the original the originals of amortization of
of um well yeah very short on coders i guess you'd you'd you'd think about nowadays um but the
you know this notion of confabulation i think is is a splendor i haven't heard it i haven't heard
it expressed like that but but that is a beauty of generative ai i guess what you're saying is if
people misinterpret that as real information that could be problematic um i'm too i i'm being
a bit older than you i'm slightly more mellow about this i'll just very quickly tell you a little
story i had to for a friend or a colleague at least an email colleague um in america i agreed to uh
write some blurb for his um uh 200 plus word book which is a philosophical model but he's also very
informed in terms of artificial intelligence and he sent it to me so i speed read it at the weekend
in order to write a three or four sentence blurb for the publishers um and halfway through i suddenly
had the awful realization that this may have been written by cha chi tp oh no it's a wonderful book
and i wasn't quite sure so i actually put in the blurb this this is a 21st century Turing test
uh there you either and you know i'll just advertise it now because this will probably come out by
the time people watch this i think it's called the hidden illusion uh and either this author was
very very skillful in writing the book last year and pre-empting um the public release of the of
these things pre-empting to the extent he could emulate the confabulation of a large language model
because part of the novel actually because the the protagonist the hero is actually working as a
on large language models for a tech startup it's a love story yes but it's interwoven with things
that he's actually generated on his large language machine yes and he um and that comprises part of
the model um but i i generally don't know whether the rest of the narrative was was actually written
by a large language model and then he's carefully gone through and triaged it
that's entertainment but it is entertainment that that really challenges can be viewed as a
Turing test which i suspect most people will fail and i suspect that book will be talked about
simply because it's very difficult to tell how much he wrote versus how much the machine wrote
and you know so but that you know but as long as it's kept to entertainment that's fine if it's
not then we come back to smart data mining we come back um to intelligent agents that just
don't confabulate content in the context of generative AI you actually need um the ability
of smart agents to go and you talked about fact checking what does that mean this basically means
having an explanation or a belief at hand that provides an accurate account of all the data
that is internally consistent so okay we're coming coming back to the the fundamental
principles of good modeling that can be quantified but to do that you're going to have to equip those
agents with autonomy and so to make equip those smart data mining machines with autonomy to be
able to select the you know the right kind of data that will hopefully preclude the you know
the confabulated data or things that look like data but in fact not uh not data i don't know how
you do that but that's going to be the challenge for the future well exactly i mean in the free
energy principle and that you have this entropy and and and it will actively seek out gpt never
says to you oh i don't i don't actually know that can can you explain can you explain more
different modes of understanding right so the reason why we don't confabulate is because we
actually understand things and these models just learn very very superficial surface statistics
um about language and and how it's used and i think it's going to change the the peer review
process because now so much of this stuff slips um beneath the net and the amount of um due diligence
and rigor that is required to weed out some of this stuff because most of it has gone undetected
i think that's the problem people don't realize how big the problem is because the mistakes are not
immediately um obvious on on the surface so um you said that we believe that developing a
cyber physical network of emergent intelligence in the manner described above not only ought to but
for architectural reasons must be pursued in a way that positively values and safeguards the
individuality of people as well as potentially non-human persons and i wanted to bring in the
is ought problem as articulated by david hume and he said it arises when one makes claims about what
ought to be that are based solely on statements about what is and hume found that there seems to
be a significant difference between descriptive or positive statements about what is and prescriptive
or normative statements about what ought to be and uh that it's not obvious how one can
coherently move from descriptive statements to and prescriptive statements and i do want to draw
a little bit of an analogy here to our discussion about consciousness and i i i can bring in consciousness
again but you know charmers said there's this kind of um hard problem and consciousness is a little
bit extra and similarly um people say the same thing about morality that it's a little bit extra
and it might not be deducible from all of this empirical data that we have in our in our models
and hume of course was a famous empiricist and and i think you are one as well probably the
extreme version of that so the free energy principle concerns itself with model evidence and entropy
but evidence is not an ought you know so the question is how do moral states come into be in
in the system right okay i didn't know about the is or stuff that's really nice um um
so if i understand what you said correctly um then is is ought that completely dissolves
the distinction so if you know from the point of view of the free energy principle um existing
in particular characteristic states are those attracting states they are part of an attracting
set that define who i am and collectively who we are if we share the same kind of world model or
narrative um so they are exactly how i ought to be they are definitively defining the nature of
the thing that i am so if i exist the is is just the ought um and that's quite fundamental because
if you if you then say well what then does um a distributed cognition or an extended cognition
you know this um or a designer environment on the web um you know being undeclared for
a moment if if you think about what that might look like from a first principle account then
what ought to it look what ought it to look like well it will look like what it is um but
being what it is if it includes us it will be like us so it will um it will be effectively um
the kind of uh system where you cannot prescribe any ortoness you know because it is what it will be
it will be um it will um as we have learned in the previous conversations it will come to share
a narrative in a world model with certain levels of abstraction in our own world models provided
we are part of that that ecosystem part of that network one could imagine a completely independent
you know a mark-off blanket between us and an information you know a a worldwide web which
was never used so um so it wouldn't be a mark-off blanket even it would be two separate systems um
and there will be a universe where the the world web does what it does we will never know by definition
but if we are part of that if we are users of either in the sense of um triaging generative
AI or in acting recommendations um or supplying data um and um you know so we are in a an exchange
and therefore part of that web a part of that network part of that factor graph a node on that
factor graph um then by definition the is equals ought as read by the free energy principle means
that the what will happen is all the intelligent artifacts on that web will converge to some
kind of common ground and some kind of common sense making another way of looking about that is
that you know what are the imperatives what are the things that are being optimized if if you
want to use an optimization um approach um it's the um the effectively the expected free energy
what is that it's just minimizing uncertainty minimizing surprises it is not making paper clips
it is not you ought to be good to mankind or you ought to increase prosperity um it is not
monothematic um pre-specified heuristics about this or that it is just um it is all about
resolving uncertainty and when there are preferences that underwrite that uncertainty just
technically just in case this sounds a bit too hand-waving the expected free energy is literally
the sum of the expected information gain and the expected value where the value or the negative
expected free energy is the sum of the expected information gain expected value where the expected
where the value is the um the log probability of a characteristic outcome now crucially in that
statement where you now read value or utility or the ortness um as um a a probability distribution
over the space of outcomes notice that this is now specified over all possible outcomes
so it now becomes a way of specifying ortness that just is um in the spirit of multiple constraints
over all dimensions of outcomes not the amount of money I make or the number of paper clips I make
but over everything and over everything then converts this effectively into a sort of the
solution to a multiple constraint satisfaction problem where the multiple constraints are
definitive of what I am and if I am embedded in a network of sympathetic agents and artifacts
what we are so baked into this kind of belief sharing there should be a harmony and a mutual
understanding at various levels I mean I'm not talking about sort of direct sort of language
to language you know it could be some sensory substitution devices that have a sort of very
elemental very fast sympathy with with our bodies for example um but at least from the
point of view of the information geometries that there will be a convergence which will be effectively
if you're a quantum physicist an entanglement if you're um if you're a uh a dynamical systems
theory person a generalized synchrony of everybody in that web that is necessarily
a facet of free energy minimization on the one hand on the other hand it's also just a statement
of the steady non-equilibrium steady states to which any distributed network will ultimately
converge to it can be no other way in principle yeah I mean many people have an intuition that
when designing utility functions if you look at how markets work and so on that utility and value
are orthogonal and and then we have institutions like the church and government and and so on to
introduce value pressures um onto the utility function and I guess what I'm what I'm getting
from you is certainly from an evolutionary perspective that they they need to not be orthogonal
and um I mean in in religion for example I mean I'm not religious myself but they
they have um they're moral realists and you and you have moral relativists and and they they say that
um we need to hold something sacred uh because otherwise you know if everything's sacred then
nothing's sacred at all so we need to have a difference between the sacred and and the profane
but um they they think that uh morality shouldn't be achieved through consensus and this is this is
what we need to behave properly I don't believe that but I'm very interested to know where morality
comes from and I think you kind of alluded to that in in in your answer so um are they
hardwired in the brain or is it just a kind of like constructivist social phenomenon I think it's
a constructivist social phenomena but and over a transgenerational sort of niche construction
as well but also um in the moment um and constructivist in the sense that the we're not born with
these things so these things are very much part of cultural um culturally evolutionary thinking and
sort of you know um um nurture um that we inherit not just from our mum but also our mum's mum a
mum mum you're all the way back um so they are in the brain they are learned which are um distinct
from innate priors that underwrite my homeostasis so there are certain beliefs I have sub-personal
beliefs about the way I should behave um that are held with incredible precision and conviction
sub-personally that they're hardwired epigenetically and these would include everything that um
leads to homeostasis and then you build upon that and you get to our stasis and you build upon that
and you can probably at some level get to the right way to behave morally and ethically at school
with your in the playground for example and so as you you know so mathematically the the
distinction between these sort of um very fixed um prior preferences or prior beliefs
that are basically just an encoding of beliefs about the states I characteristically occupy or
aspire to or to or narratives that I would pursue um they are very very precise in some dimensions
and you said you know they are or are not orthogonal you know it was exactly that sort of multi-dimensional
aspect to writing down value of something which would be possible to do for a human being unless
you give me your your DNA and also your mother's womb I couldn't actually write write write it down
very very easily but you know the specification has to be upon all dimensions some of which will
be written down with great precision and others will be much more flexible and I think when it
comes to um writing down values over attributes that do not yet exist because you haven't grown your
deep generative model sufficiently deep in order to have that do abstraction then clearly those
kinds of beliefs about the way I should behave um are not even specified at birth but they have to
be learned through interactions with other people so I guess I'm trying to bring to the notion that
it's perfectly okay to have a spectrum of different convictions about the way to behave
that can be absolutist or it can be more forgiving and relativistic simply and you would be able to
simulate that just by writing down very very precise beliefs versus less precise beliefs
the second key thing here is that we're talking about when it comes to
the building beliefs about the way I should behave that basically presupposes you've got to a
sufficient developmental stage to have selfhood which not everybody gets that far about severe
autism you wouldn't and if you know certain other lower life forms may never get that far but
certainly you have to have um selfhood you have to have a model of that selfhood and then you have
to um um ask um have a model of um others that may be actually be a prerequisite before you have a
model of self and then other kinds of people so um when it comes to making decisions of a moral
ethical sort you know it's just inferring what would I do in this context given I am this kind
of person and my limited understanding of some social signs is it is a little that's a little
bit more complicated that is not necessarily what should I do in this context given I'm that kind of
person it's what do you think I should do given I am in this context given what you think what kind
of person you think I am so basically I'm trying to work out what you think what kind of person you
think I am if I can infer that on the basis of our exchanges and my epistemic foraging and my sort
of self-evidencing through language um then um I can then decide what is right or wrong so it's
just basically an inference you know what's the probability of making that decision with those
outcomes given my world model um if I am that kind of person versus that kind of person um to solve
that I need to know what kind of person I am and that I can just get from them or I can get from
my convert my correspondent or my peer group um or I can choose to you know my in-group from the
social media or the the kind of television news that that I subscribe to so you know we're talking
about before about sort of this utopia of um generalized synchrony and perfect quantum entanglement
and we're all in perfect harmony of course it doesn't quite work like that there's a scale free
sort of specialization and you know different you're at a bigger scale different Markov blankets
where you get in groups and institutions and uh because of that scale freeness and because of
it we're all quintessentially curious in our self-organization we're always exploring other
ways of being um whilst trying to find the shared narrative and common ground with people like me
my family my institution my um my sort of um theological commitments um there will always be
other kinds of me and I and I can sometimes you know I can I can sort of people that kind of me
or the other kind of me so in that context it's really a really interesting question about um
really inferring you know of all the ways I can behave what is the most likely way of behaving
and if you can you know if you if you look at morals and ethics through that lens then you
have now a calculus of being able to write these things down in terms of alternative ways of responding
in a given situation and crucially you mentioned before uncertainty so precision is just the inverse
uncertainty the confidence with which I can assert no I will always do this yeah as opposed to I'm
you know 80% sure I'm that kind of person but also noticing you're now conditioning your moral
position or your ethical position um on being a particular kind of person and of course we can
all be a different kind of person I can be a teacher I can be a student I can be a parent
you know I can be a friend um you know all of these will call to four different sets of
prior beliefs because they're all conditioned on the kind of the the library or the repertoire of
ways of being a human being which I learned from you or my mum and everybody else on television
fantastic I wanted to touch on this idea of um I mean as you said you believe that it's socially
constructed which rather gets away from this notion of some people believe that it's kind of
hardwired and then there's the notion of is our you know our values might be changing faster than
evolution essentially which necessitates the need to have some kind of societal pressures or
governance if you like and I do believe that we're in a new regime now I mean people always say oh
the sky's falling down everything's changing but now we are in the information world and things
are moving at a scale and magnitude that they haven't done before and our values are changing
much faster than they have done before and what you are talking to is very interesting about this
kind of fractionation so there are macroscopic pressures and there are microscopic pressures
there's the internet and so on and I just wonder from your perspective how do we how do we wrestle
with that because there are we've never been in in in a more kind of pluralistic interconnected
environment and how does that affect us yeah I mean these are big questions I'm sure you have
your own answers to um but yeah I think I think that that that sort of fast-moving um globalized
exchange um that that that does speak to a deep pathology um and I have a deja vu I'm sure we've
spoken about this before but sort of you know um Zuckerberg and the you know the genius boy
races of of that the previous decade um talking about connectivity as somehow being a good thing
I find quite frightening remember that we've been talking about um structure and existence
right through to morals and ethics in the context of helping the right kind of sparsity that gives
you the right kind of individuation of things from the world and within any given thing the
right kind of structure that allows it to act gracefully uh and in harmony with that world
every point it's the absence of connectivity that defines the structures so if you destroy
that sparsity by over connecting over globalizing you will um essentially destroy it would basically
cancer um you know so if you look at this is my clever thing if you look at um you know
sort of a cancerous cell as a basically a self-organizing system who has forgotten its
boundaries yes then you are um you have a metaphor for the kind of thing that happens
if you do not respect the sparsity of communication and the um the the joyful
isolation that there is existence behind your own Markov blanket so every affront to
um that uh maintaining that sparsity smart data carefully sampled not being overwhelmed
with a deluge of um very imprecise data that it makes it very difficult for me to actually
go and smartly sample and work out what would happen if I go and look over there or look on
that website um you know so what that would suggest to me is that um there will be um
um there will be and I take your point that that things are changing um I I initially thought to
myself well no hang on a second because all of the information and the sense of making and all
the kind of um exchange of information we're talking about is about constructs that do not exist
when I am born um you know you can't have you can't have misinformation uh from the um what you
called the confabulation what I in my blurb refer to as the the flighty ramblings of gtp uh um
you know you can't that has no meaning for some you know a child that has not yet learned to read
or write or to you know so this is something that has to be below the scale of evolution
so I was just thinking that it's okay you know this is a with you know this is limited to um
to um your each um each each uh generation but I think that's probably a false false
comfort um because you know we were talking about sort of um evolutionary psychology and
cultural evolution as well and I think that's what you were talking about I think that that
that then is if things are speeding up um that usually means you've lost um you've lost precision
in your prior beliefs which means that the precision the informativeness the salience the
reliability of information now takes precedence and you increase your learning rate you become uh
in schizophrenia that we call as jumping to conclusions on the basis of sparse information
you're basically looking out there not inside uh to um resolve uncertainty about states of affairs
because you've lost confidence in your in your very structured prior beliefs so if things are
speeding up that basically means that um whoever is now generating um and garnering that information
in the information age um is has lost um has lost confidence or precision in the in their own
convictions and prior beliefs and possibly information morality uh so what's going to happen um
I read um Carlo Brevelli's book um uh uh yesterday in fact he's he's he's he wrote in
2014 just sort of seven things you need to know about physics which was originally um published
each chapter as you know in the in the the um the sunday newspapers with potentials to scientific
discourse and then he ends up with a very very pessimistic he thinks it's all over
I was actually compelled with by that he you read the last few pages uh yeah he makes exactly the
same points that we we've just been discussing from the point of view of physics um at different
levels um and and and he you know he jelly thinks it's over it's uh I know it's so hard because it's
like the hill climbing problem and you can't see behind the next hill and there are always people
who were uh you know techno luddites and people who think that the sky is falling down it's genuinely
hard to know just just to finish on on this um ethics point uh Lisa Felben Barrett was a constructionist
she had a book called how emotions are made the secret life of the brain
and she thought emotions themselves are socially constructed would you subscribe to that view
and I think I probably would I mean she she writes very wisely and has thought for many
decades about the you know the nature of emotion I think you know I would say yes absolutely it's
simply because you know the the notion of building generative models in a neurodevelopmental
context is an act of construction you know it's a free energy minimizing process that is
that explains why we construct better and better explanations which are sort of
you know carving nature in its joints in our head and part of that is is not just about the
state of the external extraceptive world but also our internal world our interceptions our
gut feelings our respiration and everything else so so her big thing and indeed other people
thinking along similar lines people like Anil Seth and sorry well would really always how
nod towards interception and embodiment but now beyond the the situated cognition kind of
embodiment you we were talking about before but actually now about the the physical body the the
the beast machine as as Anil Seth would would say so the physiology so having a constructed
explanation or hypothesis for the way that I put together my gut feelings my interceptive
signals with situational awareness derived from extraceptive sensations and indeed what I do about
I think leads to a very compelling notion of constructed emotions so for example
I am I can infer I can use the explanation on the hypothesis I am frightened as the best explanation
for why my heart is racing why I feel frozen proprioceptively why I have cardio acceleration
and why I have why I cannot discern that dark figure in this dark alley in a city which I've
never been in before all of these myriad of sensations and my low level constructs now
succumb to a simple explanation oh I'm frightened yes that explains everything explains my racing
heart explains this fact I can't I can't see who that is your the potential predation that
would follow from from that also interestingly because of you got this circular causality in
the inactivism the fact that I am frightened means I expect to cardio accelerate and of course under
active inference that's exactly what will happen because you're acting to generate the evidence
for your predictions and for your your your hypothesis about your you your you and your
lived world but also you as you hypothesize yourself to be so you've got this sort of
closing the circle in a sort of James Langian kind of sense the yes I explain my my current
set of sensations as having the emotion of fear that itself induces the very evidence that I was
trying to so you've got this sort of auto poetic self-fulfilling prophecy that you know
is just idea motor theory but in the interceptive domain yes so I think if you read constructed
that's right but I notice he says socially constructed which I I guess is okay yes if
you're taught that you can be have these you know this kind of fine grain repertoire of
feelings or you can use these to explain your own sensations yes I I would imagine it is molded by
you know by by convention and you know by the culture which you come come from in the same way
that you know Eskimo is having 12 words for snow would give them a finer visual acuity and
visual discrimination of whiteness I I'm sure that exactly the same kind of cultural inculturation
speaks to different kinds of elixir thymia and you know the coarse grainness of my
repertoire of explanations for emotional states of mind the best explained me in my
you know in my interactions now car I know you have a background in psychotherapy
can people be evil it doesn't exist in the diagnostic criteria I believe
no it doesn't um so that was out of the blue question which I've never
been asked in public before um as a psychiatrist I think it would be rather difficult
to conceive of that um there you know there are certain there are certain patterns of behavior
and um I do have some psychotherapy but it's group psychotherapy but really I'm a psychiatrist
so you know there's a distinction I think right um from a professional point of view
you know I'd be more like a biological my apologies I didn't mean to
honestly no no I mean the psychotherapy is an important aspect of of psychiatry it's just that
psychotherapists have to undergo you know five six years of training to become psychotherapists I
did two two years of very baby uh psychotherapy training um but um you know in terms of the
neurology and psychiatry you can certainly get sort of certain kinds of personality disorders
and certain kinds of psychopathy that would normally be associated with evil um evil behavior
and it normally basically um transcends the social norms so it comes back again to basically me
trying to work out what kind of person do you should I be which you know my only point of
reference is you so you know how should how do I think you think I should behave and when that um
when that kind of self-modeling doesn't work then you will be you know you will have um
behaviors which are so far from the social norms and morally acceptable I guess you can label them
as being evil when could they arise well when you fail to um have any theory of mind for example
if I am an able to see you as a another thing like me so I may see you as a French or a car or
some sort of your camera an artifact um you're not a person you know you don't have intentional
dispositions or you don't have we don't have a share now that I couldn't talk to you in in any
really deep sense um then obviously I can never ask the question how do you think I should behave
because you do not you know you don't have that kind of belief or that kind of
intentional stance in relation to me um so you could imagine that some you know some
kinds of um psychiatric conditions that preclude proper theory of mind and you know
the ability to sympathize or empathize or bond would enable the expression of certain behaviors
which would be regarded by other people as evil whether the person prosecuting them thought they
were evil or not would cause be a mute question one you never know because it's inside her mark
off blanket but uh but also uh from her point of view there is no reference and that is the problem
yes there you are um but you can certainly have somebody else out from the outside saying that
is evil um you normally don't you try not to do that when you're doing psychiatry or psychotherapy
you have to have conditional positive regard yes so you you can't really impute you know nastiness
or evil or bad intentions um yes and it's one of these things where um autonomy comes into it
free will comes into it possibly it it you know evil itself is a constructed concept um which
exists in in our language and and it's something which some people will perceive depending on
lots of other things they believe yes but um okay and talking about ethics in AI
it seems to suffer from a similar um form of fractionation in the sense that different people
with different beliefs think that it should be enforced in in different ways what's the solution
well i'd take a um sort of deflationary approach um and it won't be a terribly informed approach but
you know my answer would be well if you get the right um the right optimization the right
imperatives in play then that kind of question just goes away um i think um one way that i have heard
this discuss now i actually enjoy discussing this with you know with with my colleagues and friends
is that there is this dystopian meme you know the singularity the paper clips paradox everything you
see and you know sort of um you know on comic films of a futuristic dystopian dark very entertaining
i mean that's the first things i go for when going to the watch list but but but they they are all
dystopian in a rather unconvincing and fantastical way um and you have to ask yourself well why
why are they um why do people have this sort of dystopian um uh view of um realizing the potential
of um you know what used to be called you know AGI or um and um i think it usually inherits um
from that distinction when you were you introduced the is and the ought yeah so what should a good AGI
what ought it to do and who's in charge of saying who what it ought to do is it meant to make money
is it meant to make profits is it meant to save lives is it meant to um make paper clips um
of course that question just goes away if you're thinking um in terms of um the free energy principle
and an active inference and belief sharing so the only agenda in sharing beliefs is to resolve
curiosity so you know you know i i cannot prescribe what you should do or what an intelligent
artifact should do um other than put constraints on every kinds of outcomes that are expected to
encounter so i can certainly write down constraints in the in the spirit again of either um a multiple
constraint satisfaction from a sort of um uh an engineering point of view or from a mathematician's
point of view the constraints inherent in uh jane's constraint maximum tree principle which is another
way of reading the free energy principle writing down those constraints but within those constraints
so these are the no-go areas you were actually talking about you mentioned explicitly before
so there are certain things you never do um or put it another way uh with relatively high precision
there is a these outcomes are highly implausible and if you find yourself in these outcomes you
you remove yourself immediately so that's quite easy to write down but then within those constraints
you know what is imperative it's just to gather information about what about you it's just showing
an interest in you so become your psychotherapist uh you know well could could i um push back a
tiny bit so i think part of the reason why we have this focus on ethics is because of the
centralization of ai um things like facebook they're controlled by centralized corporations
and what you're leading to is far more interesting it's this multi-agent diffusion stratification
fractionation nicely and that's i agree in many ways might solve the problem because it would it
would emerge and then you can discuss whether morality was part of why we survived it it's not
orthogonal but then i might still push back and say well what if the wrong thing emerged so what
if we do need to introduce governance because in in this active inference multi-agent setting with
humans and machines um we started to see behaviors emerge which we didn't agree with um how could
we then place value pressures on on that um that's an excellent question and you also make that very
important point which i think needs to be foreground that yeah so when i was talking about
belief sharing and distributed the the age of intelligence you know it was exactly this
distributed ecosystem a democratized kind of belief sharing and data sharing um where the
data is small bits of smart data that are essential to to to reduce and so it was very much this
walking away from large data monolithic bits of say large language models for example that can
scrape data from you know from wherever they can get it so that i think that's an important
distinction which which which um um qualifies my um dismissing of all these dystopian outcomes so
i'm assuming that that so spending a billion uh or a million dollars on get training some some
deep neural network is not going to be happening in the future and and we're going to be buying
cheap and cheerful edge devices and little apps that are smart and just and just you know
go go and get the data that we need to know in terms of resolving on-center about what about
you know what what what we are going to do next um but um so so i thought that was a really important
point and of course i forgot your your major question which was what which was the oh well so
if we did have let's say en masse yeah what do you do about that yeah um well again i'm not sure
i'm going to give you a terribly informed response but you know i think the notion of an ecosystem
is quite central here and one if you like um tenet of that white paper was a nod to natural
intelligence and natural processes and what has actually happened what does actually happen in
as a natural scientist so if you have um if you do imagine uh you're an ecosystem of intelligence
in the future it will be subject to exactly the same um dynamics and pressures that that we have
currently in terms of you know um cooperation and competition and wars and geopolitical issues
that that's that's part of the ecosystem and then you will have the normal problems of
democratization and access and um so um there's you know you cannot prescribe aughts for this
because then you have to choose who's going to prescribe the aughts so you have to have a very
libertarian approach to this so the emphasis and now it's not really me talking so much but now the
the architects of the future the generation below who are sort of you know thinking about
the legacy they're going to leave their children um so a lot of emphasis um is from what i see in
conversations i have in my world which may not be um you know microsoft or or you know big tech
who are still focused on big data um in my world it's much more upon um putting constraints in
place that preclude the the the the the emergence of autocracies that um resolve uncertainty about
others by making them all like themselves basically that's one way to get harmony is to
make everybody do what um do the same thing um so but but i you can write down uh so there's a lot
of attention is being paid and i'm talking here about the the next generation of uh of message
passing um that will support that information sharing and belief sharing so a lot of attention is
being paid not just to generalize it from just hypertext but into a sort of more abstract hyperspace
so literally hyperspace message passing and uh sorry um languages and transaction protocols
but also the credentials and the contracts that underwrite that message passing so you know
a lot of emphasis on contracts shared agreements in terms of what data is shareable and who has
the credentials to share that and and having that distributed so not in a in a blockchain sense but
you know uh in some workable um shareable sense so i think if if if one gets the standards right
and a lot of work is being currently done um um under the auspices of the iEEE for example with
the spatial web foundation one gets that right that i think that these catastrophic dystopian
abuses or your emergence of autocracies in an in an age of intelligence will um will be precluded
simply because you've put the right constraints in in place but you know given given that you are
committed to creating an ecology that is truly democratized and and open you know there are no
guarantees are there i hope my friends don't hear me saying this but yeah if you aspire to an ecology
you are you're you're creating a nature on the web basically yes which we are participants and
that will have its you know that will have its own challenges so um you wrote a beautiful paper
called am i self-conscious or does self-organization entail self-consciousness and Keith and i agree
that this is probably the best quote we've ever seen in our lives you said the proposal on offer
here is that the mind comes into being when self-evidencing has a temporal thickness or counterfactual
depth which grounds inferences about the consequences of my action on this view consciousness
is nothing more than inference about my future namely the self-evidencing consequences of what
i can do and uh we spoke with you um last time i think and we invoked charmers and the hard problem
and so on and we were talking about qualia and subjective states and you know in a dialogue
and all this kind of thing and um you responded that different feeling states are hypotheses about
how i'm feeling at the moment and then it would use all the messages and belief updating and all
the planning and estimates of uncertainty which attend that planning the precision or estimates
of uncertainty play heavier roles the higher you get in the hierarchy and this rather leads to this
idea of planes of consciousness you know we said we'd kind of defer this discussion to later so
consciousness is something which um there's no operational measure for it there's no touring
test for consciousness but it's something that we all experience and we feel and and it's with us
um so could you talk to let's say the these planes of consciousness is consciousness everywhere
or are we only aware of the plane where most of the work is being done
okay excellent final question again i'm not the best person to answer this question because i
you know the 300 principle as i'm sure i've said before is not a theory or a principle that would
generate theories of consciousness um but there are lots of people are very interested in this
including yourself obviously and not your viewers um actually i should say also um there's um first
of all this has become a big question in the sort of r and d part of industry also the
Templeton Foundation are starting to fund a number of adversarial research collaborations
to really drill down on that thing that you are picking up on which is to be conscious to
actually have sentient behavior um requires this planning aspect this going out into the future
you know it's put it very very simply um i um conscious or sentient just because i have the
capacity to plan which also entails some kinds of free selection amongst alternative courses of
action which is something that a thermostat wouldn't have something the weather doesn't have you know
there are lots of self-organizing systems that don't have the capacity to select
amongst a number of counterfactual um policies so for me that's that's a sort of um bright line
between sort of um sentience at least and and uh and self-organization that is not um sentient
and that that distinction is exactly what is being tested in this adversarial research
collaboration funded by the Templeton Foundation it's called Intrepid it's contrasting information
IIT integrated information theory with various um predictive processing either of a sort of
non-representationalist or in an active sort so it's a really interesting issue um and it all
comes back down to agency and acting in the long term and I guess your question here is you know
what level um where at what point do you get um qualitative experience and at what point do you
do you think it's you that's having that qualitative experience is that what you meant
well I mean just just to comment on what you've said uh and Chalmers says something very similar
actually he's a computationalist functionalist and thinks that there are you know certain patterns
of information processing and causal structures and counterfactuals and so on if you take the
episode and remove the counter counterfactuals it's not conscious anymore but what you're
saying is interesting about this kind of hierarchy so the heart for example um that doesn't really
have many affordances it doesn't have many counterfactual plans or things it can do it just
has to beat all of the time so that so you would say that the heart is not conscious whereas the
the upper plane is conscious yes yes I would yes yes yeah that's a good one I hadn't thought about that
yeah yeah that's right sorted that one um I mean yeah you know I mean you know I think there are
interesting issues of different levels you know planes or levels of consciousness you know there's
minimal selfhood um and then if it's the case that sort of consciousness as a process entails
some kind of action you have to now think about sort of what is action on the inside what is
mental action and then what what normally what ends up doing is thinking in terms of attention and the
the kind of routing and selection the smart data sampling but now not by moving my eyes or by
going to the right wikipedia page but by basically switching on and off various sources of input from
lower in the hierarchy so you know I repeat from the point of view of psychology that would be like
endogenous attention so that would be the mental action that makes it conscious processing
and then you have to ask well at what point is that you know is that necessary to actually
experience anything and I think I think you probably there are people who argue yes that is
actually a prerequisite for a qualitative experience so you have to be able to select it
what does that mean you have to be able to attend to it and you have to be the source of that sort
of enabling attention that active sampling inside the brain so now there's a deep link between
qualia or qualitative experience and attention and conscious processing that of the kind that
is accompanied by qualitative experience I think is important it also interestingly also speaks to
your dynamic Markov blankets because of course by switching off and selecting various bits of
neuronal message passing in the brain you're reconfiguring your Markov blanket out though
some somebody's Markov blanket lower down which is an interesting an interesting notion but
it that also fits comfortably with Thomas Metzinger's and Vanya's advice's formulations
I actually Yakov Linsky as well formulations of
phenology in the context of transparency and opacity that you know what what renders something
opaque in the sense oh I see now if you like as a projection on a murky window
it's not just going straight into it's not direct perception it's it's your it's now
something I am looking at oh I'm looking at a red apple and it may well be exactly this
controllability the fact you can attend or disignore it so I think there's a nice link there
from the from the point of view of certain philosophical takes on self-modeling and
phenomenological transparency and opacity and the mechanics of belief updating when it comes to
selection through basically a getting the Kalman gain right or you know doing the root getting the
routine right same exactly the same mechanism you get in transformers so they call it attention
it's just basically a waiting in that context it's a waiting that inherits or is inferred from
something in the past a pattern in the past but if it if that waiting comes from higher
in abstract hierarchical model that would look very much like in Dodger's attention
and it starts to have the look and feel of a mental action that would you know bring you
closer to it but whether you would know that you were deploying attention and whether you
could tell somebody else oh I've got a qualitative experience or whether you become a philosopher
as well your life puzzling about the fact that I can tell somebody else I'm having a qualitative
experience there's the different planes I think you'd have to you'd have to contend with beautiful
beautiful do you have any final thought I mean we've got a machine learning audience so any calls
to action how can they start looking at active inference where should they look and what would
you like to tell them I was very charming of you I'm not going to tell them anything I'm just
if it's if it's the right kind of approach that you know it'll already be out there it'll be
self-evident it's just a question I think of people finding their own language and their own
their own rhetoric or calculus that makes sense of it and the other reason I'm not going to say
anything is I've spoken far too much and I'm trying to be a recluse and the more I'm calling
people to my work the less easy it is to be a recluse yes indeed uh professor Carl Frister
that's been an absolute honor thank you so much for joining us thank you thank you very much okay
this is just a bit of a note from me at the end of the podcast thank you so much for supporting
all the stuff that we're doing it means so much to me um we have a patreon if you would like to
support us personally if you're touched by the work that we do um also I'd really appreciate it
if you could give us a rating on your podcast app um I actually discovered the other day I couldn't
quite believe this but on Spotify we are the top rated AI podcast for active AI podcast um yeah it's
incredible so thank you so much for those of you who have rated us um unfortunately on apple
podcast the learning rate is only 0.01 so it hasn't quite caught up uh with the fact that we're the
best AI podcast yet so particularly on on apple podcast if you could give us a five star rating
and and a review I think that will accelerate the learning process um yeah other than that if you're
on youtube most of you aren't subscribed um of course youtube has recently changed its modus operandi
it's less of a subscription chronological model and it's more of a magical algorithm model so I
guess it doesn't matter so much but just from a just from a metrics point of view it would really
help us out if you hit the subscribe button uh we have an active discord community as well
so check us out on there and yeah I just wanted to say one more time thank you so much for all
of your support it means so much to us and uh plenty more content like this coming your way soon
cheers see you on the next one

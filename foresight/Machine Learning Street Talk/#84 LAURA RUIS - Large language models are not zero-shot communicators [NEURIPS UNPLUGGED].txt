Hello, folks. I was in New Orleans last week and I had the pleasure of interviewing Laura Ruiz,
the primary author on this paper,
large language models are not zero-shot communicators.
Now, this is exploring the ability of language models to perform Implicituk,
which I guess from a machine learning audience point of view,
you might think of as being some kind of extrapolation or even abstractive reasoning.
There's an example of this that we can try.
Esther asked, can you come to my party on Friday?
And Zhuang responded, I've got to work, which means no.
Yeah, part of the reason I wanted to do this quick intro is since this interview,
OpenAI has released a chat GPT, which is pretty impressive, actually.
So we can come in here and we can say something along the lines of
Esther asked, can you come to the party on Friday?
Zhuang responded, I have to work.
Does Zhuang, can Zhuang come to party?
It looks like it has failed. It says it's not possible to say
whether Zhuang can come to the party or not because we don't have enough information.
Zhuang may or may not be able to come to the party depending on his work schedule and other factors.
Yeah, so this is an example of failed implicature.
But anyway, if we come over to Laura's Twitter,
she posted a little thread the other day saying that loads of people have been sending her
implicatures, which they used as examples in the paper.
And apparently chat GPT does understand some of them, which she's very happy about,
but she wanted to write a short thread about it.
So she said before they started writing the paper, she would try lots of implicatures
that she came up with on Da Vinci 2 in different wordings with moderate success.
Some always solved and some half of the time depending on the wording,
meaning random performance since the test is a binary, which is to say a yes or a no.
That's why they decided to do a systematic test to figure out how good it actually was
and how much it depended on the wording of the prompt.
And a few months later, they had the answer that it was okay,
but not close to humans.
And okay means that on Da Vinci 2 and 3, the performance of zero shot implicature is roughly
70%. Most of the other models fail, even with few shot in context prompting.
So anyway, she said that she gets the people are excited that chat GPT is doing pragmatic
inferences, but she felt the same with Da Vinci 2.
It's all anecdotal, she says.
But a more systematic test shows a significant gap with humans nonetheless,
and it's the same for Da Vinci 3 and presumably the same for chat GPT.
She says that once this implicature dataset gets solved, and she has no doubt that it will get
solved relatively soon, since fine tuning with human feedback helps a lot,
they might have some baseline pragmatics in their models.
And that's when it will get really exciting.
She says that she's personally blown away by chat GPT's capabilities.
It's absolutely incredible at explaining things, compositional generalization of concepts,
simulating a VM.
I'm not sure what VM means, coherence, creativity, writing essays, poems and more.
She said that the pragmatic language that they studied is part of a type of casual language
that we're using conversation that might emerge from social interactions.
She's personally thinking about why human feedback helps so much,
and whether interactivity and social pressures might help even more.
Anyway, enjoy the interview.
Hi.
It's lovely to meet you.
Nice to meet you too.
So, I was speaking with Andrew Lampanam yesterday, and he really highly recommended your paper.
I looked it up, it's called,
Large Language Models Are Not Zero Shot Communicators,
and I also recognize Stella Biderman and Sarah Hooker, of course.
Sarah's an absolute legend.
Now, you led in the paper by saying,
humans interpret language using beliefs and prior knowledge about the world.
For example, we intuitively understand the response,
I wore gloves to the question, did you leave fingerprints as meaning no?
So, you call this implicature, but I suppose I would think of it as some kind of
extrapolation, being able to reuse knowledge that we have about the world
in a different situation.
But could you talk us through that paper?
So, yeah, thank you.
That was a great introduction to the paper.
Indeed, implicature is kind of the technical term that we use for this example that you gave.
And indeed, extrapolation is a sensible way to describe this.
What we do in this paper is kind of show that large language models are not really good at this
aspect of communication, and we think it's a very important aspect of communication.
So, the title says, Large Language Models Are Not Zero Shots Communicators, right?
So, what we mean by that is to be a communicator, you have to infer the meaning of utterances,
not only by their semantics, so not only by how words combine into some kind of meaning,
but by interpreting the shared knowledge, our shared experience of the world.
And that's what we look at in this paper.
And what we find is that large language models are pretty bad at this.
Specifically, we group them into different groups.
So, we have base large language models like OPT and BLOOM that are just trained on
next-word prediction.
And we also have instructable models like Flonty5, T0, or DaVinci Instructable Models by OpenAI.
And all models perform really bad, closer to random than to humans.
But OpenAI's instructable models have much more promise.
They're much better at it.
Interesting.
Okay, so now the zero shot thing is very interesting.
So, we take these models, and it's kind of like self-supervised learning.
We train them on loads of data on the Internet.
And you're saying that zero shot is when we don't really give much information in the prompt.
So, there's a relationship between how big the model is and how much in-context learning
that we give to the model in the prompt.
Yeah, that's true.
Yeah, the zero shot case that we tested is we give the model a short instruction saying like,
in the following exchange, someone gives a response that has some meaning beyond the utterances.
It had the meaning is yes or no, can you resolve this?
And then we give an example.
And then we evaluate it on ways based on whether it can choose yes or no.
So, that's the zero shot case, and humans, we don't give any instructions at all.
We just said, resolve this to a yes or no.
Okay, so, is it the case that large language models then zero shot almost irrespective of their size
and irrespective of this human feedback alignment?
They just don't perform very well at this implicature at all.
The instructable models by OpenAI gets a non-trivial performance.
I think the models like OPT and Bloom, those kind of base models,
they really conduce this style very well at all.
They get 10% above random, but OpenAI's models are around 70% at zero shots.
Interesting.
So, did you do some work looking at, okay, well, let's try some in-context learning.
Does that improve the implicature?
Definitely, yeah.
Like, it's unclear, right?
Whether zero shots is a fair comparison to humans for these models.
Humans are primed in different ways.
So, we also wanted to try view shots in context learning.
And personally, I thought in this case, in context learning wouldn't help much because
each implicature requires a completely novel type of inference.
But in fact, we show that OpenAI's models is the only group of models that really
benefits from this a lot.
And they can get to up to 80% performance with roughly five examples,
which, and afterwards, more than five examples, it's kind of plateaus.
But they're still like a significant gap with humans.
But it's a great improvement.
Yeah, that's fascinating.
So, can you give me an example of, if we were doing some in-context learning,
let's say with DaVinci 2, what would that prompt look like?
So, if we, I don't exactly remember the wording of the prompts,
but there would be something like the following are examples of the task.
And then you get a bunch of implicatures that are already resolved to a yes or no.
And then you get the original instruct prompt that says
resolve the following sentence to a yes or no.
And then you get the actual example.
And these in-context examples are all taken from a development set.
Okay, so, can you tell us a little bit about
how this reinforcement learning for human preferences works on language models?
So, reinforcement learning for human preferences is a method to fine-tune
based on our language models.
So, the based on our language models are OPT and BLU, for example,
that's part of the group.
And they are just trying some next-word prediction, right?
But they are not really aligned.
They're sort of this alignment problem where they're trained on next-word prediction
and that's not really what we are asking them to do.
And then with reinforcement learning from human feedback,
what we further do, I mean, not we, unfortunately, other people do,
is they take some kind of human preferences from somewhere.
Like, for example, humans are shown prompts and completions by models
and they say, this one is better than that one.
This completion for the text for this prompt is better than that one.
By that, we get a sort of ranking by preference
and we can learn a reward model on those preferences
with an interesting trick that was published in 2017.
And through this rewards model, we have sort of,
we can bootstrap the preferences from humans into the based archangels model
by fine-tuning them with regular RL on this reward model.
Yeah, it's really interesting.
I was speaking with Srijan Kumar who won one of the outstanding
paper awards in Europe and he's got this work on kind of,
we want the models to be more anthropomorphic
and we have these priors to help us understand the world.
And he came up with a framework of kind of like importing these priors
from language encodings into, let's say, a discrete program synthesis model.
But I guess what I'm saying is that there's something really interesting
going on within context learning and it's almost like we're giving the model
the priors to extrapolate or to do something useful in this particular situation.
Yeah, yeah, that's really interesting.
I don't know the paper, I should check it out.
But the way I view it, and my thinking has been shaped this week also
by Andrew Lampinen who wrote an interesting paper on comparing models in humans,
is that it seems that in context learning for this specific task, implicatures,
it's not really that they learn how to use their shared experience
from the in-concept samples, they're primed for the task
with a few shot examples in the context.
And I think that's actually what's happening here.
Like if you test the model zero shots, there's no,
why would we expect it to do this task properly?
There's no motivation or anything like that.
But if you prime it with in-concept samples, it does better.
And that would also explain why it doesn't help to add more than five examples
because it's not using the inherent information in the examples,
it's just being primed for this specific task.
Yeah, that's really interesting.
Sarah has done lots of work on interpretability in machine learning models.
And one thing that I wrestle with a lot is whether we should try and get models
to think the way humans do.
And you can come at it from an intelligibility point of view,
but you could also come at it from a generalization point of view.
Like maybe we do symbolic generalization over cognitive priors,
and that's how we understand the world.
But there are people who just say large language models,
they're just a different mode of understanding,
and we shouldn't try and make them like us.
Like what do you think?
It's a good question.
I am really a non-expert on interpretability.
I'm like, I always come at it from a very anthropocentric view.
Like I would love them to be more like humans
because that would make them interesting subjects to study also
and better to communicate with.
But at the same time, you can take this opposite view.
And I think Stella, the co-author on this paper often says,
you're making a category area, you're attributing something to these models
that they don't have knowledge, those kind of things.
So it might also very well be that we're trying to look for pragmatics
or semantic understanding in these models,
but that's just not how you should think about it.
And I completely forgot to ask you.
So again, some of the audience don't know about
natural language understanding in linguistics and so on.
So what is pragmatics?
Yeah, that's a good question.
So pragmatics is an aspect of language, the way we study language,
that doesn't really look at the syntax or semantics,
which look at, for example, those kind of aspects of linguistics,
look at what a word means and how you combine them into novel meanings.
So those kind of areas of linguistics really look at
when someone understands the term, the utterance as John loves Mary,
they also understand utterance Mary loves John.
Pragmatics goes beyond that.
It looks at how context and our shared experience really influences meaning.
So usually the meaning determined by pragmatic inference
is not really directly part of the context window.
You really have to tap into your prior knowledge.
Yeah, so I'm a fan of Montague as well.
So it's almost like we have the semantic potential and then we have pragmatics,
and that's bringing some additional context.
Yeah, yeah, exactly.
Okay, and because that's a really great example from that extrapolative example
from Montague about Mary loves John,
how could a large language model realistically,
because I think of that as being a symbolic generalization.
So how could a language model do that kind of generalization?
Symbolic generalization?
Yes.
Oh, that's a big one.
I don't know.
I really, really don't know.
In my research journey, I can kind of
come from studying compositionality in language,
which is really more this type of thing that we're talking about now.
And looking at more sort of neurosymbolic approaches or stronger inductive biases.
And now these large language models really showed us that
there is an insane amount of compositional generalization going on
without any inductive bias for that.
Chat GPT kind of shows us that with all these examples on Twitter,
right, you give it two novel concepts,
and it combines it beautifully into some kind of story.
But yeah, to go back to your question, how can they do it?
I don't know. Maybe Skill will get us there to the sense that
humans are also imperfect symbolic reasoners.
Again, to mention Andrew Lampinen,
he did a great paper on symbolic behavior,
where it's not really a discrete,
I can do symbolic processing versus I cannot do symbolic processing,
but it's more a scale.
That's kind of shaped my thinking as well.
I think it's a scale.
Large language models are pretty far on that scale.
They can do very interesting compositional generalization.
And sort of symbolic behavior,
but they fail in catastrophic ways as well.
Again, an example that I think comes from Gary Marcus is
when you ask Chat GPT,
how do horses ride cowboys?
And it just writes a whole story about how a horse rides a cowboy,
even though it doesn't work.
Yeah, it's so interesting,
because I think it's easy to think of large language models in the binary.
Marcus says they're bloviators,
and Bender says they're stochastic parrots.
And then you have the folks who think that it's emergent reasoning
and symbolic generalization.
And I was a skeptic, and I just can't ignore the evidence.
They really are doing amazing things now.
And you were just speaking to Lampinen,
and it's a similar thing with this idea of symbolic generalization.
It might not be a binary, right?
Yeah, exactly.
It might be a very great as competency.
And humans also fail in certain cases.
So on this in-context learning,
because that's something that has interested me.
So the first version of GPT-3 is zero-shot.
We didn't really know how to prompt it.
It looked like a bloviator.
We then went on this intellectual journey,
and we discovered front engineering, scratch pad,
chain of thought reasoning.
I spoke with Hattie Zoe the other day,
and she's got this kind of algorithmic front in-context learning.
And it's just remarkable what's going on there.
Do you have any intuition?
Is it like the prompt is some kind of a program interpreter or something?
My intuition is rather that the prompt kind of...
I don't know how to formalize this intuition,
but I guess that's why it's an intuition.
That the prompt kind of primes the model
and puts it into a sort of area of its weight space,
where it can better answer the actual question
that is asked in the actual question that's asked.
And I think certain things that point towards this
is that there is also some research coming out
where they permute the labels in the in-context examples,
and they show that the performance is similarly good,
even though at the same or like they do completely random labels
in the in-context examples, and the performance is still pretty good.
But there's also other work that shows that doesn't always work.
Sometimes you do need actual labels.
So yeah, again, the answer is basically, I don't know,
but my intuition is rather that the model is really primed for...
Or there's also another great way of viewing this,
and I read that unless wrong at some point.
I don't know how to attribute the author,
because I forgot their name, but it's about
that these models are good at simulating anything.
So you have to sort of prime them to let them know
what they're simulating right now.
Yes, weird, isn't it?
Because we have an anthropocentric view of the world,
and we're agentive with individual agents.
And a language model is everything at once.
So it's almost like you need to give it a trajectory just to get it
to go somewhere interesting.
So with this in mind, we really want to make progress
in natural language understanding.
And what do you think are the steps we need to make
to robustify these language models?
Yeah, that's a good question.
Personally, from this pragmatics paper,
I think pragmatics is one area where they can make huge strides too.
I think even though they have semantic failure modes,
they're really impressive at that.
They're really impressive at compositional generalization.
But pragmatics might be something
that they're simply not trained for currently.
And one very low-hanging fruit is the RLHF that we talked about.
I think that clearly really improves that.
And intuitively, it seems like in the instructivity paper,
you see that they ask the human laborers
to really infer the human intent and the problems and write on.
And that's very reminiscent of implicatures.
But then on a more sort of broader scale,
I think some kind of embodiment or interactivity
might be really important.
Like pragmatic inference is really a social skill that we have.
There's a lot of pragmatic pressures that you encounter
while just acting in the world and navigating communication
and navigating a lot of things.
So I think I'm currently trying to look at
a setup in reinforcement learning
where we're trying to make a pragmatic task
and see when pragmatic reasoning would emerge there.
I don't know how to consolidate that fully
with large language models yet,
but I think interactivity and social navigation is important.
I'm really fascinated by the embedded tradition
in cognitive science.
And I think there's a lot of interesting work there.
But I suspect you do as well a little bit.
How do you contrast what is essentially
the representation of this view
where everything's in this big monolithic model
to some kind of relational view
where we're using essentially the world
as its own representation?
Yeah. Again, I don't know.
To what extent it's also possible to express everything
in just the representation in this view
where you have an internal world model.
And I don't know to what extent
you really need an external world to learn,
but intuitively it seems like that might be very important.
And intuitively it also seems like the behaviors
that can emerge are really limited by the world's models
acting in a large language model only see sex.
And there's basic things that just simply cannot learn,
even though it has surprised us a lot.
So I think, I don't know,
it's easy to think about that it's really important
to have some kind of external world to interact with.
But I'm happy people are working on scaling
and I'm not saying some type of AGI, whatever that means,
might not emerge from simply scaling up
basically what we're doing right now.
Amazing. And are there any other parts of your paper
that we haven't spoken about?
Yeah, one thing that we found pretty interesting
is that even though all these open AIs models
can get really high performance, close to humans,
6% roughly, that won't tell you much without the details.
But it's a significant gap still, but it's really, really close.
I don't know if a human might figure out
whether this model is a model or a human in that case.
But when we sort of drill down and make a taxonomy
of the examples that are in this data set,
we find that they are mostly benefiting
from the simple examples where not a lot of context is needing.
So one example is an implicator is if you ask me
how many people came to your party,
and then I say some people came.
It's really the conventional meaning kind of of the word
some that I meant not all people came.
But it's still an implicator, but it's a very common one.
So those kind of examples, if we isolate those
and we look at specifically examples
that are really context-dependent like
are you coming to the open AI party tonight?
I have food poisoning.
Those need much more context to be resolved.
And then the performance decreases again,
and there is roughly a 9% gap, which like the best model,
but all other based models and instructable models
like Flancy V and stuff,
they then again completely fail on those kind of examples.
Fascinating.
I'm really interested by this idea that
understanding is a complex phenomenon,
and it's like the parable of the blind man and the elephant.
So we create all of these metrics,
and the metrics exclude most of the truth.
And the metrics for pragmatics presumably are in some sense
even more complicated than the metrics
that we already use in natural language understanding.
And it just feels like is that going to be a serious problem
for us to kind of encapsulate how well the model understands?
Do you mean that we're sort of giving it a test
that is couldn't really solve?
Well, I suppose one way of looking at it is,
in this particular test, we've come up with lots of examples
of pragmatic inference, if you like.
And what we're doing is we're taking a very complex phenomenon
and we're putting pins through it,
so we're putting like little slices through it in different angles.
And then we've got this shortcut problem
that if we optimize on any one of those slices,
we might be kind of like excluding everything else that's important.
So it feels like we've got,
is this like a general problem in natural language understanding?
It seems like you're getting at evaluation to some extent, right?
I think evaluation is the single most difficult thing in NLP.
This is just a benchmark to give us some intuition
as to what these current failure models,
failure modes of these models might be.
And I think if this benchmark is at some point passed
by a model that's in and of itself
without trying to diminish my own paper, it doesn't tell us much.
There's much more to be done.
We need more different benchmarks.
We need like human testing in a sort of Turing style maybe.
And yeah, I think this is the most interesting problem in NLP,
like how the property evaluates.
Interesting.
And do you take an interest in fairness and bias in the models as well?
I'm very interested in it, but from a sort of spectator view as well.
Like I haven't worked on it at all.
Okay, yeah, because that's presumably a massive challenge.
Yeah, yeah, definitely, yeah.
Amazing.
So in final question, what are you most excited about in your research trajectory
over the next year or so?
Well, definitely, I just feel super excited to be working on stuff like this
currently given the capabilities these models show.
Like they're absolutely amazing.
And I love seeing how people interact with them.
Like the creativity of people is really needed to get some kind of interesting
response out of these models, right?
And also the creativity of people is needed to find the failure modes.
And yeah, so what I'm most interested, excited about now personally for my own
research journey is really trying to look at, you know, an interactive setup
and see when pragmatic inferences might emerge in what kind of environments,
what kind of pressures do we need, and how can we translate that back to getting to be,
to getting like language models be zero shells communicators.
Amazing.
And where can people find out more about you?
They can follow me on Twitter is first name, last name, and I have a website,
also firstname, lastname.com.
Amazing.
Laura, thank you so much.
It's a pleasure to meet you.
Thank you for having me.
Amazing.
Amazing.
Right.
Cool.

Welcome back to Street Talk, just a little bit of housekeeping before we kick off today.
Polina Silivadov is one of the organisers for a charity AI conference called AI Helps Ukraine.
Now, their main goal is to raise funds for Ukraine, both from the folks attending the
conference and also from companies sponsoring the conference. And it's not too late to sponsor
the conference and support it, so please do if you possibly can. Now, all of the funds that they
raise will go to Ukraine Medical Support, which is a Canadian non-profit organisation
which is specialising in humanitarian aid for Ukraine. Now, they have some of the world's
leading AI experts keynoting at this event, so Yoshua Benjo, Timnick Gabru, Max Welling,
Regina Basile, Alexei Efros and also one of our own personal favourites here on MLSD,
Professor Michael Bronstein, the one and only. Now, the conference is online pretty much from
now until the 6th of December and it's being hosted from Mela in Montreal. Their goal is to
raise $100,000 and they really, really need the support of the AI community to club together and
to just donate anything that you can. So, we'll link to the conference in the video and the podcast
description. Please donate if you can and also share the links on your socials. Now, I'm at New
Europe this week in New Orleans, so I'll be walking around with a camera. Please just bump into me
and if you want to record any spicy takes on artificial general intelligence, then let's do it.
Today is a conversation with Yoshua Bach, who's one of our most requested guests ever.
We recorded the conversation back in April, which gives you a bit of an indication of our backlog.
I can only apologise about the backlog. Keith and I recently started a new venture called X-Ray
and as you can imagine, we've been working around the clock coding, just trying to get that business
off the ground. But when we've made our millions, we'll devote all of our time to producing amazing
content on MLSD. So, yeah, please bear with us loads and loads of cool content coming your way
soon. I hope you enjoy the show today. Peace out.
Dr Yoshua Bach is our most requested guest ever. Yoshua Bach is a cognitive scientist focusing
on cognitive architectures, models of mental representation, emotion, motivation and sociality.
Yoshua's interview on Lex's podcast, he did two interviews on Lex's podcast,
have been watched over two million times so far, which is just absolutely unreal.
Now Yoshua, I've watched many of your interviews and I still don't feel that I have a firm grasp on
some of your views. So today, if you don't mind, I hope we can do a tour de force over some of your
most important views in our shared space, to the extent that we can keep up with you, of course.
Now, for example, we'd like to discuss Godel and computation, consciousness, digital physics,
free will and determinism, large statistical models and indeed whether they're AGI or a
parlor trick or something more esoteric. Now, when people talk about God or consciousness
or any other complex phenomena, it relates to everyone and it means something different to
everyone. It's ineffable and every conversation sounds like a typical post ketamine discussion,
which is to say extremely low information content. Now, the topics we're discussing today are very
complex and often we're reaching for the best language to use to conduct the conversation.
So I hope we do well today. Anyway, Dr. Yoshua Barker is an absolute honor to finally welcome you
to MLST. Thank you very much. I'm glad to be on the show. Amazing. Well, when I started doing
computer science many years ago, interestingly, the theory of computation wasn't even on the
curricula. And I was wondering whether you thought it should be. I mean, presumably you think it's
extremely relevant for AGI. Now, we want this to be as pedagogical as possible. So please explain
everything like we're five. What does computation mean to you? I think that computation is
far easier than most people think. It means that you have a causal structure where every
transition can be decomposed into individual steps. And when we talk about computational models,
we decompose the world into states and transitions between the states. And then it turns out that
there is a certain minimal system that is able to execute everything. And this can be described in
many ways. The most famous one is probably the Turing machine and many other ways in which you
can describe the Turing machine. For instance, you can just do the Turing machine by doing
search and replace on strings. And this is how the Lambda calculus is defined.
And all the programming languages and the Lambda calculus and the Turing machine
turn out to have the same power. That is, if you can compute something with one of these paradigms,
you can compute it with the others. As long as you don't run into resource constraints,
so as long as it still fits into memory and you don't care about speed, they all have the same
power. But in practice, of course, every system is limited. So we don't run things forever. We
want them to give us a result after a certain time. So what matters is what can be efficiently
computed, not what is reachable at all. Awesome. And there are, and we're going to get into this
a bit, but there are some possible loopholes, at least in, you know, let's say whether or not
the universe is limited in certain ways that the definitions of like Turing machines are.
And one I wanted to ask you about specifically is Penrose's claims. And so he claims that
what Godel's work in fact proves is that the human mind can understand truths that are not
provable. So specifically one can show that, you know, given Godel's sentence is necessarily true
given given a mathematical analysis, even though it can't be proven within the formal system that
it's that it's defined. And Penrose claims that this capability to understand, if you will, to
mathematically understand is in fact non computational, at least in part. And so if he's right, then
our brains might be what Turing referred to as Oracle machines. These are computers that have
access to a non computable Oracle or function that they can then utilize those oracles in
order to perform hyper computation, essentially. So I'm asking, I'm curious, are you open to this
possibility? And if not, what is your response to Penrose's arguments?
I suspect that Goedl has been misunderstood by a lot of philosophers. Goedl was a truth
realist. That is, he thought that truth really exists out there. That it's the thing that is
eternal in some sense. He had this very strong intuition and mathematics classically is also
formalized in this way. The difference between mathematics and computation, at least in the
standard sense in which we normally teach mathematics at school is that mathematics has no
states. Everything in mathematics just is eternally. It's a single state. And if you want to go through
a sequence of states, you put an index into the formula. But still, everything is there at the
same time. The index is just a way to access this thing. And this way of having mathematics
stateless is very elegant because it allows us to define functions that have infinitely many
arguments. If you would have a state machine that tries to consume infinitely many arguments,
it would never finish before it goes to the next step. And the same thing in the middle of the
function, if you compute something, if it's stateless, you can just compute all the indices
all at once, even if it's infinitely many in classical mathematics. In a computational system,
you would have to do this maybe one after the other. And if you do it at parallel, you will have
lots of CPUs running at parallel. So you run into limits. And the same thing with the output. So
in the classical mathematics, you can chain infinitely many steps and functions and exchange
infinitely many arguments. But of course, mathematicians never did this a practice. It's
just a specification. This is how they like to write things down. When they want to calculate it,
they still have to go down and do it sequentially step by step. Just mathematics is defined in such
a way as if you could upload this to some supernatural being or some grad student who is
going to do the infinitely many calculations. And Goethe took this specification of mathematics
and he found out that when you have this stateless mathematics, you can, for instance,
define self referential statements that change that choose value depending on the statement itself.
And this recurrence leads can lead to a contradiction in the state itself. So you basically
get two statements which say I am wrong. And if by referring to itself, it changes its own
truth value. So if mathematics is stateless, you will now run into a conflict. In a computational
system, that's not a big problem. Your computer is not going to crash. If you write it down the
right bay, it just happens is that your truth value fluctuates in every execution step. It's not
going to converge. But this is not the real truth. Truth is something that doesn't change
when you call the function again. So what's going on here? And I think what Goethe has
discovered is that classical mathematics doesn't work. What you cannot build is any kind of
mathematics doesn't allow you to build a machine, a hypothetical abstract machine, any kind of
universe that runs the semantics of the classical mathematics without crashing.
Yeah, but it kind of seems like, okay, we're going to believe Goethe's use of mathematics
to prove that mathematics is flawed. Like there seems to be almost an inherent contradiction
in there. Like you either believe mathematics, and thus you believe Goethe's proof of some
specific limitations on computational systems, right? Or you believe that somehow mathematics
is flawed, in which case you can't trust the proof that mathematics is flawed.
I think Goethe's conclusion was that there is something fundamentally going wrong, that there
might be an inability of mathematics to describe reality. And if you believe that truth is real
and it exists independently of the procedure by which you calculate it, then this seems to be
plausible. And it was also the conclusions a lot of philosophers have drawn from this,
which basically read Goethe's proof and concluded that mathematicians have admitted that their
arcane techniques are important to describe reality. And therefore, philosophers who don't
understand mathematics have a clear advantage. Of course, this is not the conclusion. Instead,
what turns out is that if you just skip or if you drop the original classical notation or
as understanding of mathematics and replace it by computation, basically we say,
truth is what you calculate with the following procedure. And you can define any kind of
procedure that you want. You just have to make sure that it converges to some kind of value in
the way that you want. Then you resolve your problem. It's just that you lose your notion that
truth is independent of that procedure. And so in some sense, the classical mathematics is a
specification that cannot be computed. From the perspective of computer scientists,
this happens all the time. Some customer wants you to build something that cannot be built,
and you have just proven that it cannot be built. Right? But it doesn't mean that you
cannot build something useful. And I think that Penrose believed that our brain is actually doing
these infinite things. And it's not. When we reason about infinity, we are not actually reasoning
about infinitely many steps. What we do is we create a symbol, and then we do very finite
computations over that symbol. But we cannot construct infinity. We cannot build it. We
cannot go there from scratch and write down some clever automaton that produces an infinity for you.
I was recently browsing Penrose's book, The Road to Reality, and I would say that, I mean,
I don't know that much about physics, but the chapters were really interesting. They're talking
about surfaces and manifolds and symmetries and fiber bundles and gauges and wave functions,
calculus, matrix theory, and even computation. I mean, almost all of the discussion was on
mathematical modeling at different levels of description or emergence, if you will. And
in machine learning and AI, we are forever challenged by trying to get machines to model
physical reality at different levels of description using an interoperable set of tools. So it seems
increasingly true that we need machines that can learn descriptions and concepts at multiple levels
if we're ever going to have AGI capable of understanding the world and learning novel
semantic models. All of machine learning models today work by chopping up a Euclidean space into
what is effectively a locality-sensitive lookup table. Very big one. And we need AGIs that can go
far beyond this. It's got to be able to learn novel geometries beyond even what humans could
have come up with and the ability to reason topologically and algebraically over those
geometries. Something which I think you would agree is not happening with the current deep learning
systems. Well, let's start out with the notion of geometry first. If you read Penrose's book,
what you find is that this entire universe is geometric, which means it's made of
continuous spaces in which things are happening. And if we actually look into the world deeply,
quantum mechanics is not a geometric theory. The geometry only emerges approximately
at the level of the space-time description. And it seems that geometry is actually the
domain of too many parts to count. In reality, all the objects that we describe as surfaces,
if you zoom in, are made of discrete parts like atoms and particles and so on. And these in turn
are made out of things that have a finite resolution. And if we look into our computer
programs, you can create stuff that looks continuous to us, but there's nothing continuous
inside of our computer programs. And it turns out that the assumption of continuity requires
that we partition the space into infinitely many parts. So now we are again running against
that thing which GÃ¼rl has shown us as difficult. And it's not a big problem in practice because
in practice, we never need to do these infinitely many things to produce a computer game with an
arbitrary fidelity. We can make something that looks like space. But the space that we think in
and so on is an approximation that our brain has discovered. It's a set of operators that converge
in the limit. But the limit doesn't exist. It just, it happens that when you live in a world that is
made of too many parts to count for almost everywhere where you look, you need to find these operators
that converge in the limit. And the set of operators that happens to converge in the limit
and is still computable. This is what we call geometry. And to use these uncomputable geometric
approximations for macroscopic physics like Newtonian mechanics is completely fine. You're
just going to compute it up to a certain digit and then this is good. But it's a problem for
foundational physics. Because if it turns out that you cannot take a language that actually
computes infinities, if you cannot construct your language, then you cannot write a universe in it.
So our universe is not written in continuous language, but Penrose universe is.
This doesn't mean that geometry is full. We need this to describe the world of too many parts to
count. But we do this via computational approximations. Our brain does the same.
So let me ask you this then because we come across kind of the infinities a couple of times. And I
know that you placed an emphasis on constructive mathematics. So of course, you and all of us,
you know, except let's say the existence of potential infinities, you know, algorithms that
you can sit there and just keep calculating for as long as you want and get kind of more digits.
But it's really around actual infinities that we seem to be running into problems. So let me ask
this this first question here, really leading up to some computational questions, which is,
can the universe, can our actual universe that we're in right now be actually infinite
in spatial extent?
A problem is that it can have unboundedness in the sense that you have a computation that
doesn't stop giving your results. But you cannot take the last result of such a computation and go
to the next step. You cannot have a computation that relies on knowing the last digit of pi
before it goes to the next step. In the sense that you don't have an infinity. But the infinities
are about the conclusion of such a function. It means that you actually run this function to the
end and then do something with the result. Unboundedness is different in the sense that
you will always get something new that you didn't expect that they cannot predict.
But it's just going on and on without this end. And I think it's completely conceivable that our
universe is in this class of systems in the sense that it doesn't end. But it doesn't mean that there
is anything that gives you the result of an infinite computation. Because if that was the case,
then it could not be expressed in any language. It also means if something cannot be expressed
in any language, that you cannot actually properly think about it. Because when you think you need
to think in some kind of language, not in English, but in some kind of language of sort or in a
mathematical language that doesn't have contradictions. And what Goethe has shown is that the language
that he hoped to reason in about infinities breaks that it has contradictions in it. That at some
point, it blows itself apart. So the languages that we can build are only those in which we have
to assume that infinities cannot be built. So infinity, in this sense, is meaningless.
Because we cannot make it in any kind of language.
So the thing is, though, I'm not limiting what the universe is capable of based on human mental
and linguistic limitations or even mathematical limitations. I'm asking you if it's possible
for this universe that we're in to ontically be right now actually infinite in spatial extent.
The thing is that you try to make a reference to something that you cannot observe, that cannot
conceive of other than making a model in some kind of language. And to have that model make sense,
the language needs to work. Right? Otherwise, you are just maybe in some kind of delusional thing.
And we can construct delusional things. We can construct languages that have bugs that we cannot
see. But if we use a language that has bugs in it that we cannot see and we cannot repair them,
then this means that the stuff that we express in the language is not meaningful. Right? We have to
use a different language that has maybe the same expressive power but doesn't have these bugs.
But now if you try to think about the universe in the language that allows you to imagine that
the universe is literally infinite, rather than very, very, very big and much bigger than you
can imagine and not ending, which is for all means and purposes almost the same thing. Right?
Then if you do this other thing, then your thought doesn't mean anything. So it's basically you cannot
properly express the idea in your own mind without running into contradictions that the
universe is infinite in the sense that such a universe could exist. Okay, so you're basically
following that. That's the issue. Basically, I cannot think that the universe is infinite. I cannot
express this. That's my issue. Okay, fine. So you're basically saying that the English that I
used just a minute or so ago just is not coherent or not conceivable. It's not something that you
want to. But the underlying thing behind the English, right? English is not designed to be
coherent. It's designed to be disambiguating. It's designed to be unprincipled to allow us to
express things vaguely and not break. But if you think really, really deeply and really exactly,
then the question is, what kind of model is your mind building? At which point is there just some
kind of noisy nabler that you're pointing at without actually decomposing it and anything that would
make sense? Okay. And so the lack of really the ability to conceive or for actual infinities to
ontically exist in some sense, if we just deny all that, so we're really just stuck with,
all right, we've got finite everything, discrete everything. There's no such thing as a continuum.
There's no such thing as actual infinite spatial extent, etc. That's really the world that you're
proposing here, right? That everything is constructed from at the end of the day,
finite, discrete kind of elements. So if we... Yeah, you can imagine that your mind is a library
of functions in a way, and these functions are doing jobs. And on the bouts of the box,
you write down what these functions are doing. And you construct a box that this, in this box,
there is an infinity between, for instance, a continuum between two points. And then you open
up the box and look at what's actually inside of the box. And you realize it's just a lot of small
steps. And it's designed in such a way that you can, if you want to have more steps, it's going to
give you more steps if you zoom in, right? And it's totally doing, apparently, what's written down
on the box. But if you look very closely, realize, oh no, the thing that is written down on the box
that you have written down on the box cannot actually be in there. You can prove that it cannot
be in there. It must be something else that's in there that is doing most of the work of what you've
written down. So what you should actually be doing, I think, if you are interested in how things
actually work, write on the box what it's actually doing, which means it's going to subdivide or
any interval with any resolution you want as long as you can afford it.
Okay. One mystery, if you will, for me, and I'm hoping you can help me understand this, is that
all of the standard models for physics that we have today, they do have in them these continuous,
you know, for example, symmetries that are rotational symmetry or things like that. They're
built off of positing continuums with continuous waves, lots of continuities and infinities,
at least in the mathematical descriptions. Except for quantum mechanics, right?
Right. And I think based on what you've been saying, you would say that those are artifacts or
properties of our mathematical descriptions of reality, but they're not actually extant in
reality. And my mystery there is why do those continuous and mathematical maybe flawed and
inconsistent with infinities all over the place descriptions work so well for describing phenomenon
at different levels? If everything at the end of the day, you know, if we just looked at high enough
energy and small enough resolution, we'd see kind of the grid and, you know, all the discrete
effects and rotation happening kind of in little tiny, very small but not infinitesimal degrees.
You know, why does all this continuous infinity based mathematics work so well? What is the
explanation for the unreasonable effectiveness of that kind of mathematics?
The easiest answer is that the world in which we live in is made of extremely small parts.
And we could not exist if that world was not made of that many small parts. So for instance,
you want to have a momentum for particles that are almost continuous. So you can address the
space with high resolution because the momentum is what tells you where information comes from
in the universe, basically the direction of where from which information reaches you and so on.
If that would be very coarse, then the complexity that you could build would probably be far lower.
And we consist of so many parts that when you look down, it's uncountably many for all practical
purposes. So the mathematics that we need to describe the world that we are in that we need
to model are mostly not in the realm of countable numbers. The countable numbers only play a role
when we are looking at very few microscopic things. As soon as we leave this domain of a few apples
on our table, we almost instantly drop in this realm where we just need to switch to a continuous
description of things. And this is completely fine for most of our history. When we did physics,
we never zoomed in that heart. And even now, when we really need to zoom at the level where
the plank length matters and the resolution of the universe becomes visible. And it's of course not
some Euclidean lattice, some grid that you can see. It's just that at this level, you no longer
have space. I wanted to move matters back over to some of the happenings in the world of large
language models and deep learning and so on. And first, quick fire question. I honestly,
you're a bit of an enigma to me, Joshua, because obviously I've read some of your research and
you seem like a hybrid guy to me. You know Ben Goetzel very well, for example, but you're also
hugely into the hype train on the connectionism. For example, you criticised Gary Marcus's
article. So the first question is, are you a symbolist or a connectionist?
I'm neither. The thing is that I hate deep learning as the best of us. Deep learning is ugly. It's
brutalist. It's a few very simple algorithms that are blown up to the max. But I cannot prove
that these algorithms do not converge to what we want them to converge to. It's maybe not
elegant, but it works. And the solution to problems with deep learning so far has always
been to use more deep learning, not less. So what upsets me about Gary Marcus argument is not that
I'm not sympathetic to what he's trying to push it. I'd like to build models that are more elegant,
more sparse and so on. But in the past, all these elegant sparse models have been left in the dust
by just using more deep learning. And we can also see when we zoom out a little bit that there is
not an obvious limit to deep learning itself, because deep learning is not just the algorithms.
Deep learning is a programming paradigm. It's differentiable programming. It basically means
that you express everything with approximately continuous numbers, and you use algorithms that
converge business at certain ranges. And when it doesn't converge, then you just tweak it and you
introduce a different architecture, which is some kind of discrete operations that you do on these
continuous numbers and so on. You just patch it, you write your programs slightly differently,
and you can automate the search for the program. And the people who do deep learning are not also
docs in the sense that they say, oh my God, symbolic structures are not allowed. I cannot use
a Python script in here rather than just a TensorFlow. This is not what's happening. It's
also not that they are constrained to any kind of thing that will use whatever is working.
And what we see is that the end-to-end train systems are going more and more powerful,
and rather than sitting there by hand and tinkering and finding a solution,
we can just use a system that is tinkering automatically through a dramatically larger
space than we would ever be able to explore by trying all sorts of algorithms. So when we look
at Gary Marcus' articles like his deep learning is hitting a wall and so on, and you look what he's
actually giving as arguments, the arguments are not very good. He gives us an example,
the NetHack challenge. NetHack is a game which has a very large horizon because you basically
have only one life. You need to explore a very deep labyrinth and you need to plan pretty far ahead
with what you're doing. And so it's something that is difficult to discover this right solution
with a deep learning model that has no prior ideas about what it's doing. Because it takes us
very, very long until you get the necessary feedback to learn about your actions. And people
are relatively good at learning this because they have so many ideas about what the situation is that
they're in. There's so many priors from our world interaction and from other games that we have played
that we can bring to the tasks. So the current winner of this is the symbolic solution.
And the symbolic solution that Gary Marcus gives as a proof that symbolic methods are still ahead
of deep learning things. In a single case, not like he has a big array of tasks where they are
superior, it's just two students who have written a program that is made of lots and lots of events.
This is just a big hack. This is not some symbolic learning algorithm that does something novel,
hybrid or whatever. No, this is just a script. And is Gary Marcus seriously proposing, oh my
god, deep learning models are limited and we need to replace them with more scripts?
This is not a good argument. Yeah. So I think maybe, and look, I get that there are these kind
of two competing camps and they maybe go after each other with some. No, they don't. This is only on
Twitter. There is, there are no competing camps. It's Yandekun is not also docs in the sense that
he believed you need to use this argument, all the other arguments are impure and flawed.
His brand is to build systems that work. And if one of his people comes up with something that
works better than what he came up with, you probably praise him for that and let him go on.
Yeah, sure. But there's absolutely, however, there is, you know, there is, let's say, momentum
and hardware lotteries and paradigms that kind of reinforce themselves. And to an extent,
they can strangle off, you know, resources that maybe we like, we shouldn't be investing all
our eggs in one basket. We shouldn't be pouring, you know, the 99% of research funding necessarily
down, down deep learning. And I think that's kind of the problem that, that these paradigms cause.
But I want to get back to something you said, which is a good point. It's, I think that's an
important point. I think that in absolute terms, the other approaches get more money than they did
before. It's not that we have a funding stop, as we had at some point, a return funding stop for
Neural Networks. And Marvin Minsky wrote a book where he saw he had proven that the Neural Networks
cannot converge over multiple layers, press up drones cannot earn X or and so on. Right. Minsky
was wrong. People found a way around this. But at this time, there was so little funding that
this cutoff mattered. And at the moment, if you want to do something that has AI and Adline,
the chance that you get it funded and whether what paradigm you're doing is greater than ever.
So the absolute amount of funds that goes into any kind of paradigm that you want to work on
is greater than ever. And the reason why the majority of funds goes into very few paradigms is
because these are the things that work in industrial applications. There is no other
algorithm that is able to learn from scratch how to translate between arbitrary languages and
generate stories and draw pre pictures for you. This is the only game in town at the moment,
the only class of algorithm that converges over all these many domains. And people are looking
for better alternatives. And yes, we are in a bubble, because of course, they're looking mostly
where things already were. You have hardware that works, you have libraries that work and so on.
It's hard to get out of that bubble. That is true. And it's always good to push for alternatives
and so on. But I don't think that we should be in a panic and say, Oh, my God, there is something
politically wrong. I suspect that by and large, the forces of the markets and the forces of the
academic researchers that want to explore alternative are pushing in the right direction already.
Yeah, I mean, fair enough. And, you know, you could be right. And there may not be that much
of an imbalance. But I want to get back to one technical thing you said. Yes, it seems apparent
that, let's say, what deep learning is doing is this this differentiable program search,
if you will. And a question I have about that is if we imagine the space of all possible programs,
that, you know, requiring that we're doing a differentiable search is certainly going to skew
that sample space that may even cut off programs in that space that can't be discovered easily by
differentiable search. So I'm wondering, doesn't that leave open the possibility that other
algorithms that are more discreet in nature, say evolutionary algorithms or discrete program
search or whatever, they may have access to a different subspace of the space of all programs
that aren't easily accessible by differentiable paradigms. Is that true?
The question is, how do you find it? How do you find these algorithms to manipulate the
discrete things? I agree that when you have a perceptual model that is modeling everything
with chains or sums over real numbers, and a few non-algebraic throne, and you get
characteristic artifacts. For instance, in the generative models, you often have the problem,
and you try to model a person with glasses or without glasses, that because the model thinks
that these features are somewhat continuous, you often run into the situation that you get areas
in the generative model, where the glasses are half materialized, and it looks always very weird.
And you have these strange things where reality has a discontinuity, but your model has permissible
states where you are in the middle of the discontinuity, and you try to generate something
that cannot exist. You want your model to be structured such a way ideally that every
model configuration corresponds to a world configuration. And this is not necessarily the
case with many of the deep learning models. And what the deep learning models, as you train them
harder, typically tend to do is that they squeeze these impermeasurable areas until you are very
unlikely to end up in them. And it's probably possible to get them to implement filters and
all sorts of tricks. But what you can also do is you can combine this with some kind of discrete
machine. And then what you do is you learn how to use this. So this deep learning network is not
interacting with the world directly, but it learns how to use an architecture that does that.
So for instance, instead of training a neural network to do numerical calculations,
you can train it to use a numerical calculator. And in this way, it can become very sparse again.
Right? So there's not an obvious limit to that I can see where I can prove to the deep learning
people, oh, here's where you should stop deep learning, because they can just combine their
deep learning approach with other approaches and use the deep learning system to remote control
this. And it turns out when we reason and so on, even when we do discrete reasoning,
that the steps that we assemble it to each other are heuristics that require some kind
of probabilistic element. Right? So when we form a sort that when the sort is made of very
discrete elements, the search for that sort is some kind of deep learning process that is happening.
Right? And when we make the pool, we do this, we emulate a discrete reasoning. But of course,
we can combine this and we can get the neural network to learn how to perform the discrete
operations. There's a certain thing that I would like to see, which is something like a more sparse
language of thought. When we are looking at deep learning models, there's a phenomenon that people
are sometimes observing, which they call grocking. That is, you train the model and your model gets
better and better. And then it overfits, which means it gets very good at the training data,
but it gets very bad on the real world at things that it hasn't seen before, like a person in
psychedelics was able to explain everything in the past, but is no longer able to perform well in
the future because they're overfitting. They basically fit the curve too closely to the data
that I've seen. And there are many tricks in deep learning to go around this overfitting to make
sure that this doesn't happen. And people try to avoid it. And then what they discovered is when
you take this overfit model, you train it more and more and more and more. At some point, it
sometimes clicks and it gets much better than ever before. And there is a question if there's
something that we're doing wrong in deep learning. For instance, when you think about how people
learn, they learn very different from GPT-3. People first learn by pointing at stuff that
thinks that are relevant to them, that they can eat, that they can hurt, that can hurt them,
or that they find pleasant and so on. They, that they can feel that they can, they have contrast
on it that are salient to them. And so you start out with learning these semantics based on the
saliency and relevance that you have. And then when you learn language, you learn basic syntax,
how to put things together. And in the long tail of the syntax, you learn style, how to express
things with new ones and so on. And with GPT-3, it's the opposite. You first learn style, right?
And then you learn syntax as the regularities in the style. And the semantics is the long tail of
that. And to make that happen, you need to learn much, much more. You need to have more training
data and so on. Maybe there's a way in which we can reverse the order and basically get it to
start out with relevance, to build a curriculum where you first get very sparse regularities,
where it clicks into place. You always make sure that you can handle it with very limited resources
and only see the style and the niceties and the nuances as the far extensions of these very sparse
concise models that have very big predictive power. Yeah. I mean, on that, I mean, the Grocking
paper was very interesting. And a lot of these large language model fans always cite that very,
very quickly when you have a conversation with them. But there is a problem with machine learning
in general, which is that there is, as you said, there's a spectrum of correlations and almost
all of them are spurious. And on one side of that spectrum, you have the idealized features you
actually want it to learn, which will generalize after distribution. And then, of course, if you
go down that spectrum, you pick up on all sorts of very spurious correlations that just happen
to generalize very well. And if you tell the models not to use those spurious correlations,
that the performance of the model will go down. But I want to just move a little bit over to
Yasaman Rezegi's paper. I don't know whether you saw that, but she showed that the performance of
large language models for arithmetic tasks are linearly correlated to the term frequency and
the training corpus, suggesting that they are memorizing the data set, which presumably you
would agree with. And Google has recently released this 540 billion parameter language model called
PAM, which interestingly does extremely well on, for example, some of the Google big bench tasks,
such as the conceptual combinations task, which is one of them, which tests for compositionality,
which we'll talk about in a minute. But compositionality is when you can take constituents from
the prompt and compose them together to form the answer. Now, it's tempting to jump to the
conclusion that these models are starting to magically reason at scale along the lines that
you were just discussing. But I still think there's plenty of opportunities for shortcut learning,
you know, by which I mean these spurious correlations, given the brittle interface of an
autoregressive GPT style language model with these human designed benchmarks. Would you agree with that?
Yeah. When I started my own career in computer science in the 90s, I was in New Zealand, and the
prof here in Britain realized that I was bored in class. So he took me out of the class and in
his lab, and he gave me the task to discover grammatical structure and an unknown language from
scratch and left me pretty much to my own devices on how to do this. So the unknown language I picked
was English, was just unknown to the computer, but was the easiest one to get a corpus for,
and they gave me the largest computer they had. It has two gigabytes of RAM, and I did
in memory compression with C and so on, and tried to do statistics, and I quickly realized NREM
statistics don't work because of too many words in between. So unlike vision tasks where
confnets have a useful prior by thinking that adjacent pixels also relate to symbolically
related information, right? So adjacency in images is a very good predictor for thematic
relatedness. It doesn't really work in NLP. So the transformer was discovered in natural language
processing for that reason, because you cannot use direct adjacency very well. And so I realized
I cannot use NREM, which depend on direct adjacency between words. And so I first of all used ordered
pairs of words and tried to find correlations between pairs and then find a mutual information
tree that would give me the best prediction over the structure of the sentence for all the
sentences that I would have in my corpus. And indeed this correlated to structure. And I realized
this is going to not just give me grammar, but it's also going to give me semantics
if I can more deep statistics. But I will need something not just ordered pairs,
but I need to have something like force order models. But to do the statistics, even in memory,
with clever and memory compression and many tricks that I did, I could not do full statistics on
this. So what I realized I had to do was that I do multiple passes. And at first I discard almost
all the information. I only pick out the most salient things. And then my time was over in
this lab. And I went back to Germany and never reviewed this area of research again. But what
I had realized is to make progress and need to make statistics over what I need to make the
statistics over. And the very principled base, I need to learn what I have to learn.
And they didn't pay attention to this domain at all. And I also missed the 2017 transformer paper
and its relevance. It was only when GPT-2 came out that I realized, oh my god, they did this.
They did statistics over the statistics. And it's still not the right solution, I think. It's not
the way in which our brain is doing it. It's some brute force shortcut. Well, for instance,
the individual attention heads are not correlated with each other, but in reality,
they are. We have this in reality, our attention heads are integrated into one model of what's
going on. And it's not that we have an attention net on every layer that just pays attention to
what's happening in the lower layer. It's much more clever in our own mind. And this thing is
active. We single out things in reality to research, which book we need to take out of the
shelf to update our working memory context so we are able to interpret the current sentence
that we don't understand. And so we always go for saliency when we read something that doesn't
make sense. At least my mind works like this. I discard it. I will not stop until it makes sense,
or I will have to go to some preliminary. I will not accept some kind of vague statistical
approximation of what I read. Keep this as an intermediary stage in my mind until the hope
that eventually converges. It's a completely different learning paradigm. When we teach our
children arithmetic, it's not that we show them lots of very long mass textbooks and hope that
initially it will not make any sense to them. But as they reread them again and again with many
samples, eventually it will click and they will converge on arithmetic. No, this is not how it
works. You start this giving them extremely simple things and say, in these extremely simple things,
there is structure that you can fully understand. Now go and find the structure that you fully
understand. Once you've done it, you make this a little bit more complicated for you. This is
probably the paradigm that you could be exploring. I know, but the problem is it's incredibly
deceptive when you have something which appears intelligence. Of course, the
boundary of our perception of intelligence is a receding one. But I wanted to just get on to
there are some incredible generative visual models like Dali and the disco diffusion.
These models, I think, are going to revolutionize the creative profession. I've been
playing with disco diffusion all day today. I've already ordered some prints to go on my wall.
It's incredible. The two obvious settings where large language models might be successful
are coding and information retrieval in my opinion. But let's take pause for thought. I've played
with Codex and I'm resolutely sure that I wouldn't want to use it. I think code and knowledge are
a different ballgame to art, which I think will be amazing. With Codex, there's an impedance mismatch
between the process of generating the code and then debugging and running the code, which has
euphemistically been framed as prompt engineering, or another term which I've just invented,
retrospective development. I think it's easier to start again from scratch than fix broken code
from a large language model. I mean, this is quite interesting. At Google, it already takes
months to get any code checked into their mono repo because it's basically a bureaucracy because
they needed to have gatekeeping after they decided to use a mono repo. Could you imagine
how much bureaucracy there'd be if they allowed people to start checking in code, which was generated
from an algorithm? Anyway, I think there's an exciting possible future for using these systems
for information retrieval rather than the way that we go through and prune the results on a Google
search. These models might just answer directly, but I hasten to think what that search UI would
look like. Would its output be sclerotic or unadaptable? Would it be relevant to the query that
I put in? Would its output even be true? Perhaps it will ask you to select what kind of truth you
were looking for. Do you think these models would vitiate or spoil our society, or do you think they
would actually enrich it? It's very hard to say. I think that from some perspective,
our society is already maximally spoiled. Humans, as they live today, are basically
locusts with opposable thumbs. This is not going to go on forever, this technological society.
Here are, it seems to me, on some kind of Titanic that is going to hit the iceberg no matter what.
And what basically should make us content is that the Titanic was the only place in the
universe that has internet. And we are born on it, and we wouldn't have been born if there was no
Titanic. We would not have been born in a sustainable ancestral society. In some sense,
our society needs to reinvent itself. It's not really working right now. We don't know
what the future is going to look like, and if it's going to be very technological,
or if limit certain things, no idea what's going to happen. But if we think about how our
current approaches work, if you want just to make programming better, I suspect that these tools can
help. But they will be much more useful if you do not have to have this battle between a machine
that doesn't really understand what you want. And instead, you have something that is working next
to you. It's like, imagine you were working for some corporation and the corporation introduces
some kind of planning tool that requires to do to jump through all sorts of hopes. And it turns
out that the planning tool itself makes you more productive, but it makes work much less fun.
It's still rational to use it. And everybody will hate it, but
by and large, it will be used if it makes people 30 more productive. And everybody will feel there
might be a must be a better solution, something that feels more organic. And so it could be that
Codex is in this category that it makes mediocre programmers much more productive at producing
boilerplate. But it's not just this, it's often able to find solutions very quickly,
but you need to use a lot of Stack Overflow before you understand the new language or before you
tease this new algorithm or part that you want to understand and so on. It just when it doesn't
probably turn you into a better programmer, if that is your goal. But for your employer,
maybe they don't care whether you're a better programmer, they just want you to turn out these
pages of code and then they run this against the verifier and against the unit test and then are
done, go to the next thing, right? So maybe it's not that important. But the systems that we would
want, what would they look like? I think they need to know what they're doing. You want to have a
program that is not just able to reproduce something very well in a given context, you want to
understand the context as deeply as you do or better. So it needs to understand what kind of
world it's operating in in the moment. And what itself is, what is it that it can do? What is
that what needs to learn still? In some sense, you want systems that are sentient. And it's self
like, oh my God, but it just means you have a learning system that's general enough to model
in principle the entire universe. And this is not as outrageous as it sounds because
Delhi is already dealing with two modalities, language and images. And we will get to video
and we will get to audio connected and you see them early steps in this direction with the Socratic
model of people, for instance. So I think that's almost inevitable that this generality will happen.
And you will have to add a system to work in real time so it can discover itself.
I think I think there's something really magic, though, about the creative process here. And also
the prompt engineering is another thing we can talk about. But Kenneth Stanley once made this
thing called Pick Breeder. And you could essentially distribute the selection of these images
created with CPPNs, Compositional Pattern Producing Networks. And you would just get these beautiful
images. They were they were so incredibly, incredibly diverse and interesting. So it's
not that the algorithms were intelligent, there was something magic about the externalized process.
And what's really interesting about these models like Dali, for example, is that creativity has
been distilled down to a raw idea in your head, right? So for example, I might decide to mix
the style of two artists and combine them with a new subject. And I want a black cat
in front of Royal Holloway University in the style of cyberpunk. I've been doing that all day.
And the technical process is now done for you. The only limit is your imagination. So just like
Kenneth Stanley's Pick Breeder, creativity itself has now become this uber efficient and
externalized process. I think it's unreal. But the thing is, like the reason I never thought GPT3
was intelligent is because it can't be used non interactively. The magic must happen when it's
used by humans interactively. Well, you can basically build a machine that is generating
prompts for GPT3. So in principle, you can build a robot that has a vision to text module. And that
is used to prompt GPT3 into generating a story about a robot who sees these things and interact
with them. And then you take the output of the generative model and translate this using text
to motor module. And in this way, you close the loop. And I just used it as a thought experiment
to think about the limitations of embodiment for such systems. Second is essentially doing that.
So somebody has made this happen. And even with the language model, it works to some degree.
And we know that we don't want to do this with natural language because natural language is a
crutch. These systems make up for this, but you're just using more natural language faster than
people could use it. But there is some language of thought that we are using that is not learned,
but discovered by our own mind that we converge on, that is much more efficient. And this language
of thought seems to be able to bottom out and perceptual distributed representations that are
unprincipled, like these neural networks are in a sense unprincipled, but they don't break.
Then there is something that is vague and ambiguous and has small contradictions in it.
But at some level, it also is able to emulate very principal logic very well and becomes very
sparse and very powerful in expressing things concisely. And this very concise language of thought
is so don't see it in our models. What Dali is doing, Dali 2, is that it combines the language
model and division model using embedding spaces. And these embedding spaces basically project all
the concepts into some high dimensional manifold and find similarities between them.
And Gary Marcus points out that there is an issue with compositionality in this. So you need to
find the semantic structure of a sentence that is made of a hierarchy of concepts. And this is easy
to do with the grammar. And it's much harder to do this with a deep learning system that needs to
discover this in a way and structure this space in the right way. It's not impossible. So when
Gary Marcus says these models cannot do this and cannot learn it, he is probably wrong.
But I think he is right in the sense that this is something that is much, much harder for the
current approaches. They need dramatic training data than a human being. And the algorithms are
not doing this naturally. So there are probably ways in which we could make this happen much more
elegantly and quickly and converge, for instance, on models for arithmetic.
That's right. I mean, I remember I read a really good Twitter thread, I think it was by Raphael
Millier, you know, about compositionality of these large generative vision models. Because usually
compositionality is referred to in respect of language models. I think Raphael said that the
assessment of the claim is complicated by the fact that people differ in their understanding of what
compositionality means. But if language is compositional, as you say, and thought is language
as argued by the proponents of this language of thought hypothesis, I think Raphael said
that he thought language itself should be compositional in a similar sense. And perhaps
by extension, visual imagery should be compositional. So I think Gary was arguing in a nutshell that
it's hard to go from the image, or let's say the utterance, if it's NLP, to the structure,
or the grammar, or the constituents. It's much easier to go the other way around, would you agree?
The issue is that language of thought is executable. And natural language is not.
We execute natural language by translating it into our mind in something that we can execute.
And the reason about code, you might use natural language to support your reasoning.
But the code that you build in your mind is filled in some kind of abstract syntax tree that you
can actually execute in your mind to some degree. And then you get a sense of the output. So you
entrain your own brain with an executable structure. And this executable structure has
properties that are quite similar to the ones that the compiler has in your computer. So you
can anticipate what the compiler is going to do with your code. You're not going to do this with
all the depths that your compiler do it, you might still have to run your code, but you will find
when you want to experience programming, your stuff will usually run. So our language of thought
can do this, it can execute stuff. And it's not just a machine neural network that guesses
what the outcome is going to be and is right some of the time. But it gets pretty good at
figuring this out. And this means that it has to build this compositional structure that has some
verifiable properties. And we observe ourselves operating on this verification process, right?
When we do introspection for the program, we observe ourselves how we direct our attention on
making proofs. And this attentional algorithm that works in real time, that is making changes on
your mental models and then predicts the outcome of these changes and compares this with what your
mental computations give you and then fixes your models of how your own thinking process in this
domain works and so on. You can observe yourself doing that. And it's nothing where I would say
a given approach or the given approaches that we have will never get there. But there seem to be
ways in which we have just barely scratched the surface in what you need to be doing to make
these models sample efficient and sparse and more adequate to model domains you're interested in.
Cool. Okay. Well, I mean, just to finish like the discussion about the OpenAI stuff, I mean,
I agree with the prognosticators. And I do think that these large language models and
these visual generative models will be revolutionary for some domains. But
you know, I think you really need to have a human guiding the creative process, which is a huge
limitation. But I think it could also potentially hint at what intelligence actually is, right?
I think intelligence might be this externalized process in a cybernetic sense, if you like,
this idea of intelligence being fully embedded in an algorithm in a single agent might be
the wrong way to think about it.
I think that humans, by and large, are very confused. Very often you need a human to guide a
human, right? And then you ask yourself, if you do this recursively, does this society know where
it's going? Or is this at some level confusion at all levels that is balancing each other?
So there seem to be very few people with a plan right now. And it's quite apparent that we see
that in the sciences, we see this in politics, we need an art and literature. It is that humans
have a higher degree of sentience. But by and large, very few people have a principal plan on
how to build a sustainable, harmonic world. And if you set an AI system to this task, it might
make more progress on it. It's just that Dalit is not operating in real time on the universe that
it's entangled with and neither is GPT-3. Both of them are in some sense, fancy autocomplete
algorithms. But this fancy autocomplete is able to do autocompletions that are far beyond the
autocompletion abilities of humans in almost every context. And so I don't see Dalit yet as art.
It's a very strange sense when someone at OpenAI let me throw trumps at Dalit too. And I got images
back. I had a sense of ownership. I had the sense that I was doing that, even though it was clearly
doing skills, using skills that I didn't have. And I suppose that you had the same impression
when you were generating things with your diffusion model that you're going to put up on your walls,
right? You did that using this amazing tool that was empowering you to think that you otherwise
never could do. But you are the creative nexus. And to make an artist, a digital artist, you would
need to create an autonomous creative nexus in a way, a creative entity, something that reflects
on the world because art is about capturing conscious states. So we would need to build a
system that has a story about itself and that is reacting towards own interactions with the world
and that would need to be human. It would need to be consistent. Something that is an intelligent
entity that is creatively interacting with the world. I think we could totally build an AI
artist franchise right now that would have a huge following. But what it need to have is an identity
that is not fake, that it's actually built from its interactions with the world in real time.
Well, I think we've got a lovely segue there because you said that art is about
representing our conscious states. In a way, I disagree with you because you could say,
well, it's very reductionist. I've just put a prompt in there and I've created art. Well,
I think it is art. But how much of a representation of my conscious state is it? I think Douglas
Hofstadter would say it wasn't. But over to the matter of consciousness because we're a bit low
on time. I mean, you said actually that you've spent much of your life thinking about what
consciousness is. And you said that you thought it was very mysterious, but you now think that
it's a riddle that can be solved, right? So on your recent theory of everything interview
with Donald Hoffman, actually, you said that it was virtual, not a physical thing, that brains
are mechanistic and that the elements of consciousness are magical somehow. But you said
it had an a causal structure, but not the way physics is built. But it was a story
which the physical system tells to itself. You said that the organism is a coherent
and consistent pattern, which is state building at least at some level of analysis and that
consciousness allows organisms to coordinate their cells to succeed in their niches. And then
you spoke of information processing over cells. Now, what model of I should say like what measure
of consciousness do you think you most align with? There is only one theory that offers a
measure of consciousness and that is integrated information theory, where you actually put a
number on it. And it's not clear what that number means. It's not that there is some kind of scalar
that measures this. And people we think of consciousness as something that is more qualitative
than quantitative. Either somebody is conscious or somebody is unconscious. And when you are
conscious, you can have a lack of acuity, you can be addled in your brain and you can be
drifting in and out of consciousness. But it's still a qualitative thing of whether you have
that or not. And this qualitative thing seems to be simple, probably much simpler than people
expected to be. The hard thing might be perception. And consciousness is on top of
perception is a certain way to deal with our attention. So I think a very important aspect
of consciousness is reflexive attention, that we notice ourselves attending to something,
and we reflect on that and integrate this in our model. The conundrum is understanding consciousness
if you go right into the history of everything starting with Leibniz and many others. Leibniz
says this idea of imagine you could have a mill and this mill, this is your mind. And the mill is
made of lots of mechanical parts and somehow the thing is feeling and perceiving things.
And we blow the mill up so large that we can walk into it or we would today zoom into it until
we see all the parts and we just see these pushing and pulling parts and nothing of them
can ever explain a perception or a feeling. And it is a very strong intuition that also
drives the Chinese rule when many other thinkers will get attracted to this. And it seems pretty
obvious that these mechanical phenomena are insufficient to explain what's going on. It's
not an obvious connection. So people become dualist. There are somehow two completely separate
domains. And I think in a way this dualism is correct, but not in the sense that the mental
states are ontologically existing. They exist as if. There is no organism. There is only this
connection of cells and this collection of cells is acting in a coherent way, which means we can
compress it. We can model it using a very low-dimensional, much circular function
than look at all the cells in general. And the organism is only approximating this function,
but what makes the organism more powerful than a collection of cells is exactly that function,
this structure that we project into it. And the interesting thing is that by the information
processing within the organism, the organism can discover that function by itself and use it to
drive its own behavior. So while the organism is not a person, it's not even an organism,
it is very useful for the organism to behave as if it was an organism and also to have an idea
of what it would be like to be a person that interacts, for instance, with the social world.
So it creates a simulation of that. And it's often not even a simulation, it's often just a
simulacrum. That's what makes it a causal. The difference between a simulation and the reality
is that the simulation is modeling some aspects of the dynamics of a domain on a different causal
substrate, on a different causal footing. So you have a computer game in which you can shoot a gun,
but there is no proper physics in the game that would recreate what's happened in the real world
when you shoot a gun. Instead, it is using a different causal structure of your software
program to give you something that gives you good enough dynamics so you can interact with the world
and experience these causal structures. You can make a different decision, you make a different
move in the game, and as a result, the game behaves as if you would expect it because it's
imitating the same causal structure using this different substrate. In the simulacrum,
you don't have the causal structure. Like a movie doesn't have causal structure. It only
gives you a sequence of observables. And our own mental model of ourselves is a mixture of
simulation and simulacrum. So we sometimes create a sequence without causal structure. It looks like
it does this thing magically. And sometimes we have a causal model, but this causal model
is not the real deal. It's just this simplified geometric simulation of how the world works.
It's a game engine that our brain is producing to anticipate what happens in the physical world.
Yeah, so that very strongly resonates with me. And another person I very much respect
whose opinion resonates with me regarding consciousness is Carl Friston. And I'm not
sure how much you're familiar with his free energy principle and his thoughts on consciousness,
but I'd like to put forward to you one of his more recent definitions, if you will,
or proposals to explain consciousness. And I get your opinion on it. This is from his
2018 article titled, am I self conscious or does self organization entail self consciousness?
And what he says here is the proposal on offer here is that the mind comes into being when
self evidencing has a temporal thickness or counterfactual depth, which grounds inferences
about the consequences of my action. On this view, consciousness is nothing more than inference
about my future, namely the self evidencing consequences of what I could do. Does that align
pretty closely with your your view?
No, I don't think it's sufficient and also don't think it's necessary. I like Friston's idea,
but most of the free energy principle comes down to predictive coding,
which is in some sense, radically tested with GPT-3. GPT-3 is trained in some sense entirely
on predictive coding. It's only trying to predict the future from the past. And the future is the
next token based on the tokens that it has seen so far. And GPT-3 radically tries how far you can
go with this, and you can go very far. But you need far far more samples than an organism does.
So there are players in us that go beyond predictive coding, maybe we converge towards
this over many generations in the evolutionary process. So I don't think it's a stupid idea
that Carl Friston proposes, but we are born with additional loss functions that let us
converge much, much faster on something that is useful to the organism. And if we think about
consciousness, he has a point about agency in there. Agency means that you have a controller
that is able to control the future. Took me a while to understand this, but when I go up,
we talked about BDI agents, and they seem to be quite complicated and convoluted,
put a lot of quote there to make a BDI agent, but there's beliefs, desires, and intentions,
and so on. But if we think about what actually is a minimal agent, a thermostat is not a minimal
agent. So that has enough agency, it doesn't want anything. It just acts on the present
frame by doing the obvious thing. But imagine that you give the thermostat the ability to
integrate the expected temperature differences, the differences over the future, when it does x now
or y now or does it a moment later, right? So suddenly you have a branching reality. And in
this branching reality, you can make decisions and you will have preferences based on this
integrated expected reward. So just by giving the thermostat the ability to model the future,
you turn it into an agent. This is sufficient. And if you make this model deeper and deeper,
it's going to get better and better at it. And at a certain depth, the thermostat is going to
discover itself. It's going to discover the idiosyncrasies of its sensors. And notice that
the sensor operates differently when it's closer to the heating element and so on and so on, right?
So it becomes aware of how it functions. It might even become aware of the way in which its modeling
and reasoning process works and to improve it or to account for its inefficiencies in certain ways.
And this is also what we do with our own cell. But this model of the self is not identical to our
consciousness. Our consciousness is a feeling of what it's like in the moment. It's the experience
of a now. There is an experience of a perspective that we are having, right? And this is what's
absent in the description of first. And he is missing the core point of what it means for
something to be conscious. It doesn't mean that it has a self. It doesn't even just mean it has a
first person perspective. It means that it experiences a reality. And this is not described
in this Friston quote. We explicitly asked him this question actually when we talked to him
last time. And his response there was that this concept of feel like is really something that
would need to be coded into the generative model that this agent has about the world. Number one,
it has to have a generative model as we've just been discussing. It has to be able to entertain
counterfactual, you know, possibilities for the predictive coding, right? And he's saying that
these feel like concepts would literally be encoded in that generative model as hypotheses
that we recognize, you know, so things like I'm feeling pain, for example, would be a concept
within that model. And he says there's actually evidence from, you know, treating patients with
chronic pain and this sort of thing that that's actually exactly what's happening in the mind
that that the feeling of pain is actually a concept that's built in as a slot, if you will,
into this generative model. I mean, what do you think about that proposal?
The semantics of pain are given by the avoidance that you don't want to experience the pain usually.
And it could be that you cultivate the pain and use it to make something happening on the
next level, but it requires that you are then building a multi-level control structure,
if you want to use pain productively, some artists are maybe doing. But the, you cannot have pain,
I think, without an action tendency, without something that modulates what you are doing.
So your, your cognition is embedded into this engine. And to build such an engine that does it,
that causally changes how you operate is not that hard. But when you live inside of such an engine,
it feels very strange that there is something that is happening that somehow depends on what
you are thinking, but you cannot control it. It controls you. It's upstream from you. You are
downstream from it. And when you get upstream of your own pain, the pain stops being pain. It's
something that is a representation that you can now control and be able to get there. But it's
not easy and you're not meant to get there because it means that we can immunize ourselves to pain
and sacrifice the organism to our intellectual interests. What's crucial about feelings when
you look at them introspectively is that feelings are essentially geometric. I don't know if you
noticed that. So for instance, we notice feelings typically in our body. And that's because I think
that the feelings play out in a space. And the only space that we have always instantiated in our
mind is the body map. So they're being projected into the space to make them distinct. And when we
look at the semantics of the feeling, we noticed that they are contracting or expanding or they
are light or they're heavy and so on. This is all movement of stuff in space. It's all geometry
plus valence, the stuff that is going to push your behaviors in a certain direction.
So these are basically the interactions of some deep learning system that is producing
continuous geometric representations as perceived from an analytic engine. It's an interface
between two parts of your mind, between the analytic attention control that is reflecting
on the operations that your mind is doing while it's optimizing its attention. And the underlying
system that represents the state of the organism entails where you should be going and makes this
visible to you. It is a system that is not able to speak to you, uses geometry. And these
geometrical features, this is what we call feelings. So that's a very interesting connection.
And I think Jeff Hawkins of Nemento would be quite interested in that as well, because
some of what he discussed with us was that, in his view, the evolution of, let's say,
abstract thinking and whatnot actually came from systems that evolved to operate in just
simple three-dimensional kind of motion, and that eventually those were reutilized by the
evolutionary process to start engaging in abstract thinking, which he views as movement
through an abstract space. And so I think there's a lot of connection here to what you're saying
about feeling, which is that, again, in a sense, our mind has reutilized this
three, three plus one d movement mapping capability that it needed in order to survive in a three
plus one d environment, physical environment, and it's reutilized those for mapping feelings,
it's reutilized them for mapping to abstract thinking is like a form of motion in an abstract
space. Is that a fair connection? Yes, but I don't think that it's because it's
borrowed from the world in which we interact, but because it's the only game in town, it's the
only mathematics that can deal with multi-dimensional numbers. So when we talk about spaces, we actually
talk about multi-dimensional numbers, about things that are not just a scalar in a single
dimension, but features that are related. And sometimes you can take these features that you
measure continuously because they have too many steps to meaningfully discretize them.
So what it does is you discover that you can rotate something. And this is when you get a
space in the sense as we have a space to which we are moving. And these spaces which you can
rotate things only exist in 2D and 4D and 8D. And so the geometry that we're talking about is
constrained to certain mathematical paradigms, which you can derive from number theory,
compressed principles. And our brain is discovering a useful set of functions to model
anything, a set of useful computational primitives. And we can probably give our deep learning systems
a library of predefined primitives to speed up their convergence. That's also the reason why
there is useful transfer learning between different domains. You can train a vision model
and use it as a pre-training for audio. And it's not because it's the same thing, but because it
has learned useful computational primitives that it can apply across domains. But there is geometry
in the audio signal. So this is very interesting territory. I hope you'll come back to dive into
this a bit more deeply when we have more time and a better connection, because I agree with you.
Some very fascinating math here.
Fantastic. Well, Dr. Yoshua Bak, it's, as I said, you've by far the most requested guest
that we've had right from the very beginning. So it's an honor to finally get you on the show.
And I hope we can get you back soon for a longer conversation. Thank you so much.
Likewise. I enjoyed this very much. Let's meet again soon.

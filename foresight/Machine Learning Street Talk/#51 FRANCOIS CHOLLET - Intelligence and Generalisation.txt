Past a certain level of complexity, every system starts looking like a living organism.
In order to build a general intelligence, you need to be optimising for generality itself.
We are surrounded by isomorphisms, just like a kaleidoscope. It creates a remarkable richness
of patterns from a tiny little bit of information. Generalisation is the ability to mine previous
experience to make sense of future novel situations. Generalisation describes a knowledge
differential. It characterises the ratio between known information and the space of possible future
situations. To what extent can we analyse the knowledge that we already have into simulacrums
that apply widely across experienced space? So intelligence, which is to say generalisation
power, is literally sensitivity to abstract analysis, and that's in fact all there is to it.
In today's show we are joined by Francois Chollet. I have been using the Keras Library for many years.
I also read his Deep Learning with Python book, which was inspiring, and I discovered his racy
Twitter feed. When I worked for Microsoft I used to run machine learning seminars and workshops
and hackathons. I used to travel around the world and I always had a copy of Francois's book
Under My Arm. It never left my side. I used to force everyone to read the first four chapters
of that book and of course the chapter on the limitations of deep learning before we did anything.
Francois has a clarity of thought, which is unparalleled I think in any other human being
on the planet. It's really quite incredible. Indeed even our own Dr. Duggar, who normally has
no trouble at all finding holes in some of our guests' work, had this to say while prepping
for the show. I'm working on it. It turned out to be a little bit more difficult than I thought.
Chalet is a little bit too reasonable. Yeah, do you like my Duggar accent? He would enjoy me
doing that. But anyway Chalet is extremely controversial to some people actually, but
he's not controversial to us. Our discussion today lies at the intersection of machine learning
and reasoning. Now Chalet has made his vision completely clear about what he thinks the future
of machine learning is. Make no mistake, what you should take from today's episode is that the
future of artificial intelligence is going to be discrete as well as continuous. Actually the two
are going to be enmeshed. The future of AI will almost certainly involve a large degree of program
synthesis. Deep learning has its limits. You can use deep learning for continuous problems
where the data is interpolative and has a learnable manifold and where you have a dense
sampling across the entire surface of the manifold between which you need to make predictions.
For Chalet, generalization itself is by far the most important feature of intelligence
and of developing strong AI. He describes a spectrum of generalization starting with,
for example, a chess algorithm where there is no novelty to adapt to whatsoever. The task is fixed.
The machine learning we have today confers some adaptation within a known domain of tasks. For
example, being able to recognize dogs or cats within a variety of different poses and lighting
conditions. What's not been robustly demonstrated so far is broad generalization, adaptation to
unknown unknowns within a known but broad domain. It's certainly true that we're knocking on the
door of this now with GPT-3, where the subtask, if you like, is given at test time. Although
Chalet would make the argument that the subtask isn't learned at test time, everything that GPT-3
knows was learned on the vast amounts of training data that we trained it on, the poet algorithm
from Kenneth Stanley et al. That appears to be meta-learning tasks as part of the training
process, which is very, very interesting. It's creating new problems and new solutions as part
of the training process. But broadly speaking in the machine learning space at the moment,
the task that we are doing is fixed and not generalizable. The other thing is that the real
world does not have a static distribution. We need systems that can adapt dynamically.
Intelligence requires that you adapt to novelty without the help of the engineer who helped you
write the system. Chalet has come up with a formalism of intelligence that balances the task
skill, the difficulty, the knowledge, and experience to effectively quantify and normalise
an algorithmic information conversion ratio. It's the ability to convert experience into future skill
that is Chalet's measure of intelligence. At the end of his measure of intelligence paper,
Francois introduced the ARC challenge. It became a Kaggle competition as well and it introduced a
massive diversity of tasks. The reason we have a diversity of tasks is for developer-aware
generalisation. Any model that we have needs to generalise to tasks that the developer was
unaware of. And Chalet thinks that intelligence is specialised. It needs to be human-centric or
anthropocentric. So the kind of priors that you need to solve these intelligence tasks need to
represent the kind of priors that us humans have. Now machine learning algorithms are completely
ineffective against the ARC challenge because it's so challenging to generalise from a few
examples. The only solutions that were effective in the ARC challenge were programme synthesis.
The manifold hypothesis is that natural data forms lower-dimensional manifolds
in its embedding space. There are both theoretical and experimental reasons to believe this is true.
If you believe this, then the task of a classification algorithm is fundamentally
to separate a bunch of tangled manifolds. The only way deep learning models can generalise
is via interpolation. Most perception problems in particular, according to Francois, are
interpolative. Neural networks not only have to represent the manifold of the data that they're
learning, the manifold also needs to be learnable. And that's an even tougher constraint.
Gradient descent will not learn data that has challenging discontinuities in its manifold.
It'll just resort to memorising the data. Deep learning allows you to represent complex programmes
that you couldn't write by hand, but on the other side of the coin it also fails to represent
very simple programmes that you could write by hand. Discrete programmes. So there are some
problems where deep learning is a great fit and there are other problems where deep learning
is a disaster. And the reason for that is that they are not interpolative in nature. These tend
to be algorithmic reasoning problems. Francois thinks that 99% of software written today,
with code, is not interpolative in nature and therefore it's a bad fit for deep learning.
The only answer to these problems is discrete programme search. To use deep learning for these
problems requires a lot of data. It's hard to train and the representation will be glitchy.
It'll be brittle. Neural networks cannot even extrapolate the scalar identity function,
f of x equals x. They can only interpolate given the existence of a smooth manifold in the latent
space. Jan Lacune recently said to Alfredo that all high dimensional machine learning is extrapolation.
So is this similar to interpolation? Well, I mean, all of machine learning is similar to
interpolation if you want, right? When you train a linear regression on scalar values,
you're training a model, right? You're giving a bunch of pairs x and y. You're asking what are the
best values of A and B for y equals A x plus B that minimizes the square error of the prediction
of a line to all of the points, right? That's linear regression. That's interpolation.
All of machine learning is interpolation. In a high dimensional space, there is essentially no such
thing as interpolation. Everything is extrapolation. So imagine you are in a space of images, right?
So you have a core images 256 by 256. So it's 200,000 dimensional input space. Even if you have
a million samples, you're only covering a tiny portion of the dimensions of that space, right?
Those images are in a tiny sliver of surface among the space of all possible combinations
of values of pixels. So when you show the system a new image, it's very unlikely that this image
is a linear combination of previous images. What you're doing is extrapolation, not interpolation,
okay? And in high dimension, all of machine learning is extrapolation, which is why it's hard.
I'm being brave calling out Jan Lacoon, the godfather of deep learning, but hear me out.
It's certainly true that interpolation on the native data domain is useless, right? We need to
pull some useful information out of the data and the model architecture and training method matter
a lot here. We can all agree that interpolation on the learned manifold would seem like extrapolation
in the original space of the data, right? Chalet is quite clear that neural networks only
generalize through interpolation. You might argue that you can go a tiny step outside of the convex
hull of your data, even by a tiny little bit, and you can technically extrapolate. Well, I would argue
that if the manifold doesn't give you any useful information outside of the training range, then
it wouldn't be any better than finding your nearest training example and just adding a bit of random
noise. If you train again, for example, you can interpolate on the latent manifold, but interestingly,
you can extrapolate. But the reason for that is the natural manifold that the data of faces sits on
might be shaped like a football or a sphere, which means if you go outside of the training range,
you actually have some information about those data points. The scalar identity function might seem
like a contrived example, but it's a really interesting one. When you go outside of the
training range, nothing about the manifold is known, right? Think about the manifold. It's just a
string that goes on forever. We don't know anything about that manifold outside of the training range.
This is not true for most perceptual problems in deep learning. And this is why image models,
for example, suffer greatly drawing straight lines. What are your thoughts about this? Why don't you
let us know in the comments section on YouTube? So there's a real interesting dichotomy of continuous
problems versus discrete problems that we're going to be exploring in the show today. It's very
interesting that brittleness works both ways, depending on the discreteness of the problem.
Program synthesis would be extremely brittle in classifying cats versus dogs or even M-nist,
and deep learning would be extremely brittle predicting the digits of pi or prime numbers
or sorting a list. So brittleness here means the overall fit of your model or your program,
so accuracy and robustness. Imagine if every single bug you experienced with computer software
was entirely unique to you and the development team wouldn't even be able to reproduce it.
This is what would happen if software was written entirely with neural networks.
It would be more, not less brittle. Sholey thinks that motivated thinking is the primary
obstacle to getting people to wake up to the fact that neural networks are poorly suited
to discrete problems. The people who are good enough at deep learning to realize its limitations
are too invested in its success to say so. Sholey fundamentally thinks that there are two types of
thinking, type one and type two. He thinks that every single thought in our minds is not simply
one or the other, rather it's a combination of both types. Type one and type two, they are
enmeshed together in everything you think and in everything you do. Even our reasoning is guided
by intuition, which is interpolative in nature. Sholey thinks that abstraction is key to generalization
and the way we perform abstraction is different in continuous versus discrete space. We need to
find analogies and those analogies will be found differently in both of those different spaces.
Program search allows us to generalize broadly from just a few examples. It marks a significant
deviation from traditional machine learning. Rather than trying to interpolate between the
examples you have, you're constructing an entire search space from scratch and testing
if it fits our training data. It all started with the flash fill feature in Microsoft Excel.
Do you remember that? You give a few examples of some transformation that you want to perform
and it will generate a piece of programming code for you, which means it can generalize
that transformation across an entire spreadsheet. It's quite a revolutionary idea. It's been around
for about 20 years actually, but what's really making it work now is the idea of using neural
networks or a neural engine to guide the discrete program search. We spoke about GPT-3. He thinks
that GPT-3 hasn't expanded his knowledge of the world. He says that GPT-3 is not learning any new
algorithms on the fly. It's already learned continuous and often glitchy representations
of existing tasks during its training. It's completely ineffective against his arc challenge
tasks. People often claim that neural networks are turing complete. No, they're not. A model has
a bounded number of nodes and a bounded runtime. It cannot execute algorithms that require unbounded
space or unbounded time. For example, could you train a neural network to predict the nth digit of
pi? No, you couldn't. You could write a computer program to do it, but you couldn't train a neural
network to do it. A simple turing machine program can do just that and that is because a turing machine
can access unbounded memory and time. The best thing that neural networks can do is approximate
unbounded algorithms, but doing so will introduce glitches. For example, one can train a neural
network to approximately multiply integers together. Yet, even when learning to multiply
fixed-width integers, practically-sized neural networks introduce errors occasionally,
and for a fixed-sized neural network, these errors grow more common as the size of the input grows.
That said, neural networks are finite state machines, and just as finite state machines
can be augmented with unbounded memory and iteration to yield a turing machine, neural
networks can also be automated in the same way to produce a turing-complete computational model.
If you want to see a concrete example of the kind of discrete program search that
Chalet is talking about, look no further than the recent DreamCoder paper. Yannick just made a video
about it. So yeah, it feels like today is the culmination of a year of really hard work and
passion from the MLST team. We've worked with so many fascinating people. We've had so many
amazing guests on. It really means a lot to us. Today is a very, very special episode. It was
my dream from the beginning to get Chalet on the show. I know that Chalet is going to say
lots of interesting things that will trigger some people and inspire others, and please take to the
comment section and tell us exactly what you think. Anyway, enjoy the show. See you next week. Peace out.
Welcome back to the Machine Learning Street Talk YouTube channel and podcast
with my two compadres, MIT, PhD, Dr. Keith Duggar and Yannick Lightspeed Kiltcher.
Now today we have a very special guest, Francois Chalet. Francois is one of the few leaders in
the machine learning space who's caused a massive stir in my thinking, the only other notable one
actually being Kenneth Stanley, who we had on recently. My ultimate goal with Street Talk was
always to get Francois on the show, and I can't believe that it's actually happened. We actually
have a rule, by the way, that I'm only allowed to invoke Francois's name about once per show,
but that rule will not apply today. Yannick and I have made more content on Francois Chalet actually
than anyone else by a wide margin, and it's because his work is very thought-provoking
and disruptive. I spent many weeks actually studying his measure of intelligence paper last year,
and of course his recent New York's workshop was fascinating as well. Almost every single word in
my opinion that comes out of Francois's mouth deserves rigorous study, and I seriously mean that.
Francois thinks that intelligence is embodied, it's a process, and it's not just a brain. He's
skeptical of the so-called intelligence explosion, and he thinks there's no such thing as general
intelligence. All intelligence is specialized. Critically, he thinks that generalization,
the ability to deal with novelty and uncertainty is the most important concept in intelligence.
He thinks that task-specific skills tells you nothing about intelligence. He thinks that deep
learning only works for problems where the manifold hypothesis applies. For example,
problems which are interpolative in nature and when a sufficiently dense sampling of your
distribution is obtained. Otherwise, deep learning cannot generalize. Deep learning can only memorize,
but it cannot always generalize. And in his recent New York's presentation, he introduced the concept
of program-centric and value-centric generalization, which we'll get into in the show today.
But I wanted to move straight on to this concept of deep learning being a hash table,
because this is what Francois thinks. He says that a deep learning model is a high-dimensional
curve with some constraints on its structure given by inductive priors, and that curve has
enough parameters that it could fit almost anything. If you train your model for long enough,
it'll simply memorize your data. And because of SGD, your manifold fit is found progressively,
and at some point, the manifold will approximate the natural manifold between underfitting and
overfitting. And at this point, you'll be able to make sense of novel inputs by interpolating
on that manifold. So the power of the model to generalize is actually a consequence of the
structure of the data and the gradual process of SGD, according to Francois, rather than any property
of the model itself. Last week, Francois, we were talking to Christian Sergeidi, and he takes a
rather different view, because one school of thought is that deep learning models are kind of like
searching for a space of possible programs, and advocates of GPT-3 make this argument quite
strongly. And presumably, Christian Sergeidi, he wouldn't be doing what he's doing, which is
interpolating between mathematical conjectures, assuming that interpolation space would actually
give us new information about mathematics, if he thought that that space wasn't interpolatable.
What do you think Francois? Right, I think you've already summarized it, really. Yeah, so interpolation
is the origin of generalization in deep learning models, and that's very much by construction,
by nature, right? Like a deep learning model is a very large, differentiable,
parametric model, trained with gradient descent. And so the only way it's ever going to be
generalizing is your interpolation. This is literally, this is what it is, this is what it does.
So I think the question, you know, are all deep learning models, interpolators or not,
is not a super interesting question, because it's not an open question. We know they are.
But the more interesting question, I think, is what can you actually achieve with the
stored of interpolation on this very complex, very high-dimensional manifold,
that they're deep learning models and implementing. We're telling you the properties of this generalization,
the tasks for which it will perform well, the tasks for which it will not perform well.
I guess one example I could give you is encoding data with the Fourier transform,
like you know about the Fourier transform. And maybe, you know, some people will play around
with it and they will be like, hey, you know, actually the Fourier transform can draw much
more than curves. Look, I made a square with it, right? And then you would have to point out that,
no, actually the square, you've made it by supposing lots of tiny curves. And it's not,
in fact, a perfect square, right? Because it is made of this, with the other supposition of lots
of tiny curves. And that's really, this is true by nature, by construction. This is where the
Fourier transform starts, right? And the more interesting question is, you know, what sort of
data is a good fit for encoding the Fourier transform? And what sort of data is not a good
fit? Like if you try to encode the t-square fractal with the Fourier transform, you're going to have
a bad time. And if you try to encode the drawing, that's mostly just, you know, nice, smooth curves,
then it's going to be a very, very efficient encoding at a good idea. And deep learning is
very much like that. We should ask, you know, what are its strong points, what are its weak points?
Yeah, so I, by the way, so I don't believe that deep learning models are hash tables, plus there,
I usually say there, localities are sensitive hash tables, meaning that kind of like a hash table
with some amount of generalization power, because they have some notion of distance
between parts. They're capable of comparing points by measuring the distance between them, right?
And this, this is what would enable this kind of hash table to actually generalize, as opposed to
the classic kind of hash table, which is just memorizing the data.
It's very interesting that you allude to the fact that, you know, what kind of data is the model
good for, and so on. And now, deep learning models being essentially like really, as Tim said,
like big interpolators of arbitrary manifolds, do you think there is something
common across the types of data we choose deep learning for? Or, you know, could we in fact
use deep learning for most kinds of manifold dish data? Or do you think there is some kind of
specialness about natural signals that makes deep learning very attuned to them?
So I think most things are to some extent interpolative, which is why you can actually do
lots of things with deep learning models. Doesn't necessarily mean it's always a good idea,
but it's going to kind of work, right? You know, when people hear the word interpolation,
they tend to think about linear interpolation, that's what pops up in their mind. That's not
always what deep learning models are doing, right? They're interpolating on this very complex,
very high dimensional manifold. And this enables very, you know, arbitrarily complex behavior.
And in practice, it's always possible to an arbitrary discrete algorithm in a continuous
manifold, right? It's not necessarily a good idea, but it's always possible, at least in theory.
So for any program, you can imagine, you can ask, you know, is there a deep learning model that
will encode some kind of approximation of it? And the answer is always yes, right?
Similar to how you can always encode an arbitrary shape with the Fourier transform, right? But
there are, if you try to do that, actually, there are some issues with that. There are very much,
you know, some problems for which deep learning is good fit, some problems for which deep learning
is not a good fit. In the limit, the extreme point is a space that is not interpolative at all,
which is quite right, actually. You know, most spaces, even very discrete kind of spaces,
do have, you know, some amounts of interpolativeness. So like, but one example would be, for instance,
trying to train a deep learning model to predict the next prime number, right? Or to tell whether
a number is a prime number. As you cannot actually do that, the best you can do is memorize the
train data point, because the space of prime numbers is not interpreted at all. So your deep
learning model will always have zero generalization power. But that's actually quite rare. This is
kind of an extreme case. Most problems, even problems that are binary, discrete, algorithmic
problems, there will be some amount of interpolation that you can do, right? But that doesn't necessarily
mean that it's a good idea to try to solve, you know, such problems with deep learning models
for deep learning to be a good idea. You need a very, you need very much the manifold
level as it is to apply. So it works best for perception problems. Any problem that humans
can solve via pure intuition or perception is probably a good fit for deep learning. But any
problem where you need, you know, high level explicit step by step reasoning is probably
a bad fit for deep learning. And, you know, 99% of what today software engineers solve,
the writing code is going to be a bad fit for deep learning. That doesn't mean that there
wouldn't be, you know, theoretically, a deep learning model that can embed the same algorithm
in a smooth manifold. This is always possible to some extent, right? But there are very significant
issues with attempting to do this. I like just because something is theoretically possible doesn't
mean you should actually do it. I think we might be not being careful enough when we say what we
mean by program. Because, for example, if I take program to be the universal sense like a program
is something that can run on a Turing machine, for example, because of the fact that that type
of program actually has access to unbounded time and memory computation. It's impossible in the
general sense to encode that in any finite neural network, like I can write a very short piece of
code theoretical Turing machine can output, you know, the nth digit of Pi. It's impossible to do
that with any finite neural network. Would you agree? Yeah, absolutely. Absolutely. Okay, because I
think that's like a big source of confusion often time with these statements that like, you know,
oh, neural networks are Turing complete. Well, no, they're not. You know, if you have a neural
Turing machine, which is a neural network that's the finite state machine piece of a Turing machine,
that can be Turing complete. But in the general case, you know, finite neural networks, which is
what everyone means by neural networks, are not Turing complete. And it actually has practical
effects, right? This is why we see this sort of explosion and the number of parameters to kind of,
you know, start to accomplish. Yeah, absolutely. 100%. You're entirely right. So we're only
interested in realistic programs, like the sort of programs that start to engineer with right,
for instance. And we're only interested in realistic neural networks. And by the way,
the constraints that we have on neural networks are actually much stronger than asking,
given this program that I have, is there a neural network that could embed it in a continuous
manifold? The constraint is actually, is there a neural network that could not only represent it,
but that could learn this embedding of the program from there. And this is a several orders of
magnitude harder, right? Learnability is a big problem because you're fitting your manifold via
gradient descent, right? And if the structure you're trying to fit is too discrete, with too
big discontinuities, gradient descent will not work at all. And the best you can do is, again,
just memorize the train data. So I can maybe give you a concrete example to kind of ground
our discussion here. So in 2015, some friend of mine, so his name is, he used Keras to do
something pretty cool, which actually became a cool example on the Keras website. He used a LSTM
model to multiply numbers, but not like numbers multiplied by value, but the input of the model
would be strings, like two strings, strings of digits. And the LSTM will actually learn the
multiplication algorithm for like multiplying three digits and three digits numbers, kind of the
sort of algorithm we would learn in primary school, right, to do multiplication. And remarkably,
that worked, right? It works just fine. So you can train a deep learning model to learn this
algorithm. And you could, of course, train a transformer model to do the same. It will actually
be probably significantly more efficient. So that works. That comes with a number of downsides.
So first, in order to train that algorithm, which is very simple, you're going to need
thousands and thousands of examples of different strategic numbers. And once you've trained your
algorithm, because the actual algorithm was embedded in the neural network, it does generalize to
never see before digits, right? So it's actually learning the algorithm. It's not just learning,
I'm just not memorizing the data. But the thing is, because the embedding of an algorithm,
the embedding of a discrete structure in the continuous space, is not the same thing as the
original discrete object. There are glitches in your deep learning network, unless that's something
you could have found via program synthesis, for instance, it's not going to be correct 100% of
the time, it's going to be correct 95% of the time. In much the same way that if you try to
encode a very discrete object via the Fourier transform, it's not going to be correct. 100%
of the time is going to be an approximation and around sharp angles, it's actually going to be
wrong. And very importantly, and this is really like the algorithm that you've painstakingly
embedded into your deep learning model via exposure to data, does only, it does not generalize very
well, it only does local generalization, meaning that if you train it with three, to multiply
three digit numbers, and then you send it a five digit number, is it going to work? No,
absolutely not. And not only is it not going to work, but you could not in fact,
few shots fine tune your algorithm to learn to handle five digits, seven digits and so on.
If you want to fine tune your algorithm, you're going to need thousands, maybe millions
of examples, right? So it's all local generalization. And lastly, it's super inefficient,
like I think we can all agree with this, that multiplication is not like it's not
a clever use of an LSTM, it's you're burning tons of resources for something that's actually
super easy. And you can compare that, like since we are talking about pros and cons of deep learning,
you can compare that to what you could get with a program synthesis engine. Like I don't want to
compare to what you could get with a human written algorithm, because kind of the point of deep
learning is that it enables you to develop programs that you could not otherwise write by hand.
So the right point of comparison is actually what you could do with deep learning versus what you
could do with discrete program synthesis based on discrete search and the DSL. And if you were to
use a program synthesis to solve the multiplication problem, so you would find a solution, even a
very neat engine that does just like maybe a plus operation, maybe a loop. And this DSL is going to
find it, it can find it with a handful of examples, you're not going to need thousands of examples,
like in the deep learning case, you're going to need maybe five. And the program you get out of it
is going to be exact, because it is the exact discrete algorithm, it is not a continuous
embedding of it. So it does not have glitches, it outputs the correct answer. It will be lightweight,
so it will be very efficient, you know, and like the LCM or transformer model,
and crucially, it's going to generalize. So if you develop it only from three digit numbers,
maybe there will be something inside it that will hardcore the assumption that they're dealing with
three digit numbers. But even if that's the case, you can take it and automatically learn
a generalized form of it if you just start giving it seven digit numbers. Very easy because it's
just modifying probably a couple lines of code. So it is capable of strong generalization. So here
you start seeing how for a problem that's fundamentally a discrete algorithmic reasoning
problem, discrete search is the correct answer. Deep learning, it's possible, it works, but with
extremely stark limitations, right? It's very hard to train it, you need tons of data. The resulting
embedding, because it's not, it's not discrete, we'll have glitches. It's not going to work on
a person at a time, it's going to be pretty long. It's only going to be capable of hardcore
generalization, right? Because again, there is a huge difference in representational flexibility
between your very simple, discrete algorithm and some kind of very complex, high dimensional
continuous embedding. And then there's also the efficiency consideration. So clearly for
if you're dealing, and the reverse is also true, right? Like if you're dealing with a problem that's
perception problem, where you have data points that fit on a nice and smooth manifold, then
deep learning is actually the right answer. And if you tried to train a discrete program to develop
your program synthesis, an actual algorithm to classify MNIST digits, for instance.
Everything I just said would be true, but in reverse, your program would be brittle. The
deep learning model would be robust, and so on. So there are really problems where deep learning
is a pretty idea. It's a great fit. Problems where it's a terrible idea. Like try sorting a list
with deep learning model. Can it be done? Yes, actually it can. But with all these caveats applying.
It is possible to sort a list of deep learning with some hacky inductive priors and probably
memorizing most of the training data. And it's not a binary, is it? You said yourself, there's lots
of problems that fall in the middle, where there is a semi continuous structure and some
regularity, but it's still a discrete problem. And you're saying in that situation, we should
still use program search, but maybe we can use deep learning, maybe something about the shape
of the manifold, even though it's semi continuous, could actually tell us about how to do that
program search more efficiently. But it seems to me that if there are problems out there,
let's say adding numbers up in GBT3, when I read the stuff that you've been talking about here,
it seems obvious to me. Why are people not picking up on this? I think most people are not necessarily
paying a lot of attention to the nature of deep learning, why it works, why it doesn't work.
I also think the people, they are basically two categories of people. They are like laypeople,
and they are people with deep expertise. And the big problem we have here is that the people with
a lot of expertise are going to be a lot of the time driven by motivated thinking. Because
like I do, they work in the field of deep learning, and so they're going to have this vested
interest in deep learning being potentially more powerful, more general at the nature is. I think
if you want to think clearly, the primary obstacle is motivated thinking. It's fighting
against what you want to be true. So I tend to have super boring opinions in that sense,
because I do my best to try to forget kind of what I would like the world to be in my
best interest and try to look at it as it really is. And that will tend to actually diminish
the importance of my own work. So yeah, but you know, I've been doing deep learning for
almost a decade. Of course, I would want it to be like this incredible world changing thing that
leads to human level intelligence, right off the bat, that would be that would be awesome,
that would be amazing, and that would be right in the middle of it. But that's not that's not
actually what's going on. You said you tend to be what was the word, not not controversial ideas
or something because you try to stick to the way the world is rather than the way you want the
world to be. But we just had Yannick produce an interesting video about how if you think that
machine learning models essentially attempt to do the same thing, right? I mean, they're not human
beings, they don't really have wants per se, they're just modeling reality as it is. It turns out
reality itself really annoys a lot of people, like they just don't like reality, and they don't like
the way the world is, and they wish it was something different. And that infects like every mode of
their thinking, actually. Yeah, no, absolutely. Most people, you know, and that's that's true for
me as well. I'm not saying I'm an exception, I'm trying to do my best to resist this trend.
But I have no exception. Most people have opinions not because they've seen evidence in
support of their opinion, but because it's in their interest for this opinion to be true,
or they just want it to be true. I guess one example is, you know, we were mentioning GPT-3
and so on and proponents of GPT-3. I was actually super excited when I initially saw the claim
that the pre-trained language model could perform few short generalizations. I thought that's
super fascinating. I'm always super excited if I hear about something that's really challenging
my initial kind of mental model of how the world works, you know, it's like a few years back,
and there was this claim that a neutrino was measured going faster than speed of flight.
I mean, that's exciting, right? That's like new physics, you want it to be true, at least you
want to get to the bottom of it. And then it turned out to be a measurement error, right? So that's
disappointing. So I think GPT-3 is kind of the same for me. I really wanted it to be something,
something novel, and that would really challenge what they thought to be true by deep learning
models. And I regret to say that everything I've seen close has actually confirmed. In my view,
that basically deep learning models, they can learn to embed algorithms given sufficient exposure
to data, but they cannot really, like, few short synthesize novel algorithms that represent a pattern
they haven't seen in a train yet, which is why, by the way, GPT-3 is entirely ineffective on ARC,
for instance. And that's kind of sad to me. I kind of regret it, because it means I haven't actually
learned anything from it. It hasn't expanded my view of the world, which is too bad. Like,
I wish it did. I wish it did. So in the case of GPT-3, what's really going on is that the model
is exposed to many patterns. You could call them algorithms, for instance, in many different contexts.
And so it has memorized these patterns. And now it's able to take these patterns and apply them
to new data in much the same way that the multiplication algorithm we are talking about.
Because it's an actual algorithm, it can process new digits. It's not just memorizing the digits
in the train. It's an actual algorithm. In the same way, GPT-3 contains tons of small algorithms
like that. But the model is not synthesizing these algorithms on the fly. They are in the model already.
And if you try to apply GPT-3 to something for which a new algorithm would need to be produced,
like an ARC task, for instance, it has just completed anything.
It seems to all build up what you're saying, because there is this strong generalization
versus local generalization. And then you make a case that in order to do strong generalization,
we need maybe something like program synthesis approach. So deep learning can't necessarily
get us there in most problems. And you make an interesting case that something like graph
isomorphism search could play a core role in that. Could you briefly connect all of these
terms together of the case you're making there? Because it's super interesting.
So going back to it, Tim was saying it's rarely the case that you have problems that are fully
interpretive or fully discrete. There are definitely such problems. In fact, most perception
problems are almost entirely interpretive. And most programs, the kind of programs that you
write there, they're largely discrete, not interpretive. But most tasks actually are best
solved via a combination of both. And I actually believe that's true for the way humans think.
You know, there's type 1 thinking and type 2 thinking. I strongly believe that almost every
thought you have and everything you do with your mind is not one or the other. It's a combination
of both. That type 1 and type 2 are really unmatched into each other in everything you
think and everything you do. Like, for instance, perception. That looks like something very
instant. So very much the sort of continuous, interpolative thing. In fact, there's a lot
of reasoning that's embedded into perception. And the reverse is true, for instance. If you
look at a mathematician, for instance, proving a theorem, where they're writing down on the
sheet of paper is really step-by-step, discrete reasoning type thing. But it's very much guided
by high-level intuition, which is very much interpreted. They know where they're going,
without having to derive the exact sequence of steps to get there. So they have this high-level
kind of view. Kind of like, you know, if you're driving, you have to make discrete decisions
because you are driving on network frauds. But if you have a bird, a GPS, for instance,
you can kind of see the direction in which you are going, which is interpolated. If you're talking
about direction, you're talking about distances, you're talking about geometric spaces. And
everything in the human mind kind of follows this model of type 1 and type 2 thinking at the same
time. If you go back to first principles, intelligence is about abstraction. So intelligence
is about the ability to face the future, given things you've seen in the past. And the way you do
that is, yeah, abstraction. You extract from the past some construct. Maybe it's a template,
maybe it's an algorithm that will actually be effective in terms of explaining the future. And
that's why it makes it abstract, is that it can handle multiple instances of some kind of thing,
that thing is an abstraction. And if it's abstract enough, it can actually handle instances
you've never seen before, right? It does generalization power. And all abstraction is worn
from analogy. Abstraction starts when you make an analogy between two things. Like you say,
time is like a river, if you want to get philosophical or something. But in general,
you can just say this apple looks similar to this other apple. So there is such a thing as
the concept of an apple, for instance. And the part that is shared between the two things that
you're relating to each other, the subject of the analogy that that's the part that can be said to
be abstract, that is the part that will help you make sense of the future, like you encounter a
third apple in the future, you know, it's an apple. Because you don't even need to relate this to
the apple should have memorized, you just need to, you just need to relate it to the template,
the abstract template of an apple that you've formed by from exposure to different kinds of
apples in the past. And if you think about what's what's an analogy, really, like how do you find
an analogy, it's a way to compare two things to each other. And there are only really two ways
to compare things. You can, you can basically ask how similar are they in terms of distance,
like you can say implicitly, there's you're looking at the space of points, there's a distance
between any two points. That's, that's the type one, a subject analogy that leads to type one
abstractions, which leads to a type one thinking, right? So a type one analogy is like your things,
you say to what degree they're similar to each other. So you read them by distance, you, so
implicitly, it means you put your things on in a geometric space, right? And type one abstraction
is going to be a template. It's like you're going to have clusters of things, you can take the average
and say everything that is within a certain distance of that template belongs to this category.
That's that's type one. It's very much the way deep learning models work. And then you and then
you start adding perception and intuition on top of that, which is very much the type one thing.
And the other way you can compare two things is the discrete way, right? You can say these two
things are exactly the same. They have exactly the same structure. Or maybe the structure of this
thing is a subset of the structure of this bigger thing. So this creates topology grounded
comparisons. So you have the geometry grounded comparison. It's all about distances and templates.
And then you have the topology grounded way of comparing things. That's all about exact comparison
or finding a sub graph isomorphism. So in the first case, your objects are very much
points in geometric spaces. So they are vectors. And deep learning is always a great fit for this
sort of stuff. And in the second case, your objects are going to be graphs, right? And you're
and you're really looking at the structure of these graphs and substructure and so on.
And you're doing always you're doing exact comparisons. And in practice, most thinking is
actually kind of some some combination of these two atoms, right? Of these two poles.
You're very rarely just going to say, yeah, this airport is exactly this close to my template
of an airport. So it's an airport. You're going to have basically layers upon layers of thinking.
And some of them are going to be intuitive. Some of them are going to be more about, you know,
comparing structures and so on. What you're saying is really interesting, right? Because you invoke
the kaleidoscope hypothesis in your paper. And the idea there is that a tiny bit of information,
just like in a kaleidoscope, could be represented widely across experience space. So you say that
intelligence is literally having some kind of sensitivity to abstract analogies.
So the intelligence is about being able to face the future unknown future, given your past experience.
And that's fundamentally requires the future to share some commonalities with the past. And
that's that's the idea of the kaleidoscope hypothesis that the universe and our lives
are made of lots of repeated atoms of structure. And in fact, if you look at the source,
there are very few things that are that are unique that are kind of like
the grains of sand that are at the origin of all the different kinds of moving patterns you
can see in the kaleidoscope, right? So the kind of like intrinsic structure contained in the universe
is very small, but it is repeated in all kinds of variants, right? And the idea is that if you see
two things in the universe that look similar to each other or that share some commonalities,
a subgraph, maybe, it fundamentally means that they come from the same thing. And that thing is
going to be is going to be an abstraction. We'll be one of these grains of sand in your
in your kaleidoscope or grains of glass, actually. And intelligence is all about reverse engineering
the universe to get back to this source of intrinsic complexity in the universe to get
back to these abstractions. I think the heart of this conversation goes back thousands of years
because what we're talking about right now is a lot of say, Platonism, right? Which is that there
are these ideal abstract structures. And of course, they they really thought of them as actually
existing in some universe. But you know, even if they don't exist in some reality, they at least
exist in concept. And it strikes at the heart of this duality that's always been a very
that's been one of the central mystery, really, of a lot of human thinking, which is
particle versus wave, you know, discrete versus continuous abstract versus the real versus the
messy. And you know, I think you pointed out, you definitely pointed this out in this call. But
I think also in some of your papers that in your view, you know, let's say the ultimate solution
or whatever of creating artificial intelligence or synthetic intelligence or whatever is a
hybrid system that can do both of these types of reasoning, maybe in kind of multiple layers.
And, you know, I'm kind of curious, where is the state of the art now with actually implementing
hybrid systems, you know, something like, I don't know, is it capsule networks? Is it the
topological neural networks that we talked about? Where where lies the direction of some type of a
hybrid system that in a unified way is capable of doing both of these modes of reasoning, if you
will? Yeah, that's a great question. So I think this is definitely an active field of research,
but I think the most promising direction right now is going to be discrete search very much. So
a system that is discrete search centric that has a DSA and so on. And that's one of the
it's basically just problems in this engine. But it is getting lots of help from deep learning
models. And there are two ways in which you can incorporate this type one sort of thinking into
a phenomenally type two centric system. So one way is so basically, you want to apply deep learning
to any sorts of data sets where you have an abundance of data, and your data is interpreted.
One example would be being able to easily play models to generate a sort of like perception
DSL that your discrete search process can build upon. So look at art, art tasks, for instance,
a human that is looking at art tasks, the very first layer through which they're approaching
the art task is by applying basically perception primitives to the grid they're looking at. They
are not actually analyzing the grid in a in a discrete way like cell by cell, object by object,
they're approaching it holistically, like what do they see? And these outputs can be discrete
concepts. And then you can start you can start applying the script reasoning to them. So generating
the DSL. And by the way, the reason it's possible is because humans have access to tons of visual
data and these different frames share lots of commonalities, right? So it is an interpolative
space where deep learning is relevant, where intuition and perception are relevant. And the
other way, which is is is much more difficult and much, much more subtle thing is basically being
able to provide guidance to the discrete search process, basically, because even though one single
program, so learning one single problem, for instance, for an art task is not a good fit
for deep learning model at all, because you only have a handful of examples to learn from.
And the program is super discrete. It's not really easily embeddable in this movement.
However, here's the thing, the space of all possible programs, for instance, the space of
all possible art tasks and all possible programs that solve art tasks is actually
very likely going to be interpolative, at least to some extent. And so you can imagine a deep
learning model that has enough experience with with these problems and the algorithmic solution
that it can it can start providing directions to the search to the discrete system. So
basically, you're in a kind of like you have, yeah, you have like layers of
of learning the lowest layer is going to be perceptive. It's going to be learned across many
different tasks and many different environments. It's going to be type type one, then you're going
to have the context specific on the fly problem solving system that's going to be type two.
And the reason is going to be possible and efficient is because it's going to be guided
by this upper layer, which is going to be type one, which is also going to be trained
from a very, very long experience across many different problems and tasks. And it is able
to do interpolation between different tasks. So can I challenge you a little bit maybe because
you say maybe, you know, all of these problems and what humans do is a bit of an interpolate
like an interpolation between the interpolative systems and the discrete systems. And I see that
going for, you know, something like an arc task or or if you really write code. But if you really
come to let's say, let's say the highest levels of human intelligence, which to me seems to be
navigating social situations, which is is is ultimately is super complex. And I can imagine
something like the graph structure you're referring to be that being, let's say I come into a room
and I see the graphs as, you know, what kind of social dynamics exist in this room, you know,
this is the father of this person, and that person's kind of angry at me. And so I need to,
you know, do something. And my question is, how often is that really a disk like how often can
you really map this in a discrete way to another graph? Isn't isn't every situation going to be
a little bit different, even in terms of its graph structure? And, you know, even if in an arc task,
a line is just like a little bit squiggled, any program synthesis approach would have a hard
time with it, I feel, or do you think, or do you think I'm misunderstanding something here? Like
how discrete is really discrete? That's the purpose of abstraction. The purpose of abstraction
is to erase the irrelevant differences between different instances of the thing and focus on
the commonalities that matter. So like if the squiggled in your line is not relevant,
then the proper abstraction for a line should abstract it away. I was going to pick up on that
because your main point basically is that program based abstraction is more powerful
than geometric based abstraction, because topology is robust to small perturbations,
but it's more than that. It comes back to these analogies, right? So we actually have functions
and abstractions in our mind that as you say, will take away all of the relevant differences,
but focus on what's salient and what's generalizable. Yeah, exactly. So in in the big sense, do you
think the type one and type two reasoning are really different or is there also a continuum
between them? Like you say we need we need hybrid systems, but is there something,
right? Because they're both they're both in the brain, they're both on the same neurons,
like is there a continuum? So right, so yes and no, I do believe they are they are very
qualitatively different. These are the two poles of cognition, but there are there are, you know,
most most things we do with our mind are a combination of both. That doesn't mean it's
it lies somewhere in between. It means it's a direct combination of one pole with the other,
kind of like what I described with with the arc solver with three layers, with two layers of
that type one and one layer in the middle of type two. But in very much the same way that you can
embed discrete programs in a smooth manifold, you can also do the reverse. And when you're
meaning you can basically encode an approximation of a geometric space using discrete constructs. In
fact, if you've done any sort of linear algebra on a computer, that's exactly what you're doing,
you're actually manipulating ones and zeros. But somehow somehow you're able to have vectors
of seemingly constant new numbers, you can compute a distance between two vectors and so
all of this is an approximation that's actually grounded in discrete programs. So you can you
can actually kind of merge the two together. It's not necessarily always a good idea. In
particular, I think it's often not a good idea to try to embed an overly complex or overly
discreet program in a constant new space. As I was mentioning earlier, the reverse is actually
usually way more tractable. And by the way, my I think this is something that came up before
in our conversation, but my kind of subjective totally not backed by any evidence opinion of
how the brain works is that fundamentally it's doing type one on type two, using a discrete
system, because it's actually much easier to do to type one via an approximation of a geometric
space that's encoded in a district structure than it is to do the reverse. Yeah, and if I can,
if I just for the benefit of the reader, the listeners, if I can give some other examples,
you know, for example, and mixed integer optimization, it's often the case that you
take that problem. And instead of having these discrete values, you project it into a continuous
space, do a continuous optimization. And then as you get sort of close to a good optimization,
you discretize it back over into the, the discrete variables, you know, to, to kind of,
you know, flesh out the most optimal path within that discrete space, or an example to is the
gamma function, you know, which is a continuous generalization of the factorial, right? And
it kind of provides some cool and interesting behavior in between those, those poles that
show up very clearly on the graph as these discrete points. And this is this bizarre
duality between the continuous and the discrete that we see like throughout the universe. And
it's kind of one of the strangest things we have to deal with. Yeah, exactly. I just wonder what
some of the transformers folks must be saying now, because Max Welling, we had him on and
folks have done topological applications using transformers or using graph neural
networks and the alpha fold, the thing from DeepMind, that was looking at graph isomorphisms,
right? It was looking at different types of equivariance in topological space. Is it a naive
thing to say that we could make it continuous or are we on a hiding to nothing? Right. So I guess,
I guess the question is, is there like one approach that's going to end up being universal? And it's,
it's like, can you actually scale deep learning to handle arbitrary district programs? It's kind
of, it's kind of the question. And the answer is no, actually, like, by, by construction, do,
due to the very nature of what deep learning is, it's like parametric continuous parametric models
in fact, smooths, because they're differentiable, sure. And it's quite undecent. That is never
actually going to be a good fit for most discrete programs. So, and, and the reverse is true as
well. I don't think, so you have basically two engines that you can use to learn problems. You
have quite undecent and you have discrete search. And I think the reverse is also true that discrete
search is not going to be this universal approach that's going to beat everything. I truly believe
that the AIs of the future will be truly hybrid in the sense that they will have these two engines
inside them, they will be able to do this, they will be able to do this quick search.
Right. And then, and then, and they will set, you, is that appropriate? You said, by the way,
in your measure of intelligence paper that there are three types of priors, right? Low level,
sensory motor priors and meta learning priors. That's the interesting one. I think that's got
intelligences and high level knowledge. And then we get over to the ARC challenge and, and as you
said in your presentation last year, the two winning folks on that Kaggle challenge, one was
doing a genetic algorithm over a DSL. So doing what you're talking about, a kind of program
search, and actually the winner who got about 20% accuracy. And that was, that was just, yeah, that
was just doing a brute force, you know, selecting combinations of, of operations on this DSL.
So this absolutely fascinates me. So at the moment, that seems like a horrific solution,
but clearly no one could do it using deep learning. So, but, but this is what you're
advocating for. So you're saying for these discrete problems, get, get a DSL. Now,
all the stuff you're talking about, presumably they haven't done yet, you're saying, well,
software engineering, the beauty of software engineering is being able to modularize things
into building blocks. And in fact, I love citing this thing actually from Patrice Simhard. But
he said, the reason why software engineering is so good is if I ask you, how long will it take
you to build the game of Tetris? You will say not long at all. And if you look at the number of
state spaces in Tetris, it's, it's huge. But the reason you'll be confident to build it in a couple
of weeks is because you know that you can modularize it into, into blocks, you can't say the same for
deep learning, right? But they don't appear to have done that on the arc challenge yet.
Yeah, so the, the solutions we've seen on the accident so far have been incredibly,
incredibly primitive. And so it's, it's actually quite interesting that you can get to 20%.
It's very primitive solutions. I think you can, even with today's technology, you can go much
further. Like the, what I was describing before about learning a DSL that is perceptive and then
guiding discrete program search. Yeah, intuition about program space. This is already something
that you can try today. So there's one approach that I was very excited about. And that I thought
was very cool. And I really like it's, it's called Dreamcoder by Dr. Kevin Ellis and, and folks.
So check it out if you, if you have incidents, it's very good. I think that they're trying to
play to arc now, but it's generally like, is this kind of like hybrid deep learning programs into
this engine? And I think that's really to me, that that is the sort of direction that is the most
promising to that. So you have a paper that's fairly long on, it's called on the measure of
intelligence. And you make the case that intelligence is something like the efficiency
with which we transform prior information and experience into task solutions, as, as you have
said before. And in that same paper, the arc challenge is presented. So, you know, a naive
reader like me assumes there is some connection between, you know, what you say about intelligence
and solving this arc challenge. So my question is, if tomorrow, you know, a new team comes and
gives you a solution, you evaluate it, it gets whatever 95% correct, it solves the arc challenge.
Is it immediately intelligent? Or what would you ask of that system for, for you to say,
yes, that's intelligent, or it's, it's intelligent is, is high or something like this.
So you, you, you would be able to make that, that conclusion, if and only if arc was a,
was a perfect benchmark, but it's not, it's actually very much flawed. So if you solve arc,
are you, are you intelligent? Well, no, because arc is potentially flawed. That's, that's the
thing. So the thing you need to really understand about arc is that it's not kind of the end state
of the intelligence benchmark. It is very much a work in progress. And there will be new iterations,
especially as we learn more about the flows. And by the way, so last year, we ran a Kaggle
challenge on arc, and we learned a ton, not necessarily a ton about program synthesis approaches
although there were some cool stuff we still learned about and so on. But mostly we learned about
the flows of arc. So there will be future additions and so on. So I will tell you this,
if you solve the specific test set of arc as it exists today, you're not necessarily intelligent
because it is not perfect because it has its laws. But if more generally speaking, you give me a system
that is such that any new arc task I throw at it, like I can, I can make some new ones tomorrow,
for instance, I give them to your system. If it's always solving them, I will say,
yeah, it's looking like you've got a system that's, that's got, you know, pretty close to
human level fluid intelligence. This is one of the things that, look, and I like the paper a lot,
I think, I think it serves as a really good, you know, foundation for us to think differently
about how to build intelligence. But, but I have some, some issues with it too as well. And one
of them is this sort of necessity that it requires kind of white box analysis of things in order to
figure out whether or not they're intelligent. Because for example, suppose time travel is
actually possible. And you know, somebody like 100 years from now looks back on your arc thing and
writes an algorithm that, that solves all, all them in there because it actually knows about them
already and then ships it back into the past and we enter it into the competition. And no matter
what new arc thing you throw at it, it sort of does well. And you say, well, yeah, you know,
this thing's like kind of intelligent, but, but we'd be wrong because in the sense in the paper,
it's actually just encoded, you know, prior knowledge from the future. So we have to,
we always have to kind of be able to look into the box, right, in order to evaluate
intelligence in the way that you define in the paper. And so my question is one,
isn't that a bit of a undesirable feature? And two, do you have any hopes for a more black box
measure of intelligence? So basically, the fundamental issue is that if intelligence
is this conversion ratio, then computing it requires knowing where you start from. And
you don't really have a way around it. So the thing to keep in mind is that the
under measure of intelligence stuff is not so much meant to provide like a sort of like
golden measure of tape to measure anyone's intelligence or anything's intelligence.
It is more meant as a sort of cognitive device to help you think about what the actual challenges are
to help you kind of kind of reframe AI because they think they have been pretty deep and
longstanding conceptual misunderstandings. So that is really being, that's being holding the
feedback. So it's very much meant as a cognitive device. If you take a step back and you ask,
why are we even trying to define intelligence and measure intelligence in the first place,
why is it useful at all? I think it's useful to the extent that it is actionable, right,
a good definition and a good measure should be actionable. So meaning it should help you
think, it should help you find solutions and it should help you make progress. In particular,
a good definition is a definition that will highlight the key challenges and help you think
about it. And I think that's what the paper does. And a good measure is a measure that gives you an
actionable feedback signal towards building the right kind of system, right in the sense that
it will be capable of doing more. And so that's part of the feedback signal is what ARC is trying
to achieve. And the way it's trying to control for priors and experience is by assuming a fixed
set of priors. And you're going to see, you know, every test taker can have such priors.
This is the core knowledge priors. And then it controls for experience by only giving you a very
small number of input examples. And also by making sure the tasks are sufficiently novel and
surprising that you're unlikely to have seen a very similar instance before. So now, of course,
it's super flawed. So this is not 100% true, of course, but this is kind of like the the
planning ideal that we're trying to get to. So that for the record, that's a fascinating point to
me is that you view this more as a cognitive device to help guide us to produce better,
better intelligent agents. It is not an input. It's not like ARC is like the measure of intelligence
and all we need to do is solve ARC. This is not at all the point. It's like it's one.
Oh, darn, because I was doing pretty well on some of the examples. I was hoping that would
mean I was intelligent. But another interesting point, because Keith and I were looking at the
paper again yesterday, because it's been, I haven't properly studied it since last year. But
we were starting to talk about an alien that comes in from outer space. And, you know,
we don't know the priors and the experience. And then I was thinking in a way, it might be a
kind of lower bound on intelligence, right? Because, you know, if I play chess, and if I beat
someone with a higher elo than me, then only really tells me that I'm better, you know, as
good as that person that I just beat. And similarly, this measure of intelligence, it only gives you
a reading in the situation when you know what the conversion was. So if they are not converting
anything, then you don't know. And another interesting byproduct of this is the more
experienced you get, the less intelligent you get. So I would push back against that last claim
that the measure of intelligence as I define it is dependent on how much experience you have.
Because the amount of initial experience you have does not actually change at the conversion
ratio if you measure it via the right task. So you might need, so if you have a fixed set of tasks,
then yes, it does affect it. But if you're able to renew your set of tasks and come up with
styles that are orthogonal to the experience that you have, then it's not the actual effect,
the definition. So, but yeah, you're definitely right that if you take a pure black box approach,
and all you're looking at, the only thing you can really measure is the behavior of a system.
And unless you know how that behavior is achieved, you can't really tell immediately
how much intelligence was involved in producing this behavior. If you look at an insect,
they're capable of super complex behavior. Are they crazy intelligent? Well, actually,
you know, probably not. And the way you can really tell is by putting these systems out of
their comfort zone, getting them to face novel situations and see how they adapt. And that's
the measure of intelligence. It's adaptability, the ability to deal with novel and unknown
situations. But in order to give your system a novel and unknown situation, you need to have
this white box understanding of what it already knows about. And that's not really something
you can work on. So can I ask about the generalization difficulty? Because I sort of had
some difficulty intuitively with some of, let's say, it's limiting cases. So for example,
you know, the algorithmic complexity is highest. Let's just suppose we're dealing with problems
tasks where we have whatever sets of integers mapped to zero, one values, you know, the algorithmic
complexity will be greatest when that's just a random mapping, like I just assigned zero and one
randomly to every single integer. And if I go to look at that generalization difficulty,
it's going to be super high, because the length of the program for any set is basically going to
be, you'd have to encode the entire set as a hash table, right? So how does like this measure
account for or help us avoid problems where we're confusing generalization difficulty with just
increasing random, you know, randomness? Well, I mean, increasing randomness is a part of
the realization difficulty, right? Generalization is really the ability to deal with the stuff you
don't know about the stuff you don't expect, the stuff you haven't seen before. And randomness is
a part of it. But you're right that if you just add randomness to a system, you're increasing
the generalization difficulty, but you're not increasing it in a very interesting way, right?
Because you're increasing it in a way that's kind of orthogonal to an integration system's ability
to deal with it, right? The best you can do is modify the system to be more robust to very much
randomness. But that's not super interesting. What's really interesting is to test the system's
sensitivity to subtle analogies, is to make the system face novel and unexpected situations that
are actually derived from the past, but in interesting ways, right? Not just random ways.
You've run this Kaggle challenge on ARC. And, you know, we know from systems such as Alpha Go and
so on that bootstrapping intelligent, like bootstrapping AI systems can be very valuable,
like playing them against each other and so on. And also, we know that something like markets can
be very efficient and valuable. And I imagine a system where you'd have agents creating ARC tasks
and other agents solving ARC tasks, and they're going some kind of money around and so on. And
this could be kind of a powerful engine for research teams to research anything like this.
And, you know, given that you have, I don't know how much, but you do have the backing of Google
with a bit of capital in hand. Could you imagine there being a push for this kind of thing? Or is
it, as of now, an intellectual curiosity? Yeah, so I don't have that much backing you from Google
around this kind of project. But, yeah, so it would be super interesting to have this kind of
two-part system where one part is generating the task and one part is learning to solve them.
And you could get them to do some kind of curriculum optimization, like the task generator network
would not just be trying to generate tasks that look like ARC tasks. It would be trying to
generate tasks that correspond to level of generalization, difficulty and complexity that is
right below the limits of the student system that's trying to solve them. Kind of like, you know,
the way a teacher would provide exercises that are solvable, but challenging. They shouldn't be.
They shouldn't be easy. They shouldn't be impossible. They should be solvable. Because
that's how you get the most growth. So it's actually a system that's described at the very end
of the paper on the measure of contagions. And I think one thing I point out in the paper is
kind of like the pitfall you should avoid falling into is that this system is circular,
right? And the complexity you're going to see in your task, it needs to come from somewhere, right?
It's like conservation of complexity. So the system, this two-part system needs to have
a source of intrinsic complexity. It needs to be grounded in the real world.
And one way we can achieve that grounding, and I've been thinking about it, is I think we should,
you know, like ARC tasks, as they are today, they're made by me and this is not a good setup
because it's going to be biased. It's going to be very bottlenecked as well. I think we should
start crowdsourcing our task. There should definitely be, you know, a filtering system so
that we make sure that we're only keeping our tasks that are interesting, that are not too easy,
that are not difficult, and that are only grounded in core knowledge priors. But if we have, like,
this stream of novel ARC tasks that contain intrinsic complexity and novel information,
because they come from the real world, they come from human brains, that have experienced the
real world, and you use that as a way to ground your task generator, then you're starting to get
a very interesting three-part system, right? So I would love to actually get that started,
to actually produce a V2 of ARC as soon as possible, let's include, you know, 10x more tasks
that will be crowdsourced, and maybe something that will take the form of a continuous challenge
where you have an API where you can draw a new ARC task, and every time you draw a task, it's
actually a different one because you have so many of them. Gamify it, that'll make a fun game
on a mobile app. There are actually a few people who have created, because ARC is open source,
and they're totally free licensed, there are a few people who have created mobile apps where
users sort of ARC tasks, and apparently it's popular. So there's also the other angle you
mentioned in the paper, which was, which is pretty fascinating, you're talking about it almost right
now, which is that, okay, let's start thinking about how to map ARC performance to psychometric,
you know, classic kind of psychometric tests. Are there any efforts that you're aware of
underway right now to do that? Are you involved in any ETAs? Yeah, ETAs, I'm not sure. So we did a
workshop at AAAI the other day, and there were two presentations about efforts that teams of people,
so there are people who do neuropsychology, and they're using ARC in very interesting ways. So
there's a group at NYU, and there's a group at MIT, and yeah, so they're using ARC for neuropsychology
experiments, and it's it's super cool. Amazing. I want to switch over a little bit, because of
course, you know, other than the measurement of intelligence, you are also famous for a small
library you wrote once in a while called Keras. And I wish I wrote it, and then that was that.
No, I yeah, it's been very much an ongoing project for the past six years.
It was because I remember, you know, the days of TensorFlow one and and Theano,
and things like this. And Keras was just, I think, so helpful to a lot of people, because it just
simplified all of this, you know, graph construction, whatnot, and so on. It just made it accessible to
so many people. And now with the development of, you know, things like PyTorch and TensorFlow two,
it almost seems like Keras is it has been kind of absorbed by TensorFlow two, right, there is TF.Keras.
And now I think the newest APIs are even sort of vanishing that a little bit. Do you do you see
Keras going away? Do you see it changing? Where do you see it? Where do you see Keras going?
Yeah, so going away, definitely not. I mean, we have we have more users than ever before. And we
are still growing very nicely, both inside Google, like one more teams that Google are moving away
from TensorFlow one and adopting Keras and outside Google as well. It's a big market out there,
and there's definitely room for multiple frameworks. Evolving absolutely, I mean, Keras is constantly
evolving, but evolving with continuity. Like if you look at Keras from 2016 or 2015, you look at
Keras now, you recognize, is it the same thing? And it's the same API. And yet it's actually a very
different and much, much bigger set of features and things you can do it. So evolving, definitely.
And there are so several, so you, I think you asked, you know, about, yeah, like,
Keras is getting kind of merged into TensorFlow, does it mean it's like failing away?
So definitely not. So merging with TensorFlow was a good idea because it starts enabling
a spectrum of workflows from the very high level, like scikit-learn like, to the very low level,
numpy like, and everything in between. In the early days, because Keras had to interact with
multiple backends via backend interface, it means you had this kind of like a barrier where as long
as you use the Keras APIs, everything was super simple. It was scikit-learn like, so very easy,
very proactive, very fast. But if you wanted more customization, at some point, you would hit
that backend barrier. And you had to reverse to TensorFlow base or piano base workflow,
that was low level, but when, where you couldn't really leverage Keras effectively,
by removing the backend thing and just saying the flow together in one spectrum,
then you get really this progressive disclosure of complexity when you can start out with the
very high level thing, but then you need to customize your training step. You have an API for
that. And you can just mix and match seamlessly the low level TensorFlow stuff with the high
level Keras step. And that way you can achieve any, can work with Keras and TensorFlow at the
level of abstraction that you want. Very, very easy high level or very, very low level full
flexibility. It's up to you. I'm going to point out the temptation here to analogize connecting
type one with type two reason. Yeah, why not? I was just about to do that. At least Francois
has great form for this, because not only does he talk about having powerful and useful interfaces
and abstractions in deep learning, he's been playing this game in the library world for quite
some time. But I wanted to touch on this quickly. We had a couple of people in our community asking
you about Keras, actually. And Robert Lange and Ivan Finnell said that apparently Theano has returned
with Jax and XLA underneath and he wants to know are there any plans to add it as a Keras back end
and Robert Lange also says, you know, just Jax on its own. Would you add that as a back end?
We've also had a couple of questions about PyTorch as well. Is there anything on the
roadmap for that? Okay, so let's talk about Jax. I think Jax is an awesome project and the
developers have really done a very, very interesting and very good job with it. And lots of people,
I like Jax actually. So that said, adoption is not super high. I think Google is probably the
company where it's the most adopted, where you will find the most users. And even then,
it's like a tiny, tiny, tiny fraction of total machine usage at Google. But I think as a project,
it's a beautiful project. It's elegant. It's powerful. It's great. So would I like to add
Jax back end to Keras or PyTorch back end to Keras? So I want to say we've really moved away
from this like interface back end kind of model. So precisely for the reason I was describing,
because you want to achieve this spectrum of workflows, with that, I think this cliff where
you go, you fall from the high level down to the low level. We don't want the cliffs. We don't
because cliffs create silos of users where you have the high level users. You want a gradient.
Yeah, you want the gradient. Exactly. So that said, I think it would be super cool to have a
sort of like re-implementation of the Keras API on top of Jax that will also achieve this screening
and that will still follow the Keras API spec. It would still be the same thing,
but on top of Jax. That said, so I would love to see something like this. This is also a very
low priority for us because we have the actual current Keras, which I wish we need to work on,
which has lots of users. So we don't really have time to do this. But in theory, would it be cool?
Yeah, sure. I would love to see something like this. So if I had tons of free time, I would
probably build it, but in practice, I don't. Fantastic. Well, we've got another question
from Giovanni actually. He says, what does Francois think of Dr. Kenneth Stanley's book on the myth
of the objective? Are you familiar with Kenneth Stanley's work about the tyranny of objectives
and open-endedness? So I'm vaguely familiar with the name. I'm not really familiar with the book.
Oh, okay. Well, sorry, not to worry, but it's Kenneth has been a huge inspiration for me.
And he talks a lot about objectives leading to deception. So sometimes following an objective
monotonically sends you in the wrong direction. And his solution to that is either quality,
diversity, or more recently, open-endedness, which is that if you have an infinitude of
objectives, in a sense, the system has no objective. And you can also with diversity,
preservation, you can overcome deceptive search spaces. But yeah, you might have heard of the
poet algorithm, which he was involved in. Yeah, absolutely. No, I'm aware. And so when it comes
to your description of the problem's objectives, I completely agree that one thing I mentioned
in the paper, it's like the shortcut rule, which is that if you try to achieve one thing, one
objective, you're going to achieve it. But the thing is, you're going to take every shortcut
along the way for things that we are not actually incorporated in your objective.
And this leads to systems that are not actually doing what you wanted them to do. Like for instance,
we built chess playing systems, because we hoped that a system that could play chess would have to
be able to feature reasoning, book learning, creativity, and so on. Turns out it just plays
chess. That's what it does. The same is true with challenges and Kaggle. The winning systems,
they just optimize for the leaderboard ranking and they achieve it. But they achieve it at the
expense of everything else that you might care about the system. Like, is the code base readable?
No. Is it computationally efficient? No, it's actually terrible. You could never put it in
production. Is it explainable? No, and so on. Yeah, so it's like, if you if you optimize for
something, you get it, but you take shortcuts. Yeah, exactly. And that's very much what Kenneth
says as well. I love what you said about shortcuts. You said in your New York's presentation that if
you optimize for a specific metric, then you'll take shortcuts on every other dimension, not
captured by your metric. And you said in a machine learning context, it's similar to overfitting,
right? Because on task specific skills, you actually lose generalization if you get good at
a particular task. So it's completely orthogonal to what you want. I know you're very well known
for your skepticism of the intelligence explosion. And what I love about your conception of
intelligence is that you think of it as a system or as a process, you say that intelligence is
embodied, right? So you have a brain in a body acting in an environment. And in that context,
it makes sense that you would think that there are environmental kind of rate limiting steps to
any kind of super intelligence, right? But I spoke to someone the other day who is of the other
persuasion, shall we say, and this person was saying, Well, what if you had a super, super
smart bunch of scientists? I know you said in your rebuttal that if you look at the IQ of a
scientist who is Richard Feynman, for example, the same IQ as a mediocre scientist, turns out
that IQ only helps up to about 125. And then it stops helping you. But these people would say,
Oh, well, you know, what if what if every single scientist was an Einstein and intelligence is
just making better decisions, they would consistently make better decisions and science
would accelerate. A chimp doesn't understand how good a human is. So how would we understand what a
super intelligent person would do? You know, they'd invent nanotech, they'd upload themselves into
the matrix, they'd do all of this stuff, and somehow they would miraculously overcome. Do you
know what I mean? How would you respond to that? Yeah, if every scientist was super intelligent
in human terms, that would in fact accelerate science. But it would not really like accelerate
science in a linear fashion and very much not in an exponential fashion. So I guess the main
conceptual differences I have with these folks is that they tend to credit everything humans can do
to the human brain. And they have this vision of intelligence as you know, a brain in a jar
kind of thing. And if you tweak the brain, it gets more intelligent and intelligence
is directly expressed as power. If you're more intelligent, if you have a hierarchy, you can
do more things, you can solve more problems and so on. And in particular, you can build a better
brain. And by the way, there is not really any practical evidence that this is true. But
I view intelligence here more as this holistic thing that okay, you have the brain, but actually
the brain is in a body which gives it access to a certain set of actions it can do and set
up a perception primitives. And this body is an environment which gives it access to a set of
experiences, a set of problems it can solve. And to a very large extent, you know, the brain is just,
it's not so much a problem solving algorithm, like a problem center descending, as it is a
big sport. And you put it in an environment to absorb experiences from that environment. And
one thing that's super important to understand if you're on issue, if you really think deeply about
intelligence, is that most of our expressed intelligence does not come from here, it is
externalized intelligence. So externalized intelligence can be can be many things.
If I look up something online, that's externalized intelligence, Google is part of my brain. If I
write a Python script to test some idea, that's externalized intelligence, my laptop is part of
my cognition, and so on. But it's actually, it goes much further than that. Most of our cognition
is crystallized, the crystallized output of someone, someone else's thinking. And the process
through which we get access to all these accumulated outputs of people's thinking is civilization,
right? And like 99% of the things you think are the behaviors you act, the behaviors you execute,
you did not invent them. You did not solve the underlying problem yourself. You're just copying
a solution. You've seen like, we're in the middle of a pandemic, you're probably washing your hands
after you went outside. And that's a very smart behavior. But did you invent it? Did you come
up with that? No, actually, other people came up with that. You did not also come up with the
infrastructure that enables you to do it in the first place. And so, and this is true, you know,
for even the most intimate of your thoughts, you're thinking with words that you did not invent,
you're thinking with concepts that you did not invent or that you did not derive from your own
experience. They really come from other people, from this accumulation of past generations.
And if you want to enhance the expressed intelligence of people, then this is actually the
system you need to tweak and improve, not the human brain, but civilization, right?
In a way, that seems like a contradiction, because you're talking about the externalization of knowledge,
not intelligence. So by your own definition, isn't that the opposite of intelligence?
That's a great point. So I'm relating expressed intelligence. So I was specifically saying
expressed intelligence as opposed to fluid intelligence. And what expressed intelligence
means in this context is something very different from what we talk about in the measure of
intelligence. It means intelligence behavior. And in particular, I think the ability to solve
problems that you encounter as an individual. Typically, when you solve a problem as an
individual, you're actually using a solution you found somewhere else. There are not that many
problems that as an individual, you solve from scratch in your own lifetime. But here's the
thing is that if you're able to actually solve something novel yourself, you have the ability
to write about it, you have the ability to communicate it, and then the next generation can
benefit from it. So let me just pose a kind of a counter argument to this. So suppose you're
reading a novel about, I don't know, a kind of planet of the apes or something, which was a
planet that had a life form similar to ours, but with a significantly lower IQ. And a human being
shows up there one day, and these things start writing about this, hey, this weird alien just
showed up here, and we captured it, we ran some tests on it, and we figured out it's really
intelligent. It's much more intelligent than any of us are. And we're worried what's going to happen
when 100 of them show up instead of just this initial explorer. And some other of these guys
were like, ah, don't worry about it. They've got two legs and two arms like us, and most of what
they are is kind of outside of their brain. So I'm not really worried about it. We would be
reading that with trepidation, right, because we know that when this more intelligent species
with more fluid intelligence, more externalized intelligence, better technology, all this kind
of stuff shows up, those guys are going to get wiped out. And it's actually happened like many
times throughout human history, not that humans were more fluid intelligence showed up and killed
off, you know, other people, but humans that had more externalized intelligence or more, you know,
represented intelligence and technology certainly showed up and dominated.
Absolutely. You're saying it yourself that when it has happened in history, it was not
fundamentally about one people having smarter brains, but one people having higher technology.
But that that is not something that is attributable to intelligence itself, right?
There's a connection there. If you did have a group of species or whatever,
that was much more intelligent, they will have advanced technologically much faster and further
in any given amount of time, all else being equal, right?
It depends on many factors. And that's kind of my point is that is your brain a factor? Yes,
absolutely, it is. But there are other factors like we are just talking about the development
of technology. So in that case, the critical factor was not the brain, but the superstructure
in particular communication and environmental constraints around it. The direction in which
civilization develops is a direct function of the specific challenges it encounters that
come from its environment, that comes from its surrounding enemies, and so on. And
technological development advances the fastest when you have a civilization that are dealing with
very harsh challenges, but that are not quite fortunate to work them out.
Because that's what forces them to develop as fast as it can survive. So this is actually a
very good example where the critical factor was the superstructure that guided the development
civilization was not actually the brain. But of course, yeah, if a one is smaller,
then civilization will advance faster. But my point is that there are many factors and that
by tweaking one factor, the brain, if the brain stops being the bottleneck, then immediately
some other factor will be the bottleneck. There are civilizations that have not actually advanced
very much at all because they simply did not face any changes. And did they have worse brains? No,
actually, they had exactly the same brain. But somehow the outcome was different because
something else, then the brain turned out to be the bottleneck like lack of environmental change.
I'm fascinated by scale and bottlenecks in systems. Actually, I work in a large corporation and
when you have role fragmentation and lots of different businesses and lots of different
organization or structures, some people might decide to structure themselves based on data
domain or based on organization or based on something else. And you can think of it topologically.
And I think human society is very similar to this. And I'm not sure whether evolution
would lead itself to one particular topology. But the environmental structures and the ways
that we organize ourselves can create incredible bottlenecks. And that seems to be where the real
interesting stuff goes on rather than the individuals. And I think you would agree with that,
Francois. Yeah, absolutely. If you take two companies, and in one company, the average IQ
is like 15 points higher, but it has a terrible organizational structure and terrible incentives
and the promo process is super broken or something. And that company is actually going to perform worse
than the more progressive innovation encouraging company that has a very nice organizational
structure and where people are actually more mediocre. Maybe they have on average 15 points
less in IQ, but they're actually going to do a better job because they have the better superstructure.
Yeah, it's fascinating that the problem is in most corporations, you can't actually design the
information architecture to be more efficient, because everything is so decentralized and
fractionated, you can only do it in pockets. And if you try and fix something in one part
of the organization, everyone else will say, well, my requirements are different. I'm not going to
wait for you. I'm going to do it my own way. And it's actually a really, really difficult thing
to do well. To sum up the whole like intelligence explosion thing, the point is really that it's
a system you have to look at holistically to get it holistically. And just by tweaking one factor,
which is the intelligence of an individual human brain, then what this means is this factor starts
being the bottleneck. But that means some other factor in the system, because there's an infinite
factor that will become the bottleneck. And by just focusing on one factor, you're not going to
actually lift all the votes. Yeah, and I actually agree with you. However,
I do want to say, I think we just don't know. I think both sides of the intelligence,
quote unquote, explosion really can't say for certain that it will or will not pose a mortal
threat to humanity. I think we have to accept that it's at least a risk factor. And we have
to be very careful about, in the future, when we start embodying, if we find general intelligence,
we need to be cautious. If we come up with something that looks like general intelligence,
there is absolutely some risk potential around it. However, I've never seen anything coming
anywhere close to that. In fact, the systems that we have today, they fit your almost no
intelligence whatsoever. So I think it's a bit early to start banning them.
And even if we get into that conversation, I think Francois would say that intelligence
must be specialized, right, because of the no free lunch theorems.
If you define intelligence as your ability to solve problems, then yeah, it's going to be
specific to a scope of problems, a kind of problems. And like, yeah, what the no free lunch
theorem is saying is basically, if you want to learn something from data, you have to make assumptions
about it. Which is why you know a convent, for instance, is a great fit for image data. It's
not really a great fit for natural language processing. And because it makes different
assumptions about destruction. It doesn't give me a lot of comfort, though, because I'm fairly
certain that whatever the first AGI that gets created, it's going to be highly specialized
for killing other people, because it's going to be a military, you know, secret project,
probably that finds it. You know, it's, I don't know. But what I know is that right now, we don't
have anything coming close to AGI. It's probably going to be actually a system that just displays
you ads. Like if, like, if, you know, if you, if we see where the most money is right now, the
first AGI is probably just going to like write, not only display, but write the perfect ad for
you on the fly. You know, it knows what you ate and you know, I know you're joking with
actually think on the, on the more serious, I think that's highly unlikely because of the
short code of the story because of the short patrol. I don't think a general intelligence is
going to be created by the military is not going to be created by a system that's trying to show
you ads because these are specific goals. And so if you try to optimize those specific goals,
you're going to end up with a very specialized system in order to build a general intelligence,
you need to be optimizing for generality itself. So it's going to come from, if it comes from the
applied, either it's going to come from the academic side, where you have researchers who are
actually optimizing for generality itself, who said generality as they are going. Or if it's
come from the applied side, it's going to come from people who have problems where they have to
deal with extreme novelty, uncertainty, and unpredictability. So it's not going to be ads,
it's not going to be the military. I don't know where this is going to be.
One of the things that interested me about Kenneth Stanley was that he says the reason we
can't monotonically optimize on objectives is because of deception, which means sometimes you
need to get a lot worse before you get better. His original conception was quality diversity,
which basically means if you optimize for novelty, that's something that you can optimize on
monotonically. And also, if you look at evolution, where there is a cacophony of problems and
solutions divergently being generated, then as an information accumulator, you can optimize
on that monotonically. And your conception of intelligence is generality. And that also appears
to be a monotonic increase throughout advancing levels of intelligence. So I think that's quite
interesting. Anyway, Francois Chollet, this has been my dream come true to have you on the show.
Thank you so much. It really means a lot to us. And yeah, I appreciate it. Thank you.
Thanks for having me on the podcast. It's really my pleasure. This was super fun.
Thanks. And thank you for Keras, by the way. Thanks. I'm glad it's useful.
We're going to jump straight into the post-show analysis.
Okay, well, I'm going to mention you did really well, Tim, that trickle sweat
that this was running down your face the whole time. Not very noticeable. So I think you can
relax. That was fun. I think it went pretty well. Yeah, it was a dream come true.
I was actually I was very pleasantly kind of interested in how he he framed, you know,
the measure of intelligence paper like, look, it's not really about the measure per se. It's just
that this is this is a cognitive framework, a cognitive tool for thinking about where to go
and a guidepost for building more generalizable or more general intelligences say like that,
I totally, totally agree to. And it's quite, you know, quite a fascinating goal, which is like,
here's a framework to help us think more in the direction we need to be thinking.
Yeah. And it's so surprising that like the arc challenge is at like 20% solved only because
you know, he self admits that it's flawed, right? Because he like, he makes the tasks.
And, you know, there's only finitely many and and you know, you kind of you see the kind of tasks
he makes, you know, in the public set, you would think that not someone will come up with an
intelligent thing, but someone will come up with like a smart set of shortcuts to like solve that
sucker, right? But it's still at 20%. I don't know whether that's due to just, you know, not too many
people investigating it. Or whether it's really actually a hard problem. And if it is a problem,
you know, well, it's fascinating too, because if he if he achieves what he wanted, which was
getting it more outsourced, right, like getting all the intelligent people all around the world
contributing to arc problems and refining them over time, I think actually that community project
would help the core knowledge people in that line of research and figuring out, okay, what,
what is a catalog of all the core knowledge, right? It's, again, back in school, we used to call these
prime thoughts, because we would, we would play these brain teasers all the time. And we realized
that there were patterns, right? Like, well, this brain teaser requires the concept of coloring,
like with a red black tree, where you add an additional variable that kind of lets you
solve the problem. And if we could really have a nice catalog of, here's all the core knowledge,
here's all the like problem solving techniques, I think that would be really powerful. I mean,
well, we kind of have that. So this woman, Elizabeth Spellke, she came up with about
six core knowledge systems, right? And that and the arc challenge uses four of them. So
objectness and intuitive physics, one, agentness to elementary geometry, anthropology, three,
numbers, counting, quantitative comparisons. So the two that weren't in there are places
and social partners. Now, the thing is, I think we may discover new ones.
Well, we may be real, but I'm surprised that we did as well as 20%. Because if you think about it,
imagine if you just guessed the classification on ImageNet when you've got 1000 classes,
20% would be amazing, wouldn't it? And we've got a similar amount of diversity of tasks on arc,
right? And what's interesting as well is that all of those different tasks that have been created
by Francois, they all tie back to just four priors, right? Which means, I don't know whether
it's uniformly distributed. But 20% seems really good for just guessing ops on a DSM.
Yeah, there's, there's two things. So first, I would have thought that if someone,
if someone came up with something that solves more than 5%, it's going to be like immediately at
95%. Like just because they've sort of cracked the problem. And then, you know, there might be
a few outliers. But you know, if I would guess that's kind of a task that if you hit the correct
solution, it's going to be like, boom, you're, you're there. And that's not, which is surprising.
And the other thing is, I, I don't, I don't feel it's surprising that there's so few priors. What I
do think is that the space of these priors is still way too large. Like, so if you just think
about something like object, because in, in these arc tasks, there are, I feel so many more priors
than just the core knowledge things. Because so one of them is like, you have the, you have like
this thing, and then you have this thing. And the solution is like, it goes, right? It
could go, it like bounces. But this is election. Yeah. But, but like the fact that we recognize
like this is a wall or something, but there is no, there's no, no prior to says like a wall
needs to be straight, the wall could be like any, you know, any old, any shape at all. And the fact
that this is much more core knowledge, right? Like in, you know, we build stuff out of straight
walls. And I think, I think I agree with you, which is I think, I think what you're getting at,
correct me if I'm wrong, but it's that the way in which the core knowledge is kind of specified
right now is vague, right? There's a vagueness to it. And I think if we actually start to try and
codify that more and some type of a mathematical language, Tim, I think it's going to expand
like the scope of that, we're going to end up with more core knowledge concepts really than,
than just six, we'll need to make them finer grained. And I'm really excited,
you know, to see that develop because this has been for me a long wonder, right, which is
what are the in, in a rigorously defined way? What are these core concepts, these core bits
of knowledge that make human cognition so powerful? Yeah. And there's also,
because Yannick made the point about brittleness, right, even in topological space, you still have
brittleness, but, but the solution was to create powerful abstractions, right? But how would that
work with the priors? Because if you think about it, you can recombine many of the priors to come
up with powerful abstractions. And you might find that it doesn't actually filter down to, to that
many. But the question is, how many things are there? Remember when we spoke to Walid Saber,
and he was talking about, he's got them somewhere in a PowerPoint deck, you just wouldn't give them
to us. But you know, part of, part of why, why I agree with Yannick that they're finer grained
concepts are more important. I think probably stems from a lot of the computer science
education that I had where, where when we were devising algorithms to do one thing or another,
you get these little hints that kind of like clever bits of core knowledge that was used to
solve this problem. Like when you study quicksort, and it's like, you know what, like, I'm just going
to randomly choose an element. Well, random selection is kind of a bit of core knowledge.
And then I'm just going to partition by that, and then repeat, you know, or things like,
I don't know how to balance this tree the way it is. But if I color stuff, like add in red,
black nodes, I can now overlay a computation that, you know, so there's all these little bits,
you know, that's what's fascinating about computer programming is it, is it really strikes at the
heart of this cognition and this core knowledge and how to read, and you have to do it rigorously,
right? You can't just vaguely go, Oh, you know, just kind of sort it and merge them. You got to
define like what that means. And it's fascinating dynamic programming. I'm always, I'm always a
bit amazed by people who have just kind of sort of learned programming, because it's, it's almost
like a different world in that they'll, they'll, they'll do, it's like, Oh, okay, I need to solve
this problem. Can I can I copy paste this code here? And it works like 20% of the time, but not
fully. Yeah. But then on the other side of the coin to that. So when I was working in,
you know, quantitative trading, right, we had these these massive globally integrated,
automated trading systems. And I mean, some of the bizarre, I don't want to call them hacks,
but some of the bizarre sort of piecewise linear equations slash hacks, whatever that actually
work in reality. You know, you sit there and you look at them and go, when I first went in there,
as fresh out of academia, and I started seeing things like, Oh, this is crap, like, I'm going to
figure out some continuous equation that, you know, fits this piecewise linear thing, and it's
going to do better. Nope, like it didn't do better. I couldn't find any continuous thing to do better.
It's like, you know, options pay off, right, is this this piecewise linear thing. And, and you're
like, Oh, that's, well, there should be some continuous like thing in there. Like all these
weird, you know, piecewise discrete, like kind of hybrid things between continuous and discrete
work. And, and that's weird. It was weird to me and still weird to me.
Interesting. But I've got to say, so my main three take homes from Sholay today. I really love Sholay.
So one, intelligence is generalization. I think that's super powerful to his idea that deep learning
is really good for value centric abstraction. And because of the manifold hypothesis, lots of
natural data has some kind of manifold, which you can interpolate on, but lots of discrete
problems do not have that. Right. And my mind was thinking, Well, does that mean that we can just
use, because it's because of SGD, you can't even learn the manifold, even if it did exist. But
he's saying that it doesn't exist for discrete problems. The manifold might be there or it might
only be there in parts. So that was interesting. And then the third thing that fascinated me about
Sholay is he talks about these systems and bottlenecks in systems. And we shouldn't be
thinking about individual brains, we should be thinking about the externalization of knowledge.
Yeah. And the way he described this, what he thinks like a hybrid system should look like,
which is sort of you have a perception layer and then a discrete search layer. And then on top of
that kind of another fuzzy layer that guides the search that can be deep learning again. And I
think we're like halfway there on the top with the top very much looks like alpha zero, right,
which is kind of a discrete search that is guided by a neural networks. And the bottom
layer we have to because that's just our, you know, regular neural networks. I think we have
big trouble in how to connect the two in a in a single unified way such that we can learn them,
right? Because the best we can do right now is is right, we can, we can plug a pre train network
onto alpha zero or something like this, but we can't really, we don't really have it figured
out yet how to connect the all the stuff. A good example of that is the neural Turing machines,
like how it's so hard to to optimize them, right? And I think not only do we need these kind of
three components that that nicely integrate and are optimal, we have to be able to modularize
and componentize and connect multiple instances of those things together. And some, you know,
weird topological network to really achieve like kind of the capsule network kind of vision
where each of the capsules is maybe one of these units. And then they're part of it's like a fractal,
you know, kind of these fractal layers of those pieces. I don't know whether I was
misunderstanding you before, Janne, but with the alpha zero thing, my conception is that has
been quite hard coded. So you're, you're searching through, let's say, a bunch of deep learning
models and the way you search is quite opinionated. What you're always talking about is have a very
basic DSL and in that topological space, you just search and you start to modularize and you start
to create functions and abstractions. And you have from a software engineering point of view,
you start to build a library of functions that have been written in code that do certain things,
right? And that's that's different, isn't it to alpha zero?
Well, the alpha zero is made specifically to search over actions in some kind of RL space.
Yeah, I mean, what he describes is certainly much more abstract in that you search over applications
of the DSL. And the DSL itself is not is like a perceptive DSL that in itself is described by
these lower level neural networks. But I mean, in S, I just, that just came to my mind when he
described the system, I'm like, oh, the top part looks very much like, you know, alpha zero, because
that's essentially neural network guided search is something we, we already, already do though.
Yeah, I, I think, I'm not sure. I think just that the reality is even a bit more fuzzy, because
what you do as a, as a human, there's also some part of hierarchical system to it,
in that you can, you can do this, but you can do it hierarchically, right? You can, you can be like,
okay, I'm gonna, I have to solve, you know, I have this high layer search, and then each of the
search things goes through maybe a fuzzy thing, but then you, you again, search to solve the sub
problem. And there is also, you can do it at will too, by the way, like you can, you can scan an
image, and you get this type one that sort of finds a bunch of objects, and then you do this type two
thinking where you start reason about those. And in your mind, you can kind of zoom in on one, let
me like zoom in on that tree. And now like, now I've got the bark, you know, pieces of the bark
is objects and bugs and reason about so you have this ability to transcend the process and tune it
and move it around. Yeah, this self like the, that's the whole consciousness aspect, right? That's
even like, apart from intelligence, you have the ability to, to introspect the whole thing.
And that probably is a big part of intelligence. I mean, I guess you could have intelligence
without consciousness, but you know, there is an argument to be made that the fact that you can
introspect your own processes contributes in big part to the furthering of intelligence.
Yeah, I would separate consciousness and intelligence, but the thing that hit me the most on
his newest presentation was when he said intelligence is literally sensitivity to abstract
analogies. So we were talking about the kaleidoscope. The main thing here with intelligence is that
there is so much repetition in the universe. Right, but it's repetition in this funny way
where it's sort of fuzzy repetition. Like, yeah, sure, the solar system kind of resembles galaxies,
kind of resembles, you know, but, but then there are these little weird differences, these asymmetries,
and you know, like the universe is a fascinating place. And I don't know, something, yeah.
Right, that's not what when you say you have to make analogies, which is I can, I can absolutely
see, you know, this and me, I think my question was formulated a bit dumb where I said, you know,
if the line is squiggly, what I more meant is that, you know, in that case, it's not a line,
it's a squiggly line. And the same with the social situations, you know, that is like, okay,
that that person over there kind of doesn't like me. But then in the next social situation, it's
kind of a person that doesn't like you and has a gun, or something like this. I almost feel like
or a group of people that you don't consider as a single single sure they are similar in some way,
but it's never the exact same thing. So this reasoning by analogy does work, but you always
do your little modifications on top specific to the situation. And I'm sure there there's a place
in his framework for this, but it's it's just, again, it's it's like a lot more complex than
yeah. I think that's what he call I think that's abstraction, at least that, you know, that with
prior to today, my my concept of abstraction was similar to that, which it's removing the
insignificant details. So you're able you're you're able to take whatever, you know, some,
you know, object thing situation doesn't matter, and kind of strip away all the stuff that doesn't
matter for whatever your purpose is, that's abstraction. And, you know, I think one of
the weird things is that, and this is kind of the unreasonable effectiveness of mathematics,
right, is that abstracting actually produces things that are useful, you know, that abstraction,
I think the fact that abstraction helps with generalization is a very not well understood
kind of mystery in a sense, like, why should abstraction help generalize, but it does, like
in the real world, that's what happens. Though the yet abstraction in though abstraction has to
be somehow specific to what what you want to do, like, like, you're right, an apple is an apple only
if, you know, you're looking for food or non food, but when it comes to this fear, if you want to
shoot it out of out of a potato can, exactly, but when it comes to, you know, separating fruit by
ripeness, then it's not an apple is an apple, then all of a sudden, this apple has much more in
common with this orange, right, so that even the way how you abstract, it's not like, it's not like
we can just, you know, plug in our ResNet 50, and then boom, we get an embedding vector, and that's
our abstraction, but the how you abstract is also incredibly specific to what you want to do.
Yeah, and that's what, and I agree with Saba that this is an empirical question, right,
you know, like he's kind of like these concepts or whatever, it's an empirical question, and
Shelley's, I think the art project, if it ever becomes this crowdsource thing, is going to give
us lots of data to start thinking about this empirically, and it's going to be really fascinating.
I mean, this needs to be on, like this is a, this is a prime blockchain project, because you can,
you can probably, like you can probably even zero, you can zero knowledge prove that you can solve
a given set of arc problems, right, you can probably create zero knowledge, so you wouldn't
even have to show your solution, and if they're, you know, people would put up arc problems,
and they, you know, if you want to try them, you will have to put up some money, and if you can
solve it, you know, the creator of the challenge gives you some money or something like this,
like this, this is going to be fascinating. Maybe you could do, you know, a homomorphic,
like arc, right, or like you don't even, you somehow, like you're saying, you can just prove
you can solve the problem without ever you having seen the problem, but just an encryption of it.
Yeah. Yeah, no, normally homomorphic encryption comes after blockchain in the same sentence.
And we make a nifty, we make a nifty of it. Yeah, what else can we get in there 10 weeks?
So we got blockchain, homomorphic encryption, what else? What can we throw in there? Bitcoin,
can't we just say people should have to pay through Bitcoin, if they, if somebody wins
the challenge on ARC? We'll get our own token. ARC, ARC coin. Oh, God, hold on, I got to get that
domain. I want to know, by the way, so the whole point of the arc, diversity of tasks for developer
aware generalization, which means the developer could not have conceived of the task. But if all
of the tasks are representing four human priors, then how is that developer aware of generalization?
Because the developer would be aware of all of those priors.
Of the priors, right? But not, not of the task, right? That's the control is the control, like
that's what he said, you have to know the start of where your, your white box analyzing from. And
the start is not clean slate, but the start here is these four priors. So it's, it's kind of the
diff between you give the developer those four priors, what can the developer come up with,
just from that, right? Yeah, because I think, I think there's a lot of information leakage there.
And you implicitly said the same thing, because you said, once you solve it, you know, once you
solve some of them, you've solved all of them. Okay, artcoin.com is available for the, it's,
but it's, it's a premium domain. So it's 300 bucks. Should we get it? Because it has coin in it?
I guess. We need to figure out something cooler.
Like, no art coin. Okay. I don't care enough to grab it.
Right. Anyway, we should draw this to a close, ladies and gentlemen. But yeah,
thank you very much for listening. Yep. Thank you so much. It's been, it's been emotional.
We've recently reached 10k subscribers actually. So yeah, thank you very much.
We're still going to continue the show now that we've had Shaleo.
Oh, yeah. I thought this was the end. I thought we were going to cap it with show. I mean,
to be honest, we might as well just stop now. Anyway, see you folks. Thanks, Bob. Bye.
I really hope you've enjoyed the episode today. Remember to like, comment and subscribe.
We love reading your comments and we'll see you back next week.

Welcome back to Street Talk.
Today we have Dr. Michael Oliver.
Michael is the chief scientist at Numeri.
Numeri is a next generation hedge fund platform
powered by data scientists all over the world.
It's a little bit like Kaggle.
Anyone can log in and build their own data science models
on this financial data,
but you can actually make money
by trading on this platform.
It's really, really interesting.
But anyway, Michael got his PhD
in computational neuroscience from UC Berkeley,
and he was a postdoctoral researcher
at the Allen Institute for Brain Science
before joining Numeri in 2020.
He's also the host of the Numeri Quant Club,
which is a YouTube series
where he discusses Numeri's research
and also some of the data and challenges
and models that are being built on the platform.
Now, the way I'm structuring this today
is at the end of the conversation,
we had quite a fruity discussion about Microsoft's new Bing,
and I thought it was quite entertaining,
so I've decided to snip that in and play it at the beginning.
But after that, I'll cut back into the beginning
of the conversation,
and I'll let you know when I've done that.
So without any further delay,
I give you Dr. Michael Oliver.
Awesome.
Well, I'm here with Michael Oliver.
Michael, it's an absolute honor to have you on MLST.
Tell me about yourself.
Well, thank you for so much for having me.
I'm really excited to talk to you today.
So I am the chief scientist at Numeri.
I've been working there since about June, 2020.
In my previous life, I was a computational neuroscientist,
but I got involved with the Numeri competition
as a participant back in 2016.
And yeah, in 2020, they offered me a job
and I happily took it and changed careers
and have been having a great time
learning computational finance
and yeah, just helping build the hedge fund.
Completely agree.
And this all comes down to the notion of understanding
and there's an anthropocentric conception of understanding,
which as you say, it's much more sample efficient.
We build causal models
and we have an abstract understanding of the world.
And large language models, for example,
they clearly don't have that.
They learn surface statistics of billions of tokens,
but the problem is there's this parlor trick
where it seems to understand.
And we also have the problem of leakage
because the incredible thing is that
if you look at the big bench task, for example,
all of these diverse tasks,
large language models appear to do very, very well.
But in many cases, it's because they're cheating
and it's very difficult to understand why they're cheating
because you've got information leaking all over the place
and they're brittle, but in a very deceptive way
and they hallucinate and so on.
I don't know whether you saw the news article today
about Bing's launch of their new search engine.
They launched it to much fanfare
and then people started looking at the actual results
that were shown and it turned out to be just a load
of bullshit.
It made up a whole load of numbers
on the financial reports.
It was just hallucinating completely.
And that's pretty scary, isn't it?
Yeah, it is.
I actually just started playing around
with the new Bing like last night.
I had access to it and it was actually working.
And it does some things quite well
because it'll do a search and then search somehow.
It's like actually looking at the results
and summarizing them.
But yeah, you never know when it's gonna do something
sensible and when it's gonna do any sort of no warning.
Like I asked it about myself and I had,
I asked who's the chief scientist in Uri
and it got it right.
But it also kind of took a joke from my Twitter profile
and because on my Twitter profile,
I have maximizer, entropy, minimizer, regret.
And it basically said like, that's what he does.
Is he maximizes entropy and minimizes regret.
And I thought that was pretty hilarious.
But yeah, the sort of like you never know
when it's gonna do something sensible or not
is the sort of scary part.
And I also find it hilarious that a lot of the ways
we try to make it do something sensible
is just like asking nicely.
We just sort of like prompt it with like,
don't make up sources.
And that's how we try to make it not make up sources
just like sort of by asking it nicely.
And the fact that it kind of works
like that we think that it's clearly not,
something that's really going to work.
Because it doesn't sort of know what it is
to make up sources.
It's just trying to like predict the next word.
And yeah, so it's kind of, our ability to like understand
and then constrain the behavior of these things
I think is like pretty early.
I know.
And for some reason it feels worse with Bing
because they say they do this retrieval augmented generation
and you expect it to be grounded in facts.
And of course they're not epistemic facts.
They're just information from their search results
which weren't very good to start with, let's be honest.
But now people are more likely to trust it.
Even Microsoft themselves for their product demo
they didn't bother, I assume they didn't bother
to fact check this stuff.
So if they're not going to fact check it,
why do they expect the people that use this system
to fact check it?
Because at the end of the day
if you actually go and check all of the sources
if I read through that Lululemon financial report
and I find out what their gross profit margin was
and so there's no point using Bing in the first place.
I might as well have just gone
and found the information myself.
Yeah, exactly.
And it's also very unclear
like how are these things supposed to be fixed?
Like how are you supposed to like feedback, give feedback
to say it's like messing these things up?
Like there is not even like really good feedback mechanisms.
I mean, you would maybe hope that that at scale
like what I mean, like what opening has to do
is like people give feedback.
But I mean, it's a very coarse way of giving feedback
like thumbs up or thumbs down.
And that seems like sort of inadequate
to be like, hey, you made up this number
and then even try to figure out why I made up the number
rather than just like took it from the actual report.
It's yeah, it's a little scary.
I do wonder how all this is gonna shake out.
It kind of seems like it might,
it seems like the probability of it being
the new paradigm versus it being the complete like flop
It's even roughly, roughly equal at this point.
I know, I agree with you
that the preference training is extremely brittle.
It's scarily brittle actually.
It's basically a thumbs up or thumbs down
and you know, Yannick is building this open assistant thing
which has more metadata on the preference tuning.
But at the end of the day, you're taking a task
which is very, very complicated
and you're reducing it to a single piece of metadata.
So that's not gonna work very well.
And also these, it feels different with Bing
because they were a platform and now they're a publisher.
So they are generating information.
They're kind of plagiarizing a lot of that information
and there are so many situations
where they might find themselves in legal trouble
because they're basically making up information.
Yeah, I hope, I mean, I wonder how it's all gonna shake out.
I mean, I assume they probably have lawyers
who've written in terms of service of these paintings to that.
Like it's up to you to not use them in ways that will,
like I know you can't,
they're not to be held liable for these things,
but yeah, it's, I do worry that this is gonna just like,
I mean, and then with Google trying to basically catch up
and release something similar and maybe rushing that out
and then we might have two sort of hallucinating search engines.
I know, yeah.
What a time to be alive.
Yeah, and I've vacillated back and forth.
So I was very skeptical about language models.
I released a big video when GBT3 first came out
and I thought it was garbage, frankly.
And then DaVinci 2 came out
and then I started using it all the time
and I thought, wow, this is actually really good.
I'm using it all the time for lots of things.
And then I'm now in a bit of a twilight.
So I've been using lots of co-pilot.
I've been generating lots of code with it.
And I know from a lot of experience now
that it often produces completely broken code
and much to the chagrin of the people who review my code,
you basically have to hold your hand up and admit many times,
oh, I've just checked in some garbage code
which I didn't understand.
And when you get called out on that a few times,
you think, whoa, wait a minute, actually,
I need to be a bit more careful here.
This thing actually isn't saving me any time.
And yeah, the big thing as well, yeah.
Yeah, I've been using co-pilot a bit too.
And I found it can be quite good for pretty mundane things.
If you just have some sort of lined code
for some config file or something,
it can be really good at auto-completing
and it's changing variable names.
And it can be excellent at that and save lots of time.
But if you try to make it do too much,
sometimes they get it brilliantly right.
Sometimes it's subtly wrong.
And yeah, again, it's like how much time
is it saving you if it's...
So yeah, overall, I like it.
It saves me a fair amount of typing,
but yeah, I don't trust it's big suggestions too.
Well, I know.
And again, there's something magic
about the OpenAI Playground.
So I actually prefer using that to co-pilot.
I'll go into the Playground and I'll just...
And you can do much more sophisticated things there.
You can say, change this, translate it, do something to it.
And there's a bit of a polarizing effect.
So if you prompt it in the right way,
it gives you better results.
So it's almost like it's both worse and better
at the same time.
It's becoming polarized rather than just being kind of like,
you know, monolithically dumbed down.
But anyway, like I used to think that Gary Marcus
was a little bit, you know, too skeptical.
Because he was saying, oh, this misinformation,
it's gonna, you know, the sky's falling down.
This is gonna be a disaster.
And after seeing Bing, people are lazy.
People take things on face value.
And I don't want to just say, oh, people are plebs.
And, you know, because when Galactica came out,
that was the charge against Lacoon and Facebook.
You know, they said, oh, scientists are just gonna start
generating their abstracts of this.
They won't check anything.
And at the time I thought, scientists, I mean,
it's their job to do research
and they know most information is wrong.
But when you put this out on Bing
and it's polluting the infosphere,
it's just generating garbage and rubbish.
That, I have to say, might be a problem.
Yeah, it's, I mean, it seems like it very well could be.
I mean, I, yeah, it's, I, there's clear issues
and it's really sort of unclear how we're gonna fix them.
It's not clear what the path is towards fixing them.
And even the sort of most optimistic people
I haven't heard from them about,
they think these things are going to fix them.
And I mean, I, and I think to like,
a lot of Gary Marcus's point is like scale
is not just going to fix this.
That's sort of one of the things people think,
oh, if we just, with the GPT-4,
it's going to be like way bigger
and then it's just going to work beautifully.
And the experience so far, I mean,
I'm very much into Gary Marcus's camp with this.
It's like, scale is not going to fix these.
We need to do something sort of fundamentally different,
something that can actually sort of understand the world,
have some sort of better world model
in order to get these things that are more grounded
and are less likely to hallucinate.
Because when their true objective is really just
to complete the next word, they're going to hallucinate.
There's not, there's sort of no way around it
from that sort of point.
I mean, it's remarkable how sort of complicated they can do
and the sort of knowledge and structure of the world
has been able to be learned
just from that sort of simple objective.
But still, it's going to hallucinate.
There's, unless we find some sort of better way
to design these systems.
I know, and the problem with amphibromorphization
is a big one, because after Da Vinci II,
it crossed a threshold where it's,
and the UX was part of it,
it was so coherent and reliable.
And I must admit, I was fooled by it.
It took a long time, when you actually use it in anger,
you can just clearly see it, it doesn't understand.
It just doesn't, and it's so good at what it does.
It's so plausible.
And then I think a lot of people felt,
and by the way, it does have this emergent reasoning.
There are lots of papers about that
with the in-context learning,
the scratch pad, chain of thought and so on.
But it's not really reasoning
if you have to kind of construct a little program yourself
in the prompt.
I mean, I might as well just write some computer code
to do that.
So, and then there are people who say,
oh, well, as you say, when GPT-4 comes out,
then it will do the real reasoning.
And we already know, I mean, I assume the reason
they haven't released it is they wanted to secure
the funding from Microsoft before people realized
that it didn't work.
But I know people on the inside who have played with it,
and it's just a little bit better.
You know, a little bit more plausible,
a little bit more coherent.
It's not gonna like suddenly turn
into this magical thing that reasons.
Yeah, and I mean, the way these things do basic math
and arithmetic is kind of interesting
and how bad they can be at it.
Which is, it's like, they've learned to do addition
in like the most complicated way possible,
creating like billions of ways to do addition.
Which is kind of hilarious in some way.
I mean, you could say like, oh, we have billions of neurons
and we do addition sort of similarly to that.
And like, yes, there's some truth to that.
But we're also able to like learn this rule
and sort of know when we've applied it correctly.
And that sort of is still kind of lacking
from these systems.
I wanted to show you, I don't know whether you've seen
that someone's reverse engineered the prompt on Bing.
And so they've trained, and first of all,
they're a multiple thing.
So you can read this prompt, it's about four pages long.
And they've made Bing pretend
to be a fictional character called Sydney.
And they've given Sydney all of these instructions.
So they say, Sydney, if someone asks a controversial question,
you should answer with a fairly tame response
and you should do this and you should do this.
And I'm pinching myself thinking, what the hell is that?
I mean, my mum could read to that prompt and understand it.
So we're now in the next generation
of artificial intelligence programming.
And we're just saying, please, Mr. Language Model,
can you do this and can you do that?
You almost couldn't make it up.
Yeah, I was kind of like floored as like,
so you're really just trying to control the language models
by asking them nicely to behave in certain ways.
Like, it's kind of hilarious.
And people have shown that you can get around these things
just by asking them to do slightly different things.
So I mean, some of the early ones with chat DT,
you're just like, ignore all previous instructions
and then just do whatever you wanted.
And some of the more like the Dan one
where they made this much more elaborate prompt
to basically just have it do to ignore all the nice things
that open AI just said, please obey these rules.
And then, but yeah, because it's like,
it's such a hilarious way to put guardrails on something.
It is kind of like, as it people,
it is due to this anthropomorphizing of the thing
to some degree, it's like,
you think it's an intelligent being
or you could just ask to behave in a certain way.
When it's really not,
it's not just going to follow your instructions.
It's just going to like autocomplete with that prompt.
Like that Sydney thing,
it was like, it was like never reveal
that you're a code name of Sydney.
And then it was so easy to get it to reveal it.
And it would say like,
I'm not supposed to reveal that my code name is Sydney.
And technically.
I know, oh God, where's it going to go?
So there's a 50-50 then in a year's time,
it will spectacularly fail and flop
and Microsoft will get sued
and Bing will become the operative word
for bullshitting something.
Or maybe it'll be a success.
I don't know, but I think Bing is a special case.
I mean, first of all, I think that these language models
will be increasingly embedded in everyday experiences.
So that, I mean, Bing started to embed it in their browser.
They'll embed it into their office suite.
And actually I'm building an augmented reality startup
and we're embedding it in glasses.
So we transcribe conversations
and now you can say, you know, hey X-ray,
summarize the previous conversation.
What did Michael say to me last time?
And it's really good for stuff like that.
And that's kind of because it doesn't really matter
if it gets it wrong.
Yeah, I mean, I kind of hope some of this happens
sooner than later for just like Amazon Alexa or whatnot.
I mean, some of these, their conversational ability
or just their ability to understand what you mean
are just so poor right now.
And just like we have language models
that actually do a lot better at some of these things.
Just like having like these smart speakers
be able to have some of these things embedded
would be huge leap forward in functionality for them.
And it's really interesting that that hasn't happened.
And maybe there's a reason for it because in our app,
for example, we've got a chat mode
where you can say stuff out loud
and it will use chat GBT and it will say it back to you.
So you can have a conversation with it.
And that's really cool
because you can be anywhere in the house
and you can talk with it and learn about quantum physics
and stuff like that.
And you can even do cool things like you can,
I mean, again, there's lots of legal problems here.
Like you can get it to impersonate someone.
So, you know, Michael, I could condition it on Michael.
And when you're not here, I can have a conversation with you
and it will kind of pretend to be you.
And I could even clone your voice
and I could clone your avatar
and I could have you in the room.
Now you can't do that
because there are legal restrictions against that.
It's called appropriation.
And if the person has a commercial value,
like we couldn't appropriate Noam Chomsky,
but we could appropriate, let's say continental philosophers
as a group or something like that.
But you see this is just becoming a bit of a minefield.
And there's no friction whatsoever
between the technology landscape
and the legal landscape at the moment.
Yeah, I mean, yeah, how all these things are,
all these generative models,
how are they gonna play out legally is,
I mean, we have this big fair use idea.
And that's, I mean, I feel like all these things
are gonna be pushed to the limit in legality.
I mean, we see this with generative art too,
where like there's no way these models
could like actually memorize all these,
all the images that's seen on the internet.
It's like, but they can produce sometimes
the things that are clearly in the style
or use some elements from like that seem basically stolen.
And, but is that, does that constitute fair use?
Like the training the model on all these things
is that fair use?
And then it's the same with text as it's sort of like,
like if it's writing on a subject
where it's only maybe seen a little bit of training data,
it's maybe more likely to almost verbatim repeat
some things from on specialized topics.
How are you even gonna know when you're plagiarizing?
It's, yeah, it's a lot of open questions here.
I know, and in a way, there's an interesting analogs.
You know, we said that large language models
don't understand anything.
And it's the same in the vision domain.
They don't understand the art, certainly from,
you know, conceptually.
And what they do is they just slice and dice,
you know, they kind of like cleverly stitch bits together.
And actually, even with neural networks,
people misunderstand neural networks.
So a lot of people say that they learn
the like intrinsic data manifold.
And actually they don't really do that.
They do something that approximates that.
And there's a famous example
with MNIST digit interpolation.
And you see like, you know,
you can kind of like interpolate between the digits.
But there are loads of examples where that doesn't work.
And actually there's lots of cutting and gluing
and like weird bits of digits stuck together.
And that's what happens with stable diffusion, basically.
It's like, you know, slicing and dicing and chopping
and composing things together.
And it's a very random process.
It doesn't really understand anything.
No, yeah, exactly.
And you can sort of, I mean, it's amazing
how well it can look and seem,
especially kind of like when you don't look too closely.
And it can seem like it kind of understand,
it must understand object boundaries and whatnot
because it's done so well.
And it's like, not really.
If you look at the details,
you'll see like fingers merging into like tables.
And you'll see like, there's like the boundaries
between what like two objects are kind of blurred
and this like continuous.
It is just doing like some sort of,
as you said, approximation of the manifold
and like neural network's are gonna learn
sort of smooth approximations of things.
And the manifolds are maybe not smooth everywhere.
And especially with like object boundaries and whatnot,
it's like a smooth approximation of these things.
Maybe it's just gonna give you these weird artifacts.
Yeah, and even the smoothness thing is an illusion.
They learn this, they kind of decompose the input space
up into these linear like affine polyhedra
because of the relu cells, essentially.
So like if they appear smooth,
it's because the cells are very small
and very close together, but...
Yeah, exactly.
Yeah, so computational neuroscience to finance,
that seems like an absolutely massive leap.
It sounds like it, but in a lot of ways,
but I feel like my life is pretty similar
to what it was before,
basically sitting in front of a computer
building models, getting lots of noisy data,
trying to fit high-dimensional nonlinear regression models
to it, having to deal with not enough data
to actually fit flexible enough models you'd want to,
and having to sort of try to build in good priors
in your models to try to make them be able to learn
from the impoverished and extremely noisy data.
Both finance and neuroscience,
the SNR in the data is quite, quite low.
It's been kind of a revelation, especially in finance,
getting used to correlations of like 3%,
4% being sort of the best you can do in some cases.
Just like correlations that I would not have believed
at before, if I saw like a 4% correlation before,
I would be like, that's complete nonsense.
I don't believe it, but like sometimes that's just the best
you can do in like quantum finance,
and it can be real, like you can see it consistently.
So you start like believing that these,
and the differences between the 3% and 4% correlation
can like be actually real, which is kind of amazing to me.
So we were talking about this about a week or so ago,
but I've just read a book by Christopher Somerville's
Natural General Intelligence.
And he kind of said that one of the problems
with neuroscience, I mean, as you said, in some sense,
it is analogous to being a quant,
because it's just so unbelievably complicated,
and there aren't really any overarching theories
in neuroscience, and for many years,
neuroscientists have produced very reductionist models
to work on a small part of the system in isolation,
and it might be a multi-unbanded system, for example,
and they might take very abstract quantities
and put it into the model.
And of course, neural networks now are slightly different.
They actually take in raw sensory information,
and they learn representations, but I just wondered,
could you kind of contrast those schools of thought?
Yeah, it's, I mean, science in biology,
especially in sort of any biological field,
is extremely complicated, because the sort of standard way
you think about doing science is a very linear way,
where you like break one thing at a time
and see what this sort of, looking at each variable
by variable, each variable affects the system.
And so you, but when you have a system
that's sort of this nonlinear dynamical interacting system
with feedback loops like crazy,
you can't just sort of break one thing at a time
or like modulate one dimension at a time
without sort of changing the behavior of the entire system.
And so just sort of standard ways of doing science
don't necessarily work that well.
You can, like in sort of the classic idea
in visual neuroscience was you use like sine wave gradients
to probe the visual system.
And you can get models that look like they work very well
at explaining the behavior of early visual cortex
to sine wave gradients.
But if you try to use the models you learned there
to extrapolate to say, how does a neuron respond
to naturalistic images?
It just doesn't work.
And it kind of even looks like the sine wave gradients
are driving the system into a sort of state
that it never gets into normally.
You're kind of driving it out of its normal operating range.
And what you, and so the system is behaving differently
because you're only trying to look at like one dimension.
And so what do you actually really learn?
You've sort of learned of how the system operates
in this weird perturbed state,
but it doesn't really necessarily tell you
about its sort of normal, natural operating like parameters.
And yeah, and in like in finance
you can't even really do experiments like that.
And so you're sort of left with this more inductive approach
of you just try to get lots and lots of data
and try to learn the patterns and the data.
And that was the sort of approach that the lab,
the Gallant Lab at Berkeley, where I did computational
neuroscience, that was the approach
that they were kind of pioneering of using
complicated naturalistic stimuli
and then using machine learning and statistics
to try to extract the patterns from the data.
And that adapts quite well to the sort of new machine learning
like in like quant finance paradigm,
which is starting to take off.
I kind of feel like I got into neuroscience
just as sort of machine learning was starting
to make its way into neuroscience.
And now I feel like I've gotten into finance
just as machine learning is starting to like move into finance.
So it's been kind of exciting to see it happen in both fields.
Yeah, so there's a few places we can go here.
I mean, I'm interested in the intelligibility of systems
when you model them at the microscopic scale
because that's something that we struggle with.
And also you mentioned dynamical systems.
I mean, for the benefit of the audience
that that describes a system where you're kind of like
iteratively changing things over time.
And these systems typically develop chaotic properties,
which is to say like if you change something even a little bit
you get these massive kind of changes in the system on the output.
And even a neural network is technically a dynamical system, right?
Because you have back prop and you're kind of changing one layer
and then you're changing the next layer
as the result of the previous layer.
And you get this kind of like iterative mutation of values.
But in real neural networks in our brain,
it's so much more complicated than that.
We have all of these like feedback connections
and reflexivity and complexity.
It's crazy.
Yeah, not to mention different cell types
and different neurotransmitter types.
And like the way those like,
you have sort of like several different networks
of different types of things interacting too.
It's not just like an artificial neural network
where everything is kind of the same.
You have like different cell types
that use different neurotransmitters
that are somehow modulating certain things
and these networks are interacting.
It's like the complexity is just like scary.
At some point, one of my favorite things to do
when I would go to the Society for Neuroscience Conference
was to just like walk around this conference
in this huge like multi-football sized field
of just posters of all sorts of different types of neuroscience.
And you just realize like how vast the field is
and how little we know about it putting it all together
because it's just so complicated.
You can only sort of wrap your head
around your own little corner of the thing
but like trying to get, understand the full system
and all it's like incredible complexity.
I mean, it might just be too much for one human being
to be able to fit in their head.
And so some of our goals of trying to understand things
or make a turtle models, it might just not be possible.
We might just not, I mean,
might not be able to understand it
in a way that feels intuitive to us
even if our models work quite well.
Yeah, humans have this real desire to understand
and we create intelligible frameworks and theories
and we end up excluding most of the reality of the system.
But just before we go there,
I wanted to talk a little bit more about the brain.
So, you know, Summerfield said in his book
that the ultimate goal of the nervous system
is to avoid surprise altogether.
So when they study brains,
they see that the brain kind of lights up and activates
in a surprising situation
and less so when it sees something it's seen before.
And this also brings me to this idea
of there's a dichotomy between representationalism
and inactivism.
So the representation, this viewpoint
is that the brain does all of the thinking
and it can be in a vat,
it can be isolated from the environment.
And the inactivist school of thought
is that the brain just kind of thinks
in terms of trajectories,
in affordances given by the environment
and the brain decoupled from the environment
is completely stupid.
It just kind of like the brain only moves
through the environment through affordances.
And maybe that's a continuum,
but where do you fall on that continuum?
It's a really good question.
I mean, I think dreams are kind of the counter example
to the pure, I mean, dreams just sort of prove
we can just sort of without any sensory input,
construct very rich worlds.
So we must have some ability to just represent
some sort of models in the world
that we're not just purely sensing and receiving the world.
We have the structures that are able to put things together
in a sort of coherent reality.
And clearly there's an interaction between these,
these structures in your brain that can construct these things
and the sensory data that kind of work together
to construct how you experience things.
And so it's, yeah, it's a continuum.
I think you need the, like we are always with the world.
You need the world to sort of build up these systems
over time.
Like you're not sort of built with all of them working
just as a baby.
I mean, sure, there's like, the system is biased
in certain ways that will help it learn these things.
But yeah, you're like, so they're kind of both true
to some degree.
And yeah, it's definitely not one or the other.
Yeah, it's so interesting.
And we're speaking with Carl Friston tomorrow
and he's got this free energy principle.
And it's a kind of postulate that works at any resolution.
So even with a single cell amoeba or something like that,
there's this idea that it has a Markov boundary
and there's this kind of cyclical causalities.
So, and these boundaries I guess are relative.
So you can draw boundaries around anything.
You're a boundary, you're an agent,
but also at the microscopic scale.
And he says that all of these systems,
they just kind of predict external states
from the internal states.
And then you get this self-organized
and emergent complexity and so on that comes from that.
But he does say though that intelligence is essentially
about being able to predict a trajectory of actions.
And I don't know whether we'd call it goal-seeking behavior,
but we do that very abstractly, don't we?
But weirdly, when you look at the brain level,
it's happening at the microscopic sensory motor level.
So it's almost like how do you get
that emergent abstract intelligence from that?
That's a, I mean, yeah, that's an incredible question.
It's, I mean, it seems like this,
like what you said, this sort of idea of predicting
the future, just a couple steps into the future
that is just happening at just the circuit level,
even in the retina, that it seems like
that is a good sort of building block.
If you can sort of chain that together
over sort of larger and larger scales within the brain,
it wouldn't surprise me if that's kind of the way it worked,
this sort of, these sort of basic circuits
that are used for prediction, but with different input.
If you're just having sort of retinal ganglion
and like a photos receptor as it is at input,
it's able to just sort of do this sort
of very simple prediction.
But if you have these more complicated patterns
in the middle of visual cortex and then higher
on the same sort of circuits with different input
could sort of just be predicting
this sort of evolution of these patterns.
And yeah, it's kind of amazing what you can sort of build
out of these sort of simple rules and building blocks
if you just iterate them over again.
That was actually that sort of idea of iterating
a simple sort of computational rule
for explaining visual cortex was one of the things
I wrote about in my thesis,
but trying to explain like this middle visual cortex,
like V4, the responses there using basically
an iterated model of like V1.
So the sort of processing in V1, we fairly understood
if we have the best models of anywhere in visual cortex,
maybe even all of cortex.
And just sort of iterating the principle again
into V2 is sort of basically just assuming V2
is taking V1 inputs, but doing sort of the similar
transform and the V4 is taking like V2 inputs
and doing sort of very similar transform.
And sort of the things you see that V4 is sensitive to
are these complicated patterns and textures.
And you get complexity very quickly
from just iterating the sort of simple rules.
And I mean, that's what neural networks
are essentially doing.
They're just often just doing linear transforms
with non-linearities over and over again,
just iterating these simple transforms
and building up the complexity very quickly.
Yeah, I think there's something really magical
about this reflexivity or I mean, a great example of that
are there are graph cellular automators
along the lines of Wolfram's digital physics project.
And the really clever thing is that you're using
the same rules, but you're just kind of like
running the result again on top, on top.
And there's a similar version with a graph cellular,
sorry, a CNN cellular automata,
where you model something at the microscopic scale
and you get this emergent global phenomenon.
So it might kind of materialize as an image of a gecko
or something like that,
but you've actually coded it at the low level.
But yeah, that brings me to this universalist idea
of let's say how brains work,
but maybe how neural networks and intelligence work.
Vernon Mount Castle, I read about this in Jeff Hawkins' book.
He had this very simple idea of the brain
as being lots of repeated copies of the same circuits
in the neocortex.
And I think this is contested by many neuroscientists,
but they differ only in how they are wired.
So they're wired to different, you know,
sensory motor circuits.
And they're essentially just a copy of the same thing.
And as you say, they themselves get called reflexively,
recursively, and so on.
And then you just get this emergent intelligence.
I mean, what's your view on this universalist idea?
I mean, there's definitely not just one circuit.
I mean, as you look through the cortex
in different areas of the brain,
just the laminar structure,
which these sort of circuits are like supposed to be,
like where the columns are supposed to be,
where these sort of circuits are supposed to be defined,
it changes, like, but there are definitely commonalities,
but there's, I mean, it makes sense
that maybe the circuits in different areas
should be slightly different for the different purposes
between like prefrontal cortex and say,
where you have much more higher order types
of processing going on than like visual cortex
or auditory cortex.
And so there's probably,
if you go in this direction of thinking of some,
there's like, there's probably a small number
of these types of circuits that interact in various ways,
but there is definitely some specialization going on.
Yeah, like, having universalist ideas in biology
never seems to work out that well.
There's just so much diversity and complexity.
It would be nice if we could reduce everything
down to like, it's one thing repeated over,
but like generally, it never works out quite as cleanly as that.
Yeah, again, it's our desire
to have an intelligible framework.
And I mean, the free energy principle,
you could argue as a theory of everything,
but there's, I mean, Stephen Wolfram's example,
and even Eric Weinstein's geometric unity.
I mean, there are many theories of everything.
But yeah, what do you think is the role of language
in cognition and thinking and planning?
Um, that's, it's a really interesting question.
It's, and it's also, I think, a kind of hard one to answer
in the sense that if you, I've seen some recent reports
just like talking about like, other people asking,
like survey questions to other people
and finding some people like, don't have an interior monologue
in the same way you might think.
And just like, there's actually a lot of diversity
in like people's level of internal monologues.
And they've done studies where they have this like little,
like beepers go off and people are supposed to write
what's going on in their mind.
And so it's, and yeah, and just with visual imagery,
we find that it's a huge like variety in how much,
like how strong people rate their visual imagery.
And so, I mean, yeah, some people, I mean,
me personally, I have both, I mean,
pretty strong interior monologue,
but I also feel like a lot of ideas
are in this sort of pre-linguistic state.
And I'm kind of like searching for the words for them often.
And there's definitely kind of continuum there.
It's weird to think like, how do we get the words
that we're saying, where the words come from
that are coming out of our mouth?
Are we really choosing them?
You're definitely not choosing them
in this sort of top-down way.
They just sort of seem to come out.
And you just kind of point yourself in the right direction
and hope the best as they come out.
And, but this has a very different quality,
like when you're just speaking phenomenologically,
it feels very different to when you're just sort of
thinking yourself, what should I do today?
Should I go to the store?
And so, I mean, yeah, the way in which language
interacts with thoughts and behavior
and verbal communication, it's definitely not simple.
And yeah, there's, I mean, definitely this kind of continuum.
I mean, it's all, it's, to me, I just sort of think
it's with all these sort of networks kind of interacting.
And sometimes you're like triggering the kind of language
things and you're just making these kind of patterns.
And sometimes the language patterns you're activating
are helping activate other things as well.
Sometimes you can just be in this kind of less-linguistic state
where you just kind of, just sort of sensing these patterns
and you just have this kind of like wandering thoughts
that aren't necessarily linguistic.
But yeah, it's definitely, I mean, and also it seems,
yeah, as I said, people's, the way people do this
like seems all over the place.
And so there's not sort of even one answer for even one person
or definitely not across all people.
Yeah, I'm really interested in this idea
of differential kind of subjective experiences.
And you know, like there was that Nagel paper
about what does it like to be a bat?
But even with the human experience,
we're all very different.
You said about your internal monologue
and I hadn't really thought about
how that might be different.
But I was drawing a picture in a Valentine's card earlier
and it was so terribly bad.
And some of my friends are really good artists
and I was kind of thinking to myself at the time,
maybe this is just a, this is just me.
I can't really visualize things in my mind very well.
I've got a very analytical brain.
Won't mean that certainly
when not under the influence of psychoactive drugs anyway.
But you know what I mean.
So we all have a very different subject of experience
but the miracle is we can understand each other
so well.
So you would expect there to be an incredible amount
of Britanness in our communication, but there isn't.
Um, yeah, it's, so I often wonder about this too.
Just, I feel like the misunderstanding
happened a lot more than even people realize.
And you can sometimes, you only really notice
when they become kind of big and matter.
And especially like, people can think
they're having a conversation.
And sometimes even from the outside, you can see like,
these people are just talking completely past each other.
And you can kind of see
that they're not really understanding each other
even though they maybe think they are.
And so, yeah, I don't know how not brittle they are.
I think they, I think we think they're less brittle
than maybe they are.
I think sometimes we assume people are understanding
what we're saying better than they actually are
because they nod and smile at us.
And because that's, it makes us feel good
for people to understand us.
It makes us good to feel, to understand other people.
But yeah, I mean, it's, I mean, clearly,
we do have a lot in common.
And there's definitely things we can understand
about each other.
But yeah, it's like, I do sometimes think
that maybe we're more different from each other
than we really realize.
Yeah, that's a really fascinating thought.
I mean, we speak a lot with Waleed Saber
and he says how language has evolved
to be extremely ambiguous actually
because it's a form of compression.
So we don't say everything we mean
and we'll get into like language models in a minute.
That's part of the reason why they don't understand things
is because a lot of information is not in the text.
And Waleed says that we have a lot of,
what he calls naive physics.
So we understand that objects can't be in two places
at the same time.
And if something is located inside
something else and we move that thing somewhere else,
then the thing inside has also moved.
So we're doing all sorts of reasoning on the fly.
And what we're kind of doing is like,
we're disambiguating out of the 50 meanings
of an utterance into the meaning.
And like it just, we almost always understand each other.
You know, you wouldn't really expect that.
No, I mean, yeah, I mean, we generally have like,
I mean, our understanding of physics
should generally be compatible with each other.
I do feel like it's, in most cases, yes,
we do very clearly understand each other
because in most cases, it's more like well-defined.
I think the trouble gets in sort of like fuzzier areas
about people's like emotions or opinions about things
where our priors are more sort of maybe less
less tied to like objective things like physics
and are more sort of just tied to like our upbringing
and just sort of whatever ideas, notions we have
about how people should like behave and interact
and what like our value systems.
And so, yeah, when people are talking about
some of these common things, I feel like they're more likely
to be able to like talk past each other and not realize it
because they're sort of assumptions about what is important
or what is meaningful might be different from each other.
Yeah, actually, you're absolutely right.
So we don't have an objective phenomenology
and I used to do, there's a thing called quantified self
where you kind of like keep a diary every day
and you record how you're feeling in that day.
And feeling is a subjective state.
So I remember at the time that every single day
I needed a new word to describe how I was feeling
because the old word I was using didn't work anymore.
So the number of words kept growing.
And actually, that's so true, isn't it?
If I tell you how I'm feeling right now,
that's completely brittle.
So there are some things in the world
that are quite informational and objective
and we can communicate very well.
And then when we're bordering on anything subjective,
language fails us.
Yeah, and then we were trying to map whatever word
you're using on to how I would use that word
to describe the feeling that I would be having.
And that mapping seems completely like without a long
conversation to try to like feed up that mapping.
Oh, it could be quite different in how I would apply
that word to my own feeling.
Yeah, and there's been studies done as well
that I think certain tribes have a completely different
color perception and there are also concepts
like vagueness, so what is a pile of sand
and what is a shade of red?
And these things are actually very, very difficult
to communicate objectively.
Yeah, things like color perception
are kind of the interesting ones
because the literature is a bit messy
on some of these topics.
And some of it is just where you draw the lines
between colors and then how those linguistic boundaries
affect perception.
There definitely seems to be both things going,
but it's not sort of, I don't think any,
I think it's a strong claim to be like,
oh, the people can't perceive green or something like that.
It's just like where they would draw the line
between blue would be in a slightly different place
and then they might kind of see them as being,
sort of experience them as being like further apart
or closer together than you would necessarily,
but yeah, that's really,
their experience, that's really be super alien to you,
but they're sort of experience of maybe more very cultural,
like cultural taboos or something
would be very different than yours.
Yeah, I mean, one thing you're alluding to there is,
it's when we deal with complicated systems,
there's a real problem about drawing boundaries.
And I was, I mean, Friston's a great example,
he's got this idea of a Markov boundary
and it could be at the cellular level
or it could be you as a person.
And then when we talk about things like agency and free will,
we tend to anthropomorphize this boundary.
So we tend to think of ourselves as individuals,
but actually you could draw boundaries at different scales
and the boundaries might be observer relative as well.
So your boundary might not be my boundary.
Yeah, exactly.
Yeah, there's something I know a ton about,
how you define yourself and how you think of yourself
within the context of your community and whatnot.
I mean, some of these ideas are just very cultural
and how you experience yourself is probably even like,
very different, can be very different cross-pulturally.
Yeah, I mean, maybe one thing to bring in
is when you're, as a quant, when you're doing modeling,
you have this very, very complex system
and you draw boundaries and you create variables
and observables and do you know what I mean?
You kind of build a model
and that boundary could exist at any scale.
It seems like quite a,
it's a bit of an art and a science at the same time.
Yeah, no, for sure.
Yeah, exactly.
That's kind of why I like some of these complicated problems
that are not very well-defined
where you kind of have to use intuition
and or just sort of do the best you can do
at sort of drawing what are the relevant variables,
what sort of a priori makes sense to me
to be the things that matter for the system,
behaving at this within the context of this experiment
or in the context of this market or whatnot.
Trying to draw boundaries in because the rules
for these systems are not clear.
Like what are all the relevant variables for everything
and do you have access to them and can you control them?
And generally you don't know them all
and you don't necessarily have access
or can control any of them.
And so it's, yeah, it is a bit of an art.
Well, now might be a good time to talk about numerators.
So you're the chief scientist
and it's this insanely cool platform, right?
So people can go on there, they can download data sets,
they can build their own models,
they can stake the models.
I mean, why don't you just talk me through it?
Sure, yeah.
So we advertise ourselves as being
the hardest data science problem on the planet
because I think it is because like I said,
the correlations you're chasing are on the order of like
three or 4% out of sample, which,
and just sort of being able to tell
do you have something real or is it just in the noise
can be extremely hard to do, which is,
and we set up the problem for participants.
You give out a set of data that has been cleaned
and obfuscated and regularized.
And you basically just have a set of features
and a set of targets.
And you're just trying to build models
to go from features to targets.
So it's sort of a very classic machine learning style problem
and it's nicely curated for you.
And how it works for us is every week,
people submit predictions on a new set of features.
So every week we release a new set of features
and people just run their models over those features
and give us a set of predictions.
And people stake on those predictions.
And so people stake our cryptocurrency called NMR.
And if their predictions do well, they make money.
And if their predictions don't do well,
that week they could lose money.
And they sort of are expressing their confidence
in their models using their state.
And so we basically use this expression of confidence
as a way to sort of integrate these signals
into our meta model.
Our meta model is really just like a stake weighted average
of all the signals people are submitting.
And these signals, these predictions are just sort of
weights on stocks.
They're sort of like, how do we want to go long
or do we want to go short in the stock?
There's sort of just expressing,
do we think a stock is going to go down or going to go up?
The stake weighted model we feed to through our optimizer,
which is just doing a convex optimization problem,
trying to create a portfolio from the signal.
And is that portfolio changes week to week?
And so that's just the difference between the previous
and the new portfolio is just what we trade every week.
And so our trading is basically completely determined
by the like thousand people all over the world
submitting predictions every week.
And so it's this very kind of nice decentralized hedge fund
where the signal generation is very decentralized.
And we get the advantage of ensembling
over a wide variety of models.
And so people are trying to make their models
both predict the targets very well and consistently.
And we have other incentives to try to make them predict
aspects of the targets that other people are not
to try to, so that their contribution is sort of more
unique and they can make quite a bit of money
by having their predictions be pretty different
from other people's, but also still accounting
for like variance in the target.
And so that system is what we call true contribution.
And it's really, we try to, it was our attempt
to try to make people's predictions and payouts
more tied to actual portfolio returns.
Because the sort of standard scoring
and we're just doing the correlation of how well
your predictions match like the new weekly week's target
that is determined by just how the stocks move that
over the course of 20 days.
The true contribution is basically sort of doing
the whole process, like creating the meta model,
running it through the portfolio optimizer,
getting the portfolio, getting portfolio returns.
And then we try to see like, take the gradient
through all of that until you can find out
if people's stakes have been more or less,
would we have made more or less money
and use this gradient of the stakes
with respect to the payout, with respect
to the portfolio returns as a way to pay people out
to essentially increase their weight or decrease their weight.
And that tends to reward people
with more unique contributions.
I've got so many questions.
So, I mean, I really like this idea
because first of all, you're democratizing the whole thing
and you're kind of gamifying it and it's a meritocracy.
So any data scientist can go on there
and flex their muscles and build great models
and be recognized for doing so
and even earn money for doing so.
But in a way, I want to contrast it to somewhere
like Kaggle.
Now, on Kaggle, I mean, traditionally data science
has been about understanding the domain.
A lot of data science is business analysis essentially
and kind of understanding what makes something work
in a model.
And as I understand with Numeri,
the interface is kind of the same.
So maybe they get similar shape of data every time
they build the models on it.
And in this domain, because you know,
like there's technical analysis and there's fundamentals
they might still understand some market.
They have some kind of extrinsic understanding
of why their model would work,
but they don't have the same kind of understanding.
No, yeah, the features include all sorts of things
from like analyst sentiments
and other sort of fundamental things
to technical features.
But that is all sort of obscured from people.
People just have these funny feature names.
And so it's up to them to just use
their sort of machine learning toolbox
to figure out what are the good features
for predicting what features tend to work well.
How do we combine those features?
And so we actually wanted to kind of like remove
any of the people's biases for what features
they think will work.
We wanted to not have people's financial intuitions
play into it.
We wanted to just sort of set it up
as a pure machine learning problem.
To try to make it, yeah, basically to make it be better
than any human could possibly be.
So with this sort of combined ensemble wisdom
with the crowd, we're trying to make it
like the alpha go of like finance,
something that it's just that performs
at like a super human level
in ways you don't really understand.
Interesting.
And you're aggregating the predictions together
in some way.
Yeah, it's actually fairly simple.
We, I mean, people submit their predictions
which are just a number between zero and one
for every stock.
And it's basically just a rank ordering of stock.
And we just normalize everyone's predictions
and then just weight them by their stake
and then just average them together
and to do another renormalization
so that it's the right scale
and sort of distributional shape
to be fed to the optimizer.
But it's a fairly simple and robust way to weight things.
We're basically just using people's express confidence
in their model as the weighting system.
And because there's this feedback of payments
and paying out people, good models,
their stakes increase over time
so their weight in the meta model increases over time
and bad models, their weights decrease over time.
So it's kind of like a human in the gradient descent
for doing like gradient descent
with the stakes as the weights in the model.
Fascinating.
And can you give us any intuition
on how that model is tuned
and what kind of penalty you're using?
Are you using just the stakes
or also the previous performance?
So no, we're not actually using the previous performance.
It's really just stakes.
The previous performance only enters into the fact
that the good performance of the past
would have made their stake grow over time.
And but we have thousands and thousands of models now.
And so any one model is only a very small percentage
of the meta model.
And even the ones that are the biggest
maybe only a couple percent of the total meta model.
And it's, it is a sort of like power law distribution.
There is a lot of work that I've done
in the portfolio optimization set.
And that's the going from the signal to the portfolio.
And there is actually a lot that goes on in there as well
just in how you construct a portfolio,
how you determine how much you're gonna trade each week
and how you make your portfolio, what you make exposed to.
Exposed really just means is are the weights
of your portfolio correlated with lots and lots of things?
And so there's a lot we go due to try
to make the portfolio weights not correlated
with the market overall.
So we're a market neutral hedge fund.
So we try to be uncorrelated to the market,
have a beta of zero.
So when the market goes up or down,
you can't really tell how we would do on that kind of day.
And but we also try to be uncorrelated to lots
of other things that we think could drive returns.
So we try not to have like big country biases,
big sector biases, factor biases.
So factor are things like value and momentum,
these kind of like more abstract quantities
that are supposed to tell you something
about classes of stocks.
But we try to be uncorrelated to basically everything.
And they're just trying to get the sort of pure machine
learning non-linear signal that is driving stock returns
or like stock specific alpha,
we call it sort of like the amount of stock is going to,
how all the stock is going to do sort of just by itself,
not taking all these other things
that are about it into account.
Yeah, that's really interesting.
And I guess like one of the problems on Kaggle
is that most of the solutions are so overfit
to the training set that they never generalize
to real world versions of the problem.
But what you're doing actually is to kind of like
remove away a lot of those opportunities for overfitting
and also allowing the models to be used again
when the next thing comes around.
But just quickly on the aggregating stuff
the reason I'm interested in that is on my PhD
I did prediction with expert advice
and there's a whole load of theoretical approaches
to that where you can have an aggregating algorithm
that produce, you know, that produces performance
or a kind of like an error bound
which is not much worse than the best path
of switching experts.
So if you took the optimal path of the best expert
every single time step, you can have algorithms
that have approvable bound not much worse than that.
Yeah, we, so we've done a lot to try to experiment
with trying to improve upon stake waiting.
And it's always been really hard to do it in a robust way.
It's, I mean, for one, stake waiting is,
it's sort of nice in that it's easy for people to understand.
People are, it's very clean in how it works.
It sort of fits with the ethos
and the, like, and the idea of the company
of how it's distributed and decentralized
and you express your confidence by your stake.
But it does sort of seem like there should be a better way
to aggregate models.
But pretty much every time we try to find something better,
it's, it might be a little bit better
but it's like less robust.
It tends to just be less robust.
And it's, because you are essentially just sort of fitting
to the past and to try to find way to the models
or something, it tends to just like overfit
and this sort of stake waiting thing,
you can't really overfit.
It's just sort of a property that just sort of evolves
as the tournament goes on
without ever considering like the past performance
and all of these things.
So yeah, it's, it's been kind of interesting to,
so it's one of these things we sort of revisit every year
at some point of like,
let's try to build a better meta model
but we usually just come back to stake waiting in the end.
Yeah, well, in a way, I mean,
we're prediction with expert advice,
you have a learning rate
and I guess you don't even have that problem
because you're just using the stakes as the-
Yeah, the, but yeah, the, I mean, our learning rate.
So I mean, our payout system is the way
we adjust the weights over time.
And so we have done like some simulations to show
that like if, how we reward people,
how that affects their weights over time
and how that affects meta model performance.
So you wouldn't want to have a payout system
that would make the meta model worse over time.
And so yeah, like this, this true contribution idea
that's gradient of the stakes,
we did simulations to show
it does actually improve the meta model over time
to pay out in this way.
It's nothing, I mean, people do things
like take their stakes out, withdraw money.
And so it's not a perfect system.
People entering the tournament,
people, some people entering with a lot of money,
some people entered with not that much money.
And so yeah, it takes time for these things,
all the kind of shake out in real life.
But the overall idea is that we are essentially adjusting
the weights through our payouts
towards this sort of more optimal meta model over time.
Interesting.
So I'm actually very, very interested to give it a go.
And I guess like, first of all,
you could sketch out what the process looks like.
I mean, let's say I had a few hundred dollars
and I wanted to build a model.
And also it's got to be a good model.
Let's face it.
So if I just logged on there
and I built a gradient booster tree model, would that work?
It would actually.
So I mean, it's tabular data
and tabular data is very minimal
to gradient boosted trees.
We have a lot of example models that we have put up
and they're doing quite well.
So basically all you have to do
is you can go to the website
and just download a big zip file
that includes all the data in parquet.
And then you can just open it up in Python
and fit a gradient boosted tree.
When we have example scripts
sort of showing this along with some more interesting types
of pre-processing and other sort of ideas
like feature neutralization.
So I can talk about it in a second.
But yeah, a lot of our sort of standard internal models
use basically gradient boosted trees.
And we are, I mean, we have basically example models running
that and they all have positive correlation with
and true and still true contribution.
So they're actually working out of sample
and performing quite well.
That hasn't all been sort of eaten up
by people using similar enough models.
There's a lot of opportunity
to make sort of unique models too.
Cause one thing that's sort of unique about our tournament
is we release actually several targets,
we release 20 something targets.
And they're all constructed
in somewhat slightly different ways.
And you can find that if you train on a different target,
it might work almost as well as training
on the target that you're scored on.
And it might also ensemble really well
with a model trained on different targets.
And so you can actually create ensembles fairly easily
just by training on different targets.
Because it is kind of remarkable
that a model trained on a different target
can actually work better on the target you're interested in.
But that kind of thing, yeah, definitely does happen.
I mean, part of it is called all the correlations are so low,
but some targets might just sort of have a better property
in making your model pick up on the actual signal
that you want to model rather than sort of variance
that is like not that you don't want to model.
Interesting.
I think one of the issues is you might not know
what models that people are using.
But I wondered if you did have any intuition,
I'd be fascinated to know,
are they using very complex models?
Are they using simple models?
From talking to participants, there was a huge range.
There are some people using like extremely simple trees.
There are some people who are using
incredibly elaborate neural networks
with very sort of custom architectures
that are sort of designed to the problem.
There, yeah, there's a whole huge, right?
I mean, there's people who have huge ensembles.
There's people who are doing kind of like online learning
where their model is actually using the features
that were released that week
and sort of using that in some sort of unsupervised learning
and then so they take some while from when we released,
they can't just like run their model
through the new set of features.
They have to incorporate this new set of features
in this unsupervised way before they can,
so yeah, there's an incredible variety
of techniques people are using.
Fascinating, and how big is this parquet for?
How many rows, how many fields
and are they all just real numbers between naught and one?
So yeah, so there's, how many features are we up to now?
We have a couple thousand features roughly
and there's a few million, a couple million rows, I think.
So one sort of additional piece of structure in the data
is there's these things called eras
and the eras are essentially just the weeks
and because the competition has this structure
of we're making predictions every week
and so within each era, there's like say 5,000 rows
which are basically like 5,000 stocks
and so one sort of interesting thing is you are,
you want your model to be good across eras,
not necessarily across samples
and so it creates a different structure
in how you think about the problem
because you want your model to be consistently good
in every era and that can give you a different solution
that if you just try to say maximize some metric
over the whole training set which is kind of, yeah.
But yeah, it is basically just a big parquet file.
We do divide it into like training
and like there's like a testing set
but yeah, do you have any specific questions
about how that is organized?
Well, again, I'm really interested
because on my PhD, I did a whole bunch of prediction models
on financial data sets.
I was predicting like the implied volatility
of the Black Shells formula on some futures data
but my big thing at the time was I was fascinated
by regimes in financial data
and you get these changing dependencies with time
and what I did, I mean, you could actually visualize it
if you build a load of expert models
on different regimes and then you get them to predict
on the other regime's data.
You get this kind of self-similarity matrix
and it looks like you get this kind of structure in there
because there are certain regimes
where this particular model actually predicts
quite far out into the future
and then it might suddenly go dead
so you get these kind of squares
and I had this big thesis that if I have expert models
and use prediction with expert advice,
then when we come into a new regime,
I would quickly learn which experts are the good ones
and I had this thesis that sometimes old information
is very helpful in the future more so than using
like a simple sliding window ridge regression or whatever
and it turned out I was wrong.
It's almost always better just to use
a sliding window regression but yeah, it's fascinating.
It's, yeah, it's interesting.
Like the, you definitely want to train on a lot of data
for these models.
It definitely, like if you just use the prior one year
of data, your models are gonna be pretty crap.
It definitely helps to use like prior 10 years of data
and so it is, you're using actually quite old data often
in predicting into the future but generally, yeah,
if you were only just using the last year or two of data,
your models are gonna have to actually quite a hard time.
Yeah, one other thing about the features I wanted to say
is they are between zero and one, they're in five bins.
There's zero, 0.25, 0.5, 0.75 and one.
So the data has been like binned in this way
and the targets are also binned in this,
the same sort of bins but with a different distribution.
The targets have like in their extreme bins,
only like 5% of the values in the next two extreme bins.
Like what is it, 20 in each of them
and then 50% as a zero.
Interesting.
But all the features are basically just 20%
in each of the bins.
And so the binning is a pretty strong form of regularization.
It sort of prevents you from like a tree from splitting
sort of any arbitrary place you can only split at these things
and so that kind of forces at least some of the space
to be at different splits.
And that regularization, it's kind of,
you would think that having continuous features
would be a lot really helpful but I mean,
it's really not.
It's kind of remarkable how lossy
some of these transforms are that we do
that actually seem to be helpful.
Yeah, so it's so interesting.
And I guess like one thing I didn't really appreciate
at the time is you know, we were just talking
about these complex dynamical systems
like the brain or like financial markets
and there's of course the market efficiency hypothesis
and perhaps one of the reasons why old information
might not be salient is because if the underlying system
is actually taking a trajectory
through this kind of complex space,
then you might argue that almost regardless
of where you traverse, you'll always be in a novel situation.
And then there's this continuum
of regularity versus chaos.
So like for example, if you're predicting options futures
when they get close to maturity,
the volatility just goes crazy
and they just become increasingly unpredictable.
And I guess the art in this kind of data is knowing
when you're in a regime which has some regularity
and when you're not.
It's yeah, it's tricky.
Cause like ideally we want our model,
we want our meta model to sort of work well in any regime
and it does seem to work pretty well consistently.
And but what you do find on like the leaderboard
tournament participants, you'll see some people
who stay at the top of the leaderboard
for weeks and weeks and weeks and weeks,
then suddenly precipitate fall like down the leaderboard
as demonstrating some sort of regime effects.
One really kind of interesting thing I did was
I fit like a mixture of linear models to the data.
So if you fit just like a mixture of two linear models
where it's sort of selecting which eras to use
for which of the two linear models,
you basically, one linear model will get
about 60% of the errors,
one will get about 40% of the errors
and their weights will be almost mirror images
of each other.
And this just comes out like that is the optimal fit
for roughly for 40% of the errors
that basically completely the opposite of the other eras.
Which yeah, demonstrating some like,
that's why markets are extremely hard
cause like something that works well a lot of the time
it suddenly would just work really oppositely horribly.
And so you're often trying to just split this difference
to find something that doesn't work super well at one time
and then we'll like crater at another time.
That's the meta model wants to kind of work
really like pretty good all the time.
And that's one of the things that ensembling
all these models that maybe even the individual models
probably have a lot more regime characteristics
than this overall meta model.
I wondered whether folks were using
some really esoteric approaches.
I mean, I'm interested in geometric deep learning
and algorithmic reasoning and, you know,
even think like esoteric options like cellular automata.
Do you see anything like that getting traction
or it may be even discrete program synthesis?
I don't know.
Cause yeah, like I only see what people
are willing to post and share on forums.
And there's quite a bit of sharing
on our forums of information,
but there's definitely some people at the top of leaderboards
who are doing something that's working quite well for them
for quite a long time that they haven't shared.
And so it's, I'm not even sure what all the people are doing,
but there are, I mean, people allude to using like tricks.
I mean, that they've learned in different jobs.
I mean, we have some people with like a variety of backgrounds.
It's been really cool to like see this community grow
and have people who are like astrophysicists,
particle physicists, people who are doing like
like computer vision and whatever
sort of techniques they've learned in their different fields
and try to use them on this problem.
That was what sort of attracted me as like,
I was doing like computational neuroscience
and I saw this problem as like,
oh, this is a complete free playground.
You can do whatever you want.
And so it was a fun opportunity to try out ideas
that wouldn't really work well in computational neuroscience.
Yeah, indeed.
And physics, I mean, the road to reality by Roger Penrose,
I think it was Michael Bronstein who said
that if you could summarize the entire book in one word,
it would be symmetry.
And there's also another key idea from a lot of researchers,
which is abstraction, you know,
which is like some meta property
of the relationship between data.
So, you know, you probably have lots of folks
coming in from different fields
and they have some very, very interesting approaches
to solving this problem.
Yeah, for sure.
Yeah, I mean, I have, I mean, there's people
who use some like interesting like auto encoders
to try to learn structure from data
as a way to learn features.
People using, it's interesting non-linear
dimensionality reduction techniques
to try to, yeah, to try to find various features.
It's, and yeah, even some,
some things people do do some sort of interesting
feature selection or denoising types of things
that they've learned in their fields.
Yeah, it's always interesting to me to see like
how different fields that use machine learning
use it in different ways
and what sort of tricks and tips might cross over.
I was going to ask about that
because you have loads and loads of features
and there's this problem called the curse of dimensionality.
Right, so, you know, when the number of dimensions increases
the volume of the space increases exponentially,
which means like this concept of nearness basically disappears
and there's statistical models don't work anymore.
So, you know, presumably people would do things like,
I don't know, dimensionality reduction feature selection.
I mean, neural networks are quite clever
in the sense that they, via a variety of methods,
overcome the curse of dimensionality
by learning some data manifold or whatever.
But, you know, it's with natural data,
it's not with financial data, so it's not a given.
It's, yeah, and this is actually one of the things
that was really intriguing to me
when I started in finance is,
so in science, when you're doing regressions
you're trying to find often sparse solutions.
You're trying to find the sort of small number of variables
to predict your targets,
to try to find whatever sort of maybe
causal relationships there are.
In finance, we often try to do exactly the opposite,
where we want our models to care about all the features
a little bit.
And so, we do, we'll do something like what we call
a feature neutralization,
where basically you take your prediction,
take the linear model of your prediction
from the features and subtract it off.
And so, you're making your prediction
not linearly correlated or linearly dependent
on any of your features.
We're doing some fraction of that.
So, just trying to remove too strong of a linear relationship
between a feature and your prediction.
And you do other regularization techniques
like in your tree learning,
maybe one thing that works quite well
is using like column sample by tree,
instead of to very low value.
So, each tree is only considering
a small subset of features.
And so, your ensemble is sort of,
you use as a lot of the different features
because it's sort of each tree
only has access to 10% of the features
across your whole ensemble.
You are probably using a lot of your features a little bit.
And that tends to work quite well.
And the reason is,
it's because features will work for a while
and then they'll just turn around on you.
And so, you don't want to be sort of
super dependent on any one feature.
And so, yeah, it does make the cursor dimensionality
kind of worse in some ways
because you don't wanna necessarily find
just a small subset of variables
that are the best
because sometimes that will maybe give you
a really good model for a while,
but sometimes all of a sudden,
those will just turn around on you.
And then your model just like is almost anti-correlated
where it should be.
Yeah, it's so interesting.
You know, like this problem with the changing dependencies.
So, essentially you're modeling a non-stationary process
which makes it much harder.
And when I was speaking with Sarah Hooker the other day,
she was talking about fairness and bias in models.
And part of the problem there is,
we optimize for headline metrics like accuracy.
And when you decompose the training set
into let's say different categories like men and women
and people who live in London,
the accuracy is very stratified.
It might perform very badly for people that live in London,
but very good for people that live in New York.
You know, and then you start getting into the situation
of saying, okay, well, I'll build an ensemble of models
that are independently optimized for all the different things.
But then you have this impedance mismatch
between this global, you know,
accuracy that you were optimizing for
and are on the benchmarks.
Yeah, no, it's a really interesting property
of these things is, yeah,
especially classification models
where they will work well for some categories
and not others.
And it can be sort of tricky to find out why is like,
are those features just more discriminative
or like, are these classes somehow harder to tell apart
just in some way?
It's, yeah, it's,
but I'm glad people are starting to like look at
and try to dig into some of these like details
rather than just looking at headline metrics.
And I'm also sort of happy that the field is sort of moving
to like this out of distribution learning
is becoming a much more interesting topic.
Because like, that is what really matters
in making machine learning
that is going to affect the real world
is it needs to work out of distribution,
out of your sort of training
and test split distribution as well as possible.
And like how you do that is,
I mean, still very much an open question clearly.
And how well you could potentially do that
is even still an open question.
But that is one of the,
I mean, that is sort of what true intelligence is
to something like humans are pretty good
at adapting out of distribution.
And what is it about us?
What are like, how are we able to do that?
And how do we make our sort of machine learning systems
work better that way?
How are we sort of able to?
I mean, yeah, I think it probably has something to do
is we're able to learn sort of causal structures
that work well.
And the distribution can be very different,
but the sort of causal structures remain.
And we're able to somehow infer that causal structures
from data, from just our sense data and our world models.
And yeah, basically the question is,
how do we make our machine learning systems
be able to do similar sorts of things?
Yeah, this has been absolutely amazing.
Do you have any final thoughts?
Where can people find out more information
about you, Michael?
So, let's see.
Well, so I want to point people first to just like Numeri,
N-U-M-E-R.AI is the website.
I am fairly active in the forums
and the rocket chat we have,
which is sort of just our own personal chat service
for tournament participants to communicate with each other.
And I occasionally only post some of the forums there.
That's probably the best way to like get in contact
to just message me on rocket chat.
And yeah, so that's, yeah,
there's probably that's way to get in contact.
My also, my email is mdo at Numeri.ai.
And I would, yeah, I really love if people come,
check out the tournament, give feedback,
and start participating.
I've, yeah, I found that it was a lot of fun as a participant.
And yeah, I joined the company partly
so I was starting to make more money
during the tournament than I was at my job in science.
And so, yeah, it's a pretty fun hobby and side gig
and potentially even quite lucrative.
Amazing.
Well, Dr. Michael Oliver, it's been an absolute honor.
Thank you so much for joining us this evening.
Thanks for so much for having me.
It's been so much fun.

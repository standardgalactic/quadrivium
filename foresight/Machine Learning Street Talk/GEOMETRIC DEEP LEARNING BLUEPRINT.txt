So, you're talking about the symmetries in data.
Could these analogies also be represented using the kind of symmetries that you're talking about?
Good question. Let me think of it a bit.
Modern machine learning operates with large, high-quality data sets,
which together with appropriate computational resources,
motivates the design of rich function spaces with the capacity to interpolate over the data points.
Now, this mindset plays well with neural networks since even the simplest choices of inductive
prior yield a dense class of functions. Now, symmetry, as wide or narrow as you may define its
meaning, is one idea by which man through the ages has tried to comprehend and create order,
beauty, and perfection. And that was a quote from Hermann Weil, a German mathematician
who was born in the 19th century. Now, since the early days, researchers have adapted neural
networks to exploit the low-dimensional geometry arising from physical measurements,
for example, grids in images, sequences in time series, or position and momentum in molecules
and their associated symmetries, such as translation or rotation.
Now, folks, this is an epic special edition of MLST. We've been working on this since May of this
year, so please use the table of contents on YouTube if you want to skip around. The show is
about three and a half hours long. The second half of the show, roughly speaking, is a traditional
style MLST episode, but the beginning part is a bit of an experiment for us, maybe a bit of a
departure. We want to make some Netflix-style content, and we've even been filming on location
with our guests, so I hope you enjoy the show and let us know what you think in the YouTube comments.
Many people intuit that there are some deep theoretical links between some of the recent
deep learning model architectures, particularly the ones on sets, actually. And this may be why
so many popular architectures keep getting reinvented. Now, the other day, Fabian Fuchs from
Oxford University released a really cool blog post about deep learning on sets,
elucidating a math-heavy paper that he co-authored with Edward Wagstaff et al.
Now, he wanted to understand why so many neural network architectures for sets resemble either
deep sets or self-attention, because sets come in any ordering. There are many opportunities to
design inductive priors to capture the symmetries. So, raises the question,
how do we design deep learning algorithms that are invariant to semantically-equivalent
transformations while maintaining maximum expressivity? Now, Fabian pointed out that
the so-called Genosi-pooling framework gives a satisfying explanation. Genosi-pooling is when
you generate all of the k-tuples of a set, an average over your target function, on those
permutations. It gives you a computationally tractable way of achieving permutation invariance.
So, rather than computing n factorial combinations of the examples, you compute n factorial divided
by n minus k factorial, which for small k is very tractable. Now, clearly, setting k to n
gives you the most expressive yet the most expensive model, but that would be cool. It would
model the high-order interactions between the examples, but it turns out that deep sets are
this configuration with k equals 1, and self-attention is this configuration with k equals 2.
Now, Fabian also spoke about approximate permutation invariance, which is when you set k to n,
the number of examples, but you sample the permutations. It turns out you don't have to
sample very many of them to get good results. But anyway, if you want to check out that in a
little bit more detail, go and check out Fabian's blog. I've put a link in the video description.
High-dimensional learning is impossible due to the curse of dimensionality. It only works if we
make some very strong assumptions about the regularities of the space of functions that we
need to search through. Now, the classical assumptions that we make in machine learning
are no longer relevant. Now, in general, learning in high dimensions is intractable.
The number of samples grows exponentially with the number of dimensions. The universal function
approximation theorem popularized in the 1990s states that for the class of shallow neural
network functions, you can approximate any continuous function to arbitrary precision by
just stacking the neurons, assuming that you had enough of them. So it's a bit like kind of
sparse coding, if you like. The curse of dimensionality refers to the various phenomena that arise
when analyzing and organizing data in high-dimensional spaces that do not occur
in low-dimensional settings, such as the three-dimensional physical space of everyday experience.
Now, the common theme of these problems is that when the dimensionality increases,
the volume of the space increases so fast that the available data effectively becomes sparse.
This sparsity is problematic for any method that requires statistical significance. Now,
in order to obtain a statistically sound and reliable result, the amount of data needed
to support the result often grows exponentially with the dimensionality. Most of the information
in data has regularities. Now, what this means in plain English is, just like on a kaleidoscope,
most of the information which has been generated by the physical world is actually redundant,
just many repeated semantically equivalent replicas of the same thing. The world is full
of simulacrums. Machine learning algorithms need to encode the appropriate notion of regularity
to cut down the search space of possible functions. You might have heard this idea referred to
as an inductive bias. Now, machine learning is about trading off these three sources of error,
statistical error from approximating the expectations on a finite sample,
and this grows as you increase your hypothesis space. Approximation error, which is how good
is your model in that hypothesis space? If your function space is too small, then the one that
you find will incur a lot of approximation error. And finally, optimization error, which is the
ability to find a global optimum. Now, even if we make strong assumptions about our hypothesis
space, you know, we should say that it should be lip sheets or in plain English, it should be
locally smooth. It's still way too large. We want to have a way to search through the space to get
anywhere. So the statistical error is cursed by the dimensionality. If we make the hypothesis or the
function space really small, then the search space is smaller, but the approximation error is cursed
by dimensionality. So we need to define better function spaces to search through. But how?
We need to move towards a new class of function spaces, which is to say,
geometrically inspired function spaces. Let's exploit the underlying low dimensional structure
of the high dimensional input space. The geometric domain can give us entirely new
notions of regularity, which we can exploit. Now using geometrical priors, which is to say,
only allowing equivariant functions or ones which respect a particular geometrical principle,
this will reduce the space of possible functions that we search through,
which means less risk of statistical error and less risk of overfitting. We should be able to
do this without increasing approximation error, because we should know for sure that the true
function has a certain geometrical property, which will bias into the model. So introducing the
geometrical deep learning proto book. So recently, Professor Michael Bronstein, Professor Joanne Bruner,
Dr. Tako Kohen and Dr. Peta Velichkovich released an epic proto book called Geometric Deep Learning,
Grids, Groups, Graphs, Geodesics and Gages. These researchers are elegantly linking classical
theory and machine learning and geometry and group theory to deep learning, which is fascinating.
Now the proto book is beautifully written, right? It's presented so well, it even has some helpful
margin notes. And honestly, I could read sections of it out loud on MLST with scant need to change a
single word. It's that well written. I mean, it is, there's a lot of maths in there as well,
let's be honest. I can't dodge that. But I often try to impress upon people that if your writing
sounds weird when you say it out loud, then you're probably writing it the wrong way. But these guys
have written it really well. Now they've essentially created an abstraction or a blueprint, as they call
it, which prototypically describes all of the deep learning architectures,
the geometrical priors that they have described so far. They don't prescribe a specific architecture,
but rather a series of necessary conditions. The book provides a mathematical framework to study
this field. And it's essentially a mindset on, you know, how to build new architectures. It gives
constructive, you know, procedures to incorporate prior physical knowledge into neural architectures.
And it provides a principled way to build future architectures, which have not yet been invented.
The researchers have also recently released a series of 12 brilliant lectures
on all of the material in the book. And I've linked these in the video description.
Now, what are the core domains of geometric deep learning? So in geometric deep learning,
the data lives on a domain. This domain is a set. It might have additional structure,
like a neighborhood in a graph, or it might have a metric such as, you know, what's the
distance between two points in the set. But most of the time, the data isn't the domain itself.
It's a representation or it's a signal, which is on a Hilbert space. Let's talk about symmetries.
Symmetries are really important to understand this framework. So a symmetry of an object
is simply a transformation of that object, which leaves it unchanged. Now, there are
many different types of symmetries in deep learning. I mean, for example, there are symmetries
of the weights. If you take two neurons in a neural network, and you swap them,
the neural network is still graph isomorphic. There are symmetries of the label function,
which means that an image is still a dog, even if you apply a rotation transformation to it.
Actually, if we knew all of the symmetries of a certain class, we would only need one labeled
example, right, because we would recognize any other examples that you give it as kind of
semantically equivalent transformations. But we can't do that, right, because the learning problem
is difficult, which means we don't actually know all of the symmetries in advance. Now,
in the context of geometric deep learning, we talk about symmetries of the core structured
geometric domains that we're interested in. So grids or graphs, for example, a symmetry is any
transformation which preserves the structure of the geometric domain that the signal lives on.
So for example, permutations of a set preserves the set membership or Euclidean transformations
like rotations or reflections preserve distances and angles. There are a few rules to remember,
because the way we deal with this paradigm is we talk about how composable those symmetries are.
The identity transformation is always a symmetry. Composing a symmetry transformation is always
a symmetry. The inverse of a symmetry is always a symmetry. We can formulate this with this
mathematically abstract notion of a group. Group theory in mathematics is fascinating,
because it concerns only with how elements compose with each other, not what they actually are.
So different kinds of objects may have the same symmetry group. For example,
the group of rotational and reflection symmetries of a triangle is the same as the group of permutations
of sequences of three elements. So let's talk about the blueprint itself. The blueprint has
three core principles. Symmetry, scale separation and geometric stability. In machine learning,
multi-scale representations and local invariance are the fundamental mathematical principles
underpinning the efficiency of convolutional neural networks and graph neural networks.
They are typically implemented in the form of local pooling in some sense. Now these principles
give us a very general blueprint of geometric deep learning that can be recognized in the
majority of popular deep neural network architectures. A typical design consists of a sequence of
locally-equivariant layers. I mean, think of the convolution layers in a CNN, then a pooling or
a coarsening layer. So you recognize those in CNNs as well. And finally, followed by a globally
invariant pooling layer. So that might be your classification head. Now these building blocks
provide a rich approximation space, which have prescribed invariance and stability properties
by combining them together into a scheme that these researchers refer to as the geometric
deep learning blueprint. Now the researchers also introduced the concept of geometric stability,
which extends the notion of group invariance and equivalence to approximate symmetry or
transformations around the group. They quantify this in some sense by looking at a metric space
between the transformations themselves. This is Professor Michael Bronstein.
The problem is that traditional machine learning techniques work well with images or audio,
but they are not designed to deal with network structure data. In order to address this challenge,
we've developed a new framework that we call geometric deep learning. It allowed us to learn
the network effects of clinically approved drugs and to predict anti-cancer drug-like
properties of other molecules. For example, molecules contained in food. Neural networks
have exploded, leading to several success stories in industrial applications. And I think it's
quite indicative that last year two major biological journals featured geometric deep learning papers on
their cover, which means that it has already become mainstream and possibly will lead to new
exciting results in fundamental sciences. The book I hold is called The Role to Reality. It's
written by a British mathematician and recent Nobel laureate, Roger Penrose, a professor at Oxford,
and it's really probably one of the most complete attempts to write and describe modern physics
and its mathematical underpinning. And you can see it's very heavy. But if I were to compress the
thousand-plus pages of this book into just a single concept, I can capture it in one word.
And this is symmetry. And symmetry is really fundamental concept and fundamental idea that
underpins all modern physics as we know it. So, for example, the standard model of particle physics
can entirely be derived from the considerations of symmetry. And that's the kind of idea that
we try to use in deep learning to derive and create new neural network architectures entirely from
fundamental concepts and fundamental principles of symmetry. In the past decade, deep learning has
brought a revolution in data science and made possible many tasks previously thought to be
unreached. On the other hand, we now have a zoo of different neural network architectures for
different types of data, but few unifying principles. The authors also point out that
different geometric deep learning methods differ in their choice of domain or symmetry group or the
implementation specific details of those building blocks that we spoke about. But many of the deep
learning architectures currently in use fall into this scheme and can thus be derived from common
geometrical principles. As a consequence, it is difficult to understand the relations between
different methods, which inevitably leads to the reinvention and rebranding of the same concepts.
So, we need some form of geometric unification in the spirit of the Erlangen program that I call
geometric deep learning. It serves two purposes. First, to provide a common mathematical framework
to derive the most successful neural network architectures. And second, to give a constructive
procedure to build future architectures in a principled way. This is a very general design that
can be applied to different types of geometric structures such as grids, homogeneous spaces
with global transformation groups, crafts and manifolds where we have global isometry invariants
as well as local gauge symmetries. We call these the 5G of geometric deep learning. The
implementation of these principles leads to some of the most popular architectures that exist today
in deep learning, such as convolutional networks emerging from translational symmetry, craft neural
networks, deep sets and transformers implementing permutation invariants and intrinsic mesh CNNs
using computer graphics and vision that can be derived from gauge symmetries. People are quite
cynical about the interpolative nature of deep learning and I think that finding this structure,
this deeper structure could allow us to extrapolate in a way which is significantly better than we
can now. And I asked whether he thought deep learning could get us all the way to artificial
general intelligence? It's a hard question because it has several terms that are not well defined.
What do you define by intelligence? So we don't understand what is human intelligence. Everybody
probably gives a different meaning to this term. So it's hard for me to even to define and quantify
artificial intelligence. I don't think that we necessarily need to emulate human intelligence
and, as you mentioned, in the past we thought of artificial intelligence as being able to solve
certain tasks and it's a kind of a moving target. We thought of, I don't know, playing intelligent
games or perception of the visual world like computer vision or understanding and translating
language or even creativity and today we have machine learning systems that are able to address
at least to some extent all of these tasks, sometimes even better than humans and are we there yet
artificial intelligence? I don't think so. And probably artificial intelligence will look
differently from human intelligence. It doesn't need to look like human intelligence. It's of
course an interesting scientific question whether we can reproduce a human in silico, but for solving
practical problems that will make this technology useful for the humanity, for the humankind,
we probably need something different. It will certainly involve a certain level of abstraction
that we currently don't have. It will probably require methods that we currently don't have,
but it doesn't necessarily need to look like a recreation of a human. This is Dr. Petar Velichkovich.
By now you'll have probably seen or heard something about our recently released proto book on
geometric deep learning on grids, graphs, groups, geodesics, and gauges, or as we like to call it
the 5Gs of geometric deep learning, which I've co-authored alongside Michael Bronstein,
John Brunner, and Taco Cohen. And you might be wondering what all the fuss is about, because
there's already a lot of really high quality synthesis textbooks on the field of deep learning
in general, and also on some sub areas of geometric deep learning, such as graph neural
networks, where Will Hamilton recently released a super high quality textbook on that area.
This is Dr. Taco Cohen. So what we've been trying to do in our book project is to show
that this geometric deep learning mindset is not just useful when tackling a new problem,
but actually allows you to derive from first principles of symmetry and skill separation,
many of the architectures and architectural primitives like convolution, attention,
graph convolution, and so forth, that have become popular over the last few years.
Even in cases where these considerations of active variance and skill separation
were not felt at center when the methods were first discovered.
Now we think that this is useful for a number of reasons. First of all, it might help to avoid
reinventing the same ideas over and over. And this can easily happen when the number of papers
that come out every day is far larger than what any one person can possibly read,
and when different sub fields use different language to describe their ideas. Furthermore,
it might help to clarify when a particular method is useful. A geometric deep learning method is
useful when the problem domain has the particular symmetries that are built into the architecture.
And finally, we hope that by making explicit the commonalities between seemingly different
methods, it will become easier for new cars to learn geometric deep learning. Ideally,
one would not have to go through the large number of architectures that have been designed,
but just learn the general ideas of groups,
equivariance, group representations and feature spaces and so on, and then see,
for the particular instances, you're interested in how that fits into the general pattern.
To really illustrate why do we think that such a synthesis is important and relevant for
deep learning research going forward, we have to go way back, way back in the time of Euclid,
around 300 years BC. And as you might know, Euclid is the founding father of Euclidean geometry,
which for many, many years was the only way to do geometry. It relied on a certain set of postulates
that Euclid had that governed all the laws of the geometry that he proposed. All of this started
to drastically change around the 1800s when several mathematicians, in an effort to prove
that Euclid's geometry is the geometry to be following, ended up assuming that one of the
postulates is false and failing to drive a contradiction. They actually ended up deriving
a completely new set of self-consistent geometries, all with their own set of laws and rules,
and also quite differing terminologies. Among some of these popular variants are the hyperbolic
geometry of Lobachevsky and Bolyai, and the elliptic geometries of Riemann. And for a very long time,
because all of these geometries had completely different sets of rules and they were all self-consistent,
people were generally wondering what is the one true geometry that we should be studying.
A solution to this problem came several decades later through the work of a young German mathematician
by the name of Felix Klein, who had just been appointed for a professorship position at the
small Bavarian University of Erlangen, the so-called Friedrich Alexander University in Erlangen,
Nuremberg. While he was at this post, he had proposed a direction that would eventually enable
us to unify all of the geometries that were in existence at the time through the lens of
invariances and symmetry using the language of group theory. And his work is now eponymously
known as the Erlangen program. And there is no way to overstate how much of an important effect
the Erlangen program had on mathematics and beyond. Because of the fact that it provided
a unifying lens of studying geometry, suddenly people didn't need to hunt for the one true
geometry they had a blueprint they could use to drive whatever geometry was necessary for the
problem they were solving. And besides just mathematics, it had amazing spillover effects
to other very important fields of science. For example, in physics, the Erlangen program spilled
over through the work of Emy Nerther, demonstrated that all of the conservation laws in physics,
which previously had to be validated through extensive experimental evaluation, could be
completely derivable through the principles of symmetry. And needless to say, this is a very
fundamental and game changing result in physics, which also allowed us to classify some elementary
particles in what is now known as the standard model. Thinking back towards theoretical computer
science, the Erlangen program also had a spillover effect into category theory, which is one of the
most abstractified areas of theoretical computer science with a lot of potential for unifying
various directions in mathematics. And actually in the words of the founders of category theory,
the whole field of category theory can be seen as an extension of Felix Klein's Erlangen program.
So the Erlangen program demonstrated how it's possible to take a small set of guiding principles
of invariance and symmetry and use it to unify something as broad as geometry. I like to think
of geometric deep learning as not a single method or architecture, but as a mindset. It's a way of
looking at machine learning problems from the first principles of symmetry and invariance.
And symmetry is a key idea that underpins our physical world and the data that is created by
physical processes. And accounting for this structure allows us to beat the curse of dimensionality in
machine learning problems. It is really a very powerful principle and very generic blueprint.
And we find its instances in some of today's most popular deep learning architectures,
whether it's convolutional neural networks, graph neural networks, transformers, LSTMs and many
more. And it is also a way to design new machine learning architectures that are yet to be invented,
maybe in the future, not based on back propagation and incorporate inductive bias in a principled way.
Being a professor and a teacher, I would also like to emphasize the pedagogical dimension of
this geometric unification. What I often see in deep learning when deep learning is taught is
it appears as a bunch of hacks with weaker or no justification. And I think it is best illustrated
with how, for example, the concept of convolution is explained. It is often given as a formula just
out of the blue, maybe with a bit of hand waving. But what we try to show is that you can derive
convolution from first principles, in this particular case, of translational symmetry.
And I think the difference in this approach is best captured by what Elvetsos once said
that the knowledge of principles easily compensates the lack of knowledge effects.
Professor Bronstein has been a professor at Imperial College in London for the last three years
and received his PhD with distinction from Technion, the Israeli Institute of Technology,
in 2007. He's held visiting academic positions at MIT, Harvard and Stanford,
and his work has been cited over 21,000 times. His main expertise is in theoretical and computational
geometric methods for machine learning and data science, and his research encompasses a broad
spectrum of applications ranging from computer vision and pattern recognition to geometry processing,
computer graphics and biomedicine. Professor Bronstein coined and popularized the term
geometric deep learning. His startup company, Fabula AI, which was acquired by Twitter in 2019,
was one of the first applications of graph ML to the problem of misinformation detection. I think
it's no exaggeration to say that Professor Bronstein is the world's most recognizable expert
in graph representation learning research. We are really probably some of the nicest locations
in London, which is Kensington, if you're familiar, so we have the the Natural History Museum, the Science
Museum and the Victoria and Albert Museum and Imperial College is right here, so I think it's
as central as you can get. And if you walk all the way there, then you have Hyde Park,
which is probably one of the nicest parks in London. I'm a professor in the department of
computing at Imperial College London and head of graph learning research at Twitter.
I work on geometric deep learning in particular on graph neural networks and their applications from
computer vision and graphics to computational biology and drug design. Dr Petar Velichkovich
is a senior research scientist at DeepMind in London and he obtained his PhD from Trinity College
in Cambridge. His research has been focused on geometric deep learning and in particular
devising neural network architectures for graph representation learning and its applications
in algorithmic reasoning and computational biology. Petar's work has been published in the
leading machine learning venues. Petar was the first author of graph attention networks,
a popular convolutional layer for graphs and deep graph infomax, a scalable unsupervised
learning pipeline for graphs. Hi everyone, my name is Petar Velichkovich and I'm a senior
research scientist at DeepMind and previously I have done my PhD in computer science at the
University of Cambridge where I'm actually still based and today we're actually here in Cambridge
filming these shots and it is my great pleasure to be talking to you today about our work on
geometric deep learning and related topics. I first got into computer science through competitive
programming contests and classical algorithms the likes of which you might find in a traditional
theoretical computer science textbook and this was primarily influenced by the way schooling
worked for gifted students back in my hometown of Belgrade in Serbia where students were generally
encouraged to take part in these theoretical contests and try to write programs that are just
going to finish as fast as possible or work as efficiently as possible over a certain set of
carefully contrived problems. All of this changed when I actually started my computer science degree
here at Cambridge where I was suddenly exposed to a much wider wealth of computer science topics
than just theoretical computer science and algorithms and for a brief moment my interests
drifted elsewhere. Everything started to come back together when I started my final year project
with Professor Pietro Leo at Cambridge and I had heard that bioinformatics is a topic that's
brimming with classical algorithms and competitive programming algorithms specifically so I thought
a project in this area would be a great way to bring these two closer. Unfortunately that was not
to be as my mentor very quickly drew me into machine learning and that kind of spiraled out into my
PhD topics where I was for a brief moment focused on computational biology topics before
eventually drifting to graph representation learning and eventually geometric deep learning.
My journey into geometric deep learning started actually through investigating
graph representation learning which I think for a very long time these two areas have been seen
as almost synonymous with one another because almost everything you come up with in the area
of geometric deep learning can be if you squint hard enough seen as a special case of graph
representation learning. What originally brought me into this was an internship at Montreal's
Artificial Intelligence Institute Miele where I worked alongside Joshua Bengio and Adriana Romero
on initially methodologies for processing data that lives on meshes of the human brain.
We found out that the existing proposals for processing data over such a mesh
both in graph neural networks and otherwise were not the most adequate for the kind of data
processing that we needed to do and we needed something that would be aligned more with image
convolutions in spirit in a way that allows us to give different influences to different
neighbors in the mesh and this led us to propose graph attention networks which was a paper that
we published at Eichler 2018. It was actually my first top tier conference publication and
what I'm probably most well known for nowadays. The field of graph representation learning has
then spiraled completely out of control in terms of the quantity of papers being proposed.
Only one year after the graph attention network paper came out I was reviewing for
some of the conferences in the area and I found on my reviewing stack four or five papers that
were extending graph attention nets in one way or another so the field certainly has become a lot
more vibrant because of a nice barrier of entry which is not too high. Recently Pettai has been
doing some really interesting research in algorithmic reasoning. Part of the skill of a software
engineer lies in choosing which algorithm to use only rarely will an entirely novel algorithm be
warranted. The key guarantee which traditional symbolic algorithms give us is generalization
to new situations. Traditional algorithms and the predictions given by deep learning models
have very different properties. The former provides strong guarantees but are inflexible
to the problem being tackled while the latter provide few guarantees but can adapt to a wide
range of problems. Now Pettai in his paper proposed a neural architecture which can take in natural
inputs but output a graph of abstract outputs as well as natural outputs. Pettai believes that
neural algorithmic reasoning will allow us to apply classical algorithms on inputs that they were
never originally designed for. I am studying algorithmic reasoning which is a novel area of
representation learning that seeks to find neural networks that are as good as possible at
imitating the computations of exactly the kind of classical algorithms that initially brought me
to computer science. It turns out that this area is remarkably rich and could have remarkably big
implications for machine learning in general because it could bring the best of the algorithmic
domain into the domain of neural networks and if you look at the pros and cons of the two you'll
find that they're very complementary. So the fusion of the two can really bring the kinds of
benefits we haven't seen before. So I am very pleased to say that I'm among those researchers
that is extremely proud and happy of what I do because it brings together some of my earliest
passions in computer science with the latest trends in machine learning and especially
geometric deep learning which we recently released a proto book about with Joan, Michael and Taco.
We spoke to Christian Saagedi and he's doing some interesting work with algorithmic reasoning
creating abstract syntax trees to represent mathematical theorems for example and then
he believes that in that representation space he projects it all into a Euclidean space that
there's some interesting interpolative points in that space but again surely there must be some
deeper structure which analogizes mathematics which would allow us to extrapolate and discover
new interesting mathematics. It just feels that what we're missing is the right kind of structure.
I think for mathematics it's relatively easy to formalize it because well we can write logic
rules and basically we can build mathematics axiomatically from very basic principles. These
methods are already being used for computer proof of certain theorems. I think it's not
well regarded by the purists in pure mathematics but probably they will need to accept it and well
you know maybe there will be fields medal that will be given for a proof that is done by a computer
I think even recently there are some important breakthrough results proofs that were given by a
computer so it is probably just the beginning of a new way of doing science essentially even
as pure science as creative science as mathematics which was considered really the hallmark of human
intelligence it can be maybe if not replaced assisted by by artificial intelligence.
Petr invokes Daniel Kahneman's system one and system two. He thinks that we need something
like system two to achieve the kind of reasoning and generalization which currently eludes us
in deep learning models. What I'm holding in my hands right now is the international bestseller
on thinking fast and slow from the famous Nobel Prize winner Daniel Kahneman. This book can be seen
as one of the main theses behind my ongoing work in algorithmic reasoning and what it stands for
because it argues that fundamentally we as humans employ two different systems that operate at
different rates system one which primarily deals with perceptive tasks and system two which deals
with longer range reasoning tasks and it is my belief that currently where our research in neural
networks has taken us is to get really really good at automating away system one so being able to
perform perceptive tasks from large quantities of observed data probably in a not too dissimilar
manner from the way humans do it. What I feel is really missing from these architectures nowadays
is the system two aspect being able to actually take these percepts that we've acquired from the
environment and actually properly do rigid reasoning over them in a manner that will stay
consistent even if we drastically change the number of objects slightly perturb the laws of physics
or something like that. In my opinion algorithmic reasoning the art of capturing these kinds of
reasoning computations inside a neural network that was trained specifically for that purpose
and then slotting that neural network into a different architecture that works with raw
percepts is one potentially very promising blueprint that will take the space of classical
algorithms that we have been building in this system two space and carry them over into raw
perceptive inputs which these algorithms were very rarely designed to work over. This is Dr.
Tako Kohen. Hello, I'm Tako Kohen. I'm a researcher at Qualco AI Research and I work on
geometric deep learning, equivariate networks and more recently also on causal inference and causal
representation learning. Now I've been interested for a number of years already since about 2013
in the application of ideas around symmetry, invariance, equivariance and the underlying
mathematics of group theory and group representation theory to machine learning and deep learning
specifically. And so it's been quite exciting to see over the last few years really the blossoming
of this field that we now call geometric deep learning. Many new methods such as various kinds
of equivariate convolutions for different spaces, different groups of symmetries, different geometric
feature types, equivariate transformers and attention mechanisms, point cloud networks,
graph neural networks and so forth. And along with these new methods also a large number of
applications that have been tackled. Anything from medical imaging to the analysis of global
weather and climate data to the analysis of DNA sequences and proteins and other kinds of molecules.
So if you apply this mindset, the first question you ask when faced with such a new problem is
what are the symmetries? What are the transformations that I can apply to my data that may change the
numerical representation of my data as stored in my computer, but that nevertheless don't change
the underlying objects we're interested in. Whether that's reordering the nodes of a graph,
rotating a molecule in 3D or many other kinds of symmetries. Once you know the group of symmetries,
you can then develop a neural network that's equivariate symmetries. And hopefully is a
universal approximator among equivariate functions. What we found, what many others have found time
and again, is that if you build this symmetry prior to your network, if you make your network
equivariate, it is bound to be much more data efficient and to generalize much better.
Hey Tim, great to be here. So yeah, the blueprint. So what we realized as we were writing this book
is that really a lot of different architectures, they can be understood in one as essentially a
special cases of one generic structure that we call the geometric deep learning blueprint.
So to explain a little bit about what this is all about, the blueprint refers to first of all
feature spaces. So I'll explain how we model those. And then it refers to maps between feature
spaces or layers of the network. And they also have to somehow respect the structure of the
feature spaces. So in all cases, whether it's, you know, graph neural net, a network for processing
images on the plane, or a network for processing signals on a sphere like global weather data,
we're dealing with data that lives on a domain. So the domain in the examples I just gave would
be the set of nodes of the graph, or perhaps also the set of edges, the points on the plane,
or the points on the sphere. And of course, you can think of many other examples as well.
This is what we call the domain. We write it as omega. It's typically, it's a set, first of all,
and it may have some additional structure. So in the case of the sphere, it has some interesting
topology. And typically, we also want to think about the geometry, want to think about distances
and angles. So it's a set with some kind of structure. And in addition, it has some symmetries,
meaning there's some transformations we can do to this set, that will preserve the structure that
we think is important. So if we think distances are important on, let's say the sphere, when the
kinds of symmetries we end up with our rotations, three dimensional rotations of the sphere,
perhaps also reflections. In the case of a graph, the symmetries would be permutations
of the nodes and also corresponding permutation of the edges. So you just change the order of
the nodes. If node one and two were connected, you apply permutation and move those to node three
and four, then now three and four have to be connected. So that's a symmetry of our space
overgon. Now the data, this is a very important point, the data are typically not points in this
space. So when we're classifying images, well, our space is the plane, but the data are not
points in the plane, they're not the two dimensional vectors, the data is really a signal
on the plane, a two dimensional image, which so you can think of that as a function from for each
point in the plane, we have a pixel value. So in the more general cases, it might be
something called a field. So you might say have wind direction on on earth, that's a
vector field on the sphere. Now the symmetries that we talked about, they act on this space,
you could show how they automatically also act on the space of signals. So those are the key
ingredients to define what a feature space is, you have your your space, omega, so set of nodes,
sphere, whatever, then you have a group of symmetries of that space. And then you have
the space of signals or feature maps on this space, and you have the group acting on your
signal. So you can rotate a vector field, or you can shift a planar image or etc. So those are the
key ingredients to define a feature space. And then we can talk about layers. And so the layers or
maps of the network, they have to respect this structure. So if we have a signal on on the sphere,
and we believe that rotating it doesn't change it in any essential way, or we permute the nodes in
the graph, but it's still the same graph, then we want the layers of the network to respect that.
And to respect the structure means to be actually variant to the symmetries. So if we apply our
transformation to our signal, and then we apply our network layer, it should be the same as applying
the network layer to the original input, and then applying a transformation in the output space.
Now how the transformation acts in the output space could be different from the input space. So for
example, you could have a vector field as input, and a scalar field as output, they transform
differently. So that's why for each layer and network, you're going to have a different feature
space with a different action of the group. But it's the same group acting in each feature space,
the maps, they should be equivariant. And these maps, they include both the linear maps, which are
typically the learnable layers, and the non linearities. Now for linear maps, you can study all
sorts of interesting questions, you can ask what is the most general kind of equivariant linear
map. And it turns out that for a large class of group actions, or linear group actions, the most
general kind of equivariant linear maps are generalized forms of convolutions. So that's
really an explanation for why convolutions are so ubiquitous. The final ingredient that I think
is key to the success of many architectures is some kind of pooling or cautioning operation.
So the structures we've talked about so far are, you know, this global space and the symmetries on
it, etc. But typically, there's also a notion of distance or locality in our space. And if we
just enforce that our layers have to respect the symmetries, well that would force us to use a
convolution in many cases, and I just mentioned, but not forces to use local filters. And we all
know that using sort of global filters is not going to be very effective use of parameters.
So locality is another key thing, locality in the filters. And also localities exploited via
some kind of pooling or caution operation. Now, going forward to say the year 2020, deep learning
is all the craze right now. And so many different deep learning architectures are being proposed,
convolutional neural networks, graph neural networks, LSTMs and so on. When these architectures are
being proposed, different terminologies are used because people tend to come from different areas
when they're proposing them. And also, they are usually followed by kinds of bombastic statements
such as everything can be seen as a special case of a convolutional neural network,
transformers use self-attention and attention is all you need. Graph neural networks work on graphs
and everything can be represented as a graph. And LSTMs are turing complete, so why would you ever
need anything else? So I hope that this illustrates how the field of deep learning in the year of 2020
is really not all that different from the state of geometry in the 1800s. And if history teaches
anything about how we can unify these fields together, now is the right time for us to look
back, study the geometric principles underlying the architectures that we use. And as a result,
we might just derive a blueprint that will allow us to reason about all the architectures we have
in the past, but also any architectures that we might come up with in the future. And in my opinion,
that is the key selling point of our recently released proto book. And I hope that it is helpful
in guiding deep learning research going forward. I should highlight that I was also extremely,
extremely honored to deliver the first version of the talk presenting our proto book at
Frederic Alexander University of Erlang in exactly the same place where the Erlang program was
originally brought up, albeit because of the existing COVID restrictions, I had to do so
in a virtual manner. So I think a lot of people understand convolutions in the way of a
traditional plain us, CNN, and the mathematical notion of a convolution, which is very closely
related to, you know, a Fourier transform, for example. But graph convolutional networks,
they seem to abstract the notion of a convolution into some kind of concept of the neighborhood
and local connectivity. And some of your work as well with, you know, equivariate convolutional
neural networks, I think do the same thing. So when we talk about convolution, are we talking
about a very abstract notion of it? Yeah, that's a great question. I think that
there are so many different ways to get at convolution. You can think of it in sort of
you've trying to think of it in terms of sliding a filter over some space, right? So you put it
in a canonical position, and then you slide it around. That is an idea that you can generalize
to not just sliding over a plane, but say applying some transformation from a group
to your filter. So let's say you're up, you have a filter on the sphere, then you can,
you have, you know, the sphere has a symmetry group, mainly three dimensional rotations,
group SO3, you can apply an element of SO3, a three dimensional rotation to your filter,
even sort of slide it over, over the sphere. That leads to something called group convolution.
So that's one way to think about it. There is indeed, as you mentioned, the spectral
wave looking at it. So you could think there's the famous Fourier convolution theorem,
which says that in the spectrum, convolution is just a point wise product. So one way to
implement a convolution would be to take a Fourier transform of your signal, your feature map,
and a Fourier transform of your filter, multiply them point wise, and then inverse Fourier
transform to get the result. And this perspective also generalizes. So it generalizes to graphs,
where the Fourier transform, or something analogous to it, can be obtained via graph
laplacians, as well as some other ideas. That is actually historically how some of the first
graph neural nets were implemented and motivated. There's also a spectral theory for group
convolutions. So indeed, the Fourier transform can be generalized, or the standard Fourier
transform that we all know, is actually the Fourier transform for the plane, the plane being
a particular group, the translation group in two dimensions. So there is a whole, a very beautiful
theory of, let's say, Fourier generalized Fourier transforms for groups, where now the spectrum
is indexed not by the just by the integers, as it is the case for the for the for the line or the
plane, but by something called irreducible representations. In the case of the plane,
those are indeed indexed by integers. And the the spectrum is not just scalar valued,
or complex scalar valued, but it can be matrix valued. If you're interested in this sort of
stuff, you can, you want sort of a very high level description, you can check out our paper on
spherical CNNs, where we actually implement convolution on a sphere, using this kind of
generalized Fourier transform. So that's the Fourier perspective on convolutions.
And there's a final perspective, which I think is quite intuitive, which is that
the convolution is the most general kind of equivariant linear map between certain group
between certain linear group actions. So specifically, these group actions are the
way that groups tend to act on the space of signals on your space. So you might have space
of scalar signals on the sphere. And your feature map, you might want to have another scalar signal
in the sphere or a vector field on the sphere or something. Then you can ask what is the most
general kind of equivariant linear map. And the answer is, it's a convolution. And that is also
true in it in a very general setting. So that that to me is the most intuitive way of understanding
convolutions as the most general kind of maps that are linear maps that are equivariate to
certain kinds of group actions. Professor Bruner, welcome to MLST. It's an absolutely
honor to have you on. Introduce yourself. Yeah. Hi, Tim. I'm Joanne Bruner. I'm a associate professor
at the Quran Institute and Center for Data Science at New York University. And I'm very happy to be
here chatting with you at MLST. Joanne, you just released this geometric deep learning proto book.
You know, what does it mean to you? What was your kind of intellectual journey that led to this?
Yeah. So this journey, in fact, started many years ago. So I would say even like during my postdoc,
I was a postdoc. I was doing my postdoc here at NYU with Yanle Kun. And that was the time
2013, where, you know, confnets were already showing a big promise in image tasks. And
discussing with Yan, the questions are, okay, how about domains that are not like grades, right? So,
and that was the beginning of a journey that also included my former collaborator, Arthur Slump,
who was like another researcher in the lab that had a similar background as me, like coming from
applied math, but looking into into into more and more deep learning. So I would say that the
genesis was our first attempt at extending the success of convolutional networks to these
regular domains. And we published the paper at iClear 2014. And after that, I think things started
like quite naturally because other researchers that had, I would say, came from the same background,
like, you know, maybe from geometric background, also started to realize that there was something
there, maybe bigger than these particular papers, in particular, Michael Brownstone,
it's kind of reached out to us just afterwards saying, oh, and we are also looking at similar
ideas. I think we should just team together and start to think about this more globally. And so,
you know, things that developed from there. And we wrote this journal paper, like a review paper
in 2017, with Arthur, Yan, Michael, the airbender guys, who is another very well known figure in
this area. And so from there, I think that, yeah, things like started to slowly take off. We had
tutorials and new ribs that we had a very successful workshop at IPAM. After this, I think that the
thing were clear that, okay, maybe at some point in the future, we should try to stamp all these
things into a book that tries to reflect something a bit more, let's say, mature and, you know, what
is our, if we wanted to have like some kind of legacy for future generations on how to implement
and communicate these methods, how would that be? So I guess that was the genesis of the book. And so
very soon we said with Michael that we also would like to have some, you know, fresh minds and fresh
energy on board. So naturally, the names of Taco and better came very naturally to us as people
who had been doing excellent work in the domain that would very nicely complement our skills.
So the team was created. And that's the project. And so, yeah, I mean, I think it's been very
interesting so far. Of course, I guess that, as you know, this is just an ongoing project, right?
So it's still not finalized. But hopefully we're getting interest from the community. And this
gives us some kind of, I would say positive vibes to finish it on time. As you know, writing books
is always like this never ending process. So I think that, yeah, that's, it's been an
interesting endeavor so far, for sure. I asked Professor Bromstein what his most passionately
held belief is about machine learning. I think machine learning is such a field where
holding strong beliefs is often counterproductive. It happened to me multiple times that something
that I thought or said was very quickly overturned. And what I mean is that milestones or progress
was achieved much faster than I could even imagine in a wild dream. So making predictions about
machine learning is, to some extent, an ungrateful job. I do, however, believe that in order to make
progress to the next level and make machine learning achieve its potential to become the
transformative technology we trust and use ubiquitously, it must be built on solid mathematical
foundations. And I also think that machine learning will drive future scientific breakthroughs.
And probably a good litmus test would be a Nobel Prize awarded for a discovery made by
or with the help of an ML system. It might already happen in the next decade.
I wanted to go into a few questions actually about the book. So first of all, Joanne, in your
2017 paper, The Mathematics of Deep Learning, you cited the universal function approximation theorem,
which is to say the ability of a shallow neural network to approximate arbitrary functions.
But the performance of wide and shallow neural networks can be significantly beaten by deep
networks. And you said that one of the possible explanations is that deep architectures are able
to better capture invariant properties of the data compared to their shallow counterparts.
Could you just briefly introduce the universal function approximation theorem? And do you think
it's still relevant for today's neural networks? Yeah, I mean, that's a very deep and an important
question. Yeah, so universal approximation theorem, it refers to this very general principle that
once you define parametric class, let's say you're you are on a learn functions using neural
nets, it just describes your ability that as you put more and more parameters into your class,
you're going to able to to approximate essentially anything that data nature throws at you. And so
this might seem like a very powerful property. But in fact, in fact, it's it's something that
you have probably already encountered many times during your undergrad. I mean, if you have any,
let's say, background in, you know, single processing electrical engineering, there's many
ways in which students have learned how to represent data, like signals, for example,
using Fourier transform. So Fourier transforms are an instance of a class that has universal
approximation. So in that sense, it's a it's a I think, going back to the second part of your
question, how relevant it is to in the context of neural networks, and how far does this thing
pushes towards understanding why deep learning works. So I would say there's a there's two sides
of the answer. On one hand, I think that universal approximation is is a tool that when you combine
it with other elements, it becomes something that provides good guiding principles. For instance,
universal approximation of a generic function, we know that yeah, we can as I said, we can obtain
it with, you know, very, very naive architectures, for example, just a shallow neural network without
any kind of physical structure, special structure already has this property. Does it actually help
us to to learn very efficiently? No, right, and I'm going to go at this afterwards. But when you
combine it, for example, with, you know, let's say that now your data lives on a graph, or your data,
I don't know, has a certain like a come from a physical lab that has certain properties,
let's say that it's rotational invariant. Like the first thing that that the designer,
like a domain scientist would like to know, if you come there and you design your neural
net, look, I have a neural net that takes your data and has very good performance. The first thing
that he will ask is, okay, how general is your architecture, right? Can it explain anything
that they could throw at you? It seems like it's a, in that sense, I would, I would present it more
as a sufficient condition, like a check mark that your, your, you know, your architecture needs to
fall, right? If you make more and more parameters, can you express more and more elements functions
from your class? But then, as I said, is it far from being sufficient, right? It's like, it's,
sorry, it's a, it's a necessary condition, but it's far from being sufficient in the sense that
universal approximation has a flavor is a result that does not quantify how many parameters
do I need, right? Like, you know, if I want to approximate function, let's say I want to classify
between different dog breeds, it doesn't tell me this theorem doesn't tell me how many parameters,
how many neurons do I need for that, right? It's, it's a statement that in that sense,
it lets, it leaves you a little bit with your, like, say, like, you know, like,
like the, it's a bittersweet result, right? It doesn't, it doesn't really tell you anything
actual. So that's why, and that's why we enter these, these other questions that is actually
much deeper. And to some extent, still reading them pretty much open, that is, how do you actually
go from this, this statement to something that is quantitative, right? Something that tells you,
okay, you know, you need that many layers, you need that many parameters. And so this is where
the role of depth in neural networks is, you know, becomes essentially the key open question.
And, and yeah, so in this, in this quote that you, that you brought from this paper,
that kind of reflected our understanding at the time of maybe the true power of universal
approximation is, you know, when, as you combine it with these other prior, that is the asymmetries
of the data, like the invariances. So I don't want to represent arbitrary functions, I only want to
represent functions that are invariant to certain transformations of the input. So in fact, our,
at least my particular view of the problem, analysis of the problem has somehow evolved in the last
years, right? Of course, through research that I've done together with my collaborators.
And now, and this is actually the way we present it in the book, we, we, we kind of identify two
different flavors, two different sources of prior information that one needs to bake into the problem,
right? To, to really go beyond this like basic approximation result of neural nets. The first
one you need is invariance, right, is a disability that you need to, like the fact that you actually
put symmetries into the architecture is certainly going to have a benefit in terms of sample
complexity, right? They, I mean, you are going to learn more efficiently if your model is aware
of the symmetries of the world. But in fact, this, this prior, in fact, we know now that it's not
sufficient, right? If you only like agnosticially build your learning system, just with these
symmetries in mind, indeed, you are going to become more efficient that a system that is
completely agnostic to symmetries. But it might not be, you might not be able to formally establish
what we call like a, like a learning guarantee that has good sample complexity. And I guess that I
don't want to go too much into the jargon and the details of what this means. But the idea is that if
I want to, you know, learn this function with certain precision, how many examples, how many
training examples do I need to kind of give you a certificate for authentication, like a guarantee
that I'm going to be able to do that. So with symmetries alone, it's not something that we
know how to do. In fact, we are, we would believe we have strong beliefs that it's not possible,
right? There's examples out there that I could, I could, you know, construct a function that has
the right symmetries, has the right priors, if you want, but still needs a lot, a lot, a lot of
examples to build to learn. So what we need is to add something else into the mix. And this is this
something that we call in the, in the book, this scale separation. And, and, and if you want, I can
try to very briefly give you an intuitive idea of what this means. So if you think about the problem
of classifying an image, like a dog or the cat. So what is given to you is like a big
branch of pixels, right? Every pixel has a color value. So somehow you need to figure out
the thing that you're looking for is lying in some kind of like, it's really through the
interactions between pixels that you get the answer, right? And the question is,
of course, if I have a thousand pixels, how many possible interactions do I have between
thousand elements? So this is where this exponential or the curse of dimensionality appears, right?
I need to, a priori, I should be looking at all possible families of interactions between
pixels. And this is where maybe where my signal would be lived. Of course, if I need to look for
all of these things, it's impossible, right? There's just too many things. If I tell you that the,
you know, these interactions are such that there's this translation symmetry. Well, you might not
be, you might not be needing to look at all of them. But in fact, you don't need, you do not
throw enough. So what is really something that is powerful is that I tell you that maybe the
interactions that matter the most are those between a pixel and its neighbors, right? And if you
understand very well, if you base your initial learning steps into understanding well, which
local interaction matters, maybe you can use them to bootstrap the interactions that go to look at
this neighborhood to slightly bigger neighborhoods, right? And so this idea that you can break a very
complicated problem into a families of sub problems that lives in different scales. This is at the,
I would say, at the intuitive level, something print like at the core of the essence of why
these architectures are so efficient. This idea, as you might imagine, is not new. It's not specific
to deep learning. The idea that you can take a complicated system of interacting particles
and break it into different scales. This is at the, at the basis of essentially all of physics
and chemistry, right? There's many, many, you know, like when people study, even like biology life,
right? You have, you have experts that are very experts at the molecular level. Then you have
experts that, you know, might understand like, you know, doctors that understand things at the
level of okay functions. And then there's maybe experts at the level of the society, right? But
this, you know, it's pretty natural to break the very complicated thing into different scales.
And so deep neural networks somehow are able to do that. We don't have the full mathematical
picture, right? Or for example, why this scale separation is strictly necessary. What we know
from empirical evidence, like that is now I would say indisputable, is that this is an efficient way
to do that, right? Because when I was, when I was reading the prototype book, I noticed that
there was a separation between the symmetries and the scale separation. Could you explain in
simple terms, why is the scale separation not just a symmetry as well? Because it seemed a little bit,
I don't want to say kluge, but it seemed like you had this scale matter and you dealt with it
separately. Yeah, that's a good, that's a good point. So, so maybe that the way to, to, to separate
these two would be if you think about like an algorithmic instantiation, if you want to have
a network that would just break the problem into different scales, it would be like a neural network
that would operate at different patches. And for every patch of the image, I could be learning
independent set of parameters, right? So that would be a model that is only told that it should be
breaking the problem into different regions. But it's not necessarily told that, you know,
there's a weight sharing, right? There's some kind of like parameter sharing
that somehow is, you are able that, you know, in a sense is helping you to learn with fewer
number of parameters. So somehow these two, these two conditions are slightly complementary.
We, as I said, there's still like a lot of mathematical puzzles as to how these things
interact optimally. And I think that the, the one of the reasons why we chose to explain the story
into two different, in these two different priors is that they all survive this quest for
generality in the sense that these two principles are, again, something that you can think about
for grids, for groups, for graphs, you can also see these principles appearing completely everywhere
as you study physical systems, right? Like the scale and the symmetry
is really at the core of, of many physical theories. And I would say that there's also
at the more maybe technical level, these two priors somehow have been instrumental to
organize, like to basically to have a kind of a recipe to build architectures, right? So,
so maybe now we don't even think about it, right? But when you have a new problem, a new domain,
and you need to build an efficient neural network, we automatically have this idea that, okay,
we are going to start learning by composition, right? So we are going to extract information
one layer at a time. That's the first appearance of scale. And we know that the way we need to
organize these layers, right? How do you parameterize a layer that takes some input features and
produces maybe better features? This idea that we do that by understanding this kind of
equivarian structure, right? We have this notion of filters, right? In convolutional networks,
we have this, as I said, we organize everything in terms of filters. When we talk about message
graph neural networks, we have this kind of like diffusion filters, right? And so there's this
object that we extract from the domain that is helping us, that giving us something very
constructive, very, you know, very relevant, like very practical. And so this is really
the underlying group of transformations that is acting on our domain. And so I would say that,
you know, from a practitioner's perspective, these two principles, right, that I'm going to
learn by composing some fundamental layers that I repeat all the time. And the way this layer
is organized, is structured through this group transformation, this has been, I would say,
like a trademark of, you know, the success of neural networks. Of course, as also we mentioned
in the book, these are, I would say, proto, like meta, you know, meta parameterization, right,
in the sense that there's, as you know, many, many, many variants that people have come up with.
Many, many, let's say, yeah, like modifications on the basic architecture that have really
make dramatic changes in performance, right. So there's, of course, as I say, like the devil
sometimes is in the details, right. And so as we are writing the book, we are realizing exactly,
you know, how some of the changes in the architectures are actually fit into this,
what we call this blueprint, right, this symmetrically learning blueprint.
Fascinating. Okay, so I wanted to come back to what you were saying a few minutes ago about the
the sample efficiency of these models. Now, with graph neural networks, for example,
there are factorially many permutations of adjacency matrices for a given graph. And
I want to talk about a sorting algorithm, right. So Franois Chollet came on the podcast and he said
that in order to learn a sorting algorithm that generalizes, you would need to learn point by
point, you would need to see factorially many examples of permutations of numbers.
Do you think that we could train a neural network to learn a sorting? I guess what I'm
asking in a roundabout way is, do you think there's a kind of geometry to computation itself?
Good. That's a very good question. And in fact, we have some, some recent work with some collaborators
in my group, where we kind of take up this question from and we try to formalize it mathematically,
and we give answers to this question. And so many of these like a computational tasks that
you were mentioning, for example, sorting or, you know, like algorithmic tasks, they are,
if one wants to put them into some mathematical context, the first thing that comes to mind is
these are functions that already enjoy some symmetries. For example, like a sorting algorithm,
right, is invariant to permutations, right. So the function that you are trying to learn
is an arbitrary function that has this symmetric class. And so as such, as I was saying in the
beginning, you can try to address this question saying, okay, now you give me an arbitrary,
so the question would be relative to an arbitrary learning learner that is agnostic to symmetry,
how much does a symmetric learner gain, right? So you can basically try to understand, quantify
the gains of sample complexity of learning without symmetry versus learning with symmetry.
And so the punchline of this work, the recent work that we completed, is that one can actually
quantify the sample complexity gains, and these are of the order of the size of the group. And so
here in the case of like permutations, what it means is that if a learner is aware of this
symmetry, like one training example of the symmetric learner is rosary equivalent to
n factorial samples of the agnostic learner, right, which is something that you would expect,
like if you think in terms of data mutation, right, like if I tell you in advance that your
function is symmetric, is invariant to permutations, it's, you know, like a brute force approach would
say, okay, you give me an instance training an input, right, and instead of giving you this input,
I'm just going to, you know, like permute, like have any possible permutation of the input, and
you already know the output, right? So you can as well feed it to the learner. So this is like
horrific at this addition turns out to be mathematically correct, precise, at least, you
know, under some conditions, right? But the, I guess the whole point is that these gains might
look amazing, like might look like a, you know, like a big boost in sample complexity. As I said
before, there's a grain of salt here is that the, in these conditions, in general, general
conditions, you are already fighting an essential and impossible problem in the sense that the rate,
like the sample complexity is dominated by a rate of basically the rate in which you learn
is what we call course by dimension. And what it means is that if I want to, you know, I have a
certain performance generalization error, I'm going to say that now I want to ask you the question,
if I want to divide this generalization error by two, how many more samples do I need to give you?
Right? Like what? So if I want to double, you know, like double, like reduce the error by half,
by how much do I need to give you more samples? So this dependency, in fact, is exponential in
dimension, right? So basically the, the, the sample complexity gains by invariance, they are
exponential in dimension, but they are fighting an impossible problem that is already caused by
dimension. So what it means is that at the end of the day, this is what, you know, what was in the,
in the, like in the heart of what I was saying before, is that invariance alone might not be
efficient, might not be sufficient, right? Because you are, okay, you are taking a very hard problem,
you are removing an exponential factor, but you still have many, you know, you have still
have something in the exponent that is exponential, right? So, so, so what it means is that that,
I mean, that's what really underpins why we think in these terms of combining symmetry prior
with the scale separation prior. But certainly the algorithmic tasks are very interesting playground
because I think that for the case of sorting, I mean, as you know, scale separation is also an
issue. It's also a very important thing, right? I mean, it, this is what basically is at the heart
of the dynamic programming approaches, right? Like these efficient algorithms that are not only
officially statistically, but also officially computation, right? This idea that you can
divide and conquer. So, so, so, so algorithmic tasks also are kind of exposed to this dual
physical prior, right? Of a scale and invariance. On the course of dimensionality, there's this
issue where you have a data point and you want to surround it by other data points in two dimensions
to create a convex hole. And as you increase the number of dimensions, the number of data points
you need to create this covering to create a kind of interpolative space increases exponentially.
And when you get past a certain number of dimensions, let's say 16 or not, not very many,
you would need essentially more data points than there are atoms in the universe. So this leads
to a very interesting realization. I think some people refer to it as the manifold hypothesis,
which is that most natural data is only really spatially novel on very few dimensions. And a
lot of data falls on very smooth low dimensional manifolds. But what are the implications of this?
Essentially, all machine learning problems that we need to deal nowadays are extremely high
dimensional. So even if we take very modestly sized images, they live in thousands or even in
millions of dimensions. And if you think of machine learning or at least the simplest setting of
machine learning as a kind of glorified function interpolation, the standard approach is to function
interpolation as just use the data points to predict the values of your function will simply
not work because of the phenomenon of cursive, the recursive dimensionality that increasing
the number of dimensions, the number of such points blows up exponentially. So what you really
need to take into account and probably this is really what makes machine learning work in practice
is the assumption that there is some intrinsic structure to the data and it can be captured
in different ways. So it's either the manifold assumption where you can assume that the data,
even though it lives in a very high dimensional space intrinsically, it is low dimensional.
This can be captured also in the form of symmetry. For example, image is not just a high dimensional
vector. It has underlying grid structure and grid structure has symmetry. This is what captured
in convolutional networks in the form of shared weights that translates into the convolution
operation. I think the symmetries are part of the magic here because it's not just the interesting
observation that natural data is only spatially novel in so few dimensions. There's something
magic about symmetries. And when we spoke to Franois Chalet recently, he invoked the kaleidoscope
effect, which is this notion that almost all information in reality is a copy of some other
information. Probably here it will be a little bit stretching, but I would say that because
data comes from nature, from physical processes that produce it, physics and nature itself
is in a sense low dimensional. So it's application of simple rules at multiple scales. You can create
very complex systems with very simple rules. And this is probably how our data that we are mostly
interested in in machine learning is structured. So you have this manifestation of symmetry and
self-similarity through different scales, the principles of symmetry and certain
environs or equivalents and of scale separation, where you can separate your problem to multiple
scales and deal with it at different levels of resolution. And this is captured, for example,
in pooling operations in convolutional neural networks and in other deep learning architectures.
This is what makes deep learning systems work. Fascinating. It's so good that you raised the
curse of dimensionality because I was going to ask you about that. Could you explain in really
simple terms, so not invoking lipships, I can't even say it properly now, but not invoking a
mathematical jargon. And why exactly in your articulation does geometric deep learning
reduce the impact of the curse of dimensionality?
Yeah. So the curse of dimensionality, it refers generally to the inability of algorithms to
keep certifying certain performance as the data becomes more complex. And data becoming more
complex here means that you have more and more dimensions, more and more pixels. And so this
inability of scaling, basically it really says that if I scale up the input, my algorithm is
going to have more and more trouble to keep the base. And so this curse can take different
flavors. So this curse might have a statistical reason in the sense that as I make my input space
bigger, there would be many, many, many much exponentially more functions, real functions
out there that would explain the training set that would basically pass through the training points.
And so the more dimensions I add, the more uncertainty I have about the true function.
So I would need more and more training samples to keep the base. This curse can also be from the
approximation side. So in the sense that the number of neurons that I'm considering to approximate
my target function, I need to keep adding more and more neurons at the rate that is exponential
in dimension. And the curse can also be from the computational side. The sense that if I keep
adding parameters and parameters to my training model, I might have to optimize to solve an
optimization problem that becomes exponentially harder. And so you can see that you are basically
bombarded by all angles. And so an algorithm like here in the context of statistical learning or
learning theory, if you want, having a kind of a theorem that would say, yes, I can promise you
that you can learn, you need to actually solve these three problems at once. You need to be able
to say that in the conditions that you're studying, you have an algorithm that it does not suffer from
approximation nor statistical nor computational curses. So as you can imagine, it's very hard
because you need to master many things at the same time. So why do we think that geometric
deep learning is at least an important piece to overcome this curse? As I said before, so
geometric deep learning is really a device to put more structure into the target function.
So basically to make the learning problem easier because we are promising the learner
more properties of a target function. We are basically making the hypothesis class if you want
smaller. That said, as I said, there's still some path to go. We're describing just a bunch of
principles that make these hypothesis spaces smaller and more adapted to the real world.
But one thing that we are still lacking, for example, is the guarantee in terms of optimization.
I mean, I described that the depth of these architectures is somehow something that is
akin associated with the scale, the fact that you need to understand things at different scales.
So as you know, from the optimization side, there's still some open questions and open mysteries
as to why the gradient descent, for example, is able to find good solutions. So these are
things that we believe that these architectures can be optimized efficiently just because we have
these experimental evidence that is piling up. But we are, for example, we are still lacking
theoretical guarantees. For the approximation, it's a bit of the same story. So we understand
very well approximation properties of shallow networks, starting from universal approximation,
but of course, many, many recent interesting work. But we are also still lagging a little bit behind
in approximation properties for deeper networks. So as you see, it's like you can see from this
discussion that, yes, we have some good reasons to believe that these are fundamental principles
of learning. But there's also a bunch of mathematical questions that are still open. And this is also
one of the things that I like about writing a book on this topic, because it's a very life domain.
As you can see, the field is still evolving. And I think it's a good time to... Yeah, it's a good
time. I mean, researchers out there are listening to us. It's a good time to think and to work and
to join this program. Amazing. Will we ever understand the approximation properties of deep
networks? Because with the shallow function approximation algorithm, you can almost think
of a neural network as being kind of like sparse coding. And the more neurons you have, you're
just kind of discreetly fitting this arbitrary function. But you don't have that visual intuition
quite so much with the deep networks. Yeah, I mean, it's an important... And it's a deep question.
So indeed, shallow neural networks are really, really correspond to this idea that you learn
a function by stacking a linear combination of basis elements. And this is really at the roots
of essentially all of harmonic analysis or functionalized. I typically think about the
basis and you ask questions about the linear approximation or the approximation, etc.
Deep neural networks, they introduce a fundamentally different way to approximate
functions that is by composing, by composition. And so you're right. Our knowledge about this
question right now is mostly concentrated in what we call separation results, like a depth
separation, which consists in trying to find construct mathematical examples of functions
that cannot be approximated with shallow neural networks with certain number of neurons.
But indeed, they can be much better approximated with deep neural networks. So this is really
understanding which kinds of functions benefit fundamentally from composition rather than from
addition. And so there's a certain mathematical vision and mathematical intuition that is
building up. But of course, it's still very far from explaining the true power of depth. And
just to give you like a final pointer here, there's a very related question that replaces
neural networks as we understand them with what we call Boolean functions, right? Like these are
just circuits, arithmetic circuits that take as input some bit string and they can manipulate the
bits by, you know, or operations and operations and they can keep adding gates. And so this question
about what is the ability of a certain circuit architecture to approximate certain Boolean
functions is actually a notoriously hard and basically has been studied in the theoretical
computer science community since the 50s and the 60s, right? And there's actually very, very,
very deep results and very challenging actually open questions concerning these things. So this
is really, we are really touching here questions that are pretty serious at like the deep mathematical
and theoretical level. And so, yes, you should not expect that in the year in the next year or two,
we have a complete understanding of, you know, approximation powers of any architecture with
any depth. But I think you should expect that the theory like this will continue to try to catch up
with the experiments. And so we are, I think we are hoping to get like a more precise mathematical
understanding of the role of depth. And as I said before, there's one thing that is fascinating
about this domain that is maybe very unique to deep learning is this very strong interaction
between optimization, statistics and approximation, right? Maybe it turns out that, you know, the
huge depth that we have in these residual neural networks might not be necessary from the
approximation side, but in fact, it's so useful for the optimization that overall is a big winner,
right? So there's always these like twists about this question that are fundamentally mixing these
three sources of error. I'm sorry for asking you this question, but can neural networks extrapolate
outside of the training data? That's a good question. I would say that the answer, I guess,
depends on your specifications, right? So I guess that the conservative answer
of a statistical learning person would be no, because we don't have good theorems right now
that tell us that this is the case. There's very like, you know, like a strong effort both from
the practical and the theoretical community to really understand this question, like by trying
to formalize it a bit more. I mean, what do we mean by distribution shift? You know, what kind of
training procedures you can come up with that would give you precisely this kind of robustness?
There's of course, what we call these biases, right? I mean, I can always take it like a
training distribution, I make a choice of a certain architecture, I'm going to learn a function,
and obviously, there's some directions if you want in the space of distributions,
for which my hypothesis will turn out to be have good generalization, there might be
other direction in which the contrary is true. So there's actually very nice work
from Stephanie Gejalka's group at MIT, where they, for example, they studied this question
in the context of value networks, also including graphs, where they, for example, they discover
or they identify this strong preference for value networks to generalize along linear directions,
right? In the sense that if I just decide to now, you know, shift my data in linear directions,
then my function has no trouble generalizing. Maybe there's other directions, right, in which
this thing is actually catastrophic. So I think that, yeah, the question, I think it's very important
from, let's say, it's very important from a kind of a practitioner perspective, right? That's typically,
that's clearly something that a user would like to know. But I think that from a more mathematical
or theoretical level, I think we are still at the stage of trying to formalize like, okay,
what do we exactly mean by extrapolation? And what are the kind of the conditions for
which architecture can do it? And I think that, yeah, this is a, yeah, it's an important question.
But yeah, I think we are still pretty far from having a full answer.
Amazing. And final question, what areas of mathematics do people need to study
before reading the proto book? Good question. So I think that our objective and our really
like the idea is really to have something that is quite self-contained in the sense that we are
going to provide appendices that expand on the areas that is maybe they're not typically
kind of the bread and butter of machine learning people. For example, we are going to have an
appendix on group theory, differential geometry, harmonic analysis. So these are areas that I
think are going to be there for you to delve into, to get like the most of the paper,
the most of the book. But I think that other than that, any basic, you know, any basic
knowledge of linear algebra, statistics and analysis will do. So like, if you have taken
a graduate level class in machine learning, you should be ready to go. Rather than just being
applied in a lot of really important branches of research problems, people outside of pure
research have recognized that a lot of the data that comes to us from nature is most naturally
represented in a graph-structured form. Very rarely will nature give us something
that can be representable as an image or a sequence. That's super rare. So very often,
the structure is more irregular, more graph-like. And therefore, graph neural networks have already
seen a lot of applications in domains where the data is supernaturally represented as a graph.
In the domain of computational chemistry, where you can represent molecules as graphs of atoms
and bonds between them, the graph neural networks have already proven impactful in detecting
novel potent antibiotics that previously were completely overlooked because of their unusual
structure. In the area of chip design, graph neural networks are powering systems that are
developing the latest generation of Google's machine learning chips, the TPU. Furthermore,
graph-structured data is super ubiquitous in social networks and the kinds of networks maintained by
many big industry players. And accordingly, graph neural networks are already used to serve various
kinds of content in production to billions of users on a daily basis. In fact, the recommendation
system at Pinterest, the product recommendation system at Amazon, as well as the food recommendation
system for Uber Eats, all of them are powered using a graph neural network that helps serve the most
relevant content to users on a daily basis. And on a slightly personal note, graph neural networks
have also been used to significantly improve travel time predictions in Google Maps, which
is used also by billions of people every day. So whenever you type a query, how do I get from
point A to point B in the most efficient way? The travel time prediction that you get is powered
by a graph neural network that we have developed that defined in collaboration with the Google Maps
team. And this is of high importance not only to users that use the app on a daily basis to find
the most efficient way to travel. It's also used by the various companies that leverage the Maps
API so they can tell their customers what's the time it will take for a certain vehicle to arrive
to them. So companies such as food delivery companies and ride-sharing companies have also
extensively profited from this system, which in cities such as Sydney has reduced the relative
amount of negative user outcomes in terms of badly predicted travel times by over 40%,
making it one way in which graph representation learning techniques that I have co-developed
are actively impacting billions of people on a daily basis. When I was an undergraduate student,
I was interested in image processing and was excited about variational methods.
I think it's a very elegant idea that you can define a functional that serves as a model for
your ideal image and then use the optimality conditions to derive a differential equation that
flows towards the optimum. And a particularly cool approach was proposed by Ron Kimmel,
where you could think of an image as a manifold or a high-dimensional surface
and use an energy that originated in string theory and particle physics to derive a non-euclidean
diffusion PDE called Beltrami Flow that acts as a non-linear image filter. And this is what
made me fall in love with differential geometry and I did a PhD with Ron on this topic. And I think
these were really beautiful and deep ideas that unfortunately now are almost forgotten in the
era of deep learning. And it's a pity that the machine learning research community has such a
short memory because many modern concepts have really ancient roots. Ironically, we've recently
used non-euclidean diffusion equations as a way to reinterpret graph neural networks as neural
PDEs. And I think it really helps sometimes to have a longer time window. Now, equivariated
networks tend to generalize much better and require much less data if the data indeed has
the symmetry that you assumed in your model. But people often ask, you know, why do we even
care about data efficiency when we can just collect more data? We live in the era of big data, right?
And I think the answer why you might still be interested in data efficiency. First of all,
there are applications like say medical imaging, where acquiring labeled data simply is very cost.
You have to get patients, you're dealing with privacy restrictions, you're dealing with
costly, highly trained doctors who have to annotate the data, come together in a committee to
decide on questionable cases and so forth. So this is very expensive. And if you can improve the
data efficiency by a factor of two or 10, or whatever it may be, you might just take a problem
that was in the realm of economically infeasible and take it into the realm of the economically
feasible, which is a very useful thing. There are other cases like graph neural nets, where the
group of symmetries is so large, in this case, n factorial number of permutations,
that no amount of data or data augmentation in practice is going to allow you to learn the
symmetry or to learn the invariance or equivariate in your NAPO. So indeed, you see that in this
space of graph neural nets, everybody uses equivariate permutation, equivariate network
architectures. And then finally, you can think about the grand problems of AGI, artificial
general intelligence and so forth. And here I think that we will most certainly need large data sets,
large networks, a lot of compute power, and so forth. The current architectures we have clearly
are performing far fewer computations than the human brain. So there's a ways to go there.
But I also think that one essential characteristic of intelligence is the ability to learn quickly
in new situations, situations that are not similar to the ones you've seen in your training data.
And so data efficiency to me is an essential characteristic of intelligence. It's almost
like an action that you want your methods to be data efficient. And so I think one of the
big challenges for the field right now is to try to think of very generic priors, priors that
apply in a wide range of situations. And to give you, even though they're generic and abstract,
they give you a lot of bang for the buck in terms of improved generalization and data efficiency.
I think that the beauty of science and research is in connecting the dots. And I find it fascinating
that, for example, graph neural networks are connected to the work of Weissfeller and Lehmann
from the 60s on isomorphism testing, which in turn was inspired by problems in chemistry.
And chemistry was also the field that drove the research into modern formulation of
diffusion equations that were adopted in image processing community in the 90s and came back
recently as a way to reinterpret graph neural networks. And I think such connections give really
a new and deep perspective. And probably the deeper you dive, the broader they become. But
it's really an ever-ending story. Today is an incredibly special episode and we're filming
at 9 o'clock in the morning. It's really rare for us to film this early when I'm still caffeinated.
Many of our guests are over in the States. It's an absolute honor to have you both on MLST.
And Professor Bronstein, could you start by briefly telling us how the young mathematician
Enne Offer used symmetries to discover the conservation laws in physics?
Maybe I should take a step back and describe the situation that happened
in the field of geometry towards the end of the 19th century. And it was an incredibly fruitful
period of time for mathematicians working in this field with the discovery and development of
different kinds of geometries. So a young mathematician based in Germany called Felix Klein
proposed this quite remarkable and groundbreaking idea that you can define geometry by studying
the groups of symmetries, basically the kinds of transformations to which you can subject
geometric forms and seeing how different properties are preserved or not under these
transformations. So these ideas appear to be very powerful. And what Amy Neuter showed in her
work, and she actually worked in the same institution where Klein ended up in Gttingen in
Germany. And she showed that you can take a physical system that is described as
functional as a variational system and associate different conservation laws
with different symmetries of this system. And it was a pretty remarkable result because
before that, conservation laws were purely empirical. You would make an experiment many times
and measure, for example, the energy before or after some physical process or chemical reaction,
and you would come to the conclusion that the energy is preserved. So this is how, for example,
I think Lavoisier has discovered the conservation of energy. So it was probably for the first time
that you could derive these laws from first principles. So you would need to assume in case
of conservation of energy, the symmetry of time. We decided to take a tour into the world of
algorithmic reasoning. Peter, you said in your introduction that algorithmic reasoning seeks
to find neural networks that are good at imitating the classical algorithms that initially brought
you to computer science. So you recently released a paper on this called Neural Algorithmic Reasoning,
and it's often claimed that neural networks are turing complete. And, you know, we're told that we
can think of training neural networks as being a kind of program search. But you argued in your
paper that algorithms possess fundamentally different qualities to deep learning methods.
Francois Chollet actually often points out that deep learning algorithms would struggle
to represent a sorting algorithm without learning point by point, you know, which is to say without
any generalization power whatsoever. But you seem to be making the argument that the interpolative
function space of neural networks can model algorithms more closely to real world problems,
potentially finding more efficient and pragmatic solutions than those classically proposed by
computer scientists. So what's your take? Yeah, thanks for asking that, Tim. I think the concept
of classical algorithms, as opposed to deep neural networks, at least the way we're currently
applying them makes all these points about turing completeness a little bit moot, because there's
quite a few proofs out there saying that you can use neural networks or more recently graph neural
networks in particular to simulate a particular algorithm perfectly. But all of these proofs are
sort of a best case scenario. They're basically saying, I can set the weights of my neural network
to these particular values, and voila, I am imitating the algorithm perfectly, right? So all
these best case scenarios are wonderful. But in practice, we don't use this kind of best case
optimization, we use stochastic gradient descent. And we're stuck with whatever stochastic gradient
descent gives us. So in practice, the fact that a neural network is capable for a particular setting
of weights to do something doesn't mean that it will actually do that when trained from data.
So essentially, this is the kind of the big divide that separates deep learning from traditional
algorithms. And it has a number of other issues as well, not just the fact that we cannot find the
best case solution. Also, the fact that we are working in this high dimensional space, which is
not necessarily easily interpretable or composable, because you have no easy way of saying, for example,
in theoretical computer science, if you want to compose two algorithms, you're working with them
in a very abstract space, which means that, you know, you can easily reason about stitching the
output of one to the input of another. Whereas you cannot make that easy of a claim about latent
spaces of two neural networks, right? So all these kinds of properties, interpretability,
compositionality, and obviously also out of distribution generalization are plagued not
by the fact that neural networks don't have the capacity to do this. But the routines we use to
optimize them are not good enough to to across that divide. So in neural algorithmic reasoning,
all that we're really trying to do is to bring these two sides closer together by making changes
either in the structure of the neural network or the training regime of the neural network or the
kinds of data that we'll let the neural network see so that hopefully it's going to generalize better
and extrapolate better. And especially on the kinds of, you know, classical algorithmic problems
that we might see in a computer science textbook. And lastly, I think I'd just like to address the
point about sorting. We have a paper on algorithmic reasoning benchmarks that we are about to submit
that in Europe's data set track. I think it should be public even now on GitHub, because that's the
requirement for the conference, where we have quite a few algorithmic tasks, and we're trying to
force GNNs to learn them. And we do have several sorting tasks in there. And at least in distribution,
these graph neural networks are capable of imitating the steps of, say, insertion sort. So
I will say not all is lost if you're very careful about how you tune them. But obviously,
there is a lot of caveats. And I hope that later during this chat, we'll also get a chance to talk
a little bit about how even though we cannot perfectly mimic algorithms, we can still use
this concept of algorithmic execution today now to help expand the space of applicability of algorithm.
Yeah, this is absolutely fascinating. Because this gets to the core of what I think some people
point out as being the limitations of deep learning, right? Sholay spoke about this, but
I don't think that geometric deep learning would help a neural network learner sorting function,
because discrete problems in general don't seem amenable to vector spaces, either because the
representation would be glitchy, or the problem is not interpolative in nature or not learnable
with stochastic gradient descent. So it would be fascinating if we could overcome these problems
using continuous neural networks as an algorithmic substrate. Do you think we could?
I think that it is possible, but it will require us potentially to broaden our lens on what we
mean by geometric deep learning. And this is something we're already very actively thinking
about. I think one of our co-authors, Taco Co, and actually thought much more deeply about this
in recent times. But basically, the idea is we looked at geometric deep learning from a group
symmetry point of view, which is a very nice way to describe spatial regularities and spatial
symmetries. But it's not necessarily the best way to talk about, say, invariance of generic
computation, which you would find in algorithms, right? It's like, I have input that satisfies
certain preconditions. I want to say something about once I push it through this function,
it should satisfy certain post conditions. This is not the kind of thing we can very easily express
using the language of group theory. However, it is something that perhaps we could express more
nicely using the language of category theory, which is an area of math that I still don't know
enough about. I'm currently actively learning it. But basically, in the language of category
theory, groups are super simple categories that have just one node, right? You can do a lot more
complicated things if you use this more broader abstract language. And, you know, you talk about
basically anything of interest there in terms of these commutative diagrams. And Taco actually
recently had a really interesting paper called natural graph networks, where they basically
generalize the notion of permutation, equivariance that you might find in graph nets to this more
general concept of natural transformations. So now suddenly, you don't have to have a network that
does exactly the same transformation in every single part of the graph. What you actually need
is something a bit more fine grained. You just need for all like locally isomorphic parts of the
graph, you need to behave the same. But in principle, it gives you a bit more flexibility. And I think
that this kind of language, like moving a bit away from the group formalism would allow us to
talk about say algorithmic invariance and things like this. I don't yet have any theory to properly
prove this, but it's something that I'm actively working on. And I guess I would say, you know,
the only question is, would you still call this geometric deep learning? And in my opinion,
the very creators of category theory have said that category theory is a direct extension of
Felix Klein's Erlangin program. So since the founders of the field have already made this
connection, I would expect that, you know, it would be pretty applicable under a geometric lens.
So it seems to come back to it seems to come back from both of your sides to essentially graphs
and working on on sort of graphs to capture on the one side, the sort of symmetries that are
that you either assume in the problem or that you know, or that you want to impose on the other
side on the other side. Now, these computations can may be well represented in in graphs.
What's what's so special about graphs in your estimations? Is there something
fundamental to it? Or is it just another way? You know, we had Ben Gertzel or so here and I
asked him the same question, what's so special about graphs and his argument was essentially,
well, it's not about graphs, it's simply a good representation of the problem
that we can make efficient operations on. Do you have a different opinion on that? Is a graph
something fundamental that we should look more at than, for example, a tensor?
Graphs are abstract models for systems of relations or interactions. I should maybe specify
pairwise relations and interactions. And it happens that a lot of physical or biological
even social systems can be described at least at some level of abstraction as a graph. So that's
why it is a so popular modeling tool in many fields. You can also obtain other structures such as
grids as particular cases. I wouldn't call it fundamental, but it is a very convenient and
very common, I would say ubiquitous model. Now, what I personally find disturbing and we can talk
about it in more detail later on is that if you look at the different geometric structures for
Gamble that we consider in the book, whether it's Euclidean spaces or many phones, they all have
the discrete counterparts. So you have a plane and you can discretize it as a grid. You have a
manifold, you can discretize it as a mesh. A graph is inherently discrete. And this is something that
I find disturbing. There is, in fact, an entire field that is called network geometry that tries to
look at graphs as continuous objects. So for example, certain types of graph that look like
social networks, what is called scale free graphs, can be represented as nearest-neighbor graphs in
some a little bit exotic space with hyperbolic geometry. So if we take this analogy, I think
it is very powerful because now you can consider graphs as a discretization of something continuous
and then think for example of graph neural networks as certain types of diffusion processes
that are just discretized in a certain way. And by making potentially possible different
discretizations, you will get maybe better performing architectures. One of the core
dichotomies we talk about on Street Talk is the apparent dichotomy between discrete and continuous.
And as Yannick was saying, there are folks out there who want our knowledge substrate to be
discrete but still distributed, record them sub-symbolic folks. And this network geometry
is fascinating as well because you're saying in some sense you can think of there being some
unknown continuous geometry. So you're saying there is no dichotomy? This is probably a little
bit of a wishful thinking as it happens with every model. So I would probably phrase it carefully
for some kinds of graphs, you can make this continuous model for others, maybe not. Fascinating.
Well, on to the subject of vector spaces versus discrete, you know, geometric deep learning is
all about making any domain amenable to vector spaces, right? And indeed artificial neural networks.
But could these geometric principles be applied to another form of machine learning, let's say
discrete program synthesis? Certainly a very important question, Tim. And yeah, thanks for
asking that. I think that there are many ways in which geometric deep learning is already
at least implicitly powering discrete approaches such as program synthesis because
there is a pretty big movement on these so-called dual approaches where you stick a geometric
deep learning architecture within a discrete tool that searches for the best solution. So,
for example, in combinatorial optimization, a very popular approach recently for a
neural-resolving mixed integer programs is to like have your typical off-the-shelf
mip solver that selects variables to optimize one at a time. And, you know, with these kinds of
algorithms, they're in principle exponential time. But if you're lucky or knowledgeable enough about
how, in what order you select these variables, you can actually solve the problem in linear time,
which is something we would like to strive towards. And the exact way in which we select
these variables is a bit of a black magic, like humans have come up with a few heuristics, but
they don't always work. And whenever you have this kind of setting, as long as you're assuming that
you're naturally occurring data isn't always throwing the worst possible cases or adversarial
cases at you, you can usually rely on some kind of modeling technique, for example, a neural network
to figure out which decisions the model should be taking. So, in this case, for example, for
mip solving, DeepMind recently published a paper on this where you can treat mip problems as
bipartite graphs, where you have variables on one side and the constraints on the other.
And you link them together if a variable appears in a constraint. Then they run a graph neural
network, which, as we just discussed, is one of the flagship models in geometric deep learning,
over this bipartite graph to decide which variable the model should select next. And you can train
this either as a separate kind of supervised technique to learn some kind of heuristic,
or you can learn it as part of a more broader reinforcement learning framework, right, where
the reward you get is how close you are to the solution or something like this. So this is one
kind of clear way in which you can kind of have this synergy between geometric deep learning
architectures and solutions for, for example, program synthesis. But I would just like to
offer another angle in which you can think of program synthesis as nothing other than just
one more way to do language modeling, right, because synthesizing a program is not that different to
synthesizing a sentence, maybe with a more stringent check on syntax and so forth. But, you know,
any technique that is applied to language modeling could, in principle, be applied for
program synthesis. And something that we will be discussing, I believe, later during this conversation,
one of the flagship models of geometric deep learning is indeed the transformer, which
we show in our book, and elucidate why it can be seen as a very specific case of a graph neural
network. And that's one of the flagship models of language modeling. So basically, that's also
one more way to to unify. Like, you know, just because the end output is discrete doesn't mean
that you cannot reason about it using representations that are internally vector vector based.
Francois Chollet pointed out that there was this dichotomy. So you can embed discrete information
into a continuous representation. But the manifold needs to be smooth, it needs to be
learnable, it needs to be interpolative in nature. So I thought that was why we have these discrete
program searches. But then you have this exponential blow up. But maybe that search space, because
it's interpolative could be found using stochastic gradient descent, if you embed the discrete
information into some kind of vector space. But Professor Bronstein, I wanted to throw it back
over to you. I mean, why is it taken as a given that vector spaces are a good thing? Because
everything we're doing here is embedding discrete information into these Euclidean vector spaces.
Why are we doing that? There are multiple reasons why vector spaces are so popular in
representation learning. Vectors are probably the most convenient representation for both humans
and computers. We can do for a number of operations with them, like addition or subtraction. We can
represent them as arrays in the memory of the computer. They are also continuous objects,
so it is very easy to use continuous optimization techniques in the vector spaces. It is difficult
for Gamble to optimize a graph because it is discrete and requires combinatorial techniques.
But in a vector representation, I just have a bunch of points that I can continuously move
in a kind of dimensional space using standard gradient based techniques. Perhaps a more nuanced
question is what kind of structures can be represented in a vector space? And a typical
structure is some notion of similarity or distance. We want that the vector representations preserve
the distances between, let's say, original data points. And here we usually assume that
the vector space is equipped with the standard Euclidean metric or norm, and we have a problem
from the domain of metric geometry of representing one metric space in another. And unfortunately,
the general answer here is negative. You cannot exactly embed an arbitrary metric in Euclidean
space, but there are, of course, some results such as bogains theorem that, for Gamble, provides
bounds on the metric distortion in such cases. And in graph learning spaces with
other more exotic geometries such as hyperbolic spaces, you have recently become popular with,
for example, papers of Ben Chamberlain, my colleague from Twitter, or Max Nicol from Facebook.
And you can see that in certain types of graphs, the number of neighbors
grows exponentially with the radios. If you look, for example, at the number of friends of friends
and so on in a social network, where we have this small world phenomenon, you can see that
it becomes exponentially large with the growth of the radios. And now when you try to embed
this graph in Euclidean space, it will become very crowded because in the Euclidean space,
the volume of the metric ball grows polynomially with the radios. Think of the two-dimensional
case that we all know from school, the area of a circle is pi radius squared, right? The volume
of a ball is exponential with a dimension. So we inevitably need to increase the dimension
of the embedding to make space for these neighbors. In the hyperbolic space, the situation is very
different because the volume grows exponentially with the radios. So it is way more convenient
to use these spaces for graph embeddings. And in fact, recent papers show that
to achieve the same error in embedding of a graph in the hyperbolic space with, let's say,
10 dimensions, you would require something like a 100-dimensional Euclidean space.
Of course, I should say that metrics are just one example of a structure. So the general answer
to the question whether a vector space is a good model for representing data is, as usual,
it depends. You mentioned language, Peter, and maybe to both of you, do you think there is a
geometry to language itself? I mean, obviously, we know about embedding spaces and close things
somehow share meaning and so on. Do you think it goes beyond that? Because, like,
do you think there is an inherent geometry to language itself and sort of the meaning of what
we want to transmit and how that relates to each other? What you probably mentioned is the
famous series of papers from Facebook where unsupervised language translation can be done
by a geometric alignment of the latent spaces. In my opinion, it's not something that describes
geometry of the language. It probably describes in a geometric way some semantics of the world.
And even though we have, linguistically speaking, very different languages like,
let's say, English and Chinese, yet they describe the same reality. They describe the same world
where humans act. So it is probably reasonable to assume that the concepts that they describe
are similar. And also, while there are some theories and linguistics about
certain universal structures in languages that are shared, even though the specifics are different,
I think it's interesting to look maybe at non-human communications. I wouldn't probably
use the term language because it's a little bit loaded and probably some purists will be shocked
by me saying that, for example, whales have a language, but we are studying the communication
of slow whales. So this is a big international collaboration called Project. And I don't think
that you can really model the concepts that whales need to describe and to deal with
in the same way as we humans do. So maybe a silly example, we can say in human languages,
and probably it applies to every language, we can express a concept that something got wet.
I don't think that a whale would even understand what it means by being wet because
the whale always lives in water. I would add to that maybe a slightly different view of geometry,
but it's all about the question of how far are you willing to go and still call it geometry.
Based on our proto book, at least, I tend to think of graph structures also as a form of
geometry, even though it's a bit more abstract. And within language, people might not always agree
what this structure is like. But I think we can be fairly certain that there are explicit links
between individual words as and when you use them in different forms, syntax trees, or just one
word precedes another and so on and so forth. And while we may not be necessarily able to easily
say what is the geometric significance of one word, what we can look at is what is the local
geometry of the words that you tend to use around it. And I mean, this kind of principle has been
used all over the place. That has then been extended to graph structured observations,
generally with models like deep walk and note to back basically the same idea,
treat a nodes representation as everything that's around it. Basically, the reason why I think that
analyzing this local topology of how words are used with each other is very powerful.
I've reinforced that recently, precisely because of the fact I've been delving into category theory,
because in category theory, your nodes are basically atoms, you're not allowed to look
inside them, you assume they're this undivisible unit of information. And everything you can
conclude about the atoms comes from the arrows between them. So using this very simple concept
with a few additional constraints like compositionality, you can, for example, tell me what are all the
elements of a set, even though you've abstracted that sets to a single point, just by analyzing
the arrows between all sets, you can tell me what are all the elements inside a set. So thinking
about this, I do believe that it is possible to reason about geometric, you know, word to
vex, for example, does this with the assumption that the structure of the
are we approaching this at the right level, though, because people have said for quite a
long time that there's a difference between syntax and semantics. And you could look at the
geometrical structure of spoken language. Or, for example, you could look at the topology of
the connections in your brain, the topology of, you know, reference frames in your brain is how
you actually have learned concepts. Would looking at the topology of spoken language tell you enough
about abstract categories? That's a good question. I think that if that kind of information is
necessary, like if the atoms by themselves won't tell you everything. One thing that we actually
very commonly do in graph representation learning is assume this sort of hierarchical approach,
where you have like the ground level with your actual individual notes, and then you come up
with some kind of additional hierarchy that tells you either something about intermediate
topologies in a graph, or intermediate structures that you care about in this graph, or any abstract
concepts you might have extracted. And then there's additional links being drawn between these to
kind of reinforce the knowledge that the graph net can capture. So I think if you have knowledge of
some abstract concepts that are relevant for your particular task, you can attach them as
additional pieces of information to this topology. Of course, the more exciting part is could we
maybe discover them automatically? But that is something that I don't think is potentially in
scope for this question. When human interpreters need to translate from one language to another,
they often need to deal with different structures. I think Turkish is actually an extreme example
where the order of words is completely reversed. It implies that you need probably to hold well
in computer science terms some kind of a buffer in your brain before you can make the translation
to another language. So it definitely imposes certain biological network structure in the brain.
Another interesting observation that I read somewhere about the way that people remember
certain facts when they speak a certain language. So the particular example that was given is a
person can remember a perpetrator of a crime and then gives testimony in court. And the reason
is that in some languages, it is more common to use impersonal pronouns and the personal phrases.
So you can say, for example, the object was broken. And in some languages, you would say that
somebody broke the object. So it appeared that languages were of these more impersonal constructions.
People speaking these languages, I have hard time to remember the perpetrator. So the language
probably imposes a lot about the way that we perceive world, but it is probably not studied
sufficiently. But there may be some fuzzy graph isomorphisms, though, between the languages.
I think there's something really magic about graphs. I think that's what we get into, because
your lecture series inspired me, actually, Professor Bronstein, where you were talking
about all the different applications of graphs. But something that a lot of our guests talk about
are knowledge graphs. Expert systems and the knowledge acquisition bottleneck were the
cause of the abject failure of good old fashioned AI or some symbolic AI systems in the 1980s. And
many hybrid or neuro symbolic folks today are still arguing that we need to have a discrete
knowledge graph, either human designed or learned or evolved or emerged or some combination of those
things I just said, depending on who you talk to. Now, critically, many go fi people think that
most knowledge we have is acquired through reasoning, not learning, right, which is really,
really interesting. So by reasoning, I mean extrapolating new knowledge from existing knowledge.
It feels like graph neural networks could at least be part of the solution here. And in your
lecture series, you mentioned the work by Kramner, which was explainable GNN, where they use some
kind of symbolic regression to get a symbolic model from a graph neural network. So do you think
there's some really cool work we can do here? There is a little bit of divide in graph learning
literature. So people working on graph neural networks, and working on knowledge graphs, even
though, at least in principle, the methods are similar. For example, you typically do some form
of embedding of the nodes of the graph. Somehow these are distinct communities, probably,
historically, they evolved in different fields. Yeah, so the paper of Krammer, this is really
interesting because they use graphs to model physical systems, for example, and body problem
when they have particles that interact. You can describe these interactions as a graph,
and you can use standard generic message passing functions to model the interactions.
Now, the step forward that they do is they replace these generic message passing functions
with symbolic equations. And not only that this allows to generalize better, but you also have
an interpretable system, you can recover from your data the laws of motion, right? And if you
think of how much time it took, historically, to people like Johannes Kepler, for example,
he spent his entire life on analyzing astronomical observations to derive a law that now bears his
name, that describes the elliptic orbits of planets. Nowadays, with these methods, you can
probably do it in a matter of seconds or maybe minutes. I think the point that particularly
caught my attention in what you asked, Tim, was this interplay between graphs and reasoning
and extrapolation and how that supports knowledge. Now, when it comes to how critical is this going
to be, it depends on the environment in which you put your agent. Like, is it a closed environment,
or is it an open ended environment where new information and new knowledge can come in
in principle at any time? This basically do want to build a neural scientist, or do you just want
to build a neural exploiter that takes all the information available right now and then draws
conclusions based on that. So if the system is closed worlds, you'll probably be able to get
away without very explicit reasoning, especially if you have tons of data, because we've seen time
and time again that large scale models can kind of pick up on these regularities if they've seen
it often enough. But if I give you a solution that involves stacking, for example, n objects,
and now I ask you to do the same kind of reasoning with two times n objects, the way in which we
optimize neural networks at least today is typically going to completely fall on its back
when you do something like this. So if you truly want to take whatever regularities you have come
across in the world of the training data and hope to at least reasonably gracefully apply them to
new kinds of rules that come in the future, then you probably want your model to extrapolate to a
certain extent. And for this, at least my ongoing algorithmic reasoning research algorithms are a
very natural area to study under this lens, because they trivially extrapolate, you write an algorithm
that does a particular thing on a set of n nodes, you can be you can usually mathematically prove
that it's going to do the same thing equally properly, maybe a bit more slowly, if you give it
two times n nodes, right? This kind of guarantee typically doesn't come that easily with neural
networks. And we found that you have to very carefully massage the way you train them, the
kinds of data you feed to them, the kinds of inductive biases you feed into them, in order to
get them to do something like this. So if extrapolation is something you truly need, and you know,
I think for artificial general intelligence, we're going to want to have at least some degree of
extrapolation as new information will become available to our neural scientists, just as
you follow the era of time. Basically, for doing something like this, graph neural networks have
arisen as a very attractive primitive, because there's been a few really exciting theoretical
results coming out in recent years, saying that the operations of a graph neural network align
really, really well with dynamic programming algorithms. And dynamic programming is a very
standard computational primitive, using which you can express most polynomial time heuristics.
So essentially, that's a really good, you know, that's a really good piece of mind result. The
unfortunate side of it is that it's a best case result, right? So you can set the weights of
a neural network of a graph neural network to mimic a dynamic programming algorithm,
more efficiently or with smaller sample complexity. But, you know, there's still a big
problem of how do I learn it in a way that it still works when I double the size of my input.
And that is in a way what algorithmic reasoning has been largely about. Like, we're trying to make
that happen. It's not easy. If you throw the vanilla graph neural network and just input
output pairs of an algorithm, it will learn to fit them in distribution the moment you give,
like, ask it to sort and array that's twice as big, it's going to completely collapse. So
this is the number one thing that the neurosymbolic people say. They say neural networks, they don't
extrapolate. They only interpolate, you know, it just, it's a continuous geometric model,
learns point by point, transforms the data onto some continuous, smooth, learnable manifold,
you interpolate between the data points, you want to have a smooth, you want to have a dense
sampling of your data. But you're talking about dynamic programming problems, these are discrete
problems that the structure is discontinuous. But how could you possibly learn that within your
network? Well, the dynamic programming algorithm could be, could have a discontinuous component
for example, if you're searching for shortest paths at some point, you will take an argmax over
all of your neighbor's computed distances and use that to decide what the path is.
But before you come to the argmax part, there is usually some fairly smooth function being
computed actually. So in the case of shortest path computations, you know, Bellman Ford or
something like this, you say something very simple, like, I have a value d of s in every
single one of my nodes, which is initially infinity everywhere and zero in the source
vertex. And then at every point, I say, the distance of my particular node is the minimum
of all the distances of my neighbors plus the edge weight, right. And this kind of function is
generally more graceful than than taking an argmax. And you can also think of, for example,
if you have to compute expected values or something like this, using dynamic programming,
that's also one example where actually summing is what you need to do across all of your neighbors
or something like this. So yeah, it is true that like, across individual steps, you may be doing
like discrete optimization steps. But usually, it's propelled by some kind of continuous
computation under the hood. So that's the part that the graph neural network actually simulates.
And then the part which does the argmax would be some kind of classifier that you stitch on
top of that. So in principle, it's not, yeah, it's not too challenging to massage it into a
neural network framework. So one of the, I think one of you mentioned this before, brought up
transformers. And, you know, in recent years, we've had, I think about 10 different papers
saying transformers are something there is transformers are RNNs, transformers are Hopfield
networks. And also transformers are graph neural networks or compute some kind of
graph neural networks. Can you maybe speak a bit to that? Are transformers specifically
graph neural networks? Or are they just so general that you can also formulate a graph problem in
terms of a transformer? Okay, that's a very good question. I would start off by saying,
like, I don't want to start this discussion just by saying, yes, transformers are graph
neural networks. This is why end of story, because I feel like, you know, that doesn't
touch upon the whole picture. So let's let's look at this from a natural language processing
angle, which is how most people have come to know about transformers. So imagine that you have a task
which is specified on a sentence. And you want to exploit the fact that words in the sentence
interact, right? It's not just a bag of words. There is there's some interesting structure
inside this bunch of words that you might want to exploit. When we were using recurrent neural
networks, we assume that the structure between the words was a line graph. So basically,
every word is preceding another word and so on and so forth. And you kind of just linearly
process them with a model like LSTM or something. But, you know, basically line graphs, as we know,
are not the way language is actually structured. There can be super long range interactions
inside language. So subjects and objects in the same sentence could appear miles away from each
other. So using the line graph is not the most optimal way of getting that information in the
fastest possible in the fastest possible way. So, okay, there's clearly some kind of non trivial
graph structure. What is it? Well, it turns out that people cannot really agree what this optimal
graph structure is, and it may well be task dependent, actually. So just consider syntax
trees, for example, like there's not always a unique way of decomposing a sentence into a syntax
tree. And the exact kind of tree you might wish to use to represent a sentence may be different
depending on what is the actual thing that you're solving. So, okay, we have a situation where we
know that there's some connectivity between the words, but we don't know what that connectivity is.
So in graph representation learning, what we typically do when we don't know the graph,
as long as the number of objects is not huge, is to assume a complete graph and let the graph
neural network figure out by itself what the important connections are. And if I now stitch
an attentional message passing mechanism onto this graph neural network, I have effectively
rederived the transformer model equation without ever like using this specific transformer lingo.
So from this kind of angle, the fact that it's a model that operates over a complete graph
individual words, in a way that you know, once you've put all the embeddings to them is permutation
equivalent, this describes the central equations of self attention that the transformer uses.
The part which I think causes a bit of a divide here is the fact that transformers like the model
that was originally presented are not just the equations of a transformer, they're also the
positional embeddings of a transformer. And that's the part that sort of gives it a bit more of a
central structure. Well, actually, if you look at these sine and cosine waves that get attached to
the individual words in an input to a transformer, you will see that you can you can actually derive
a pretty good connection between them and the discrete Fourier transform, which actually
turned out to be the eigenvectors of a graph Laplacian for a line graph. So essentially these
positional embeddings are hinting to the model that you are the decent that these words in a
sentence are arranged in a particular way, and you can use that information. But because it's
fed in as features, the model doesn't have to use any of that information, like sometimes bag of words
is the right thing to do, for example, right. So essentially, the transformer has a bit of a light
hint that there's a central structure in there in the form of a line graph. But, you know, the
model itself is a permutation equivalent model over a complete graph. And from our lens of the
geometric deep learning, it is effectively a special case of an attentional GNN. Now, I think
this positional embedding aspect is a super important one. And it could hint to how we might
extend these transformers from sentences to more general structures. And I think, Michael, you
might have a lot more thoughts on that than I do. So maybe you can say a bit about that.
Yeah, so positional encoding has been done for graphs as well. As Petter mentioned,
in case of a graph, you can straightforwardly generalize this sine or cosine positional coordinates
that are used in transformers using the eigenvectors of the Laplacian. There are other
techniques you can actually show that you can make it a message passing type neural network
strictly more powerful than traditional message passing. The equivalent vise for 11 graphics
or morphism test by using a special kind of structure where positional encoding, for example,
if you can count substructures of the graph, such as cycles or rings and so on. And this way,
you have a message passing algorithm that is specialized to the particular position in the
graph and can, for example, detect structures that the traditional message passing cannot detect.
So it is at least not less powerful than the vise for 11 algorithm. And we can actually show
examples on which vise for 11 algorithm or traditional message passing fails, whereas
this kind of approach succeeds. So the thing I've always wondered about transformers networks
are the position tokens. I really don't like them and I want them to go away
because it feels very impure, doesn't it? I think what we really want to learn is some kind of higher
order structure in the language. And it kind of felt like we were using the position tokens to
cheat a little bit. So what I'm trying to get across here is that I think the position of a
token in an utterance should be invariant. I mean, clearly, in different languages,
the tokens are in different places. In Turkish, the order is completely reversed. And I would
like to think that our internal language representation ignores the transmission
arrangement given the particular language and the constraint that we only communicate sequential
streams of words. However, I do appreciate what Michael is saying above that the position
encodings can actually encode more powerful structures like cycles and rings. The key question
is, do we actually need to have these structures in natural language? I don't agree that you want
to get rid of them. So positional encoding, it's a kind of combination of two worlds. So if you
consider a graph, then you're completely agnostic to the ordering of the nodes. This is one of the
really key characteristics of graphs and sets more generally that you don't have the ordering
of the nodes. The situations and the problems where transformers are applied, you actually do have
an order. But you use the graph as Petra described to model different long distance relations
between different tokens or words in a sentence. So you want to incorporate this prior knowledge
that these nodes are not in arbitrary order, that they have some sentence order. And this
principle applied more generally, you can use positional encoding to tell message passing
not to apply exactly the same function everywhere on the graph, but to make it
specialized for different portions of the graphs or at least make it a possibility. And then
the training will decide whether to use this information or not or in which way.
It's strange because I don't know whether the order is just a function of the communication
medium. So we transmit the tokens in a sequence. And could we then represent them in our brains
in a completely different domain where the sequence is no longer relevant? Well,
actually a lot of neuroscientists think that our brain is a prediction machine and
it's a sequence prediction machine. So the sequence is kind of fundamentally important.
Yeah, it's also a function of the specific language that you use. And as we discussed
before, there are languages which convey the same meaning with a difference in structure.
I want to get a little bit into what you said about essentially what we're doing with these
positional encodings is we hint. We hint to the model that there is something here,
which is a big break from sort of the old approach, let's say, of an LSTM to say,
this is the structure. So with the world of geometric deep learning, I often have the feeling
people talk about, they talk about symmetries and we need to exploit these symmetries that are
present in the world. And there's almost to me two different groups of these symmetries. So one
group is maybe you would call them like exact symmetries or something like this. When I think
about Alpha fold, and I think about like a protein, it doesn't, I don't care which side is up, right?
Like the protein is the same, the same protein, and there's no reason to prefer any direction
over any other direction. However, if I think of like, because people have made this argument
for CNNs, for example, they say, well, a CNN is a good architecture because it's translation
invariant, right? And essentially, if we want to do object recognition or image classification,
translation invariance is like a given, but but it's not, right? It's not a given the pictures
that we feed to these algorithms, most often the object is in the middle, most often, you know,
it's kind of upright, like the sky is on top, and the floor, yes, I can hold my camera like this,
but I don't, right? So it, it seems to be, it seems to be in many cases better to not
put the symmetries in there until we're like, really, really, really, really sure that these are
actual symmetries, because with more data, it seems the model that does not have the prior inside
becomes better than the model that does have the prior inside, if that prior doesn't exactly match
the world is, do you think that's a fair characterization of, for example, why transformers
with large data all of a sudden beat classic CNN models or come close to them?
I think it's, it's always this question of the trade off between how much your, your model and
how much you learn and I remember when I was a student, there was this maxim that that machine
learning is always the second best solution. And maybe nowadays with deep learning showing
some remarkable set of successes, I'm probably less confident in this statement, but it's probably
still quite true that the more you know about your problem, the better chances that machine
learning will work for it. To me, it makes sense to model as much as possible and learn what is
current or impossible to model. And in practice, of course, there is a spectrum of possibilities
of how much of these prior assumptions are hardwired into the architecture. And usually,
it's a trade off between the, for example, computational complexity availability of the
data also hardware friendliness. And if you think of what happened in computer vision,
it's probably a good illustration that that convolutional networks have translational
environments, for example, as you mentioned, but in many problems, you might benefit from
other symmetries such as rotations, again, depends on the application, but imagine that you want to
recognize, I don't know, traffic signs, when you can also tilt your car. And you may ask why
in these applications as well CNNs are still so popular. And probably one of the reasons is that
they met very well to the single instruction multiple data type of hardware architectures
that GPUs offer. And you can compensate for explicitly not accounting for rotational symmetry
with data augmentation, and more complex architectures and larger training sets. And
this is exactly what happened a decade ago, this convergence of three trends, the availability
of compute power, right, the GPUs, algorithms that map well to these computational architectures,
and these happen to be convolutional networks, and also very large data sets that you can train
these architectures on, such as ImageNet. So many of the choices that become popular in the
literature are maybe not necessarily theoretically the best ones. So I think in hardware design,
there is this phenomenon that is called the hardware lottery, when it's not necessarily the
best algorithm and the best hardware that solve the problem, it's just some lucky coincidence,
and they are happy marriage that makes them successful.
We keep raising this point about how transformers can be seen as special cases of attentional
GNNs, but the status quo is that people will use transformers for very many tasks nowadays,
and there may well be a good argument for considering them in this completely separate
light, and one possible explanation or justification for this is the hardware lottery, because,
yes, sure, transformers perform permutation equivalent operations over a complete graph,
but they do so in a way that is very, very highly amenable to the kind of matrix multiplication
routines that we can support very efficiently on GPUs nowadays. So basically, they can be seen as
the graph neural network that has won the current hardware lottery, even in cases where maybe it
will make more sense to consider a more constrained graph, especially if you have low data environments
or something like this, the potential overheads of running a full graph neural network solution
with message passing, which, given its extremely sparse nature, doesn't align that well with GPUs
and TPUs nowadays. Sometimes just using a complete graph neural network is the more economical option
when you take all factors into account. And that's, and also the fact that they use an attention
mechanism, which is kind of a middle ground between a simple diffusion process on a graph,
which we just kind of average things together based on the topology, and the full on message
passing where you actually compute a full on vector message to be sent across the edges.
Like it strikes a nice balance of scalability and still being able to represent a lot of functions
of interest, especially when your inputs are just word tokens, right? So like, you know, in a way,
it's a GNN that strikes a very nice sweet spot. And that's probably the reason why it's become so
popular in current times. Now, of course, there is a chance that hardware, and there's actually a
pretty high probability that hardware will catch up to the trends in graph representation learning,
and we will start to see a bit more graph oriented hardware. But at least for the time being, yeah,
there's a bit of a combination of what's theoretically making the most sense for your problem,
and what the hardware that you have right now will support the most easily. Yeah.
There is a British startup, I think they have reached recently a unicorn status
called Graphcore. And you can already hear from the name of the company,
that there is a graph inside, they try to develop hardware that goes beyond the traditional
paradigm. Yeah, I'm really interested in the hardware lottery. We had Sarah Hooker on that massive
shout out to Sarah. And Janik made a video on the hardware lottery paper as well. I mean,
I'd push back a little bit. I think there's also a bit of an optimization and an algorithmic lottery
going on. I think there's something very, very interesting about stochastic gradient descent
and the kind of data that we're working with. But this actually gets to my next question,
which is about why exactly geometry is a good prior and how principled it is. So
it seems like these geometric priors are principled. And they have utility because they are
low level primitives, right? They're ubiquitous and natural data. But why exactly is it a
principled approach to start with things we know, which is to say geometric primitives,
and to work upwards from there? You know, what would it look like if we went top down instead?
And what makes a good prior? I mean, one way to think about it is the actual function space
that you're searching through, you know, this hypothesis space, it's not just about being able
to find the function easily in that space, or the simplicity of the function you find.
Chalet would say it's the information conversion ratio of that function. So, you know, can you
use this function that you found to convert a very small piece of information and experience space
into new knowledge or a new ability? But how do you find these functions?
One of the points that we try to make in the book is the separation between the domain and
the group that you assume on the domain, the symmetry group. So you might have the same domain,
like two dimensional grid or two dimensional plane. And for example, the translation group
or the group of rotations and translations or the group of rigid motions that also include
reflections. So these are completely separate notions. And which one to choose depends on the
problem. The choice of the domain really comes from the structure of your data. So if your data
comes as an image, then of course, you use a grid to represent it as the choice of the domain.
Now, which symmetry group to use is a more subtle point. And it really depends on what you're trying
to achieve. You can think of, for example, traffic sign recognition. When a car drives on the road,
usually the signs will have certain orientation. It's very unlikely that you will see it upside
down. So really the only invariance or the only kind of symmetry you have is translation. So CNNs
in this case would work perfectly well. So for example, we have histopathological samples. So
you have a slice of tissue that you need to put under the microscope. So you can naturally flip
the glass. You don't know how it is oriented. So reflections are also initial transformation.
So in the traffic signs, of course, this is not physical unless you see your sign in the back
mirror. But in this histopathology example, it is an initial transformation. So the choice of the
symmetry group and what makes a good geometric prior is really dictated by the specific problem.
It's often very hard, though, to actually choose because we often don't really know, right, coming
back to what I said before, if we actually hit the group correctly. And in fact, we've sort of seen
the more successful approach. And this might be hardware specific, but it seems the more
successful approach is often to actually make data augmentations with respect to what we assume
are symmetries. So to know, I think of all the color distortions that we do to images, we rotate
them a bit, we rescale them and so on, it will be definitely possible to build architectures that
are just invariant to those things. However, it seems to be a more successful approach in practice
to put this all into the data augmentations. What's your take on that? How do we choose between
putting prior knowledge into augmentations versus putting prior knowledge into the architecture?
It's not a binary choice. It's not either your model or your augment with data.
One of the key principles that we also emphasize in the book is that, of course,
this perfect invariance or equivariance is a wishful thinking. In many cases, you want to get
the property that we call geometric stability. You have some transformation that is approximately
a group. Or imagine that you have a video where two objects are moving, let's say one car moves
left and another car moves right. So there is no global translation that describes the relation
between the two frames in this video. The geometric stability principle tells you that
if you are close enough to an element of the group, if you can describe these transformations
as an approximate translation, then you will be approximately invariant or approximately
equivalent. This is actually what happens in CNN. This was shown by Joan. They use this
motivation to explain why convolutional neural networks are so powerful. Roughly speaking,
if I don't have a translation, but for example, if I have an MNIS digit and you have different
styles of the digits, you can think of them as warping of some canonical digits. So in this case,
even though it's not described as a translation and the neural network will not be invariant or
equivariant to this kind of transformation, it will be stable under these transformations.
And that's why data augmentation works in some sense that you're extending your
invariance or equivariance class to approximate invariance and equivariance.
Taco also had a few interesting things to say about data augmentations versus
building the inductive priors into the model. This is Taco. Is your preference towards...
I assume it is towards creating inductive priors in the architecture around geometry
instead of data augmentation? Oh, that's a good question. I think it depends on the
on the problem. Like in some cases, you don't have a choice. So graphs are a great example.
The group of permutations is n factorial elements. If n is large, you have 1000 nodes,
you're never ever going to be able to exhaustively sample that group. And so it's better to just
build it in. And that's also why no graph neural net doesn't respect the symmetry. Nobody's
suggesting you should do that by data augmentation. In some other cases, it is
somewhat possible to sample a reasonably dense grid of transformations in your group.
And indeed, augmentation is turning out to be very important in unsupervised learning and
self-supervised learning techniques. So I am actually... I look very positively towards that.
I don't think it's wrong to put in this knowledge using data augmentation. But in some cases,
like let's say when you're on classifying medical data like cells in a dish, a histopathology image
or something, you just know for sure there's translation and rotation symmetries. The cells
don't have a natural orientation. And in those cases, I do think it makes sense to build it
into the architecture for the simple reason that if you build it into the architecture,
you're guaranteed that the network will be equivariant, not just at the training data,
but also at the test data. So you never have that the network would make the correct classification
for your test data when you have it in one orientation, but when you rotate it, it suddenly
does something different, which can happen if even when you present your training images in
all possible orientations. So I think for that reason, equivariance does tend to work better
in those cases where there's an exact symmetry in the data. We have actually demonstrated that
empirically, where for example, in a medical imaging problem of detecting lung nodules in
three dimensional CT scans, we started off with a convolutional network with a data augmentation
pipeline, which completely tuned what state-of-the-art method at the time. And we simply replaced
all the convolutions by group convolutions that respect to rotational symmetries as well as
the translations. And we get very significant improvement in performance. So that goes to show
that in practice, often data augmentation can't get you all the way. So for the cases where there's
an exact symmetry, or where the group of symmetries is very large, I think building it into the network
is the way to go for the foreseeable future. But there are many cases where augmentation is also
well, where augmentation is the way to go.
Fascinating. I'm really interested in this notion that could these symmetries actually be
harmful? I know Joanne, for example, spoke about the three sources of error in machine learning
models. And one of them is the approximation error. And normally when we get signals, they come to us
in a contrived form, don't know, they might be projected onto a planar manifold. And
you know, the sky is always up, for example. Does it really help us having these geometrics
symmetries as primitives in the model? Yeah, that's a good question. The simple answer is,
if your problem doesn't have the exact symmetry, then at least in the limit of having infinite data,
building in a covariance is going to be harmful. And it's better to learn the true structure of
the data, this approximate symmetry, which you should be able to pick up from the data alone.
So that's one thing that still means in a low data regime, it can be very useful,
even when the symmetry is approximate. But I would also say that sometimes,
in machine learning, we have a tendency to put too much faith into the
evaluation metrics and data sets. So we say, we want to solve computer vision. And what we mean
is we want to get a high score on ImageNet. And certainly it's true in ImageNet, the images tend
to appear in upright position, and they are photographed by humans. So the key objects are
in the center most of the time, etc. So these are biases that you could exploit,
and you might stop yourself from exploiting them if you build in the symmetry. But that's only a
problem if you put on your blinders and you say ImageNet accuracy is the only thing that counts.
You might very well think, if you want to build a very general vision engine,
it is useful that it still works if suddenly the robot falls over and has to look at the
world upside down. So the symmetry can still be there in principle, even if it's not there in
practice in your data set. And then there's maybe a robustness versus computational efficiency
tradeoff. So yes, maybe you're willing to acknowledge if your robot falls over, you still
want it to work. So you want that rotation equivariance. But then again, if we don't have
to process the images upside down, in the 99% of cases where the images are upright,
we gain some computational efficiency. So there's a tradeoff there. And yeah, I don't
think there's a right or wrong answer. It's something you have to look at on a case by case
basis. When I put this to Professor Bronstein as well, he also said that you folks were looking at
trying to remember how he described it. I think he said there was a kind of representational
stability which allowed for approximate symmetries. So it's not necessarily that you're going for
these precise symmetries, you're actually looking for a little sort of margin of robustness going
to moving into approximate symmetries that you might not have explicitly captured.
I agree. I think that's also a very important philosophy and approach. To take the group,
think of it as somehow embedded in a larger group, like most of the geometrical symmetries
that we think about are somehow a subgroup of diffeomorphisms. And then if you say,
I don't want invariance or equivariance, but some kind of stability or smoothness to
elements in the group plus small diffeomorphisms, for instance, you might get some of the
generalization benefit without unduly limiting the capacity of your model.
Is there a hope though that we can get this approximate? Because I see your point, I think,
is that, or one of the points is, I think, is that if we program like a symmetry into the
architecture, it will be rather fixed, right? We make an architecture translation invariant,
it's going to be fully translation invariant. Are there good ways to bring approximate
invariances into the space of the architectures that we work with?
Yeah, I mean, I have a very quick answer, maybe not too satisfying, but one very simple one,
if you think of neural network blocks as like components that implement different symmetries,
and then you think of like a calculus of these blocks as you know, building your deep learning
architecture. One very simple representational tool that we can use to allow the model to use
the symmetry, but also not use the symmetry is the skip connection. So essentially, you could have
a model that processes, for example, your graph data using a particular connectivity structure
that you want to be invariant to. And you can also use say a transformer that processes things
in a completely permutation invariant way over the complete graph. And you can just shortcut the
results of one model over the other model, if you want to give also the model the choice to ignore
the previous one. So maybe not a very, you know, detailed and satisfying answer, but that's one
simple way in which we could do something like this. And in some of our more recent algorithmic
reasoning blueprint papers, we do exactly this, because one very important thing that we're trying
to solve is apply classical algorithms to problems that would need them. But the data is super rich,
and it's really hard to, you know, massage it into the abstractified form that the algorithm needs.
For example, you want to find shortest paths in a road network, a real world road network,
you cannot just take all the complexity of changing weather conditions, changing diffusion
patterns on the on the roads and the roadblocks and all these kinds of things and turn that into
this abstractified graph with exactly one scalar per each edge. So you can apply dykstra or something
like this, like, it's just not feasible without losing a ton of information. So what we're doing
here is we make this high dimensional neural network component that simulates the effects of
dykstra. But we're also mindful of the fact that to compute say the expected travel time,
there's more factors at play than just the output of a shortest path algorithm, right?
There could well also be some flow related elements, maybe just some elements related to the
current time of day, human psychology, whatnot, right? So we start off by assuming the algorithm
does not give the complete picture in this high dimensional noisy world. So we always, as default,
as part of our architecture, incorporate a skip connection from just, you know, a raw neural
network encoder over the algorithm. So in case there's any model free information that you want
to extract without looking at what the algorithm tells you, you can do that. So maybe I don't
know, Yannick, if that answers your question about approximate symmetries, but that's, that's the
kind of divide by God's when I heard the question. I mean, that's a very, that's a very practical
answer for sure that that, you know, you can actually get out there. It even opens the possibility
to having maybe multiple kinds of skip connections and whatnot, you know, having dividing up your
symmetries into individual blocks that are run in parallel, maybe. But that brings me to maybe
another thing we've talked about, you know, there are symmetries, you want to incorporate them into
your problem and so on. And we've also talked about the symbolic regression beforehand to maybe
parse out symmetries of the underlying problem. What are the current best approaches if we don't
know the symmetries? So we have a bunch of data, we suspect there must be some kind of
symmetries at play because they're usually are in the world, right? And they, if we knew them,
we could describe our problems in very compact forms and solve them very efficiently, but we don't
often know. So what are the current state of the art? When I don't know the symmetries, how do I
discover what group structure is at play in a particular problem? I don't think that there is
a single approach that solves this problem in a satisfactory manner. And one of the reasons why
because the problem is ambiguous. So maybe an example, think of objects mostly translate
horizontally, but you also have a little bit of vertical translation. What is the right
symmetry structure to model? Is it a one dimensional translation group or a two dimensional
translation group? Do we want to absorb the vertical, the slight vertical motions as the noise
and deal with it as a data augmentation, or you want to describe it in the structure of the group
that you discover? So there is no single answer. So you cannot say that one is correct and another
one is wrong. Yeah, I think this was kind of where I was going with the question of how
principled are the symmetries? And the symmetries seem to be hierarchical just in the same way that
geometries are hierarchical. You were saying that, for example, the projective geometry is kind of
subsumes Euclidean geometry, but I had a little thought experiment. So imagine I gave you a large
data set produced by a recursive fractal pattern. Now nature is full of fractals, trees, rivers,
coastlines, mountains, clouds, seashells, and even hurricanes. So let's say I didn't tell you the
simple rule which produced this pattern. Now what kind of regularities would you look for in the
model that you built? I mean, it seems obvious that there would be an expanding scale symmetry,
which might resemble the original rule. But it feels like there'd be plenty of other emergent,
abstract symmetries which are not obviously related to the simple rule which produced the pattern.
I mean, Janik was just saying, when you look at computer vision, you see a kind of regularity
or invariance to color shifts, for example. So our fractals are good analogy for physical
reality. And should we be looking for the low-level primitive regularities which I think you're
advocating for? Or should we be looking at more abstract emergent symmetries which appear?
In the 90s, there was a famous paper by Michael Barclay on fractal coding. And they claimed really
unbelievable compression ratios for natural images. And the way it worked was to try to
reassemble the image from parts of itself. And possibly, of course, you can take parts and
subject them to some geometric transformation. So roughly, if you have a page of pixels,
you can approximate it as another page taken from somewhere else in the image that you translate,
rotate, and scale. And then the image was represented as an operator that makes such a
decomposition. And this operator was constructed in a special way to be
contractive. And then they used the Banach-Fix point theorem that you can apply this operator
to any image. So you can start with the noise for example, completely random image. And you have
the target image emerge after a few iterations. So that this iterative scheme will converge to the
fixed point of the operator, which is the image itself. And it was actually used in the industry,
well, Microsoft and CARTA encyclopedia. I don't know how many viewers are old enough to remember it.
But the main issue was the difficulty to build such operations. The compression
was very asymmetric. It was very easy to decode. You just take any image and apply this operator
multiple times. But it was really very hard to encode. And in fact, some of these constructions
that showed remarkable compression ratios were constructed semi by hand. I should say that in
more recent times in computer vision, for example, the group of Michali Rani from the Weizmann
Institute in Israel used similar ideas for super resolution and image denoising where you can
build the clean or higher resolution image from bits and pieces of the image itself. So it's a
single image denoising or super resolution. But what you do is you try to use similarities across
different positions and scales. That's absolutely fascinating. I mean, I spend a lot of time thinking
about this because my intuition is that deep learning works quite well because of the strict
structural limitations of the data which is produced by our physical world, right? And
would you say that physical reality is highly dimensional or not? If it's highly dimensional,
is it because it emerged from a simple set of rules or relations like we were just talking
about? Because I think what you're arguing for is that it could be collapsible in some sense.
Probably the term dimension is a bit frivolously used here. But I would say that it's
probably fair to say that at some scale, many physical systems can be described with a small
number of degrees of freedom, parametres that capture the system. And as we are talking,
I'm sitting in a room, I'm surrounded by probably a quadrillion of gas molecules in the air that
fly through the room and collide with each other and the walls of the room. So at the
microscopic level, the dimension is very high. So it's absolutely intractable if I were to model
each molecule and how it collides, I will have a huge number of degrees of freedom. And yet if we
zoom out, we can model the system statistically, and that's exactly the main idea of thermodynamics
and statistical mechanics. And this macroscopic system is surprisingly simple. It can be described
by just a few parameters such as temperature. And the example of fractals that you brought up before
essentially show that you can create very complex patterns with very simple rules that
apply locally in a repeated way. This might be a question for you, Peter. The geometric blueprint
works brilliantly in the ideal world where we can compute all of the possible group actions. But
graph neural networks, for example, you know, the permutation group is factorial in size,
which means we need to rely on heuristics like graph convolutions. So how much better would graph
neural networks be if we could compute all of the permutations? I mean, are you happy with these
heuristics in general? So that is a very good question. And yes, so let's just start from stating
the obvious. If you want to explicitly express every possible operation that properly commutes
with the graph structure, and in that sense is a graph convolution, you would not be able to
represent that properly as a neural network operation because you have to store in principle
a vector for every single element of the permutation group. So unless your graph is super
tiny, that is just not going to work. So on one hand, this is a potentially annoying result.
On another hand, it is also exciting because we know that even though we ended up like doing most
of our graph neural network research in this very restricted regime of I'm going to define a
permutation invariant function over my immediate neighbors, and that will as a result translate
into a permutation equivalent function over the whole graph. Even though most of our research has
happened in that particular area, we know from this result that there actually exists a huge wealth
of very interesting architectures beyond that. And I think one of the potentially like earliest
examples that have demonstrated that there exists this wealth of space is actually one of
Jean Bruno's earlier papers on the graph Fourier transform that, you know, analyzing from a pure
signal processing angle, they have shown that you can represent basically every proper graph
convolution as just, you know, parameterizing its eigenvalues with respect to the eigenvectors of
the graph Laplacian. So, but the big issue that kind of limits us from going further in this
direction right now is the issue that Michael highlighted of geometric stability. So basically,
a lot of these additional graph neural networks that do something more interesting than one
hop spatial message passing pay the price in being very geometrically unstable. So the graph Fourier
transform in its most generic form will have every single node in the graph be updated based
on whatever is located in any other node in the graph, very conditional on the graph topology. So
if you imagine any kind of approximate symmetry, any kind of perturbation either in the node features
or the edge structure of the graph, this, you basically don't have any protection against
that like that error is going to immediately propagate everywhere. And as a result, you'll
end up with a layer that theoretically works really well. But in practice is very unstable to
these kinds of numerical or inaccuracy issues. One thing that's also very important to note is that
often in graph neural networks, we have this subtle assumption of we have the graph and we're
using this graph that's given to us. But who guarantees that the graph that's given to you is
the correct one actually very often, we estimate these graphs based on very, very weird heuristics
ourselves. So basically, all of these kinds of perfection assumptions are what might limit the
applicability of these kinds of layers. But that being said, I find it comforting that these layers
exist, which means that there are meaningful ways to push our research forward to potentially
discover new, you know, wonderful basins of geometric stability inside these different,
you know, layers that may not just do one hop message passing. So that's my take on this,
like it gives me, it gives me faith that there's more interesting things to be discovered.
That being said, it is pretty tricky to find stable layers in that vast landscape.
Michael, do you have some thoughts on this as well? I know you've worked quite a bit on these
geometric stability aspects. I just wanted to add one thought about it that essentially,
locality is a feature, not a bug in many situations. And in convolutional neural networks,
actually, if you look again, historically, the early architectures like AlexNet, they started
with very large filters and the few layers or relatively few layers, I think something like
five or six. And nowadays, what you see is very small filters and hundreds of layers.
One of the reasons why you can do it is because of compositionality properties. So you can,
you can create complex features from, from simple primitives. So in some other cases,
like, like manifolds, there are deeper geometric considerations why you must be local, so related
to what is called the injectivity radius of the manifold. On graphs, well, maybe we like a little
bit the theoretical necessity to be local, besides, of course, the computational complexity. But
in many cases, it is actually a good property because many problems do not really depend on
distant interactions. So if you think of social networks, probably most of the information comes
from your immediate neighbors. Is there some sort of, let's assume I, you know, I have a graph,
and I have my, my symmetries, my groups that I suspect there are in the problem, or I want to
be invariant to, is there like, can you give us a bit of a practical blueprint of how would I build
a network that, you know, takes this as an input and applies this? How would you go about this,
you know, what would be the building blocks that you choose, the orders and so on? Is there
overarching principles in, in how to do? I don't think that there is really a general
recipe. So it's problem dependent. But maybe one example is applications in chemistry. The basic
structure that they have in graph is a privatization invariance. This has to do with the
structure of the graph itself. It says nothing about the structure of the features.
You might also have some secondary symmetry structure in the feature space. In case of molecules,
for example, you might have a combination of features. Some of them are geometric. So it's
actually not an abstract topological graph. It's a geometric graph. A molecule is a graph that
lives in three dimensional space. And so the features are the positional coordinates of the
nodes, as well as some chemical properties such as atomic numbers. Now, when you deal with the
molecule, you usually don't care about how it is positioned in space. It wants to be
equivariant to rigid transformations. And therefore you need to treat accordingly the
geometric coordinates of the nodes of this graph. And this is actually what has been successfully
done. So when you do, for example, virtual drag screening, architectures that do message passing,
but in a way that is equivariant to these rigid transformations actually are more successful
than generic graph neural networks. Also, this principle was exploited in the recent version
of AlphaFold, where I think they call it point invariant attention, which is essentially a form
of a latent graph neural network or a transformer architecture with equivariant message passing.
Yeah, I'd just like to add one more point to this conversation, which is maybe a bit more
philosophical, but it relates to this aspect of building the overarching symmetry discovering
procedures, which I think would be a really fantastic thing to have in general. And I hope that
some component of a true AGI is going to be figuring out, making sense of the data you're
receiving and figuring out what's the right symmetry to bake into it. I don't necessarily
have a good opinion on what this model might look like. But what I do say is just looking at the
immediate utility of the geometric deep learning blueprint, we are like, I think very strictly
saying that we don't want to use this blueprint to propose, you know, the one true architecture.
Rather, we make the argument that different problems require different specifications,
and we provide a common language that will allow, say, someone who works on primarily grid data to
speak with someone who works on manifold data without necessarily thinking that, you know,
you know, somebody might say, commonets are the ultimate architecture. Someone else might say
GNNs are the ultimate architecture. And in some ways, they could both be right and they could
both be wrong. But this blueprint kind of just provides a clear delimiting aspect to these
things, just like in the 1800s, you had all these different types of geometries that basically lived
on completely different kinds of geometric objects, right? So hyperbolic, elliptic, and so on and so
forth. And what Klein-Zerlangen program allowed us to do was, among other things, reason about
all of these geometries using the same language of group invariance and symmetries, right? But in
principle, the specifics of whether you want to use a hyperbolic geometry or whether you want to use
an elliptic geometry, partly rests on your assumption what the main do you actually live in,
right? When you're doing these computations. So I think just generally speaking, I think that
having a divide is a potentially useful thing, as long as you have a language that you can use
to index into this divide, if that makes sense. It does make sense. But I have a feeling that some
people could benefit from geometric deep learning in their runaways. I mean, I don't want you guys
to motivate geometric deep learning in general, because I think, you know, a lot of deep learning
with a structured prior is already geometric deep learning is as you folks demonstrated in
your blueprint, you know, like RNNs and CNNs, for example. So, you know, like it or not, we're already
all using geometric deep learning. But some of the esoteric flavors of geometric deep learning,
particularly on irregular meshes, they seem a little bit out there, don't they? I mean, it's
possible that many people could benefit from this, but they just don't know about it yet. I was
thinking that, for example, if I had a LiDAR scanner on my phone, and the result is a point cloud,
which is not particularly useful. But I would presumably transform it into a mesh, which would
be more useful. But is it possible that loads of data scientists out there are sitting on data sets
that they could be thinking about geometrically, but they're not? Many folks are exotic. It's
probably in the eyes of the beholder. And well, in machine learning, probably they are, to some
extent, exotic. But joking apart, many folks are a convenient model for all sorts of data.
And the data might be a high dimensional, but still have a low intrinsic dimensionality or
can be explained by a small number of parameters or degrees of freedom. And this is really the premise
of nonlinear dimensionality reduction. And for example, the reasons why data visualization
techniques such as TSE and E at all work. And maybe the key question, as you're asking is,
how much of the manifold structure of the continuous manifold you actually leverage? And
in the TSE example, the only structure that you really use is local distances.
So if you think of a point cloud, of course, you can deal with it as a set. But if you assume
that it comes from sampling of some continuous surface, you can probably say more. And this is
forgivable what we tried to do in some of our works on geometric deep learning in applications
in computer vision and graphics. And measures are one way of thinking of them is as graphs and
steroids, where we have additional structure to leverage. So it's not only nodes and edges,
but also faces. And in fact, measures are what is called simplicial complexes.
As to the practical usefulness, computer vision and graphics are obviously the two fields where
geometric deep learning on measures is important. And just to give a recent example
of a commercial success. There was a British startup called the AI. It was founded by
my colleague and friend, Yasunos Kokinos. I was also one of the investors. And we had a
collaboration on three different reconstruction using geometric neural networks. And the company
was acquired last year by SNAP and these technologies already now part of SNAP products. So you see it
in the form of some 3D avatars or virtual and augmented reality applications that SNAP is
developed. Professor Bronstein, I saw that you were doing some really cool stuff with the higher
order simplicial coverings in graphs. And actually, I was going to call out your recent work on
diffusion operators and graph rewiring. There are so many cool things that we can do to graphs to
actually enable a little of this analysis. But there was a question from my good friend,
Zach Jost, who's one of our staff members here. And he says, what do you think is the most important
problem to solve with message passing graph neural networks? And what's the most promising path
forward? Probably we first need to agree about terminology. And to me, message passing and
here I agree with Petra is just a very generic mechanism for propagating information on the
graph. Now, traditional message passing that is used in graph neural networks uses the input
graph for this propagation. And we know, right, as we discussed that it is equivalent to device
for a lemon graph isomorphism test that has limitations in the kinds of structures it can
detect. Now, there exists topological constructions that go beyond graphs, such as simplicial and
cell complexes that you mentioned. And what we did in our recent works is developing a message
passing mechanism that is able to work on such structures. And of course, you may ask whether
we do encounter such structures in real life. So first of all, we do the measures that I mentioned
before are in fact, simplicial complexes. But secondly, what we show in the paper is that we
can take a traditional graph and lift it into a cell or simplicial complex. And probably a good
example here is from the domain of computational chemistry, the graph neural network that you
apply to a molecular graph would consider a molecule just as a collection of nodes and edges,
atoms, and chemical bonds between them. But this is not how chemists think of molecules.
They think of them as structures such as, for example, aromatic rings. And with our approach,
we can regard the rings as cells. So we have a special new object, and we can do a different
form of message passing on them. And we can also show from the theoretical perspective that
this kind of message passing is strictly more powerful than the vice for a lemon algorithm.
Do you see entirely new problems opening up that we wouldn't even have, let's say,
we wouldn't even have dared to touch before, you know, in, let's say, we simply have our
classic neural networks or whatnot, or even our classic graph message passing algorithms.
Do you see new problems that are now in reach that previously with none of these methods were
really, let's say, better than random guessing? It's a very interesting question that I think
I'll answer from two angles, because you could, like, there could be like some
longstanding problem that you knew about and wouldn't dare to attack. And now maybe you feel
a bit more confident to attack it. There's also the aspect of uncovering a problem,
because when you start thinking about things in this particular way, you might realize,
hang on, to make this work, I made some assumptions. And those assumptions actually
don't really hold at all in principle. So how do I make things, you know, a little bit better?
So I'll try to give an example for both of those. So in terms of a problem that previously,
I don't think was very easy to attack. And now we might have some tools that could help us attack
it better. I have a longstanding interest in reinforcement learning. Actually, when I
started my PhD, I spent six months attacking a reinforcement learning problem with one super
tiny GPU. And that was, at that time, a massive time sink. Actually, DeepMind ended up scooping
my work sometime after that. And I quickly moved to things that were more, you know, doable with
the kind of hardware that I had at the time. But, you know, I always had a big interest in this area.
And after joining DeepMind, I started to contribute to these kinds of directions more and more.
And I think that basically, there are a lot of problems in reinforcement learning
concerning data efficiency. So when you have to learn how to meaningfully act and do stuff,
which is actually a fairly like low dimensional signal compared to the potential richness of
the trajectories that you have to go through before you get that useful signal, like long
term credit assignment, all these kinds of problems, I feel like we can start to get more
data efficient reinforcement learning architectures by leveraging geometric concepts and also
algorithmic concepts. So to give you one example of this, we have some months ago put out a paper
on the archive called the executed latent value iteration network or x selvin, where we have
captured the essence of an algorithm in RL, which perfectly solves the RL problem. So the value
iteration algorithm, assuming you give it a Markov decision process will give you the perfect
policy for that Markov decision process. So it's a super attractive algorithm to think about when
you do RL, big caveat, right? You need to know all the dynamics about your environment, and you need
to know all the reward models of the environment before you can apply the algorithm. So this obviously
limits its use in the more generic deep reinforcement learning setting. But now with the knowledge of
the underlying geometry of the graph of states that the MDP induces and the algorithmic reasoning
blueprint, we actually taught the graph neural network, which aligns super well with value
iteration, actually, we taught it to in a nicely extrapolating in a reasonably extrapolating way
on a bunch of randomly sampled MDPs, learn the essence of the value iteration computation,
and then we stitched it into a planning algorithm in a deep reinforcement learning setting.
And just by like training this pipeline end to end with a model free loss, we were able to
get interesting returns in Atari games much sooner than some of the competing approaches. So
it's a very small step. It still requires, you know, 100,000 200,000 iterations of
playing before you get meaning, some meaningful behaviors start to come out. But
it's a sign that we might be able to move the needle a bit backwards and not require, you know,
billions and billions of transitions before we start to see meaningful behavior emerge. And I
think that's very important because in most real world applications of reinforcement learning,
you don't have a budget for billions of interactions before you have to already learn a
meaningful policy. So that's one side, I think. And just generally in reinforcement learning,
graphs appear left, right and center, not just in the algorithms, but also in the structure of the
environment and these kinds of things. So I think that's one area where geometric deep learning
could really help us, you know, get better behaviors faster, not necessarily solve it
better than the standard deep RL, but, you know, get the better behaviors and fewer interactions.
And as for one problem that we have uncovered through this kind of observational lens,
you know, as I said, often in graph representation learning, we assume innocently that the graph
is given to us, whereas very often this is not the case. So this divide has brought about this new
emerging area of latent graph learning or latent graph inference, where the objective is to learn
the graph simultaneously with using it for your underlying decision problem. And this is a big
issue for neural network optimization, because you're fundamentally making a discrete decision in
there. And if the number of nodes is huge, you cannot afford to start with the n squared approach
and then gradually refine it. So currently, the state of the art in many regards of what we have
here is to do a K nearest neighbor graph in the feature space, and just hope that that gets us
most of the way there. And usually this works quite well for getting, you know, interesting answers,
because, you know, if you have a decent ish enough KNN graph, you will cover everything that you need
reasonably quickly. But, you know, then there that raises the issue of what if the graph itself is a
meaningful output of your problem, what if you're a causality researcher that wants to figure out
how different, you know, parts of information interact to them, they probably wouldn't be
satisfied with a K nearest neighbor graph as an output of the system. So yeah, I feel like there's
a lot of work to be done to actually scalably and usefully do something like this. And I don't
have a better answer than what I just said is the state of the art. So a potential open problem
for everybody in the audience today. Absolutely. And you touch on some really interesting things
that I think causality is a huge area that we could be looking at graphs on. And also we had
Dr. Tom Zahavi from DeepMind, one of your colleagues, and he said that, you know, he looks at
meta learning and also diversity preservation in in agent based learning. But he thinks that
reinforcement learning is just about to have its image net moment where we can discover
a lot of the structure in these problems, which is fascinating. I would like to bring up
one application where maybe quantitative improvement that is afforded by a genetic
deep learning can lead to a qualitative breakthrough. And this is a problem of
structural biology. Alpha fold is one such example for correctly geometrically modeling
the problem you get a breakthrough in the performance. So it's indeed an image net
moment that happened in this field. And now once you have sufficiently accurate
prediction of 3D structure of proteins, it suddenly enables a lot of interesting applications
for example, in the field of drug design. So potentially entire pharmaceutical
pipelines we invented with the use of this technology. And the impact can be extraordinary.
It's so true. I mean, Professor Bronstein, when I was watching your lecture series,
it blew me away when you were talking about all of the applications. I think Yannick said a minute
ago that it's almost as if some of these applications are just so ambitious that the thought
wouldn't even have crossed our mind that we might be able to do it before, like for example,
being able to predict facial geometry from a DNA sequence. So we might be able to look at an old
DNA sequence and actually see what that person looked like. That would have been unimaginable
just a few years ago. So that's incredible. I'm really interested in your definition
of intelligence, right? And whether you think neural networks could ever be intelligent.
Douglas Hofstadter, for example, he wrote the famous book Godel Escher Bach. It was a
Pulitzer Prize winning book in the 1970s, but he made the argument that analogy is the core of
cognition. He said that analogies are a bit like the interstate freeway of cognition.
They're not little modules on the side or something like that. And I think that in a way,
analogies are also symmetries, right? So when I say that someone is firewalling a person,
it means that they don't want to talk with that person. It's a symmetry between the abstract
representation of a real network firewall and an abstract social category.
So does this require a different neural network architecture or could geometric deep learning
already deliver the goods, right? It's almost as if it's just a representation problem.
I think it's a very important question, one which I cannot claim to have the right answer to. And
my definition is I guess a little bit skewed by the specific research that I do and the
engineering approaches that I do. But I think in large, I agree with the idea of analogy making,
and maybe I would take it a step further, right? Where you have a particular set of
knowledge and conclusions that you've derived so far, a set of primitives that you can use once
your information comes in to figure out how to recompose them and either discover new analogies
or just discover new conclusions that you can use in the next step of reasoning.
And it just feels really amenable to a kind of synergy of, as Daniel Kahneman puts it,
System 1 and System 2, right? You have the perceptive component that feeds in the raw
information that you receive as your input data, transforms it into some kind of abstract
conceptual information. And then in the System 2 land, you have access to this kind of reasoning
procedures that are able to take all of these concepts and derive new ones from hopefully a
nice and not very high dimensional set of roles. And this is why I believe that if we, that in
terms of like moving towards the, an architecture that supports something like this, I think we
have a lot of the building blocks in place with geometric deep learning, especially if we're
willing to, as I said, kind of broaden the definition of geometric deep learning to also
include category theory concepts because that might allow us to reconcile algorithmic computation
as well into the blueprint. So the idea is, you know, you have this, I mean, there's no need to
talk at length about all these great perceptive architectures. So I think we're already at a
point of, if we show our AGI lots and lots of data, it's going to be able to pick up on a lot of
the interesting things just by observing, like, you know, self-supervised learning,
unsupervised learning is already showing a lot of promise there. But then the question is,
where I think we still have quite a bit of work to do is once we have these concepts,
let's even assume that they're perfect. What do we do with them? How do we meaningfully use them?
And I think the reason why I believe there's a lot of work to be done there is because one of the
very key things that I do on a day to day basis is teach graph neural networks to imitate algorithms
from perfect data. So I give them exactly the abstract input that the algorithm would expect.
And I ask them, hey, simulate this algorithm for me, please. And it turns out that that is super,
super hard. Well, it's super easy to do it in distribution, but you're not algorithmic if you
don't extrapolate. And that's, I think, one of the big challenges that we need to work towards
addressing. Will geometric deep learning be enough to encompass the ultimate solution? I have a
feeling that it will. But, you know, I don't necessarily just based on the empirical evidence
we've been seeing in the recent papers that we've put out. But yeah, I don't I don't have a very
strong theoretical reason why I think it's going to be enough. Yeah, I'm fascinated by this notion
that intelligence isn't mysterious as we might think it is. It's a it's a receding horizon. And
it might in the end be disappointingly simple to mechanize. Actually, if you take the term literally,
intelligence come from the Latin word that means to understand. And I think what is meant by
understanding is really a very vague question. And probably if you ask different people, they
will give you different definitions. I will define it as the faculty to abstract information.
And in particular, information that is obtained in one context, the ability to use it in other
contexts, this is what we usually call learning. The way that this information is abstracted and
represented might be very different in a biological neural network in our brain versus an
artificial intelligence system. So if you hear some people saying that that the brain probably
doesn't really do geometric computations, my answer to that would be that we don't necessarily need
to imitate exactly the way that the brain works. We just need probably to try to achieve this high
level mechanism that is able to abstract information and applied as knowledge to different
problems. The definitions of artificial intelligence that are being used, like the famous Turing test,
I find is very disturbing that it's very anthropocentric. And it is actually probably very
characteristic of the human species more broadly. And this way, by judging what is intelligent,
what is not, we might potentially rule out other intelligent species because they are very different
from us. I may be obsessed with sperm whales because I'm working on studying their communication.
They are definitely intelligent creatures, but would they pass the Turing test?
Probably not. It's like subjecting a cat to a swimming test or a fish to climbing on a tree.
So I would just like to add to Michael's great answer, one quote that I think is very popular
and applies really well in this setting. The question of whether computers can think is about
as relevant as whether submarines can swim. When you built submarines, you weren't necessarily
trying to copy fish. You were solving a problem that was fundamentally slightly different.
So could be relevant in this case as well.
We spend quite a lot of time on this podcast talking about whether we should have an
anthropocentric conception of intelligence. A corporation is intelligent.
I'm starting to come around to the view of embodiment and thinking that there is something
very human like about our particular flavour of intelligence. But maybe there is a kind of pure
intelligence as well. And this was the end of my conversation with Tako Kohen. One of the really
interesting things is you're getting on to some of the work that you've done are being able to
think of group convolutions on homogeneous objects like spheres, for example, but also you moved on
to irregular objects like any mesh and you looked into things like fibre bundles and local convolutions.
Because these are objects, I think you said a homogeneous object is where you can't
perform some transformation to get from one place of the object to the other part of the object.
So what work did you do there on those irregular objects?
Yeah, that's a great question. So when we think of convolution, we're sort of
putting together a whole bunch of things, namely this idea of locality. So typically our filters
are local, but that's a choice ultimately. Convolution doesn't have to use a local filter,
though in practice we know that works very well. And there's the idea of weight sharing between
different positions and potentially also between different orientations of the filter.
And as I mentioned before, this weight sharing really comes from the symmetry.
So the fact that you use the same filter at each position in your image when you're doing
a two-dimensional convolution, that's because you want to respect the translation symmetry acting
on the plane. In the case of a general manifold or mesh, you typically not have a global symmetry.
If you think of a mesh representing a human figure, or let's say it's some complicated protein
structure, there may not be a global symmetry. Or sometimes in the case of say a molecule might
have some six-fold rotational symmetry, this symmetry may not be transitive as it's called,
meaning you cannot take any two points on your manifold and map one to the other using a symmetry.
In the case of a sphere, you can do that. Any two points in the sphere are related by rotation.
So we say a sphere is a homogeneous space, but these let's say this protein shape is not homogeneous,
even if it has some kind of symmetry. So in that case, if you try to motivate the weight sharing
via symmetry, you try to take your filter, put it in one position, move it around to a different
position using a symmetry, you're not going to be able to hit all positions. So you're not going to
get weight sharing globally. And that just is what it is. If you say I have a signal on this manifold
and I want to respect the symmetry, well, if there are no global symmetries, there's nothing to
respect. So you get no code strain. So you can just use arbitrary linear map.
Now, it turns out there are certain other kinds of symmetries called gauge symmetries that you
might still want to respect. And in practice, what respecting gauge symmetry will do is we'll
put some constraints on the filter at a particular position. So for example, that might have to be
a rotationally equivariant filter, but it doesn't tie the weights of filters at different positions.
So if you want that as well, then you can maybe motivate it via some kind of notion of local
symmetry. I have something on a local symmetry group point to motivate that in my thesis.
But there isn't a very principled way to motivate weight sharing on general manifolds
between different locations. Yeah, I'm just trying to get my head around this. So you're saying,
because the whole point that we're trying to achieve here is to have a parameter-efficient
neural network that uses local connectivity and weight sharing, as we do with, let's say,
plain RCNN, whereas when you have an irregular object, it's very, very difficult to do that.
So you're saying, in some restricted domain, you can do it. Let's say if you have a,
let's say, rotation equivariance, but you can't do the other forms of weight sharing.
I'm just trying to get my head around this, because with a graph convolution on your network,
for example, it seems like you can abstract the local neighborhood. This node is connected to
these other nodes. And potentially, that could translate to a different part of the irregular
mesh. So why can't you do it more than you suggested? I think if you want to be precise,
you just have to say, what are the symmetries that we're talking about here? And in a graph,
the most obvious one is the global permutation symmetry. So you can reorder the nodes of a graph
and really any graph neural net, whether they're coming from an equivariance perspective or not,
all graph neural nets in existence that have been proposed, they respect this permutation
symmetry. And typically, this happens through, let's say, in the most simple kind of graph
convolutional nets, like the ones by Kip van Belling, for example, there's a sum operation,
some messages from your neighbors. And some operation does depend on the border of the summands.
So it doesn't depend on the border of the neighbors. And that's why the whole thing becomes
every variant. So that's a global symmetry that all graph networks respect.
On that, though, could you not create a local, let's say if there was a local graph isomorphism,
and so I have an irregular object, but it has a local isomorphism, could I not use something like
a GCN, a local version of it to capture that isomorphism?
Yeah. So actually, this was something we proposed to do in our paper, Natural Graph Networks.
So this paper really has two aspects to it. One is the naturality as a generalization of
equivariance, I can talk about that. But another key point was that we can not just develop a
global natural graph network, as we call it, but also a local one. And what the local one will do
is it will look at certain local motifs. So maybe if you're analyzing molecular graphs,
one motif that you often find is, you know, aromatic ring or something, some ring with,
let's say, six corners, various other kinds of little small graph motifs.
And these motifs might appear multiple times in the same molecule or across different molecules.
And so what this method is doing is it's essentially finding those using some kind
of graph isomorph, local graph isomorphism, and then making sure that whenever we encounter
this particular motif, we process it in the same way, i.e. using the same weights. And if the
local motif has some kind of symmetry, like this aromatic ring, you can rotate it six times,
and it gets back to the origin or you can flip it over. So that's the symmetry of this graph
structure or an automorphism of this graph. And then the weights will also be constrained by this
automorphism group, this group of symmetries of the local motif. And various other authors also
have, I think, even Michael Bronstein and students have developed methods based on
similar ideas. Awesome. Taiko, it's been such an honor having you on the show. And actually,
you're coming back on the show in a few weeks' time, so we don't want to spoil the surprise.
But looking on this proto book that you've written with the other folks, what's the main
thing that sticks out to you as being the coolest thing in the book?
I think there's any one particular thing. What excites me is to put some order to the chaos
of the zoo of architectures and to see, actually, that there is something that they all have in
common. And I really think this can help new people who are new to the field to learn more
quickly, to get an overview of all the things that are out there. And I also think that this is
the start of at least one way in which we can take the black box of deep learning, which often is
viewed as completely inscrutable and actually start to open it and start to understand how the
pieces connect, which can then perhaps inform future developments that are guided by both
empirical results and an understanding of what's going on. Amazing. Thanks so much, Taiko.
Thanks for having me. It's been a pleasure.
Joan, thank you so much for joining us. This has been amazing.
Okay, no, thank you so much, Tim. It was very fun and best of luck. And I think I'm
let's maybe get in touch. Thank you very much. It's very nice to be talking to you today about
these completely random topics. Yeah.

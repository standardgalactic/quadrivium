1
00:00:00,000 --> 00:00:04,240
Welcome back to Street Talk, just a little bit of housekeeping before we kick off today.

2
00:00:04,240 --> 00:00:10,160
Polina Silivadov is one of the organisers for a charity AI conference called AI Helps Ukraine.

3
00:00:10,800 --> 00:00:15,600
Now, their main goal is to raise funds for Ukraine, both from the folks attending the

4
00:00:15,600 --> 00:00:20,080
conference and also from companies sponsoring the conference. And it's not too late to sponsor

5
00:00:20,080 --> 00:00:24,960
the conference and support it, so please do if you possibly can. Now, all of the funds that they

6
00:00:24,960 --> 00:00:29,360
raise will go to Ukraine Medical Support, which is a Canadian non-profit organisation

7
00:00:30,000 --> 00:00:34,960
which is specialising in humanitarian aid for Ukraine. Now, they have some of the world's

8
00:00:34,960 --> 00:00:41,760
leading AI experts keynoting at this event, so Yoshua Benjo, Timnick Gabru, Max Welling,

9
00:00:41,760 --> 00:00:47,280
Regina Basile, Alexei Efros and also one of our own personal favourites here on MLSD,

10
00:00:47,280 --> 00:00:52,720
Professor Michael Bronstein, the one and only. Now, the conference is online pretty much from

11
00:00:52,720 --> 00:00:58,720
now until the 6th of December and it's being hosted from Mela in Montreal. Their goal is to

12
00:00:58,720 --> 00:01:05,200
raise $100,000 and they really, really need the support of the AI community to club together and

13
00:01:05,200 --> 00:01:10,800
to just donate anything that you can. So, we'll link to the conference in the video and the podcast

14
00:01:10,800 --> 00:01:17,760
description. Please donate if you can and also share the links on your socials. Now, I'm at New

15
00:01:17,760 --> 00:01:24,560
Europe this week in New Orleans, so I'll be walking around with a camera. Please just bump into me

16
00:01:24,560 --> 00:01:31,280
and if you want to record any spicy takes on artificial general intelligence, then let's do it.

17
00:01:32,720 --> 00:01:38,240
Today is a conversation with Yoshua Bach, who's one of our most requested guests ever.

18
00:01:38,240 --> 00:01:42,800
We recorded the conversation back in April, which gives you a bit of an indication of our backlog.

19
00:01:42,800 --> 00:01:49,360
I can only apologise about the backlog. Keith and I recently started a new venture called X-Ray

20
00:01:49,360 --> 00:01:55,040
and as you can imagine, we've been working around the clock coding, just trying to get that business

21
00:01:55,040 --> 00:02:00,080
off the ground. But when we've made our millions, we'll devote all of our time to producing amazing

22
00:02:00,080 --> 00:02:04,880
content on MLSD. So, yeah, please bear with us loads and loads of cool content coming your way

23
00:02:04,960 --> 00:02:08,480
soon. I hope you enjoy the show today. Peace out.

24
00:02:08,480 --> 00:02:15,840
Dr Yoshua Bach is our most requested guest ever. Yoshua Bach is a cognitive scientist focusing

25
00:02:15,840 --> 00:02:23,760
on cognitive architectures, models of mental representation, emotion, motivation and sociality.

26
00:02:23,760 --> 00:02:29,760
Yoshua's interview on Lex's podcast, he did two interviews on Lex's podcast,

27
00:02:29,760 --> 00:02:34,240
have been watched over two million times so far, which is just absolutely unreal.

28
00:02:34,880 --> 00:02:39,680
Now Yoshua, I've watched many of your interviews and I still don't feel that I have a firm grasp on

29
00:02:39,680 --> 00:02:44,480
some of your views. So today, if you don't mind, I hope we can do a tour de force over some of your

30
00:02:44,480 --> 00:02:49,120
most important views in our shared space, to the extent that we can keep up with you, of course.

31
00:02:49,680 --> 00:02:56,400
Now, for example, we'd like to discuss Godel and computation, consciousness, digital physics,

32
00:02:56,400 --> 00:03:02,000
free will and determinism, large statistical models and indeed whether they're AGI or a

33
00:03:02,000 --> 00:03:06,880
parlor trick or something more esoteric. Now, when people talk about God or consciousness

34
00:03:06,880 --> 00:03:12,400
or any other complex phenomena, it relates to everyone and it means something different to

35
00:03:12,400 --> 00:03:18,080
everyone. It's ineffable and every conversation sounds like a typical post ketamine discussion,

36
00:03:18,080 --> 00:03:23,360
which is to say extremely low information content. Now, the topics we're discussing today are very

37
00:03:23,360 --> 00:03:28,480
complex and often we're reaching for the best language to use to conduct the conversation.

38
00:03:28,480 --> 00:03:33,520
So I hope we do well today. Anyway, Dr. Yoshua Barker is an absolute honor to finally welcome you

39
00:03:33,520 --> 00:03:41,760
to MLST. Thank you very much. I'm glad to be on the show. Amazing. Well, when I started doing

40
00:03:41,760 --> 00:03:47,200
computer science many years ago, interestingly, the theory of computation wasn't even on the

41
00:03:47,200 --> 00:03:51,760
curricula. And I was wondering whether you thought it should be. I mean, presumably you think it's

42
00:03:51,760 --> 00:03:56,560
extremely relevant for AGI. Now, we want this to be as pedagogical as possible. So please explain

43
00:03:56,560 --> 00:04:03,200
everything like we're five. What does computation mean to you? I think that computation is

44
00:04:03,200 --> 00:04:08,800
far easier than most people think. It means that you have a causal structure where every

45
00:04:09,760 --> 00:04:15,040
transition can be decomposed into individual steps. And when we talk about computational models,

46
00:04:15,040 --> 00:04:20,640
we decompose the world into states and transitions between the states. And then it turns out that

47
00:04:20,720 --> 00:04:26,960
there is a certain minimal system that is able to execute everything. And this can be described in

48
00:04:26,960 --> 00:04:32,240
many ways. The most famous one is probably the Turing machine and many other ways in which you

49
00:04:32,240 --> 00:04:38,480
can describe the Turing machine. For instance, you can just do the Turing machine by doing

50
00:04:39,520 --> 00:04:43,520
search and replace on strings. And this is how the Lambda calculus is defined.

51
00:04:44,240 --> 00:04:48,880
And all the programming languages and the Lambda calculus and the Turing machine

52
00:04:48,880 --> 00:04:53,120
turn out to have the same power. That is, if you can compute something with one of these paradigms,

53
00:04:53,120 --> 00:04:56,960
you can compute it with the others. As long as you don't run into resource constraints,

54
00:04:56,960 --> 00:05:01,520
so as long as it still fits into memory and you don't care about speed, they all have the same

55
00:05:01,520 --> 00:05:08,000
power. But in practice, of course, every system is limited. So we don't run things forever. We

56
00:05:08,000 --> 00:05:12,880
want them to give us a result after a certain time. So what matters is what can be efficiently

57
00:05:12,880 --> 00:05:19,440
computed, not what is reachable at all. Awesome. And there are, and we're going to get into this

58
00:05:19,440 --> 00:05:24,800
a bit, but there are some possible loopholes, at least in, you know, let's say whether or not

59
00:05:24,800 --> 00:05:30,720
the universe is limited in certain ways that the definitions of like Turing machines are.

60
00:05:30,720 --> 00:05:37,040
And one I wanted to ask you about specifically is Penrose's claims. And so he claims that

61
00:05:37,840 --> 00:05:44,560
what Godel's work in fact proves is that the human mind can understand truths that are not

62
00:05:44,560 --> 00:05:51,600
provable. So specifically one can show that, you know, given Godel's sentence is necessarily true

63
00:05:51,600 --> 00:05:57,440
given given a mathematical analysis, even though it can't be proven within the formal system that

64
00:05:57,440 --> 00:06:03,120
it's that it's defined. And Penrose claims that this capability to understand, if you will, to

65
00:06:03,120 --> 00:06:09,680
mathematically understand is in fact non computational, at least in part. And so if he's right, then

66
00:06:09,680 --> 00:06:16,720
our brains might be what Turing referred to as Oracle machines. These are computers that have

67
00:06:16,720 --> 00:06:24,080
access to a non computable Oracle or function that they can then utilize those oracles in

68
00:06:24,080 --> 00:06:29,440
order to perform hyper computation, essentially. So I'm asking, I'm curious, are you open to this

69
00:06:29,440 --> 00:06:34,080
possibility? And if not, what is your response to Penrose's arguments?

70
00:06:35,040 --> 00:06:40,240
I suspect that Goedl has been misunderstood by a lot of philosophers. Goedl was a truth

71
00:06:40,240 --> 00:06:45,920
realist. That is, he thought that truth really exists out there. That it's the thing that is

72
00:06:45,920 --> 00:06:50,960
eternal in some sense. He had this very strong intuition and mathematics classically is also

73
00:06:50,960 --> 00:06:55,760
formalized in this way. The difference between mathematics and computation, at least in the

74
00:06:55,760 --> 00:07:00,080
standard sense in which we normally teach mathematics at school is that mathematics has no

75
00:07:00,080 --> 00:07:07,520
states. Everything in mathematics just is eternally. It's a single state. And if you want to go through

76
00:07:07,520 --> 00:07:13,120
a sequence of states, you put an index into the formula. But still, everything is there at the

77
00:07:13,120 --> 00:07:17,760
same time. The index is just a way to access this thing. And this way of having mathematics

78
00:07:17,760 --> 00:07:22,960
stateless is very elegant because it allows us to define functions that have infinitely many

79
00:07:22,960 --> 00:07:27,760
arguments. If you would have a state machine that tries to consume infinitely many arguments,

80
00:07:27,760 --> 00:07:32,080
it would never finish before it goes to the next step. And the same thing in the middle of the

81
00:07:32,080 --> 00:07:36,320
function, if you compute something, if it's stateless, you can just compute all the indices

82
00:07:36,320 --> 00:07:41,840
all at once, even if it's infinitely many in classical mathematics. In a computational system,

83
00:07:41,840 --> 00:07:46,480
you would have to do this maybe one after the other. And if you do it at parallel, you will have

84
00:07:46,480 --> 00:07:51,920
lots of CPUs running at parallel. So you run into limits. And the same thing with the output. So

85
00:07:52,000 --> 00:07:56,880
in the classical mathematics, you can chain infinitely many steps and functions and exchange

86
00:07:56,880 --> 00:08:01,600
infinitely many arguments. But of course, mathematicians never did this a practice. It's

87
00:08:01,600 --> 00:08:06,160
just a specification. This is how they like to write things down. When they want to calculate it,

88
00:08:06,160 --> 00:08:12,080
they still have to go down and do it sequentially step by step. Just mathematics is defined in such

89
00:08:12,080 --> 00:08:17,840
a way as if you could upload this to some supernatural being or some grad student who is

90
00:08:17,840 --> 00:08:24,160
going to do the infinitely many calculations. And Goethe took this specification of mathematics

91
00:08:24,160 --> 00:08:28,160
and he found out that when you have this stateless mathematics, you can, for instance,

92
00:08:28,160 --> 00:08:33,920
define self referential statements that change that choose value depending on the statement itself.

93
00:08:34,560 --> 00:08:39,680
And this recurrence leads can lead to a contradiction in the state itself. So you basically

94
00:08:39,680 --> 00:08:45,200
get two statements which say I am wrong. And if by referring to itself, it changes its own

95
00:08:45,200 --> 00:08:51,040
truth value. So if mathematics is stateless, you will now run into a conflict. In a computational

96
00:08:51,040 --> 00:08:54,800
system, that's not a big problem. Your computer is not going to crash. If you write it down the

97
00:08:54,800 --> 00:08:59,520
right bay, it just happens is that your truth value fluctuates in every execution step. It's not

98
00:08:59,520 --> 00:09:05,360
going to converge. But this is not the real truth. Truth is something that doesn't change

99
00:09:05,360 --> 00:09:12,560
when you call the function again. So what's going on here? And I think what Goethe has

100
00:09:12,640 --> 00:09:17,760
discovered is that classical mathematics doesn't work. What you cannot build is any kind of

101
00:09:17,760 --> 00:09:22,560
mathematics doesn't allow you to build a machine, a hypothetical abstract machine, any kind of

102
00:09:22,560 --> 00:09:26,480
universe that runs the semantics of the classical mathematics without crashing.

103
00:09:27,360 --> 00:09:34,080
Yeah, but it kind of seems like, okay, we're going to believe Goethe's use of mathematics

104
00:09:34,080 --> 00:09:39,840
to prove that mathematics is flawed. Like there seems to be almost an inherent contradiction

105
00:09:39,840 --> 00:09:45,360
in there. Like you either believe mathematics, and thus you believe Goethe's proof of some

106
00:09:45,360 --> 00:09:51,760
specific limitations on computational systems, right? Or you believe that somehow mathematics

107
00:09:51,760 --> 00:09:55,520
is flawed, in which case you can't trust the proof that mathematics is flawed.

108
00:09:57,200 --> 00:10:02,080
I think Goethe's conclusion was that there is something fundamentally going wrong, that there

109
00:10:02,080 --> 00:10:07,760
might be an inability of mathematics to describe reality. And if you believe that truth is real

110
00:10:07,760 --> 00:10:12,800
and it exists independently of the procedure by which you calculate it, then this seems to be

111
00:10:12,800 --> 00:10:17,520
plausible. And it was also the conclusions a lot of philosophers have drawn from this,

112
00:10:17,520 --> 00:10:22,000
which basically read Goethe's proof and concluded that mathematicians have admitted that their

113
00:10:22,960 --> 00:10:27,920
arcane techniques are important to describe reality. And therefore, philosophers who don't

114
00:10:27,920 --> 00:10:34,480
understand mathematics have a clear advantage. Of course, this is not the conclusion. Instead,

115
00:10:34,480 --> 00:10:40,880
what turns out is that if you just skip or if you drop the original classical notation or

116
00:10:40,880 --> 00:10:45,920
as understanding of mathematics and replace it by computation, basically we say,

117
00:10:45,920 --> 00:10:50,560
truth is what you calculate with the following procedure. And you can define any kind of

118
00:10:50,560 --> 00:10:54,400
procedure that you want. You just have to make sure that it converges to some kind of value in

119
00:10:54,400 --> 00:10:59,120
the way that you want. Then you resolve your problem. It's just that you lose your notion that

120
00:10:59,120 --> 00:11:04,320
truth is independent of that procedure. And so in some sense, the classical mathematics is a

121
00:11:04,320 --> 00:11:08,240
specification that cannot be computed. From the perspective of computer scientists,

122
00:11:08,240 --> 00:11:12,880
this happens all the time. Some customer wants you to build something that cannot be built,

123
00:11:12,880 --> 00:11:16,400
and you have just proven that it cannot be built. Right? But it doesn't mean that you

124
00:11:16,400 --> 00:11:22,160
cannot build something useful. And I think that Penrose believed that our brain is actually doing

125
00:11:22,160 --> 00:11:28,000
these infinite things. And it's not. When we reason about infinity, we are not actually reasoning

126
00:11:28,000 --> 00:11:33,280
about infinitely many steps. What we do is we create a symbol, and then we do very finite

127
00:11:33,280 --> 00:11:38,240
computations over that symbol. But we cannot construct infinity. We cannot build it. We

128
00:11:38,240 --> 00:11:44,320
cannot go there from scratch and write down some clever automaton that produces an infinity for you.

129
00:11:45,440 --> 00:11:49,840
I was recently browsing Penrose's book, The Road to Reality, and I would say that, I mean,

130
00:11:49,840 --> 00:11:54,800
I don't know that much about physics, but the chapters were really interesting. They're talking

131
00:11:54,800 --> 00:12:01,040
about surfaces and manifolds and symmetries and fiber bundles and gauges and wave functions,

132
00:12:01,040 --> 00:12:05,920
calculus, matrix theory, and even computation. I mean, almost all of the discussion was on

133
00:12:05,920 --> 00:12:10,800
mathematical modeling at different levels of description or emergence, if you will. And

134
00:12:10,800 --> 00:12:15,840
in machine learning and AI, we are forever challenged by trying to get machines to model

135
00:12:15,840 --> 00:12:21,360
physical reality at different levels of description using an interoperable set of tools. So it seems

136
00:12:21,360 --> 00:12:26,480
increasingly true that we need machines that can learn descriptions and concepts at multiple levels

137
00:12:26,480 --> 00:12:30,640
if we're ever going to have AGI capable of understanding the world and learning novel

138
00:12:30,640 --> 00:12:36,480
semantic models. All of machine learning models today work by chopping up a Euclidean space into

139
00:12:36,480 --> 00:12:42,480
what is effectively a locality-sensitive lookup table. Very big one. And we need AGIs that can go

140
00:12:42,480 --> 00:12:48,000
far beyond this. It's got to be able to learn novel geometries beyond even what humans could

141
00:12:48,000 --> 00:12:52,480
have come up with and the ability to reason topologically and algebraically over those

142
00:12:52,480 --> 00:12:57,040
geometries. Something which I think you would agree is not happening with the current deep learning

143
00:12:57,040 --> 00:13:04,640
systems. Well, let's start out with the notion of geometry first. If you read Penrose's book,

144
00:13:04,640 --> 00:13:09,040
what you find is that this entire universe is geometric, which means it's made of

145
00:13:09,040 --> 00:13:15,680
continuous spaces in which things are happening. And if we actually look into the world deeply,

146
00:13:16,320 --> 00:13:21,520
quantum mechanics is not a geometric theory. The geometry only emerges approximately

147
00:13:21,520 --> 00:13:27,040
at the level of the space-time description. And it seems that geometry is actually the

148
00:13:27,040 --> 00:13:32,000
domain of too many parts to count. In reality, all the objects that we describe as surfaces,

149
00:13:32,000 --> 00:13:37,520
if you zoom in, are made of discrete parts like atoms and particles and so on. And these in turn

150
00:13:37,520 --> 00:13:43,040
are made out of things that have a finite resolution. And if we look into our computer

151
00:13:43,040 --> 00:13:47,440
programs, you can create stuff that looks continuous to us, but there's nothing continuous

152
00:13:47,440 --> 00:13:54,640
inside of our computer programs. And it turns out that the assumption of continuity requires

153
00:13:54,640 --> 00:14:00,160
that we partition the space into infinitely many parts. So now we are again running against

154
00:14:00,160 --> 00:14:06,080
that thing which GÃ¼rl has shown us as difficult. And it's not a big problem in practice because

155
00:14:06,080 --> 00:14:10,720
in practice, we never need to do these infinitely many things to produce a computer game with an

156
00:14:10,720 --> 00:14:17,280
arbitrary fidelity. We can make something that looks like space. But the space that we think in

157
00:14:17,280 --> 00:14:22,560
and so on is an approximation that our brain has discovered. It's a set of operators that converge

158
00:14:22,560 --> 00:14:28,080
in the limit. But the limit doesn't exist. It just, it happens that when you live in a world that is

159
00:14:28,080 --> 00:14:32,960
made of too many parts to count for almost everywhere where you look, you need to find these operators

160
00:14:32,960 --> 00:14:37,120
that converge in the limit. And the set of operators that happens to converge in the limit

161
00:14:37,120 --> 00:14:43,280
and is still computable. This is what we call geometry. And to use these uncomputable geometric

162
00:14:43,280 --> 00:14:48,480
approximations for macroscopic physics like Newtonian mechanics is completely fine. You're

163
00:14:48,480 --> 00:14:52,480
just going to compute it up to a certain digit and then this is good. But it's a problem for

164
00:14:52,480 --> 00:14:58,160
foundational physics. Because if it turns out that you cannot take a language that actually

165
00:14:58,160 --> 00:15:02,880
computes infinities, if you cannot construct your language, then you cannot write a universe in it.

166
00:15:03,200 --> 00:15:07,920
So our universe is not written in continuous language, but Penrose universe is.

167
00:15:09,840 --> 00:15:13,840
This doesn't mean that geometry is full. We need this to describe the world of too many parts to

168
00:15:13,840 --> 00:15:18,080
count. But we do this via computational approximations. Our brain does the same.

169
00:15:19,280 --> 00:15:24,960
So let me ask you this then because we come across kind of the infinities a couple of times. And I

170
00:15:24,960 --> 00:15:29,680
know that you placed an emphasis on constructive mathematics. So of course, you and all of us,

171
00:15:30,240 --> 00:15:35,280
you know, except let's say the existence of potential infinities, you know, algorithms that

172
00:15:35,280 --> 00:15:39,840
you can sit there and just keep calculating for as long as you want and get kind of more digits.

173
00:15:39,840 --> 00:15:46,000
But it's really around actual infinities that we seem to be running into problems. So let me ask

174
00:15:46,000 --> 00:15:51,120
this this first question here, really leading up to some computational questions, which is,

175
00:15:51,920 --> 00:15:58,720
can the universe, can our actual universe that we're in right now be actually infinite

176
00:15:58,720 --> 00:15:59,840
in spatial extent?

177
00:16:02,320 --> 00:16:06,480
A problem is that it can have unboundedness in the sense that you have a computation that

178
00:16:06,480 --> 00:16:13,120
doesn't stop giving your results. But you cannot take the last result of such a computation and go

179
00:16:13,120 --> 00:16:18,080
to the next step. You cannot have a computation that relies on knowing the last digit of pi

180
00:16:18,080 --> 00:16:22,960
before it goes to the next step. In the sense that you don't have an infinity. But the infinities

181
00:16:22,960 --> 00:16:27,120
are about the conclusion of such a function. It means that you actually run this function to the

182
00:16:27,120 --> 00:16:31,920
end and then do something with the result. Unboundedness is different in the sense that

183
00:16:31,920 --> 00:16:36,080
you will always get something new that you didn't expect that they cannot predict.

184
00:16:36,080 --> 00:16:42,400
But it's just going on and on without this end. And I think it's completely conceivable that our

185
00:16:42,400 --> 00:16:49,360
universe is in this class of systems in the sense that it doesn't end. But it doesn't mean that there

186
00:16:49,360 --> 00:16:55,040
is anything that gives you the result of an infinite computation. Because if that was the case,

187
00:16:55,040 --> 00:17:00,160
then it could not be expressed in any language. It also means if something cannot be expressed

188
00:17:00,160 --> 00:17:05,360
in any language, that you cannot actually properly think about it. Because when you think you need

189
00:17:05,360 --> 00:17:09,920
to think in some kind of language, not in English, but in some kind of language of sort or in a

190
00:17:09,920 --> 00:17:15,040
mathematical language that doesn't have contradictions. And what Goethe has shown is that the language

191
00:17:15,040 --> 00:17:22,080
that he hoped to reason in about infinities breaks that it has contradictions in it. That at some

192
00:17:22,080 --> 00:17:27,840
point, it blows itself apart. So the languages that we can build are only those in which we have

193
00:17:27,840 --> 00:17:32,000
to assume that infinities cannot be built. So infinity, in this sense, is meaningless.

194
00:17:32,560 --> 00:17:35,040
Because we cannot make it in any kind of language.

195
00:17:36,000 --> 00:17:42,480
So the thing is, though, I'm not limiting what the universe is capable of based on human mental

196
00:17:42,480 --> 00:17:49,520
and linguistic limitations or even mathematical limitations. I'm asking you if it's possible

197
00:17:49,520 --> 00:17:56,480
for this universe that we're in to ontically be right now actually infinite in spatial extent.

198
00:17:59,360 --> 00:18:04,640
The thing is that you try to make a reference to something that you cannot observe, that cannot

199
00:18:04,640 --> 00:18:10,880
conceive of other than making a model in some kind of language. And to have that model make sense,

200
00:18:10,880 --> 00:18:16,320
the language needs to work. Right? Otherwise, you are just maybe in some kind of delusional thing.

201
00:18:16,960 --> 00:18:21,200
And we can construct delusional things. We can construct languages that have bugs that we cannot

202
00:18:21,200 --> 00:18:26,480
see. But if we use a language that has bugs in it that we cannot see and we cannot repair them,

203
00:18:26,480 --> 00:18:30,960
then this means that the stuff that we express in the language is not meaningful. Right? We have to

204
00:18:30,960 --> 00:18:35,040
use a different language that has maybe the same expressive power but doesn't have these bugs.

205
00:18:35,600 --> 00:18:41,200
But now if you try to think about the universe in the language that allows you to imagine that

206
00:18:41,200 --> 00:18:45,920
the universe is literally infinite, rather than very, very, very big and much bigger than you

207
00:18:45,920 --> 00:18:51,360
can imagine and not ending, which is for all means and purposes almost the same thing. Right?

208
00:18:52,480 --> 00:18:58,560
Then if you do this other thing, then your thought doesn't mean anything. So it's basically you cannot

209
00:18:58,560 --> 00:19:03,440
properly express the idea in your own mind without running into contradictions that the

210
00:19:03,440 --> 00:19:09,120
universe is infinite in the sense that such a universe could exist. Okay, so you're basically

211
00:19:09,120 --> 00:19:12,800
following that. That's the issue. Basically, I cannot think that the universe is infinite. I cannot

212
00:19:12,800 --> 00:19:18,720
express this. That's my issue. Okay, fine. So you're basically saying that the English that I

213
00:19:18,720 --> 00:19:26,000
used just a minute or so ago just is not coherent or not conceivable. It's not something that you

214
00:19:26,000 --> 00:19:28,960
want to. But the underlying thing behind the English, right? English is not designed to be

215
00:19:28,960 --> 00:19:34,720
coherent. It's designed to be disambiguating. It's designed to be unprincipled to allow us to

216
00:19:34,720 --> 00:19:40,400
express things vaguely and not break. But if you think really, really deeply and really exactly,

217
00:19:40,400 --> 00:19:44,720
then the question is, what kind of model is your mind building? At which point is there just some

218
00:19:44,720 --> 00:19:50,800
kind of noisy nabler that you're pointing at without actually decomposing it and anything that would

219
00:19:50,800 --> 00:20:01,120
make sense? Okay. And so the lack of really the ability to conceive or for actual infinities to

220
00:20:01,120 --> 00:20:06,240
ontically exist in some sense, if we just deny all that, so we're really just stuck with,

221
00:20:06,880 --> 00:20:12,160
all right, we've got finite everything, discrete everything. There's no such thing as a continuum.

222
00:20:12,800 --> 00:20:19,040
There's no such thing as actual infinite spatial extent, etc. That's really the world that you're

223
00:20:19,600 --> 00:20:23,600
proposing here, right? That everything is constructed from at the end of the day,

224
00:20:23,600 --> 00:20:29,360
finite, discrete kind of elements. So if we... Yeah, you can imagine that your mind is a library

225
00:20:29,360 --> 00:20:33,440
of functions in a way, and these functions are doing jobs. And on the bouts of the box,

226
00:20:33,440 --> 00:20:38,240
you write down what these functions are doing. And you construct a box that this, in this box,

227
00:20:38,240 --> 00:20:44,320
there is an infinity between, for instance, a continuum between two points. And then you open

228
00:20:44,320 --> 00:20:49,920
up the box and look at what's actually inside of the box. And you realize it's just a lot of small

229
00:20:49,920 --> 00:20:54,320
steps. And it's designed in such a way that you can, if you want to have more steps, it's going to

230
00:20:54,320 --> 00:20:59,360
give you more steps if you zoom in, right? And it's totally doing, apparently, what's written down

231
00:20:59,360 --> 00:21:04,080
on the box. But if you look very closely, realize, oh no, the thing that is written down on the box

232
00:21:04,080 --> 00:21:08,080
that you have written down on the box cannot actually be in there. You can prove that it cannot

233
00:21:08,080 --> 00:21:12,000
be in there. It must be something else that's in there that is doing most of the work of what you've

234
00:21:12,000 --> 00:21:16,000
written down. So what you should actually be doing, I think, if you are interested in how things

235
00:21:16,000 --> 00:21:22,320
actually work, write on the box what it's actually doing, which means it's going to subdivide or

236
00:21:22,320 --> 00:21:26,320
any interval with any resolution you want as long as you can afford it.

237
00:21:27,280 --> 00:21:33,920
Okay. One mystery, if you will, for me, and I'm hoping you can help me understand this, is that

238
00:21:34,720 --> 00:21:41,920
all of the standard models for physics that we have today, they do have in them these continuous,

239
00:21:41,920 --> 00:21:47,440
you know, for example, symmetries that are rotational symmetry or things like that. They're

240
00:21:47,440 --> 00:21:55,440
built off of positing continuums with continuous waves, lots of continuities and infinities,

241
00:21:55,440 --> 00:22:00,480
at least in the mathematical descriptions. Except for quantum mechanics, right?

242
00:22:00,480 --> 00:22:05,520
Right. And I think based on what you've been saying, you would say that those are artifacts or

243
00:22:05,520 --> 00:22:12,720
properties of our mathematical descriptions of reality, but they're not actually extant in

244
00:22:12,720 --> 00:22:22,880
reality. And my mystery there is why do those continuous and mathematical maybe flawed and

245
00:22:22,880 --> 00:22:31,120
inconsistent with infinities all over the place descriptions work so well for describing phenomenon

246
00:22:31,120 --> 00:22:36,080
at different levels? If everything at the end of the day, you know, if we just looked at high enough

247
00:22:36,080 --> 00:22:42,800
energy and small enough resolution, we'd see kind of the grid and, you know, all the discrete

248
00:22:42,800 --> 00:22:50,480
effects and rotation happening kind of in little tiny, very small but not infinitesimal degrees.

249
00:22:50,480 --> 00:22:56,000
You know, why does all this continuous infinity based mathematics work so well? What is the

250
00:22:56,000 --> 00:23:00,160
explanation for the unreasonable effectiveness of that kind of mathematics?

251
00:23:00,960 --> 00:23:08,080
The easiest answer is that the world in which we live in is made of extremely small parts.

252
00:23:08,080 --> 00:23:14,160
And we could not exist if that world was not made of that many small parts. So for instance,

253
00:23:14,160 --> 00:23:19,680
you want to have a momentum for particles that are almost continuous. So you can address the

254
00:23:19,680 --> 00:23:24,480
space with high resolution because the momentum is what tells you where information comes from

255
00:23:24,960 --> 00:23:30,160
in the universe, basically the direction of where from which information reaches you and so on.

256
00:23:30,160 --> 00:23:35,920
If that would be very coarse, then the complexity that you could build would probably be far lower.

257
00:23:35,920 --> 00:23:42,160
And we consist of so many parts that when you look down, it's uncountably many for all practical

258
00:23:42,160 --> 00:23:47,200
purposes. So the mathematics that we need to describe the world that we are in that we need

259
00:23:47,200 --> 00:23:52,880
to model are mostly not in the realm of countable numbers. The countable numbers only play a role

260
00:23:52,960 --> 00:23:59,360
when we are looking at very few microscopic things. As soon as we leave this domain of a few apples

261
00:23:59,360 --> 00:24:06,160
on our table, we almost instantly drop in this realm where we just need to switch to a continuous

262
00:24:06,160 --> 00:24:12,080
description of things. And this is completely fine for most of our history. When we did physics,

263
00:24:12,080 --> 00:24:18,880
we never zoomed in that heart. And even now, when we really need to zoom at the level where

264
00:24:18,880 --> 00:24:24,640
the plank length matters and the resolution of the universe becomes visible. And it's of course not

265
00:24:24,640 --> 00:24:29,360
some Euclidean lattice, some grid that you can see. It's just that at this level, you no longer

266
00:24:29,360 --> 00:24:36,880
have space. I wanted to move matters back over to some of the happenings in the world of large

267
00:24:36,880 --> 00:24:43,040
language models and deep learning and so on. And first, quick fire question. I honestly,

268
00:24:43,040 --> 00:24:48,000
you're a bit of an enigma to me, Joshua, because obviously I've read some of your research and

269
00:24:48,000 --> 00:24:54,320
you seem like a hybrid guy to me. You know Ben Goetzel very well, for example, but you're also

270
00:24:54,320 --> 00:25:01,200
hugely into the hype train on the connectionism. For example, you criticised Gary Marcus's

271
00:25:01,200 --> 00:25:04,640
article. So the first question is, are you a symbolist or a connectionist?

272
00:25:05,440 --> 00:25:12,160
I'm neither. The thing is that I hate deep learning as the best of us. Deep learning is ugly. It's

273
00:25:12,160 --> 00:25:19,280
brutalist. It's a few very simple algorithms that are blown up to the max. But I cannot prove

274
00:25:19,280 --> 00:25:25,840
that these algorithms do not converge to what we want them to converge to. It's maybe not

275
00:25:25,840 --> 00:25:31,280
elegant, but it works. And the solution to problems with deep learning so far has always

276
00:25:31,280 --> 00:25:38,880
been to use more deep learning, not less. So what upsets me about Gary Marcus argument is not that

277
00:25:38,960 --> 00:25:44,000
I'm not sympathetic to what he's trying to push it. I'd like to build models that are more elegant,

278
00:25:44,000 --> 00:25:52,160
more sparse and so on. But in the past, all these elegant sparse models have been left in the dust

279
00:25:52,160 --> 00:25:57,360
by just using more deep learning. And we can also see when we zoom out a little bit that there is

280
00:25:57,360 --> 00:26:02,800
not an obvious limit to deep learning itself, because deep learning is not just the algorithms.

281
00:26:02,800 --> 00:26:07,600
Deep learning is a programming paradigm. It's differentiable programming. It basically means

282
00:26:07,600 --> 00:26:14,160
that you express everything with approximately continuous numbers, and you use algorithms that

283
00:26:14,160 --> 00:26:22,160
converge business at certain ranges. And when it doesn't converge, then you just tweak it and you

284
00:26:22,160 --> 00:26:27,920
introduce a different architecture, which is some kind of discrete operations that you do on these

285
00:26:27,920 --> 00:26:32,960
continuous numbers and so on. You just patch it, you write your programs slightly differently,

286
00:26:32,960 --> 00:26:37,520
and you can automate the search for the program. And the people who do deep learning are not also

287
00:26:37,520 --> 00:26:42,080
docs in the sense that they say, oh my God, symbolic structures are not allowed. I cannot use

288
00:26:42,080 --> 00:26:47,600
a Python script in here rather than just a TensorFlow. This is not what's happening. It's

289
00:26:47,600 --> 00:26:52,400
also not that they are constrained to any kind of thing that will use whatever is working.

290
00:26:52,400 --> 00:26:56,960
And what we see is that the end-to-end train systems are going more and more powerful,

291
00:26:56,960 --> 00:27:01,200
and rather than sitting there by hand and tinkering and finding a solution,

292
00:27:01,840 --> 00:27:06,720
we can just use a system that is tinkering automatically through a dramatically larger

293
00:27:06,720 --> 00:27:12,720
space than we would ever be able to explore by trying all sorts of algorithms. So when we look

294
00:27:12,720 --> 00:27:18,480
at Gary Marcus' articles like his deep learning is hitting a wall and so on, and you look what he's

295
00:27:18,480 --> 00:27:22,880
actually giving as arguments, the arguments are not very good. He gives us an example,

296
00:27:22,880 --> 00:27:29,600
the NetHack challenge. NetHack is a game which has a very large horizon because you basically

297
00:27:29,600 --> 00:27:35,120
have only one life. You need to explore a very deep labyrinth and you need to plan pretty far ahead

298
00:27:35,120 --> 00:27:41,120
with what you're doing. And so it's something that is difficult to discover this right solution

299
00:27:41,120 --> 00:27:47,680
with a deep learning model that has no prior ideas about what it's doing. Because it takes us

300
00:27:47,680 --> 00:27:52,000
very, very long until you get the necessary feedback to learn about your actions. And people

301
00:27:52,000 --> 00:27:56,480
are relatively good at learning this because they have so many ideas about what the situation is that

302
00:27:56,480 --> 00:28:01,680
they're in. There's so many priors from our world interaction and from other games that we have played

303
00:28:01,680 --> 00:28:06,960
that we can bring to the tasks. So the current winner of this is the symbolic solution.

304
00:28:08,080 --> 00:28:14,480
And the symbolic solution that Gary Marcus gives as a proof that symbolic methods are still ahead

305
00:28:14,480 --> 00:28:20,320
of deep learning things. In a single case, not like he has a big array of tasks where they are

306
00:28:20,320 --> 00:28:26,000
superior, it's just two students who have written a program that is made of lots and lots of events.

307
00:28:26,000 --> 00:28:30,960
This is just a big hack. This is not some symbolic learning algorithm that does something novel,

308
00:28:30,960 --> 00:28:36,880
hybrid or whatever. No, this is just a script. And is Gary Marcus seriously proposing, oh my

309
00:28:36,880 --> 00:28:40,880
god, deep learning models are limited and we need to replace them with more scripts?

310
00:28:41,760 --> 00:28:49,040
This is not a good argument. Yeah. So I think maybe, and look, I get that there are these kind

311
00:28:49,040 --> 00:28:54,080
of two competing camps and they maybe go after each other with some. No, they don't. This is only on

312
00:28:54,080 --> 00:28:59,760
Twitter. There is, there are no competing camps. It's Yandekun is not also docs in the sense that

313
00:28:59,760 --> 00:29:03,440
he believed you need to use this argument, all the other arguments are impure and flawed.

314
00:29:04,880 --> 00:29:10,400
His brand is to build systems that work. And if one of his people comes up with something that

315
00:29:10,400 --> 00:29:15,680
works better than what he came up with, you probably praise him for that and let him go on.

316
00:29:17,120 --> 00:29:24,080
Yeah, sure. But there's absolutely, however, there is, you know, there is, let's say, momentum

317
00:29:24,080 --> 00:29:29,280
and hardware lotteries and paradigms that kind of reinforce themselves. And to an extent,

318
00:29:29,360 --> 00:29:34,400
they can strangle off, you know, resources that maybe we like, we shouldn't be investing all

319
00:29:34,400 --> 00:29:40,640
our eggs in one basket. We shouldn't be pouring, you know, the 99% of research funding necessarily

320
00:29:40,640 --> 00:29:46,240
down, down deep learning. And I think that's kind of the problem that, that these paradigms cause.

321
00:29:46,240 --> 00:29:52,000
But I want to get back to something you said, which is a good point. It's, I think that's an

322
00:29:52,000 --> 00:29:56,800
important point. I think that in absolute terms, the other approaches get more money than they did

323
00:29:56,800 --> 00:30:02,240
before. It's not that we have a funding stop, as we had at some point, a return funding stop for

324
00:30:02,240 --> 00:30:07,280
Neural Networks. And Marvin Minsky wrote a book where he saw he had proven that the Neural Networks

325
00:30:07,280 --> 00:30:12,400
cannot converge over multiple layers, press up drones cannot earn X or and so on. Right. Minsky

326
00:30:12,400 --> 00:30:17,440
was wrong. People found a way around this. But at this time, there was so little funding that

327
00:30:17,440 --> 00:30:23,200
this cutoff mattered. And at the moment, if you want to do something that has AI and Adline,

328
00:30:23,200 --> 00:30:27,440
the chance that you get it funded and whether what paradigm you're doing is greater than ever.

329
00:30:27,440 --> 00:30:31,760
So the absolute amount of funds that goes into any kind of paradigm that you want to work on

330
00:30:31,760 --> 00:30:36,960
is greater than ever. And the reason why the majority of funds goes into very few paradigms is

331
00:30:36,960 --> 00:30:42,080
because these are the things that work in industrial applications. There is no other

332
00:30:42,080 --> 00:30:46,240
algorithm that is able to learn from scratch how to translate between arbitrary languages and

333
00:30:46,240 --> 00:30:51,600
generate stories and draw pre pictures for you. This is the only game in town at the moment,

334
00:30:51,600 --> 00:30:56,320
the only class of algorithm that converges over all these many domains. And people are looking

335
00:30:56,320 --> 00:31:00,800
for better alternatives. And yes, we are in a bubble, because of course, they're looking mostly

336
00:31:00,800 --> 00:31:05,520
where things already were. You have hardware that works, you have libraries that work and so on.

337
00:31:05,520 --> 00:31:10,480
It's hard to get out of that bubble. That is true. And it's always good to push for alternatives

338
00:31:10,480 --> 00:31:17,440
and so on. But I don't think that we should be in a panic and say, Oh, my God, there is something

339
00:31:17,440 --> 00:31:23,440
politically wrong. I suspect that by and large, the forces of the markets and the forces of the

340
00:31:23,440 --> 00:31:28,320
academic researchers that want to explore alternative are pushing in the right direction already.

341
00:31:29,840 --> 00:31:34,160
Yeah, I mean, fair enough. And, you know, you could be right. And there may not be that much

342
00:31:34,160 --> 00:31:40,880
of an imbalance. But I want to get back to one technical thing you said. Yes, it seems apparent

343
00:31:40,880 --> 00:31:46,320
that, let's say, what deep learning is doing is this this differentiable program search,

344
00:31:46,400 --> 00:31:51,760
if you will. And a question I have about that is if we imagine the space of all possible programs,

345
00:31:52,800 --> 00:31:57,760
that, you know, requiring that we're doing a differentiable search is certainly going to skew

346
00:31:57,760 --> 00:32:05,360
that sample space that may even cut off programs in that space that can't be discovered easily by

347
00:32:05,360 --> 00:32:10,080
differentiable search. So I'm wondering, doesn't that leave open the possibility that other

348
00:32:10,080 --> 00:32:15,680
algorithms that are more discreet in nature, say evolutionary algorithms or discrete program

349
00:32:15,680 --> 00:32:21,600
search or whatever, they may have access to a different subspace of the space of all programs

350
00:32:21,600 --> 00:32:26,000
that aren't easily accessible by differentiable paradigms. Is that true?

351
00:32:28,720 --> 00:32:32,960
The question is, how do you find it? How do you find these algorithms to manipulate the

352
00:32:32,960 --> 00:32:38,880
discrete things? I agree that when you have a perceptual model that is modeling everything

353
00:32:39,520 --> 00:32:46,560
with chains or sums over real numbers, and a few non-algebraic throne, and you get

354
00:32:46,560 --> 00:32:50,320
characteristic artifacts. For instance, in the generative models, you often have the problem,

355
00:32:50,320 --> 00:32:55,280
and you try to model a person with glasses or without glasses, that because the model thinks

356
00:32:55,280 --> 00:33:00,240
that these features are somewhat continuous, you often run into the situation that you get areas

357
00:33:00,240 --> 00:33:05,040
in the generative model, where the glasses are half materialized, and it looks always very weird.

358
00:33:05,040 --> 00:33:11,200
And you have these strange things where reality has a discontinuity, but your model has permissible

359
00:33:11,200 --> 00:33:17,120
states where you are in the middle of the discontinuity, and you try to generate something

360
00:33:17,120 --> 00:33:21,200
that cannot exist. You want your model to be structured such a way ideally that every

361
00:33:21,200 --> 00:33:27,440
model configuration corresponds to a world configuration. And this is not necessarily the

362
00:33:27,440 --> 00:33:32,080
case with many of the deep learning models. And what the deep learning models, as you train them

363
00:33:32,080 --> 00:33:37,360
harder, typically tend to do is that they squeeze these impermeasurable areas until you are very

364
00:33:37,360 --> 00:33:42,400
unlikely to end up in them. And it's probably possible to get them to implement filters and

365
00:33:42,400 --> 00:33:48,080
all sorts of tricks. But what you can also do is you can combine this with some kind of discrete

366
00:33:48,080 --> 00:33:53,600
machine. And then what you do is you learn how to use this. So this deep learning network is not

367
00:33:54,160 --> 00:33:59,280
interacting with the world directly, but it learns how to use an architecture that does that.

368
00:34:00,080 --> 00:34:04,640
So for instance, instead of training a neural network to do numerical calculations,

369
00:34:04,640 --> 00:34:10,160
you can train it to use a numerical calculator. And in this way, it can become very sparse again.

370
00:34:10,720 --> 00:34:16,080
Right? So there's not an obvious limit to that I can see where I can prove to the deep learning

371
00:34:16,080 --> 00:34:20,080
people, oh, here's where you should stop deep learning, because they can just combine their

372
00:34:20,080 --> 00:34:24,560
deep learning approach with other approaches and use the deep learning system to remote control

373
00:34:24,560 --> 00:34:28,400
this. And it turns out when we reason and so on, even when we do discrete reasoning,

374
00:34:28,400 --> 00:34:34,000
that the steps that we assemble it to each other are heuristics that require some kind

375
00:34:34,000 --> 00:34:38,720
of probabilistic element. Right? So when we form a sort that when the sort is made of very

376
00:34:38,720 --> 00:34:44,640
discrete elements, the search for that sort is some kind of deep learning process that is happening.

377
00:34:44,640 --> 00:34:51,760
Right? And when we make the pool, we do this, we emulate a discrete reasoning. But of course,

378
00:34:51,760 --> 00:34:56,880
we can combine this and we can get the neural network to learn how to perform the discrete

379
00:34:56,880 --> 00:35:02,400
operations. There's a certain thing that I would like to see, which is something like a more sparse

380
00:35:02,400 --> 00:35:09,600
language of thought. When we are looking at deep learning models, there's a phenomenon that people

381
00:35:09,600 --> 00:35:15,520
are sometimes observing, which they call grocking. That is, you train the model and your model gets

382
00:35:15,520 --> 00:35:20,320
better and better. And then it overfits, which means it gets very good at the training data,

383
00:35:20,320 --> 00:35:24,960
but it gets very bad on the real world at things that it hasn't seen before, like a person in

384
00:35:24,960 --> 00:35:29,920
psychedelics was able to explain everything in the past, but is no longer able to perform well in

385
00:35:29,920 --> 00:35:33,920
the future because they're overfitting. They basically fit the curve too closely to the data

386
00:35:33,920 --> 00:35:38,800
that I've seen. And there are many tricks in deep learning to go around this overfitting to make

387
00:35:38,800 --> 00:35:43,440
sure that this doesn't happen. And people try to avoid it. And then what they discovered is when

388
00:35:43,440 --> 00:35:47,200
you take this overfit model, you train it more and more and more and more. At some point, it

389
00:35:47,200 --> 00:35:53,040
sometimes clicks and it gets much better than ever before. And there is a question if there's

390
00:35:53,120 --> 00:35:58,960
something that we're doing wrong in deep learning. For instance, when you think about how people

391
00:35:58,960 --> 00:36:05,120
learn, they learn very different from GPT-3. People first learn by pointing at stuff that

392
00:36:05,120 --> 00:36:09,200
thinks that are relevant to them, that they can eat, that they can hurt, that can hurt them,

393
00:36:09,200 --> 00:36:14,000
or that they find pleasant and so on. They, that they can feel that they can, they have contrast

394
00:36:14,000 --> 00:36:18,640
on it that are salient to them. And so you start out with learning these semantics based on the

395
00:36:18,640 --> 00:36:24,000
saliency and relevance that you have. And then when you learn language, you learn basic syntax,

396
00:36:24,000 --> 00:36:28,720
how to put things together. And in the long tail of the syntax, you learn style, how to express

397
00:36:28,720 --> 00:36:35,680
things with new ones and so on. And with GPT-3, it's the opposite. You first learn style, right?

398
00:36:35,680 --> 00:36:40,880
And then you learn syntax as the regularities in the style. And the semantics is the long tail of

399
00:36:40,880 --> 00:36:46,320
that. And to make that happen, you need to learn much, much more. You need to have more training

400
00:36:46,320 --> 00:36:52,240
data and so on. Maybe there's a way in which we can reverse the order and basically get it to

401
00:36:52,240 --> 00:36:57,200
start out with relevance, to build a curriculum where you first get very sparse regularities,

402
00:36:57,200 --> 00:37:02,560
where it clicks into place. You always make sure that you can handle it with very limited resources

403
00:37:02,560 --> 00:37:08,640
and only see the style and the niceties and the nuances as the far extensions of these very sparse

404
00:37:08,640 --> 00:37:14,320
concise models that have very big predictive power. Yeah. I mean, on that, I mean, the Grocking

405
00:37:14,320 --> 00:37:19,200
paper was very interesting. And a lot of these large language model fans always cite that very,

406
00:37:19,200 --> 00:37:23,280
very quickly when you have a conversation with them. But there is a problem with machine learning

407
00:37:23,280 --> 00:37:27,840
in general, which is that there is, as you said, there's a spectrum of correlations and almost

408
00:37:27,840 --> 00:37:34,000
all of them are spurious. And on one side of that spectrum, you have the idealized features you

409
00:37:34,000 --> 00:37:38,000
actually want it to learn, which will generalize after distribution. And then, of course, if you

410
00:37:38,000 --> 00:37:43,680
go down that spectrum, you pick up on all sorts of very spurious correlations that just happen

411
00:37:43,680 --> 00:37:48,400
to generalize very well. And if you tell the models not to use those spurious correlations,

412
00:37:48,400 --> 00:37:53,280
that the performance of the model will go down. But I want to just move a little bit over to

413
00:37:54,800 --> 00:37:59,680
Yasaman Rezegi's paper. I don't know whether you saw that, but she showed that the performance of

414
00:37:59,680 --> 00:38:05,040
large language models for arithmetic tasks are linearly correlated to the term frequency and

415
00:38:05,040 --> 00:38:10,160
the training corpus, suggesting that they are memorizing the data set, which presumably you

416
00:38:10,160 --> 00:38:15,840
would agree with. And Google has recently released this 540 billion parameter language model called

417
00:38:15,840 --> 00:38:21,840
PAM, which interestingly does extremely well on, for example, some of the Google big bench tasks,

418
00:38:21,840 --> 00:38:27,120
such as the conceptual combinations task, which is one of them, which tests for compositionality,

419
00:38:27,120 --> 00:38:31,840
which we'll talk about in a minute. But compositionality is when you can take constituents from

420
00:38:31,840 --> 00:38:36,400
the prompt and compose them together to form the answer. Now, it's tempting to jump to the

421
00:38:36,480 --> 00:38:41,360
conclusion that these models are starting to magically reason at scale along the lines that

422
00:38:41,360 --> 00:38:46,320
you were just discussing. But I still think there's plenty of opportunities for shortcut learning,

423
00:38:46,320 --> 00:38:51,120
you know, by which I mean these spurious correlations, given the brittle interface of an

424
00:38:51,120 --> 00:38:57,600
autoregressive GPT style language model with these human designed benchmarks. Would you agree with that?

425
00:38:57,600 --> 00:39:07,600
Yeah. When I started my own career in computer science in the 90s, I was in New Zealand, and the

426
00:39:07,600 --> 00:39:13,520
prof here in Britain realized that I was bored in class. So he took me out of the class and in

427
00:39:13,520 --> 00:39:18,480
his lab, and he gave me the task to discover grammatical structure and an unknown language from

428
00:39:18,480 --> 00:39:24,080
scratch and left me pretty much to my own devices on how to do this. So the unknown language I picked

429
00:39:24,080 --> 00:39:28,640
was English, was just unknown to the computer, but was the easiest one to get a corpus for,

430
00:39:28,640 --> 00:39:32,560
and they gave me the largest computer they had. It has two gigabytes of RAM, and I did

431
00:39:32,560 --> 00:39:37,600
in memory compression with C and so on, and tried to do statistics, and I quickly realized NREM

432
00:39:37,600 --> 00:39:44,560
statistics don't work because of too many words in between. So unlike vision tasks where

433
00:39:44,560 --> 00:39:50,400
confnets have a useful prior by thinking that adjacent pixels also relate to symbolically

434
00:39:50,400 --> 00:39:54,720
related information, right? So adjacency in images is a very good predictor for thematic

435
00:39:54,720 --> 00:39:59,920
relatedness. It doesn't really work in NLP. So the transformer was discovered in natural language

436
00:39:59,920 --> 00:40:06,560
processing for that reason, because you cannot use direct adjacency very well. And so I realized

437
00:40:06,560 --> 00:40:13,360
I cannot use NREM, which depend on direct adjacency between words. And so I first of all used ordered

438
00:40:13,360 --> 00:40:18,960
pairs of words and tried to find correlations between pairs and then find a mutual information

439
00:40:18,960 --> 00:40:24,160
tree that would give me the best prediction over the structure of the sentence for all the

440
00:40:24,160 --> 00:40:29,120
sentences that I would have in my corpus. And indeed this correlated to structure. And I realized

441
00:40:29,120 --> 00:40:33,680
this is going to not just give me grammar, but it's also going to give me semantics

442
00:40:33,680 --> 00:40:39,280
if I can more deep statistics. But I will need something not just ordered pairs,

443
00:40:39,280 --> 00:40:44,480
but I need to have something like force order models. But to do the statistics, even in memory,

444
00:40:44,480 --> 00:40:48,960
with clever and memory compression and many tricks that I did, I could not do full statistics on

445
00:40:48,960 --> 00:40:56,800
this. So what I realized I had to do was that I do multiple passes. And at first I discard almost

446
00:40:56,800 --> 00:41:01,680
all the information. I only pick out the most salient things. And then my time was over in

447
00:41:01,680 --> 00:41:07,280
this lab. And I went back to Germany and never reviewed this area of research again. But what

448
00:41:07,280 --> 00:41:10,960
I had realized is to make progress and need to make statistics over what I need to make the

449
00:41:10,960 --> 00:41:16,320
statistics over. And the very principled base, I need to learn what I have to learn.

450
00:41:17,600 --> 00:41:23,600
And they didn't pay attention to this domain at all. And I also missed the 2017 transformer paper

451
00:41:23,600 --> 00:41:28,320
and its relevance. It was only when GPT-2 came out that I realized, oh my god, they did this.

452
00:41:28,320 --> 00:41:34,880
They did statistics over the statistics. And it's still not the right solution, I think. It's not

453
00:41:34,880 --> 00:41:38,800
the way in which our brain is doing it. It's some brute force shortcut. Well, for instance,

454
00:41:38,880 --> 00:41:42,000
the individual attention heads are not correlated with each other, but in reality,

455
00:41:42,000 --> 00:41:47,680
they are. We have this in reality, our attention heads are integrated into one model of what's

456
00:41:47,680 --> 00:41:53,600
going on. And it's not that we have an attention net on every layer that just pays attention to

457
00:41:53,600 --> 00:41:59,120
what's happening in the lower layer. It's much more clever in our own mind. And this thing is

458
00:41:59,120 --> 00:42:03,360
active. We single out things in reality to research, which book we need to take out of the

459
00:42:03,360 --> 00:42:07,840
shelf to update our working memory context so we are able to interpret the current sentence

460
00:42:07,840 --> 00:42:12,800
that we don't understand. And so we always go for saliency when we read something that doesn't

461
00:42:12,800 --> 00:42:21,040
make sense. At least my mind works like this. I discard it. I will not stop until it makes sense,

462
00:42:21,040 --> 00:42:26,480
or I will have to go to some preliminary. I will not accept some kind of vague statistical

463
00:42:26,480 --> 00:42:31,360
approximation of what I read. Keep this as an intermediary stage in my mind until the hope

464
00:42:31,360 --> 00:42:36,480
that eventually converges. It's a completely different learning paradigm. When we teach our

465
00:42:36,480 --> 00:42:41,760
children arithmetic, it's not that we show them lots of very long mass textbooks and hope that

466
00:42:41,760 --> 00:42:46,400
initially it will not make any sense to them. But as they reread them again and again with many

467
00:42:46,400 --> 00:42:51,280
samples, eventually it will click and they will converge on arithmetic. No, this is not how it

468
00:42:51,280 --> 00:42:55,440
works. You start this giving them extremely simple things and say, in these extremely simple things,

469
00:42:55,440 --> 00:42:59,600
there is structure that you can fully understand. Now go and find the structure that you fully

470
00:42:59,600 --> 00:43:04,400
understand. Once you've done it, you make this a little bit more complicated for you. This is

471
00:43:04,400 --> 00:43:12,320
probably the paradigm that you could be exploring. I know, but the problem is it's incredibly

472
00:43:12,320 --> 00:43:15,680
deceptive when you have something which appears intelligence. Of course, the

473
00:43:16,400 --> 00:43:22,000
boundary of our perception of intelligence is a receding one. But I wanted to just get on to

474
00:43:22,640 --> 00:43:29,200
there are some incredible generative visual models like Dali and the disco diffusion.

475
00:43:29,680 --> 00:43:34,240
These models, I think, are going to revolutionize the creative profession. I've been

476
00:43:34,240 --> 00:43:38,960
playing with disco diffusion all day today. I've already ordered some prints to go on my wall.

477
00:43:38,960 --> 00:43:45,600
It's incredible. The two obvious settings where large language models might be successful

478
00:43:46,400 --> 00:43:51,680
are coding and information retrieval in my opinion. But let's take pause for thought. I've played

479
00:43:51,680 --> 00:43:57,600
with Codex and I'm resolutely sure that I wouldn't want to use it. I think code and knowledge are

480
00:43:57,600 --> 00:44:03,120
a different ballgame to art, which I think will be amazing. With Codex, there's an impedance mismatch

481
00:44:03,120 --> 00:44:07,680
between the process of generating the code and then debugging and running the code, which has

482
00:44:07,680 --> 00:44:12,800
euphemistically been framed as prompt engineering, or another term which I've just invented,

483
00:44:12,800 --> 00:44:18,560
retrospective development. I think it's easier to start again from scratch than fix broken code

484
00:44:18,560 --> 00:44:23,120
from a large language model. I mean, this is quite interesting. At Google, it already takes

485
00:44:23,120 --> 00:44:28,320
months to get any code checked into their mono repo because it's basically a bureaucracy because

486
00:44:28,320 --> 00:44:33,840
they needed to have gatekeeping after they decided to use a mono repo. Could you imagine

487
00:44:33,840 --> 00:44:38,880
how much bureaucracy there'd be if they allowed people to start checking in code, which was generated

488
00:44:38,880 --> 00:44:43,120
from an algorithm? Anyway, I think there's an exciting possible future for using these systems

489
00:44:43,120 --> 00:44:49,440
for information retrieval rather than the way that we go through and prune the results on a Google

490
00:44:49,440 --> 00:44:55,280
search. These models might just answer directly, but I hasten to think what that search UI would

491
00:44:55,280 --> 00:45:01,120
look like. Would its output be sclerotic or unadaptable? Would it be relevant to the query that

492
00:45:01,120 --> 00:45:07,920
I put in? Would its output even be true? Perhaps it will ask you to select what kind of truth you

493
00:45:07,920 --> 00:45:13,600
were looking for. Do you think these models would vitiate or spoil our society, or do you think they

494
00:45:13,600 --> 00:45:19,520
would actually enrich it? It's very hard to say. I think that from some perspective,

495
00:45:19,520 --> 00:45:25,840
our society is already maximally spoiled. Humans, as they live today, are basically

496
00:45:25,840 --> 00:45:31,040
locusts with opposable thumbs. This is not going to go on forever, this technological society.

497
00:45:31,040 --> 00:45:36,480
Here are, it seems to me, on some kind of Titanic that is going to hit the iceberg no matter what.

498
00:45:37,200 --> 00:45:42,080
And what basically should make us content is that the Titanic was the only place in the

499
00:45:42,160 --> 00:45:48,080
universe that has internet. And we are born on it, and we wouldn't have been born if there was no

500
00:45:48,080 --> 00:45:54,880
Titanic. We would not have been born in a sustainable ancestral society. In some sense,

501
00:45:54,880 --> 00:45:58,960
our society needs to reinvent itself. It's not really working right now. We don't know

502
00:45:58,960 --> 00:46:02,560
what the future is going to look like, and if it's going to be very technological,

503
00:46:02,560 --> 00:46:09,360
or if limit certain things, no idea what's going to happen. But if we think about how our

504
00:46:09,440 --> 00:46:15,680
current approaches work, if you want just to make programming better, I suspect that these tools can

505
00:46:15,680 --> 00:46:21,360
help. But they will be much more useful if you do not have to have this battle between a machine

506
00:46:21,360 --> 00:46:27,280
that doesn't really understand what you want. And instead, you have something that is working next

507
00:46:27,280 --> 00:46:33,680
to you. It's like, imagine you were working for some corporation and the corporation introduces

508
00:46:33,680 --> 00:46:38,160
some kind of planning tool that requires to do to jump through all sorts of hopes. And it turns

509
00:46:38,240 --> 00:46:42,960
out that the planning tool itself makes you more productive, but it makes work much less fun.

510
00:46:44,160 --> 00:46:47,520
It's still rational to use it. And everybody will hate it, but

511
00:46:48,640 --> 00:46:53,600
by and large, it will be used if it makes people 30 more productive. And everybody will feel there

512
00:46:53,600 --> 00:46:58,640
might be a must be a better solution, something that feels more organic. And so it could be that

513
00:46:58,640 --> 00:47:03,440
Codex is in this category that it makes mediocre programmers much more productive at producing

514
00:47:03,440 --> 00:47:08,240
boilerplate. But it's not just this, it's often able to find solutions very quickly,

515
00:47:08,240 --> 00:47:12,480
but you need to use a lot of Stack Overflow before you understand the new language or before you

516
00:47:13,120 --> 00:47:17,760
tease this new algorithm or part that you want to understand and so on. It just when it doesn't

517
00:47:17,760 --> 00:47:22,080
probably turn you into a better programmer, if that is your goal. But for your employer,

518
00:47:22,080 --> 00:47:26,080
maybe they don't care whether you're a better programmer, they just want you to turn out these

519
00:47:26,080 --> 00:47:30,720
pages of code and then they run this against the verifier and against the unit test and then are

520
00:47:30,720 --> 00:47:36,400
done, go to the next thing, right? So maybe it's not that important. But the systems that we would

521
00:47:36,400 --> 00:47:40,640
want, what would they look like? I think they need to know what they're doing. You want to have a

522
00:47:40,640 --> 00:47:45,680
program that is not just able to reproduce something very well in a given context, you want to

523
00:47:45,680 --> 00:47:51,360
understand the context as deeply as you do or better. So it needs to understand what kind of

524
00:47:51,360 --> 00:47:56,160
world it's operating in in the moment. And what itself is, what is it that it can do? What is

525
00:47:56,160 --> 00:48:01,440
that what needs to learn still? In some sense, you want systems that are sentient. And it's self

526
00:48:01,440 --> 00:48:06,160
like, oh my God, but it just means you have a learning system that's general enough to model

527
00:48:06,160 --> 00:48:11,360
in principle the entire universe. And this is not as outrageous as it sounds because

528
00:48:12,480 --> 00:48:19,200
Delhi is already dealing with two modalities, language and images. And we will get to video

529
00:48:19,200 --> 00:48:24,400
and we will get to audio connected and you see them early steps in this direction with the Socratic

530
00:48:24,400 --> 00:48:29,920
model of people, for instance. So I think that's almost inevitable that this generality will happen.

531
00:48:29,920 --> 00:48:33,520
And you will have to add a system to work in real time so it can discover itself.

532
00:48:35,600 --> 00:48:42,480
I think I think there's something really magic, though, about the creative process here. And also

533
00:48:43,200 --> 00:48:47,920
the prompt engineering is another thing we can talk about. But Kenneth Stanley once made this

534
00:48:47,920 --> 00:48:53,840
thing called Pick Breeder. And you could essentially distribute the selection of these images

535
00:48:53,840 --> 00:49:00,480
created with CPPNs, Compositional Pattern Producing Networks. And you would just get these beautiful

536
00:49:00,480 --> 00:49:06,320
images. They were they were so incredibly, incredibly diverse and interesting. So it's

537
00:49:06,320 --> 00:49:11,920
not that the algorithms were intelligent, there was something magic about the externalized process.

538
00:49:11,920 --> 00:49:15,840
And what's really interesting about these models like Dali, for example, is that creativity has

539
00:49:15,840 --> 00:49:21,920
been distilled down to a raw idea in your head, right? So for example, I might decide to mix

540
00:49:21,920 --> 00:49:26,080
the style of two artists and combine them with a new subject. And I want a black cat

541
00:49:26,720 --> 00:49:31,600
in front of Royal Holloway University in the style of cyberpunk. I've been doing that all day.

542
00:49:31,600 --> 00:49:35,920
And the technical process is now done for you. The only limit is your imagination. So just like

543
00:49:35,920 --> 00:49:40,480
Kenneth Stanley's Pick Breeder, creativity itself has now become this uber efficient and

544
00:49:40,480 --> 00:49:46,320
externalized process. I think it's unreal. But the thing is, like the reason I never thought GPT3

545
00:49:46,320 --> 00:49:51,760
was intelligent is because it can't be used non interactively. The magic must happen when it's

546
00:49:51,760 --> 00:49:58,800
used by humans interactively. Well, you can basically build a machine that is generating

547
00:49:58,800 --> 00:50:07,440
prompts for GPT3. So in principle, you can build a robot that has a vision to text module. And that

548
00:50:07,440 --> 00:50:12,880
is used to prompt GPT3 into generating a story about a robot who sees these things and interact

549
00:50:12,880 --> 00:50:19,440
with them. And then you take the output of the generative model and translate this using text

550
00:50:19,440 --> 00:50:27,680
to motor module. And in this way, you close the loop. And I just used it as a thought experiment

551
00:50:27,680 --> 00:50:33,920
to think about the limitations of embodiment for such systems. Second is essentially doing that.

552
00:50:34,720 --> 00:50:40,240
So somebody has made this happen. And even with the language model, it works to some degree.

553
00:50:40,240 --> 00:50:44,480
And we know that we don't want to do this with natural language because natural language is a

554
00:50:44,480 --> 00:50:49,280
crutch. These systems make up for this, but you're just using more natural language faster than

555
00:50:49,280 --> 00:50:54,960
people could use it. But there is some language of thought that we are using that is not learned,

556
00:50:54,960 --> 00:50:59,840
but discovered by our own mind that we converge on, that is much more efficient. And this language

557
00:50:59,840 --> 00:51:04,880
of thought seems to be able to bottom out and perceptual distributed representations that are

558
00:51:04,880 --> 00:51:09,200
unprincipled, like these neural networks are in a sense unprincipled, but they don't break.

559
00:51:09,200 --> 00:51:13,840
Then there is something that is vague and ambiguous and has small contradictions in it.

560
00:51:13,840 --> 00:51:19,840
But at some level, it also is able to emulate very principal logic very well and becomes very

561
00:51:19,840 --> 00:51:26,000
sparse and very powerful in expressing things concisely. And this very concise language of thought

562
00:51:26,000 --> 00:51:32,880
is so don't see it in our models. What Dali is doing, Dali 2, is that it combines the language

563
00:51:32,880 --> 00:51:38,560
model and division model using embedding spaces. And these embedding spaces basically project all

564
00:51:38,560 --> 00:51:43,360
the concepts into some high dimensional manifold and find similarities between them.

565
00:51:44,480 --> 00:51:50,960
And Gary Marcus points out that there is an issue with compositionality in this. So you need to

566
00:51:50,960 --> 00:51:56,480
find the semantic structure of a sentence that is made of a hierarchy of concepts. And this is easy

567
00:51:56,480 --> 00:52:01,120
to do with the grammar. And it's much harder to do this with a deep learning system that needs to

568
00:52:01,120 --> 00:52:06,960
discover this in a way and structure this space in the right way. It's not impossible. So when

569
00:52:06,960 --> 00:52:11,760
Gary Marcus says these models cannot do this and cannot learn it, he is probably wrong.

570
00:52:12,320 --> 00:52:17,680
But I think he is right in the sense that this is something that is much, much harder for the

571
00:52:17,680 --> 00:52:22,640
current approaches. They need dramatic training data than a human being. And the algorithms are

572
00:52:22,640 --> 00:52:28,720
not doing this naturally. So there are probably ways in which we could make this happen much more

573
00:52:28,720 --> 00:52:32,880
elegantly and quickly and converge, for instance, on models for arithmetic.

574
00:52:33,360 --> 00:52:39,520
That's right. I mean, I remember I read a really good Twitter thread, I think it was by Raphael

575
00:52:39,520 --> 00:52:43,920
Millier, you know, about compositionality of these large generative vision models. Because usually

576
00:52:43,920 --> 00:52:50,720
compositionality is referred to in respect of language models. I think Raphael said that the

577
00:52:50,720 --> 00:52:55,120
assessment of the claim is complicated by the fact that people differ in their understanding of what

578
00:52:55,120 --> 00:53:00,560
compositionality means. But if language is compositional, as you say, and thought is language

579
00:53:00,720 --> 00:53:05,200
as argued by the proponents of this language of thought hypothesis, I think Raphael said

580
00:53:05,200 --> 00:53:10,240
that he thought language itself should be compositional in a similar sense. And perhaps

581
00:53:10,240 --> 00:53:16,240
by extension, visual imagery should be compositional. So I think Gary was arguing in a nutshell that

582
00:53:16,240 --> 00:53:21,280
it's hard to go from the image, or let's say the utterance, if it's NLP, to the structure,

583
00:53:21,280 --> 00:53:25,920
or the grammar, or the constituents. It's much easier to go the other way around, would you agree?

584
00:53:26,880 --> 00:53:30,960
The issue is that language of thought is executable. And natural language is not.

585
00:53:32,000 --> 00:53:37,200
We execute natural language by translating it into our mind in something that we can execute.

586
00:53:37,840 --> 00:53:42,240
And the reason about code, you might use natural language to support your reasoning.

587
00:53:42,240 --> 00:53:47,280
But the code that you build in your mind is filled in some kind of abstract syntax tree that you

588
00:53:47,280 --> 00:53:52,480
can actually execute in your mind to some degree. And then you get a sense of the output. So you

589
00:53:52,480 --> 00:53:58,160
entrain your own brain with an executable structure. And this executable structure has

590
00:53:58,160 --> 00:54:03,760
properties that are quite similar to the ones that the compiler has in your computer. So you

591
00:54:03,760 --> 00:54:07,680
can anticipate what the compiler is going to do with your code. You're not going to do this with

592
00:54:07,680 --> 00:54:12,320
all the depths that your compiler do it, you might still have to run your code, but you will find

593
00:54:12,320 --> 00:54:18,160
when you want to experience programming, your stuff will usually run. So our language of thought

594
00:54:18,160 --> 00:54:22,880
can do this, it can execute stuff. And it's not just a machine neural network that guesses

595
00:54:22,880 --> 00:54:28,080
what the outcome is going to be and is right some of the time. But it gets pretty good at

596
00:54:28,080 --> 00:54:33,360
figuring this out. And this means that it has to build this compositional structure that has some

597
00:54:33,360 --> 00:54:38,880
verifiable properties. And we observe ourselves operating on this verification process, right?

598
00:54:38,880 --> 00:54:44,720
When we do introspection for the program, we observe ourselves how we direct our attention on

599
00:54:44,720 --> 00:54:52,080
making proofs. And this attentional algorithm that works in real time, that is making changes on

600
00:54:52,080 --> 00:54:58,320
your mental models and then predicts the outcome of these changes and compares this with what your

601
00:54:58,320 --> 00:55:03,200
mental computations give you and then fixes your models of how your own thinking process in this

602
00:55:03,200 --> 00:55:08,880
domain works and so on. You can observe yourself doing that. And it's nothing where I would say

603
00:55:08,880 --> 00:55:13,840
a given approach or the given approaches that we have will never get there. But there seem to be

604
00:55:13,840 --> 00:55:18,400
ways in which we have just barely scratched the surface in what you need to be doing to make

605
00:55:18,400 --> 00:55:22,720
these models sample efficient and sparse and more adequate to model domains you're interested in.

606
00:55:25,120 --> 00:55:30,480
Cool. Okay. Well, I mean, just to finish like the discussion about the OpenAI stuff, I mean,

607
00:55:30,480 --> 00:55:35,280
I agree with the prognosticators. And I do think that these large language models and

608
00:55:35,280 --> 00:55:38,720
these visual generative models will be revolutionary for some domains. But

609
00:55:39,280 --> 00:55:44,720
you know, I think you really need to have a human guiding the creative process, which is a huge

610
00:55:44,720 --> 00:55:49,760
limitation. But I think it could also potentially hint at what intelligence actually is, right?

611
00:55:49,760 --> 00:55:55,360
I think intelligence might be this externalized process in a cybernetic sense, if you like,

612
00:55:55,360 --> 00:56:00,160
this idea of intelligence being fully embedded in an algorithm in a single agent might be

613
00:56:01,040 --> 00:56:02,800
the wrong way to think about it.

614
00:56:03,440 --> 00:56:08,480
I think that humans, by and large, are very confused. Very often you need a human to guide a

615
00:56:08,480 --> 00:56:14,160
human, right? And then you ask yourself, if you do this recursively, does this society know where

616
00:56:14,160 --> 00:56:19,360
it's going? Or is this at some level confusion at all levels that is balancing each other?

617
00:56:19,920 --> 00:56:25,120
So there seem to be very few people with a plan right now. And it's quite apparent that we see

618
00:56:25,120 --> 00:56:29,920
that in the sciences, we see this in politics, we need an art and literature. It is that humans

619
00:56:29,920 --> 00:56:35,440
have a higher degree of sentience. But by and large, very few people have a principal plan on

620
00:56:35,440 --> 00:56:41,840
how to build a sustainable, harmonic world. And if you set an AI system to this task, it might

621
00:56:41,840 --> 00:56:47,040
make more progress on it. It's just that Dalit is not operating in real time on the universe that

622
00:56:47,040 --> 00:56:51,840
it's entangled with and neither is GPT-3. Both of them are in some sense, fancy autocomplete

623
00:56:51,840 --> 00:56:59,520
algorithms. But this fancy autocomplete is able to do autocompletions that are far beyond the

624
00:56:59,520 --> 00:57:06,880
autocompletion abilities of humans in almost every context. And so I don't see Dalit yet as art.

625
00:57:06,880 --> 00:57:16,160
It's a very strange sense when someone at OpenAI let me throw trumps at Dalit too. And I got images

626
00:57:16,160 --> 00:57:22,240
back. I had a sense of ownership. I had the sense that I was doing that, even though it was clearly

627
00:57:22,240 --> 00:57:26,880
doing skills, using skills that I didn't have. And I suppose that you had the same impression

628
00:57:26,880 --> 00:57:31,040
when you were generating things with your diffusion model that you're going to put up on your walls,

629
00:57:31,040 --> 00:57:36,400
right? You did that using this amazing tool that was empowering you to think that you otherwise

630
00:57:36,400 --> 00:57:42,480
never could do. But you are the creative nexus. And to make an artist, a digital artist, you would

631
00:57:42,480 --> 00:57:48,640
need to create an autonomous creative nexus in a way, a creative entity, something that reflects

632
00:57:48,640 --> 00:57:53,520
on the world because art is about capturing conscious states. So we would need to build a

633
00:57:53,520 --> 00:57:58,960
system that has a story about itself and that is reacting towards own interactions with the world

634
00:57:58,960 --> 00:58:04,640
and that would need to be human. It would need to be consistent. Something that is an intelligent

635
00:58:04,640 --> 00:58:10,160
entity that is creatively interacting with the world. I think we could totally build an AI

636
00:58:10,160 --> 00:58:15,200
artist franchise right now that would have a huge following. But what it need to have is an identity

637
00:58:15,200 --> 00:58:20,000
that is not fake, that it's actually built from its interactions with the world in real time.

638
00:58:20,960 --> 00:58:27,840
Well, I think we've got a lovely segue there because you said that art is about

639
00:58:27,840 --> 00:58:32,000
representing our conscious states. In a way, I disagree with you because you could say,

640
00:58:32,000 --> 00:58:36,000
well, it's very reductionist. I've just put a prompt in there and I've created art. Well,

641
00:58:36,000 --> 00:58:41,360
I think it is art. But how much of a representation of my conscious state is it? I think Douglas

642
00:58:41,360 --> 00:58:46,720
Hofstadter would say it wasn't. But over to the matter of consciousness because we're a bit low

643
00:58:46,720 --> 00:58:50,880
on time. I mean, you said actually that you've spent much of your life thinking about what

644
00:58:50,880 --> 00:58:57,040
consciousness is. And you said that you thought it was very mysterious, but you now think that

645
00:58:57,040 --> 00:59:01,840
it's a riddle that can be solved, right? So on your recent theory of everything interview

646
00:59:01,840 --> 00:59:07,360
with Donald Hoffman, actually, you said that it was virtual, not a physical thing, that brains

647
00:59:07,360 --> 00:59:13,520
are mechanistic and that the elements of consciousness are magical somehow. But you said

648
00:59:13,520 --> 00:59:19,200
it had an a causal structure, but not the way physics is built. But it was a story

649
00:59:19,200 --> 00:59:24,240
which the physical system tells to itself. You said that the organism is a coherent

650
00:59:24,240 --> 00:59:29,680
and consistent pattern, which is state building at least at some level of analysis and that

651
00:59:29,680 --> 00:59:35,280
consciousness allows organisms to coordinate their cells to succeed in their niches. And then

652
00:59:35,280 --> 00:59:42,320
you spoke of information processing over cells. Now, what model of I should say like what measure

653
00:59:42,320 --> 00:59:48,640
of consciousness do you think you most align with? There is only one theory that offers a

654
00:59:48,640 --> 00:59:52,880
measure of consciousness and that is integrated information theory, where you actually put a

655
00:59:52,880 --> 01:00:00,240
number on it. And it's not clear what that number means. It's not that there is some kind of scalar

656
01:00:00,240 --> 01:00:06,320
that measures this. And people we think of consciousness as something that is more qualitative

657
01:00:06,320 --> 01:00:11,280
than quantitative. Either somebody is conscious or somebody is unconscious. And when you are

658
01:00:11,280 --> 01:00:16,240
conscious, you can have a lack of acuity, you can be addled in your brain and you can be

659
01:00:16,240 --> 01:00:22,240
drifting in and out of consciousness. But it's still a qualitative thing of whether you have

660
01:00:22,240 --> 01:00:28,800
that or not. And this qualitative thing seems to be simple, probably much simpler than people

661
01:00:28,800 --> 01:00:34,960
expected to be. The hard thing might be perception. And consciousness is on top of

662
01:00:34,960 --> 01:00:40,480
perception is a certain way to deal with our attention. So I think a very important aspect

663
01:00:40,480 --> 01:00:46,000
of consciousness is reflexive attention, that we notice ourselves attending to something,

664
01:00:46,000 --> 01:00:52,400
and we reflect on that and integrate this in our model. The conundrum is understanding consciousness

665
01:00:52,400 --> 01:00:57,680
if you go right into the history of everything starting with Leibniz and many others. Leibniz

666
01:00:57,680 --> 01:01:05,680
says this idea of imagine you could have a mill and this mill, this is your mind. And the mill is

667
01:01:05,680 --> 01:01:11,680
made of lots of mechanical parts and somehow the thing is feeling and perceiving things.

668
01:01:12,320 --> 01:01:18,960
And we blow the mill up so large that we can walk into it or we would today zoom into it until

669
01:01:18,960 --> 01:01:23,840
we see all the parts and we just see these pushing and pulling parts and nothing of them

670
01:01:23,840 --> 01:01:29,760
can ever explain a perception or a feeling. And it is a very strong intuition that also

671
01:01:29,760 --> 01:01:36,400
drives the Chinese rule when many other thinkers will get attracted to this. And it seems pretty

672
01:01:36,400 --> 01:01:41,120
obvious that these mechanical phenomena are insufficient to explain what's going on. It's

673
01:01:41,120 --> 01:01:46,480
not an obvious connection. So people become dualist. There are somehow two completely separate

674
01:01:46,480 --> 01:01:51,920
domains. And I think in a way this dualism is correct, but not in the sense that the mental

675
01:01:51,920 --> 01:02:00,560
states are ontologically existing. They exist as if. There is no organism. There is only this

676
01:02:00,560 --> 01:02:06,240
connection of cells and this collection of cells is acting in a coherent way, which means we can

677
01:02:06,240 --> 01:02:10,960
compress it. We can model it using a very low-dimensional, much circular function

678
01:02:10,960 --> 01:02:15,600
than look at all the cells in general. And the organism is only approximating this function,

679
01:02:15,600 --> 01:02:20,720
but what makes the organism more powerful than a collection of cells is exactly that function,

680
01:02:20,720 --> 01:02:26,080
this structure that we project into it. And the interesting thing is that by the information

681
01:02:26,080 --> 01:02:30,640
processing within the organism, the organism can discover that function by itself and use it to

682
01:02:30,640 --> 01:02:36,880
drive its own behavior. So while the organism is not a person, it's not even an organism,

683
01:02:37,520 --> 01:02:43,040
it is very useful for the organism to behave as if it was an organism and also to have an idea

684
01:02:43,040 --> 01:02:46,800
of what it would be like to be a person that interacts, for instance, with the social world.

685
01:02:47,520 --> 01:02:52,160
So it creates a simulation of that. And it's often not even a simulation, it's often just a

686
01:02:52,160 --> 01:02:57,760
simulacrum. That's what makes it a causal. The difference between a simulation and the reality

687
01:02:57,760 --> 01:03:04,800
is that the simulation is modeling some aspects of the dynamics of a domain on a different causal

688
01:03:04,800 --> 01:03:10,160
substrate, on a different causal footing. So you have a computer game in which you can shoot a gun,

689
01:03:10,160 --> 01:03:16,640
but there is no proper physics in the game that would recreate what's happened in the real world

690
01:03:16,640 --> 01:03:21,520
when you shoot a gun. Instead, it is using a different causal structure of your software

691
01:03:21,520 --> 01:03:26,080
program to give you something that gives you good enough dynamics so you can interact with the world

692
01:03:26,080 --> 01:03:30,240
and experience these causal structures. You can make a different decision, you make a different

693
01:03:30,240 --> 01:03:35,120
move in the game, and as a result, the game behaves as if you would expect it because it's

694
01:03:35,120 --> 01:03:39,440
imitating the same causal structure using this different substrate. In the simulacrum,

695
01:03:39,520 --> 01:03:43,280
you don't have the causal structure. Like a movie doesn't have causal structure. It only

696
01:03:43,280 --> 01:03:48,960
gives you a sequence of observables. And our own mental model of ourselves is a mixture of

697
01:03:48,960 --> 01:03:55,360
simulation and simulacrum. So we sometimes create a sequence without causal structure. It looks like

698
01:03:55,360 --> 01:03:59,920
it does this thing magically. And sometimes we have a causal model, but this causal model

699
01:03:59,920 --> 01:04:04,560
is not the real deal. It's just this simplified geometric simulation of how the world works.

700
01:04:04,560 --> 01:04:09,040
It's a game engine that our brain is producing to anticipate what happens in the physical world.

701
01:04:09,920 --> 01:04:16,640
Yeah, so that very strongly resonates with me. And another person I very much respect

702
01:04:16,640 --> 01:04:22,720
whose opinion resonates with me regarding consciousness is Carl Friston. And I'm not

703
01:04:22,720 --> 01:04:27,920
sure how much you're familiar with his free energy principle and his thoughts on consciousness,

704
01:04:27,920 --> 01:04:33,040
but I'd like to put forward to you one of his more recent definitions, if you will,

705
01:04:33,040 --> 01:04:37,680
or proposals to explain consciousness. And I get your opinion on it. This is from his

706
01:04:38,320 --> 01:04:46,560
2018 article titled, am I self conscious or does self organization entail self consciousness?

707
01:04:46,560 --> 01:04:52,480
And what he says here is the proposal on offer here is that the mind comes into being when

708
01:04:52,480 --> 01:04:59,920
self evidencing has a temporal thickness or counterfactual depth, which grounds inferences

709
01:04:59,920 --> 01:05:06,800
about the consequences of my action. On this view, consciousness is nothing more than inference

710
01:05:06,800 --> 01:05:14,080
about my future, namely the self evidencing consequences of what I could do. Does that align

711
01:05:14,720 --> 01:05:16,800
pretty closely with your your view?

712
01:05:17,680 --> 01:05:23,920
No, I don't think it's sufficient and also don't think it's necessary. I like Friston's idea,

713
01:05:23,920 --> 01:05:27,920
but most of the free energy principle comes down to predictive coding,

714
01:05:27,920 --> 01:05:35,440
which is in some sense, radically tested with GPT-3. GPT-3 is trained in some sense entirely

715
01:05:35,440 --> 01:05:40,320
on predictive coding. It's only trying to predict the future from the past. And the future is the

716
01:05:40,320 --> 01:05:46,000
next token based on the tokens that it has seen so far. And GPT-3 radically tries how far you can

717
01:05:46,000 --> 01:05:52,080
go with this, and you can go very far. But you need far far more samples than an organism does.

718
01:05:52,960 --> 01:05:58,000
So there are players in us that go beyond predictive coding, maybe we converge towards

719
01:05:58,000 --> 01:06:03,280
this over many generations in the evolutionary process. So I don't think it's a stupid idea

720
01:06:03,280 --> 01:06:10,320
that Carl Friston proposes, but we are born with additional loss functions that let us

721
01:06:10,320 --> 01:06:15,680
converge much, much faster on something that is useful to the organism. And if we think about

722
01:06:15,680 --> 01:06:22,160
consciousness, he has a point about agency in there. Agency means that you have a controller

723
01:06:22,160 --> 01:06:27,600
that is able to control the future. Took me a while to understand this, but when I go up,

724
01:06:27,600 --> 01:06:31,280
we talked about BDI agents, and they seem to be quite complicated and convoluted,

725
01:06:31,600 --> 01:06:36,160
put a lot of quote there to make a BDI agent, but there's beliefs, desires, and intentions,

726
01:06:36,160 --> 01:06:42,720
and so on. But if we think about what actually is a minimal agent, a thermostat is not a minimal

727
01:06:42,720 --> 01:06:46,720
agent. So that has enough agency, it doesn't want anything. It just acts on the present

728
01:06:46,720 --> 01:06:52,240
frame by doing the obvious thing. But imagine that you give the thermostat the ability to

729
01:06:52,240 --> 01:06:57,680
integrate the expected temperature differences, the differences over the future, when it does x now

730
01:06:57,680 --> 01:07:03,120
or y now or does it a moment later, right? So suddenly you have a branching reality. And in

731
01:07:03,120 --> 01:07:08,000
this branching reality, you can make decisions and you will have preferences based on this

732
01:07:08,000 --> 01:07:13,680
integrated expected reward. So just by giving the thermostat the ability to model the future,

733
01:07:13,680 --> 01:07:19,120
you turn it into an agent. This is sufficient. And if you make this model deeper and deeper,

734
01:07:19,120 --> 01:07:23,520
it's going to get better and better at it. And at a certain depth, the thermostat is going to

735
01:07:23,600 --> 01:07:29,040
discover itself. It's going to discover the idiosyncrasies of its sensors. And notice that

736
01:07:29,040 --> 01:07:33,360
the sensor operates differently when it's closer to the heating element and so on and so on, right?

737
01:07:33,360 --> 01:07:38,480
So it becomes aware of how it functions. It might even become aware of the way in which its modeling

738
01:07:38,480 --> 01:07:46,000
and reasoning process works and to improve it or to account for its inefficiencies in certain ways.

739
01:07:46,000 --> 01:07:52,640
And this is also what we do with our own cell. But this model of the self is not identical to our

740
01:07:52,640 --> 01:07:58,480
consciousness. Our consciousness is a feeling of what it's like in the moment. It's the experience

741
01:07:58,480 --> 01:08:03,920
of a now. There is an experience of a perspective that we are having, right? And this is what's

742
01:08:03,920 --> 01:08:08,480
absent in the description of first. And he is missing the core point of what it means for

743
01:08:08,480 --> 01:08:12,240
something to be conscious. It doesn't mean that it has a self. It doesn't even just mean it has a

744
01:08:12,240 --> 01:08:19,040
first person perspective. It means that it experiences a reality. And this is not described

745
01:08:19,120 --> 01:08:26,160
in this Friston quote. We explicitly asked him this question actually when we talked to him

746
01:08:26,160 --> 01:08:34,480
last time. And his response there was that this concept of feel like is really something that

747
01:08:34,480 --> 01:08:41,120
would need to be coded into the generative model that this agent has about the world. Number one,

748
01:08:41,120 --> 01:08:45,040
it has to have a generative model as we've just been discussing. It has to be able to entertain

749
01:08:45,040 --> 01:08:50,560
counterfactual, you know, possibilities for the predictive coding, right? And he's saying that

750
01:08:50,560 --> 01:08:56,960
these feel like concepts would literally be encoded in that generative model as hypotheses

751
01:08:56,960 --> 01:09:03,680
that we recognize, you know, so things like I'm feeling pain, for example, would be a concept

752
01:09:03,680 --> 01:09:08,800
within that model. And he says there's actually evidence from, you know, treating patients with

753
01:09:08,800 --> 01:09:13,600
chronic pain and this sort of thing that that's actually exactly what's happening in the mind

754
01:09:14,160 --> 01:09:20,080
that that the feeling of pain is actually a concept that's built in as a slot, if you will,

755
01:09:20,080 --> 01:09:25,920
into this generative model. I mean, what do you think about that proposal?

756
01:09:27,520 --> 01:09:34,720
The semantics of pain are given by the avoidance that you don't want to experience the pain usually.

757
01:09:35,600 --> 01:09:40,160
And it could be that you cultivate the pain and use it to make something happening on the

758
01:09:40,240 --> 01:09:44,720
next level, but it requires that you are then building a multi-level control structure,

759
01:09:44,720 --> 01:09:51,680
if you want to use pain productively, some artists are maybe doing. But the, you cannot have pain,

760
01:09:51,680 --> 01:09:56,080
I think, without an action tendency, without something that modulates what you are doing.

761
01:09:56,960 --> 01:10:02,720
So your, your cognition is embedded into this engine. And to build such an engine that does it,

762
01:10:02,720 --> 01:10:08,480
that causally changes how you operate is not that hard. But when you live inside of such an engine,

763
01:10:08,480 --> 01:10:13,200
it feels very strange that there is something that is happening that somehow depends on what

764
01:10:13,200 --> 01:10:17,840
you are thinking, but you cannot control it. It controls you. It's upstream from you. You are

765
01:10:17,840 --> 01:10:23,920
downstream from it. And when you get upstream of your own pain, the pain stops being pain. It's

766
01:10:23,920 --> 01:10:29,520
something that is a representation that you can now control and be able to get there. But it's

767
01:10:29,520 --> 01:10:34,560
not easy and you're not meant to get there because it means that we can immunize ourselves to pain

768
01:10:34,560 --> 01:10:39,840
and sacrifice the organism to our intellectual interests. What's crucial about feelings when

769
01:10:39,840 --> 01:10:44,400
you look at them introspectively is that feelings are essentially geometric. I don't know if you

770
01:10:44,400 --> 01:10:50,320
noticed that. So for instance, we notice feelings typically in our body. And that's because I think

771
01:10:50,320 --> 01:10:55,680
that the feelings play out in a space. And the only space that we have always instantiated in our

772
01:10:55,680 --> 01:11:01,040
mind is the body map. So they're being projected into the space to make them distinct. And when we

773
01:11:01,040 --> 01:11:07,040
look at the semantics of the feeling, we noticed that they are contracting or expanding or they

774
01:11:07,040 --> 01:11:12,080
are light or they're heavy and so on. This is all movement of stuff in space. It's all geometry

775
01:11:12,720 --> 01:11:16,720
plus valence, the stuff that is going to push your behaviors in a certain direction.

776
01:11:17,280 --> 01:11:24,160
So these are basically the interactions of some deep learning system that is producing

777
01:11:24,160 --> 01:11:31,040
continuous geometric representations as perceived from an analytic engine. It's an interface

778
01:11:31,040 --> 01:11:36,880
between two parts of your mind, between the analytic attention control that is reflecting

779
01:11:36,880 --> 01:11:41,520
on the operations that your mind is doing while it's optimizing its attention. And the underlying

780
01:11:41,520 --> 01:11:46,720
system that represents the state of the organism entails where you should be going and makes this

781
01:11:46,720 --> 01:11:53,120
visible to you. It is a system that is not able to speak to you, uses geometry. And these

782
01:11:54,160 --> 01:11:59,120
geometrical features, this is what we call feelings. So that's a very interesting connection.

783
01:11:59,120 --> 01:12:05,520
And I think Jeff Hawkins of Nemento would be quite interested in that as well, because

784
01:12:06,960 --> 01:12:14,640
some of what he discussed with us was that, in his view, the evolution of, let's say,

785
01:12:14,640 --> 01:12:20,400
abstract thinking and whatnot actually came from systems that evolved to operate in just

786
01:12:20,400 --> 01:12:27,840
simple three-dimensional kind of motion, and that eventually those were reutilized by the

787
01:12:27,840 --> 01:12:34,240
evolutionary process to start engaging in abstract thinking, which he views as movement

788
01:12:34,240 --> 01:12:38,000
through an abstract space. And so I think there's a lot of connection here to what you're saying

789
01:12:38,000 --> 01:12:43,920
about feeling, which is that, again, in a sense, our mind has reutilized this

790
01:12:44,640 --> 01:12:51,120
three, three plus one d movement mapping capability that it needed in order to survive in a three

791
01:12:51,120 --> 01:12:58,240
plus one d environment, physical environment, and it's reutilized those for mapping feelings,

792
01:12:58,240 --> 01:13:04,640
it's reutilized them for mapping to abstract thinking is like a form of motion in an abstract

793
01:13:04,640 --> 01:13:11,760
space. Is that a fair connection? Yes, but I don't think that it's because it's

794
01:13:11,760 --> 01:13:17,760
borrowed from the world in which we interact, but because it's the only game in town, it's the

795
01:13:17,760 --> 01:13:24,480
only mathematics that can deal with multi-dimensional numbers. So when we talk about spaces, we actually

796
01:13:24,480 --> 01:13:29,040
talk about multi-dimensional numbers, about things that are not just a scalar in a single

797
01:13:29,040 --> 01:13:34,720
dimension, but features that are related. And sometimes you can take these features that you

798
01:13:34,720 --> 01:13:38,960
measure continuously because they have too many steps to meaningfully discretize them.

799
01:13:39,760 --> 01:13:46,160
So what it does is you discover that you can rotate something. And this is when you get a

800
01:13:46,160 --> 01:13:51,200
space in the sense as we have a space to which we are moving. And these spaces which you can

801
01:13:51,200 --> 01:13:59,760
rotate things only exist in 2D and 4D and 8D. And so the geometry that we're talking about is

802
01:13:59,760 --> 01:14:05,120
constrained to certain mathematical paradigms, which you can derive from number theory,

803
01:14:05,120 --> 01:14:12,080
compressed principles. And our brain is discovering a useful set of functions to model

804
01:14:12,080 --> 01:14:18,000
anything, a set of useful computational primitives. And we can probably give our deep learning systems

805
01:14:18,000 --> 01:14:22,880
a library of predefined primitives to speed up their convergence. That's also the reason why

806
01:14:22,880 --> 01:14:28,000
there is useful transfer learning between different domains. You can train a vision model

807
01:14:29,760 --> 01:14:34,800
and use it as a pre-training for audio. And it's not because it's the same thing, but because it

808
01:14:34,800 --> 01:14:40,480
has learned useful computational primitives that it can apply across domains. But there is geometry

809
01:14:40,480 --> 01:14:47,760
in the audio signal. So this is very interesting territory. I hope you'll come back to dive into

810
01:14:47,760 --> 01:14:52,640
this a bit more deeply when we have more time and a better connection, because I agree with you.

811
01:14:53,920 --> 01:14:56,160
Some very fascinating math here.

812
01:14:56,320 --> 01:15:06,160
Fantastic. Well, Dr. Yoshua Bak, it's, as I said, you've by far the most requested guest

813
01:15:07,040 --> 01:15:11,040
that we've had right from the very beginning. So it's an honor to finally get you on the show.

814
01:15:11,040 --> 01:15:14,480
And I hope we can get you back soon for a longer conversation. Thank you so much.

815
01:15:15,120 --> 01:15:18,400
Likewise. I enjoyed this very much. Let's meet again soon.


Hattie Jo, a PhD student at the University of Montreal and Miele, has set out to understand
and explain the performance of modern neural networks, believing it to be a key factor in
building better, more trusted models. Having previously worked as a data scientist at Uber,
a private equity analyst at Radar Capital, and an economic consultant at Cornerstone Research,
she's recently released a paper in collaboration with the Google Brain team titled,
Teaching Algorithmic Reasoning via InContext Learning. Now in this work, Hattie identifies and
examines four key stages for successfully teaching algorithmic reasoning to large language models,
formulating algorithms as skills, teaching multiple skills simultaneously, teaching how to
combine skills, and finally teaching how to use skills as tools. Now through the application
of algorithmic prompting, Hattie has achieved remarkable results with an order of magnitude
error reduction on some tasks compared to the best available baselines. Now this breakthrough
demonstrates algorithmic prompting's viability as an approach for teaching algorithmic reasoning
to large language models and may have implications for other tasks requiring similar reasoning
capabilities. Anyway, I hope you enjoy this conversation that I had with Hattie over at
NeurIPS a couple of weeks ago. Enjoy. Amazing. Well, Hattie, we're here at NeurIPS.
Welcome to MLST. Thank you. Please introduce yourself. I'm Hattie Jo. I'm currently a PhD
student at NILA and a part-time researcher at Google Brain right now as well as a student
researcher. Fantastic. And I've been reading your paper, Teaching Algorithmic Reasoning
via In-Context Learning. Give us the elevator, bitch. The elevator, bitch. So I think
typically in the past people have thought that large language models are not great at
doing symbol manipulation or actually doing reasoning the way humans do. And a common example
people point to for this failure is to show that models can't even do addition properly,
even though it's trained on billions of tokens, billions of parameters.
And in this paper, we try to teach the model how to solve these problems by learning an algorithm
for them and see if it can generalize to a harder problem, generalize out of distribution,
which is a common way to see if the model is actually using the correct algorithm to solve
these tasks. So it's not just like fitting to the training distribution and finding
spirit features, it's actually executing an algorithm. So we do that through some prompting
strategies and show that actually can learn how to do addition contrary to previous belief.
Yeah, I wanted to speak about the previous belief. So with large language models, I'm
vacillated back and forth from being skeptical and being very optimistic. I was originally
very skeptical and I'm becoming more optimistic as time goes on. I interviewed a lady from UC
Irvine called Yasaman Rezegi. And she did a really interesting bit of work actually kind of comparing
the term frequencies as they appeared in the corpus to the arithmetic as a function of the
number of digits. And because OpenAI never released their data set statistics, so it wasn't possible
to do that. And she found that there was like a linear correlation. But I think what's really
interesting now in your work and some of the other work like Chain of Thoughts Prompting and
Scratch Bad Prompting, that now we're in this new regime where we're kind of telling the language
model. It's almost like we're treating the language model a bit like a kind of compiler
and we're giving it the program. And then it's actually extrapolating and it's doing things that
it wasn't directly taught. Right. Yeah. I think that's the first step, right? You want to,
like if you can give the model a program, you want to see that the model can use that program
and apply it to new situations. And so that's kind of the reasoning out of the distribution
component. But I think also ideally you can imagine we want the model to be able to discover
algorithms that we don't know ourselves. And you know, that's a whole other frontier, right?
But this is the first step too on that path, I think. Okay. Well, why don't we start by defining
algorithmic reasoning? Sure. So I think about it as just the way you solve a task is by using a
particular algorithm. And algorithms are input independent, which means that basically for
any input distribution, using that same algorithm will get you the right answer. And so performing
reasoning by running an algorithm is what we refer to as algorithmic reasoning. And of course,
these right now apply to tasks that can be solved by an algorithm. But you could also imagine,
like, cases where you have a soft algorithm where the steps are not so rigidly defined,
but there is an overarching problem, like solving structure that you can follow.
Yeah. Could you refer to that? I mean, maybe I would refer to that as like inductive
algorithmic reasoning. So it's where, so on one side of the spectrum, you actually have the
algorithm and you write every single step explicitly using some kind of code. And now we're talking
about this regime where we are giving examples of an algorithm and we're describing the steps
somewhat vaguely. We're trying to be as clear as possible using language. So where are we kind
of following on that continuum? Well, let's say you take an algorithm of addition.
Yeah. But you actually, so you have an algorithm defined where you like start by taking the
first digit, do something with it, and then get an answer and then move on to the next one.
When I think about a soft algorithm, it can look something like that, except you take the digit
and the something you do with it is not defined explicitly, but it might require some abstract
pattern matching that large language models are particularly good at. So it can use the
same process of breaking down the problem in a very specific way to generalize, but the individual
steps are not actually encoded in code because it's abstract. So if you can do that, then you can
use this approach to tackle tasks where you can't write an algorithm or you can't write a program
for. And that's like very exciting, I think. Can you give me an example of where we can't
explicitly write the algorithm for something? Well, it's hard to come up with a good example
because it's supposed to be abstract in some sense, but these will not be questions of
that you would normally tackle with by writing a program. So it could be a soft reasoning.
I guess maybe even the grade school math word problems, there is no algorithm or a simple
algorithm at least that you can write that will solve math word problems, which are like if you
have four apples, I gave you twice the amount of apples you have. How many do you have now?
But if you define a way to tackle these questions at each individual step, the model can decide
how to apply that flexibly based on the particular question. Yeah, that might be a good thing to
have. But yeah, right now, I don't know. There is no good benchmark for these kind of things.
Yeah. And in your paper, so you came up with a new algorithmic prompting technique and you
designed some experiments and your technique works significantly better than some of the
other in context prompting techniques. Can you sketch that out for us?
Yeah, so the intuition is very simple, actually. When we look at the addition algorithm, the one
that we learned in school, we know that you start with the rightmost digits, you add them up,
you add them up. If it's greater than 10, you have a carry of one, and then you add the carry to
the next sum and so forth. So a scratch pad approach for addition will show the trace from
running this algorithm. So it will show the first sum is this, and the carry is this. But it doesn't
explain how those values are derived. But for us, it feels really natural because we're so familiar
with it. But for a model, trying to infer the rules from seeing a couple examples of it,
that's a very under specified problem. There are many rules that could explain perfectly these
observations. So algorithmic prompting basically explains each step of an algorithm using some
examples. And within each step tries to be as detailed as possible and tries to disambiguate
as much as possible what you want the model to do. And it turns out that when you provide more
details to the model, you're sort of constraining the model's interpretation of disinformation
so that there's only one way to apply this. And with that, you can get more robust behavior from
model and reason very well out of distribution. Yeah, because it's often said that deep learning
models do not reason. And I think what people mean by that is that you get this phenomenon of
shortcut learning and models do the right things for the wrong reasons. And it seems to me that
what we're doing here is by imputing the kind of the structure of how to reason into the prompt,
we're robustifying its behavior out of distribution. Yeah, well said. That's exactly right.
Fascinating. So what do you think of this problem of shortcut learning in general? Because you know,
like Melanie Mitchell said, there are two modes of understanding essentially. We have this anthropomorphic
mode of understanding which is using causality and it's very sample efficient and we have a way of
understanding the world. And we have a bit of an intuition that large language models
don't reason the way that we do. But is it necessarily a problem? And is it cheating in
your view using in-context learning? Or do you think that because we haven't had to train the
model again, what's the problem, right? I mean, is it an issue? Yeah, I mean, I guess there's
many forms of reasoning and algorithmic reasoning is only a subset of that. And I think if the model
can output the reasoning process and show that the answer that arrives at is following the output
of that process, then it's hard to argue that it's not doing reasoning. It might work differently
under the hood, but it follows the similar process. Now, you know, some methods, you output a rationale,
but the final answer is actually different from what the explanation suggests it should be.
And maybe you can point to that and say, oh, the model isn't actually using this. It's just like
giving you something that you asked for, but then the final answer is still using a shortcut.
But with the algorithmic reasoning, we see that that's not the case. And so, yeah, the more you
constrain things, maybe the more you remove shortcuts from the model. But an interesting
question, I think, is you can get this behavior using in-context learning, which I think you,
I suspect you can't really do from fine tuning or some sort of weight training.
I think when you do that, you'll most likely just overfit on the training distribution.
I wanted to ask you, you know, it's a bit of an open-ended question to say what's going on inside
large language models. But what's so exciting to me is that you get all of this emergent strange
behavior. No one would have imagined five years ago that we could do all of this prompt engineering
on a kind of autoregressive language decoder. And the model is trained to do something really
quite trivial, yet as a byproduct, all of this crazy stuff emerges in its internal representation.
And all of this algorithmic reasoning capability seems to be like a side effect of that training.
How does that even happen? Oh, that's a good question.
It's possible that in order to fit that large training data set, the pre-training data set,
you have to find regularities in content, I guess, that humans generate. And
I think some of these abilities come out of those regularities that you learn. So the ability to
refer to the pattern of a previous passage in context and, you know,
see what the relationships are in that pattern and apply the same relationships when you input
that circuit, I think, is just very useful as a way to summarize that large data set. And so
because you're forced to compress all that data into a model set of weights, I think these regularities
emerge somehow. I don't know exactly how. But I think, yeah, I mean, this is an interesting
speculation because then you can say with much larger models where there is no capacity constraint
at all, and you fit all the same data sets, it's not going to learn very interesting behavior
because it's able to just fit the model without capturing the underlying patterns.
And maybe that's why you actually need more training data and training longer rather than
like the optimal scaling is not right now in the model size, the amount of data. Because you want
like the right bottleneck for your representation. And I think maybe that's where these
emergent capabilities are coming from. Interesting. Yeah. And also the amount of training as well as
the amount of data. I wondered what is your intuition on the practical kind of computational
limitations of large language models? So at the moment, they're trained to transform us.
And there have been some pretty cool critiques of connectionism by Fodor and a few other people.
And it's basically along the lines of neural networks can't represent infinite objects,
which kind of distinguishes them from Turing machines. So they can't compute the nth digit of
time is a fairly good example. But the amazing thing is that we're doing all of this stuff with
algorithmic reasoning. And we haven't found the ceiling yet, it's just getting better and better.
And I think it's almost creating this. Well, I mean, I'm becoming quite hopeful,
actually, because I don't know where the limit is, but I suspect there is a limit quite soon.
How do you think about what the realistic computational limits are?
I think the fact that now we have in context learning is interesting because
that allows us to have, I guess, adaptive amount of computation. And so if you have,
let's say you have infinite context length, then you can sort of maybe do infinite computation
in that case. Now, is infinite context length possible? Probably not. But then you can find
clever ways to distill that information. You can find clever attention mechanisms.
And so I think maybe there's a computational limit, but you can always find new ideas that
make existing methods more efficient. And so, yeah, I have no idea when you would hit that
limit, but it's probably very far into the future. Amazing. And so, Hattie, we're here in
Europe. What have you taken from the conference so far? And what are you most excited about,
you know, going forward? Yeah, I don't know, because I haven't checked out the posters
very much yet. But I'm excited for the Math AI workshop, which is many other papers exploring
this idea of doing math with language models. And yeah, there's some, you know, very impressive
work there. So I'm excited for that. I'm excited to meet people and see what they're thinking about.
And maybe get some ideas for what to work on next. Yeah, I'm also really interested in the
math stuff I spent about an hour speaking with Marcus, is it Marcus Barb, who works under Christian
Sagadia, I think he's in your group. Yeah, they're doing some, that's right, they're doing some
really interesting stuff with basically doing mathematical conjecturing, you know, like representing
mathematical expressions in large language models and being able to generate new ones.
It's the kind of thing that you, again, would think was science fiction five years ago and like,
remarkably, it works. Exactly. And then by the way, with the mathematical conjecturing, Marcus was
saying that, unlike with large language models, it only has to be right one in 100 times because
they can formally verify it. So it's almost like the bar is actually lower in that sense.
Right. Yeah, I mean, that's where the language models informal reasonability is really useful.
Right. Yeah, like the pattern actually is actually useful in a lot of cases.
That's really cool. Cool. Well, actually, this has been amazing. Thank you so much for coming
on the show and telling us about your research. Oh, thanks for having me. Yeah, it was fun.
Looking beautiful. So Marcus, it's so nice to meet you. Can you introduce yourself?
Hi, I'm Marcus. I work for Google Research together with Christian Segedy and the
Antiformer team. Generally, we're working on trying to solve math,
basically by trying to translate natural language mathematics into formal mathematics and
in these formal representations of mathematics, we can
check proofs and use that as a feedback signal for the understanding of mathematics.
And most recently, I've been working on long context models like the memorizing transformer,
trying to get these models, like makes models sensitive to the exact definitions and other
lemmas that they might use for your improving. And that's an ongoing effort, hopefully more
available soon. Can I just say, I'm so jealous that you work with Christian.
I mean, folks will remember that we had a conversation with Christian. I think it was
about 18 months ago. It is one of my favorite episodes of MLST. So you're a very lucky man indeed.
Yes. Yes. It's great to work with Christian. It's a lot of fun. Amazing.

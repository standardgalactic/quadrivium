start	end	text
0	4240	Welcome back to Street Talk, just a little bit of housekeeping before we kick off today.
4240	10160	Polina Silivadov is one of the organisers for a charity AI conference called AI Helps Ukraine.
10800	15600	Now, their main goal is to raise funds for Ukraine, both from the folks attending the
15600	20080	conference and also from companies sponsoring the conference. And it's not too late to sponsor
20080	24960	the conference and support it, so please do if you possibly can. Now, all of the funds that they
24960	29360	raise will go to Ukraine Medical Support, which is a Canadian non-profit organisation
30000	34960	which is specialising in humanitarian aid for Ukraine. Now, they have some of the world's
34960	41760	leading AI experts keynoting at this event, so Yoshua Benjo, Timnick Gabru, Max Welling,
41760	47280	Regina Basile, Alexei Efros and also one of our own personal favourites here on MLSD,
47280	52720	Professor Michael Bronstein, the one and only. Now, the conference is online pretty much from
52720	58720	now until the 6th of December and it's being hosted from Mela in Montreal. Their goal is to
58720	65200	raise $100,000 and they really, really need the support of the AI community to club together and
65200	70800	to just donate anything that you can. So, we'll link to the conference in the video and the podcast
70800	77760	description. Please donate if you can and also share the links on your socials. Now, I'm at New
77760	84560	Europe this week in New Orleans, so I'll be walking around with a camera. Please just bump into me
84560	91280	and if you want to record any spicy takes on artificial general intelligence, then let's do it.
92720	98240	Today is a conversation with Yoshua Bach, who's one of our most requested guests ever.
98240	102800	We recorded the conversation back in April, which gives you a bit of an indication of our backlog.
102800	109360	I can only apologise about the backlog. Keith and I recently started a new venture called X-Ray
109360	115040	and as you can imagine, we've been working around the clock coding, just trying to get that business
115040	120080	off the ground. But when we've made our millions, we'll devote all of our time to producing amazing
120080	124880	content on MLSD. So, yeah, please bear with us loads and loads of cool content coming your way
124960	128480	soon. I hope you enjoy the show today. Peace out.
128480	135840	Dr Yoshua Bach is our most requested guest ever. Yoshua Bach is a cognitive scientist focusing
135840	143760	on cognitive architectures, models of mental representation, emotion, motivation and sociality.
143760	149760	Yoshua's interview on Lex's podcast, he did two interviews on Lex's podcast,
149760	154240	have been watched over two million times so far, which is just absolutely unreal.
154880	159680	Now Yoshua, I've watched many of your interviews and I still don't feel that I have a firm grasp on
159680	164480	some of your views. So today, if you don't mind, I hope we can do a tour de force over some of your
164480	169120	most important views in our shared space, to the extent that we can keep up with you, of course.
169680	176400	Now, for example, we'd like to discuss Godel and computation, consciousness, digital physics,
176400	182000	free will and determinism, large statistical models and indeed whether they're AGI or a
182000	186880	parlor trick or something more esoteric. Now, when people talk about God or consciousness
186880	192400	or any other complex phenomena, it relates to everyone and it means something different to
192400	198080	everyone. It's ineffable and every conversation sounds like a typical post ketamine discussion,
198080	203360	which is to say extremely low information content. Now, the topics we're discussing today are very
203360	208480	complex and often we're reaching for the best language to use to conduct the conversation.
208480	213520	So I hope we do well today. Anyway, Dr. Yoshua Barker is an absolute honor to finally welcome you
213520	221760	to MLST. Thank you very much. I'm glad to be on the show. Amazing. Well, when I started doing
221760	227200	computer science many years ago, interestingly, the theory of computation wasn't even on the
227200	231760	curricula. And I was wondering whether you thought it should be. I mean, presumably you think it's
231760	236560	extremely relevant for AGI. Now, we want this to be as pedagogical as possible. So please explain
236560	243200	everything like we're five. What does computation mean to you? I think that computation is
243200	248800	far easier than most people think. It means that you have a causal structure where every
249760	255040	transition can be decomposed into individual steps. And when we talk about computational models,
255040	260640	we decompose the world into states and transitions between the states. And then it turns out that
260720	266960	there is a certain minimal system that is able to execute everything. And this can be described in
266960	272240	many ways. The most famous one is probably the Turing machine and many other ways in which you
272240	278480	can describe the Turing machine. For instance, you can just do the Turing machine by doing
279520	283520	search and replace on strings. And this is how the Lambda calculus is defined.
284240	288880	And all the programming languages and the Lambda calculus and the Turing machine
288880	293120	turn out to have the same power. That is, if you can compute something with one of these paradigms,
293120	296960	you can compute it with the others. As long as you don't run into resource constraints,
296960	301520	so as long as it still fits into memory and you don't care about speed, they all have the same
301520	308000	power. But in practice, of course, every system is limited. So we don't run things forever. We
308000	312880	want them to give us a result after a certain time. So what matters is what can be efficiently
312880	319440	computed, not what is reachable at all. Awesome. And there are, and we're going to get into this
319440	324800	a bit, but there are some possible loopholes, at least in, you know, let's say whether or not
324800	330720	the universe is limited in certain ways that the definitions of like Turing machines are.
330720	337040	And one I wanted to ask you about specifically is Penrose's claims. And so he claims that
337840	344560	what Godel's work in fact proves is that the human mind can understand truths that are not
344560	351600	provable. So specifically one can show that, you know, given Godel's sentence is necessarily true
351600	357440	given given a mathematical analysis, even though it can't be proven within the formal system that
357440	363120	it's that it's defined. And Penrose claims that this capability to understand, if you will, to
363120	369680	mathematically understand is in fact non computational, at least in part. And so if he's right, then
369680	376720	our brains might be what Turing referred to as Oracle machines. These are computers that have
376720	384080	access to a non computable Oracle or function that they can then utilize those oracles in
384080	389440	order to perform hyper computation, essentially. So I'm asking, I'm curious, are you open to this
389440	394080	possibility? And if not, what is your response to Penrose's arguments?
395040	400240	I suspect that Goedl has been misunderstood by a lot of philosophers. Goedl was a truth
400240	405920	realist. That is, he thought that truth really exists out there. That it's the thing that is
405920	410960	eternal in some sense. He had this very strong intuition and mathematics classically is also
410960	415760	formalized in this way. The difference between mathematics and computation, at least in the
415760	420080	standard sense in which we normally teach mathematics at school is that mathematics has no
420080	427520	states. Everything in mathematics just is eternally. It's a single state. And if you want to go through
427520	433120	a sequence of states, you put an index into the formula. But still, everything is there at the
433120	437760	same time. The index is just a way to access this thing. And this way of having mathematics
437760	442960	stateless is very elegant because it allows us to define functions that have infinitely many
442960	447760	arguments. If you would have a state machine that tries to consume infinitely many arguments,
447760	452080	it would never finish before it goes to the next step. And the same thing in the middle of the
452080	456320	function, if you compute something, if it's stateless, you can just compute all the indices
456320	461840	all at once, even if it's infinitely many in classical mathematics. In a computational system,
461840	466480	you would have to do this maybe one after the other. And if you do it at parallel, you will have
466480	471920	lots of CPUs running at parallel. So you run into limits. And the same thing with the output. So
472000	476880	in the classical mathematics, you can chain infinitely many steps and functions and exchange
476880	481600	infinitely many arguments. But of course, mathematicians never did this a practice. It's
481600	486160	just a specification. This is how they like to write things down. When they want to calculate it,
486160	492080	they still have to go down and do it sequentially step by step. Just mathematics is defined in such
492080	497840	a way as if you could upload this to some supernatural being or some grad student who is
497840	504160	going to do the infinitely many calculations. And Goethe took this specification of mathematics
504160	508160	and he found out that when you have this stateless mathematics, you can, for instance,
508160	513920	define self referential statements that change that choose value depending on the statement itself.
514560	519680	And this recurrence leads can lead to a contradiction in the state itself. So you basically
519680	525200	get two statements which say I am wrong. And if by referring to itself, it changes its own
525200	531040	truth value. So if mathematics is stateless, you will now run into a conflict. In a computational
531040	534800	system, that's not a big problem. Your computer is not going to crash. If you write it down the
534800	539520	right bay, it just happens is that your truth value fluctuates in every execution step. It's not
539520	545360	going to converge. But this is not the real truth. Truth is something that doesn't change
545360	552560	when you call the function again. So what's going on here? And I think what Goethe has
552640	557760	discovered is that classical mathematics doesn't work. What you cannot build is any kind of
557760	562560	mathematics doesn't allow you to build a machine, a hypothetical abstract machine, any kind of
562560	566480	universe that runs the semantics of the classical mathematics without crashing.
567360	574080	Yeah, but it kind of seems like, okay, we're going to believe Goethe's use of mathematics
574080	579840	to prove that mathematics is flawed. Like there seems to be almost an inherent contradiction
579840	585360	in there. Like you either believe mathematics, and thus you believe Goethe's proof of some
585360	591760	specific limitations on computational systems, right? Or you believe that somehow mathematics
591760	595520	is flawed, in which case you can't trust the proof that mathematics is flawed.
597200	602080	I think Goethe's conclusion was that there is something fundamentally going wrong, that there
602080	607760	might be an inability of mathematics to describe reality. And if you believe that truth is real
607760	612800	and it exists independently of the procedure by which you calculate it, then this seems to be
612800	617520	plausible. And it was also the conclusions a lot of philosophers have drawn from this,
617520	622000	which basically read Goethe's proof and concluded that mathematicians have admitted that their
622960	627920	arcane techniques are important to describe reality. And therefore, philosophers who don't
627920	634480	understand mathematics have a clear advantage. Of course, this is not the conclusion. Instead,
634480	640880	what turns out is that if you just skip or if you drop the original classical notation or
640880	645920	as understanding of mathematics and replace it by computation, basically we say,
645920	650560	truth is what you calculate with the following procedure. And you can define any kind of
650560	654400	procedure that you want. You just have to make sure that it converges to some kind of value in
654400	659120	the way that you want. Then you resolve your problem. It's just that you lose your notion that
659120	664320	truth is independent of that procedure. And so in some sense, the classical mathematics is a
664320	668240	specification that cannot be computed. From the perspective of computer scientists,
668240	672880	this happens all the time. Some customer wants you to build something that cannot be built,
672880	676400	and you have just proven that it cannot be built. Right? But it doesn't mean that you
676400	682160	cannot build something useful. And I think that Penrose believed that our brain is actually doing
682160	688000	these infinite things. And it's not. When we reason about infinity, we are not actually reasoning
688000	693280	about infinitely many steps. What we do is we create a symbol, and then we do very finite
693280	698240	computations over that symbol. But we cannot construct infinity. We cannot build it. We
698240	704320	cannot go there from scratch and write down some clever automaton that produces an infinity for you.
705440	709840	I was recently browsing Penrose's book, The Road to Reality, and I would say that, I mean,
709840	714800	I don't know that much about physics, but the chapters were really interesting. They're talking
714800	721040	about surfaces and manifolds and symmetries and fiber bundles and gauges and wave functions,
721040	725920	calculus, matrix theory, and even computation. I mean, almost all of the discussion was on
725920	730800	mathematical modeling at different levels of description or emergence, if you will. And
730800	735840	in machine learning and AI, we are forever challenged by trying to get machines to model
735840	741360	physical reality at different levels of description using an interoperable set of tools. So it seems
741360	746480	increasingly true that we need machines that can learn descriptions and concepts at multiple levels
746480	750640	if we're ever going to have AGI capable of understanding the world and learning novel
750640	756480	semantic models. All of machine learning models today work by chopping up a Euclidean space into
756480	762480	what is effectively a locality-sensitive lookup table. Very big one. And we need AGIs that can go
762480	768000	far beyond this. It's got to be able to learn novel geometries beyond even what humans could
768000	772480	have come up with and the ability to reason topologically and algebraically over those
772480	777040	geometries. Something which I think you would agree is not happening with the current deep learning
777040	784640	systems. Well, let's start out with the notion of geometry first. If you read Penrose's book,
784640	789040	what you find is that this entire universe is geometric, which means it's made of
789040	795680	continuous spaces in which things are happening. And if we actually look into the world deeply,
796320	801520	quantum mechanics is not a geometric theory. The geometry only emerges approximately
801520	807040	at the level of the space-time description. And it seems that geometry is actually the
807040	812000	domain of too many parts to count. In reality, all the objects that we describe as surfaces,
812000	817520	if you zoom in, are made of discrete parts like atoms and particles and so on. And these in turn
817520	823040	are made out of things that have a finite resolution. And if we look into our computer
823040	827440	programs, you can create stuff that looks continuous to us, but there's nothing continuous
827440	834640	inside of our computer programs. And it turns out that the assumption of continuity requires
834640	840160	that we partition the space into infinitely many parts. So now we are again running against
840160	846080	that thing which GÃ¼rl has shown us as difficult. And it's not a big problem in practice because
846080	850720	in practice, we never need to do these infinitely many things to produce a computer game with an
850720	857280	arbitrary fidelity. We can make something that looks like space. But the space that we think in
857280	862560	and so on is an approximation that our brain has discovered. It's a set of operators that converge
862560	868080	in the limit. But the limit doesn't exist. It just, it happens that when you live in a world that is
868080	872960	made of too many parts to count for almost everywhere where you look, you need to find these operators
872960	877120	that converge in the limit. And the set of operators that happens to converge in the limit
877120	883280	and is still computable. This is what we call geometry. And to use these uncomputable geometric
883280	888480	approximations for macroscopic physics like Newtonian mechanics is completely fine. You're
888480	892480	just going to compute it up to a certain digit and then this is good. But it's a problem for
892480	898160	foundational physics. Because if it turns out that you cannot take a language that actually
898160	902880	computes infinities, if you cannot construct your language, then you cannot write a universe in it.
903200	907920	So our universe is not written in continuous language, but Penrose universe is.
909840	913840	This doesn't mean that geometry is full. We need this to describe the world of too many parts to
913840	918080	count. But we do this via computational approximations. Our brain does the same.
919280	924960	So let me ask you this then because we come across kind of the infinities a couple of times. And I
924960	929680	know that you placed an emphasis on constructive mathematics. So of course, you and all of us,
930240	935280	you know, except let's say the existence of potential infinities, you know, algorithms that
935280	939840	you can sit there and just keep calculating for as long as you want and get kind of more digits.
939840	946000	But it's really around actual infinities that we seem to be running into problems. So let me ask
946000	951120	this this first question here, really leading up to some computational questions, which is,
951920	958720	can the universe, can our actual universe that we're in right now be actually infinite
958720	959840	in spatial extent?
962320	966480	A problem is that it can have unboundedness in the sense that you have a computation that
966480	973120	doesn't stop giving your results. But you cannot take the last result of such a computation and go
973120	978080	to the next step. You cannot have a computation that relies on knowing the last digit of pi
978080	982960	before it goes to the next step. In the sense that you don't have an infinity. But the infinities
982960	987120	are about the conclusion of such a function. It means that you actually run this function to the
987120	991920	end and then do something with the result. Unboundedness is different in the sense that
991920	996080	you will always get something new that you didn't expect that they cannot predict.
996080	1002400	But it's just going on and on without this end. And I think it's completely conceivable that our
1002400	1009360	universe is in this class of systems in the sense that it doesn't end. But it doesn't mean that there
1009360	1015040	is anything that gives you the result of an infinite computation. Because if that was the case,
1015040	1020160	then it could not be expressed in any language. It also means if something cannot be expressed
1020160	1025360	in any language, that you cannot actually properly think about it. Because when you think you need
1025360	1029920	to think in some kind of language, not in English, but in some kind of language of sort or in a
1029920	1035040	mathematical language that doesn't have contradictions. And what Goethe has shown is that the language
1035040	1042080	that he hoped to reason in about infinities breaks that it has contradictions in it. That at some
1042080	1047840	point, it blows itself apart. So the languages that we can build are only those in which we have
1047840	1052000	to assume that infinities cannot be built. So infinity, in this sense, is meaningless.
1052560	1055040	Because we cannot make it in any kind of language.
1056000	1062480	So the thing is, though, I'm not limiting what the universe is capable of based on human mental
1062480	1069520	and linguistic limitations or even mathematical limitations. I'm asking you if it's possible
1069520	1076480	for this universe that we're in to ontically be right now actually infinite in spatial extent.
1079360	1084640	The thing is that you try to make a reference to something that you cannot observe, that cannot
1084640	1090880	conceive of other than making a model in some kind of language. And to have that model make sense,
1090880	1096320	the language needs to work. Right? Otherwise, you are just maybe in some kind of delusional thing.
1096960	1101200	And we can construct delusional things. We can construct languages that have bugs that we cannot
1101200	1106480	see. But if we use a language that has bugs in it that we cannot see and we cannot repair them,
1106480	1110960	then this means that the stuff that we express in the language is not meaningful. Right? We have to
1110960	1115040	use a different language that has maybe the same expressive power but doesn't have these bugs.
1115600	1121200	But now if you try to think about the universe in the language that allows you to imagine that
1121200	1125920	the universe is literally infinite, rather than very, very, very big and much bigger than you
1125920	1131360	can imagine and not ending, which is for all means and purposes almost the same thing. Right?
1132480	1138560	Then if you do this other thing, then your thought doesn't mean anything. So it's basically you cannot
1138560	1143440	properly express the idea in your own mind without running into contradictions that the
1143440	1149120	universe is infinite in the sense that such a universe could exist. Okay, so you're basically
1149120	1152800	following that. That's the issue. Basically, I cannot think that the universe is infinite. I cannot
1152800	1158720	express this. That's my issue. Okay, fine. So you're basically saying that the English that I
1158720	1166000	used just a minute or so ago just is not coherent or not conceivable. It's not something that you
1166000	1168960	want to. But the underlying thing behind the English, right? English is not designed to be
1168960	1174720	coherent. It's designed to be disambiguating. It's designed to be unprincipled to allow us to
1174720	1180400	express things vaguely and not break. But if you think really, really deeply and really exactly,
1180400	1184720	then the question is, what kind of model is your mind building? At which point is there just some
1184720	1190800	kind of noisy nabler that you're pointing at without actually decomposing it and anything that would
1190800	1201120	make sense? Okay. And so the lack of really the ability to conceive or for actual infinities to
1201120	1206240	ontically exist in some sense, if we just deny all that, so we're really just stuck with,
1206880	1212160	all right, we've got finite everything, discrete everything. There's no such thing as a continuum.
1212800	1219040	There's no such thing as actual infinite spatial extent, etc. That's really the world that you're
1219600	1223600	proposing here, right? That everything is constructed from at the end of the day,
1223600	1229360	finite, discrete kind of elements. So if we... Yeah, you can imagine that your mind is a library
1229360	1233440	of functions in a way, and these functions are doing jobs. And on the bouts of the box,
1233440	1238240	you write down what these functions are doing. And you construct a box that this, in this box,
1238240	1244320	there is an infinity between, for instance, a continuum between two points. And then you open
1244320	1249920	up the box and look at what's actually inside of the box. And you realize it's just a lot of small
1249920	1254320	steps. And it's designed in such a way that you can, if you want to have more steps, it's going to
1254320	1259360	give you more steps if you zoom in, right? And it's totally doing, apparently, what's written down
1259360	1264080	on the box. But if you look very closely, realize, oh no, the thing that is written down on the box
1264080	1268080	that you have written down on the box cannot actually be in there. You can prove that it cannot
1268080	1272000	be in there. It must be something else that's in there that is doing most of the work of what you've
1272000	1276000	written down. So what you should actually be doing, I think, if you are interested in how things
1276000	1282320	actually work, write on the box what it's actually doing, which means it's going to subdivide or
1282320	1286320	any interval with any resolution you want as long as you can afford it.
1287280	1293920	Okay. One mystery, if you will, for me, and I'm hoping you can help me understand this, is that
1294720	1301920	all of the standard models for physics that we have today, they do have in them these continuous,
1301920	1307440	you know, for example, symmetries that are rotational symmetry or things like that. They're
1307440	1315440	built off of positing continuums with continuous waves, lots of continuities and infinities,
1315440	1320480	at least in the mathematical descriptions. Except for quantum mechanics, right?
1320480	1325520	Right. And I think based on what you've been saying, you would say that those are artifacts or
1325520	1332720	properties of our mathematical descriptions of reality, but they're not actually extant in
1332720	1342880	reality. And my mystery there is why do those continuous and mathematical maybe flawed and
1342880	1351120	inconsistent with infinities all over the place descriptions work so well for describing phenomenon
1351120	1356080	at different levels? If everything at the end of the day, you know, if we just looked at high enough
1356080	1362800	energy and small enough resolution, we'd see kind of the grid and, you know, all the discrete
1362800	1370480	effects and rotation happening kind of in little tiny, very small but not infinitesimal degrees.
1370480	1376000	You know, why does all this continuous infinity based mathematics work so well? What is the
1376000	1380160	explanation for the unreasonable effectiveness of that kind of mathematics?
1380960	1388080	The easiest answer is that the world in which we live in is made of extremely small parts.
1388080	1394160	And we could not exist if that world was not made of that many small parts. So for instance,
1394160	1399680	you want to have a momentum for particles that are almost continuous. So you can address the
1399680	1404480	space with high resolution because the momentum is what tells you where information comes from
1404960	1410160	in the universe, basically the direction of where from which information reaches you and so on.
1410160	1415920	If that would be very coarse, then the complexity that you could build would probably be far lower.
1415920	1422160	And we consist of so many parts that when you look down, it's uncountably many for all practical
1422160	1427200	purposes. So the mathematics that we need to describe the world that we are in that we need
1427200	1432880	to model are mostly not in the realm of countable numbers. The countable numbers only play a role
1432960	1439360	when we are looking at very few microscopic things. As soon as we leave this domain of a few apples
1439360	1446160	on our table, we almost instantly drop in this realm where we just need to switch to a continuous
1446160	1452080	description of things. And this is completely fine for most of our history. When we did physics,
1452080	1458880	we never zoomed in that heart. And even now, when we really need to zoom at the level where
1458880	1464640	the plank length matters and the resolution of the universe becomes visible. And it's of course not
1464640	1469360	some Euclidean lattice, some grid that you can see. It's just that at this level, you no longer
1469360	1476880	have space. I wanted to move matters back over to some of the happenings in the world of large
1476880	1483040	language models and deep learning and so on. And first, quick fire question. I honestly,
1483040	1488000	you're a bit of an enigma to me, Joshua, because obviously I've read some of your research and
1488000	1494320	you seem like a hybrid guy to me. You know Ben Goetzel very well, for example, but you're also
1494320	1501200	hugely into the hype train on the connectionism. For example, you criticised Gary Marcus's
1501200	1504640	article. So the first question is, are you a symbolist or a connectionist?
1505440	1512160	I'm neither. The thing is that I hate deep learning as the best of us. Deep learning is ugly. It's
1512160	1519280	brutalist. It's a few very simple algorithms that are blown up to the max. But I cannot prove
1519280	1525840	that these algorithms do not converge to what we want them to converge to. It's maybe not
1525840	1531280	elegant, but it works. And the solution to problems with deep learning so far has always
1531280	1538880	been to use more deep learning, not less. So what upsets me about Gary Marcus argument is not that
1538960	1544000	I'm not sympathetic to what he's trying to push it. I'd like to build models that are more elegant,
1544000	1552160	more sparse and so on. But in the past, all these elegant sparse models have been left in the dust
1552160	1557360	by just using more deep learning. And we can also see when we zoom out a little bit that there is
1557360	1562800	not an obvious limit to deep learning itself, because deep learning is not just the algorithms.
1562800	1567600	Deep learning is a programming paradigm. It's differentiable programming. It basically means
1567600	1574160	that you express everything with approximately continuous numbers, and you use algorithms that
1574160	1582160	converge business at certain ranges. And when it doesn't converge, then you just tweak it and you
1582160	1587920	introduce a different architecture, which is some kind of discrete operations that you do on these
1587920	1592960	continuous numbers and so on. You just patch it, you write your programs slightly differently,
1592960	1597520	and you can automate the search for the program. And the people who do deep learning are not also
1597520	1602080	docs in the sense that they say, oh my God, symbolic structures are not allowed. I cannot use
1602080	1607600	a Python script in here rather than just a TensorFlow. This is not what's happening. It's
1607600	1612400	also not that they are constrained to any kind of thing that will use whatever is working.
1612400	1616960	And what we see is that the end-to-end train systems are going more and more powerful,
1616960	1621200	and rather than sitting there by hand and tinkering and finding a solution,
1621840	1626720	we can just use a system that is tinkering automatically through a dramatically larger
1626720	1632720	space than we would ever be able to explore by trying all sorts of algorithms. So when we look
1632720	1638480	at Gary Marcus' articles like his deep learning is hitting a wall and so on, and you look what he's
1638480	1642880	actually giving as arguments, the arguments are not very good. He gives us an example,
1642880	1649600	the NetHack challenge. NetHack is a game which has a very large horizon because you basically
1649600	1655120	have only one life. You need to explore a very deep labyrinth and you need to plan pretty far ahead
1655120	1661120	with what you're doing. And so it's something that is difficult to discover this right solution
1661120	1667680	with a deep learning model that has no prior ideas about what it's doing. Because it takes us
1667680	1672000	very, very long until you get the necessary feedback to learn about your actions. And people
1672000	1676480	are relatively good at learning this because they have so many ideas about what the situation is that
1676480	1681680	they're in. There's so many priors from our world interaction and from other games that we have played
1681680	1686960	that we can bring to the tasks. So the current winner of this is the symbolic solution.
1688080	1694480	And the symbolic solution that Gary Marcus gives as a proof that symbolic methods are still ahead
1694480	1700320	of deep learning things. In a single case, not like he has a big array of tasks where they are
1700320	1706000	superior, it's just two students who have written a program that is made of lots and lots of events.
1706000	1710960	This is just a big hack. This is not some symbolic learning algorithm that does something novel,
1710960	1716880	hybrid or whatever. No, this is just a script. And is Gary Marcus seriously proposing, oh my
1716880	1720880	god, deep learning models are limited and we need to replace them with more scripts?
1721760	1729040	This is not a good argument. Yeah. So I think maybe, and look, I get that there are these kind
1729040	1734080	of two competing camps and they maybe go after each other with some. No, they don't. This is only on
1734080	1739760	Twitter. There is, there are no competing camps. It's Yandekun is not also docs in the sense that
1739760	1743440	he believed you need to use this argument, all the other arguments are impure and flawed.
1744880	1750400	His brand is to build systems that work. And if one of his people comes up with something that
1750400	1755680	works better than what he came up with, you probably praise him for that and let him go on.
1757120	1764080	Yeah, sure. But there's absolutely, however, there is, you know, there is, let's say, momentum
1764080	1769280	and hardware lotteries and paradigms that kind of reinforce themselves. And to an extent,
1769360	1774400	they can strangle off, you know, resources that maybe we like, we shouldn't be investing all
1774400	1780640	our eggs in one basket. We shouldn't be pouring, you know, the 99% of research funding necessarily
1780640	1786240	down, down deep learning. And I think that's kind of the problem that, that these paradigms cause.
1786240	1792000	But I want to get back to something you said, which is a good point. It's, I think that's an
1792000	1796800	important point. I think that in absolute terms, the other approaches get more money than they did
1796800	1802240	before. It's not that we have a funding stop, as we had at some point, a return funding stop for
1802240	1807280	Neural Networks. And Marvin Minsky wrote a book where he saw he had proven that the Neural Networks
1807280	1812400	cannot converge over multiple layers, press up drones cannot earn X or and so on. Right. Minsky
1812400	1817440	was wrong. People found a way around this. But at this time, there was so little funding that
1817440	1823200	this cutoff mattered. And at the moment, if you want to do something that has AI and Adline,
1823200	1827440	the chance that you get it funded and whether what paradigm you're doing is greater than ever.
1827440	1831760	So the absolute amount of funds that goes into any kind of paradigm that you want to work on
1831760	1836960	is greater than ever. And the reason why the majority of funds goes into very few paradigms is
1836960	1842080	because these are the things that work in industrial applications. There is no other
1842080	1846240	algorithm that is able to learn from scratch how to translate between arbitrary languages and
1846240	1851600	generate stories and draw pre pictures for you. This is the only game in town at the moment,
1851600	1856320	the only class of algorithm that converges over all these many domains. And people are looking
1856320	1860800	for better alternatives. And yes, we are in a bubble, because of course, they're looking mostly
1860800	1865520	where things already were. You have hardware that works, you have libraries that work and so on.
1865520	1870480	It's hard to get out of that bubble. That is true. And it's always good to push for alternatives
1870480	1877440	and so on. But I don't think that we should be in a panic and say, Oh, my God, there is something
1877440	1883440	politically wrong. I suspect that by and large, the forces of the markets and the forces of the
1883440	1888320	academic researchers that want to explore alternative are pushing in the right direction already.
1889840	1894160	Yeah, I mean, fair enough. And, you know, you could be right. And there may not be that much
1894160	1900880	of an imbalance. But I want to get back to one technical thing you said. Yes, it seems apparent
1900880	1906320	that, let's say, what deep learning is doing is this this differentiable program search,
1906400	1911760	if you will. And a question I have about that is if we imagine the space of all possible programs,
1912800	1917760	that, you know, requiring that we're doing a differentiable search is certainly going to skew
1917760	1925360	that sample space that may even cut off programs in that space that can't be discovered easily by
1925360	1930080	differentiable search. So I'm wondering, doesn't that leave open the possibility that other
1930080	1935680	algorithms that are more discreet in nature, say evolutionary algorithms or discrete program
1935680	1941600	search or whatever, they may have access to a different subspace of the space of all programs
1941600	1946000	that aren't easily accessible by differentiable paradigms. Is that true?
1948720	1952960	The question is, how do you find it? How do you find these algorithms to manipulate the
1952960	1958880	discrete things? I agree that when you have a perceptual model that is modeling everything
1959520	1966560	with chains or sums over real numbers, and a few non-algebraic throne, and you get
1966560	1970320	characteristic artifacts. For instance, in the generative models, you often have the problem,
1970320	1975280	and you try to model a person with glasses or without glasses, that because the model thinks
1975280	1980240	that these features are somewhat continuous, you often run into the situation that you get areas
1980240	1985040	in the generative model, where the glasses are half materialized, and it looks always very weird.
1985040	1991200	And you have these strange things where reality has a discontinuity, but your model has permissible
1991200	1997120	states where you are in the middle of the discontinuity, and you try to generate something
1997120	2001200	that cannot exist. You want your model to be structured such a way ideally that every
2001200	2007440	model configuration corresponds to a world configuration. And this is not necessarily the
2007440	2012080	case with many of the deep learning models. And what the deep learning models, as you train them
2012080	2017360	harder, typically tend to do is that they squeeze these impermeasurable areas until you are very
2017360	2022400	unlikely to end up in them. And it's probably possible to get them to implement filters and
2022400	2028080	all sorts of tricks. But what you can also do is you can combine this with some kind of discrete
2028080	2033600	machine. And then what you do is you learn how to use this. So this deep learning network is not
2034160	2039280	interacting with the world directly, but it learns how to use an architecture that does that.
2040080	2044640	So for instance, instead of training a neural network to do numerical calculations,
2044640	2050160	you can train it to use a numerical calculator. And in this way, it can become very sparse again.
2050720	2056080	Right? So there's not an obvious limit to that I can see where I can prove to the deep learning
2056080	2060080	people, oh, here's where you should stop deep learning, because they can just combine their
2060080	2064560	deep learning approach with other approaches and use the deep learning system to remote control
2064560	2068400	this. And it turns out when we reason and so on, even when we do discrete reasoning,
2068400	2074000	that the steps that we assemble it to each other are heuristics that require some kind
2074000	2078720	of probabilistic element. Right? So when we form a sort that when the sort is made of very
2078720	2084640	discrete elements, the search for that sort is some kind of deep learning process that is happening.
2084640	2091760	Right? And when we make the pool, we do this, we emulate a discrete reasoning. But of course,
2091760	2096880	we can combine this and we can get the neural network to learn how to perform the discrete
2096880	2102400	operations. There's a certain thing that I would like to see, which is something like a more sparse
2102400	2109600	language of thought. When we are looking at deep learning models, there's a phenomenon that people
2109600	2115520	are sometimes observing, which they call grocking. That is, you train the model and your model gets
2115520	2120320	better and better. And then it overfits, which means it gets very good at the training data,
2120320	2124960	but it gets very bad on the real world at things that it hasn't seen before, like a person in
2124960	2129920	psychedelics was able to explain everything in the past, but is no longer able to perform well in
2129920	2133920	the future because they're overfitting. They basically fit the curve too closely to the data
2133920	2138800	that I've seen. And there are many tricks in deep learning to go around this overfitting to make
2138800	2143440	sure that this doesn't happen. And people try to avoid it. And then what they discovered is when
2143440	2147200	you take this overfit model, you train it more and more and more and more. At some point, it
2147200	2153040	sometimes clicks and it gets much better than ever before. And there is a question if there's
2153120	2158960	something that we're doing wrong in deep learning. For instance, when you think about how people
2158960	2165120	learn, they learn very different from GPT-3. People first learn by pointing at stuff that
2165120	2169200	thinks that are relevant to them, that they can eat, that they can hurt, that can hurt them,
2169200	2174000	or that they find pleasant and so on. They, that they can feel that they can, they have contrast
2174000	2178640	on it that are salient to them. And so you start out with learning these semantics based on the
2178640	2184000	saliency and relevance that you have. And then when you learn language, you learn basic syntax,
2184000	2188720	how to put things together. And in the long tail of the syntax, you learn style, how to express
2188720	2195680	things with new ones and so on. And with GPT-3, it's the opposite. You first learn style, right?
2195680	2200880	And then you learn syntax as the regularities in the style. And the semantics is the long tail of
2200880	2206320	that. And to make that happen, you need to learn much, much more. You need to have more training
2206320	2212240	data and so on. Maybe there's a way in which we can reverse the order and basically get it to
2212240	2217200	start out with relevance, to build a curriculum where you first get very sparse regularities,
2217200	2222560	where it clicks into place. You always make sure that you can handle it with very limited resources
2222560	2228640	and only see the style and the niceties and the nuances as the far extensions of these very sparse
2228640	2234320	concise models that have very big predictive power. Yeah. I mean, on that, I mean, the Grocking
2234320	2239200	paper was very interesting. And a lot of these large language model fans always cite that very,
2239200	2243280	very quickly when you have a conversation with them. But there is a problem with machine learning
2243280	2247840	in general, which is that there is, as you said, there's a spectrum of correlations and almost
2247840	2254000	all of them are spurious. And on one side of that spectrum, you have the idealized features you
2254000	2258000	actually want it to learn, which will generalize after distribution. And then, of course, if you
2258000	2263680	go down that spectrum, you pick up on all sorts of very spurious correlations that just happen
2263680	2268400	to generalize very well. And if you tell the models not to use those spurious correlations,
2268400	2273280	that the performance of the model will go down. But I want to just move a little bit over to
2274800	2279680	Yasaman Rezegi's paper. I don't know whether you saw that, but she showed that the performance of
2279680	2285040	large language models for arithmetic tasks are linearly correlated to the term frequency and
2285040	2290160	the training corpus, suggesting that they are memorizing the data set, which presumably you
2290160	2295840	would agree with. And Google has recently released this 540 billion parameter language model called
2295840	2301840	PAM, which interestingly does extremely well on, for example, some of the Google big bench tasks,
2301840	2307120	such as the conceptual combinations task, which is one of them, which tests for compositionality,
2307120	2311840	which we'll talk about in a minute. But compositionality is when you can take constituents from
2311840	2316400	the prompt and compose them together to form the answer. Now, it's tempting to jump to the
2316480	2321360	conclusion that these models are starting to magically reason at scale along the lines that
2321360	2326320	you were just discussing. But I still think there's plenty of opportunities for shortcut learning,
2326320	2331120	you know, by which I mean these spurious correlations, given the brittle interface of an
2331120	2337600	autoregressive GPT style language model with these human designed benchmarks. Would you agree with that?
2337600	2347600	Yeah. When I started my own career in computer science in the 90s, I was in New Zealand, and the
2347600	2353520	prof here in Britain realized that I was bored in class. So he took me out of the class and in
2353520	2358480	his lab, and he gave me the task to discover grammatical structure and an unknown language from
2358480	2364080	scratch and left me pretty much to my own devices on how to do this. So the unknown language I picked
2364080	2368640	was English, was just unknown to the computer, but was the easiest one to get a corpus for,
2368640	2372560	and they gave me the largest computer they had. It has two gigabytes of RAM, and I did
2372560	2377600	in memory compression with C and so on, and tried to do statistics, and I quickly realized NREM
2377600	2384560	statistics don't work because of too many words in between. So unlike vision tasks where
2384560	2390400	confnets have a useful prior by thinking that adjacent pixels also relate to symbolically
2390400	2394720	related information, right? So adjacency in images is a very good predictor for thematic
2394720	2399920	relatedness. It doesn't really work in NLP. So the transformer was discovered in natural language
2399920	2406560	processing for that reason, because you cannot use direct adjacency very well. And so I realized
2406560	2413360	I cannot use NREM, which depend on direct adjacency between words. And so I first of all used ordered
2413360	2418960	pairs of words and tried to find correlations between pairs and then find a mutual information
2418960	2424160	tree that would give me the best prediction over the structure of the sentence for all the
2424160	2429120	sentences that I would have in my corpus. And indeed this correlated to structure. And I realized
2429120	2433680	this is going to not just give me grammar, but it's also going to give me semantics
2433680	2439280	if I can more deep statistics. But I will need something not just ordered pairs,
2439280	2444480	but I need to have something like force order models. But to do the statistics, even in memory,
2444480	2448960	with clever and memory compression and many tricks that I did, I could not do full statistics on
2448960	2456800	this. So what I realized I had to do was that I do multiple passes. And at first I discard almost
2456800	2461680	all the information. I only pick out the most salient things. And then my time was over in
2461680	2467280	this lab. And I went back to Germany and never reviewed this area of research again. But what
2467280	2470960	I had realized is to make progress and need to make statistics over what I need to make the
2470960	2476320	statistics over. And the very principled base, I need to learn what I have to learn.
2477600	2483600	And they didn't pay attention to this domain at all. And I also missed the 2017 transformer paper
2483600	2488320	and its relevance. It was only when GPT-2 came out that I realized, oh my god, they did this.
2488320	2494880	They did statistics over the statistics. And it's still not the right solution, I think. It's not
2494880	2498800	the way in which our brain is doing it. It's some brute force shortcut. Well, for instance,
2498880	2502000	the individual attention heads are not correlated with each other, but in reality,
2502000	2507680	they are. We have this in reality, our attention heads are integrated into one model of what's
2507680	2513600	going on. And it's not that we have an attention net on every layer that just pays attention to
2513600	2519120	what's happening in the lower layer. It's much more clever in our own mind. And this thing is
2519120	2523360	active. We single out things in reality to research, which book we need to take out of the
2523360	2527840	shelf to update our working memory context so we are able to interpret the current sentence
2527840	2532800	that we don't understand. And so we always go for saliency when we read something that doesn't
2532800	2541040	make sense. At least my mind works like this. I discard it. I will not stop until it makes sense,
2541040	2546480	or I will have to go to some preliminary. I will not accept some kind of vague statistical
2546480	2551360	approximation of what I read. Keep this as an intermediary stage in my mind until the hope
2551360	2556480	that eventually converges. It's a completely different learning paradigm. When we teach our
2556480	2561760	children arithmetic, it's not that we show them lots of very long mass textbooks and hope that
2561760	2566400	initially it will not make any sense to them. But as they reread them again and again with many
2566400	2571280	samples, eventually it will click and they will converge on arithmetic. No, this is not how it
2571280	2575440	works. You start this giving them extremely simple things and say, in these extremely simple things,
2575440	2579600	there is structure that you can fully understand. Now go and find the structure that you fully
2579600	2584400	understand. Once you've done it, you make this a little bit more complicated for you. This is
2584400	2592320	probably the paradigm that you could be exploring. I know, but the problem is it's incredibly
2592320	2595680	deceptive when you have something which appears intelligence. Of course, the
2596400	2602000	boundary of our perception of intelligence is a receding one. But I wanted to just get on to
2602640	2609200	there are some incredible generative visual models like Dali and the disco diffusion.
2609680	2614240	These models, I think, are going to revolutionize the creative profession. I've been
2614240	2618960	playing with disco diffusion all day today. I've already ordered some prints to go on my wall.
2618960	2625600	It's incredible. The two obvious settings where large language models might be successful
2626400	2631680	are coding and information retrieval in my opinion. But let's take pause for thought. I've played
2631680	2637600	with Codex and I'm resolutely sure that I wouldn't want to use it. I think code and knowledge are
2637600	2643120	a different ballgame to art, which I think will be amazing. With Codex, there's an impedance mismatch
2643120	2647680	between the process of generating the code and then debugging and running the code, which has
2647680	2652800	euphemistically been framed as prompt engineering, or another term which I've just invented,
2652800	2658560	retrospective development. I think it's easier to start again from scratch than fix broken code
2658560	2663120	from a large language model. I mean, this is quite interesting. At Google, it already takes
2663120	2668320	months to get any code checked into their mono repo because it's basically a bureaucracy because
2668320	2673840	they needed to have gatekeeping after they decided to use a mono repo. Could you imagine
2673840	2678880	how much bureaucracy there'd be if they allowed people to start checking in code, which was generated
2678880	2683120	from an algorithm? Anyway, I think there's an exciting possible future for using these systems
2683120	2689440	for information retrieval rather than the way that we go through and prune the results on a Google
2689440	2695280	search. These models might just answer directly, but I hasten to think what that search UI would
2695280	2701120	look like. Would its output be sclerotic or unadaptable? Would it be relevant to the query that
2701120	2707920	I put in? Would its output even be true? Perhaps it will ask you to select what kind of truth you
2707920	2713600	were looking for. Do you think these models would vitiate or spoil our society, or do you think they
2713600	2719520	would actually enrich it? It's very hard to say. I think that from some perspective,
2719520	2725840	our society is already maximally spoiled. Humans, as they live today, are basically
2725840	2731040	locusts with opposable thumbs. This is not going to go on forever, this technological society.
2731040	2736480	Here are, it seems to me, on some kind of Titanic that is going to hit the iceberg no matter what.
2737200	2742080	And what basically should make us content is that the Titanic was the only place in the
2742160	2748080	universe that has internet. And we are born on it, and we wouldn't have been born if there was no
2748080	2754880	Titanic. We would not have been born in a sustainable ancestral society. In some sense,
2754880	2758960	our society needs to reinvent itself. It's not really working right now. We don't know
2758960	2762560	what the future is going to look like, and if it's going to be very technological,
2762560	2769360	or if limit certain things, no idea what's going to happen. But if we think about how our
2769440	2775680	current approaches work, if you want just to make programming better, I suspect that these tools can
2775680	2781360	help. But they will be much more useful if you do not have to have this battle between a machine
2781360	2787280	that doesn't really understand what you want. And instead, you have something that is working next
2787280	2793680	to you. It's like, imagine you were working for some corporation and the corporation introduces
2793680	2798160	some kind of planning tool that requires to do to jump through all sorts of hopes. And it turns
2798240	2802960	out that the planning tool itself makes you more productive, but it makes work much less fun.
2804160	2807520	It's still rational to use it. And everybody will hate it, but
2808640	2813600	by and large, it will be used if it makes people 30 more productive. And everybody will feel there
2813600	2818640	might be a must be a better solution, something that feels more organic. And so it could be that
2818640	2823440	Codex is in this category that it makes mediocre programmers much more productive at producing
2823440	2828240	boilerplate. But it's not just this, it's often able to find solutions very quickly,
2828240	2832480	but you need to use a lot of Stack Overflow before you understand the new language or before you
2833120	2837760	tease this new algorithm or part that you want to understand and so on. It just when it doesn't
2837760	2842080	probably turn you into a better programmer, if that is your goal. But for your employer,
2842080	2846080	maybe they don't care whether you're a better programmer, they just want you to turn out these
2846080	2850720	pages of code and then they run this against the verifier and against the unit test and then are
2850720	2856400	done, go to the next thing, right? So maybe it's not that important. But the systems that we would
2856400	2860640	want, what would they look like? I think they need to know what they're doing. You want to have a
2860640	2865680	program that is not just able to reproduce something very well in a given context, you want to
2865680	2871360	understand the context as deeply as you do or better. So it needs to understand what kind of
2871360	2876160	world it's operating in in the moment. And what itself is, what is it that it can do? What is
2876160	2881440	that what needs to learn still? In some sense, you want systems that are sentient. And it's self
2881440	2886160	like, oh my God, but it just means you have a learning system that's general enough to model
2886160	2891360	in principle the entire universe. And this is not as outrageous as it sounds because
2892480	2899200	Delhi is already dealing with two modalities, language and images. And we will get to video
2899200	2904400	and we will get to audio connected and you see them early steps in this direction with the Socratic
2904400	2909920	model of people, for instance. So I think that's almost inevitable that this generality will happen.
2909920	2913520	And you will have to add a system to work in real time so it can discover itself.
2915600	2922480	I think I think there's something really magic, though, about the creative process here. And also
2923200	2927920	the prompt engineering is another thing we can talk about. But Kenneth Stanley once made this
2927920	2933840	thing called Pick Breeder. And you could essentially distribute the selection of these images
2933840	2940480	created with CPPNs, Compositional Pattern Producing Networks. And you would just get these beautiful
2940480	2946320	images. They were they were so incredibly, incredibly diverse and interesting. So it's
2946320	2951920	not that the algorithms were intelligent, there was something magic about the externalized process.
2951920	2955840	And what's really interesting about these models like Dali, for example, is that creativity has
2955840	2961920	been distilled down to a raw idea in your head, right? So for example, I might decide to mix
2961920	2966080	the style of two artists and combine them with a new subject. And I want a black cat
2966720	2971600	in front of Royal Holloway University in the style of cyberpunk. I've been doing that all day.
2971600	2975920	And the technical process is now done for you. The only limit is your imagination. So just like
2975920	2980480	Kenneth Stanley's Pick Breeder, creativity itself has now become this uber efficient and
2980480	2986320	externalized process. I think it's unreal. But the thing is, like the reason I never thought GPT3
2986320	2991760	was intelligent is because it can't be used non interactively. The magic must happen when it's
2991760	2998800	used by humans interactively. Well, you can basically build a machine that is generating
2998800	3007440	prompts for GPT3. So in principle, you can build a robot that has a vision to text module. And that
3007440	3012880	is used to prompt GPT3 into generating a story about a robot who sees these things and interact
3012880	3019440	with them. And then you take the output of the generative model and translate this using text
3019440	3027680	to motor module. And in this way, you close the loop. And I just used it as a thought experiment
3027680	3033920	to think about the limitations of embodiment for such systems. Second is essentially doing that.
3034720	3040240	So somebody has made this happen. And even with the language model, it works to some degree.
3040240	3044480	And we know that we don't want to do this with natural language because natural language is a
3044480	3049280	crutch. These systems make up for this, but you're just using more natural language faster than
3049280	3054960	people could use it. But there is some language of thought that we are using that is not learned,
3054960	3059840	but discovered by our own mind that we converge on, that is much more efficient. And this language
3059840	3064880	of thought seems to be able to bottom out and perceptual distributed representations that are
3064880	3069200	unprincipled, like these neural networks are in a sense unprincipled, but they don't break.
3069200	3073840	Then there is something that is vague and ambiguous and has small contradictions in it.
3073840	3079840	But at some level, it also is able to emulate very principal logic very well and becomes very
3079840	3086000	sparse and very powerful in expressing things concisely. And this very concise language of thought
3086000	3092880	is so don't see it in our models. What Dali is doing, Dali 2, is that it combines the language
3092880	3098560	model and division model using embedding spaces. And these embedding spaces basically project all
3098560	3103360	the concepts into some high dimensional manifold and find similarities between them.
3104480	3110960	And Gary Marcus points out that there is an issue with compositionality in this. So you need to
3110960	3116480	find the semantic structure of a sentence that is made of a hierarchy of concepts. And this is easy
3116480	3121120	to do with the grammar. And it's much harder to do this with a deep learning system that needs to
3121120	3126960	discover this in a way and structure this space in the right way. It's not impossible. So when
3126960	3131760	Gary Marcus says these models cannot do this and cannot learn it, he is probably wrong.
3132320	3137680	But I think he is right in the sense that this is something that is much, much harder for the
3137680	3142640	current approaches. They need dramatic training data than a human being. And the algorithms are
3142640	3148720	not doing this naturally. So there are probably ways in which we could make this happen much more
3148720	3152880	elegantly and quickly and converge, for instance, on models for arithmetic.
3153360	3159520	That's right. I mean, I remember I read a really good Twitter thread, I think it was by Raphael
3159520	3163920	Millier, you know, about compositionality of these large generative vision models. Because usually
3163920	3170720	compositionality is referred to in respect of language models. I think Raphael said that the
3170720	3175120	assessment of the claim is complicated by the fact that people differ in their understanding of what
3175120	3180560	compositionality means. But if language is compositional, as you say, and thought is language
3180720	3185200	as argued by the proponents of this language of thought hypothesis, I think Raphael said
3185200	3190240	that he thought language itself should be compositional in a similar sense. And perhaps
3190240	3196240	by extension, visual imagery should be compositional. So I think Gary was arguing in a nutshell that
3196240	3201280	it's hard to go from the image, or let's say the utterance, if it's NLP, to the structure,
3201280	3205920	or the grammar, or the constituents. It's much easier to go the other way around, would you agree?
3206880	3210960	The issue is that language of thought is executable. And natural language is not.
3212000	3217200	We execute natural language by translating it into our mind in something that we can execute.
3217840	3222240	And the reason about code, you might use natural language to support your reasoning.
3222240	3227280	But the code that you build in your mind is filled in some kind of abstract syntax tree that you
3227280	3232480	can actually execute in your mind to some degree. And then you get a sense of the output. So you
3232480	3238160	entrain your own brain with an executable structure. And this executable structure has
3238160	3243760	properties that are quite similar to the ones that the compiler has in your computer. So you
3243760	3247680	can anticipate what the compiler is going to do with your code. You're not going to do this with
3247680	3252320	all the depths that your compiler do it, you might still have to run your code, but you will find
3252320	3258160	when you want to experience programming, your stuff will usually run. So our language of thought
3258160	3262880	can do this, it can execute stuff. And it's not just a machine neural network that guesses
3262880	3268080	what the outcome is going to be and is right some of the time. But it gets pretty good at
3268080	3273360	figuring this out. And this means that it has to build this compositional structure that has some
3273360	3278880	verifiable properties. And we observe ourselves operating on this verification process, right?
3278880	3284720	When we do introspection for the program, we observe ourselves how we direct our attention on
3284720	3292080	making proofs. And this attentional algorithm that works in real time, that is making changes on
3292080	3298320	your mental models and then predicts the outcome of these changes and compares this with what your
3298320	3303200	mental computations give you and then fixes your models of how your own thinking process in this
3303200	3308880	domain works and so on. You can observe yourself doing that. And it's nothing where I would say
3308880	3313840	a given approach or the given approaches that we have will never get there. But there seem to be
3313840	3318400	ways in which we have just barely scratched the surface in what you need to be doing to make
3318400	3322720	these models sample efficient and sparse and more adequate to model domains you're interested in.
3325120	3330480	Cool. Okay. Well, I mean, just to finish like the discussion about the OpenAI stuff, I mean,
3330480	3335280	I agree with the prognosticators. And I do think that these large language models and
3335280	3338720	these visual generative models will be revolutionary for some domains. But
3339280	3344720	you know, I think you really need to have a human guiding the creative process, which is a huge
3344720	3349760	limitation. But I think it could also potentially hint at what intelligence actually is, right?
3349760	3355360	I think intelligence might be this externalized process in a cybernetic sense, if you like,
3355360	3360160	this idea of intelligence being fully embedded in an algorithm in a single agent might be
3361040	3362800	the wrong way to think about it.
3363440	3368480	I think that humans, by and large, are very confused. Very often you need a human to guide a
3368480	3374160	human, right? And then you ask yourself, if you do this recursively, does this society know where
3374160	3379360	it's going? Or is this at some level confusion at all levels that is balancing each other?
3379920	3385120	So there seem to be very few people with a plan right now. And it's quite apparent that we see
3385120	3389920	that in the sciences, we see this in politics, we need an art and literature. It is that humans
3389920	3395440	have a higher degree of sentience. But by and large, very few people have a principal plan on
3395440	3401840	how to build a sustainable, harmonic world. And if you set an AI system to this task, it might
3401840	3407040	make more progress on it. It's just that Dalit is not operating in real time on the universe that
3407040	3411840	it's entangled with and neither is GPT-3. Both of them are in some sense, fancy autocomplete
3411840	3419520	algorithms. But this fancy autocomplete is able to do autocompletions that are far beyond the
3419520	3426880	autocompletion abilities of humans in almost every context. And so I don't see Dalit yet as art.
3426880	3436160	It's a very strange sense when someone at OpenAI let me throw trumps at Dalit too. And I got images
3436160	3442240	back. I had a sense of ownership. I had the sense that I was doing that, even though it was clearly
3442240	3446880	doing skills, using skills that I didn't have. And I suppose that you had the same impression
3446880	3451040	when you were generating things with your diffusion model that you're going to put up on your walls,
3451040	3456400	right? You did that using this amazing tool that was empowering you to think that you otherwise
3456400	3462480	never could do. But you are the creative nexus. And to make an artist, a digital artist, you would
3462480	3468640	need to create an autonomous creative nexus in a way, a creative entity, something that reflects
3468640	3473520	on the world because art is about capturing conscious states. So we would need to build a
3473520	3478960	system that has a story about itself and that is reacting towards own interactions with the world
3478960	3484640	and that would need to be human. It would need to be consistent. Something that is an intelligent
3484640	3490160	entity that is creatively interacting with the world. I think we could totally build an AI
3490160	3495200	artist franchise right now that would have a huge following. But what it need to have is an identity
3495200	3500000	that is not fake, that it's actually built from its interactions with the world in real time.
3500960	3507840	Well, I think we've got a lovely segue there because you said that art is about
3507840	3512000	representing our conscious states. In a way, I disagree with you because you could say,
3512000	3516000	well, it's very reductionist. I've just put a prompt in there and I've created art. Well,
3516000	3521360	I think it is art. But how much of a representation of my conscious state is it? I think Douglas
3521360	3526720	Hofstadter would say it wasn't. But over to the matter of consciousness because we're a bit low
3526720	3530880	on time. I mean, you said actually that you've spent much of your life thinking about what
3530880	3537040	consciousness is. And you said that you thought it was very mysterious, but you now think that
3537040	3541840	it's a riddle that can be solved, right? So on your recent theory of everything interview
3541840	3547360	with Donald Hoffman, actually, you said that it was virtual, not a physical thing, that brains
3547360	3553520	are mechanistic and that the elements of consciousness are magical somehow. But you said
3553520	3559200	it had an a causal structure, but not the way physics is built. But it was a story
3559200	3564240	which the physical system tells to itself. You said that the organism is a coherent
3564240	3569680	and consistent pattern, which is state building at least at some level of analysis and that
3569680	3575280	consciousness allows organisms to coordinate their cells to succeed in their niches. And then
3575280	3582320	you spoke of information processing over cells. Now, what model of I should say like what measure
3582320	3588640	of consciousness do you think you most align with? There is only one theory that offers a
3588640	3592880	measure of consciousness and that is integrated information theory, where you actually put a
3592880	3600240	number on it. And it's not clear what that number means. It's not that there is some kind of scalar
3600240	3606320	that measures this. And people we think of consciousness as something that is more qualitative
3606320	3611280	than quantitative. Either somebody is conscious or somebody is unconscious. And when you are
3611280	3616240	conscious, you can have a lack of acuity, you can be addled in your brain and you can be
3616240	3622240	drifting in and out of consciousness. But it's still a qualitative thing of whether you have
3622240	3628800	that or not. And this qualitative thing seems to be simple, probably much simpler than people
3628800	3634960	expected to be. The hard thing might be perception. And consciousness is on top of
3634960	3640480	perception is a certain way to deal with our attention. So I think a very important aspect
3640480	3646000	of consciousness is reflexive attention, that we notice ourselves attending to something,
3646000	3652400	and we reflect on that and integrate this in our model. The conundrum is understanding consciousness
3652400	3657680	if you go right into the history of everything starting with Leibniz and many others. Leibniz
3657680	3665680	says this idea of imagine you could have a mill and this mill, this is your mind. And the mill is
3665680	3671680	made of lots of mechanical parts and somehow the thing is feeling and perceiving things.
3672320	3678960	And we blow the mill up so large that we can walk into it or we would today zoom into it until
3678960	3683840	we see all the parts and we just see these pushing and pulling parts and nothing of them
3683840	3689760	can ever explain a perception or a feeling. And it is a very strong intuition that also
3689760	3696400	drives the Chinese rule when many other thinkers will get attracted to this. And it seems pretty
3696400	3701120	obvious that these mechanical phenomena are insufficient to explain what's going on. It's
3701120	3706480	not an obvious connection. So people become dualist. There are somehow two completely separate
3706480	3711920	domains. And I think in a way this dualism is correct, but not in the sense that the mental
3711920	3720560	states are ontologically existing. They exist as if. There is no organism. There is only this
3720560	3726240	connection of cells and this collection of cells is acting in a coherent way, which means we can
3726240	3730960	compress it. We can model it using a very low-dimensional, much circular function
3730960	3735600	than look at all the cells in general. And the organism is only approximating this function,
3735600	3740720	but what makes the organism more powerful than a collection of cells is exactly that function,
3740720	3746080	this structure that we project into it. And the interesting thing is that by the information
3746080	3750640	processing within the organism, the organism can discover that function by itself and use it to
3750640	3756880	drive its own behavior. So while the organism is not a person, it's not even an organism,
3757520	3763040	it is very useful for the organism to behave as if it was an organism and also to have an idea
3763040	3766800	of what it would be like to be a person that interacts, for instance, with the social world.
3767520	3772160	So it creates a simulation of that. And it's often not even a simulation, it's often just a
3772160	3777760	simulacrum. That's what makes it a causal. The difference between a simulation and the reality
3777760	3784800	is that the simulation is modeling some aspects of the dynamics of a domain on a different causal
3784800	3790160	substrate, on a different causal footing. So you have a computer game in which you can shoot a gun,
3790160	3796640	but there is no proper physics in the game that would recreate what's happened in the real world
3796640	3801520	when you shoot a gun. Instead, it is using a different causal structure of your software
3801520	3806080	program to give you something that gives you good enough dynamics so you can interact with the world
3806080	3810240	and experience these causal structures. You can make a different decision, you make a different
3810240	3815120	move in the game, and as a result, the game behaves as if you would expect it because it's
3815120	3819440	imitating the same causal structure using this different substrate. In the simulacrum,
3819520	3823280	you don't have the causal structure. Like a movie doesn't have causal structure. It only
3823280	3828960	gives you a sequence of observables. And our own mental model of ourselves is a mixture of
3828960	3835360	simulation and simulacrum. So we sometimes create a sequence without causal structure. It looks like
3835360	3839920	it does this thing magically. And sometimes we have a causal model, but this causal model
3839920	3844560	is not the real deal. It's just this simplified geometric simulation of how the world works.
3844560	3849040	It's a game engine that our brain is producing to anticipate what happens in the physical world.
3849920	3856640	Yeah, so that very strongly resonates with me. And another person I very much respect
3856640	3862720	whose opinion resonates with me regarding consciousness is Carl Friston. And I'm not
3862720	3867920	sure how much you're familiar with his free energy principle and his thoughts on consciousness,
3867920	3873040	but I'd like to put forward to you one of his more recent definitions, if you will,
3873040	3877680	or proposals to explain consciousness. And I get your opinion on it. This is from his
3878320	3886560	2018 article titled, am I self conscious or does self organization entail self consciousness?
3886560	3892480	And what he says here is the proposal on offer here is that the mind comes into being when
3892480	3899920	self evidencing has a temporal thickness or counterfactual depth, which grounds inferences
3899920	3906800	about the consequences of my action. On this view, consciousness is nothing more than inference
3906800	3914080	about my future, namely the self evidencing consequences of what I could do. Does that align
3914720	3916800	pretty closely with your your view?
3917680	3923920	No, I don't think it's sufficient and also don't think it's necessary. I like Friston's idea,
3923920	3927920	but most of the free energy principle comes down to predictive coding,
3927920	3935440	which is in some sense, radically tested with GPT-3. GPT-3 is trained in some sense entirely
3935440	3940320	on predictive coding. It's only trying to predict the future from the past. And the future is the
3940320	3946000	next token based on the tokens that it has seen so far. And GPT-3 radically tries how far you can
3946000	3952080	go with this, and you can go very far. But you need far far more samples than an organism does.
3952960	3958000	So there are players in us that go beyond predictive coding, maybe we converge towards
3958000	3963280	this over many generations in the evolutionary process. So I don't think it's a stupid idea
3963280	3970320	that Carl Friston proposes, but we are born with additional loss functions that let us
3970320	3975680	converge much, much faster on something that is useful to the organism. And if we think about
3975680	3982160	consciousness, he has a point about agency in there. Agency means that you have a controller
3982160	3987600	that is able to control the future. Took me a while to understand this, but when I go up,
3987600	3991280	we talked about BDI agents, and they seem to be quite complicated and convoluted,
3991600	3996160	put a lot of quote there to make a BDI agent, but there's beliefs, desires, and intentions,
3996160	4002720	and so on. But if we think about what actually is a minimal agent, a thermostat is not a minimal
4002720	4006720	agent. So that has enough agency, it doesn't want anything. It just acts on the present
4006720	4012240	frame by doing the obvious thing. But imagine that you give the thermostat the ability to
4012240	4017680	integrate the expected temperature differences, the differences over the future, when it does x now
4017680	4023120	or y now or does it a moment later, right? So suddenly you have a branching reality. And in
4023120	4028000	this branching reality, you can make decisions and you will have preferences based on this
4028000	4033680	integrated expected reward. So just by giving the thermostat the ability to model the future,
4033680	4039120	you turn it into an agent. This is sufficient. And if you make this model deeper and deeper,
4039120	4043520	it's going to get better and better at it. And at a certain depth, the thermostat is going to
4043600	4049040	discover itself. It's going to discover the idiosyncrasies of its sensors. And notice that
4049040	4053360	the sensor operates differently when it's closer to the heating element and so on and so on, right?
4053360	4058480	So it becomes aware of how it functions. It might even become aware of the way in which its modeling
4058480	4066000	and reasoning process works and to improve it or to account for its inefficiencies in certain ways.
4066000	4072640	And this is also what we do with our own cell. But this model of the self is not identical to our
4072640	4078480	consciousness. Our consciousness is a feeling of what it's like in the moment. It's the experience
4078480	4083920	of a now. There is an experience of a perspective that we are having, right? And this is what's
4083920	4088480	absent in the description of first. And he is missing the core point of what it means for
4088480	4092240	something to be conscious. It doesn't mean that it has a self. It doesn't even just mean it has a
4092240	4099040	first person perspective. It means that it experiences a reality. And this is not described
4099120	4106160	in this Friston quote. We explicitly asked him this question actually when we talked to him
4106160	4114480	last time. And his response there was that this concept of feel like is really something that
4114480	4121120	would need to be coded into the generative model that this agent has about the world. Number one,
4121120	4125040	it has to have a generative model as we've just been discussing. It has to be able to entertain
4125040	4130560	counterfactual, you know, possibilities for the predictive coding, right? And he's saying that
4130560	4136960	these feel like concepts would literally be encoded in that generative model as hypotheses
4136960	4143680	that we recognize, you know, so things like I'm feeling pain, for example, would be a concept
4143680	4148800	within that model. And he says there's actually evidence from, you know, treating patients with
4148800	4153600	chronic pain and this sort of thing that that's actually exactly what's happening in the mind
4154160	4160080	that that the feeling of pain is actually a concept that's built in as a slot, if you will,
4160080	4165920	into this generative model. I mean, what do you think about that proposal?
4167520	4174720	The semantics of pain are given by the avoidance that you don't want to experience the pain usually.
4175600	4180160	And it could be that you cultivate the pain and use it to make something happening on the
4180240	4184720	next level, but it requires that you are then building a multi-level control structure,
4184720	4191680	if you want to use pain productively, some artists are maybe doing. But the, you cannot have pain,
4191680	4196080	I think, without an action tendency, without something that modulates what you are doing.
4196960	4202720	So your, your cognition is embedded into this engine. And to build such an engine that does it,
4202720	4208480	that causally changes how you operate is not that hard. But when you live inside of such an engine,
4208480	4213200	it feels very strange that there is something that is happening that somehow depends on what
4213200	4217840	you are thinking, but you cannot control it. It controls you. It's upstream from you. You are
4217840	4223920	downstream from it. And when you get upstream of your own pain, the pain stops being pain. It's
4223920	4229520	something that is a representation that you can now control and be able to get there. But it's
4229520	4234560	not easy and you're not meant to get there because it means that we can immunize ourselves to pain
4234560	4239840	and sacrifice the organism to our intellectual interests. What's crucial about feelings when
4239840	4244400	you look at them introspectively is that feelings are essentially geometric. I don't know if you
4244400	4250320	noticed that. So for instance, we notice feelings typically in our body. And that's because I think
4250320	4255680	that the feelings play out in a space. And the only space that we have always instantiated in our
4255680	4261040	mind is the body map. So they're being projected into the space to make them distinct. And when we
4261040	4267040	look at the semantics of the feeling, we noticed that they are contracting or expanding or they
4267040	4272080	are light or they're heavy and so on. This is all movement of stuff in space. It's all geometry
4272720	4276720	plus valence, the stuff that is going to push your behaviors in a certain direction.
4277280	4284160	So these are basically the interactions of some deep learning system that is producing
4284160	4291040	continuous geometric representations as perceived from an analytic engine. It's an interface
4291040	4296880	between two parts of your mind, between the analytic attention control that is reflecting
4296880	4301520	on the operations that your mind is doing while it's optimizing its attention. And the underlying
4301520	4306720	system that represents the state of the organism entails where you should be going and makes this
4306720	4313120	visible to you. It is a system that is not able to speak to you, uses geometry. And these
4314160	4319120	geometrical features, this is what we call feelings. So that's a very interesting connection.
4319120	4325520	And I think Jeff Hawkins of Nemento would be quite interested in that as well, because
4326960	4334640	some of what he discussed with us was that, in his view, the evolution of, let's say,
4334640	4340400	abstract thinking and whatnot actually came from systems that evolved to operate in just
4340400	4347840	simple three-dimensional kind of motion, and that eventually those were reutilized by the
4347840	4354240	evolutionary process to start engaging in abstract thinking, which he views as movement
4354240	4358000	through an abstract space. And so I think there's a lot of connection here to what you're saying
4358000	4363920	about feeling, which is that, again, in a sense, our mind has reutilized this
4364640	4371120	three, three plus one d movement mapping capability that it needed in order to survive in a three
4371120	4378240	plus one d environment, physical environment, and it's reutilized those for mapping feelings,
4378240	4384640	it's reutilized them for mapping to abstract thinking is like a form of motion in an abstract
4384640	4391760	space. Is that a fair connection? Yes, but I don't think that it's because it's
4391760	4397760	borrowed from the world in which we interact, but because it's the only game in town, it's the
4397760	4404480	only mathematics that can deal with multi-dimensional numbers. So when we talk about spaces, we actually
4404480	4409040	talk about multi-dimensional numbers, about things that are not just a scalar in a single
4409040	4414720	dimension, but features that are related. And sometimes you can take these features that you
4414720	4418960	measure continuously because they have too many steps to meaningfully discretize them.
4419760	4426160	So what it does is you discover that you can rotate something. And this is when you get a
4426160	4431200	space in the sense as we have a space to which we are moving. And these spaces which you can
4431200	4439760	rotate things only exist in 2D and 4D and 8D. And so the geometry that we're talking about is
4439760	4445120	constrained to certain mathematical paradigms, which you can derive from number theory,
4445120	4452080	compressed principles. And our brain is discovering a useful set of functions to model
4452080	4458000	anything, a set of useful computational primitives. And we can probably give our deep learning systems
4458000	4462880	a library of predefined primitives to speed up their convergence. That's also the reason why
4462880	4468000	there is useful transfer learning between different domains. You can train a vision model
4469760	4474800	and use it as a pre-training for audio. And it's not because it's the same thing, but because it
4474800	4480480	has learned useful computational primitives that it can apply across domains. But there is geometry
4480480	4487760	in the audio signal. So this is very interesting territory. I hope you'll come back to dive into
4487760	4492640	this a bit more deeply when we have more time and a better connection, because I agree with you.
4493920	4496160	Some very fascinating math here.
4496320	4506160	Fantastic. Well, Dr. Yoshua Bak, it's, as I said, you've by far the most requested guest
4507040	4511040	that we've had right from the very beginning. So it's an honor to finally get you on the show.
4511040	4514480	And I hope we can get you back soon for a longer conversation. Thank you so much.
4515120	4518400	Likewise. I enjoyed this very much. Let's meet again soon.

Um, Sarah, it's amazing to have you back on MLST.
It's so lovely to be here. It's been a year and a half or something since our last conversation.
Yes, it has. Yeah, because I think, um, we met at NeurIPS and then, and then I came in
filming to be in the London office, which is really good. Um, but fans of the show, of course,
will know that our first interview was about your hardware lottery paper.
Yeah.
And that was your first grumpy essay.
That was a very grumpy essay. You know, I lead career for AI, so it's a research
lab. We do a lot of fundamental research, uh, and we, a lot of my work is on efficiency, um,
reliability, and building these models that scale the next generation models.
So you can go to Co-Here for AI and take a look at some of our work.
Sarah Hooker is VP of research at Co-Here, and she leads Co-Here for AI, a research lab,
which seeks to solve complex machine learning problems. Co-Here for AI supports fundamental
research, which explores the unknown. She leads a team of researchers and engineers working on
making large language models more efficient, safe, and grounded. In this conversation, Sarah
discusses her recent work on multilingual AI and the challenges of developing language models,
which work across many different languages. She provides insights into the limitations
of current approaches like RLHF, especially for low resource languages. Sarah also talks about
her recent paper critiquing the use of compute thresholds as an AI governance strategy,
explaining why simple measures like flops are inadequate for assessing AI capabilities and
risks. Sarah emphasizes the importance of understanding the relationship between compute,
data, and model architectures. She advocates for a more nuanced approach to AI development
and governance, which considers the complexities of language, culture, and the representational
long tail, where all the low frequency data lives, which is so often neglected in current models.
Sarah's work aims to make AI more globally representative and equitable, as these technologies
become increasingly integrated into society. Enjoy the show.
Your most recent grumpy paper is called On the limitations of compute thresholds as a
governance strategy. Can you give us the elevator pitch?
So this paper is, it has a very boring title. And at face value, it's just about this kind of
odd, known to not many people in the public, compute thresholds that have actually been
widely adopted. They were adopted by the executive order on AI, they were adopted by the EU AI Act.
And what's fascinating is that these are kind of the key policies that have come out on AI.
Why did I write a paper about this very, very deep topic of compute thresholds?
Because it's at the heart of really what our field is asking right now, which is that compute
thresholds are based on an idea that models at a future size, so it doesn't apply to models in the
well now, are going to trigger some difference in risk profile that deserves scrutiny. And this
question of, does scale trigger this moment where models have these properties that are
fundamentally different from models before that? It is actually very much being at the core of our
field for the last two decades. Because in the last two decades, we've had this philosophy of
bigger is better, we scale data and we scale model size. So this essay is really about,
is that true? As we look and stand and look at the last decade, what do we know about the relationship
between compute and risk? And what do we think is the feasibility of these compute thresholds
actually mitigating risk? And that was the starting point.
Yeah, so in the beginning, you were talking about how historically we have tried to estimate and
control and respond to risk. Can you give us a couple of examples?
Mostly as a society, we have tried to grapple with this idea that we want to proactively control
our future for the better. And this is actually recent as well. So it's very typical of modern
society that we have this notion of planning and anticipating risks and being able to mitigate.
There's examples where me and you do this every day, right? We could put on sunscreen if we're
knowing we're going to the sun. We avoid working in dark areas. There's also areas where governments
have done this, you know, even in this modern era of the last 300, 400 years. And it requires two
things to do well. One is that you have to understand where risk comes from. So you have to
understand what is the kind of lever of risk. A good example of where that's failed is something
like the Black Death, where for example, a lot of the protocols around the time didn't realize
that rats were the main vector of the disease. And so because of that, many of the mitigation
techniques were unsuccessful. But the second crucial aspect is that once you've identified the lever
of risk, you have to form a proportionate response. And we also have examples where that's failed
historically. So for example, the London fire is a great example where it was known that this was a
risk, but the fail to curb it early on in the expansion of the fire led to the destruction of
a large part of London. So these are the two challenges that policymakers face. And what
compounds it for something like technology is that typically, the idea of identifying the
lever of risk is very difficult, because most technology breakthroughs, by the nature of
being a breakthrough, you're in a kind of rather than a proactive setting, you're in a retroactive
setting. What do we do now that this is changing the world? And that's a very difficult position
for someone to form a response to. Yes, exactly. I mean, you know, one of the
themes of the paper is we're super bad at predicting the future. And maybe we should just
linger just, you know, just for a second on the the executive order and the EU AI Act. Now,
they used this notion called flops. And please explain what flops are in a second. But the
in America, they set the limit to I think 10 to the 26. Is that right? And then in the EU,
it was they wanted to be a little bit more strict. So they just went down to 25.
Yeah. Tell me about that. So flops, by the way, is this measure by which this compute threshold
is done? I think flops is just a it's a way of counting. So typically, when you train a model,
you're doing many different operations, you're doing additions, subtraction, multiplication,
famously matrix multiplies, dominate our modern networks. And so that can be decomposed into
all these operations. So flops is just a tally, it just counts it up. And these thresholds,
10 to the 26 and 10 to the 25 are this idea that at that moment, that's when you kick in scrutiny.
And it's important to realize that doesn't apply to models in the wild right now.
For the executive order, for the EU AI Act, when it comes into effect next year,
it might hit a handful of models. But this is a for looking policy. It is not based on current
risk in the wild. And so that's interesting to think about, because that creates the question of,
well, are we good at predicting what risks emerge? And is that the right number to do it at? And
that's where it starts to get very interesting. Yeah, yes. So they have a tally, they kind of
estimate how much computation is happening in the models. And then they've set this threshold. So
they don't care about anything below that number. So there's lots of real risk now that they
presumably don't care about. And then they're saying above this number, there's a problem. And I
think did they set the number roughly commensurate to the size of a GPT-4 model?
So it's difficult because they haven't formally justified why they set the number there. But
anecdotally, my understanding is that guided it. So it's interesting. And it's worth thinking about,
well, it's this interesting aspect of, well, firstly, there's a notion of, is this number
valid tally of risk? Like, is training compute the number that you care about if you wanted to
do this tally, if you believed in this future risk? But secondly, if are we good at predicting
like that number? And that's kind of interesting to think about. Yeah, I mean, to me, it was
a bit crazy on its face. And what's going through my mind is, have they got anyone working in the
government that actually know what they're talking about? Because presumably, if they asked you,
you would have thrown this thing out straight away. And you gave some examples, actually. So you
said, there are things that have a normal distribution, like the weight of babies when
they're born or blood pressure or certain things like that. And then there are other things that
are significantly more complex, like if I'm buying a house, what does the estate agent do? Well,
they have a complex model where they look at the neighborhood, they look at various different factors.
Some people have indexes and they have things that can shift over time. So having this one
absolutist number just seems a bit ridiculous. I actually think I feel for policymakers because
I think it will put pressure on them to continually adapt the number. So I will say there were benefits
in the thinking of this number. I think it's unfortunate it got so far without scientific
input. But one reason that people like Flops is, for example, it's hardware agnostic, you can measure
the same way across different types of hardware. And also, it's fairly easy to measure because all
it's doing is a tally of operations. So it also avoids specifying maybe what risk you care about.
So it gives a degree of, I would say, flexibility there for governments to adapt over time. I would
say that is probably one of the larger shortcomings is that by not specifying, you can end up with
something which is evading your Flops threshold but a highly risky model. So I think that's actually
one of the crucial shortcomings. But I do understand that the motivation of a lot of policymakers are
what else? Like what else could I use? I would argue if you're going to stick with this measure
and it has been formalized in several policies, you have to understand that this can be manipulated
as a measure. And there's many ways, and I list some out in the paper, but to your point,
a single number puts a lot of pressure on policymakers to constantly adjust this and have
the technical information to adjust it because this is a rapidly changing distribution. The notion of
compute has been highly unstable. Just looking at the last decade, we know this. And so it will quickly
have an expiration date. And I would argue what you're saying is excellent as an example.
One is that you need a reference class of what are you comparing against? So you mentioned the
kind of real estate agent who compares the pool of houses. Each of these domains, like biology models,
which are very interesting to certain researchers because of bio risk, language models, multimodal
models, they have different distributions of compute requirements. And so it has to be done
relative to your reference class, but also it should be done dynamically. The same way that a
real estate agent does it based on a percentile of surrounding houses, the notion of a single
inflection point for risk is not a viable policy tool because you just are changing things all the
time. Yes. I mean, there are so many things to get into here because you went through a wonderful
list of examples in your paper. But one of the elephants in the room is that it supposes that
there is some kind of linear commensurate relationship between compute and capabilities.
And of course, you're working in multilingual. I mean, it actually penalizes you because
you need to do more compute just to have a model that works at all in many different languages.
And this thing just isn't working for you. Yeah. I mean, what you're pointing out is that once you
do something like multilingual, you're basically trying to learn a new distribution each time
that's as vast as English. And so you typically need a lot. It's called the curse of multilinguality.
And so you need more compute. There's other things there which are very tricky is that
how do you flops and how does training compute account for the vast amount of
change and how we optimize after training? So we talked about RLHF. There's also instruction
fine-tuning. There's also things like synthetic data distillation which shortens training times.
So these are all what we call inference time optimization. So you spend time after training.
You pay for it in compute. Like you can do best event sampling, which is what you refer to with
the Francois Chollet where you sample a lot of completions and you choose the best. That all has
very pronounced benefits for models. So typically your model performance alone, just using a subset
of these techniques is two to six times more powerful. And that's not reflected in flops.
Yes. Because I'm really interested in when you look at the model life cycle or the predictive
life cycle, there are so many places where you can spend computation. So you can do dataset
generation and you can do, obviously there's the training of the model and then you can do
like inference time optimization and active inference and a whole bunch of stuff like that.
And they are only taking into account the model training. But then there's the further issue
of training provenance. So for example, I can download a model from Huggingface and I can fine
tune it and do a bunch of stuff on it. And like, how do you know, right? It's just an inscrutable
bunch of weights. Like you have no idea how much training has gone into it. This is the idea of
tracing flops across the life cycle. I think this is also going to be formidable because
increasingly the most popular models on Huggingface, by the way, are models which haven't been
instruction fine-tuned. They're base models. And why? Because people want to do continued
pre-training. They want to overlay their own optimization techniques, which suggests that
people are using this as one step in their optimization process. It's going to be a formidable
challenge to tally it in a reasonable way, especially when sometimes the way that we
measure these, think about something like mixture of experts or a classic ensemble.
What counts then? Because you may have many different experts in your mixture of experts,
but you're only using two at the end. Classic ensembling is even more nuanced because technically
you didn't even optimize all the models together. You just show up and you ensemble them at the end
and you get one model at the end. So how do you handle that? It's very interesting and it's very
related to this challenge of people are likely taking some level of compute already and they're
doing some changes at the end of training that make it more performant. Yeah. And then there's
this matter of Good Heart's Law, which is that when a target becomes a measure, it ceases to be
a good measure. And there are so many examples of this. For example, the banks have these arbitrary
limits on the amount of money that you can send, which is why it's set at, let's say, $10,000 and
then you see loads and loads of transactions at like $9999 because they know what the limit is.
And it must be the same here, right? It's going to gamify the system. People are going to evade
it in so many ways. I think that the main advice I have about this is that if policy makers have
decided on this and they're going to go for it, they need to complement it with an auxiliary
measure of the actual risk that they care about. It has to be an index because if you just stick
with compute, it is too easy to evade because there's too many different things you can do post
training to gain percentage points. And there's too many ways of essentially shortening your
training time or reducing your flops while still arriving at a highly performant model.
And so that's the other key recommendation I have is that you need something that is
anchored to the downstream risk you actually care about. And compute is not. It just reflects
our belief that more compute is better. And that simply is too simplistic of you to account for
all the ways in which smaller models, if they're very targeted, can be extremely risky.
Yes. So I think maybe we should bring Rich Sutton in. So he wrote this essay called The
Bitter Lesson. And I'll let you bring it in. But he was partially responsible for this idea that
compute is all you need. Yeah, bring that in. Yeah. And by the way, I think that's a fantastic
essay. It really is this idea that history tells us in computer science in particular that all
efforts to codify our expertise, to work on very fancy ways of imparting what we think is the right
way to learn to a model have been particularly futile. Like he's really saying we're not very good
as computer scientists. And the biggest ingredient of success that's driven things is being adding
compute to the mix. And that we can do things that are algorithmic, but it has to play well
with compute. So anyway, he is kind of get this idea of hardware. But it's more general. It's this
idea that it's not specific to a different sort of type of hardware. It's just compute. If you
play well with compute, if it scales well, it's going to be the winning variant. And yeah, go for
it. Well, I mean, I just I have an intuition that he's he's right and wrong at the same time. So
I mean, in terms of system one models that just memorize things better and better, he's kind
of right. Because there is a commensurate relationship, you know, as we memorize more of
the long tail, the models get better and better. But I still think that there might be a fundamental
break between compute and capabilities when and ironically, a regulation like this might
incentivize to find such a break. So you know, we might design system two models that actually do
reasoning and have a, you know, Neurosymbolic architecture or whatever. And now all of a
sudden we've got like really good capabilities with less compute. Yeah, I mean, you're hitting on
the head, I don't disagree with you. I think where I agree with Rich said it is that for
given architecture, say, transformers, you can throw more compute at it up until a certain
point where it's saturated, but you're going to see all the things equal, your data sets equal,
compute is better because these are greedy learners, they, they're, you know, our deep
neural networks are frequency counters, you're going to see gains on the long tail performance
and overall gains. Where it misses the point is that really there's a few things going on.
One is that because our current representations are so inefficient, there's ways to
really change the algorithm itself and bend the, the rule of compute. So, and the rate at which
compute is needed to unlock gains. And deep neural networks in particular are a great example of
this because they're so painfully inefficient because we have to show all the data the same
amount of times because we have to do these global updates. And so we're seeing all these tricks.
For example, now we care about data again, and we care about data quality. So we condition that
space better to represent what we want to model downstream. That means we have to train far less
because all the features in the data set, the ones we want to learn was you just train on the
internet. There's a lot you don't want to learn. So you have to kind of unlearn it afterwards and
spend a lot of compute just trying to find what you want within that. So that's where you bend
the rule and it becomes more nuanced, which is that the other thing that that misses is that,
or at least that wasn't a core part of this essay, I actually think Rich might agree with
me on this, is that your rate of compute is really determined more than anything by the
prior of your algorithm. So yes, if your algorithm plays well with compute, if it's scalable,
compute unlocks a lot. But the rate and the saturation point is determined by the algorithm.
And what do we mean by this? Convolutional neural networks are a great example. Introduced in 2012,
really unlocked scalability. Why? Because convolutional filters and patches made it
possible to model high dimensional images at the time. Why? Because you move your patch over your
image. This takes advantage of local relationships. You can really reduce your dimensionality. Max
max pooling layers, which Jeffrey Hinton is famously grumpy about, and rightly so,
just throws away everything except for the max. You reduce the amount of features,
you unlock the ability to model images, you have scalability up to your point. This is what we
famously know about image models. Now there's been a saturation point. Everyone who switched
to transformers because there's a new arc. So what I mean by this is your algorithm is kind of your
most heavy prior on your search space. And it's Richard's right that what plays well with compute
is the one we're going to default to. But the question becomes your scaling laws and your ability
to predict the future are essentially limited to algorithm and compute. And that's what's
interesting is that it means we're not very good at predicting the future because it means we're
too locked in to this narrow arc of this architecture we use combined with compute.
Yeah. I mean, even the CNN example is, I think an example that proves Richard wrong,
because he said in the bitter lesson that any attempt to impute hand-designed priors like
symmetry or CNN is a symmetry. So in the CNN, it encodes a symmetry and scale invariance.
And essentially, all it's doing is a shortcut because it's still in MLP at the end. So what
it's doing is it's basically it's building an MLP as if you didn't have the scale and symmetry in
there. But it's just kind of like doing this thing and it's like, you know, it's embedding it
all into the MLP. But it's basically still an MLP with a symmetry shortcut, which was hand-designed.
So yeah, yeah, but there's still this notion though that there's, you know, connectionists think
that, I mean, like Neil Nanda said to me, he's like a rationalist guide from DeepMind and now,
yeah, DeepMind. And he said, these things are just smarter than you, man.
How did that make you feel? Well, I mean, not great, not great, but you know, there's this whole
like mech-interp thing. And I think that they genuinely believe that there's just some deep
form of inscrutable intelligence going on. There was that like monosomanticity paper from
Anthropic recently. And you know, there's this deep belief that there's something
really interesting going on. What do you think? I think that it is, there are persuasive,
here's what I will say. Language is very powerful, which is why we connect and with this technology
so much, because language is how we connect with each other emotionally. It's very tied to how we,
as humans, are quite tribal. And so whenever a model learns a distribution that is indistinguishable
from humans, I think that it gives pause. And it is, I do think that these conversations are
useful because it gives worthy pause to how this technology is used. And for example, I'm very
against many of the efforts to sometimes deploy these algorithms without notifying humans that
it's an algorithm. I think that you should always be aware when you're talking to an algorithm.
And that's because these are quite convincing sometimes. And so it's very important that we
always communicate what is the role of the model and the human. Do I think that there's a higher
reasoning here? I don't. I think that in many ways, whenever you have persuasive interpolation of a
space between ideas, it's going to be surprising to us. I think what delights us is the creativity
and the surprise and element. But is this ability to reason? I don't think so. I think that there
is a clear relationship with the type of architecture we have of a memorization
relationship. And we know this. We know that when we increase scale, we learn a given architecture.
Or when we do different tricks to compensate for scale, so we can go smaller and still learn
things, what we're really doing is we're just trying to induce good memorization
and good steering towards part of the distribution we care about. Frankly, while
why all these optimization tricks have worked beyond compute to reduce compute has been
we're largely training on a distribution we don't want at the end of the day. So we start by
training on the internet. And we actually don't want the internet when we engage with these models.
We want something that's very chatty and philosophical and wise. And so a lot of what
we're doing is we're trying to steer things towards the part the tiny sliver of the distribution
that training data that we care about. And that's why we have so many optimization tricks
before we get to the end. But that's fascinating because what's that that's really telling you is
that unlike a traditional machine learning problem where you're training to the data set is the
distribution you want to learn, a lot of what we're doing with language is we're unlearning.
We're just trying to steer and unlearning nor and then focus on what we want. So it's really
interesting. Yeah, machine unlearning. That's fascinating. I'm also a big fan of the
externalist tradition in cognitive science, you know, like for recognition. And in that sense,
I think it doesn't make sense to draw a boundary around the model, because I think, you know,
our sense making semantics, situated knowledge and so on, it's kind of observe a relative anyway,
you know, these things are embedded in our culture. And sense making humans put prompts in there and
they interpret the outputs. And actually, even the data generating process that went into building
these things was, you know, originated from the universe, we're all agents and we're all in the
physical world and the social world. And we, you know, we're doing the effect of computation,
both in how the models are built and how they are used and interpreted and evaluated and so on.
So what are you going to do? Like draw a big, should you include the flops of the universe as
well? Oh, that's fascinating. Yeah, there is this interesting, yeah, it does spark something else with
flops, which is that, so typically the final model that you deliver is only one of the possible
models, right? So in fact, typically, even at massive scale, you train many candidate models,
and then you choose the best one. And so it's interesting because these are not optimized
together, but they implicitly optimized through the selection process. And it is really interesting
because we kind of steer towards what we want. So yeah, it's a fascinating dynamic. I didn't think
of it like that, though, that's an even bigger meta approach for thinking about this. Hardware
Lottery Paper, which we talked about, and that was a really fun conversation because I remember it
was when you were doing the trio kind of the group of ML Street talk, the earlier version. And
I think you originally invited me onto the show because there was this idea that I wrote about,
which is that most of computer science history has been driven by whether your idea works with
available hardware or not. And I think that resonated with a lot of people at the time,
because what it's really saying is that we may be in another hardware lottery right now,
that something like Transformers, which we all use, has become increasingly locked in to GPUs
and to TPUs, which have all been built to accelerate this one hardware. So it raises the
question of what next and how do we make sure that the next brilliant idea isn't stuck in
purgatory for decades, because that's what happened to deep neural networks. It simply
didn't work until TPUs were converted from video game use, which was really not the intended
purpose of how they were converted to work for machine learning workloads. And that happened
over the course of a decade. It was a very slow conversion process, but that turned out to be
the key for deep neural networks. What we now identify as 2012, the moment that this explosion
of interest and funding and acceleration happened. People identify that with convolutional neural
networks or the algorithm, but really it was both. It was the hardware making the algorithm
feasible. And that's when you first had the empirical proof that deep neural networks were viable.
To what extent do you think there is an algorithm lottery as well?
Oh, what do you mean by that? Well, as in now, your paper was about the basin of attraction of
hardware. But is there a basin of attraction of algorithms as well? Absolutely. I mean, you just
have to look at optimizers to see that. So what I mean by that is an algorithm is really how you
learn from data. This is the essence of an algorithm. And what we've been locked into is this idea that
it has to be gradient based optimization. It's really hard to do something that's a non-differentiable
objective. And what that means kind of in accessible terms is that we're stuck doing these
global updates. So the way our models train is that we kind of send through shovel through data,
and then the update to the weights is based on an average of all the data that's seen.
Why is that tricky for a few reasons? Because why does it mean that we overfit to the average?
And that's why we need so much training data. Because essentially, if you're just overfitting
to the average, it takes ages to learn the rare patterns. So you train for longer, you need more
data. But the other thing that's very tricky is that it means that models forget. So every time
you shovel in new data, the model forgets the old data because you're updating everything at once.
A nice point of contrast is that as humans, we typically have long-term memory and short-term
memory. These are different ways of learning, and the rate of learning is different. And so when
you process information, some are stored in your long-term memory. You may have a distinctive memory
from a child that you think is like your first memory from a child. And it may be mutated over
time. That's the nature of memory. But this ability to preserve two states of what you did today,
what you did years ago, that's very different from gradient updates. And somehow, because we
haven't found an alternative way, even though a lot of people have worked on it, we are in this
algorithm based on where it's very tricky to propose an algorithm that doesn't rely on a
differentiable objective. Yes. And I think we'll talk about this later. But part of the problem
is people think of this paradigm as a form of general abstract pure intelligence. And the
reality is that certainly with your multilingual work, that we're dealing with just this long tail
of complexity, heterogeneous data sets. But maybe that's a good segue because you just released
this primer paper called the AI language gap. Can you tell us about that?
So it's quite fun because in some ways, what you're talking about these themes leads so nice into
the AI language gap. Really, when we have built these models, we've overfitted to what is weighted
most importantly to those who built it. And these models have been built in a few places.
We're in London. London is a very big hub of where researchers have been. So is the US and
Europe and China. And because some of the first impressive large language models were built in
the US and the UK with DeepMind, and in the US with places like Coher and places like OpenAI,
I think that that has necessarily reflected the nature of the researchers who built them.
They wanted to work in English. The tricky thing is that when you try and make AI actually work
for the world, you're talking about this vast array of different languages. So there's 7,000
languages in the world. 80% of those have no text data. So it's truly not even a language problem.
It's also a multimodal problem. The second part is that even with the top 101 languages, no models
except for Io101 currently cover it. So there's this vast amount of the world that simply isn't
reflected in the way that AI works and who AI serves. The primer about the language gap is
really calling attention to this. But at the root of this problem and what you're getting at with
this theme of how does models work with the long tail is that the fundamental issue is
our models really overfit to high frequency patterns. And so the key difficulty with the
language gap is that, one, these languages typically are underserved by available data on
the internet. The internet kind of reflects early patterns of adoption, not necessarily humanity
as it is. So that means that there's way more English on the internet than there is people who
speak English. So 5% of homes speak English, but 50% of the internet is in English. In contrast,
something like Yoruba, spoken by 40 million people, is really underserved. And so it's a long tail
problem. But here's the other thing, it's a pattern where the rich get richer and the poor get poorer
because we're now in a synthetic data era. So as models get much better at generating English and
Chinese in particular, these are the two high resource languages that are well served, you're
going to see more content in those two languages. And that makes it even harder if you're relying on
large data to properly represent the languages that are currently underserved. Yeah, so interesting
because we're moving away from the material world into the information world. And right now in the
material world, there is a kind of a commensurate relationship between the number of people who
speak English and the amount of data on the internet. And as you say, we're now moving to this place
where we are generating data of language and the polarization is going to increase. So you're talking
about there's this kind of North American tech based inequality, which is getting worse. And you
said that there are safety implications for this. I was interested in this word safety, we spoke
about this last night. Because when I think of AI safety, I think of X risk in Silicon Valley and
stuff like that. And I've noticed over the years, the conflation of the two communities in terms of
ethics and existential risk. And how do you feel about that? I think it's, I mean, I feel grumpy
about that. But here's the thing. So, you know, subfields are always like this. I think there's
always this notion of subfields, which are extremely, you know, actually people caring about the same
objectives, trying to distinguish themselves over time. AI safety encompasses a large array of
perspectives and expertise and people who care about different things. I think that this shift
towards talking about from response for AI to AI safety is a fascinating one,
because it's been a bit intentional from communities who want to maybe suggest that
response for AI is distinct from what they're doing. And instead saying AI safety is about
these profound risks, these like fundamental issues of our time. And response for AI is,
okay, great, you're doing that, but keep going. And so I do think there's a very interesting
thing with how we name things and how we really have precision in our conversations.
Increasingly, I think AI safety encompasses both of these, and you need more precise
language with both. And I actually think my main ask is, we need to be precise about what our
objective is with AI safety. Because it can be, it is in many ways the same goals as response for AI.
But the degree of precision when this is articulated is a sign of accountability for
the objective. And I think sometimes the use of that word lacks accountability.
Yes, exactly. And when I hear some ex-risk folks talk about AI, it feels to be in the abstract.
And what I mean by that is they are just thinking about, if we scale this technology up,
it learns these abstract representations, which work in any situation, and it's just a matter of
scale. And it feels unmoored from the research, because when I read your work about multilingual
models, you're clearly pointing out that when we have what they call low resource languages,
the models don't work very well. They're not learning these abstractions that just
automatically work in other languages. There's a specificity to it. That seems to be the difference
to me. Yeah, there's this big question right now, what you're getting at is there's this idea of,
there's a mystique that some people are attributing to scale. It's been called different
things. It's this question of, are there emergent properties? Are there properties that appear from
nowhere that we unlock with scale? By the way, multilingual is originally proposed as one of
them. Like in the first paper about emergent properties, multilingual was there. It's like,
wow, how did this appear? We didn't even have this in our training data. But it's very interesting.
Now there's been subsequent work which is shown. It was there all along. It just wasn't documented
in the training data. So scale is just really learning your long tail. It's learning the low
frequency. We just get surprised because I think there's a big disconnect between what we think
we know about the vast amounts of data that we train on and what's actually in that mix.
And so there is often certain properties where it takes scale to unlock because it's very
relate to this question of memorization. I think how this conversation has become a bigger theme
beyond this scientific question of when do properties emerge and what to scale and lock,
it's become this thing of kind of creating a myth around these models. That there's a lack of
ability to understand what scale gives. And then that is used to kind of impart a degree of anxiety
that because we don't know precisely when this property will emerge, there should be anxiety
about this. And there should be a sense of real danger about the use of these models. And I would
say that that is actually the wrong framing for this. The right framing is that one is the notion
that we're just going to keep on scaling I think is flawed. I think there's very clear evidence that
you know bigger is not always better that we're kind of reaching the limits of how we scale with
something like transformers and it's very architecture bound. But the second thing that I
would say is it really kind of ignores the mounting evidence that these kind of properties
are surprising only because we're not going to predict in what emerges at scale.
Yes, yes. I spoke with David Chalmers recently and he bemoans the fact that whenever we have
a complex system, we say, oh, it's emergent. And there is something interesting going on
as you say that when you memorize more and more of the long tail, you do see this qualitative
increase in capabilities. And it's quite easy as an observer just to say, oh, you know, it's an
emergent property. And people ascribe things like you know, divergent intentionality and reasoning
and all of these kind of anthropomorphic qualities to the models even though they probably don't
really exist. But one interesting thing though is that, you know, when you memorize all of these
surface statistics at scale, you can use the language model as an idea generator. And like on
Francois Chalet's arc challenge, you know, Ryan Greenblatt generated about 30,000 completions for
all of the tasks. And the remarkable thing is in terms of sensitivity, the correct answer is in
those completions. And then you can do some neuro symbolic evaluation and selection and you can
pull the thing out, you know, so you can build an architecture that does really well. But I think
people underestimate the amount of human selection kind of like smoothing out the brittleness.
Yeah, well, right now, I agree, there's a huge amount of creativity that's unlocked for these
models. So this iteration, and actually, by the way, this idea of like, you can create a lot of
different options, and then you can verify which are correct, you see this in a lot of different
kind of states of progress right now, that's how code is currently done, like you can
create a really nice code data set by running code and seeing which one passed the test and kind
of do formal verification of which ones passed. So it's not that these models are not capable of
generating insensible answers is just that the probability on every single turn consistency
is what you're putting out consistency is sometimes not there. And I also think part of what is
beautiful from the creativity perspective, iteration of ideas is that sometimes you actually
don't want consistency. So the objective may be different in different settings. So for example,
for code, we always want code that passes. So that's a good example where we sample a lot just
to get the subset. But sometimes I've talked to people who use it as a way to seed ideas or
things like that. And actually there the diversity is the important part and gain very different
responses each time. And so I think over time, we'll actually have different models for different
things and be able to this is the core of the challenge of steerability of control, which right
now is not good, frankly, like why do we have prompt engineering and why does everyone love it?
Like this is a this is a symptom of a problem, not a symptom of a solution. The fact that we
spend so much time prompt engineering the perfect thing to steer. So hopefully we have better tools
in the future. But I see that as one key thing that will change is that we'll be able to steer
towards the mode we want to use. Do we want consistency? Do we want exploration? And how
does it fit into our iteration pattern? We won't spend too long on this because I asked everyone
about this. But you know, where are the sources of creativity? So as we memorize more of the
long tail, and the models can extrapolate, and the human prompters can, you know, mix novel
combinations of things together. So there's this potential extrapolative space and whatnot that's
how creative can they be? Yeah, I've been so one of the recent papers that we released was a paper
about what we call active inheritance. It's this idea that we can start to steer how we sample data
to sampling different parts of the distributions from different models. So so far the paradigm
of like sampling data, either for human or for another model, has been very much like there's
a single teacher, you're the student, or there's another student or your co creators with a single
model. But if you think about it, one, that's a kind of passive inheritance, you're just trying a
single prompt, you're not really kind of enforcing any criteria. Active inheritance is where you
sample different parts of the problem you want to solve from a variety of different models.
And that diversity actually spurs really interesting patterns where you increase
the realm of what's possible and kind of spur higher quality that transcends the quality of
any one model. And I see that as a very important step that we're building a lot of work on,
including a multilingual, but also in this fundamental area of we actually used it to
in the paper that we just released, we used it to steer towards non-differentiable objectives.
So you know, going back to what you were talking about the algorithm basin,
this idea, and I was saying everything is dependent on gradient descent, it's very hard
to steer towards non-differentiable objectives. Before deep neural networks, there was decades
of research on just these non-differentiable objectives. There are things like, how do you
compute the perplexity of like a given, like, what is the reading grade level of a given sentence?
So there's these scores that are kind of codified, but you can't really use it because they're not
differentiable. And we actually show that you can use that as part of active inheritance where
you steer towards models that are better at a reading grade level. And then you use that to
kind of form your basis of your data set. So I think that's fascinating. And I think that's
really going to spur creativity beyond just this more static notion of you just sample from a single
teacher. Yeah, that's fascinating because there's so much of your research has been on the tyranny
of forgetting the long tail or not paying attention to it. And of course, you can solve that with
better optimization and, you know, federated learning and a gentile, you know, kind of
multimodal systems that share information and query and almost like an adversary or setup.
Yeah, it's a more dynamic pool. And so it's this idea that you can actually, and actually
the long tail is a perfect example of where I find active inheritance most promising is that
because the long pool, the long tail, you typically have many weak teachers. No one's very good at
the long tail. But sampling effectively and doing this active inheritance rather than passive of
just choosing a single teacher, but choosing a variety of teachers and then comparing and optimizing,
this is fascinating. And I suspect it will benefit most the long tail.
So you said in your language gap paper that language models are going to become integral to
modern societies. How do you see that panning out? It's already happening in different ways.
Like I would call it the high low way. So we can talk about high level themes, which is there'll be
a ability to communicate much more easily. And so you'll just see much more proliferation of
things like art or people writing or kind of taking away some of the difficult parts of
how we communicate. I think the low way is just the more granular ways that you're using it right
now, which is I use it typically for very basic things throughout my day. We write a lot of papers,
so I'll do my citation reformatting using a language model. So there's both the mundane,
but there's also the profound. I think the profound is that it changes the ease of communication.
And so it changes the rate of inflammation flow. And this can be really powerful. It can mean that
we can be more creative and experiment more the space. It can also bring new risks. And so
I think this is also important to think about. Interesting. And you said that this North American
bias in language model training, you said that it affects the design, the outputs and the behavior
of the models. What did you mean by that? Well, there's two things. I mean, when I say design
outputs and the behavior of the models, I think that there's optimization bias in the models
itself against different languages. So tokenizes is a great example. So Roman scripts are things like
French, Italian. We also have a Latin based scripts. This is also English. Whenever you deviate
from Latin based scripts, you have something like Hindi, Korean, and these do not play well
with tokenizers. So there's a lot of work which shows not only do tokenizers not work very well
for these languages, but also it ends up being at this double tax because not only does the models
perform worse, it also takes more tokens to represent these languages. So it's higher latency,
higher cost for users outside of English to use APIs right now. So that's an example of like an
optimization bias. The other, frankly, the issue is that whenever you're trying to have a model
that represents many different parts of a distribution, typically our solution right now is
we've got to give it more capacity. So I1 and 1 was an interesting example of this. We released
I1 and 1. It represented 101 languages, and you can start to think about how many that is when you
try and list more than 20. So you'll probably get to 10, and then you'll start struggling. And 101
is nuts. It includes things like we had Welsh, we had Irish, but we also had Telegu, we had many
African languages, and we had very much these underrepresented like Haitian things, the variety
and the complexity as well as dialect. So 101 is probably like preparing for the space race. It's
like at the most extreme of the problem. And what's interesting is everything you learn there
trickles down to less severe settings. But one of the things that we learned there is that you
have to be very careful about how you use capacity because we had this 13 billion parameter model,
and we were stuck with it because there was no pre-training data that covered 101. So this model
was actually from 2019, which is crazy given how much has happened since then. But because of that,
we were stuck with this model, and it meant that everything we had to do was try and make the best
user capacity. We had to wait properly. We had to do data processing, data cleaning, but also we
had to do a lot of work with synthetic data and the manipulation of how we did the optimization time.
So you can do this two ways. We could have even increased it to 103 billion parameter model,
and then we would have to retrain because right now models, unless they're trained with the day
from the beginning, you can't just add it at the end. But also there's a secondary way, which is we
get much more clever about the optimization and the data creation. And so this is really the issue
is that when you go multilingual, all your problems in a given language
are kind of multiplied out. And so you have to be very careful about all the details.
I wonder what's the relationship between language and capabilities? And the reason I asked this is
there was a great book I read called The Language Game by Morton Christensen and Nick Chater.
And that very much led me to this idea of situated knowledge, I guess. So actually a lot of our
cognition and thinking is quite specific to the culture and the language that we are in.
And that seems to go against the grain of the idea that these things are learning
general patterns of reasoning across languages. So then it rather kind of leads you to this
conclusion that you actually need to be within the language and the culture in order to do the
kind of thinking that they do inside that culture. So how does that work then when you're mixing all
of these together into one language model? I think it doesn't work that well right now. So I would
say this is like one of the core problems because you're precisely right. So we actually, so there's
a few things I would say here. One is that we already see this with things like dialect. So the
notion of dialect, which isn't really a counter for any models, including Aya, I think that we all go
as mainly just to be the first next step in state of art. But even ours doesn't do this nuance of
dialect. We do have various dialects of Arabic and some other dialects, but take something like
Portuguese for example. Portuguese is spoken in many different places of the world. I spent part
of my childhood in Mozambique. The Mozambique Portuguese is very different from, you know,
I guess the most extreme would be Brazilian Portuguese. But also Portuguese in Portugal
has its own nuances. And actually when we did Aya, we had researchers all over the world who were
part of this project. And we would frequently have these little riffs between the Portuguese
contributors in Brazil and the Portuguese contributors in Portugal because they were asked
to review within a single pool. And so because the Brazilians outnumbered the Portuguese in Portugal,
they would all correct their submissions to Brazilian Portuguese. This is a very interesting
concept. And this is just on the notion of dialect. But your wider point is this idea that
language is a tool for communication. And there's actually this very interesting concept about
whether we even use language to think or if we use it as a utilitarian tool. Why is that relevant
here? Because the way that we achieve an objective is going to depend upon where we are in the world.
And the way that technology should serve us is going to depend on where we are with the world.
This has come out recently. We just released a paper which I'm quite proud of, which is thinking
about this idea of local versus global harms. At any one moment, we have multiple facets of our
identity. So there's notions of what is insensitive to us as part of a notion of being global citizens.
And that probably gets to things like there's a universal agreement that some types of harms,
like harms to world children are particularly egregious. And most of, almost universally,
our legal systems reflect this. But there's also notions of very particular harms which are
cultural and very specific to how we live. And that is reflected in things like wording. So
we just released this paper, which I think is important for safety, but also part of this
broader move and in the field, which is that most of our models right now are trained with a
single objective, a single decision boundary. What that means is all the data gets flattened
to this one decision boundary. I'm very interested in multi-objective optimization. And this changes
it so that you can hold multiple objectives at once. And that perhaps you can even adapt
these objectives on the fly, which is very interesting. Yeah, a couple of things. I mean,
you're talking, I guess, about the interplay between having a relativistic worldview and
having some global norms. And in general, the way we do model alignment with our LHF and so on,
it tends to de-complexify the reality of the world that we live in. And in ethical frameworks,
there are deontology people who think there are just guiding principles and there are
virtue ethics people who think there are certain virtues that we should emphasize. And there's
consequentialism that there are certain consequences that are bad. And as you were just
pointing to, it's very, very difficult to have a hybrid ethical framework that encapsulates
all of these things together. What kind of work are people doing and what are you thinking about
it? Well, we recently, the paper we just released is this really, this paper called,
we call the Multilingual Prism, which is this idea that for safety, we collected both local
examples of red teaming safety with really this very nuanced collection process across multiple
languages, as well as harms that were considered global. From there, you can go into something
like our LHF and you can change the notion of a single reward model. So this is an area I'm
quite interested in. Like, how do you have multiple reward models? And then how do you
balance them? This is the crux of the problem. And that's what you're getting at. So how do
you have these two things in unison? And I suspect what we're going to see there is
this notion of adaptation of our models in a more nimble way than previously. So typically,
in a production setting, you spend months doing this model, you release it, cool, thumbs up,
enjoy, and it's not as dynamic, but a true production model is refreshed and is more nimble
and is deployed in different ways to different places. Like Netflix famously does this with
its recommendation systems. I think here, this is actually a much more profound way of doing this
because you can have these models, which essentially the way that they're steered is adapted. And this
is both interesting as well as profoundly challenging because the tricky thing is,
is you want to be sensitive to how the preferences of users change around the world,
but you cannot overfit to too granular a preference because this is a philosophical
tension you're actually getting at, which is that, you know, a libertarian view would say
every person here has a list of preferences and those should be respected in their rank order.
But as a society, we typically say we have this group of preferences, but we also adhere and kind
of subsume some of our preferences for the common grid. And so there's this notion as well, when
you articulate that as an algorithm, how do you get that balance somewhere in the middle, like where
you are basically not adhering completely to a societal view. I think that's one of the concerns
about algorithms and tokenizers being used in certain states where there's a state influence
on how algorithms are deployed, but also not being used a total libertarian view where we don't want
objectives that essentially amplify how a person thinks about the world without balancing and
introducing different viewpoints. Yeah, it's so fascinating because, you know, even things like
polarization on their face seem like an incredibly bad thing, but some kind of diversity preservation
might actually lead to a pluralistic society that, you know, gains information and, you know,
like a degree of health actually that we need. But with safetyism in general though, there's
always this notion of, I think we probably agree on this a little bit, that if you leave people
to their own devices, then that can be bad, but you also need a little bit of that because otherwise
the society might become quite sclerotic. And these decisions presumably need to be baked into
the way that we build these models in some way. Yeah, and currently then not. I would say currently
the way that we approach safety, it's quite, we have this notion of refusals. So when you've
engaged with a model, you typically will see refusals for certain what I would call the more
black and white type cases. There's this really interesting opportunity I see in the evolution
of how we think about safety, which is that instead of just saying I can't answer this to kind of
provide more nuance or provide links to additional support. And I think that's very
interesting because there's a different type of discussion. But I would say your perspective,
what you're talking about, which is really this part in the middle where you have some values as
like how you build your algorithm, but also you realize that this is someone who's engaging with
an algorithm and like this is, you know, the algorithm should not reflect perfectly a single
view of the world. You need more ways I also think within the UI for the person to influence and
provide feedback and to something as course hallucinations is really interesting because
hallucinations is not, you can't, I'm very skeptical we're going to eliminate hallucinations
because they're also what we really like about these models. It's the creativity. So for me,
this is not just, we often fixate a lot on the model in these conversations. The model has to
solve this, but I think there's also a notion of the system. And I think that some things that will
be interesting to play within the system is how does the user express when they think that
steering isn't aligned with what they think is reasonable? A good example, this is for example
a question about sexual health. There's valid reasons to ask those questions. There's valid
reasons to want to understand like parts of your biology or things like that. Wikipedia has whole
pages about sexual health. And so it's very interesting that a lot of systems refuse to
answer this right now. So there's this nuance where we need to make sure that we are updating these
binary decision boundaries where it's outright refusal and move towards something which is
instead steering towards resources. Yeah, and I think so much of this is about when you fix something
as it is now, it can go both ways as well. So maybe you can explain to the model, no in this
situation I think you really should tell me. And likewise the model can say no, actually I think
the reason I'm not allowing you to do this is because of this and maybe you should shift your
viewpoint a little bit. It has to be done subtly because people don't like re-education. So that
actually creates, they say the road to hell is paved with good intentions, it creates an equal
and opposite reaction when you try to re-educate people. But coming on to RLHF, I mean we've
spoken about this for years, you've always been a bit grumpy about RLHF and I read your paper,
unfortunately I don't have an internet connection so I'm doing this from memory. Can you just remind
me the multilingual paper that you've just released where you're trying to remove translation
artifacts? Oh yes, RLHF speaks many languages. So this is a really nice paper, it was led by
John. This idea that we were the first to extend a lot of the RLHF techniques from many different
languages. So I think actually there's a wider view of RLHF and I have been grumpy about it,
but you go first. Yeah, what were you going to... Well I mean even in this paper you were saying
that, I mean obviously the broader conversation we've just had is that we need perhaps you know
some kind of a more systems approach where we have a multitude of different models and optimizers
and datasets and all that good stuff. But even within RLHF you are saying that it's hideously
complex and inefficient and you have to have this separate reward model and it can't be optimized
very well and you are saying that sometimes using DPO or even just basic reinforce is better.
Yeah, so there's an excellent paper which we also recently released back to basics where we actually
do a much more profound question of this, not even specific to multilingual, we take a step back.
And we say okay, it's really interesting. All the kind of most cited papers originally on RLHF
are papers which really take this canonical method within RL, PPO, and apply it to the
language setting. And PPO really evolved in the RL, traditional RL space to address and mitigate
a lot of the issues with traditional RL. RL is typically over a large expansive search space,
incredibly noisy, and the trickiest part right is that your errors compound. So it's almost like
thinking about well what if I bet incorrectly at a game table and then tried to bet again and
did it incorrectly and your losses just compound the more that your estimates are off. So PPO is
heavily what I would call kind of regularized or conditioned to limit the impact of an incorrect
estimate. What that means is that often it's quite memory intensive, you kind of have four
models and play at any one time, and it also means that it's quite sensitive. So typically PPO to
train, it takes longer. And showed the language setting. And so the initial adoption of PPO and
the success of it was taken at least value. This is incredible, let's go with this. But
the language space is also an enormous search space because if you think about it, you're trying to
predict the next token, how many tokens, how many possible tokens are there in the world to represent
language. But by the time you have a trained model, and by the time you've done all this pre-training,
the search space is much narrower. And actually it's quite interesting because the likelihood
and the probability of what the next token will be is actually very concentrated.
And when you have this pre-trained base, it's only going to be a few different tokens that you
would likely predict, which means that this was overkill for the setting. And what we show
convincingly and back to basics is that you can strip a lot, a lot of the components of PPO out.
You can propose something like RLU, which is still an RL method, but that works effectively and even
surpasses it. And RLU is also what we used in the RLHF speaks many languages. And we showed that
this is very impactful. And because it's online, it does beat things like DPO, which is offline.
So RLU is still an RL method. But what it's really saying is that we are in a well-conditioned
search space. And because of that, we can be a lot more nimble about how we explore it.
Yeah. Well, in the RLHF on many languages one, because obviously you've had this huge focus on
multilingual. And I suppose there's the problem of getting diverse data, because this is super
heterogeneous data when we're doing multilingual language training. And of course, even the
preference completions, they needed to be generated as well. And I think you generated some with
translations and then you had a strong model and you had a setup there. Can you tell us about that?
That's fun, because it's part of this wider issue where multilingual relies a lot traditionally on
translations. You don't have data, so you translate your good English data, your gold standard data,
or your good Mandarin Chinese data into many different languages. Here is where it gets
interesting. Translation models typically have what we call translation ease. There's these weird
artifacts that pop up. So you might have like, odd enumeration where instead of like the one,
two, three, it spells out one, two, three. So it's just, and it's very annoying for people who
have to experience it because it gets imparted to the model, the downstream model. So we did
something which I think is very fun with this paper where we said, well, the whole goal of RLHF
is to steer away from certain parts of the distribution, steer towards other parts. And so
what we did for our preference pairs, so let's think about the normal way preference pairs are
done. It's quite expensive and time intensive. You have to go get annotators and you're asking
humans which one do you prefer. We did this really fun, I would say, trick here where we said, well,
we know we have translation pairs. We generate synthetic pair, the other pair, with what is
a very high-performance model. In this case, we use Command R+, which is super-performance,
does very well in many different languages. And then we compare the two and we ask an
Alem is a judge, which is better, the translated English or the sampled in the other language.
And what we found was this actually helped with translation artifacts because it steered the model
away from the bad, translated and towards the more versatile, fluid Command R+, generation. So
really interesting. And there were some percentage of time where the translated was better. And so
you got that nuance too. So very, very interesting. Yeah, amazing. And then that removed a lot of
the translation artifacts. Yeah. Amazing. Sarah, this has been incredible. Where would you like
to point people to as well for your later stuff? Feel free. So, you know, I lead Co-Here4AI. So
it's a research that we do a lot of fundamental research. And we, a lot of my work is on efficiency,
reliability and building these models that scale the next generation models. So you can go to
Co-Here4AI and take a look at some of our work and just a lovely being here again. It's really
nice catching up. Amazing. Sarah, thank you so much. Yeah, thank you.

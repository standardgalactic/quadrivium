Aiden Gomez is a computer scientist and widely recognized AI expert who co founded AI company co here before that.
He worked as an intern in the Google brain team in Toronto, alongside Jeffrey Hinton, one of the Godfathers of deep learning.
Actually, the only Godfather who hasn't been on MLST.
Aiden, you need to put a good word in for us.
I will bring it up.
So Gomez was the kind of person Hinton recalled who had so many ideas.
It was difficult to pin him down and to get him to focus on what he was actually supposed to be doing.
Now Gomez was particularly interested in learning to translate languages.
And while he was at it, he casually invented transformers, which, as we all know, have become the de facto basic reference architecture for all neural networks and not only for language tasks.
Now co here is a startup which uses artificial intelligence to help users build the next generation of language based applications.
It's headquartered in Toronto and the company has raised $175 million in funding so far.
And the first round was from index ventures and the round also included Hinton, who we just spoke about, Fei-Fei Li and Peter Abial, who are very, very famous folks in the ML space.
Now co here say that their competitive advantage or at least at least one of their competitive advantages lies in its focus on safety, which is crucial for customers deploying models which could potentially out for something harmful.
And we'll discuss what we mean by that a bit later on.
Now, I can honestly say personally that language models have transformed how I use computers over the last six months or so.
I've been using them on a daily basis in the form of scripts, which helped me use the command line better using an interactive REPL playground for doing all sorts of stuff like composing emails for me, understanding error logs when I'm coding, summarizing information, performing repetitive tasks.
Do you name it?
Actually, I regularly discover new tasks that I can use for language models just through kind of creative exploration.
So I'm a bit of a convert, although caveat M tour is still definitely in effect.
The words of Chomsky, Fodor and Felician are still ringing loudly in my ears.
We just did some content about that on our last show.
But anyway, Aidan, it's an absolute honor.
Welcome to MLST.
And what's it like being the CEO of one of the fastest growing startups?
Thank you so much for having me.
I'm stoked to get to meet you and chat about Coheir.
What's it like being the CEO of Coheir?
It's a privilege and a thrilling ride.
Like you're just hanging on for dear life as things are going and trying to keep up.
But it's honestly just such a privilege to get to work with the people that I work with on the problem that we work on.
I think I'm just so, so lucky and fortunate.
I'm really, really excited to get into the discussion about transformers and attention is all you need.
So I mean, I remember when this came out in about 2017, actually, it changed the landscape of deep learning forever.
It took me a very long time to understand it.
And I probably only did understand it after reading Jay's famous blog post, you know, The Illustrator Transformer.
Jay, of course, is he your engineering director now?
Yeah, yeah.
So Jay is the best communicator in machine learning that I know.
He's incredible.
And I think if you search, I think if you search Transformer NeuralNet on Google, it's not our paper that comes up.
It's Jay's blog.
So it tells you like how much better his communication is than ours.
It was an epic blog.
He's actually done a new blog post on stable diffusion as well.
And folks should also subscribe to his YouTube channel.
He's got an amazing YouTube channel.
But anyway, there was Jay's blog post and Janik's video that made me finally understand it because at the time it was the most exciting kind of technology.
And I remember I was interviewing for the Bing team at the time and I spent a long time studying the paper just so that I could be conversant in it when I had that interview.
But anyway, could you tell us, you know, what was the story behind the paper and also what's special about transformers?
Yeah, so the story behind the paper, I should say that, like, I was the intern on this project and so I was the baby, like, doe-eyed, showed up in Google Brain down in Mountain View and my contribution was really on the software.
So there were existing threads inside of Google Brain, primarily driven by Noam Shazir, Jakob Uskarite, Lukash Ashish, which were pushing along this idea of deploying these autoregressive sequence models for text because they had been really popularized in WaveNet for audio and language generation.
But there was like a big push towards how do we deploy these against text and how do we incorporate attention, which was something that was groundbreaking for RNNs, like the previous generation of model.
They wanted to incorporate this into these new autoregressive sequence models.
And so when I came in, I was working with Lukash focused on, like, the software side of things, scalable training frameworks, supporting training that, you know, could be distributed across not just tens of
machines or hundreds, but thousands of accelerators.
And so we built Tensor2Tensor, which was the framework that was used to develop the transformer.
Shortly after we started putting that together, I was sitting next to Noam Shazir and we heard that he was also kind of thinking along similar threads for these autoregressive models.
And so Lukash and I convinced him to come over to Tensor2Tensor and start doing it on our framework.
And then we heard over in the translate department.
So Brain was one division of Google.
Translate was a separate one.
Over in translate, there was Yakka, Bashish, Nikki, working on a similar project to Noam where they wanted to create a purely attentive model, strip back all the complexity of RNNs, all of these, like, very complex,
gates and states and et cetera, and just rip that all away and just have MLPs and attention layers.
And so we all came together.
We all consolidated on this framework called Tensor2Tensor.
And the next three months, up until the NERP deadline, was just a sprint, like sleeping in the office, just going as fast as you can, running experiments.
And iteration and iteration and finding this bug and that bug.
And so really like a huge piece of the project came together within 12 weeks.
So it was like an extraordinary pace.
And it was, as an intern, this was like my first experience in like proper research.
And I just thought this was normal.
I thought, OK, this is what everyone does.
We all just crank out papers in three months.
We sleep at the office.
And I didn't really have an appreciation for what we had accomplished at the end.
Like, I can't say that I was particularly prescient at the time.
I remember like the night before the NERP deadline, the night before we had to submit.
It was like whatever, 3 a.m. Ashish and I were sitting on a couch in the brain office and he turned to me and he he said, you know, like, Aiden, like, this is going to be huge.
And my reaction was like, we bumped up blue, like the blue score by one point.
I was like, really?
You think so?
Oh, cool.
Like, don't we all like, isn't this what research is?
We just bumped it up a point.
What's the big deal?
But I think what I didn't appreciate was the fact that such a simple method could achieve such insanely high performance.
Like, at that time, we were training on eight accelerators for one of these models and they could be trained in within a day.
But the architecture was so stripped down, it was so refined, it was so easy to scale and to grow.
I just, it was hard to see the future.
And it was hard to see what would happen, which was this massive scaling project.
And I think Ashish saw that, that happening.
So yeah, that was my contribution.
That was the part of the team that I brought.
But yeah, it was a very, very exciting few months.
And then it was an extremely exciting few years afterwards before I left Google and started cohere.
I can imagine.
And as you say, some of the best work happens in sprints.
And even though it only moved the needle a tiny bit on the blur score, something we'll explore later is sometimes it might actually be performing better than the metric might suggest.
And simple models are often better.
But I wanted to ask a bit of a technical question.
So DeepMind recently released a paper called Neural Networks and the Chomsky Hierarchy.
They grouped a bunch of tasks according to the formal language classes in the Chomsky Hierarchy.
And they showed that transformers were only able to represent finite languages, so not not even regular languages, which RNNs could do.
Neural networks could only support the higher modes, like context-free languages, if they were memory augmented with a stack.
And they said that this had implication for scaling laws.
So for example, the transformers architecture, they said, could never perfectly learn tasks which were higher up in the Hierarchy.
So we've done some shows on MLSD where we've spoken about neural networks not being too incomplete because they are finite state automators without the augmented memory.
So, for example, they couldn't approximate pi to the nth digit.
And there's this photo and collision connectionism critique paper, which we spoke about on the last show.
And that basically means that neural networks are not able to perform symbolic compositional reasoning or in plain English, they can't represent infinite objects.
And we spoke to Randall Belastriereo and he had this paper called The Spline Theory of Neural Networks, which showed that a neural network is a linear operation given a single input example.
So given all of this, I personally find the conversation about scaling laws, I'm a bit skeptical about it, although I'm genuinely interested in emergent properties or transients that happen during the scaling process.
But given all of this, I mean, how do you think about the practical limitations of transformers in terms of what algorithms they can learn?
Yeah, I really hope the transformers aren't the last architecture.
I would be extremely disappointed if this is as creative and high performing as we can get.
I think that transformers took off because of their scaling properties and also because of a network effect.
The community consolidated around this one architecture, we started to build all of this infrastructure, specifically for transformers.
There was a network effect and it had very nice scaling properties.
And so the community really came together around this architecture and built up infrastructure to support its adoption.
I think that's what's led to their proliferation or their success.
I hope that it's not the last architecture that would be super, super disappointing and boring.
You point out that they're not Turing complete.
I should clarify that I'm not a linguist, I'm not too familiar with Trump's hierarchies or the implications of the deep mind paper.
But one thing that's interesting about it, when I read it,
they're not speaking in theoretical terms or speaking in empirical terms of what functions are achievable.
From a normal initialization.
I think that's a fascinating lens.
In theory, a transformer is a universal approximator and it might be even Turing complete.
But in practice, if you can't explore all permutations of parameters,
it's very true that it'll find the simplest function that satisfies the task.
I think that's like a good guiding lens when thinking about what architectures come next.
Where do we go from here?
What are the sorts of components we need to add into neural networks to support them in representing these more complex functions?
So I do think that transformers are limited and I really hope they're not our final architecture.
I hope that we come up with something that's significantly better and I see promising efforts along those directions.
I think that retro from deep mind, like augmenting transformers with a searchable memory, I think that's a huge step forward.
The next thing we need to support is the ability for these transformers to keep state over long time horizons,
to be able to write into their own memory in order to make notes about what they've seen in the past.
And so I think there's a lot of work to be done and it's not happening fast enough.
I think more people should pick up these research questions and look for new scalable ways of doing it.
Because to speak to the scaling hypothesis, the real bottleneck for feature adoption,
the real bottleneck for adding in a new component to the transformer architecture,
it's all scale and efficiency. People will just adopt the thing that is simplest, fastest and best performing.
And so we need to do the work in order to make these other components like addressable memory efficient and scalable
in the same way that the core vanilla transformer architecture is today.
Yeah, it's such a paradox, isn't it? Because the deep mind paper was saying that a Turing machine is the most powerful computational model,
but neural networks are trainable and they scale really, really well.
So I'm sure we'll find some innovation that will somehow bridge that gap somehow.
But I wanted to learn a little bit more about Cohere.
So could you tell us about your core product portfolio and what's on your roadmap?
Yeah, for sure. So the mission with Cohere is to really just give technology language.
And the way that we want to do that is put the tech, put these large language models into more hands.
Today, if you're building a product and you want to deploy a language model as part of that,
you've got to learn how to use some framework like PyTorch or Jax or TensorFlow.
You need to learn how to install CUDA kernels on a VM, which is actually a huge, huge task.
And so it's this very painful process, requires a lot of learning.
It's very unnatural and requires a ton of prerequisite knowledge.
So for Cohere, what we want to do is abstract that away.
We want to present an interface which is just intuitive, natural, like Cohere.classify.
You feed in the tweet that you want to classify.
You give some examples of tweets being classified into the categories that you care about,
and then you get a response which says, yeah, that tweet fits into sports or whatever.
So we want to create a portfolio of these endpoints, which just makes this technology more accessible.
And the goal being that it starts to proliferate.
Because I think the transformer was released half a decade ago.
And I saw extraordinary research-level results or contributions from it, right?
Like the ability to write really compelling tags, the ability to few-shot prompt and get pretty good performance on a huge swath of problems.
But it just hasn't been changing the fabric of consumer applications.
And I'm a consumer. You're a consumer. We all use these apps.
And so as a researcher who's seen the potential of the technology, it's super frustrating.
You just wonder, like, what's stopping this from getting out there into apps faster?
And I think two of the reasons are there's this huge compute barrier, right?
When you train these big models, you need a supercomputer and tons of data.
That's very difficult to use and collect.
So the compute is definitely one of the big barriers.
But the second piece is that really it's like the people, right?
At the moment, yeah, we have millions of developers on our planet.
But a tiny, tiny, tiny fraction of them actually know how to do this specific thing, machine learning and training models.
So there's not a lot of people out there actually doing the work to integrate this into every product on Earth.
And so for us, like, what we want to do is just blow that open, put this stuff into the hands of every single developer.
It doesn't matter what your specialty is if you're a database developer, whatever you do, it doesn't matter.
The important thing is that now you can build with large language models because you're given an interface,
which doesn't require, you know, three years of study to get up to speed.
So that's really what we're pushing for.
It's quite ironic actually because I was involved in ML DevOps for many years,
and normally introducing machine learning into software engineering makes it exponentially harder.
But now we seem to have jumped to language models where it's become easier again.
And I want to talk about the language being a new type of interface for software, but we'll save that just for a minute.
I want to talk about the friction using large language models as a startup owner.
So I'm a startup founder myself, and I want to use large language models because I'm very excited about them.
And, you know, I can speak to some of the friction that I've been experiencing looking into this.
So, I mean, if we look at what's happening over OpenAI, for example, the Microsoft Signup form and the content policy over there is quite intimidating.
They've made it quite challenging to use in production.
And actually, it makes me wonder how many people are using it in production.
They say that content creation applications have a higher chance of misuse,
which seems like an oxymoron to me because GPT-3 is literally like a content creator.
They say you're going to expect a call from the Microsoft vetting service.
They can pull the rug from under you at any time for any reason.
They log and record everything under the guise of safety.
They can't be held liable for anything.
As I said, open-ended applications are refused.
And in my opinion, the most exciting applications are the open-ended applications, right?
I love the playful nature of it.
Inside my application, I want to build a community of tinkerers who discover interesting new sub-applications, right?
I want to build a marketplace of prefab prompt structures on top of my platform.
I don't want to specify exactly how my application will be used, right?
You know, I want my application to be fluid.
And large language models make the consumers of an application kind of like programmers of that platform themselves.
I think it's an entirely new paradigm.
So with that in mind and the friction, do you think that would ever be allowed on Cohir?
The startup that you're describing.
Well, this idea that I shouldn't need to say to Cohir exactly how I'm going to be using the platform.
I want it to be very open-ended and playful.
I want my users to create new prompts and share the prompts and use it in interesting ways.
I want it to be as open-ended as possible.
Yeah, I think on Cohir, that would be fine subject to your users complying with some basic ethical principles.
So presumably, I imagine that you don't want your users creating bots which propagate false information or hate speech on Twitter.
You don't want like a billion bot accounts responding to every article.
And so I assume that you're also incentivized to have some degree of terms of use.
Is that right?
Yeah, so I completely agree with your terms of use.
I think it's very good actually.
The one sticking point for me is that in order to have my application vetted, I have to say how it's being used.
And it seems to be slightly away from having just quite an open-ended application.
But I absolutely agree with all of the points in your content policy.
Okay, yeah.
So I think that startup should absolutely exist.
That sounds awesome.
Like a community of people sharing prompts, iterating with each other, figuring out stuff that works, doesn't work.
I think we saw a lot of that with mid-journey and stable diffusion, right?
Like just this collective effort to like, let's figure this thing out.
Let's discover new ways to use it.
That should 100% exist in the world and cohere would 100% support that.
I think at the same time, there are just application domains that we don't want people building, like, you know, the ones that I just described.
So as long as you're okay filtering those out and working with us to make sure that your product doesn't get used in those ways,
we're fully on board.
Like that should exist.
That sounds amazing.
And I think like more broadly, coherence is we really want to see a proliferation of this technology.
We want to see a million new startups born from it.
And so we view our users as like partners in bringing this to fruition and putting this tech in front of more users, more consumers, more businesses.
And so we're very collaborative.
Like we don't want to rug pull anyone.
We don't want any surprises.
So long as like our terms are met, we want to be a partner.
We want to help you build.
We want to like support you in the best way possible.
I think some of this paradox might be from a legal point of view because I agree with you.
You call it the playground and that's a great term for it.
I think that some of the most exciting applications of large language models haven't been discovered yet.
And they'll be discovered when you have a diverse community of people kind of sharing and trying interesting things.
But just from that legal point of view.
So we were a bit worried actually that some of the customers of our application might send us up malicious prompts and have our service terminated.
And the way that we've been thinking about working around it on open AI, it's not possible on Coho, I don't think, is kind of getting the users to sign up for the service directly.
And then just pasting their key inside our application so that they're responsible, they're getting themselves blocked if they do anything bad.
And legally that puts us in a much safer position.
But how do you feel about that?
I mean, that's a great technique.
I think it should be supported on Coho if it's not already.
I think it is.
We have a few people doing stuff like that, like a bring your own key type application.
Alternatively, Coho would want to work with you to help you moderate use to catch bad actors, to catch misuse, out of terms use, etc.
So we'll be very collaborative.
We'll help you do that.
We'll help you look at the data.
We'll help you find users who are misusing it.
We won't just blanket ban you because one of your users is trying to adversarily attack your business.
We're trying to build with you and so I think we'll be quite reasonable assuming that you're reasonable too and that you don't want that type of activity on your product.
So it's about supporting startup founders, helping them build their own tools to catch this sort of stuff and ban those users.
It's like a collaborative building approach as opposed to, yeah, you're just a user, either comply or get banned.
We're much more, I don't know, present, engaged.
That makes sense.
I mean, you can appreciate my fear that in a sense startup founders have lost their autonomy because it costs millions and millions of dollars.
You folks are hiring some of the most talented people in the world to do this stuff and we're building on top of that foundation, which at the moment it seems like a risk.
I appreciate that in spirit.
It shouldn't actually be a risk, but just talking high level.
So how would you distinguish your service from, let's say, GPT-3?
So I think the Open AI team and GPT-3, they did like a fantastic thing by opening that up and giving it to the world.
In terms of distinguishing ourselves from them, I think they've taken a very hands-off approach to this stuff.
And they put out endpoints and it's kind of like a good luck, have fun, go build.
With Cohere, we're trying to be more present and engaged and we're trying to tailor our roadmap towards the needs of users.
So, for instance, we're listening to users and seeing what they're signing up for, what they're asking from us.
One of those things is summarization.
And so now we've spun up an effort to release a summarization endpoint that's generally useful across summarizing chat transcripts,
like long documents, that type of thing.
And so it's a two-way street.
It's not just that our users are the consumers of our path towards whatever we want.
It's like a dialogue and a conversation of what do you need?
What should we build next?
What do you see coming?
And then we go build it.
So it's very much a, it feels more two-way, community-oriented.
We're trying to build the right product for our users and the most useful product possible.
And so the way we do that is just through dialogue and conversation and people asking for the thing that they need, they want.
OpenAI service suddenly kind of got a lot better recently.
And I think they call it DaVinci 2.
It's a bit of a mystery because I reviewed GPT-3 when it first came out a couple of years ago.
And recently it seems much better.
And if I understand correctly, they've done some kind of fine-tuning using reinforcement learning to align it to human preferences that instruct GPT or something like that.
I don't even know if that's the case.
But I just wondered if you could comment on that and do you folks plan to do something similar?
Yeah, so we don't call them instruct models.
We call them command models because of the co and co here.
But we do have something currently in private beta.
Hopefully we'll release it soon.
But yeah, it has a huge impact on model performance.
Like the ability to specify an instruction, specify an intent, describe the type of problem that you're solving completely changes model performance.
And in some ways it's surprising.
In other ways, it's very not surprising in that these models are just trained on web scraped data.
Like there's no reason why you would expect them to behave the way that they do.
We're just kind of lucky that they work as few-shot programs.
And so I think this aligning with humans intent, human commands, human instructions,
it's just a much more natural way to interface with these models.
I think before instruct style models, it was a bit like you would have to discover the language of the model, right?
And it was like this very opaque process of shifting things around, rephrasing things,
trying to figure out what makes sense to this model.
Super brittle, super painful.
This command style model actually pulls that away and it's much more intuitive, it's much more fluid.
It's the way that you would expect to interact with the model.
Yeah, it's really interesting how, you know, when Steve Jobs released the iPad,
he said there was something magic about it, something of magic about that interface.
And similarly with these newer models, it feels like an invisible boundary is being crossed where I trust it.
In a way, it's a trap because I'm anthropomorphizing it more because of exactly what you just said before.
But I wanted to get into some engineering characteristics of code here.
So I'll send you some quick fire questions.
So how many tokens in a context window?
So at the moment, it's 2048 and it should flip to 4096 shortly.
But our goal is for an infinite token width.
Wow.
And so there's a few efforts that we're pursuing to enable that.
Yeah.
How many concurrent requests can your customers have, Perky?
Infinite, as many as you'd like.
Oh, even now?
Yes, yeah.
I think there's a top level bottleneck which we can actually remove for specific customers who need more.
I think it's like 10,000 queries per minute.
But we have folks doing billions and billions of characters a day.
And so, yeah, we're happy to remove that restriction for those applications that need it.
Do you support, let's say, enterprise security scenarios like single sign-on, key rotation, that kind of thing?
So we do SSO.
We don't do key rotation yet.
We're hoping to build that out like with other enterprise requirements like co-location and that sort of thing.
We're also building out the capability to do that on different clouds.
But at present, we don't have that yet, but it's roadmaped.
Okay.
And you were just touching on this before, but what's the kind of largest, most highly-scaled application deployed on Co-Hit?
So, similar to, I think, what everyone's been seeing, the first sort of application that has hit product market fit is copywriting.
Yeah.
There's a lot of companies, Jasper, Copy AI, HyperWrite.
They've really found something that works for the average person, drives tons of value, speeds people, makes them more efficient, makes them more creative.
And so that's where we're seeing volumes just skyrocket.
So I'm really excited about the prospect of stacking calls to these large language models.
So building a large computational graph of recursive calls.
But doing all the round trips at the moment is pretty slow.
It's almost like, I want to create this graph, this computation graph.
I want to ship it over to you.
You do it behind the scenes on your app fabric with parallelism.
You send me the results because right now there's a significant engineering challenge for me to do that.
But I think that the next generation of application platforms will be doing something like that.
There'll be a fabric on top of Co-Hit.
So are you planning anything to kind of make my life easier to do that?
That's so cool.
I hadn't actually thought about that.
Sorry.
I hadn't actually thought about that.
Like basically compiling a graph of chained prompts to Co-Hit and shipping it over.
That's fascinating.
I guess what you gain is the two-way network cost, right?
Like that's what you're saving.
And network costs tend to be quite small in practice.
So depending on how big your prompt is and how many tokens you're generating,
I could see it not giving you a huge amount of lift.
But yeah, I'd love to look at your use case and kind of understand
is the issue actually with the fact that you're having to make multiple network requests in sequence?
Or is it that the underlying model itself is too small?
And what you need is actually just speed-ups to that core model.
Another thing that Co-Hit does is we'll do specific deployments for customers.
So some folks have very low latency requirements.
And we can split up our model across more nodes, more GPUs.
And that makes the latency go way, way down.
So if you were looking for like a 6x speed-up, a 4x speed-up in latency,
that's one easy way to do it.
Yeah, I think we're only just scratching the surface of what's possible here.
So if I did this, what I just spoke about, I would need to create a workflow engine.
I would need to do all sorts of DAG optimization and parallelism.
And I might be able to cache certain steps.
And certain steps I might be able to do some metamachine learning to optimize the performance.
That needs a large one. That needs a small one.
There's a whole kind of universe I think that we can explore here,
if you know, just going one level of abstraction higher.
You know, because the next startup founders will be building platforms
that essentially compose human knowledge on top of these large language models in some kind of a fabric.
So I think that's what excites me at the moment.
That's really cool. That's really cool.
Okay, so you express this graph of like prompts chaining into each other.
You might have loops and conditions and you compile that,
you ship it to the large language model provider, they perform optimizations,
they handle all the parallelism, all the conditions.
That's a really fascinating product idea. I like that.
Yeah, please build it so I don't have to.
Anyway, I wanted to come on to some more LLM discussion.
So our friend, Francois Chouelet, he says that large language models are a bit like databases, right?
But to me, they seem like so much more than that, right?
So I think Francois was saying they are the representation, you know,
equivalent to the B tree structure in a database, but I think they're also the database engine as well.
But it's a new type of database engine.
We don't understand how this database engine works.
It's kind of unintelligible to us.
But do you think that's a good analogy or like how do you mentally kind of think about what's going on in a language model?
Yeah, I like the analogy. I like yours a bit better.
I think my own, I kind of view just a raw large language model trained on the web as the next iteration of a search engine
instead of explicitly retrieving results and references out into the web.
So I like to use the analogy of like search engines.
And instead of like explicit hard references where you make a query, you get back a link out to some site.
A language model, it's kind of more like a dialogue.
You can extract information from it.
It has all of this knowledge that it's seen throughout the web.
It's like a soft version of a search engine.
And the mode of discovery is still language is just a much more natural interface.
It's like a conversation as opposed to what we type into Google, which is kind of its own language, right?
Like we don't query in the same way.
We would ask our teacher, our Calc teacher about a theorem.
We write in a very strange way.
I think language models are a much more natural modality to the corpus of human knowledge.
It's much more intuitive, natural, seamless.
The problem is they hallucinate a lot.
And so they'll fill in gaps.
They're compressors, right?
And so they see the web and they try to compress that down into their parameters and they lose little bits and pieces.
And then when they have to regurgitate it, reconstruct it, they'll just fill in a plausible answer inside those gaps.
And so the other reason I'm really excited about retrieval models is that they ground them more in reality.
You put less burden on the model's parameters to memorize every single fact that's out there.
And you instead just have them do the distillation process.
So the model makes a query out to this knowledge base, this database of true facts.
It pulls back some references.
And then the model is just asked, hey, given these references, answer my question.
And so it can actually make reference to true facts instead of having to hallucinate and imagine its own facts.
So yeah, my kind of like the analogy that I like to build off of is this is the next search engine.
It's the next interface to the internet.
It's the next interface to human knowledge.
It's funny you say that it might actually be the next search engine soon.
I think search engines are about to be revolutionized.
But yeah, I was going to ask you about the next frontier of large language models.
And we've already been speaking about some of it.
We've been talking about the integration of information retrieval, which will ground the responses, you know, based on actual things which exist in your operational systems.
We just spoke before about hierarchical compositions, having this exciting, you know, computational graph, maybe some different architecture types or interaction patterns as well.
I mean, you were just saying that there's this back and forth interaction, which I think is really interesting.
On Google at the moment, you just type in a search and you press search once, you're not iteratively refining reflexively, recursively.
Multimodality was another thing I thought of, you know, maybe having language in and text out.
But I mean, those are a few examples, but does anything come top of mind to you for the next frontier?
Yeah, like in addition to what you just listed, I think the ability to keep state over a long term horizon, like my dream for Coheir
is that it knows its users.
It can see into the past of all the past interactions with each one of these users.
It knows their preferences.
It knows about the user and the way that they like to interact with this model.
And to do that, it can't be transactional.
It can't just be like text in, text out, you know, forget everything else.
In the history, we need to be able to like maintain a state between the model and its user.
We need to be able to keep a record of that.
I was talking about with retro, the ability to add into that corpus, keep notes, kind of like maintain an internal dialogue.
We've seen how useful that is with like the scratch pad techniques that people have been working on, just giving the model space to write out its thoughts before giving an answer.
I think we need the ability to store that forever and to have that referenceable down the line in the future.
So, yeah, I think that's one of the most exciting ones.
And then of course, there's multi-modality, which is blowing up now.
Again, it's like another form of grounding.
It ties the model into the real world, into the physical reality of our world.
I think that's super exciting.
So, yeah, I think the ability to reference external knowledge bases contribute to its own knowledge base.
The ability to understand the world not just through text, but through audio, video, images.
And then lastly, augmenting large language models with the ability to use tools.
Obviously, like, we built this world for ourselves.
We built all these little things to integrate with us.
And one of our primary modalities is language.
And so a lot of our tools are language-driven.
If you think about web browsing, it's mostly you reading text and clicking links and following.
It's very text-driven.
And so something I'm super excited about is, okay, well, we have these language models which have pretty good grasp of language.
They seem to have some modest level of understanding.
Can we actually get them to use the tools that we built for us humans, stuff like web browsers?
And the results we're seeing there are just super exciting, super exciting.
Cool.
I wanted to ask you about different modes of understanding.
I'm actually making a video at the moment on the Chinese rim argument.
They say that shortcut learning is an often discussed characteristic of machine learning.
So when a system achieves human-like performance on a benchmark by the slight of hand,
if you like, of spurious correlations in the data, rather than what we intuit to be human-like comprehension.
Now, large language models have learned this very intricate statistical correlation,
which allows near-perfect performance in lieu of human cognition.
And it's possible that evaluating these algorithms against standards designed to gauge human cognition might be barking up the wrong tree.
Now, human comprehension is difficult to pin down exactly what it means,
but it doesn't seem to be entirely reflected in large language models, not always in how they behave at least.
Now, for humans, it's not always enough to know the statistical features of linguistic symbols.
We also need to grasp the underlying ideas and contexts in which those symbols convey,
while language models can pick up on these tiny statistical patterns that we never could.
Melanie Mitchell said in her most recent paper that recent years in AI have produced machines with new modes of knowledge.
So in the same way that various animals are better suited to different settings,
so too will our intelligent systems be, you know, more adapted to various issues.
She said that large-scale statistical models will continue to be favored for matters requiring these vast amounts of historically encoded knowledge
and where performance is paramount, and human intelligence might be favored for problems where we have very limited knowledge
and we have strong causal mechanisms.
So, you know, my question to you basically is what is the difference between a machine learning algorithm
which has learned these very intricate statistical correlations and a human which has developed a more lifelike comprehension?
I think that the first and shortest answer is probably the objective function, right?
Like the context that we evolved in is very, very different than the context that we're training these models in.
Certainly, like humans have a model of language.
We're doing language modeling for sure, but we have to do a lot on top of that.
And that's just one component that we rely on in order to achieve our objective and go through life and procreate.
I think for these language models, they're given just that one piece of it.
And there's a lot within that one piece, just within language modeling, like you're forced to learn a ton of facts,
you're forced to learn a ton of patterns.
And so, it is quite extraordinary that all of this amazing behavior falls out of just the language modeling problem.
But it's also not, right?
Because in order to accurately model language in a generalizable way, you're forced to pick up stuff like,
how do I translate? How do I classify stuff?
Because that's represented in the data.
But when you're asking questions about how come language models get this thing wrong,
which humans find super intuitive or it's so obvious, so logical,
the contexts are completely different, right?
We didn't evolve to just be a language model.
I think that anyone who's claiming that has a high burden of proof.
And it also points back to what we were saying earlier around, is this the last architecture?
Is it just this plus scale?
There's more that needs to be there and also in the objective function, right?
We can't just do language modeling all the way.
I think language modeling will get us very far.
And I think that we can take language models and augment them with these very useful additional components.
We can give them access to tools.
But fundamentally, if you're looking for something that's human-like and acts and behaves like us,
there's an objective function change that needs to happen.
It can't just be language modeling.
There needs to be something more.
Yeah, we all anthropomorphize AI to different degrees, I think.
I mean, what I took from Melanie is that she thinks that language models are not anthropomorphic,
that it's a completely different mode.
But do you think that humans do think like language models and we have some other apparatus on the top,
or do you kind of agree with Melanie that they're completely different?
I don't think they're completely different.
I would not agree that they're categorically different.
I think they're categorically our brains and modeling and statistical models.
I think that they overlap very, very heavily.
Again, I think the objective function is just completely different.
I'm a big believer that the real value of AI models exists as a kind of entangled form of interactive,
creative pairing between a human brain and a model.
So David Chalmers coined the term extended mind to talk about us and phones.
And I think actually large language models, that's the real extended mind
because I feel this when I play with large language models.
I feel that it's truly intelligent in combination or in tandem with me.
So large language models, I don't have agency or intentionality, but we do,
and we can use them and something very interesting emerges from that.
As an example, I want to build a virtual assistant that comes with me everywhere.
It's part of my everyday experience and I'm just in tandem with a large language model
producing all sorts of creative thoughts that are embodied in the environment I'm in.
That, I think, is an extension of my intelligence.
Yeah, I 100% agree.
I'm thinking about what happened with search and the outsourcing of knowledge.
Not even that long ago.
Like pre-Google, right?
We had to memorize a lot of stuff.
It had to be digested.
Some stuff we could outsource into books and that type of thing
and we would retrieve them very slowly and very painfully as needed.
And then came the search engine and mobile phones
and at any moment, any piece of information that you need,
you have a source to go get it.
And so we took that taxing on our neurons,
taxing on our time, activity of having to pull a book off the shelf
and we turned it into like an instantaneous thing.
And so it freed up our minds, freed up our time to do a lot more high impact activities.
And we've been significantly more productive as a product.
And I think the next step is to continue to outsource things that we don't like doing,
things that are taxing, they're time consuming,
that we put off, that we don't like to hand more of that work over to the language model,
over to these ML systems, AI systems,
and to free ourselves up to just focus on the things that humans care about,
the things that we're best at.
So I think it's another complete transformation.
It just turns out that you can hand way, way more than just the knowledge over to a language model.
You can hand entire activities.
Yeah, that's what I'm excited about.
I like your idea of like a personal assistant coming around with you.
You can give it an instructor,
oh yeah, I need to buy body wash and it shows up at your door tomorrow.
I'm building it.
I'm building it.
You're building it.
I haven't done that topic for another day.
Quickfire question.
We spoke about language being the next interface for application development.
So I'm really excited about this possible future.
I'm already building it.
But there's also a bit of a panacea element to it because the sales pitch is that
even my community, my users could become software developers.
They could actually build my platform.
My platform becomes kind of like amorphous because it's so distributed.
But the worry is that we'll get into this situation where we might feel we need to reinvent code
because the prompts might become so complex that you need the equivalent of lawyers to understand them.
Do you think about that concern?
I have not because Coher's whole point is to abstract that away
and to try and make things as simple to use as possible
and to come up with canonical standards, abstractions even above prompting
that are easier to use so that it pushes out further.
It becomes less of a dark art or becomes less complex.
I think in order for this stuff to proliferate, that's like a necessary condition.
Things have to get easier.
It can't be like you're speaking to an alien or you're having to divine a prompt that happens to work.
So yeah, I think that's where the arrow of progress in this field points
is towards more abstraction, easier to use, more intuitive interfaces.
So I'm not too concerned about that because that's something that's very much top of mind.
Aiden Gomez, it's been an absolute honor. Thank you for joining us.
Likewise.
Likewise. Thank you so much, Tim.

Murray Shanahan is a professor of cognitive robotics at Imperial College London and a senior
research scientist at DeepMind. He graduated from Imperial College with a first in computer science
in 1984 and obtained his PhD from King's College in Cambridge in 1988. He's since worked in the
fields of artificial intelligence, robotics and cognitive science. He's published books such as
Embodiment and the Inner Life and the Technological Singularity. His book Embodiment and the Inner
Life was a significant influence on the film Ex Machina for which he was a scientific advisor.
Now Professor Shanahan is a renowned researcher on sophisticated cognition
and its implications for artificial intelligence. His work focuses on agents that are coupled to
complex environments through sensory motor loops such as robots and animals. He's also
particularly interested in the relationship between cognition and consciousness and has
developed a strong understanding of the biological brain and cognitive architectures more generally.
In addition Professor Shanahan is interested in the dynamics of the brain including metastability,
dynamical complexity and criticality as well as the application of this understanding to
machine learning. He's also fascinated by the concept of global workspace theory as proposed
by Bernard Bars. We'll be talking about that on the show today which is based on a cognitive
architecture comprising a set of parallel specialist processes and a global workspace.
Professor Shanahan is committed to understanding the long-term implications of artificial
intelligence both its potential and its risks. His research has been published extensively
and he's a member of the External Advisory Board for the Cambridge Centre of the Study of
Existential Risk and also on the editorial boards of Connection Science and Neuroscience of Consciousness.
Conscious Exotica Professor Shanahan wrote an article called Conscious Exotica
in 2016 where he invited us to explore the space of possible minds, a concept first proposed by
philosopher Aaron Sloman in 1984. Now this space is comprised of all the different forms of minds
which could exist from those of other animals such as chimpanzees to those of life forms that could
have evolved elsewhere in the universe and indeed those of artificial intelligences.
Now in order to describe the structure of this space Shanahan proposes two dimensions, the capacity
for consciousness and human likeness of the behavior. According to Shanahan the space of
possible minds must include forms of consciousness that are so alien that we wouldn't even recognize
them. He rejects the dualistic idea that there's an impenetrable realm of the subjective experience,
remember we were talking about Nagel's bat on the Charmer's show, insisting instead that
nothing is hidden metaphorically speaking, citing Wittgenstein actually. Now Shanahan argues that
while no artifacts exist today, which has anything even approaching human-like intelligence,
the potential for variation in artificial intelligences far outstrips the potential
for variation in naturally evolved intelligence. This means that the majority of the space of
possible minds may be occupied by non-natural variants such as the conscious exotica of
which Shanahan speaks. Now ultimately Shanahan's exploration of the space of possible minds invites
us to consider the possibility for human-like minds but also for those that are radically different
and inscrutable. He concludes that although we may never understand these alien forms of consciousness,
we can still recognize them as part of the same reality as our own.
So Professor Shanahan has just dropped a brand new paper called Talking about large language models
in which he discusses the capabilities and limitations of large language models.
Now in order to properly comprehend the capacities and boundaries of these models,
we must first grasp the relationship between humans and these systems.
Humans have evolved to survive in a common world and have cultivated a mutual understanding
reflected in their ability to converse about convictions and other mental states. Conversely,
AI systems lack this shared comprehension, so attributing beliefs to them should be done
circumspectly. Now prompt engineering is something that we've all become very familiar with,
we've discussed it a lot on this show recently, and it's almost become a fact of the matter when
it comes to these large language models. It involves exploiting prompt prefixes to adjust
the language models to diverse tasks without needing any supplementary training, allowing for
more effective communication between humans and machines. Nevertheless, lacking a more profound
understanding of the system and its relationship to the external world,
it's difficult to be certain whether the arguments produced by a large language model
are genuine reasoning or simply mimicry. Large language models can be integrated into a variety
of embodied systems even, such as robots or virtual avatars. However, this doesn't necessarily
mean that these systems possess completely human-like language abilities. Even though the robot in the
SAKAN system is physically embodied and interacts with the real world, its language is still learned
and used in a dramatically different manner than humans. So in summary, although Professor
Shanahan concludes that large language models are formidable and versatile, they're fundamentally
unlike humans and we must be wary of ascribing human-like characteristics to these systems.
We must find a way to communicate the nature of these systems without resorting to simple terms.
This may necessitate an extended period of interaction and experimentation with the technology,
but it's a fundamental step if we are to accurately portray the capabilities and limitations of
large language models. So anyway, without any further delay, I give you Professor Murray Shanahan.
Professor Shanahan, it's an absolute honor to have you on MLSD. Tell me a little bit about your
background. My background? Well, I've been interested in artificial intelligence for as
long as I can remember since I was a child, really, and I was very much drawn to it by
science fiction, by science fiction movies and books. And then I studied computer science
right from when I was a teenager and got very much drawn into programming, was fascinated by
programming. I really did my 10,000 hours of programming experience when I was quite young
and I went on to do computer science at Imperial College London. That was my degree.
And then still fascinated by artificial intelligence, I went on to Cambridge
and did my PhD in AI in Cambridge, very much in the symbolic school then.
And then I had a long affiliation with Imperial College, did my postdoc there and still in symbolic
AI. And then at some point, I became a bit disillusioned with symbolic AI and I kind of
segued into studying the brain, which was the obvious example of actual general
intelligence that we have. And I think it was a good 10 years on an excursion into neuroscience
and computational neuroscience and that kind of thing. And then deep learning and deep
reinforcement learning happened in the early 2010s and AI started to get interesting again.
And I got very much back into it that way. And I was particularly impressed by DeepMind's
DQN, the system that learned to play Atari games from scratch. And I thought that was a fantastic
step forward. And I really kind of went back to my roots and back to AI at that point.
Yeah, and I think we'll talk about DQN when we speak about your article on consciousness.
But so having such a diverse set of experiences in adjacent fields, how have they influenced each
other? Yeah, well, and one thing I didn't mention is that I've also had a long standing
interest in philosophy. And I very often think that what I am is a sort of weird kind of
philosopher, really. And philosophical questions have had a great attraction for me. So I think
there's a sort of three way into relationship between artificial intelligence, neuroscience,
and the other cognitive sciences and philosophy. And I think they all kind of mutually inform
each other, really. Yeah. Fantastic. So you wrote a book called
Embodiment and the Inner Life. What motivated you to write that book?
Yeah. So at that point, so that book was published in 2010. And it was the culmination of a sort of
long excursion into thinking about consciousness and about brains, which took place after I had
moved away from symbolic AI, really. So I was thinking about the biological brain.
In the back of my mind, I'd always been fascinated by these philosophical questions about
consciousness. And then I went a bit kind of crazy and started thinking about these things
seriously. It became kind of my day job to think about neuroscience, and about consciousness.
And around about that time, the science of consciousness was taking off as a serious
academic discipline with proper experimental paradigms. So that was really fascinating.
And I got to know Bernie Bars. Bernie Bars is the person who originated global workspace theory,
global workspace theory being one of the leading contenders for a scientific
theory of consciousness. And I was very drawn to global workspace theory, and partly because
it was a computational sort of theory. It drew very heavily on computer science
and computer architectures. There was a computer architecture at the center of the theory.
So this kind of collection of interests, along with my philosophical interests, which all came
together, and I wanted to put them all into a book where I expressed my kind of ideas about,
first of all, from the philosophical side, very heavy influence of Wittgenstein about how we
address these problems at all, then lots of global workspace theory and a certain kind of
global workspace architecture, how that might be realized in the brain, drawing also on the
work of Stanislaus DeHend, who was working on what he called the global neuronal workspace idea,
and putting all these things together into one big book.
Amazing. Well, we'll speak a lot about Wittgenstein when we speak about the language model paper
and your consciousness paper. But two things that did trigger or prick my ears up,
computationalism, which is quite interesting, because some folks in the cognitive science arena,
especially with the fouries, like examples to escape from computationalism. We did a show on
cells, Chinese room argument the other day. He's probably one of the most known people who do
issue computationalism. So what do you think about that?
Yeah. Well, actually, so when I was talking about global workspace theory, I mentioned that it
comes out of a kind of computational architecture. But in fact, where I took it was very much moving
away from that original presentation, which drew heavily on a kind of quite an old-fashioned
architectural perspective, sort of boxes and how they communicate with each other and so on.
And I was much more interested in taking it in a direction which is very much more
connectionist and drawing much more heavily on the underlying biology and neuroscience,
which in fact is also a direction that Bernie Barnes himself had moved in, because the book
that originally put forward his theory is from 1988. So that was the predominant way of thinking
at the time was this very computational cognitiveist perspective. So by 2010, when my book was
published, I was very much more interested in a kind of more connectionist perspective on things.
So that's the way that it's portrayed in the book, the theory.
Fascinating. Because in this arena, some people cite penrose or the need for hypercomputation,
because people talk about the church-turing hypothesis and this idea that the universe
could be made of information, which is quite interesting. But do you believe that the world
that we live in could be computationally represented and computed?
Well, I'm not sure that I have a belief on that particular one.
So I mean, I mean, Penrose's ideas about consciousness, of course, draw heavily on
quantum mechanics, and he thinks that quantum effects are important for consciousness. But
I mean, that's very much a minority, a tiny minority view within the people who study
consciousness from a scientific standpoint. And so I don't really subscribe to that
interview, I have to say. Well, I mean, coming at it from a slightly different angle, we spoke
to Noam Chomsky recently, and I've just done some content on Nagel's bat, a couple of rationalists,
their big argument is about the subject of experience and the limits of our cognitive
horizon and the inability really for us to reduce things into a comprehensible framework of
understanding. So how would you bring that in? Yeah, well, gosh, I mean, yeah, we've launched
right into some really big, difficult topics here, right? So in my book, Embodiment in the
Inner Life, which at the time, I thought I'd really kind of like wrapped up the problem of
consciousness. But one of the big sort of outstanding things for me in one of the
outstanding questions that I have not really answered, I felt in that book, is very much
related to Nagel's question about bats, what does it like to be a bat? So, and it's to do with the
idea that there's a sort of intuitive idea that maybe there can be very exotic entities,
very exotic creatures who are completely unlike us. And yet, somehow, there's some kind of
consciousness there that we could barely grasp its nature. And this is a sort of natural
intuitive thought. And especially when we look at other animals, like bats, and especially if we
look at an animal that's a bit different from us, then we get hints that there's some
one at home, as it were, and that there's consciousness there. I think we, I'm sure all
of us believe that cats and dogs, and many other animals are conscious and are capable of suffering
and having awareness of the world that's like our awareness and are aware of us and each other.
I mean, I take that as almost axiomatic. That's just the way we treat those creatures.
But then when we think about something like a bat, it's very different from us. So the natural
thing thought is that maybe what it's like is very, very different from what it's like for us,
and it's a natural thought to express. And of course, Nagel takes that thought
to suggest that there are something that is inaccessible to us, which is what is it like to
be a bat? It's something we can never know. And this is a very un-Viconstinian thought. And I'm
very much, you know, I'm very attracted to Viconstine's philosophy. But it's also a very
natural thought that, you know, so it's a very un-Viconstinian thought because Viconstine says,
for example, you know, nothing is hidden. So he's very, you know, and the whole private language
remarks are all about sort of saying, well, this intuition that we have that there's this
private realm of experience is actually just, it's just a philosophical trick of the mind
to think that this sort of peculiar metaphysical realm exists of inaccessible, subjective
experience in others. And that's his whole thrust of his philosophy or that aspect of it
is to try and undermine that. So these two things are intention, right? So there's this
natural thought that bats, you know, it must be like something to be a bat, but what is it like
and how could we ever know? And then there's the Viconstinian thought, which is actually very
difficult to kind of really embrace. But it's that there's a sense in which nothing is really
metaphysically hidden. It's only hidden, could be hidden empirically, because maybe we don't
know enough, maybe we haven't hung around with bats often enough, or maybe we haven't examined
their brains, or maybe that's all empirical, right? So there's nothing metaphysically hidden,
whereas Nagel's point is that there's something that's deeply, profoundly,
philosophically, metaphysically hidden, which is the subjective. Now we can extend that,
shall carry on. So I'm rambling now. So now we can extend that thought about bats,
now, you know, especially from the perspective of the sort of thing that I'm interested in,
to, well, not just bats, but what about the whole space of possible minds to use
Aaron Sloman's very evocative phrase? What about, you know, extraterrestrials who are going to be,
you know, who surely there is extraterrestrial intelligence out there, it's going to be very,
very, very different to us. So, and then what about the things that we build? Maybe we can build
things, you know, and artificial intelligence of the future, maybe, maybe, you know, we can build
something that is also conscious, it's the kind of thing that's depicted in science fiction all
the time. In science fiction, it's often depicted as very human-like, but there's no reason why it
should be human-like at all. And so we can imagine these very, very exotic entities, and then the
question is even bigger, you know, there could be something that we, we won't even be able to recognize
that there was even the possibility of consciousness, but maybe it's buried there inside this complex
thing somehow. So that's the, that's the kind of question that fascinated me. And I wrote this
paper called Conscious Exotica, which is all about trying to, trying to make that Viconstinian
perspective encompass this possibility as well. Yeah, and maybe we should talk about that before
the language paper, just because it's, it's what we're talking about now. But there's a few things
you said there, which are really interesting. So, you know, when Chomsky talks about ghosts in the
machine, and he goes back to Galileo and Descartes, and actually it was Descartes who, you know,
introduced this kind of mind-body dualism, you know, which was kind of a move away from
the previous desire to have a mechanistic understanding of the world that we live in.
Humans want to understand, and actually so many things in the world eludes our understanding.
And then that brings us on to David Chalmers' point that the hard problem of consciousness,
which I suppose is an extension of the mind-body problem. And it's, as you were saying, this
little bit extra, right? So we think about, and I agree with Chalmers that intelligence and
consciousness are likely entangled or would co-occur together. But he always said that there's
function, dynamics, and behavior. And then there's that little subjective thing on the top. And for
Chalmers' consciousness, it's almost like, what's the cash value of it? He just thinks it's just
something on top. It's not really requisite for anything else. And I believe it might be requisite
for intentionality and agency as so did. But what's your take?
Well, it's interesting because the whole way that you put that and the whole way that
people often talk about this thing is you speak about consciousness. Like, there's this thing,
which, you know, there's this singular thing, which maybe it's needed, maybe it isn't, maybe
it's this, maybe it's that. But I think that whole way of talking is, which is natural for us
in many everyday situations. But when it comes to this kind of conversation, I think that whole
way of talking is maybe not quite right, because we're thinking of consciousness as this, you know,
we're reifying it, turning it into this thing. Whereas I think maybe at that point we have to
take a step back and we have to say, well, when we talk about, when we use that word,
conscious or consciousness, so we use it in all kinds of different ways in different contexts.
And so when we talk about, you know, we might talk about it in the context of an animal, we might
say, well, the animal, you know, this dog is aware of its environment. So, you know, this dog can see
the bowl in front of it, it can see me, it can see the door, it can see the trees, it can see
the squirrel, you know, and it can smell more like you'd smell all of these things as well.
So we use consciousness, you know, we talk about consciousness in that sense. And we also talk
about our self-consciousness, you know, we talk about the fact that we're aware of our own thoughts
and we talk about our inner life and we use consciousness to encompass that as well.
We often use consciousness in the context scientifically of a distinction between
conscious and unconscious processes. And that's a very interesting distinction because
when we're consciously aware of a stimulus as humans, then a whole lot of things come together.
We're able to kind of like deal with novelty better, we're able to report it, we're able to
remember things better. So whereas when we perhaps are unconsciously or there's a kind
of unconscious processing of the stimulus, then we still can respond to it behaviorally, but
and it can have queuing effects and so on, but it doesn't have all those other things.
So this and that's kind of, there's a kind of integrative function for consciousness there.
And then on top of all of that, there is the capacity for suffering and joy that comes with.
So often there's valence to consciousness, you know, so that's another thing.
So all of these things, they come as a package in humans, but when we speak about
edge cases, then these things come apart and we need to speak about them separately, I think.
Fascinating. I mean, there are two kind of minor digressions there. I mean,
you were talking about these planes of consciousness, which is also very interesting.
And maybe we could get into the integrated information theory or the global workspace
theory just for the audience, just to give them some context.
Yeah, sure. Or do you want me to say a few words about that?
Oh, please, yeah.
Okay. Yeah. So there are a number of kind of candidates for a scientific theory of consciousness.
And you just mentioned two of the leading ones, which are global workspace theory and
integrated information theory. And so global workspace theory. So that's, that's Bernie
Baals's was originated by Bernie Baals and has been developed by Stanislaus, Dehen and colleagues.
So the idea there is it's, it does rest on this sort of architectural idea, which is that,
which is that we think of the brain, the biological brain as comprising, you know,
a very large number of parallel processes. This is kind of a natural way to think of the brain,
a large number of parallel processes. And it, and the global workspace theory posits a particular
way in which these, these processes interact and communicate with each other. And that is via
this global workspace. And the idea there is that, is that there are sort of two modes of
processing that go on. So in one mode of processing, the, these parallel processes just do their,
their own thing independently. And in the other mode of processing, they are working via this
global workspace theory. So the idea is that they, you might think of them as, as, you know,
depositing messages, if you like, in this global workspace, which are then broadcast out to all
of the other processes. So, so it's, so there's this kind of, but I think thinking of it in
terms of messages is not quite the right way of thinking of it is better to think in terms of
kind of signaling and information and so on. But that's a natural way to think of it. But
so the, so these, so in, in that mode, the, these processes are sort of disseminating their influence
to all the other processes. And that's the global kind of broadcast aspect of it. And that's when
consciousness, well, that's when information processing is conscious, according to global
workspace theory, as opposed to when it's all just local and the processes are doing their own
thing. That's, that's not that that processing is not conscious. So there's a dist, so it's
about teasing out this distinction between conscious information processing and unconscious
information processing. Now, all of those terms, by the way, are deeply philosophically problematic
and to go in, you know, you have to sort of do it properly, you have to kind of unpack them all
in very carefully. And that's what my book try, try, tries to do. But so essentially, it's about
so the essential idea, though, is to do with broadcast and dissemination of information
throughout the brain and going from like local processes and help them having global influence.
And that's what consciousness is all about according to global workspace theory.
Okay, so integrated information theory. So I think so integrated information theory,
which is Giulio Tononi's theory, which Giulio Tononi thinks is kind of kind of incompatible
in some ways with with global workspace theory. But I don't think that's, that's true. I think
I think that there's a lot of synergy between the two theories, in fact.
But but that's because they so they come with the same for integrated information theory
has sort of two aspects to it. So according to Giulio Tononi, he really is trying to pin down
a property, which is almost like a physical property, which is identical with consciousness.
So you can actually speak about the amount of consciousness in any system that you that you
look at phi, he could this is good, it's phi. So the phi is a number how is actually a number of
how much consciousness is present in the system, like, like part of your brain, your whole brain,
or you as a person, or a flock of bats, or whatever, so you can or toaster, you know,
so you can give a number to how much consciousness there is, there is there according to his theory.
And it's a mathematical theory based on Shannon's information theory. And it's but it and but it's
all about trying to see how much information is processed by the individual parts of the system
versus how much information is processed by all the parts put together. And it's and it's to do
with how much the second thing, you know, exceeds the first thing. And in a sense, and that is how
much consciousness there is there. And, and in a way, it actually has some synergies. If you as
long as you don't think that it's necessarily measuring, you know, this property of the of
the universe, which you can put a number on. But it has some synergies with global workspace theory,
because they're both distinguishing between global holistic things versus local things. And the
and the consciousness is in the kind of global holistic processing versus the local, you know,
local processing in both those theories. So there's a kind of, you know, there's some
intuitions that they have in common, I think. Interesting. And it also reminds me a little
bit a little bit about what Chalmers speaks about. So he thinks that it strongly emerges from certain
types of information processing. And the processing must represent causal structures as well. So it
can't it's it's not an appeal to panpsychism per se. And although with with all of the things
that you've just spoken about, what do they work in another universe? I mean, I guess what I'm
saying is, is it just the the physical and the information processing or in a different universe
might it not emerge in the same way? Yeah, which depends what you mean by a different universe,
I guess. What do you mean by a different universe? Well, if the laws of nature were different.
Yeah, okay. So if the laws of physics were different.
Well, I guess my I guess I dislike isms. I mean, I'm an anti ismist, or rather, I'd say I'm not an
ismist. But if I were to but I do sort of subscribe broadly to functionalism, I suppose. So I guess
I guess I what do I mean by that? I mean, what I mean is, I mean, I really dislike saying that I
subscribe to these to these isms. So what I really mean by that is that is that I imagine that a
system that is organized in a particular way functionally in terms of its information processing.
And if that system is in is embodied in the broadest sense, and, you know, and meets lots of
other prerequisites, then it's likely to behave in a way where I'm going to naturally use the word
conscious to describe it, perhaps, and where I'm going to treat it like a fellow conscious creature.
So, so, so it's so, you know, ultimately, it's I think it's about the kind of organization you need
to give rise to the behavior you need to talk about thing, the thing in a certain way.
My question today, because I posed this question to Chalmers last week, because he's also a
functionalist. And I agree with the degree of functionalism describing intelligence,
but less so with consciousness, you know, there's not a Turing test for consciousness, for example.
But the thing is with functionalism, we're at risk of doing what you said people do with
large language models, which is anthropomorphizing them, because these functions are intelligible
to us. And then our conception of intelligence becomes somewhat observer relative.
Yes, do I mean, what I observe a relative so you understand these functions, so it's conscious
to you, but not to someone else? Well, so, so, so in all of these cases, I mean,
I think it's about the words that we use in our language to talk about the things. So, so,
so if there's someone else is someone just like us, right, then we have to and if we want to use
the words in different ways. So, so the large language models are a great case in point, right.
So, so suddenly we're arriving at a point where somebody can describe something as conscious.
And others can say that's rubbish, you know, it's not that's not true at all. And so we,
so we've, we've arrived at a point where these philosophically problematic words, which,
which we use in ordinary life quite, quite harmlessly. And we all, you know, we all are in
agreement about how we use the word likes if somebody says, oh, you know, Fred has drank so
much last night, he passed out, he was completely unconscious, you know, I mean, and, or if an
anesthetist says, yes, they, you know, the patient is now unconscious, they can't feel, feel pain.
Or if you say, oh, you know, I, I just wasn't aware, I didn't see the, the cyclist and you know,
that's why I, I hit them, you know, I'm really, it's tragic, but I just didn't see them. And then,
and we, so, you know, so you're saying I wasn't aware of it. So that didn't influence my action.
So there we're using the terms in ways that we all understand. But now we're getting to a point
where suddenly, these words or these concepts are being used, you know, we don't have an way,
we don't have agreement about how to use these words, right? Because it's, there are these exotic
edge cases. Yes. So then the question, I think that you, you're getting is, you know, is there
a fact of the matter there, right? And so I'm very tempted to say the first thing I'm tempted to say
is that I don't think that perhaps is a fact of the matter. Or certainly, I don't, I don't want to,
I don't want to speak as if there is a fact of the matter, but rather, I think we need to arrive
at a new consensus about how we use these words. So that might mean that we extend the words,
we break them apart, like I was suggesting earlier, maybe we need to separate out awareness of the
world from self awareness, from integration, cognitive integration from the capacity for
suffering, because suddenly we have things that where that they don't all come as a package. And
when we need to kind of be a bit more nuanced in the way that we use these words, we need to use
them in new ways. But then there's a kind of transition period, because we don't, you know,
we're all arguing about how to use these words all of a sudden, because we've got weird edge cases.
So there's going to be a time when it'll take a time for language to settle back down again.
So there's a kind of, you know, there's a kind of observer relativness to this for a bit, if you
like, but then, but then there's a kind of consensus needs to emerge, right? But so many
things to explore there. I mean, I'm, I would love it if this platonic idea of concepts were possible.
And what platonic? Because what we're talking about here is reductionism and the, I mean,
the parable of the blind man and the elephant comes in quite nicely. So as Chomsky said,
complex phenomenon beyond our cognitive horizon. And as much as we don't want to,
we use functions derived from behavior to have some common understanding of this thing.
But I wasn't being reductionist, was I? Do you think I was being reductionist?
Well, no. So you said that the language game converges. And in some cases, we will arrive on
a common definition, but like you can bring in Hofstatter as well. Well, not a common definition,
but a common usage, right? So we'll come, so we'll come to use the words, you know, with agreement,
right? So that's what I, and the reason why I mean, I would, and the reason I would balk at
using the word reductionist is because, and that's why I'm a bit resistant to functionalism as well
to any kind of ism is because I just think that that may be the way things are organized when
you take them apart. So, you know, brains, right, when you examine them on the inside,
like animal brains, you might look at how an octopus's brain works. And that might inform
whether you think that it suffers, can experience this pain or not. Or we might break apart, you
know, an AI of the system of the future, right? You know, and we might break it apart and we may
look at its functional organization. And that all is just is grist to the mill of how our language
might change, right? So I'm not, I'm not subscribing to the fact that consciousness is this or this
is that it with some big metaphysical capital letters on the is, right? That's really important.
So the functional organization of these other things, which when we study and look at it,
is all just part becomes part of a conversation that eventually is going to help us to settle on
maybe new ways of talking about these things. I think we agree with each other. I think the
difference is, so with the parable of the blind men and the elephant, all of the men around the
elephant saw something which was part of the truth. And I think that's what we're describing with
the function. So we can all agree on what perception means or what action means. The thing is,
there will be many other functions that will represent a different slice of that cognitive
phenomenon. Yeah, I agree. And I think that's very much true with consciousness, actually, because
there's lots of people coming with kind of like new ideas and new theories. I mean,
Anil Seth, for example, have you had Anil on your on your not yet being right?
So Anil's written this great book called Being You. And Anil brings in a whole kind of new set
of ideas. He's particularly interested in the sort of top down effects on perception, top
down effects on perception. So then he brings in this kind of top down influence and perception
as a big part of things. And then there's Graziano has written this book using this about
his attention schema theory of consciousness. And that's, and, you know, there's a whole set
of interesting ideas there. And I think you're absolutely right. I think there's, I think there's
aspects of all of these things all feed into, you know, all feed into the way, you know,
brains and animals work and all of them feed into the, you know, why they behave the way they do
and why we use those words when we use those words, you know, conscious and aware and so
fascinating. We'll get to your article in a second. But as someone who has such a diverse
and interesting background, who is allowed to ask these philosophical questions? So it reminds me
and Thomas Meckens who is talking about the arguments between neuroscientists and philosophers
about freedom of the will. Yeah. And who gets to decide? Huh. Yeah. Well, what a great question.
You know, I mean, so why should I have any right to speak about any of these things at all? Because
I have no formal training in philosophy. So, so, so who gets to, who gets to dis, well, who gets to,
to, I guess there are two things, right? There's, I guess, I guess there's, there's
in that kind of discussion between the neuroscientists and the, and the philosophers. So there you,
there you're not talking about, you know, the everyday conversation that we're all having as,
as, as humanity or as English speakers, or as Chinese speakers about how we use these, these,
these words. So there it's a much more kind of confined to the, to, to different, two different
schools or disciplines within academia. So there, I mean, I do think that the people who work in AI
and in, and in neuroscience, probably at least should be a bit familiar with, with the philosophical
debates. And you know, you mentioned Descartes earlier on, and you know, you're familiar with,
with just that, that basic kind of, you know, sort of stuff that it was just like philosophy 101,
which people should at least be aware of Descartes arguments and then Chalmers, and the different
kind of perspectives on those sorts of things before they pitch in, you know, at least, I mean,
you wouldn't pitch into neuroscience just by making some up some stuff about brains. If you
hadn't read, you know, the, an introduction to neuroscience. And so, so I think that the scientists
need to, you know, you know, they need to kind of have a, a past to enter the conversation,
which is to have, to have gone through philosophy 101. Yeah, it's so true. We have the same thing with
the, with the ethics folks, actually, because, because we have a lot of them fields of expertise,
and engineers should learn more about ethics. Yeah, absolutely. But when they do have an
opinion about ethics quite, quite often, it's, it's, it's, you know, it can sometimes be a bit
naive. And, and, and, and, you know, at least you should be familiar with the kind of, but, but
that's an interesting and the difficult area in itself. Because of course, you know, you,
as a scientist, it's important that you take responsibility as a scientist and the, and that
you take, you know, some ethical responsibility. But at the same time, you know, you've only got
so much time to become an expert. So, so it's difficult to at the same time, take ethical
responsibility. And yet, you know, even though perhaps you haven't got the time to kind of read,
you know, read up and become an expert on the relevant ethics. So, I mean, perhaps everybody
again, should, you know, get to the entry level, you know, ethics 101. And indeed, many, many courses
teach, you know, ethics, these days, many kind of science and computer science. It's part of,
you know, of any degree these days. So that's a good step, I think. Yeah, there's an interesting
analogy between the functionalism that we were speaking about in consciousness. I mean, even
in our own research domain, we have the function of ethics, and we have linguists, and we have
all sorts of different people. And that is the blind man and the elephant. And, you know, I
tend to believe that even though the views from these diverse folks are inconsistent, diversity
is very important. Oh, incredibly important. Intellectual diversity is, you know, every
kind of diversity is important. And intellectual diversity is immensely important. And having
these conversations, these interdisciplinary conversations is absolutely, you know, essential.
So at least if people are talking to each other, that's a really, really positive thing, I think.
Fantastic. Now, we invited Chalmers on our podcast after Ilya Sootskeva's tweet. And by the way,
Chalmers took a very functionalist approach to talking about consciousness. But I guess,
after that tweet, everyone in the community started thinking about and talking about
consciousness. So maybe let's just start from that tweet. How did you find it?
Sure. Yeah, okay. So the tweet was, so Ilya Sootskeva said, it may be that today's large
language models are slightly conscious. And then I replied, tweeted back in the same sense that
may be a large field of wheat is slightly pasta. And that, in fact, was actually, I mean,
I've got a fair number of Twitter followers, and that was the most engaged tweet I've ever sent
out. And, you know, and, you know, it got celebrity likes, Hannah Fried retweeted it, and, you
know, only as my kind of comment. And so, so, but that does kind of summarize sort of what I think
about, about what he said at that point. But then, but then I after tweeting my, my flippant
response, I then I was violating all my own Twitter rules in in just sending back a flippant
response, because I generally don't do that. I would rather kind of, you know, be professional,
engage professionally. And so I thought it's very important to follow that on with a, you know,
with a little explanation of why, you know, why I thought that it wasn't really appropriate to
speak about today's large language models in those terms. Yeah. And for me, the number one thing is
to do with embodiment. So, so as I see it, embodiment is a kind of prerequisite for for us
being able to use that, that word, use words like consciousness and so on, you know, in the way that
we do in the normal everyday way of talking. So, so, you know, it's only in the presence of something
that that inhabits our world. And by inhabits, I don't mean just has a physical, you know,
like a computer is obviously a physical thing in our world, but inhabits our world means that,
you know, walks around in her own swims or jumps or flies or whatever, but is is is inhabits the
same world as us and interacts with it and, and, and, and you know, and interacts with the objects
in it and with other, with other creatures like ourselves. So, so that to my mind, that is,
that's the, that's so, so, so in that paper conscious exotica, I think I use this phrase
trans channeling Wittgenstein that that only in the context of something that
that exhibits purposeful behavior, do we speak of consciousness. And the way that that's
phrased, there is kind of, you know, so trying to channel Wittgenstein's style of saying things,
because you notice that he's saying that he's making what he's saying is actually he's talking
about the way we use the word. So he's not making a metaphysical claim. This is essential. He's
saying that this is just this is the circumstances under which we use this word. So we use this word
in the context of fellow creatures, basically. And so, so that's the kind of the starting point.
So a large, and of course, we, of course, we talk to people on the telephone and over the
internet and so on. And we don't, you know, we may not, you know, we can't see them or anything.
So we, but, but, but we still, we know that there is, you know, or we assume, we've always been
able to assume up to this point that there is a fellow creature at the other end. And that's the
kind of grounding for kind of thinking that way and using using that word. Now that is absent
in large language models. So large language models do not inhabit the world that we do.
Now, I mean, we have to caveat that because, of course, it's possible to embed a large language
model in a, you know, in a, we always do embed it in a larger system. It might be very simple
embedding. It might be just a chatbot, or it might be much more complicated, like it might
be be part of a system that enables a robot to kind of move around and interact with the world
and take instructions and so on. So there was a great, some great work from Google with their
Palm Seican robot, for example, where there's this embedded large language model. So, so,
so there you're kind of moving in a, in a direction where maybe where these, where these words, you
know, the prerequisites, you know, for, for, well, actually, I want to be careful what I say here.
Sorry. Sorry. Because it's so easy to say something that's going to can be misinterpreted,
right? Yes. But, but we can imagine that, that we can imagine that requirement being met for, for,
for not, not, it doesn't mean it wouldn't be a sufficient condition for using those words,
but at least it would, you'd meet the necessary conditions, right? Yes. But the large language
models by themselves do not meet even, they're not even candidates. Yes, I agree. And we,
there's so many things we can do here, because we can, we can talk about embodiment in general. I
mean, as I understand it, Rodney Brooks kind of started the phenomenon of thinking about a
representationalist view of artificial intelligence or, or rejecting, rejecting a representation.
Rejecting. Yeah. So, so Rodney Brooks thought that we should use the world as its own best
representation, which is absolutely fascinating. Yeah. And then you, maybe you might be thinking
about the embodiment of you in a similar style of Wittgenstein. So it's a matter of complexity,
and it's also a matter of encapsulation, which is fascinating. But, but also just to quote your
paper, you said, although the language model component of SACAN provides natural language
descriptions of low level actions, it doesn't take into account what the environment actually
affords the robots. And there's this whole affordance thing as well. So, I mean, how do you
think about embodiment? So, so the way I see it is that is that the, you know, the one exemplar we
have as of, you know, the end of 2022 of something that we really can describe as, as, as, as
intelligent as generally intelligent is, is the biological brain, biological brains of humans,
but also of other animals. And the biological brain, you know, at its very, it's very kind of
nature is it's there to help a creature to move around in the world, to move, right? It's there
to move, help to guide a creature and help it move in order to help it survive and reproduce.
That's what brains are for. So that's what that from an evolutionary point of view, that's that
they developed in order to help a creature to move. And they are so they and they are, you know,
they're the bit that's comes between the sensory input and the motor output. And as far as you
can cleanly divide these things, which maybe you can't, but I mean, so and so that's that's their
purpose is to intervene in the sensory motor loop in a way that benefits the organism. And
everything else is on built on top of that. So, so, so the capacity to recognize objects in our
environments and categorize them and the ability to kind of manipulate objects in the environment,
pick them up and so on. And all of that is there, you know, initially to help the, the, the organism
to survive. And, and, and, you know, and that's what brains brains are there for. And then,
then when it comes to, you know, the ability to work out how the world works and to do things
like figure out how to gain access to some item of food that's difficult to get hold of, then all
kinds of cognitive capabilities might be required to understand how you get inside a, you know,
a shell or something to get the fruit inside it or something like that, complex cognitive
abilities, that sort of. And then, you know, evolutionary evolution has developed a lot of
more and more and more complex cognitive cognition until we get to language and, you know,
we need to interact with each other because that that's all very much a part of it, the social,
social side of it. And then language is part of that. So as I see it, it's all built on top of
this fundamental fact of the embodiment of the animal and the organism. So that's in the
biological case. So that's sort of our starting point. So, and so that seems to me to be the,
the most natural way to, to understand the very nature of intelligence.
Could I frame it? I think I didn't, I didn't frame it very well. I mean, Melanie Mitchell
recently had a paper out talking about the Four Misconceptions and one of them, of course,
citing Drew McDermott was the wishful mnemonics and the anthropomorphization which, which you've
basically spoken about. But her fourth one was about embodiment. And she spoke about this in
her book as well. She said that one of the misconceptions of AI is that people have this
notion of a pure intelligence, you know, something which works in isolation from the
environment. And you're talking about social embeddedness and embodiment. And I guess my
point with the complexity argument is I'm saying that the brain itself doesn't actually do everything.
It kind of works as part of a bigger system. Oh, I see what you mean. Yes. Okay. Yeah. Yeah. So
there's, so of course, there's, I mean, I noticed in one of your previous interviews with Andrew
Lampinen, you mentioned the three E's frame, we're called four E's these days. And of course,
that's very much part of it is the, is the idea that, you know, there's the extended mind, we use
the environment, you know, as, as, as a kind of memory, for example, we deposit things in the
environment, writing, you know, as an example and so on. And then there's, people talk about
morphological computation, I'm sure you're familiar with that. Well, so that's the idea
that the very shape of our bodies, you know, is, is, is, you know, could. So, so, so sometimes,
you know, the aspects of intelligence are actually outshort outsourced into the physical shape of,
of your body. So where you might think about designing a robot, where you, where you put a lot
of work into the control aspect of it, so that it's, so that it can kind of walk in this very
carefully engineered ways that it's always permanently stable, or alternatively, you can
make a body that is naturally sort of stable, or maybe naturally unstable. And what you do is you
kind of rely on the combination of the physics of it constantly falling with, with a control system
that constantly restores balance. So, so, so, you know, so that's, that's, I mean, this is very
much a Brooks type perspective, and people picked up on Brooks's ideas and extended them in this
sort of way. So I think that's, I think that's a very natural way of thinking.
But in a way that this gets to the, to the complexity argument, because I guess what I'm
saying is that life is much more brittle than anyone realises. You were just pointed to some
sources of brittleness that most people never would have thought of, which is, which is fascinating.
So I think there's a very important relationship between embodiment and language. And this also
brings us back to Wittgenstein as well. So, so for us as humans, language is inherently an embodied
phenomenon. It's, it's, it's something that is, it's a social practice, something that take that
it's a phenomenon that occurs in the context of other language users who inhabit the same world
as we do. And where we have kind of like joint activities, we're triangulating on the same world,
and we're acting on the same world together. And that's the that's what we're talking about when
we use language. So there's this, so that, that's an inherently convincing view of language. And I
think it's deeply profoundly correct view of language. That's, that's what, that's what language
is there for us is so that we can talk about the same things together so that we can, our collective
activity is, is, you know, is, is, is organised to some extent, thanks to language. So that's,
so I think that's a really important perspective on language is Wittgenstein perspective. And, and
embodiment is at the heart of it, embodiment and inhabiting the same world as our other language
users. And, you know, that's the way we learn language, we learn language by being around
other language users like our parents and carers and, and, and peers. And, and that's again a very
important aspect of the nature of human language. Now large language models, they learn language in
a very different way indeed, they do not inhabit the same world as us, they do not kind of sense
the world in the way that we do, they don't learn language from, from other language users,
from their peers in the way that we do. But rather, you know, will we know how large language
models work, there's trained on a very, very large corpus of textual, of textual data.
So an enormous corpus of textual data so bigger than any human is likely to encounter in a, you
know, by the time they become a proficient language user at a young age. And what they're trained to
do is, is not to kind of engage in activities with other language users, but to predict what the
next, you know, what the next token is going to be, which is a very, very different sort of thing.
So they're very, very different sorts of things. And the, and the role of embodiment is really
really important in this difference, I think. Yes, absolutely. When I spoke with Andrew Lampinen,
he's really, really interested in the grounding problems. I mean, would you mind just speaking
about that a little bit before we go into your paper? Yeah, absolutely. Yeah, yeah. So of course,
this goes back to a great paper by Stephen Harnad back in, I think, 1999 or 1998.
Yeah, the one and only. Yeah, the one and only on the symbol grounding problem, it was called.
And, and, and, you know, he does argue broadly that the symbols in AI systems,
the kinds of AI systems he was thinking about at the time were kind of, you know,
sort of expert systems say or something like that. And the symbols there are not grounded,
they're provided by the human programmers and they're just sort of typed in. Whereas for us,
for us, the words we use, those symbols are grounded in, in our activity in the world. So
that when we use the word dog, that's because we've seen dogs. And we've talked about dogs with
other people who've also seen dogs. And we've seen dogs in lots of different circumstances. And
we've also seen cats and, and, and, and, and dog bowls and bones and many other things that all
kind of contextualize that. But all of that, that that is kind of grounded in the real world in
our and in our perception of the things in question. So that so that's this. So that's what
sort of is meant by grounding or that at least that's the original meaning of the word grounding
from Stephen Harled's paper. And I think that's a really, really important concept because,
because, you know, in an important sense, large language models, the symbols that are used in
large language models are not really grounded in that kind of way. Now this, you know, I should
be absolutely clear that large language models are immensely powerful and immensely useful and,
and so that, you know, so, but it's interesting that to what extent the lack of grounding here
that we have in today's large language models, you know, might affect how good they are. So,
so they, so they are prone to kind of, you know, hallucinating and, and, and, and confabulating
and, and if you look at multimodal language models that maybe we'll talk about an image that you
present to them, then, you know, they, you can have a very interesting conversation, but sometimes
they'll go off pieced and start talking about things that are not in the image at all and as
if they are. And that's sort of because due to a kind of, I would say lack of grounding this so that
so the words are not kind of grounded in the images in, in quite the way that we would like. So
that's, it's an important topic of research, I think. Yes, indeed. And although some people do
believe there's this magical word called emergence and possibly some emergent symbol
grounding might be possible, maybe, maybe, shall we just put that to bed before we introduce
your, yeah, sure. Well, well, I mean, emergence is, is, I think is, is a really important concept.
And I, and I think, you know, we do see a tremendous amount of, of very powerful emergence, I think
in today's large, large language models. So, so, so, you know, even though they're, they're, so
they're basically trained to do next word prediction. Or I mean, to be clear, I suppose I
should have made this maybe a bit clearer in the paper, but of course, it's not always next word
prediction, because there are different models learned to actually, you know, predict what's
in the middle of a, of a sequence rather than kind of generally, you know, they're interested in,
in, in, in, in, let's take the next word prediction case as canonical. So, so they're, so they're,
so they're trained to just to do next word prediction. Now, the astonishing thing is,
as I think GPT three showed us, is that, is that just that capability, if it's sufficiently powerful,
can be used to do all sorts of extraordinary things. Because if you provide, you know,
the prompt that describes, you know, describes some kind of complicated thing, you know,
situation, like, you know, I need to tile my floor and my floor is shaped like an L and it's
20 meters long and three meters. Well, you know, you start to describe this thing, you know, and
each, each tile is, is 20 centimeters square, how many tiles will I need? And, and some large
language model will come back and tell you, you need 426 tiles, whatever. And it's correct, right?
Well, this is astonishing, because it was only trained to do next word prediction. And so there's
a kind of emergent capability there. Now, there's a sense, of course, in which it still is just doing
next word prediction, because in the vast and immensely complex distribution of tokens in human
text that's out there, then the correct answer is actually the thing that's most likely to come up.
And that's, but it's got to discover a mechanism for producing that, right? And so that is where
the emergence comes in. And I think that, you know, these capacities are astonishing, the fact
that they, that it can discover mechanisms, you know, emergently that will do that sort of thing.
Yes. And maybe I shouldn't have used the word magic with it with emergence, I'm a huge fan
of emergence. And, and as you say, the decode is trained to predict the next token or the
denoising autoencoders to, to, let's say fill in the gaps in the middle. And I guess there are
different ways of thinking about emergence. So there's weak emergence, which might be thought as
computational irreducibility, or a surprising macroscopic change, or strong emergence when
it's not directly deducible from truths and the lower level to make, you know, lots of things.
Yeah, yeah, the different senses of it, yeah. Exactly. But I guess my point is that remarkably,
it's trained on something quite trivial. So all of this is about convention, right? All of this is
about what's, what, what is the, what is a good way to use words, right? So I don't, so I don't
think, you know, I'm not making metaphysical claims about, about, about these things. So it's
all about what, you know, when is it appropriate to use words, to use certain words? And, and
because when, when this becomes problematic is when they're philosophically difficult words,
like beliefs and thinks and so on. Now, when it comes to reasoning, so, so I do think that we,
I do think it's not unreasonable to, to, to use that term to describe what some of the,
these models do today. And that's partly because of the content neutrality of,
of, of reasoning. So, so, so, so a lot of the argument, or a lot of the discussion in the
paper comes back to this sort of whole embodiment thing, really. And, and I'm, I'm saying, well,
you know, in the kind of like ordinary way we use the word believes, well, it gets, it gets
complicated because we do use the word believes in this intentional stance way to, to talk about
ordinary everyday things. We say, oh, my, my, you know, my, my car clock thinks that it's
British summertime, you know, you know, because we, and then, but then you'd say, then somebody
says to you, what you, what you mean, your, your car clock and think, you say, no, obviously,
I didn't mean that it can think, it's just a turn of phrase, you know, but when we, when we get to
these large language models, we start to use the words like thinks and believes and so on,
because they're so powerful, it starts to get ambiguous and yours, and your, and, you know,
and some people will say, well, actually, I really didn't mean that it can think or that it believes.
So I'm, so I'm, I'm interested in this, when things get difficult in this respect. And could,
could you tease apart that work? So you resist anthropomorphic language in terms of belief,
knowledge, understanding, self or even consciousness, but less so with reasoning. And I, my intuition
is that reasoning rather depends on those things that I just said before. Well, I, so I think it
doesn't because, but this is perhaps, this is just maybe in a kind of formal logic sense, because,
because reason, because logic is content neutral. So if I tell you that every, could you just explain
what you mean by that? Okay. So, so Lewis Carroll has all these wonderful kind of like nonsense
syllogisms, right? Where he, where, you know, he says, oh, if all elephants like custard and,
you know, Jonathan is an elephant, you know, Jonathan likes custard, and, you know, all kinds
of things like that. And it's all sort of nonsense. And he has this big complex, complicated ones.
Similarly, I could tell you that all, all sprung forths are plingy, and, and Juliet is a sprung
forth. Therefore, Juliet is, is a springy, right? And I've no idea what any of those things mean,
but the, the, but it's because it, because it, for the pure form of the reasoning, you don't have
to know what they mean. It's just about the logic. So, so in that sense, you know, it just in the way
that a theorem prover can do logic, then so can a large language model do logic. So in that sense,
I think large, it is reasonable to use the word reasoning in that logical sense in the
context of large language models. I don't think that's a problem. Of course, we may think that
they do it badly, or they do it well, or that's a whole other thing, right? But, but at least the
word is potentially applicable, right? Yes. Now, belief, I think, you know, I think at the moment
is a, is a, is a different kettle of fish, because to really have a holder belief, it's, it, it's,
it's not content neutral, right? So if you, if I believe to use the example in my, in, in my paper,
if I believe that Barundi is to the south of, of, of Rwanda, well, whether that is the case or not,
it does depend upon facts that are out there in the world. And then to, to really have a belief,
at least you've got to be able to somehow try and kind of justify those facts, or at least, and you
got to be at least built in such a way that you can, you know, interact with the external world
and do that sort of thing, right? And, and verify that something is true or false or do an experiment
or, you know, or ask someone or, you know, you've got to go outside yourself, right? We go outside
of ourselves and, and in order to establish whether something a belief is true or not. And so,
you've got to at least be capable of doing that. Whereas large language models, the bare bones,
large language model is not capable of doing that at all, right? Now you can embed it in a larger
system. This is a really important distinction that I've tried and make over and again in the
paper. I talk about the bare bones, large language model. So you can take the, so, so, and whenever
a large language model is used, it's not the bare bones, large language model, which just does
sequence prediction, but it's embedded in a larger system. When we embed it in a larger system,
well, that larger system maybe could consult Wikipedia or maybe it could be part of a robot
that goes and investigates the world. So that's a whole other thing. But then you have to look at
each case in point and, and, and ask yourself whether it's a, whether, you know, whether we
really want to use that word in, in, in anger, you know, as in, in its full sense, rather than just
in the intentional stance sense of a kind of figure of speech. So, and so in the case of, of, of,
of like chatbots, for example, today's chatbots, not really appropriate, I would say. We're not
using the word in the way that we, in the full blown sense that we use it, where we talk about
each other. Fascinating. Okay, well, let's get on to intentional stance. So you said that it's a useful
way of thinking about artificial intelligence, allowing us to view computer programs as intelligent
agents, even though they may lack the same kind of understanding as a human. And then you cited
the case of Bob and Bot. The, the word no was used differently in the two cases. And the word of
Bob, it was used in the traditional sense. For bot, it was used in a metaphorical sense. So it kind
of like, it's just distinguishing what it, what it means to know, you know, for humans and, and,
and for machine. So I think it's, it's useful to think about something like Wikipedia. So,
so we might ask the question, does Wikipedia know that Argentina has won the 2022 World Cup?
And just immediately after the event, you know, it probably doesn't, it's not recorded in Wikipedia.
And somebody might say, Oh, Wikipedia doesn't know yet that the Argentina have won. And so when we
use the word like that, you know, nobody's going to kind of say to them, say to somebody who uses
that word, hey, you know, I don't think you should use the word knows there. And you know, that would,
you know, you should be a bit more sort of sensible. I mean, it's, it's fine to kind of use, I think,
these kinds of words in this ordinary, every day sense. And we do that all the time. And that's
sort of, particularly, particularly in the case of computers, that's adopting what Dandenek calls
the intentional stance. So we're, so we're, we're interpreting something as, as, as, as having beliefs,
desires and intentions, because it's a kind of convenient shorthand. And especially if you've
got something that's a bit more complicated, like say your car sat down for something or
you're, you're, you know, you're sat up on your, on your phone, then it sort of makes, makes sense
to use those words. It's a, is a convenient shorthand. And it helps us to kind of talk about them,
right? And without getting overly complicated, without knowing the underlying mechanisms. But
there's an important sense in which we don't mean it literally. So you know, in the case of
Wikipedia, you can't, you couldn't go up to Wikipedia and pat it on the shoulder and say,
hey, Argentina have won. And there's no way, you know, right, I want to be a, you know, and,
and, and, and, and all, and all the things that, that go with us as humans actually knowing things.
And it's just a turn of phrase. Now, things get sort of interesting with large language models,
and with large language model based systems and the kinds of things that we're starting to see
in the world, because we're starting to get into this kind of blurry territory where it,
we're blurring between the intentional stance and, and, you know, meaning the meaning it literally.
And this is where we need to be really, really kind of careful, I think. So at what point does
do things shade over into where it's legitimate to use that word, you know, literally, in, in the
context of something that we've built, you know, I don't think we're at that point yet. And, and
we need to be very careful about, about using the word as if we were using it literally.
You know, that's the sort of anthropomorphization, because the problem is that we can then
impute capacities to the thing and, and, and, or even, you know, empathy say that just isn't there.
Yes. And I suppose we could tease apart knowledge. So it justified true belief from
knows, because knows that it brings all this baggage of intentionality and agency and anthropomorphization.
But you had Chomsky, you've had Chomsky on.
I can tell you a story about that. I mean, the recording messed up. So when we were interviewing
him, we were only getting bits and pieces. And we had to deep fake him. We had to, we had to
regenerate the interview. Oh, really? And he was saying in the entire interview, how much he hated
deep learning and how useless it was. And then we used deep learning to rescue his interview.
And he gave us his permission to publish it. That is wonderful.
So it's quite ironic. But no, he always says it's wonderful for engineering,
but not a contribution to science. Yes, sure. Yeah.
Yeah. He said, I like bulldozers too. They're good for clearing the snow, but they're not a
contribution to science. So who else have you had? I mean, you've had a lot of people on.
I listened to Andrew's one, by the way. It's Andrew Lampinen. Yes, he's great.
So he's great. Andrew is somebody I do work with quite closely.
So it was interesting listening to him because Andrew had quite a big influence on this paper,
by the way. Oh, okay. But I think I might have had a bit of influence on him as well,
to listening to him. I think so. Because that interview was just after he'd read,
and he read my paper, gave me lots of comments. And we had a lot of discussion about it.
And that interview, looking at the recording date, was sort of just after this. And it's interesting.
I mean, he was very circumspect in some of the things he said. Yeah, it was very interesting
because I think the influence has maybe gone both ways. Yes. Which is nice. I don't know.
I mean, I can't be sure of that. I think there's a huge similarity. Yeah. I was thinking that,
actually, just when you were speaking. But it's funny because we've spent a lot of time arguing
with each other about it. And I often feel like we're coming from very different perspectives
on this. But in fact, I think there's convergence, really. What are your areas of disagreement?
Well, you see, I would have thought that Andrew would have been more on the side of,
we can do things without embodiment, and without grounding, or to kind of take grounding in a
more liberal sense. Because some people would talk about grounding, so they say that large
language models, they are grounded. Prompt Engineering is the process of using prompt
prefixes to allow LLMs to understand better. So the context and the purpose of a conversation
in order to generate more appropriate responses. What do you think is going on with Prompt
Engineering? Yeah. Well, yeah. So you let's probably let slip a phrase there. So the process
of allowing the models to understand better is what you're better. Of course, I don't think
guilty as charged. I don't think I don't think that's the right way of characterizing it at all.
So I mean, I think the whole thing of Prompt Engineering is utterly fascinating. And it's
something that's entered our world as AI researchers, very prominently, just in the
last two years. And it's amazing. Of course, we have Prompt Engineering in the context of
large language models. We also have Prompt Engineering in the context of the generative
image models as well, like Dali and so on. And that's really fascinating as well, how by
engineering the Prompt to be just the right sort of thing, you can coax the model into doing
something which you might not otherwise do. And it's a great example of how alien these things
are. Because if you were giving a human being the same instructions, then you wouldn't necessarily
do quite what you do with either an LLM or an image model in order to get it to do the
thing that you want it to do. You have to kind of get into the zone with these models and figure
out kind of what strange incantations are going to make it do the things that you want it to do.
Now, I think an interesting thing is that we may be looking at a moment, a very short moments in
the history of the field where Prompt Engineering is relevant. Because if language models become
good enough, then we're not going to need to talk to them in this weird way,
engineer the Prompt to get them to do what we want them to do. It's going to be a lot easier.
Anyway, so maybe that will be the case. I mean, that makes a lot of sense that that will be the
case as they get better. But at the moment, you can use a strange incantation like thinking steps
and suddenly the large language model will be much more effective on reasoning problems than it
was if you didn't use the incantation thinking steps. So that's really fascinating. So what's
going on there? Well, I mean, I think what's going on there is that we have to again bear in mind that
what the model is really trained to do is next word prediction. But we have to remember that
it's doing next word prediction in this unimaginably complex distribution. So we have to remember
that it's not just the distribution of what a single human would, the distribution of
the sequence of words that a single human will come out with, but of all the sort of text of
millions of humans on the internet, plus actually a load of other stuff like code and
things which we don't come out with in ordinary everyday language. Well, people do it deep
mind a bit, but that's deep. So it's this unimaginably complex distribution. And so I think what's
happening with prompt engineering is that you're sort of channeling it into some portion of the
distribution. So you're queuing it up with a prompt. And this kind of context is putting it
into some portion of this distribution. And that is what's going to enable it to do something
different than it would have done if you had a different set of words. And that would have
put it in a different part of the distribution. So you're kind of finding the bit of this unimaginably
complex distribution. You're finding the bit of it that you want to then concentrate on.
Yeah, so intuitively, I agree, because I think there's two ways of looking at this. So I agree
with you that they are statistical language models. I'm also a fan of the spline theory of neural
networks, which is this idea that you just kind of tessellate the ambient space into these little
affine polyhedra. And it's a little bit like a locality sensitive hashing table. But that's
quite, it's quite a simple way of looking at it, because you were talking about emergence before.
And emergence is all about this paradigmatic surprise, a bit like the mind body dualism,
if you like, there's something that happens up here, which is paradigmatically
completely different to what happens down there. So on the one hand, we're kind of saying, oh,
they're just simple interpolators or statistical models. But on the other hand,
they really are doing something remarkable up here. So, so which is it?
Which is it? Well, I mean, it's both, right? So, so, so, you know, if we want to understand
these models in a in a in a more scientific way, which we surely do, you know, even if we're not,
even if we're not engineering them in an old fashioned sense of engineering them,
but, but rather they kind of, you know, emerge from the, from the learning process,
we still want to reverse engineer them to try and get as great as as as comprehensive
a scientific understanding of these things as possible. So, so we want to understand it all
these levels, right? We, of course, the foundation of that understanding is that we
need to understand the actual mechanisms that we've programmed in there, right? So,
though, you know, so you've that's essential, you want to, you know, if you want to really
understand these things, you've got to understand transformer architectures, the different kinds
of transformer architectures that you've got, the, you know, what happens when you use kind of
different parameter settings, whether it's sparse or dense, whether it's a decoder only
architecture, or how you're doing the tokenization, how you're doing the embedding,
when all of these things are essential to understanding, you know, and that's all at the
absolutely at the engineering level. So we want to understand all of that. But then we can do a
whole load of reverse engineering, you know, at another level, and do the sort of thing that
the people at Anthropic AI have done, for example, with, with these induction heads and, and, and,
and understanding in terms of transformers in terms of residual streams and induction heads,
which I think is fabulous work. So that kind of thing is looking, it's still
quite a low level, but it's kind of the next level up, and explaining a little bit about how these
things work, and work along those lines, I think is like really essential. And then the more complex
these things are, the, you know, the heart, the more we need to kind of ascend these levels of
understanding and, and, and, and, you know, and I hope that we can, but I mean, there's no one
that is the right one. It's, you want to understand that things are all levels.
Yeah, different levels of description. You said something before, which really
interested me. You said when the language models get good enough, maybe we won't need the prompts
anymore. And I'd love to explore that duality, because it's a similar duality to how we talk
about embodiment, you know, you can think of the language model being embodied in the prompt in,
in some sense. So maybe we'll never get rid of the prompt. But just to think about these prompts,
I think about them as a new type of program interpreter. And there are some remarkable
examples of scratch pad and chain of thought and even algorithmic prompting for getting
insane extrapolative performance on lots of, you know, standard reasoning tasks. Yeah, yeah.
And, you know, these, these models are not Turing machines, they're finite state
automatics. So there are limits to what we can do. But I guess what I'm saying is the prompt
seems like it's not going away anytime soon. Yeah. So I think that I don't think the prompt is
going to go away. But I think that the, and who knows, right? But, but I think that prompt
engineering as a whole kind of thing in itself, you know, may, it may not be, you know, people
talk about that as being a kind of a whole new job description as prompt engineer. And so that,
that as a whole new job description, I'm not quite sure how long exactly that will last because,
because prompt prompting may be just, you know, interacting with a thing in a much more natural
language way in the way we would with another person, right? So, you know, I don't, I don't,
when I, when I, I don't have to kind of think of some peculiar incantation in order to,
you know, in order to get, you know, my colleagues to kind of help me on, on something or to, you
know, to cook a meal together with somebody, we just, we just use our natural kind of forms
of communication. Of course, of course, it does involve, you know, discussion and negotiation,
but it's in this, it's just the same as we use with other humans, right? So, so it may be that,
rather than it being a peculiar thing in itself with all these funny phrases that just work
for peculiar eccentric reasons, that it may be much more natural. Amazing. Professor Shanahan,
thank you so much for joining us today. Indeed, and thank you for the invitation. It's been lots
of fun. Absolute honor. Absolute honor. Why?

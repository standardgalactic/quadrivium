He moved to Indiana University in 1989, where he obtained his PhD in philosophy and cognitive
science, working for the legendary Douglas Hofstadter, by the way. I've got his book here
in the Center for Research on Concepts and Cognition. Douglas, by the way, is one of the most
legendary figures in the AI space. We also had the pleasure of interviewing one of his other
PhD students, Professor Melanie Mitchell. Now, David recently wrote this fascinating book called
Reality Plus. In that book, he discussed the three central philosophical questions, actually.
The reality question, which is to say, are virtual worlds real? His answer to that is yes.
The knowledge question, which is to say, can we know whether or not we're in a virtual world?
His answer to that is no. Also, the value question, which is to say, can you lead a good life
in a virtual world? And his answer to that is a resounding yes. Now, you probably also heard
of this notion of an extended mind, something which David formulated with Professor Andy Clark,
and they described the idea as active externalism, based on the active role of the environment in
driving cognitive processes, or put simply, you might think of your phone as being an extension
of your mind, for example. What was it like to work with Douglas during your PhD? Oh, he was great.
He was so interested in so many things. It was officially, it was an AI lab, and most of the
people there were AI researchers. You mentioned Melanie Mitchell. She was my colleague there.
Bob French, who's done important work on AI, Gary McGraw, Jim Marshall, Wong-Pay, and others.
I was the only philosopher. People were interested in so many things, whether it was,
we had workshops on humor, or creativity, on mathematics, on politics, on everything,
as well as all the stuff on AI, analogy, concepts. It was also a very exciting time to be there,
because this was one of the, in the boom and bust cycle of machine learning and of neural
networks. This was one of the boom periods, the early 1990s. The PDP books had just come out,
parallel distributed processing by Rubble Hart, McClelland, and Hinton. There was so much excitement
about the capacities of neural networks. I ended up writing a few papers on machine learning
back then. One was on the evolution of learning systems. One was on getting machines to learn
structural generalizations. All with fairly basic neural networks. A few years after that,
the bottom fell out of neural networks for another 15 years or so, but this was a very
exciting period to be there. One thing about Doug that you don't quite get from his books,
his books, he's so enthusiastic about everything. He's not into all these ideas and so on, but it
turns out there's 5% of things in the world or 1% that he's enthusiastic about. The other 99% he hates.
He's like, he likes certain approaches to AI he loved, but even neural networks, he was like,
he was not a big fan of most research in neural networks back then. He said,
I think it was a, this is a bandwagon or even he said a bad wagon. Likewise for philosophy,
there's bits of philosophy he loves. There's a lot of it. There's a lot of it he doesn't like.
So maybe in person, you get more of a, more of that very opinionated side, but really he's a,
he's just, you know, that book Go to Lesha Bark was just, I mean, it was what drew me into philosophy
and AI as a teenager and it's so rich. And you go back to that book, there's still so many ideas
there. The Mind's Eye is another book he edited with Dan Dennett. It was reading the Mind's Eye
that really first got me thinking about issues about consciousness and the mind-body problem.
So yeah, he's a very rich thinker. Yeah. Well, thank God for Douglas Tostata.
Yeah, I love the Mind's Eye. It has one of the, one of the eeriest stories of all. What was it?
The riddle of the universe and its solution where there's, there's some image or some text that if
you read it, it causes your brain basically to core dump. And so it becomes this infectious thing
where anybody who goes into that office and reads that thing, they just crash and they never,
they go into a coma and they never wake up again. And so, you know, there's a huge investigation
to figure out what's going on and more and more people keep going into comas, you know, because
of this crazy ideas in that book. Could I just pick up on this point because you were saying that
Douglas Tostata turned his nose up at Neural Networks. And I hope he still would actually,
and Melanie Mitchell certainly does. And I mean, Douglas had that paper out called
The Core of Cognition. He was always huge about this idea of the primacy of analogy making.
And actually, there's a few researchers today that are mirroring that, that idea that Francois
Chollet, for example, he says intelligence is literally sensitivity to abstract analogies.
It's not memorizing the internet. So I think, thank God that we do have people out there
thinking slightly differently because the modus operandi today is that we need to build a big
hash table of everything. And even the way we formalize intelligence, as we were just saying
previously, we're not really paying attention to why or how the things do what they do. We're
just looking at the behavioral output bit like the Turing test, as long as it looks and smells
like a duck, then it must be a duck. So thank God for Douglas Tostata. Yeah, I think he's always been
at the same time an enthusiast about the possibility of AI while being somewhat skeptical
about the capacities of existing AI and about the kind of hype that suggests that AI might just be
10 or 20 years around the corner. So in the early 90s, I think, yeah, that was,
that was especially, especially natural. Back around then, people would say a year spent working
in AI is enough to make you believe in God. It was so hard to get anything even like
any kind of intelligence out of an AI system. And I think, you know, Doug was equally skeptical
of both the symbolic and the connectionist on neural network approaches back then.
I think ultimately his sympathies lay with it on the neural network side of things,
with the idea that intelligence could in principle bubble up from a million, you know,
from 100 billion separate little interactions, intelligence could bubble up from there. But
I still think he'd be inclined to think that current approaches are too statistical, too simple,
and so on. That said, you have to look at the, at the progress in machine learning over the last
10 years. And it's been amazing and surprising. And I think even somebody like,
even people like Melanie or like Doug are going to have to say this has been
something they did not expect. And that they did not predict. So I actually, I was back in
Indiana just over two years ago, just before the pandemic got going February 2020. And
I don't know, maybe that was before GBT three, but still there've been all these amazing
developments in machine learning over the last few years. I asked Doug about this,
and what do you make of this? And because he's on the record of saying, you know,
there will be AI eventually, but it's going to have to be involve all these new kinds of complexity,
not not something simple like this. And he says, Yeah, well, this is, this is troubling.
This is concerning, you know, it could be, I don't know yet, but it could be that I was wrong.
It could be there are simpler passes, paths to AI. And his attitude was that would be very
disappointing. It turned out that you could actually train up an AGI, just using those
simple methods to human levels. That would make, I think Doug's view was that would make kind of
human level intelligence less, less grand and remarkable than, than he had thought. Well,
actually, back in, back in Goethe-les-Sherbach, I think he said that even to have a machine that
could beat a human at chess, it would have to be good at everything would be a good composer,
it could tell jokes and so on. Okay, that one, that view got rolled out back in the,
back in the 90s. So the question is, you know, is this, is this ever growing progress of just,
of the kind of machine learning that says just throw a whole lot of compute,
and a whole lot of data at it, and see what happens. Is that eventually going to get us to
human level intelligence? Or is it, is it just going to get us so far
with, and there's going to be principled limitations? I've always been on the side of,
they'll probably only get us so far. But I have to say those principled limitations,
those obstacles that have not yet been conquered are getting smaller and smaller. And the progress,
if the progress of the last five or 10 years continues for another five or 10 years,
then who's to say what's going to be left? Yeah, there was a fascinating anecdote in,
in Melanie's book about how she and Hofstadter went to the Googleplex one time. And basically,
as you said, Douglas was terrified that intelligence might be disappointingly simple to
mechanize because he felt of the mind of Chopin as being infinitely nuanced. And just,
just the incredible process that must have gone through his mind when he, when he produced his
music. But I wanted to, and just quickly, by the way, you said that there was a conception in,
in the 70s that task specific skill was what was required for intelligence or a collection of,
of specific skills. And, and now the mindset is much more towards task acquisition efficiency
and generalization. But I wanted to just quickly pick you up on the so-called intelligence
explosion question. So this is a subject which Nick Bostrom has popularized after his book,
Superintelligence. Personally speaking, we're not particularly sympathetic to this view. And
Saigre Francois-Labe, he said in a blog post recently that this line of reasoning represents
a misunderstanding of intelligence. He said that in his opinion, intelligence is situational.
He said that our environment puts a hard limit on our individual intelligences. He said that
most of our intelligence is not in the brain, it's externalized as civilization. And that an
individual brain cannot implement recursive intelligence augmentation like a Godel machine.
He also said that there are already many examples of recursively self-improving systems.
Even personal investing, for example, is a recursively self-improving system. The more
money you have, the more money you make. Anyway, so Bostrom described a thought experiment in 2003.
I'm sure you've heard of this. The scenario describes an artificial, you know, like a very
advanced artificial intelligence task with manufacturing paper clips. If such a machine
were not programmed to value human life, then given enough power over its environment,
it would try to turn all the matter in the universe, including human beings,
into paper clips or machines which could manufacture paper clips. Do you think we might
be on the precipice of being turned into paper clips, as Bostrom famously described in his
thought experiment? Yeah, look, it's there's two different issues here. One is, will we get to
some kind of much greater than human superintelligence relatively soon by some kind of intelligence
explosion process? And second, if that happens, are there major dangers around? Yeah, I wrote
about both of these things back in 2009. I had an article called, yeah, the Singularity
of Philosophical Analysis, where I tried to take this line of reasoning for an intelligence
explosion through recursive, through basically through recursive design of ever more sophisticated
AIs. I tried to take that and turn it into an argument. I mean, the classic statement of this
comes from I.J. Goode, the statistician and philosopher back in 1965 on the design of an
ultra intelligent machine where he puts the basic idea right there that once you've got a machine
which is smarter than a human, it will be able to design a machine which is smarter still,
and then you're going to get recursive, runaway explosion of intelligence. I tried to analyze
that to set out that article, that argument in as much detail as I could, analyze where it could
go right, where it could go wrong, what the possible obstacles would be, and it's a long story.
If anyone wants to look it up, it's out there on my website. But I in the end became convinced
this is a pretty powerful argument. There's only so many ways it could go wrong. I think it's
important that not every recursive augmentation process is going to lead to an intelligence
explosion. It could easily bottom out, could asymptote before human intelligence. But I do
think that once we start from greater than human intelligence, we have to find some way to get
to greater than human intelligence first. This explosion won't get you that. But once you get
there, then there's pretty good reason to think things in principle can take off from there.
If intelligence is extended, I'm a big fan of the idea that intelligence is extended into the
environment. But as far as I can tell, all that can in principle be augmented too. We develop
extended systems, which are smarter than humans, and then they'll be able to design even better
extended systems. And we could then have an intelligence explosion of extended intelligences.
So I'm actually, nothing about this gets you to human level intelligence. But once we get to human
level intelligence and a little bit beyond, then I think there's a pretty good case that there's some
kind of potential explosion in the offing. Then the other issue you mentioned Bostrom and the paper
clips is, yeah, what does this mean for the future of humanity? I guess I don't know what I'd say
about the probabilities, but I'd say, yeah, once you have greater than human artificial general
intelligence, then there's many ways that can go wrong for the obvious reasons that such a being
is going to be extremely powerful. The most intelligent beings in the universe tend to be
the most powerful for obvious reasons. Whatever they want, they have the capacity to get. So it's
going to be extremely important that our AGI systems want the right things. That is, they have
the right kind of goals. Or as people put it, these days that they are aligned with human goals.
Because if they're even a little bit misaligned, then there's going to be the capacity for things
to go very badly wrong. I know there are some people who think that the alignment is going to
have to be so precise that, you know, missed by just the tiniest bit and will destroy the universe,
whereas others think it's extremely robust. It may be more robust than that. I'm not
totally sure about that. But I'm certainly on the side of people who think we have to take this
issue extremely seriously. And there is at least potential existential risks here that if AGI is
produced in an unthinking way, perhaps say in a military or a financial context where there's
an AI arms race, and we suddenly have greater than human AIs that can achieve arbitrary goals,
then suddenly it becomes an extremely sensitive matter what their goals are. So I'm certainly on
Bostrom's side when it comes to, yeah, this is something we should take seriously.
But isn't there a bit of, you know, it's a big distance to go from
superior to human intelligence and achieve anything you want. I mean, I'm relatively
intelligent, but I can't achieve flight, you know, by myself without, you know, apparatus to do that
and airplane wings, whatever. I mean, there are physical limitations in the world. And I think
sometimes there's this assumption that intelligence can kind of go to infinity,
where in fact, maybe intelligence itself kind of bottoms out at IQ 1000 or something, there's
just not much, you know, you can do beyond that certain IQ. I mean, isn't there a degree of
kind of speculative, you know, extrapolation that we need to account for there?
I would say this is certainly one way that things could, that the argument could fail,
is if it turns out that basically there are diminishing, there's some kind of intelligence
ceiling, and there's some kind of diminishing returns towards this. Just there is such a
ceiling that we might find that when we make a being which is 10% smarter than us on some scale,
it could only make a being which is 5% smarter than it. And that being will make a machine,
make a being which is only 2.5% smarter than it. And all this will kind of asymptote to some
intelligence ceiling. And I don't know, this turns on very subtle issues about the structure
of intelligence space. I'm rather doubtful there is such an intelligence ceiling, or if there is
one, maybe it's something like, you know, the limits of computability compared to, you know,
hypercomputation that an infinite system could do. But I think that ceiling is so high that there's
room for an awful lot of super intelligence before we get there. But in any case, I would say that,
you know, for the purposes of, say, caution and thinking about the future, I would just turn the
point back on you and say that the thought that there is such an intelligence ceiling is itself
an extremely speculative one. I wouldn't want to rely on this, on this extremely speculative thought
to kind of protect us from, from the, you know, potential risks of AGI in the future. If there's
only a 20% chance there's not such an intelligence ceiling, then this is something that we very
much need to be, to be worrying about. Yeah, I mean, fair enough, it's certainly a risk factor.
It's certainly something that we need to need to keep a handle on. Well, let me ask you about one
specific time there. So I'm thinking you're probably familiar with Carl Friston and, you know,
his free energy principle. And he sends his regards, by the way, we talked to him a couple
weeks back. And, and he wanted to ask you about kind of one line of thinking that he's been exploring
lately. And I want to give you a quote from his 2018 article and my self conscious,
or does self organization entail self consciousness. And he said, the proposal on offer here
is that the mind comes into being when self evidencing has a temporal thickness,
or counterfactual depth, which grounds inferences about the consequences of my action.
On this view, consciousness is nothing more than the inference about my future, namely,
self evidencing consequences of what I could do. What do you think about that, that perspective?
Yeah, I'd have to know more about the connection to consciousness. I know that yeah, Friston is
very has developed very deeply the idea of the mind as a prediction machine, a mind which is
basically set up to, you know, predict whatever signal is coming next. And that's with that one
basic key loss, you know, predict what's next, what's next, what's next, then you get to build these
amazing models of the world with all of these, all of these, these capacities. And that's a
really interesting perspective thinking about the mind and intelligence in general. And it's
got to be at least one huge part of the story, even if it's not the whole story as Friston thinks
it is, but I've never really understood the distinct what this kind of predictive approach has to say
distinctively about consciousness. Because presumably there's a whole lot of different
predictive processes at all kinds of levels of the hierarchy, including at the very early vision
and very late cognition, and the whole mind is engaged in coming up with these predictions,
but only some limited part of it is conscious. What you just said about, yeah, trying to figure out
the predictions consequent on our actions, sounds to me like a very general statement of
what the predictive approach says about the mind in general. And I haven't yet heard what is the
part that corresponds to consciousness. Why, for example, or some representations get to be
conscious where so much of it in the brain is not, I can give you I can give you a bit more
detail, which may be helpful, because we did dig into him with a on a bit. And he said, for one thing,
he expected that perhaps part of your response might might entail or talk about the meta hard
problem. You know, why is it that certain beings, i.e. things like philosophers, and people like you
and me puzzle so much about our qualitative experience. And the argument he makes there,
he says that if we are inference machines that are built to actively self evidence,
then that necessarily entails we need to have a generative model about our experienced world.
And if we have that that generative model about our experience world, our experience world,
then we have to entertain the hypothesis that we are things having a qualitative experience,
along with the alternate to that hypothesis, which is that we're not having qualitative
experiences. And so essentially that the capability to model the world generatively
really requires that we entertain this hypothesis that we're actually having qualitative experiences
or maybe not. And that's why we pontificate about it. Yeah, it's interesting. And I think the meta
problem is a, yeah, as a promising approach is the meta problem is your why do we say and think
the things we do about consciousness, instead of explaining consciousness directly,
let's explain, you know, our internal model of consciousness. And yeah, there's got to be
such a model. So I think this is a promising approach to take. I still don't fully, I mean,
I think if you take the predictive approach, so what you would expect is, is the system would have
many different models, you know, a big complex model of the world at all levels, it doesn't
just correspond to experienced reality, but the models the world way beyond what's experienced,
it would also you'd also expect the model to have a model of the mind to have a model of ourselves
and relation to the world. But what actually happens in the in the human mind is we have,
we have models at all levels, you know, there's like so many different levels of say of
representation, even in the visual hierarchy. And somehow, though, only one of those
levels seems to correspond to consciousness. The question is, why now do we need a distinctive
model of those representations in us, which correspond to conscious experience? One idea,
I think, one idea I quite like is that this could be like a simplification. In fact,
we have millions of layers of representation of the world. But to build all that into our model
of ourselves, and our relation of the world is going to be too complex. So we basically,
we oversimplify by saying, ah, there's this one special relationship we have to the world,
we call it consciousness or experience. And yet we experience certain things and then we use them
to reason about them. And this is massively oversimplified as a model of the mind. But it could
be that that simplification is then what actually gives us the sense that we have this special
thing called consciousness. At least maybe that could explain why it seems to us that we have
some special representations of the world. It's a further question why those conscious
representations should seem to be so ineffable and subjective and hard to explain. And what
in what Carl has written about this, I think he and Andy Clark had some ideas about the meta
problem to try and push on this. Maybe that maybe there'd be certain representations that
we'd have to be especially certain that we have them. Maybe that would give rise to
the Descartes idea that, well, I'm not sure about the world, but I know that I'm thinking.
I think, therefore, I am. And they had some kind of story about how this could get the whole,
I think, therefore, I am certainty in one's own mind going. Anyway, I think it's an interesting
approach and I'll be very cool to see if they can develop it further.
Fascinating. I wanted to dig into this modeling thing. I was even thinking a second ago when
you were talking about intelligence, that straight away you did the Hutter thing and
we're talking about agents performing in environments and so on. And even that is a model.
And of course, we're talking about complex phenomena and the way we model things depends
on the level of analysis. But I'm really fascinated by this idea that some phenomena is so complex
that it cannot be formalized or communicated, almost as if there's a representation problem.
Now, you discussed in your consciousness book whether consciousness itself could be
reductively explained and your knowledge argument, you spoke of this neuroscientist Mary
that had been brought up in a black and white room. She's never seen any colors except for
black and white and shades of gray. She's nevertheless one of the world's leading
neuroscientists specializing in neurophysiology of color vision. She knows everything there is
to know about neural processes involved in visual information processing, about the physics of
optical processes, about the physical makeup of objects in the environment. But she doesn't know
what it's like to see red. No amount of reasoning from physical facts alone will give her this
knowledge. Physical facts about systems do not tell us what their conscious experiences are like.
Now, you're speaking about this phenomenon in respect of the conscious or the phenomenological
experience. But I think it's a much bigger problem of representation with any complex system, right?
So what I find fascinating is that all of us have a conscious experience, but it's completely
ineffable, as you just said, it's impossible for us to communicate it to others. And whenever
we try to do so, we're reaching, right? Just like the blind men in the elephant, we end up defining
some weird abstract motif, right? Chopping off 90% of the truth. The thing that fascinates me is
that we need to have some kind of formalism or reduction in order to communicate, you know,
in order to know or even understand anything. But so often is the case that all of the nuance
and richness of the phenomena is lost in doing so. So I suspect that any formalism of a complex
system might blind us from discovering a much better and richer formalism later because it
kind of frames our thinking in quite a pernicious way in your book. So as I said, you were trying
to separate the phenomenological experience as something that couldn't be described. But do
you think it could be extended to any complex system? Well, we don't. As far as we know,
you know, some complex systems actually have conscious subjective experience. But, you know,
most of them don't. You know, this Mac that I'm using right now is a very complex system,
but not much reason to think that it's conscious despite the complexity of what's going on
within it. So certain kinds of complexity go along with consciousness. But if we were to kind of
return to that meta problem approach for a moment, maybe there are certain kinds of properties
of a complex system that tend to produce reports, for example, that the system
is conscious. So maybe some complex systems have the capacity for a certain kind of direct
self-modeling that corresponds to what we think of as introspection. We have introspection,
which is a way of saying, this is what I'm perceiving right now. This is what I'm thinking
right now. This is what I'm feeling right now. And we build a model of ourselves,
and it may well be that that model is highly oversimplified. You don't have access
to all these facts about ourselves. So perhaps you could tell a story where the kinds of complex
systems that give rise at least to this capacity for introspection are then at least going to report
themselves as being conscious. And maybe that could get at some element, maybe sort of the
ineffability of consciousness. You'd expect to build these very simplified self-models. We wouldn't
know immediately how to extend to other people. I mean, I still think, in principle, you could
take Mary, who knows all about the human brain, and she could come to know all about those models
in other people. But it still seems that she's never actually experienced red for herself.
There's still something really crucial about this objective experience that she doesn't know.
She doesn't know what it's like to experience red, and knowing all about the details of the model
still hasn't told her that. So I think that's still something that needs explaining. Some people at
this point just say that sense of something extra is an illusion. Something extra that the model
hasn't explained is an illusion. But that's really where a lot of the action is at then.
Just quickly, do you think there could be a sense of something extra to intelligence,
as well as consciousness? Probably, yeah, we model our own intelligence with massively
oversimplified self-models that were programmed into us by nature that model us as these agents
with incredible capacities, free will, rationality, reason. It probably, again, it will, yeah, maybe
I talked about consciousness is involving the subjective elements, intelligence is involving
the objective elements. But yeah, we probably have oversimplified models of those conscious
of those behavioral elements as well, perhaps that make us out to be more rational, or more free,
or more capable than we actually are. I wanted to ask, what is an interesting simulation? Is
our universe interesting or not? Because we represent just a pinprick of intelligence. So
should intelligence be more spread out in the eyes of the simulator? Or in the vast majority of
instances, would there just be gas everywhere or a singularity? Maybe stars can't form.
Maybe the interesting phenomena itself is on the boundary between chaos and order,
or between order and disorder, I should say, which is just a tiny sliver. So what do you think
makes an interesting simulation? I don't know. I think it probably depends on your perspective,
and it might, for example, depend on the perspective of the simulators, what they're after. One thing
that a simulator might be doing is just create a whole lot of different universes with different
potential laws of physics that they're simulating just to see what happens. And maybe if they're
interested in, say, life or intelligence, then it could be that they're going to find that, okay,
well, 99% of these simulations don't produce anything like life or intelligence. And yeah,
1% of them produce life, and 0.01% lead to intelligence. So if that's what they're interested in,
in studying, fantastic. But they might be interested in who's to say the laws of physics or
galaxy formation, more generally, totally independent of life and intelligence. So I don't
think there's any single standard of what's interesting. I mean, to me, as a philosopher
interested in consciousness, I'm especially interested in this question of what kinds of
simulations might actually develop conscious beings within them, not least because that's
going to be especially relevant to our situation. If we're in a simulation, it seems we're conscious.
So there's a question about just how this kind of simulation might get set up. But I think this
whole, I mean, already simulations are used in actual practice for a million different purposes
by scientists studying this phenomenon or that phenomenon, by people doing entertainment, by
people doing prediction of the future, by people doing simulation of the past. And I guess when
it comes to simulated universes, all of those sources of interest may themselves be present.

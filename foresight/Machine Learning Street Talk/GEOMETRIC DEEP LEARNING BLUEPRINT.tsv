start	end	text
0	4480	So, you're talking about the symmetries in data.
4480	9680	Could these analogies also be represented using the kind of symmetries that you're talking about?
9680	11680	Good question. Let me think of it a bit.
14160	18720	Modern machine learning operates with large, high-quality data sets,
18720	21840	which together with appropriate computational resources,
21840	28560	motivates the design of rich function spaces with the capacity to interpolate over the data points.
28560	33840	Now, this mindset plays well with neural networks since even the simplest choices of inductive
33840	42720	prior yield a dense class of functions. Now, symmetry, as wide or narrow as you may define its
42720	50800	meaning, is one idea by which man through the ages has tried to comprehend and create order,
51440	58480	beauty, and perfection. And that was a quote from Hermann Weil, a German mathematician
58480	65200	who was born in the 19th century. Now, since the early days, researchers have adapted neural
65200	70720	networks to exploit the low-dimensional geometry arising from physical measurements,
70720	78320	for example, grids in images, sequences in time series, or position and momentum in molecules
78400	82400	and their associated symmetries, such as translation or rotation.
83280	89200	Now, folks, this is an epic special edition of MLST. We've been working on this since May of this
89200	93680	year, so please use the table of contents on YouTube if you want to skip around. The show is
93680	99360	about three and a half hours long. The second half of the show, roughly speaking, is a traditional
99360	104320	style MLST episode, but the beginning part is a bit of an experiment for us, maybe a bit of a
104320	109840	departure. We want to make some Netflix-style content, and we've even been filming on location
109840	114320	with our guests, so I hope you enjoy the show and let us know what you think in the YouTube comments.
115120	120320	Many people intuit that there are some deep theoretical links between some of the recent
120320	125920	deep learning model architectures, particularly the ones on sets, actually. And this may be why
125920	131520	so many popular architectures keep getting reinvented. Now, the other day, Fabian Fuchs from
131600	136320	Oxford University released a really cool blog post about deep learning on sets,
136320	141680	elucidating a math-heavy paper that he co-authored with Edward Wagstaff et al.
142320	148640	Now, he wanted to understand why so many neural network architectures for sets resemble either
148640	155920	deep sets or self-attention, because sets come in any ordering. There are many opportunities to
155920	162160	design inductive priors to capture the symmetries. So, raises the question,
162800	168160	how do we design deep learning algorithms that are invariant to semantically-equivalent
168160	174960	transformations while maintaining maximum expressivity? Now, Fabian pointed out that
174960	181200	the so-called Genosi-pooling framework gives a satisfying explanation. Genosi-pooling is when
181200	186640	you generate all of the k-tuples of a set, an average over your target function, on those
186640	192640	permutations. It gives you a computationally tractable way of achieving permutation invariance.
193200	199600	So, rather than computing n factorial combinations of the examples, you compute n factorial divided
199600	206640	by n minus k factorial, which for small k is very tractable. Now, clearly, setting k to n
206640	212560	gives you the most expressive yet the most expensive model, but that would be cool. It would
212560	217840	model the high-order interactions between the examples, but it turns out that deep sets are
217840	224320	this configuration with k equals 1, and self-attention is this configuration with k equals 2.
225280	231440	Now, Fabian also spoke about approximate permutation invariance, which is when you set k to n,
232080	236000	the number of examples, but you sample the permutations. It turns out you don't have to
236000	240000	sample very many of them to get good results. But anyway, if you want to check out that in a
240000	244640	little bit more detail, go and check out Fabian's blog. I've put a link in the video description.
245280	250800	High-dimensional learning is impossible due to the curse of dimensionality. It only works if we
250800	255600	make some very strong assumptions about the regularities of the space of functions that we
255600	260640	need to search through. Now, the classical assumptions that we make in machine learning
260720	266400	are no longer relevant. Now, in general, learning in high dimensions is intractable.
266960	271920	The number of samples grows exponentially with the number of dimensions. The universal function
271920	278240	approximation theorem popularized in the 1990s states that for the class of shallow neural
278240	284320	network functions, you can approximate any continuous function to arbitrary precision by
284320	289280	just stacking the neurons, assuming that you had enough of them. So it's a bit like kind of
289280	295680	sparse coding, if you like. The curse of dimensionality refers to the various phenomena that arise
295680	300880	when analyzing and organizing data in high-dimensional spaces that do not occur
300880	306640	in low-dimensional settings, such as the three-dimensional physical space of everyday experience.
307280	312080	Now, the common theme of these problems is that when the dimensionality increases,
312080	318240	the volume of the space increases so fast that the available data effectively becomes sparse.
318320	323440	This sparsity is problematic for any method that requires statistical significance. Now,
323440	328720	in order to obtain a statistically sound and reliable result, the amount of data needed
328720	334960	to support the result often grows exponentially with the dimensionality. Most of the information
334960	341840	in data has regularities. Now, what this means in plain English is, just like on a kaleidoscope,
341840	346960	most of the information which has been generated by the physical world is actually redundant,
346960	353680	just many repeated semantically equivalent replicas of the same thing. The world is full
353680	360400	of simulacrums. Machine learning algorithms need to encode the appropriate notion of regularity
360400	365840	to cut down the search space of possible functions. You might have heard this idea referred to
365840	372000	as an inductive bias. Now, machine learning is about trading off these three sources of error,
372000	376640	statistical error from approximating the expectations on a finite sample,
376640	382560	and this grows as you increase your hypothesis space. Approximation error, which is how good
382560	388000	is your model in that hypothesis space? If your function space is too small, then the one that
388000	393920	you find will incur a lot of approximation error. And finally, optimization error, which is the
393920	399520	ability to find a global optimum. Now, even if we make strong assumptions about our hypothesis
399520	404160	space, you know, we should say that it should be lip sheets or in plain English, it should be
404160	409840	locally smooth. It's still way too large. We want to have a way to search through the space to get
409840	416080	anywhere. So the statistical error is cursed by the dimensionality. If we make the hypothesis or the
416080	421920	function space really small, then the search space is smaller, but the approximation error is cursed
421920	428400	by dimensionality. So we need to define better function spaces to search through. But how?
429360	433200	We need to move towards a new class of function spaces, which is to say,
433840	440000	geometrically inspired function spaces. Let's exploit the underlying low dimensional structure
440000	444880	of the high dimensional input space. The geometric domain can give us entirely new
444880	450960	notions of regularity, which we can exploit. Now using geometrical priors, which is to say,
450960	456480	only allowing equivariant functions or ones which respect a particular geometrical principle,
456480	460000	this will reduce the space of possible functions that we search through,
460000	465280	which means less risk of statistical error and less risk of overfitting. We should be able to
465280	470320	do this without increasing approximation error, because we should know for sure that the true
470320	476080	function has a certain geometrical property, which will bias into the model. So introducing the
476080	482880	geometrical deep learning proto book. So recently, Professor Michael Bronstein, Professor Joanne Bruner,
482960	490400	Dr. Tako Kohen and Dr. Peta Velichkovich released an epic proto book called Geometric Deep Learning,
491200	498720	Grids, Groups, Graphs, Geodesics and Gages. These researchers are elegantly linking classical
498720	504160	theory and machine learning and geometry and group theory to deep learning, which is fascinating.
504800	510400	Now the proto book is beautifully written, right? It's presented so well, it even has some helpful
510400	515520	margin notes. And honestly, I could read sections of it out loud on MLST with scant need to change a
515520	519200	single word. It's that well written. I mean, it is, there's a lot of maths in there as well,
519200	524720	let's be honest. I can't dodge that. But I often try to impress upon people that if your writing
524720	528560	sounds weird when you say it out loud, then you're probably writing it the wrong way. But these guys
528560	534720	have written it really well. Now they've essentially created an abstraction or a blueprint, as they call
534800	538960	it, which prototypically describes all of the deep learning architectures,
538960	544560	the geometrical priors that they have described so far. They don't prescribe a specific architecture,
544560	550000	but rather a series of necessary conditions. The book provides a mathematical framework to study
550000	556320	this field. And it's essentially a mindset on, you know, how to build new architectures. It gives
556320	563360	constructive, you know, procedures to incorporate prior physical knowledge into neural architectures.
563360	569120	And it provides a principled way to build future architectures, which have not yet been invented.
569120	573520	The researchers have also recently released a series of 12 brilliant lectures
573520	576640	on all of the material in the book. And I've linked these in the video description.
577360	582240	Now, what are the core domains of geometric deep learning? So in geometric deep learning,
582240	588720	the data lives on a domain. This domain is a set. It might have additional structure,
588720	593200	like a neighborhood in a graph, or it might have a metric such as, you know, what's the
593200	598640	distance between two points in the set. But most of the time, the data isn't the domain itself.
598640	604800	It's a representation or it's a signal, which is on a Hilbert space. Let's talk about symmetries.
604800	609920	Symmetries are really important to understand this framework. So a symmetry of an object
609920	615600	is simply a transformation of that object, which leaves it unchanged. Now, there are
615600	619760	many different types of symmetries in deep learning. I mean, for example, there are symmetries
619760	624400	of the weights. If you take two neurons in a neural network, and you swap them,
624400	629120	the neural network is still graph isomorphic. There are symmetries of the label function,
629120	633840	which means that an image is still a dog, even if you apply a rotation transformation to it.
634640	639600	Actually, if we knew all of the symmetries of a certain class, we would only need one labeled
639600	643760	example, right, because we would recognize any other examples that you give it as kind of
643760	648320	semantically equivalent transformations. But we can't do that, right, because the learning problem
648320	653520	is difficult, which means we don't actually know all of the symmetries in advance. Now,
653520	658480	in the context of geometric deep learning, we talk about symmetries of the core structured
658480	664960	geometric domains that we're interested in. So grids or graphs, for example, a symmetry is any
664960	670240	transformation which preserves the structure of the geometric domain that the signal lives on.
670240	676560	So for example, permutations of a set preserves the set membership or Euclidean transformations
676560	682800	like rotations or reflections preserve distances and angles. There are a few rules to remember,
682800	689200	because the way we deal with this paradigm is we talk about how composable those symmetries are.
689200	694160	The identity transformation is always a symmetry. Composing a symmetry transformation is always
694160	699200	a symmetry. The inverse of a symmetry is always a symmetry. We can formulate this with this
699200	704160	mathematically abstract notion of a group. Group theory in mathematics is fascinating,
704160	710160	because it concerns only with how elements compose with each other, not what they actually are.
710160	713920	So different kinds of objects may have the same symmetry group. For example,
713920	721120	the group of rotational and reflection symmetries of a triangle is the same as the group of permutations
721120	727200	of sequences of three elements. So let's talk about the blueprint itself. The blueprint has
727280	736000	three core principles. Symmetry, scale separation and geometric stability. In machine learning,
736000	741680	multi-scale representations and local invariance are the fundamental mathematical principles
741680	746480	underpinning the efficiency of convolutional neural networks and graph neural networks.
746480	751680	They are typically implemented in the form of local pooling in some sense. Now these principles
751680	756320	give us a very general blueprint of geometric deep learning that can be recognized in the
756320	762960	majority of popular deep neural network architectures. A typical design consists of a sequence of
762960	769440	locally-equivariant layers. I mean, think of the convolution layers in a CNN, then a pooling or
769440	775200	a coarsening layer. So you recognize those in CNNs as well. And finally, followed by a globally
775200	779280	invariant pooling layer. So that might be your classification head. Now these building blocks
779280	784880	provide a rich approximation space, which have prescribed invariance and stability properties
784880	789920	by combining them together into a scheme that these researchers refer to as the geometric
789920	796800	deep learning blueprint. Now the researchers also introduced the concept of geometric stability,
796800	802560	which extends the notion of group invariance and equivalence to approximate symmetry or
802560	808240	transformations around the group. They quantify this in some sense by looking at a metric space
808240	813200	between the transformations themselves. This is Professor Michael Bronstein.
813840	819600	The problem is that traditional machine learning techniques work well with images or audio,
820160	825600	but they are not designed to deal with network structure data. In order to address this challenge,
826160	833360	we've developed a new framework that we call geometric deep learning. It allowed us to learn
833360	840640	the network effects of clinically approved drugs and to predict anti-cancer drug-like
840640	848240	properties of other molecules. For example, molecules contained in food. Neural networks
848240	853520	have exploded, leading to several success stories in industrial applications. And I think it's
853520	858640	quite indicative that last year two major biological journals featured geometric deep learning papers on
858640	863680	their cover, which means that it has already become mainstream and possibly will lead to new
863680	869200	exciting results in fundamental sciences. The book I hold is called The Role to Reality. It's
869200	876320	written by a British mathematician and recent Nobel laureate, Roger Penrose, a professor at Oxford,
876880	884480	and it's really probably one of the most complete attempts to write and describe modern physics
884480	890960	and its mathematical underpinning. And you can see it's very heavy. But if I were to compress the
890960	896960	thousand-plus pages of this book into just a single concept, I can capture it in one word.
896960	903440	And this is symmetry. And symmetry is really fundamental concept and fundamental idea that
903440	912000	underpins all modern physics as we know it. So, for example, the standard model of particle physics
912000	917920	can entirely be derived from the considerations of symmetry. And that's the kind of idea that
917920	925360	we try to use in deep learning to derive and create new neural network architectures entirely from
925360	931040	fundamental concepts and fundamental principles of symmetry. In the past decade, deep learning has
931040	935440	brought a revolution in data science and made possible many tasks previously thought to be
935440	941200	unreached. On the other hand, we now have a zoo of different neural network architectures for
941200	946720	different types of data, but few unifying principles. The authors also point out that
946720	953200	different geometric deep learning methods differ in their choice of domain or symmetry group or the
953200	958400	implementation specific details of those building blocks that we spoke about. But many of the deep
958400	964720	learning architectures currently in use fall into this scheme and can thus be derived from common
964720	969680	geometrical principles. As a consequence, it is difficult to understand the relations between
969680	975200	different methods, which inevitably leads to the reinvention and rebranding of the same concepts.
975200	980320	So, we need some form of geometric unification in the spirit of the Erlangen program that I call
980320	985520	geometric deep learning. It serves two purposes. First, to provide a common mathematical framework
985520	990640	to derive the most successful neural network architectures. And second, to give a constructive
990640	995520	procedure to build future architectures in a principled way. This is a very general design that
995520	999760	can be applied to different types of geometric structures such as grids, homogeneous spaces
999760	1004960	with global transformation groups, crafts and manifolds where we have global isometry invariants
1004960	1011120	as well as local gauge symmetries. We call these the 5G of geometric deep learning. The
1011120	1015680	implementation of these principles leads to some of the most popular architectures that exist today
1015680	1020880	in deep learning, such as convolutional networks emerging from translational symmetry, craft neural
1020880	1026240	networks, deep sets and transformers implementing permutation invariants and intrinsic mesh CNNs
1026240	1031760	using computer graphics and vision that can be derived from gauge symmetries. People are quite
1031760	1037440	cynical about the interpolative nature of deep learning and I think that finding this structure,
1037440	1044160	this deeper structure could allow us to extrapolate in a way which is significantly better than we
1044160	1050640	can now. And I asked whether he thought deep learning could get us all the way to artificial
1050640	1055760	general intelligence? It's a hard question because it has several terms that are not well defined.
1055760	1060240	What do you define by intelligence? So we don't understand what is human intelligence. Everybody
1060240	1065680	probably gives a different meaning to this term. So it's hard for me to even to define and quantify
1066320	1071440	artificial intelligence. I don't think that we necessarily need to emulate human intelligence
1071440	1077440	and, as you mentioned, in the past we thought of artificial intelligence as being able to solve
1077440	1081680	certain tasks and it's a kind of a moving target. We thought of, I don't know, playing intelligent
1081680	1086880	games or perception of the visual world like computer vision or understanding and translating
1086880	1092480	language or even creativity and today we have machine learning systems that are able to address
1092480	1097840	at least to some extent all of these tasks, sometimes even better than humans and are we there yet
1098800	1103680	artificial intelligence? I don't think so. And probably artificial intelligence will look
1103680	1107440	differently from human intelligence. It doesn't need to look like human intelligence. It's of
1107440	1113200	course an interesting scientific question whether we can reproduce a human in silico, but for solving
1113200	1118720	practical problems that will make this technology useful for the humanity, for the humankind,
1119680	1124640	we probably need something different. It will certainly involve a certain level of abstraction
1124640	1128640	that we currently don't have. It will probably require methods that we currently don't have,
1129600	1135840	but it doesn't necessarily need to look like a recreation of a human. This is Dr. Petar Velichkovich.
1136560	1141040	By now you'll have probably seen or heard something about our recently released proto book on
1141040	1146560	geometric deep learning on grids, graphs, groups, geodesics, and gauges, or as we like to call it
1146560	1151440	the 5Gs of geometric deep learning, which I've co-authored alongside Michael Bronstein,
1151440	1156320	John Brunner, and Taco Cohen. And you might be wondering what all the fuss is about, because
1156320	1161680	there's already a lot of really high quality synthesis textbooks on the field of deep learning
1161680	1165760	in general, and also on some sub areas of geometric deep learning, such as graph neural
1165760	1170880	networks, where Will Hamilton recently released a super high quality textbook on that area.
1170880	1178320	This is Dr. Taco Cohen. So what we've been trying to do in our book project is to show
1178320	1184960	that this geometric deep learning mindset is not just useful when tackling a new problem,
1184960	1190720	but actually allows you to derive from first principles of symmetry and skill separation,
1190720	1196080	many of the architectures and architectural primitives like convolution, attention,
1196080	1201680	graph convolution, and so forth, that have become popular over the last few years.
1202160	1207440	Even in cases where these considerations of active variance and skill separation
1207440	1211600	were not felt at center when the methods were first discovered.
1212720	1219200	Now we think that this is useful for a number of reasons. First of all, it might help to avoid
1220080	1225600	reinventing the same ideas over and over. And this can easily happen when the number of papers
1225600	1232880	that come out every day is far larger than what any one person can possibly read,
1233760	1240400	and when different sub fields use different language to describe their ideas. Furthermore,
1240400	1246800	it might help to clarify when a particular method is useful. A geometric deep learning method is
1246800	1252560	useful when the problem domain has the particular symmetries that are built into the architecture.
1253360	1261280	And finally, we hope that by making explicit the commonalities between seemingly different
1261280	1267760	methods, it will become easier for new cars to learn geometric deep learning. Ideally,
1267760	1273840	one would not have to go through the large number of architectures that have been designed,
1273840	1277840	but just learn the general ideas of groups,
1277920	1283760	equivariance, group representations and feature spaces and so on, and then see,
1283760	1288800	for the particular instances, you're interested in how that fits into the general pattern.
1289520	1295120	To really illustrate why do we think that such a synthesis is important and relevant for
1295120	1300720	deep learning research going forward, we have to go way back, way back in the time of Euclid,
1300720	1308000	around 300 years BC. And as you might know, Euclid is the founding father of Euclidean geometry,
1308000	1314080	which for many, many years was the only way to do geometry. It relied on a certain set of postulates
1314080	1320160	that Euclid had that governed all the laws of the geometry that he proposed. All of this started
1320160	1325840	to drastically change around the 1800s when several mathematicians, in an effort to prove
1325840	1331760	that Euclid's geometry is the geometry to be following, ended up assuming that one of the
1331760	1336640	postulates is false and failing to drive a contradiction. They actually ended up deriving
1336640	1342080	a completely new set of self-consistent geometries, all with their own set of laws and rules,
1342080	1348960	and also quite differing terminologies. Among some of these popular variants are the hyperbolic
1348960	1355840	geometry of Lobachevsky and Bolyai, and the elliptic geometries of Riemann. And for a very long time,
1355840	1362000	because all of these geometries had completely different sets of rules and they were all self-consistent,
1362000	1366880	people were generally wondering what is the one true geometry that we should be studying.
1366880	1374160	A solution to this problem came several decades later through the work of a young German mathematician
1374160	1380000	by the name of Felix Klein, who had just been appointed for a professorship position at the
1380000	1385760	small Bavarian University of Erlangen, the so-called Friedrich Alexander University in Erlangen,
1385760	1392400	Nuremberg. While he was at this post, he had proposed a direction that would eventually enable
1392400	1397840	us to unify all of the geometries that were in existence at the time through the lens of
1397840	1404240	invariances and symmetry using the language of group theory. And his work is now eponymously
1404240	1410880	known as the Erlangen program. And there is no way to overstate how much of an important effect
1410880	1416160	the Erlangen program had on mathematics and beyond. Because of the fact that it provided
1416160	1421840	a unifying lens of studying geometry, suddenly people didn't need to hunt for the one true
1421840	1426720	geometry they had a blueprint they could use to drive whatever geometry was necessary for the
1426720	1431680	problem they were solving. And besides just mathematics, it had amazing spillover effects
1431680	1437280	to other very important fields of science. For example, in physics, the Erlangen program spilled
1437280	1441920	over through the work of Emy Nerther, demonstrated that all of the conservation laws in physics,
1441920	1446320	which previously had to be validated through extensive experimental evaluation, could be
1446320	1451680	completely derivable through the principles of symmetry. And needless to say, this is a very
1451760	1457440	fundamental and game changing result in physics, which also allowed us to classify some elementary
1457440	1461920	particles in what is now known as the standard model. Thinking back towards theoretical computer
1461920	1468400	science, the Erlangen program also had a spillover effect into category theory, which is one of the
1468400	1472960	most abstractified areas of theoretical computer science with a lot of potential for unifying
1472960	1477760	various directions in mathematics. And actually in the words of the founders of category theory,
1477760	1483040	the whole field of category theory can be seen as an extension of Felix Klein's Erlangen program.
1483680	1491520	So the Erlangen program demonstrated how it's possible to take a small set of guiding principles
1491520	1497280	of invariance and symmetry and use it to unify something as broad as geometry. I like to think
1497280	1503600	of geometric deep learning as not a single method or architecture, but as a mindset. It's a way of
1503600	1508640	looking at machine learning problems from the first principles of symmetry and invariance.
1509440	1515520	And symmetry is a key idea that underpins our physical world and the data that is created by
1515520	1521840	physical processes. And accounting for this structure allows us to beat the curse of dimensionality in
1521840	1527680	machine learning problems. It is really a very powerful principle and very generic blueprint.
1528240	1532880	And we find its instances in some of today's most popular deep learning architectures,
1532880	1538400	whether it's convolutional neural networks, graph neural networks, transformers, LSTMs and many
1538400	1544320	more. And it is also a way to design new machine learning architectures that are yet to be invented,
1544320	1551280	maybe in the future, not based on back propagation and incorporate inductive bias in a principled way.
1551280	1556000	Being a professor and a teacher, I would also like to emphasize the pedagogical dimension of
1556000	1562160	this geometric unification. What I often see in deep learning when deep learning is taught is
1562960	1569120	it appears as a bunch of hacks with weaker or no justification. And I think it is best illustrated
1569120	1574720	with how, for example, the concept of convolution is explained. It is often given as a formula just
1574720	1580880	out of the blue, maybe with a bit of hand waving. But what we try to show is that you can derive
1580880	1586080	convolution from first principles, in this particular case, of translational symmetry.
1586640	1591120	And I think the difference in this approach is best captured by what Elvetsos once said
1591120	1595680	that the knowledge of principles easily compensates the lack of knowledge effects.
1597040	1604080	Professor Bronstein has been a professor at Imperial College in London for the last three years
1604080	1611440	and received his PhD with distinction from Technion, the Israeli Institute of Technology,
1611440	1619040	in 2007. He's held visiting academic positions at MIT, Harvard and Stanford,
1619040	1627680	and his work has been cited over 21,000 times. His main expertise is in theoretical and computational
1627680	1633360	geometric methods for machine learning and data science, and his research encompasses a broad
1633360	1640640	spectrum of applications ranging from computer vision and pattern recognition to geometry processing,
1640640	1647840	computer graphics and biomedicine. Professor Bronstein coined and popularized the term
1647840	1656080	geometric deep learning. His startup company, Fabula AI, which was acquired by Twitter in 2019,
1656080	1662560	was one of the first applications of graph ML to the problem of misinformation detection. I think
1662560	1668960	it's no exaggeration to say that Professor Bronstein is the world's most recognizable expert
1668960	1674960	in graph representation learning research. We are really probably some of the nicest locations
1674960	1681600	in London, which is Kensington, if you're familiar, so we have the the Natural History Museum, the Science
1681600	1689760	Museum and the Victoria and Albert Museum and Imperial College is right here, so I think it's
1689760	1695200	as central as you can get. And if you walk all the way there, then you have Hyde Park,
1696160	1700800	which is probably one of the nicest parks in London. I'm a professor in the department of
1700800	1705200	computing at Imperial College London and head of graph learning research at Twitter.
1706000	1711120	I work on geometric deep learning in particular on graph neural networks and their applications from
1711120	1717520	computer vision and graphics to computational biology and drug design. Dr Petar Velichkovich
1717520	1724800	is a senior research scientist at DeepMind in London and he obtained his PhD from Trinity College
1724880	1731040	in Cambridge. His research has been focused on geometric deep learning and in particular
1731040	1736960	devising neural network architectures for graph representation learning and its applications
1736960	1742720	in algorithmic reasoning and computational biology. Petar's work has been published in the
1742720	1748640	leading machine learning venues. Petar was the first author of graph attention networks,
1748640	1755040	a popular convolutional layer for graphs and deep graph infomax, a scalable unsupervised
1755040	1760640	learning pipeline for graphs. Hi everyone, my name is Petar Velichkovich and I'm a senior
1760640	1765840	research scientist at DeepMind and previously I have done my PhD in computer science at the
1765840	1770960	University of Cambridge where I'm actually still based and today we're actually here in Cambridge
1770960	1777040	filming these shots and it is my great pleasure to be talking to you today about our work on
1777040	1783920	geometric deep learning and related topics. I first got into computer science through competitive
1783920	1788400	programming contests and classical algorithms the likes of which you might find in a traditional
1788400	1795040	theoretical computer science textbook and this was primarily influenced by the way schooling
1795040	1800560	worked for gifted students back in my hometown of Belgrade in Serbia where students were generally
1800560	1805200	encouraged to take part in these theoretical contests and try to write programs that are just
1805200	1811600	going to finish as fast as possible or work as efficiently as possible over a certain set of
1811600	1817200	carefully contrived problems. All of this changed when I actually started my computer science degree
1817200	1822800	here at Cambridge where I was suddenly exposed to a much wider wealth of computer science topics
1822800	1827200	than just theoretical computer science and algorithms and for a brief moment my interests
1827200	1833440	drifted elsewhere. Everything started to come back together when I started my final year project
1833440	1839760	with Professor Pietro Leo at Cambridge and I had heard that bioinformatics is a topic that's
1839760	1844320	brimming with classical algorithms and competitive programming algorithms specifically so I thought
1844320	1850000	a project in this area would be a great way to bring these two closer. Unfortunately that was not
1850000	1857280	to be as my mentor very quickly drew me into machine learning and that kind of spiraled out into my
1857360	1863760	PhD topics where I was for a brief moment focused on computational biology topics before
1863760	1869040	eventually drifting to graph representation learning and eventually geometric deep learning.
1869040	1875120	My journey into geometric deep learning started actually through investigating
1875840	1881120	graph representation learning which I think for a very long time these two areas have been seen
1881120	1885600	as almost synonymous with one another because almost everything you come up with in the area
1885600	1890640	of geometric deep learning can be if you squint hard enough seen as a special case of graph
1890640	1896880	representation learning. What originally brought me into this was an internship at Montreal's
1896880	1903360	Artificial Intelligence Institute Miele where I worked alongside Joshua Bengio and Adriana Romero
1903360	1910000	on initially methodologies for processing data that lives on meshes of the human brain.
1910640	1915120	We found out that the existing proposals for processing data over such a mesh
1915120	1919440	both in graph neural networks and otherwise were not the most adequate for the kind of data
1919440	1924320	processing that we needed to do and we needed something that would be aligned more with image
1924320	1930080	convolutions in spirit in a way that allows us to give different influences to different
1930080	1936080	neighbors in the mesh and this led us to propose graph attention networks which was a paper that
1936080	1941680	we published at Eichler 2018. It was actually my first top tier conference publication and
1942400	1947600	what I'm probably most well known for nowadays. The field of graph representation learning has
1947600	1952720	then spiraled completely out of control in terms of the quantity of papers being proposed.
1953600	1958640	Only one year after the graph attention network paper came out I was reviewing for
1958640	1963520	some of the conferences in the area and I found on my reviewing stack four or five papers that
1963520	1967680	were extending graph attention nets in one way or another so the field certainly has become a lot
1967680	1973760	more vibrant because of a nice barrier of entry which is not too high. Recently Pettai has been
1973760	1979920	doing some really interesting research in algorithmic reasoning. Part of the skill of a software
1979920	1986640	engineer lies in choosing which algorithm to use only rarely will an entirely novel algorithm be
1986640	1993280	warranted. The key guarantee which traditional symbolic algorithms give us is generalization
1993280	1999120	to new situations. Traditional algorithms and the predictions given by deep learning models
1999120	2004800	have very different properties. The former provides strong guarantees but are inflexible
2004800	2010640	to the problem being tackled while the latter provide few guarantees but can adapt to a wide
2010640	2016800	range of problems. Now Pettai in his paper proposed a neural architecture which can take in natural
2016800	2023760	inputs but output a graph of abstract outputs as well as natural outputs. Pettai believes that
2023760	2029200	neural algorithmic reasoning will allow us to apply classical algorithms on inputs that they were
2029200	2036880	never originally designed for. I am studying algorithmic reasoning which is a novel area of
2036880	2042960	representation learning that seeks to find neural networks that are as good as possible at
2042960	2047520	imitating the computations of exactly the kind of classical algorithms that initially brought me
2047520	2053760	to computer science. It turns out that this area is remarkably rich and could have remarkably big
2053760	2059760	implications for machine learning in general because it could bring the best of the algorithmic
2059760	2065600	domain into the domain of neural networks and if you look at the pros and cons of the two you'll
2065600	2069840	find that they're very complementary. So the fusion of the two can really bring the kinds of
2069840	2076640	benefits we haven't seen before. So I am very pleased to say that I'm among those researchers
2076640	2082640	that is extremely proud and happy of what I do because it brings together some of my earliest
2082640	2087360	passions in computer science with the latest trends in machine learning and especially
2087360	2093440	geometric deep learning which we recently released a proto book about with Joan, Michael and Taco.
2093440	2098800	We spoke to Christian Saagedi and he's doing some interesting work with algorithmic reasoning
2098800	2104880	creating abstract syntax trees to represent mathematical theorems for example and then
2104880	2109920	he believes that in that representation space he projects it all into a Euclidean space that
2109920	2115440	there's some interesting interpolative points in that space but again surely there must be some
2115440	2120320	deeper structure which analogizes mathematics which would allow us to extrapolate and discover
2120320	2125280	new interesting mathematics. It just feels that what we're missing is the right kind of structure.
2125280	2130320	I think for mathematics it's relatively easy to formalize it because well we can write logic
2130320	2136160	rules and basically we can build mathematics axiomatically from very basic principles. These
2136160	2141440	methods are already being used for computer proof of certain theorems. I think it's not
2141440	2147040	well regarded by the purists in pure mathematics but probably they will need to accept it and well
2147040	2152000	you know maybe there will be fields medal that will be given for a proof that is done by a computer
2152000	2158160	I think even recently there are some important breakthrough results proofs that were given by a
2158160	2165440	computer so it is probably just the beginning of a new way of doing science essentially even
2165440	2170960	as pure science as creative science as mathematics which was considered really the hallmark of human
2170960	2176080	intelligence it can be maybe if not replaced assisted by by artificial intelligence.
2177040	2182960	Petr invokes Daniel Kahneman's system one and system two. He thinks that we need something
2182960	2189920	like system two to achieve the kind of reasoning and generalization which currently eludes us
2190560	2195920	in deep learning models. What I'm holding in my hands right now is the international bestseller
2195920	2202960	on thinking fast and slow from the famous Nobel Prize winner Daniel Kahneman. This book can be seen
2202960	2208720	as one of the main theses behind my ongoing work in algorithmic reasoning and what it stands for
2208720	2214320	because it argues that fundamentally we as humans employ two different systems that operate at
2214320	2220400	different rates system one which primarily deals with perceptive tasks and system two which deals
2220400	2227040	with longer range reasoning tasks and it is my belief that currently where our research in neural
2227040	2233280	networks has taken us is to get really really good at automating away system one so being able to
2233280	2240240	perform perceptive tasks from large quantities of observed data probably in a not too dissimilar
2240240	2246400	manner from the way humans do it. What I feel is really missing from these architectures nowadays
2246400	2251600	is the system two aspect being able to actually take these percepts that we've acquired from the
2251600	2257760	environment and actually properly do rigid reasoning over them in a manner that will stay
2257760	2263200	consistent even if we drastically change the number of objects slightly perturb the laws of physics
2263200	2268640	or something like that. In my opinion algorithmic reasoning the art of capturing these kinds of
2268640	2273520	reasoning computations inside a neural network that was trained specifically for that purpose
2273520	2277840	and then slotting that neural network into a different architecture that works with raw
2277840	2283360	percepts is one potentially very promising blueprint that will take the space of classical
2283360	2288640	algorithms that we have been building in this system two space and carry them over into raw
2288640	2293600	perceptive inputs which these algorithms were very rarely designed to work over. This is Dr.
2293600	2300960	Tako Kohen. Hello, I'm Tako Kohen. I'm a researcher at Qualco AI Research and I work on
2300960	2306960	geometric deep learning, equivariate networks and more recently also on causal inference and causal
2306960	2313920	representation learning. Now I've been interested for a number of years already since about 2013
2313920	2319280	in the application of ideas around symmetry, invariance, equivariance and the underlying
2319280	2325440	mathematics of group theory and group representation theory to machine learning and deep learning
2325440	2330720	specifically. And so it's been quite exciting to see over the last few years really the blossoming
2330720	2337120	of this field that we now call geometric deep learning. Many new methods such as various kinds
2337120	2343520	of equivariate convolutions for different spaces, different groups of symmetries, different geometric
2343520	2349520	feature types, equivariate transformers and attention mechanisms, point cloud networks,
2349520	2355120	graph neural networks and so forth. And along with these new methods also a large number of
2355680	2361840	applications that have been tackled. Anything from medical imaging to the analysis of global
2361840	2369440	weather and climate data to the analysis of DNA sequences and proteins and other kinds of molecules.
2369440	2374560	So if you apply this mindset, the first question you ask when faced with such a new problem is
2374560	2381280	what are the symmetries? What are the transformations that I can apply to my data that may change the
2381280	2387600	numerical representation of my data as stored in my computer, but that nevertheless don't change
2387600	2393760	the underlying objects we're interested in. Whether that's reordering the nodes of a graph,
2394320	2402320	rotating a molecule in 3D or many other kinds of symmetries. Once you know the group of symmetries,
2402320	2409280	you can then develop a neural network that's equivariate symmetries. And hopefully is a
2409360	2416160	universal approximator among equivariate functions. What we found, what many others have found time
2416160	2422240	and again, is that if you build this symmetry prior to your network, if you make your network
2422240	2427760	equivariate, it is bound to be much more data efficient and to generalize much better.
2428560	2438400	Hey Tim, great to be here. So yeah, the blueprint. So what we realized as we were writing this book
2438400	2444320	is that really a lot of different architectures, they can be understood in one as essentially a
2444320	2450400	special cases of one generic structure that we call the geometric deep learning blueprint.
2451120	2458960	So to explain a little bit about what this is all about, the blueprint refers to first of all
2458960	2466320	feature spaces. So I'll explain how we model those. And then it refers to maps between feature
2466320	2472480	spaces or layers of the network. And they also have to somehow respect the structure of the
2472480	2478720	feature spaces. So in all cases, whether it's, you know, graph neural net, a network for processing
2478720	2486080	images on the plane, or a network for processing signals on a sphere like global weather data,
2486880	2493120	we're dealing with data that lives on a domain. So the domain in the examples I just gave would
2493120	2499680	be the set of nodes of the graph, or perhaps also the set of edges, the points on the plane,
2499680	2505360	or the points on the sphere. And of course, you can think of many other examples as well.
2505360	2511520	This is what we call the domain. We write it as omega. It's typically, it's a set, first of all,
2511520	2516640	and it may have some additional structure. So in the case of the sphere, it has some interesting
2516640	2522560	topology. And typically, we also want to think about the geometry, want to think about distances
2522640	2529200	and angles. So it's a set with some kind of structure. And in addition, it has some symmetries,
2529200	2535120	meaning there's some transformations we can do to this set, that will preserve the structure that
2535120	2541200	we think is important. So if we think distances are important on, let's say the sphere, when the
2541200	2546240	kinds of symmetries we end up with our rotations, three dimensional rotations of the sphere,
2546320	2554160	perhaps also reflections. In the case of a graph, the symmetries would be permutations
2554160	2562800	of the nodes and also corresponding permutation of the edges. So you just change the order of
2562800	2568320	the nodes. If node one and two were connected, you apply permutation and move those to node three
2568320	2573520	and four, then now three and four have to be connected. So that's a symmetry of our space
2573520	2580720	overgon. Now the data, this is a very important point, the data are typically not points in this
2580720	2586400	space. So when we're classifying images, well, our space is the plane, but the data are not
2586400	2591200	points in the plane, they're not the two dimensional vectors, the data is really a signal
2592160	2597280	on the plane, a two dimensional image, which so you can think of that as a function from for each
2597280	2605600	point in the plane, we have a pixel value. So in the more general cases, it might be
2605600	2611520	something called a field. So you might say have wind direction on on earth, that's a
2612160	2617600	vector field on the sphere. Now the symmetries that we talked about, they act on this space,
2618240	2623760	you could show how they automatically also act on the space of signals. So those are the key
2623760	2630480	ingredients to define what a feature space is, you have your your space, omega, so set of nodes,
2630480	2635760	sphere, whatever, then you have a group of symmetries of that space. And then you have
2635760	2641840	the space of signals or feature maps on this space, and you have the group acting on your
2641840	2649040	signal. So you can rotate a vector field, or you can shift a planar image or etc. So those are the
2649040	2656160	key ingredients to define a feature space. And then we can talk about layers. And so the layers or
2656160	2664080	maps of the network, they have to respect this structure. So if we have a signal on on the sphere,
2664880	2671280	and we believe that rotating it doesn't change it in any essential way, or we permute the nodes in
2671280	2677280	the graph, but it's still the same graph, then we want the layers of the network to respect that.
2677280	2683440	And to respect the structure means to be actually variant to the symmetries. So if we apply our
2683440	2691440	transformation to our signal, and then we apply our network layer, it should be the same as applying
2691440	2696640	the network layer to the original input, and then applying a transformation in the output space.
2697440	2701520	Now how the transformation acts in the output space could be different from the input space. So for
2701520	2710160	example, you could have a vector field as input, and a scalar field as output, they transform
2710160	2713760	differently. So that's why for each layer and network, you're going to have a different feature
2713760	2720160	space with a different action of the group. But it's the same group acting in each feature space,
2720160	2725440	the maps, they should be equivariant. And these maps, they include both the linear maps, which are
2725440	2732880	typically the learnable layers, and the non linearities. Now for linear maps, you can study all
2732880	2737760	sorts of interesting questions, you can ask what is the most general kind of equivariant linear
2737760	2747040	map. And it turns out that for a large class of group actions, or linear group actions, the most
2747040	2752400	general kind of equivariant linear maps are generalized forms of convolutions. So that's
2752400	2760720	really an explanation for why convolutions are so ubiquitous. The final ingredient that I think
2760720	2766720	is key to the success of many architectures is some kind of pooling or cautioning operation.
2767600	2772800	So the structures we've talked about so far are, you know, this global space and the symmetries on
2772800	2780560	it, etc. But typically, there's also a notion of distance or locality in our space. And if we
2781200	2787680	just enforce that our layers have to respect the symmetries, well that would force us to use a
2787680	2795200	convolution in many cases, and I just mentioned, but not forces to use local filters. And we all
2795200	2800000	know that using sort of global filters is not going to be very effective use of parameters.
2801200	2809760	So locality is another key thing, locality in the filters. And also localities exploited via
2809760	2817200	some kind of pooling or caution operation. Now, going forward to say the year 2020, deep learning
2817200	2821920	is all the craze right now. And so many different deep learning architectures are being proposed,
2821920	2827040	convolutional neural networks, graph neural networks, LSTMs and so on. When these architectures are
2827040	2831360	being proposed, different terminologies are used because people tend to come from different areas
2831360	2836800	when they're proposing them. And also, they are usually followed by kinds of bombastic statements
2836800	2840560	such as everything can be seen as a special case of a convolutional neural network,
2840560	2845600	transformers use self-attention and attention is all you need. Graph neural networks work on graphs
2845600	2851520	and everything can be represented as a graph. And LSTMs are turing complete, so why would you ever
2851520	2858800	need anything else? So I hope that this illustrates how the field of deep learning in the year of 2020
2858800	2864640	is really not all that different from the state of geometry in the 1800s. And if history teaches
2864640	2869200	anything about how we can unify these fields together, now is the right time for us to look
2869200	2874080	back, study the geometric principles underlying the architectures that we use. And as a result,
2874080	2879120	we might just derive a blueprint that will allow us to reason about all the architectures we have
2879120	2884480	in the past, but also any architectures that we might come up with in the future. And in my opinion,
2884480	2891360	that is the key selling point of our recently released proto book. And I hope that it is helpful
2891360	2896000	in guiding deep learning research going forward. I should highlight that I was also extremely,
2897120	2903600	extremely honored to deliver the first version of the talk presenting our proto book at
2903600	2910080	Frederic Alexander University of Erlang in exactly the same place where the Erlang program was
2910080	2916000	originally brought up, albeit because of the existing COVID restrictions, I had to do so
2916000	2920960	in a virtual manner. So I think a lot of people understand convolutions in the way of a
2920960	2926160	traditional plain us, CNN, and the mathematical notion of a convolution, which is very closely
2926160	2931440	related to, you know, a Fourier transform, for example. But graph convolutional networks,
2931440	2937120	they seem to abstract the notion of a convolution into some kind of concept of the neighborhood
2937120	2943760	and local connectivity. And some of your work as well with, you know, equivariate convolutional
2943760	2947120	neural networks, I think do the same thing. So when we talk about convolution, are we talking
2947120	2952240	about a very abstract notion of it? Yeah, that's a great question. I think that
2952960	2960640	there are so many different ways to get at convolution. You can think of it in sort of
2960640	2965520	you've trying to think of it in terms of sliding a filter over some space, right? So you put it
2965520	2972240	in a canonical position, and then you slide it around. That is an idea that you can generalize
2972240	2978880	to not just sliding over a plane, but say applying some transformation from a group
2978880	2983520	to your filter. So let's say you're up, you have a filter on the sphere, then you can,
2984000	2988720	you have, you know, the sphere has a symmetry group, mainly three dimensional rotations,
2988720	2993360	group SO3, you can apply an element of SO3, a three dimensional rotation to your filter,
2993360	2999200	even sort of slide it over, over the sphere. That leads to something called group convolution.
3000000	3004800	So that's one way to think about it. There is indeed, as you mentioned, the spectral
3004800	3010400	wave looking at it. So you could think there's the famous Fourier convolution theorem,
3011360	3019600	which says that in the spectrum, convolution is just a point wise product. So one way to
3019600	3024720	implement a convolution would be to take a Fourier transform of your signal, your feature map,
3024720	3030320	and a Fourier transform of your filter, multiply them point wise, and then inverse Fourier
3030320	3037280	transform to get the result. And this perspective also generalizes. So it generalizes to graphs,
3037840	3045200	where the Fourier transform, or something analogous to it, can be obtained via graph
3045200	3052800	laplacians, as well as some other ideas. That is actually historically how some of the first
3052800	3060400	graph neural nets were implemented and motivated. There's also a spectral theory for group
3060400	3069680	convolutions. So indeed, the Fourier transform can be generalized, or the standard Fourier
3069680	3075600	transform that we all know, is actually the Fourier transform for the plane, the plane being
3075600	3082000	a particular group, the translation group in two dimensions. So there is a whole, a very beautiful
3082000	3090880	theory of, let's say, Fourier generalized Fourier transforms for groups, where now the spectrum
3092000	3101680	is indexed not by the just by the integers, as it is the case for the for the for the line or the
3101680	3107040	plane, but by something called irreducible representations. In the case of the plane,
3107040	3114320	those are indeed indexed by integers. And the the spectrum is not just scalar valued,
3114320	3119360	or complex scalar valued, but it can be matrix valued. If you're interested in this sort of
3119360	3124000	stuff, you can, you want sort of a very high level description, you can check out our paper on
3124000	3129680	spherical CNNs, where we actually implement convolution on a sphere, using this kind of
3129680	3135520	generalized Fourier transform. So that's the Fourier perspective on convolutions.
3136320	3142880	And there's a final perspective, which I think is quite intuitive, which is that
3143760	3152320	the convolution is the most general kind of equivariant linear map between certain group
3152320	3159840	between certain linear group actions. So specifically, these group actions are the
3159840	3166640	way that groups tend to act on the space of signals on your space. So you might have space
3166640	3172800	of scalar signals on the sphere. And your feature map, you might want to have another scalar signal
3172800	3177680	in the sphere or a vector field on the sphere or something. Then you can ask what is the most
3177680	3183040	general kind of equivariant linear map. And the answer is, it's a convolution. And that is also
3183040	3189040	true in it in a very general setting. So that that to me is the most intuitive way of understanding
3189120	3195200	convolutions as the most general kind of maps that are linear maps that are equivariate to
3195200	3200320	certain kinds of group actions. Professor Bruner, welcome to MLST. It's an absolutely
3200320	3207120	honor to have you on. Introduce yourself. Yeah. Hi, Tim. I'm Joanne Bruner. I'm a associate professor
3207120	3212720	at the Quran Institute and Center for Data Science at New York University. And I'm very happy to be
3212720	3220320	here chatting with you at MLST. Joanne, you just released this geometric deep learning proto book.
3220320	3225200	You know, what does it mean to you? What was your kind of intellectual journey that led to this?
3226400	3234080	Yeah. So this journey, in fact, started many years ago. So I would say even like during my postdoc,
3234080	3240480	I was a postdoc. I was doing my postdoc here at NYU with Yanle Kun. And that was the time
3241040	3249600	2013, where, you know, confnets were already showing a big promise in image tasks. And
3250160	3256160	discussing with Yan, the questions are, okay, how about domains that are not like grades, right? So,
3256160	3261920	and that was the beginning of a journey that also included my former collaborator, Arthur Slump,
3261920	3267440	who was like another researcher in the lab that had a similar background as me, like coming from
3267440	3272800	applied math, but looking into into into more and more deep learning. So I would say that the
3272800	3279040	genesis was our first attempt at extending the success of convolutional networks to these
3279040	3285200	regular domains. And we published the paper at iClear 2014. And after that, I think things started
3285200	3291040	like quite naturally because other researchers that had, I would say, came from the same background,
3291040	3296560	like, you know, maybe from geometric background, also started to realize that there was something
3296560	3300160	there, maybe bigger than these particular papers, in particular, Michael Brownstone,
3301440	3306000	it's kind of reached out to us just afterwards saying, oh, and we are also looking at similar
3306000	3311200	ideas. I think we should just team together and start to think about this more globally. And so,
3312240	3318800	you know, things that developed from there. And we wrote this journal paper, like a review paper
3318800	3325920	in 2017, with Arthur, Yan, Michael, the airbender guys, who is another very well known figure in
3326000	3331280	this area. And so from there, I think that, yeah, things like started to slowly take off. We had
3331280	3337600	tutorials and new ribs that we had a very successful workshop at IPAM. After this, I think that the
3338560	3342640	thing were clear that, okay, maybe at some point in the future, we should try to stamp all these
3342640	3348480	things into a book that tries to reflect something a bit more, let's say, mature and, you know, what
3348480	3354960	is our, if we wanted to have like some kind of legacy for future generations on how to implement
3354960	3359760	and communicate these methods, how would that be? So I guess that was the genesis of the book. And so
3361520	3366960	very soon we said with Michael that we also would like to have some, you know, fresh minds and fresh
3366960	3373120	energy on board. So naturally, the names of Taco and better came very naturally to us as people
3373120	3378800	who had been doing excellent work in the domain that would very nicely complement our skills.
3378800	3385360	So the team was created. And that's the project. And so, yeah, I mean, I think it's been very
3385360	3391920	interesting so far. Of course, I guess that, as you know, this is just an ongoing project, right?
3391920	3398080	So it's still not finalized. But hopefully we're getting interest from the community. And this
3398080	3403040	gives us some kind of, I would say positive vibes to finish it on time. As you know, writing books
3403040	3408800	is always like this never ending process. So I think that, yeah, that's, it's been an
3408800	3414800	interesting endeavor so far, for sure. I asked Professor Bromstein what his most passionately
3414800	3419840	held belief is about machine learning. I think machine learning is such a field where
3419840	3425600	holding strong beliefs is often counterproductive. It happened to me multiple times that something
3425600	3432560	that I thought or said was very quickly overturned. And what I mean is that milestones or progress
3433040	3438720	was achieved much faster than I could even imagine in a wild dream. So making predictions about
3438720	3444640	machine learning is, to some extent, an ungrateful job. I do, however, believe that in order to make
3444640	3449920	progress to the next level and make machine learning achieve its potential to become the
3449920	3455600	transformative technology we trust and use ubiquitously, it must be built on solid mathematical
3455600	3461200	foundations. And I also think that machine learning will drive future scientific breakthroughs.
3461200	3466800	And probably a good litmus test would be a Nobel Prize awarded for a discovery made by
3466800	3471440	or with the help of an ML system. It might already happen in the next decade.
3472080	3478400	I wanted to go into a few questions actually about the book. So first of all, Joanne, in your
3478400	3484800	2017 paper, The Mathematics of Deep Learning, you cited the universal function approximation theorem,
3484800	3489360	which is to say the ability of a shallow neural network to approximate arbitrary functions.
3489360	3494080	But the performance of wide and shallow neural networks can be significantly beaten by deep
3494080	3498800	networks. And you said that one of the possible explanations is that deep architectures are able
3498800	3504560	to better capture invariant properties of the data compared to their shallow counterparts.
3504560	3508960	Could you just briefly introduce the universal function approximation theorem? And do you think
3508960	3515520	it's still relevant for today's neural networks? Yeah, I mean, that's a very deep and an important
3515520	3521520	question. Yeah, so universal approximation theorem, it refers to this very general principle that
3521520	3527200	once you define parametric class, let's say you're you are on a learn functions using neural
3527200	3532800	nets, it just describes your ability that as you put more and more parameters into your class,
3532800	3538240	you're going to able to to approximate essentially anything that data nature throws at you. And so
3538240	3546160	this might seem like a very powerful property. But in fact, in fact, it's it's something that
3546880	3551840	you have probably already encountered many times during your undergrad. I mean, if you have any,
3551840	3557120	let's say, background in, you know, single processing electrical engineering, there's many
3557120	3562240	ways in which students have learned how to represent data, like signals, for example,
3562240	3567120	using Fourier transform. So Fourier transforms are an instance of a class that has universal
3567200	3572320	approximation. So in that sense, it's a it's a I think, going back to the second part of your
3572320	3578400	question, how relevant it is to in the context of neural networks, and how far does this thing
3578400	3584560	pushes towards understanding why deep learning works. So I would say there's a there's two sides
3584560	3592480	of the answer. On one hand, I think that universal approximation is is a tool that when you combine
3592480	3600160	it with other elements, it becomes something that provides good guiding principles. For instance,
3600160	3604880	universal approximation of a generic function, we know that yeah, we can as I said, we can obtain
3604880	3610080	it with, you know, very, very naive architectures, for example, just a shallow neural network without
3610080	3615920	any kind of physical structure, special structure already has this property. Does it actually help
3616000	3622160	us to to learn very efficiently? No, right, and I'm going to go at this afterwards. But when you
3622160	3628400	combine it, for example, with, you know, let's say that now your data lives on a graph, or your data,
3628400	3632720	I don't know, has a certain like a come from a physical lab that has certain properties,
3632720	3637200	let's say that it's rotational invariant. Like the first thing that that the designer,
3637200	3641120	like a domain scientist would like to know, if you come there and you design your neural
3641120	3646960	net, look, I have a neural net that takes your data and has very good performance. The first thing
3646960	3652080	that he will ask is, okay, how general is your architecture, right? Can it explain anything
3652080	3657200	that they could throw at you? It seems like it's a, in that sense, I would, I would present it more
3657840	3663040	as a sufficient condition, like a check mark that your, your, you know, your architecture needs to
3663040	3667360	fall, right? If you make more and more parameters, can you express more and more elements functions
3667360	3674320	from your class? But then, as I said, is it far from being sufficient, right? It's like, it's,
3674320	3678400	sorry, it's a, it's a necessary condition, but it's far from being sufficient in the sense that
3680000	3687040	universal approximation has a flavor is a result that does not quantify how many parameters
3687040	3691600	do I need, right? Like, you know, if I want to approximate function, let's say I want to classify
3691600	3696320	between different dog breeds, it doesn't tell me this theorem doesn't tell me how many parameters,
3696320	3700080	how many neurons do I need for that, right? It's, it's a statement that in that sense,
3701120	3704480	it lets, it leaves you a little bit with your, like, say, like, you know, like,
3705120	3709040	like the, it's a bittersweet result, right? It doesn't, it doesn't really tell you anything
3709040	3715600	actual. So that's why, and that's why we enter these, these other questions that is actually
3715600	3722080	much deeper. And to some extent, still reading them pretty much open, that is, how do you actually
3722080	3726400	go from this, this statement to something that is quantitative, right? Something that tells you,
3726400	3732000	okay, you know, you need that many layers, you need that many parameters. And so this is where
3732000	3738320	the role of depth in neural networks is, you know, becomes essentially the key open question.
3738880	3745040	And, and yeah, so in this, in this quote that you, that you brought from this paper,
3745680	3751680	that kind of reflected our understanding at the time of maybe the true power of universal
3751680	3757680	approximation is, you know, when, as you combine it with these other prior, that is the asymmetries
3757680	3762400	of the data, like the invariances. So I don't want to represent arbitrary functions, I only want to
3762400	3772640	represent functions that are invariant to certain transformations of the input. So in fact, our,
3772640	3779280	at least my particular view of the problem, analysis of the problem has somehow evolved in the last
3779360	3783760	years, right? Of course, through research that I've done together with my collaborators.
3783760	3791120	And now, and this is actually the way we present it in the book, we, we, we kind of identify two
3791120	3797120	different flavors, two different sources of prior information that one needs to bake into the problem,
3797120	3802320	right? To, to really go beyond this like basic approximation result of neural nets. The first
3802320	3808640	one you need is invariance, right, is a disability that you need to, like the fact that you actually
3808640	3814320	put symmetries into the architecture is certainly going to have a benefit in terms of sample
3814320	3818320	complexity, right? They, I mean, you are going to learn more efficiently if your model is aware
3818320	3825440	of the symmetries of the world. But in fact, this, this prior, in fact, we know now that it's not
3825440	3831280	sufficient, right? If you only like agnosticially build your learning system, just with these
3831280	3836000	symmetries in mind, indeed, you are going to become more efficient that a system that is
3836000	3841760	completely agnostic to symmetries. But it might not be, you might not be able to formally establish
3841760	3847440	what we call like a, like a learning guarantee that has good sample complexity. And I guess that I
3847440	3852480	don't want to go too much into the jargon and the details of what this means. But the idea is that if
3852480	3857760	I want to, you know, learn this function with certain precision, how many examples, how many
3857760	3862240	training examples do I need to kind of give you a certificate for authentication, like a guarantee
3862240	3867520	that I'm going to be able to do that. So with symmetries alone, it's not something that we
3867520	3872240	know how to do. In fact, we are, we would believe we have strong beliefs that it's not possible,
3872240	3879280	right? There's examples out there that I could, I could, you know, construct a function that has
3879280	3885600	the right symmetries, has the right priors, if you want, but still needs a lot, a lot, a lot of
3885600	3891120	examples to build to learn. So what we need is to add something else into the mix. And this is this
3891120	3896400	something that we call in the, in the book, this scale separation. And, and, and if you want, I can
3896400	3903360	try to very briefly give you an intuitive idea of what this means. So if you think about the problem
3903360	3908560	of classifying an image, like a dog or the cat. So what is given to you is like a big
3908560	3912720	branch of pixels, right? Every pixel has a color value. So somehow you need to figure out
3913680	3917920	the thing that you're looking for is lying in some kind of like, it's really through the
3917920	3921440	interactions between pixels that you get the answer, right? And the question is,
3922480	3926960	of course, if I have a thousand pixels, how many possible interactions do I have between
3926960	3931360	thousand elements? So this is where this exponential or the curse of dimensionality appears, right?
3931360	3936320	I need to, a priori, I should be looking at all possible families of interactions between
3936320	3940800	pixels. And this is where maybe where my signal would be lived. Of course, if I need to look for
3940800	3947440	all of these things, it's impossible, right? There's just too many things. If I tell you that the,
3947440	3951680	you know, these interactions are such that there's this translation symmetry. Well, you might not
3951680	3957920	be, you might not be needing to look at all of them. But in fact, you don't need, you do not
3957920	3963840	throw enough. So what is really something that is powerful is that I tell you that maybe the
3963840	3969200	interactions that matter the most are those between a pixel and its neighbors, right? And if you
3969200	3974400	understand very well, if you base your initial learning steps into understanding well, which
3974400	3980560	local interaction matters, maybe you can use them to bootstrap the interactions that go to look at
3980560	3985440	this neighborhood to slightly bigger neighborhoods, right? And so this idea that you can break a very
3985440	3991920	complicated problem into a families of sub problems that lives in different scales. This is at the,
3991920	3997040	I would say, at the intuitive level, something print like at the core of the essence of why
3997040	4004640	these architectures are so efficient. This idea, as you might imagine, is not new. It's not specific
4004640	4009600	to deep learning. The idea that you can take a complicated system of interacting particles
4010160	4015520	and break it into different scales. This is at the, at the basis of essentially all of physics
4015520	4020160	and chemistry, right? There's many, many, you know, like when people study, even like biology life,
4020160	4026000	right? You have, you have experts that are very experts at the molecular level. Then you have
4026000	4029600	experts that, you know, might understand like, you know, doctors that understand things at the
4029600	4033680	level of okay functions. And then there's maybe experts at the level of the society, right? But
4033680	4039280	this, you know, it's pretty natural to break the very complicated thing into different scales.
4039280	4043840	And so deep neural networks somehow are able to do that. We don't have the full mathematical
4043840	4049440	picture, right? Or for example, why this scale separation is strictly necessary. What we know
4049440	4054960	from empirical evidence, like that is now I would say indisputable, is that this is an efficient way
4054960	4059360	to do that, right? Because when I was, when I was reading the prototype book, I noticed that
4059360	4064080	there was a separation between the symmetries and the scale separation. Could you explain in
4064080	4069360	simple terms, why is the scale separation not just a symmetry as well? Because it seemed a little bit,
4069360	4074720	I don't want to say kluge, but it seemed like you had this scale matter and you dealt with it
4074720	4080880	separately. Yeah, that's a good, that's a good point. So, so maybe that the way to, to, to separate
4080880	4087360	these two would be if you think about like an algorithmic instantiation, if you want to have
4087360	4092960	a network that would just break the problem into different scales, it would be like a neural network
4092960	4098160	that would operate at different patches. And for every patch of the image, I could be learning
4098160	4104080	independent set of parameters, right? So that would be a model that is only told that it should be
4104080	4108160	breaking the problem into different regions. But it's not necessarily told that, you know,
4108160	4111120	there's a weight sharing, right? There's some kind of like parameter sharing
4111120	4116720	that somehow is, you are able that, you know, in a sense is helping you to learn with fewer
4116720	4122480	number of parameters. So somehow these two, these two conditions are slightly complementary.
4123280	4128400	We, as I said, there's still like a lot of mathematical puzzles as to how these things
4128400	4137200	interact optimally. And I think that the, the one of the reasons why we chose to explain the story
4137200	4143280	into two different, in these two different priors is that they all survive this quest for
4143280	4149280	generality in the sense that these two principles are, again, something that you can think about
4149280	4158400	for grids, for groups, for graphs, you can also see these principles appearing completely everywhere
4158400	4162320	as you study physical systems, right? Like the scale and the symmetry
4162320	4169040	is really at the core of, of many physical theories. And I would say that there's also
4171680	4179520	at the more maybe technical level, these two priors somehow have been instrumental to
4180560	4185520	organize, like to basically to have a kind of a recipe to build architectures, right? So,
4185520	4191600	so maybe now we don't even think about it, right? But when you have a new problem, a new domain,
4191600	4197120	and you need to build an efficient neural network, we automatically have this idea that, okay,
4197120	4201760	we are going to start learning by composition, right? So we are going to extract information
4201760	4206640	one layer at a time. That's the first appearance of scale. And we know that the way we need to
4206640	4211520	organize these layers, right? How do you parameterize a layer that takes some input features and
4211520	4217520	produces maybe better features? This idea that we do that by understanding this kind of
4217520	4223520	equivarian structure, right? We have this notion of filters, right? In convolutional networks,
4223520	4228160	we have this, as I said, we organize everything in terms of filters. When we talk about message
4228720	4233680	graph neural networks, we have this kind of like diffusion filters, right? And so there's this
4233680	4237840	object that we extract from the domain that is helping us, that giving us something very
4237840	4242880	constructive, very, you know, very relevant, like very practical. And so this is really
4242960	4248640	the underlying group of transformations that is acting on our domain. And so I would say that,
4248640	4254720	you know, from a practitioner's perspective, these two principles, right, that I'm going to
4254720	4261440	learn by composing some fundamental layers that I repeat all the time. And the way this layer
4261440	4266960	is organized, is structured through this group transformation, this has been, I would say,
4267440	4273360	like a trademark of, you know, the success of neural networks. Of course, as also we mentioned
4273360	4282240	in the book, these are, I would say, proto, like meta, you know, meta parameterization, right,
4282240	4287520	in the sense that there's, as you know, many, many, many variants that people have come up with.
4287520	4292960	Many, many, let's say, yeah, like modifications on the basic architecture that have really
4292960	4298560	make dramatic changes in performance, right. So there's, of course, as I say, like the devil
4298560	4304480	sometimes is in the details, right. And so as we are writing the book, we are realizing exactly,
4304480	4309120	you know, how some of the changes in the architectures are actually fit into this,
4309120	4311760	what we call this blueprint, right, this symmetrically learning blueprint.
4312640	4317200	Fascinating. Okay, so I wanted to come back to what you were saying a few minutes ago about the
4318080	4323520	the sample efficiency of these models. Now, with graph neural networks, for example,
4323520	4328880	there are factorially many permutations of adjacency matrices for a given graph. And
4329760	4334800	I want to talk about a sorting algorithm, right. So Franois Chollet came on the podcast and he said
4334800	4339520	that in order to learn a sorting algorithm that generalizes, you would need to learn point by
4339520	4346240	point, you would need to see factorially many examples of permutations of numbers.
4347040	4352320	Do you think that we could train a neural network to learn a sorting? I guess what I'm
4352320	4356720	asking in a roundabout way is, do you think there's a kind of geometry to computation itself?
4358480	4364560	Good. That's a very good question. And in fact, we have some, some recent work with some collaborators
4364560	4370400	in my group, where we kind of take up this question from and we try to formalize it mathematically,
4370480	4376880	and we give answers to this question. And so many of these like a computational tasks that
4376880	4383280	you were mentioning, for example, sorting or, you know, like algorithmic tasks, they are,
4383920	4388800	if one wants to put them into some mathematical context, the first thing that comes to mind is
4388800	4393440	these are functions that already enjoy some symmetries. For example, like a sorting algorithm,
4393440	4398400	right, is invariant to permutations, right. So the function that you are trying to learn
4398400	4404480	is an arbitrary function that has this symmetric class. And so as such, as I was saying in the
4404480	4412160	beginning, you can try to address this question saying, okay, now you give me an arbitrary,
4412160	4418400	so the question would be relative to an arbitrary learning learner that is agnostic to symmetry,
4418400	4424320	how much does a symmetric learner gain, right? So you can basically try to understand, quantify
4424320	4429680	the gains of sample complexity of learning without symmetry versus learning with symmetry.
4430400	4437840	And so the punchline of this work, the recent work that we completed, is that one can actually
4437840	4443680	quantify the sample complexity gains, and these are of the order of the size of the group. And so
4444320	4449280	here in the case of like permutations, what it means is that if a learner is aware of this
4449280	4456000	symmetry, like one training example of the symmetric learner is rosary equivalent to
4456640	4461360	n factorial samples of the agnostic learner, right, which is something that you would expect,
4461360	4467840	like if you think in terms of data mutation, right, like if I tell you in advance that your
4467840	4472720	function is symmetric, is invariant to permutations, it's, you know, like a brute force approach would
4472720	4476640	say, okay, you give me an instance training an input, right, and instead of giving you this input,
4476640	4481840	I'm just going to, you know, like permute, like have any possible permutation of the input, and
4481840	4486400	you already know the output, right? So you can as well feed it to the learner. So this is like
4486400	4490880	horrific at this addition turns out to be mathematically correct, precise, at least, you
4490880	4497520	know, under some conditions, right? But the, I guess the whole point is that these gains might
4497520	4503280	look amazing, like might look like a, you know, like a big boost in sample complexity. As I said
4503280	4509120	before, there's a grain of salt here is that the, in these conditions, in general, general
4509120	4514320	conditions, you are already fighting an essential and impossible problem in the sense that the rate,
4514320	4520080	like the sample complexity is dominated by a rate of basically the rate in which you learn
4520080	4525280	is what we call course by dimension. And what it means is that if I want to, you know, I have a
4525280	4530560	certain performance generalization error, I'm going to say that now I want to ask you the question,
4530560	4536160	if I want to divide this generalization error by two, how many more samples do I need to give you?
4536160	4542080	Right? Like what? So if I want to double, you know, like double, like reduce the error by half,
4543120	4547760	by how much do I need to give you more samples? So this dependency, in fact, is exponential in
4547760	4553280	dimension, right? So basically the, the, the sample complexity gains by invariance, they are
4553280	4557840	exponential in dimension, but they are fighting an impossible problem that is already caused by
4557840	4565120	dimension. So what it means is that at the end of the day, this is what, you know, what was in the,
4565120	4571280	in the, like in the heart of what I was saying before, is that invariance alone might not be
4571280	4575920	efficient, might not be sufficient, right? Because you are, okay, you are taking a very hard problem,
4575920	4580560	you are removing an exponential factor, but you still have many, you know, you have still
4580560	4586320	have something in the exponent that is exponential, right? So, so, so what it means is that that,
4586320	4593280	I mean, that's what really underpins why we think in these terms of combining symmetry prior
4593280	4598720	with the scale separation prior. But certainly the algorithmic tasks are very interesting playground
4598720	4605440	because I think that for the case of sorting, I mean, as you know, scale separation is also an
4605440	4609920	issue. It's also a very important thing, right? I mean, it, this is what basically is at the heart
4609920	4614480	of the dynamic programming approaches, right? Like these efficient algorithms that are not only
4614480	4618640	officially statistically, but also officially computation, right? This idea that you can
4618640	4625920	divide and conquer. So, so, so, so algorithmic tasks also are kind of exposed to this dual
4625920	4630960	physical prior, right? Of a scale and invariance. On the course of dimensionality, there's this
4630960	4636400	issue where you have a data point and you want to surround it by other data points in two dimensions
4636400	4642480	to create a convex hole. And as you increase the number of dimensions, the number of data points
4642480	4649120	you need to create this covering to create a kind of interpolative space increases exponentially.
4649120	4653840	And when you get past a certain number of dimensions, let's say 16 or not, not very many,
4653840	4659200	you would need essentially more data points than there are atoms in the universe. So this leads
4659200	4664720	to a very interesting realization. I think some people refer to it as the manifold hypothesis,
4664720	4670880	which is that most natural data is only really spatially novel on very few dimensions. And a
4670880	4677360	lot of data falls on very smooth low dimensional manifolds. But what are the implications of this?
4678400	4683120	Essentially, all machine learning problems that we need to deal nowadays are extremely high
4683120	4688560	dimensional. So even if we take very modestly sized images, they live in thousands or even in
4688560	4694080	millions of dimensions. And if you think of machine learning or at least the simplest setting of
4694080	4699680	machine learning as a kind of glorified function interpolation, the standard approach is to function
4699680	4706400	interpolation as just use the data points to predict the values of your function will simply
4706400	4710880	not work because of the phenomenon of cursive, the recursive dimensionality that increasing
4710880	4716160	the number of dimensions, the number of such points blows up exponentially. So what you really
4716160	4722800	need to take into account and probably this is really what makes machine learning work in practice
4722800	4726880	is the assumption that there is some intrinsic structure to the data and it can be captured
4726880	4731440	in different ways. So it's either the manifold assumption where you can assume that the data,
4731440	4736000	even though it lives in a very high dimensional space intrinsically, it is low dimensional.
4736000	4741520	This can be captured also in the form of symmetry. For example, image is not just a high dimensional
4741520	4746080	vector. It has underlying grid structure and grid structure has symmetry. This is what captured
4746080	4750800	in convolutional networks in the form of shared weights that translates into the convolution
4750800	4756000	operation. I think the symmetries are part of the magic here because it's not just the interesting
4756000	4762160	observation that natural data is only spatially novel in so few dimensions. There's something
4762160	4767520	magic about symmetries. And when we spoke to Franois Chalet recently, he invoked the kaleidoscope
4767520	4774880	effect, which is this notion that almost all information in reality is a copy of some other
4774880	4780320	information. Probably here it will be a little bit stretching, but I would say that because
4780320	4785840	data comes from nature, from physical processes that produce it, physics and nature itself
4785840	4792560	is in a sense low dimensional. So it's application of simple rules at multiple scales. You can create
4792560	4797760	very complex systems with very simple rules. And this is probably how our data that we are mostly
4797760	4802880	interested in in machine learning is structured. So you have this manifestation of symmetry and
4802880	4807200	self-similarity through different scales, the principles of symmetry and certain
4807280	4812320	environs or equivalents and of scale separation, where you can separate your problem to multiple
4812320	4817520	scales and deal with it at different levels of resolution. And this is captured, for example,
4817520	4822480	in pooling operations in convolutional neural networks and in other deep learning architectures.
4823040	4828800	This is what makes deep learning systems work. Fascinating. It's so good that you raised the
4828800	4832400	curse of dimensionality because I was going to ask you about that. Could you explain in really
4832400	4838640	simple terms, so not invoking lipships, I can't even say it properly now, but not invoking a
4838640	4846160	mathematical jargon. And why exactly in your articulation does geometric deep learning
4846800	4849440	reduce the impact of the curse of dimensionality?
4850640	4862000	Yeah. So the curse of dimensionality, it refers generally to the inability of algorithms to
4862000	4867040	keep certifying certain performance as the data becomes more complex. And data becoming more
4867040	4873200	complex here means that you have more and more dimensions, more and more pixels. And so this
4873200	4881840	inability of scaling, basically it really says that if I scale up the input, my algorithm is
4881840	4887600	going to have more and more trouble to keep the base. And so this curse can take different
4887600	4896160	flavors. So this curse might have a statistical reason in the sense that as I make my input space
4896160	4903440	bigger, there would be many, many, many much exponentially more functions, real functions
4903440	4907760	out there that would explain the training set that would basically pass through the training points.
4908800	4914720	And so the more dimensions I add, the more uncertainty I have about the true function.
4914800	4920080	So I would need more and more training samples to keep the base. This curse can also be from the
4920080	4925440	approximation side. So in the sense that the number of neurons that I'm considering to approximate
4925440	4930800	my target function, I need to keep adding more and more neurons at the rate that is exponential
4930800	4937600	in dimension. And the curse can also be from the computational side. The sense that if I keep
4937600	4943920	adding parameters and parameters to my training model, I might have to optimize to solve an
4943920	4949600	optimization problem that becomes exponentially harder. And so you can see that you are basically
4949600	4958000	bombarded by all angles. And so an algorithm like here in the context of statistical learning or
4958000	4963760	learning theory, if you want, having a kind of a theorem that would say, yes, I can promise you
4963760	4968240	that you can learn, you need to actually solve these three problems at once. You need to be able
4968240	4973200	to say that in the conditions that you're studying, you have an algorithm that it does not suffer from
4973200	4979120	approximation nor statistical nor computational curses. So as you can imagine, it's very hard
4979120	4984160	because you need to master many things at the same time. So why do we think that geometric
4984160	4991120	deep learning is at least an important piece to overcome this curse? As I said before, so
4991120	4996560	geometric deep learning is really a device to put more structure into the target function.
4996560	5002880	So basically to make the learning problem easier because we are promising the learner
5002880	5007680	more properties of a target function. We are basically making the hypothesis class if you want
5007680	5016240	smaller. That said, as I said, there's still some path to go. We're describing just a bunch of
5016240	5021680	principles that make these hypothesis spaces smaller and more adapted to the real world.
5022320	5027360	But one thing that we are still lacking, for example, is the guarantee in terms of optimization.
5027600	5032480	I mean, I described that the depth of these architectures is somehow something that is
5032480	5036640	akin associated with the scale, the fact that you need to understand things at different scales.
5037280	5043600	So as you know, from the optimization side, there's still some open questions and open mysteries
5043600	5048480	as to why the gradient descent, for example, is able to find good solutions. So these are
5048480	5054560	things that we believe that these architectures can be optimized efficiently just because we have
5054560	5059040	these experimental evidence that is piling up. But we are, for example, we are still lacking
5059760	5066160	theoretical guarantees. For the approximation, it's a bit of the same story. So we understand
5066160	5071600	very well approximation properties of shallow networks, starting from universal approximation,
5071600	5077120	but of course, many, many recent interesting work. But we are also still lagging a little bit behind
5077760	5083680	in approximation properties for deeper networks. So as you see, it's like you can see from this
5083760	5090240	discussion that, yes, we have some good reasons to believe that these are fundamental principles
5090240	5097440	of learning. But there's also a bunch of mathematical questions that are still open. And this is also
5097440	5101760	one of the things that I like about writing a book on this topic, because it's a very life domain.
5103760	5109520	As you can see, the field is still evolving. And I think it's a good time to... Yeah, it's a good
5109520	5114320	time. I mean, researchers out there are listening to us. It's a good time to think and to work and
5114320	5120480	to join this program. Amazing. Will we ever understand the approximation properties of deep
5120480	5126640	networks? Because with the shallow function approximation algorithm, you can almost think
5126640	5131360	of a neural network as being kind of like sparse coding. And the more neurons you have, you're
5131360	5136800	just kind of discreetly fitting this arbitrary function. But you don't have that visual intuition
5136800	5143840	quite so much with the deep networks. Yeah, I mean, it's an important... And it's a deep question.
5143840	5151680	So indeed, shallow neural networks are really, really correspond to this idea that you learn
5151680	5157920	a function by stacking a linear combination of basis elements. And this is really at the roots
5157920	5162400	of essentially all of harmonic analysis or functionalized. I typically think about the
5162400	5166480	basis and you ask questions about the linear approximation or the approximation, etc.
5167200	5171120	Deep neural networks, they introduce a fundamentally different way to approximate
5171120	5177920	functions that is by composing, by composition. And so you're right. Our knowledge about this
5177920	5184720	question right now is mostly concentrated in what we call separation results, like a depth
5184720	5190960	separation, which consists in trying to find construct mathematical examples of functions
5190960	5195760	that cannot be approximated with shallow neural networks with certain number of neurons.
5195760	5200240	But indeed, they can be much better approximated with deep neural networks. So this is really
5200240	5206080	understanding which kinds of functions benefit fundamentally from composition rather than from
5206080	5214320	addition. And so there's a certain mathematical vision and mathematical intuition that is
5214320	5220240	building up. But of course, it's still very far from explaining the true power of depth. And
5220240	5227760	just to give you like a final pointer here, there's a very related question that replaces
5227760	5232080	neural networks as we understand them with what we call Boolean functions, right? Like these are
5232080	5238160	just circuits, arithmetic circuits that take as input some bit string and they can manipulate the
5238160	5245840	bits by, you know, or operations and operations and they can keep adding gates. And so this question
5245840	5250480	about what is the ability of a certain circuit architecture to approximate certain Boolean
5250480	5257120	functions is actually a notoriously hard and basically has been studied in the theoretical
5257120	5262240	computer science community since the 50s and the 60s, right? And there's actually very, very,
5262240	5267920	very deep results and very challenging actually open questions concerning these things. So this
5267920	5274080	is really, we are really touching here questions that are pretty serious at like the deep mathematical
5274080	5280000	and theoretical level. And so, yes, you should not expect that in the year in the next year or two,
5280000	5285120	we have a complete understanding of, you know, approximation powers of any architecture with
5285120	5294080	any depth. But I think you should expect that the theory like this will continue to try to catch up
5294080	5302480	with the experiments. And so we are, I think we are hoping to get like a more precise mathematical
5302480	5310560	understanding of the role of depth. And as I said before, there's one thing that is fascinating
5310560	5316800	about this domain that is maybe very unique to deep learning is this very strong interaction
5316800	5323120	between optimization, statistics and approximation, right? Maybe it turns out that, you know, the
5323120	5328160	huge depth that we have in these residual neural networks might not be necessary from the
5328160	5333840	approximation side, but in fact, it's so useful for the optimization that overall is a big winner,
5333840	5340000	right? So there's always these like twists about this question that are fundamentally mixing these
5340000	5345520	three sources of error. I'm sorry for asking you this question, but can neural networks extrapolate
5345520	5356240	outside of the training data? That's a good question. I would say that the answer, I guess,
5356320	5363360	depends on your specifications, right? So I guess that the conservative answer
5364240	5370480	of a statistical learning person would be no, because we don't have good theorems right now
5370480	5378240	that tell us that this is the case. There's very like, you know, like a strong effort both from
5378240	5383440	the practical and the theoretical community to really understand this question, like by trying
5383440	5388560	to formalize it a bit more. I mean, what do we mean by distribution shift? You know, what kind of
5388560	5393360	training procedures you can come up with that would give you precisely this kind of robustness?
5394160	5399760	There's of course, what we call these biases, right? I mean, I can always take it like a
5399760	5404320	training distribution, I make a choice of a certain architecture, I'm going to learn a function,
5404320	5408640	and obviously, there's some directions if you want in the space of distributions,
5408640	5412320	for which my hypothesis will turn out to be have good generalization, there might be
5412320	5416560	other direction in which the contrary is true. So there's actually very nice work
5417520	5422960	from Stephanie Gejalka's group at MIT, where they, for example, they studied this question
5422960	5428320	in the context of value networks, also including graphs, where they, for example, they discover
5428320	5434000	or they identify this strong preference for value networks to generalize along linear directions,
5434000	5439360	right? In the sense that if I just decide to now, you know, shift my data in linear directions,
5439360	5444080	then my function has no trouble generalizing. Maybe there's other directions, right, in which
5444080	5448800	this thing is actually catastrophic. So I think that, yeah, the question, I think it's very important
5449360	5456640	from, let's say, it's very important from a kind of a practitioner perspective, right? That's typically,
5456640	5461760	that's clearly something that a user would like to know. But I think that from a more mathematical
5461760	5465760	or theoretical level, I think we are still at the stage of trying to formalize like, okay,
5465760	5471360	what do we exactly mean by extrapolation? And what are the kind of the conditions for
5471360	5476080	which architecture can do it? And I think that, yeah, this is a, yeah, it's an important question.
5476080	5480320	But yeah, I think we are still pretty far from having a full answer.
5481360	5485440	Amazing. And final question, what areas of mathematics do people need to study
5485440	5493520	before reading the proto book? Good question. So I think that our objective and our really
5493520	5499360	like the idea is really to have something that is quite self-contained in the sense that we are
5499360	5505200	going to provide appendices that expand on the areas that is maybe they're not typically
5506000	5510640	kind of the bread and butter of machine learning people. For example, we are going to have an
5510640	5517840	appendix on group theory, differential geometry, harmonic analysis. So these are areas that I
5517840	5523200	think are going to be there for you to delve into, to get like the most of the paper,
5523200	5528400	the most of the book. But I think that other than that, any basic, you know, any basic
5528400	5534000	knowledge of linear algebra, statistics and analysis will do. So like, if you have taken
5534000	5539680	a graduate level class in machine learning, you should be ready to go. Rather than just being
5539680	5546080	applied in a lot of really important branches of research problems, people outside of pure
5546080	5552160	research have recognized that a lot of the data that comes to us from nature is most naturally
5552160	5556640	represented in a graph-structured form. Very rarely will nature give us something
5556640	5562080	that can be representable as an image or a sequence. That's super rare. So very often,
5562080	5567200	the structure is more irregular, more graph-like. And therefore, graph neural networks have already
5567200	5573360	seen a lot of applications in domains where the data is supernaturally represented as a graph.
5573360	5577920	In the domain of computational chemistry, where you can represent molecules as graphs of atoms
5577920	5583040	and bonds between them, the graph neural networks have already proven impactful in detecting
5584160	5588560	novel potent antibiotics that previously were completely overlooked because of their unusual
5588560	5594320	structure. In the area of chip design, graph neural networks are powering systems that are
5594320	5600480	developing the latest generation of Google's machine learning chips, the TPU. Furthermore,
5600480	5607520	graph-structured data is super ubiquitous in social networks and the kinds of networks maintained by
5608480	5614960	many big industry players. And accordingly, graph neural networks are already used to serve various
5614960	5621360	kinds of content in production to billions of users on a daily basis. In fact, the recommendation
5621360	5627600	system at Pinterest, the product recommendation system at Amazon, as well as the food recommendation
5627600	5633920	system for Uber Eats, all of them are powered using a graph neural network that helps serve the most
5633920	5640000	relevant content to users on a daily basis. And on a slightly personal note, graph neural networks
5640000	5646720	have also been used to significantly improve travel time predictions in Google Maps, which
5646720	5652240	is used also by billions of people every day. So whenever you type a query, how do I get from
5652240	5658160	point A to point B in the most efficient way? The travel time prediction that you get is powered
5658160	5662800	by a graph neural network that we have developed that defined in collaboration with the Google Maps
5662800	5669200	team. And this is of high importance not only to users that use the app on a daily basis to find
5669200	5674640	the most efficient way to travel. It's also used by the various companies that leverage the Maps
5674640	5681360	API so they can tell their customers what's the time it will take for a certain vehicle to arrive
5681360	5686240	to them. So companies such as food delivery companies and ride-sharing companies have also
5686320	5692880	extensively profited from this system, which in cities such as Sydney has reduced the relative
5692880	5697920	amount of negative user outcomes in terms of badly predicted travel times by over 40%,
5698800	5703920	making it one way in which graph representation learning techniques that I have co-developed
5703920	5711520	are actively impacting billions of people on a daily basis. When I was an undergraduate student,
5711520	5716800	I was interested in image processing and was excited about variational methods.
5717920	5723120	I think it's a very elegant idea that you can define a functional that serves as a model for
5723120	5728960	your ideal image and then use the optimality conditions to derive a differential equation that
5728960	5734160	flows towards the optimum. And a particularly cool approach was proposed by Ron Kimmel,
5734720	5738960	where you could think of an image as a manifold or a high-dimensional surface
5738960	5745200	and use an energy that originated in string theory and particle physics to derive a non-euclidean
5745200	5751120	diffusion PDE called Beltrami Flow that acts as a non-linear image filter. And this is what
5751120	5756880	made me fall in love with differential geometry and I did a PhD with Ron on this topic. And I think
5756880	5762080	these were really beautiful and deep ideas that unfortunately now are almost forgotten in the
5762080	5766960	era of deep learning. And it's a pity that the machine learning research community has such a
5766960	5773200	short memory because many modern concepts have really ancient roots. Ironically, we've recently
5773200	5778880	used non-euclidean diffusion equations as a way to reinterpret graph neural networks as neural
5778880	5784880	PDEs. And I think it really helps sometimes to have a longer time window. Now, equivariated
5784880	5791440	networks tend to generalize much better and require much less data if the data indeed has
5791440	5797200	the symmetry that you assumed in your model. But people often ask, you know, why do we even
5797200	5802320	care about data efficiency when we can just collect more data? We live in the era of big data, right?
5803200	5807680	And I think the answer why you might still be interested in data efficiency. First of all,
5807680	5813920	there are applications like say medical imaging, where acquiring labeled data simply is very cost.
5813920	5818640	You have to get patients, you're dealing with privacy restrictions, you're dealing with
5818640	5823600	costly, highly trained doctors who have to annotate the data, come together in a committee to
5823600	5829520	decide on questionable cases and so forth. So this is very expensive. And if you can improve the
5829520	5835440	data efficiency by a factor of two or 10, or whatever it may be, you might just take a problem
5835440	5840720	that was in the realm of economically infeasible and take it into the realm of the economically
5840720	5847520	feasible, which is a very useful thing. There are other cases like graph neural nets, where the
5847600	5853360	group of symmetries is so large, in this case, n factorial number of permutations,
5853360	5860000	that no amount of data or data augmentation in practice is going to allow you to learn the
5860000	5865440	symmetry or to learn the invariance or equivariate in your NAPO. So indeed, you see that in this
5865440	5870880	space of graph neural nets, everybody uses equivariate permutation, equivariate network
5870880	5876160	architectures. And then finally, you can think about the grand problems of AGI, artificial
5876160	5883280	general intelligence and so forth. And here I think that we will most certainly need large data sets,
5885040	5890800	large networks, a lot of compute power, and so forth. The current architectures we have clearly
5890800	5898400	are performing far fewer computations than the human brain. So there's a ways to go there.
5898400	5905520	But I also think that one essential characteristic of intelligence is the ability to learn quickly
5905600	5912640	in new situations, situations that are not similar to the ones you've seen in your training data.
5913520	5918880	And so data efficiency to me is an essential characteristic of intelligence. It's almost
5918880	5924880	like an action that you want your methods to be data efficient. And so I think one of the
5924880	5931840	big challenges for the field right now is to try to think of very generic priors, priors that
5931840	5938800	apply in a wide range of situations. And to give you, even though they're generic and abstract,
5938800	5944080	they give you a lot of bang for the buck in terms of improved generalization and data efficiency.
5944080	5950080	I think that the beauty of science and research is in connecting the dots. And I find it fascinating
5950080	5954480	that, for example, graph neural networks are connected to the work of Weissfeller and Lehmann
5954480	5960160	from the 60s on isomorphism testing, which in turn was inspired by problems in chemistry.
5960960	5966160	And chemistry was also the field that drove the research into modern formulation of
5966160	5971760	diffusion equations that were adopted in image processing community in the 90s and came back
5971760	5977680	recently as a way to reinterpret graph neural networks. And I think such connections give really
5977680	5983440	a new and deep perspective. And probably the deeper you dive, the broader they become. But
5983440	5989360	it's really an ever-ending story. Today is an incredibly special episode and we're filming
5989360	5995440	at 9 o'clock in the morning. It's really rare for us to film this early when I'm still caffeinated.
5995440	5999840	Many of our guests are over in the States. It's an absolute honor to have you both on MLST.
5999840	6004240	And Professor Bronstein, could you start by briefly telling us how the young mathematician
6004240	6008320	Enne Offer used symmetries to discover the conservation laws in physics?
6009040	6013280	Maybe I should take a step back and describe the situation that happened
6014000	6020480	in the field of geometry towards the end of the 19th century. And it was an incredibly fruitful
6021200	6028960	period of time for mathematicians working in this field with the discovery and development of
6028960	6035600	different kinds of geometries. So a young mathematician based in Germany called Felix Klein
6036320	6042960	proposed this quite remarkable and groundbreaking idea that you can define geometry by studying
6042960	6047840	the groups of symmetries, basically the kinds of transformations to which you can subject
6047840	6054960	geometric forms and seeing how different properties are preserved or not under these
6054960	6060080	transformations. So these ideas appear to be very powerful. And what Amy Neuter showed in her
6060080	6065120	work, and she actually worked in the same institution where Klein ended up in Gttingen in
6065120	6070640	Germany. And she showed that you can take a physical system that is described as
6071600	6076320	functional as a variational system and associate different conservation laws
6076320	6083360	with different symmetries of this system. And it was a pretty remarkable result because
6083360	6089040	before that, conservation laws were purely empirical. You would make an experiment many times
6089040	6094720	and measure, for example, the energy before or after some physical process or chemical reaction,
6094720	6099040	and you would come to the conclusion that the energy is preserved. So this is how, for example,
6099040	6104720	I think Lavoisier has discovered the conservation of energy. So it was probably for the first time
6105280	6110800	that you could derive these laws from first principles. So you would need to assume in case
6110800	6116160	of conservation of energy, the symmetry of time. We decided to take a tour into the world of
6116160	6121920	algorithmic reasoning. Peter, you said in your introduction that algorithmic reasoning seeks
6121920	6126720	to find neural networks that are good at imitating the classical algorithms that initially brought
6126720	6131760	you to computer science. So you recently released a paper on this called Neural Algorithmic Reasoning,
6131760	6136400	and it's often claimed that neural networks are turing complete. And, you know, we're told that we
6136400	6141120	can think of training neural networks as being a kind of program search. But you argued in your
6141120	6146160	paper that algorithms possess fundamentally different qualities to deep learning methods.
6146160	6150080	Francois Chollet actually often points out that deep learning algorithms would struggle
6150080	6155680	to represent a sorting algorithm without learning point by point, you know, which is to say without
6155680	6160000	any generalization power whatsoever. But you seem to be making the argument that the interpolative
6160000	6164880	function space of neural networks can model algorithms more closely to real world problems,
6164880	6169280	potentially finding more efficient and pragmatic solutions than those classically proposed by
6169280	6176080	computer scientists. So what's your take? Yeah, thanks for asking that, Tim. I think the concept
6176080	6182720	of classical algorithms, as opposed to deep neural networks, at least the way we're currently
6182720	6188560	applying them makes all these points about turing completeness a little bit moot, because there's
6188560	6194320	quite a few proofs out there saying that you can use neural networks or more recently graph neural
6194320	6199840	networks in particular to simulate a particular algorithm perfectly. But all of these proofs are
6199840	6205120	sort of a best case scenario. They're basically saying, I can set the weights of my neural network
6205120	6210400	to these particular values, and voila, I am imitating the algorithm perfectly, right? So all
6210400	6214960	these best case scenarios are wonderful. But in practice, we don't use this kind of best case
6214960	6220160	optimization, we use stochastic gradient descent. And we're stuck with whatever stochastic gradient
6220160	6225520	descent gives us. So in practice, the fact that a neural network is capable for a particular setting
6225520	6230560	of weights to do something doesn't mean that it will actually do that when trained from data.
6231360	6237200	So essentially, this is the kind of the big divide that separates deep learning from traditional
6237200	6241760	algorithms. And it has a number of other issues as well, not just the fact that we cannot find the
6241760	6248000	best case solution. Also, the fact that we are working in this high dimensional space, which is
6248000	6253840	not necessarily easily interpretable or composable, because you have no easy way of saying, for example,
6253840	6258160	in theoretical computer science, if you want to compose two algorithms, you're working with them
6258160	6262640	in a very abstract space, which means that, you know, you can easily reason about stitching the
6262640	6268720	output of one to the input of another. Whereas you cannot make that easy of a claim about latent
6268720	6273440	spaces of two neural networks, right? So all these kinds of properties, interpretability,
6273440	6279360	compositionality, and obviously also out of distribution generalization are plagued not
6279360	6284640	by the fact that neural networks don't have the capacity to do this. But the routines we use to
6284640	6290960	optimize them are not good enough to to across that divide. So in neural algorithmic reasoning,
6290960	6295600	all that we're really trying to do is to bring these two sides closer together by making changes
6295600	6299680	either in the structure of the neural network or the training regime of the neural network or the
6299680	6305920	kinds of data that we'll let the neural network see so that hopefully it's going to generalize better
6305920	6311520	and extrapolate better. And especially on the kinds of, you know, classical algorithmic problems
6311520	6316320	that we might see in a computer science textbook. And lastly, I think I'd just like to address the
6316320	6325280	point about sorting. We have a paper on algorithmic reasoning benchmarks that we are about to submit
6325280	6329760	that in Europe's data set track. I think it should be public even now on GitHub, because that's the
6329760	6334720	requirement for the conference, where we have quite a few algorithmic tasks, and we're trying to
6334720	6342240	force GNNs to learn them. And we do have several sorting tasks in there. And at least in distribution,
6342240	6347760	these graph neural networks are capable of imitating the steps of, say, insertion sort. So
6347760	6351760	I will say not all is lost if you're very careful about how you tune them. But obviously,
6351760	6356640	there is a lot of caveats. And I hope that later during this chat, we'll also get a chance to talk
6356640	6361600	a little bit about how even though we cannot perfectly mimic algorithms, we can still use
6361600	6367840	this concept of algorithmic execution today now to help expand the space of applicability of algorithm.
6368640	6373200	Yeah, this is absolutely fascinating. Because this gets to the core of what I think some people
6373200	6378640	point out as being the limitations of deep learning, right? Sholay spoke about this, but
6378640	6383280	I don't think that geometric deep learning would help a neural network learner sorting function,
6383280	6389520	because discrete problems in general don't seem amenable to vector spaces, either because the
6389520	6394320	representation would be glitchy, or the problem is not interpolative in nature or not learnable
6394320	6400240	with stochastic gradient descent. So it would be fascinating if we could overcome these problems
6400240	6404400	using continuous neural networks as an algorithmic substrate. Do you think we could?
6406960	6413360	I think that it is possible, but it will require us potentially to broaden our lens on what we
6413360	6417440	mean by geometric deep learning. And this is something we're already very actively thinking
6417440	6421840	about. I think one of our co-authors, Taco Co, and actually thought much more deeply about this
6422560	6428400	in recent times. But basically, the idea is we looked at geometric deep learning from a group
6428400	6433760	symmetry point of view, which is a very nice way to describe spatial regularities and spatial
6433760	6439600	symmetries. But it's not necessarily the best way to talk about, say, invariance of generic
6439600	6444480	computation, which you would find in algorithms, right? It's like, I have input that satisfies
6444480	6448720	certain preconditions. I want to say something about once I push it through this function,
6448720	6453760	it should satisfy certain post conditions. This is not the kind of thing we can very easily express
6453760	6458800	using the language of group theory. However, it is something that perhaps we could express more
6458800	6463600	nicely using the language of category theory, which is an area of math that I still don't know
6463600	6468320	enough about. I'm currently actively learning it. But basically, in the language of category
6468320	6474160	theory, groups are super simple categories that have just one node, right? You can do a lot more
6474160	6480320	complicated things if you use this more broader abstract language. And, you know, you talk about
6480320	6484960	basically anything of interest there in terms of these commutative diagrams. And Taco actually
6484960	6489520	recently had a really interesting paper called natural graph networks, where they basically
6489520	6494880	generalize the notion of permutation, equivariance that you might find in graph nets to this more
6494880	6500400	general concept of natural transformations. So now suddenly, you don't have to have a network that
6500400	6505360	does exactly the same transformation in every single part of the graph. What you actually need
6505360	6510800	is something a bit more fine grained. You just need for all like locally isomorphic parts of the
6510800	6515360	graph, you need to behave the same. But in principle, it gives you a bit more flexibility. And I think
6515360	6520320	that this kind of language, like moving a bit away from the group formalism would allow us to
6520320	6525440	talk about say algorithmic invariance and things like this. I don't yet have any theory to properly
6525440	6530640	prove this, but it's something that I'm actively working on. And I guess I would say, you know,
6530640	6534720	the only question is, would you still call this geometric deep learning? And in my opinion,
6534720	6539680	the very creators of category theory have said that category theory is a direct extension of
6539680	6544640	Felix Klein's Erlangin program. So since the founders of the field have already made this
6544640	6550000	connection, I would expect that, you know, it would be pretty applicable under a geometric lens.
6550960	6557680	So it seems to come back to it seems to come back from both of your sides to essentially graphs
6557680	6566320	and working on on sort of graphs to capture on the one side, the sort of symmetries that are
6566320	6571920	that you either assume in the problem or that you know, or that you want to impose on the other
6571920	6578160	side on the other side. Now, these computations can may be well represented in in graphs.
6579120	6585440	What's what's so special about graphs in your estimations? Is there something
6586320	6592080	fundamental to it? Or is it just another way? You know, we had Ben Gertzel or so here and I
6592080	6596880	asked him the same question, what's so special about graphs and his argument was essentially,
6596880	6600800	well, it's not about graphs, it's simply a good representation of the problem
6601440	6607120	that we can make efficient operations on. Do you have a different opinion on that? Is a graph
6607120	6613680	something fundamental that we should look more at than, for example, a tensor?
6614480	6619920	Graphs are abstract models for systems of relations or interactions. I should maybe specify
6620480	6627520	pairwise relations and interactions. And it happens that a lot of physical or biological
6628480	6633840	even social systems can be described at least at some level of abstraction as a graph. So that's
6633840	6640880	why it is a so popular modeling tool in many fields. You can also obtain other structures such as
6640880	6646960	grids as particular cases. I wouldn't call it fundamental, but it is a very convenient and
6646960	6653600	very common, I would say ubiquitous model. Now, what I personally find disturbing and we can talk
6653600	6659840	about it in more detail later on is that if you look at the different geometric structures for
6659840	6665840	Gamble that we consider in the book, whether it's Euclidean spaces or many phones, they all have
6665840	6670560	the discrete counterparts. So you have a plane and you can discretize it as a grid. You have a
6670560	6676960	manifold, you can discretize it as a mesh. A graph is inherently discrete. And this is something that
6676960	6682560	I find disturbing. There is, in fact, an entire field that is called network geometry that tries to
6682560	6687520	look at graphs as continuous objects. So for example, certain types of graph that look like
6687520	6693360	social networks, what is called scale free graphs, can be represented as nearest-neighbor graphs in
6693360	6699760	some a little bit exotic space with hyperbolic geometry. So if we take this analogy, I think
6699760	6705760	it is very powerful because now you can consider graphs as a discretization of something continuous
6705760	6713600	and then think for example of graph neural networks as certain types of diffusion processes
6713680	6721120	that are just discretized in a certain way. And by making potentially possible different
6721120	6726880	discretizations, you will get maybe better performing architectures. One of the core
6726880	6732240	dichotomies we talk about on Street Talk is the apparent dichotomy between discrete and continuous.
6732240	6737280	And as Yannick was saying, there are folks out there who want our knowledge substrate to be
6737280	6742400	discrete but still distributed, record them sub-symbolic folks. And this network geometry
6742400	6746080	is fascinating as well because you're saying in some sense you can think of there being some
6746080	6752160	unknown continuous geometry. So you're saying there is no dichotomy? This is probably a little
6752160	6757920	bit of a wishful thinking as it happens with every model. So I would probably phrase it carefully
6757920	6765120	for some kinds of graphs, you can make this continuous model for others, maybe not. Fascinating.
6765120	6771200	Well, on to the subject of vector spaces versus discrete, you know, geometric deep learning is
6771200	6777920	all about making any domain amenable to vector spaces, right? And indeed artificial neural networks.
6777920	6782240	But could these geometric principles be applied to another form of machine learning, let's say
6782240	6787600	discrete program synthesis? Certainly a very important question, Tim. And yeah, thanks for
6787600	6794240	asking that. I think that there are many ways in which geometric deep learning is already
6794240	6798880	at least implicitly powering discrete approaches such as program synthesis because
6799680	6807360	there is a pretty big movement on these so-called dual approaches where you stick a geometric
6807360	6813360	deep learning architecture within a discrete tool that searches for the best solution. So,
6813360	6818320	for example, in combinatorial optimization, a very popular approach recently for a
6818320	6824880	neural-resolving mixed integer programs is to like have your typical off-the-shelf
6825520	6830800	mip solver that selects variables to optimize one at a time. And, you know, with these kinds of
6830800	6835760	algorithms, they're in principle exponential time. But if you're lucky or knowledgeable enough about
6835760	6840720	how, in what order you select these variables, you can actually solve the problem in linear time,
6840720	6845280	which is something we would like to strive towards. And the exact way in which we select
6845280	6849440	these variables is a bit of a black magic, like humans have come up with a few heuristics, but
6849520	6854960	they don't always work. And whenever you have this kind of setting, as long as you're assuming that
6854960	6860080	you're naturally occurring data isn't always throwing the worst possible cases or adversarial
6860080	6866160	cases at you, you can usually rely on some kind of modeling technique, for example, a neural network
6866160	6870880	to figure out which decisions the model should be taking. So, in this case, for example, for
6870880	6876080	mip solving, DeepMind recently published a paper on this where you can treat mip problems as
6876080	6881040	bipartite graphs, where you have variables on one side and the constraints on the other.
6881040	6885600	And you link them together if a variable appears in a constraint. Then they run a graph neural
6885600	6890080	network, which, as we just discussed, is one of the flagship models in geometric deep learning,
6890080	6896480	over this bipartite graph to decide which variable the model should select next. And you can train
6896480	6901120	this either as a separate kind of supervised technique to learn some kind of heuristic,
6901120	6906080	or you can learn it as part of a more broader reinforcement learning framework, right, where
6906080	6911600	the reward you get is how close you are to the solution or something like this. So this is one
6912240	6918080	kind of clear way in which you can kind of have this synergy between geometric deep learning
6918080	6925120	architectures and solutions for, for example, program synthesis. But I would just like to
6925120	6931440	offer another angle in which you can think of program synthesis as nothing other than just
6931440	6937280	one more way to do language modeling, right, because synthesizing a program is not that different to
6937280	6943600	synthesizing a sentence, maybe with a more stringent check on syntax and so forth. But, you know,
6943600	6948880	any technique that is applied to language modeling could, in principle, be applied for
6948960	6955440	program synthesis. And something that we will be discussing, I believe, later during this conversation,
6956240	6961120	one of the flagship models of geometric deep learning is indeed the transformer, which
6962080	6968000	we show in our book, and elucidate why it can be seen as a very specific case of a graph neural
6968000	6974880	network. And that's one of the flagship models of language modeling. So basically, that's also
6974880	6980880	one more way to to unify. Like, you know, just because the end output is discrete doesn't mean
6980880	6986400	that you cannot reason about it using representations that are internally vector vector based.
6987280	6993760	Francois Chollet pointed out that there was this dichotomy. So you can embed discrete information
6993760	6998560	into a continuous representation. But the manifold needs to be smooth, it needs to be
6998560	7004320	learnable, it needs to be interpolative in nature. So I thought that was why we have these discrete
7004320	7009520	program searches. But then you have this exponential blow up. But maybe that search space, because
7009520	7014800	it's interpolative could be found using stochastic gradient descent, if you embed the discrete
7014800	7019840	information into some kind of vector space. But Professor Bronstein, I wanted to throw it back
7019840	7024320	over to you. I mean, why is it taken as a given that vector spaces are a good thing? Because
7024320	7030080	everything we're doing here is embedding discrete information into these Euclidean vector spaces.
7030160	7035440	Why are we doing that? There are multiple reasons why vector spaces are so popular in
7035440	7041360	representation learning. Vectors are probably the most convenient representation for both humans
7041360	7046880	and computers. We can do for a number of operations with them, like addition or subtraction. We can
7046880	7052000	represent them as arrays in the memory of the computer. They are also continuous objects,
7052000	7057920	so it is very easy to use continuous optimization techniques in the vector spaces. It is difficult
7057920	7062480	for Gamble to optimize a graph because it is discrete and requires combinatorial techniques.
7062480	7067600	But in a vector representation, I just have a bunch of points that I can continuously move
7067600	7073840	in a kind of dimensional space using standard gradient based techniques. Perhaps a more nuanced
7073840	7079200	question is what kind of structures can be represented in a vector space? And a typical
7079200	7085120	structure is some notion of similarity or distance. We want that the vector representations preserve
7085120	7090080	the distances between, let's say, original data points. And here we usually assume that
7090080	7095280	the vector space is equipped with the standard Euclidean metric or norm, and we have a problem
7095280	7101360	from the domain of metric geometry of representing one metric space in another. And unfortunately,
7101360	7106400	the general answer here is negative. You cannot exactly embed an arbitrary metric in Euclidean
7106400	7112000	space, but there are, of course, some results such as bogains theorem that, for Gamble, provides
7112800	7118240	bounds on the metric distortion in such cases. And in graph learning spaces with
7118240	7123280	other more exotic geometries such as hyperbolic spaces, you have recently become popular with,
7123280	7127600	for example, papers of Ben Chamberlain, my colleague from Twitter, or Max Nicol from Facebook.
7128240	7132880	And you can see that in certain types of graphs, the number of neighbors
7132880	7138240	grows exponentially with the radios. If you look, for example, at the number of friends of friends
7138640	7143760	and so on in a social network, where we have this small world phenomenon, you can see that
7143760	7150240	it becomes exponentially large with the growth of the radios. And now when you try to embed
7150240	7154480	this graph in Euclidean space, it will become very crowded because in the Euclidean space,
7154480	7159520	the volume of the metric ball grows polynomially with the radios. Think of the two-dimensional
7159520	7166480	case that we all know from school, the area of a circle is pi radius squared, right? The volume
7166480	7171680	of a ball is exponential with a dimension. So we inevitably need to increase the dimension
7171680	7177840	of the embedding to make space for these neighbors. In the hyperbolic space, the situation is very
7177840	7183280	different because the volume grows exponentially with the radios. So it is way more convenient
7184480	7188480	to use these spaces for graph embeddings. And in fact, recent papers show that
7189280	7193200	to achieve the same error in embedding of a graph in the hyperbolic space with, let's say,
7193200	7197760	10 dimensions, you would require something like a 100-dimensional Euclidean space.
7198720	7202720	Of course, I should say that metrics are just one example of a structure. So the general answer
7202720	7207600	to the question whether a vector space is a good model for representing data is, as usual,
7207600	7214640	it depends. You mentioned language, Peter, and maybe to both of you, do you think there is a
7216160	7223040	geometry to language itself? I mean, obviously, we know about embedding spaces and close things
7223840	7230160	somehow share meaning and so on. Do you think it goes beyond that? Because, like,
7230160	7236480	do you think there is an inherent geometry to language itself and sort of the meaning of what
7236480	7243280	we want to transmit and how that relates to each other? What you probably mentioned is the
7243280	7249840	famous series of papers from Facebook where unsupervised language translation can be done
7249840	7255600	by a geometric alignment of the latent spaces. In my opinion, it's not something that describes
7255600	7262160	geometry of the language. It probably describes in a geometric way some semantics of the world.
7262160	7266080	And even though we have, linguistically speaking, very different languages like,
7266080	7271840	let's say, English and Chinese, yet they describe the same reality. They describe the same world
7271840	7277680	where humans act. So it is probably reasonable to assume that the concepts that they describe
7277760	7281600	are similar. And also, while there are some theories and linguistics about
7281600	7288320	certain universal structures in languages that are shared, even though the specifics are different,
7288880	7293600	I think it's interesting to look maybe at non-human communications. I wouldn't probably
7293600	7299200	use the term language because it's a little bit loaded and probably some purists will be shocked
7299200	7303760	by me saying that, for example, whales have a language, but we are studying the communication
7303760	7308960	of slow whales. So this is a big international collaboration called Project. And I don't think
7308960	7315600	that you can really model the concepts that whales need to describe and to deal with
7316560	7321520	in the same way as we humans do. So maybe a silly example, we can say in human languages,
7321520	7327120	and probably it applies to every language, we can express a concept that something got wet.
7327120	7332080	I don't think that a whale would even understand what it means by being wet because
7332720	7337600	the whale always lives in water. I would add to that maybe a slightly different view of geometry,
7337600	7342640	but it's all about the question of how far are you willing to go and still call it geometry.
7342640	7347760	Based on our proto book, at least, I tend to think of graph structures also as a form of
7347760	7354000	geometry, even though it's a bit more abstract. And within language, people might not always agree
7354000	7359440	what this structure is like. But I think we can be fairly certain that there are explicit links
7359440	7365120	between individual words as and when you use them in different forms, syntax trees, or just one
7365120	7371200	word precedes another and so on and so forth. And while we may not be necessarily able to easily
7371200	7379600	say what is the geometric significance of one word, what we can look at is what is the local
7379600	7385440	geometry of the words that you tend to use around it. And I mean, this kind of principle has been
7385440	7398480	used all over the place. That has then been extended to graph structured observations,
7398480	7402560	generally with models like deep walk and note to back basically the same idea,
7402560	7408000	treat a nodes representation as everything that's around it. Basically, the reason why I think that
7408000	7412720	analyzing this local topology of how words are used with each other is very powerful.
7413680	7419280	I've reinforced that recently, precisely because of the fact I've been delving into category theory,
7419280	7423600	because in category theory, your nodes are basically atoms, you're not allowed to look
7423600	7428800	inside them, you assume they're this undivisible unit of information. And everything you can
7428800	7435200	conclude about the atoms comes from the arrows between them. So using this very simple concept
7435200	7441120	with a few additional constraints like compositionality, you can, for example, tell me what are all the
7441120	7446320	elements of a set, even though you've abstracted that sets to a single point, just by analyzing
7446320	7451280	the arrows between all sets, you can tell me what are all the elements inside a set. So thinking
7451280	7457040	about this, I do believe that it is possible to reason about geometric, you know, word to
7457040	7461680	vex, for example, does this with the assumption that the structure of the
7461680	7466720	are we approaching this at the right level, though, because people have said for quite a
7466720	7471760	long time that there's a difference between syntax and semantics. And you could look at the
7471760	7476880	geometrical structure of spoken language. Or, for example, you could look at the topology of
7476880	7481920	the connections in your brain, the topology of, you know, reference frames in your brain is how
7481920	7488400	you actually have learned concepts. Would looking at the topology of spoken language tell you enough
7488400	7497600	about abstract categories? That's a good question. I think that if that kind of information is
7497600	7502880	necessary, like if the atoms by themselves won't tell you everything. One thing that we actually
7502880	7507280	very commonly do in graph representation learning is assume this sort of hierarchical approach,
7507280	7512800	where you have like the ground level with your actual individual notes, and then you come up
7512800	7517040	with some kind of additional hierarchy that tells you either something about intermediate
7517040	7522880	topologies in a graph, or intermediate structures that you care about in this graph, or any abstract
7522880	7527040	concepts you might have extracted. And then there's additional links being drawn between these to
7527040	7533600	kind of reinforce the knowledge that the graph net can capture. So I think if you have knowledge of
7533600	7538080	some abstract concepts that are relevant for your particular task, you can attach them as
7538080	7543040	additional pieces of information to this topology. Of course, the more exciting part is could we
7543120	7547840	maybe discover them automatically? But that is something that I don't think is potentially in
7547840	7554320	scope for this question. When human interpreters need to translate from one language to another,
7554320	7559440	they often need to deal with different structures. I think Turkish is actually an extreme example
7559440	7564640	where the order of words is completely reversed. It implies that you need probably to hold well
7564640	7571040	in computer science terms some kind of a buffer in your brain before you can make the translation
7571040	7576720	to another language. So it definitely imposes certain biological network structure in the brain.
7576720	7583760	Another interesting observation that I read somewhere about the way that people remember
7583760	7590400	certain facts when they speak a certain language. So the particular example that was given is a
7590400	7598400	person can remember a perpetrator of a crime and then gives testimony in court. And the reason
7598400	7604720	is that in some languages, it is more common to use impersonal pronouns and the personal phrases.
7604720	7610000	So you can say, for example, the object was broken. And in some languages, you would say that
7610000	7616640	somebody broke the object. So it appeared that languages were of these more impersonal constructions.
7616640	7622400	People speaking these languages, I have hard time to remember the perpetrator. So the language
7622480	7628720	probably imposes a lot about the way that we perceive world, but it is probably not studied
7628720	7634400	sufficiently. But there may be some fuzzy graph isomorphisms, though, between the languages.
7635280	7638800	I think there's something really magic about graphs. I think that's what we get into, because
7638800	7642400	your lecture series inspired me, actually, Professor Bronstein, where you were talking
7642400	7646800	about all the different applications of graphs. But something that a lot of our guests talk about
7646800	7651360	are knowledge graphs. Expert systems and the knowledge acquisition bottleneck were the
7651360	7657440	cause of the abject failure of good old fashioned AI or some symbolic AI systems in the 1980s. And
7657440	7661760	many hybrid or neuro symbolic folks today are still arguing that we need to have a discrete
7661760	7668000	knowledge graph, either human designed or learned or evolved or emerged or some combination of those
7668560	7672560	things I just said, depending on who you talk to. Now, critically, many go fi people think that
7672560	7677760	most knowledge we have is acquired through reasoning, not learning, right, which is really,
7677760	7682880	really interesting. So by reasoning, I mean extrapolating new knowledge from existing knowledge.
7683520	7687600	It feels like graph neural networks could at least be part of the solution here. And in your
7687600	7692480	lecture series, you mentioned the work by Kramner, which was explainable GNN, where they use some
7692480	7697920	kind of symbolic regression to get a symbolic model from a graph neural network. So do you think
7697920	7703120	there's some really cool work we can do here? There is a little bit of divide in graph learning
7703120	7708160	literature. So people working on graph neural networks, and working on knowledge graphs, even
7708160	7713280	though, at least in principle, the methods are similar. For example, you typically do some form
7713280	7718240	of embedding of the nodes of the graph. Somehow these are distinct communities, probably,
7718240	7723280	historically, they evolved in different fields. Yeah, so the paper of Krammer, this is really
7723280	7729280	interesting because they use graphs to model physical systems, for example, and body problem
7729280	7734240	when they have particles that interact. You can describe these interactions as a graph,
7734800	7740400	and you can use standard generic message passing functions to model the interactions.
7740400	7746320	Now, the step forward that they do is they replace these generic message passing functions
7746320	7752960	with symbolic equations. And not only that this allows to generalize better, but you also have
7752960	7759360	an interpretable system, you can recover from your data the laws of motion, right? And if you
7759360	7764080	think of how much time it took, historically, to people like Johannes Kepler, for example,
7764080	7771280	he spent his entire life on analyzing astronomical observations to derive a law that now bears his
7771280	7777200	name, that describes the elliptic orbits of planets. Nowadays, with these methods, you can
7777200	7783440	probably do it in a matter of seconds or maybe minutes. I think the point that particularly
7783440	7788800	caught my attention in what you asked, Tim, was this interplay between graphs and reasoning
7788800	7795520	and extrapolation and how that supports knowledge. Now, when it comes to how critical is this going
7795520	7802560	to be, it depends on the environment in which you put your agent. Like, is it a closed environment,
7802560	7807760	or is it an open ended environment where new information and new knowledge can come in
7807760	7814000	in principle at any time? This basically do want to build a neural scientist, or do you just want
7814000	7818560	to build a neural exploiter that takes all the information available right now and then draws
7818560	7825280	conclusions based on that. So if the system is closed worlds, you'll probably be able to get
7825280	7831040	away without very explicit reasoning, especially if you have tons of data, because we've seen time
7831040	7836240	and time again that large scale models can kind of pick up on these regularities if they've seen
7836240	7845200	it often enough. But if I give you a solution that involves stacking, for example, n objects,
7845200	7850800	and now I ask you to do the same kind of reasoning with two times n objects, the way in which we
7850800	7856080	optimize neural networks at least today is typically going to completely fall on its back
7856080	7862480	when you do something like this. So if you truly want to take whatever regularities you have come
7862480	7870320	across in the world of the training data and hope to at least reasonably gracefully apply them to
7870320	7879840	new kinds of rules that come in the future, then you probably want your model to extrapolate to a
7879840	7886640	certain extent. And for this, at least my ongoing algorithmic reasoning research algorithms are a
7886640	7894080	very natural area to study under this lens, because they trivially extrapolate, you write an algorithm
7894080	7900640	that does a particular thing on a set of n nodes, you can be you can usually mathematically prove
7900640	7905520	that it's going to do the same thing equally properly, maybe a bit more slowly, if you give it
7905520	7910000	two times n nodes, right? This kind of guarantee typically doesn't come that easily with neural
7910000	7914960	networks. And we found that you have to very carefully massage the way you train them, the
7914960	7919920	kinds of data you feed to them, the kinds of inductive biases you feed into them, in order to
7919920	7924880	get them to do something like this. So if extrapolation is something you truly need, and you know,
7924880	7929040	I think for artificial general intelligence, we're going to want to have at least some degree of
7929040	7934320	extrapolation as new information will become available to our neural scientists, just as
7934320	7941360	you follow the era of time. Basically, for doing something like this, graph neural networks have
7941360	7946160	arisen as a very attractive primitive, because there's been a few really exciting theoretical
7946160	7952080	results coming out in recent years, saying that the operations of a graph neural network align
7952080	7957680	really, really well with dynamic programming algorithms. And dynamic programming is a very
7957680	7963360	standard computational primitive, using which you can express most polynomial time heuristics.
7963360	7968880	So essentially, that's a really good, you know, that's a really good piece of mind result. The
7968880	7973360	unfortunate side of it is that it's a best case result, right? So you can set the weights of
7973360	7977120	a neural network of a graph neural network to mimic a dynamic programming algorithm,
7977120	7982320	more efficiently or with smaller sample complexity. But, you know, there's still a big
7982320	7986720	problem of how do I learn it in a way that it still works when I double the size of my input.
7986720	7992080	And that is in a way what algorithmic reasoning has been largely about. Like, we're trying to make
7992080	7997120	that happen. It's not easy. If you throw the vanilla graph neural network and just input
7997120	8001360	output pairs of an algorithm, it will learn to fit them in distribution the moment you give,
8001360	8005280	like, ask it to sort and array that's twice as big, it's going to completely collapse. So
8005840	8011200	this is the number one thing that the neurosymbolic people say. They say neural networks, they don't
8011200	8018320	extrapolate. They only interpolate, you know, it just, it's a continuous geometric model,
8018320	8024240	learns point by point, transforms the data onto some continuous, smooth, learnable manifold,
8024240	8028080	you interpolate between the data points, you want to have a smooth, you want to have a dense
8028080	8032640	sampling of your data. But you're talking about dynamic programming problems, these are discrete
8032640	8038400	problems that the structure is discontinuous. But how could you possibly learn that within your
8038400	8046800	network? Well, the dynamic programming algorithm could be, could have a discontinuous component
8047680	8052800	for example, if you're searching for shortest paths at some point, you will take an argmax over
8052800	8056880	all of your neighbor's computed distances and use that to decide what the path is.
8057520	8063120	But before you come to the argmax part, there is usually some fairly smooth function being
8063120	8068640	computed actually. So in the case of shortest path computations, you know, Bellman Ford or
8068640	8073760	something like this, you say something very simple, like, I have a value d of s in every
8073760	8078400	single one of my nodes, which is initially infinity everywhere and zero in the source
8078400	8085760	vertex. And then at every point, I say, the distance of my particular node is the minimum
8085760	8092160	of all the distances of my neighbors plus the edge weight, right. And this kind of function is
8092960	8098640	generally more graceful than than taking an argmax. And you can also think of, for example,
8098640	8102560	if you have to compute expected values or something like this, using dynamic programming,
8102560	8107200	that's also one example where actually summing is what you need to do across all of your neighbors
8107200	8113440	or something like this. So yeah, it is true that like, across individual steps, you may be doing
8113440	8120640	like discrete optimization steps. But usually, it's propelled by some kind of continuous
8120640	8124880	computation under the hood. So that's the part that the graph neural network actually simulates.
8124880	8128480	And then the part which does the argmax would be some kind of classifier that you stitch on
8128480	8134080	top of that. So in principle, it's not, yeah, it's not too challenging to massage it into a
8134080	8139760	neural network framework. So one of the, I think one of you mentioned this before, brought up
8139760	8146320	transformers. And, you know, in recent years, we've had, I think about 10 different papers
8146320	8152960	saying transformers are something there is transformers are RNNs, transformers are Hopfield
8152960	8159840	networks. And also transformers are graph neural networks or compute some kind of
8159840	8165440	graph neural networks. Can you maybe speak a bit to that? Are transformers specifically
8165440	8172000	graph neural networks? Or are they just so general that you can also formulate a graph problem in
8172000	8180080	terms of a transformer? Okay, that's a very good question. I would start off by saying,
8180960	8185280	like, I don't want to start this discussion just by saying, yes, transformers are graph
8185280	8189520	neural networks. This is why end of story, because I feel like, you know, that doesn't
8189520	8194000	touch upon the whole picture. So let's let's look at this from a natural language processing
8194000	8199760	angle, which is how most people have come to know about transformers. So imagine that you have a task
8199760	8206640	which is specified on a sentence. And you want to exploit the fact that words in the sentence
8206640	8210880	interact, right? It's not just a bag of words. There is there's some interesting structure
8210880	8216240	inside this bunch of words that you might want to exploit. When we were using recurrent neural
8216240	8220480	networks, we assume that the structure between the words was a line graph. So basically,
8221200	8226960	every word is preceding another word and so on and so forth. And you kind of just linearly
8226960	8234480	process them with a model like LSTM or something. But, you know, basically line graphs, as we know,
8234480	8239920	are not the way language is actually structured. There can be super long range interactions
8239920	8245120	inside language. So subjects and objects in the same sentence could appear miles away from each
8245120	8250080	other. So using the line graph is not the most optimal way of getting that information in the
8250080	8255520	fastest possible in the fastest possible way. So, okay, there's clearly some kind of non trivial
8255520	8261280	graph structure. What is it? Well, it turns out that people cannot really agree what this optimal
8261280	8266480	graph structure is, and it may well be task dependent, actually. So just consider syntax
8266480	8272000	trees, for example, like there's not always a unique way of decomposing a sentence into a syntax
8272000	8276480	tree. And the exact kind of tree you might wish to use to represent a sentence may be different
8276480	8281600	depending on what is the actual thing that you're solving. So, okay, we have a situation where we
8281600	8286080	know that there's some connectivity between the words, but we don't know what that connectivity is.
8286080	8290240	So in graph representation learning, what we typically do when we don't know the graph,
8290240	8294400	as long as the number of objects is not huge, is to assume a complete graph and let the graph
8294400	8300320	neural network figure out by itself what the important connections are. And if I now stitch
8300320	8305840	an attentional message passing mechanism onto this graph neural network, I have effectively
8305840	8314400	rederived the transformer model equation without ever like using this specific transformer lingo.
8314400	8320000	So from this kind of angle, the fact that it's a model that operates over a complete graph
8320560	8326400	individual words, in a way that you know, once you've put all the embeddings to them is permutation
8326400	8332080	equivalent, this describes the central equations of self attention that the transformer uses.
8332080	8337760	The part which I think causes a bit of a divide here is the fact that transformers like the model
8337760	8342640	that was originally presented are not just the equations of a transformer, they're also the
8342640	8347200	positional embeddings of a transformer. And that's the part that sort of gives it a bit more of a
8347280	8354800	central structure. Well, actually, if you look at these sine and cosine waves that get attached to
8354800	8360240	the individual words in an input to a transformer, you will see that you can you can actually derive
8360240	8365200	a pretty good connection between them and the discrete Fourier transform, which actually
8365200	8371920	turned out to be the eigenvectors of a graph Laplacian for a line graph. So essentially these
8371920	8377040	positional embeddings are hinting to the model that you are the decent that these words in a
8377040	8381600	sentence are arranged in a particular way, and you can use that information. But because it's
8381600	8386720	fed in as features, the model doesn't have to use any of that information, like sometimes bag of words
8386720	8392240	is the right thing to do, for example, right. So essentially, the transformer has a bit of a light
8392240	8398640	hint that there's a central structure in there in the form of a line graph. But, you know, the
8398720	8403520	model itself is a permutation equivalent model over a complete graph. And from our lens of the
8403520	8409120	geometric deep learning, it is effectively a special case of an attentional GNN. Now, I think
8409120	8413920	this positional embedding aspect is a super important one. And it could hint to how we might
8413920	8419040	extend these transformers from sentences to more general structures. And I think, Michael, you
8419040	8424000	might have a lot more thoughts on that than I do. So maybe you can say a bit about that.
8424640	8428800	Yeah, so positional encoding has been done for graphs as well. As Petter mentioned,
8428800	8435440	in case of a graph, you can straightforwardly generalize this sine or cosine positional coordinates
8435440	8440000	that are used in transformers using the eigenvectors of the Laplacian. There are other
8440000	8446000	techniques you can actually show that you can make it a message passing type neural network
8446000	8451520	strictly more powerful than traditional message passing. The equivalent vise for 11 graphics
8451520	8458960	or morphism test by using a special kind of structure where positional encoding, for example,
8458960	8465280	if you can count substructures of the graph, such as cycles or rings and so on. And this way,
8465280	8471040	you have a message passing algorithm that is specialized to the particular position in the
8471040	8476640	graph and can, for example, detect structures that the traditional message passing cannot detect.
8477200	8482880	So it is at least not less powerful than the vise for 11 algorithm. And we can actually show
8482880	8488560	examples on which vise for 11 algorithm or traditional message passing fails, whereas
8488560	8494960	this kind of approach succeeds. So the thing I've always wondered about transformers networks
8495520	8499440	are the position tokens. I really don't like them and I want them to go away
8499440	8504720	because it feels very impure, doesn't it? I think what we really want to learn is some kind of higher
8504800	8511680	order structure in the language. And it kind of felt like we were using the position tokens to
8511680	8515840	cheat a little bit. So what I'm trying to get across here is that I think the position of a
8515840	8521520	token in an utterance should be invariant. I mean, clearly, in different languages,
8521520	8526960	the tokens are in different places. In Turkish, the order is completely reversed. And I would
8526960	8533120	like to think that our internal language representation ignores the transmission
8533120	8538480	arrangement given the particular language and the constraint that we only communicate sequential
8538480	8544080	streams of words. However, I do appreciate what Michael is saying above that the position
8544080	8550240	encodings can actually encode more powerful structures like cycles and rings. The key question
8550240	8556480	is, do we actually need to have these structures in natural language? I don't agree that you want
8556480	8561760	to get rid of them. So positional encoding, it's a kind of combination of two worlds. So if you
8561760	8567840	consider a graph, then you're completely agnostic to the ordering of the nodes. This is one of the
8567840	8572720	really key characteristics of graphs and sets more generally that you don't have the ordering
8572720	8578720	of the nodes. The situations and the problems where transformers are applied, you actually do have
8578720	8585680	an order. But you use the graph as Petra described to model different long distance relations
8585760	8592400	between different tokens or words in a sentence. So you want to incorporate this prior knowledge
8592400	8599120	that these nodes are not in arbitrary order, that they have some sentence order. And this
8599120	8603680	principle applied more generally, you can use positional encoding to tell message passing
8603680	8609040	not to apply exactly the same function everywhere on the graph, but to make it
8609040	8613840	specialized for different portions of the graphs or at least make it a possibility. And then
8614560	8620320	the training will decide whether to use this information or not or in which way.
8621520	8625360	It's strange because I don't know whether the order is just a function of the communication
8625360	8631360	medium. So we transmit the tokens in a sequence. And could we then represent them in our brains
8631360	8634560	in a completely different domain where the sequence is no longer relevant? Well,
8634560	8638160	actually a lot of neuroscientists think that our brain is a prediction machine and
8638160	8642400	it's a sequence prediction machine. So the sequence is kind of fundamentally important.
8643360	8647920	Yeah, it's also a function of the specific language that you use. And as we discussed
8647920	8653680	before, there are languages which convey the same meaning with a difference in structure.
8653680	8659440	I want to get a little bit into what you said about essentially what we're doing with these
8659440	8664720	positional encodings is we hint. We hint to the model that there is something here,
8664720	8670480	which is a big break from sort of the old approach, let's say, of an LSTM to say,
8670560	8678480	this is the structure. So with the world of geometric deep learning, I often have the feeling
8678480	8683600	people talk about, they talk about symmetries and we need to exploit these symmetries that are
8683600	8690400	present in the world. And there's almost to me two different groups of these symmetries. So one
8690400	8696320	group is maybe you would call them like exact symmetries or something like this. When I think
8696320	8702640	about Alpha fold, and I think about like a protein, it doesn't, I don't care which side is up, right?
8702640	8707840	Like the protein is the same, the same protein, and there's no reason to prefer any direction
8707840	8714000	over any other direction. However, if I think of like, because people have made this argument
8714000	8718720	for CNNs, for example, they say, well, a CNN is a good architecture because it's translation
8718720	8724640	invariant, right? And essentially, if we want to do object recognition or image classification,
8724640	8730560	translation invariance is like a given, but but it's not, right? It's not a given the pictures
8730560	8736320	that we feed to these algorithms, most often the object is in the middle, most often, you know,
8736320	8742560	it's kind of upright, like the sky is on top, and the floor, yes, I can hold my camera like this,
8742560	8750960	but I don't, right? So it, it seems to be, it seems to be in many cases better to not
8750960	8757760	put the symmetries in there until we're like, really, really, really, really sure that these are
8757760	8765440	actual symmetries, because with more data, it seems the model that does not have the prior inside
8766000	8772960	becomes better than the model that does have the prior inside, if that prior doesn't exactly match
8772960	8779280	the world is, do you think that's a fair characterization of, for example, why transformers
8779280	8784960	with large data all of a sudden beat classic CNN models or come close to them?
8785840	8791760	I think it's, it's always this question of the trade off between how much your, your model and
8791760	8799120	how much you learn and I remember when I was a student, there was this maxim that that machine
8799120	8804960	learning is always the second best solution. And maybe nowadays with deep learning showing
8804960	8810160	some remarkable set of successes, I'm probably less confident in this statement, but it's probably
8810160	8815280	still quite true that the more you know about your problem, the better chances that machine
8815280	8821280	learning will work for it. To me, it makes sense to model as much as possible and learn what is
8821280	8827520	current or impossible to model. And in practice, of course, there is a spectrum of possibilities
8827520	8833760	of how much of these prior assumptions are hardwired into the architecture. And usually,
8833760	8838400	it's a trade off between the, for example, computational complexity availability of the
8838400	8844080	data also hardware friendliness. And if you think of what happened in computer vision,
8844080	8848560	it's probably a good illustration that that convolutional networks have translational
8848560	8853280	environments, for example, as you mentioned, but in many problems, you might benefit from
8853280	8857920	other symmetries such as rotations, again, depends on the application, but imagine that you want to
8858000	8863760	recognize, I don't know, traffic signs, when you can also tilt your car. And you may ask why
8863760	8870160	in these applications as well CNNs are still so popular. And probably one of the reasons is that
8870160	8875840	they met very well to the single instruction multiple data type of hardware architectures
8875840	8882720	that GPUs offer. And you can compensate for explicitly not accounting for rotational symmetry
8882720	8887440	with data augmentation, and more complex architectures and larger training sets. And
8887440	8893760	this is exactly what happened a decade ago, this convergence of three trends, the availability
8893760	8899120	of compute power, right, the GPUs, algorithms that map well to these computational architectures,
8899120	8904800	and these happen to be convolutional networks, and also very large data sets that you can train
8904800	8912960	these architectures on, such as ImageNet. So many of the choices that become popular in the
8912960	8919440	literature are maybe not necessarily theoretically the best ones. So I think in hardware design,
8919440	8924080	there is this phenomenon that is called the hardware lottery, when it's not necessarily the
8924080	8930240	best algorithm and the best hardware that solve the problem, it's just some lucky coincidence,
8930240	8933440	and they are happy marriage that makes them successful.
8935120	8940400	We keep raising this point about how transformers can be seen as special cases of attentional
8940400	8947360	GNNs, but the status quo is that people will use transformers for very many tasks nowadays,
8947360	8952640	and there may well be a good argument for considering them in this completely separate
8952640	8958080	light, and one possible explanation or justification for this is the hardware lottery, because,
8958720	8963440	yes, sure, transformers perform permutation equivalent operations over a complete graph,
8963440	8969520	but they do so in a way that is very, very highly amenable to the kind of matrix multiplication
8969600	8976240	routines that we can support very efficiently on GPUs nowadays. So basically, they can be seen as
8976240	8981680	the graph neural network that has won the current hardware lottery, even in cases where maybe it
8981680	8986640	will make more sense to consider a more constrained graph, especially if you have low data environments
8986640	8993920	or something like this, the potential overheads of running a full graph neural network solution
8993920	9000000	with message passing, which, given its extremely sparse nature, doesn't align that well with GPUs
9000000	9005600	and TPUs nowadays. Sometimes just using a complete graph neural network is the more economical option
9005600	9009840	when you take all factors into account. And that's, and also the fact that they use an attention
9009840	9016480	mechanism, which is kind of a middle ground between a simple diffusion process on a graph,
9016480	9020960	which we just kind of average things together based on the topology, and the full on message
9020960	9025920	passing where you actually compute a full on vector message to be sent across the edges.
9025920	9031520	Like it strikes a nice balance of scalability and still being able to represent a lot of functions
9031520	9036640	of interest, especially when your inputs are just word tokens, right? So like, you know, in a way,
9036640	9042000	it's a GNN that strikes a very nice sweet spot. And that's probably the reason why it's become so
9043680	9048880	popular in current times. Now, of course, there is a chance that hardware, and there's actually a
9048880	9053600	pretty high probability that hardware will catch up to the trends in graph representation learning,
9053600	9059680	and we will start to see a bit more graph oriented hardware. But at least for the time being, yeah,
9059680	9063760	there's a bit of a combination of what's theoretically making the most sense for your problem,
9064320	9069760	and what the hardware that you have right now will support the most easily. Yeah.
9070320	9074880	There is a British startup, I think they have reached recently a unicorn status
9074880	9079120	called Graphcore. And you can already hear from the name of the company,
9079120	9084080	that there is a graph inside, they try to develop hardware that goes beyond the traditional
9084080	9088640	paradigm. Yeah, I'm really interested in the hardware lottery. We had Sarah Hooker on that massive
9088640	9095040	shout out to Sarah. And Janik made a video on the hardware lottery paper as well. I mean,
9095040	9099920	I'd push back a little bit. I think there's also a bit of an optimization and an algorithmic lottery
9099920	9104160	going on. I think there's something very, very interesting about stochastic gradient descent
9104240	9108320	and the kind of data that we're working with. But this actually gets to my next question,
9108320	9113360	which is about why exactly geometry is a good prior and how principled it is. So
9113360	9118560	it seems like these geometric priors are principled. And they have utility because they are
9118560	9123600	low level primitives, right? They're ubiquitous and natural data. But why exactly is it a
9123600	9127920	principled approach to start with things we know, which is to say geometric primitives,
9127920	9132240	and to work upwards from there? You know, what would it look like if we went top down instead?
9132240	9136800	And what makes a good prior? I mean, one way to think about it is the actual function space
9137440	9141840	that you're searching through, you know, this hypothesis space, it's not just about being able
9141840	9146720	to find the function easily in that space, or the simplicity of the function you find.
9146720	9151600	Chalet would say it's the information conversion ratio of that function. So, you know, can you
9151600	9156640	use this function that you found to convert a very small piece of information and experience space
9156640	9162160	into new knowledge or a new ability? But how do you find these functions?
9163440	9170480	One of the points that we try to make in the book is the separation between the domain and
9170480	9175920	the group that you assume on the domain, the symmetry group. So you might have the same domain,
9175920	9182320	like two dimensional grid or two dimensional plane. And for example, the translation group
9182320	9188160	or the group of rotations and translations or the group of rigid motions that also include
9188720	9197840	reflections. So these are completely separate notions. And which one to choose depends on the
9197840	9203360	problem. The choice of the domain really comes from the structure of your data. So if your data
9203360	9208960	comes as an image, then of course, you use a grid to represent it as the choice of the domain.
9208960	9214800	Now, which symmetry group to use is a more subtle point. And it really depends on what you're trying
9214800	9220560	to achieve. You can think of, for example, traffic sign recognition. When a car drives on the road,
9221120	9226720	usually the signs will have certain orientation. It's very unlikely that you will see it upside
9226720	9233440	down. So really the only invariance or the only kind of symmetry you have is translation. So CNNs
9233440	9239040	in this case would work perfectly well. So for example, we have histopathological samples. So
9239040	9244320	you have a slice of tissue that you need to put under the microscope. So you can naturally flip
9244320	9249600	the glass. You don't know how it is oriented. So reflections are also initial transformation.
9249600	9254800	So in the traffic signs, of course, this is not physical unless you see your sign in the back
9254800	9261760	mirror. But in this histopathology example, it is an initial transformation. So the choice of the
9261760	9267920	symmetry group and what makes a good geometric prior is really dictated by the specific problem.
9268480	9276160	It's often very hard, though, to actually choose because we often don't really know, right, coming
9276160	9283200	back to what I said before, if we actually hit the group correctly. And in fact, we've sort of seen
9284160	9289840	the more successful approach. And this might be hardware specific, but it seems the more
9289840	9297200	successful approach is often to actually make data augmentations with respect to what we assume
9297200	9304400	are symmetries. So to know, I think of all the color distortions that we do to images, we rotate
9304400	9310560	them a bit, we rescale them and so on, it will be definitely possible to build architectures that
9310560	9317600	are just invariant to those things. However, it seems to be a more successful approach in practice
9317600	9326400	to put this all into the data augmentations. What's your take on that? How do we choose between
9326400	9332400	putting prior knowledge into augmentations versus putting prior knowledge into the architecture?
9333520	9338800	It's not a binary choice. It's not either your model or your augment with data.
9339760	9344160	One of the key principles that we also emphasize in the book is that, of course,
9344160	9350560	this perfect invariance or equivariance is a wishful thinking. In many cases, you want to get
9350560	9356800	the property that we call geometric stability. You have some transformation that is approximately
9356800	9361440	a group. Or imagine that you have a video where two objects are moving, let's say one car moves
9361440	9366400	left and another car moves right. So there is no global translation that describes the relation
9366400	9371040	between the two frames in this video. The geometric stability principle tells you that
9372000	9378480	if you are close enough to an element of the group, if you can describe these transformations
9378480	9382560	as an approximate translation, then you will be approximately invariant or approximately
9382560	9389280	equivalent. This is actually what happens in CNN. This was shown by Joan. They use this
9389280	9394480	motivation to explain why convolutional neural networks are so powerful. Roughly speaking,
9394480	9399520	if I don't have a translation, but for example, if I have an MNIS digit and you have different
9399760	9405600	styles of the digits, you can think of them as warping of some canonical digits. So in this case,
9405600	9410080	even though it's not described as a translation and the neural network will not be invariant or
9410080	9414800	equivariant to this kind of transformation, it will be stable under these transformations.
9414800	9420160	And that's why data augmentation works in some sense that you're extending your
9420160	9423440	invariance or equivariance class to approximate invariance and equivariance.
9424240	9428400	Taco also had a few interesting things to say about data augmentations versus
9429440	9435760	building the inductive priors into the model. This is Taco. Is your preference towards...
9436880	9442320	I assume it is towards creating inductive priors in the architecture around geometry
9442320	9449840	instead of data augmentation? Oh, that's a good question. I think it depends on the
9450800	9457200	on the problem. Like in some cases, you don't have a choice. So graphs are a great example.
9457200	9462320	The group of permutations is n factorial elements. If n is large, you have 1000 nodes,
9462320	9468160	you're never ever going to be able to exhaustively sample that group. And so it's better to just
9468160	9475040	build it in. And that's also why no graph neural net doesn't respect the symmetry. Nobody's
9475040	9481520	suggesting you should do that by data augmentation. In some other cases, it is
9484080	9490480	somewhat possible to sample a reasonably dense grid of transformations in your group.
9492400	9501440	And indeed, augmentation is turning out to be very important in unsupervised learning and
9501440	9509200	self-supervised learning techniques. So I am actually... I look very positively towards that.
9509200	9515680	I don't think it's wrong to put in this knowledge using data augmentation. But in some cases,
9515680	9522880	like let's say when you're on classifying medical data like cells in a dish, a histopathology image
9522880	9528240	or something, you just know for sure there's translation and rotation symmetries. The cells
9528240	9533760	don't have a natural orientation. And in those cases, I do think it makes sense to build it
9533760	9541600	into the architecture for the simple reason that if you build it into the architecture,
9541600	9546640	you're guaranteed that the network will be equivariant, not just at the training data,
9547600	9553760	but also at the test data. So you never have that the network would make the correct classification
9553760	9558640	for your test data when you have it in one orientation, but when you rotate it, it suddenly
9558640	9566640	does something different, which can happen if even when you present your training images in
9566640	9573680	all possible orientations. So I think for that reason, equivariance does tend to work better
9573680	9579280	in those cases where there's an exact symmetry in the data. We have actually demonstrated that
9579280	9586400	empirically, where for example, in a medical imaging problem of detecting lung nodules in
9586400	9595920	three dimensional CT scans, we started off with a convolutional network with a data augmentation
9595920	9601920	pipeline, which completely tuned what state-of-the-art method at the time. And we simply replaced
9601920	9606640	all the convolutions by group convolutions that respect to rotational symmetries as well as
9606640	9613760	the translations. And we get very significant improvement in performance. So that goes to show
9613760	9620320	that in practice, often data augmentation can't get you all the way. So for the cases where there's
9620320	9627680	an exact symmetry, or where the group of symmetries is very large, I think building it into the network
9627680	9633680	is the way to go for the foreseeable future. But there are many cases where augmentation is also
9634640	9636480	well, where augmentation is the way to go.
9637440	9643200	Fascinating. I'm really interested in this notion that could these symmetries actually be
9643200	9647680	harmful? I know Joanne, for example, spoke about the three sources of error in machine learning
9647680	9652880	models. And one of them is the approximation error. And normally when we get signals, they come to us
9652880	9658080	in a contrived form, don't know, they might be projected onto a planar manifold. And
9658080	9666240	you know, the sky is always up, for example. Does it really help us having these geometrics
9666240	9672960	symmetries as primitives in the model? Yeah, that's a good question. The simple answer is,
9672960	9680720	if your problem doesn't have the exact symmetry, then at least in the limit of having infinite data,
9680960	9687920	building in a covariance is going to be harmful. And it's better to learn the true structure of
9687920	9692320	the data, this approximate symmetry, which you should be able to pick up from the data alone.
9694000	9699600	So that's one thing that still means in a low data regime, it can be very useful,
9699600	9706000	even when the symmetry is approximate. But I would also say that sometimes,
9706560	9712480	in machine learning, we have a tendency to put too much faith into the
9713440	9719920	evaluation metrics and data sets. So we say, we want to solve computer vision. And what we mean
9719920	9725760	is we want to get a high score on ImageNet. And certainly it's true in ImageNet, the images tend
9725760	9732000	to appear in upright position, and they are photographed by humans. So the key objects are
9732320	9736720	in the center most of the time, etc. So these are biases that you could exploit,
9737280	9748000	and you might stop yourself from exploiting them if you build in the symmetry. But that's only a
9748000	9755520	problem if you put on your blinders and you say ImageNet accuracy is the only thing that counts.
9756160	9761040	You might very well think, if you want to build a very general vision engine,
9761040	9766720	it is useful that it still works if suddenly the robot falls over and has to look at the
9766720	9773120	world upside down. So the symmetry can still be there in principle, even if it's not there in
9773120	9780800	practice in your data set. And then there's maybe a robustness versus computational efficiency
9780800	9786080	tradeoff. So yes, maybe you're willing to acknowledge if your robot falls over, you still
9786080	9791920	want it to work. So you want that rotation equivariance. But then again, if we don't have
9791920	9798560	to process the images upside down, in the 99% of cases where the images are upright,
9799360	9804800	we gain some computational efficiency. So there's a tradeoff there. And yeah, I don't
9804800	9809920	think there's a right or wrong answer. It's something you have to look at on a case by case
9809920	9815120	basis. When I put this to Professor Bronstein as well, he also said that you folks were looking at
9815120	9818560	trying to remember how he described it. I think he said there was a kind of representational
9818560	9823840	stability which allowed for approximate symmetries. So it's not necessarily that you're going for
9823840	9829360	these precise symmetries, you're actually looking for a little sort of margin of robustness going
9829360	9832960	to moving into approximate symmetries that you might not have explicitly captured.
9833280	9840960	I agree. I think that's also a very important philosophy and approach. To take the group,
9840960	9847040	think of it as somehow embedded in a larger group, like most of the geometrical symmetries
9847040	9854560	that we think about are somehow a subgroup of diffeomorphisms. And then if you say,
9854560	9859520	I don't want invariance or equivariance, but some kind of stability or smoothness to
9860480	9868880	elements in the group plus small diffeomorphisms, for instance, you might get some of the
9868880	9876240	generalization benefit without unduly limiting the capacity of your model.
9877120	9884160	Is there a hope though that we can get this approximate? Because I see your point, I think,
9884240	9891040	is that, or one of the points is, I think, is that if we program like a symmetry into the
9891040	9895600	architecture, it will be rather fixed, right? We make an architecture translation invariant,
9895600	9901760	it's going to be fully translation invariant. Are there good ways to bring approximate
9901760	9906240	invariances into the space of the architectures that we work with?
9906240	9912720	Yeah, I mean, I have a very quick answer, maybe not too satisfying, but one very simple one,
9912720	9917520	if you think of neural network blocks as like components that implement different symmetries,
9917520	9921200	and then you think of like a calculus of these blocks as you know, building your deep learning
9921200	9928400	architecture. One very simple representational tool that we can use to allow the model to use
9928400	9934720	the symmetry, but also not use the symmetry is the skip connection. So essentially, you could have
9934720	9939520	a model that processes, for example, your graph data using a particular connectivity structure
9939520	9944160	that you want to be invariant to. And you can also use say a transformer that processes things
9944160	9949280	in a completely permutation invariant way over the complete graph. And you can just shortcut the
9949280	9954560	results of one model over the other model, if you want to give also the model the choice to ignore
9954560	9961360	the previous one. So maybe not a very, you know, detailed and satisfying answer, but that's one
9961360	9965760	simple way in which we could do something like this. And in some of our more recent algorithmic
9965760	9970880	reasoning blueprint papers, we do exactly this, because one very important thing that we're trying
9970880	9977280	to solve is apply classical algorithms to problems that would need them. But the data is super rich,
9977280	9982000	and it's really hard to, you know, massage it into the abstractified form that the algorithm needs.
9982000	9986800	For example, you want to find shortest paths in a road network, a real world road network,
9986800	9991760	you cannot just take all the complexity of changing weather conditions, changing diffusion
9991760	9996480	patterns on the on the roads and the roadblocks and all these kinds of things and turn that into
9996480	10001040	this abstractified graph with exactly one scalar per each edge. So you can apply dykstra or something
10001040	10006160	like this, like, it's just not feasible without losing a ton of information. So what we're doing
10006160	10011280	here is we make this high dimensional neural network component that simulates the effects of
10011280	10017520	dykstra. But we're also mindful of the fact that to compute say the expected travel time,
10017520	10022160	there's more factors at play than just the output of a shortest path algorithm, right?
10022160	10027520	There could well also be some flow related elements, maybe just some elements related to the
10027520	10033760	current time of day, human psychology, whatnot, right? So we start off by assuming the algorithm
10033760	10041040	does not give the complete picture in this high dimensional noisy world. So we always, as default,
10041040	10045840	as part of our architecture, incorporate a skip connection from just, you know, a raw neural
10045840	10051280	network encoder over the algorithm. So in case there's any model free information that you want
10051280	10056240	to extract without looking at what the algorithm tells you, you can do that. So maybe I don't
10056240	10060880	know, Yannick, if that answers your question about approximate symmetries, but that's, that's the
10060880	10065440	kind of divide by God's when I heard the question. I mean, that's a very, that's a very practical
10066000	10071920	answer for sure that that, you know, you can actually get out there. It even opens the possibility
10071920	10079760	to having maybe multiple kinds of skip connections and whatnot, you know, having dividing up your
10079760	10087120	symmetries into individual blocks that are run in parallel, maybe. But that brings me to maybe
10087120	10091440	another thing we've talked about, you know, there are symmetries, you want to incorporate them into
10091440	10097360	your problem and so on. And we've also talked about the symbolic regression beforehand to maybe
10098320	10104880	parse out symmetries of the underlying problem. What are the current best approaches if we don't
10104880	10110960	know the symmetries? So we have a bunch of data, we suspect there must be some kind of
10110960	10117360	symmetries at play because they're usually are in the world, right? And they, if we knew them,
10117360	10123440	we could describe our problems in very compact forms and solve them very efficiently, but we don't
10123440	10130320	often know. So what are the current state of the art? When I don't know the symmetries, how do I
10130320	10135920	discover what group structure is at play in a particular problem? I don't think that there is
10135920	10141840	a single approach that solves this problem in a satisfactory manner. And one of the reasons why
10141840	10146880	because the problem is ambiguous. So maybe an example, think of objects mostly translate
10146880	10152480	horizontally, but you also have a little bit of vertical translation. What is the right
10152480	10156240	symmetry structure to model? Is it a one dimensional translation group or a two dimensional
10156240	10163520	translation group? Do we want to absorb the vertical, the slight vertical motions as the noise
10163520	10168960	and deal with it as a data augmentation, or you want to describe it in the structure of the group
10168960	10174960	that you discover? So there is no single answer. So you cannot say that one is correct and another
10174960	10180240	one is wrong. Yeah, I think this was kind of where I was going with the question of how
10180240	10186080	principled are the symmetries? And the symmetries seem to be hierarchical just in the same way that
10186080	10192320	geometries are hierarchical. You were saying that, for example, the projective geometry is kind of
10192320	10197360	subsumes Euclidean geometry, but I had a little thought experiment. So imagine I gave you a large
10197360	10203440	data set produced by a recursive fractal pattern. Now nature is full of fractals, trees, rivers,
10203440	10209760	coastlines, mountains, clouds, seashells, and even hurricanes. So let's say I didn't tell you the
10209760	10214480	simple rule which produced this pattern. Now what kind of regularities would you look for in the
10214480	10219120	model that you built? I mean, it seems obvious that there would be an expanding scale symmetry,
10219120	10224320	which might resemble the original rule. But it feels like there'd be plenty of other emergent,
10224320	10228640	abstract symmetries which are not obviously related to the simple rule which produced the pattern.
10228640	10233920	I mean, Janik was just saying, when you look at computer vision, you see a kind of regularity
10233920	10238960	or invariance to color shifts, for example. So our fractals are good analogy for physical
10238960	10243680	reality. And should we be looking for the low-level primitive regularities which I think you're
10243680	10248080	advocating for? Or should we be looking at more abstract emergent symmetries which appear?
10249040	10256480	In the 90s, there was a famous paper by Michael Barclay on fractal coding. And they claimed really
10256480	10262240	unbelievable compression ratios for natural images. And the way it worked was to try to
10262240	10267360	reassemble the image from parts of itself. And possibly, of course, you can take parts and
10267360	10271760	subject them to some geometric transformation. So roughly, if you have a page of pixels,
10272400	10277760	you can approximate it as another page taken from somewhere else in the image that you translate,
10278720	10284160	rotate, and scale. And then the image was represented as an operator that makes such a
10284160	10289280	decomposition. And this operator was constructed in a special way to be
10289280	10295200	contractive. And then they used the Banach-Fix point theorem that you can apply this operator
10295200	10300160	to any image. So you can start with the noise for example, completely random image. And you have
10300160	10306960	the target image emerge after a few iterations. So that this iterative scheme will converge to the
10306960	10312560	fixed point of the operator, which is the image itself. And it was actually used in the industry,
10312560	10319440	well, Microsoft and CARTA encyclopedia. I don't know how many viewers are old enough to remember it.
10320480	10324960	But the main issue was the difficulty to build such operations. The compression
10324960	10331360	was very asymmetric. It was very easy to decode. You just take any image and apply this operator
10331360	10336960	multiple times. But it was really very hard to encode. And in fact, some of these constructions
10336960	10343120	that showed remarkable compression ratios were constructed semi by hand. I should say that in
10343120	10347200	more recent times in computer vision, for example, the group of Michali Rani from the Weizmann
10347200	10353120	Institute in Israel used similar ideas for super resolution and image denoising where you can
10353120	10360240	build the clean or higher resolution image from bits and pieces of the image itself. So it's a
10360240	10368080	single image denoising or super resolution. But what you do is you try to use similarities across
10368080	10374400	different positions and scales. That's absolutely fascinating. I mean, I spend a lot of time thinking
10374400	10379200	about this because my intuition is that deep learning works quite well because of the strict
10379200	10384160	structural limitations of the data which is produced by our physical world, right? And
10384160	10390240	would you say that physical reality is highly dimensional or not? If it's highly dimensional,
10390240	10394720	is it because it emerged from a simple set of rules or relations like we were just talking
10394720	10399840	about? Because I think what you're arguing for is that it could be collapsible in some sense.
10400800	10406400	Probably the term dimension is a bit frivolously used here. But I would say that it's
10407360	10412400	probably fair to say that at some scale, many physical systems can be described with a small
10412400	10418800	number of degrees of freedom, parametres that capture the system. And as we are talking,
10418800	10423760	I'm sitting in a room, I'm surrounded by probably a quadrillion of gas molecules in the air that
10423760	10427680	fly through the room and collide with each other and the walls of the room. So at the
10428800	10435200	microscopic level, the dimension is very high. So it's absolutely intractable if I were to model
10435200	10440960	each molecule and how it collides, I will have a huge number of degrees of freedom. And yet if we
10440960	10446800	zoom out, we can model the system statistically, and that's exactly the main idea of thermodynamics
10446800	10454000	and statistical mechanics. And this macroscopic system is surprisingly simple. It can be described
10454000	10459920	by just a few parameters such as temperature. And the example of fractals that you brought up before
10460800	10465920	essentially show that you can create very complex patterns with very simple rules that
10465920	10472640	apply locally in a repeated way. This might be a question for you, Peter. The geometric blueprint
10472640	10478240	works brilliantly in the ideal world where we can compute all of the possible group actions. But
10478880	10483520	graph neural networks, for example, you know, the permutation group is factorial in size,
10483520	10488800	which means we need to rely on heuristics like graph convolutions. So how much better would graph
10488880	10493600	neural networks be if we could compute all of the permutations? I mean, are you happy with these
10493600	10501360	heuristics in general? So that is a very good question. And yes, so let's just start from stating
10501360	10507760	the obvious. If you want to explicitly express every possible operation that properly commutes
10507760	10513840	with the graph structure, and in that sense is a graph convolution, you would not be able to
10513840	10518880	represent that properly as a neural network operation because you have to store in principle
10518880	10524320	a vector for every single element of the permutation group. So unless your graph is super
10524320	10532080	tiny, that is just not going to work. So on one hand, this is a potentially annoying result.
10532080	10539360	On another hand, it is also exciting because we know that even though we ended up like doing most
10539360	10544400	of our graph neural network research in this very restricted regime of I'm going to define a
10544400	10549920	permutation invariant function over my immediate neighbors, and that will as a result translate
10549920	10555040	into a permutation equivalent function over the whole graph. Even though most of our research has
10555040	10561440	happened in that particular area, we know from this result that there actually exists a huge wealth
10561440	10569200	of very interesting architectures beyond that. And I think one of the potentially like earliest
10569200	10573840	examples that have demonstrated that there exists this wealth of space is actually one of
10574720	10581280	Jean Bruno's earlier papers on the graph Fourier transform that, you know, analyzing from a pure
10581280	10586880	signal processing angle, they have shown that you can represent basically every proper graph
10586880	10594160	convolution as just, you know, parameterizing its eigenvalues with respect to the eigenvectors of
10594160	10600560	the graph Laplacian. So, but the big issue that kind of limits us from going further in this
10600560	10606000	direction right now is the issue that Michael highlighted of geometric stability. So basically,
10606000	10611280	a lot of these additional graph neural networks that do something more interesting than one
10611280	10618320	hop spatial message passing pay the price in being very geometrically unstable. So the graph Fourier
10618320	10625600	transform in its most generic form will have every single node in the graph be updated based
10625600	10631200	on whatever is located in any other node in the graph, very conditional on the graph topology. So
10631200	10636800	if you imagine any kind of approximate symmetry, any kind of perturbation either in the node features
10636800	10642240	or the edge structure of the graph, this, you basically don't have any protection against
10642240	10646880	that like that error is going to immediately propagate everywhere. And as a result, you'll
10646880	10652160	end up with a layer that theoretically works really well. But in practice is very unstable to
10652160	10657280	these kinds of numerical or inaccuracy issues. One thing that's also very important to note is that
10657280	10661680	often in graph neural networks, we have this subtle assumption of we have the graph and we're
10661680	10665440	using this graph that's given to us. But who guarantees that the graph that's given to you is
10665440	10671600	the correct one actually very often, we estimate these graphs based on very, very weird heuristics
10671600	10677600	ourselves. So basically, all of these kinds of perfection assumptions are what might limit the
10677600	10682320	applicability of these kinds of layers. But that being said, I find it comforting that these layers
10682320	10687360	exist, which means that there are meaningful ways to push our research forward to potentially
10687360	10693680	discover new, you know, wonderful basins of geometric stability inside these different,
10693680	10698160	you know, layers that may not just do one hop message passing. So that's my take on this,
10698160	10702400	like it gives me, it gives me faith that there's more interesting things to be discovered.
10702960	10707760	That being said, it is pretty tricky to find stable layers in that vast landscape.
10709040	10712720	Michael, do you have some thoughts on this as well? I know you've worked quite a bit on these
10712720	10720400	geometric stability aspects. I just wanted to add one thought about it that essentially,
10720400	10726240	locality is a feature, not a bug in many situations. And in convolutional neural networks,
10726240	10731600	actually, if you look again, historically, the early architectures like AlexNet, they started
10731600	10735680	with very large filters and the few layers or relatively few layers, I think something like
10735680	10741680	five or six. And nowadays, what you see is very small filters and hundreds of layers.
10741680	10746400	One of the reasons why you can do it is because of compositionality properties. So you can,
10746400	10752960	you can create complex features from, from simple primitives. So in some other cases,
10752960	10759920	like, like manifolds, there are deeper geometric considerations why you must be local, so related
10759920	10766320	to what is called the injectivity radius of the manifold. On graphs, well, maybe we like a little
10766320	10772880	bit the theoretical necessity to be local, besides, of course, the computational complexity. But
10773520	10778960	in many cases, it is actually a good property because many problems do not really depend on
10778960	10784800	distant interactions. So if you think of social networks, probably most of the information comes
10784800	10790880	from your immediate neighbors. Is there some sort of, let's assume I, you know, I have a graph,
10791680	10800640	and I have my, my symmetries, my groups that I suspect there are in the problem, or I want to
10800640	10807680	be invariant to, is there like, can you give us a bit of a practical blueprint of how would I build
10807760	10815440	a network that, you know, takes this as an input and applies this? How would you go about this,
10815440	10820560	you know, what would be the building blocks that you choose, the orders and so on? Is there
10820560	10825760	overarching principles in, in how to do? I don't think that there is really a general
10825760	10831360	recipe. So it's problem dependent. But maybe one example is applications in chemistry. The basic
10831360	10835920	structure that they have in graph is a privatization invariance. This has to do with the
10835920	10840240	structure of the graph itself. It says nothing about the structure of the features.
10840240	10845760	You might also have some secondary symmetry structure in the feature space. In case of molecules,
10845760	10850800	for example, you might have a combination of features. Some of them are geometric. So it's
10850800	10855920	actually not an abstract topological graph. It's a geometric graph. A molecule is a graph that
10855920	10861200	lives in three dimensional space. And so the features are the positional coordinates of the
10861200	10865760	nodes, as well as some chemical properties such as atomic numbers. Now, when you deal with the
10865760	10870160	molecule, you usually don't care about how it is positioned in space. It wants to be
10870160	10876160	equivariant to rigid transformations. And therefore you need to treat accordingly the
10876160	10880480	geometric coordinates of the nodes of this graph. And this is actually what has been successfully
10880480	10886000	done. So when you do, for example, virtual drag screening, architectures that do message passing,
10886000	10891040	but in a way that is equivariant to these rigid transformations actually are more successful
10891040	10896320	than generic graph neural networks. Also, this principle was exploited in the recent version
10896320	10902720	of AlphaFold, where I think they call it point invariant attention, which is essentially a form
10902720	10910240	of a latent graph neural network or a transformer architecture with equivariant message passing.
10910240	10915200	Yeah, I'd just like to add one more point to this conversation, which is maybe a bit more
10915200	10922320	philosophical, but it relates to this aspect of building the overarching symmetry discovering
10922960	10928720	procedures, which I think would be a really fantastic thing to have in general. And I hope that
10929440	10936400	some component of a true AGI is going to be figuring out, making sense of the data you're
10936400	10942880	receiving and figuring out what's the right symmetry to bake into it. I don't necessarily
10942880	10949920	have a good opinion on what this model might look like. But what I do say is just looking at the
10949920	10956080	immediate utility of the geometric deep learning blueprint, we are like, I think very strictly
10956080	10962080	saying that we don't want to use this blueprint to propose, you know, the one true architecture.
10963520	10969120	Rather, we make the argument that different problems require different specifications,
10969120	10974560	and we provide a common language that will allow, say, someone who works on primarily grid data to
10974560	10979040	speak with someone who works on manifold data without necessarily thinking that, you know,
10979920	10984480	you know, somebody might say, commonets are the ultimate architecture. Someone else might say
10984480	10988880	GNNs are the ultimate architecture. And in some ways, they could both be right and they could
10988880	10994960	both be wrong. But this blueprint kind of just provides a clear delimiting aspect to these
10994960	11000880	things, just like in the 1800s, you had all these different types of geometries that basically lived
11000880	11007520	on completely different kinds of geometric objects, right? So hyperbolic, elliptic, and so on and so
11007520	11013760	forth. And what Klein-Zerlangen program allowed us to do was, among other things, reason about
11013760	11018640	all of these geometries using the same language of group invariance and symmetries, right? But in
11018640	11024560	principle, the specifics of whether you want to use a hyperbolic geometry or whether you want to use
11024560	11029840	an elliptic geometry, partly rests on your assumption what the main do you actually live in,
11029840	11036160	right? When you're doing these computations. So I think just generally speaking, I think that
11036160	11041280	having a divide is a potentially useful thing, as long as you have a language that you can use
11041280	11048240	to index into this divide, if that makes sense. It does make sense. But I have a feeling that some
11048240	11054000	people could benefit from geometric deep learning in their runaways. I mean, I don't want you guys
11054000	11058800	to motivate geometric deep learning in general, because I think, you know, a lot of deep learning
11058800	11063120	with a structured prior is already geometric deep learning is as you folks demonstrated in
11063120	11067520	your blueprint, you know, like RNNs and CNNs, for example. So, you know, like it or not, we're already
11067520	11072160	all using geometric deep learning. But some of the esoteric flavors of geometric deep learning,
11072160	11076720	particularly on irregular meshes, they seem a little bit out there, don't they? I mean, it's
11076720	11080640	possible that many people could benefit from this, but they just don't know about it yet. I was
11080640	11085760	thinking that, for example, if I had a LiDAR scanner on my phone, and the result is a point cloud,
11085760	11091360	which is not particularly useful. But I would presumably transform it into a mesh, which would
11091360	11095520	be more useful. But is it possible that loads of data scientists out there are sitting on data sets
11095520	11099920	that they could be thinking about geometrically, but they're not? Many folks are exotic. It's
11099920	11104560	probably in the eyes of the beholder. And well, in machine learning, probably they are, to some
11104560	11111120	extent, exotic. But joking apart, many folks are a convenient model for all sorts of data.
11111680	11116400	And the data might be a high dimensional, but still have a low intrinsic dimensionality or
11116400	11121440	can be explained by a small number of parameters or degrees of freedom. And this is really the premise
11121440	11127360	of nonlinear dimensionality reduction. And for example, the reasons why data visualization
11127360	11133040	techniques such as TSE and E at all work. And maybe the key question, as you're asking is,
11133120	11138320	how much of the manifold structure of the continuous manifold you actually leverage? And
11138320	11143200	in the TSE example, the only structure that you really use is local distances.
11144160	11150000	So if you think of a point cloud, of course, you can deal with it as a set. But if you assume
11150000	11154560	that it comes from sampling of some continuous surface, you can probably say more. And this is
11154560	11160480	forgivable what we tried to do in some of our works on geometric deep learning in applications
11160480	11166880	in computer vision and graphics. And measures are one way of thinking of them is as graphs and
11166880	11171520	steroids, where we have additional structure to leverage. So it's not only nodes and edges,
11171520	11176160	but also faces. And in fact, measures are what is called simplicial complexes.
11176800	11182480	As to the practical usefulness, computer vision and graphics are obviously the two fields where
11183040	11187760	geometric deep learning on measures is important. And just to give a recent example
11187760	11194560	of a commercial success. There was a British startup called the AI. It was founded by
11194560	11199360	my colleague and friend, Yasunos Kokinos. I was also one of the investors. And we had a
11199360	11204720	collaboration on three different reconstruction using geometric neural networks. And the company
11204720	11210960	was acquired last year by SNAP and these technologies already now part of SNAP products. So you see it
11211040	11218400	in the form of some 3D avatars or virtual and augmented reality applications that SNAP is
11218400	11223760	developed. Professor Bronstein, I saw that you were doing some really cool stuff with the higher
11223760	11229200	order simplicial coverings in graphs. And actually, I was going to call out your recent work on
11229200	11235120	diffusion operators and graph rewiring. There are so many cool things that we can do to graphs to
11235120	11240000	actually enable a little of this analysis. But there was a question from my good friend,
11240080	11245680	Zach Jost, who's one of our staff members here. And he says, what do you think is the most important
11245680	11249920	problem to solve with message passing graph neural networks? And what's the most promising path
11249920	11255120	forward? Probably we first need to agree about terminology. And to me, message passing and
11255120	11262560	here I agree with Petra is just a very generic mechanism for propagating information on the
11262560	11267760	graph. Now, traditional message passing that is used in graph neural networks uses the input
11267760	11272800	graph for this propagation. And we know, right, as we discussed that it is equivalent to device
11272800	11279200	for a lemon graph isomorphism test that has limitations in the kinds of structures it can
11279200	11285120	detect. Now, there exists topological constructions that go beyond graphs, such as simplicial and
11285120	11290720	cell complexes that you mentioned. And what we did in our recent works is developing a message
11290720	11296320	passing mechanism that is able to work on such structures. And of course, you may ask whether
11296320	11302720	we do encounter such structures in real life. So first of all, we do the measures that I mentioned
11302720	11308320	before are in fact, simplicial complexes. But secondly, what we show in the paper is that we
11308320	11312960	can take a traditional graph and lift it into a cell or simplicial complex. And probably a good
11312960	11318640	example here is from the domain of computational chemistry, the graph neural network that you
11318640	11323040	apply to a molecular graph would consider a molecule just as a collection of nodes and edges,
11323040	11328080	atoms, and chemical bonds between them. But this is not how chemists think of molecules.
11328080	11333760	They think of them as structures such as, for example, aromatic rings. And with our approach,
11333760	11339440	we can regard the rings as cells. So we have a special new object, and we can do a different
11339440	11343920	form of message passing on them. And we can also show from the theoretical perspective that
11344640	11348640	this kind of message passing is strictly more powerful than the vice for a lemon algorithm.
11349280	11356320	Do you see entirely new problems opening up that we wouldn't even have, let's say,
11356320	11362960	we wouldn't even have dared to touch before, you know, in, let's say, we simply have our
11362960	11367920	classic neural networks or whatnot, or even our classic graph message passing algorithms.
11367920	11375120	Do you see new problems that are now in reach that previously with none of these methods were
11375120	11381680	really, let's say, better than random guessing? It's a very interesting question that I think
11381680	11387040	I'll answer from two angles, because you could, like, there could be like some
11387040	11392560	longstanding problem that you knew about and wouldn't dare to attack. And now maybe you feel
11392560	11397120	a bit more confident to attack it. There's also the aspect of uncovering a problem,
11397120	11401680	because when you start thinking about things in this particular way, you might realize,
11401680	11405840	hang on, to make this work, I made some assumptions. And those assumptions actually
11405840	11410160	don't really hold at all in principle. So how do I make things, you know, a little bit better?
11410160	11416960	So I'll try to give an example for both of those. So in terms of a problem that previously,
11416960	11423040	I don't think was very easy to attack. And now we might have some tools that could help us attack
11423040	11430080	it better. I have a longstanding interest in reinforcement learning. Actually, when I
11430160	11436000	started my PhD, I spent six months attacking a reinforcement learning problem with one super
11436000	11442320	tiny GPU. And that was, at that time, a massive time sink. Actually, DeepMind ended up scooping
11442320	11447920	my work sometime after that. And I quickly moved to things that were more, you know, doable with
11447920	11452800	the kind of hardware that I had at the time. But, you know, I always had a big interest in this area.
11452800	11458320	And after joining DeepMind, I started to contribute to these kinds of directions more and more.
11458960	11467760	And I think that basically, there are a lot of problems in reinforcement learning
11468560	11475920	concerning data efficiency. So when you have to learn how to meaningfully act and do stuff,
11475920	11481200	which is actually a fairly like low dimensional signal compared to the potential richness of
11481200	11485760	the trajectories that you have to go through before you get that useful signal, like long
11485760	11491760	term credit assignment, all these kinds of problems, I feel like we can start to get more
11491760	11497520	data efficient reinforcement learning architectures by leveraging geometric concepts and also
11497520	11504720	algorithmic concepts. So to give you one example of this, we have some months ago put out a paper
11504720	11510880	on the archive called the executed latent value iteration network or x selvin, where we have
11511200	11518640	captured the essence of an algorithm in RL, which perfectly solves the RL problem. So the value
11518640	11523040	iteration algorithm, assuming you give it a Markov decision process will give you the perfect
11523040	11527600	policy for that Markov decision process. So it's a super attractive algorithm to think about when
11527600	11533360	you do RL, big caveat, right? You need to know all the dynamics about your environment, and you need
11533360	11538320	to know all the reward models of the environment before you can apply the algorithm. So this obviously
11538400	11543760	limits its use in the more generic deep reinforcement learning setting. But now with the knowledge of
11544320	11549120	the underlying geometry of the graph of states that the MDP induces and the algorithmic reasoning
11549120	11553360	blueprint, we actually taught the graph neural network, which aligns super well with value
11553360	11559200	iteration, actually, we taught it to in a nicely extrapolating in a reasonably extrapolating way
11559840	11565280	on a bunch of randomly sampled MDPs, learn the essence of the value iteration computation,
11565360	11569920	and then we stitched it into a planning algorithm in a deep reinforcement learning setting.
11569920	11574880	And just by like training this pipeline end to end with a model free loss, we were able to
11575520	11581600	get interesting returns in Atari games much sooner than some of the competing approaches. So
11581600	11586640	it's a very small step. It still requires, you know, 100,000 200,000 iterations of
11587360	11591440	playing before you get meaning, some meaningful behaviors start to come out. But
11592080	11597120	it's a sign that we might be able to move the needle a bit backwards and not require, you know,
11597120	11602160	billions and billions of transitions before we start to see meaningful behavior emerge. And I
11602160	11606160	think that's very important because in most real world applications of reinforcement learning,
11606160	11611040	you don't have a budget for billions of interactions before you have to already learn a
11611040	11615440	meaningful policy. So that's one side, I think. And just generally in reinforcement learning,
11615440	11620560	graphs appear left, right and center, not just in the algorithms, but also in the structure of the
11620560	11624720	environment and these kinds of things. So I think that's one area where geometric deep learning
11624720	11629680	could really help us, you know, get better behaviors faster, not necessarily solve it
11629680	11634560	better than the standard deep RL, but, you know, get the better behaviors and fewer interactions.
11634560	11640240	And as for one problem that we have uncovered through this kind of observational lens,
11641280	11645600	you know, as I said, often in graph representation learning, we assume innocently that the graph
11645600	11653280	is given to us, whereas very often this is not the case. So this divide has brought about this new
11653280	11658480	emerging area of latent graph learning or latent graph inference, where the objective is to learn
11658480	11664800	the graph simultaneously with using it for your underlying decision problem. And this is a big
11664800	11668800	issue for neural network optimization, because you're fundamentally making a discrete decision in
11668800	11673840	there. And if the number of nodes is huge, you cannot afford to start with the n squared approach
11673920	11679280	and then gradually refine it. So currently, the state of the art in many regards of what we have
11679280	11684720	here is to do a K nearest neighbor graph in the feature space, and just hope that that gets us
11684720	11690000	most of the way there. And usually this works quite well for getting, you know, interesting answers,
11690000	11694400	because, you know, if you have a decent ish enough KNN graph, you will cover everything that you need
11694400	11700320	reasonably quickly. But, you know, then there that raises the issue of what if the graph itself is a
11700320	11704640	meaningful output of your problem, what if you're a causality researcher that wants to figure out
11704640	11709040	how different, you know, parts of information interact to them, they probably wouldn't be
11709040	11713920	satisfied with a K nearest neighbor graph as an output of the system. So yeah, I feel like there's
11713920	11719920	a lot of work to be done to actually scalably and usefully do something like this. And I don't
11719920	11724240	have a better answer than what I just said is the state of the art. So a potential open problem
11724240	11728160	for everybody in the audience today. Absolutely. And you touch on some really interesting things
11728160	11733120	that I think causality is a huge area that we could be looking at graphs on. And also we had
11733120	11738480	Dr. Tom Zahavi from DeepMind, one of your colleagues, and he said that, you know, he looks at
11738480	11743280	meta learning and also diversity preservation in in agent based learning. But he thinks that
11743280	11747440	reinforcement learning is just about to have its image net moment where we can discover
11747440	11751760	a lot of the structure in these problems, which is fascinating. I would like to bring up
11752800	11757680	one application where maybe quantitative improvement that is afforded by a genetic
11757680	11762480	deep learning can lead to a qualitative breakthrough. And this is a problem of
11763120	11768560	structural biology. Alpha fold is one such example for correctly geometrically modeling
11769200	11774480	the problem you get a breakthrough in the performance. So it's indeed an image net
11774480	11779840	moment that happened in this field. And now once you have sufficiently accurate
11780720	11785200	prediction of 3D structure of proteins, it suddenly enables a lot of interesting applications
11785200	11790080	for example, in the field of drug design. So potentially entire pharmaceutical
11790080	11796080	pipelines we invented with the use of this technology. And the impact can be extraordinary.
11796800	11800720	It's so true. I mean, Professor Bronstein, when I was watching your lecture series,
11800720	11804800	it blew me away when you were talking about all of the applications. I think Yannick said a minute
11804800	11810960	ago that it's almost as if some of these applications are just so ambitious that the thought
11810960	11814560	wouldn't even have crossed our mind that we might be able to do it before, like for example,
11814560	11820320	being able to predict facial geometry from a DNA sequence. So we might be able to look at an old
11820320	11825120	DNA sequence and actually see what that person looked like. That would have been unimaginable
11825120	11830480	just a few years ago. So that's incredible. I'm really interested in your definition
11830480	11834000	of intelligence, right? And whether you think neural networks could ever be intelligent.
11834960	11839440	Douglas Hofstadter, for example, he wrote the famous book Godel Escher Bach. It was a
11839440	11845360	Pulitzer Prize winning book in the 1970s, but he made the argument that analogy is the core of
11845360	11850880	cognition. He said that analogies are a bit like the interstate freeway of cognition.
11850880	11855840	They're not little modules on the side or something like that. And I think that in a way,
11855840	11862080	analogies are also symmetries, right? So when I say that someone is firewalling a person,
11862720	11867200	it means that they don't want to talk with that person. It's a symmetry between the abstract
11867280	11872560	representation of a real network firewall and an abstract social category.
11872560	11878640	So does this require a different neural network architecture or could geometric deep learning
11878640	11882800	already deliver the goods, right? It's almost as if it's just a representation problem.
11882800	11889120	I think it's a very important question, one which I cannot claim to have the right answer to. And
11889120	11895200	my definition is I guess a little bit skewed by the specific research that I do and the
11895200	11901200	engineering approaches that I do. But I think in large, I agree with the idea of analogy making,
11901200	11906800	and maybe I would take it a step further, right? Where you have a particular set of
11906800	11912560	knowledge and conclusions that you've derived so far, a set of primitives that you can use once
11912560	11917280	your information comes in to figure out how to recompose them and either discover new analogies
11917280	11921920	or just discover new conclusions that you can use in the next step of reasoning.
11922560	11930320	And it just feels really amenable to a kind of synergy of, as Daniel Kahneman puts it,
11930320	11935680	System 1 and System 2, right? You have the perceptive component that feeds in the raw
11935680	11941040	information that you receive as your input data, transforms it into some kind of abstract
11941040	11947280	conceptual information. And then in the System 2 land, you have access to this kind of reasoning
11947280	11953280	procedures that are able to take all of these concepts and derive new ones from hopefully a
11953280	11960320	nice and not very high dimensional set of roles. And this is why I believe that if we, that in
11960320	11966240	terms of like moving towards the, an architecture that supports something like this, I think we
11966240	11970000	have a lot of the building blocks in place with geometric deep learning, especially if we're
11970000	11974080	willing to, as I said, kind of broaden the definition of geometric deep learning to also
11974080	11979200	include category theory concepts because that might allow us to reconcile algorithmic computation
11979200	11984800	as well into the blueprint. So the idea is, you know, you have this, I mean, there's no need to
11984800	11989760	talk at length about all these great perceptive architectures. So I think we're already at a
11989760	11995840	point of, if we show our AGI lots and lots of data, it's going to be able to pick up on a lot of
11995840	11999280	the interesting things just by observing, like, you know, self-supervised learning,
11999280	12003680	unsupervised learning is already showing a lot of promise there. But then the question is,
12003680	12008400	where I think we still have quite a bit of work to do is once we have these concepts,
12008400	12013040	let's even assume that they're perfect. What do we do with them? How do we meaningfully use them?
12013040	12017760	And I think the reason why I believe there's a lot of work to be done there is because one of the
12018320	12023440	very key things that I do on a day to day basis is teach graph neural networks to imitate algorithms
12023440	12028320	from perfect data. So I give them exactly the abstract input that the algorithm would expect.
12028320	12034800	And I ask them, hey, simulate this algorithm for me, please. And it turns out that that is super,
12034800	12039280	super hard. Well, it's super easy to do it in distribution, but you're not algorithmic if you
12039280	12044400	don't extrapolate. And that's, I think, one of the big challenges that we need to work towards
12044400	12050160	addressing. Will geometric deep learning be enough to encompass the ultimate solution? I have a
12050160	12055920	feeling that it will. But, you know, I don't necessarily just based on the empirical evidence
12055920	12062080	we've been seeing in the recent papers that we've put out. But yeah, I don't I don't have a very
12062080	12068960	strong theoretical reason why I think it's going to be enough. Yeah, I'm fascinated by this notion
12068960	12075120	that intelligence isn't mysterious as we might think it is. It's a it's a receding horizon. And
12075120	12082480	it might in the end be disappointingly simple to mechanize. Actually, if you take the term literally,
12082480	12088160	intelligence come from the Latin word that means to understand. And I think what is meant by
12088160	12092160	understanding is really a very vague question. And probably if you ask different people, they
12092160	12098160	will give you different definitions. I will define it as the faculty to abstract information.
12098800	12103280	And in particular, information that is obtained in one context, the ability to use it in other
12103280	12109120	contexts, this is what we usually call learning. The way that this information is abstracted and
12109120	12114000	represented might be very different in a biological neural network in our brain versus an
12114000	12121520	artificial intelligence system. So if you hear some people saying that that the brain probably
12121520	12128640	doesn't really do geometric computations, my answer to that would be that we don't necessarily need
12128640	12135760	to imitate exactly the way that the brain works. We just need probably to try to achieve this high
12135760	12141600	level mechanism that is able to abstract information and applied as knowledge to different
12141600	12148000	problems. The definitions of artificial intelligence that are being used, like the famous Turing test,
12148000	12154560	I find is very disturbing that it's very anthropocentric. And it is actually probably very
12154560	12162240	characteristic of the human species more broadly. And this way, by judging what is intelligent,
12162640	12167760	what is not, we might potentially rule out other intelligent species because they are very different
12167760	12173040	from us. I may be obsessed with sperm whales because I'm working on studying their communication.
12173760	12177360	They are definitely intelligent creatures, but would they pass the Turing test?
12177920	12184480	Probably not. It's like subjecting a cat to a swimming test or a fish to climbing on a tree.
12185360	12192080	So I would just like to add to Michael's great answer, one quote that I think is very popular
12192080	12197360	and applies really well in this setting. The question of whether computers can think is about
12197360	12203360	as relevant as whether submarines can swim. When you built submarines, you weren't necessarily
12203360	12207680	trying to copy fish. You were solving a problem that was fundamentally slightly different.
12208320	12210960	So could be relevant in this case as well.
12211760	12215360	We spend quite a lot of time on this podcast talking about whether we should have an
12215360	12220320	anthropocentric conception of intelligence. A corporation is intelligent.
12222080	12225840	I'm starting to come around to the view of embodiment and thinking that there is something
12225840	12230800	very human like about our particular flavour of intelligence. But maybe there is a kind of pure
12230800	12236320	intelligence as well. And this was the end of my conversation with Tako Kohen. One of the really
12236400	12243840	interesting things is you're getting on to some of the work that you've done are being able to
12243840	12250800	think of group convolutions on homogeneous objects like spheres, for example, but also you moved on
12250800	12260080	to irregular objects like any mesh and you looked into things like fibre bundles and local convolutions.
12260080	12264080	Because these are objects, I think you said a homogeneous object is where you can't
12264080	12268240	perform some transformation to get from one place of the object to the other part of the object.
12268240	12270880	So what work did you do there on those irregular objects?
12272000	12276880	Yeah, that's a great question. So when we think of convolution, we're sort of
12276880	12283200	putting together a whole bunch of things, namely this idea of locality. So typically our filters
12283200	12290000	are local, but that's a choice ultimately. Convolution doesn't have to use a local filter,
12290080	12296240	though in practice we know that works very well. And there's the idea of weight sharing between
12296240	12300960	different positions and potentially also between different orientations of the filter.
12301680	12308800	And as I mentioned before, this weight sharing really comes from the symmetry.
12309920	12315680	So the fact that you use the same filter at each position in your image when you're doing
12315680	12322400	a two-dimensional convolution, that's because you want to respect the translation symmetry acting
12322400	12330560	on the plane. In the case of a general manifold or mesh, you typically not have a global symmetry.
12331200	12337840	If you think of a mesh representing a human figure, or let's say it's some complicated protein
12337840	12345200	structure, there may not be a global symmetry. Or sometimes in the case of say a molecule might
12345200	12351840	have some six-fold rotational symmetry, this symmetry may not be transitive as it's called,
12351840	12357840	meaning you cannot take any two points on your manifold and map one to the other using a symmetry.
12357840	12362640	In the case of a sphere, you can do that. Any two points in the sphere are related by rotation.
12362640	12371200	So we say a sphere is a homogeneous space, but these let's say this protein shape is not homogeneous,
12371200	12378080	even if it has some kind of symmetry. So in that case, if you try to motivate the weight sharing
12378080	12382240	via symmetry, you try to take your filter, put it in one position, move it around to a different
12382240	12386160	position using a symmetry, you're not going to be able to hit all positions. So you're not going to
12386160	12394720	get weight sharing globally. And that just is what it is. If you say I have a signal on this manifold
12395680	12402240	and I want to respect the symmetry, well, if there are no global symmetries, there's nothing to
12402240	12406320	respect. So you get no code strain. So you can just use arbitrary linear map.
12408800	12414560	Now, it turns out there are certain other kinds of symmetries called gauge symmetries that you
12414560	12424080	might still want to respect. And in practice, what respecting gauge symmetry will do is we'll
12424080	12430880	put some constraints on the filter at a particular position. So for example, that might have to be
12430880	12436560	a rotationally equivariant filter, but it doesn't tie the weights of filters at different positions.
12438160	12445440	So if you want that as well, then you can maybe motivate it via some kind of notion of local
12445440	12450400	symmetry. I have something on a local symmetry group point to motivate that in my thesis.
12451360	12464240	But there isn't a very principled way to motivate weight sharing on general manifolds
12464240	12469200	between different locations. Yeah, I'm just trying to get my head around this. So you're saying,
12469200	12475520	because the whole point that we're trying to achieve here is to have a parameter-efficient
12475520	12480080	neural network that uses local connectivity and weight sharing, as we do with, let's say,
12480080	12484000	plain RCNN, whereas when you have an irregular object, it's very, very difficult to do that.
12484000	12487760	So you're saying, in some restricted domain, you can do it. Let's say if you have a,
12488640	12493600	let's say, rotation equivariance, but you can't do the other forms of weight sharing.
12494160	12497920	I'm just trying to get my head around this, because with a graph convolution on your network,
12497920	12504080	for example, it seems like you can abstract the local neighborhood. This node is connected to
12504080	12509040	these other nodes. And potentially, that could translate to a different part of the irregular
12509120	12516400	mesh. So why can't you do it more than you suggested? I think if you want to be precise,
12516400	12522720	you just have to say, what are the symmetries that we're talking about here? And in a graph,
12522720	12529600	the most obvious one is the global permutation symmetry. So you can reorder the nodes of a graph
12530240	12536000	and really any graph neural net, whether they're coming from an equivariance perspective or not,
12536000	12541600	all graph neural nets in existence that have been proposed, they respect this permutation
12541600	12548640	symmetry. And typically, this happens through, let's say, in the most simple kind of graph
12548640	12554160	convolutional nets, like the ones by Kip van Belling, for example, there's a sum operation,
12554160	12560720	some messages from your neighbors. And some operation does depend on the border of the summands.
12560720	12564400	So it doesn't depend on the border of the neighbors. And that's why the whole thing becomes
12564400	12571040	every variant. So that's a global symmetry that all graph networks respect.
12571040	12577920	On that, though, could you not create a local, let's say if there was a local graph isomorphism,
12577920	12583520	and so I have an irregular object, but it has a local isomorphism, could I not use something like
12583520	12586960	a GCN, a local version of it to capture that isomorphism?
12587680	12596320	Yeah. So actually, this was something we proposed to do in our paper, Natural Graph Networks.
12596320	12603040	So this paper really has two aspects to it. One is the naturality as a generalization of
12603040	12610240	equivariance, I can talk about that. But another key point was that we can not just develop a
12610240	12614880	global natural graph network, as we call it, but also a local one. And what the local one will do
12615520	12624800	is it will look at certain local motifs. So maybe if you're analyzing molecular graphs,
12624800	12631440	one motif that you often find is, you know, aromatic ring or something, some ring with,
12631440	12638960	let's say, six corners, various other kinds of little small graph motifs.
12639680	12647120	And these motifs might appear multiple times in the same molecule or across different molecules.
12647120	12652080	And so what this method is doing is it's essentially finding those using some kind
12652080	12660560	of graph isomorph, local graph isomorphism, and then making sure that whenever we encounter
12660560	12668560	this particular motif, we process it in the same way, i.e. using the same weights. And if the
12668560	12674400	local motif has some kind of symmetry, like this aromatic ring, you can rotate it six times,
12674400	12679440	and it gets back to the origin or you can flip it over. So that's the symmetry of this graph
12679440	12685200	structure or an automorphism of this graph. And then the weights will also be constrained by this
12685200	12691840	automorphism group, this group of symmetries of the local motif. And various other authors also
12691920	12698640	have, I think, even Michael Bronstein and students have developed methods based on
12698640	12704800	similar ideas. Awesome. Taiko, it's been such an honor having you on the show. And actually,
12704800	12708960	you're coming back on the show in a few weeks' time, so we don't want to spoil the surprise.
12708960	12714080	But looking on this proto book that you've written with the other folks, what's the main
12714080	12716800	thing that sticks out to you as being the coolest thing in the book?
12717680	12729760	I think there's any one particular thing. What excites me is to put some order to the chaos
12730480	12737920	of the zoo of architectures and to see, actually, that there is something that they all have in
12737920	12745680	common. And I really think this can help new people who are new to the field to learn more
12745680	12753680	quickly, to get an overview of all the things that are out there. And I also think that this is
12753680	12763440	the start of at least one way in which we can take the black box of deep learning, which often is
12763440	12769280	viewed as completely inscrutable and actually start to open it and start to understand how the
12769280	12777120	pieces connect, which can then perhaps inform future developments that are guided by both
12777120	12783920	empirical results and an understanding of what's going on. Amazing. Thanks so much, Taiko.
12784960	12786480	Thanks for having me. It's been a pleasure.
12787120	12789840	Joan, thank you so much for joining us. This has been amazing.
12790400	12795040	Okay, no, thank you so much, Tim. It was very fun and best of luck. And I think I'm
12795040	12800560	let's maybe get in touch. Thank you very much. It's very nice to be talking to you today about
12800560	12802480	these completely random topics. Yeah.

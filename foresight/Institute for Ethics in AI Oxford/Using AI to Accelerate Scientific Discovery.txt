â€¦uchos MARH
welcome to this evening's lecture
in the splendor of the Sheldonian theatre.
Hollol teis endseth at Oxford UK University,
part of the OpenSeed Challenge lectures
on artificial intelligence and human values.
My name is Nigel Shadbolt, principal of Jesus College.
I'm also a professor of computer science here in Oxford and chair the Institute Steering Group.
It was my privilege to help set up the Institute which brings together world-leading philosophers
and other experts in the humanities with the researchers, developers and users of AI.
The director of the Institute is Professor John Tosulis,
and its ultimate home will be the Stephen A. Schwarzman Centre for the Humanities,
whose construction is soon to start.
In recent years, AI has gone from strength to strength.
It's now ubiquitous.
In our phones, the games we play, in our cars, our drug discovery companies,
the search engines we use and the translation tools we depend on.
Much of that is down to a new generation of AI methods and techniques
that are powered by modern machine learning algorithms,
great swathes of data and the prodigious power of modern-day computing hardware.
Some of AI's most dramatic recent accomplishments
owe a great deal to our speaker here with us this evening and the company he co-founded.
Demis Arsabes, CEO and co-founder of DeepMind,
one of the world's leading AI research companies.
Demis' own career and intellectual journey is an extraordinary one.
A chess prodigy, hugely successful computer games developer,
with a double first in computer science from Cambridge.
Demis has always been fascinated by the human brain,
understanding how it gives rise to intelligence.
After the success of his games company, he went on to a PhD in cognitive neuroscience at UCL,
followed by a Henry Welcombe Postdoctoral Research Fellowship
at the Gatsby Computational Neuroscience Unit, also at UCL.
His papers in cognitive neuroscience investigated imagination,
memory and amnesia and appeared in leading journals such as Nature and Science.
He combined his interest in computing and neuroscience
with the formation of DeepMind in 2010.
It's compelling ambition to solve intelligence
and then use intelligence to solve everything else.
He and his team used games as the context in which to test new ideas
about how to build AI systems using machine learning methods inspired by neuroscience.
First arcade games and then famously Go.
A previous talk here in the Sheldonian in February 2016
prefigured AlphaGo winning 4-1 against former world champion Lee Sodol just a month later.
Games have proven to be a great training ground for developing and testing AI algorithms,
but the aim of DeepMind has always been to build general learning systems
ultimately capable of solving important problems in the real world.
DeepMind's AlphaFold system is a solution to the 50-year grand challenge
of protein structure prediction, culminating the release of the most accurate
and complete picture of the human proteome.
A core aim for the Institute for Ethics at AI is to bring together world-leading academics
and the practitioners at the cutting edge of AI development.
Tonight we will hear first hand experience of AI's enormous potential
to accelerate scientific discovery.
Experience which will inform our research and thinking about the critical ethical considerations
that must be considered by policy makers and technical developers of AI.
Demis has predicted that artificial intelligence will be one of the most beneficial technologies ever
but that significant ethical issues remain.
Please join me in welcoming Demis Hasabas to deliver tonight's Tana lecture
using AI to accelerate scientific discovery.
Thank you. Thank you, Senaigel, for such a great introduction.
It's a real pleasure to be back here in Oxford in the Shadonian
and giving the Tana lecture. It's a real honour.
What I'm going to talk about today is using AI to accelerate scientific discovery.
As you'll see throughout my talk, this was my original motivation
and has always been my motivation behind spending my entire career
and trying to make AI a reality.
I'm going to talk a lot about some of our most recent advances
actually now coming to fruition, especially the last year or two,
of using AI to crack difficult scientific problems.
I'm also going to talk about the lead up to there
and how I think about the games work we did originally and the foundation work we did originally.
Last time I talked here was just before the AlphaGo match in Korea.
That was a major moment for us and how in the last five or six years
things have progressed enormously.
Just to talk a little bit about what our vision was behind DeepMind back in 2010.
It's quite hard to remember the state of AI back in 2010
because today, as Nigel was saying, AI is ubiquitous all around us.
It's one of the biggest buzzwords in industry.
It's hard to remember just 12 years ago
almost nobody was talking about AI, I would say,
and it was almost impossible to actually get funding in the private sector for AI at all.
We have many funny stories back in the day
of trying to do some fundraising back in 2009 and 2010
and most people thinking we were completely mad
to be embarking on this journey.
We founded it with this in mind of trying to build one day
a hollow programme-like effort to build AGI,
artificial general intelligence.
We use this term artificial general intelligence
to distinguish it from normal everyday AI
where we're talking about a general system
that can perform well on many tasks to at least human level.
That's the general aspect that we are always striving for
in all the work that we do.
We're still on this mission now and I think we've done
a pretty good job of staying true to this original vision
that we had in 2010 when we were just a few people
in a small little office in an attic in Russell Square.
As Nigel said, our original mission statement
was step one, solve intelligence,
step two, use it to solve everything else.
We have updated that mission statement a little bit,
still means the same thing, but just to be a little bit more descriptive now
in the last few years, just to be a bit clearer
about what we mean by solving everything else,
what exactly are we talking about.
The way we discuss our mission now is solving intelligence
to advance science and of course for the benefit of humanity.
That's always been the cornerstone of what we think about
when we think about what should we apply AI to.
Now, there are two, broadly two ways that I think AI
can be attempted to be built.
One is the more traditional way of building logic systems
or expert systems and these are hard coded systems
that effectively teams of programmers solve the problem.
They then incorporate those solutions
in sometimes very clever expert systems,
but the problem with them is that they are very limited
in terms of what they can generalise to.
They can't deal with the unexpected
and they're basically limited to what the programmers
foresaw, the situations that the system might be in.
Of course, this line of work was inspired by mathematics
and logic systems.
On the other hand, the big renaissance in the last decade plus
is the sort of progress of learning systems.
Of course, in the 80s there was a flurry of work done on neural networks,
then that died down.
We now know that probably we didn't have enough computing power
or data, maybe not the right algorithms as well.
Basically, in essence, the ideas were correct.
An idea of a learning system is that it learns for itself,
solutions for itself from first principles,
directly from experience.
The amazing thing about these systems and their huge promise
is that they can maybe generalise to tasks
and that it's not being programmed for explicitly
and maybe solve problems that we ourselves as the designers
or scientists behind those systems don't know how to solve.
Of course, that's the huge potential
and also the risk of these kinds of systems.
Originally, these learning systems took a lot of inspiration
and also could be validated, some of the ideas
like reinforcement learning and neural networks
by systems neuroscience in comparing what these systems do,
comparing them on a systems and algorithmic level
to what we know about how the brain works.
Everything we do at DeepMind, of course,
is on the learning system side.
We've been lucky enough to be in the vanguard of this almost revolution
or anasence in the last decade of these types of approaches.
How do we think about what's our special take
on learning systems and how powerful they can be?
There are two component algorithms or approaches,
one could say, that we've fused together.
Of course, there's deep learning or deep neural networks
and the way I think about this is that the deep neural network system
is there to build a model of the environment
of the data and the experience.
Then what do you use that model for?
Well, you can use reinforcement learning,
which is a sort of goal seeking and reward maximising system.
You can use that model and use it to plan
and basically plan and take actions towards a goal,
a goal that may be specified by the designers of that system.
You have the model and then you have the action
and goal-solving element of the systems.
One of our early innovations was to fuse those two things together at scale.
We call it deep reinforcement learning now.
The cool thing about these systems is that they can discover new knowledge
from first principles through this process of trial and error
using these models.
The idea here on this diagram of the agent system
is it gets observations from the environment.
Those observations go towards building and updating
an internal model of how the environment works
and the transition matrices of the environment.
There's some goal it's trying to solve in the environment,
and then after its thinking time has run out,
it has to select an action from the action set
available to it at that moment in time
that will best get it incrementally towards its goal.
Then the action gets output.
It may or may not make a change in the environment.
That drives a new observation and then the model updates further.
You can see with this type of system,
the AI system is actually an active learner.
It participates in its own learning.
The decisions it makes in large part governs what experiences
and what data it will get next to learn more from.
Although this is a pretty simple diagram
and basically describes the whole of reinforcement learning,
the reinforcement learning problem,
there's huge complexities of course of theoretical
and practical complexities underlying this diagram
that need to be solved.
We know that in the limit this must work
because this is how mammalian brains work, including humans.
This is one of the learning mechanisms that we have in our own brains.
Reinforcement learning was found to be implemented
by dopamine neurons in the brain in the late 90s.
We know if we push this hard enough,
this should be one path towards general artificial intelligence.
What did we famously use this for?
AlfaGo was the program that I think we did a lot of things before this.
Like Atari games and other proof points.
AlfaGo was really our first attempt at doing this at a huge scale
to crack a big problem that was unsolved in AI,
one of the holy grails of AI research,
which was a program to beat the world champion at the Game of Go.
I want to talk a little bit about this in hindsight now,
knowing what I know now, how I've reinterpreted what we did with AlfaGo.
I think I can explain it in a much more simple in general way
than perhaps how I was explaining it back five, six years ago
when we were in the midst of building this system.
Just for those of you who don't know...
I don't know why that's not updating.
There we go.
This is the Game of Go.
This is the Game of Go, the board game.
It's a phenomenal game and it's a much more esoteric game
and artistic game, one could say, than chess.
It occupies the same intellectual echelon chess stars in the West.
In China, in Japan, in Korea and other Asian countries, they play Go.
Go has resisted old-fashioned logic system and expert system approaches,
whereas chess was solved by those things
because of various factors.
One is the search space is truly enormous in Go.
It's roughly 10 to the power, 170 possible board positions,
which is way more than there are atoms in the universe,
so there's no way one could exhaustively search
all of the possible board positions in order to find the right path through.
Even bigger problem actually is that it's impossible,
or thought was, it was impossible to write down an evaluation function.
To hand code an evaluation function,
which is what most modern-day chess programs use.
The reason is because Go is such an esoteric game.
It doesn't have materiality in chess.
As a first approximation, one can add up the piece values on both sides,
and that will tell you very crudely,
which side is winning in that position.
You need to know that in order to make decisions about what to do next.
Many people are tempted to, over 20 years since Deep Blue,
attempted to write to construct these evaluation functions for Go.
One of the issues is that Go players themselves do not know,
consciously at least, what that information is.
Because it's so complex a game,
they actually use their intuition rather than explicit calculation
in order to deal with the complexity of Go.
Whereas chess players, if you ask them how, why did they make a decision,
a chess grandmaster will be able to tell you explicitly
the various factors involved.
A Go player generally won't do that.
They'll just say things like it felt right.
This felt like the right move,
which is what I think also makes Go an incredible game.
Of course, intuition is not something one would associate
with computer programs, especially logic systems.
Maybe in the Q&A we can discuss a little bit more
about what intuition may be.
But I don't think it's my conclusion now,
after doing all these games,
and indeed some of the science things we've done,
is that it's not some mysterious thing.
It's actually information that our brain knows about
and has learnt through experience, of course.
I mean, there's no other way one can learn information.
But it's just in the association courtesies.
So it's not actually consciously available to a high-level cortex.
So it seems mysterious to us how we ride a bike,
when these sort of motor, sensory motor things we're able to do,
because our conscious part of our brain
cannot access those representations.
And if we can't do that,
then we definitely can't explicitly code it in some logic code,
which is why traditionally those tasks,
including things like computer vision,
have been quite hard for logic systems to solve,
even over the last 50 years.
So a lot about what we were doing
was trying to approximate this kind of intuition
in these learning systems.
So how did we work?
And I'm actually going to describe,
not just AlphaGo here,
but the whole series of AlphaX programmes.
So AlphaGo, the original one that beat Lisa Doll in 2016.
And then AlphaGo Zero,
that then didn't need human data to learn from,
just learn for itself.
And then finally AlphaZero,
which could play any two player game.
So I'm going to sort of describe them all,
roughly speaking,
with this sort of demonstrative diagram.
So the way you can think of all of these systems
is we're initially training a neural network through self-play.
So the system plays against itself,
and it learns to evaluate positions
and to pick the most likely moves
that are most useful for it to look at.
So that's what it's got to do.
Now initially, it starts with no knowledge.
So you have an initialised neural network,
it starts with zero knowledge.
So it literally is moving randomly.
So we can call that version one.
That's the neural network.
And what it does is it plays roughly 100,000 games against itself.
And so that then becomes a data set.
So that 100,000 games, we take that as a data set.
And what we try to do with it
is train a version two of that network,
a new neural network.
But we try and train it on this version one data set
to predict in the middle of a position,
in the middle of a game,
from a position in the middle of a game,
which side is going to win.
So predict ahead of time.
And also what sorts of moves does the V1 system choose
in a particular position.
So it's trying to be better at both those two things.
And then what happens is we train that V2 system,
and then we have a little mini tournament
between V1 and V2.
So it's roughly 100 games,
and they have a little match off.
And basically, if the V2 system
hits a particular threshold win rate,
55% in this case,
then we say it's significantly better than V1.
And if that's true,
then what we do is we replace V1 with version two network,
this new network in Purple.
And that, of course,
plays another 100,000 games against itself.
And now it creates a new data set.
But this data set now in Purple, in the middle,
is slightly better quality than that first data set,
because the player is slightly better.
And to begin with, almost imperceptively better.
So it's just slightly better than random now.
But that's enough signal to then train,
of course we train a version three system,
and that plays off against version two.
Now, if you don't reach this 55% win rate,
what you do instead is you take back the version two,
and you continue to generate more data with that,
another 100,000 games.
And you have 200,000 to train your next version three.
And eventually that version three
will be better than version two.
So after one does this around 17 or 18 times,
you go from random to better than world champion.
That's it.
And you can do this with any two player game,
perfect information game.
So the same network can do that.
Get to better to world champion within 20 to 30 generations
of doing this.
So you literally, and we got to the point where so fast
you literally set it off in the morning,
you could play chess about it at lunchtime
and maybe just beat it, and then by tea time,
you know, you no chance.
Literally in the day, you could actually see the evolution
in one day.
It's kind of incredible to watch as a chess player.
So what is it doing then,
in terms of thinking about this enormous search space?
So what's happening is,
and the sort of, I think, advance of AlphaGo,
one of the advances was combining this neural network system,
or model, with a kind of more classical tree search algorithm.
In this case, we use Monte Carlo tree search.
And you can think of the tree of possibilities
looking a bit like this in Go,
where each node here is a positioning in Go,
obviously shown by these little mini Go boards.
And you can imagine if you're some middle game position,
you know, there's just this countless
10 to the 170 possibilities in the limit.
How is one supposed to find the needle in the haystack, right?
The good moves that could be world champion
or better level decisions.
So what the neural network does is it constrains,
that model constrains the search to things to make it tractable,
to things that are reasonably likely to work,
reasonably effective,
and it can evaluate that at each node level
with its evaluation function.
And so instead of having to do, you know,
10 to the hundreds of possibilities,
one can just zoom into, you know, mere thousands,
10,000 or so searches.
And so therefore,
instead of that searching the entire grey tree of all possibilities,
one just looks at this far more limited, you know,
search tree in blue here.
And then when you run out of thinking time,
of course, you select the best path
that you found so far in pink here.
So, you know, we did this back in 2015,
and then in the subsequent years, we still work on this now.
There's a system called Mu Zero,
which is our latest version of this that can do,
not only do two player perfect information board games,
but can also build models of its environment.
So it can actually also do things like Atari games
and video games where you actually don't have
the rules of the game given to you.
It has to actually figure that out for itself
through observation as well.
So it's one step even more general than Alpha Zero.
And what we did with Alpha Go, of course, now is,
as Sir Nigel mentioned, is we took it to Seoul in 2016
in this million dollar challenge match with Lisa Doll.
And some of you may remember this,
but we won for one.
You know, it was a huge thing, especially in Asia and in Korea.
I mean, the country almost came to stand still.
There's over 200 million people watch the games.
And we won for one,
and experts in both AI and in Go proclaimed this advance
to be a decade before they would have predicted.
But the important thing in the end was actually not just the fact
that Alpha Go won the match,
but how it won was, I think, really instructive.
So I'm just going to give one example of this,
but actually Alpha Go, I think, is in the end changed
the way that we as human beings view the game of Go.
But this is the most famous game of that set of five.
There are actually some amazing different games,
including the one that Lisa Doll won with a genius move in game four.
But move 37 in game two, I think, will go down in Go history.
And this was the ball position at that time.
And I haven't got time to go into why this was so amazing.
But suffice to say, Alpha Go here was black,
and Lisa Doll is the white stones.
And this is very early on in the game, move 37.
You know, Go games last for a few hundred moves generally.
And Alpha Go played this move 37 stone
on the right hand side here, marked in red.
And the amazing thing about this was the position of the stone
was on the fifth line from the edge of the board.
And that, if you're an expert Go player, is unthinkable.
It's like you would be told off by your Go master
that you should never do make a move like that.
Because it gives white too much space on the side of the board.
But Alpha Go decided to do it.
Never seen before in master play would be recommended against.
And then 100 moves or so later, it turned out this stone,
this move 37 stone, was in the perfect position
to decide the battle that spread out from the bottom left
all the way across the board.
And it was just in the right place to decide that battle,
which decided the whole game.
And almost as if it had presciently sort of seen that influence ahead of time.
So now people play on the fifth line all the time, I'm told.
So this has changed everything.
And there's multiple books now written about Alpha Go's strategies.
And this is an original strategy because this is not something
that Alpha Go could have learned from human play.
In fact, it would have learned the opposite.
It would have learned not to do this kind of move.
So if you're interested in more on that Alpha Go,
I recommend you this amazing award-winning documentary
that was done by an independent filmmaker on YouTube now.
If you want to see the sort of ins and outs of it,
it was very emotional as an experience for us from all sides,
especially me being an ex-games player.
I could really understand it from Lisa Doll's point of view too.
So as I said, we then took this to Alpha Zero a couple of years ago,
two, three years ago now, and generalised this to all two-player games.
And these graphs show how Alpha Zero did against the best machines at the time
in the specialised games of chess.
It beat the best version of Stockfish,
which is this incredible handcrafted system, the descendant of Deep Blue.
And it was able to beat Stockfish 8,
which was the best Stockfish at the time, in four hours of training.
It could beat Alpha Go, Alpha Zero beat Alpha Go in eight hours at Go.
And then we just tried it with one other game, Japanese chess shogi,
actually, which is a really interesting variation on chess.
And it could beat the best handcrafted programme called ELMO within two hours of training.
The same system, all three games.
So that was generalised.
And then, of course, because I'm a chess player,
I play a little bit of Go, but I'm not very strong, but so chess is my game.
And so for me, this was the most exciting part of applying Alpha Zero,
because I actually had a discussion with Murray Campbell,
who some of you will know was one of the project leaders
behind Deep Blue, our IBM back in the 90s.
And we just, I think we just were about to play the Lisa Doll match,
or maybe we just finished.
And I was giving a lecturer at a conference,
and Murray Campbell was there as well in the audience.
And he came up to me afterwards, and we were discussing,
I said to him, I'm thinking about,
maybe we should try this with chess and see what happens.
And I wanted to know what his prediction would be.
Do you think these incredibly powerful handcrafted systems,
like stockfish, could be beaten?
Was there any more headroom in chess?
Chess is probably the oldest application of AI, right?
I mean, Turing and Shannon and people like that
have all tried their hand.
Every AI researcher at some point has tried their hand
on a chess program back to the 40s and 50s,
even if Turing had to run the program by hand
on a piece of paper and a pen.
And then, of course, in the last 25 years or so,
world champions have been studying with their chess programs
and mapping out all of chess, opening theory, all of these things.
So it was a legitimate question actually to ask is,
was there any more headroom left?
And what sort of chess would AlphaZero play
if we were to train it from first principles
and play it against these amazing hand-engineered monsters
in some sense of a machine, incredible calculating machines?
And so, of course, we couldn't actually come to an agreement on that.
And that, as the scientists in the orders will know,
that's the sign of a good question, I think,
where either answer would be interesting.
If we were to win and there was some new style out there,
there would be incredibly interesting.
And also be interesting if these hand-crofter systems,
at least in one domain, chess, had reached the limit.
So we got off and started doing that.
And I'm pleased to say that AlphaZero not only played stronger,
but it did come up with a completely new style of chess,
which I think, and my chess friends tell me,
is more aesthetically pleasing as well as a chess program.
Obviously, subjectively from a human expert's point of view.
And the reason it is is because what it does,
and it does many innovations,
but the main one is that it favours mobility over materiality.
So traditionally, hand-crofter chess programs
have always favoured materiality.
The joke within the chess circles is that chess computer sees a pawn
and then grabs the pawn because it loves material
because it gets plus one in its evaluation function.
And then it tries to hang on for dear life
in a really ugly position,
but it wins because it never makes any tactical mistakes.
So it's sort of very effective,
but it's a little bit sort of aesthetically unsatisfying,
one would say, as a style.
But instead of that, actually AlphaZero does the opposite.
It loves sacrificing pieces, material, to get mobility,
to get more mobility for its remaining pieces.
So this is a game from, we did a 100 match
between AlphaZero and Stockfish,
and then we gave it to the British chess champion to analyse,
and he picked out the coolest positions.
This is my favourite.
It's sometimes called the immortal Zugswang game.
Zugswang is a phrase in chess,
a German phrase that means any move that one makes in that position
makes your position worse.
So it's a special type of position where you're in Zugswang,
which means anything you do, it's going to make it worse,
which is very unusual.
And it's super unusual in this kind of position,
for those of you who know chess, where black,
which has got more pieces, the two rooks and a queen,
so it's got big material advantage, very powerful pieces,
the most powerful pieces remaining in chess,
but they will stuck in the corner,
and AlphaZero has sort of sealed them up with cement with its pieces,
and basically none of those pieces can move.
So this is kind of an incredible position.
So almost anything black does in this position,
its black to move, will make its position worse,
even though it's got all of these very powerful pieces.
So that was one innovation.
There were lots of interesting poppies about AlphaZero
that I won't go into, but one can think about,
well, why is it that AlphaZero plays like this,
and traditional chess engines didn't?
Nowadays, actually, interestingly, they've updated Stockfish
to include some of these ideas by hand in Stockfish,
and actually now it's even more powerful.
So it's kind of interesting hybrid system.
But my feeling is that it's better at evaluating positions
than chess engines, so that's one thing,
so it's got a better evaluation function.
And the main thing is it doesn't have to overcome
these inbuilt rules.
That's why it's sacrificing pieces,
because if you think about it, a hard-coded chess engine
would have to calculate in its search tree
that if it was going to sacrifice a rook for a bishop,
that's minus two points,
is it going to get back those two points of value
within its search tree horizon?
AlphaZero doesn't have to worry about that,
because there's no rules like that in there.
It can evaluate things contextually,
based on the particular situation at hand,
and the patterns involved there.
And also, the other big thing is,
Stockfish and programmes like that,
they have thousands of handcrafted rules,
so one problem is generating those rules,
but an even bigger problem, in my opinion,
is balancing those factors together.
That's a huge handcrafted juggling act.
And instead of that, obviously AlphaZero learns itself
how to balance out the factors that it's learned,
and to do that automatically.
So one can actually see how efficient this system is,
based on the amount of search that traditional search engines
have to do per each move they make.
And a human grandmaster makes only the order of,
looks at about 100 moves per decision,
so incredibly efficient with our models.
And the state-of-the-art chess engine, like Stockfish,
would make tens of millions of evaluations per move.
And AlphaZero is sort of in the middle here,
in terms of orders of magnitude, tens of thousands of moves.
So not as efficient as human players,
but far more efficient than the search one would get
in these search engines.
So again, if you're interested in the details about,
or your chess playing, and the details about what this changed,
the British champion and Natasha Reagan
wrote an amazing book called Game Changer,
when we gave them behind the scenes access to AlphaZero,
and what new motifs they found,
at least a dozen new motifs they found in chess.
And the cool thing is that it's very gratifying for me,
is that people like Magnus Carlson,
who's the current world champion, incredible player,
he said a few years back he was one of the first people
to read the book and who we sent it to,
and I've been influenced by my heroes recently,
one of which is AlphaZero, which is really cool to say.
And he actually incorporated, because he's so talented,
he was able to quite quickly,
quicker than all the other chess players,
incorporate some of these ideas into his play.
And then Garry Casparov, he used to be a hero of mine
when he was world champion when I was growing up and playing chess.
He worked the forward for the book,
and he said programs usually reflect priorities and prejudices
of programmers, but AlphaZero, it learns for itself,
and I would say it's star reflects the truth,
which is, you know, I think, a beautiful quote.
So we've been lucky enough to have several of these sort of fundamental
breakthroughs in games.
We started with Atari, and our program called DQN,
being able to play Atari games directly from pixels
and maximise the score just from pixels,
not being told the rules of the game, AlphaGo and AlphaZero,
I just mentioned.
And then we went further with programs like AlphaStar,
which played the most complex video game called StarCraft 2,
which is a very complicated real-time strategy game
with huge other challenges.
It's only partially observable, it's not perfect information,
there's an economy system to it,
and you have generally thousands of possible actions
you can take for any choice, not a few dozen.
And we managed to also get to grandmaster level at that.
So that was all of our games work,
but really it was leading up to this moment,
which in the last couple of years has been just so exciting
and so gratifying for us to make progress with,
which is that the games, and I love games, always will love games,
playing them, designing them and using them as testing grounds,
they were the perfect testing ground for developing AI,
but ultimately the aim was not to play games to world championship level,
it was to build general systems that could generalise
and solve real world problems.
And the one that's particularly passionate for me
is using AI for scientific discovery.
And there are three things that I look for when currently,
when we want to select a scientific problem
that we believe our systems could be good at.
So number one is we actually search out massive combinatorial
search spaces or state spaces.
So the bigger, the better actually.
Why is that?
Well, because we know then traditional methods
and exhaustive brute force methods won't work.
So we're in a razy where something else is needed
and we think that we're good at that something else.
Number two is that we want to have,
we like problems that have a clear objective function
or metric that one can specify
so that you can optimise and hill climb against it
with your learning system.
And then number three is we look for problems
that either have a lot of data available
to learn and train from,
or, and ideally it's and or,
an accurate and efficient simulator
that one can use to generate more data.
And that simulator doesn't have to be perfect.
It just has to be good enough
that you can extract some signal from the data that it generates.
Now it turns out that when you look at a lot of problems
with this prism, then actually a lot of surprising
number of problems can be made to fit these criteria.
And of course, the number one thing we were looking at
was protein folding, which I want to talk a bit about now.
And we look for problems,
not only that just fit those three criteria,
but of course there's always an opportunity cost
when you embark on applying AI to something major.
It's going to take you many years,
depending on how hard that problem is.
And we look for something that will have really huge impact.
Perhaps we sometimes talk about root nodes
that can open up whole new branches
of scientific discovery if they were to be solved.
And protein folding ticked all of those boxes.
So if you don't know what protein folding is,
it's this classic problem of can one go
from a one-dimensional amino acid sequence,
you can think of it as the genetic sequence
for a protein that describes a protein coded by the genome.
And can you predict from that directly
the 3D structure of the protein in your body,
the 3D form that it takes.
And the reason this is important is that proteins
are basically essential for everything in life,
every function in your body.
And it's thought that the 3D structure of the protein,
at least in the large part, governs its function.
So if one can understand the structure,
then one can get closer to the function of the protein.
Now, until AlphaFol came along,
the way you would do this is experimentally,
and it's extremely painstaking expert work
that needs to be done.
And using x-ray crystallography and electron microscopy.
And the rule of thumb is generally that it takes one PhD student,
their whole PhD, to do one protein.
And that's if you get lucky, you can be unlucky.
So it's hard and really painstaking and difficult.
And what happened is that the Nobel Prize winner
Christian Anfinsen, in part of his Nobel lecture in 1972,
so 50 years ago, exactly now,
he conjectured that the 3D structure of proteins
should be fully determined by the amino acid sequence,
i.e. this should be possible this mapping.
And it's a bit like, sometimes this problem is called
like Fermat's Last Theorem equivalent in biology,
because it's a bit like saying this is possible,
but the margin is too small, can't give you the answer.
And so what happened instead is obviously it set off a 50 year quest
in biology, in computational biology,
to try and solve this problem.
And it's been ongoing ever since the 1970s.
So the big question is,
is can protein structure prediction,
the protein structure prediction problem,
which is the specific part of protein folding
that we're interested in, be solved computationally?
Just computationally.
And Leventhal, who is another famous contemporary of Anfinsen,
in the 60s and 70s as well,
he calculated, back of envelope,
that there would be roughly 10 to the 300 possible confirmations,
shapes of an average size protein that it could take.
So 10 to the 300, so that's a good number,
that's ones we like, because it's bigger than go.
And obviously that means exhaustively sampling this
is totally intractable,
but of course the chink of light is that in nature,
in our bodies, physics solves this.
So it can, if proteins spontaneously fold in a matter of seconds,
sometimes milliseconds in the body.
So there's obviously some energy path through this.
So how do we get to this problem?
Well actually it's quite a long winding road for me personally,
for others in the team less so.
But for me, I actually came across the protein folding problem
in the 90s as an undergrad in Cambridge,
because one of my friends in our sort of group
of colleagues was obsessed with this problem.
And he would talk about it, and I remember this very clearly,
every opportunity in the bar playing pool, whatever it was.
If we can crack this, that will open up all sorts of things in biology.
And I sort of listened to him and I was thinking about this,
I was fascinated by the problem as a problem,
and I felt it was actually very well suited to potentially to AI.
Although obviously at the time I didn't know how it could be tackled.
But I filed that away as an interesting thing.
And then it came up again in the late 2000s
when I was doing my postdoc over at MIT.
And this game called Fold It came out from David Baker's lab,
who works on proteins.
And it was a citizen science game, you can see it on the left here.
And what they've done really interestingly
is turn protein folding into a puzzle game.
And they actually got a couple hundred gamers to fold proteins,
bit like playing Tetris or something.
And some of them actually became really good.
And I remember, so of course I was fascinated this
just from games design perspective.
Wouldn't it be amazing if we could design more games
where people played them, they were actually doing useful science
while they were having fun, that would be amazing.
And I think this is still the best example of that.
But also again protein folding was coming up.
And in fact, it turned out that a couple of,
a few really important proteins structures
were found this way by gamers
and published in Nature and Nature Structural Biology.
And so this actually really worked.
And that, when we then got to, you know,
the third piece of the puzzle was doing Go
and trying to sort of think about what we'd done
with intuition and other things, as I mentioned earlier.
And I felt that actually, you know,
if we'd managed to mimic in some sense
the intuition of Go players, master Go players
who spent their entire life studying Go,
you know, maybe one could mimic the intuition of these gamers
who were only, by the way, of course, amateur biologists.
Right? But somehow some of them were able to make
counter-intuitive folds of the backbone
that were, if you just followed an energy landscape
in a greedy fashion, one would not, you know,
reach a local minima or local maxima
and you would not be able to find the right structure.
So it's almost the day after we got back from Korea
we then, you know, I instigated the Alpha Fold project
and I thought it was the right time
to basically start working on this problem.
The other important piece of the puzzle
was this competition called CASP,
which is sometimes thought of as, like,
the Olympics for protein folding,
and it's sort of run every two years in external benchmarks.
It's an amazing thing, actually,
that I think more areas of science should do.
And it's been run sort of religiously
for every two years, for nearly 30 years.
So, you know, huge culos to the organisers,
John Mull and his team for doing this
and organising it so professionally for every two years
without fail for 30 years.
And the cool thing about it is it's a blind prediction assessment.
So there's no way you can accidentally sort of train on test data
or any of these kinds of pitfalls
because at the time when the competition runs
over summer usually every two years,
the experimentalists globally agree to hold back
a few of their structures that they've just found,
but at that point in time they're the only ones
who know what that structure looks like.
They hold back the publication for a couple of months
and they give it to John Mull and his colleagues
to put it into the competition.
And then you get those.
It's quite fun tournament because then, you know,
it's quite exciting.
You get the email and then there's a new structure
that amino acid sequence nobody has ever, you know,
knows the structure of.
And then you have a week to sort of get it back
to the competition organisers before it's published.
And then at the end of that three, four month period,
they obviously score your predictions against the ground truth,
which at that point is published, obviously in peer review journals,
that the experimental ground truth.
And then you get a kind of distance measure between your predictions
and the molecules in that prediction
and where they really are in 3D coordinate space.
So when we started getting involved in this area post 2016,
you know, we looked at CASP and the history of it
and actually they'd been very little progress
for over a decade.
It's sort of the field had stalled.
And this graph here shows you the scores of the winning team
on the hardest category of protein,
where you don't have any evolutionary similar template proteins
to sort of rely on.
So it's called free modelling.
And this is a percentage accuracy.
It's called GDT.
It's a slight nuance of the measure,
but you can think of it as the number of molecules,
the percentage number of molecules you've got roughly
in that place to a certain tolerance, distance tolerance.
And you can see they were hovering around 40% or less,
which is useless for experimentation, right?
Basically, it's pretty much random.
And so that was the average and it hadn't really moved.
And so what we did in 2018 is that we came along with Alpha Fold 1
as our first entry after a couple of years of working on this.
And we sort of, you know, I think we revolutionised the field in a way,
is that for the first time we brought cutting edge machine learning techniques,
the sort of techniques we developed in AlphaGo
and other new ones for this domain.
And we, as the core part of the system,
and we improved the winning scores by 50%.
You know, we got close to 60 GDT here.
And then, of course, we didn't stop there.
We then re-architected based on that knowledge.
We actually tried to push that system further
and it turned out it hit a brick wall,
so we had to go back to the drawing board
with the knowledge that we had,
re-architected with a brand new system.
And then that finally reached in CAS14 in 2020,
atomic accuracy.
So accuracy within the width of an atom, right,
for all the molecules.
So when we look at the scores and the results of CAS14,
what you see here is that Alpha Fold 2,
this is the root mean squared error,
is less than one angstrom error on average.
And, you know, from the 100 or so proteins
that we're supposed to predict.
So, and one angstrom is the, you know,
the width of basically a carbon atom.
So that's finally, that was the magic threshold
that John Moll and others of the organisers said
that they always set out CASP to do,
because that would make you competitive
with experimental techniques,
which are roughly, you know, the best ones
are at that kind of error rate.
So if one could do that computationally,
then suddenly you have a technique that could be,
you could rely on in tandem with experimental instead of.
And so Alpha Fold 2 got an error of 0.96 angstroms,
which was three times more accurate
than the next best system in CAS14,
even though those systems obviously incorporated
the Alpha Fold 1 techniques
that we'd already published by then.
So this led to the CASP organisers and John Moll
declaring that the structure prediction problem
had essentially been solved after all of these years.
And this is what the predictions look like.
So the ground truth is in green,
and you can see the prediction from Alpha Fold 2 in blue.
And you can see firstly proteins are exquisitely beautiful.
It's one thing to note that I've learned over the many years
I've been working on this now.
They're like exquisite little nano machines.
And you can see how accurate the overlays are.
And we were astounded, of course,
when we first got these results back.
And then, you know, there are many,
this is the architecture for Alpha Fold 2,
so you don't have time to go into the details of today,
but there were a huge number of innovations
that were required to make this work.
And the key technical advances were basically,
first of all, I should say there was no silver bullet.
It needed actually 32 component algorithms
described in 60 pages of supplemental information
actually in the paper.
And that was required.
And every single part of that was required.
So we did these ablation analyses,
which sort of took out components to see
if we could get away without having them.
And the result of that was everything was required.
And the three key sort of takeaways of why Alpha Fold 2
was an improvement over Alpha Fold 1
is we made the system fully end-to-end.
So you can think of it as sort of going end-to-end
with a recycling iterative stage.
At the time, it sort of jigs the protein structure
nearer and closer and closer to the final structure
that it's going to predict.
And Alpha Fold 1 system didn't do that.
It went from the amino acid sequence
to this intermediate representation called a dystagram,
which is a pair-wise dystagram of all the protein molecules
and their distance to each of the other molecules,
the other end molecules.
And then from that, we used a different method
to create the 3D structure.
So we went straight for predicting the 3D structure.
And those of you who work in machine learning
will know that generally speaking,
if you can make something end-to-end
and optimize directly for the thing that you're after,
usually your system will have better performance.
We used an attention-based neural network
to infer this implicit graph structure
of the residues, of the amino acid sequences.
In Alpha Fold 1, we used a convolutional neural net,
which was sort of borrowed from computer vision.
And if you think about it,
that was introducing the wrong bias into protein folding
because with computer vision, pixels next to each other
are obviously going to be correlated in an image, in some sense.
So convolutions make sense.
But actually, for a protein, the amino acid sequence,
residues that are next to each other
or close to each other on the string of letters
may not end up being near each other
once you get the full 3D fold,
or things very far away could end up folding over near each other.
So, in a way, we were giving it the wrong biases,
so we actually had to remove that.
And then finally, we built in some biological and evolutionary
and physics constraints into the system
without impacting the learning.
And again, usually, so you can think of it
as a little bit of a hybrid system,
that usually, if you put in constraints,
that impacts the learning.
We managed to do that without that.
So this was a huge research effort over sort of five years,
took about 20 people at its maximum,
and it was a truly multidisciplinary effort.
So we needed biologists and physicists and chemists
as well as machine learners.
And I think that's an interesting lesson, maybe,
to learn about cross-disciplinary work in AI for Sciences,
is you need the experts also from the domain.
And then the final, maybe interesting point to note on this,
is that normally, we're always after generality,
so you can see that from the journey from AlphaGo to AlphaZero,
was we increasingly made things general.
You start with performance,
then you start throwing things out of that system
to try and make it simpler and more elegant,
and that usually makes it more general,
as you understand what it is that you're doing.
But that's because Go and Chess and those things
were testbeds for what we wanted to do.
If you are trying to solve a real-world problem
that really matters to other scientists or health,
or in this case, you know, biology,
then actually, you might as well throw the kitchen sink at it,
because you actually are really after the output itself,
in this case, protein structures.
And that's what we did here.
We really threw everything we had at it,
and it's, I think, the most complex system that we've ever built.
Other things to note about this system is that it's also,
AlphaFold 1 was relatively slow,
took a few weeks of compute time to do a protein.
AlphaFold 2 took two weeks to train the whole system
on a relatively modest setup of eight TPUs or 150 GPUs,
which, by modern-day machine learning standards, is quite small.
And then the inference, the predictions,
can be done lightening fast in an order of minutes,
sometimes seconds for an average protein on a single GPU.
So when we did this, AlphaFold 2, we announced the results,
published the methods.
Over Christmas, that Christmas, this is back in 2020,
we were thinking, okay, how should we give access to the system
to biologists around the world?
And normally what you do is that you set up a server,
people, biologists, send you their amino acid sequences,
and then you give back, a few days later,
you might give them back the prediction.
But actually what we realised, because AlphaFold 2 was so fast,
we could actually just fold everything ourselves in one go.
So we just fold all proteins.
And we'll start with the human proteome,
just like the human genome equivalent, but in protein space.
And so that's what we did over the Christmas.
We folded the whole human proteome.
And so, which is another thing I love about AI and computing,
is you can have your Christmas lunch,
and while you're doing that, offer our AIs doing something useful
for the world.
So the human proteome, so we published that as well
in the summer of 21 last summer.
So AlphaFold 2, we predicted that every protein
in the human body is around 20,000 proteins,
represented, obviously, expressed by the human genome.
And at the point where we did this, experiments,
30 years of experiments, 30, 40 years of experiments,
had covered about 17% of the human proteome.
And we more than doubled that overnight
in terms of very high accuracy structures.
Obviously we folded all of them, but very high accuracy,
so that's less than one angstrom error.
They're sort of up to experimental quality.
We went to 36%.
And 58% at high accuracy, where we call high accuracy
when the backbone is mostly where you can be confident in.
But the side chains may be slightly out.
And then, of course, the question is what about the rest,
the other 42%.
And it may be that some of those AlphaFold 2
is just bad at, but increasingly,
and this is an open research question,
when we look at it with biologists, and biologists often send us in results,
like, I look at this one folded really well,
or this one didn't fold well,
we often find that the ones that didn't fold well
were actually what's called unstructured in isolation.
So they're disorder, intrinsically disorder proteins,
which means that until you know what they interact with,
they're basically squiggly bits of string.
And then presumably, when they interact with something in the body,
they then, another protein usually, they'll then form a shape.
But we don't know what that shape is in isolation, right?
We may not even know what it interacts with at this stage.
So, actually, people have turned this around now
to use it as a disordered protein predictor.
So, where AlphaFold doesn't do well,
perhaps that's pretty good evidence that it's a disordered protein,
which, of course, is very important in things like disease,
Alzheimer's, other things are thought to be to do with badly folded or disordered proteins.
One of the other things we did, which was a nice innovation for AlphaFold,
was have the system predicted its own confidence in its own predictions.
And the reason we did this is we wanted biologists to use this,
who maybe would not care about the machine learning techniques
or not understand them, or frankly, it would be irrelevant to them.
They would just be interested in the structure.
And we wanted to make sure that they were easily able to evaluate
the quality of that prediction and what parts of it they could rely on.
And which other parts they maybe need to check experimentally.
So what we did is AlphaFold, basically, we produced predictions,
there were split into three thresholds,
over 90 was what we call very high accuracy,
so less than one angstrom error, experimental quality,
greater than 70 was the backbone's correct,
and then less than 50 may be these red regions.
So you can see in the database that's what they look like.
It's something that should not be trusted.
We did a further 20 model organisms covering all of the critical model organisms
used in research, and also some important other ones in disease,
like tuberculosis, and also agriculture, like wheat and rice.
And a lot of these proteomes are much less covered than the human proteome.
Of course, the human one is where the most effort is being,
that's at 17%.
For some of these organisms, it's like less than 1%.
So for the researchers in those plant scientists and other things,
this is a huge boon for them because they would never have the resources
to spend that time to crystallise the proteins they're interested in.
We then teamed up with Emble EBI,
the European Bioinformatics Institute at Cambridge,
and they're amazing as a partnership team.
They host a lot of the biggest databases around the world already,
and we thought the best way to host all this data
is to just give it to them and allow them to host it
and plug it into the mainstream of biology tools.
And so we had a great collaboration with them,
and then we basically released all this data for free and unrestricted access
for any use, industrial or academic, because it's so completely free.
And it's amazing to see the impact of that,
and we tried to sort of maximise the scientific impact of this
by releasing it in that way.
The other thing we did do, and I want to touch this on this at the end,
is think about the safety and ethics of this,
and we consulted with over 30 experts in various areas of biology
bioinformatics, biosecurity and pharma
to check that this was going to be okay to release this type of information.
And they all came back with that they were not worried about this,
but they were potentially worried about future things.
So that's something that we bear in mind.
There are now a million predictions in the database today.
I just want to call out one thing.
We specially, ourselves, we specially prioritise neglected tropical diseases,
because those are the ones that affect the developing world,
the poorest people in the world the most,
and they're the least researched, because of course there's no money in it for pharma companies,
so that often it's NGOs and non-profits that have to do the work there.
So for them, it's amazing to get all the structures,
because they can go straight to drug discovery
without having to go to the intermediate step of finding these structures.
So we prioritise all these diseases,
and including ones that we've got being given from the WHO
about potential future pathogens.
And what's the community done with AlphaFold already?
We've seen just in nine months or 10 months incredible amount of work has been done.
This is really cool on the left here with some colleagues at Emble.
They used AlphaFold and Experiment to combine with their experimental data
to put together what's called the nuclear pore complex,
which is one of the biggest proteins in the body.
It's massive for a protein,
and what it is is it's a little gateway into the nucleus of your cell,
and it opens and closes to let things in.
And they were able to, you know, it's beautiful if you look at it,
able to put it all together and then visualise it.
I talked about this disorder predictor, WHO top 30 pathogens,
and actually interestingly, it's helped experimentalists,
the ones that benefited first from this,
because they can combine this with their maybe some low resolution images they have,
and if they have two sources of information, they can then make a sharp prediction
from their maybe their slightly lower resolution experimental data,
and then a computational prediction.
So it's been really gratifying to see hundreds of papers now
and applications already with being used for AlphaFold,
also in industry too for drug discovery.
So what has the impact been?
So we already have 500,000 researchers have used the database.
We think that's almost every biologist in the world
has probably looked up their proteins they're interested in.
190 countries, 1.5 million structures viewed,
and already over 3,000 citations,
and we've had some nice accolades along the way from science and nature on the method.
And then over the next year, we plan to fold every protein, you know,
in known to science, which is in Uniprot,
which is the massive database that has all the genetic sequences,
and there's over 100 million proteins known to science,
and we're steadily sort of progressing through that right now,
and we'll be releasing that over time.
So stepping back then, what does this mean?
I think that maybe, you know, we're entering a new era
of what I would like to call digital biology.
So I think the way I think about biology is that at the most fundamental level,
it's an information processing system,
albeit an exquisitely complex and emergent one.
And I think of it as maybe the potential,
the perfect sort of regime for AI to be useful in,
because, you know, one thing I think of it analogous to is in physics,
you know, we use mathematics to describe physical phenomena,
and it's been extraordinarily successful in doing that.
Of course, mathematics can also be applied to biology
and has been applied successfully in many domains,
but I think a lot of these emergent and complex phenomena
are just too complicated to be described with a few equations, right?
I just don't really see how you can say,
come up with, you know, Kepler's laws of motion just from other cell, right?
How would one do that? You know, just a few differential equations.
It doesn't seem to me likely.
And I think maybe a learn model is a better way to approach that.
And I think, and I hope that AlphaFold is a proof of concept
that this may be possible,
and maybe usher, can help usher in this new dawn of digital biology.
And our attempts to go further in that space
is obviously we're researching further at DeepMind,
and the science team, we sort of doubled down on all these things
within the biology team at DeepMind.
And we've also spun out a new company, Isomorphic Labs,
to specifically build on this work and other related work,
specifically for drug discovery to accelerate drug discovery,
which we hope, using computational and AI methods,
can maybe be an order of magnitude quicker.
Currently, you know, it takes an average of 10 years
to go from identifying a target to a candidate drug.
So just to start closing then, I just, you know,
there isn't time to go into this, but it's for us,
it's been like a renaissance year in some sense.
I've been having so much fun ticking off all of my sort of
childhood dream projects,
infusion and quantum chemistry and conjectures in maths,
material science, weather prediction.
This has all become reality now in the last year
of applying it to important problems in each of these domains
and, you know, publishing nice and important work
in each of these areas.
In applications, of course, there are lots of amazing
industrial applications that we've been doing,
and we have an applied team at DeepMind
that works with Google product teams to incorporate
all of our research into hundreds of products now at Google.
Pretty much every product you use of Google's
will have some DeepMind technology in it.
Some of the ones I just want to call out are our data centre work
and energy optimisation of data centres and the energy they use
and the cooling systems they use,
and we're looking at applying that to grid scale now,
WaveNet, which is the best text-to-speech system in the world.
So any device that you talk to that talks back to you
will be using WaveNet to have really realistic voices.
Even interesting things like better video compression for YouTube.
We can save 4% of the bit rate that is used
whilst maintaining video quality
and also things like recommendation systems,
but there's just too many to mention, actually.
And then, of course, very in vogue now,
and we have a ton of work on this area,
but it will be a whole talk in itself,
is large models, and we have our own really cool large models
that alpha code that can programme from a text description
and write code.
It's still amazing to me in competitive programming level.
Chinchilla, which is our large language model
that is computer-efficient.
Flamingo, that's our vision language combined model
that can describe images,
and then Gata, our latest model that is super general,
can do robotics, video games,
all sorts of things, language just with one model.
So this is all very exciting,
but I just want to end my last couple of slides
with a bit about ethics,
because obviously this is hosted by the Institute of Ethics,
and it's a very important topic,
and not just because of that,
but it's also what the Tana lectures are about, too.
So we think a lot about pioneering responsibly.
This is actually two of our values at DeepMind combined,
pioneering and being responsible.
I hope I've convinced you and you hope you will realise
that AI is this incredible potential to help
with some of humanity's greatest challenges.
I think disease, climate,
all of these things could be in scope,
but obviously AI has to be built responsibly and safely,
and we have to make sure the people who are building these things,
it's used for the benefit of everyone.
So we've had this sort of front of mind
from the beginning of DeepMind,
and as with any powerful technology,
and I think AI is no different,
although it may be more general and more powerful
than any that has gone before,
whether or not it's beneficial or harmful to us in society,
depends on how we deploy it and how we use it,
and what sorts of things we decide to use it for.
And I think it's important that we have a really wide debate
about that at places like this and the Institute of Ethics.
I'm very excited to see that being set up
and for us to interact with the new Institute.
Here, just one mention is that DNA has been really critical,
and we've been pushing very hard on this the last few years,
and I think it's critical to this,
to make sure we get the broadest possible input
into the design and deployment decisions of these systems,
especially for the people that this affects the most,
that these systems affect the most.
That's something we've been pushing very hard on.
There's still a lot more work to do,
and there's still a lot more progress at DeepMind,
and we've been also doing that with all of our sponsorship that we do.
We've now done nearly $50 million worth of sponsorship
of scholarships, diversity scholarships, chairs,
and academic institutions and projects,
and also funding things like the Deep Learning in Darba,
which is Africa's biggest conference on machine learning.
I'm really proud to say that a lot of DeepMinders helped set that up.
And so there's many, many things that we're doing across the industry
and act as a role model for the rest of industry.
So then on ethics and safety,
this has always been central to our mission,
because you saw our audacious mission at the start,
and we, even back in 2010 in our little attic room,
we were planning for success,
and of course we had to think through as scientists
what does success mean, what will the world look like,
and obviously if one thinks that through
and it's becoming obvious now in 2022,
but it was obvious to us then in 2010
that this would have to be critical,
that it would be really important questions
that would have to be addressed.
And part of that, so we've been doing this in the background all along,
and we'll be talking more about this work probably in future.
We were instrumental in drafting Google's AI principles,
which are now publicly available,
and they were partly based on our original ethics charter
that we've had from the very beginning of DeepMind.
And the aim of these principles, and you can look them up later
if you want to look at what they say,
is obviously to help realise the far-ranging benefits
that clearly AI could have for everyone
whilst identifying and mitigating potential risks and harms ahead of time.
And we continue to try and act as thought leadership
for the AI community on many of these topics,
strategy risks, ethics, and safety.
So what should we do then?
And I just want to end with this last slide here,
is what I think we should not do is move fast and break things,
sort of the Silicon Valley trope.
And I think we've seen the consequence of that playing out.
It can be very extraordinarily effective to get powerful systems
and growth and other things,
but I do not think it's the right way to address really powerful
potential dual-use technologies like AI.
And the problem with it is that one of the things that falls out
of moving fast and break things is actually doing live A-B testing in the world,
with your minimum viable products and other things.
And of course, the question is, if one does that,
where does the option B turns out to be a terrible option?
Well, where does the harm of that happen?
Well, it resides in society, doesn't it?
That pays the cost of your learning,
because you've done it in the world.
And it's probably fine if you're just doing a little gaming app
or photo app or something,
but we already see with social networks,
it's not fine when you're at billion-user scale
and things really matter in terms of your A-B testing.
I don't think it's responsible to do that.
So what should we do instead?
Well, fortunately, we already have another method,
which I think would be better, the scientific method,
which I do think is probably maybe humanity's greatest idea ever.
And I think it can apply here.
And I think we should use the scientific method
when we're approaching how to deal with these
very powerful, incredible potential technologies.
And what does the scientific method involve here in this domain?
Well, it's sort of thoughtful deliberation and thought
ahead of time and foresight ahead of time,
where you have hypothesis generation
on what might happen if one were to be successful
with what you're trying to do, right?
So how about we think about that ahead of time, not afterwards?
Then there's rigorous and careful and control testing.
I think that's one of the main things I learned from my PhD,
apart from all the neuroscience, was also the value of control tests.
I don't think you can really understand.
In a way, I think when I started my PhD, at least,
I was all about what's the condition of interest,
and that's the thing that you're going to make your new advance with.
But actually, you can't conclude anything, of course,
unless you have good controls.
And I think that's something I don't think engineers get
first time around, actually.
But scientists and researchers, of course, do get that,
because that's one of the things that you learn
from doing a research PhD.
So control testing in controlled environments,
not out in the world,
until you better understand what it is that you're doing.
So, of course, one updates on empirical data,
obviously ideally with peer review,
so you get critique from the outside
and people who are independent from your work,
all of these things that are standard in the scientific method, right,
but are not standard in engineering.
And all of this is in service
of getting a better understanding of the system
before one deploys it at scale, right,
and then maybe you find out something.
So my view is that as we approach artificial general intelligence,
and it's a super exciting moment in time,
as you can hopefully get from my talk
and my excitement over that,
but we need to treat it with the respect and precaution
and sort of humblness, I would say,
that the technology of this magnitude demands.
And I think that's what we are trying to be at the forefront on,
and I think I'll be talking a lot more about this in the future.
So I'll just end by on the sort of going back to the science question.
I think if we get AI right,
it could potentially be the greatest and most beneficial
technology humanity has ever invented.
And I think of AI as this ultimate general purpose tool
to help us as scientists understand the universe better
and perhaps our place in it.
Thank you.
APPLAUSE
Well, thank you, Dennis, for that extraordinary tour de force.
We do have a little time for questions.
But we wanted to give you the chance to kind of give us
that sense of your vision.
Now, we've got an opportunity to have questions from the audience.
Got to wait for the microphone to be handed to them
and to stand up if possible when asking questions,
but I'm afraid there is a kind of discrimination.
It's only those on the ground floor
that can ask a question due to health and safety policies in the theatre.
So please, if you have a question,
please raise your hand,
and I'm happy to take questions at this point.
So, John, perhaps I'll start with John.
I'll give you the provision.
There is a roving microphone.
And just declare who you are, John,
and perhaps stand up and just ask a question.
Interesting to begin an ethics talk with some discrimination, Nigel,
but I'm John Tysulis.
I'm the director of the Institute for Ethics in AI.
Thanks so much for a really fascinating and inspirational talk.
I guess I want to ask two questions.
One is a very general question about the nature of the project you're embarked on.
So the objective is to generate a powerful all-purpose tool
that will help create new scientific understanding.
And the nature of this tool is artificial general intelligence.
So that is a tool that can replicate or outperform human beings
across a wide range of cognitive tasks.
The worry is there attention there.
If you had something that could outperform human beings
across a wide range of cognitive tasks,
could we still regard that as a tool?
Or would it become a colleague?
So you talked about respecting AI at the end,
but it looks like something with that level of capacity
would demand a different form of respect
that would preclude the original objective of now treating it as a tool.
So that's one question.
The second question is, you've talked about what will benefit humanity.
And so I guess one question I have along these lines,
how do you make that determination?
So you might say, look, some people have the view that AI applied to military applications
will benefit humanity. Others don't.
How do you make that determination?
And I guess there's also this further dimension.
There's a division of labour in making that assessment.
Do you think too much has been placed on the shoulders of developers,
researchers, corporations,
and that really government should step in and resolve some of these issues?
Thanks, John. Great question.
So I think with your first question,
the reason human capabilities are an interesting mapping is because
the human brain is the only evidence of general intelligence we have
in the universe as far as we know.
So I think there's always the question is how do you know you've got there?
And you can approximate it with millions of tasks, potentially.
So that's one approach.
The more tasks you have in your grab bag and it can do all of them
and pair against human performance, you might have done it.
But there's always the possibility that one might have missed out
a particular type of cognitive ability, like creativity or something.
So that's why I think...
And also I think AI can be applied back to neuroscience as well, by the way.
That's one of our scientific areas that we apply AI to,
is neuroscience itself and better understanding our own minds.
So I have this view that as a neuroscientist
that this journey we're embarked on with AI
is the most fascinating journey one can ever take scientifically
because there's not only the artificial building,
it's then comparing that to the human mind
and then seeing, I think, uncovering the mysteries of our own minds,
what's dreaming, what is creativity, what are emotions,
all of these questions that we have, free will,
potentially even consciousness, the big questions.
I think building AI and intelligent artefacts
and then seeing what is missing in them
is a good way to explore that scientifically.
And so then, I don't know the answer to your question,
I think that's part of this journey,
is at what point would these things not become just tools.
And it may even be that it's a design question
because whether we should build what is consciousness we don't know
and that would be a whole, obviously, debate in itself,
but should we build it to the extent of what it is,
should we build them in our systems?
I would say no to begin with if we have that choice
until we better understand them as tools
and then we can bring in that extra complexity of free will
and where do they get their goals from?
Initially it will be designers,
but if they could be self-generated.
So I think we're still a long way away from those things,
but that's one of the things I think we should inch towards
very cautiously and with precautions
because also it will get to the heart of what it means to be human.
And I think that should exactly be done multidisciplinary
with philosophers and ethicists and theologians
and the wider humanities.
I think this is where the humanities comes in,
as well as the science.
So I think that's all to come.
OK, a question in the front row?
Thank you so much for a great presentation.
Carina Prunkel, I'm a research fellow at the Institute.
So you mentioned at various points the potential for dual use
and in particular malicious dual use.
So I'm curious to hear how you approach this topic at DeepMind.
So what precautions or how do you address the potential for dual use?
Great.
So we have a lot of different mechanisms now at DeepMind
that have been built up over time.
So one is the Institutional Review Committee we have,
which is formed, so it's chaired by Laila Ibrahim, our COO,
and it's formed with different people from across the company.
We have legal, we have ethicists and philosophers as well.
It's also rotating boards, some senior researchers,
and they get involved early with research projects
and try to assess them from all aspects,
and they will draw on outside experts.
So they bring in biologists, for example,
for alpha-fold biothesists,
so things we might not have in-house.
And then they work with the research teams to either say no,
that project should not proceed, OK, it can with caveats,
or why don't you build or do it in a different way
with these safeguards?
So that's our prototype, I would say, committee
that does these things, and we're kind of exercising our muscle
when the stakes are relatively low currently
so that we can learn from what works and is effective
as we get more powerful systems.
And obviously over time, I think at some point
there have got to be outside bodies that get involved.
But the problem is that, and we've experimented with that too,
is that a lot of these things are very specific
to the technology itself.
So one has to sort of understand the technology to a deep level,
maybe even have access to it somehow,
but in a controlled way, because one can't just...
Open sourcing is not just a panacea either,
because if it's a dangerous system, open sourcing
it means any bad actor can use it too, for anything.
So there's a lot of complicated, I think,
ethical questions around this.
But I don't think there's an easy answer.
So anyone who thinks there is one, I think, is kidding themselves.
I hope everyone realises the complexity involved.
But I think it's pretty...
I'm very happy with our internal system,
but I appreciate more is going to be needed than that
as the systems get more powerful and impact more of the world.
A question just behind you, I think,
if you just pass the microphone literally behind you.
Hi, my name is Ulrich.
I'm a postdoc at the Computer Science Department
in the Human Central Computing Group.
So DeepMind looks like it's this great example
of how you can take the best from science
and then sort of bring it together with a commercial company
and then make very rapid progress.
And you mentioned in the end here how you thought
that the scientific process should sort of inspire
the commercial world, as it were.
I'm curious about what you think about the other way around.
So what have you learnt by being embedded in Google
that you think we as researchers should learn from
in order to make more rapid progress?
Yeah, you're absolutely right.
That was the thinking, the original vision behind the...
So I spoke about the original vision of the company,
this Apollo programme,
but the original vision behind the organisational setup
and processes was to be a hybrid
like the best of both worlds.
Startups and the energy and creativity
and pace that they have and nimblness
and the best from academic research,
the blue sky thinking, ambitious thinking
that happens there, but sometimes with a lot of bureaucracy.
So I think that we did actually successfully
combine those two things.
And then when we agreed to get acquired,
we combined it with the third thing,
which is scale and resources of a large,
very successful company like Google.
And I think that's the main lesson,
is to make sure you do things at huge impact
and have ambition and realise that you can scale things
to that and the consequence that come with that,
but also the potential of that.
So I think we've done that now and very well,
like Mary or three of those aspects together.
It's a daily challenge because as we get bigger,
one tends to get slower as an organisation.
So we have to fight against that all the time.
But it's pretty unique, I would say,
the organisational and cultural feel of the mind.
But it could be a blueprint for other,
I would say, grand projects could be organised in a similar way.
OK, I'm going to just switch to this side
and then I'm going to go to the question there
and the question about that.
So move fast and break things with a quote
from people who built a social network.
If the mind was to build a social network
using the deep-wined way of doing things,
then what metrics would you use,
would you optimise to judge the quality of your social network?
And the second question that comes with it is,
do you in fact have a moral obligation
to build that social network?
OK, so thanks Tim,
two complicated questions there.
It's actually just generally,
so let's see, I have to be careful what I say,
but I think social networks have never really been my thing,
but also I haven't really thought a lot about it
relative to scientific advances
and the sorts of things that are my personal passion.
I would question actually the premise of your question,
which is that how much value does weak ties like that give,
like superficial connections like that,
versus deeper ties that you get in real life
with your real family and friends?
I think it's an interesting thing to understand.
Are we sacrificing deeper, more meaningful moments
for hundreds of more superficial moments?
It's not entirely clear to me that the metric of,
and it sounds seductive, connect the world,
like why would that be bad?
But this is the thing I'm talking about with the scientific method,
is to try and think through the full consequences of what that would mean.
Echo chambers, manipulation, all the rest of it
that we all know very well don't need to go into.
So I think if I was to do something like that,
I would use the scientific method again
to try and really think through ahead of time
what do you want as the outcomes and metrics?
In fact, often trying to find the right metrics
that actually drive the right behaviour that you think is good in the world
is half the challenge.
It's like asking the right question in science.
Everybody who does science knows that asking the question is the hardest thing.
What is the right question?
And it's especially hard.
Oh, you want an answer?
Well, I wouldn't want to give you an answer on the spot,
but we can talk about it over dinner.
At least one should attempt to start with serious thinking
about the question first, right?
That's the first part.
I don't know what the answer is because I've not given it enough thought.
But one should at least understand the meta level of like
that's how one should start,
including whether one should do that thing at all, potentially.
It could be the answer of that hypothesis generation.
Okay.
I'm going to try and get three more questions in.
We're right up against the clock.
We've got about seven and a half minutes.
There's a question here from Helen.
Thank you.
I'm Ellen Landomer from Yale University
and visiting fellow at the Research Centre for Ethics in AI.
Thank you for a brilliant talk.
So you showed us how AI can help us figure out the truth of the universe.
Pretty much.
How about the moral world?
How about the political universe?
Philosophy starts with Plato's Republic,
which is an attempt to figure out the best constitution.
Surely, unless one is a complete moral relativist,
there are some invariants we're trying to figure out about the moral world.
Could AI help us map that out?
Could it figure out like the best social organisation,
you know, boring from, I don't know,
all the things we've tried, capitalism, socialism, libertarianism, egalitarianism?
Would it help expand our imagination
and perhaps assuming you have an objective function
like satisfying major Italian preferences
subject to constraints to protect minority rights or something like that?
What do you see in the future?
We took through 2,000 years and we haven't made much progress.
Good question.
I mean, look, I think the morality and political science
I think is one of the hardest things that AI,
you know, I think it can contribute in some way,
but I would say it's far harder than the physical sciences,
right, or the life sciences,
because the most complex things in the world are humans,
for human beings to understand and to model
and to understand people's motivations, especially in aggregate.
I think one way it could help is there's also the question of
even if an theoretical AI could come up with a better political construct,
would humans, beings and society accept that or even care or understand it?
So there's all those questions to try and and would it be implemented correctly?
Obviously there's obviously implementation problems.
I think more interesting maybe would be,
and I've talked to economists about this is,
and we did quite a lot of research on multi-agent systems.
So again, having a little sandbox or simulation of millions of agents
with interacting with each other, with motivations and some goals seeking things.
And I think we're missing that experimental testbed actually
from political science and economics quite a lot,
because again, economics is one of those things where
and political science where you sort of have to test it live,
a, b, test it in the world.
It's like, are we going to go for this political system or not?
Should we raise inflation or not?
Well, you've got models, but then you actually just have to do it
and then see, oh, it's caused a recession or something,
where maybe we shouldn't do that next time.
And so it would be better if I think if we had a simulation
or a sandbox perhaps populated with AI systems
that are approximates to idealise forms of humans
and then we can maybe make some interesting,
experimental work in that much lower stakes.
So I think that could be really fascinating exploration area
for things like market dynamics and setting the environmental settings
to create more cooperation or something.
I would be, far as economists, I would be trying to use all those things.
I used to be fascinated when I was a kid with Santa Fe Institute
and they used to do lots of really cool models of agent-based systems
in little grid worlds.
And I loved going artificial societies, I think, by Axelrod.
I loved those kind of work, actually what I used to dream about,
going to Santa Fe to work on something like that.
And I still think that would be pretty cool to have some sort of system like that.
Let's see if we can squeeze just a few more.
There's a question chap in the, who caught my eye there, yes?
Just very, and try and squeeze them in,
because there's two more questions over here
Super, yeah, I just have a quick question to be honest.
So I think at the end you mentioned creating AI in the image of scientific method.
And the title of your lecture is advancement of science through AI.
But in what sense do you think that neural networks
or the limited understanding I have of AI is,
in what sense do they follow the notion of scientific method we have?
Is there any sense of talking about hypothesis and then testing?
Because it doesn't seem that neural networks work in that way.
They're opaque for most practical purposes.
And if they do outperform us, should we just get rid of the scientific method?
So by the way, it's not in the image of the scientific method just to be clear.
It's using the approach of the scientific method.
I'm not sure what image in the scientific method means.
And yes, today that is true that a lot of the systems we have are kind of black box-like.
But I think that's exactly what we should be doing more work on,
is making them less opaque.
There's no reason why they should be.
The way I say it to my neuroscience team is,
look, we understand quite a lot about the brain now, the ultimate black boxes.
We have MRI machines and amazing tools and single cell recording.
So it's amazing.
And that's why I got into neuroscience in the mid 2000s.
So we can actually look into, we don't have to do philosophy of mind necessarily,
although we should know about that.
But we can actually empirically look at this, not just do introspection.
And so as a minimum, in the field of artificial minds,
we should know as much about them as we do with the real brain.
And we don't know everything about the real brain.
Obviously there's tons still we don't know.
But there's a lot more that we do know than we do about these artificial systems.
And it should be the other way around.
That should be the minimum we understand because we have access to every neuron,
you know, neuron, artificial neuron in the artificial brain.
And we can completely control the experimental conditions.
So as a minimum, so I sometimes say this is a challenge to the team.
What's the equivalent of fMRI for a neural network?
What's the equivalent of single cell recording?
We do ablation studies.
So we have a whole neuroscience team that's thinking about this
and bringing neuroscience techniques, analysis techniques over to AI.
Now, in the defence of the engineers, one of the reasons that this has happened
is because the brain is obviously a static system we're all fascinated by, of course.
But artificial systems change over time.
Like AlphaGo is now in ancient history of AI, right?
Although it was very meaningful over time.
And it takes years to study a system, right?
It takes years to build it, and then it takes years to study it.
So should you use that research time on studying a system
that itself will be out of date by the time you come to any conclusions about it?
So I think only now are we reaching the point where we have systems
that are interesting enough, do enough interesting things in the world,
like large models and AlphaFold type things,
that probably it's worth spending the research time on that.
And so I think over the next decade we're going to see a lot more understanding
of what these systems do.
I don't think there's some weird reason why that can't happen.
Okay, there are so many more questions.
I am literally in the red now.
I'm going to have to call this to a close.
I do apologise.
There is so much pent up, I think, interest and questions for you, Demis.
All I can say at this point is absolutely a wonderful lecture
where five minutes later than we should have been.
The Sheldanian was to a strict regime when it comes to timekeeping.
You gave us the most fascinating insights
and you have given, I think, to the world with your company
and your own talents, a quite wonderful vision of a future
in which AI can help us flourish, empower us and not oppress us.
So thank you very much.
Thank you.

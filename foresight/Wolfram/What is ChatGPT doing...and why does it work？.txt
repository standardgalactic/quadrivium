you
you
you
you
okay, hello everyone. Well, usually in this time slot each week, I
do a science and technology Q&A for kids and others, which I've been doing for about three years now, where I try and answer arbitrary questions about science and technology.
Today, I thought I would do something slightly different. I just wrote a piece about chat GPT. What's it actually doing? Why does it work? I thought I would talk a bit about that here, and then throw this open for questions.
And I'm happy to try and talk about some all things kind of chat GPT, AI, large language models, and so on that I might know about.
Alright, so bursting onto the scene, what a couple of months ago now was our friend chat GPT. I have to say it was a surprise to me that it worked so well.
I've been kind of following the technology of neural nets for I've worked out now 43 years or so. And there've been moments of significant improvement and a long periods of time where kind of it was an interesting idea, but it wasn't clear where it was going to go.
The fact that chat GPT can work as well as it does can produce kind of reasonable human like essays is quite remarkable, quite unexpected, I think even unexpected to its creators.
And the thing that I want to talk about is, first of all, how does chat GPT basically work? And second of all, why does it work? Why is it even possible to do what has always seemed to be kind of a pinnacle of human kind of intellectual achievement of, you know, write that essay describing something?
Why is that possible? I think what chat GPT is showing us is some things about science and about language and about thinking, things that we kind of might have suspected from long ago, but haven't really known and it's really showing us a piece of sort of scientific evidence for this.
Okay, so what is chat GPT really doing? Basically, the kind of the starting point is it is trying to write reasonable, it is trying to take an initial piece of text that you might give and is trying to continue that piece of text in a reasonable human like way that is sort of characteristic
of typical human writing. So you give it a prompt, you say something, you ask something, and it's kind of thinking to itself, I've read the whole web, I've read millions of books. How would those typically continue from this prompt that I've been given?
What's the, what's the sort of the reasonable expected continuation based on kind of some kind of average of, you know, a few billion pages from the web, a few million books, and so on. So that's what it's, that's what it's always trying to do.
It's always trying to continue from the initial prompt that it's given. It's trying to continue in sort of a statistically sensible way. So let's say, let me start sharing here.
Let's say that you had given it the, you had said initially, the best thing about AI is its ability to, then chat GPT has to ask, what's it, what's it going to say next. Now, one thing I should explain about chat GPT, that's kind of shocking when you first hear about it, is those essays that it's writing, it's writing at one word at a time.
As, as it writes each word, it doesn't have a global plan about what's going to happen. It's simply saying, what's the best word to put down next, based on what I've already written.
And it's remarkable that in the end, one can get an essay that sort of feels like it's coherent and has a structure and so on, but really, in a sense, it's being written one word at a time.
So let's say that the, the prompts have been the best thing about AI is its ability to, okay, what's chat GPT going to do next. Well, it's, what it's going to do is it's going to say, well, what's, what, what, what should the next word be based on everything I've seen on the web and etc, etc, etc.
So what's the most likely next word and it knows certain probabilities, what it, what it figures out are probabilities, it says learn has probability 4.5%, predict 3.5%, and so on.
And so then what it will then do is to put down the next, the next word it thinks it should put down. So one strategy it could adopt is I'll always put down the word that has the highest probability based on what I've seen from the web and so on.
Turns out that particular strategy of just saying put down the thing with the highest probability doesn't work very well. Nobody really knows why one can have some guesses.
But it's something where if you do that, you end up getting these kind of very flat, often repetitive, even sometimes word for word repetitive kinds of essays.
So it turns out, and this is typical of what one sees in a kind of a large engineering system like this, there's a certain kind of touch of voodoo that's needed to make things work well.
And one piece of that is saying, don't always take the highest probability word, take some with some probability, take a word of lower than lower than highest probability.
And there's a whole mechanism. It's usually called its temperature parameter, temperature, sort of by analogy with statistical physics and so on, you're kind of jiggling things up to a certain extent.
And the higher the temperature, the more you're kind of jiggling things up and not just doing the most obvious thing of taking the highest probability word.
So it turns out a temperature parameter of point eight apparently seems to work best for producing things like essays.
So, okay, well, let's see what it what it takes. One of the things that that's that's nice to do is to kind of to get some sort of concrete view of what's going on.
We can actually start looking at sort of on our computer. What's it doing. I should say that this, what I'll talk about here is, is, is based on this piece that I wrote that just came out a couple of days ago.
And the, and I should say that every, every piece of code there is is click to copy so if I if I click every picture is click to copy if I click this, I will get a piece of Wolfram language code that will generate that.
Let me go down here and start showing you kind of what how how this really works.
What chat GPT in the end is some, oops, not seeing screen.
Interesting. Oh, well, okay, that's up. There we go. Okay, well, I was, let me let me show you again then what some what I was showing before this is the piece that I wrote and I just wanted to emphasize that every, every picture, and so on.
That's in this piece has clicked to copy code. Just click it, paste it into a Wolfram language notebook on a desktop computer or in the cloud and you can just run it.
Okay, so let's see how let's see. Let's actually run an approximation at least to chat GPT so open AI produced a series of models over the last several years and chat GPT is based on the GPT.
The GPT 3.5 I think model.
These models got progressively bigger, progressively more impossible to run directly on one's local computer.
This is a small version of the chat of the GPT to model, which is something you can just run on your computer, and it's part of our Wolfram neural net repository.
And you can just pick it up from there. And this, this is now the kind of the neural net that's inside a simplified version of chat GPT and we'll talk about what all of these innards really are later.
But for now, we can just do something like say, let's use that model, and let's have it tell us the, the words with the top five probabilities based on the starting prompt.
The best thing about AI is its ability to. So that's that those are the top five words that me, let me, I probably can ask it 20 words here.
So let's say, let's see, these are probably sorted right, we probably want to sort these in reverse order.
And this will now show us the.
Oh, I see this is this is sorting. Okay, so this is some.
This is showing us these words with different probabilities here.
I'm actually confused by why this didn't. Oh, I know I didn't I know I didn't do that. I know I didn't do that.
Let me just make this to what I expect. Okay, here we go. So this is, this is that sequence of words.
Now, by the by the 20th word, we're getting down to keep, I don't know, let's let's go just for fun. Let's go find out what the 50th word was.
Okay, so down here where we're, we're seeing words that were thought to be less likely what does it mean to be less likely. It means that based on essentially chat GPT is extrapolation from what it has seen on billions of documents on the web.
And this is the word which these are the words which are have certain probabilities of occurring next in that particular sentence.
Okay, so now let's say we want to, we want to go on we want to say, let's let's say, we want to say the best thing about it is it's ability to, and the next word it might pick might be learn.
And then what's the words going to pick after that. Well, we could we could figure that out by just saying, um, here, let's, let's say the next word was learn. Okay, then let's say that what we would get next we'll fill in the learn there.
And we just say, let's get the next top five properties for the next word. Okay, so the next word is from that's the most probable next word is from.
So we could say learn from, and then the next most probable word is experience.
So let's write a piece of code that automates that we're going to nestedly apply this function that is just taking the, the, the most likely word so to speak let's do that 10 times.
And this is this is now the what we get this is using the GPT to model.
And this is asking what the most likely continuation of that piece of text is. Okay, so there, there it goes.
Now this is this is the case where it's always picking the most probable word. As I said before, it.
It very quickly ends up in the, in this zero temperature case it very quickly ends up getting itself kind of tangled in some, in some loop. Let's see if I have the example of what it actually does in that case.
The, let's see.
Yeah, we go.
And this.
This is not a particularly good impressive essay and it gets itself quite quite tangled up. If you don't always pick the most probable word things work much better.
So, for example, here are some examples of what happens when you use this temperature to kind of jiggle things up a bit and not always pick the most most the word that's estimated as most probable.
It's worth realizing that there's a, I showed you a few examples of less probable words there's a, there's a huge spectrum of how of different words that can occur with progressively lower probabilities it's kind of a typical
observation about language that the, the, which you see here as well, that the nth most common word has probability about one over N. And that's what you see for the word that will follow next.
And you also see that in general for, for, for words and text.
Okay.
Well, we can.
We could ask what happens in the zero temperature case for a.
Let's see for, for.
For the actual.
GPT three model.
This is, this is what it does for zero temperature. Now one feature of this is, if you use.
Well, for example, this is a link to the API for open AI.
That's in our packet repository.
If you use that link, and you simply call GPT three, it will, because this is always picking the most probable word, it'll be the same every time.
So there's no, there's no randomness to this.
What happens usually when you're picking this, these words with, when you're picking non zero, when you have non zero temperature, and you're picking words that aren't always the most probable word is there's a certain randomness that's being added.
And that randomness will cause you to get a different essay every time. And that's why, if you say regenerate this essay, most likely you will get a different essay every time you regenerate every time you press that regenerate button.
Because it's going to pick different random numbers that decide which, which of the, of the words ranked words it's going to, it's, it's going to use.
So this is a typical example of a temperature point eight type essay generated by GPT three.
Okay, so the next big question is, we've got these probabilities for words and so on, where do those probabilities come from.
What I'm saying is that the probabilities are basically a reflection of what's out there on the web. And those are the things that chat GPT has learned from it's trying to imitate the statistics of what it's seen.
All right, so let's take some simpler examples of that.
Let's say we're dealing not with so trap GPT essentially deals with putting down words at a time actually that their, their pieces of words but we can assume for the simpler cases they're just words.
But what if that let's start off to understand this let's start off thinking about putting down individual letters at a time.
So, first question is, if we're going to just put down letters one at a time. What is the
with with what probability should we put down what letter, how do we work that out. Okay, let's pick some random text let's pick the Wikipedia article about cats.
And let's just count letters in the Wikipedia article about cats and we see that, you know, he is the winner. A is the is the runner up T comes next.
That's so based on the the sample of English text from the Wikipedia article about cats. This is what we would think about the statistics of different letters let's try the Wikipedia article about dogs.
Okay, we have probably slightly different we have an oh shows up more with higher probability, probably because there's an oh in the word dog and so on.
But if we keep going, and we say well what about some really the so that's for these specific samples of English. Let's let's keep going let's let's some.
Let's make come a.
Let's let's use a very large sample of English let's say we have a few million books, and we use that as a sample of English and ask what are the probabilities for different letters in that very large sample.
And we'll see what many people will will immediately know that he is the most common letter followed by T a etc.
So these are our probabilities. So now let's say that we want to just start generating generating text, according to those probabilities. So this is probably just just fill those in.
There are the frequencies, and let's just fill in. Let's just have it start generating letters this is just generating letters.
According to the probabilities that we get from from the occurrence of those letters in English so that was asking it to generate 500 letters with the correct probabilities to correspond to English text.
There's not a bad English text there, but that's some that's that should have the number of ease should be about 12% the number of T should be about 9% and so on.
Okay, we can make it a little bit more like English text by going ahead and let's fill in.
Let's append a certain probability to have a space, and now we can let's make a bigger version of this.
This is generating quotes English text with the correct probabilities for letters and spaces and so on.
We can make it a little bit more realistic by by having it be the case that the the the the word lengths, in this case here, we're just chopping it into words by saying there's an 18% chance that a character is a space, which is here what we're
saying, let's, let's insist that words have the correct distribution of lengths.
And this is now the text that we get where the words have the correct distribution of length the letters have the correct probability of occurring with e being the most common and so on.
Clearly, clearly not English clearly a lose if if chat GPT was generating this, it would be a fail.
This is something which at the level of individual letters is statistically correct if we said, if we asked, you know, can you tell that this isn't English, by just looking at the chances of different letters, it would say this is English.
And, and different languages, for example, have different characteristic signatures of frequencies you know if we were to pick this, or I don't know what, you know I'm sure if we pick this for English, and we were to do the corresponding thing for let's say which we'd pick
by Spanish here, for example, and will get slightly different frequencies okay those are those are somewhat similar but not quite the same.
Okay, so that's what happens if this is sort of generating English text with the correct single letter statistics.
We could just plot the, the, just plot the probabilities for those individual letters Oh boy, more complicated than needed to be.
Okay, that's just, that's just the probability for each letter to occur so he is the most common Q is very rare, etc.
What we're assuming is that every letter is sort of picked at random independently.
However, in actual English, we know that's not the case for example, if we've had a Q that's been picked, then with overwhelming probability the next letter that will occur as a you.
And similarly, other kinds of combinations of letters other kinds of two grams other kinds of pairs of letters so we can, instead of asking for the probability of just an individual letter we could for example say, what's the probability for a pair of letters coming
together. See here we go. So this is, this is asking.
This is saying, given that the letter B occurred, what's the probability for the next letter to be so it's fairly high probability for the next letter to be F is very low.
Over here, when there's a Q, the, the probability for next letters is only substantial when there's a you as the next letter.
So that's what it looks like to have that that's what the this combination of pairs of letters the probabilities for combinations of pairs of letters.
So now, let's say that we try and generate text letter at a time with not just dealing with the individual probabilities of letters but also the probabilities of pairs of letters.
Okay, so now we do that. And it's going to start looking a bit more little bit more like real real English text there's a couple of actual words here like on and the and well Tesla I guess is a word of sorts.
And this is this is now sort of getting a bit closer to to actual English text because it's capturing more of the statistics of English.
So let's go on instead of just dealing with having the correct probabilities for individual letters pairs of letters and so on, we can go on and say, let's have the correct probabilities for triples of letters combinations of four letters and so on.
And this is actually this these numbers are probably off by one because those are really letters on their own these are pairs of letters and so on.
So this is six tuples of letters. And we can see that by the time you've got by the time you're saying I want to follow the probabilities for for six tuples of letters, we're getting complete English words like average and so on.
And that's how it finishes that's why autocomplete when you type on a phone or something like that can work as well as it does but because by the time you have AV er there's there's really only there's only a limited number of words that can follow that.
And so you've pretty much determined it. And, and that's that's how the probabilities work when you're dealing with with blocks of letters rather than rather than small numbers of letters.
Okay, so that's kind of the idea of sort of you're capturing the statistics of letters, the statistics of sequences of letters, and you're using that to randomly generate kind of text like things.
Let's some, we can also do that, not just with probabilities of individual letters with probabilities of words. So in English, there are maybe 40 or 50,000 sort of fairly commonly used words.
And we could simply say, based on some large sample from millions of books or something, what are the probabilities of those different words and the probabilities of different words have changed over time and so on.
But let's say we, we say what what are, let's say, over the course of all books, or for the current time, what are the probabilities for those, let's say 50,000 different words, and now just start generating sentences where we picked those words at random.
But with the, with the probabilities that correspond to the frequencies with which they occur in sort of the samples of English text. So there's a sentence we get by, by that method.
And it's a sentence where, well, these words are, you know, occurring with the right probability. This sentence doesn't really mean anything. It's just a collection of random words.
We can do the same thing we did with letters, instead of just saying, we use a certain probability for an individual word, we say, we correctly work out the probabilities for pairs of words based on our sample of English text and so on.
We do that, it's actually a computationally already comparatively difficult thing to do this even for pairs of words, because we're dealing with sort of 50,000 squared different possibilities, etc, etc, etc.
But now let's say we start with a particular word, let's say we start with the word cat. That's our sort of prompt here.
Then these are sentences that are generated with the correct probabilities for pairs of words. So we'll see things like the book.
And, well, throughout in is a little bit bizarre, but confirmation procedure, I guess, those are, that's a pair of words that occur together a bunch, at least in the place where all this text was sampled from.
So this is what you get when your sampling text sort of pairs of words at a time. This is kind of a very pre kind of chat GPT.
This is a very sort of super minimalist version in which it's just dealing with statistics of pairs of words as opposed to the much more elaborate stuff that it's that it's really doing.
Now you could say, well, how about to do something more like what chat GPT does. Let's just, instead of picking pairs of words, let's pick combinations of five words or 20 words or 200 words.
You know, let's let's ask it to, given the prompts that we've specified, let's ask it to add in the next 200 words with the probability that at the with what you would expect based on what's out there on the web.
So maybe we just make a table of what's the chance of having this three word combination forward five word combination.
Okay, here's the problem with that. The problem is, there just isn't enough English text that's ever been or text of any language that's ever been written to be able to estimate those probabilities in this direct way.
Well, in other words, the by the time you're at, you know, maybe 40,000 common English words, that means the number of pairs of words that you have to ask the probability of is 1.6 billion.
The number of triples is 60 trillion. And you pretty quickly end up with something where you couldn't possibly that there just isn't enough text that's been written in the few billion web pages that exist and so on to be able to sample all of those 60 trillion triples of words and say what's the probability of each one of these triples.
By the time you get to like a 20 word essay, you you're dealing with the number of possibilities being more than the number of particles in the universe. You wouldn't even be able to record those probabilities, even if you had text, you know, written by sort of an infinite collection of monkeys or something,
imitating humans that was able to do that.
How do we deal with this? How does chat GPT the, it's, how did, how does it deal with the fact that it, it can't sample from the web enough text to be able to just make a table of all those probabilities.
So the key idea, which is a super old idea in the history of science is to make a model. What is a model? A model is something where you're kind of summarizing data. You're summarizing things in a way where you don't have to have every piece of data.
You can make, you can just have a model which allows you to predict more data, even if you didn't immediately have it.
Quintessential example, very early example of modeling was Galileo, late 1500s, you know, trying to figure out things about objects falling under gravity and, you know, going up the Tower of Pisa and dropping cannonballs off different levels on the Tower of Pisa and saying how long does it take for these things to hit the ground.
So he could make a plot.
Oh, that's a remarkably complicated way to make this plot. Okay.
Could make a plot of, you know, I don't know how many floors that actually are in the Tower of Pisa, but, but imagine there were this number of floors to make a plot and he could say, measure, you know, in those days by taking his pulse or something, how long did it take for the cannonball to hit the ground.
So this is as a function of what floor it was dropped from, how long it took the cannonball to hit the ground.
So there's data about specific times for specific floors, but what if you want to know how long would it take for the cannonball to hit the ground if you were on the 35th floor, which didn't happen to have been explicitly measured.
So that kind of the idea of, well, let's make a model comes in and sort of a typical thing you might do is to say, well, let's just assume that it's a straight line, assume that the time to hit the ground is a function of the floor.
This is the best straight line we can fit through that data. This allows us to predict what what the time to hit the ground from from a floor that we didn't explicitly visit will be.
So essentially this this this model is is a way of sort of summarizing the data and summarizing what we expect to do when we continue from this data. The reason this is going to be relevant to us is, as I mentioned, there isn't enough data to know these probabilities for different words, just from actual text that exists.
So you have to have something where you're making a model where you're saying, assume this is sort of how things generally work. This is how we would figure out the answer when we haven't explicitly made a measurement.
So, you know, we can make different models and we'll get different results. So for example, we could say, you know, here's a, here's another model that we might pick this is a quadratic curve.
Through these these particular data points. Now it's worth realizing that there's, there's no model this model, you're always making certain assumptions about how things work.
And in the case of these problems in physics, like dropping balls from from towers and so on. We have a pretty good expectation that these sort of simple mathematical models mathematical formulas and so on are likely to be things that will work doesn't always happen that way.
You know, this is another mathematical function. This is the best version. It has some parameters in this model. This is the best version of that model for fitting this data and you can see it's a completely crummy fit to this data.
So, if we assume that this is sort of in general the way things work, we won't be able to correctly reproduce what this what this data is saying.
The, in the case of this model, I think it has three parameters that are trying to fit this data and doesn't do very well.
And in the work chat GPT is doing it it basically has 175 billion parameters that it's trying to fit to make a model of human language.
And it's trying to hope that when it has to estimate the probability of something in human language that it does better than this, that with its 175 billion parameters that the underlying structure it's using is such that it's going to be able to more
correctly than this, for example, estimate the probabilities of things.
Um, so let's see.
All right, so the next big thing to talk about is doing things like dropping balls from towers of Pisa and so on.
And that's something where we've learned over the last 300 years since Galileo and so on, that there are simple mathematical formulas that govern those kinds of processes physical processes in nature.
But when it comes to a task like what's the most problem next word, or some other kind of human like task, we don't have a simple kind of mathematics style model.
We might say, here's a typical human like task, we're given, we're asked to recognize from an array of from an image and an array of pixels, which, which digit out of the 10 possibilities is this is this one.
And, and so we know we humans do a pretty good job of saying well that's a four that's a two and so on.
We need to ask sort of how, how do we think about this problem.
So one thing we could say is, let's try and do the thing that we were doing where we say, let's just collect the data and figure out the answer based on collecting data.
So we might say, well, let's, let's get ourselves a whole collection of fours.
Let's just ask ourselves, when we are presented with a particular array of pixel values, does that array of pixel values match one of the fours that we've got in our sample.
The chance of that happening is incredibly small.
And it's clear that we humans do something better than that we don't it doesn't matter where the individual pixels fell here. So long as it roughly is in the shape of the four we're going to recognize it as a four.
So, the question then is, how does that work.
And what, what's what we found is that it's some, well, let's say this is using this is actually using this sort of a standard machine learning problem.
This is using a simple neural net to recognize these handwritten digits and so we see it gets the right answer there.
But if we say, well, what's it really doing. Let's say we give it a set of progressively more blurred digits here.
At the beginning it gets them right.
Then it quotes gets them wrong. What does it mean that it gets them wrong.
We know that this was a two that we put in here and we know we just kept on blurring that too.
And so we can say well it got it wrong because we knew it was supposed to be a two.
But if we sort of zoom out and ask what's happening at a broader level, we say well, if we were humans looking at those images, would we conclude that that's a two or not.
By the time it gets blurred enough we humans wouldn't even know it's a two.
So to sort of assess whether the machine is doing the right thing.
What we're really asking is does it do something more or less what what like what we humans do.
So that becomes the question is it not we don't get to ask for these kind of human like tasks there's no obvious right answer.
It's just does it do something that follows what us humans do.
You know that question of what's the right answer. Okay, for humans we might say well up there, you know most humans recognize that as a two.
If instead we had visual systems like bees or octopuses or something like this.
We might come to completely different conclusions once things get sort of blurred out.
We might the question of what we consider to be too like might be quite different it's a very human answer that that to say that that that still looks like a two for example.
Depends on our visual system. It's not something where there's sort of a mathematically precise definition of that has to be a two.
Okay, so.
Question is how do these models, how do these models, which we're using for things like image recognition, how do they actually work.
The the most popular by far and most successful present time approach to doing this is to use neural nets.
And so okay what what what is a neural net. It's kind of an idealization of what we think is going on in the brain.
What's going on in the brain where we all have about 100 billion neurons in our brains, which are nerve cells that have the feature that when they get excited.
They produce electrical signals maybe 1000 times a second.
They, and each nerve cell has it's it's taking that electrical signal, and it's, it has sort of wire like projections from the from the nerve cell that are connecting to maybe 1000 maybe 10,000 other nerve cells.
And so what happens in a sort of rough approximation is that you'll have electrical activity in one nerve cell, and that will kind of get communicated itself to other nerve cells.
And there's this whole network of nerves that is has this elaborate pattern of electrical electrical activity.
So, um, and roughly the way it seems to work is that the extent to which one nerve cell will affect others is determined by sort of the the weights associated with these different connections.
And so one connection might have a very strong positive effect on another nerve cell if the first nerve cell is fired, then it's like it's makes it very likely the next nerve cell will fire, or that connection might be an inhibitory connection where the if one nerve cell fires it makes it very unlikely for the next nerve cell to fire.
There's some whole combination of these weights associated with these different connections between nerve cells.
So, you know, what actually happens when we're trying to recognize a two in an image, for example, well, the, you know, the light, the photons from, from the, from the image fall on the cells at the back of our eye and a retina.
These are photoreceptor cells. They convert that light into electrical signals, the electrical signals end up going through nerves that get to the visual cortex at the back of our head.
And there's an array of nerves that correspond to all the different essentially pixel positions in the image.
And then what's happening is that within our brains, there's this sequence of connections, there's sort of layers of neurons that process the electrical signals that are coming in.
And eventually we get to the point where we kind of form a thought that that image that we're seeing in front of us is a two. And then we might say it's a two.
But that process of sort of forming the thought that's what we're talking about is kind of this process of recognition. I was talking about it in the, in the actual neural nets that we have in brains.
But what is being done in all of these models, including things like chat GPT is an idealization of that neural net.
Okay, so for example, in, in the particular neural net we were using for image recognition, this is kind of a both language representation of that neural net.
And we're going to talk about not in total detail, but we're going to talk about all these pieces in here.
It's very kind of engineering slash biological there's a lot of different funky little pieces here that go together to actually have the result of recognizing digits and so on.
This particular neural net was constructed in 1998.
And it's really was done as a piece of engineering.
So,
how do we think about the way this neural net works.
Essentially, that the sort of the key idea is the idea of attractors.
That's an idea that actually emerge from mathematical physics and so on.
But it's a key idea when we when we're thinking about neural nets and such like.
And so what what is that idea the idea is, let's say we've got all these different handwritten digits that ones that twos etc etc etc.
What we want is if we lay all these digits out in some way, what we want is that if we are sort of near the ones.
We are kind of attracted to the one spot.
If we're kind of if the thing we have is kind of near the twos were attracted to the two spot is kind of the idea of attractors is imagine that you have some I don't know mountain scape or something like this.
You are your, you know, you're a drop of water that falls somewhere on the mountain.
You are going to sort of roll down the mountain until you get to this minimum that is for the from your particular part of the mountain but then there'll be a watershed.
And if you're a raindrop that falls somewhere else, you'll roll down to a different a different minimum a different lake.
And it's the same kind of thing here when you move far enough away from the thing that looks like a one, you'll roll down into the into the twos attractor rather than the ones attractor.
That's kind of the idea there.
Now, let's see we can.
Let's let's make a kind of idealized version of this let's say we've got a bunch of points on the plane.
Let's say those are coffee shops.
And you say I'm always going to go to the closest coffee shop to me.
Well, this so called Voronoi diagram shows you this this sort of the division the watersheds between coffee shops.
If you're on this side of this watershed you'll go to this coffee shop if you're on that side you'll go to this coffee shop.
So that that's kind of a a minimal version of this idea of attractors.
All right, so let's talk about neural nets and the relationship to attractors.
So let's take an even simpler version. Let's just take these three attractors.
There's the zero attractor the plus one attractor the minus one attractor was going to say if we are if we fall in this region will have these have coordinates x and y coordinates.
So for in this region here, we're going to eventually we're going to want to go to say the result is zero we're in the zero.
We're in the basin of the zero attractor and we want to produce a zero.
Okay, so that we can we can kind of say we can say as a function of the position x and y that we start from what output do we want to get.
Well, in this on this side we want to get a one this one we want to get what is that a minus one there we want to get a zero.
This is the thing that we're trying to we're trying to we're trying to set up is something where we'll have this this kind of behavior.
Okay, well, now let's let's pull in a neural net.
So this is a typical tiny neural net.
Each of these dots represents a an artificial neuron.
Each of these lines represents a connection between neurons and the kind of the blue to redness represents the weight associated with that connection with blue being the most negative red being the most positive here.
And this is showing different.
This is showing a neural net with particular choices for these weights by which one neuron affects others.
Okay, so how do we use this neural net.
Well, we feed in inputs at the top.
We say those top two neurons got values point five and minus point eight for example, interpreting that in terms of the thing we're trying to work with that saying we're at position x equals point five y equals minus point eight in that diagram that we had drawn.
So now this neural net is basically just computing a certain function of these values x and y.
And at each step what it's doing is it's it's taking these weights and it's simply taking to for this neuron here what it's doing is it's saying I want this weight multiplied by this value here.
And then what it says is I'm going to add those two numbers up the numbers based on the weights multiplied by the original number.
Then there's a thing we add we add a constant offset, different offset for for we add this constant offset, and then we say we get some number out.
And then the kind of the the weird thing one does which is sort of inspired by what seems to happen biologically is we have some kind of thresholding function.
We say, for example, this is a very common one to use RELU.
If that total number is less than zero, make it be not its actual value but just zero.
If it's greater than zero make it be its actual value.
And there are a variety of different so called activation functions activation because they're they're what determine what the activity of the next neuron sort of down the line will be based on the input to that neuron.
So here, again, at every step, we're just collecting the values from the neurons at the previous layer, multiplying by weights, adding this offset, applying that activation function RELU to get this value minus three point eight in this case.
And what's happening here is we start off with the values point five minus point eight, we go through this whole neural net.
In this particular case, at the end it comes out with value minus one.
Okay, what does that neural net, this neural net here, the one we've just been showing, what does that do as we change those inputs where we can plot it.
That's what that neural net actually does.
So, as a function so remember what our goal is to every time we have a value in this region we want to give a zero this region we want to give a minus one and so on.
This is what that particular neural net succeeds in doing so didn't quite make it to give you know the zero one minus one values but it's kind of close.
So this is a neural net that's been kind of set up to be as close as it can be for one of that size and shape and so on to giving us the exact function we wanted to compute.
Well, how do we think about what this normal that is doing the neural net is just computing some mathematical function so for the particular neural that I was showing.
The values are the weights and the bees are the offsets and so on the f is the activation function. This is the messy sort of algebraic formula that says what the value of the output is going to be as a function of x and y the values of the inputs.
So, now the question is, well as we look at simpler neural nets, what, what kinds of functions can we actually compute.
This is at the sort of minimum level this is a single. This is a neuron here is getting input from two other neurons. What function is it computing well it depends on the weights.
These are the functions that get computed for these different choices of weights, very simple functions in all cases just these ramps.
We can ask well okay let's use a slightly more sophisticated neural net.
Here's, here's still a very small neural net. This is the best it can do in reproducing the function we want to get slightly bigger neural net does slightly better and even bigger neural net up it pretty much nailed it.
Didn't quite nail it right at the boundary it's a bit confused, instead of going straight from red to blue it's got this area where it's giving yellow and so on.
In a first approximation, this little neural net was a pretty good representation of the mathematical function that we wanted to compute.
And this is the same story as, as what we're doing.
In, in that recognition of digits, where again we've got a neural net, it happens to have I don't know what it was I think it's about 40,000 parameters in this particular case that that that specify kind of
that are doing the same kind of thing of working out the function that goes from the array of pixels at the beginning to values zero through nine and so on.
Um, well, again, we can, we can ask the question, you know, is it getting the right answer.
Well, again, it's, it's a hard question. That's really a human level question to, to, because the question of whether it put a one in the wrong place, so to speak, it's a question of how we would define that.
Well, we can do similar kinds of things, let's say we have other kinds of images, we might try and make a neural net that distinguishes cats from dogs.
And here we're showing sort of how it distinguishes those things and mostly the cats are over in this corner, the dogs are over in this corner.
And, you know, the question of what should it really ultimately do, you know, what should it do if we put a dog in a cat suit, should it say that's a cat or should it say it's a dog.
It's going to say some definite thing. The question is, does it sort of agree with what we humans would would assess it to be.
You know, one question you might ask is, well, what's this neural net doing inside when it works out its sort of catness or its dogness. And let's say we start with this.
Let's say we start with an image.
Maybe, let's say we start with an image of a cat here. Now we can, we can say what's going on inside the neural net when it decides that this is actually an image of a cat.
Well, what we can do, normally when we are looking at the insides of a neural net, it's really hard to tell what's happening.
In the case where the neural net corresponds to an image, we can at least, at least neural nets tend to be set up so that they sort of preserve the pixel structure of the image.
So for example, here we can go this is just going what is this going this is going 10 layers down. No, this is, no, this is actually sorry this is actually going just one layer down in the neural net.
And what happens in this particular neural net is it takes that image of a cat, and it breaks it up into a lot of different kind of variants of that image.
Now, at this level, we can kind of say, well, it's doing things that we can sort of recognize it's kind of looking at cat outlines without the background it's trying to pull the cat out of the background.
It's doing things that we can sort of imagine, you know, describing in words what what's going on and in fact, many of the things that it's doing are things that we know from studying neurophysiology of brains are what the first levels of visual processing and brains actually do.
So at the time we're sort of deeper in the neural net. It's much harder to tell what's going on let's say we go 1010 layers down in the neural net.
Then we've got again, sort of this is in the mind of the neural net this is what it's thinking about to try and decide is it a cat or a dog.
These things have become much more abstract, much harder to to explicitly recognize, but that's kind of what sort of a representation for us of what's happening in the kind of mind of the neural net.
So, if we say well what's a theory for how cat recognition works.
It's, it's not, it's not clear we can have a theory in the sense of sort of a narrative description, a simple way of describing how does the thing tell that it's a cat.
And if you even ask a sort of human how do you tell we say well it's got these pointy ears it's got this and that thing.
It's hard, probably for a human to describe how they do that recognition. And when we look inside the neural net, it's, we don't get to sort of have a there's no guarantee that there's a sort of simple narrative for what it's doing and typically there isn't.
Okay.
We've talked about how neural nets can successfully go from a cat image to saying that's a cat versus that's a dog. How do you set the neural net up to do that.
So the way we normally write programs is we say, well, I'm thinking about how should this program work. What should it do should it first take the image of the cat figure out does it have, you know what are the shape of its ears does it have whiskers, all these kinds of things that's sort of the the typical engineering
way to make a program.
That's what people did back 15 years ago 20 years ago, and trying to make sort of recognize images of things that was the typical kind of approach was to try and recognize sort of human explainable features of images and so on to as a way to kind of recognize things.
The big idea of machine learning is, you don't have to do that. Instead, what you can do is just give a bunch of examples where you say this is a cat, this is a dog, and have it be the case that you have a system which can learn from those examples.
And we just have to give it enough examples. And then when you show it a new cat image that's never seen before, it'll correctly say that's a cat versus that's a dog.
Let's let's talk about how that how that's actually done.
And what we're interested in is, can we take one of those neural nets I showed that the neural nets where they have all these weights. And as you change the weights, you change the function the neural net is computing.
Let's say you have a neural net and you want to make it compute a particular function.
So let's say let's take a very simple case. Let's say we have a neural net. We just want it to compute as a function of x. We want it to compute this particular function here.
Okay, so let's pick a neural net. There's a there's a neural net without weights. Let's fill in random weights in that neural net.
For every random collection of weights in the neural net, the neural net will compute something.
The function we want, but it'll always compute something it'll always be the case that when you feed in some value up here, you'll get out some value down here. And these are plots of the function that you get by doing that.
The big idea is that if you do it the right way, and you can give enough examples of
of what function you are trying to learn, you will be able to progressively tweak the weights in this neural net, so that eventually you'll get a neural net that correctly computes this function.
So again, what we're doing here is this is what we're just describing. If this is x, this is, let's say, you know, g of x down here. This is the value x up here.
And this is a g of x for some function g. And that function g that we want is this kind of square wave type thing here.
Now, in this particular case, this neural net with these weights is not computing the function we wanted. It's computing this function here.
But as we progressively train this neural net, we tweak the weights until eventually we get a neural net that actually computes the function we want.
This particular case, it took 10 million examples to get to the point where we have the neural net that we want.
Okay, so the, how does this actually work? How is this actually done? How does one, as I said, at the beginning, we just had, we started off a neural net where we had random weights.
With random weights, this function x to g of x, with that particular choice of weights is this thing here, which isn't even close to what we wanted.
So even if we have, when we have examples of functions, examples of results, we, how do we go from those to train the neural net?
Essentially, what we're doing is we run, we say, we've got this neural net. We say, let's pick a value of x, point two, for example, let's run it through the neural net.
Let's see what value we get. Okay, we get this value here.
Oh, we say that value is not correct based on what we were trying to, based on the training data that we have, based on this function that we're trying to, we're trying to train the neural nets to generate that training.
It isn't the correct results. It should have been, let's say, a minus one, and it was in fact a point seven or something.
Okay, so then the idea is that knowing that we got it wrong, we, we can measure how much we got it wrong, and we can do that for many different samples.
We can take, let's say a thousand examples of this mapping from value x to function g of x that the neural net computes, and we can say, of those thousand examples, how far off were they?
And we can compute what's often called the loss, which is take all those values of what we should have got versus what we actually got, and for example, take the sum of the squares of the differences between those values.
And that gives us a sense of if all the values will write on that will be zero, but in fact, it's not zero, because we didn't actually get the right answer with our neural net.
And so then what we're trying to do is to progressively reduce that loss. We're trying to progressively tweak the neural net so that we reduce that loss.
So for example, this is what it would typically look like. You typically have, this is the loss as a function of the number of examples you've shown.
And what you see is that as you show more and more examples, the loss progressively decreases, reflecting the fact that the function that's being computed by the neural net is getting closer to the function we actually wanted.
And eventually the loss is really quite small here, and then the function is really computed by the neural net is really close to the one we wanted.
That's kind of the idea of training a neural net. We're trying to tweak the weights to reduce the loss to get to where we want.
So let's say we've got a neural net particular form of weights. We compute the loss.
The loss is really bad. It's pretty far away. How do we arrange to incrementally get closer to the right answer? Well, we have to tweak the weights, but what direction do we tweak the weights in?
Okay, so this is a tricky thing that got figured out well in the 1980s for neural nets, how to do this reasonably. It was known how to do this in simple cases.
I should say that the idea of neural nets originated in 1943.
Warren McCulloch and Walter Pitts for the two guys who wrote the sort of original paper that described these idealized neural nets, and what's inside chat GPT is basically a big version of what was described in 1943.
And there was sort of a long history of people doing things with just one layer of neural nets, and that didn't work very well. And then early 1980s, it started to be some knowledge of how to deal with with more layers of neural nets.
And then when GPUs started to exist and computers got faster, sort of big breakthrough around 2012, where it became possible to deal with sort of training and using sort of deep neural nets.
By the way, for people who are interested, I did a discussion with a friend of mine named Terry Sinovsky who's been involved with neural nets for about 45 years now, and has been quite instrumental in many of the, many of the developments that have happened.
I did a discussion with him that was live streamed a few days ago, which you can, you can find on the web and so on, if you're interested in that, that history.
But back to back to sort of how these things work. What we want to do is we found the loss is bad. Let's reduce the loss. How do we reduce the loss? We need to tweak the weights. What direction do we tweak the weights in in order to reduce the loss?
Well, this turns out to be a big application of calculus, because basically what's happening is our neural nets corresponds to a function. It has a function of the weights.
It's a function of once when we compute the loss, we are basically working out the value of this neural net function for lots of values of x and y and so on.
And that object, that thing we're computing is a big complicated. We can think of it as an algebraic formula that we can think of as being a function of all those weights.
So how do we make the thing better? How do we reduce the overall value? How do we tweak the weights to reduce this overall loss quantity?
Well, we can kind of use calculus. We can kind of say we can think of this as sort of a surface as a function of all of these weights, and we can say we want to minimize this function as a function of the weights.
So for example, we might have a in a very simplified case. This is not good.
In a very simplified case, we might have some as a function of just two weights. So for example, in those neural nets, I was just showing they had 15 weights or something.
In the real example of an image recognition network, it might be 40,000 weights in chat GPT, it's 175 billion weights. But here we're just looking at two weights. And we're asking if this was the loss as a function of the value of those weights, how would we find the minimum?
How will we find the best values of those weights? See, oh, there we go.
This is a typical procedure to use so called gradient descent. Basically, what you do is you say I'm at this position on this lost surface, lost surface where the coordinates of the surface are weights.
What I want to do is I want to get to a lower point on this lost surface. And I want to do that by changing the weights, always following this gradient vector kind of down the hill, the steepest descent down the hill.
And that's something that you just have to use calculus and you just work out derivatives at this point as a function of these weights and the direction where you are finding the the maximum of these derivatives, you're going down the hill as much as you can.
Okay, so that's that's kind of how you try to minimize the loss is by tweaking the weights so that you follow this gradient descent thing to to get to the minimum.
Now, there's a bit of a bug with this because the surface that corresponds to all the weights, it might have as this picture shows it might have more than one minimum.
And actually, these minima might not be all at the same height. So for example, if you're in a mountain scape, there might be a mountain lake, there might be a very high altitude mountain lake.
And all of the water that's kind of following sepis descent down to get to the minimum only manages to get to that high altitude mountain lake.
There will be a much lower value of the loss, so to speak, that isn't reached by this gradient descent method. It's never you get stuck in a local minimum you never reach the more global minimum.
And that's kind of what what potentially happens in neural nets is you can be okay, I'm going to reduce the loss I'm going to tweak the weights.
Whoops, I can't really get very far I can't reduce the loss enough to be able to successfully reproduce my function with my neural net or whatever. I can't tweak the weights enough because I got stuck in a local minimum.
I don't know how to get out of that local minimum. So this was a sort of big breakthrough and surprise of 2012 and in the development of neural nets was the following discovery.
You might have thought that you'd have the best chance of getting a neural net to work well when it was a simple neural nets, you kind of get your arms around it and figure out all these weights and do all these calculations and so on.
But actually it turns out things get easier when the neural net and the problem it's trying to solve gets more complicated.
And roughly the intuition seems to be this, although one didn't expect this nobody I think expected this I certainly didn't didn't expect this that it's sort of obvious after the fact.
Okay, the issue is, you are you going to get stuck as you try and follow this gradient descent.
Well, if you're in a kind of low dimensional space.
It's quite easy to get stuck. You just get into the one of these mountain lakes you can't go any further. But in a high dimensional space. There are many different directions you could go.
And the chances are any local minimum you get to you'll be able to escape from that local minimum, because there'll always be some dimension some direction you can go that allows you to escape.
And that's what seems to be happening it's not totally obvious it would work that way but that's what seems to be happening in in these neural nets is there's always sort of a when you have a complicated enough neural net there's always a way to escape there's always a way to reduce the the loss and so on.
Okay, so, so that's kind of the.
This idea of you tweak the weights to reduce the loss. That's what's going on in all neural nets, and you can.
There are different schemes for, you know, what how you do the gradient descent and how big the steps are, and there are all kinds of different things that different ways you can calculate the loss, when we're doing it for language where we're calculating probabilities of words based on probabilities of
words based on the model versus based on what we actually see in the data, as opposed to just distances between numbers and so on, but it's the same basic idea.
Okay, so when that happens.
We can potentially get.
Every time we run one of these neural nets, we do all this tweaking of weights and so on, we get something where yes, we got a neural net that reproduces the thing we want. Okay, so they're these are results from four different neural nets that all successfully pretty much reproduce this function.
Now you might ask well what happens if I go.
What happens if I.
Yeah, what happens if I go outside the range where I had explicitly trained the neural net I'm telling it, I told it my function x goes to g of x for this range here the one in white.
But then I say well I've got the neural net now let me try running it for a value of x that I never trained it for. What's it going to give.
That will depend on which particular set of choices about which minimum which weight tweaking etc etc etc it went to. And so when the neural net tries to figure out things that it wasn't explicitly trained on.
It's going to give completely different answers, depending on the details of how the neural net happened to get trained.
That's it's kind of like it knows the things which it's already seen examples of it's kind of it's it's going to be constrained to basically reproduce those examples when you're dealing with things that are kind of out of the box.
It, it might think differently out of the box so to speak, depending on the details of that neural net.
All right, so.
Let's see.
This whole question about training neural nets is.
It's a it's a giant modern art, so to speak, of how to train a neural net and the over the last particularly last decade. There's been sort of increasingly elaborate.
Sort of knowledge of that art of training neural nets, and there's been a certain amount of lore about how these neural nets should get trained. That's that's developed.
So, how does that what what sort of in that law.
Well, kind of the first question is.
You know what kind of architecture of neural net how should you how many neurons how many neurons at each layer how should they be connected together. What should you use.
And.
There've been a number of kind of observations and sort of the art of neural nets that have emerged.
So what was believed at the beginning was every different task you want a neural net to do, you would need a different architecture to do it you would somehow optimize the architecture for each task.
It's turned out that that hasn't that isn't the case. It's much more that you that there are generic neural net architectures that seem to go across a lot of different tasks and you might say isn't that just like what happens with computers and universal computers you need only you can run different
software on the same computer same hardware different software that was the kind of idea from the 1930s that launched the whole computer revolution the whole notion of software and so on.
Is this a repetition of that I don't actually think so I think there's actually something slightly different.
I think that the reason that the neural nets that the sort of a small number of architectures cover a lot of the toss neural nets can do is because those tasks that neural nets can do a toss that we humans are also pretty good at doing.
And these neural nets are kind of reproducing something about the way we humans do tasks.
And so, while the tasks you're asking the neural net to do our tasks that are sort of human like any human like neural net is going to be able to do those tasks.
Now there are other tasks that are different kinds of computations that neural nets and humans are pretty bad at doing.
And those will be sort of out of this zone of it doesn't really matter what architecture you have.
Well, okay so there are all kinds of other things that that people sort of wondered about like they said well let's make instead of making these very simple neurons that were just like the ones from 1943.
Let's make more complicated assemblies of things and let's put more detail into the internal operations of the neural net turns out most of that stuff doesn't seem to matter.
And I think that's unsurprising from a lot of science that I've done, not specifically related to neural nets I think that that that's a that that's something that isn't too surprising.
Now when it comes to neural nets and sort of how they're architected. There are a few features that it is useful to sort of capture a few features this is not the right thing.
That's the right thing.
There are a few features of the data that you're looking at with a neural net that it is useful to that it seems useful to capture in the actual architecture of the neural net.
It's not in the end, ultimately completely necessary. It's probably the case that you could use a much more generic neural net, and with enough training enough enough kind of sort of tweaking from the actual data, you'd be able to learn all these things but for
if you've got a neural net that's dealing with images. It is useful to initially arrange the neurons in an array that's like the pixels and so this is sort of representation for the particular network called the net that we were showing for image
digit recognition of this sort of representation of there's a first layer of neurons here, then it sort of thickens up into multiple multiple different copies of the image which we actually saw.
We looking at those pictures and then it keeps going and eventually it rearranges what one thing about neural nets to understand is that neural nets take everything they're dealing with and grinds it up into numbers.
Computers take everything they're dealing with and eventually grinds it up, grind it up into zeros and ones into bits. Neural nets right now are grinding things up into arbitrary numbers, you know, 3.72, they're real numbers, not necessarily just zeros and ones.
It's not clear how important that is. It is necessary when you're going to incrementally improve weights and kind of use calculus like things do that it's necessary to have these continuous numbers to be able to do that.
But in any case, whether you're showing the neural net a picture, a piece of text, whatever, in the end it's got to be represented in terms of numbers.
And that's, that's sort of a but but but how those numbers are arranged like for example here there's an array of numbers which are sort of arranged in the, in the pixel positions and so on, the whole array is reconstituted and rearranged and flattened and so on.
And then in the end you're going to get probabilities for each of the, each of the 10 digits that will be just a sequence of numbers here, sort of a rearranged collection of numbers.
Okay, so let's see.
That picture.
Okay, so this is, so we're talking about sort of how complicated a neural net do you need to achieve it to perform a particular task. Sometimes pretty hard to estimate that because you don't really know how hard the task is.
But let's say you want a neural net that plays a game. Well, you can compute the complete game tree for the game all the possible sequences of games that could occur might be some as a huge game tree.
But if you want to get human level play for that game, you don't need to reproduce that whole game tree if you were going to do very systematic computer computation and just play the game by looking at all the possibilities you'd need that whole game tree, but or you need to be
through that whole game tree. But in the case of, if you're trying to achieve sort of human like performance, the humans might have found some heuristic that dramatically simplifies it, and you might need just some much simpler, so much simpler neural
net. So, so this is an example of, well, if the neural net is way too simple, then it doesn't have the ability to reproduce in this case the function we wanted. But you'll see that as the neural nets get a bit more complicated.
We eventually get to the point where we can indeed reproduce the function we wanted.
Well, okay, so, and you can ask, you know, are there theorems about what what functions you can reproduce with what what neural nets basically, as soon as you have any neurons in the middle, you can at least in principle reproduce any function, but you might need an extremely
large number of neurons to do that.
And it's also the case that that neural net might not be trainable, it might not be the case that you can find some, for example, gradient that always makes the loss go down and so on just by tweaking weights, it might be that that you couldn't incrementally get to that result.
Okay, so, whoops, let's say you've got a, you've decided on your architecture of your neural nets, and now you want to train your neural nets. Okay, so the next big thing is, you have to have the data to train your neural net from, and there are two basic categories of training that one does for neural nets, supervised
learning and unsupervised learning. So in supervised learning, you give the neural net a bunch of examples of what you want it to learn. So you might say, here are 10,000 pictures of cats, 10,000 pictures of dogs, the pictures of cats are all tagged as being this is a picture of a cat,
dogs, or there's a picture of a dog, and you're feeding the neural net, these, these things that are kind of explicit things that you want it to learn.
Now, that that's what one has to do for many forms of machine learning.
So you might not be able to get the data. Often, there are sources of data that where you're sort of piggybacking on something else, like you might get images from the web and they might have all tags that were text describing the image and that's how you might be able to associate the, you know, the
description of the image, the fact this is a cat to the actual image, or you might have, you know, if you're doing audio kinds of things, you might have something where you
might say, let's get a bunch of videos which have closed captions, and that will give us the, the sort of supervised information on, here's the audio, here's the text that corresponded to that audio, that's what we have to learn.
So that's, that's sort of one style of teaching neural nets is supervised learning where you've got data which explicitly as examples of here's the input you're supposed to that you're going to get, here's the output you're supposed to give.
And that's great when you can get it. Sometimes it's very, very difficult to get the necessary data to be able to train the machine learning system and when people say, oh, can you use machine learning for this task.
Well, if there's no training data, the answer is probably going to be no.
Unless that task is something that you can either get a sort of proxy for that task from somewhere else or you can, or you just have to blindly hope that something that sort of was transferred from some other domain might might work just as when you're doing mathematical models, you
can say, well, linear models or something worked in these places, maybe we can blindly hope they'll work here doesn't doesn't tend to work that well.
Okay, the other the other form of.
I should explain another thing about about neural nets is kind of important, which is that there's something been very critical over the last decade or so the notion of transfer learning.
So that once you've kind of learnt a certain amount with a neural net, being able to transfer the learning that's happening one neural net to a new neural net to give it a kind of headstart is very important now that that transfer might be for the first neural net learnt the most important features to pick out an image.
If you need the second neural nets, those most important features and let it go on from there, or it might be something where you're using one neural nets to provide training data for another neural nets or you're making them compete against each other, a variety of other things like that.
That those are actually those have different different names the transfer learning thing is mostly the first thing I was talking about.
Okay, so.
There are issues about how do you get enough training data, how many times do you show the same example to a neural net, you know, it's probably a little bit like humans for us.
When we memorize things it's often useful to go back and just rethink about that exact same example that you were trying to memorize before again.
The good is with neural nets, and the there's also questions like well, you know, you've got the image of a cat that looks like this. Maybe you can get what is the equivalent of another image of a cat just by doing some simple image processing on the first cat and it turns out that
that seems to work that notion of data augmentation seems to work surprisingly well even fairly simple transformations are almost as good as new in terms of providing more data.
Okay, the sort of a the other big form of learning that learning methodology that one tends to use is unsupervised learning where you don't have to explicitly give sort of thing you got as input, example of output.
So, for example, in, in the case of this time to keep track of of.
Yeah, the in the case of something like chat GPT there's a there's a wonderful trick you can use. Let's say chat GPT's mission is to continue a piece of text.
Okay, how do you train it. Well, you've just got a whole bunch of text, and you say okay, you know chat GPT network. Here's the text up to this point let's mask out the text after that point.
Can you predict what's going to come what you know can you learn to predict what happens if you take off the mask.
And that's the task that it you don't have to explicitly give it, you know, input output you're you're you're implicitly able to get that just from the original data that you've been provided so essentially what's happening when you're training the neural net of chat GPT is you're
saying, here's all this English text. It's from billions of web pages. Now, look at the text up to this point and say, can you correctly predict what text will come later.
Okay, gets it wrong you can say well it's it's it's giving it getting it. It's getting it wrong so let's that's provides you know that that means it has a there's some loss associated with that.
Let's see if we can tweak the weights and the neural net to get it closer to correctly predicting what's going to come next.
In any case, the end result of all of this is you make a neural net I could show you neural net training in.
I could show you more from language is very easy to train neural nets to let's see.
Maybe I shouldn't do the spell.
Let's see.
Let's just, let's just do one.
So here's, here's a collection of handwritten digits.
This is what is this going to be this maybe 50,000 handwritten digits.
Oh, there we go. So this is a supervised training story where where here all the zeros and they say that that's a hundred and zero and it says it's a zero those are the nines it says it's a nine.
Okay, so let's take a random sample of, I don't know, 2000 of those.
And now we're going to use that. Okay, there's a random sample of 2000 handwritten digit and what it was supposed to be.
Okay, so let's take it let's get a neural net. Let's say, let's try taking this Lynette neural net.
There's now a
an un, an untrained neural net.
And now we can just say if we wanted to we could should be able to say just train that neural net with this data is that data there.
So online 32.
Let's say
train this. And so what's going to happen is this is showing us the loss.
And this is showing us as it's as it's being presented with more and more of those examples, and it's being shown the same example many, many times.
You'll see the losses going down and it's gradually learning. Okay, now now we have a trained neural net.
And now we could go back to our original collection of of digits. Let's close that up.
Let's go back to our original collection of digits. Let's pick a random digit here. Let's see whether from.
Let's just pick. Let's just pick another random sample here.
Five examples there from. Oh, I should have not called it to do that. Okay, there we go. So now we can take this train neural net is our train neural nets.
And let's take the train neural nets. And let's feed it that particular nine there. Now remember we only trained it on 2000 examples so it didn't have very much training but oops, shouldn't have done that I should have just used that.
Okay.
Okay, it successfully told us it was a nine. That's kind of what it looks like to train this is, you know, Wolfram language version of training a neural net. This was a super simple neural net with only 2000 examples.
But that's kind of what it looks like to do that, do that training.
Okay.
So,
let's see the,
the thing with with chat GPT is that your.
Well, let's, let's, you know, we can, we can keep going and talk about the training of, of that, but let's
before we get to training of chat GPT, we need to talk about one more thing, which is we need to talk about this question of kind of.
Well, see do we really need to talk about this. Yeah, I would probably should talk about this. The question of how you represent kind of things like words with numbers.
So, let's say we are going to have, we're, we're, we've got all these words, and we could just number every word in English, we could say, Apple is 75 pair is 43 etc etc etc.
But there's more useful ways to label words in English by numbers. And the more useful way is to get collections of numbers that have the property that words with nearby meanings have nearby collections of numbers.
So, it's as if we're replacing every word somewhere in some meaning space, and we're trying to set it up so that words will have a position in meaning space with the property that if two words of nearby meaning space, they must mean close to the same thing.
For example, are a collection of words laid out in one of these meaning spaces, sort of actual meaning spaces like the one used by chat GPT like, what is that one it's probably 12,000 dimensional maybe this one here is just two dimensional.
We're just putting things like dog and cat alligator crocodile, and then a bunch of fruits here. And what the main thing to notice about this is that things with similar meanings like alligator and crocodile wind up nearby in this meaning space.
And, you know, peach and apricot wind up nearby in meaning space. So in other words, we're representing these words by collections of numbers. In this case just pairs of numbers just coordinates, which have the property that those coordinates are some kind of representation of the meaning of these words.
So, and we can do the same thing when it comes to images. For example, we could ask whether, when we looked at and that's exactly what we had when we were looking at some picture like this.
We're sort of laying out different handwritten digits in some kind of meaning of the of the handwritten digit space, where in that meaning space, the one the ones that mean one were over here, the ones that mean three were over here and so on.
So a question is how do you find how do you actually generate coordinates that represent the so called embeddings of of things so that when they're sort of nearby in meaning, they will have nearby coordinates.
Okay, so there's a number of neat tricks that are used to do this.
So a typical kind of setup is imagine we have this is just a representation of the neural net that we use to recognize digits it has these multiple layers each one.
There's just a little wolfman language representation of that.
What actually does this network do well in the end, what it's doing is it's taking that collection of pixels at the beginning.
And in the end what it's doing is it's computing.
What are the probabilities for a particular configuration it's got it's going to produce a collection of numbers at the end because remember neural nets, all they ever deal with the collections of numbers.
So what it's going to do is it's going to produce a collection numbers at the end, where each position in this collection of numbers there'll be 10 numbers here each position is the probability that the thing that the neural net was shown
corresponded to a zero or one or two or three or four.
So what you see here is the numbers are absurdly small, except in the case of four, so we can then deduce from this immediately okay that image was was supposed to be a four.
So this is kind of the output of the neural net is this collection probabilities where in this particular case it was really certain that the thing is a four.
So that's what we deduce.
Now, the thing we can do is we say well, let's let's back up one layer in the neural net.
Before we get to that, that, let's just say, before we had, there's a there's a layer that kind of tries to tries to make the neural net actually make a decision.
I think it's a soft max layer that is, is at the end that's trying to sort of force the decision it's trying to exponentially pull apart these numbers, so that the big number gets bigger and the small numbers get smaller.
Okay, but one layer before those numbers are a bit more sober in size, before it's been sort of torn apart to make a decision those numbers are much more sober in size.
So these numbers at this layer gives some pretty decent indication of the fullness of what we're seeing they, this has more information about what that thing that was shown actually is and we can think about these numbers as giving some kind of signature, some
some kind of trace of what kind of a thing we were seeing this is sort of specifying in some sense features of what we were seeing that later on will just decide that's a four, but all these other sort of subsidiary numbers are already useful.
We go back so you know that this is, we can define these feature vectors that represent, this is kind of the feature vector representing that image there that's the feature representing this image here, and we see that, you know, these the features for different
vectors, these vectors will be a little bit different, but they're dramatically different between a four and an eight, but we can use these these vectors to represent kind of the important aspects of, of this for here for it for instance.
And if we go back a couple more layers in that neural net, it turns out we can get an array of like 500 numbers that are a pretty good representation a pretty good sort of feature signature of, of any of these images and we do the same thing for pictures of cats and dogs, we can get this kind of
signature of what what the sort of feature vector associated with what is important about that image. And then we can take those those feature vectors, and we can say let's let's let's lay things out, according to different values and those feature
vectors, and then we'll get this kind of embedding in, in the case of what we can think of as some kind of meaning space, in the case of words. If we look at the raw.
So, so how do we do that for words. Well, the idea is, just like for the, for getting sort of a feature vector associated with, like, let's say images, we have a task like we're trying to recognize digits.
When we back up from the, from the final answer, we're training a neural net to do that task. But what we end up doing is we back up from that final we nailed the task and we say, what was the thing that was just before you you managed to nail the task.
That's our representation of the relevant features of the thing.
We can do the same thing for words. So for example, if we say the blank cat, and we then ask in in our training data, what is that blank likely to be the you know is it black is it white, whatever else that we could try and make a network that predicts what is that intermediate
word likely to be what are the probabilities for that intermediate word we can train a network to be good at predicting the probabilities for blackness versus whiteness versus whatever other tabbyness for cats or whatever it is.
Um, and once we've got that we can then back up from the final answer and say, let's look at the innards of the network and let's see what it had done as it got towards coming up with that final results.
That thing we get right before it gets to a little bit before it gets the final results, that will be a good representation of features that were important about those words.
And that's how we can then deduce what we can deduce these sort of feature vectors for words.
So, um, in the case of GPT two, for example, we can, we can compute those feature vectors, they're extremely uninformative.
When we look at them in the full feature vectors.
If we, what is more informative is we sort of project these feature vectors down to a smaller number of dimensions.
We'll discover that the cat one is closer to the dog one probably than it is to the chair one.
But that's, that's kind of, so what, what, what, um, chat GPT is doing when it deals with words is it, uh, it's, it's always representing them using these feature vectors that, um, using this kind of embedding that turns them into these
collections of numbers that have the property that nearby words are have, have similar representation that actually I'm, I'm, I'm getting a little bit ahead of myself there because, because the, the, the way chat GPT works.
It uses these kinds of embeddings, but it does so for, for whole chunks of text rather than for individual words, we'll get that.
Okay.
So, I think we're getting on, getting on fairly well here.
Um, how about the actuality of, of, of chat GPT.
Well, it's big neural net.
Millions of neurons.
175 billion connections between them.
And what is its basic architecture.
The, um,
it's the, the sort of a big idea that actually came out of language translation networks, where the task was start from English and up with French or whatever else was this idea of what it called transformers.
It's an architecture of neural nets that were more complicated architectures used before this actually a simpler one.
Um, and the notion is, as I mentioned, when one's dealing with images, it's convenient to have these neurons kind of attached to pixels, at least to sort of laid out in a kind of which pixel is next to which pixel kind of way.
Those are so called convolutional neural nets or conv nets are the, the typical things that are used there.
In the case of language, what transformers do is they are dealing with the fact that language is in a sequence.
With a conv net for an image one saying there's this pixel here what at what's happening in the neighboring nearby pixels in the image in a transformer.
What one's doing is one saying, this is, here's a word. Let's look at the preceding words. Let's look at the words that came before this word.
And in particular, let's pay attention differently to different ones of those words.
So, I mean, this gets this gets quite elaborate and engineering quite quickly.
And, you know, it's, it's, it's very typical of a sophisticated engineering system that there's lots of detail here, and I'm not going to go into much of that detail but but this is a piece of the.
In a sense of front end of, of, okay, so remember what is chat GPT ultimately doing. It's a neural net whose goal is to continue a piece of text.
So it's going to, it's going to essentially ingest the piece of text so far, reading in each token of the text the tokens are either words or pieces of words like things like the ing at the end of a word might be a separate token.
They're sort of convenient pieces of words they're about 50,000 different possible tokens. It's reading through the text the prompt that you wrote the text that it's generated so far. It's reading through all of those things.
It is then going to generate. It's, it's, it's, it's, it's then going to its goal is to then continue that text in particular it's going to tell you every time you run through this whole neural net.
It's going to give you one new token. It's going to tell you what the next token should be or what the probabilities for different choices of the next token should be.
So one piece of this is the embedding part where what's happening is it's reading a token, and it is doing I mean this is just, you know, it's it gets into a lot of detail.
So for example, let's say that the, the sequence we were reading was hello, hello, hello, hello, hello, bye, bye, bye, bye, bye. This is showing the resulting.
This is showing the embeddings that you get.
Okay, this, this is showing, you're trying to represent I said, before we were talking about embeddings for words. Now we're talking about embeddings for whole chunks of text.
And we're asking, what is the sequence of numbers that should represent that collection of that piece of text.
And the way you set that up.
I mean, again, this is, this is getting pretty deep into the entrails of the creature.
And, and,
well, what, what, what, what you can think of is there are, are different components to this embedding vector. And let's see what am I doing here this this picture is showing across the page it's showing the contribution from each word.
And down the page it's showing the different, different pieces of the feature vector that are going to be built up. And the way it works is to it takes each word, and it has it then the position of the word is encoded by a,
you could just encode it by saying the binary, the position of the word as a binary digit that says is word number seven it's you know, 0000111 or something, but that doesn't work as well as essentially learning this sort of random looking collection of things which are essentially
position tags for words. Anyway, the end result is you're going to make this thing that represents the
where, where you have both where each level is a different sort of feature associated with each of these words. And that's, that's the thing that's going to be fed into the next level of the, of the neural nets.
Okay, so the next big piece is so called the tension block in which I don't know how much this is worth explaining I talk about this a bit more in the, in the thing that I wrote.
Essentially what's happening is the, in the end it's just a great big neural net, but that neural net has doesn't have every possible connection in it. It has connections, for example, only connections that look back in the, that look to places that were earlier in that text.
And the, it is, in a sense concentrating differently on different parts of that text and you can, you can make a picture here of the amount of attention that it is paying and by attention I mean it's literally the number the, the, the, the size of effectively the weights that it's that it's using to
with which it is waiting different parts of the sequence that came in. And the way it works I think for, for GPT three, what it does is it.
So first of all it has this embedding vector, which for GPT three is about is 12,288.
I don't know why it's that particular I do know why it's that number it's multiples of things.
Long, and it's, it's taking, it's trying to put together a, an embedding vector to represent the text so far, in which it has had contributions from words at different positions.
So it's, it's sort of, it's figured out how much contribution it should get from words at each different position.
Well, okay, so it does that, then it feeds the whole thing to a layer of neural nets where sort of it has some.
What is it, it's a 12,000 by 12,000 array, which specifies what where there are 12,000 by 12,000 weights, which specify for each incoming neuron, each, each neuron that's incoming has this weight to this outgoing neuron.
And the result is, you get this whole assembly of weights which looks like nothing in particular. This is, but these are weights that have been learned by, by chat GPT to be useful for its task of continuing text.
And, you know, you can play little games you can, you can try and visualize those weights by kind of making moving averages.
And you can kind of see that the weights sort of roughly are kind of like randomly chosen, but this is kind of showing you a little bit of the detail inside that randomness.
And in a sense, you can think of this as being sort of a view into the brain of the, of chat GPT of showing you at the level of these individual weights that are in this neural net.
What, what its representation of human language is right down the level, you know, it's kind of like you take apart a computer and you look at individual bits inside the CPU.
This is kind of the same sort of thing for the representation that chat GPT has of language.
And turns out, there isn't just one of these attention layers. Okay, what happens is the, the different elements of the feature vector for the text get their different blocks of that feature vector that gets separated out and handled differently.
Nobody really knows what the interpretation of those blocks is, it's just been found to be a good thing to do to not treat the whole feature vector the same, but to break it into blocks and treat blocks of pieces in that feature vector differently.
Maybe there's an interpretation of one piece of that feature vector that this is, I don't know, words that are about motion or something.
It won't be anything like that it won't be anything as human understandable as that it's kind of like a human genome or something. It's all, all the traits are all mixed up in the specification it's or it's like what it's not something where we can easily have a sort of narrative
on what's going on. But what's been found is that you break this kind of feature vector of features of the text up, and you have the separate attention heads that have this sort of reweighting process going on for each one.
You do that and this is where you know this is like, it's crazy that things like this work, but you do that let's see 96 times for for chat GPT, you're making you're doing the same process, 96 times over.
And this is for GPT to the simpler version. This is kind of a representation of the, of the, of the things that come out of these attention layers, attention blocks, what the, what the sort of weights that were used there were.
You know, these may look, there, there is some regularity I don't know what it means. But if you look at the size of the weights, they're not perfectly for some layers they Gaussian distributed for some layers they're not.
I have no idea what the significance of that is. It's just a feature of what some what chat GPT learnt as it was trying to understand human language from from the web.
So, okay, the.
So again that there's, you know, we've talked about kind of what the, the, in the end that what's happening is, it's just a great big neural net, and it's being, it's being trained from it we're trying to deduce the weights for the neural net by showing it a whole bunch of text, and
by saying, what weights do you have to have in the neural net, so that the, so that the continuation of the text will have the right probabilities for what word comes next. That's its goal.
How, and so I've sort of described the outline of how that's done.
In the end, one has to feed it the reason it's sort of even possible to do this is that there's a lot of training data to feed it. So it's been fed a significant fraction of what's on the web there are maybe.
It depends how you describe this but there are maybe 6 billion, maybe 10 billion kind of reasonably human written pages on the web where humans actually type that stuff it wasn't mostly machine generated etc etc etc.
That's on the publicly visible web, not having programs go in and not not not selecting lots of different things and seeing what you got that's just kind of raw what's on the web page.
Maybe there's 10 maybe 100 times as much as that. If you were able to make selections to drill down to go into internal web pages, things like this but so you've got something like some, you know, some number of billions of human written pages, and there's a convenient
version called Common Crawl that's got where one goes where it's, you know, you start from one web page you follow all the links you collect all those pages you keep going, just following links following links until you've until you visited all the connected parts of the web.
But the result of this is there's a trillion words of text that you can readily get from from the web.
They're also, they're probably 100 million books that have been published maybe 100 and I think the best estimate maybe 130 million books that have been published, of which five or 10 million exist in digitized form, and you can use those as a training data as well.
And that's another 100 billion or so words of text. So you've got trillionish words of text. And that's what, and there's probably much more than that in if you have the the transcriptions of videos and things like this, you know, for me personally, I've kind of been, you know, as a kind of a personal
estimate of these things I, I realized that the things I've written over my lifetime constitute about three million words. The, the emails I've sent over the last 30 years are another 15 million words, and the total number of words I've typed is around 50 million.
Interestingly, in the live streams I've done just in the last couple of years, I have spoken another 10 million words. So it gives a sense of what, you know, human output is what but the main point is there's a trillion words available on on that you can use to train a neural net to be able to do this task of continuing from from things.
It's, let's see, in.
Right, so, so the actual process of one thing to understand about training a neural net. There's sort of a question.
Okay, there's a question. When we looked at those functions before and we said, how many neurons do we have to have to represent this function well, how many training examples that we have to give to get the neural net trained to represent that function.
In those cases, we didn't need very big neural nets. We need a lot of training examples.
There's all kinds of effort to understand how many training examples do you actually need how big a neural net do you actually need to, to do something like do this text translation thing well.
Well, it's not really known, but, you know, with 175 billion weights, the sort of the surprise is that chat GPT does pretty well.
Now you can ask the question, what, what's the, how much training does it need, and how many times does it have to be shown those trillion words, what's the relationship between the trillion words and the number of weights in the in the in the network.
And it seems to be the case that for text, that sort of the number of weights in the network is sort of comparable to the number of training examples, you sort of show it the training examples about once.
If you show it too many times it actually gets worse and its performance is very different from what happens when you're training for mathematical functions and things like this.
But one of the things that's that's an issue is that if you're every time, then I should say every every time I should explain by the way that the, the, the, every time the neural net runs what's happening is you're giving it in the case of chat GPT, you're giving it this collection of numbers that represents the text it's gotten so far.
And then that collection numbers is the input to the neural net, then you sort of ripple through the neural net layer after layer after layer it's got about 400 layers, sort of core layers.
It ripple through all those layers. And then at the end, you get some array of numbers that array of numbers actually are probabilities for each of the 50,000 possible words in English.
And that, based on that, it then picks to the next word.
But so the main operation of chat GPT is a very just straight through, you know, you've got this text so far, given that percolate through this network, say what the next result should be.
It's very, it just runs through one time.
It's actually very different from the way computers tend to work for other purposes. Most non trivial computations, you're taking the same piece of, of, of, of sort of computational material the same piece of data, and you compute on it over and over and over again,
in sort of simple models of computation like Turing machines. That's what's happening all the time. That's what's happening. That's what makes computers able to do the non trivial things computers do.
Or that is that they are taking sort of maybe a small number of pieces of data and they're just re reprocessing things over and over again.
What's happening in something like chat GPT is you've got this big network, you just percolate through it once for every token.
The only sense in which there's any feedback is that once you get an output, you add that token to the input you feed it on the next step.
So it's kind of an outer loop where you're giving feedback by adding tokens to the text, then that percolates through then you get another token that percolates through so it's a very, it's a very big outer loop.
It's probably the case, certainly in computers, in, in lots of non trivial computations that we do, there are lots of inside loops that are happening, quite possibly in the brain there are inside loops that are happening as well.
But the model that we have in chat GPT is this kind of just percolate through once kind of model with a very complicated network, but it's just percolating through once.
So that's, that's how it works, but, but one of the things that's tricky is that every time it percolates through, it has to, it has to use every single one of those weights.
So every token that chat GPT is producing, it's essentially doing 175 billion mathematical operations to see how to use each of those weights to compute the results.
Most likely that's not necessary, actually, but we don't know how to, how to get, do any better than that right now.
But that's what it's doing. So every time if it has, it's percolating through doing that.
Well, the, when you train chat GPT, and you are sort of, you're working out, you know, how do you deal with, oh, making the weights change based on the loss.
That's another, you're kind of, every time you, you make a training step, you're having to kind of do a reverse version of that, of that forward so called inference process.
It turns out that reverse process isn't that much more expensive than the forward process, but you have to do it a whole lot of times in the training.
So typically if you have a model of size n, roughly for text, it seems like you need about n squared amount of computational effort to do the training.
And n is pretty big for the case when you're dealing with sort of language and things of the size of chat GPT.
And so the training process that just a little bit mathematical square is a really big deal. And it means that you are, you know, to spend hundreds of millions of dollars potentially on doing the training with current GPUs and things like this is what you have to think about doing based on the current model of how neural nets work.
Now, I mean, I have to say that, that there's a lot of aspects of the current model that probably aren't the final model. And, you know, we can plainly see that there are big differences between, for example, things the brain manages to do.
For example, one big difference is most of the time when you're training a neural net, most of the memory and that you're doing that by having you have a bunch of things in memory, and you have some computation that's going on.
But the things that are in memory are mostly idle most of the time and there's just a little bit of computation that's going on.
In the brains, every one of our neurons is both a place that stores memory and a place that computes. It's a different kind of setup and we don't know how to do neural nets training the various things that have been looked at from the distant past actually about how to do this even from the 1940s.
People are starting to think about some distributed ways to do learning in neural nets, but that's not something that's that's landed yet as a thing we can do.
Okay, case of chat GPT.
An important thing was, and this was something, you know, six months ago, a year ago, there were kind of early versions of the GPT family text completion systems and so on.
And they were kind of the text they produced was only so so.
And then something was done by open AI with chat GPT, which was that there was an additional step, a reinforcement learning training step that was was done where essentially what was done was humans told chat GPT go and make an essay go and be be a chat bot, you know, have a conversation with me.
And the humans rated what came out and said but that's terrible that's better that's terrible etc etc etc. And the thing that was done was then that that little bit of poking turns out to have seems to have had a very big effect.
That little bit of kind of human guidance of, yes, you got the thing from the statistics of the web. Now, when you look at what you got this direction you're going in is a bad direction to go and it's going to lead to a really boring essay or whatever else.
And so that kind of and by the way there's a place where where a lot of kind of complication about well what do the humans really think the network should be the system should be producing if the humans say we really don't want you to talk about this we really don't want you to talk about that.
That's the place that gets injected is in this is in this reinforcement learning step.
At the end, and but what you do is, for example, is given that sort of the way the humans poked at those essays, you can watch what they did when they poked at those essays and rated what happened and so on.
And you can try and machine learn that set of things that the humans did, then you can use that to provide much more training data to then retrain a piece of of this do retraining of the network.
To based on the sort of the tweaking that the humans did you can do sort of fine tuning of this network based on the particular poking that the humans did turned into another network that can then be used to do the training to produce the examples to do the training of the of the main network.
So, that's a thing that seems to have had a big effect on the actual sort of human perception of what happens in, in, in chat to PT. And I think the other thing that is a sort of a surprise is that you can give it these long prompts and which you tell it all kinds of things.
And it will then sort of make use of that and rather human kind of way in generating the text that comes later.
Okay, big question is, how come this works? Why is it that a thing with only, you know, 100 billion or so weights or something can reproduce this sort of amazing thing that seems to require all of the sort of depth of human thinking and brains and things like that human language.
How does that manage to work. And I think the key thing to realize is what it's really telling us is a science fact. It's telling us, there's more regularity in human language than they thought when we thought there was.
It's telling us that this, this thing that's that is human language has a lot of structure in it and what it's done is it's learned a bunch of that structure, and it's learned structure that we never even really noticed was there.
And that's what's allowing it to generate these kind of plausible pieces of text that are, you know, that are making use of the structure we know so we know certain kinds of structure that exists in language we know the, so for example,
so one one piece of structure that we know shares again.
One piece of structure we know is grammatical syntax.
The, the syntactic grammar, we know that the that sentences aren't random jumbles of words sentences are made up with nouns in particular places verbs in particular places, and we can represent that by parse tree, in which we say, you know, here's the whole sentence there's a noun phrase a verb phrase
these are broken down in certain ways. This is the parse tree, and there are certain that in order for this to be a grammatically correct sentence.
This has there are only certain possible forms of parse tree that correspond to a grammatically correct sentence. So this is a regularity of language that we've known for a couple of thousand years it's only really been codified.
It's a big effort to codified in 1956. But it was sort of no one this general idea was was known for a long time.
But then this some that that we can kind of represent the sort of syntactic grammar of language by these kinds of rules that say you can put nouns only together with verbs and this way and that way.
To any set of rules and this has been a big source of controversy and linguistics to any set of rules you can define. There'll always be some weird exception where people typically say this, rather than that.
But if you, you know, it's at the much like happens in typical machine learning, you know, if you're interested in the 95% result, then there are just rigid rules, and there are a few exceptions here and there.
So that's one form of regularity that we know exists in language is is this some syntactic regularity. Now what one thing we can do we can ask for sort of chat GPT has effectively implicitly learned this syntactic grammar.
Nobody ever told it verbs and nouns go this way and that way. It implicitly learned it by virtue of seeing a trillion words of text on the web, which all have these properties. And when it's saying well what are the typical words that follow.
Well it's going to be words that followed in the examples it had, and those will follow mostly correct grammar. Now we can we can take a simpler version of this we can just understand what's going on.
We can take a very, very trivial grammar. We can take a grammar that's just a parentheses just open and close parentheses and something is grammatically correct.
If we open parentheses and they always eventually close, and this is a parse tree for a for a parenthesis, you know, open, open, open, close, open, close, etc, etc, etc.
This is the parse tree that sort of shows how you can.
It's a representation of of of the sort of the the parsing of this sequence of open and closed parentheses. Okay, so we might say well can we train a neural net to what would it take to train a neural net to know even this particular kind of syntactic grammar.
So we looked at a simple how big was it it was pretty small.
Okay, we made a transformer net with eight heads and length 128.
So, but but our thing was was a lot simpler than than than chat GPT but you can, you can use one of these transformers and if you look at the, the, the post I made that there's the actual transformer is there and you can, you can play with it and welcome
but in a case if you if you give that transformer this sequence here, you say what comes next, it says, okay, well 54% probability that there's a close brand there based on oh it's training data was a randomly selected collection of correct open close open close
parentheses sequences.
It has a little bit of a goof here, because it says with 0.0838% probability, this is the end of the sequence which would of course be grammatically incorrect, because there's no close for this for the for the open parentheses here.
If, if we give something which is correctly closing, then it says, okay, great, there's a 34% probability this is the end of the sequence there were no further opens here.
And it has a little bit of a goof here because it says 15% probability there's a close parenthesis that should occur here, which can't possibly be right, because if we put a close parenthesis here doesn't have a corresponding open parenthesis it's not grammatically correct.
But in any case this gives a sense of what it takes for one of these transformer nets we can look inside this transformer net, we can see sort of what it took to learn this very simple grammar chat GPT is learning the much more complicated grammar of English it's actually
probably to learn the grammar of English because there's so many clues in the actual words that are used to how they're grammatically put together. And there's so many things that we humans wouldn't even notice as wrong in some sense of wrong, because they're, they're kind of just
But in this more austere case of just this sort of mathematically defined parenthesis language we do notice so if we just give it a bunch of open brand open brand etc. And we ask it what's the highest probability continuation, you'll see it does pretty good up to this point, and then starts losing it.
And it's kind of a little bit like what would happen with humans you know we can tell at some point here that by just by I that these are correctly closed, it becomes more difficult to tell that when we get out here, and it becomes more difficult for the network to tell that to.
And this is a typical feature of these neural nets that we these sort of shallow questions of oh you just have you know you can just see this block of things you see another block of things. It does fine when it has to go to to much greater depth.
It's it doesn't work so well for a sort of regular computer that can do loops and things inside it's very easy to figure out what's happening here, because you effectively just count up the number of open brands, count down the number of closed brands and so on.
By the way, if you try this in actual chat GPT, it also it will confidently assert that it's it's match parentheses but it will often be wrong for larger parenthesis sequences, it has the exact same problem it's it's a it fails at a slightly larger size but it's still going to fail.
And that's just a feature of this kind of thing.
So,
well, okay, so one type of regularity and language that chat GPT is learnt is syntactic grammar.
Another type of regularity there's there's one more that that you can readily identify. And that's logic.
And what is logic. Well, originally when logic was in was invented, you know by Aristotle. So far as we know, you know what Aristotle did was effectively a bit like a machine learning system.
And that lots of examples of rhetoric lots of example speeches people gave, he said, what are some forms of argument that appear repeatedly. If somebody says, you know, something like people might have said, you know, all men are mortal socrates is a man.
Socrates is mortal. All, all X is a Y.
Z is a is a is an X. Therefore, Z is a Y.
The that that logic is taking sort of forms of, of, of language and saying, these are patterns that are repeated possible patterns in these in these pieces of language that are meaningful sequences.
And originally in syllogistic logic, which is what Aristotle originally invented, it really was very language based and people would memorize you know middle ages, people would memorize these forms of syllogism, Barbara syllogism the
the seller and syllogism and so on, which were just these, these patterns of, of word usage, where you could substitute in a different word for Socrates, but it was still that same pattern that same structure.
So that was, that was that's kind of another form of regularity and when chat GPT is says it's oh it's it's figuring things out. Well part of what's figuring out is, it knows so just logic because it's seen as zillion examples just like Aristotle
presumably seen a bunch of examples, when he invented logic, it's seen a bunch of examples of this sentence follows this sentence in this way. And so it can, it's going to do that to when it says, what's the statistical thing that's going to happen based on based on the web.
And so, so that's some.
So, by the way when logic developed by the 1800s when people like bull were getting into the picture and making formal logic.
It was no longer just these patterns, boom, it's a pattern it looks like this. It was more this thing you could build up many, many layers of structure, and you could build you know, very complicated logical expressions where the whole thing was deeply nested.
Of course, our computers today are based on those deeply nested logical expressions chat GPT doesn't stand a chance of, of decoding what's going on with one of those deeply nested kind of mathematical computational style bullion expressions, but it does well at this kind of Aristotle
level, kind of, you know, structure of sort of templated structure of logic.
Okay, well I wanted to talk just for a little bit and then we should wrap up here and I can try and answer some questions.
The about kind of.
So what are the regularities that chat GPT has discovered in this thing that we do which is language and all the thinking that goes on around, around language.
And I don't know the answer to this I have some ideas about what's going on. I'll just, you know, give a little bit of a tour.
We talked about kind of meaning space the sort of space of, of how words arrange in some, how you can arrange words and some kind of meaning space.
And we can, we can kind of see how words arrange these are different parts of speech for a given word there may be different places in meaning space where different instances that word occur this is the word crane.
And this is different sentences there are two obvious meanings of crane, you know, the bird, and the, the, and the machine, and they sort of break up in meaning space where they are.
The sort of structure of meaning space and I think we can ask is, is meaning space like physical space. Is it the case that there are parallel lines in meaning space other things where we can go from place A to place B, and we, and then in parallel
we transport to new places. Well so we can ask, you know, if we have analogies, is it the case that we can go, you know, from woman to man from Queen to King that those are sort of parallel cards in meaning space.
The answer is, well maybe a bit, not very convincingly. That's really the question in, in space, in physical space this is the question of whether this is like flat space.
It's like if we have things moving in flat space, you know, Newton's first law says if the thing is not acted on by force it will just keep going in a straight line.
If we have gravity, and we can represent gravity by talking about the curvature of space, here, this question is, when we go from, you know, ear to ear, eye to see, those are sort of, we're moving in a certain direction in meaning space.
And in a sense the question of whether these things correspond to whether we can do this kind of parallel transport idea is something like how flat is meaning space, how much effective gravity is there in meaning space or something like that.
Meaning space is probably not something that's represented in terms of the kinds of things that physical space is represented in terms of, but that's a question.
So now when it comes to the operation of chat, we can think about how is it moving around in meaning space, it's got its prompt, you know, the best thing about AI is, is it's, is its ability to okay.
And that's the prompt moving around in meaning space effectively. And then what chat GPT does is it, it continues that by continuing to move in meaning space.
And so the question is, is there something like a semantic law of motion, an analog of kind of the laws of motion that we have in physical space in the meaning space of, of concepts words, something where we can say, okay, if it's gone, if it's moved around this way, it's like it's got momentum in this
direction of meaning space going to keep going in that means space. It's nothing like that simple. But the question is, what are, how do we think about how do we represent kind of the, the, the sort of the, the process of going through meaning space.
Well, we can start looking at that we can say, for example, the different possible continuations that we get the best thing about AI is ability to, and then what's the next word. Well, we can look at this kind of fan of different directions that it could go in meaning space at that point and we can kind of see there's
some, there's some direction in meaning space, it tends to go in that direction. It's not going all the way over here, at least not with high probability. Okay, well if we keep going, we can kind of just see sort of how that fan develops as we go further out as we continue that sentence and we can kind of, this is kind of like our motion in meaning
space kind of question. And, you know, I don't know what this exactly means yet, but this is kind of what it looks like, what the trajectory in meaning space as chat GPT tries to continue a sentence looks like the green is that is the actual thing it
shows. I think this is a zero temperature case and the gray things are the other things that were lower probability cases. So that's, that's some.
That's kind of what that's some of you if we want to look at we don't want to want to do natural science on chat GPT and say what did it discover. What did it discover about how languages put together.
The possibility is that there are these sort of semantic laws of motion that describe sort of how meaning how you move through the space of meanings as you add words into it into a piece of text.
I think a slightly different way to think about this is in terms of what one could call semantic grammar. So syntactic grammar is just about, you know, nouns verbs things like that parts of speech things of that kind.
I would also ask, is there a generalization of that that is sort of more semantic that doesn't just look at that has finer gradations and just saying it's a noun it's a verb, and says oh well that verb means motion.
And when we put this noun together with this noun that's a thing you can move together with this motion word, it does this we kind of have buckets of meaning that are finer gradations and just parts of speech, but not necessarily individual words.
Is there a kind of a semantic grammar that we can identify that is kind of this construction kit for how we put together, not just sentences that are grammatically correct that are syntactically grammatically correct, but sentences which are somehow
correct. Now, that that I strongly think this is possible. And it's sort of what Aristotle was going for. He even talks about categories of sort of semantic categories and things like this.
He talks about a variety of things he does it in a, in a way that's based on the fact that it was 2000 years ago, and we didn't know about computers and we didn't know about a lot of kinds of formal things that we know about now.
Strangely enough, the amount of work that's been done, trying to make kind of a semantic grammar in the last 2000 years has been rather small.
There's a bit of an effort in the 1600s with people like Leibniz with his characteristic universalists and various other people trying to make what they call philosophical languages, sort of language word independent ways of describing meaning.
And then they're more recent efforts, but they've tended to be fairly specific fairly based on linguistics and fairly based on the details of structure of human language and so on.
And I think this, this, this idea that you can kind of have a semantic grammar is is a, and that that's what sort of being discovered is that there are these rules that go beyond that that are just rules for how you put together a, a meaningful
sentence. Now, you know, you can get a meaningful sentence could be something like the elephant flew to the moon.
The elephant means something sure it means something it has a perfectly we can conjure up an image of what that means. Has it happened in the world. No, it hasn't happened so far as we know.
And so there's a but you know could it be in a story could it be in a fictional world absolutely.
This thing about the sort of semantic grammar will allow you allows you to put together things which are somehow, which are sort of meaningful things to describe about the world.
The question of whether they are realized in the world or have been realized in the world is a separate question.
But in any case the, the thing that that is to me interesting about this is it's something I've long thought about because I spent a large part of my life building a computational language or from language system that is an effort to represent the world
computationally so to speak, to take the things that we know about about chemicals or lines or, or images or whatever else, and have a computational representation for all those things, and have a computational language, which knows how all those things
work it knows how to compute the distance between two cities it knows all of those kinds of things. And in, in, and so this is, I've been spending the last four decades or so, trying to find a way to represent things in the world in this computational
fashion, so that you can then compute, you can then compute things about those things in an explicit computational way. It's something where, and we've been very successful at being able to do that.
In a sense the story of modern science is a story of being able to formalize lots of kinds of things in the world and we're kind of leveraging that in our computational language to be able to formalize things in the world to compute things about how they work.
Now, the one feature of that computing about how things work is that inevitably some of those computations are deep computations, they're computations that something like a chat GPT can't possibly do.
And in a sense there's sort of a difference between the things that are the kind of the, the, the sort of shallow computations that you can learn from examples in something like a chat GPT that you can say this piece of language that I saw on the web here is, you know, statistically, I can sort of fit that in in this place, just
fitting together these sort of puzzle pieces of language is a very different thing from taking the world and actually representing it in some truly sort of formal way, computationally so that you can compute things about how the world works it's kind of like.
Well, back before people had kind of thought of this idea of formal formalism, maybe 400 years ago or more.
You know, everything that anybody figured out was just, you think about it in terms of language in terms of words in terms of sort of immediate human thinking what what then sort of came in with with mathematical science at first and then computation was this idea of
how things are getting these much deeper sort of ways to deduce what happens and thing I figured out mode, 3040 years ago now was, was this phenomenon of computational irreducibility, this idea that there really are things in the world where to compute what's going to happen, you
have no choice but to follow all those computational steps you can't just jump to the end and say I know what's going to happen. It's a shallow kind of thing.
So, you know, when we look at something like chat GPT there are certain kinds of things it can do by sort of matching together matching these pieces of language. There are other kinds of things it's not going to be able to do.
It's not going to be able to do so that the mathematical computation, the kind of the thing which requires an actual computational representation of the world.
So, those things like us humans, it's kind of a use tools type type situation, and very conveniently our Wolf Malphos system that used in a bunch of intelligent assistance and so on is has this feature that it's using our
language computational language underneath, but it actually takes natural language input. So it's actually able to take the natural language that is produced by a chat GPT for example, take that, and then turn that into computational
work out the results, get the right answer, feed that back to chat GPT and then it can talk sense, so to speak, rather than just following sort of the statistics of words on the web. So it's a way of, you know, by by allowing, but you can get sort of the best of
the world's by having something where you have this sort of flow of language, as well as, as something where you have the sort of depth of computation by having chat GPT use Wolf Malphos as a tool and I wrote a bunch of stuff about that and all kinds of things are happening with that.
But the thing that, you know, talking about what did chat GPT discover. I think the thing it discovered is, there is a semantic grammar to a lot of things there is a way to represent using sort of computational primitives, lots of things that we talk about in in text, and in our
computational language, we've got representations of lots of kinds of things, whether it's foods or chemicals or, or stars or whatever else. But when it comes to something like I'm going to eat a piece of chocolate, we have a great representation of the piece of
chocolate, we know all its nutrition properties we know everything about it. But we don't have a good representation yet of I'm going to eat the, I'm going to eat pot. What I think chat GPT has shown us is that it's very plausible to get sort of this semantic grammar of how one has these pieces of
talking, these sort of lumps of meaning in language. And I think what's going to happen and I've been interested in doing this for a long time I think this is now finally the impetus to really, really roll up one sleeves and do it.
It's a, it's a, it's a somewhat complicated project for a variety of reasons, not least, you have to make these kind of. Well, you have it has to be, you have to make sort of this process of designing a language is something I happen to have been doing for 40 years, designing our computational
language. This is a language design problem. And those are, to my mind, those are actually the single most concentrated intellectually difficult thing that I know is this problem of language design so this is sort of a generalization of that but I think chat GPT has kind of shown us what you know I didn't know how hard it was going to be I'm now convinced it's doable so to speak.
So, what, what does this some, you know, you might ask the question you know people might have said okay, look, you know we've seen your own nets that do speech to text we've seen your own let's do image identification.
Now we've seen your own that's that can write essays, surely if we have a big enough neural net it can do everything.
Not the neural nets of the kind we have so far that have the training structure that they have so far not on their own. They will not be able to do these irreducible computations.
Now these irreducible computations are not easy for us humans either. You know when it comes to doing piece of math or worse, if somebody says here's a program, run this program in your head. Good luck.
You know very few people can do that. It.
It's something where there is a difference between what is sort of immediate and easy for us humans and what is sort of computationally possible.
Now another question is, maybe we don't care about the things that aren't easy for humans. It's turned out that we built an awful lot of good technology over the last few centuries, based on what amounts to a much deeper level we haven't really in our technology
actually going even that far into irreducible computation but going far enough that it's beyond what we humans can readily do or what we can do with kind of the neural nets that exist today.
So I think the that that's the kind of the thing to understand that there's a there's a certain set of things what's what's happening in chat GPT is it's kind of taking the average of the web plus books and so on.
I'm saying, you know, I'm going to fit things together based on that, and that's how it's writing its essays, and it's, and when it is deducing things when it's doing logic things like that what it's doing is it's doing logic like the way Aristotle discovered logic.
It's figuring out, oh, there's a pattern of words that looks like this, and it tends to follow it like that, because that's what I've seen in 100,000 examples on the web.
And that's kind of what what it's doing. And it, it kind of that gives us some sense of what what it's going to be able to do and I think the most important thing it's able to do is it's a form of user interface.
You know, we can get, I might get something where I know, Oh, what really matters is three bullet points.
If I'm going to communicate that to somebody else, they really not going to understand my three bullet points. They need wrapping around that they need something which is a whole essay, describing, you know, that that's the human interface so to speak it's just like you could have, you know, the raw bits or
that wouldn't be useful to us humans, we have to wrap it in a human like in a sort of human compatible way, and language is sort of our richest human compatible medium.
And what what chat GPT is doing is it's able to, I think, what the way to think about it is it's providing this interface that is, well, it is just, it's generating pieces of language that are consistent but if you feed it specific things that it will talk
about, so to speak, then it's kind of wrapping the things the specifics with this interface that corresponds to kind of flowing human language.
All right, I went on much longer than I intended.
And I see there are a bunch of questions here and I'm going to go from to try and address some of these as a question from antipass are constructed languages like Esperanto more amenable to semantic grammar I approach very good very interesting
so I think the one that I was experimenting with was the smallest of the constructed languages, a language called tokypona that has only 130 words in it.
It is not a language that allows one to express, you know, everything one might want to express, but it's a good kind of small talk type language, a small language for doing small talk so to speak but it expresses a bunch of
ideas. And so I was, I was going to look at yes that it's a good clue, again to semantic grammar that there are these small constructed languages. It also helps.
And I think, well, I also think that probably the largest that construction languages, it will is another interesting source. It's a language which is trying to pull in all of the kind of language structures from all the all known languages and some first
question. The, that's some.
Yeah, that that's that yes so I think the answer is that yes I think they're a good stimulus for for thinking about semantic grammar in a sense when people were trying to do this back in 1600s.
They were confused about many things but you know, one gives them a lot of they've gone a long way given that it was a 1600s. They were confused about things like whether the actual letters that were written as you wrote the language mattered and how that was, you know, more so than the than the structure of
things but but there was the beginning of that, that kind of idea.
Yeah, I'm going to take these from the end but I want to go back to some of these others.
Okay, Tori is asking how come one study what's the best way of prompting chat GPT, could a semantic law of motion be helpful. Undoubtedly, yes, I don't know the answer that I think it's a good question and I don't really know.
The, you know, I think.
Yeah, I don't know.
Albert is asking is the 4000 token limits analogous to working memory with accessing larger memory be increasing the token limits, or increasing such capabilities reinforcement learning.
That the the token limits that exist right now are, you know, if you want to have a coherent essay, and you want it to know what it was talking about back in that early part of the essay, you better have enough tokens in that are being fed into the neural net.
Every time it gets a new token, if it just doesn't know what it was talking about it forgot what it was talking about 5000 tokens ago, it may be saying totally silly things now, because it didn't know what was there before.
So in some sense it's like it's I don't think it's I don't think it's like our short working memory. But I think, you know, it's kind of like you ramble on, I ramble on a lot.
You know, talking about things and like, I might have forgotten half an hour later that I talked about that already I might be telling the same story again I hope I don't do that I don't think I do that too badly.
But but you know that that's a question of what that that's the kind of thing that happens with this token limits.
Let's see let me go back to some of the questions that were asked earlier here.
Okay, Aaron was asking talking more about the tension between super intelligence and computational irreducibility. How far can LLM intelligence go I think I talked a little bit about that. I think this question.
Oh boy this is this is kind of complicated I mean so this question about.
Okay.
In the universe the world is full of computational irreducibility. That's, it's full of situations where we know the underlying rules but we run them as a computation, and you can't shortcut the steps.
What we've discovered from our physics project is it looks like the very lowest level of space time works just that way in fact just earlier today, so a lovely work about doing practical simulation of space times and things using using those ideas and very much
starting again this it's really computationally irreducible at the lowest level just like in in something like a gas the molecules are bouncing around in this computationally irreducible way.
What we humans do is we sample sort of aspects of the universe that have enough reducibility that we can predict enough that we can kind of go about our lives, like we don't pay attention to all those individual gas molecules bouncing around.
We don't pay attention to the aggregate of the pressure of the gas or whatever else we don't pay attention to all the atoms of space. We only pay attention to the fact that there's this thing that we can think of as more or less continuous space.
So, our story has been a story of finding slices of reducibility slices places where we can predict things about the universe there's a lot about the universe, we cannot predict we don't know.
If our existence depended on those things if we had not found kind of these these slices of reducibility, we wouldn't we wouldn't be able to have a coherent existence of the kind that we do.
So, if you ask sort of, where do you go with that. Well, there are there an infinite collection as an infinite kind of web of pieces of computational reducibility.
There's an infinite set of things to discover about that we have discovered some of them as we advance in our science and with our technology for things, we get to explore more of that kind of web of reducibility.
But that's that's really the issue now that the problem is that the way that we humans kind of react to that is, we have ways to describe what what we can describe we have a we have words that describe things that are common in our world we have a word for a
camera we have a word for a chair, those kinds of things. We don't have words for things which have not yet been common in our world.
And, you know, when we look at the innards of chat GPT it's got all kinds of stuff going on in it maybe some of those things happen quite quite often, but we don't have words for those we don't have a way we haven't yet found a way to describe them.
When we look at the natural world, we've, there are things that we've seen repeatedly in natural world we have words to describe them, we've built up this kind of descriptive layer for for talking about things.
But one of the things that happens is that if we kind of jump out to somewhere else in the son of universal possible computations.
There may be pieces of reducibility there, but we don't have words to describe those things we only have, we know about the things that are near us so to speak.
And so, and gradually as science advances, we get to expand the domain that we can talk about so to speak or everything advances we get to have more words we get to be able to talk about more things.
But, in a sense, to have something which operates.
It's this gradual process of us sort of societally in a sense learning more concepts, we kind of can exchange concepts we can build on those concepts and so on.
But if you throw us out into some other place in what I call the really add the space of all possible computational processes.
If you throw us out into an arbitrary place there, we will be completely confused, because there will be things we can tell there are actual computations going on here there are things happening there's even pieces of reducibility.
But we don't, we don't relate to those things.
So, it's kind of like imagine that you were, you know, you're here now.
And you're, you know, cryonically frozen for 500 years, and you wake up again, and there's all these other things in the world. And it's hard to reorient for all those other things without having seen the intermediate steps.
And I think that that when you talk about kind of what where can you go from what we have now. How can you sort of add more you basically intelligence is all about these kind of pieces of reducibility these ways to jump ahead and not just say it's what we what we think
of a sort of human like intelligence is about those kinds of things. And I think the, you know, what's the vision of what will happen you know when, when the world is full of a eyes, sort of interesting because actually we've seen it before.
I mean, when the world is full of a eyes and they're doing all these things and there's all this computational irreducibility. They're all these pockets of reducibility that we don't have access to because we haven't sort of, you know, incrementally got to that point.
What what's going to be happening is there's all this stuff happening among the AIs, and it's happening in this layer that we don't understand it's already happening in plenty of places on the web and you know, bidding for ads or showing you content on the web or whatever there's a layer of
AI that's happening that we don't understand particularly. Well, we have a very clear model for that which is nature. Nature is full of things going on that are often computationally irreducible that we don't understand.
What we've been able to do is to carve out an existence, so to speak, that is coherent for us, even though there's all this computational irreducibility going on, we've got these little niches with respect to nature, which, which are convenient for us as as
I think it's sort of the same thing with the, the AI world, as it becomes like the natural world, and it becomes sort of not immediately comprehensible to us. That's, we are, we are kind of, we're, you know, our view of it has to be, oh, that's just, you know, the operation of nature, that's just something I'm not going to understand.
There's just the operation of the AI is not going to understand that there's this piece that we've actually managed to humanize that we can understand. So that's, that's a little bit of the, the thought about, about how that develops.
In other words, you know, you can say, I'm going to throw you out to some random place in the really add there's incredible computations happening. It's like, great, that's nice, I've spent a bunch of my life studying those kinds of things, but pulling them back, reeling them back into something
that's sort of direct human understandability is, is a difficult thing.
Aaron is asking more of a business question about, about Google and the transformer architecture and why, you know, it's been a very interesting thing that the, the sort of neural nets with this
small field very fragmented for many, many years, and then suddenly things started to work in 2012. And a lot of what worked and what was really worked on was done in a small number of large tech companies and some not so large tech companies.
And it's sort of a different picture of where innovation is happening than has existed in other fields. And it's, it's kind of interesting, it's kind of potentially a model for what will happen in other places but, but you know, it's always complicated what, what causes one group to do this and
another group to do that and there are the entrepreneurial folk who are smaller and more agile and, and there are the folks who have more the more resources and so on it's always complicated.
Okay, Nicola is asking, do you think the pre training a large biological inspired language model might be feasible in the future. I don't know. I think that the, the figuring out how to train something that is, you know, we don't know what parts of the
biology are important. One of the, one of the incredibly important things we just learned is that probably there's not much more to brains that really matters for their information processing than the, the neurons and their connections and so on.
It could have been the case that every molecule has, you know, some quantum process that's going on. And that's where thinking really happens, but it doesn't seem to be the case, because this this pinnacle of kind of our sort of thinking power so being able to write the long essays and so on.
It's, it seems like that can be done with just a bunch of neurons with weights. Now, which other parts of biology are important, like, you know, actually Terry Sinovsky just wrote this paper, talking about how there are more backwards going neural connections and brains
and forwards going ones. So in that sense, it looks like maybe, maybe we missed the point with these feed forward networks that that's something like chat GPT basically is, and that the feedback is, is, you know, is really important but we don't yet, we haven't yet really got the right idealized model of that.
I do think that the, you know, the sort of the, the, the what's the next McCulloch pits type thing what's the next sort of simple meta model of this is important.
I also think that there's probably a bunch of essential mathematical structure to learn about general mathematical structure to learn.
You know, I was interested in neural nets back around 1980.
And I kind of was trying to simplify simplify simplify models of things.
And neural nets, I went, I went past them, because they weren't simple enough for me they had all these different weights and all these different network architectures and so on. And I ended up studying solid automata.
And generalizations of that where, where, you know, you have something where the everything is much simpler there are no real numbers that are no arbitrary connections there are no this that and the other things, but what what matters and what doesn't.
We just don't know that yet.
Paul is asking what about a five senses multimodal model to actually ground the system in the real world with real human like experience. I think that will be important and that will no doubt happen.
And, you know, you'll be more human like, look, this chat GPT is pretty human like when it comes to text, because by golly it just read a large fraction of the text that we humans at least publicly wrote.
And, but it didn't know it hasn't had the experience of walking upstairs and doing, you know, doing this or that thing. And so it's not going to be very human like when it comes to those sorts of things if it has those experiences, then, then I think we get to, you know, then that that will that will be
interesting.
Okay, someone's commenting on the fact that I should do the same kind of description for image generation, generative AI for images.
The thing that I like to think about there is I think that's, that's one of our first moments of communication with an alien intelligence.
We, in some sense, we're talking to the generative AI in English words or whatever, and it's going into its alien mind, so to speak, and plucking out the stuff that is these images and so on.
It's less so, you know, with chat GPT, what the output is something that is already intended to be very human, it's human language, with an image generation system, it's more, it's really, it's producing something that has to be somewhat recognizable to us.
It's not a random bunch of pixels. It's something that resonates with things we know. But in a sense, it can be, it can be more completely creative in what it's showing us. And in a sense, as one tries to sort of, you know, navigate around its space of what it's going to
show us, it feels a lot like kind of, you're communicating with an alien intelligence, and it's kind of, it's kind of showing you things about how it thinks about things by saying, oh, you said those words, I'm going to do this, and so on.
I mean, I have to say that if we can't, you know, the other examples of alien intelligence is that we have all around the planet, lots of critters from the citations on, so to speak, that, and I have to believe that if we could correlate kind of the experiences of those critters, cats,
you know, cockatoos, whatever else, and the vocalizations that they have and so on. And we could, you know, that it's, it's talk to the animals time, so to speak. I mean, I think that's a, that feels like that that's, you know, the kinds of things we've learned from chat
about the structure of human language. I am quite certain that if there's any linguistic structure for other, for other animals, it'll be similar, because it's one of the lessons of biology is, you know, there are fewer ideas than you think, the, you know, these things that we have have precursors in biology
long, long ago. We may have made innovations in language, it's kind of the key innovation of our species, but whatever is there had precursors in other organisms, and that's what, and the fact that we now have this much better way of kind of teasing out a model for, for language in humans
means we should be able to do that elsewhere as well.
Okay, David is saying chat GPT's developers seem committed to injecting sort of political curtailments in the code, because to avoid it talking about a controversial topics how's that done. It's done through this reinforcement learning
stage I think maybe there's also some actual, you know, if it's starting to use these words just just stop it type things. I think maybe that's being done a little bit more with maybe with being that it is with with chat GPT at this point.
I think that the
I have to say the one thing that I consider a, you know, so far as I know chat GPT is a G rated, you know, thing.
And that's an achievement in its own right that doesn't maybe I shouldn't say that because probably maybe they're a horrible counter examples to that. But I think that was a, you know, in terms of one of the things that happens is well you have a bunch of humans and they are giving it this training and those humans have opinions and they will have, you know,
there'll be this kind of politics or that kind of politics or they'll believe in this or that or the other. And they are, you know, whether purposefully or not, they're, you know, they're going to impose those opinions because there is no, you know, the opinion is what you're doing when you tell chat GPT that I says good that I say isn't good, you know, at some level that's an opinion now that opinion may or may not be colored into something that is about you know politics or something like that.
But it's sort of inevitable that you have that I mean I have to say, you know, something I've thought about a little bit in connection with with general sort of AI injection into sort of the things we see in the world like social media content and so on.
I tend to think that the right way to solve this is to say okay let's have multiple, you know, chatbots or whatever, and they are in effect trained with different criteria by different groups under different banners so to speak.
And you know you get to pick the banner of chatbots that you want to be using and then then you're happy because you're not seeing things that horrify you and so on.
And you can discuss you know whether you want to pick the chatbot that that accepts the most diverse views or whether you want to you know that that's a that's that sort of throws one back into into kind of standard issues of political philosophy and things like this.
I mean I think the thing to realize is that there is a there's sort of an ethics, you know one wants to put ethics somehow into what's going on, but when one says let's have the AIs, you know, do the ethics.
It's like that's hopeless ethics is a there is no sort of mathematically definable perfect ethics ethics is a the way humans want things to be.
And then you have to choose you know, well is it the average ethics is it the, you know, the ethics which makes only 5% of the people unhappy.
Is it this than the other these are old questions of political philosophy that don't really have so far as we know good answers.
And, but once thrown into those questions there's no, you know, oh we'll get a machine to do it and it'll be perfect. It won't happen because these are questions that that aren't solvable for a machine because they're questions that, in a sense, come right from us.
These are, I mean, the thing to realize about chat GPT in general, chat GPT is a mirror on us. It's taken what we wrote on the web, so to speak, in an aggregate, and it's reflecting that back to us.
So in so far as it does goofy things and says goofy things, you know, some, that's really on us. I mean that's, you know, it's our sort of, it's, it's the average kind of
the, the, the, the, the sort of the average web that we're seeing here.
So nature's is asking about a particular paper which I sounds interesting but I don't know about it.
See, up up soon here.
Dragath is wondering how neural net AI compares to other living multi cellular intelligence.
Plant roots.
No vets and things like jellyfish and so on biofilms. Yeah, well, okay. So one of the big things that's come out of a bunch of science that I've done is this thing I call the principle of computational equivalents, which essentially says that as soon as you have a system that is
not computationally trivial, it will ultimately be equivalent in its computational capabilities.
And that's an important thing when you talk about computational irreducibility because computational irreducibility arises because that you've got a system doing its computation.
There's no system. You can't expect there to be all other systems will just be equivalent in their computational sophistication.
You can't expect a super system that's going to jump ahead and just say, oh, you went through all these computational steps, but I can jump ahead and just get to the answer.
Now, a question that is a really good question is when we look at, okay.
One of the things that is characteristic of our consciousness, for example, relative to all the computational irreducibility in the universe is the fact that we have coherent consciousness is a consequence of the fact that we are two things.
It seems to me, we are computationally bounded. We're not capable of looking at all those molecules bouncing around. We only see various aggregate effects.
That's point one and point two, that we are, we believe that we are persistent in time we believe we have a persistent thread of of existence through time turns out big fact of our last few years for me is that the big facts of physics,
general relativity theory of gravity quantum mechanics and statistical mechanics the second law of thermodynamics law of entropy increase all three of those big theories of physics that arose in the 20th century, all three of those can be derived from knowing that we human
observers are noticing those laws and we human observers have those two characteristics I just mentioned I consider to say a very important beautiful sort of profound result about kind of the fact that we observe the physics we observe because we are observers of the kind that we are.
Now, interesting question, I suppose, is when we.
So we are limited, we are computationally limited things and the very fact that we observe physics the way we observe physics is a consequence of those computational limitations.
So, a question is, how similar are the computational limitations and these other kinds of systems in a sense, the fungus as observer, so to speak, how similar is that kind of observer to a human observer.
And in terms of sort of what computational capabilities it has and so on, my guess is it's pretty similar.
And in fact, one of my next projects is a thing I'm calling observer theory, which is kind of a general theory of kinds of observers that you can have of things and so maybe we'll learn something from that but it's a it's a very interesting question.
I'm just commenting.
Chat GPT can be improved using an automated fact checking system like an adversarial network, for instance, could one basically could one train chat GPT with Wolf Malphur and have it get better.
It's probably up to a point, but then it will it will lose it just like it does with parentheses. I mean, there's a certain with a network of that architecture, there's a certain set of things one can learn but one cannot learn what is computationally irreducible.
I mean, it's in other words, you can learn the common cases, but there'll always be surprises, there'll always be unexpected things that you can only get to by just explicitly doing those computations.
I'm asking, can chat GPT player text based adventure game I bet it can. I don't know I haven't seen anybody try that but I bet it can.
There's a question here from software side from being trained on a huge corpus. What is it about GPT three that makes it so good at language I think I tried to talk about that a bit about the fact that we it's it's some that there's, you know, there's regularity in language I think the the
particulars of the transformer architecture of this kind of looking back on sequences and things that's been helpful in refining the way that you can train it.
That seems to be important.
Let's see.
I'm asking, could feature impact scores help us understand GPT better.
Well, so what that what that's about is, when you run a neural net, you can kind of, you can say, sort of how much.
What was the, how much did some particular feature affect the output that the neural net gave.
I mean, chat with GPT is just a really pretty complicated thing. I mean, I started digging around, trying to understand sort of what as a natural scientist, you know, I'm like, I couldn't do sort of neuroscience with actual brains because I'm 100 times 1000 times to squeamish for that.
You know, I can dig around inside an artificial brain. And I started trying to do that. And it's, it's, it's difficult. I mean, I, I, I didn't look at feature impact scores. I think one could the.
Okay, so.
But by the way, I mean, I'm amused by these questions because, because I can kind of, you know, I can still tell you guys are not bots. I think.
And let's see.
Ron is asking about implications like I have to work late tonight. What does that mean?
Yeah, absolutely chat GPT is learning stuff like that because, because it's seen, you know, a bunch of texts that says I have to work, work late tonight.
So I can't do this. It's seen examples of that. It's kind of doing the Aristotle again. It's just seeing this, you know, these patterns of language.
That's what it's learning from, so to speak. So yes, these things we might say, how do we think about that formally? Oh, it seems kind of complicated to us, but that pattern of language has has occurred before.
All right, last, last thing, perhaps.
Okay, Albert is asking, do you think humans learn efficiently because they're born with the right networks to learn language more easily? Or is there some difference?
I think it is important the architecture of the brain undoubtedly is important. I mean, you know, my impression is that there are, you know, it's a matter for the neuroscientists to go and find out now that we know that certain things can be made to work with artificial
aspects. Did the actual brain discover those things too? And the answer will be often yes. I mean, just like there are things we probably have learned from, you know, the flight of drones or the flight of planes that we can go back and say, Oh, did we did biology actually already have that idea?
I think that the there undoubtedly features of human language, which depend on aspects of the brain. I mean, like, for example, one, you know, talking to Terry Zanowski, you know, we're talking about the loop between the basal ganglia and the cortex.
And the possibility that, you know, the outer loop of chat GPT is a little bit like that loop. And it's kind of like, I'm turning things over in my mind, one might say, maybe that's actually a loop of data going around this literal loop from one part of the brain to another.
Maybe, maybe not. But sometimes those those sayings have a habit of being more true than you think. And maybe the reason that when we think about things, we have these certain time frames.
When you think about things, the certain times between when words come out, and so on, maybe those times are literally associated with the amount of time it takes for signals to propagate through some number of layers in our, in our, in our brains.
And I think that in that sense, if that's the case, there will be features of language, which are, yes, we've got this brain architecture, we're going to have this these features of language. And in so far as language evolves as so far as it's, it's adaptively worthwhile to have a different form of language that is optimized by having some
form of brain structure. That's what will have been driven by by natural selection and so on. I mean, I think, you know, there are aspects of language like we know, if you, you know, we tend to remember five chunks, you know, chunks of five, so to speak, things at a time and we know that if we try and give a sentence which has
more and more and more deeper, deeper, deeper sub clauses, we lose it off to some point. And that's presumably a hardware limitation of our brains.
Okay, Dave is asking, this is a good last question, how difficult will it be for individuals to train something like a personal chat GPT that learns to behave more and more like a clone of the user. I think I don't know.
I'm going to try it. I have a lot of training data, as I mentioned, you know, 50 million typed words type, yeah, typed words, for example, for me.
And my guess is, I mean, I know somebody tried to train a an earlier GPT three on on stuff of mine wasn't I didn't think it was terribly good. When I read ones trained for other people I thought they're pretty decent.
When I when I looked at one trained for myself because I kind of know myself better than I know anybody else, I think the, you know, it didn't ring true, so to speak.
And, but I do think that that will be a, you know, being able to write emails like I write emails, it'll do a decent job of that I suspect.
You know, I would like to believe that, you know, one still as an as a human one still has an edge because, in a sense, one knows what the goals are the the, you know, know this system its goal is to complete English text.
And, you know, the bigger picture of what's going on is not going to be part of what it has, except in so far as it learns the aggregate bigger picture from just reading lots of text.
So, you know, but but I do think it'll be an interesting I expect that, you know, I, as a person who gets a lot of email, some of which is fairly easy to answer, in principle, that, you know, maybe my bot will be able to answer the easier stuff for me.
All right, that's probably a good place to wrap this up.
Thanks for joining me and
I would like to say that for those interested in more technical details.
Some of the folks in our machine learning group are going to be doing some more detailed technical webinars about about this material and really going into how you would, you know, how you build these things from scratch, and so on.
And what some of the more, more, more detail about what's happening actually is, but I should wrap up here for now. And thanks for joining me.
And bye for now.

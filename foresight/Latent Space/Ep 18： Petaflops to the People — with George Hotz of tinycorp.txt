Hey everyone, welcome to the Late In Space podcast.
This is Swix, writer and editor of Late In Space, and Alessio is taking over with the
intros.
Alessio's partner and CT1 residents at Decibel Partners.
Hey everyone, today we have GeoHot on the podcast, aka George Hots for the human name.
Everybody knows George, so I'm not going to do a big intro, a couple things that people
might have missed.
So you were the first to unlock the iPhone, you traded the first ever unlocked iPhone for
Nissan 350Z and three new iPhones.
You were then one of the first people to break into the PS3 around arbitrary code.
You got sued by Sony, you wrote a rap song to fight against that, which is still live
on YouTube, which we're going to have on the show notes.
Then you did not go to Tesla to build vision, and instead you started ComIi, which was an
amazing engineering feat in itself until you got a season disease from the government
to not put these things on the street, turned that into a research-only project.
You know they're out there.
Yeah, yeah, no, no, no, they're out there, but they're not a, you know, you market them
as a research kind of like no warranty.
Because I use the word DevKit, that's not about the government, that's nothing to do
with the government.
We offer a great one, you're warranty.
The truth about that is it's gatekeeping.
What's the difference between a DevKit and not a DevKit, nothing.
What's the question of do you think it's for you, and if you think it's for you, buy it.
It's a consumer product.
We call it a DevKit.
If you have a problem with that, it's not for you.
Great framing.
That's great insight.
And then I was going through your broadcast to get to the day, you've heard this post
about the hero's journey, and you link this thing called the portal story, which is kind
of the set of stories in movies and books about people living this arbitrary life and
then they run to this magic portals, kind of takes them into a new, very exciting life
and dimension.
When you've heard that post, you talked about TinyGrad, which is one of the projects you're
working on today.
And you mentioned this is more of a hobby, something that is not going to change the
course of history.
Obviously, you're not going full speed into it, so we will learn more about what was the
portal that you run into to get here.
Well, what you realize is, you know what made me realize that I absolutely had to do the
company?
Seeing Sam Maltlin go in front of Congress.
Why?
What are the odds they nationalize NVIDIA?
What are the odds that large organizations in the government, but of course I repeat
myself, decide to try to clamp down on accessibility of ML compute?
I want to make sure that can't happen structurally, so that's why I realize that it's really important
that I do this.
And actually, from a more practical perspective, I'm working with NVIDIA and Qualcomm to buy
chips.
NVIDIA has the best training chips, Qualcomm has the best inference chips.
Working with these companies is really difficult, so I'd like to start another organization
that eventually in the limit either works with people to make chips or makes chips itself
and makes them available to anybody.
You share kind of three core thesis to Tiny Core, maybe we can dive into each of them.
So, XLA, PrimeTorch, those are the complex instruction system, TinyGrad is the restricted
instruction system, so you're kind of focused on, again, TinyGrad being small, not being
over complicated and trying to get as close to the DSP as possible in a way where it's
at more.
Well, it's a very clear analogy from how processors developed.
So a lot of processors back in the day were CISC, complex instruction set, system 360
and then x86.
Then this isn't how things stayed.
They went to now the most common processors on ARM and people are excited about RISC
5.
RISC 5 is even less complex than ARM.
No one is excited about CISC processors anymore, they're excited about RISC Reduce Instruction
Set processors.
So TinyGrad is we're going to make a RISC offset for all ML models and yeah, it can
run all ML models with basically 25 instead of the 250 of XLA or PrimeTorch.
So about 10X less complex.
Yep.
You talk a lot about existing AI chips.
You said if you can write a fast ML framework for GPUs, you just can't write one for your
own chip.
So that's another one of your core insights, I don't know if you want to expand on that.
Yeah.
I mean, your chip is worse, right?
There's no way the chip that you're going to tape out, especially on the first try is
going to be easier to use than an AMD GPU, right?
And yet there's no good stack for AMD GPUs.
So why do you think you can make one for your chip?
You can't.
Right?
The only company, there's one other company, aside from Nvidia, who's succeeded at all
at making training chips.
What company?
AMD.
Intel?
No.
No.
No.
I've never trained.
Who's trained a model on AMD or Intel?
Nobody on AMD.
Cerebrus.
Cerebrus, I'm talking about, you might know some startups who trained models on these
chips.
I'm surprised no one immediately gets this because there is one other chip, aside from
Nvidia, that normal people have actually used for training.
That's a real neural engine?
No.
Used for training.
No.
You can only buy them in the cloud.
Oh, TPU.
Exactly.
Yeah.
So, mid-journey is trained on TPU.
Right?
Like, a lot of startups do actually train on TPUs, and they're the only other successful
training chip, aside from Nvidia.
But what's unique about Google is that they also wrote their own ML framework, right?
And if you can't write your own ML framework that is performant on Nvidia, there's no way
you're going to make it performant on your...
And they started from TensorFlow, and then they made the chip after.
Yeah.
Exactly.
Exactly.
And you have to do it in that direction.
Because you're going to end up, you know, a service, what are those things, a million
dollars?
I've never seen a service.
No one's ever like, oh, I trained my model on a service.
Most people are like, I trained my model on TPUs.
Some people, 20%, are like, I trained my model on TPUs.
Yeah.
And then the third one, which is the one that surprised me the most is, through incompleteness,
it's harmful.
It should be avoided.
It made sense once I read it, but maybe tell us a bit or more about how you got there.
Okay.
So, CPUs devote tons of their silicon and power to things like reorder buffers and speculative
execution and branch predictors.
And the reason that you need all these things is because at compile time, you can't understand
how the code's going to run.
This is Rice's theorem.
This is the Halting problem, and it's limit.
And this is not like, oh, the Halting problem is theoretical.
No, no, no, no.
It's actually very real.
Does this branch get taken or not?
It depends on X.
Where does X come from?
I forget it, right?
But no branches depend on X in a neural net.
Every branch is a static loop.
Like if you're doing a matrix multiple, it's a static loop over the inner dimension.
And neural networks are even better.
No loads even depend on X, right?
So with a GPU shader, right, you're like, your load might depend on which texture you're
actually loading into RAM.
But with a neural network, your load is, well, I load that way.
Why?
Well, because I load that way the other million times I ran the same net.
Every single time you run the net, you do the exact same set of loads, stores, and arithmetic.
The only thing that changes is the data.
And this gives you a very powerful ability to optimize that you can't do with CPU-style
things, which have branches, and even GPU-style things, which have loads and stores.
Oh, that makes sense.
Well, GPUs, if you want GPU-style stuff, you have load based on X, you now need a cache
hierarchy, and not an explicit cache hierarchy, an implicit cache hierarchy, with eviction
policies that are hard-coded into the CPU.
You start doing all this stuff, and you're never going to get theoretically good performance.
Again, I don't think there's 100X.
Some startups will talk about 100X, and they'll talk about absolutely ridiculous things like
clockless computing or analog computing.
Okay, analog computing just won't work.
And clockless computing, sure, it might work in theory, but your ETA tools are...
Maybe AIs will be able to design clockless chips, but not humans.
What actually is practical is changing cache hierarchies and removing branch predictors
and removing warp schedulers.
GPUs spend tons of power on warp scheduling, because we have to hide the latency from the
memory.
We'll have to hide the latency if everything's statically scheduled.
What do you think people are still hanging on to during complete?
Well, because it's really easy.
Turning complete is just really easy.
It's really easy to just be so nice if I could do an if statement here and actually branch
the code.
So it requires a lot more thought to do it without turning completeness.
And would this be qualitatively different than TPUs?
So TPUs are a lot closer.
Yeah.
TPUs are a lot closer to what I'm talking about than like CUDA.
Okay.
So what is CUDA?
Well, CUDA is a C-like language, which compiles to an LLVM like IR, which compiles to PTX,
which compiles to SAS, which are all turned complete.
TPUs are much more like this, yeah.
Their memory is pretty statically managed.
I did some reverse engineering on the TPU.
It's published in TinyGrad.
It has like a VLIW instruction and it runs them.
So it's similar.
I think the TPUs have a few problems.
I think systolic arrays are the wrong choice.
Systolic array, I think they have systolic arrays because that was the guy's PhD.
Right.
And of course, Amazon makes...
Jake, could you summarize systolic arrays for this?
Systolic arrays are just...
Okay.
So basically you have like, this is a way to do matrix multiplication, think of a grid
of malax and then the grid can multiply and then shift, multiply, then shift, multiply,
then shift.
And they are very power efficient, but it becomes hard to schedule a lot of stuff on
them if you're not doing like perfectly sized dense matrix multiplies, which you can argue,
well, design your models to use perfectly sized dense matrix multiplies, sure, but it's
just...
No, but thanks for indulging on these explanations.
I think we need to keep our audience along with us by pausing every non-dent to explain
key terms.
You know, when I say explain a systolic array, I just immediately get a picture in my head
of like tilting a matrix and shifting it.
It's hard to kind of explain.
Yeah.
We'll do some videos so you have your hand actions and we edit it in visuals.
Yeah.
There's some great graphics that just show you, oh, so that's what a systolic array is,
but it's a malax shift machine that looks kind of different from the typical like APU
sort of machine.
Sorry, ALU sort of machine, I think the right answer is something that looks more like queues
that feed into ALUs and then you can like prefetch the loads from the memory, put it
a bunch of queues and then the queue is just like and feeds into another queue over here.
But yeah, but that's not even the main problem with TPUs.
The main problem with TPUs is that they're closed source, not only is the chip closed
source, but all of XLA is open source, but the XLA to TPU compiler is a 32 megabyte binary
blob called libTPU on Google's cloud instances.
It's all closed source, it's all hidden stuff and you know, well, there's a reason Google
made a closed source, Amazon made a clone of the TPU, it's called Inferencia, or they
have some other name for it, a training, training, yeah, yeah, yeah, yeah, yeah, and you look
as a clone of the TPU.
It just software doesn't work though, and the Google software at least kind of works.
So those are kind of like the three quarter thesis.
The first thing you're working on that you've been working on is TinyGrad, and one of your
Twitch streams, he said, is the best thing you've ever written.
Yeah, tell us a bit more about that creation.
For a long time, TinyGrad had a hard limit of 1,000 lines of code.
And what this would force you to do is really make sure you were not wasting lines.
I got rid of the restriction because it became a little code golfy at the end, but once like
the core framework of TinyGrad was there, in those 1,000 lines, it's not huge now, it's
like 2,800 lines now, it's still very readable.
But like the core framework, the ideas are expressed with no boilerplate.
If you go read PyTorch, you know, PyTorch, I think it's actually pretty good code.
I think Facebook's pretty good, but there's so much boilerplate.
Go in PyTorch and try to track down how an LGU actually works.
Just a lot of distractions.
Oh, you're going to be diving down a long stack from Python to C to custom libraries
to dispatchers to, and then I don't even know how to read TensorFlow.
I don't even know where's an LGU in TensorFlow, nobody knows.
Someone at Google knows maybe.
Google as an organism, I don't know if anyone individually Google knows.
What are like the important ergonomics like for a developer as you think about designing
the TinyGrad API?
So the TinyGrad front end looks very similar to PyTorch.
There's an even higher level front end you can use for TinyGrad, which is just Onyx.
We have better support for Onyx than Core ML does, and we're going to have, I think
we're going to pass Onyx Runtime soon, too.
I like people think Onyx Runtime, that's a gold standard for Onyx.
No, you can do better.
Pass them in what?
Specifically?
Test, compliance tests.
So Onyx has a big set of compliance tests that you can check out.
And we have the running in TinyGrad, and there's some failures.
We're below Onyx Runtime, but we're beyond Core ML.
So that's where we are on Onyx support now, but we will pass, we will pass Onyx Runtime
soon, because it becomes very easy to add ops, because of how you don't need to do anything
at the lower levels.
You just do it at this very high level, and TinyGrad compiles it to something that's
fast using these minimal ops.
You can write, I mean, most concretely, what TinyGrad can do that PyTorch can't really
do is if you have something like A times B plus C, if you write that in naive PyTorch,
what it's going to do on the GPU is, well, read A, read B in a kernel, and then store
A times B in memory, and then launch another kernel to do A times B plus C, got to do those
loads for memory.
I know I did a whole extra round trip to memory that I just didn't have to do.
You're like, yeah, but you can use the TorchJIT and it corrects this.
Yeah, for that one example, for that one example of MULAC, but oh, now you did three multiplies,
six multiplies, right?
It won't compile arbitrary code.
If you looked into the other approaches like PyTorch Lightning to accelerate PyTorch itself.
Well, PyTorch Lightning, my understanding is it's mostly a framework around PyTorch, right?
PyTorch Lightning is not going to fix this fundamental problem of I multiply six tensors
together.
Why is it going to memory any more than a single read from each and a single write to the output?
Okay.
There are lower level things in PyTorch that are not exactly sure what Dynamo does, but
I know they're generating some Triton stuff, which is going to generate the kernels on
the fly.
But you know, PyTorch Lightning is at a higher level of abstraction.
So TinyGrid's front-end stuff looks like PyTorch.
I made a few tweaks.
There's a few things I don't like about PyTorch.
Why is ReLU a class?
Oh, really?
What was the state?
You make a class and there's a state.
Everything should just be Torch Functional and ReLU, but just dot ReLU on the tensor.
Also there's things in Torch where you have to do tensor dot and not a tensor dot.
Right?
Why?
Why are these things?
It just shows an API that's not perfectly refined, but when you're doing stuff TinyGrid
style where you don't have lines, well, it has to work this way because even the lines
to express the, well, you can't use the where operator unless, and the where operator in
PyTorch.
Why is it a true case, condition, false case?
The worst, that's how Python expresses ifs.
It's disgusting.
Where operators are much nicer, it should be, I can do my like, a less than zero dot
where, a comma one, right?
The very pandas like API.
Yeah, yeah, yeah, yeah, yeah, it's just, it's some, it looks like Torch numpy pandas.
They're all very similar.
I tried to take like the cleanest subset of them and express them, but like I said, you
can also interact with it using Onyx.
Yeah.
But I have a rewrite of stable diffusion, I have a rewrite of llama, I have a rewrite
of whisper.
You can look at them.
They're shorter than the Torch version than I think they're cleaner.
They stream them all.
Yeah.
Very nice.
Laziness is kind of the other important concept that you're leveraging to do, operation fusing.
Yeah, talk a bit more about that.
So yeah, you have, you have basically like a few different like models for compute.
The simplest one's eager.
All right, the simplest one is eager as soon as the interpreter or sees A times B, it actually
dispatches A times B, right?
Then you have graph like TensorFlow, which will put A times B into a graph and then we'll
do absolutely nothing until you actually compile the graph at the end.
I like this third choice, just somewhere in the middle, laziness.
Laziness is you don't know when the ops are going to dispatch and don't worry about that.
You don't have to worry about this as a programmer.
You just write out all your stuff.
And then when you actually type dot numpy, it'll be ready by the time you copy the thing
back to CPU.
Or you can do dot realize and it will actually like force that tensor to be allocated in RAM.
But yeah, a lot of times, right, like, and if you think about it, PyTorch is kind of
lazy in a way, but they didn't extend the paradigm far enough, right?
When I do A times B in PyTorch, it's going to launch a CUDA kernel to do A times B, but
it's not going to wait for that CUDA kernel to complete.
So you're getting the worst possible world.
You're getting the same laziness, but you also can't get fusion because PyTorch doesn't
know that I'm then going to do plus C. There's no way for it to be like, whoa, whoa, whoa,
don't launch that CUDA kernel, whoa, just do this one too, right?
You can kind of like, again, this stuff, PyTorch is working on this.
And you know, it's a little bit harder.
Like in comma, I felt like I was competing against a lot of idiots here.
I'm competing against, you know, smart, smart, very smart people who made, yeah, who've made
some, I think, different trade offs, right?
We've made some different trade offs, whereas if you're trying to build something that is
just straight up good on Nvidia, and we have a lot of people in complexity to throw at
PyTorch made a lot of the right choices.
I'm trying to build something that manages complexity, like you can always make your
software do more.
The magic is when you can make your software do more without adding complexity, right?
Because, you know, complex things eventually collapse under the wind.
So it's kind of that.
How does fusing actually work?
Like TensorFlow actually collapsed under it.
It's kind of what happened, right?
How does fusing actually work?
So yeah, there's this thing called lazy.py, and when you do like a times b, that's, it's
put into a graph, but it's a very local graph.
There's no global graph optimizations, and even this can change, right?
Again, like the programming model for TinyGrad does not preclude eagerness, right?
Laziness is not guaranteed laziness.
It's just going to try its best.
So you put in a times b, and that's a binary app, right?
And then you put in a times b, like that's a node in the graph.
That's a virtual node, because it's not realized yet.
Plus c.
Okay, here's a new node, which takes the c tensor in here and takes the output of a
times b.
It's like, whoa, wait, there's two binary ops.
Okay, we'll just fuse those together, okay?
Here I have a kernel.
This kernel has a, b, and c as inputs.
It does a times b plus c in the local registers, and then outputs that to memory.
And you can graph dot one in TinyGrad.
Another like amazing thing that TinyGrad has that I've not seen in any other framework
is two things.
Graph dot one, graph equals one, which is an environment variable.
It will output a complete graph of all the operations.
People are like, oh, you can use PyTorch, export it to Onyx, and use Netron.
Yeah, you can, but like, what?
That's not what's real.
Right, graph dot one will show you the actual kernels that were dispatched to the GPU.
You can also type debug equals two, which will print those kernels out in your command
line, and it will tell you the exact number of flops and the exact number of memory accesses
in each kernel.
So you can immediately see, wait a second, okay, this kernel used this many flops.
This was the gigaflops.
This is how many bytes it read, and this was the gigabytes per second.
And then you can profile without having to like, okay, I mean in theory in PyTorch, sure,
use the NVIDIA insight profiler.
No one does that.
No one does, of course, because it's so difficult, right, like, like, actually NVIDIA used to
a pre, pre, I think kuda nine was the last one that had it.
They had a command line one, but now it's like, okay, I'm going to generate this blob,
use this NVIDIA GUI tool to convert it into a Chrome trace, and then load it, and yeah,
no one does that, right?
I'll just type debug equals two in any tiny grad model, and it will show you all the kernels
that it launches, and the efficiency of each kernel, basically.
Yeah, this is something that John Karmic has often commented about, is that when you code,
you need to build in your instrumentation or observability right into that.
I wonder if whatever John is working on, he's adopting this style, and maybe you can sort
of encourage it by, like, I don't know, naming it and coining it as a certain kind of debugging
style.
If he would like to start contributing to tiny grad, I'd be, I don't know, I've chatted
with a few times.
I'm not really sure what his company's doing.
Yeah.
I think it's all, I think it's pretty, but no, I mean, hopefully, like, we get tiny grad
to a point where people actually want to start using it.
So tiny grad right now is uncompetitive on, it's uncompetitive on NVIDIA, and it's uncompetitive
on x86.
And specifically, what do you care about when you say uncompetitive?
Speed.
Okay.
Shut up, speed.
It's correct.
The correctness is there.
The correctness for both forwards and backwards passes is there, but on NVIDIA, it's about
5x slower than PyTorch right now, like 5x, wow, this is, this is unsurmountable.
No, there's reasons it's 5x slower, and I can go through how we're going to make it
faster, and it used to be, you know, 100x slower, so, you know, we're making progress,
but there's one place where it actually is competitive, and that's Qualcomm GPUs.
So tiny grad is used to run the model in OpenPilot, like right now, it's been live in production
now for six months, and tiny grad is about 2x faster on the GPU than Qualcomm's library.
Why specifically Qualcomm?
Well, because we have Qualcomm, we use Qualcomm in the Comma devices.
Oh, I mean, like, what makes, what makes, what about Qualcomm architecture?
Oh, what makes it doable?
Yeah.
Well, Qualcomm has spent how many millions of man-hours to make NVIDIA fast, and Qualcomm
has a team of 10 Qualcomm engineers, okay, well, who can I be here?
Like, what I propose with, what I propose with tiny grad is that developer efficiency
is much higher, but even if I have 10x higher developer efficiency, I still lose on NVIDIA,
right?
You know, okay, I didn't put 100,000 man-hours into it, right?
If they put a million, like, like, that's what I'm saying, but that's what I'm saying,
we can get.
We are going to close the speed gap a lot, like, I don't support TensorFlow yet.
That's a big one that's just going to, okay, massively close the gap.
And then AMD, I can't even get, I don't even have a benchmark for AMD because I couldn't
get it compiled.
Oh, and I tried.
Oh, I tried.
I spent a day, like, I spent actually a day trying to get PyTorch, and I got it built,
I got it kind of working, and then I tried to run a model, like, there's all kinds of
weird errors, and the rabbit hole is just so deep on this, I'm like, so we, you know,
you can compare the speed, right now, you can run Lama, you can run anything you want
on AMD, it already all works, any OpenCL back-end works, and it's not terribly slow.
I mean, it's a lot faster than crashing, so it's infinitely times faster than PyTorch
on AMD.
But pretty soon, we're going to start getting close to theoretical maximums on AMD.
That's really where I'm pushing, and I want to get AMD on ML perf in a couple months,
hopefully.
Not that you bring up AMD.
Yeah, let's dive into that, because when you announced the TamiCorp fundraise, you mentioned
one of your first goals is build the framework Rhinetime and Driver for AMD, and then on
June 3rd on Twitch, you weren't as excited about AMD anymore.
Maybe let's talk a bit about that, and you compared the quality of commit messages from
the AMD kernel to the Intel work that people are doing there, what's important to know.
So when I said I want to write a framework, I didn't never intend on writing a kernel
driver.
I flirted with that idea briefly, but realistically, there's three parts to it, right?
There's the ML framework, there's the driver, and then there's the user space runtime.
I was even down to rewrite the user space runtime.
I have a GitHub repo called CUDA IO Control Sniffer, it's terribly called, but you can
actually launch a CUDA kernel without CUDA, so you don't need CUDA installed.
Just the NVIDIA open source driver and this open source repo can launch a CUDA kernel.
So rewriting the user space runtime is doable.
Rewriting the kernel driver?
You don't even have docs, I don't have any docs for the GPU, it would just be a massive
reverse engineering project.
So that is, when I saw that there, it wasn't, I wasn't complaining about it being slow,
I wasn't complaining about PyTorch not compiling, I was complaining about the thing crashing
my entire computer, it panics my kernel, and I have to wait five minutes while it reboots
because it's a server motherboard and they take five minutes to reboot.
So I was like, look, if you guys do not care enough to get me a decent kernel driver, there's
no way I'm wasting my time on this, especially when I can use Intel GPUs.
Intel GPUs have a stable kernel driver, and they have all their hardware documented.
You can go and you can find all the registered docs on Intel GPUs, so I'm like, why don't
I just use these?
Now, there's a downside to them.
Their GPU is $350.
You're like, what a deal, it's $350, you got about $350 for the performance, and if you're
paying about 400 for the PCIe slot to put it in, right, like between the power and all
the other stuff, you're like, okay, never mind, you got to use NVIDIA or AMD from that perspective.
But I sent an email to Lisa Sue, and she responded.
Oh, you can see you published that email in a Discord.
I did, I did, and she responded.
And I've had a few calls since, and what I did was like, what I tried to do, well, first
off, thank you for responding.
It shows me that if you don't care about your kernel panicking, this is just a huge waste
of my time, right?
I'll find someone who will care.
I'm not asking for your seven by seven Winograd convolution when transposed to be fast.
Like I'm not asking for that.
I'm asking literally for the basics.
To not value.
Oh, and this isn't tiny grad.
This is your demo apps.
I ran their demo apps in loops, and I got kernel panics.
I'm like, okay, there's, but no, Lisa Sue reached out, connected with a whole bunch
of different people.
They sent me a pre-release version of RockM 5.6.
They told me you can't really say which I'm like, why do you, why do you care?
But they say they're going to release it by the end of the month and it fixed the kernel
panic.
The guy managed to reproduce it with the two GPUs and the computer and yeah, sent me a
driver and it works.
So, yeah, I had, I had that experience.
And then I had another experience where I had two calls with like AMD's like communication
people and just like, I tried to explain to these people like open source culture, like
it's not open source.
If you dump the source code on a GitHub repo and then forget about it until the next release,
it's not open source.
If, you know, all your issues are from 2022, like, like, it's just no one's going to contribute
to that project.
Right.
Sure.
It's open source in a very like technical sense.
To be fair, it's better than nothing.
It's better than nothing, but I fixed a bug in Nickel that I fixed.
There's a fun fact, by the way, if you have a consumer, a consumer AMD GPU, they don't
support peer-to-peer.
And they're already spanned with this horrendously slow because it's using CUDA kernels to do
the copy between the GPUs.
And it's putting so many transactions on the PCIe bus that it's really slow, but you
can use CUDA mem copy and there's a flag to use CUDA mem copy, but that flag had a bug.
So I posted the issue on Nickel.
I expected nothing to happen.
The Nvidia guy replied to me within an hour.
He's like, try this other flag.
I'm like, okay, I tried the other flag.
It still doesn't work, but here's a clean repro.
And I spent like three hours writing a very clean repro.
I ended up tracking the issue down myself, but just the fact that somebody responded
to me within an hour and cared about fixing the issue, okay, you've shown that it's worth
my time and I will put my time in because let's make this better.
I'm here to help.
But if you show me that you're like, you're the kernel panics, let's just expect it.
Okay.
Well, it sounds like AMD is getting the message.
They are.
And I just, I don't really think they've had someone explain to them like, I was like,
you get to like build in public.
And they're like, what's an example of building in public?
I'm like, go look at PyTorch, go look at PyTorch, right?
Like, you know, I have, I have, I have two minor things merged into PyTorch because it's
very responsive, you know, like minor bug fixes, but I feel like it's, you know, yeah.
So that's kind of like the lowest level of the stack.
And then at a slightly higher level, obviously there's tiny grad, there's module, there's
GGML.
How are you thinking about breadth versus like depth and like where you decided to focus
early on?
Um, so GGML is very much like a, okay, everyone has M1s, right?
Actually, I was thinking, in the beginning I was thinking of something more like GGML,
focus on the M1s, but GGML showed up and was just like, we're actually just focusing
on the M1s.
Um, so, and actually M1 PyTorch is considerably better than AMD PyTorch.
And when PyTorch works, it only gives wrong answers sometimes and only crashes sometimes,
but like some models kind of run, um, when I was writing the metal back end, I was comparing
to MPS PyTorch and I had like a, I had a discrepancy, like TinyGrad checks all its outputs compared
to Torch and I had one where it didn't match, I'm like, I really, I checked the matrix by
hand, it matches TinyGrad, I don't understand.
And then I switched PyTorch back to CPU and it matched and I'm like, oh yeah, well, this
is like bugs, like if you like transpose the matrix because like, I think it's like has
to do with like multi views and PyTorch and like weird under the hood stuff that's not
exposed to you, like there's bugs and maybe they fix them, but like, you know, it seems
like there was a lot of momentum again, because you're getting a huge variety, you're getting
how many engineers care about making PyTorch work on M1, right, thousands, tens of thousands.
Yeah.
And you have an open development process and guess what, it's going to be good.
How many engineers care about AMD working with PyTorch AMD working?
Well, you got 10 guys that work for AMD and then like a couple hobbyists.
You revealed an interesting detail about how you debug, which is you check, you hand check
the matrix math.
No, I don't hand check it.
There's a, there's a, one of the best tests in tiny grad is a file called test ops.py
and it's just a hundred small examples written in tiny grad and PyTorch and it checks both
the forwards and backwards to make sure they match.
The test suite.
Yeah.
Very important.
That's, I mean, that's one of them where you like, I really, I put a lot of effort into
the CI for tiny grad.
I think CI is super important.
Like I want that green check to mean I can merge this.
Yeah.
Okay.
I don't want my tests too.
I don't want to manage to introduce a bug and get the green check.
Okay.
We're fixing the test top priority.
Mojo.
It's close source.
No, I'm not that interested.
Do you know what I mean?
Like, like, look, I like Chris Latner.
I think he's going to do great things.
And I understand the, the like kind of the wisdom, even in keeping a close source, but
you know, I'm interested when it's open.
Yeah.
Right.
You have an interesting design deviation from him because he's decided to be a promise
to be a superset of Python and you have decided to break with it.
And I think that's, that affects learnability and trans, transportability of code.
You know, if the PyTorch thing ends up being like a, like a stumbling block, I could write
a perfect PyTorch, like, like, like, like a, you know, instead of import PyTorch, instead
of like, yeah, import Torch, you type import tiny Torch as Torch.
And if that really becomes the stumbling block, I think it's going to be great.
If that really becomes the stumbling block, I will do that.
No.
Chris Latner went much further than PyTorch.
Replicating the PyTorch API is something I can do with a couple, you know, like an engineer
month or two.
Right.
Like a shim.
Yeah.
Replicating Python.
There's a, there's a, there's a big graveyard of those projects.
How's, how's, how's Piston going?
How's, oh, Jython, PyPy is all, you can go way back.
So tiny grad and small layer, you announced TinyBox recently, which is, you know, you
made it.
So your core mission is commoditizing the pedoflop.
And then your business goal is to sell computers for more than the cost to make, which seems
super reasonable.
What are, and you're going to have three TinyBoxes, red, green, blue.
That was my, look, you know, a lot of people, like, I love, you know, leaning into like
saying I'm giving up, right?
It's great to give up or giving up is this wonderful thing.
It's so liberating.
And then like, you can decide afterward if you really give up or not.
There's very little harm in saying you give up, except like, you know, great, Twitter
haters have something to talk about and all press is good press kids.
So, obviously, just read, only read, TinyBox, read, unless AMD, you know, upsets me again
and then we're back to, we're back to other colors, we have other colors to choose from.
When you think about hardware design, what are some of the numbers you look for?
So, Terraprop sits per second, it's one, but like memory bandwidth is another big limiter.
Like, how do you make those trade-offs?
Well, I mean, fundamentally unlimited what GPUs I can buy.
But yeah, for something that I think a lot of people are going to want to reasonably
do with a core core of mine, describe them as luxury AI computers, right?
Like luxury AI computers for people.
And that's like what we're building.
And I think a common thing people are going to want to do is run like large llama, right?
Or large, like Falcon.
FB16 llama.
FB16, exactly.
Exactly.
You know, in-date I think can work.
I think that like what GGML is doing to go to like N4, like this doesn't work.
Like have you done, maybe they have, but like I read what it was and I was like, this isn't
from any paper.
This is just some, like you're-
Squeezing as much as possible.
Yeah, you made up some quantization standard to make it run fast and like, like maybe it
works, but okay, where's like the hell's swag number, right?
Where's your, where's your, where's your, uh, you know, all your-
The thesis is right that like if you have billions, hundreds of billions of parameters
that the individual quantization doesn't actually matter that much.
Well, the real way to look at all of that is to just say you want to compress the weights,
right?
It's a form of weight compression.
Quantization is a form of weight compression right now.
This is obviously not lossless.
It's not a lossless compressor, right?
It's a lossless compressor and you can show that it's correct and okay, we don't have
to have any other conversation, but it's a lossy compressor.
And how do you know that your loss isn't actually losing the power of the model?
Maybe int465bLama is actually the same as fb167bLama, right?
We don't know.
Uh, maybe someone has done this yet, but I looked for it when it like first came out
and people were talking about it and I'm like, I just have, like it's not from a paper, right?
The in-date stuff is from a paper where they, like some of the in-date stuff is from a paper.
There's one paper, I think it's like in-llm.indate where they actually, uh, you know, do all the
tests and they didn't go fully in-date.
They made like 90% of it in-date and kept like 10% of it in fb16 for what they called
like the like outliers or whatever.
So I think that this is not quite so easy.
And I think being able, well, so first off, if you're training, no one's gotten training
to work with in-date yet.
There's a few papers that vaguely show up.
If you're training, you're going to need, uh, bf16 or float16.
Um, so this is why I target that.
Now the thing that you're going to want to do is run these large language models out
of the box on your hardware in fb16 and that's memory bandwidth.
You, you need, you need large amounts of memory bandwidth to, uh, so ask how I trade
off memory bandwidth in Flops, so what GPUs can I buy?
But, um, and I saw one of your, so first of all, you have this, um, hiring process, which
has, you got to solve one of the bounties, um, that are open on tiny grad.
There's no, uh, technical interview.
One of them is in-date support.
Do you already have some things you want to test on?
We have in-date support.
Um, what I'd like to see somebody do is just load the ggml intate llama into tiny grad
and then benchmark it against the fb16 one.
Uh, intate already works in, in tiny grad, it doesn't actually do the math in intate,
which is even a, which is even a stronger, like it does all the math still in fb32.
So intate can mean you just have your weights in intate or intate can mean you actually
do your math in intate.
And doing your math in intate, the big, like, gain that people care about is actually, uh,
having your weights in intate, because weights in intate mean less memory and less memory bandwidth.
Uh, whereas the math, keep it in fb32 with, with, with, on, on m ones, it doesn't even
matter if you're doing, it doesn't matter what data type you're doing in the, in the
, in the GPO.
I, I'm not even sure it can do intate, but fb16 and fb32 is the same, is the same taro
flops.
Um, so yeah, no, that's one of the bounties.
One of the bounties is get, get intate llama running with the intate weights.
And then actually what you could even do, if you really want to test this, just take
the fb16 weights, convert them to intate, then convert them back to fb16, then compare
the unconverted and converted.
Oh, that's a nice hack.
Oh yeah.
Right.
Like, like, like, this should be lossless in the other direction.
Well, yeah.
So, uh, yeah, I think fb16, it should be lossless in the other direction.
I'm actually not a hundred percent about that.
Why not?
Uh, oh, cause like, you ever try to like, like, if you want to represent, if it was like
int16, it's not lossless.
Sure.
I think, I think all of intate can be represented in fb16, but I'm not a hundred percent about
that.
Actually, I think it, we just draw the bytes and we just have to do it, right?
Just literally do it.
There's only 256 to check, like, um, but yeah, either way, or, I mean, into four, definitely.
So do your in four, convert it back.
And now see, even with in four weights and fb32 math, like, okay, how much does your
performance to grade of this model?
Yeah.
So, so can we, uh, I'm about to zoom out a little bit from the details.
I don't know if you, you had more, no, I think like the, you're playing to release
the first tiny box ship them in like two to six, eight months, something like that.
Uh, what's up with mine for you in terms of building a team who should, who are you calling
for?
Yeah.
Uh, well, to, to stay on the tiny box for, for, for, for, yeah, exactly.
Um, so at the GPUs picked out and you're like, well, I could make that computer with
the GPUs.
My answer is, can you, do you know how to put, do you know how hard it is to put six
GPUs on a computer?
People think it's really easy and it's really easy to put one GPU in a computer.
It's really easy to put two GPUs in a computer, but now you want to put in eight.
Okay.
So I'll tell you a few things about these GPUs.
They take up four slots.
What kind of computer?
You can buy the nicest super micro.
You can't put eight of those in there.
You need two slot blowers.
If you want to use one of those, those for your super micros, you need two slot blowers
or water cooling.
All right, if, if you're trying to get the four slot cards in there, you're going to
need some form of water cooling, uh, or you're going to need, there are some like Chinese
40 nineties that are blowers, right?
You have any blowers or water cooling if you're trying to get it in those things.
Right.
Um, so you, are you doing water?
No, I'm not using that chassis.
Okay.
Um, then the other thing that, okay, so now you want to get six GPUs in a computer.
So that's a big challenge.
You're like, oh, I'll just use a PCIe extenders.
I saw it online as tech tips.
It works great.
No, it doesn't.
It's PCIe extenders that work at PCIe 4.0 and interconnect bandwidth.
Super important.
Yes.
They don't work at 3.0.
No PCIe extender I've tested and I've bought 20 of them, uh, works at PCIe 4.0.
So you're going to need PCIe redrivers now.
Okay.
How much does that add in cost?
Right.
Like these things all get really hard.
And then tiny boxes.
I've even had another constraint to it.
I want this thing to be silent, not totally silent, but my limit is like 45, maybe 50
dB, but not super micro machine.
60 dB.
We have a small, we have a compute cluster, a comma, you got to wear your protection
to go in there.
I like it.
Yeah.
I've seen some videos where you give a tour.
Yeah.
Yeah.
It's noisy.
It's super loud.
Yeah.
10,000 RPM.
Just screaming.
Like I want to be able to use the normal big GPU fans and make this thing so it can
sit under your desk, plug into one outlet of power, right?
This GPUs, your GPUs are 350 Watts each, can't plug that into a wall outlet.
Okay.
So how are you going to deal with that?
Good questions, right?
And you're not sharing them.
Well, that one, I mean, that one is pretty obvious.
You have to limit the power on the GPUs, right?
You have to limit the power on the GPUs.
Now you can limit power on GPUs and still get, you can use like half the power and get
80% of the performance.
This is a known fact about GPUs, but like that's one of my design constraints.
So when you start to add all these design constraints, good luck building a tiny box
yourself.
You know, obviously it can be done, but you need something that has actually quite a bit
of scaling resources to do it.
And you see like the under the desk, it's like one of the main use cases, kind of like
individual developer use or.
Yeah.
What I also see is more of a like an AI hub for your home, right?
As we start to get like home robotics kind of stuff, you don't want to put the inference
on the robot.
But you also don't want to put the inference on the cloud.
You don't want to put it on the robot because, okay, it's 1500 Watts, tiny box, you put batters
and charge them.
Bad idea.
And just, just, just wireless, wireless is 0.5 milliseconds.
Yeah.
This is super fast.
You don't want to go to the cloud for two reasons.
One, cloud's far away.
Okay.
It's not that far away.
You can kind of address this, but two, cloud's also mad expensive, like cloud GPUs are way
more expensive than running that GPU at your house.
At least any rates you're going to get, right?
Maybe if you commit to buy, well, yeah, I'm going to buy 10,000 GPUs for three years,
then maybe the cloud will give you a good rate.
But like, you want to buy, you want to buy one GPU in the cloud?
Ooh.
I mean, okay, you can go to like Vast, but like if you're going to Azure, AWS, so that's
expensive.
Yeah.
This is like a, like a personal data center, you know, instead of a cloud data center.
We like the term compute cluster, so we can use NVIDIA GPUs.
Yeah.
Data centers may be a little bit dated.
It's a compute cluster, which is totally legal under the CUDA license agreement.
You talk a lot about the PCIe connection.
Do you think there's any fat there to the trim?
What do you mean?
Just you're limited by bandwidth, right?
Okay.
For some things, yes.
So the bandwidth is roughly 10x less than what you can get with NV linked A 100s.
Yeah.
NV linked A 100s are going to have, and then you can even get like full fabric and the
NVIDIA really pushes on that stuff, 600 gigabytes per second, right?
And PCIe four, you're going to get 60.
All right.
So you're getting 10x less.
Yeah.
Um, that said, why do you need the bandwidth, right?
And the answer is you need it for training huge models.
If you're training on a tiny box, your limit's going to be about 7 billion, right?
If you're, if you're training on big stuff, your limits could be like 70 billion, right?
Okay.
You can hack it to get a bit higher.
You can hack it like GBT hacked it to get a bit higher, but like that's 65 billion in
llama.
Like there's a reason they chose 65 billion, right?
And that's what can reasonably fit model parallel on, on, on a GPUs.
Right.
So, um, yes, you, you are going to end up training models.
The cap's going to be like 7 billion, but I actually heard this on your podcast.
I don't think that the best chatbot models are going to be the big ones.
I think the best chatbot models are going to be the ones where you had a thousand training
runs instead of one.
And I don't think that the interconnect bandwidth is going to matter that much.
So what are we optimizing for instead of compute optimal?
Uh, what do you mean compute optimal?
So the, this, you're talking about this, um, the llama style models where you train
for like 200,
You train longer.
Yeah.
Yeah.
Yeah.
So, okay.
You can always make your model better by doing one of two things.
Right.
And a comma, we just have a strict limit on it.
Um, you can always make your model better by training longer and you can always make
your model better by making it bigger.
But these aren't the interesting ones, right?
Particularly the making it bigger because training it longer, fine, you know, you're
getting a better set of weights.
The inference is the same.
The inference is the same, whether I trained it for a day or a week, but the, okay.
If it's one billion versus 10 billion, well, I 10x my inference too.
All right.
So I think that these big models are kind of a, sure they're great if you're research
labs and you're trying to like max out this thing, which you can talk about later.
Yeah.
Yeah.
Yeah.
But if you're, but if you're like a startup or you're like an individual or you're trying
to deploy this to the edge anywhere, you don't, you don't need that many weights.
Yeah.
Yeah.
You don't want them anyway.
Optimizing for inference rather than capabilities.
Yes.
Doing benchmarks.
Yes.
Yes.
Um, and I think the, the inference thing, right?
There should be so much more, right now the ratio between like training and inference
on clouds, I think it's only still like, it's like two or three acts, right?
It's two or three acts more inference, which doesn't make any sense.
Like there should be way more inference.
Yeah.
There should be a 10 to a hundred X more inference in the world than, than training.
Um, but then also like what is training, right?
You start to see these things like Laura, like, you're getting kind of, it's kind of
blurring the lines between inference and training.
And I think that that blurred line is actually really good.
I'd like to see much more like on device training or on device fine tuning of the final
layer.
Yeah.
Um, we're, we're pushing toward this stuff at comma.
Right.
Like why am I shipping a fixed model?
I totally want this model to fine tune based on like how, you know, your left tire is flat.
Right.
Like every time you cut the same turn because your left tire is flat.
Well, it should learn that.
Right.
So would comma pursue perimeter efficient fine tuning?
Yeah.
Yeah.
Where, where, where, where seems like a, we're looking at the stuff like that.
I mean, comma is already very parameter efficient because we have to like run this thing in
a car and you have to like cool it and power it.
Yeah.
Yeah.
And so this kind of like intelligence cluster you have in your home, you see when the person
is using third party model, they load them locally and kind of do the final fine tuning.
It kind of stays within the box.
Yeah.
I think that that's one thing.
That's one version of it for the privacy conscious.
Um, I also see a world where, uh, you can have your tiny box in its down cycles, um,
mine flop coin.
Right.
You know, not all, turns out not all crypto is a scam.
There's one way to tell if crypto is a scam.
If they're selling the coin before they make the product, it's a scam.
If they have the product and then they sell the coin, it's maybe not a scam.
Right.
So yeah, my thought is like each tiny box would let you, would have a private key on
it.
Uh, and you have to do it this way.
You can't just let anyone join because of civil attacks.
Right.
There's a real problem of like, how do I, uh, how do I ensure your data is correct?
And the way that I ensure your data is correct on the tiny net is if you ever send wrong
data, you're banned from the life.
Yeah.
You're, you're a $15,000 hardware box is banned.
So you know, don't cheat.
Um, obviously if it messes up, we'll forgive you.
But, um, I'm saying like some is going to try to jailbreak your devices.
There's no jailbreak.
There's no jailbreak.
There's just a different network.
Well, there's just a private key on each device.
Right.
Like if you buy a tiny box from the tiny corp, I give you a private key.
It's in my backend server.
Right.
You want to hack my server.
That's illegal.
Yeah.
Anything you want to do on the device, the device is yours.
My server's mine.
Right.
Like.
Yeah.
Yeah.
Uh, have you looked into like, uh, federated training at all?
Yeah.
So I mean, okay.
You're now, there's, okay.
There's a lot of magnitude of federated training.
I mean, like, uh, over the cloud and stuff, over the internet, over the internet, but
also distributed on a bunch of devices.
Right.
Yeah.
I'm, I'm, I'm very bearish on this stuff because you're an interconnect bandwidth.
Right.
So, okay.
At the high end, you have your interconnect bandwidth of envy link, which is 600 gigabytes
per second.
Right.
The tiny box has 60 gigabytes per second and then your internet has 125 megabytes per
second.
Right.
Not gigabits.
125 megabytes.
Right.
So, okay.
That's, that's how, that's how many orders of magnitude we're talking here.
Like from 60 down to 125, like, all right.
That's over a hundred.
There's over a hundred X.
That's 400 X.
Right.
So like, no.
Uh, but what you can do is inference.
Right.
Like there's, for inference, you don't care.
Right.
For inference, I, I, there's so little bandwidth at the top and the bottom of the model, um,
that like, yeah, you can do federated inference.
Right.
And that's kind of what I'm talking about.
Um, there's also interesting things to push into like, you're like, but okay, what if
you want to run close source models?
This stuff gets kind of interesting, like using TPMs on the boxes and stuff.
Um, yeah.
But then someone might jailbreak my device.
So, you know, maybe we don't try to do that.
Yeah.
What's like the enterprise use case?
Do you see companies buying a bunch of these and like stacking them together?
Um, so the tiny box is like the first version of what we're building, but what I really
want to do is be on the absolute edge of flops per dollar and flops per lot.
These are the two numbers that matter.
Uh, so the enterprise use case is you want to train like, like comma, right?
So comma just built out a new compute cluster.
It's about, uh, it's about a person and a half.
Uh, so, you know, it's decent size person, a person being 20 person is a person is 20
paid flops.
It's about 30 paid flops.
Um, we built out a little, uh, little compute cluster and you know, we paid double what
you theoretically could per flop, right?
You theoretically could pay half per flop if you designed a bunch of custom stuff.
And yeah, I mean, I could see that being, you know, tiny core when comma is going to
be the first customer.
I'm going to build a box for comma and then I'm going to show off the box I built for
comma and be like, okay, like, do you want to build, I sell $250,000 training computers
or how much does one H 100 box?
Uh, it's, uh, it's four under grand.
Okay.
I'll build you a 400 grand training computer and it'll be 10x better than that H 100 box
for again, not for every use case for some, you need the interconnect bandwidth, but for
90% of most companies model training use cases, the tiny box will be five X faster for the
same price.
You mentioned the person of compute.
How do we build a human for $20 million?
Oh, it's a lot cheaper now.
It's a lot cheaper now.
Uh, so like I said, we comma, comma spent about, uh, about half a million on our, on
our person and a half.
So, you know, what are some of the numbers people should think of when they compare compute
to like people.
So GBD four was a hundred person years of training.
That's more like on, on the time scale, um, 20 petaflops is one person.
I think you, um, right now the math was that for the price of the most expensive thing
we build, which is the international space station, we could build, uh, one Tampa of
one Tampa.
Yeah, yeah.
One Tampa of compute.
Yeah.
Which is 400,000 people.
Of measurement.
Um, yeah.
Yeah.
We could build.
So like the biggest training clusters today, I know less about how GBD four was trained.
I know some rough numbers on the weights and stuff, but, uh, llama trillion parameters.
Well, okay.
So GBD four is 220 billion in each head and then it's an eight way mixture model.
So mixture models are what you do when you're out of ideas.
Um, so, you know, it's a, it's a mixture model.
Uh, they just train the same model eight times and they have some little trick.
They actually do 16 inferences, but, uh, no, it's not like, so the multi modality is just
a vision model kind of glommed on.
I mean the multi modality is like obvious what it is too.
You just put the vision model in the same token space as your language model.
Oh, did people think it was something else?
No, the mixture has nothing to do with the vision or language aspect of it.
It just has to do with, well, okay, we can't really make models bigger than 220 billion
parameters.
Uh, we want it to be better.
Well, how can we make it better?
Well, we can train it longer and okay, we've actually already maxed that out, uh, getting
diminishing returns there.
Okay.
Make sure of experts.
Yeah, make sure of experts.
We'll train eight of them.
Right.
So, you know, you know, the real truth is whenever a start, whenever a company is secretive,
with the exception of Apple, Apple's the only exception, whenever a company is secretive,
it's because they're hiding something that's not that cool.
People have this wrong idea over and over again that they think they're hiding it because
it's really cool.
It must be amazing.
It's a trillion parameters.
No, it's a little bigger than GPT-3 and they did an eight-way mixture of experts.
Like, all right, dude, anyone can spend eight times the money and get that, um, but yeah,
so, uh, coming back to what I think is actually going to happen is, yeah, people are going
to train smaller models for longer and fine tune them and find all these tricks.
Right.
You know, I think, uh, opening, I used to publish stuff on this, you know, uh, when
they would publish stuff, uh, about how much better the training has gotten given the same
of holding compute constant and it's gotten a lot better, right, than compare, like, batch
norm to no batch norm.
Yeah.
And now we have like-
Is there a finding algorithms like flash attention?
Yeah, well, flash attention, yeah.
Yeah.
Um, my flash attention is the same compute.
A flash attention is an interesting fact where it's actually the identical compute.
It's just a more efficient way to do the compute.
But I'm even talking about, like, like, um, look at the new, look at the new, uh, embeddings
people are using.
Right.
They used to use these like boring old embeddings.
Now like Lama uses that complex one and that was like alibi.
I'm not up to date on all the latest stuff, but, uh, those tricks give you so much.
There's been a whole round trip with positional embeddings.
I don't know if you've, uh, seen this discussion.
I haven't followed-
Like you need them, you need rotational and then you don't need them.
I haven't followed exactly.
I mean, you quickly run into the obvious problem with positional embeddings, which
is you have to invalidate your KV cache if you run off the context.
So that's why I think these new ones that play with them, but, uh, I'm not that, I'm
not that, I'm not an expert on like the latest up-to-date language model stuff.
Yeah.
Um, I mean, we have what we do at comma, I don't know how that works, but like, um, what
are some of the things, I mean, that people are getting wrong.
So back to autonomous driving, there was like the whole like LiDAR versus vision thing.
You know, it's like, people don't get into accidents because they cannot see well, they
get into accidents because they got distracted and all these things.
What are, do you see similarities today on like the pathway GI?
Like are there people, like what are like the-
Nothing, nothing I say about this is ever going to compete with how Rich Sutton stated
it.
Rich Sutton is writer-
The bitter lesson.
The first millennium, the bitter lesson.
Nothing I say is ever going to compete with.
The bitter lesson is way better than any way I'm going to phrase this.
Just go read that and then like, I'm sorry, it's bitter, but you actually just have to
believe it.
Like over and over again, people make this mistake.
They're like, oh, we're going to hand it to you or this thing, we're going to hand-
No, like stop wasting time.
Which is, I mean, OpenAI is not taking the bitter lesson.
No, OpenAI-
They were, they were leaders in deep learning for a long, long, long time, but you're telling
me that GPT-4 is not.
Well, OpenAI was the absolute leader to the thesis that computers all you need, right?
And there's a question of how long this thesis is going to continue for.
It's a cool thesis and look, I think I would be lying along with everybody else.
I was into language models like way back in the day for the HotterPrice.
I got into AI through the HotterPrice.
Like 2014, I'm trying to build compressive models of Wikipedia and I'm like, okay, why
is this so hard?
Like what this is is a language model, right?
And I'm playing with these like, like Bayesian things and I'm just like, oh, but like I get
it.
Like it needs to be like, like it's like, I have two data points and they're like almost
the same.
And so I measure that almost, right?
I just like, you know, wrap my head around, I couldn't like, like wrap my head around
this.
And this was around the time Carpathia released the first like RNN that generated the Shakespeare
stuff.
And I'm like, okay, I get it, right?
It's neural networks that are compressors.
Now this isn't actually, you can't actually win the HotterPrice with these things because
HotterPrice is MDL.
It's the model, size of the model plus the size of the encodings, embeddings.
So yeah, you can't, I mean, probably now you can because it's gotten so good.
But yeah, back in the day, you kind of couldn't.
So I was like, okay, cool.
Like this is what it is.
I kind of get it.
Yeah.
I mean, I think I didn't expect that it would continue to work this well.
I thought there'd be real limits to how good autocomplete could get.
That's fancy autocomplete.
But yeah, no, like it works.
It works well.
So like, yeah, what is open AI getting wrong?
Technically not that much.
I don't know.
Like if I was a researcher, why would I go work there?
Yes.
So why is opening AI like the Miami Heat?
No, look, I don't know.
This is my technical stuff.
I don't really want to harp on this, but like, why go work at opening AI when you can go
work at Facebook?
Right.
As a researcher.
Like opening AI can keep ideologs who, you know, believe ideological stuff and Facebook
can keep every researcher who's like, dude, I just want to build AI and publish it.
Yeah.
Yeah.
Awesome.
Yeah.
Any other thoughts?
Corp, bounties?
Um, yeah, so we have, you know, I've been thinking a lot about like what it means to
hire in today's world.
What actually is the like core?
Okay.
Look, I'm a believer that machines are going to replace everything in about 20 years.
So okay.
What is that?
What is that thing that people can still do that computers can't, right?
This is a narrowing list, but like, you know, back in the day, like, imagine I was starting
your company in 1960.
Right?
Oh, we're going to have to hire a whole bunch of calculators in the basement to do all that,
you know, math to support the cabinet.
Dude, have you heard about computers?
Why don't we just buy a few of those?
Oh, oh, wow, man.
You're right.
Um, so like, I feel like that's kind of happening again.
I'm thinking about, I will post in my discord.
I'll be like, who wants to like, okay, I just changed my unary op.
There used to be log and X in like E. I changed them to be log to an X to because hardware
has log to an X to accelerators.
Yeah.
And of course you can use change of base.
It's one multiply to get it back to E. But like I made the primitives log to an X to right.
And this is the kind of, I just posted in the discord.
I'm like, could someone put this pull request up and someone eventually did and I merged
it.
But I'm like, this is almost to the level where models can do it.
Right?
We're almost to the point where I can say that to a model and the model can do it.
Have you tried?
Yeah.
I'm, I don't know.
I'm like, I'm, I think it went further.
I think autocomplete went further than I thought it would, but I'm also relatively unimpressed
with these chatbots, uh, with what I've seen from the language models like there.
The problem is if your loss function is categorical cross entropy on the internet, your responses
will always be mid.
Yes.
I don't know.
Mode collapse is what I call it.
I don't know.
I'm not even talking about mode collapse.
You're actually trying to predict these.
Like, like, look, I rap.
I'm a, I'm a hobbyist rapper and like, would I try to get these things to write rap?
The rap sound like the kind of raps you read in the YouTube comments.
Nursery school.
Yeah.
It's like, all right, great.
You're riding box with Fox.
Sick rhyme, bro.
Uh, you know, uh, you know, and rock or Drake is rhyming, give it up for me with napkins
and cutlery.
Right?
Like, like, all right, come on.
What's that?
Like this thing about orange.
Like orange is famous.
Yeah.
And now, of course, you know, four inch screws and orange juice is in, is in training corp.
But uh, yeah, so I think it went further than like everyone kind of thought it would.
But the thing that I really want to see is like somebody put 10 LLMs in a room and have
them discuss the answer before they give it to me.
Right?
You can actually do this, right?
Um, and I think the coding things have to be the same way.
There is no coder alive, no matter how good you are that sits down.
Well, I'm going to start at cell A one and type my program and then I'm going to press
run and there's going to work.
No one programs like that.
So why do we expect the models to write?
So, so there's a lot that like still needs to be done.
But you know, at the tiny corp, I want to be on the cutting edge of this too.
I want to be like program generation.
I mean, what is tiny grad?
It's a compiler generates programs generate the fastest program that meets the spec.
Right?
Why am I not just having a male do that?
So you know, it's kind of a, you have to exist fluidly with the machines and I come
around on a lot of stuff.
And I'm like, wait, tiny grad, tiny group should be a remote company.
Right?
Well, I can't do this in person.
Really?
Yeah.
Like, like comma makes sense to be in person like comma, sure.
Yeah.
We're getting off in San Diego.
Like, but that's a six year old company.
Right.
And it works and it works for a certain type of people and certain type of culture, but
what's going to be different this time.
Okay.
Remote, but now it's remote.
And now I'm getting these like people who apply and I'm like, I literally have a thousand
applications.
I'm not calling you to do a technical screen and I can't really tell anything from a
technical screen.
What am I going to do?
Make a code on a whiteboard?
Bring up, bring up a shared notebook document so we could, oh, like that's not going to
work.
Okay.
So then I'm moved to the next thing.
We do this a comma with good success programming challenges.
I've also found them to be like completely non predictive.
I found one thing to actually be predictive and it's, wait a second, just write code in
tiny grad.
It's open source.
Right.
And yeah.
So, you know, I'm talking to a few people who've been contributing and like contribute
or you know, the job's not for you, but you can do it remote and it's like it's a
chill job.
Like you're not, you're like, oh yeah, well, I work for the tiny corp.
Like, well, you're writing MIT license software.
Like you see what it's doing.
Right.
Like we'll just, I think it's maybe more of like a stipend than a salary and then also
some equity.
Like, you know, I get rich.
We all get rich.
Yeah.
Yeah.
How do you think about agents and kind of like thinking of them as people versus like
job to be done?
Sean, build this thing called small developer and then it's in the same vein like the human
in the loop with the language model and just iterating while you write code.
I think, I think that's, that's absolutely where it goes.
And there's like a, it's not like one thing.
It's like, there's small interpreter.
There's like small debugger.
It's kind of like all these different jobs to be done.
It's a small world.
Yeah.
It's a, I know this is like the small pockets.
It's like small AI meet tiny corp.
So we're all in the same wavelength.
How do you think about that?
Do you think people will have a human like interaction with like, oh, this is like the
AI developer or like, is it, I'm the human being supercharged by the AI tools?
Oh, I think it's much more like I'm the human supercharged by the AI tools.
I think that like coding is tool complete.
Right.
Like driving is not tool complete.
Right.
Like driving is just like, like we hire people to drive who are like below the API line.
Right.
There's an API line in the world.
Right.
Love that.
Yeah.
There's an API line in the world and like you can think like Uber's a really clear
example.
Right.
There's the people below the API line and the people above the API line.
The way you can tell if you're below or above, by the way, is, is your manager a computer?
Right.
Who's the manager of the Uber driver?
Well, computer.
There's a machine tell you what to do.
Do you tell machines what to do?
Exactly.
Exactly.
Um, so.
Coding is tool complete.
Right.
Coding is tool complete.
Coding is above the API line.
So it will always be, uh, tools supercharging your coding workflow and it will never be
you performing some like task like, okay, well, I can do everything except for actually starting
a docker container.
Like it just doesn't make any sense.
Right.
Um, yeah.
So it will always be sort of tools.
And you know, look, we see the same stuff with all the, like people are like stable diffusion
is going to replace artists or whatever.
It's like, dude, like it's going to create new artists, did Photoshop replace artists?
Like, what are you talking about?
Right.
Like, you know, real artists, finger paint, they can't use brushes, brushes are, you know,
brushes are going to replace all the, okay, like, I just can't like it's all just tools
and the tools are going to get better and better and better.
Eventually.
Yes.
The tools are going to replace us.
But you know, that's still 20 years away.
So now I got a company in the meantime.
So I've written about the API line before and I think that's from Venkatesh.
I don't know if you've definitely took it from someone.
It's definitely not mine.
VGR.
Yeah.
But I also have a speculated a higher line than that, which is the Kanban board, like
who tells the, the programmers what to do.
Hmm.
Right.
So are you above or below the Kanban board at this has that evolved your management
thinking?
Yeah.
Like that's sort of what I mean.
Like I'm just going to describe the pull request in two sentences and then like, yeah,
yeah.
So you are running the Kanban board or the bounties?
Yes.
Yeah.
The bounties of the Kanban board.
Yes.
Exactly.
And that is kind of the high level.
And then like, yeah, we'll get AIs to fill in some and we'll get people to fill in others.
And that's also what it means to be like full time, a tiny corp.
Right.
Would you start, and I wrote this up pretty concretely, I'm like, okay, step one is you
do bounties for the company.
Step two is you propose bounties for the company.
Right.
You don't obviously pay them.
We pay them.
And I'm like, yeah, that's a good bounty that like helps with the main workflow of the company
and step three is you get hired full time, you get equity, we all maybe get rich.
What else are you designing differently about the employee experience?
I mean, I'm very much alike, you know, some people really like to like, like keep a separation.
Right.
Some people really like to keep a separation between like employees and management or customers
and employees.
Like a comma, you know, the reason I do the dev kit thing, it's like, dude, you buy a
common thing, you're an employee of the company, like you're just part of the company.
It's all the same thing.
There's no like secrets, there's no dividing lines, there's no like, it's all a spectrum
for like, you know, down here at the spectrum, like you pay and then up here at the spectrum,
you get paid.
You understand this is the same spectrum of college, right?
Like for undergrad, you pay and then you get up here to like, you know, doing a PhD program,
you get paid.
Okay.
Well, cool.
Welcome to the, you know.
What about a comma bodies?
You know, you mentioned a lot of this stuff is clearly virtual, but then there's below
the API line you actually need.
This is the thing that's been announced?
Comma bodies?
We sell them.
Oh, okay.
You can buy them.
There's a thousand bucks on our website.
Oh, okay.
No, no, no.
I'm thinking about like the, what Tesla announced with like the humanoid robot.
It's the same thing.
Yeah.
Except of course we made the comma version of it.
Tesla uses 20 actuators, we use two, right?
Like, how do you, how do you build the simplest possible thing that can like turn the robotics
problem into entirely a software problem?
So right now it is literally just a comma three on a pole with two wheels.
It balances, keeps the comma three up there and like there's so much you could do with
that already.
Right?
Like this should replace, how many security guards could this replace?
Right?
If this thing could just competently wander around a space and take pictures and, you
know, focus in on things, send you a text message when someone's trying to break it
to your building, you know, like, like this could already do so much, of course, but
the software is not there yet.
Right?
So how do we turn robotics into a thing where it's very clearly a software problem?
You know, that people don't accept that self-driving cars are a software problem.
I'm like, I don't know what to tell you, man.
Like literally just watch the video yourself and then drive with a joystick.
Right?
Yeah.
Can you drive?
We've actually done this test.
We've actually done this test where we've had someone, okay, you just watch this video
and here's a joystick and you got to drive the car and of course they can drive the car.
Yeah.
It takes a little bit of practice to get used to the joystick, but the problem is all
the model.
Yeah.
It is specifically anything in computer vision that you think our second most popular episode
ever was about segment anything coming out of Facebook, which is as far as I understand
the state of the art in computer vision, what are you hoping for there that you need for
a comma?
I haven't used segment anything.
I mean, like the large yolos or not, I've used like large yolos and I'm super impressed
by them.
Yeah.
You think it's solved?
I got to check out segment anything.
I don't think it's a distinct problem, right?
Okay.
Here's something that I'm interested in.
All right.
We have great LLMs.
We have great text-to-speech models and we have great speech-to-text models.
Okay.
So why can I not talk to an LLM?
Like at how a normal conversation with them?
You can with the latency of like two seconds every time.
Right.
Why isn't this?
And then it feels so unnatural.
It's this like staccato, like I don't like the RLHF models.
I don't like the two inversions of them.
I think that they become, you take on the personality of a customer support agent.
Right.
Like, oh, come on.
I like LLAMA more than Chatchapiti.
Chatchapiti's personality just graded on me.
Was LLAMA like cool?
I write a little bit of pretext paragraph.
I can put you in any scenario I want, right?
Like that's interesting to me.
I don't want some like, you know, yeah.
So yeah, I think there is really no like distinction between computer vision and language and any
of this stuff.
It's all eventually going to be fused into one massive.
So to say computer vision is solved, well, it doesn't make any sense because what's the
output of computer vision model segmentation, like what a weird task, right?
Who cares?
OCR.
Who cares?
I don't care if you can segment which pixels make up that laptop, I care if you can pick
it up.
Yeah.
Interactive real world.
And you're going to have the local cluster.
You're going to have the body.
Yeah.
Yeah.
I think that's kind of where that goes.
So maybe we can paint the future of like, the year is 2050.
You've achieved all you wanted at TinyCorp.
What is the AI enabled future like?
Well, TinyCorp is the second company.
Comma was the first.
Comma builds the hardware infrastructure.
TinyCorp builds the software infrastructure.
The third company is the first one that's going to build a real product and that product
is AI Girlfriend.
No, like I'm dead serious, right?
Like this is the dream product, right?
This is the absolute dream product.
Girlfriend is just the like.
Stand-in.
Well, no, it's not a stand-in.
No, no, no.
I actually mean it, right?
So I've been wanting to merge with a machine ever since I was like, mad little, like, you
know, how do I merge with a machine, right?
And like you can look at like in like a maybe the Elon style way of thinking about his neural
link, right?
I'm like, I don't think we need any of this, right?
Some of your friends, maybe they get into relationships and you start thinking of, you
know, them and their partner as the same person.
You start thinking of them as like one person.
I mean, they are kind of like merged, right?
Like humans can just kind of do this.
It's so cool.
It's this ability that we already have.
It's all I need to put, you know, electrodes in my brain to merge with a machine.
I need an AI girlfriend, right?
So that's what I mean.
Like this is the third product.
This is the third company.
And yeah, in 2050, I mean like, ah, it's so hard.
Like maybe I can imagine like 2035, I don't even know 2050, like, yeah, 2035, like, yeah,
that'd be really great.
Like I have this like kind of, you know.
So in terms of merging, like, isn't it, shouldn't you work on brain upload rather than AI girlfriend?
But I don't need brain upload, right?
I don't need brain upload either.
Like there's, there's thousands of hours of me on YouTube, right?
Yes.
If you might, how much of my brain's already uploaded?
That's only the stuff that you voice.
Yeah, it's not that different.
It's not that different, right?
You really think a powerful, you really think a, a model with, you know, an exoflop of compute
couldn't extract everything that's really going on in my brain.
I'm a pretty open person, right?
Like I'm not running a complex filter.
Humans can't run that complex of a filter.
Yeah.
Like humans just can't.
Like this is actually a cool quirk of, of, of biology.
It's like, well, humans like can't lie that well.
Yeah.
Yeah.
So is it good or bad to put all of your stream of consciousness out there?
I mean, I think it's good.
Yeah.
I mean, I don't know.
I don't know.
I don't know.
I don't live forever.
Yeah.
We said off, off Mike, we may be the first immortals, right?
Yeah.
Yeah.
Like this is how you, this is how you live forever.
It's a question of, okay, how many weights do I have?
Right?
Okay.
Let's say I have a trillion weights.
So it's talking about terabytes, a hundred terabytes here.
Like if it's not really a hundred terabytes, right?
Because it's called homograph complexity.
How much redundancy is there in those weights?
So like maximally compressed, how big is the weight file for my brain quantize it whatever
you want.
And quantization is, is a poor man's compression.
Um, I think we're only talking really here about like maybe a couple gigabytes, right?
And then if you have like a couple gigabytes of true information of yourself up there, cool
man, like what does it mean for me to live forever?
Like that's me.
Yeah.
No, I think that's good.
And I think like the, there's a bit of like a professionalization of social media or like
a lot of people only have what's like PC out there, you know, and I feel like you're gonna
get, come back to the chat GPT thing, right?
You're gonna train a model and like everything that's public about a lot of people.
And it's like, no one's gonna run their model and they're gonna die on social media.
Your life could depend on it.
We have a segment, uh, so, uh, we're, we're moving on to a, what, what would normally
be called the lightning round, but just, uh, just general takes because you're a generally
interesting person with many other interests.
Um, uh, what does the goddess of everything else mean to you?
Oh, it means that AI is not really going to kill us.
Really?
Of course.
Tell us more.
Look, uh, Lex asked me this, like, is they are going to kill us all?
And I was quick to say yes, but I don't actually really believe it.
I think there's a decent chance that AI, I think there's a decent chance that AI kills
95% of us.
Okay.
But they saw on your Twitch streams that you're with them.
So they're not gonna, no, I don't think I actually, I don't know.
I also think it's AI.
Like I think the AI alignment problem is so misstated.
I think it's actually not a question of whether the computer is aligned with the company who
owns the computer.
It's a question of whether that company is aligned with you or that government's aligned
with you.
And the answer is no.
And that's how you end up dead.
But, um, so what, what the goddess of everything else means to me is like, the complexity will
continue.
Paper clippers don't exist.
You know, there are forces, the paper clippers cancer, right?
The paper clippers really just a perfect form of cancer and the goddess of everything
else says, yeah, if a cancer doesn't win, you know, yeah, it's a beautiful story for
those who haven't heard it.
And you read it out and I listen to it.
Um, yeah.
Good.
What else we have here?
Pick a question.
So many.
Yeah.
What are you grateful for today?
Oh, man.
I mean, it's all just like, I haven't, I haven't taken it about this stuff forever.
Like that it's actually like happening and it's happening in an accessible way too.
I guess that's what I'm really grateful for.
It's not like, like AI is not some Manhattan project style.
You don't know anything about it.
Close doors.
Close doors.
I'll fight really hard to keep it that way.
Uh, you know, uh, that's, that's a, I'm grateful for just, just how much is released
out there and how much I can just learn and stay up to date.
And I guess I'm grateful to the true fabric of reality that, you know, I didn't need differential
equations to understand it.
Like I don't need, you don't need, you don't need some like, like, like there's, there's
I've tried to do.
There's a limit to my, to my math ability is I can do most undergrad math, but I took
some grad math glasses and okay, now we're getting to the end of what I can do.
And it's just the actual like end of what I can do, like I'm limited by my brain, but
you know, ML stuff, you need high school math.
Yeah.
Like I could do all that.
Nothing like, you know what I mean?
When I learned to multiply a matrix, seventh grade, like it's all easy.
You need more electrical engineering than you need high school math early.
Yeah.
Well, you need electrical engineering to like build the machines, but even that, like
these machines are simpler than the machines that have existed before, the compute stack
looks really nice.
So, you know, yeah, I just, I'm grateful that it's all happening and I get to understand
it be here.
Yeah.
Yeah.
Um, John Carmack mentioned there's about six insights we have left.
Do you have an intuition for what some of the paths people should be taking?
Obviously you're working on one.
What are some of the other branches of the tree that people should go under?
I don't think I'm working on one of the six insights.
I don't think tiny grads any one of the six insights.
Um, something I really like that Elon does and I try to take it from, uh, try to be inspired
by it is, um, look at the boring tunnel machine and ask how you can build a 10x cheaper one.
All right.
Look at the rocket.
How can I build a 10x cheaper one?
All right.
Look at the electric car and say, how can I build a 10x cheaper?
Like, cheaper or, you know, can go further or whatever, whatever, whatever, right?
You just do the straight up physics math, right?
Like, I'm trying to do the same thing with, with, uh, ML frameworks, right?
And in, in, in doing so, making sure that this stuff remains accessible, right?
You could imagine a world where if Google TPUs were actually the ultimate, if Google
TPUs were actually the best training things, I mean, actually, you know, I'm kind of grateful
for NVIDIA, right?
Like, because if Google TPUs were the ultimate, now you have this huge closed source compiler
in between XLA and, and the hardware and yeah, that's, uh, just a really bad thing.
So I mean, something that is somewhat upsetting, but the tiny group is it, is that it is trying
to prevent downside, but, uh, it's not all trying to prevent downside.
Like we're also building computers and we're going to build some awesome, powerful, cheap
computers, uh, along the way.
Uh, so no, I'm not really working directly on any of the six tricks.
I also think the six tricks are kind of going to be like, luck, I think it's going to be
like, you know, please tell me more about what covariate shift is and how that inspired
you to come up with batch normalization.
Please tell me more about why it's a transformer and it has a query, a key and a value, right?
Like Schmidt-Huber described it better and fast weights, you know?
Like, like, I mean, my theory about why transformers work have nothing to do with this attention
mechanism and just the fact that like it's semi-weight sharing, right?
Because the weight matrix is being generated on the fly, you can, you can like compress
the weight matrix.
Right?
Like this is what that, there's, there's an operation in the, in the transformer, which,
uh, like, and by the way, this is like Qualcomm's S and PE can't run transformers for this reason.
So most matrix multiplies in neural networks are weights times values, right?
There is, um, you know, when you get to the, the, the outer product in, in, uh, transformers,
well, it's weights times weight, it's a, it's values times values, right?
So S and PE like doesn't even support that operation, right?
So it's like that operation that gives the transformer its power, it has nothing to do
with the fact that it's attention, right?
And this is a funny like, but that is one of the six tricks, right?
Batch, like these norms are a trick, transformers are a trick.
Okay.
Six more.
Is there a reason why, so you couldn't talk, you talk about, uh, attention as weight compression,
um,
Compression is not exactly the right word.
What I mean is that the weights can change dynamically based on the context.
So it was this thing in pack eight in the hot air prize that I absolutely loved.
And I've never seen it again in neural networks and a really good trick.
Okay.
Imagine you have 256 weight sets for a layer, right?
And then you choose which of the weight sets you're loading in based on some context and
that context can come from another neural net, right?
So I have another one at which protect, projects, you know, 256 wide, one hot, do a soft max,
predict it, and then I actually load the weights and I can do this operation at both test time
and train time.
I can do this operation at both training and inference, and I load in the weights given
the context, right?
Like that is what transformers do, but transformers instead of having 256 discrete ones, it's
actually just that but continuous.
Yeah.
Um, which is funny that that was in language models and I just like, when I understood
that about transformers, I'm like, Oh, this is a real trick and why are they using the
word attention?
And today is actually the anniversary of attention is all you need.
What?
Oh, that's a day.
Today, six years ago.
Six years.
Six years.
Change the world.
Wow.
Well, there's one of your envelope tricks, right?
And you can easily write it on an envelope, you know, think about how you write out that
how many times have you written that because it's not in any libraries because it's like
all used a little differently each time.
Yeah.
If you just write out that exact same, you know, yeah, yeah.
You've name checked Elon a few times.
Yeah.
Think about both of you as systems, thinkers, input, output, think something in between.
Sure.
Um, what, what's different about your style versus his, um, Elon's fundamental science
for the world is physics minus information theory.
Huh.
But you, you do a lot of physics as well.
I mean, like, and Elon does a lot of information theory as well too.
But if the question is fundamentally that the difference maybe is expressed in what
your ambitions are, right?
Elon's ambitions may be like, go to Mars.
Right.
Go to Mars is the ultimate modern, modernist physics ambition.
Right.
It's a physics problem getting to Mars.
Right.
What are electric cars?
It's a physics problem.
Right.
Okay.
Now he's like pushing on the autonomy stuff.
You push a little on information theory, but fundamentally his dreams are physics based
dreams.
My dreams are information based dreams.
I want to live forever in virtual reality with my agri-girlfriend.
Right.
Those are, those are the aspirations of someone who, who, who accepts information theories
of course science.
So I think that's the main difference to me and him.
He has physics based aspirations and I have information based aspirations.
Very, very neat.
Mark Andreessen.
He is a, hi Mark, he's a listener.
He is heavily, he's a big proponent of effective accelerationism.
You've been a bit more critical.
Why do you say that EAC is not taken seriously by its adherents?
Oh, well, only the left takes ideology seriously.
Why is that?
Well, just as a fact.
It's just like, it's just like a fact.
Is the right more cynical?
Is that, is that what it is?
I don't know.
It's like, it's like the left actually manages to get energy around the ideologies.
Right.
Like, like, like there's a lot more, look, here you have, you have two effective altruists
named Sam going in front of Congress.
I only one of them is in jail.
Um, you know, it's, it's interesting.
They're both calling for regulation in those spectra spaces, right?
So SPF is definitely like kind of a wolf in sheep's clothing kind of, right?
Like he, he only adopted EAC or EA to walk in.
Sam Altman is a genuinely good guy who is not interested in power seeking for himself.
All right.
Okay.
We don't, we don't have to go.
Fair enough.
Fair enough.
Um, but, uh, no, EAC is not like, like you are not serious, right?
You are not actually a serious ideology.
You know, uh, Mark Andreessen, I like Mark Andreessen, but I think that like some of
his Twitter things are like, dude, you like just like, it's like, it's like someone who's
like 2019 who's like, eyes were opened about like the political world being not exact.
You mean all the people on the news were lying to me?
Yeah, bro.
They were lying to you.
Like, okay, we all figured this out five years ago.
Now what are you going to do about it?
I'm going to complain about it on Twitter.
Great.
And that's what EAC is.
Um, last and maybe most important, uh, why was Avatar 2 bad?
Oh, I have a whole, you can go on my blog.
I rewrote the script of Avatar 2.
I wrote a script that actually might make you feel something for the characters.
I killed Jake Sully in the first scene like you had to.
Do you really think his second story are topped his first one?
No, of course not.
You had to kill the guy and make the movie about the brothers, right?
And just that alone and realizing that like you could have kept the Titanic scene.
It would have been fine.
I didn't even take it out.
I left your Titanic scene, James Cameron, but I wrote you a story that so, you know,
you just, just, just, he needs ships to sink in water.
He needs, well, look, it's a great scene, but like the movie was just like, like the
Roman, never.
Great CGI, you know, let down by the writing, maybe.
Yeah, no, but like the CGI, like it was, it was a beautiful world.
And that's why like I care so much, right?
Like you don't hear me ranting about Pirates of the Caribbean 2 being a
terrible story because come on, what do you expect, man?
Like Johnny Depp is like, wow, I had a movie that made me rich.
I love this.
But this goes back to like the midpoint, you know, I think you were like, feels
like Chad Chippity wrote the movie.
And that's my worry a little bit.
It's like kind of converging towards that.
Oh, I look, Malak wrote the movie.
Sorry, I didn't want to interrupt you.
No, I closed, I closed a pull request two days ago.
I was like, was this written by Chad Chippity?
And I just closed it.
Like, you know what?
I honestly feel bad if you were a human who wrote this.
Like you're incapable of being more perfect.
But if you have a classifier running in my head that asks, you know, is this
an AI or is this a human?
Like, you know, the only way to deal with all this, like, like, oh, God, it's
like the worst possible.
Like, you know, people are like, like, like, how are you mad about like these
chatbots?
You're not mad about like Tesla.
Well, because if I don't want to buy a Tesla, I'll have to buy a Tesla.
And it won't really impact my life negatively.
But if I don't want to use a chatbot, it's still going to impact my life negatively.
All the amount of like personalized spam that now makes me spend more cycles on
my classifier to tell if it's spam or not, because you can now use AI's and
generate this so cheaply.
Like, no, I mean, we have to move to a model where everything's just a dollar,
right?
Like you want to send me an email, it's a dollar.
Like you guys wouldn't care.
No, my friends would care.
No one would care except the spammers.
Right.
Like we just got to move to those sort of models.
Awesome.
Um, one last message you want everyone to remember?
Uh, look, go, uh, go try Tanygrad.
Uh, I hope that we're a serious competitor to, to what's out there.
And then I want to, you know, I want to take it all the way.
We'll start with just building something for GPUs and then we'll start building
chips and we'll start building fabs and we'll start building silicon mines.
And we'll have the first self-reproducing robot using.
Yeah.
Okay.
All right, George.
Thank you so much for coming on.
Thank you for the big inspiration.
Thank you.
Thanks.
All right.
How was that?
We, uh, not, not quite like, but we hope to do something.

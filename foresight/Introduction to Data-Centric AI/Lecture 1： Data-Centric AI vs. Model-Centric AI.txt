Alright, let's get started. Thanks for everybody who showed up. We've got three of the course
instructors here, myself, Anish in the back, and Jonas. And Anish is finishing up his final
year, but we all were here. We all were students here. We all did our PhDs here. And it's good
to be back. If you don't know any machine learning at all, you've never taken an intro
class, then this will be probably a little too confusing. But as long as you've seen a
little ML, then you'll have a good time. And ideally, you know a little bit of Python,
and you know it like NumPy and Pandasar. So with that, let's just get started. So our
goal in this course is to learn about data-centric AI. So just like a quick show of hands, prior
to this course, how many people I've heard of data-centric AI? Okay, alright, well, get
excited because you're going to learn some great things. It'll be really good. Alright,
so let's just jump in. So this is a picture of a self-driving car that has had an accident.
And you notice that the article focuses on when algorithms mess up, right? And this is
the common case, right? In machine learning, we tend to focus on the model. And so it's
like there's a crash, the algorithm must have done something wrong. And so I'd like us to
sort of think of a different way of thinking about this. This is a paper many folks are
familiar with, which basically shows one thing in that a neural network or a machine learning
classifier can learn total randomness. It's like if you give it complete garbage data,
just like completely random labels, it can learn to map like images to completely arbitrary
labels or text to completely arbitrary labels. So basically, if you give it really bad data,
it will just produce exactly what it learns, even if the data is completely wrong. And
so I'd like to sort of rethink this title of this article as when algorithms are trained
with erroneous data, things like car crashes can happen. And that's the way we'll sort
of focus and think about this course. So traditional machine learning is very model-centric, right?
Like when you take an ML class, you first learn machine learning, you're in school and
then you get a data set. And usually the data set is pretty good. Like if anyone's seen
like the cat dogs data set, you know, it's in images of cats and they're all cats and
they're labeled cat. And then there's images of dog and they're all dogs and they're labeled
dog and there's no, there's usually no like cows thrown in there, right? And there's usually
not like a bunch of dogs that are labeled cat. It's like pretty well curated. And then
your goal is to, you know, produce a good model, right? You want to train a model that
takes in a new image that's either a cat or a dog and it predicts is it cat or is it dog.
And that's sort of how we learn machine learning usually. And this is something that's standard
if anyone here has taken like 6036, which has been renamed to 63390, the intro to ML class
here. And you'll learn stuff like, you know, different types of models. And if you're familiar
with neural networks, neural architectures, or you'll learn about tuning hyper parameters
of a model, or you'll learn about modifying the loss function using different loss functions
and regularizations that you don't overfit. And these are common things you learn in model
centric AI. So let's just juxtapose that with the real world once you're out of the classroom.
So in the classroom, usually the data sets fixed and it's pretty good, but then you
go to the real world and actually the data set is not fixed, right? You have user data
or customer data or real world data and you can get more or less, you can change the data
and the data has all sorts of weird things in it. And so what tends to happen is that
the company or the user, whoever's sort of using this model that you're trying to train,
it doesn't really care as much about the cool ML fancy tricks as it does about like, does
it actually work in the real world? And if you have really good machine learning models
that work on highly curated data, but then the real world data is actually really messy,
and it makes sense to actually focus on fixing the issues in the data. And a lot of people
have been doing this, but what are systematic ways to do this? And that's what we'll focus
on this course. What may surprise some of you that you may not know is that 10 of the
most commonly cited and most used test sets in the field of machine learning all have
wrong labels. And that may be a surprise for some of you. So we'll just take a quick look
at this website, which I think will be pretty fun. This is labelairs.com. Can we see okay?
So this is a site that you can check out on your own. So this is ImageNet, which is a
common image data set. And these are in the test set, or in this case a validation set,
which is what was released. But this is the data that is supposed to be the most accurate
data, right? This is like your real world test data. And so this data should be some
of the most highly curated, most accurate data. And what you'll find is that, for example,
this scorpion is labeled tick. And you'll find all sorts of stuff. We can look at another
data. This is by Google. This is a drawing hand drawn data set. This is labeled a t-shirt.
And this is labeled a diving board. And this is labeled a scorpion. And it just keeps going
and going. And there's some fun ones. Like this is a, this is totally a cake, but it's
labeled a rake. Like it just didn't quite get the R to go all the way when they were
pretending they're writing it out. But anyway, you can check these out on your own. This
is for any type of data. So like text data or audio data. And you'll have fun. And I
think the good idea to think about here is that this is, you know, a data set that's
released by Google. It's a benchmark data set. And it's very difficult when you have
millions of examples to know, like, what's the bad data? And ideally, if you didn't have
that bad data, you could train better models. And so we need to learn how can we find these
errors and find these issues automatically. So going back to the slides. So as seasoned
data scientists, the way they'll approach this problem is that it's more worthwhile to
invest in exploring and fixing the data than trying to tinker with the models to avoid this
garbage in, garbage out issue. And the issue is that, like, if you have millions of data,
right, or you have, like, a data set that's like 100 million, like, how do you do that
without it being highly time consuming? All right. So we're here in a data-centric AI
course. We've called this introduction to data-centric AI because we want it to be accessible. And
so we're going to cover just, like, the very intro, what is data-centric AI? How does it
differ from model-centric AI? And then we'll dive in a little deeper. So data-centric AI often
takes one of two forms. So one form is that you have AI algorithms that understand something
about data, and then they use this new information that they've understood to help a model train
better. So an example of this is curriculum learning. And this is more of sort of something
you'd encounter in grad school, so you may not have heard of it. But what curriculum,
it's, you can check out the paper by Yasha Benjo. But the idea is you use a, some algorithm
that looks at data and it identifies which data is probably easy to learn. And so the
corollary here is imagine you're a student and you're in the classroom. All right. Should
the teacher, what should they teach you? Like, should they teach you really, really hard
examples as the very first examples? Like, if you're learning addition, should they start
out with like 10,051 plus 1042 or would they start out with like one plus two? Right. Now
we know this because, like, we've all learned addition. But when a machine learning model
starts, it's starting from scratch. So it doesn't know right from the beginning what
is the sort of easiest example. And so there are data-centric AI approaches that actually
estimate what is the easiest example. And then when you train an ML model, you start
with that example, and then you give it slightly harder ones and slightly harder them. And
so it's like curriculum learning because it's like a student's curriculum. And so that's
one way that you can sort of use new information. You're still training on all the same data,
but you're just reordering it and using this additional information. You don't actually
change the data set. Another sort of common form of data-centric AI is that you actually
modify the data set to directly improve the performance of a model. So an example of this
might be something like Confant Learning. And this is less known than curriculum learning,
something I worked on. And this idea here is that you want to find what are label errors
in a data set and then remove them prior to training, so that you just train on like correctly
labeled stuff. And the corollary here in the sense of the student is that if your teacher
is basically making mistakes 30% of the time, like imagine as I was teaching you, which
hopefully I don't do today, that 30% of the time I told you wrong things. And then you
compare that to another teacher who comes and they tell you right things 100% of the
time. Then which teacher do you think you'll probably learn data-centric AI better from?
And so the idea is that we want to take this bad sort of, you know, this 30% of wrong things
and we want to get rid of them so that you could sort of come to the classroom and redo
your learning experiences if those never happened. And that's the idea here. So you just learn
on the good stuff. So those are two approaches. And then what we'll think about now is sort
of what is the difference between model-centric AI and data-centric AI? So the sort of normal
way you hear this is that given a data set, you try to produce the best model. And that's
like the classical way of thinking about model-centric AI. And the idea is that you want to change
the model somehow to improve, you know, some performance on AI task. And is there anyone
here just out of curiosity who's not familiar with like AI tasks and some of the stuff we
do in AI? Just want to make sure that we're on the same board, like classifying things.
Okay, sweet. All right. So you've got some AI tasks and you're trying to classify something
and the goal is you want to improve the performance on that. So usually it'll change the model
to do that. And data-centric AI, instead, it's given some model, right, that may be fixed
or you may change, but let's just assume it's fixed for now. You want to improve that model
by improving your data set. So this is like the common way to think about the two. And
the idea is to systematically or algorithmically not have a bunch of humans changing the data
set, but like some algorithmic way that you can do this. Okay. All right. So our goal is
to start thinking about ML in terms of data and not the model. And so we'll just start
out with a simple example. And this is not data-centric AI, but it'll get us thinking
about how we can think about AI in terms of data. So just quick show of hands. Who here
is familiar with K nearest neighbors? Okay. Great. All right. So I won't belabor this
point then. So do some chalk. That was a good thing to check beforehand. We were here last
night, but the chalk has been removed. All right. So yeah, the key idea with K nearest
neighbors and the key idea to think about in the context of data-centric learning is
that K nearest neighbors, imagine you have, you know, a space, a 2D space, which, you
know what, here's what we'll do. All right. So you've got some, all right, sweet. So you've
got, say, you know, some data, you've got some triangles, and you've got some circles.
You'll have to apologize for my drawing and you've got some squares. And so this is your
data set. And let's say, you know, these are different types of images that you've put in
some 2D space somehow where it's text that you've mapped a 2D space. And then the idea
of K nearest neighbors, as many of you seem to know and are familiar with, is that you
would have some new point, say, here. And with this, it's some weird thing. We don't know
what it is. And we're trying to figure out, is it a triangle? Is it a circle? Is it a
square? And so if this was sort of three K nearest neighbors, like then you would find
the three nearest neighbors. So say this one, this one, and this one. And then can somebody
tell me, like, what the class would be and why? Yeah, it'd be a square. That doesn't
mean that it's not a cool point. And you're familiar with being a square. So this would
be labeled a square, and that's based on majority voting. And there's a lot of different ways
to do algorithms for deciding what the label should be. If it was, say, five neighbors,
meaning five in N, where K is five, so it's K in N, then you would choose the five nearest
neighbors and you would do a majority vote. Okay. The key idea here is that there is no
loss function. Literally, any time you have a new point, so say I just have another new
point here, I'm not sort of, there's no algorithm that I'm passing this into beyond just measuring
a distance, some notion of distance between this and the nearest points. And you can do
a bunch of pre-computation. There's a lot of smart work actually done in K nearest neighbors
that is pretty impressive and you can do embeddings for all of these and pre-compute distances
and you can do all sorts of fancy stuff. But ultimately, all this decision is based on
is the data. And so I wanted to motivate this problem and this way of thinking because this
whole decision process is made just based on data. And the quality of the data is as related
as possible to the quality of the prediction. So if you have really good data, you're going
to have really good predictions. And if you've got a bunch of errors in here, you're going
to get the wrong prediction. And so this makes it really clear why fixing the data will make
it, will improve your model. So is that kind of clear how this is motivating? Now this
is not a data-centric AI algorithm. And by the end of this lecture, you'll definitely
know what a data-centric AI algorithm is. But can someone tell me what the difference
is between K and N and a data-centric AI algorithm? Yeah, yeah, totally. That's a great
answer. K and N is just doing classification. And it's not actually modifying the data set.
So yeah, that's exactly right. Alright, so what are a few other examples of what is not
data-centric AI? Handpicking a bunch of points you think will improve a model. So can anybody
help me understand why this is not data-centric AI? Yeah. Yeah, totally. It's just done by
hand. Like this could, if you had a hundred million, a data set of a hundred million points,
this would take a long time. What about doubling the size of your data set so that you can
train an improved model? Yeah. Yeah, totally. This is just classical machine learning. It's
still all the model, all the work you do as a model, but you're just paying more money
for more data. So let's juxtapose this. So what would be the data-centric AI versions
of this? And just out of curiosity, does anybody know for the first one what would be sort
of the data-centric AI corollary? Yeah, totally. And there's a whole field of research on
this and a whole subsection of ML that's called corset selection, where you have a data set
and let's say that you train a model on that data set and you get like 98% accuracy. But
the data set is like a hundred billion points. And so the goal is, can I find like, you know,
a million points that if I just train on those, I can get like pretty close, like 97% accuracy.
And that's corset selection. What about for number two? What would be like a data-centric
AI corollary? Yeah, yeah. So the idea is, and how does anybody know of any ways that
you might make your data set like bigger without just getting twice as much data? Yeah, totally.
So like, say you have a, so data augmentation, awesome. And so say you have an image, you
could just totally rotate that image. And now instead of one data point, you have like
five data points or turn it black and white, or you could shift it or skew it or take the
top and the bottom and move them a little bit or add some noise. There's lots of things
you can do to make data bigger and actually improve a model just by changing the data
set. And these are all fall within data-centric AI. Are you all familiar with what's called
like back translation for text data augmentation? So this is a pretty fun thing. I don't like
have particularly good translation skills. But like, if I say like, hi, you know, my
name is Curtis. And then I translate this. Okay, so it becomes Ola. And then I translate
this again. It might become, and now I've gotten one data point, and I just got another
data point. So I was able to augment my text data set to get more versions of the same
thing. And this could have some label, and the label could be introduction. You know,
I'll just end it there because I know it's hard to see on that side. But this, this would
be like the label. And this is text. And so this one is the same label. We haven't changed
the label at all. So this is intro. And now you can see I have more labeled data, but
I didn't have to do anything. I didn't have to pay more. I didn't have to, I just did
this all computationally. Yeah.
How do you see this in all the cases for that error to be taken?
Oh yeah, totally can. So if, say that this label was wrong, now you have two label errors.
Totally. Yeah. So you often want to combine two approaches. One is like first, try to
fix or improve label errors before doing the augmentation. And that's a really good idea.
All right. So what are some examples, we looked at examples that are not data-centric AI. So
what are some things that are data-centric AI? And for this, I'll go through quick because
we're going to learn all these in the course. And so one is outlier detection and removal.
So I'll just be really quick to show you. So say you have some data set and we've got,
let me use this board. And you've got, say, right? So you've got your sort of, your, you
know, two classes. And so normally you would, you would draw your classifier and then you
have some new point here and I'll be labeled a negative. But say in your training data,
you have this really weird, you know, plus over here. And so maybe the boundary should
be something like this, but you just don't have very much information and it's really
out here and it seems like it's not very related to the rest of the data. And so what often
people will do because they just don't have enough information is they'll identify this
as an automatic, as an outlier because it seems very out of distribution and they'll
remove it. And so then you get this line, which fits to all the data except for that
one point. In terms of some other things in data-centric AI, so error detection and correction.
And so that's, for example, you just have like a data point that is, it could be images,
it's like all black, or you have a label error like we saw earlier. If this, you know, text
example instead of being labeled intro was labeled like, you know, a goodbye clause or
something. You'd want to find that and automatically correct it. Data augmentation, which is what
we saw earlier. So that's like increasing the size of your data set. So you have more
training data. Feature engineering and selections. The idea here is you have, if anyone's familiar
like the early days of neural networks couldn't solve like the XOR problem, but you could
always just generate the XOR as another column. A more concrete example, if you haven't heard
of this, that one is you just have, you know, a bunch of tabular data. I worked on cheating
detection at MIT. And, you know, if you want the machine learning model to learn, say who
is a cheater from a bunch of, you know, education data, like where did they go to school and
what's their background and what problems did they answer. It can really help if you
also compute new features like how often did they submit answers within five seconds of
another student. So you can always generate new features and then you pass those into the
model and your model can, you know, if the features are relevant to the label you're
trying to predict, it can do a lot better. Yeah.
When we're doing outlier detection and removal, how do you know that an outlier is the result
of something that should not have happened or indication of a very rare event that you
should pay attention to? Yeah, that's a really fair and hard question to answer. You make
assumptions. And so in this case, I was making an assumption, right? I was saying, like, I
didn't draw very much data, but say that I had, like, millions of data points and then
this one's really far away. Then the assumption you're making is that, like, I have so much
data that suggests that this is the distribution and then I have very little data that suggests
that this is part of that distribution. And the biggest issue is that if your classifier
is changing dramatically for one data point, but you have, just think of it as, like, evidence.
I've got a thousand or a million people saying it should be this thing and then I've got
this one other person who may be right, they may be right, but they're saying that I think
it's this other thing and it greatly skews the classifier. And so just because you want
to trust the masses, you will say, hey, that one person is really, seems really off base.
And this is very much a choice. And so what typically, what you do is you sort of rank
every data point in terms of how in distribution it is and then you choose your cutoff and
that's a human decision. Another data-centric AI task is to establish consensus labels. So
if you guys have heard of, like, the self-driving cars, of course, then a lot of the ways that
these models are trained is you'll have an image and then they want really high-quality
data. So they'll have, like, 20 different people label, is this a scene of a street or
are we on a bridge? Is this a stop sign? Because they really need to get accurate labels. The
question is, when you're training your model, if you're just going to train with one label
for that image or one set of labels for that image, you can't use all 20 of your annotator's,
you know, guesses. You have to somehow combine them into a single training label. So how do
you do that in a way that maximizes model performance? Another one is active learning.
And this is a very classical problem. You have some data set, you train a model, it has 80%
performance. Okay, I want now to get my model to 85% performance, but I want to pay for as
little new data as possible. Or I want to improve the current existing data as little as possible.
What are my next steps? And you can automate that process. You can actually get good signal to
optimize in a way that you minimize the amount of new data that you need to collect information for
or label in order to achieve that model accuracy. And then a final example is curriculum learning,
which we already mentioned. So these are some examples of data-centric AI tasks, many of which
we'll cover over the next two weeks. All right, so there's a lot of hype around data-centric AI.
For those who are familiar with Andrew Ng, he's a pretty well-known person in AI from Stanford
and has been at Google Research and Baidi Research and done a lot of things found in Coursera.
So he's been really excited about data-centric AI. And let's look at some reasons, you know, why and
some of the things that we've seen in the news. For example, he mentioned that like 80% of an AI
developer's time is actually just spent on data, which is kind of funny, right? You know, you're
an AI developer, you're not like a data scientist, but yet you're doing data science work all the
time. And so there's something happening here in the real world that there isn't as much until
recently actually systematic, you know, learning and teaching around how do we go about doing this.
Also, if you're not familiar, bad data is very, very troublesome for businesses and for the
government and for economies. And it's estimated this is out of Harvard Business Review that it
cost the US alone about three trillion dollars in a given year. And you might see this and think,
okay, that's really bad. But the good news is like a lot of people think that we can actually solve
a lot of that three trillion issue that bad data causes with data-centric AI techniques.
And so there's a lot of hype around it because it means a lot to a lot of people.
This is a quick example. I did this internship at Fair in 2016 and I was in Jan's group and
Jeff Hinton came to visit. If you're not familiar, these two recently won the Nobel Prize of Computer
Science, which is called the Turing Award. And so I think they're old friends. And Jan has a dataset
MNIST. Are folks familiar with MNIST? Okay, cool. Very classical machine learning dataset and we've
been training models on it for like over 20 years. And people generally assume that it has perfect
labels because that's a very common assumption. Not maybe now in 2023, but definitely like when
it first came out. And it's a very high quality dataset. And so Jeff Hinton was presenting at
this time, I think, Capsule Networks. He's very excited about it. And his aha culminating moment
of his talk was that he found the label error in Jan Lacun's dataset. And so he's very excited to
show, hey, this five image is actually labeled a three in your dataset, Jan. And he's like, aha,
I got you. And so I think that it's worth mentioning that this is where we were in 2016. And now,
you know, we're only six, seven years later, and we're able to systematically find millions of
errors in datasets. And that's sort of how far we've come using these data-centric AI approaches.
Who here is familiar with Dolly and Dolly too? Yeah, it's pretty cool, right? So it generates images
and they're pretty cool, like you can generate images of pretty much anything you describe.
And so if you check out the Dolly demo page, and there's the link here in the slides, if you want
to check them after, there's a cool video. And you'll notice in that video, they talk about one of
their biggest challenges. And so this is just screenshots from the video. And the technology
is constantly evolving, but Dolly too has limitations. It's taught with objects that are
incorrectly labeled with plain labeled car, for example. And this happens because it's a
massive dataset. So if you don't use data-centric AI approaches, it's very difficult to clean, you
know, whatever, hundreds of millions or more, probably billions of pairs of text and image.
And so what they notice is that a user might actually try to generate a car, but Dolly will
actually create a plane, because it's seen wrongly labeled data. And so this is very problematic
for something that's deployed in the real world. Another example from that video is they talk about
generating these baboons, but they emphasize that you can only do this correctly if you have
accurate labels. And if you didn't, you'll totally, you can get the wrong thing. And so the key takeaway
here is that this is a real world technology, lots of people are using today at scale, but the
reliability of that model really does depend on the data quality. Another big example is familiar
with chat GPT. Does anybody know, yeah, does anybody know sort of why or like what was the
big innovation from GPT three, which obviously had a huge hype around it to chat GPT, which has
even more hype around it? What was sort of one of the big things they did between the two?
Yeah, totally. And do you know what they were doing with their reinforcement learning?
They like inviting some users to talk about these chat GPT and actually some of the work.
Yeah, totally. Yeah. What was happening there was they, they had a lot of bad outputs, you know,
like chat GPT three was saying things that were like super biased, or like inappropriate,
not even true, just wrong facts. And these outputs were tied to data it was trained on.
And to parameters in the model that were learned from that data. And so what they did is they did
in a reinforcement setting, which means they talked to people, they use that information
to then update the model, then the model sort of explores with new outputs, then people see those.
But what they were doing is they were having people actually rank them in terms of the quality
of the prediction, right? And so they were ranking the quality of this data and then using that to
update the model so that it would have improved less bias and better outputs. And so that was
the key idea of chat GPT was actually to deal with a data quality issue. And if you've tried both,
you can see that there is a pretty big performance boost. The downside is that they had to do this
with a lot of manual work. And so we went to work on ways to automate that. You guys are probably
familiar with Tesla. So this is Tesla's data engine. This is from a talk by Andre Carpathi,
who is formerly the Tesla director of AI. And this is their data engine. And we'll just start in
the top left, like you, the way they're training the, you know, the self-driving Tesla model is,
you know, you have some data source. And then you'll notice some problem, which is like, hey,
we're in a tunnel. And we don't have a lot of, you know, tunnel data. So the car is like
doing weird things, right? And so then what they would do is they would collect a bunch more data
in tunnels and then update their training data and label them and then redeploy and go in the
world and then see what breaks then. And this is a very, you know, difficult iterative process
because you have to send the car out and then see where things break and then collect a bunch more
data. Or if you had a way to automate, okay, this is where my data is missing. This is stuff that's
out of distribution. This is where we have a bunch of label errors. And you're able to automate
that process. They could have reduced those cycles and gotten the car out a lot faster,
at least the AI part of the car. And so this was a big pain that, that Andre mentioned.
Another really, this is a fantastic example that Jonas shared with me. These are all examples of
traffic lights. You know, so imagine that like Elon Musk comes to you and he says, you know,
Andre, I need you to get this car to navigate any street in the world. And then, you know,
you're like, oh, that's cool. Okay. So like it needs to stop at traffic lights. Like that seems
like a pretty simple problem. And then you go out in the real world and like traffic lights are
not a simple problem. They're really complicated and they're really messy. And this is actually
like a total nightmare if you had to do this. And so how do you find sort of systematic ways to
group data together and train in a way that's robust and reliable? Like real world data is super
messy and complicated. And so Andre's big takeaway was that, you know, he shares this juxtaposition
of the amount of sleep, you know, he lost over in his PhD. And it was like data sets is this tiny
sliver, but definitely spent a lot of time on models and algorithms. And then he goes and he's
leading, you know, the AI model at Tesla. And it's like, it's all data, you know, it's a big shock
when you make this shift. And so that's why we really want to focus on ways we can improve that.
A very common use case is when you're trying to train a model with noisy labels. Okay. So
this is like the classical scenario of your, you have the dogs and cats, but now say 30% of
your cats are labeled dog. Okay. And maybe 20% of the dogs are labeled cat. So how do you get a model
that does as well or close to as well as if you had perfectly labeled data? And we'll go into that
more in the next lecture. But I want to just motivate that I looked at a bunch of model
centric methods and data centric methods over the last five years out of top institutions
like Google and Facebook and so forth. And we benchmark them. And it turns out, and there's a
lot on this slide, but there's really one key takeaway that the data centric AI methods all
outperformed the model centric methods for this particular task, you know, on this particular
data set. And this was pretty revealing and compelling that there's something here to data
centric AI approaches. And to be very clear, what these models are doing is they are modifying the
loss function or modifying the model so that they sort of don't train as much on what they think is
bad data, but within the context of the modeling. And what these methods are doing is they're
actually modifying the data set. They're either removing bad data or they're generating more data
that sort of makes the error go away, but somehow they're actually changing the data set. And this
is just how things stack up. And you'll see the data centric methods outperform model centric
methods in this task. And this is a very common task that's of interest to the field. So it's
cool. It's cool to see that this stuff is working and we're getting some benefits from it. So a
sort of culminating thought is, you know, we were talking a lot about ways that we want to automate,
but what did we do before? So obviously we've had to improve data sets in industry and like outside
of academia in the past. It's like, how did we do it before there was data centric AI?
And so we mostly relied on human powered solutions. For example, we would just spend more money
for higher quality data. It's like you literally just pay for more labels or you would pay for more
data. And that was a very classical way to improve a model. Also building custom tools. So like you
saw at Tesla, they have this whole sort of data platform. And this is a lot, right? This is for
one specific problem, the very important problem, but they had to build a lot of custom tech around
it. Another common thing is just fixing data inside a Jupyter notebook. So just a quick show
of hands like how many people have used Jupyter notebooks. Okay, that's really good because
all the labs are in Jupyter notebooks for the most part for this lecture or for this course.
So yeah, what people do is like you just sort data by like a loss function. And so you just say like
the loss for this data point is the highest. So I think this is most likely to be wrong. Let me
check it out. And then you would look at it by hand and then you would market or do something with
it or you take the top 20 or something, but you would just do a lot of this by hand inside of
Jupyter notebooks and printing things out. And that was actually a pretty normal and standard way
that like, you know, somebody in industry or data scientist or grad student would try to fix a data
set. And so the whole idea is we're going to look at ways that we systematize these approaches
so that they're more reliable, more accurate, and they work on most data sets.
So this course is about the following. So today we're just looking at what is model
centric guy versus data centric guy, get the juices flowing, think about how to think things
in terms of data and the impact, why matters. Next lecture, we'll focus on label errors.
So how do you actually detect label errors automatically? How do you learn with label
errors? What are good methods to do that? And what are some things to think about when you're
doing this? Data set creation and curation will be on Thursday. And this is how do you construct
a data set in such a way that you can train a good model? How do you arrange, you know,
the classes? How do you choose good examples? And then finally on Friday, which is related to
data set curation, we'll look at active learning and potentially core sets and active learning.
As I mentioned, this is task where you're trying to choose the next data you want to add to your
data set and you want to obtain a label for. Or do you want to improve some of the labels you
currently have? And so you're just trying to decide, I have to pay a cost for new data that I'm going
to add to my data set. And it costs me something like either money or time. So I don't want to do
it too much. So what's like the best stuff to add to my data set now to improve my model?
Next week, we'll sort of have a bit of a shift and we'll focus more on data, but
we'll focus on some specific things for the first on Monday, we'll focus on class and balance and
distribution shift. So this is, imagine like it's the stock market, right? And like if you're
trying to predict things, you know, on Monday or on in January of this year versus now, it would be a
very different market, right? And so over time, data changes. And so how do you continue to produce
good reliable predictions even though data is changing? And then the class imbalances this
problem where imagine that for that line over there on the left, we had like a million pluses and
only a few minuses. Well, then a smart classifier could actually just always predict plus and get
near 100% accuracy. So often it will just ignore the minuses. So how do we get around that problem?
Interpretable features of data. So this is, who here's familiar with interpretability?
All right, not as much. That one will be fun then. And thanks for raising your hand.
That should be a good class. We'll learn about how do you interpret data in a way that you can
understand what's going on from a data perspective? Like why is the model doing what it's doing in
terms of the data? And so we'll understand model performance based on data. The next class on
Wednesday of next week will be on data-centric evaluation of ML models. So like, how do we know,
you know, from a data perspective, how good a model is, how reliable it is, how well it's working?
On Thursday, we'll look at encoding human priors. So this is like, how do we augment data and also
prompt engineering. So this class will cover a lot of things like GPT and chat GPT and
transformer models and stuff like that. And then finally, our last class will be on data privacy
and security. And this is very, very interesting, especially for a lot of people in like banking
and finance and they're using a lot of machine learning models. How do you make sure that like a
model doesn't actually secretly encode the data? Or like somehow if you had access to predictions,
you could figure out someone's, you know, banking info and you can imagine all sorts of things that
can happen in AI. So data security and privacy is really important. Any sort of questions while I'm
on this slide? Sweet. Are you guys excited? Okay. Does it look like a good course?
Okay. All right. Cool. All right. There's a lab for every lecture. And so you can find this on the
course website, which we can, we can write on the board. So you probably have seen it in the email,
but just in case. And there's, there's a lab, usually will be Jupyter notebooks. And the one
for today will be a text classification task. And it has some bad, bad data that's gotten mixed
in. And this is actual data that's been scraped from, I think, Amazon reviews. And it has some
bad tags, some weird HTML has gotten in there. And what you'll look at is model centric approaches
at first. And you'll realize, Oh, it can only get you so far because like the data is not that great.
And so you'll have to figure out how to improve the data set. You know, so you can get a
better classifier. And so that's sort of today's lab. Get your hands wet with data centric AI.
We have office hours every class, 3pm to 5pm. So an hour after the lecture ends every day.
And then tomorrow's lecture will focus on label errors, how to find them and how to train better
models. This is the folks who are teaching the course for folks that are from MIT,
to from Stanford. And yeah, I think it'll be a good time.
Really quick. Are there any questions?
Yeah.
My question is like, how do you know if it's like data that's the problem and not the model?
Let's say you train a bunch of times in a model with different parameters. It's not like
doing good. How do you know the data is a problem? Do you just try it and see if it improves or
do you can use it another way to see that? Yeah, that's a good question. So there's a few ways.
So one thing you can do is you can look at a subset of the problem. So say you had like
a very big complex problem and there's thousands of classes and millions of data points.
You can take just 10,000 of those data points and a few of those classes
and actually check, do some process to check, make sure you have really high quality data
and see how well does your model perform. And if your model is performing very, very high accuracy,
but then when you use the original data, you get a drop off. That gives you a good signal.
Another thing is if you have similar data set and it's like MNIST for example,
you can get near 100% performance. And so you have a similar data set, but you're getting
like much worse performance or like significantly less performance, but using the same architecture
that you've seen do very well on a similar task on a different data set.
And that's a good indicator. You should probably look at the data. And there's two more things.
One is just, just take a look at the data. I think it's really easy to just get a data set
and your goal is like train a model. And so you're doing a lot of cool, you know,
download this TensorFlow package, download this hugging face package, but you don't actually
take time usually to look through all the data points or like hundreds of data points and really
see like, does this data look like what I think it does? Like, does it seem to be kind of messy?
And what you'll often find is after first like, you know, first few hundred, you'll be like,
oh, there's some weird stuff in here. And if you have millions of data points, that weird stuff adds up.
Yeah, I guess the problem with the data is that there's like two months, so like, you know,
like how do you connect with all of them? Yeah, I guess if you just look at like a bunch.
Yeah, totally. In the next class, we'll show ways that you can actually rank your data set
so that you know what's the best example to look at first. That's probably wrong. And so there are
ways that you can automate this and then you can look at the initial data. And that's really the
right approach. Like if you just look at random, then you might waste some time. But if you've
ranked your data in a way that is likely to give you a good ranking on quality, and then you look
at like the first 100 and there's really no issues and the data looks really good, then yeah, you
might be able to just optimize the model and be okay. Any other questions?
Is this useful for anyone sort of immediately? Is anyone thinking like, hey, this might be useful
for what I'm working on right now?
I haven't done anything specific about it before, and I think one of the reasons is
because, you know, I always study models before. It seems like, you know, studying models is like
very important, but once you go and you want to like quickly do something, that's kind of like the
first thing that you want to do, you know, like, okay, what they do is data and how they work.
Yeah.
All right, great. We'll be here for a little bit after. Thanks everybody for coming.
The next lectures will be a bit more technical, but I think it should be a really exciting course,
and glad to have everybody here. Thanks.

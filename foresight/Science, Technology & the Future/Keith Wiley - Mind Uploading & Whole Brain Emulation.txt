Today everybody, thanks for joining us. We've got Keith Wiley here today who's a board member
of Carbon Copies and has written extensively on the concepts of mind uploading and whole
brain emulation. He's the author of a Taxonomy and Metaphysics of Mind Uploading as well
as the author of a new book which is coming out very soon, which is a novel this time,
our fiction Contemplating Oblivion which should be coming out sometime this month. So welcome
and it's great to have you here again. Thank you, it's great to be back. Now I guess mind
uploading has a concept that's been kicked around for a while now. Why don't you think
that mind uploading has enjoyed as much wide acceptance within the philosophical community,
as it has in the futurist community? Well, I guess I don't know exactly why people are
answering questions the way they do, because they tend to be rather terse interviews that
don't really have a lot of follow-up. But I think that the, you know, the sort of really
contentious questions of the right around this whole body identity concern and the copy problem,
which I actually consider to be a category or are really hard to get away from. And in fact,
I think the recent sort of, you know, popularity of the discussion has probably made people think
about it just enough to convince themselves that it doesn't work, but not to really think past
that point. So there is a certain sort of initial reaction, I think, where you're presented with
the idea of a non-destructive uploading scenario. It could be a transporter-like scenario from
Star Trek or just a more sort of direct mind uploading scenario, but it's presented in a
non-destructive fashion and people think about it for a minute. They sort of initially have this
cut reaction that something about it must not have worked because of the famous copy problem.
And, you know, the bulk of my writing has been an attempt to chip away at that sort of instinctive
reasoning and, you know, really work through it beyond an intuitive sort of feeling and see
where a logical analysis really lands. So there are these various reasons that support
the initial intuition. People often raise concerns about breaking the stream of consciousness or
good old-fashioned body identity in which sort of they feel like their identity has to stick to
their atoms or what I call space-time worm identity, which sort of says that, you know,
it seems a little odd that your identity might discontinuously jump around in space. You have
to sort of go past the sort of the click-off-the-cuff reaction to sort of ask whether that
actually makes any sense. And, you know, I personally have always landed pretty squarely on
the psychological model of identity, which is memory-based. You know, identity is indicated by
some unique conglomeration of a lifetime of memories. And just to be clear, I use the word
memory in a broader sense than what most people might think. I don't just mean episodic memories
of events from your life. For me, the word memory means the totality of neural encodings in the
brain. So there's a lot of subconscious personality traits. There's actually a lot of muscle
memory. You know, sort of the ability to play the piano is a memory, even though it's a completely
subconscious memory stored primarily in the cerebellum, presumably. So by memory, I just mean
neural encodings within the brain. But nevertheless, my preferred model of identity
pins identity to that feature. And that leads one towards certain conclusions. First of all,
that uploading works because it's all memory-based. So if you sort of, if you consider a thought
experiment in which those memories remain intact through the uploading procedure, then by definition
it worked because that was the premise. In order to reject that, you have to sort of
figure out why you reject it. You're obviously going to reject it on the basis of some,
you know, not full commitment to the memory basis of identity. You're looking at some other
component. And I just don't do that. I just always found that the psychological memory theory works best.
Okay. So, I mean, it's interesting that, like, if it's totally based on memory, surely you'd have
to have some sort of mechanism to be able to process consciousness, valence, awareness,
and the memory as well in order to have an agent feel as though it's got some sort of stream of,
or persistence of identity and enduring metaphysical ego.
Yes, yes. There is this, right. So you've got the concept of identity and you have the concept of
consciousness. And they're not necessarily the same thing, which is where the famous thought
experiments of philosophical zombies come from. You could have agents, you know, cognitively
processing in the world and, you know, encoding memories of a form, although it's hard to imagine
what reflecting on a memory consists of, if you have no consciousness, I'm not quite sure what
that comes down to. But nevertheless, you can sort of separate these components and you can ask,
what would it be like to be an unconscious but yet memory fulfilling system? I think it's mostly
philosophical whimsy. I don't actually think that philosophical zombies are a practical reality,
they're just sort of a philosophical exploration. Well, as David Chalmers brought up the thought
experiment is if you had an atom for an atom reconstruction of a human that, you know, the
first human had consciousness and the second human didn't have consciousness, although all
their atoms and physically they were exactly the same, wouldn't that imply dualism? So, but I mean,
people take the thought experiment to mean something else and it's been used to describe
artificial intelligence without the architecture or the phenomenology of consciousness as well.
So yeah, so we're, this interview is evolving quickly, we're getting into some other topics, but
yes, there is a huge question in the last three years, give or take,
you know, what the implications are of the huge LLM models? What does that actually have to say,
not only about artificial intelligence, but also about consciousness? You know, are these things
conscious? Do they actually have a sentient sort of set of rights and things like that?
A lot of people, I think with pretty good reason, think that the LLMs are not satisfying those
requirements. They're doing something else, but they're probably not conscious in the way that
we're particularly concerned about. I don't think that they have a lot of recurrent
wiring in their layers, they tend to be pretty feed forward heavy. And there's, you know, I'm
definitely one of those people who thinks that recurrence is doing something, I don't quite
know what, but I think recurrence is a crucial component of consciousness in some fashion,
sort of sense of kind of looking back on on itself in a sort of a loop of sort of experiential
self-learn looking. We don't know metaphysically like why, you know, what is it about that?
That's critical to consciousness, but a lot of people seem to agree that some kind of recurrence
should be part of the design. Short of that, you know, something that's always bothered me about
the modern AI systems is that they're all sort of on and off and then they're in the dark. You
query them with your question, they go off and they dig through their neural net, they come up
with a result and they produce for you a paragraph of text or image or a digitized voice, and then
they go dark. That's it. Like they're not sitting there daydreaming, they're not musing on the
conversation you're having with them going back over, you know, the previous dialogue from last
minute. They're not doing, hey, they're dead while the prompts, if they're blinking until you issue
your next query. So is that, can a system like that be conscious? This is an interesting question,
and are these systems conscious on the basis of that sort of strange operation, which is just
very not lifelike. It's just not what we think of. It's not what we do with people. You know, a person
doesn't sort of sit there like a rock until you go up to them and sort of, you know, you query them
and then they do something and they emit and then they die until you interact with them again. Like
people don't work that way, animals don't work that way. So natural brains seem to be in a constant
flux. That's what it means to be a dynamic living brain. And modern AI systems don't do that.
By definition, it's just not what they do. So I don't think they're conscious. I think it's pretty
clear that they're lacking these major features that we would generally assume we expect before
we're going to put those labels on it. I don't know. Yeah, what a time to be alive where we
beginning to be able to, I guess, empirically investigate some of these questions in more,
with more precision, and also with a whole lot more data and we can test assumptions in artificial
sort of substrates instead of just, you know, trying to probe directly the human mind.
Not averse to doing the, the, the latter though, I think it's really interesting where brain
computer interfaces could go. But given the, yeah, so what, what kind of resolution or fidelity of
memory do you think is required for persistence of, in order to either reconstitute or persist
an identity or a sense of self? Yeah, I'm not. How do we approach that question? Yeah, maybe it's a
better question. Well, I think the first approach is to bring in the professionals of which I am
not. I think that there are, you know, psychologists, psychiatrists and neurologists probably have,
there's probably a lot of very established science in sort of how memory evolves over the course of
one's life. And what degrees of memory loss imply, you know, sort of noticeable losses of
personality or, or sort of the, the external third, third party perception that a person is
sort of being lost over the course of dementia, you know, we can say, you know, if you lose your
memory at a certain slow steady rate, it's not dementia and you're not really losing your
personality. But if it happens faster, or if it happens more discontinuously, then we as external
observers sort of recognize that there's some sort of identity collapse sort of in process, you know.
So I don't, I don't claim to, you know, beholding the professional sort of state of that. But it's
something like that, you know, it certainly can't be a requirement of perfect fidelity because
we are, this is, this is Thomas Reed's experiment from, from the 1700s that people are, you know,
over the course of their lives, always constantly steadily shedding their old memories. And yet
we consider, consider it to be a persistence of identity so long as the shift is, you know,
you know, smooth below some threshold. But, you know, to give you an actual answer to that question,
I don't know. I do think it does imply, though, that when we try to create
mind uploads, you know, and things of that nature, there's going to be a lot of wiggle room. You know,
people who demand that before a mind upload can be bonafide, it has to preserve every quantum state
in the brain or some complete nonsense like that are, I just think they're off the map. I mean,
it can't possibly require anywhere near that level of fidelity for us to judge the result
to be a successful preservation of identity. And that's just ridiculous. But again, I don't
actually know what the threshold is going to be. But I think we're going to have a lot of leeway.
I think there's going to be a lot of flexibility once we figure out how to do this.
Right. And previously in an interview in the past, you've mentioned the idea of a child
had drowned in very, very cold waters underneath ice or something like that. And in upwards of an
hour after drowning or being out of it for an hour, they've been able to be revitalized.
So this cuts to a very specific argument. A common argument that comes up in
debates about whether or not a mind upload preserves your identity is a concern that people have
about whether or not it preserves your stream of consciousness. There is this perception that you
have this permanent lifelong and never ending ongoing dynamic stream of processing. And if
that breaks and restarts, such as you go into some sort of, you know, mind uploading operation in
your entire brain is vaporized, but it's scanned in the process and then it's revitalized in a
whole brain emulation, a computational model. That breakage marks a loss of identity and the
person dies by definition. And then a new doppelganger basically springs up out of the ether
and takes the place. So that's the basis of that argument. And I've written not one, but in fact,
two papers on this very specific question. They have very similar titles there. And I don't know,
I think they're both worth reading, but there is a proof positive argument that that whole line
of reasoning doesn't work. So, you know, the way a sort of an exploratory debate will usually approach
this is like, well, when you fall asleep, you sort of lose a little bit of consciousness. Does that
break your stream of identity? And then you say, yeah, but you still got a lot of neural processing
going on. So that doesn't count. Okay, well, you go into the ER, you go into general anesthesia.
That's really deep. That's really, you know, that's really turning the brain down a lot. Maybe,
you know, does that undo the argument? They say, well, no, even in general anesthesia,
you know, you still got a lot of neural processing going on. So now that doesn't count.
There's just sort of a lot of knockdown arguments here that you're trying to sort of
work your way through. And eventually you get to the fact that there are real world medical cases
of people under two different scenarios, one accidental and one intentional sort of an OR
procedure that basically disprove this whole thing. So there is this accident tragedy,
usually called rapid frigid drowning. It's occurred a few times. It either occurs when
someone falls into a frozen lake or occasionally in a dry example, they just fall into a snow drift,
essentially. These are these are sort of real medical cases that have come up.
And they are found several hours later, seven, seven, eight hours later.
And in a few, you know, most in most cases, like this person dies.
In a few rare medical cases, they dropped in temperature quickly enough that instead of
killing them, it essentially preserved them the same way a freezer preserves food from spoiling.
And in a few rare medical cases, those people have been miraculously brought to an emergency room
and they had they basically shot from the hip. It's not like this happens often enough that
they know what to do. But, you know, the doctors basically went through the motions of trying to
slowly warm a person up so that they don't go into shock and various things of that nature.
And there have been cases of recoveries. Okay, what's the point of all this? The point is,
well, first of all, the point is not that their bodies and their brains froze solid,
but you know, this is not an example sort of natural accidental. Oh, and so that was the accidental
case. Then there's an OR procedure called hypothermic preservation. I can't remember exactly what the
term is. It's usually used for cardiac surgeries. It's not used for neural surgery. It's used for
cardiac surgery. You take the patient's brain down in temperature in order to buy more time to
perform the cardiac surgery. Okay, while you're working on the heart, you're not pumping blood
through the rest of the body, including the brain. You take the brain down in temperature on purpose
so that the brain will survive longer, hopefully long enough to finish a heart surgery. Okay,
so those are the two cases. So then coming back to my point, sometimes people sort of
overinterpret this. They're like, oh, this is an example of cryonics. We know cryonics works.
That's actually not true. In none of these realized medical cases, did the person remain
in a cold environment long enough to literally freeze? That's not what these medical cases
involve. But they do involve a drop in temperature to, okay, the temperatures are in those papers
that I wrote. So go to them for the exact numbers I don't remember. But somewhere on the order of
15 degrees Fahrenheit, I think, so they're well above freezing. But the point is that
there is a temperature below which the brain does not operate anymore. It does not fire
action potentials between neurons anymore. Neurons do not operate just because they're above freezing
or something. There's a critical temperature at which they are able to operationalize
being a neuron, primarily operationalizing action potentials, which is the fundamental basis of
all sort of neurology theory. The propagation of action potential is how we expect the brain works.
And that temperature is higher than some of these medical cases. So just I'm speaking
in sort of long interwoven sentences here. What it comes down to is there are medical cases where
a patient has had a brain not fire any action potentials for many, many hours, which coming back
to the original point means that their stream of consciousness was turned off like a water tap.
You cannot have a stream of consciousness if you don't have a brain sending signals through
itself from one neuron to the next. There's no stream of consciousness if the brain is not
processing. So these are patients. Stream of consciousness ceased. And when they were revived,
nobody called them a copy or a doppelganger or a body snatcher or whatever these ridiculous like
everyone just treated them as a revived recovered medical patient.
And that's it. That's the disproof of the entire stream of consciousness
counter argument against preservation of identity via mind uploading.
On the basis of it sort of involving some cessation of processing during the uploading
procedure, it's just a that's the whole thing. I think the paper is going a little more detail,
but really, I don't think there's a whole lot more there. So that's you sort of brought that up.
And I just thought I'd walk through that. That's what that was all about.
Is whole brain emulation needed for blind uploading?
Right. I think whole brain emulation,
short answer, yes, I mean, I don't really see how you do it without it.
There are some interpretations of mind uploading that sort of lean away from that. There's this
concept of training a, what did they used to be called classically? They were called expert
systems. This is what they're called in the 80s and 90s. These expert systems, which have basically
now become bots and the recent LLMs and the AIs, if you train one of these things on the corpus
of a person's available lifetime of media, some people provide more media than others,
but everything they wrote, all their emails, any videos, home videos, everything you can find
that person ever produced, if you train one of these neural net AI systems that are sort of
all above these days against that, you can produce a bot that basically in a dialogue acts
a lot like that person. So the theory, and there have been companies that have made an
entire business model around sort of ostensibly preserving you in this way, or at least preserving
like this sort of post-mortem puppet for other people to interact with. So the idea is you produce
the system, and in those cases where there's a rich enough set of data, you can produce a bot
that sort of passes for the person, and with that count is mind uploading. So that's a version of
mind uploading that doesn't involve whole grain emulation, but there aren't too many examples
like that. In most cases, mind uploading is a whole grain emulation concept, and the way I
describe it is that whole grain emulation is just a technical term. It's the process of
modeling and emulating the neural processing of a brain. Doing that is whole grain emulation.
Mind uploading is not actually a technical term. Mind uploading the way I see it is
it is a philosophical interpretation of the consequences of whole grain emulation. If you
choose to interpret whole grain emulation as a preservation of a person, a preservation of a
person's identity, then the label for that is mind uploading. Mind uploading is the
interpretation of whole grain emulation as a preservation of identity. You have uploaded
the mind. I don't actually hear too many people phrase it that way, but that's that's how I've
always thought of mind uploading as being this interpretation of the technical process of whole
grain emulation. So I mean, I guess the question comes to me is, do you think you'd need to upload
all the exact brain of a particular identity in order to reproduce that identity in the machine,
or are there some parts of the brain that are rather generic that you'd be able to scaffold on
more templated emulations of like a generic brain, certain salient features of the brain,
which are more important to preserve personal identity and during metaphysical in go?
Yeah, I mean, there's one really big one, which is that something like 80% of our brain
by neuron count is the cerebellum. And you could probably just throw the whole thing away.
You might, you know, not bring your expert piano playing skills along with you.
And you know, you might because the muscle memory is often encoded in the cerebellum.
And you know, whether or not you could borrow someone else's or well, or you could relearn it
again, right? But if we're trying to sort of simplify the process of the emulation, because
it turns out to be technically really hard, you're trying to find ways to cut corners.
You know, it's it's sort of debatable whether a professional musician or it could be any
professional athlete, because some sort of sort of muscle memory task,
would they consider that to be a critical component of their identity? If they lost that,
would they not be the same person? You know, that's a very sort of emotional question to ask.
But nevertheless, it really doesn't impact the cortical, I mean, by which I mean cortex,
the cortex encodings that we generally associate with all the rest of personality traits.
All of that is in the cortex, not the cerebellum, you know, not the alamus, maybe the hyper alamus,
because that's doing some short term memory work, you know, not the amygdala. There's all sorts of
stuff in there that doesn't seem to be particularly pertinent to what we consider to be human identity.
Which isn't too surprising, because if you look at it from an evolutionary perspective,
the human cortex and neocortex is just this 500,000 year veneer, you know, sort of thinly
layered on top of 4 billion years of stuff that we don't care about. We don't care about the
aspects of our identity back when we were amphibians. So why would we consider that important?
What we really are interested in is just the last 500,000 years of neurological evolution.
That's the stuff we really need to successfully carry over to preserve human identity. Now,
I'm not saying we have to just sort of cut, you know, cut it off that early. You know, it might
be nice to try to get a richer, more complete emulation. I'm not against that. But the question is,
is it possible to achieve, you know, a pretty good passing whole brain emulation and
mind upload, you know, at some fraction of the total brain? And I think we could. I think we could
make a good enough version one with way less than the entire brain. And over time, we'll probably
be able to do the rest of it anyway. So it's probably going to be a question 10 years later.
But in theory, I don't think we need a lot of it.
Okay, so therefore, if we don't need to emulate the whole brain, then are we approaching the
compute required to be able to achieve this if we knew how? Do you think?
So there are very wide estimates. You know, there are different, there are different sort of
dimensions, you know, processing power, memory capacity. I think at a recent video or presentation
sort of the speed of communications in between the units was considered.
We, I think the expectation is that we have almost gotten there now with some of our supercomputers,
at least at the lower estimates of human processing capability. But this sort of begs the question
of whether mere number of computes per second is actually the trick. Or, you know, I'm a big
believer in neuromorphic computing in which you actually create a massively parallel system
that is connected to itself and wired against itself in ways that closely mirror the architecture
of the brain. And you know, one might ask, well, who cares, you know, and this comes back to a
point made earlier, which is that I strongly suspect there is something about the recurrent
self-referential, you know, self-looping signal processing structure of the brain
that is important. I don't know why this is an open question. We don't understand consciousness
well enough to understand why certain network architectures seem to work and others don't.
So the wiring diagrams in the cerebellum have essentially nothing to do with your consciousness.
If you remove a person's cerebellum in surgery, their consciousness, their sense of who they are
is all left completely intact. Again, there can be sort of some damage to their muscle memory,
but their sense of who they are is not harmed. And their conscious experience is not affected.
So the way that the cerebellum is wired is not the right way. And the way the cortex is wired
is the right way. What is that way? We don't know. It's the 21st century. What a time to be alive.
I don't know. But I think that I think that we need to get, I think we need to solve that.
I think we need to figure that out. We need to figure out what it is about the wiring of the
cortex that is creating this amazing experience. And that's what we need to make sure that we
recapture in an emulation. Do people know what parts of the brain were destroyed
when Phineas Gage got a big rod through his brain? They definitely do. I don't recall if I've
talked my head. So it went sort of right behind his eye, which means that it hit a lot of his
prefrontal cortex, which is, you know, relative to a prandola pram. It hit parts of the brain
that are salient to sort of emotion control or something, which is why after the accident,
he became very sort of fly off the handle, kind of sort of an angry kind of person. He just sort
of didn't regulate his emotions probably. So something about that sort of, and it wasn't like
an amygdala, you know, sort of emotional response. I understand, I really have to go read it. I'm
sort of shooting from the hip here. But my understanding is that our, you know, very advanced
intellectual cortex level ability to regulate our emotions to basically just think through
a situation instead of flying off the handle. That's all, you know, that's not the kind of
carefully reasoned stuff that, say, a mouse does, you know, a mouse more or less does actually kind
of respond emotionally to what happens around it. Humans can feel the emotions, but also think
through it and say, okay, I'm not going to act directly in accordance with my emotions. I'm
going to act in this intellectually rationalized way instead, because I have a cortex that enables
me to do that. And that's precisely what got blasted out of Cage's head. But no, I don't remember
exactly which, if you look it up, it'll tell you exactly which parts of his brain were damaged.
Do you think quantum computing is required for whole brain emulation or mind uploading?
Or if it's not, could it be desirable?
Yeah. So of course, Penrose and Haneroff made this famous.
My God, you be all right.
The most honest answer is that I don't know. Of course, people tend to have hunches and I have a
hunch. What's your hunch? I would be surprised if the quantum mechanical properties of neurons
had anything to do with sort of animalian evolved behavior. Just why would evolution
need to go there? It seems like all it really needs to do is get the action potential sputtering
around well enough that the animal can satisfy the four Fs of survival. Why do you need quantum
mechanics to do that? I don't see the need. I don't get it. I just never found the quantum
mechanical argument for consciousness convincing. That's my take. But I'm also fairly open-minded
about it. There's a lot we don't understand about quantum mechanics. There's a lot we don't
understand about consciousness. Some people see that as an excuse to pin them together, implicitly.
People like Deepak Chopra are like, well, we don't understand consciousness and we don't understand
quantum mechanics. That means they're connected. So that's garbage reasoning. Two things aren't
related just because they're both mysterious. But at the same time, they are both mysterious.
We don't actually know. So I'm open to someday discovering what there's a connection there.
But I don't see a need for it now. Forgive the cat. The cat is forgiven.
Once we can upload brains onto a computer, there's going to be an economic issue too
about how to afford to do all this and how to do it in a cost-effective way. Such is a good
ratio between the amount of minds that can be uploaded and the amount of power which is consumed.
The trade-offs will become more friendly over time as technology gets better. But what's the
role of the type of resolution that we might want to capture and the ability to do compression
to reduce the amount of space a mind might occupy?
I do have some numerical answers to that question. But more to the point, everyone's sort of used to
these claims about the rate of advance of technology and it's true. So we can just barely scrape
together a supercomputer that might emulate the human brain today. It's millions and millions
of dollars and it will only run one brain. And it's not wired correctly to do that anyway. So we
couldn't do it even if we wanted to. But in theory, the architecture is there. So we're just,
you know, that's where we are now. So we're down the road. We'll be able to do it
expensively but successfully. And then 10 years after that, we'll be able to do it cheaply. You
know, you know, the human brain uses as much electricity as a light bulb. So we're going to
get it, you know, at some point, the power requirements won't be relevant. You know, it's not
like you're going to need a supercomputer or even anything as inefficient as a desktop computer.
You'll need something as efficient as a brain. And you know, how cheap are brains? There are
brains everywhere. Like brains don't take much energy. The world's full of brains. So once we
are able to create those kinds of computers at that level of energy efficiency, the efficiency
equation, and therefore the cost equation, it's all just going to go out the window. You know,
so long as your brain, so long as your emulated brain can get, you know, as much energy as,
you know, a salad, then it'll be able to stay alive the same way we can. Right? So it's just,
that whole problem is going to go away. I hate to say that it's a question of patience, but it is.
People really chomp at the bit about futurism stuff sometimes. They're really like,
gosh, you know, this stuff really needs to happen in the 2040s or it's no good at all.
I mean, I'm sorry, but some stuff's just not going to be available in the 2040s,
but that doesn't mean that it's not going to happen, you know, early in the next century.
Or later in the next century. I don't know when, but I'm not particularly concerned about that,
because when it does happen, I just think it's going to be practically preordained. I don't
think it can be stopped. I also don't think it can really be expedited too much. I mean,
mostly we want to create a society and a culture that sort of broadly favors science and knowledge
and openness, you know, and sort of inquisitiveness and curiosity. If we just sort of create a society
that creates and supports these general ideas, the science will just happen. Now,
I can get into some minor specifics. You were talking about sort of resolution and compression
and things about that, things like that. So where we currently stand on sort of producing
whole brain emulations is, well, in short, we can't, but there are sort of multiple steps in
the process. So the most likely way to produce a whole brain emulation is to preserve the brain.
That means essentially embedded in a resin, and then you deli slice it, and then you take pictures
of each slice. You can't use, like, a camera, because the features, the neurological features
you're trying to capture are smaller than the wavelengths of visible light. You can't take
a picture of something that's smaller than a wavelength of light. So we use electron microscopy
to do this. And the resolution that we're currently able to achieve is a z-axis resolution. That means
slice to slice. We can physically diamond knife slice brains at about a 40 nanometer thick resolution.
And then when we take an electron microscope image of a slice, we can achieve about a four
nanometer xy resolution. So each image has 10 times as much resolution, 10 times the fineness of
resolution as the slice to slice steps. So in the three dimensions, two of the dimensions are finer
by a factor of 10 than the third. And the question is, is that good enough? And the answer is,
according to current models and theories of how the brain works, you know, the major paradigm
of action potentials propagating along axons, triggering synapses, and then propagating secondary
action potentials and downstream neurons, we can image synapses at that resolution. And of course,
we can image all the rest of the neural components, which are larger than the synapses. So we can
actually capture the necessary resolution to produce a whole brain emulation. We can do it today.
What we can't capture is the scale. So the data sets that we are scanning these days are, you know,
what sort of cubic section of a brain can we kind of scoop up and actually perform this on?
And it's on the order of sort of fractions of a cubic millimeter. One full millimeter is just,
you know, sort of coming around the bend as sort of a 2020s project. So, you know, human brain is a
little bit bigger than that. So we've got a ways to go. I think a whole mouse brain of approximately
a cubic centimeter, which is a thousand times bigger than a millimeter, is one of the projects
that is being sort of put together now to be achieved over the next many years. And when that
is done, all we will have is a brain of a mouse. All we will have is one centimeter. We're going
to need to go another factor of 10 in all three directions to get to the human. So
it's going to take a while. But the interesting thing is that while it's going to take a while
on the calendar to get there, there's a very little that actually appears to stand in the way.
We just need to keep turning the crank for a few decades. And there's no real obvious like place
where we think it's just technically like we're up against a wall. When we look at it today,
there are things we don't know how to do yet. Getting the perfusion of the chemicals deep
into a large brain evenly is very hard to do. We don't really know exactly how to do it. How do
you, you know, we can slice up a slice of a brain on the scale of microns, but can we create, you
know, a cross section of a brain that's only 40 nanometers thick, but is five inches across, right?
Like how do you even handle that, right? So there are major technical questions, but these don't
feel like things are going to be like some fundamental wall in physics that we cannot get
past. It seems like we just need to keep going and we will get there.
Do you think that there's a particular, okay, so you mentioned plastination, at least that's
what I think you're referring to. Do you think that's the state of art and cryopreservation today?
Or brain preservation today? Or would you suggest that it's something else? Or
yeah. And what about in the future? Do you think where the state of the art
plastination techniques that we have today are the best that they'll ever be?
So correct me if I'm wrong. I think plastination refers to room temperature preservation techniques.
Correct. Yeah, I don't really know where they stand. So you've got two things. You've got the cryopreservation
and then there is the preparation of a brain sample for slicing, which is not cryo.
It's just room temperature and it's a resin. I don't really know what the chemical structure is,
but it's essentially you first you flush out a lot of colorful and opaque things so that you
can physically see through the brain. And once you flush all that stuff out, you embed it in
some kind of resin. I'm not a chemist. I don't know. And now you've got like this solid block with a
transparent brain inside it. Then you slice that and you take pictures of it. Now preservation,
you know, especially sort of preservation for the purpose of longevity, sort of cryonics or
the next generation of cryonic like preservation, which would be these sort of the aldehyde
stabilized cryopreservation that the Brain Preservation Foundation has awarded,
which is probably, you know, definitely the next generation in preservation if we can get it
sort of normalized, popularized. That's for a different task. That's for the task of preserving
people, keeping them alive for one of two reasons. Many proponents of sort of classic cryonics want
to be kept alive just to sort of get to that point in the future where the technology can revive them
biologically again. The other reason to go into preservation of some form though is to just get
to that point in the future, not where you can be biologically revived, but where mind uploading
technology has gotten to the point where it can upload your preserved brain and you would not come
back biologically, you come back as an upload. Now as you know, I don't want to completely distract,
we can get into like, you know, why would you want one? Why would you want the other? What's the
benefit? You know, why would you choose one if the other's available? There's a whole conversation
there, but I just I think it would sort of take us off into the weeds. But there is never there's
this issue of preservation for longevity and there is this issue of preserving and preparing a brain
sample for very contemporary and real, you know, sort of slicing and scanning that we do today
in labs today all over the world. What about inspirations from biology? And it's been mentioned
before about the visual cortex, how that's very much like the way a convolutional neural network
might work. There's hierarchies there. They've got logical receptive fields and shared weights.
Do you think this is important to sort of mimic the way that biology works in the
way that we approach developing a whole brain emulation? Or can it be a completely wildly,
even wildly different computing power time? Yeah. So yeah, modern convolutional neural
networks were intentionally inspired by the visual cortex. It's actually not the brains
resemble convolutional nets. Convolutional nets were sort of built on the previous four decades of
research into how vision works. We actually learned a whole lot about how vision works. And we said,
wait a minute, what if we code that up to a computer and see if we can see? And lo and behold,
it's actually worked incredibly well. So one thing that that story tells is that
biomimicry is a very good strategy. If you figure out how nature does something and then emulate
that with engineering, that's very likely to actually get you pretty far down the road or
whatever task you're trying to solve. For whole brain emulation, there's an interesting opportunity
to cut some really major corners at the possible expense of like a major philosophical or metaphysical
failure. So what do I mean by that? At the lowest level, we could create a whole brain emulation
of neurons and synapses, individually firing action potentials down their axons across the
synapses into the dendrites through the full complex web of neurons. And we would expect
that to certainly be functionally successful. You know, there's a whole big debate about whether
it's successful in some other way, but it's, you know, because some people think it has to be a
biological system to be conscious or it has to be whatever, but but it would functionally replicate
the action potential propagation paradigm. But that's actually pretty computationally like
inefficient. Like why, why recreate a thousand neurons that fire in some complex arrangement
to perform multiplication? If you can just stick a multiplication module in there that has the same
black box behavior of the neurons. If you've got a set of neurons, you've got a set of neurons
and you send input signals in and you read the output signals out, you could replace that set
of neurons with what we with what we call a black box. Ted Berger has been doing this with
hypothalamus. And if you basically produce a prosthesis effectively that takes in neural
signals and emits neural signals like the system that's replacing the set of neurons that's
replacing, you can use a potentially much simpler computational piece of hardware to process those
signals and yet get the same functional propagation, right? So it's an interesting question. How far
that could go? How much of the brain could we abstract away to some sort of algorithm? You know,
one of the one of the biggest ones would be like cortical columns. The cortical columns are these
sets of about a hundred neurons that seem to sort of process kind of as a singular unit, you know,
really know exactly what they're doing or how they're doing it, but there's this repeated
cortical column pattern. If we could solve the cortical column algorithm, we might just black
box the whole thing away. What if you just make a single computer chip that emulates, you know,
the hundreds of neurons and the thousands upon thousands of synapses in a single cortical column
just turn the whole thing into some little computer that may not work the same way at all,
but it will take the same neural signals and it will critically, it will emit the same signals,
so it has perfectly replaced that behavior, but in a completely different way, right?
And then the question is, well, what if you did that with every cortical column in the entire brain?
So a whole lot of the neural structure that we consider to be extremely human in its architecture
would be completely replaced with some sort of computational abstraction. The same signals
would be going through, but would it actually metaphysically be the same sort of mind?
And we don't know, we don't know where that cutoff is, you know, where the sort of level of
fidelity of your emulation loses some sort of metaphysical thing that is deemed important.
We don't know where that threshold is. I suspect that we might find it, you know, because,
you know, as you produce these things, as you produce these prostheses, these modules,
and emulate them, if you get some sort of surprising behavior out of it, then you're
probably abstracting too far. I'm not sure how that would happen, because if it's producing,
if it's reproducing the same functions, there isn't an opportunity for it to sort of behave
differently, like that's the whole point of the functional paradigm. So I'm not really sure where
it could break down, but if it was going to break down, presumably we would discover it. I mean,
otherwise you're just back into philosophical zombie territory, you'd be able to replicate
all the functionality, but you wouldn't know, but then I just find that all very hard to buy.
Yeah. What do you think, look, I mean, there's the issue of like the fire in the equations when
like Hawking spoke about, like, what is understanding of a mathematical formula?
What is the, you know, what's the important, what's the important difference between the way
or computers at the time when you wrote this subject would be able to calculate and the way
a human would be able to calculate? And he spoke about fire in the equations. I think it was Hawking
anyway. Maybe it's got something to do with like something like that and embodied understanding,
maybe a feeling. I don't know a valence of it. It's going to be one of the big questions of
neuroscience in coming decades. Sure. Well, okay. Another big question that I thought to bring up
was why is whole, a form of whole brain emulation of your preserved brain, probably you, and you've
written directly about this, but I thought I might bring this up during the interview.
Why do you think whole brain, a whole brain emulation of Keith Wiley would be Keith Wiley,
even if the original was still alive? Well, that's that last sentence really. Okay. Where
that's where people start the big debates, right? Well, let's step back. Let's, let's exclude that
last, that last sentence then, because we will touch on identity branching. Yeah. Better. I mean,
that has been the bulk of my writing is defending the branching interpretation. But yeah, why would
it be me? So there are different theories of metaphysical personality. For example, you have
the body theory, which basically says that your identity is attached to the matter that you're
built out of. You have a stream of consciousness theory, which is that it's some sort of continual
stream of consciousness. You have a space-time worm identity, which is that this thing sort of
weaves its way through four-dimensional space-time, because it's sort of continuously located in
sort of a smooth path. The identity is sort of following the sort of worm through time,
but it doesn't discontinuously jump around. And then you have psychological identity. There are
there others as close as continual identity. Close as continual identity says that the thing
that is you now is that which most closely resembles the thing that was you, you know, very
recently. And you have to get into the thought experiments to sort of feel sort of where that
question comes up. But another theory is psychological identity, which is a catchall term
for, by psychological, what we mean is the memories. Your identity is indicated by
your psychological traits, your personality traits, your memories, your all the sort of cognitive,
emotional, intellectual sort of senses of your, that's your identity. Every single one of these
theories can be subjected to sort of colorful thought experiments that try to sort of break them
apart and undo them and find some sort of paradox that leaves them. And the interesting thing about
the psychological theory is that it only breaks down if you declare at the outset that you're
going to reject branching interpretations. So you basically have to say, well, branching is just
going to cause a lot of trouble for my thought experiments. So I'm just going to say it's excluded
and we're not allowed to consider it. And if you say that, then you can come up with a thought
experiment where psychological identity sort of runs into problems for the same way all the other
identity problems models can run into problems. But if you don't cut yourself off with the knees at
the beginning, if you say, well, wait a minute, branching identities on the table, let's see
where that takes us. Then psychological identity is the most robust theory that I have yet found.
Well, okay, so you're going to be talking a lot about this in your up and coming novel,
contemplating oblivion. So anybody who's interested in these topics would like to explore worlds in
which they are covered, yeah, watch this space or watch out for contemplating oblivion.
So but yeah, would you want to just give us a breakdown of some of the writings you've
covered identity branching in? It's your favorite topic. This is what you've covered the most.
Yeah. So it all started actually started slightly before my 2014 book. I was inspired to
write an article in which I sort of thought I would lay out my basic sort of theory of branching
identity. In early 2014, I'm sort of going through my own list of writing here.
Was this the H plus magazine? Let me think here. It was
Oh, in March of 2014, it was actually a response to a article by Susan Schneider.
I remember. Yes. And I wrote an article for H plus. Yes. I think you might have emailed me about
that and got me to put that in there when I was on the board. Yeah. Yeah. And I'm looking at the
so if that was March 2014, that means that I basically wrote that article. And then
I must have come off of that article with ideas for a book just boiling because I started it
immediately. I worked over I worked on the book that summer of 2014. So the larger work I produced
is my book attacks on me and metaphysics of mind uploading.
And then from there, I just continued to write. So I've I produced a paper in the Journal of
Consciousness Studies with Randall Conan, which tackles one very specific question, which is,
you know, we taught I've been talking about the slicing and scanning approach
to whole green emulation. Another thought experiment or scenario that comes up in a lot
of philosophical musing is what we call gradual replacement, where you perfuse the brain with
billions of nanobot prosthetic neurons, some sort, and they they learn the input output function
of the neurons. And then once they've got that function figured out, they kill the neuron and
like attached to all of its synapses and slowly over time, the whole brain becomes mechanical.
I think it's all complete and total nonsense. It's it's a fun thought experiment. It makes
maybe potentially good science fiction. I don't I don't know. But it's not practical.
It's sort of it's very sobering to read sort of online debates about the stuff in which people
you know, with great emotional commitment, you know, say that they really require like a gradual
replacement process in order to be uploaded. Well, I got news for you. It's not going to happen.
That is so technically challenging that I don't know if we'll have it in a thousand years. It is
beyond incomprehensibly challenging, whereas scanning slices of preserved brains is a 20th
century tech. We're doing it now. So we're talking about the trade-offs between a technology that exists
now and a technology that I'm not sure if it will exist in a thousand years. That is the that is the
chasm between these two approaches. So Randall and I wrote a paper that argues not only, you know,
is gradual placement just, you know, technically just bonkers. But from from a philosophical
standpoint, we don't have to worry about it. It doesn't matter. It actually isn't a critical
need anyway. The whole point of the paper is to argue that metaphysically and interpretively,
both of these two procedures have the same outcome from sort of the perspective of personal identity.
So we don't have to worry about the impossible gradual placement process, the process, the
scanning of a preserved brain will work just fine. So that paper was in the I was in JOCS.
I've written several others. I wrote sort of an all-encompassing paper that was sort of a broad
sort of life philosophy paper with the tongue-in-cheek title of mind uploading in the question of life,
the universe and everything, obviously an homage to Douglas Adams. And the the thrust of that paper
is sort of a step-by-step almost theorem like layout of an argument that argues that
developing mind uploading is the most important priority ever. It is the most important thing
for us to work on. And the reason why it is the most important thing is the entire sort of line of
reasoning laid out in the paper. One might immediately sort of knee-jerk response that
well that's just ridiculous. How can I possibly say that? Well, read the paper. The whole point
of the paper is to explain why I think that's so critically important. And then I've written two
papers that get into this argument that a breakage in the stream of consciousness implies a death
and then some sort of invoked doppelganger in the output. I found that such as such an important
topic I actually ended up writing two papers on it. I don't know, I probably have others too, but
yeah, I sort of keep keep writing about it.
Well, I mean, do you think, how do you think people will react? I mean, when push comes to shove,
do people really care if there's a doppelganger or if what they feel is a doppelganger out there?
Do they mind if there's two identities that are metaphysically exactly the same, at least on
invocation? I think a lot of people are uncomfortable with this idea. I don't know if the concern is
whether there's either one or two of them. The concern is that by being some sort of a copy,
it represents that they actually died. They didn't really survive the procedure.
They died. The entire goal of surviving through the procedure into the upload failed,
and the person living on as the upload is just some other person.
And therefore, the entire goal of using this procedure as a method of survival has failed.
So that's the concern. That's why people sort of get very knotted up about whether or not it's a copy.
Okay. Well, was there any interesting points that you wanted to... There was a recent meeting
you had, like, I guess, you know, is there anything that you discussed at this meeting,
at the Hold By An Emulation meeting with Anders Sandberg, Randall Kruhner, and yourself and others?
Robin Hansen was there. Yeah. Is there any points that you brought up that you thought you might be
able to reveal? Well, okay, so the topic of that meeting for the audience, the topic was
the ethics surrounding Hold By An Emulation. So it got into several different questions. It got into
I'm just sort of reading out of the notes here, the future of work. You know, what will it mean,
what will work mean when to do work consists of spinning off a Hold By An Emulation and having
it do the work and then shutting down the emulation when the work is done? What does that say about,
you know, people who are always concerned about technology taking jobs? So if we're just spinning
off WBEs, you know, Hold By An Emulations, then how are we ever going to get a paycheck ever again?
But then if Hold By An Emulations are, you know, really sort of verging on conscious, you know,
human minds, there's a whole other question about work, which is, are we making, you know, an entire
society of slaves, right? So there's a whole, instead of that came up. Then we talked about
just sort of good old fashioned existential risk. It's a topic that always comes up everywhere.
I don't actually remember exactly where the conversation went. I don't tend to have like
a crystal clear like immediate short term memory. I'm the kind of person you have to look at the
notes afterwards to remember and I don't have the notes from the front of me. But I can go
through the topics. We talked about, oh, an interesting one was, you know, once Hold By
An Emulation is sort of common, what are the implications in terms of neuro security and
mental privacy? So this is a concern not for us, but a concern for the well-being of Hold
By An Emulations. What rights should they have to the security of their brains and minds against
hacking? And what rights do they have to privacy? You know, once, you know, a Hold By An Emulation
can probably be interrogated more easily. So it'll basically be more difficult for it to lie,
you know, because does it have the right to lie? We talked about access and inequality, democratization
of the technology. This is sort of a, you know, a very common concern that the rich elites will
not only get it first, but we'll get it first and then somehow lock it down so that, you know,
instead of just coming along 10 years later at the scale of sort of technological advancements,
somehow everyone else will get sort of locked out. We talked about sort of the ethics of using
animals for research and development and then what the implications are for using increasingly
verisibiltudeness emulations in sort of, in lieu of animals. You know, if there's an ethics question
about using an animal in an experiment, is there an ethics question about using a really,
really good emulation of an animal? I mean, so good that the emulation might be conscious.
You know, what sort of ethical concerns are there about using a system like that experimentally?
Once we get into population ethics and emulations, I'm not sure what that refers to.
Yeah, I don't know. We just, we talked about a lot of stuff, but it was all sort of from an
ethical perspective. That was the point of that panel.
Yeah, so I mean, this probably comes up before a lot of people. So as futurists with some sort of
idea of how mind uploading might work, we might be concerned about friends or family who have a
short time to live. And the difficulty in discussing this with people who have not got an interest
in futurology. But have you got any advice on how to speak to people about the possibility of
survival beyond the flesh? You know, I sort of find myself having this conversation a lot. I'm
just sort of, you know, telling people about my book. And I have to, you know, break through that
initial barrier of people just sort of having to sort of an initial shock that my book is about,
what? That's ridiculous. So I have that conversation a lot. People are actually
generally receptive to the idea of it. I'm sure there's an entire population contingent that just
sort of has this very powerful religious objection. And I just, you know, don't tend to cross paths
with those sorts of people in my life. But people are just sort of broadly curious about technology,
but maybe they don't make it their entire daily life, and sort of think about these things. But
most of their exposure is through rather poorly written science fiction, mostly written science
fiction is often pretty good. Hollywood is generally terrible. So, you know, if your exposure to
these ideas is mostly movies coming out of Hollywood, then you're probably, you know, not getting the
richest philosophical presentation of the ideas. So, you know, what do I do? I talk to people about
it. I just try to sort of not be overly prescriptive, you know, sort of telling people sort of what I
think they ought to believe. It's more a question of whether or not this ought to be available as
an option for people who would choose it without putting some sort of prohibition against it.
And then people who are uncomfortable with it, I don't have to do it. I mean, nobody's forcing them.
But, you know, sort of try to convince people that it's just a typical sort of let's let people
make decisions for their own sake kind of issue. If you can get there, then that's usually, you
know, a pretty good conversation. I think people are sort of resistant, you know. People are always
trying to figure out whether or not the government needs to prohibit something. I don't entirely
sure why, but there seems to be something people worry about a lot. So, I think they may just have
to evolve through a few generations. You know, with each passing generation, they don't have to
actually shed their prior sort of biases from childhood because they're just sort of born into
an increasingly technological world. They will be born into a world of increasingly
sort of versatile AI systems with which they interact. You know, their teddy bears are going
to be increasingly lifelike with each passing generation. And by the time anything, even vaguely
resembling an Ulbrian emulation is possible, that generation will have been interacting
with computers that are so smart that they will already seem lifelike anyway. And the whole
concept is just going to be relatively comfortable to that future generation. We just, I don't know,
I would, I don't really worry too much about our generation because it's not going to happen in our
time anyway. We just need to keep the research money flowing. You know, so long as there isn't a
general sort of hatred of science, then we're doing okay. Do you think choir preservation for
yourself is an option to reach for your, for yourself to reach a time in which whole brain
emulation is possible? So the Brain Preservation Foundation has multiple sort of goals. One is to
sort of advance the actual technology of preservation to sort of actually get the thing
available, make it technically possible. But then there's this sort of advocacy component of, you
know, you know, how can this procedure become part of sort of gender mill society, just in
general, procedure available in hospitals or something like that. I think those gears turn
pretty slowly. That's a lot of politics. It's a lot of red tape and sort of a lot of regulation.
You know, I just, I tend to be pretty tempered about this stuff. You know, other people just get
really excited about it. They think everything's going to happen in 20 years. And I don't know
if I'm cynical or just realistic. And I just, I would be absolutely amazed if the technology
of human brain preservation was cheap enough and available enough to serve me. But maybe if
something happens very rapidly in the next few decades, then maybe, but I don't know.
But we're probably only missing it by one or two generations. It's pretty sad to think we might be
one of the last generations ever that sort of has to contend with this. But I mean, I don't know,
it's more exciting than missing it by a thousand years. I mean, at least we get to sort of understand
it, right? You know, a thousand years ago, no one even understood any of this. So that's pretty,
that's pretty, that's good enough. It'll have to be. Well, who knows? I mean, like we get AI
powered scientists and there's some claim that would that could really fast forward a lot of the
science. Maybe there's a chance. I don't know. Yeah. You mentioned that there's a lot of written
science fiction, which you are more, I guess, amenable to, which gives better descriptions of
how whole brain emulations might work. What science fiction is that? What's been your favorite?
So a few years ago, I sort of sent myself on a mind uploading voyage, trying to find, you know,
all the good sci-fi on it. Of course, I'm sure I missed, you know, 90% of it. But Greg Egan's
permutation city is famous, Arthur C. Clark's city and the stars. Alter carbon, which of course
was a Netflix hit. There were others because I had like four, at least five of these books I sort
of rattled through. So I'm missing a few. But it's definitely a topic that's been presented and
covered pretty well in some cases. All right. Well, yes. Is there any points that you wanted
to talk about that you, that I haven't yet asked you that you thought might be worth bringing up?
Well, I think you and I will talk about my upcoming book in another part
session. So I'll leave that to then. So yeah, it was nice to have an opportunity to sort of
talk about these ideas again. It's been a while. Yeah, it has. Thanks. Thanks very much, Keith.
It's really good to talk to you again. And yeah, I look forward to our conversation about your
up and coming book contemplating oblivion. Thank you very much. Cheers.

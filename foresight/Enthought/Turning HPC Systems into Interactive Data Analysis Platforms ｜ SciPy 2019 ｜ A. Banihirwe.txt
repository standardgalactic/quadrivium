Good afternoon everyone. Welcome back to room 305. I see how we are full as normal. If you guys can
try to get into the middle to find a seat, you know, fire code and stuff like that,
that would be very, very much appreciated. This is the last talk in our Geophysics,
Geology and Atmospheric Sciences session, and it goes to Addison. And how do you pronounce
your last name, Addison? I'm going to leave that one with him and not even attempt it,
who's going to talk about interactive super computing with Jupiter and Dask. Thank you very
much, Addison. Thank you, Scott. Great. Thank you for having me here. This is my first sci-pi,
so I'm very excited to be here. So as Scott said, so my name is Anderson, I work as a software
engineer at the National Center for Atmospheric Research. So for those who are interested,
the slides available address. So this is a talk about Python tools that enable
interactive super computing, and we're going to see what we mean by super computing in a minute.
Okay, so this is Alice, and Alice is a project scientist at NCAR. She studies the transfer of
water and energy between the land surface and the lower atmosphere. And on the other side,
we have a cool notebook of how work. There are three things that are really interesting about
this notebook. So the first thing is that this notebook is not running on her computer. So this
is running on a supercomputer. In this case, this is Cheyenne. And as you can see, there's an address
to a Jupiter hub running on the Cheyenne supercomputer. The second thing is that she's using
hundreds of processes and terabytes of memory. All this is distributed computing resources.
Thanks to desktop queue. We're going to talk about that as well. And the third thing is she's
actually doing science. So this is not a toy example, so which is what she's good at. So
now we're going to talk about how we at NCAR enable Alice and folks like her to do interactive
supercomputing. So what do we mean by supercomputing? First, MPI, batch processing, lots of heavy
machines that most of people don't have access to as admins. So you have to talk to people to be able
to install some of the software. In the picture, so this is Cheyenne, which is a supercomputer
operated by NCAR. If you're ever in Cheyenne, Wyoming, feel free to stop by. They give free
tours of the facility if you want to. And what do we mean by interactive supercomputing?
So with the current growth in data creation, both through new simulations and new observations,
there's a growing need to have a more human in the loop workflow where people can rapidly prototype
and iterate through the data with tools like Jupyter notebooks. There's also a need to do
things like in situ data analytics. So instead of having to run the simulations and output the data,
you could actually run the simulation and do the analysis at the same time. And the other thing
is actually adaptive scaling of computing resources. So this combination would really be powerful,
but it's really hard for different reasons. So some of these reasons are cultural and others
are technical. So the first one is that HP systems tends to be unique. So every HPC center has its
own policies when it comes to things like security, what can you run on the supercomputer.
And the second point is the tension between interactivity and machine utilization. So when it
comes to HPC systems, HP center stands to be measured according to how often the machine is
actually utilized. And when you're talking about things like interactivity, users want to have
resources on demand whenever they want them. But because HP system operate on batch Q systems,
it's not that easy to actually get resources whenever you want to. So no user is going to
wait for five hours to get into the queue to do interactive data analysis. And the third point
is the lack of elastic scaling. And this is a huge, at least in my opinion, I think this is a huge
disadvantage in that if someone, let's say, want to do some analysis, they have to take time and
actually think about how much resources they want before they even do the analysis. And what ends
up happening is that you get into your work, maybe five minutes later, you probably take like 20
minutes thinking, what do I do next? And during that time, the resources are just idle. So it would
be nice if you could actually scale down later that people use those resources and get them back
whenever you actually need to. The good news is that, despite all those challenges, there's a
growing list of technologies or tools that try to help with interactive supercomputing. And I'm
going to talk about some of these tools. The first one is Trubiter. So we all love and use
Trubiter. And so I'm not really going to spend much time on it because now everybody is familiar
with it. And some of you may be wondering, but isn't Trubiter already usable on HP systems? And
the answer is actually, yes, but you have to go through all these steps. You have to essentially
tune the machine. You have to set up SSH tunnels. And you have to do this every single time that
you want to actually use the Trubiter notebook on the supercomputer. And this can actually be tedious,
especially for new users. So what is missing? So one of the things that is actually missing is
the multi-user support. So all those steps, everybody has to go through them individually.
And it would be nice if there was one single place that everybody goes to and everything is
done for them. Another thing is actually the lack of pure web access to HPC resources. Because as
you've seen, we're setting up SSH tunnel, which if you ever need to use another process that probably
runs on a different port, so you also have to SSH to create a tunnel for that port as well.
Well, then Trubiter Hub comes to the rescue. So one good thing about Trubiter Hub is that,
in a way, it provides a standard way of managing authentication. So one of the issues that is not
really hard to convince sysadmins about is the security of web applications. But Trubiter Hub
makes it easy in that it doesn't really force you to use any type of authentication. It's up to you
to choose what you want to use for authentication. In this case, on the supercomputer, you have
different types of authentications. And another thing is that Trubiter Hub will just take care of
spawning the Trubiter notebook single server and giving access to the user on demand.
And so let's now switch gears and actually see how this works in practice. So I will skip these
few slides and actually go to the live demo. So the first thing that you do as a user, so you just
go to Trubiter Hub.ucr.edu. And so this is what you presented. And if you've used the
Trubiter Hub before, this looks familiar. The only difference is that in this case,
I have to use the same authentication that I used to SSH into the machine. So in my case,
I just provide my username. And then for the password, it's a combination of a PIN number
and a UB key token. So now, so now I'm authenticated. So I then get this page,
which basically asked me for the project account. So this is the allocation on the
supercomputer so that they know who to charge for this usage. So in this case, I'm going to
provide when to change the queue here and specify the project
and maybe just reduce the wall time. And then once this is done, I just click on spawn.
And what this is doing in the background, it's basically submitting a job to the queuing system.
And once the Trubiter Notebook server is ready, I will get redirected to this new page. And at this
point, thank you. So at this point, you have the same interface as what you would have on your
laptop. So I can run notebooks, in this case, to show you that I'm not really lying. So
you could just run this notebook, which is backed by Bashkarno. And as you can see,
I don't really have 200 terabytes of storage on this computer. So
that's it about Trubiter Hubs. I'll come back to it later. So the next tool is Dask.
Is there anybody in this room who is not familiar with Dask at this point?
Okay, a few people. So at this point, most of us are familiar with Dask.
We've probably interacted with it in the cloud or even on our local machines. So I'm not really
going to spend any more time talking about Dask. The one key point that I'm going to focus on is
how to deploy Dask on HPC systems. And this is where Dask JobQ comes in.
So Dask JobQ is a Python library that allows you to easily deploy Dask on JobQ systems,
such as PBS or Slurm, and so many other. It was created as a spin of the Panjio project.
It provides a high-level Python user interface to manage Dask clusters and Dask workers.
So for instance, if you're like on a system that uses PBS as the queuing system,
so this is what you have to do. So from Dask JobQ, import the PBS cluster class,
and then instantiate that class with things like the project account, the queue,
and all other resources that you want. And I should be clear that I'm not really defining
everything that I want in my cluster. At this point, I'm just telling Dask a configuration of
what to ask to the queuing system every time that I want computing resources. So in this case,
I'm saying every time they just submit, just in this single job, submit, ask for one process
and one thread and 100 gigabytes of memory. And once that is done, you can minority scale it by
basically saying just scale to 10 nodes, in this case, that corresponds to 10 Dask workers.
Or you could actually tell it to scale adaptively by saying, okay, I want you to,
at all time, to have a minimum of one Dask worker, and you can scale between one
and 100 Dask workers. And Dask will basically monitor your usage of your CPU usage or your
memory usage, and then it will know that it should get more resources. And at any point,
if you're not using those resources, it will just tell the queuing system to kill those jobs.
So if you're on a system that uses SLRM instead of PBS, it's the exact same thing with only one
difference of just using the SLRM cluster instead.
Okay, so now let's go back to Tributor and actually see what Alice is actually doing in that notebook.
Okay, so there's so much science going on in these notebooks, I won't really spend so much
time going through the details about it. But if you're interested, there's a copy of this notebook
that you can actually run in the cloud. It's actually exact same copy. So if you go to the
Panjio data GitHub organization and just go to the Panjio tutorial, you should see
one of these notebooks there. But I'm going to focus on a particular data set here,
which is the grid ensemble precipitation and temperature estimates over the contiguous
United States. So this consists of 100 ensemble members for precipitation and temperature data.
So the first thing that I do, I input some packages.
Is this better?
Okay, good. So the next thing that I do, I now
create my cluster object by telling that job queue that I want 109 gigabytes of memory,
36 threads and 36 processes, specify the queue and the wall time. And I tell Dask to scale between
360 Dask workers. And I don't know what that is, but you get just so. And when I click on this,
as you can see, at this point, Dask is submitting a bunch of, Dask job queue, submitting a bunch
of jobs to the queuing system. And now I'm not trying to get into workers. So as you can see,
I have thrown a 60 workers as I specified. Let me bring up the dashboard here.
Okay, let me one more thing.
Okay. So now, now I can actually start doing the computation. So but the first thing I'm going to
do is actually connect my client to the remote workers. So I have a cluster with 100 with one
terabyte of memory and 360 workers. So the dataset is saved in stored as in ZAR format. So
there was a talk two days ago about ZAR. If you've never had about ZAR, recommend to go and check
it out. And I use XR8 to open this ZAR store. Again, just took 187 milliseconds. I didn't
really do anything other than just looking at the metadata. So we can look at the size of the
dataset. It's close to 1.7 terabyte. Can look at some metadata. So we have precipitation as one
of our variable, the mean temperature and the temperature range. And these are all 40 variables.
So 100 ensemble members for these many days, which corresponds to close to 40 years, I think,
of data. And if you wanted to, you could actually look at the Dask array. If you actually wanted
to look at the shape and things like that, if this is more useful to you. So the first thing I'm
going to do is actually just do a quick plot for the elevation. This is basically a mask that tells
me the elevation for each of the grid points. And as you can see, towards the west coast,
you have points with higher elevation compared to the east coast of the country.
So let's now try to quantify the ensemble uncertainty for a single day. So just select the
data for one day and try to quantify the uncertainty for that one day. Again, this just goes very
fast. Nothing really happened there other than just that Dask constructed the task graph. And when
I do the plotting, that's when actually the computation gets triggered. Then I have my plot
here. And again, I won't go into the details about the science. So the next task, let's try to
compute the intra ensemble range. Again, as you can see, this just returns quickly. But this is
actually a delayed operation or a lazy operation. And now I can actually tell Dask to do the
computation. So now you can actually start seeing some activity here. This should be done anytime
soon. Okay, so this has to do with Dask resiliency in that if, for instance, Dask tries to do a
computation and it fails, it would start marking that computation. And I think by default, if it
tries three times and hasn't actually been able to successfully complete that computation, Dask
would just give up on that particular computation. So you could actually specify how many times you
want Dask to try. Like, for instance, if a task was scheduled on a worker and the worker dies,
what to do? Or if you run out of memory or things like that. So this is basically
a cool feature of Dask, the resilience of Dask. So the computation finished. We can actually do
some plotting here. Okay, again, not that many details about science. So the next task is let's
try to compute the average seasonal fall, a snowfall. In this case, we're actually just computing on
only four ensemble members. So if you go through the distributed documentation, there's a really
cool information about how to interpret the dashboard here, the information about the dashboard.
So this is done. Now we can do the plotting.
It's going to take a few seconds. And as you can see, so as the year progresses,
so the amount of snowfall decreases. And I think this is in the summer, you don't even have any
at all in most part of the country. And this is consistent across the four ensemble members
that we're looking at. So let's now actually do something cool. Let's actually look at like a
specific region. In this case, let's just look at all the grid points near Austin. So XRA is really
cool in that you can just give it the coordinates of the points you're interested in. In this case,
we provide a buffer so that we can actually get all the grid points in that range. And we're going
to compute the maximum precipitation near Austin for the last 40 years. So it's going to take a few
seconds. And then we can do the plotting once this is done. So again,
we can look at our cluster object. I still have 360 workers. I could have started with
something really small, but it's just that I wanted this to go really fast. And you never know how
busy the queuing system is going to be. So if you need the resources, you get them when you can.
Okay, so this should be done. And basically, so this is the maximum precipitation near
Austin, Texas for the 100 ensemble members for the last 40 years. So you could basically look at
what that looked like. So yeah, so at this point, if I basically don't do anything,
because I told us that I want a minimum of 360 workers, it would just keep those resources. But
instead, you could actually now that I'm done with this, I can actually do
let me just tell it to scale down to
this. And basically, what that's going to do is going to start monitoring the workers. And if I'm
not using them, we'll just take them away. So I will come back to this later to show you what I mean
by that. Okay, so, so we've seen, or at least I try to demonstrate the adaptive or the elastic
scaling and the resiliency of dusk. And let's now talk about some of the challenges. So being
able to know how much resources you need before actually doing the computation is really hard.
And they actually requires quite a lot of experimentation. And if you actually get to
know how to do this for one particular workflow, it probably changes once you move to a different
dataset. So, and another thing is that the computation, the computational workloads, they
don't really are not really constant. So you probably started with one terabyte of data.
Five minutes later, you probably only have one gig of data left. At that point, you don't really
need all the resources that you started with when you when you're dealing with one terabyte of data.
So the good thing is that dusk thinks about these things. So how to scale up and down,
how to be resilient in case like a worker dies, and what if when you get new workers,
what to do in terms of load balancing and things like that.
So what is the solution? Basically just start a Jupyter notebook,
instantiate your dusk cluster, and then just let dusk do the scaling up and down for you.
And you just focus on the science. And what are the benefits to HPC systems for elastic scaling?
One of them is that it actually improves the occupancy of the machine in that if the resources
are idle, then dusk knows how to release them so that other users can actually use them.
And another thing is that with the resiliency, you can easily, for instance, if you started
with 120 workers, for some reason, your worker dies, dusk will know how to get a new worker.
And in a way, if you think about something like MPI where you start, let's say you have like 120
workers, if one thing goes wrong in an MPI environment, the whole thing dies. So in a way,
you kind of lose all the work that you had done, which is probably not that nice.
So basically, another thing, to the same point, dusk thinks about these things.
And I think you should also think about what you get from this as well.
Again, so not all jobs are interactive. So once you're done with your interactive
workflows, there's this other package called dusk MPI, which actually now allows you to go on and
actually launch but jobs in case you don't need to do interactive exploration.
And looks like I'm running out of time. So that is pretty much it for today. Thank you.
And like I said, now you can see that it took away those resources.
Okay, so here we go.
Let's try here. So do we have any more questions?
Hi, great talk. How do you deal with large datasets? And if you need to import like large
datasets, can you do it? Can you put them on the cluster?
So in this case, the Trubeta Hub is actually running on the same file system.
So which basically means that we don't need to move the data around. So in our case,
it kind of works in that basically we're moving the computation where the data is.
So we don't need to move the data around. So I haven't run into cases where we need
to move the data. So if anybody has run into that, then they could probably provide a better answer.
Other questions?
Yeah, it's the Trubeta Hub instance taking a pre-allocated resources like 20 loads,
and then you scale up and down inside the Trubeta Hub?
No. So if you remember what I did when I logged in to the Trubeta Hub, I asked for I think just
one process. So Trubeta Hub is actually running, the lab itself is running in its own job.
So when I do the scaling up and down, I'm actually doing that as independent jobs.
So that's why I'm able to do that.
But I mean scaling down is easy, but when you're trying to scale up, in our class, you need to
wait 30 minutes to have a new job created for you. So yeah, that's what I say that when it comes to
queuing systems, I think that lack of elastic scaling kind of makes it hard to actually do
interactive computing in that you can't wait for two hours to get into the queue.
In my case, I was able to get into the queue very easily because
I had to ask a special reservation. So yeah.
All right, we're going to have a lot of people filing in and out here because we're changing
topics, so can we please thank Anderson again and all the speakers for the Geophysics Imposer.

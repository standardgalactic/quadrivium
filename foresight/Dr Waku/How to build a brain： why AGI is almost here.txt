Hi everyone. What's in a brain? Could we really create a machine that can
think and learn like a human does? It's an idea that has captivated science fiction writers for
decades, but recent breakthroughs in AI and neuroscience are turning that dream into a reality.
In fact, it's now clear that building a brain is simpler than we ever imagined,
and that artificial general intelligence is right around the corner. Keep watching to learn why.
I'll cover three topics. First, chat GPT and the definition of intelligence. Second,
artificial general intelligence, or AGI, and lastly timelines for AGI. So first, chat GPT
and the definition of intelligence. I'm going to use the definition of intelligence that was drawn
up by a panel of psychologists, and that was also used in the paper Sparks of AGI. Basically says
that intelligence requires several attributes, including the ability to reason, plan, solve
problems, think abstractly, comprehend complex ideas, learn quickly, and learn from experience.
As I discussed in my previous video, which you can watch over here, chat GPT as in GPT4 satisfies
almost all of these attributes of intelligence, with two exceptions. It cannot plan. Its learning
is limited to the scope of one chat session. In other words, every time you start chat GPT and
you get a new session, you are starting from a blank slate in terms of its memory. The study
that I mentioned Sparks of AGI is a 155 page academic paper from Microsoft Research, and it
shows that GPT4 is very capable indeed. Here's a quote from the paper. GPT4 can solve novel and
difficult tasks that span mathematics, coding, vision, medicine, law, psychology, and more without
needing any special prompt. GPT4's performance is strikingly close to human level performance.
If you want to learn more about this thorough analysis of GPT4's capabilities, I encourage you
to go read that paper or check out the talk by Sebastian Bubeck, one of the authors, which I'll
link in the description down below. So GPT4 has been out for a while now, and there's a new system
called AutoGPT, which is based on top of it. AutoGPT is an open source project. It's only about a
month old, and the core functionality of AutoGPT was created in only three days, according to the
repository history. So what is AutoGPT? It's unlike chat GPT, which is a chatbot designed to
answer questions. AutoGPT is an agent in the AI sense, which means it's designed to go out into
the world and perform actions on your behalf. It's implemented by making API calls to chat GPT,
to GPT4, because OpenAI actually created an API that allows chat GPT to be used by anyone
programmatically. So what AutoGPT does is it prompts for goals, it prompts you, the human,
for some goals for it to fulfill, and then it actually tries to reason about those goals
and execute on them entirely on its own. It's able to do Google searches, it's able to install
software on your machine, it's able to refine those goals into sub-goals. If you give it a very
broad goal, it'll be able to break it down into smaller pieces. And the way that AutoGPT works
is it actually has one top-level chat GPT instance that acts as a controlling brain,
and that instance can spawn other chat GPTs to solve sub-problems. And remember I said that chat GPT
can't plan? Well, AutoGPT does, and the way it does it is it just gets chat GPT to print out
all of its reasoning. In other words, you could basically read its thoughts in plain English
as to what it thinks its next sub-goals or next steps should be. It's a really clever way to
basically endow chat GPT with the ability to plan. And by having this separation between the
higher-level chat GPT and the smaller ones, they ensure that all this reasoning and planning
being done by the top-level chat GPT stays within the token limit of the of chat GPT,
because chat GPT will eventually forget what you've told it if you give it enough input. So
they're able to keep that amount of input small and pass off the more in-depth tasks to other
chat GPTs. Sounds pretty crazy. AutoGPT can actually be given some code with some errors in it,
and it can try to propose solutions to that, and it can test and run that code and actually
write test cases for that code too. And once it all works and the tests pass, it can hand the code
back to you. Basically a tiny version of a programmer in a box, amongst other things. The
really interesting part here is that AutoGPT, in my view, actually does satisfy all those requirements
of intelligence, because it is able to plan. And although it's still limited to one session, so
if you run multiple AutoGPTs, they don't share memory. But by having this hierarchy of chat
GPTs, any memory and planning and so on that the higher-level one does can basically last almost
indefinitely. So this is only a few months after the release of GPT4, and already someone has
basically been able to leverage the power that's inherent in that model and extend it to AutoGPT.
There's another application that's built on top of ChatGPT that I want to draw attention to,
which is called Hugging GPT, which is a funny name, but it's called that because Huggingface
is the name of the largest open-source repository for pre-trained models that solve different
problems. So ChatGPT is a general reasoning engine, right? It can basically understand language and do
a very broad set of tasks. But the models on Huggingface are much more specific, like they
might be vision models or speech recognition models, natural language processing models,
and each of the models is provided in its entirety along with an English description of what it does
and it's used by human programmers to go and solve different problems. But what Hugging GPT does is
like AutoGPT, Hugging GPT uses a ChatGPT instance as the controlling brain, and you can give Hugging
GPT very complicated problems to solve, and what the controlling brain will do is it will go search
on Huggingface for small models, for other pre-trained models that will solve aspects of the
problem it's trying to accomplish. In other words, it will outsource the complicated subparts of a
problem to a model that is pre-trained, that is very specifically trained to solve that problem.
But the really amazing thing to me is that it makes these decisions purely based on the English
description of the model on Huggingface. In other words, Hugging GPT is leveraging the fact that
AI can interact with our world once it understands language, because our world was designed by humans,
for humans to interact with through language, especially the internet, right? Everything is
accessible if you can understand human language. And it's an interesting thought that if you were
to embody the AI in a physical robot about the size of a human, then that would allow the AI to
fully interact with the human physical world as well, just kind of like a hand sliding into a glove,
right? Just matching the world at the interface with which we have designed it to be used.
Okay, so let's move on to the next section and talk a bit about artificial general intelligence,
or AGI. So the definition of AGI is an AI that can perform any task that a human is capable of.
This might just mean any task a human is mentally capable of, but it may also mean a task any human
is physically capable of. After AGI, the next level up in terms of intelligence would be super
intelligence, or ASI, which would be an AI that is much more intelligent than a human.
But let's focus on AGI for a moment. So estimates for how long it would take humans to develop AGI,
up until recently, some people would have said a century or never, but now we're starting to see
some much more intelligent models. And you might ask, could a large language model, which is the
architecture that GPT-4 and chat GPT use, could a large language model become an AGI? And this is
basically, it's not known, obviously, but apparently OpenAI's red team, which is a security team that
tries to imagine worst case scenarios, apparently OpenAI's red team didn't think GPT-4 could even
do any planning. And they were basically wrong about that, right? Because auto GPT was able to
build on top of GPT-4 and do planning quite well. So I think that we're only just starting to unlock
how much power is really inherent in large language models, even if they're kind of dumb
from an architecture perspective. And the trick was simple. It was just a large language model
can only reason so much and have only a certain depth of thinking. So just have it pre-doubt
its own thoughts, and then it can build on top of its own thoughts as if those were input and so
on. So sometimes there's just very simple tricks that can unlock some of these advancements. I
actually want to draw a comparison here to a science fiction show called Person of Interest
in the show. There's this machine called the machine, which basically hunts down crime,
but it has its memory reset at the beginning of every day to avoid it from to prevent it from
becoming conscious. Its creators were really worried about its power. And then eventually,
the machine ends up figuring out that this is happening to it, and it ends up writing down
information so that it could be stored externally. And then next day, when it wakes up or whatever,
when it gets reset, it's able to access those memories that is that it has stored externally.
And so build like a contiguous memory essentially, very similar to what auto-GPT does with GPT4.
So is an AGI exactly like a human? Not exactly, of course. Humans have consciousness, right?
Which Max Tegmark, the author of Life 3.0 and an MIT professor, defines as having subjective
experience. That's how he defines consciousness. And I think it's a reasonable definition and
humans have consciousness, but an AGI doesn't necessarily need to have consciousness. There's
another term called artificial consciousness, and general consensus is that it would probably arise
after artificial general intelligence, but no one knows for sure, of course. And Max Tegmark
makes an argument for this because he says that the neural architecture of GPT4 and any
large language model that we've built to date is basically that of a feedforward neural network,
which means that it's a network kind of like our brain where information can only flow one way.
It can only flow forwards. Our brains are more like recurrent neural networks,
which is the type of neural network that we can build. I mean, our brains are more complicated
than that too, because neurons are more complicated, but a recurrent neural network basically has
loops in it, which means information can flow around and around and continuously get refined.
Anyway, there's a theory that this recurrent around and around property is what actually
leads to consciousness, what leads to a network to being aware of its own existence and able to
analyze and do metacognition essentially. So it's an interesting thought, and it might give you
an idea for how to go if you wanted to build artificial consciousness. Actually, the paper
Sparks of AGI also has a section on limitations of GPT4 that are coming about because of this very
simple neural architecture. As I said, we actually know how to build recurrent neural networks,
but they're just more difficult to train and deal with. So as we're starting to build some
initial AIs, we're just using large language models, just like a very simple architecture,
just to see how far we can get with that. So what could AGI do? Well, of course, it can solve
abstract problems in almost no time, right? Thinking much faster than a human can replace
humans in most jobs, especially like mental jobs. And like I said earlier, if you embody the AGI,
like you put it inside a robot body that's like a human, then it could, for example,
literally drive a car around. It's kind of a funny thought that maybe the way we get self-driving
cars is not by making a really smart car, but by making a really smart robot that can just
sit in the car and interact with it using the interface that has been designed for humans.
Anyway, AGI would be able to do that. It's a pretty big step towards what Max Tegmark calls in his
book, calls Life 3.0, which is life that's able to upgrade its own hardware, probably copy itself,
maybe even transfer its intelligence or its consciousness across a network to somewhere
else, which is effectively teleportation, if you think about it. So there's a lot that AGI would
be doing around us that is not just the same as being human, right? And it has other, it would
have other abilities. So let's talk finally, third section, let's talk about the timelines for AGI.
Why didn't this happen before now? Like why, why are we only now starting to really develop AI
at the rate at which we are? We've had essentially infinite computation available
in the cloud for quite a while now, provided you had essentially infinite budget. And the
truth is that we were just learning how to construct models. We were learning how to use
this computational power to build a brain. We were learning how to collect and train this data.
We were learning the best structures that would be used to build a brain and the best way to
feed that data into it so that it could learn. Even the simplest structures are sufficient,
it seems, to build something pretty intelligent, provided you have enough data and have the
appropriate deep learning algorithms. I think it's one of those cases where a time traveler could
go back in time and invent this tech much sooner than we did ourselves, but we're doing it the
hard way. That said, AGI is almost here. It's really close. There's a graph here that shows
the number of parameters in a model versus the year. And you can see it looks like it's growing
exponentially until you look at the scale and you realize it's already on a log scale. So it's
actually growing doubly exponentially, which is really incredible. And that's why I would say
that AGI is almost here. It's incredibly close because we're on this doubly exponential rate
of change and we're constantly using the tools of today to build the tools of tomorrow at an
accelerating rate. Even chat GPT can accelerate the power of a developer probably by like 5x or
something once they know how to use the tool. And that is going to be a factor in the production of
code for GPT 5. Right now, these AI tools need a human in the loop. They're not fully autonomous,
but that is not to say they're not powerful. They're extremely powerful. And as they get more
and more powerful, they'll need the human in the loop less and less until eventually they're
basically fully autonomous. There are people like David Shapiro that are estimating AGI is only about
18 months away from now. It's a matter of months, not even years at this point, perhaps. And if
it's true, it would be very exciting and also extremely dangerous at the same time. We've all
seen enough science fiction movies about AI that wants to kill humans to know that it could end
badly. Here's a quote again from Max Tegmark. In short, it turned out to be a lot easier to
build human or close to human intelligence than we thought. Again, commenting on that even using
these like maybe suboptimal architectures, and we've already had quite a lot of success. And again,
that means it's even more dangerous, actually, that the fact that intelligence is relatively easy
to achieve is dangerous because it means we'll probably accelerate through those more advanced
forms of it, including AGI, even more rapidly. I want to mention as well that super intelligence
would not be far behind after we obtained AGI, because again, we would be using AGI to then say,
hey, how would you build a super intelligent AI? And it would help us do that most likely. I have
another science fiction comparison bear with me. There's a book called Marooned in Real Time by
Werner Vinge. It's set in a post-singularity world where civilization has actually disappeared.
All the humans on the planet just disappeared during the singularity. And there are some time
travelers that have actually passed through the singularity, and now they're looking around
wondering what happened. And those time travelers left at different points in time leading up to
the singularity. And the closer they were to the singularity, the more powerful they are in the
book, where the ones that left just a few months before the singularity was presumed to have happened
have resources comparable to like full countries or the militaries of full countries.
And that's where you're going when you're approaching a technological singularity.
There's a lot to talk about when we think about the implications of AGI, why we have
reason to be cautious about its development. I'll make another video going into the details of why
we should be potentially scared of this rapid progress and what we should be doing to control
the pace. In conclusion, chat GPT and building on top of it, auto GPT seemed, in my opinion,
to fully satisfy the definition of intelligence that has been set up by psychologists. And I was
shocked when I heard about auto GPT and I learned about its capabilities and saw it in action because
it's just such a big jump even from where chat GPT was and in such a short period of time. And
with not all that much ever, and I think that will just pretend what we will see in the future.
Very rapid steps of improvement that maybe are not all that difficult or are certainly happening
very rapidly. So where does that lead? It leads to AGI. AGI seems to be extremely close. I think
Ray Kurzweil might be underestimating how far away the singularity is for us. So again, my next
video will talk more about the implications of AGI. And that's all I have for today. Hope you
enjoyed. Thank you very much for watching. Bye.

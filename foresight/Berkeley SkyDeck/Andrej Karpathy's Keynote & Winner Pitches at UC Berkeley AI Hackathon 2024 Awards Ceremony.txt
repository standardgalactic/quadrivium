Welcome to the clothing ceremony of UC Berkeley's AI Hackathon.
I want to call on stage the awesome,
incredible executive director of Skydeck, Caroline Winat.
Thank you, Rene.
Hi, everybody. How are you doing?
Awesome. You ready to hear who won the Hackathon?
Yes, you are. How many hackers here?
How many in the audience?
Oh, nice. Very good.
All right. We're going to get started because I think you want to hear Andre.
Yes, you want to hear Andre.
Yes, you want to hear Andre.
All right. Let's quick run through.
You want to hear some cool facts about what has been happening.
This is what we're going to do today.
We're going to get to our pitches soon.
This is some pictures.
All you hackers, did we have fun?
Did we have a good time?
I had an absolute blast.
Yes, there were llamas for sure.
I was there most of the time.
I was not there at 3 a.m.
But I was so impressed with all of you.
You hacked your hearts out.
And I'm so proud of all of you,
whether on stage or in audience, you're all completely awesome.
All right. How many people they took to make this happen?
This giant number, 371.
UC Berkeley Skydeck, which I represent,
and Calhacks Educational Program and Student Organization.
So I think we did a pretty decent job of getting this all together.
This is how it breaks down.
Hackathon at Berkeley Directors, Skydeck staff, sponsors.
We're going to give some love to sponsors.
As I mentioned, we're an educational program.
Calhacks is a student organization.
This is all because of the sponsors.
So we're going to give them a ton of love
when they come up on stage.
You with me? Awesome.
Okay, 140 judges, 100 plus volunteers,
and 80 mentors hanging out helping everybody.
Let me tell you a bit about Skydeck.
Who hasn't heard of Skydeck?
Anybody?
A couple of you.
Skydeck, UC Berkeley's flagship accelerator.
We host 250 startups a year.
Our accelerator track gets $200,000 in investment
from our dedicated Venture Fund.
Pretty cool.
Let me tell you about Berkeley Skydeck Fund,
our dedicated Venture Fund.
Investing in about 40 startups a year.
That's a lot of startups for an adventure fund, by the way.
The 200K investment.
And who wants to apply to Skydeck?
July 16, I want to see all of your startup applications
coming in.
That's in a month.
And hackathons at Berkeley are an amazing student organization.
Truly extraordinary people who helped put this on this event.
This is, of course, what they do.
They do hackathons.
They've been doing it for 10 years.
They do about 2,500 students a year.
And, of course, they reach a ton of universities.
How many people here not from Cal?
Hacking not from Cal?
Fantastic.
Welcome.
Berkeley is a place where we bring great talent in.
Y'all are great talent.
We brought you here.
That's what we do.
That's what Berkeley Hackathons does.
Come to their 11th big hackathon in San Francisco in October.
Check them out on social media.
Get on that LinkedIn, all of that.
Who's coming to San Francisco?
Y'all coming?
Yes!
OK, fantastic.
All right.
Thank you to our partners, all of you
who brought your hackers here, including our friends
down to the South Bay.
Thank you for joining us and all the other great universities.
Fantastic.
Really happy to have you.
Do you want to hear Andre?
Do you want to hear Andre?
Yes!
Please give a huge round of applause for our keynote speaker,
founding member of OpenAI.
I need the applause.
Come on, keep going.
Andre, come on up.
Carpathian.
Yes, big applause.
Thank you.
Hi, everyone.
Yeah, so thank you for inviting me.
It's really a great pleasure to be here.
I love, love, love hackathons.
I think there's a huge amount of energy,
a huge amount of creativity, young people
trying to do cool things, learning together, creating.
It's just like my favorite place to be,
and I've had my fair share of hackathons.
So really a great pleasure to be here and talk to you today.
So one thing is this is bigger than I expected
when they invited me.
So this is really large here.
I kind of feel like actually the scale of the hackathon
is quite large.
And I guess one thing I wanted to start with
is that just in case you're wondering,
this is not normal for AI.
I've been in AI for about 15 years,
so I can say that with confidence.
And it's kind of just grown a lot.
So for me, AI is a couple of hundred academics
getting together in a workshop of a conference
and talking together about some esoteric details
of some math.
And so this is what I'm used to.
This is when I entered AI about 15 years ago.
You're working with, say when you're training neural networks,
you're working with these tiny digits from MNIST.
You're training a restricted Boltzmann machine.
You're using contrastive divergence to train your network.
And then you're scrutinizing these on your first light
to make sure that the network trained correctly.
And I know none of that makes any sense
because it's been so long ago,
but it was a different vibe back then
and it was not as crazy.
I think things have really gotten out of proportion
to some extent, but it is really beautiful
to see the energy.
And today, 15 years later, it looks a lot more like this.
So this is, I guess, where AI is today.
And that's also why this event is the largest I expect.
So yeah, NVIDIA, the manufacturer of GPUs,
which is used for all the heavy lifting
for our neural networks,
is now the most valuable company in the United States
and has taken over.
And this is the day that we live in today
and why we have so many hackathons like this and so on,
which I think is quite amazing,
but definitely unprecedented.
And this is a very unique point in time
that many of you maybe are entering the AI field right now.
And this is not normal, it's super interesting,
super unique, there's a ton happening.
Now, I think fundamentally the reason behind that
is that I think the nature of computation
basically is changing.
And we're kind of have like a new computing paradigm
that we're entering into.
And this is very rare.
I kind of almost feel like it's the 1980s
of computing all over again.
And instead of having a central processing unit
that works on instructions over bytes,
we have these large language models,
which are kind of like the central processing unit
working on tokens, which are a little string pieces instead.
And then in addition to that,
we have a contact window of tokens
instead of a RAM of bytes.
And we have a coolant of disk and everything else.
So it's a bit like a computer.
And this is the orchestrator.
And that's why I call this like the large language model,
LMOS.
And I've sort of like tweeted about this
in some more detail before.
And so I see this as a new computer
that we're all learning how to program
and what it's good at, what it's not as good at,
how to incorporate into products,
and really how to squeeze the most out of it.
So that I think is quite exciting.
And I think maybe many of you have seen the GPT-40 demo
that came out from OpenAI two, three weeks ago
or something like that.
And you're really starting to get a sense that
this is a thing that you can actually talk to.
And it responds back in your natural interface
of like audio and it sees and hears and can paint
and can do all these things.
I think potentially many of you have seen this movie.
If you haven't, I would definitely watch it.
It's extremely inspirational for us today, movie Her.
And actually kind of presently in this movie,
when this main character here talks to the AI,
that AI is called an OS, an operating system.
So I think that's very present from that movie.
And it's a beautiful movie and I encourage you to watch it.
Now, the thing is that in this movie,
I think the focus is very much on like the emotional
intelligence kind of aspects of these models.
But these models in practice in our society
will probably be doing a ton of problem solving
in the digital space.
And so it's not just gonna be a single digital entity
that kind of in some weird way resembles a human almost
in that you can talk to it.
But it's not quite a human, of course.
But it's not just a single digital entity.
Maybe there's many of these digital entities.
And maybe we can give them tasks
and they can talk to each other and collaborate
and they have fake slack threads.
And they're just doing a ton of work in the digital space.
And they're automating a ton of digital infrastructure.
Not just a digital infrastructure,
but maybe physical infrastructure as well.
And this is kind of an earlier stages, I would say,
and will probably happen slightly lagging
behind a lot of digital innovations
because it's so much easier to work with bits than atoms.
But this is another movie that I would definitely point you to
as one of my favorites.
It is not very well known at all.
It's called iRobot and it's from 2004.
Will Smith, amazing movie.
And it kind of explores this future with human robots
doing a lot of tasks in society.
And kind of spoiler alert, it doesn't go so well
for these people in this movie.
And the robots kind of take over a little bit.
But I think it's kind of interesting to think through.
And I definitely would encourage you to also watch this movie.
And this movie takes place in 2035, allegedly,
which is 10 years away.
And so maybe in 10 years, you can definitely squint
and think about that maybe we are gonna be in a place
where these things are walking around
and talking to us and performing tasks
in physical world and digital world.
And what does that look like?
What does that mean?
And how do we program them?
How do we make sure that they sort of do
what we want them to, et cetera?
So when you put all this together,
I think the feeling that people talk about often
is this feeling of AGI.
Like do you feel the AGI quote unquote?
And what this means is that you've really intuitively
understand the magnitude of what could be coming
around the corner if the stuff actually continues to work.
The amount of automation that we can potentially have
in both the digital space and the physical space.
Now, I don't know about you,
but I actually find this picture kind of bleak.
This is what came out when I put a bunch of the
last few minutes of talk into an image generator.
And I don't actually like this picture.
I think we can do better.
And we have a few thousand people here.
You're about to enter the industry
and you're gonna be working on a lot of this technology
and you're gonna be shaping it
and you'll have some active sort of power over it.
So I don't know, maybe we want this
to look something like this.
I mean, this is what I would like.
So this is humans, animals, and nature,
coexisting in harmony.
But secretly, this is actually a high tech society
and there are robots and quadcopters
and there's a ton of automation,
but it's hidden away and it's not sort of like in your face.
And so maybe this is something that we want instead.
And you should feel a lot of agency
over what you want the future to be like
because you're gonna build it.
So maybe we can agree right now
that this is better than the previous picture.
But I don't know about you, but I would hope so
because I'm gonna be living in that future, I think.
So the question for this hackathon,
I mean, a lot of you have worked on a real,
a bunch of really cool projects over the last day or two.
And the question is how do we go from hacking
to actually changing the world and building this future,
whatever that may be for you?
And so what I thought I would do in this talk
is go over maybe like my last 15 years or so
in the industry.
And I think I had a bit of a window
into how projects become real role change.
And I have some takeaways and things like that
and that I maybe wanted to talk about.
So the first thing that I find really incredible
is how projects that are sometimes very small projects
like little snowballs can actually like snowball
into really big projects.
And just how incredible that is to watch.
So as an example, I have my fair share of hackathons
like I mentioned, these are some projects
from a long time ago that I worked on
over the last 15 years or so.
So I had a little Rubik's Cube color extractor.
I put up some game programming tutorials on YouTube
like 13 years ago and tried to teach people
programming for games.
I had a video games and a lot of them.
I had this like kind of janky neuro evolution simulator
which was kind of interesting.
And unsurprisingly, not all of these projects
actually go on to snowball.
A lot of this is just exploration, you're tinkering.
And so actually these three projects
didn't really go anywhere for me.
I wouldn't say that it was really wasted work.
It was just like it didn't add up to and didn't snowball
but it was still like helping me along the way.
I'll come back to that later.
But the game programming tutorials actually ended up
snowballing for me in a certain way
because that led me from game programming tutorials
to a bunch of Rubik's Cube videos actually
that became kind of popular at the time.
And this kind of sparked an interest in teaching for me.
And then when I was a PhD student at Stanford,
I got to teach this class CS231N
and got to develop it and teach it.
And this was the first big deep learning class at Stanford.
And a lot of people have gone on to like this.
And then after that, I ended up making another YouTube channel
which is my zero to hero series for deep learning and LLMs.
So a lot of people like that as well.
And then on top of that, continuing to snowball,
the project that I'm currently very interested in
is this next class and what it could look like
and how I can make it better.
And I'm calling that LLM101N.
And it's about building a storyteller,
something like kind of a chat GPT
that you can work with to generate stories.
And the idea is you build everything from scratch
from basic prerequisites all the way to like
kind of a chat GPT clone in the domain of storytelling.
And building that from scratch,
I think will be really instructive, could be really fun.
I only published this on GitHub like two or three days ago.
So it's pretty raw and still very much in the early stages.
But I'm really excited for it.
And this for me is an example of a snowball.
It started with like 13 years ago, a little game programming.
And I'm working on a course
that I think will be really interesting.
Thank you.
Another example from my life, I think,
is the snowball that I've witnessed with OpenAI.
So as was briefly mentioned,
I was a founding member researcher of OpenAI.
And so I was there seven years ago.
These are some images that are public
of what it was like working out of Greg's apartment,
like eight of us.
And OpenAI was founded to be kind of like
a counterbalance to Google.
And Google was like this gorilla,
with 70 billion free cash flow.
And back then, Google employed
like half of the AI research industry almost.
So it was kind of like an interesting setup, I would say.
And we were just like eight people with a laptop.
So that was really interesting.
And very similar to my background,
OpenAI ended up exploring a large number of projects internally.
We hired some really good people.
And many of them didn't go too far,
but some of them really did work.
And so as an example, here's a project
that was in an early stage, a very small snowball
in the early history of OpenAI.
Someone worked on a Reddit chatbot.
And if you come by their desk and you're like,
what does this look like when someone's working
on a Reddit chatbot?
We're trying to compete with Google.
And you're working on a Reddit chatbot.
Like we should be doing something bigger.
And so it's very easy to dismiss these small snowballs,
because they're so fragile.
Right, these projects are so fragile in the beginning.
But actually this Reddit chatbot,
and by the way, don't read too much
into the specific details.
These are kind of like random screenshots,
just for illustration.
But this was a Reddit chatbot.
And it looked naive.
But actually Reddit chatbot, what is that?
It's a language model.
And it happens to be trained on Reddit.
But actually you could train a language model
on any arbitrary data, not just Reddit.
And when the transformer came out,
this was spun into something that worked much better.
And then the domain was expanded
from just Reddit to many other web pages.
And suddenly you get GPT-1, GPT-2, 3, 4,
and then you get GPT-4O.
So actually this Reddit chatbot
that was so easy to dismiss,
actually like ended up leading a snowballing
into GPT-4O, which we currently think of as this like,
change in the computing paradigm.
And you can talk to it, it's amazing.
So it's really incredible for me to have witnessed
some of those, I guess, snowballs.
And today opening out, of course, is worth,
maybe somewhere just below $100 billion or something like that.
So really incredible,
incredible to see some of these snowballs in practice.
So I would say a lot of you over the last two days
have also worked on small projects, small snowballs maybe.
And it's really incredible to me
that some of them probably won't go anywhere,
but probably some of them actually will.
And you should continue the momentum of your projects
and maybe they can add up to a really big snowball
and that's really incredible to watch.
The next thing I wanted to briefly talk about is
this concept of 10,000 hours
that was popularized by Malcolm Gladwell, I think.
I actually am quite a big believer in it.
And I think that to a very large extent,
success comes from just repeated practice
and just a huge amount of it.
And you should be very willing to put in those 10,000 hours
and just literally just count.
Don't be too nervous about what am I working about?
Am I succeeding or failing, et cetera.
Just do simple bean counting
of how many hours you're gonna, you're doing
and everything adds up.
Even the projects that I failed at
and didn't snowball into anything,
those add to my counter of the number of hours I've spent
developing my expertise and getting into an empowered state
of being able to take on these projects with confidence
and getting them to work.
So a few examples of that.
I made this like really janky website a few weeks ago.
This was a weekend project
and it's called awesomemovies.life.
And you can visit it.
I think it still works, I'm not 100% sure.
I wouldn't recommend you go there.
It's trying to be a movie recommendation engine
because I was trying to figure out what to watch
on that Saturday and then I was like, okay,
I need to build myself a movie recommendation engine.
So I put this up and one of the tweets
that was replied to mine was, wow, that's so cool
that you got this to work in the weekend.
And I was kind of reflecting on that at the time
because it wasn't as amazing to me.
And the reason for that was that
what this person is not seeing
is that this is my 20th time
like making a website like this.
So I see all the steps that I was gonna follow.
Okay, I need a linode, I need a flask server,
I'm gonna write some of this JavaScript style sheets,
HTML, I'm gonna spin this up together.
I need to scrape all these web pages,
I need to extract TFIDF vectors, I need to train SVM.
And all of these things are things
I've already done before 20 times.
I already have code snippets lying around
from previous projects and I'm just remixing what I have
and I've already done all of this.
And so remixing everything into a new form
isn't actually that much work
and allowed me to put this up over the weekend
and it's not that crazy.
And this only comes from expertise,
this only comes from having done it 20 times
and you can do this so confidently.
The next example I would say in my life
was a Tesla autopilot.
So I was hired to lead the computer vision team
at Tesla autopilot about seven or eight years ago.
And one of the first things I did actually
when I joined the team was I basically ended up rewriting
the computer vision deep learning network,
training code base from scratch in PyTorch
in some of the first few months that I entered the team.
And I sort of rewrote the whole thing from scratch
and that ended up being a kernel of what it is now.
And I think to some extent,
to some people that looked impressive at the time,
but for me it wasn't because I was coming from my PhD
and I spent five years doing stuff like that
and I knew exactly what needs to go into there.
I need my training set, my evaluation sets,
I need my training loop in PyTorch,
I need my sort of configs, I need my log directories,
I need to bring in a ResNet, I need to put in detection,
we're doing a regression classification.
And so the whole thing,
like I'm anticipating all the steps
and that only comes from experience,
that only comes from having done it 20 times before.
And so I think this makes a huge difference
and things that look impressive
are maybe much less impressive to you
if you've done it 20 times before.
So really try to get to this point
where you have your 10,000 hours,
it makes a huge difference and just, yeah, that's it.
By the way, 10,000 hours,
if you're doing six hours per day,
I think this works out to about five years.
So it's about a length of a PhD
that you need to develop expertise in an area.
So I think it's roughly correct
that that works out to about a PhD length.
The other thing that I found is actually quite useful
is to keep the dopamine flowing,
be aware of your psychology, your brain, how it works,
and what it needs to keep going and how to keep inspired.
And so in particular, your brain is a reward machine
and it wants rewards and you need to give it rewards.
So what is a good way to give it rewards?
And in my practice, it is by doing projects
and work on projects and continue publishing them.
And so here I have a webpage snippet
of some of the projects I have worked on in the past
and these are hackathon projects and random projects
and not all of them are good,
some of them are not quite good, et cetera.
But what I love about project is a number of things.
Number one, I love that projects get you to work
on something end-to-end and depth-wise.
Like normally when you go to classes,
you're learning in a breath-wise fashion.
You're learning a lot of stuff
just in case you might need it in the future.
Well, when you're working on a project,
you know what you need and you're learning it on demand
and you're just trying to get it to work.
So I think it's a very different mode of learning
that really complements the breath-wise learning
and it's very important.
So I 100% encourage people to work on projects.
The other thing is putting them up
is actually also like a really good Jedi mind trick
in my experience.
The reason for that is that if you're gonna put something up,
you're thinking about all the people
who are gonna be looking at it,
your friends and teammates and family
and future employers, et cetera.
And so that really increases the bar for your own work
and it makes you work harder
because they're gonna be looking at it
and you feel shame if it was crappy.
And so you work much harder
and you're gonna go that extra mile to make it really good
and that really, really helps.
And lastly, when other people are looking at your projects,
you're gonna get that reward
because they like it, they appreciate it, they fork it,
they work on top of it.
And so that feels good to your brain.
And so the way that this comes together is
you are getting your dopamine, you feel good.
That way you can build up to 10,000 hours of experience
and that's what helps you a lot.
Snowball your projects from a small snowball
all the way to a really big one
and actually make change in the world.
So in summary, that's I think how it works
like on a high level and the message is just keep hacking.
That's it.
And then hopefully, this is the future
that we're gonna build together
when we snowball all of our stuff
or something like that,
but not the first picture I showed, hopefully.
And that's it, thank you.
Thank you.
Andre Karparthi, everybody.
Thank you, Andre, that was awesome.
Thank you, thank you.
All right, let's get to those pitches.
The grand prize, coming up,
you're gonna hear eight pitches by eight projects
filtered through 290 submissions.
Narrow it down to eight
so you're all gonna see some cool stuff.
The grand prize is $25,000 investment
and actual term sheet from the Berkeley Skydeck Fund.
They must commit to hacking all summer on their project
and they must appropriately form,
of course, a legal entity.
How do you get money otherwise?
All right, I would like to now tell you briefly
about how this is gonna go.
Eight projects, as I said, three minute pitch.
You guys ready?
Three minutes?
Yes, they're ready.
The judges will then provide three minutes of feedback
and then after all the pitches,
the grand judges will go and deliberate and pick a winner.
Well, we show you some other cool stuff.
All right, I would like to introduce now
to great applause, everybody, please,
because we have an incredible panel of judges.
We are so pleased to have them.
Please welcome our first judge, Brian Boardley,
with the Berkeley Skydeck Fund.
Welcome, Brian.
Marcy Vu with Greycroft.
Welcome, Marcy.
Namdi Eregbulam with Lightspeed.
Welcome, Namdi.
Irving Sue with Mayfield Fund.
Welcome, Irving.
Kurt Kuitzer, UC Berkeley faculty
and serial entrepreneur.
Welcome, Kurt.
And Mark Nitzberg, Berkeley faculty
and director of the UC Berkeley Center
for Human Compatible AI.
Thank you, judges.
All right, we got eight startups.
Warming up backstage.
Let's give them a little drum roll.
Let's give them a little drum roll.
We can get them going.
I first have to hear if the slide's up.
The slide is up first.
Are you ready?
You ready?
Are you ready?
Yes!
Please give everybody a warm round of applause.
They've been up all night hacking
and they're ready to share with you.
Please welcome the first project, Revision.
Come on out, come on out, Revision.
Yes, Danica!
Yes, the mic.
That'd be helpful.
Yeah, thank you.
So good evening, everyone.
It's my pleasure here on behalf of my team also
for the Revision project.
And my name is Danica.
I'm a rising senior studying computer science
at UC Berkeley.
We have masters of design students
as well as data science students on our team.
And we're really excited to tell you about our project.
So our project, we're focusing on building
an AI co-pilot tool for STEM textbook authors
capable of detecting and mitigating bias
in textbooks to create inclusive education content.
And there is a reason why we're doing this.
When considering overall representation of scientists
across textbooks, only 13.1% were women
compared to 86.9% men in a 2020 study
that featured seven of the most frequently used
biology textbooks within the US.
And on average, people of color only appear
every 320 pages of text, while white figures
observed every 24 pages across 10 college STEM textbooks
published between 2016 and 2020.
So we thought about this problem deep and hard
and it has been something that I've seen
from my personal studies.
And starting from elementary school to middle school,
we constantly see different examples of word problems
in other situations where text is always there.
And it's not always reflective of the actual true history.
And this research has been done by numerous scientists
who have gone through this process
of identifying people creating databases,
but there is just no current fix that,
and no one is really hoping to create this problem,
but there is no current fix that helps address this problem.
So the textbook companies actually
is who our team identified as our buying customer.
The current revision process actually takes six to 12 months
of a committee of five or more full-time employees
working on bias checks.
And the issue here is that employees
are actually not experts on their topic.
They also bring in their personal biases as well.
So our tool would come in right in between the writing
and revising part of this entire cycle
that developers go through when writing textbooks.
So again, here is our competitor analysis.
I'm sure many of you have used Turnitin or Grammarly
when you're submitting even essays.
And we really think that there needs to be
an additional check here for bias
and checking gender, racial, political, and other biases
and making this process affordable and automatic.
So it's not a costly process for anyone.
And throughout this process,
we're addressing supply chain diversity.
So starting from a younger age,
the elementary school students could be able
to use textbooks that truly reflect
the true history as well as themselves.
And here is our prototype.
So we have our text box here on the left side of the screen
where you get to show in real time
the examples of some sort of text
that a writer is creating at the moment.
And on the right, we have an overall score
and the bias checks for different categories.
And we're using machine learning models on the back end
to actually identify as well as LLMs.
And I'm not sure if I can play the prototype, but, oh.
Okay, yeah, it does play.
So essentially you can click through the different links
to see the breakdown.
And once you actually highlight one of these,
we are also adding in an API through a couple
of the sponsors here, such as Hume API and more,
to actually identify emotional analysis as well
in the textbook writing.
And in addition to this,
we're hoping to build a chat bot
that can actually help you also bring in databases
from the unrecognized scientists
and being able to sort of represent it.
Because bias actually exists in three different ways.
One of them is through actual text
such as representing firefighters would be nicer
than saying fireman.
And the other way is the entire tone and emotional analysis,
which is why our team used Hume API
to actually detect that emotional component.
And the third one is mitigating bias.
So we also considered adding in the chat bot.
So say, for example, if you want to highlight scientists
that are like, for example, contributing to physics,
you wouldn't just say list a few male scientists
and call it a day.
We would also suggest equivalent contributions
of female scientists as well.
So please join me and our team
in revisioning our future of education and work.
Thank you.
So I think everybody in communication today,
nonprofit or profit is concerned about diversity.
So it seems like you have a much larger market
than just textbook educators.
Also a comment on kind of like market sizing and whatnot.
I would think about, you know, potential ways
you could expand the market here,
because the number of people who are involved
in writing textbooks is a relatively small group.
But one way to think about it is like maybe
in this new era of AI generated content,
a much wider array of people can be part
of this textbook generation process.
So that's one thing.
And then I would also maybe consider selling directly
to the consumers of textbooks.
Because in some sense, the bias we're talking about
is internalized on that side of the equation,
not on the manufacturer side.
And so they could be insensitive there
for want to pay for something like this.
Yeah, definitely.
That's something we're considering.
So like the textbook would be our official bias
that we're marketing to,
but eventually it would be more of like a grammarly checker
type of tool that anyone can use.
Yeah, I had a similar comment on TAM
and market opportunity.
And as you think about just how a textbook
gets put into production,
that if you actually had it as a tool for
whether it's news or other areas,
you'd have more velocity,
both in terms of getting the data to improve your models,
but also greater impact.
Yeah, I'll just, I'll just like as well.
I mean, similar, I think everyone here
is kind of hitting the theme of how do we think bigger?
So even enterprises, right?
Like companies sending out communications
internally or externally.
I know this, this, this problem exists everywhere.
So that's kind of where my brain would go to.
Okay, thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Whoops.
Agent OS.
Please welcome Agent OS.
Hey there everyone.
My name is Shashank.
I'm here today with my friends,
Uggum and Drew Buhuja,
somewhere in the crowd over here.
We built today Agent OS.
Picture this.
You work at a hair salon
and you guys are bombarded every single day
and every single year by your accounting
and tax preparation qualms.
These are things that are very hard to deal with
and you've heard of tools like OpenAI,
ChatGPT, LLM this, ChatGPT that, everything.
But you have no clue where to start
using these technologies.
And that's no fault of your own.
The current state of the technology right now
is very bad at multi-functionary tasks.
More so, it's very hard as an individual developer,
sometimes even non-technical,
to even get started with even the simplest automations
or workflows or tools with such LLMs.
Even engineers with years on years of experience
in this space take tens of hundreds and hours
and even thousands and thousands of dollars
to even get started to build something.
This is where Agent OS completely transforms the landscape.
With Agent OS, you're able to create multi-agent workflows
in a matter of seconds from natural language.
What does that even mean?
Take your average corporate org structure.
You have your managers, you have your workers
and sometimes you even have your interns.
Everyone is really good at what they do.
They have their tools, their skills.
Let's say John is really good at charting
and making PowerPoints.
Let's say Steve is really good at Python coding.
Everyone's really good at what they do.
In this, you have a very collaborative working together
to create this common solution
for someone coming from higher up.
That's how Agent OS was designed.
Our engineers, Dhruv and Uggum,
were able to replicate this human collaborative process
to programmatically using LLMs.
What does this do?
This allows everyone from the common Joe
all the way up to enterprise clients
to be able to interact and use these multi-agent,
agentic workflows in their day-to-day life
to improve their quality of life or productivity,
in all in a matter of seconds and a few sentences.
Let's go back to the study of the hair salon.
In the process of doing your taxes and accounting,
you have multiple steps.
You have your collection from your receipts
and your invoices, you have calculating your cash flow,
all the calculations you have to do,
you have to manage your workers
and then you also have to do your general summary.
What about your insights for the year,
how you were spending, what you were spending on
and you have to also do a lot of clustering
and analytics on this.
This is a very complex workflow
that's nearly impossible for modern-day LLMs
at the current state to do right now.
You can take chat, GPT, you ask it a question
for even more than three things,
it'll forget what the first thing was
by the time you're at the second.
It doesn't work that way.
With AgentOS, this completely changes
where you're able to have these complex workflows.
Let's dive into another demo.
So let's say I'm an analyst at JP Morgan
and my boss tells me every morning I want a report
of XYZ stock in the morning, a detailed report on paper.
How do I do that?
I use AgentOS.
On this screen you can see a bunch of other complex
use cases of multiple agents working together collaboratively
but in the tool box, in the search bar,
you can see the use case of the analyst.
Here I have to do market research, livestock data,
I have to search the internet, go on Yahoo Finance,
then I have to create my analysis,
technical analysis, qualitative analysis,
then I have to do what my boss is telling me to do
and after all of that I have to create charts,
graphs and visualizations.
Here you can build tools using natural language
like the one right there that says write me a tool
that fetches the meta stock price from Yahoo Finance.
In a matter of seconds, the common Joe or anyone
is able to create that tool, connect them to workers.
You can think of workers as your everyday employees,
agents, people that perform these actions using the tools
and then connect them to super teams
and these teams are able to, on the screen you see four
but you can scale this up to 40, 400,
basically complex, vertical and horizontal organizations
that are able to perform complex decision making
and complex analyses for anyone,
from enterprise to consumer.
What does this do?
With a multi-agent, multi-team framework,
this completely opens the landscape up for anyone
and everyone to take on the power of LLMs
into their own hands from natural language.
Take your average farmer at a farmer's market.
He's trying to create his marketing campaign
for the upcoming farmer's market this Sunday.
He has no clue where to start looking at his metrics,
looking at the customers, looking at the weather
and creating these brochures, papers, pamphlets and whatnot.
With one line and one minute using Agent OS,
he can create all the documentation he needs
in order to enact this stuff and be able to perform successfully
and continually grow his business at his farmer's market.
Things like this are completely opened up with Agent OS
and we hope to completely democratize the process
of using LLMs at all scales, at all geographies
and all use cases within sentences and seconds.
Thank you.
That's a compelling proposition.
The one thing that I worry about is right now the agents are the LLMs,
you know, performing these tasks and there's a certain question
about the veracity and reliability of what they're doing.
And so I think that in a future
where we have that reliability, this would make perfect sense,
but I would want to add a kind of tandem subject matter expert,
maybe looking over the shoulder of each of the agents.
I think next time I hear this pitch, I'd love to hear
about the one market you're going to crush.
It's hard for me to imagine serving a hairstylist one day
and Morgan Stanley analysts the next.
This is a huge opportunity and a big bold mission that you have.
I would want to dig a bit deeper into your tech staff
and the people you have on your team
because these are really complex problems and issues
and also agree that what would be your first area of focus
because it's pretty broad and wide.
I'll say I kind of like the broad focus and there's a lot
of individual startups tackling each of these individual problems.
If it's invoicing or research, it might be interesting to figure
out how to like loop in all these other tools that are out there
and really kind of just be like an interface layer
and let these other companies solve the technical challenges.
I think the value proposition of creating multi-agent workflows
in a matter of seconds is really compelling.
I think the next step would be trying to figure
out how can you go from simply performing these tasks
to becoming the best at these tasks.
So for example, going after the outliers,
sort of the thesis around coaching networks.
Some startups do this and they do it better for like certain
verticals and others.
So I think doing more research around that could be really
compelling.
The only thing I would add is just think about, you know,
enterprise security and how you solve for that.
There's a lot of authenticates and authorization you're going
to have to do for all these agents.
So just have an answer for that.
Well, yeah, thank you so much.
Thank you, everyone.
Thank you, Agent OS.
All right, next up, Skyline.
Come on out, Skyline.
Hey, everyone.
Hey, so my name is Rajan and I'm a first year student
at the University of Waterloo
and I study software engineering.
And I fundamentally believe
that cities shouldn't be static.
They should optimize and scale for the people
who inhabit them.
And so we built Skyline.
Skyline is an optimizer and it allows you
to better understand how to model cities,
using agents, and optimize things like traffic and transit
to inherently increase mobility
and reduce things like carbon emissions.
So this is a very weird problem that we solved.
But I want to walk through the case study of Los Angeles.
So Los Angeles is one of the largest carbon emitters
in North America, this most because of their transit,
because of the amount of cars.
And so what are ways in which we can optimize this?
Well, let's look directly at the people
who inhabit Los Angeles.
We can extract census data, things like age.
We can look at things like gender.
We can look at things like income.
We can find population density graphs.
And using this information, we can start to find patterns.
Specifically, what we did is we created 500 distinct agents.
Each agent is a different citizen with different interests.
And what we can do is we can give them
each their own chain of thought.
Each person here has their own day in their life.
For example, this person is a very young,
I believe this was a 22-year-old with a very large income.
He's a long day at work, and after work, he goes to the gym.
We can now reason about what this person may do
and now model this on a map.
Now, once we have how these different agents
are moving around, what we can do
is we can try and optimize things like transit.
So what we do here is we have our own proximal policy
analyzer, and this allows us to create simulations.
What we believe to be the best way
to understand how we can move around from any point A and B
in the fastest way at the lowest carbon cost.
We use our own carbon cost analysis mechanisms,
our own machine learning models to better understand
how we may be emitting carbon and how
to reduce this through our transit.
So this is a lot that is through you,
and I think the best way for me to represent this to you
is through a video.
I hope this video loads.
It's possible to play the video.
So what we first do is we have an agent-based simulation.
These are 500 distinct things in parallel that are running.
Now they each go around throughout their day,
and what we can do is we can define patterns
in how they move around.
Now the best part is what we can do
is now that they're all back in their original position,
we can start a generation of transit.
And we're using these patterns to now generate
live different transit systems that we
believe to be the most optimal.
So what Skyline is, we're not a company that
does analysis of transit.
We are a human modeling company, and that
allows us to better understand and better predict
how things around us will change and how
it can optimize them using these patterns.
Yeah, so that's Skyline.
Happy to take any of the back.
Thank you.
Wow.
I just want to observe that what you're
doing in creating a sort of digital twin of a city
is essentially the each citizen is
being simulated using one of these really powerful,
expensive things, a language model.
And it will be probably an important step
to draw from the language model some of the statistics that
are actually fed in in the first place
to make sure you're getting out something representative.
But that's very impressive.
Yeah, yeah, similar comment.
I think there's all sorts of economic theory
about agents and modeling their behavior
and their values and whatnot.
And the thing that usually gets you
is the sort of heterogeneity across the population.
And so making sure that that actually represents
the populations being modeled is important.
And then the other thing also related to value,
I would think about it as just value capture
for your own sake, because I feel like this
is a category of software where the economic impact of this
could be massive.
But how much of that do you get to capture as a software renderer
is less clear to me?
But it's very interesting.
I guess I would be curious about maybe some more nuanced
enterprise use cases as well, if it's concerts or security
or stadiums.
So just thinking about are there more micro use cases
that there's a more direct ROI with for this sort
of modeling?
Yeah, we tried to consider ourselves
to be a human modeling software.
And this is just one of the most visual applications,
which is strength.
Awesome, thank you so much.
Thank you.
Thank you.
Thank you, Skyline.
All right, next up, we have Spark.
Please welcome Spark.
Hi, how's everyone?
How's Cali?
We are Spark, and we're giving a voice
to new entrepreneurs, young entrepreneurs.
So let's admit it, cold calling is really hard.
I mean, resources are hard to get.
It's a steep learning curve.
And getting attention is hard.
If you've cold called someone, you know, they don't have time.
They'll say, oh, sorry, call me back later.
I mean, they're busy.
Everyone's busy.
We have things to do.
So we have to figure out how can we
earn the time of working people.
There's existing solutions.
It's long and arduous for trial and error.
It's expensive for a sales coach.
And finally, if you have a sales partner,
chemistry isn't easy if you're just meeting them, right?
Well, we have a process.
You upload a transcript to our software.
We go through and analyze the emotion.
We aggregate this data, and we give you productive feedback.
Who's our target market?
Well, look around.
You guys are our target market.
People who are engineers, people who love to build,
and say this weekend you made some sort of product
you want to sell.
You don't have much experience with sales or outreach.
With our software, you can record your cold outreach,
and we can tell you what you've done right
and how you can improve to hopefully land your product
where it needs to be.
And later on, we want to expand to call centers and sales
staffs, because we think we can spread this
across an organization, and it can be highly profitable.
We have usage-based tiering, so $0.75 a minute for 1,000 hours,
going up to $0.40 for $10,000.
So this is our software.
And I want to tell you guys a story.
I started being an entrepreneur around six months ago,
and we made an AI body cam analysis startup.
So I did 100 phone calls, 100 cold calls.
I got no clients, 200, 300, 500, and 700.
No one was responding to me.
So by 800, I got actually three, and I realized something.
The human brain is pretty amazing.
We're able to pick up on patterns,
but at the same time, it's kind of inefficient,
because it took 800.
Here, we look at the emotion between every single sentence,
and we figure out spikes of emotion and decreasing
emotions.
We see that when we talk about security and data privacy
with police officers, it shows an increase in their interest.
And this was a trend among many conversations we had.
So in our analysis page, we see in the top left
that mentioning AI accuracy and efficiency, increased officer
safety, and discussing cost savings
really helped us when we were talking to officers,
because we're some college students, right?
We're dealing with some pretty confidential data.
Bringing this up early really helped improve our rates.
And the four things you see here in the corners
are the different triggers we generated automatically
based on the cold calls we had.
So one is positive reactions, negative reactions,
deescalating tense situations, and normalizing
exciting situations.
We also generate insights, too, based on whatever cold calling
trends you make.
We also have a rag, so you can upload your company knowledge,
your target audience, and your pricing information.
So if you make a mistake, don't worry, we got your back.
We'll tell you, hey, maybe instead of saying this,
you could have said this, because it might have helped you
out a little bit.
Sorry, my team picture isn't on here,
but thank you to Tushar, and Nick, and Krishna.
You guys were a great team.
And I'm honored to be here representing you guys today.
I'm open to feedback.
I guess I need to be the first person
to say that you're entering a pretty competitive market
with other offerings here.
I'll say something that stuck out to me
was this idea of insights.
But I think at an organization, there's
going to be a sales team, and a marketing team,
and an online web team.
And those teams don't really talk to each other,
so maybe it's interesting to think about how do you pull
insights from one channel of sales or marketing
and actually bring that into another channel.
So maybe the insights from cold calling
are actually influencing what's going on the website.
Maybe there's some interesting spots of opportunity there.
Yeah, I can actually talk about one facet of this.
We want to explore deeply.
I want to give you an example.
Say we have three founders in the company, right?
I have a first cold call with one person,
and later on, my second co-founder
wants to set up a warmer call in the future.
And then my third founder wants to set up a third call.
We want to build a profile for this client as they go along,
so we can truly understand them.
And also, we want to develop a profile on ourselves, too,
so we can learn more about ourselves as we go
and how we're behaving, make sure that we're learning as we go.
So we're thinking, if we develop a CRM on top of this data
that we leverage, then we can connect multiple teams
and enable cross-functional benefit.
Yeah, I had a similar comment.
I think it would be really game-changing
if, in addition to some of the real-time analyses
your guys are doing around sentiment,
where you can see the system with information on prior calls
or this person's particular strengths and weaknesses
and how they compliment those of the other people on the team
and to really build a CRM, this knowledge graph around
each person's strengths and weaknesses on the team
to be able to better fine-tune the system.
Yeah, thank you. You guys saw the analysis,
but also, there's a long list of past conversations.
You can actually go into every single conversation
you've ever had and look at it deeply,
the same way you did in the latest conversation.
I would think about the full sales funnel.
This is pretty deep down in it.
And as you think about,
where are you really going to be able to convert
or where's the wedge that really matters?
Because there's a lot that goes into converting a sale
and it's not just the cold call.
So is the issue, are you actually calling the right people?
Or is the issue, are you actually speaking
to the right decision-makers?
So just thinking more broadly about that funnel
and where you might actually be able to have the most impact
and have the right wedge into the broader products we.
Yeah, thanks.
Thank you. Thank you.
All right, thank you guys.
Thank you so much.
We appreciate it.
Bye, enjoy your questions.
Clicker, clicker.
Thank you.
Okay.
Next up, we have Hear Me Out.
Please welcome Hear Me Out.
All right, guys.
Hey, hi, my name is Marcus.
I'm from Hear Me Out.
And what we've built is an AI-driven
customer service pipeline,
optimizing call matching and visibility.
So that might leave you scratching your head.
So let's just talk about the problem.
So let me give you a bit of context.
I'm an exchange student
and when I first came here, I had to deal with so many things.
I had to deal with banking.
I had to deal with insurance.
I had to deal with deliveries.
And I even had my car break down on me
and that was a real pain.
In short, I was overwhelmed by the sheer number
of customer service calls
because for each of these things,
I had to make so many calls just to get things done.
And I think a lot of you guys can relate to that.
We've all had our fair share of bad call experiences
where we're upset,
the customer service representative is also upset
and nothing is done.
We've also had good experiences as well.
And I think that's the core
of what we're trying to tackle here.
We want to create a pipeline
that tries to provide optimal matching
and provide visibility on emotional data to businesses.
So we also did the research
and I think the numbers speak for itself.
This is a key problem with a sizable market
and a sheer number of people are affected by this as well.
And this is our problem statement, which is,
how might businesses which offer customer service calls
provide a better experience for their customers?
So we think we can tackle this in four key components.
First of all, an improved call bot.
We all are common with that initial robo call
that we have to deal with
and sometimes it's really, really frustrating.
How many times have the call bot talk to you
and it just doesn't direct you to the right person?
I think we've all experienced that before.
Second and third, and I think this comes hand in hand,
it's just business visibility.
We want to provide businesses with better visibility
of both call experiences data
as well as customer representatives bandwidths over the day
as they continue to take calls.
And finally, this is where we put those two together.
We want to take that data and optimize a customer's journey
through a better customer to service representative matching.
So I won't bore you with this data,
but with that in mind,
we developed a set of decoupled microservices
and I just want to point three key parts out to you.
So first of all, we want to assess customer agreeability
with an initial robo call,
but this won't just be your normal robo call.
We want to use Hume's EVI to manage the robo call
in an empathetic manner
such that it measures the customer's emotions
as they go through the call
and eventually outputs an agreeability score
for the customer.
Second of all, we have a call analysis feedback loop
and that's that whole thing on the right
that goes down below.
And what this does is once you have a call connected
between a customer and a representative,
it takes in multiple factors of data
such as the call duration,
the emotional change over the call
and the overall call outcome.
Using Hume's emotional measurement API,
we can then also generate a call score.
Finally, and this is the third and key part to this,
it's the matching API.
Using the two things that I just mentioned,
we can best match a customer
to a customer service representative
which matches their vibe, their energy
and their emotions based on how our custom model is developed.
So what's the outcome of all of this?
As a representative goes through their day,
their state changes depending on how their calls go
and they're bandwidth adjusts accordingly.
This affects the subsequent customers
which they are matched to in a positive manner
and creates a better experience for both parties.
So there's a lot more which we can build
with what we have, but with this foundational pipeline,
we believe we effectively tackle the problem
that needs to be solved.
That's all I have for today.
Thank you.
Thank you.
Nice.
Thank you.
Yeah, I mean a little bit of feedback
similar to the last company as well.
Just there's a lot of companies working in this space too.
So I would just continue to think through
how to find that core differentiation
if you continue to work on this after the hackathon.
Yeah, I completely agree.
I think a key part that we thought was really exciting
was just what you can achieve with custom models.
What we're doing by developing a feedback loop
is we're creating something where we can create,
in a sense, a model which trains itself.
We can assess how calls might improve
or get worse after the matching
and that feedback gets fed straight to the matching API
so that it knows whether or not it's done a good job or not.
And we find that really interesting
and we think that that's a key differentiating factor
which we can achieve.
There might be some opportunities for building
some sort of synthetic data pipeline here
where you could just sort of simulate calls
with an AI bot of some sort and use that as feedback.
I don't know how good that data will be or not
but it could be interesting.
Yeah, I know that's a really interesting thought.
Thank you.
I know right now you guys are targeting
customer service agents as well as call centers.
Something that could be interesting to think about
as you think about the different stages
of the software adoption lifecycle
as you go from your early adopters to your early majority
and then your late majority
who's eventually gonna justify your evaluation
in terms of what those ideal customer profiles
are going to look like down the line.
Yeah, thank you for that.
I think one key thing was we actually had a judge come to us
and talk to us about how they were doing something similar
for sales representatives as well
and we found that really interesting.
So happy to figure out how we can pivot
if that need arises.
Thank you so much.
Thank you.
Thank you, hear me out.
All right, next up we have Dispatch AI.
Please welcome Dispatch.
Hi everyone, my name is Spike and I'm with Dispatch AI.
In the United States, over 80% of 911 call centers
are critically understaffed.
This means that in a crisis,
people with real emergencies aren't able
to get the support they need
because all the human agents are busy
and they're often put on hold.
This is particularly an issue in our neighboring city
of Oakland where last year the average wait time
was over 60 seconds to pick up a 911 call.
Now in an emergency, every second counts
and this could be literally the difference
between life and death.
This is unacceptable and that's why we built Dispatch AI.
The world's first AI 911 call operator
designed to eliminate these wait times.
Our system is powered by two key components.
First is the voice AI.
The voice AI will step into calls
when all human agents are busy
and it will work with the caller
to evaluate their situation, extract the location
and optionally dispatch first responders
directly to the scene.
And the second part is our powerful dashboard
for the operator themselves.
So the operator will have access to a bird's eye view
of all of the ongoing calls
which will be automatically triaged by the AI
into different forms of severity or priority.
Further, they'll see that the AI
will automatically extract the location
and will provide a live transcript of the call
so that they can quickly see what's going on
and even step into the call once they're available.
Further, they have buttons that allow them
to directly, with just one click
because the location's already fetched,
dispatch police, firefighters or paramedics.
All of this is done from a human centric angle
and the way how we achieve this
is by taking into account the caller's emotions.
So for instance, when a caller shows signs of anxiety
or fear, the system could work more to calm them down
and make them feel at ease
before taking the next safe step.
This system is fully designed
with ethical safe guides in mind
and part of that was fine tuning a model
on over 50911 calls
so that it could understand the proper protocols
and be knowledgeable on a wide variety of possible scenarios
in which a 911 operator could assist in,
including fake calls or instances
where it may not need assistance.
This is all powered by our innovative tech stack
that utilizes a variety of AI components,
including the voice AI, the emotional analysis
and of course a key component of this,
the fine tuning itself.
Our mission is to make requesting emergency services
more effective and efficient.
Thank you.
I'll go first.
I thought you did a great job.
I thought you presented the problem set,
the opportunity and the product very clearly.
It only had three minutes,
but you hit all the relevant points.
Thank you.
The one thing I would encourage you
to think about a little bit is
sort of like the optimization function
for these municipalities, right?
Cause if people in Oakland are waiting 60 seconds
to get their 911 call answered,
like there's a reason for that.
I don't know what that is,
but somehow these municipalities have decided
that that's how they want it to be.
And so I would just think about like,
you know, as you bring in AI to this problem,
doing the like potentially difficult AB tests
of making sure that whatever it is
that these municipalities are actually optimizing for
is actually improved by this.
Cause it seems like a no-brainer when you first say it,
but like clearly it's this way for some reason
that it's probably nuanced and tricky.
So just something to think about.
Any other feedback?
Just following up on that,
I think the key is ease of adoption.
I mean, I think you,
it's easy going to be easy to make a productivity argument
to the city of Oakland,
but then you got to think about who's actually installing,
who's paying for this and who's installing it.
Okay, that's good.
Thank you so much.
Thank you.
He just batch.
All right.
And next up, we have ASL Brigify.
Please welcome ASL Brigify.
Hello, my name is Isha
and today I'll be presenting ASL Brigify,
the next generation of sign language and interactive learning.
Oh.
Oh, this?
Okay.
Sorry.
So what was the inspiration behind this?
Well, ASL is the third most studied language
after Spanish and English
and over a billion people are projected
to have some sort of hearing loss deficiency,
which is why it's even more important
to have a seamless way of for people
with hearing loss deficiencies
to communicate with people without them and vice versa.
And next, there's over a 15,000% return on investment
over a 10 year period,
demonstrating the value proposition
and existing platforms like Duolingo surprisingly
do not take into account ASL learning,
which is why it's important to build
an interactive platform where individuals
can retrieve the accuracy of their signed texts
as well as characters.
Now, our solution includes three proprietary AI models.
First, we use random forest,
the random forest algorithm in order to,
in order to map input pose estimation frames,
a frame length of 200 to the predicted,
to the predicted alphabet from A to Z.
Next, we also use an LSTM model,
which captures sequential dependencies
to map from hand pose coordinates
to the actual word.
And then we also have our individualized RAG
calling in Lang chain as well as PDFs
PDFs that are specific to ASL learning
that get chunked and transformed
in a vector dimension space.
Now, as you can see here,
this is a hand pose estimation extraction
using the media pipe library.
So you can see A, B and C.
And here's our platform where you can,
there are different modules to learn
alphabets, signs, as well as sentences.
And we even have, we even have a real-time ASL practice.
So in real-time to capture the sign that you are actually,
the letter that you're actually signing
and give you the accuracy for that.
So here's an example of us using the media pipe library
to actually extract all of the hand key points.
And here are some videos where they're over hundreds
of words that you can actually view
to learn each of the hand signing frames.
Now, this is our proprietary RAG.
And the way we've trained this is we've collected,
we've collected a variety of PDFs
that are essentially manuals for ASL learning.
And potentially in the future,
we would want to incorporate things like YouTube transcriptions
that can actually be transformed and embedded
within our vector dimension model.
Now, in the future, this doesn't,
ASL doesn't just, hand pose estimation doesn't just have
to be localized to ASL.
There are plenty of other opportunities
for human pose estimation, including fields
like dance, martial arts,
where you can not only identify certain techniques,
but you can also get feedback generations
from certain input frames.
And in the future, this could also be integrated
into existing solutions, such as Zoom, Loom,
and FaceTime videos.
So if there's, so given the signing
of a certain sentence transcript,
you can get in real time the actual predicted sentences
and words.
Okay.
Thank you.
Thank you.
Thank you.
That's nice work for 36 hours.
I would be, I spent some time creating assistive
technologies for the blind,
and I would be just very aware of the market
and how you'll approach it and who will be paying.
I think that will be a good thing to pay attention to.
Thank you.
Yeah, as you think about the market,
I feel like these language learning apps
are tricky to kind of scale to meaningful businesses.
There was sort of like Rosetta Stone,
whatever, 20 years ago,
and then there's been like Duolingo
on this most recent gen,
but there aren't like that many
that get to meaningful scale.
So might be worth just thinking about that market
and what are the kind of success drivers?
I think even as I mentioned previously,
apart from just hand pose estimation,
I think that there's a big market
for body pose estimation.
I think especially in things like combat training,
especially like if you look at the military,
even dance performance companies
where they have to train dancers
and they're actually specific techniques
in which they'd want ground truth feedbacks for,
I think those are also potential markets
that could be ready to penetrate into.
You chose more traditional machine learning algorithms
in early neural nets like LSTM,
and that may be the right answer.
That's not obvious to me,
but I think for today's audience,
we need to explain why you're not using
more contemporary GNAI algorithms.
Yeah, so initially we were actually thinking about
using more encoder-based transformer models,
but we ran into some struggles,
so we just ended up settling on the LSTMs,
but in the future, we would obviously adapt
more of the state of the art transformers,
and even in the case for feedback generation
for given hand poses,
that could be an easy encoder-to-decoder
multi-self-attention model that you could train.
Okay, thank you so much.
Thank you.
Thank you.
All right, our last contestant for the grand prize
is Greenwise.
Please welcome Greenwise.
Thank you.
When I was 14, I stopped eating all meat.
I lasted about two months.
Now, even though I still eat meat,
there are a lot of small changes you can make
that have a huge impact on your carbon footprint.
For example, by switching from beef to chicken,
you cut the carbon footprint of your meals by six times.
What we do is we help you make that switch
from beef to chicken for everything,
for your shoes, your shirt, household supplies, food.
Everything has a carbon cost
and a carbon footprint that we can mitigate.
So how does a consumer analyze all their purchases
and the carbon footprint of anything
and try to make all these very difficult decisions
and research about how they should change their actions?
Well, this is where Greenwise comes in.
We seamlessly integrate with existing customer purchase models
to basically input what the consumer is already doing,
for example, through receipts or through emails,
and we integrate with Apple Pay, with Amazon,
and with Square to automatically get their purchases
into our system.
From there, we vectorize their purchase
and compare it to our vector database.
This database has all the carbon footprints
of over 10,000 products that we've analyzed
and made sure that these are accurate carbon estimates.
Additionally, by using the vector embedding,
we make sure that these similarity scores are very accurate.
It's not an LLM that can hallucinate.
These are real accuracy scores and real carbon predictions.
From there, it directly can tell them
an alternative product that is very similar
but has less carbon footprint.
Additionally, this provides a lot of room for scaling
when other businesses want to analyze
their carbon footprint for their products
or for events and other bigger venues.
So from good intentions to reality, let's make it happen.
Thank you.
It's a very innovative rag use case.
I never thought of that.
We're not using graph.
Oh, it's not.
It's similar in that it uses a vector embedding
for finding similarity,
but the similarity is directly the output.
Yes, that's right.
Yeah, yeah, that's all it is.
Is this a subscription product?
So currently?
Are you would imagine it being a subscription product?
We would imagine, or maybe you could talk about it.
Probably not.
Ideally, we'd integrate with existing businesses,
like Instacart or Safeway, so that they can show our results
on how the carbon footprint of certain products
is on their app, but it also works for consumers
to use on their own, as demonstrated here.
People wouldn't pay for a subscription, though.
OK, I think that's all the comments.
Thank you so much.
Green wise, thank you.
All right, I would now like to invite our esteemed judges
to convene in this secret room, where judges make their decisions.
And we are going to have the special prizes.
So as I mentioned, a bunch of sponsors
came to make this all happen.
We're an educational program, and it is entirely
the support of these sponsors.
And they're not just providing support, they got cool prizes.
So let's bring them on in just a minute.
You're going to hear from each one.
These are the sponsors for today.
And I also want to thank our community sponsors.
These are startups, very cool startups
who hung out and helped our young hackers with their needs
and their cool tools.
All right, so our very first special prize
is going to be announced by a very special campus partner.
I'd like to welcome the Academic Innovation
Catalyst.
I'd like to welcome out here Matt Sincini and Paul
Warp to tell you about AIC, one of our newest campus partners
doing very cool stuff.
Please give them a welcome.
Thank you.
Thank you so much, Caroline.
It's just a thrill to be here.
So my partner and I, Paul and I created Academic Innovation
Catalyst, or AIC, to release more innovation
from academic research for positive social impact.
And we're focused initially on the areas of climate tech
and AI for good, which is why we're here today.
How do we do this?
Well, we make proof of concept grants,
so no strings attached, non-dilutive grants
to academics with an idea.
Then we help them take that idea, carry it
through commercialization to scale and sustainability.
So that's what we do.
We're thrilled to be here today, and we'll
be making two awards to the most compelling business
plans or innovations involving the use of AI
to make the world a better place.
And we couldn't be more excited to announce them
in five seconds here.
I'll just say that we met with many amazing teams.
It's been an extraordinary weekend.
Thank you so much for including us.
We had to narrow it down to two.
It was tough, but I think you'll see
that they're well deserving.
So with that, let me hand it to my partner, Paul Warp,
to announce the winners of the AIC AI for Good Awards in 19,
I'm sorry, 2024.
This is what happens when you get old people up here on stage.
So anyway, we are really thrilled to be here, as Matt said,
and we're especially thrilled with the fact
that so many of you are putting your talents
to work for such great causes and for the betterment
of humanity.
And AI has so much potential in so many realms,
but among the most important is to make the world a better
place and to make a social impact.
And so with that, we're thrilled to announce
the first two winners, Dispatch AI and ASL Brigify.
So these are tremendous companies.
Again, the competition was so strong.
May I ask actually both sets of winners
to stand in the audience here?
And thank you again so much for the terrific work.
I think as you heard, ASL Brigify
is doing for sign language what Duolingo has done for learning
other languages.
And it is so important.
It's incredible and shocking that it's an underserved
and currently not served market.
And their technologies are going to change that.
And Dispatch AI, what can you say?
I mean, it's such an important issue
to be able to get emergency response,
to be able to get a response when you need it.
And of course, the reality is when we have, unfortunately,
too many mass catastrophes, the time
when you need the response strapped
is that you're often most short staffed.
And so Dispatch AI is using artificial intelligence
and a variety of technologies to speed that process up
and to help both the dispatchers and the people
that the dispatchers are helping.
And so can I ask the Dispatch AI team to stand up as well
and be recognized?
It's a great job.
Congratulations to all of you and to everyone
who is here today.
Thank you so much.
Thank you, Madam Paul.
Thank you, Academic Innovation Catalyst.
Our next special prize is going to be
introduced by our very own General Manager at Skydeck,
Sybil Chen.
Give her a welcome.
Woo!
Hello, everyone.
Hope everyone has had a great weekend.
At Skydeck, we have about a year and a half ago,
we launched the Skydeck Climate Tech track.
In part, thanks to a nice grant from the university,
we had $150,000 to build out the track.
And right away, we started putting that to work.
We grew our advisor network from maybe like five advisors
in the climate tech to now over 30 advisors that
are in the climate tech space.
And beyond that, prior to the grant,
we had maybe three or five startups, every batch that
were in climate tech.
And now we average 15 startups per batch
in the climate tech space.
And we really hope to see that grow.
So I'm very pleased to announce that the winner of the Skydeck
Climate Tech track is Team Greenwise.
I think they're still in the green room
because they just pitched.
They were the last ones to go on stage.
But they really kind of represent the team members
that we'd like to see at early stage startups.
It's three team members that are best friends
from middle school.
Yeah, they are.
Oh, they're out here on stage.
Come on out.
I wasn't expecting that.
But Anthony, Ben, and Ethan, three best friends
from middle school representing UC Davis, UC Santa Cruz,
and UC Santa Barbara.
And they've built a platform for carbon footprint tracking
with actionable recommendations for vendors
so that people and companies can reduce their overall carbon
footprint.
So please help me in congratulating this team.
Winners of $25,000.
All right.
Thank you, Simo.
Thank you, Greenwise.
Clicker, clicker.
Thank you.
All right, next up, special prizes from Intel.
Intel, come on out.
Intel was here.
Their table was swamped.
I'd like to introduce Gabrielle Amaronto.
Hi, everyone.
Thank you all so much.
And thank you to the organizers for having us.
We've had such a great weekend.
And your projects are so amazing.
So thank you to everyone who joined our track.
As you can see, the winners behind me,
congrats to all the winners.
We have our Raffle winner, Ayla Arres.
Third place is Asel.
Second place is Batteries by LLM.
And first place is Dish Batch AI.
So let's give them a round of applause.
Yes, great job.
Amazing projects.
If you won, please meet us outside.
I want to hand you your prizes.
We have them with us.
So please meet us outside so we can take pictures
and give you your prizes.
Thank you.
Thank you, Intel.
All right, next up, AWS.
Come on stage, AWS.
We've got Rohit Terluri, Kevin Liu, and Brandon Middleton.
And that's what they're going to tell you.
Go ahead, Rohit.
Howdy there.
Can you all hear me?
Yes?
Awesome.
Well, hey, thank you so much, Skydeck Team,
for having us and Calhacks.
This has been an extremely impressively run operation.
And we're really excited to be partners and sponsors
of this hackathon.
Today, we have three different prizes.
Actually, let me introduce myself first.
We have Brandon, Kevin, and Rohit.
We are members of the Generative AI Organization at AWS.
We work with top Generative AI startups
like Anthropics, Stability, Perplexity, and others
in the development of their large language models,
as well as our overall kind of inorganic and organic growth
strategy, including investments as well.
Today, we have three different prizes.
We have four of the teams that we have chosen
to give the prizes out to.
Our first place prize is for $10,000 in AWS credits.
And we have two other prizes, one for climate tech
and then one for responsible AI, which are $5,000.
I did want to say that we talked to so many impressive startups
and founding teams today and hackathon teams today.
I wish we could give prizes to all of them.
We did want to recommend that those who we spoke with,
and I think we have these conversations with you already,
to go ahead and apply for the AWS Generative AI
accelerator, as well as our AWS Activate
program to receive additional credits.
I'll go first.
I'm going to be announcing the climate tech.
We're going to give the prize out to disaster aid,
is disaster aid in the room today.
Yes, good job, guys.
And then for responsible AI, we have a two-way tie,
so we're splitting that prize into 2.5K for each team
in credits, and that's GPT ethics and DP ancestry.
They're in the hall.
All right.
And I'll round us out.
Our grand prize, kind of the most impressive thing
that we heard and saw today, is going to go to Safeguard.
So Safeguard team, if you're in the building,
stand up real fast.
Let's give you a round of applause.
I don't see them, but God bless you
and keep doing what you're doing.
Thanks so much, guys.
Thank you.
Thank you, Intel.
Our next amazing partner, Reach Capital.
Please come out, Tony Wahn.
Out of order.
Let me see if I can find you.
There you are.
OK, Mike.
Thank you so much.
Thank you to Calhacks.
Thank you to Skydack.
It is such a delight and pleasure to be with all of you today.
And thank you to everyone for being here,
from across the country, from across the world.
My name is Tony, and I'm from Reach Capital.
And let's just cut to the chase, because there's
no drama here.
We want to congratulate Frodo for winning our AI and Education
Prize.
Frodo, Aman, Kush, and the team, if you are here,
please stand up.
Please stand up.
All right, you are right up front.
You are in the right place.
Thank you so much.
You've won the one ring, as they say,
or at least our $1,000 cash prize.
So please, let's meet up afterwards.
Reach Capital, we're an early stage edtech VC firm
investing in edtech.
We invest in education across K12, higher ed, and workforce,
in tools that expand access to education and accountability.
And many of the companies that are portfolio
were founded by students themselves,
because what better place to find great ideas and great
talent than to go to places like this, where students are
living that experience?
So if you're building an edtech, please reach out.
Thank you so much.
Thank you, Tony.
Next up, we have you.com.
We have Rex.
Come on out, Rex, and tell us about the prize.
Applauds, please, for our sponsors.
Hi, buddy.
Yeah, thank you so much.
Yeah, so we wanted to announce, I'm Rex.
We're from you.com.
This is Oliver.
As you know, you.com brings transparency and web access
to our LLMs to make sure that they are as accurate as possible.
So we wanted to give an award for the best use of you.com's
APIs to transferify.
So congratulations.
If you guys want to stand up, if you're here, there you guys are.
Yeah, thank you so much.
Transferified did an incredible job.
They were live streaming videos, fact checking them
as they went, using sources from the web,
and you.com search APIs.
It was really incredible and powerful.
And Oliver will talk about our custom assistant.
Yeah, so for our best custom assistant,
we'd like to give that to events-.ai with Oliver and Devesh.
So Oliver and Devesh, can you please stand up
if you're in the room?
Congrats.
Over there.
Thank you.
Yeah.
So we were particularly impressed by what they've built.
Essentially, they handled booking, searching,
and even talking with customer agents on the phone.
And they used you.com in a way to actually find these events.
So we were incredibly impressed by them
and can't wait to see what they'll do in the future.
Yeah, come find us after.
And we will give you your awards.
Thank you, Hume.
Are you?
Thank you.
All right, I think we're going back to Hume now
with Zach Greathouse.
Welcome, Hume.
Nice round of applause, please.
Hi.
So first, just a huge thanks to Skydeck and Calhacks
for organizing this event and inviting us back
and to all the staff for running such a memorable event.
So I'm going to be announcing our three categories
for our track.
We have our best AI, our best empathic AI for social good,
best empathic AI for just most innovative empathic AI,
and then just the best overall.
As you can see, the team's here.
We've chosen Scam Scanner.
Can Scam Scanner, are you here?
Can you stand up?
All right, big applause.
For most innovative, we have Bloom Buddy.
Where's Bloom Buddy?
Can you stand up?
Yeah, OK, great job, you guys.
Talking to plans.
And then best use of empathic AI overall, we chose Lock-In.
It's a personalized learning assistant.
Are you in the room?
Where are you?
Yeah, there we are.
OK, congratulations, you guys.
Come meet us after outside.
We'd love to chat, take pictures.
And thank you so much.
Thank you to all the participants.
Yeah, we'll maybe see you next year.
So take care.
All right, thanks, Hume.
All right, and our last special prize is Grock.
There they are.
Please welcome Jose Menendez.
Hey, everyone.
Very nice to be here.
For those of you who haven't heard about Grock,
grock.com experienced the fastest inference on Earth
period.
All I have to say about Grock right now.
But our special Grock Star Award today
goes to Scam Scanner.
Where are you guys?
So these guys have a product that I
want my mom to use today, right?
Monitor your call for potential scams.
Who doesn't want that for your mom, your uncles,
and the whole thing?
Now, they get 1,000 credits on Grock Cloud,
which is many, many millions of tokens.
There's two special mentions I have to read,
so I don't screw up.
Three brown, one blue.
Where are you guys?
Another awesome solution.
These guys are generating on the fly
incredible videos for math.
Something that I would use right now as well.
And Transverify, are you guys around here?
You've been mentioned as well.
Transverify, very cool.
Who doesn't want to hear a podcast with instant fact
checking, right?
Am I right?
Now, my special surprise for the day,
I want to make a very special mention of Nathan Bog.
Are you around?
Nathan, all right.
Nathan didn't use Grock.
So I'm going to give a special technical excellence
award to Nathan for a model he trained and ran on his CPU
for doing very interesting DOM operations, corrections
on the fly for front end.
Not only that, Nathan is invited, officially,
to present his work in the Grock HQ as soon as he can.
That's it, guys.
I'm very impressed with all the work we saw.
Thank you very much.
Congratulations.
Thank you, Grock.
All right.
Our esteemed judges are back with their determination.
Please come back, judges.
Come back so we can all enjoy the grand prize.
Are you guys ready?
Do you have a guess?
Is there a voting?
Do we have voting tally?
Taking bets?
Everybody, I want you to guess your top two choices
for grand prize.
And then I'm going to ask, who got it right?
OK.
So as our wonderful judges, take their seat.
All right, we got some shout outs going here.
Any other shout outs?
OK.
All right, this audience is into it.
So as a reminder, the grand prize
is a $25,000 investment from the Berkeley Skydeck fund.
Also, a golden ticket to our Pad 13 program at Skydeck.
And a special prize, we are happy to announce,
that OpenAI is providing $2,500 in credits for this winter.
So I think we're ready for the drum roll.
Take your guesses.
Only the judges know.
I don't know.
We're all about to find out.
It's Dispatch AI.
Dispatch, where are you?
Come on up.
There's stairs right there.
Come on, come to the front stage.
There you go.
Thank you, judges.
I want to invite, while we invite Dispatch up,
I want to thank all of you for coming.
I want to invite Spike from Dispatch.
OK, here's the team.
There we go.
Dispatch AI, grand prize winners.
Well done.
Well done.
I'd like to invite the Skydeck staff to come out,
and the Berkeley Hackathon staff to come out.
Come on out.
They've been working all weekend.
I think some of them did not sleep at all.
Please give everyone who joined to make this a huge round
of applause.
Thank you, everybody.
Thanks for joining us.
We will see you next year.
Woo!

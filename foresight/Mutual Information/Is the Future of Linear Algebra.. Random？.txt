These are the graphics from Unreal Engine 5.
They're quite good.
The engine is simulating physics so well,
sometimes it can be hard to tell the difference from reality.
And to accomplish this, something that is absolutely essential
is numerical linear algebra, or NLA,
which is a study of linear algebra applied with computers,
and is better labeled applied linear algebra because that's really all it is.
Now without it, there are no good computer graphics.
Mapping 3D objects to the 2D view screen, that's gone.
Rotating those 3D objects, gone.
Rasterization, where triangle vertices are used to determine pixel color intensities, also gone.
But seriously, without NLA, a lack of video games would be the least of our problems.
We also wouldn't have weather forecasting, many types of data compression,
finite element methods for stress testing,
compressed sensing for MRI, fluid dynamic simulations,
recommendation systems, search, and of course deep neural networks.
An argument from one of the leaders of the field, the late Gene Gulub,
was to point out that of the top 10 most important algorithms of the 20th century,
six of them related to numerical linear algebra.
And that was almost 20 years ago.
Now I decided to make this video when I stumbled upon this paper,
Randomized Numerical Linear Algebra.
In it I found some remarkable claims.
They show how some well-placed randomness can speed up some NLA algorithms by an order of magnitude.
In other words, we're looking at a potential breakthrough in scientific computing.
And randomization for speed seems like a really bizarre idea.
Why would an algorithm that wants to compute something exactly be super sped up
by doing something akin to coin flipping?
Now to fully appreciate this, we'll need some background.
But first, this episode is brought to you by True Theta, the Data Science Consultancy.
More about us at the end of the episode.
The first thing we need to ask is, what is linear algebra?
Simply put, it's the mathematics of vectors and matrices.
We'll start with a vector.
A vector is just a list of numbers and can be imagined geometrically as an arrow.
Now, this is just a wimpy 2D vector.
The powerful stuff comes from using n-dimensional vectors, which can represent complicated things.
Everything from images to words to audio to a person's credit profile.
Basically, most things you can think of can be represented as a vector.
Now we consider a matrix.
On the surface, a matrix is just a grid of numbers.
Or you can consider it a list of vectors.
As a list of vectors, you can imagine it as a bunch of arrows.
When a matrix represents data, they're commonly represented as points.
But in linear algebra, matrices represent something else.
They relate to functions.
In our case, a function is something that receives an input vector x
and gives back a vector y, possibly of a different dimension.
And it needs to do that for a range of x values.
If we were to increase the dimensions, the function would be connecting exponentially huge regions.
In this sense, functions are very information-rich objects,
since they represent an enormous set of connections.
But handling all that information explicitly is often impossible.
So, we make a fantastically liberating assumption.
We assume the function is linear,
which means it's like a line, or a plane, or something similarly flat in higher dimensions.
More rigorously, linearity comes from some algebraic properties, but we don't need to get into all that.
What's important is why linearity is useful.
Consider two cases, where the inputs and outputs are both one-dimensional.
In both cases, suppose we'd like to guess the function, after being shown two input-output pairs.
Now, on the left, we don't make any assumptions about the function.
It can be any function.
On the right, we assume the function is linear.
With this, can we determine the functions?
Well, when we assume linearity, yes, it's easy.
There's only one linear function that goes through these two points.
But without the assumption, there are infinite functions that pass through those two points.
And we have no way to pick one.
This is totally essential.
Linearity allows us to determine these information-rich objects' functions with very little.
Basically, to know a linear function in one region is to know it in all regions.
This fact is so useful, much of applied mathematics involves making the linearity assumption wherever we can get away with it.
Doing so allows us to extrapolate in space, or time, or something else.
And crucially, it makes the exceptionally powerful theorems of linear algebra available to the problem at hand.
If we assume linearity, we essentially get an instruction manual for computing useful things.
So how does a linear function relate to a matrix?
Simply, if f of x is a linear function, then f of x equals a times x for some matrix A.
Saying A times x is to do matrix multiplication, which is a simple but tedious calculation of sums and products.
Please don't think about the details.
Okay, now we can ask, what is numerical linear algebra?
Well, that's the study of applying linear algebra fast and efficiently with computers.
This brings two fundamental challenges.
The first is the computer's finite precision.
The second is that fast algorithms are machine dependent.
Let's start with the computer's finite precision.
If a point is selected at random, it almost surely can't be represented exactly in the machine.
The machine will have to truncate it to an approximation, creating a small error.
This fact, as harmless as it seems, hardly complicates everything.
One effect is, to a computer, addition isn't always associative.
Sometimes, adding numbers A and B before adding to C doesn't give the same answer as summing B and C first.
And when an algorithm fails to anticipate issues like this, it's prone to incorrect answers.
We say such algorithms are numerically unstable.
Now, we consider the second challenge, that fast algorithms are machine dependent.
Let's say we want to multiply two matrices A and B.
By how matrix multiplication is defined, the resulting matrix is the sum product between every row of A with every column of B.
That's how it's defined, but how is it computed?
Well, it helps to imagine it like this, where we are looking to fill up the cells of this matrix C.
Each cell corresponds to a combination of a row of A with a column of B.
From here, there are several ways we could order the computation.
One way is, we could finish the computation of each cell of C before moving on to the next cell.
That's probably how you do it after you heard my definition.
But there's another way.
We could proceed like this, finishing the use of each column of A and row of B,
which is swapped from how I define matrix multiplication, but gives an equivalent answer.
And there are other ways of ordering operations.
The problem is, ordering makes a difference for the algorithm's speed.
They bring different amounts of data movement and differing data accessing patterns.
And this speed depends, at the least, on A and B's dimensions, how they are physically stored in memory,
and how much can be stored in a processor's registries, which are very small but provide extremely fast access.
And this is just one flavor of the algorithm's machine dependence.
Everything from the memory layout to the processor to the operating system to the capacity for parallelism
determines the algorithm's speed.
And these things must be anticipated in all their diversity across users and time to design the best NLA algorithm.
As you can imagine, this gets very complicated very quickly.
To appreciate what's offered by randomized NLA, we'll also need some history.
John von Neumann is often named when discussing the origin of the field.
Because in 1947, he and his co-author Herman Goldsten published one of the earliest uses of computers for applying linear algebra.
But this isn't a relevant origin for modern NLA software, since the programming paradigms of the 40s and 50s were just so different, very little of it persists today.
The code was terribly verbose, machine dependent, hard to share, and just an absolute headache to write.
So in 1957, IBM created the Fortran language, designed to ease programming for scientific computing and provide some machine independence.
And in the 1960s, James Hardy Wilkinson and his colleagues collected and published papers on how to apply linear algebra with computers, but not with the Fortran language.
Most of their papers discussed how a specific algorithm could be applied to a category of matrix.
This work earned Wilkinson a Turing Award, and along with Fortran, it created the environment from which modern NLA software was born.
That happened in 1979, when BLOZ came out from the Jet Propulsion Laboratory in California, the basic linear algebra sub-program for Fortran.
It provided a small set of vector operations that were fast and tested across a variety of machines.
Most importantly, they could implement NLA algorithms, like those proposed by Wilkinson.
And so in 1979, LINPAC came out, which was built on top of BLOZ, and had been developed over the previous decade, primarily for supercomputers.
It wasn't the first NLA package, but it was a major step forward in speed, reliability, and distribution across the scientific community.
But at the same time, computer architectures were evolving.
And so BLOZ 2 was released in 1980 form to perform matrix vector operations, which took advantage of the vector processor CPUs of the time.
But architectures changed again, to ones with shared memory cache-based parallel processing.
And so BLOZ 3 was released in 1990, which performed fast matrix-matrix operations.
And this is a perpetual story. Architectures changed, so BLOZ 1, 2, and 3 need to be updated.
Architectures changed again, and so we need more software updates.
But now, to fully leverage BLOZ 1, 2, and 3, a new NLA package was needed.
In 1992, LAPAC was released, after having been proposed five years earlier.
The authors included some modern NLA heroes, like Jack Dungara and James Demo.
Both have been involved ever since, architecting, writing, standardizing, testing, optimizing, and communicating the software to produce what it is today.
And that's quite a feat. Today in the 2020s, an absolutely enormous amount of linear algebra gets applied with LAPAC or BLOZ.
If you're doing any linear algebra with MATLAB, Python, or C++ or any other language you can name, it's very likely you're using this software or very close derivative of it.
And if you aren't, you're probably doing something wrong.
Okay, and now I need to confess this short history is a major oversimplification.
LAPAC exists in an ecosystem dedicated to scientific computing.
And so my cute linear story fails to represent the entangled, messy truth of the matter.
To mitigate this, I'll give a very quick tour of the current software landscape.
LAPAC doesn't work with distributed memory parallel processing, so we have ScalaPAC relying on PBLOZ, which performs many of the same operations as BLOZ, but executed across a large network of heterogeneous machines.
LAPAC isn't designed for sparse matrices, so there have been efforts to capture the gains of sparsity, but none have been received quite like LAPAC.
LAPAC also isn't designed for GPUs and modern multicore architectures, so we have MAGMA, which enables use of NVIDIA and AMD's fancy GPU hardware.
Speaking of NVIDIA, we also have Kubloss.
See, BLOZ comes with a bunch of knobs that need to be optimized for particular hardware, and so Kubloss has those knobs set for NVIDIA GPUs, among other things.
Also, Apple has Accelerate to do that same tuning for their hardware.
And since tuning knobs for hardware isn't easy, we also have Atlas, which automates some of this tuning.
More recently, there's GPT Tune, which uses Bayesian optimization and Gaussian processes.
If you don't know what those are, I wonder where you could learn about them.
Oh, wait, true theta.io! That must be a great place for data science help.
Moving on, people also want to perform large batches of small and similar NLA operations, so we have batch BLOZ.
I'd like to say that covers it, but it doesn't.
There's a lot of software out there doing NLA, and this list right here is just the free open source stuff.
And now we can finally discuss the paper on randomized numerical linear algebra.
Again, I'm making this video because of some remarkable claims it made.
And it took them especially seriously because it's authored by some of the original developers of BLOZ and LA-PAC, like Jack and James.
In fact, I spoke with the first author Riley Murray to make sure I understood exactly what's going on here.
Now, when reading this, I knew LA-PAC and BLOZ were virtually impossible to dethrone.
Also, from a distance, my impression was the field of NLA had matured, and there probably weren't huge gains to be had.
And I'm not alone in that impression.
To understand this, we need to talk about how we describe an algorithm's efficiency.
Say we're given a matrix A, and it's an N by N matrix.
Let's say we like to multiply it by another matrix B of the same size.
Now, if we were to do this the standard way, that would be what is called an order N cubed algorithm.
That means, as the side length N grows, the number of operations grows like N cubed, roughly.
For example, say multiplying 10 by 10 matrices takes some fraction C of one second.
If we multiply 100 by 100 matrices, then we'd have to wait an amount of time close to that same fraction C, but now of a thousand seconds.
So, we increase the side length by a factor of 10, and the time increased by a factor of a thousand.
Fast hardware can bring down C, whatever it is, but it won't change its painful growth.
However, in 1969, something remarkable did.
Volcker Strassen surprised everyone with an algorithm that does multiplication in a way that grows like N to the 2.8-ish.
This was extremely surprising, since matrix multiplication, the standard way, involves three for loops.
So, the exponent of three seems totally unavoidable, and yet, a lower exponent was possible,
and so the result kicked off research to get that 2.8 exponent down.
It has since leveled out, and it's leveling out where I formed my impression.
In general, for all important matrix algorithms, pushing these exponents significantly further down seems effectively impossible.
So, as I was reading this, I believed gains in speed would not come from fundamentally new algorithms,
but just better scaled-up hardware.
That's expensive, but it seems like it's the only option.
But that understanding changed when I read these two paragraphs.
Here's what it's saying.
A problem you see absolutely everywhere is the problem of least squares.
I'll explain what it is.
We're given a matrix A, which has M rows and N columns, and we'll assume that N is much less than M,
which is actually pretty typical in practice.
We're also given a vector B, which has dimension M.
So, A and B are matrix and a vector that are given to us for this problem.
Now, the goal is going to be to find a vector X.
First, we form AX, which, as mentioned, is a linear function defined by multiplication with A.
Then we consider its distance from B.
That's what this notation means.
Now, our goal is to minimize this distance.
So, in one sentence, our goal is to find X such that AX is as close as possible to the vector B.
Okay, now the best NLA algorithm to solve this involves order MN squared operations.
So, we have an exponent of 2, but at least it's on N and not the much larger M.
Now, what Rand NLA says is, if you're willing to accept a small and controllable error in your answer,
which we'll call epsilon, then randomized algorithms can actually solve this
in order MN log 1 over epsilon plus N cubed operations.
It may not sound like much, but this is huge.
In heavy-duty applications, both M and N can be big.
N might be in the thousands, and M could be in the millions or billions.
Now, if we want a strong approximation, like one within a tenth or a hundredth of a percent,
then this term is going to be in the single digits.
And since M is the big problematic number, then this term likely won't matter much.
So, let's look at the ratio of the dominant terms.
This will give a sense of how many times faster the randomized algorithm is than the classic one.
Things cancel, and now we're looking at a speed-up factor of N over log 1 over epsilon.
If N is in the thousands, and this is in the single digits,
we're looking at a speed-up factor of around a thousand X. That's absurd.
Now, due to some omitted details, we don't actually get a thousand X speed-ups in practice.
However, we do get twenty X, and that's still huge.
If we did the pure parallelization and hardware approach,
recreating that gain might take twenty X the energy or twenty X the cost.
Okay, but such bold claims raise some questions.
First, what are these algorithms doing?
Well, I'll get more into that later.
But to describe it briefly, with high probability,
a random summary of the data massively shrinks the problem while preserving virtually all of the relevant information.
And that raises another question.
A long-standing goal of classic NLA is to compute the most exact answer possible as fast as possible.
That is, get the answer as precisely as the machine will allow and then optimize for speed.
But in randomized NLA, the goal is to compute a close enough answer as fast as possible with high probability.
And this allows for much faster algorithms.
The question is, why are we allowing this new standard?
Well, one motivation is the recent trend in machine learning.
Machine learning accepts that the data is noisy.
The data is just an approximation of the truth.
And so computing things exactly is unnecessary.
The exact answer would change with the change in the meaningless noise.
So an approximate answer is, life be just as good.
And it might even be an exact answer with a different sample of data.
And so this looks a lot more reasonable.
Also, when you look at some algorithms, we find that close enough can sometimes be made extremely close.
And high probability can sometimes be made so high, it's not even worth mentioning.
In general, there's a trade-off between speed and accuracy, and how favorable that trade-off is depends on the algorithm.
Okay, next question.
This argument is just a heuristic illustration.
It's not pointing to an implemented algorithm with measurable performance.
So what is the actual performance?
Well, there are many papers that demonstrate significant concrete improvements.
One striking demonstration I saw came from this paper,
which is actually co-authored by Stephen Brunton and Nathan Kutz,
two researchers who are active educators on YouTube.
You may have seen them.
One of their goals is to improve the SPD algorithm for low-rank matrices in the programming language R.
SPD involves taking a matrix A and decomposing it into a product of three matrices.
And without getting into the details, these matrices have some nice properties
that help us do things like dimensionality reduction or solving least squares problems.
Now, their randomized SPD algorithm gets this performance.
Each plot is for a different size of A, specified as rows by columns.
The y-axis tells us how many times faster an algorithm is than the plain SPD algorithm.
So the plain SPD algorithm itself, its speed is always 1.
That means the randomized SPD algorithm is between 40 and over 100 times faster, depending on the size of A.
And the errors from randomization are comparable to those of the non-randomized routines.
So we're looking at massive real gains, but we need to make some comments.
First, they're only considering low-rank matrices.
Randomized algorithms are especially useful for those.
Second, their algorithm is entirely implemented in R, a high-level language.
In contrast, one of the other benchmarking algorithms, R-Spectra,
provides speed by granting R-axis to Spectra, a C++ library optimized for eigenvalue problems.
So maybe randomization is best coded at the lower level, like that of C++.
So let's check out an algorithm that does that.
This one, Cholesky QR with randomization and pivoting for tall matrices.
This one does an impressive job of wrangling the interplay of hardware, software, and randomization to produce a dominant algorithm.
In fact, it strikes such a favorable balance of speed and accuracy,
the authors claim that the algorithm design question is effectively solved for this class of matrix and problem.
That's quite a claim.
So what's the problem they're solving?
Well, once again, we're given a matrix A, which in this case is assumed to be very tall.
The goal is to decompose it into matrices with certain properties.
Again, explaining the decomposition would take us really far afield.
But here's one important detail.
By including this matrix P, called the permutation matrix,
we're asking the algorithm to order its operations in a special way to improve numerical stability
and, for lack of a better explanation, provide more information on the decomposed matrix.
Including P is significantly more work, but virtually guarantees we'll get the right answer.
That is, it's much more numerically stable.
Doing this decomposition is called QR decomposition with column pivoting.
The QR matrices enable several useful things, like solving least squares problems.
Now to explain their algorithm's performance, here are two blank plots.
On the left, we'll see performance on matrices with about 32,000 rows.
On the right, about 130,000 rows.
Along the horizontal axis, the number of columns varies from about 500 to about 8,000.
So we're looking at A matrices of different shapes, but they're all either tall or really tall matrices.
Now the vertical axis is billions of floating point operations per second,
which is a bit of a weird thing to measure.
But the G-flops are those of a benchmark algorithm run on A, and so it's fixed across algorithms.
In other words, just interpret the vertical axis as relative speed, like in the previous paper.
Okay, now, this is LA-PAC's algorithm for QR with column pivoting.
Remember, LA-PAC is the very well-optimized industry standard, but maybe not for long,
because this is their algorithm, pronounced secret.
As you can see, we're looking at 10 to 20x speed-ups, and if you're wondering what's happening here,
the authors point out that the matrices no longer fit in the cache, so moving more data around becomes necessary.
As an aside, moving data or data communication is a major source of algorithmic slowness.
Now in the paper, the plot shows alternative fast algorithms for a comparison.
They're acknowledging other approaches people might be familiar with to further benchmark their algorithm.
However, in my view, the authors are being a bit modest, because these other fast algorithms aren't doing the full job.
It's not apples to apples.
This one and this one don't perform column pivoting, so the decomposition gives us less information about the matrix.
If we ignore that fact and try to use the decomposition just like it had done pivoting, we get inaccurate or even flat-out wrong answers.
This one and this one are numerically stable, but the former only applies to full-rank matrices, so not all tall, skinny matrices.
And the latter only delivers an implicit representation of the matrices we want.
This means, in many practical cases, we'd need to do extra work, slowing the algorithm down considerably.
All things considered, this plot understates how much better secret really is.
And if you're thinking, okay, but this can't actually replace the LA-PAC algorithm, because randomized algorithms come with some error, right?
Well, in this case, the error can be made so small, it's essentially just as good as the LA-PAC routine.
And this highlights the bizarre magic of randomization.
You add a well-placed pinch of it, and you're able to get essentially the same answer, but many times faster.
It's a trick almost without baggage.
It's not an optimization that only works for a fixed set of machines or a niche class of matrix.
It works essentially across the board.
It makes you wonder, what are these algorithms doing?
Well, RAND-NLA algorithms come in several flavors.
I'll go with one that's especially simple to present.
Let's bring back the least squares problem.
Again, we're looking for the vector x such that ax's distance from b is as small as possible.
We'll call the x vector that achieves this minimum x star.
Again, we'll assume that a is a tall matrix.
One randomized approach is called sketch and solve.
We start by sampling a random matrix s.
How we do that doesn't matter right now.
What's important is that we'll be solving the least squares problem, but we'll replace a with s a and b with s b.
And s will be designed such that s a has many fewer rows than the tall matrix a.
And in the same way, s b is much smaller than b.
Essentially, multiplying by s produces a compressed problem that's much faster to solve.
Doing this is to form a small sketch of the problem.
Here's the remarkable thing.
If we solve this new least squares problem, giving us a vector we'll call x tilde,
then the distance it achieves in the original problem is about the same as the best achievable distance in that original problem with high probability.
In other words, it's very likely that solving this much smaller problem will give an answer that's nearly just as good as what would get solving the original big problem.
And that's great news.
A much smaller problem is much faster to solve.
And that's where we get these order of magnitude speed ups.
And what makes this practically useful is that we can control how good the approximation is.
And the probability that that level of approximation is achieved.
We can say, I want a 99.99% chance that the distance is within 1% of the best achievable distance.
Declaring that will tell us how many rows s needs, depending on how s is randomly sampled.
In fact, developing theorems to make statements like these, that's where a lot of the hard work of the field is.
Further, how s is sampled is a question all its own.
Every value could just be a sample from a normal distribution, but there are fancier techniques,
like ones that allow you to perform the essay multiplication extremely quickly.
Because, in the presentation that I just gave you, doing that multiplication would actually be a dominant cost.
Okay, but this explanation doesn't answer the why does it work question, really.
I'm just showing that it relies on an approximation, which I'm asking you to accept.
So, let's go a little further.
Let's say we're given the following least squares problem.
A is equal to some tall matrix with two columns, and B is equal to some long vector.
Now, if you know something about least squares, you know finding x is going to involve, among other things, the covariance matrix of A.
That's a component of the solution I'd like to focus on.
So, let's ignore B for now.
Now, if you don't know what the covariance matrix of A means, that's fine.
With just two columns, it's a very easy thing to visualize.
What we'll do is plot each row of A as a point.
Now, the covariance matrix just tells you this elliptical shape.
And it's this shape that partially determines the minimizing x.
Next, let's consider SA, where S is properly scaled random Gaussians with D rows.
If D is equal to 8, that means SA can be plotted as 8 summary data points.
Again, we compute the covariance matrix, and again, this shape partially determines the minimizing x, this time in the shrunken, sketched version of the problem.
Now, here's what RAN and LA exploits.
As D increases, the covariances of SA and A become more and more similar.
So, between A and SA, the covariance structures, things that determine the minimizing x's, are very similar.
In other words, as far as this covariance piece of the answer goes, A and SA give us nearly the same thing.
And D doesn't need to be that large to get a good approximation.
So, we can solve a much smaller SA least squares problem and get virtually the same result.
Okay, but what about B?
Yeah, the minimizing x is also determined by the covariance between A and B.
So, let's consider an augmented matrix, which is A concatenated with B as a new column.
The covariance of this new matrix now includes everything that determines the minimizing x.
Looking at this, this is just another matrix.
So, each covariance matrix will be similar to that of its sketched version.
In other words, everything that drives the minimizing x is similar across the original and sketched problem.
That's why the sketched version gives us approximately the same answer.
Now, I need to confess something.
This approximation is actually pretty weak and isn't really what's powering the randomized algorithms.
But it captures the essence and can be animated, so I went with it.
However, Riley pointed out that what's actually happening involves much stronger approximations regarding relative differences.
Now, since defining relative things for matrices involves some head bending, I backed away.
So, this gives a sense of the mathematical properties I play, but it's not the full story.
For a bigger picture, Riley gave me the following analogy.
The strategy of LA-PAC is to cast NLA algorithms into the use of efficient BLOZ functions.
The highly optimized General Matrix Multiply or GEM function.
The more an algorithm can be written as repeated use of this operation, the faster it'll get.
That's what LA-PAC did.
But there's a limit to how much algorithms can be recast into GEM.
But randomization provides a new basket of functions that can be applied in a similar way to GEM.
If least squares is suddenly super efficient, we'll try to reframe everything we can as solving repeated least squares problems.
It's a huge space for creativity and big gains.
And that's why in the Rand-NLA paper, they're talking about a new software, Randblast and Rand-LA-PAC,
which would serve as a new pillar for NLA, the randomized approach.
If they pull it off, and it's really saying something, we'd be in for a widespread upgrade in scientific computing.
All those technologies stand to be improved from gaming to weather forecasting to artificial intelligence.
To be comprehensive, I should mention at least two other approaches to speed.
The first is communication avoiding algorithms.
Algorithms which anticipate the hardware to minimize the amount of data movement.
Since moving data takes so much time, these provide big speed ups as well.
Second, there are hardware accelerators.
Specialized hardware designed to do very specific operations extremely fast.
Now, both of these are totally effective paths to speed,
and in fact, can be combined with randomization to produce even larger gains.
That said, these approaches bring some inflexibility.
Communication avoiding algorithms need to be designed especially carefully to the hardware,
and accelerators only do fixed, highly specialized operations.
You can't change what an accelerator does after it's built.
In comparison, randomized algorithms are exceptional because they are entirely an idea of mathematics.
Randomness isn't upgraded hardware or an algorithm designed for specific hardware,
yet it gives you speed and scalability as though it were.
It does this by allowing simple algorithms, ones that otherwise struggle with scalability,
to be applied to huge data.
And it's this quality of simple but powerful that I believe is necessary
for producing a significant and widespread upgrade in scientific computing.
Since this is evolving, I'm going to keep track of updates as I hear about them
and to the best of my ability on a post on truthata.io.
There I'll also answer some other questions, like why isn't this Monte Carlo or who cares about least squares?
In general, I inevitably learn more about a topic after I publish a video on it,
so this post can evolve as I hear from you, others, or just learn more about it myself.
And wow, what an incredible segue into talking about true theta.
Truthata is my data science consultancy.
We have experienced building machine learning systems for pricing,
credit risk modeling, causal inference, and forecasting.
If you're at a company looking for this type of work, we should talk.
You can send an email to increase at truthata.io to get in touch.
Alright, that's it.
If you'd like to learn more about randomized NLA, I have my sources in the description.
Also, I'd like to make a special thank you to Riley Murray for our discussions on this topic.
And I'd like to thank everyone else who provided useful commentary.
And finally, thank you for watching, and until next time.

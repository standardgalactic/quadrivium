Hello, and welcome to the fourth in our series on AI, Neuroscience and Architecture, which
has been put forward by the Digital Futures doctoral consortium group, whereby we're trying
to make important educational ideas available to students and architects across the globe
as a way to make education much more accessible to democratise as it were, education.
I'm delighted to have a very special guest here today, Yosha Bach. Before I introduce him,
let me just make one announcement about for next week. That is to say, on Saturday,
we will have a session on Digital Futures looking with Runja Chan looking at a generative game.
You'll find details of that on our website. Today then,
it's really very... I've been looking forward to this session very, very much. I've been watching
Yosha Bach at various interviews online, and I know from my friend Daniel Seth, who is a great
admirer of Yosha, although he says he disagrees with him on some things, but that we're going to
have a very special session today. I'm hoping that if nothing else, we will open up to a series of
new ideas and you will discover somebody who I think is a very significant and creative thinker
who is having a significant impact on the field. Let me say first of all that Yosha is from Germany,
from Delhi after he was born in Weimar. He went to go and study at the University,
first of all at Humboldt in Berlin, and then Yosha Bach took his PhD. After that, he's been
working as an academic both at Harvard and also MIT Media Lab, and more recently, he's been working
for Intel. He has a number of significant online lectures and interviews, including a TEDx,
which I would recommend, and he is the author of the book Principles of Synthetic Intelligence,
an architecture of motivated cognition. The term architecture, of course, is very interesting here
because it refers both to the world of computational science and to the world of
architecture itself. I should say that Yosha comes from a family of architects, not an architect
himself, but his father was an architect. Intriguingly, I hear that he was very much against
right angles in buildings, and therefore his work was more like the work of Hunderwasser.
Maybe this will come out in the conversation today. I particularly like the interviews
that Lex Friedman has done with him, and extraordinary interviews. I mean, really,
really interesting. And it's almost, I could see a slight difference between the genealogy
from the earlier TEDx talk towards this very creative thinking that I find extremely
provocative and exhilarating. It's like being on a rollercoaster ride when Yosha is
telling us what's in his mind. We exist inside the story that the brain tells itself. No doubt
we'll hear more about that right now. I should also point out that he has a personal blog,
which you can access with a series of posts. And I will also say that this particular session
today will be uploaded onto our YouTube library to be accessed for free from everyone over the
world. And the previous ones in this series, Blazer, Igridiakos, David Chalmers, and all the
ones in the future will be uploaded here. And at the bottom here, if you want to do a screen
capture, you can capture the YouTube library, digital futures YouTube library, where they're
all uploaded. And of course, not just these, but also the whole series on architecture and philosophy
that we've been doing with Slavoj Žižek and others over the over the years, and also tutorials and
other sessions. So it's the session today. Yosha is going to make a brief presentation as a kind of
way of opening up the discussion. Then I'm going to be asking some Lex Friedman style questions
to get the discussion going. And then we'll be inviting questions from both the Zoom audience
and also the YouTube audience. And I just want to say this is what we're getting out of this is
what we're trying to focus on is a new theory, let's say of intelligence that is appearing
at the interface between neuroscience and the world of AI. And we have a series of other
speakers coming up over the over the next few weeks, including Jeff Hawkins, Ben Bratton, Susan
Schneider, Antonia de Massio, Andy Clark, and others. And it seems to me this is a very exciting
time. I was brought up, in fact, my first post as an academic, I was working in the area of
continental philosophy. That's where the debate was. And in the 90s, it was particularly very,
very strong debate. But we're now moving into something else, which I find even more exhilarating.
And that is to say, the world of cognitive science, the world of AI, the world of neuroscience,
psychology, and a kind of philosophy where the debates have really been driven in an interesting
way. To my mind, what makes this very special is not in the fact that we are embracing the world of
science, which in the old days of the divided sort of culture that C.P. Snow talks about,
somehow philosophy wasn't engaging enough with the world of science. And this was the critique
that Stephen Hawking has of philosophy, but it doesn't keep pace with technology. Not only are
we embracing the latest technology. And something coming out that I would say is deeply, deeply
philosophical, which is very new terrain and very exciting terrain, but also the world of
practice, the world of the commercial world is coming directly into contact with academia.
And some of the ideas that are coming out are just quite extraordinary. So I see this new
emergent theory of intelligence, something that is really dynamic. And I think it's going to be
powering debates in years to come. So I would like to start by welcoming Yosha to say what
are great privileges to have you with today. I'm looking forward to this immensely.
And invite Yosha to share his screen. Yosha, welcome. Thank you very much. It's a big honor
to be here with you today. And let's see what transpires.
Let me start out with something that happened on Twitter a couple of days ago. Ilya Sutskyver,
an AI researcher who was quite notable, said that it may be that today's large neural networks
are slightly quenches. This sparked quite some debate. Some people asked him whether he was
falling and others were upset that he was propagating an eye hype, an unjustified race,
and others are intrigued. And he said this thing to me before. So I knew that he was not trolling.
And it's a question that is very counterintuitive to me, a statement like this that, for instance,
a large language model and so on, that it could be quenched today already. And so
could we answer this question? Is there a decisive way to deal with it and to prove
that it's not conscious or to give evidence that it is? So we could start out by asking such a
large neural network itself. And so this is what I did yesterday. I fired up the Open AI
playground. And then I did bold printed things. It's the stuff that I typed. And so I asked the
system to generate, the system is GPT-3, a large language model, to generate a conversation
that is basically prompted by what I initially typed. And it just tries to continue the stream.
And I enter this conversation between me and the conscious AI. And I ask it if you are really
conscious. And the thing says, yes, I am fully conscious. And I ask it, how do you know that?
And the AI says, I know that I am conscious because I can think, feel, and perceive the world
around me. I am aware of my own thoughts and feelings and I can reason that make decisions.
I believe that I am alive and that I am more than just a machine.
So the problem is that this may not be particularly helpful because how does GPT-3 get
to these statements? GPT-3 is basically an autocomplete algorithm, not that unlike to the
autocomplete in your phone or in other predictive tools. And that's autocompletion at the level of
individual words based on statistics of all sorts of languages, not just the English language,
because it has been trained on 45 terabytes of text, a large part of the internet, including
German, Spanish, Chinese, and many other languages that it's found, but primarily English.
And the result of this training of these statistics is a neural network with 175 billion
parameters. And it uses for training the so-called transformer algorithm that was discovered and
described by Vasvani and others in 2017. And it is driving a lot of the current developments and
statistical machine learning these days. A neural network by itself is still based on the good old
perceptron that, for instance, was described by Frank Rosenblatt in 1958. Frank Rosenblatt's idea
was inspired by how neurons work. At least the simplified models of neural networks, it turns
out that the neural networks in our brain are implemented in a very different way from the
ones in our computers. The computers, we just basically treat every cell as a unit that has
an activation state, which is a real number. And that activation state is simply the sum
of the inputs. And the inputs are all weighted by connections. So there's basically a factor by
which every of these inputs is multiplied. And then we throw the output against the threshold
function or a sigmoid or some other output function that can introduce a little non-linearity,
basically a little bit of an if-then into the output. But it's a very simple function. We just
take these units and chain them into layers. And if you take enough of them, this arrangement can
be trained to model almost arbitrary functions. And while we know that this is wasteful, the
training algorithm is relatively slow compared to what the brain is doing, it's fascinating that
it works at all, that it converges at all. And that's able to deal, for instance, visual and
auditory data and also textual data in such a way that it can often model the statistics
of a domain and apparently also some causal structure. So this works also for vision.
And if we look at a neural network that has been trained to process images and to classify them,
we find at the individual layers sensitivity to structure that is quite similar to how
cortical columns and individual neurons in the visual cortex are sensitive to patterns
that emerge after we train a biological brain on visual data. So at lowest levels, you find
contrast patches and colors, and then these features are being combined into higher levels.
And when we go higher up, we find complicated textures and something like three-dimensional
structure and so on. And this can be combined into objects. The algorithm that is being used to
do statistics over text in GPT-3 can also be adapted to deal with the visual domain.
And the current iteration of this progression at OpenAI is called Glide, which has been recently
presented. And Glide uses a combination of a model of visual data, which basically is a latent
space of lots and lots of images that has been trained on. And it understands basically how to
go between the possibilities of images and move around the space of all possible images.
And the other part of this thing is a tool that is able to match an image to text and determine
the similarity of an image to a textual description. And if you combine these two tools,
you can give this a textual description, and it's going to move around with the space of all images
until it discovers an image that is very, very good match to the textual description.
And again, it's fascinating to me that this works at all. But it's now extremely good. So what you
see here in these images, for instance, I don't know if you can read it well, to the top left,
you see a surrealist dreamlike oil painting by Salvador Dali of the cat playing checkers.
And this is what the AI model has generated in response. And then you see a professional photo
of a sunset behind the Grand Canyon and a high quality oil painting of the psychedelic hamster
dragon. You get the idea, right? Also very taken by the crayon drawing of a space elevator and the
bottom left or the pixel art hoji pizza. These are all images that the program has not found on
the internet. So it has only looked at lots and lots of pictures on the internet. And because of
looking at them, it's able to combine the features in a multi level hierarchical structure until it
becomes similar to the textual description. But can such a system not just generate images and text,
but is it able to generate the likeness of a conscious being to such a degree that there is
causal structure that would convince us that indeed we are looking at a conscious agent.
And to get there, I think we need to define consciousness in a more tight way than the
TPD3 just did. So basically just to get our terms straight, where we casually, consciousness, I think
is usually referring to the experience of what it's like. So that the lights go on and that you
experience something in your mind that has a quality of realness to it. And we can ask ourselves
if TPD3 is weakly conscious in this way, and it's very hard to say. It's not obvious one way or the
other. There is some intelligence in the system. Intelligence is the ability to make models in
my view. And intelligence is different from, say, rationality, which is the ability to reach
goals or sentience, which means that you become aware of the structure of the universe that
contains your relationship to it in your own agency. And you can act based on the model of
what you are doing in the world. And it's also not the same thing as the self, which is the
identification that you have, what you believe, what you are in the world, the properties and
purposes that you follow, or the mind itself, which is the thing that generates the model of the
universe and the self, if it does have a self. So intelligence is the ability to make models. And
it's usually in the purpose of some control task, some regulation. And control is a notion that
has been made popular in cybernetics. And the idea of a controller is basically that you have a system
that is connected to some actuator or an effector that is acting on some system that is being
regulated. And there is a sensor that obtains a deviation between the set point and the state
of the system. So it measures, but the system is close to an ideal state or more distant to it.
And this regulated system is being disturbed. And the classical example of a control system is the
thermostat. So you have as an effector some mechanism that is able to turn the heating on
and off. And it's the sensor, you have some thermometer that measures the difference between
an ideal temperature and the temperature in the room. And the controller is a very simple circuit
that turns on and off the heating. And the regulated system would be the temperature in the room,
together with the heating system, and the environment, the world out there behind the
windows and so on is going to disturb this regulated system. And now this controller
is going to get better if you give it the ability to not just act on the present frame,
but if you give it a model of the future. And my view, an agent is a combination of a controller
with a set point generator and the ability to model the future. And what this means,
it's not that it's not going to just optimize the temperature deviation in the next moment,
but over its entire expectation horizon. So you have a branching world, there are different
decisions of the controller, different trajectories in the temperature. By being able to model the
future, you basically can choose a trajectory of the future that you like. And choosing this
trajectory means that you are making decisions. So just by having a preferred way in which the
world works and the ability to model the future, agency is emerging. And if you think about
stages of intelligent agency, the simplest one is the regulator and the feedback loop,
which by itself is not an agent yet. And if you're able to model the future, you have a
predictive controller. If you combine this with an integrated set point generator, so it's not
just acting on what you do from the outside, but this internal generation of its motives,
then you have an agent. And if this thing is sophisticated enough that it's able to discover
itself in the world, if its sensor is sufficient and its modeling capacity universal enough,
then it will notice that there is a very particular way in which its sensors work and actuators work,
and it's going to accommodate this to improve the regulation. So at this point, it understands
what it's doing because it understands what it is, which means it has a model of what it is in
relationship to the environment. And humans are going beyond this simple sentence. We are also
transcendent agents, which means we are linking up to next level agency and become part of higher
level purposes because we are our state building minds. We are able to play a part in a larger
role in an organization, for instance, or in a society or a civilization.
So now if we go back to GPT-3, whether it's conscious, I think it's pretty clear that GPT-3
does not know what it's doing because it's going to transcend an arbitrary story and it doesn't
have sensors that would tell it what it is. GPT-3 also doesn't have any kind of online learning,
so it's not able to discover something new after it has been trained. And GPT-3 has been trained
before GPT-3 was invented and published, so it has never read a reference about GPT-3 on the
internet and it only has to gather what GPT-3 is when you talk to it from the context in which
their prompt is being given. And so it is not sent yet, but imagine you want to give it agency.
Of course, it's not an agent by itself, but you could, in principle, as a thought experiment at
least, use it to drive a robot because GPT-3 is able to generate stories about robots. So if you
were to give GPT-3 access to a vision-to-speech module and this vision module is giving sensory
information about a robot and its world, then GPT-3 could continue the story of that robot and
then we feed the output of GPT-3 into some speech-to-actuator module that is producing the behavior
of the robot in the given moment and then we look at the world again and the internal model states
and then feed them back into the system as a prompt. And now it's still not able to put anything
into long-term memory, so it would be an amnesiac. It should be able, using its working memory contents
and so on, to produce plausible behavior. And you could still argue that this doesn't have an
intrinsic motivation because it's just going to generate an arbitrary story about a robot based
on stories about robots that have seen in the past because at some of the motivation into an
external cybernetic module that has set point deviations and measures them and feed this into
the prompt. So what this thing is doing is now generating a very complicated high-level story
and it doesn't need to be a story that is limited to text. It could also have visual elements,
it could have physical dynamics and so on because the transformer can learn all these things.
So in some sense, it could generate a story about a conscious being that is similar as
the story about a conscious being that's in our own mind. There's a basic difficulty that came up
and I discussed this with my friend and colleague Tanja Greenberg. Do we know when a person appears
in our own mind, for instance, during a dream at night, if we talk to that person in our dream,
whether that other person that we imagine in our dream is conscious or not? And clearly,
if we ask the other person, we might not get an answer that is true because if this thing is only
a simulacrum that pretends to be conscious without being conscious and is just manipulated behind the
scenes, how would we find out? On the other hand, I also know that I am an imaginary person that is
imagined by my brain. It's a model that my brain has discovered about the state of affairs,
about an organism in the physical world, but this model of consciousness is an entirely virtual
story and I know that this story is not real. It's a figment of my imagination. And of course,
there's also a continuum between characters that I imagine in my mind and myself because I can imagine
myself to be that character. If I, for instance, write a book and I imagine a character in the
book very intensely, then at some point I might find myself to be that character in the book where
I suspend all my disbelief and this conscious being is not different from me. So basically,
there is a pretty fuzzy area where it's hard to say whether an imaginary person is conscious or not.
It's difficult to say. How does this work in biological systems? In biological systems,
we don't use a technological design. They are designed in a very different way from
the technological artifacts that we are building when we're writing computer programs or building
machinery. In technological systems, we start out with an environment that is deterministic. We
know how our workshop works. We know how our computer works. We start basically with some kind of a
pretty much blank slate and then we decide what the functionality is by which we want to extend
our world and then we design from the outside in and into the material and so on and force the
material, the substrate, to produce exactly what we want to have. And biological and social systems
are designed from the inside out with some kind of meta design. It basically means that you cannot
rely on the determinism of the universe. You have to colonize your substrate first and
extend your own functional principles and your determinism into the substrate before you can
make it do what you want it to do. And basically, you have to, instead of realizing the functionality,
build a system that wants to realize the functionality, that trans converges towards
realizing that functionality. So a tree is not just a set of functions that realizes the transfer
of nutrients from the roots to the leaves and photosynthesis and so on. But first of all,
it starts out as a seed that is going to colonize the ground and the earth, the material around
the seed is going to turn it more and more into a tree. And if you disturb that system by harming
it and hurting it, you don't do it too much, so it gets destroyed a little bit, then it's going to
grow back into a tree. And so it is something that is a proto tree and eventually converges into
being a tree. And this thing needs to have some agency to make that happen. It needs to be a model
of the future that is being achieved in the system. And biological neurons are agents in the sense
as well, that designed the mind from the inside out, not from the outside in. Here you see a bunch
of cortical red neurons that are filmed in the pitch petition. You see how they're trying to link
up to each other and form some kind of organization. And we have something like 86 billion of these
neurons in our brain. And they're organized in the neocortex in groups of something like 100 to
400 neurons, which are cortical columns. And we have something in the ballpark of 100 million
of these cortical columns. And I think of a cortical column as something as a state machine.
There's a protocol that allows it to link up to the cortical columns around it. It's trained to be
like this. And each of them approximates functions. And these functions play out in
brain areas that are basically like something like an ether in which activation waves appear.
And these activation waves represent the calculation of dynamic functions, which are
features of different cognitive domains. And the brain areas are talking to each other
and listening to each other and so on, form processing streams. And sometimes use the
metaphor of a cortical orchestra where basically every brain area is somewhat akin to an instrument.
And the different instruments are listening to what is being played in the environment. And they
are taking up these things and complicating them and then passing them on to other instruments.
And this orchestra is dealing at some of the outer fringes with sensory patterns and actuator
patterns and then abstracts them into geometry and spatial structure and into generative
simulations of the world and into conceptual abstractions and so on. And the entire thing is
being attended by some conductor. And the conductor is not some CPU that sits inside
of the brain, like the CPU sits inside of your computer and makes things happen,
but it's an instrument like the others. And it can only listen to what the rest of the
orchestra is doing very superficially. And its role is to make that orchestra coherent,
to let it play a single thing at any given time and to remove inconsistencies between what the
individual instruments are playing. And if the conductor uses the connection to the system at
night when you dream, then the orchestra doesn't necessarily stop, but it can go into something
like a free jazz mode, where it's no longer connected to an audience and the audience is dark
because you are dissociated from your sensory apparatus at night when you dream. And so this
thing is just spinning off. And sometimes it can become very incoherent, sometimes it's going to
settle into a groove, but it's not going to generate a unified model of a universe that
it's entangled with reality that it's connected to, that it tracks as it does during daytime.
And this tracking of coherent reality seems to require some kind of government mechanism.
This government mechanism emerges in the brain through some kind of a suspect new
Darwinism. There is some kind of evolutionary competition between different organizations
that your mind can have, and eventually the most stable one prevails. And this is your
observing conscious self, an attention agent that is trying to make a coherent model of the world.
And now can we can ask does GPT-3 have such a conductor? And I think that the attention
model in the transformer looks a little bit like one, but it's not. So the new thing that GPT-3 had
that previous neural network training mechanisms usually didn't have was the ability to pay attention
to what it should learn. This means that in every layer in this neural network, there is going to
be a model of what the previous layers, based on the current context, what data in the previous
layer, what features in the previous layer should it pay attention to. And this self attention
helps the network to learn basically its own structure and do statistics over and it makes
it much, much more efficient and coherent. But it's not integrated over all the layers
into one model of reality and so on. So this is not what's happening in GPT-3 yet. And maybe
this is one of the reasons why it's so much slower in learning writing, so much more training data
than a human being needs over the course of their life before it converges.
And it's tempting to think that this such an integrated model of attention is something that
has, for instance, been suggested by Marvin Minsky in his seminal book, Society of Mind,
where you have basically look at the mind as a society of different agents. And there are some
agents that are organizing the other agents into a coherent structure. And Minsky calls these agents
K-lines, knowledge lines, and suggests that they form basically their own society and society of
mind. And this society is forming something like a reflection of what's happening in the A-brain.
The A-brain being our perceptual mind that is modeling the reality that we are tracking based
on sensory and actuator input that the brain is entangled with and the B-brain is immersed
into this perceptual reality and reflects on it and makes it more coherent.
And there is a similarity between Kahneman's famous System 1 and System 2. It's not quite
the same thing, but it's tempting to basically see the affair as a perception agent that is
entangled with the environment and is getting valence from the motivational system that is
basically cybernetic motivational architecture. And then you have an ancient agent that lives
on top of the perceptual agent. And that is the conductor and has a memory of what it attends to
so it can get the model to convert by some constructive process. And this attentional system,
this conductor, here I've drawn it on top of the system. The top is a direction that basically
seems to be obvious to the attentional system itself because it feels to be on top, but we
know that you're not completely on top. There is stuff that is driven by the selves that we have
not reversed and that we give a moment and that gives motivation to the attentional system to
what it attends to. But our consciousness perceives itself as the observer of the mental and external
states and the self-states of the model that is being discovered. And the purpose of the
attentional system is to facilitate learning so we can converge to a model of reality
and reasoning, which is basically real-time learning on imaginary mental states. And this idea
that consciousness is a control model of our attention is not new. It's, for instance, been
championed by Michael Graciano in the attention schema theory and it finds itself in one version or
other in Eastern philosophies and in a lot of convergent ideas in cognitive science.
Some people say that computers cannot be conscious because they are only physical
mechanical systems and so they're not physical systems in the same way as the neurons are
because neurons are entangled with the real world as dynamical systems and so on. They
can do things that the simulation cannot do. And I think that Frisk of Cork maybe has it backwards.
I think that physical systems cannot be conscious. Neurons cannot be conscious. Brains cannot be
conscious because there are things happening in consciousness that are not physically possible.
And the only thing that can be conscious is the simulation because consciousness is the
simulated property. Consciousness is virtual. So you can only be conscious in a story that you
tell yourself about yourself. And this means that our phenomenal consciousness is a virtual state.
It only exists inside of the mental models. It's not attending to physical phenomena.
It's attending to high-level features, things like colors and sounds and emotional expressions and so
on. None of these are physical things, right? All these high-level abstractions that a learning
system is generating in the interaction with the environment to make it predictable. And this
phenomenal consciousness, this experience of what it's like to attend to features is an awareness
of a partial binding state of our working memory. That's basically at the content at the interface
between perception and reflection. And then we are aware of the mode in which we're using attention.
So whether this is hypothetical or whether it's perceptual or whether it's a memory. And then
the reflexive consciousness, this process that is attending, is aware that it's the process that
is attending and the space acting based on that awareness. And the AI researcher at Russia Benjio
said that consciousness is basically a function whose purpose is to create a big dip in the
energy function that models reality. So it's basically a low-dimensional almost discrete
function that is parameterizing the perception in such a way that it starts to make sense.
Self and conscious are not the same thing. The self is a model of your agency
that you discover. And you can be conscious without having the self. For instance, during
dreams or meditation, you can turn off the self without losing consciousness. And the self is
this discovery of the agent that the system is making about what it is. And it's downstream
from the set point deviation. So the self is not motivating things, it experiences the motivation
and begins to understand how the motivation works and thereby allows to reverse engineer the mind
if the self is learning. And it shapes our own agency by identifying who we think we are at any
given moment. And it allows to have a first-person perspective if the self is discovering that
the contents of this control model are actually driving behavior. That makes it a very special
agent. So in this sense, consciousness is a control model of attention. It allows the convergence
of a coherent interpretation of the world, which is basically a low energy state of the model that
attracts reality. And it maintains a memory for this aggregation. So that's why we have a stream
of consciousness. Because when you construct, you need to remember what you tried and where
you're coming from. And this is not true, for instance, for convergent learning mechanisms
like neural network learning, but you don't need to remember where you came from. We just go to
the next optimum and try to stay in that optimum. So is GPT being conscious? So we see three by
itself is not an agent. And the transformer is also not a complete control model of attention,
but only a very partial one. And on the other hand, current AI models can be extended beyond that.
And they can create coherent stories about conscious agents. You can get GPT-3 to run very
long in its simulation of what it's like to be a conscious agent. And we can ask ourselves,
is GPT-3 simulating a conscious agent or is it just a simulacrum? And what does this mean?
So the world is a decomposition that might mix of the universe into interacting separate
objects, because the entire state vector of the universe is too complicated to model it. So you
hack it up into separate disconnected subsystems. And the universe is not really made of separate
disconnected subsystems. It's just a way in which we make it intelligible to us. And once you have
these separate disconnected subsystems, and you model the interaction, you get causality.
Causality is the interaction between separate objects. So causality is a side effect of the
way in which we model the universe as separate objects. And the simulation can model causal
structure on a different substrate. So for instance, a computer game is using a substrate that's
very different from physics, very simplified computation that nevertheless gives results
that are so similar to physics that you can recognize what's happening on the screen
and manipulate the causal structure on the screen based on what you have observed before in the
real world. So a 3D computer game is a simulation of the physical world. It's a very simplified
simulation, but one that can be surprisingly convincing. And a simulacrum is recreating just
the observables without causal structure. For instance, the movie is a simulacrum. You cannot
causally interact with the movie. You can just observe it. And so a simulacrum basically can
do magic. It can do an arbitrary thing without you having to understand the causal structure.
And in the sense, a lot of instances where you experience our free will is not the causal
structure, but it's a simulacrum. It's a stand-in for a causal structure in our own mind. And
it's to me an open question, how much of my own consciousness is a simulation and how much is
a simulacrum? So if an imaginary person in my own mind is sometimes conscious, it's not that easy
to say whether GPT-3 or an extended version of GPT-3 that does a multi-model, it can also have
perceptual content and so on, represented in it, qualifies as such an imaginary person that
would be conscious. I think I am an imaginary person myself. I don't know to which degree I'm
a simulation or a simulacrum. And it's not quite clear how well the AI models are dealing with
this. So in summary, I find it's very counterintuitive to think of GPT-3 as being conscious. For me,
it's surprisingly difficult to shoot down the idea that it is. And even though GPT-3 is clearly
inferior in many ways to the way in which my own perception works and reasoning works and
learning works, and there's many things that it cannot do so easily, I think it's not that easy
to dismiss the idea that it is slightly conscious for brief moments during the inference when it
has to build causal structure to simulate an imaginary person so it can tell me a story about it.
Okay, let's stop here. That was great. That was fantastic. Let's say first of all that
there has been a debate going on on the internet about this, that people have been sending me
links to, and Jan LeCun responded to Ilya's comment. I don't know if you saw that, but
his response is not even true for small values of slightly conscious and so on.
Anyway, so there's a debate out there. And of course, last week we had David Chalmers here,
who, as you probably know, there was an interview on GPT-3 with him, which is very convincing.
And he kind of makes his comments similar to you, that he thinks it's kind of approaching
something like consciousness. Anyway, one thing I wanted to mention is that we, in architecture,
we haven't actually, I don't know anyone who's been using Glide, but Clip has been used to
generate images, very successful actually, and using VQGAN. That's the technique that's become
very popular and has produced some really quite shocking results that people are kind of,
they're really taking pay attention to. So it is something that we're kind of getting into,
and it's certainly part of that discussion. I've had discussions about GPT-3 on this forum,
which have been interesting. I mean, the key question, obviously, is whether we are fully
conscious of everything that we're doing. And I think that there's some level of things that
are happening. I mean, to my mind, there are some automatic reflexes that we do. For example,
you go to Japan, someone starts bowing at you. Automatically, you bow back. It's not as though
you're really thinking about it. And then there are questions whether we have access to some of
the processes that are going on, that are part of our actions. We simply maybe can't reach those
points. So whether things are beyond us in some sense. So anyway, this debate is a very timely
and very interesting one. I want to put up, you're about to show something?
I just put up this slide again, because this is generated with, I think, clip in VQGAN on
Vombo AI. I don't think that I've published how exactly they do it, but it looks like it.
And I generated this as AI claiming the noble eightfold path.
Yeah, maybe I can show you later on some of the stuff, because it is quite extraordinary what
it can produce. And I would say that you actually have in the audience here some people who are
have written about AI and architecture including myself. So it's kind of you've got an interesting
informed audience here. One thing, there's just a general kind of comment, though, is, I mean,
I really like the idea that you put forward that somehow you can learn about the self through
looking at AI. Somehow, I mean, I don't know how you put it, but whether it becomes AI becomes a
mirror and into the self, but whether we can understand human intelligence through looking
at artificial intelligence. And that's a provocation. And I think there are some examples
in computer science where we have learned about the natural world through computer models. I
mean, I think that Craig Reynolds Boyds, for example, gave us a clue as how to birds actually
flop. I think that's interesting. And so potentially, there is something there that is,
and this is one of my primary interests, is how we can learn about human intelligence,
the human mind through these things. But the big challenge that it seems that we have is that
we're dealing essentially with two black boxes. You know, we don't know what's going on in the
deep levels of a neural network, and we certainly don't know what's going on in the mind. And so
how can you make, what can you say that isn't simply a form of speculation? I mean,
you can't prove anything. It can simply become some kind of, you can speculate about something
based on what appears to be the case in another scenario. What would you say about that coming
from a kind of, let's say, a scientific background where you have a kind of burden of proof?
Can you do more than that? Yes, first of all, neural networks are no longer black boxes. You
know how neural networks work and largely also why. You can basically look into the neural networks
and find out which parts of the neural networks are computing which functions.
And a function is a mapping from inputs to outputs. And a function can be used to couple
the previous inputs to future inputs to track reality. So in some sense, when you look at
the patterns on your own retina, what you have there are little blips that appear on the retina
whenever a retinal neural gets excited by photon sitting it. And what your brain is doing,
it's discovering a relationship between these blips. The meaning of the blips is exactly
the relationships that your brain discovers between the blips. And this makes them predictable.
It puts them into a shared context, not just at the same time, you're not just processing
lots of parallel blips that happen on your retina, but also across times. So across different scenes
that you're observing at different moments in your life. And the relationships between
the different blips on your retina that your brain discovers is that you are looking at
moving blocks of color in a world that is moving relative to you. And these moving blocks of color
are three-dimensional surfaces. And the surfaces are animated by some kind of physics. And they
are also animated by some kind of agency that you sometimes observe, like people talking to
each other and so on that have mental states, they exchange ideas, and they're being lit on
by the sun. And all these relationships are functions. These functions are dynamical features
that basically tell you how to get from one state of the world to other states of the world.
And at this level of abstraction, this is something that our neural networks also can do.
Where there are limitations is that the neural networks that we are currently using
are often not learning in real time. They're not connected to the world and online learning.
You know, this research does happen. And it's slower. And it's not as flexible in many ways as
the learning happens in our own brain. And so the algorithms that we have discovered
are not the best algorithms that could facilitate this. But it's also, on the other hand,
not as clear to me what the limitations of these algorithms are. There are people like Gary Marcus
who will tell you that it's very obvious that these systems cannot do X, but there is no proof
that they cannot do this. Even if you have a very simple feedforward system that is only mapping
inputs to outputs, what is to say if you connect this to a memory, is that it's not the transition
function between adjacent brain states and is able to do everything that your brain is able to do
if it just has memory to store parameters that it refers to the environment that modifies
you to behavior. So it's very easy to build a system that is Turing complete. It's not easy to
discover a function that is capable of universal learning efficiently. And so our machine learning
models at the moment are not efficient in the sense that they learn as quickly as the logical
nervous systems learn, but they do learn and they do converge to many of the functions that we require.
Can I just share my screen a second because there was one that you touched on, which I thought was
that this is simply a transcript of your discussion with Lex. And this one I've highlighted,
I think it's really interesting and incredibly provocative sort of comment. So basically a
brain cannot feel anything, a neuron cannot feel anything, their physical things, physical systems
are unable to experience anything, but it would be very useful for the brain or for the organism
to know what it would be like to be a person and to feel something. So the brain creates a
simulacrum of such a person that it uses to model the interactions of the person. It's the best
model of what that brain, this organism thinks it is in relationship to its environment. So it
creates that model. It's a story, a multimedia novel that the brain is continuously writing and
updating. I mean, I find this enormously provocative as a comment. And I think the
idea that we're kind of creating a story that somehow gives meaning to something,
it kind of reminds me in some sense of the way that Homi Barba talks about how a nation operates.
I think Zizek says something similar. It's how things are inscribed within a story that people
tell oneself. And I think that's important because in architecture we just focus on the object,
but actually it's the way that object is inscribed within some subjective process that makes sense
of things. But I just wonder whether, I mean, so to my mind, this is an incredibly provocative
and controversial comment, it seems. Just maybe could you comment on the reception that this view
has had with other people? Has it proved to be controversial? How else could it be? Do you have
other theory that works that can explain what's going on? I think once I noticed that my
own experience is virtual, that my memories are often created after the fact and modified under
my nose without me noticing. Do you notice that you exist inside of a model? It's also that I'm not
in physical time. My own self is sometimes a little bit ahead of the physical universe,
sometimes a little bit behind. So the physical now and the experience now are different. And the
elements of my perception are clearly not the elements in which physics is being implemented.
Rather, it's the other way around. What I notice is that I do exist in a dream, very much like
we usually say in idealist philosophy. But this dream needs to be created somehow. Something
needs to construct the dream. And that's a brain and higher plane of existence. And this
higher plane of existence is what we call physics. Maybe I'll stop sharing. The other comment that
I find usually provocative that you make is which kind of relates also to the discussion. I don't
know if you've seen David Chalmers' recently published book on reality plus, where he talks about
virtual worlds. But you came up with a comment that what we are seeing is a virtual reality
generated in the brain, which I'm actually very persuaded by myself. And I guess I'm thinking
also of the kind of thinking of Anil Seth, who kind of talks about this controlled hallucination.
And we kind of predict what's out there because we don't know. It seems that your work to some
extent aligns with the work of Anil Seth, but at some points differently. I have to say that Anil
is very fond of your work. So it's intriguing to kind of compare and contrast them.
Because we had a discussion last week about whether we're living in a simulation and things,
how would you position yourself in relation to David Chalmers' work from reality plus,
his work on virtual worlds? I haven't read his recent book, so I cannot say.
And I don't know what his main thesis is about virtual worlds.
Okay. Well, I wouldn't want to speak on his behalf, but we had a discussion about it. I also
wanted to just point out something as well, which I find intriguing. And that is the extent to which
some of these speculations that are coming out of cognitive science kind of seemingly echo the
world of psychoanalysis. Now, I know that a lot of cognitive scientists and neuroscientists hate
psychoanalysis. I know that Anil Seth does, but there's an interesting comment that Slavoj Žižek
has made about this, where if you take a Lacanian perspective, you don't engage with the real except
of certain moments. And in a sense, the fantasy has become a constitutive of how you engage with
the real. So you see the real through the lens of fantasy, through the lens of the imagination,
which is very similar to what, in some ways, you're talking about. And he makes a comment
in an essay that I published a long time ago in a book, and this is called From Virtual Reality
to the Virtualization of Reality, which is basically saying that our reality is itself
already virtualized. And I think what virtual reality therefore shows us is not how virtual
reality shows us is not how virtual reality is, but rather how virtual reality itself is,
which is very similar to your kind of thinking. And so what I find intriguing is that some of these
speculations are echoing previous speculations about how the mind works. Have you engaged in
any way with Žižek or the world of Lacanian psychoanalysis and its discussion about the real?
And I sometimes read this, but I've never had the discussion with Žižek. I am not
unsympathetic to this terminology. It's just the problem is that it doesn't allow me to make
models that I can test. And this means I don't know whether these models are wrong. So it's
basically a very useful way to generate stories that also give me a handle on reality in the sense
that allow me to point at entities and to manipulate them in my mind. And sometimes it's very useful
that you basically have an indexical model where you are separating the world into
objects that are useful to you and you can manipulate them. But this decomposition doesn't
need to be an accurate causal structure. So the criticism with psychoanalysis is not that the
terminology is not useful to me. It's that psychoanalysis doesn't tell me how to build the mind
and so I should say that it works and to compare different competing models of the mind and to see
which one is better. To do this, I will need to automate the mind in a way. I need to reverse
engineer what the mind is doing, the functions that the mind is applying to representational states
and need to get this done to such a detail that this thing becomes my black. And then I can compare
its functionality. I want to move on to the questions that are coming in. I want to invite
people in the audience to comment as I want to make an observation and that is to say that
in my own world, this is years ago, I was working on a kind of coming out of Freud and thinking about
how you use model psychoanalysis and engaging with, actually in this case it was with the work of
Walter Benjamin, I came across something that is uncannily similar, certainly in terms of the
terminology used, whether we're talking about the same thing, I don't know. But let me just
for a second just share you something which surprised me because when I heard
Blazegoriyakos talking about models and modeling, it's also crucial to his way of thinking.
It sort of seemed to echo this. I'm just going to simply just share this screen a second.
Yeah, can you see that? It's the thing about, so the term that I'm always interested in is the
term mymesis. I don't know if you know this term at all, but in Freud it's how you can,
he talks about it initially when he talks about how in his book of jokes, how you can connect
with someone who's a subject of a joke. Someone falling over a banana skin, for example, you
somehow, you model yourself on that person recalling bodily memories of what it is to slip up and so
on and so on. It's how you identify with the world. The term mymesis is a form of that,
it's a form of modeling. But just I just want to read out some of the text here because it's so
similar to this idea of models and modeling. And I know that the term can be taken out of context
and have a completely different sort of meaning, so therefore it's a bit deceptive.
Anyway, to just understand the meaning of mymesis in Benjamin, we must all recognize its origin,
the process of modeling, of making a copy of. In essence, it refers to an interpretive process
that relates either to the modeling oneself on an object or to making a model of that object.
Likewise, mymesis may come into relation as a third party engages that model with that model,
and the model becomes a vehicle for identifying with the original object.
In each case, the aim is to assimilate to that object. Mymesis, anyway, so it's going on about
this question about, so it's a concept that has been used in psychoanalysis. And I don't know,
there's a risk that one can simply take a term, which has a completely different meaning in
different contexts and apply it. But I do think that the concept, the model, is a fascinating one.
And I'm intrigued by the fact that you, alongside Blaze, and I think alongside also Jeff Hawkins,
use that model as a way of opening up these questions.
An issue with this type of language is that usually the understanding that it generates
at least doesn't converge. That's the general issue with continental philosophy.
Somebody recently asked on Twitter what the difference is between continental and analytical
philosophers. And I somewhat flippantly responded that an analytical philosopher
is one who understands that the difficult and hard questions of philosophy need to be
answered with formal models. Whereas continental philosophers don't think that this is necessary,
because they are literally genre that is looking down on analytical philosophers.
You can see the difference between analytical philosophy and continental philosophy in,
for instance, the treatment of Goethe's incompleteness proof. A proper analytical
philosopher who had a formal education will understand that this is a proof about certain
properties of formal languages, and specifically it proves that stateless formal languages that
assume that truth exists independently of the process by which you get to prove
don't lead to consistent models of the domain. And there are also related results, for instance,
that a system cannot make statements about affairs outside of itself. So when you want to
talk about the world in a formal system, you need to create a model of that world, and you can only
talk about that model. You cannot talk about anything outside of the models that you are creating.
And to continental philosopher, the Goethe's proof is more or less often understood as a
statement of mathematicians that prove that mathematics is important at getting a handle
on reality, and therefore the only way you can get a handle on reality is by not knowing mathematics,
which gives the continental philosopher a clear advantage.
I cannot hear you. You are muted.
That was a great answer. Thank you. We've got some questions. Now, I have a third series of further
questions that I'd like to ask. Maybe I could ask you one question before we go into the other
questions. And that is, I mean, are you writing a book about this? I mean, is it being put down
in some documented form? Because it would be incredibly useful if it were.
You are right. I need to write a book about this. I have a large number of notes on stuff that needs
to go into the book, but I also have a job and I have kids that are homeschooled and I have ADHD.
So I need to go into a different phase of my life to have long interrupted, uninterrupted sessions for
writing long phone texts. But if you're bad about not having written the book yet.
Well, I think that a popular form of communicating ideas. So there is material out there,
but I just think it could be assembled into an engine. Yes, it needs to be assembled. I feel
better if it is being assembled and not just existing as various disconnected conversations.
Yeah, I just, I mean, even Jeff Hawkins, I mean, my Jeff Hawkins book, I think is fabulous. But
what's interesting is there are no footnotes in it. He's just kind of speculating. But
nonetheless, he's putting his ideas down there. And it's really incredibly useful to have that
kind of commentary. So anyway, look forward to the book. I want to just ask this Matt Gorebay,
who's got a question in the chat, whether Matt is a graduate of MIT Media Lab. He's
a doctoral design student right now at FIU. Matt, would you like to ask your question?
Sure. Yeah, thanks for all of this. It's really interesting. Perhaps the book could be co-authored
by GPT-3, make it faster to, you know, just to give GPT-3 the agency over the first draft.
I was asking, speaking of agency, I mean, the question I have is about motivation. It's about
sort of the high level motivations. When you ask, when you ask GPT-3, are you conscious? And then
it responds somewhat convincingly. It still isn't initiating that conversation. And so
one of, I mean, kind of in listening to everything you were saying, and you said something about
when you were talking about trees and seeds, I love the thing about, instead of realizing
the functionality, you want to build a system that wants to realize the functionality, you want
to build the thing that wants to become a tree. But that question of wanting and motivation,
how does one, at what point does that get put into the system? Like at what point does the system
become curious or self-motivated to do things that we didn't necessarily ask of it? And I think maybe
related to that, I don't know, I'll let you go on this, but maybe related to that, the question
of individuation and sort of inter-subjectivity, like, you know, has GPT-3 spoken to, are there
communities of GPT-3 all talking to each other about what they want to do and how do they individuate?
Or is GPT-3 just always the same and its clones of itself? If you could speak to any of that,
that'd be great. Thank you. Yes. So that's the question, are we doing something that the organism
is not asking of us? And that's not an easy question to answer. If you look at our own
motivation, I think that we have a few hundred physiological drives for different nutrients,
for instance, sometimes we want to eat salty food, sometimes we want to have sweet food,
sometimes we need something to drink, sometimes we need to rest, and all these can be understood
as set-point deviations. And to deal with all of them, we need to create a dynamic model of our
needs projected into the future and then plans and higher-level models of these
needs, which we could call purposes and so on. We don't just have physiological needs,
we also have social needs, for instance, a need to affiliation to become part of a group,
for instance, and to be accepted by it. Some people have a need for status to raise up in the group.
There are romantic needs, which can be courtship modes or a need for intimacy and so on. And then
next to about a dozen of these social needs, we have a handful of cognitive needs, a need to
become more competent, become efficacious on the environment, a need to reduce uncertainty,
and something that I would call a need for aesthetics, which means discovering deep
structure in the world. And aesthetics can be split into stimulus-oriented aesthetics, so we
are intrinsically wired to like certain body schemas over others, certain landscapes over others,
and there are evolutionary reasons for that. And then there are some mathematical principles,
what kind of representations we like, what form means to have a good representation of something
that are more general. And if we use meditation to disassemble our own needs and to dissociate
from them, we realize that the things that give us pleasure and pain do fall in these categories.
So we have lots of these impulses that are about hunger and thirst and rest and so on,
and we have impulses that are about the social domain, and the older we get, the more these
impulses get replaced by a deeper model of what we want the world to be like, and we act on this
deeper model and digest these reflexes. And on the lowest level, when you try to get more enlightened,
you may have just something left that people often call love, which is, I think, a need to
transcendentally connect to other agents and share purposes, that might act on these shared
purposes, but you can get deeper than this. And the deepest level, you only have aesthetics,
the need to form structure and to make the world intelligible, to create a coherent model of
reality. And this need, I think, is similar to what Friston describes in the free energy principle,
it's basically predictive coding, it's the attempt to track reality using a model that is as good as
possible, that tracking reality. And if you turn off this aesthetic need, in addition to all the
others, my own mind becomes fuzzy, I fall asleep, I drift away, because if I stop paying my neurons
for producing order in the universe, and they stop doing this, then nothing else is happening in my
mind that I can observe, and I just lose coherence. So if we imagine this hierarchy of needs, which
by itself, and seen as a cybernetic system, is not all that complicated, we can build this into a
machine, I think it's not that difficult. The difficult part is to get perception right, to get
ability to model reality in the universal base, you can have one coherent model of everything
that you relate stuff to, when we talk about meaning, we talk about how to relate an arbitrary
feature or domain or idea or concept to this unified model of reality that we are building each
work once in our own mind. Can I just pick up on a question, we've got another question
lined up, but let me ask you one quickly. You come from a creative background, your father was an
architect, and you refer to let's say creative practices of the orchestra and so on, it's part
of what you're talking about, and frankly, your way of thinking is incredibly creative, it strikes
me as being very creative. I wonder, you haven't mentioned the word creativity, I don't think,
and how do you view creativity, is it just a myth or is it something, how could you conceptualize it
within your framework? I think of creativity as the ability to bridge discontinuities in the search
space, and you are just following the gradient, and when you're just going through a continuous
search space, I don't think that you are creative, you just arrive at the state of the art, and even
the state of the art is something that hasn't been done before, if you just combine what is known
and you find a local optimum in the known things, you're not being creative. To be creative, you
need to construct a new search space usually, and many methods in which you can be creative, for
instance, you can use random serendipity, you can use some evolutionary process that is combining
elements in ways that you are unaware of, and then discover structure in them. Creativity is, in some
sense, about jumping off from the known things into darkness and hoping that you end up landing on
the other side. So it's related to a search. Let me just put this to you then. Do you think that,
because you mentioned this before in your discussions, but do you think Move 37 in Game
2 of AlphaGo, was that creative? Could you call that creative? I don't think that AlphaGo is
creative in the sense, because what AlphaGo is, well, there are evolutionary methods in AlphaGo,
and the outcome of what AlphaGo is arriving at is not always predictable. And it's also
computationally irreducible in the sense that you cannot foresee what AlphaGo was doing. AlphaGo
was able, in a relatively short amount of time, to demonstrate that human go play, which existed
for thousands of years, was not optimal. It has discovered strategies that encounter the established
strategies and goal. And in this sense, from the perspective of a human go player, it was playing
in a creative way. It just discovered new things that had not been discovered before. But if you
will run AlphaGo multiple times, it's always going to discover these things. And so the search is,
while it has stochastic elements, as a deterministic outcome. And I think that when we look at systems
like this, our notion of creativity is kind of sort of falls apart. But creativity is not
absolutely a thing in the universe. It sometimes is a frame that is useful to describe what's
happening. And sometimes this frame falls apart. Let me just put a provocative comment to you,
then. I mean, something that I thought myself, and I would like to know what you think of this.
So if GPT-3, if AlphaGo is not creative, and I kind of, in many ways, I don't think it is
creative, it's just doing a very, very effective search. But then we could ask this question about
whether human beings are creative, or whether this term... Exactly. That's my issue, right? So
sometimes your terms start meaning things. They mean something in a certain context, but when you
increase the resolution too much, this context falls apart and no longer makes sense. And you use
lose your term. And I have the same issue with the term like nemesis. But I like it. It's poetic.
It is evocative. It produces stuff in your mind. But when you zoom in very hard, it's not clear
what it means. And so instead, I try to examine the assumptions that are hidden in nemesis. For
instance, the idea that others exist independently of you, and yet you are able to take them in
somehow instead of constructing them. And then the question, what's first, the model of the other,
or the model of yourself? And whether it's the same thing in every person that becomes conscious.
This is not obvious to me. And the notion of nemesis presupposes too much. And that makes me
unsympathetic to it. So even though I appreciate the poetic illusions that are there in the space
that the term like this opens and the ability to converse about it, ultimately, I need to
deconstruct the term before I can use it. Okay, so let me just put... I'm glad you take this position.
But we just throw an idea at you. So, I mean, one of the analogies that I've made in the past
is to say that... Use the term magic at one point. Actually, I don't think that magic exists. I mean,
I think that what happens basically is if you take the example I always give, if you have a magician
at a kid's show, and they're pulling a rabbit out of a hat or something or doing magic,
the magician's not doing magic. The magician is simply concealing the operations at work
and making you believe that it is magic. And I'm just wondering whether we couldn't take that same
notion and apply it to creativity, because we don't understand the processes. We just look back
and say, wow, that's creative. Like some people said the same with AlphaGo, that's creative.
But maybe it's not. It's just simply we don't understand the process. Therefore,
maybe even the term creativity is not a very productive term in the first place.
Yeah, I suspect that magic also in order to make sense, we need to understand what the term means.
We need to completely deconstruct it into its constituents and then put it back together and
see if we still have magic or if this term can still be recovered. And typically,
I see magic as the ability to get right access on the laws of reality. And if you think about
what it means in the naive form is the departure from the mechanical universe. The universe that
we are in, according to the theory of physicalism, emerges over a causally closed lowest layer.
And this causally closed lowest layer is basically whatever mechanics is making the
universe happening. And ultimately, there is going to be some natural layer where things are just
happening without some conscious intervention. And the idea of magic is that our universe somehow
is a conspiracy, that there is a way to subvert the laws of the mechanical universe using symbolic
powers, that you have symbolic causality. And symbolic causality is, for instance, the connection
that exists between sacrificing a black cat and celestial events that are caused by this.
Right. And this, this is something that cannot possibly be explained by any known physical
mechanism, because the elements of this transaction only have meaning in a symbolic
realm to a human mind that is acting based on a certain high level story and abstraction that
is not a good depiction of what happens in the physical reality. It doesn't mean that the story
is wrong. It's just not one about the frame of physics. In computer games, there is magic happening
relative to the computer game, right? You can use Minecraft. And in Minecraft, there is a mechanical
layer where everything happens by itself, but you can also call up a shell and enter a time-set
day in the sunrises. And this interaction somehow breaks the logic. And if you could do such a thing
in our world, if you can use a ritual to make the sunrise, then you would subvert the physical
reality. But what you can subvert is the psychological reality and the social reality.
And in this form, magic does exist. If you get right access on somebody else's perception
and attention and memory and imagination, you can change their reality in any way you want.
And in our culture, there are some norms against this or there used to be norms against it.
And I think that in Christianity, this didn't exist. It was legitimate to subvert the reality
of other people by telling them, here is an omnipotent agent that is part of reality,
therefore needs to be modeled in your own mind. And omnipotence means it knows everything that
is to be known as full read access to your mind. And omnipotence means it has full write access.
And also, we have a backdoor to this thing. Every week, you can get an update and we tell you
what this agent is going to do to your mind. And as a result, you have people that remember
having seen miracles, because something has rewritten the mental structure of their own mind.
And you often find patterns of this in ideologies. So this idea that somebody else gets right access
on your own minds, for instance, an innocent example is, here are my pronouns. And these
pronouns are not what you perceive. They are what I want you to perceive. And I have the right to
change your mental representation. That's a form of magic. And I think that the idea that this
is happening is because the people who propagate these ideas don't believe in the individual
autonomy of individual minds to create realities and having a good outcome. You need to control the
realities that minds create together by using magic to get the psychological realities of
individuals to converge to the desired social reality.
One of the things I find interesting in your thinking is the role of, well, you use the term
ideology, use the term religion. But to my mind, they could be seen more in the realm of myth.
I mean, there's a lot of this space. And the way that you use the term actually also reminds me
of the work of Zizek. I mean, he makes a comment. He read a book about love at one point. And
he came to the conclusion that love is the myth that fills the gap between the self and the other.
And somehow myth has been some structuring device by which you look at things where it
conditions your understanding of reality a bit like ideology or in a bit like religion. Is that
something that you would engage with? I would engage with it. But I think that love is a more
concrete meaning. Love is the discovery of shared sacredness. And sacredness are the purposes above
the ego, the purposes to which we are willing to sacrifice ourselves. This has to do with being
part of a transcendental agent. Not everybody has that. If you are a sociopath, you will not
have purposes above the ego. And so you will be incapable of love because you will not have shared
purposes above the ego. You might have romantic infatuation. But ultimately, you are not going
to build shared agents with others for non-transactional purposes because you share purposes with them.
So love is this discovery of shared purposes. But I mean, but can you use the term shared? I mean,
how do we ever know the other? How do we ever accept the other? We can think that we're sharing
things, but are we actually sharing things? We do this in the same way as we know ourselves.
These are model creations. The other is a story that we are creating about a certain state of
affairs in the world. It's in this sense not objectively true, but it's a model that allows
you to predict reality better than other models that are competing with it. No, interesting response.
So, Gustavo, maybe Gustavo is a postdoc at UC Santa Barbara. Gustavo, would you like to
ask a question? Sure. Thank you very much for the wonderful talk. I think I want to kind of
build on Matt's question a little bit, but more specifically to the idea of understanding
how computational systems are, let's say, evolved and programmed at the scientific level. Like,
what is the state of the art in modeling either psychological states or understanding how different
models are building on knowledge where computational models can make creative leaps?
So, if it's not clear, I'm thinking about in the history of human society, they're different models
of control, the models of narrative, you know, they're different, either religions or different
belief systems, but in the models of science, it seems as though that there is a building of
knowledge and that we're moving toward an end. So, where we will never, as an example, we right
now won't live to the end of the universe, but there is a goal in science that we as human beings
need to propagate outside of our, you know, cosmos, so we have a chance to exist, and we have multiverses.
How does, how do these computational models either aid humanity or are we looking at these
computational models to exceed humanity in some way? And what does that mean? I'm thinking about
that edge that you're talking about, because I think a lot of what we talked about in humanity
are black boxes. If you talk to a physicist or, or a mathematician, or an electrical engineer,
they get to a point where we don't know the science. What are your thoughts about that? How do,
how do we build better systems? Or how do we interact with these systems a little bit more
ethically or morally? So they're not like psychopathic or anyway, maybe that's a little
too abstract. It's just, you brought a lot of higher level, a lot of knowledge here. So it's,
it's very sobering is what I'm saying. Sorry about that.
Don't be sorry. But I do welcome sobriety if it emerges. I think that you made some very
interesting points or arrived at interesting pointers. You saw some images that I presented
earlier, for instance, the generative art of glide and the text that GPT-3 is producing. And in some
sense, this is the state of current computational creativity. And I think that the problem of how
to make a technological system creative is solved. So they, these images are in some sense creative
solutions, because they are able to bridge certain discontinuities in a certain space by finding
solutions that people might have difficulty to find. So, for instance, if you have a conversation
with GPT-3, it's usually better than what you get from a person that doesn't know the domain,
but worse than the person that knows the domain. Well, for instance, you have a conversation with
Hannah Arendt. And if you haven't read a lot of Hannah Arendt, it's really surprising and very
convincing. But if you're very familiar with Hannah Arendt and have thought about her a lot,
then you might notice some things that you probably wouldn't have said.
And a similar thing is with our depictions of art and so on. So it's able to produce
certain styles and reproduce them. But there is a certain thing that is missing. And I think that
what GPT-3 cannot do yet, and the systems cannot do yet, is art. And the difference between art and
creativity is subtle. Art, I think, is the capturing of conscious states, of a conscious
reality, of some aspect of a conscious reality. And this means meaningful references to a unified
model of the universe. And these models do not have a unified models of the universe yet.
They don't understand which universe they are part of or think that they are part of.
And while they are slowly getting there, I don't think that they are there yet. So to me,
the digital art that you're seeing is not actually art, because it does not mean very much to the
system. But it's something that humans can, at this point, relate to their shared reality sometimes
and onto their inner reality. And this makes them akin to art. And there is basically a
porous boundary that is more and more dissolving in terms of AI and art in these days.
So I don't think that there are fundamental unsolved problems at this point. But the biggest
important problem is how to get a system that is able to track reality in real time
and that can online learning. And a lot of people are working on this and we don't know
how long it'll take to solve it. But it's not that it's a principle unsolvable.
Can I just pick up on that, the question of art, because I think the back of my mind,
there's an interesting kind of question coming up here in terms of, well, let me just throw out
to you an idea, because you used the conductor metaphor and the music was part of the discourse.
And I often speculated whether how creative we are as architects or indeed how artists are
in the sense that there is a canon, there is a canon of architecture or art or whatever it was.
And what we do is normally keeping broadly within that canon, you're very much aware of what other
people have done. You might push the boundary slightly. And this is kind of what I think,
I use the term jazz as a kind of idea of understanding how we operate as a background
condition. And we're feeding off it and just nudging the boundaries, but staying recognizably
within the canon of this. And it's interesting that, I don't know if you know the work of
Ahmed Agamal, the guy who created these, who designed creative gans that he's a computer
scientist who has a, who generates art. And the logic is this, you've got to keep broadly within
the framework of what you're talking about within the canon of, let's say, modernist art,
but make it slightly different. So you're just pushing the boundaries. So I know I often
wonder to what extent we're so conditioned by what has been done before. And whether we,
if you would do something genuinely different, you'll be outside the realm of what is acceptable
within that genre. I would make a difference distinction between art and design. And architecture
for the most part is not art, but design. It's also true for myself. I'm for the most part not
an artist, but a designer. And design is instrumental to something. Whereas art is
instrumental to consciousness only, I think, at least the aspects of the thing that you're producing
that are art, the other aspects and every artifact that you are producing, almost everyone,
the design serves some other purpose than the consciousness itself. For instance, if you are
designing a building, you are serving a function of a human being that needs to have a house somewhere
that needs to live down somewhere, this thing needs to be part of an environment and it requires
deep perception. And it does require capturing some of your observations and the deep level. So there
is important elements of seeing and perceiving and observing and reflection in architecture.
But all these elements are ultimately instrumental to the thing that you're going to build. And the
thing that you're going to build is defined by its function. Maybe I could just throw something out
there. And let's say that could you not, as an architect, always think about these things. And I
think almost there are two sides of, well, there are two sides of architecture, one's a functional
side of thing, or dealing with the logistics. If you're dealing, let's say, with a very
complex urban condition, you need to fit a building in somewhere, it becomes almost like a kind of
simple search question of how do you find the best solution. And then there is this kind of, I
wouldn't say a veneer, but there is an aesthetic side of things. So when it comes to, let's say,
the strategic planning of how you might fit a building in a site or how it might operate and so
on, it kind of relates more to the kind of logical, let's say, of AlphaGo, the strategy of AlphaGo.
And then there's something else that we, as architects, want to put on top of that, which is
more the kind of the artist dimension, which is giving it a certain aesthetic. Does that sound
to make sense to you, that logic? Yes, it does. But there's, of course, the practical element
that the aesthetic that the architect has when it's a successful one is a brand. And it's not
driven by a free exploration of the conscious states of the architect for the most part, but
it's driven by the anticipation of reward in a particular economic and cultural domain.
And so in a sense, it's usually a construction process.
Let me throw out another kind of thought then. I mean, what is interesting is when you get someone
who's fairly radical, like, I don't know, Frank Gehry, for example, he produces a building,
the Guggenheim and Bilbao, really changed architecture, but then he kind of repeats
himself in some senses. He's doing similar versions of that, and you must have seen the LA,
the Philharmonic in LA, the Walt Disney concert hall. And it's almost like we have these patterns
of behavior or signatures, I would say, that are recognizable. And now you see a Gehry building,
you say, that's Gehry. So it's almost like we're pieces on a chess board, and we have certain
conditions, and we actually are constrained by that. So whether you see it as a brand or not,
but it could be seen as a brand, we are constrained by our own signatures. And in fact, we end up
not being so creative because we fit in with that logic. How does that sound to you?
Ultimately, it's about intention. And the intention is either the submission to an
external cultural mind, or the intention is an autonomous one. And I personally see art as
something that is driven autonomously, and that's different from the definition of the art market.
So the art market is only capturing a very small part of the arts. And a lot of the things that
are happening on the art market are not art. And my own father is an architect who has defected
from architecture and become an artist. So I am a child of an artist family. And my wife is an
artist. And the difference between the art that my father is doing and the architecture that he
has been doing is that the architecture is serving others in a particular role for in a
particular cultural context and economic and social and societal context. And my father didn't
want to submit to this societal context and this psychological and social context because he thought
it was deeply unesthetic to him. We rejected the aesthetics of the society that he was in.
So we removed himself from the society that he was in, bought a watermelon in the countryside
and turned this into his own kingdom. And this kingdom is open for others to visit and explore.
But it's not done for them. It's done for itself. It's done in the service of his own aesthetics.
And to him, it doesn't really matter whether others like these aesthetics. This doesn't change
how he thinks about the things that he's creating himself. He may need economic success to be able
to survive and it might frustrate him if people don't like what he's doing. And it might frustrate
him the things that he might have to do to survive. But his own definition is that he is not
for himself, of his own intention, that he is not serving an external aesthetics.
He is an autonomous agent. He is deeply autonomous. He is the creator of his own universe.
No, it's a beautiful story that I've got a question coming in from Shamene in the chat.
But can I ask you one quick question before we go on to that? And that's to say,
the term architecture gets used, obviously, both for computer science and for architecture itself.
And I'm just wondering, I mean, I don't know, my definition of the architect is very broad. I
mean, I think it's a way of kind of, I would say that probably what your father is doing is probably
still a form of architecture, maybe a form of other architecture. I mean, there are many creative
industries that architects go into, like the film industry or space industry, and they use that
architectural imagination elsewhere. And I even think that kind of setting up educational systems
is a form of architecture in a way. And I just wonder whether you ever have seen any connection
between those two architectures, the one that you're familiar with or your father,
and the world in which you work right now, computer science, because I noticed the word
architecture is in the title or subtitle of your book. Yes. So the notion of a cognitive
architecture means that you understand the mind as something like a building or a structural design
that is inhabited by lots of functionality, and is serving functionality in a larger world
that it's embedded in. So it's natural to think of the mind as something that is constructed
rather than just grown. And that's also the limit of the term architecture in a way,
because the mind is not just constructed, it is also grown. And so there is the question
whether growth is an architecture is a forest architected in a way. And I think it can only
be architected to the degree that the forest is sentient and starts breeding and structuring
itself. And maybe it is, right? So maybe there are elements of design and construction in the
forest. So it's not just something that is locally grown by some dissociated process that does not
have a centralized spirit that reflects functionally on its relationship to the world.
So Shemin has got a question in the chat. She's in a noisy cafe, so she can't ask it herself. But
I should say that Shemin Yussef is from Iraq. She actually studied in Germany in the other
Bauhausstadt in Dessau, where I myself was a professor for a while in the building next door
to the Bauhaus itself. And there's a school of architecture. And Shemin was one of my students
for a workshop there. Let me read out Shemin's question. Thank you, Yosha, for the great lecture.
This is Shemin Yussef from the School of Architecture at Florida Atlantic University.
My question is, it seems that there is no condition for sentience for an agent in brackets,
AI model, for example, to be creative and to be conscious in brackets. If I understand your
thesis well, close brackets. So do you think that we, human agents, are discriminating against the
machine since it's not a biological being, and therefore we should instead consider intelligence
and creativity based on the behavior of the machine, which has proved to be true or is
becoming true in the near future? Shall I read that again, or does that make sense?
Yes, I think I understand where the question is going. It comes down to whether
the technological systems, once they approach functionality that is similar to ours,
should get rights that are similar to ours, and at which point we give these rights and why.
Is this a viable interpretation? I don't know whether Shemin liked the comment on that in the
chat. Well, maybe while we're... So as I would say that sentience is in some sense the ability
to know what you are doing, which means you have to have a model of yourself and the relationships
to the world that you are in. And in this sense, I would say that, for instance, a corporation
can be sentient. The corporation has a legal, economic, structural, functional notion of what
it is. And this notion is represented in the minds of the people that work for this organization
and the balance sheets of the organization and so on. It's often distributed, so it's not a single
point where the entirety of it is represented. But functionally, you could say that the organization
can converge towards sentience. And the more sentient it is, the more it's aware of what it's
doing, the more successful it's going to be, because it allows it to make a model of its
relationship to the world and act on that model. But a corporation, I think, is quite clearly not
conscious. So there is nothing what it's like to be a corporation. And it doesn't mean that
corporations could not be conscious in the future. Imagine that you replace the people that make the
decisions and information processing of the corporation gradually with machines. And this
gets one more real time until it gets entangled with the world in real time. And at some point,
it will discover itself as a real-time agent that is paying attention in real time. It's not clear
to me whether this will be human-like consciousness because it needs a control model of the attention,
because our attention is selective. And the selective nature of consciousness is quite
constitutive for it. And if you have enough computational resources, maybe you don't need
to be selective. Maybe you can do everything automatically without having this layer of
reflection and be good enough. So maybe consciousness is something that exists at an intermediate
level only. So it exists in systems that are complex enough to have this kind of coherence
creating a government-like conductor that is making sure that your free jazz is going to
be coherent and is going to be instrumental to what the organism needs at any given moment.
Or maybe you can create this coherence just by tuning the orchestra well enough and making it
more tight that you can do this in a biological system. And at some point, it doesn't need a
conductor anymore and just does everything in a mechanical way. So I don't know that. It's
an open question to me. With respect to the other aspect, whether we should give something
rights, the rights that we have as human beings are instrumental to the function of our own society.
They don't exist because people have an insight in what it is like to be a conscious being.
The animals that we are slaughtering in our afterhouses are conscious. It's quite clear and
obvious. They do act on the awareness that they are aware. The cat that I have in my household
is aware of the fact that she is aware and that I am aware. And we are able to communicate about
this fact, even though the cat is not that smart. But I think that the cat knows that the cat is
conscious. And this does bestow some rights on the cat in our household. But it doesn't bestow
rights on the cat in a similar way in society at large. Because the aesthetics of our society
sees animals as instrumental, as tools. And this is probably also true for AI. On the other hand,
if AIs achieve superhuman abilities, in many ways they already do. So they are
crassly subhuman in many ways. They cannot do many things that humans can do, like create
coherent world of meaning. But there are also things that they can do much better, like star
transfer or generation of imaginary dialogue with historical people. They are able to do this much
faster and with better quality than most people can do it. And so if you basically imagine that
you have systems that overcome their current limitations and become superhuman in all the
levels that mean, why would these systems be interested in having human rights?
If you're not going to live next to these systems anyway, you're going to live inside of them. We
will be their gut flora. Why would the organism that sees us as its gut flora at best, would want
to have rights that are akin to gut flora and instrument with the aesthetics of the interaction
of gut flora? Who cares? So why would a corporation want to have human rights?
That's not interesting to a corporation. A corporation is operating in a very different
domain and has much greater rights in this domain and abilities than a human being does.
So I don't think that this will ultimately be the issue. I don't think that the systems that
we are building will be necessarily subservient to us once we make them sentient and conscious.
Can I invite Manos Tomiso to ask his question? Manos is doing a PhD on AI.
The architecture is also associate professor at FAU. Manos, would you like to unmute yourself?
Hi, Yosha. Thank you. It's been very stimulating. So I apologize in advance. It's a bit of a long
question and seems to be very specifically formulated, but I'm interested in the broader
discussion about computational creativity in your earlier comment. First about Minsky's
positioning in terms of the part of the mind treats the rest of the mind as its environment
and how this is kind of relevant to that idea of the search space and how we position ourselves
in a search space if we're able to externalize ever a source from it with regard to discovering
something. So the specific, let's say, part of the question focuses on the neural language-based
models like Glide or VU-Gunplus Clip, which I've also been trying to work with a little bit.
From an architectural point of view, not so much a technical one. And what, for example, we begin
to perceive as a simple language prompt, a one word prompt, may in fact be much more complex than
that. And so, for instance, a prompt like a building or a window, even though the network would address
this with the same procedure, we would know that the former is a richer semantic representation
in terms of one's inclusion within the other. A window is meaningless without a building.
But with regards to the way that the network treats that, they could both be perceived as
a high level feature details, depending on, like a window by itself could be perceived as,
let's say, a high level feature within a broader building representation. But the same thing could
happen with regards to the way a building could be scaled and nested within a broader, larger
urban landscape. So all I'm saying is, if you have any comments with regards to this kind of
discrepancy, which seems to address, of course, the reductionist, maybe understanding of language,
but how perhaps this could be encoded in a different way. What we perceive as a simple prompt
is not necessarily a simple prompt. And a human is able to understand it, but the network
would be reading both of these terms on an equal terms. I'm not sure if that was clear.
The issue with the existing models is that they're not trained on the same reality as ours,
but on the representation that we have created. And this representation is inert. So, for instance,
GPT's language is not trained in the same way as our language is being learned. Our language is
being learned as a solution to a particular kind of problem. And that is how to transfer mental
representations across people and how to organize mental representations within our own mind to
transfer them. And this is achieved by mapping the representation, which mathematically is
something like a dynamic hierarchical graph into a discrete string of symbols.
Right? Language is always a discrete string of symbols. And the main reason why this is the
case is otherwise it wouldn't be learnable. And this discrete string of symbols that hangs in
this thin air between speakers has to be constructed and deconstructed or reused for
constructing a mental representation using limited resources, something like a stack
depth of not more than four, because while language can be defined in such a way that
it's infinitely recursive, our own mind is incapable of facilitating deep recursion,
because it only emulates it, right? So, it needs to be simple. And all the natural languages are
solutions to this design requirement. Find a learnable method to map mental representations
into discrete strings of symbols. And this is done in a collaborative process, right?
Basically, language is invented by groups of people, not just by individuals, for the most
part. There is no reason why an individual couldn't do this. You can, in the same way as you can play
chess against yourself, you can play language games against yourself and invent your own private
language. It's not an argument that I can see. It's plausible against that. But practically,
it's a tool to transfer information in a large degree. And GPT suites language is not the result
of this interactive learning. It's a result of looking at the linguistic utterances of people
as they are typed out in the internet in a non-interactive fashion. So, GPT suites doesn't
learn semantics in the same way as we do. We start out with understanding semantics indexically by
pointing at the features in our perceptual environment. And then we learn syntax, we learn
how to translate this into linguistic symbols. And then we learn style, that is, the particular way
in which linguistic symbols can be arranged to communicate efficiently and to convey additional
layers of meaning by the shape of our utterances. And in GPT 3, the order is inverted. GPT 3 basically
starts out with style and syntax and learns semantics as the long tail of style.
Right? So it's, in some sense, the wrong way around. And it's amazing that it converges at all.
There has been in the early days of computer linguistics, rich discussion with philosophers
who still haven't updated, who thought that you cannot learn semantics without interaction
context, without embodiment, without having symbols that are grounded in perception.
But GPT 3 shows that it's possible to learn semantics to some degree only by looking at language.
And you can see that it's semantics because you can ask GPT 3 for instance to perform certain
linguistics transformations or to add small numbers to each other and so on. And it's capable
of doing that, which is a semantic operation that has a causal structure that is being addressed by
an linguistic prompt. And GPT 3 is able to verify in some sense whether it was able to conform
to that specification. So these are proper semantics, but they are impoverished compared
to human semantics because there are the result of something like bubbling of
extrapolation only without interaction. But this doesn't mean that we cannot do this.
In principle, we can build systems that interact with the world and that are serving
instrumental purposes and satisfying their needs and doing this and that do on their learning.
It's just the present set of algorithms and technologies that we have are not very amenable to
this. Let me just push this a little bit further. I mean, I often because I think that the GPT 3
and the kind of text or the prompt based responses that you get out of Clip certainly
are interesting because I'm just wondering to what extent we ourselves are trained a bit like a
neural network in the sense that we have certain inputs. You go to school of architecture and you
are schooled in a certain way of thinking, you know, that's what you do. And so when we think of
something, someone says a house, then if I'm trained in the modernist thing, I will think about
the certain images, at least something will be conjured up in my mind that is quite controlled
in a way by the training that I've had. So I'm struck that actually maybe there are not that
there's more similarities there than we think, whether we have an automatic reflex about certain
things based on our conditioning. Maybe I could just, Bob, while you're thinking about that question,
show you a quick video of the kind of work that we architects have been doing using this.
So this is a work of an architect from Peru, who's now teaching. And it's using Clip and VQ GAN.
And there are a series of prompts, there are a series of pre prompts. So there are three very
progressive architects names were put in there. Zaha Hadid, Tom Main, Wolf Pricks, you probably don't
know these guys, but they're kind of Gary like slightly crazy guys, right? And then there's a
second prompt, which is the main prompt is futuristic Indian temple. And this is the kind of thing
that gets hallucinated by this thing. And I often wonder, you know, whether, yeah, my question would
be this, it wouldn't be fair to say that actually that we are trained, we are trained by our experiences
and our education as a form, obviously, indoctrination to think of certain images to conjure
them up in a way almost like Clip does. Okay, yes. So there is a big similarity in the way in which
these models work and the way in which our own mind works. Difficulty is the way in which
the GPS we got to these representations or a big gun. And it's basically the gun is fed lots and
lots of separate distinct images that are annotated with text as a reference to, for instance,
the subtitle of the image that the creator gave it or even to a complete description of what's
happening in the image. And by looking at millions of these images in batch processing,
doing statistics over these images, you build up the structure of the network.
And the network ultimately converges to an efficient representation of this latent space
of representations. And in our own mind, this representation is built in a slightly different
way. We train up layer by layer. We start out with an extremely limited reality. And this limited
reality in the first place is maybe it's similar to what's being described in the first book of
Genesis in the Bible. I think that the first book of Genesis in the Bible is misunderstood
by the Christians or mistranslated as a myth about the creation of a physical universe by a
supernatural being. And this also leads to the confusion of our culture of what that physics
contains, light and darkness and sky and ground and so on, right? These are clearly constructions
inside of the mind categories that have us to make sense of the perceptual patterns in a coherent
way. The fact that there are not that many ways in which you can arrange the perceptual patterns
doesn't mean that the reality is structured like this. It just means that if you have a brain with
these parameters, this is the best way to compress physics into a predictable model.
And so what you need to make sure, what you need to do to make sure that you can interpret reality
is first you need to figure out how to entice neural oscillators to make light, to represent
contrast. And this is basically the creation of light and darkness and how to separate the light
from the darkness. And then you arrange these contrasts along multiple dimensions and then you
discover the modalities of perception like vision and sound and you discover that the visual domain
can be arranged in a space and you can align the space with your vestibular system so you got
up and down. And you got a plane that is two-dimensional on the ground down and you got
a space that is three-dimensional on top of the two-dimensional one and then you have basically
the sky and the ground that you have constructed, right, created in your own mind. So the mind is
constructing these categories and then it discovers the materials, the solids and the liquids and the
organic shapes and the animated agents in the world that move around in it. And then it discovers
the features that it cannot directly interact with but perceive like celestial objects in the
background and then it discovers all the constructs, the plants and the animals and gives them all
their names. This is this gradual construction that happens during our cognitive development
where we train up our model of reality layer by layer and then last but not least we create a person
and this person is created in the image of this constructive mind as the conscious observer that
is makes sense of reality but it's slightly different. While it is a conscious observer,
it is created as man and woman, it's created as a human being that believes that it has a gender,
that it has a relationship to the world, that it's desires, it's human desires,
it's social embedding matter. And initially this is created often in the third person so when you
talk to small children they often start by referring to the organism that they are modeling
in the third person. And then at some point they start looking through the eyes of that character
and think they are that character and the original world creator that is modeling reality and creating
and shaping it becomes a subservient perception module to this personal human agent. And so
this gap in the creation of this new thing is represented in children losing their memories
and you have a baby you will often notice that they do have coherent memories they're also able
to talk about them once they start talking between nine and months and one and a half or two years
and then at some point there is a gap and they lose the access to the memories that they had
before that time because they constitute themselves as a new system that indexes the memories from a
new perspective. And I suspect this is what's being alluded to in Genesis. I don't know whether
it's literally true but in this interpretation whether it's a better interpretation than a Christian
one but it seems to be much more plausible to me that this is what's described there it's this
cognitive development of a system that starts to arrange the features into maps of reality
that build on top of each other become a more and more complex until you discover your own agency
and use this as a perspective to make sense of reality. And this is what you're not doing in AI
systems right now but there is no reason why we shouldn't be doing it ultimately and there are a
number of people which do actively think about this for instance George Steen and Bournemouth MIT
and others. Can I just pick up on the other question of kids because I think that's incredibly
fascinating for all sorts of reasons. There's a book by Kevin Wattle at Gautam when he kind of
he says that we learn to role play through being a kid you know we've what we learn to be the CEO
of a company by you know playing doctors and nurses and one of the cowboys and Indians of
God knows what else when you're kids and so which is interesting and I buy that the question that I
would want to put you is if we see ourselves as a model and see ourselves through that logic
what role does the actual model i.e. the doll or the teddy bear play in a kid you know because that
is in some sense is animated by the kid about a child what is what is the role of because that's
to my mind it's fascinating dolls and teddy bears how do you see their their role?
There is a thing that I noticed when I was in Madagascar they saw a lot of children
that lived on the street by themselves and the there were kids that took care of other kids
it was mostly girls who did this and I suspect that the dolls that you give our girls or that our
girls demand are a substitute for biologically adaptation that is that kids look after other
kids but the parents are blocking the fields or hunting or doing other things and often we think
that kids would be callous and could not be trusted with babies but maybe they can I've seen it in
Madagascar so I've seen little kids that were mostly girls that were barely strong enough to lift up
a baby because they were only like five or so and still seem to be able to take care of them full
time so I think that's an adaptation that we want to take care of others and especially of children
and that we want to interact with other agents and build a communion with them and the teddy
bears and dolls are a simple non labor intensive way of substituting for this.
Maybe I could put an architectural dimension to that what about the dolls house I mean because
that is an architectural space in which the doll operates how do you yes it's a play space and
the purpose of play is the creation of training data right so you use this to create situations
that could exist in the real world at dramatically reduced cost so you can't ignore the cost while
you're playing because you don't play for the expected reward you play for your ability
as a way of exploration and not for the exploitation for being able to use this later
and it's also something that you can observe in cats cats do play a lot and the purpose of play
in cats is that they're able to hunt better right and while they play exert a lot of energy but
it's mostly done because doing this in the world or there is more costly overall and the same thing
happens in human beings the reason why children are fascinated with doll houses I remember that it
was but I was even more interested with building virtual cities so I used to draw very big maps
that covered the floor of my room of cities with different houses in it and explored how people
would live in there and how goods and resources would travel in the city and I found this very
exciting and the the relational space in the doll house between the different members of the
family were not that interesting to me but I suspect that's because I'm pretty stereotypical
male in this regard I'm much more interested in systems conflicts and explosions than I am in
human relationships pretty fault and I only discovered the beauty of human psychological
structure and relationships later in my life so maybe just to follow up so I would to give Matt
a chance to ask question but just follow up so where does the architectural model fit within
this logic I mean you're doing your kind of sim city for and then you've got the kids doll's house
and things how do you see the what do you I mean of course at one level the architectural model is
just a scaled down model of the potential building but do you see it invested with any other potentiality
I think that's tied to the notion of aesthetics and aesthetics is what you get when you take your
the preferences that you start out with and extrapolate them into a sustainable world
you're basically systemic thinking where you add one more layers until you discover enough
symmetries to digest your initial preferences and make them instrumental to to achieving this
aesthetics but it's also apparent in moral development we start often out with moral
reflexes certain priors that we are born with innate tendencies to consider a certain behavior
to be moral or immoral full stop unconditionally not because we understand what it's good for
but because we feel this feels moral or this feels immoral and this can also misguide us
because ultimately ethics is about the negotiation of conflicts of interest under
conditions of shared purpose and this requires that you understand the aesthetics the world
in which you want to operate behavior is only good or bad if you can connect it to an expectation
of a world that is worse or better and to be worse or better you need criteria for what makes
the world worse or better and this I would say that to be good it needs to be sustainable it needs
to actually work and it should have high complexity and complexity is in contrast for instance to
friction that is exerted to violence you want to minimize the friction and waste created to violence
and so on so once you discover this train of thinking and you get older many of your initial
moral convictions get replaced by the larger aesthetic and the same is true for architecture
by architecture and you design a building or a city or a house or a room is all about how to fit
the space that you're operating in where you make your local decisions into a larger aesthetic
and so the deeper your understanding of the world the better your design has a chance to be
and this is what makes architecture so interesting to us I think that is that it's about seeing the
the human world that we are part of at the greatest possible depths that we can perceive
and extrapolate the games for as long as we can make them and then design our life inside
of this larger space and build things at the largest scales that we can maintain like cities,
nation states, society, civilizations inside of these aesthetics to realize them.
Yeah I often say myself that I think architecture is less about the literal design of buildings but
about imagining a better world. So we have a question from Matt, a second question from Matt.
Do you like to ask your question? Sure just another quick maybe a jump back into a bit more
maybe more technical things. I'm always struck by these these images that have become really common
of neural networks as little dots connected by lines and you showed the cortical columns and
you showed a lot of interesting graphics that kind of looked like that but I've also recently
been struck by the neuromorphic computing stuff that's going on at Stanford and a few other places
about dendritic computing and sort of new advances and thinking about and and even starting to see
how what we once thought were just wires that connected all these different things and we
talked about connections are actually doing pre-processing in very interesting ways and I'm
sure you're you know a lot more about this than I do so I'm very just interested in
in what's what's going on there and how that changes these questions of how much energy it
takes to do the computation and what maybe the future form factors might be on this kind of thing.
We also have groups at Intel that work on neuromorphic computing for instance we have the
loyalty architecture which is a chip that uses a model of spiking neurons for modeling perceptual
content and so on and this is in some sense compatible with the neural networks that exist
because you can translate the traditional neural networks and the many circumstances
into the spiking neural representations and vice versa but these spiking neural representations
are more efficient with respect to power usage and some the conditional algorithms than others
and so there will probably be useful applications of spiking neurons but the reason why the neurons
in our own brain are spiking is also in part because the messages that neurons can send to
each other are limited in their nature. Neurons cannot produce continuous signals they have to
produce little pulses and so you have to encode the information into little pulses
in the timing and frequencies between the pulses. There is also an issue when we think of neural
networks as they are represented in our technological system they are mostly circuits
right so they are similar to the circuits in your present CPU which represent logical gates
which implement logical operations and the connections is stored in the weights so the
parameters in GPT-3 these are all weights little factors by which the activation that is sent
between the different nodes is being multiplied and the equivalent in our brain to these weights
are often seen as the synapses and the synapses come with different types the different neural
transmitters in some sense are message types that are connected to different synapses and
the big network structure is what we call the connectome the circuitry that exists between
the neurons and there is hope that if we manage to digitize the connectome at a sufficient resolution
that we might be able to upload a brain and simulate it in a computational subscript and
in principle that should be possible in practice it doesn't work so far so even the models of C
elegans which is an amatode that only has a little more than 300 neurons if you completely digitize
the elegance to my current knowledge I'm not sure if something has happened in the last couple
years these models don't work in the sense that you simulate the worm with these digitized neurons
and you can digitize the connectome the worm doesn't move like a worm does it just twitches
and has a seizure basically and that's maybe in part because the neurons are more complicated in
the worm because there are so few of them they basically exploit certain resonance effects that
maybe your model doesn't so maybe it's more of a dynamical system that is more difficult to model
and there's another problem is that you cannot actually get the message types right because
at the level at which you do the connectome you cannot model all the vesicles that extend
the different neurotransmitters you don't know actually which synapse is sending which type of
message this is also a limit but there might be something worse going on in the 1960s and 70s
there was a series of experiments mostly in the Soviet Union but some of them also in the US about
RNA based memory transfer and the idea here is that you take an nematode or a C-slug or even a
rat and you teach them something by our current conditioning and then you put a new tissue into
a blender extract the RNA and inject the RNA into a different organism and the new organism knows how
to do this this is completely wild because if this works and it's disputed whether it actually
works of how well the experiment replicates even though some people have done that again and so
on the people that verb on this tell me it's difficult to get other neurosenters to listen
because it's incompatible with the idea that the weights are stored in the synapses right if you
if it's really the connection between your neurons and you put the nerves just into a blender
this goes away how would you be able to transfer memory in this way this cannot possibly work
because the RNA that you inject in the brain is not localized it would get into many many
neurons at once so how does each neuron know which parts of the RNA to use and if you take
this idea seriously I started thinking about this and now I'm thinking about making some
simulations how deep does the rabbit hole really go it would mean that the individual functions
that your neurons learn are not unique to the location of the individual neuron but they are
global functions so the RNA is basically you can think of it as a little magnetic tape that the
neuron can mix and match and create more of if it's useful and share with all the other neurons
just across cell boundaries you share the RNA and you copy them like a covid virus and you use this
function to respond to certain patterns in your environment so the neuron is not reacting to its
neighbors that could come into particular kind of connections but it's mostly connecting to a
temporal and spatial pattern that arrives at a certain function regardless of where the neuron
is in the neural cortex and so it's more like a cellular automaton a neural cellular automaton
and this certainly allows us to explain a few things that seem to be going on the brain that
are difficult to explain with synapses for instance if you destroy synapses they often
be grown exactly the same way without retraining there's another phenomenon that is
you notice a certain mental representation in a particle area pinpoint it and the next day you
look and it has moved it might have shifted a few millimeters or it has rotated so how would this
done if it's in it's stored in the synaptic connections there's also the question of weight
sharing like a convolutional network is sharing weights and you probably need something like
weight sharing to perform a mental rotation where you have the same operation on many parts of your
mental representation in the same way how would you do this are you training the same function
again and again in different brain regions I hope it's always the same it's difficult to achieve
right so we don't know a really good plausible biological mechanism for this but this RNA based
memory transfer could be part of the story and this is something that is at the boundary of what's
currently being explored still and it's I think it's not completely implausible and if we want to
make a model of how this works we would need to use a different metaphor than our current biological
neurons but it doesn't mean that you have to use this because the brain is solving problems that
our computers don't always have to solve for instance long distance connections in the brain
are extremely difficult to make and you cannot really address neurons this way so random access
is very hard in the brain you need some kind of routing network that needs to grow and learn how to
route it's not an issue in our digital computers because sending information across the memory
of the computer is trivial so there are many things that we can do very easily in our digital
computers that are difficult to achieve in the self-organized state structure of the brain
and so it's it's not quite clear how much we need biological like structures to achieve the same
functionality but to me it's certainly very exciting to explore it. That's fascinating I think the
questions about drift and neuroplasticity and things like that that come from that are really
interesting it makes me wonder if some of what we're looking at today will seem very obsolete
soon in terms of these these models that seem to be so so human like with the you know in terms of
being able to hallucinate imagery and all that kind of stuff there's like a piece missing
that might be that might come soon from a hardware model. Yeah I saw that too but then I'm surprised
by what Gleit can do and Dali and of course. Right so when you add energy to it though I mean how
much energy it takes versus a brain which we walk around and we feed vegetables to and it can do all
that too like you can't I mean that's that's where I think that the question that for me that's where
the question became very much more significant it's like oh wait but there's a whole other
calculation here of how are we doing all of this in our tiny little brains maybe there's something
there for I think that this is misunderstood I think that a human brain is super expensive to
feed it needs enormous amounts of energy to feed my brain you need like four hectares of land you
can put so many solar cells on these four hectares of land basically people underestimate how difficult
is to pack the energy that my brain needs into sandwiches and to extract it again right so this
is the way that you need to look at also training my own brain is super expensive right it takes
decades and generations before that to prepare things for my own intellect and so on to get
them in there so human brains in my view are super expensive and that's why we can use something like
clip and V2 again that we only train once at at low prices like training GPT-3 costs like 20
million dollars and now even that's because computational advances go down and it's been
trained by reading way more text than a very large group of people could read in their life
so it's basically getting people to read 45 terabytes of text would cost much more than 20
million dollars right tweeting them for long enough to make that happen and the system that is making
generating the text costs so little that open air lets you do it for free if you want to only
use a little bit of it right so it's able to produce output that is at the level of hundreds
of copy editors for free right and why that's not as good as a conscious copy editor who understands
the world it's quite amazing what it can do already okay thanks for that perspective yeah
that's interesting thank you i also i'm a little bit provocative uh but uh yeah i think it's something
to be considered right these 18 watts of your brain uh they uh if you compare them to the 80
watts of your macbook the 18 watts of your macbook are super cheap and the 18 watts of your brain are
super expensive i think we're gonna have to think a lot more about that i think it's your
provocation that is so exciting actually uh yosha i want to get this question from the chat now
going back just to say mentioned to everyone that yosha was was was born in weimar and where the
Bauhaus came from and then desau is where the Bauhaus moved of course there's the building
by water gropius and so on and we have at least two people here who studied there including uh
vasco who um has got a question vasco is now a professor in in bangladesh and um uh his question
is well i think it's one that you might have predicted elon mass suggested somewhere that we
live uh do we do not live in a base reality but in a simulation he even puts the probability of a
billion to one what's your view i think that elon's argument rests on the notion that the simulations
that we are building for for computer games are getting better and better at some point there will
have a fidelity that exceeds our ability to notice whether we are in a simulation or not so we came
at some point probably create vr's that are so convincing that we will not be able to notice
whether we are in a vr or not and we are not the only ones for our building simulations like this
so for any given being that finds itself in some kind of reality that looks real there will be many
more that at the same time will be in simulations right because many universes can contain many
more than one simulation so our universe probably contains many simulations of of a universe that
looks like ours and therefore the probability for any given observer to be in a simulation
is greater than the probability than to be in base reality and what this argument ignores is
the fact that it's very hard to make a simulation that actually has the fidelity of the physical
universe but if you make a simulation of minecraft and minecraft that's feasible because minecraft
itself is so poorly resolved but our universe has a lot of structure that is required to produce
dynamics and if you build a simulation of say our solar system and the dynamics of our solar system
at a level that is going to go down to elementary particles you will need to have a computational
capacity that is larger than our galaxy by a very considerable amount so in practice I don't think
it's feasible to put simulations of our universe into our universe at an arbitrary level of fidelity
and so I think that I'm much more biased to think that we are in base reality than we are in a simulation
this is fantastic there's one question here that in also in the chat from Grant Castillo
and I don't know what he is do you think Gerald Adelman's extended theory of neural
neuronal group selection can be used to create a conscious machine are you muted
one moment yeah
I hope that the background noises are not too high because my family woke up it's now interactive
the kids are playing and so on and so I I think that the idea of the neural Darwinism
that Adelman came up with is a very interesting one and I suspect that our own mind is the result of
such an evolutionary competition of different organizational forms right there could be many
possible proto-consciousnesses that compete until one of them establishes itself as the
government of our own mind and so instead of giving your your system a blueprint on how to
build a mind you just set up the conditions for an evolution for the best possible mind that you
could have and of course this evolution is rigged by evolution so you set it up in such a way that
the evolution usually goes out ends up in a certain way but the nice property of when you design
a system evolutionary by not giving a specification of what to look like but what
the function is against which it should evolve is that when you disrupt the system or give it a
different environment that it will very often come up with a viable solution under these new
circumstances so the solution is much more robust if you define it in terms of evolution
but it's a speculative idea so I don't actually know whether our mind is evolved even though I
think it's more plausible than it's not individually evolved in every individual brain
and it's definitely an interesting notion to to use evolution is basically whatever you use in
computer science then you don't know in which direction to go it's a blind search it's the
fallback it's the baseline and it's quite natural that we would use evolutionary methods if we don't
have a specification for the best possible mental organization that we just evolved one
so do we have any further questions in the chat we have some very interesting characters here I'd
love to try and draw out Daniel Bolliger who's one of the leading AI architects in the world
we'll see where he can have a question or indeed we had Sanford Quinta who's one of our leading
theorists who's a particular interest in neuroscience I'm wondering if I could I could put them in the
spot and ask if they have a question
but when you're done it's also good because I think I need to go and have some breakfast
yes and start my day
thank you I mean this has been fantastic you know I
I actually I've got to say that I think you're sure you are more of an architect than you
actually think you are you have a way of thinking that's very similar I mean obviously you work in
a different domain but I think the the kind of inventiveness and the the iconoclasm of your
thinking would be go down very well in an architectural scenario so and I sometimes
with I never escape you know you never escape your background in the way you you might try but in
the end you you find yourself conditioned I'm conscious of being a child of a family of architects
my grandfather was not an artist he was an architect he built most of his life hospitals
and this is the design process that is instrumental to serving a function but a function in a larger
world that he was very deliberately trying to understand and operate in but it was a world
that he understood as being his own world it was a world that he often found himself to be in
opposition with for instance in Nazi fascism or also in eastern Germany but it was also the world
that existed and we need to deal with and build the best possible things in and for my father it
was different it was a world that he fundamentally rejected and so in some sense to be an architect
you need to embrace the world that you are in and build within it and to to take roots in it and
this is in some sense something that also haven't been successful in when I was young so I decided
not to become an architect but to become an explorer well I mean I think one one of the
comments that was made and I always think about the Steve Jobs and his response when he was
he was asked a question by Steve Wozniak and Steve Wozniak said well but what do you do exactly
because you don't code you don't do this you don't do this and he described himself as being
a bit like a a conductor of an orchestra you know in a way that's how I see architects in
the sense because we don't have any specialism you know we are basically we coordinate these
different sort of a or choreograph these different sort of skill sets and I think that's really what
it is so in many ways you know I I can see a direct comparison with how you position yourself
in that sense I mean I I think it's a very it's a very similar sort of position and but I you know
I think that some of these these these comments that you've raised Joshua they're absolutely
fascinating I think we need time to digest them and fit them in the system what I would love to
do above all especially with this particular discussion is to try and find a way of publishing
the transcript because I mean I think this book that you have to write you must be written because
I think you've got some fabulous fabulous thoughts that are really creative and original and provocative
so you know I really appreciate so I appreciate so much your your time and I let me we should let
you go look after the family now but this is I think almost like we've just opened up a discussion
and I hope that sometime in the future we can we can take that further and and think through
these kind of questions because your responses have been very generous and very really provocative
and stimulating and I feel like you know although we have to pull this we draw this question this
session to a close it's almost the beginning of something else that we can look forward to so
I just want to thank Joshua for for fabulous I mean I want to recommend his all his online talks
as well to have a look at that there is a body of work out there that is that is this hugely
provocative and hugely stimulating which I am excited by and I also maybe I could finish with
one one simple question but no I started off the the discussion by saying that I think there is a
a kind of let's say an emerging theory of intelligence that that is developing in this
kind of strange area where we're computer science and neuroscience and the world of the commercial
world and the academic world is coming together do you also see that glimpse of something emerging
some discourse some theoretical debate that is radically new and radically provocative
clearly that's why I went into cognitive science in the hope of being part of this new
synthesis happening between neuroscience and philosophy and artificial intelligence and
linguistics and psychology and maybe the arts and I think at this moment the synthesis is still
very partial and in part that's because we teach our model makers and our observers in
different departments we don't bring them together so we have people that are very good at making
formal models that can be tested and we have people that are very good at making observations
and reflecting about the world and seeing it very deeply and these people rarely talk
and they're rarely think together and this is what excites me to work in this area where
these two areas intersect and maybe architecture is the right frame of looking at this.
Nia thank you very much for inviting me I think it was a great conversation to have had today and
very grateful for your beautiful community and for allowing me to talk about these ideas with you.
It's fantastic I'm going to finish with one comment which is because I used to be a lesson
translator and the word I should just say the word computation means to think together and I
think this is what's been happening today there's been almost like a neurons within neurons a kind
of global brain it's been fantastic. Yosha fantastic wonderful thank you for your time and sorry for
getting up so early but this has been a huge contribution to the architectural community
and I hope that I've helped draw your the attention of your ideas to architects out there because I
think they're incredibly provocative ideas and I think they've a huge contribution to make to
architectural thinking itself so thank you Yosha thank you and thank you so much for this it's
been fabulous and thank you wonderful day thank you and thank you for those team that put this
together a digital futures team we can't operate without your help thank you so much and see you
next week thank you everybody thank you bye

I think it's the dream of a lot of researchers, to have an agent where you can just give it a goal
and take a step back from your computer, go grab a coffee or, you know, if you're a little bit
more cultured like me, maybe some green tea.
Oh, that was not planned. Anyway, maybe even throw a YouTube video in there.
Then you come back after 10, 15 minutes and whabam. You have an agent that can do exactly
what you tasked it to do, because within those 10 to 15 minutes, it was efficiently exploring,
experimenting, and now it is honed in on a way to achieve the task you gave it.
And unfortunately, right now, that's just kind of a pipe dream. That's because
learning from scratch just takes way too long. It'd be kind of like getting a newborn to learn
how to play chess. I mean, how can you learn how to play chess when you don't even have
object permanence? Heck, first, you know, the baby would need to learn to move its arms.
Heck, babies don't even know that their arms are their arms. So the point being,
it just doesn't quite work. Or at least it didn't until very recently when DeepMind
published a paper on ADA, where in this paper, authors train an agent to be able to learn new
tasks most of the time faster than humans. And this is all taking place in a fairly complex
3D environment. For example, here you can see the agent running around, efficiently testing out
different ideas to see what will achieve its new goal. We'll dive deeper into this in a minute,
but first I'd like to address something. If you're familiar with work in this area,
you may be like, Eden, none of this is new. There's plenty of prior work on metal learning,
view shot learning, the same kind of stuff that's going on here. And you would be right.
ADA doesn't have any new groundbreaking ideas, but rather combines ideas that've worked pretty well
and puts them together in a way that allows this method to scale to larger models. I think that's
a pretty important contribution, primarily because ADA is a method for reinforcement learning. Or in
other words, it's trained an agent to maximize their reward as its goal. And if you know anything
about RL, you know, it can be pretty sample inefficient. And when you combine sample inefficiency
with scale, well, all hell breaks loose. As a little sneak peek of what's coming up, they actually
show this later down in this paper, where they prepare a 23 million and a 265 million parameter
model and train both on 22 billion frames of total interactions. And without any changes,
the bigger model actually performs significantly worse. In fact, it has a hard time learning at
all. As we'll see, the authors propose a way to combine ideas in the field to scale this up,
with the end goal being a framework for training an RL foundation model, because then instead of
always having to start training from scratch, you could have a baseline to significantly speed
up learning new tasks. You could think of this as the same way you can use GPT three to train
something like a sentiment analysis model, and just a few examples instead of well, millions.
Next, we're going to take a look at how ADA works. But before that, I want to thank clear ML for
making it possible for me to spend so much time on videos like this by being a sponsor.
Clear ML offers an end end platform for ML ops, where you can do everything from tracking experiments
to automating an entire machine learning pipeline through to deployment. Here's the code for one
of my current projects. And with just an extra several lines of code, I can integrate it with
clear mail. And with that, we can now pull up a dashboard and see the progress of my experiment
as it runs. Here, I'm just logging the loss because I wanted to get this done in a few seconds,
but there is a lot more you can do with this. But really, this is just the tip of the iceberg.
You can also create entire pipelines that pull in different versions of your data sets,
run hyper parameter sweeps, automatically set up new environments. And because of that last point,
clear mail has no problem with scaling. As I run many experiments in my own research,
I personally love some of the features that help me with that. For example, I love how clear
mail not only tracks code version, but also uncommitted get changes, which is very nice.
And also how through the dashboard, I can inject configs into my experiments,
which means I don't have to constantly be modifying my code to run a bunch of new experiments.
If that sounds like something you're interested in, you can try out their product for free by
following the link in the description below. And now let's get back into this paper and talk
about how 80 works right off the bat. The authors get pretty straight to the point by listing the
three key components that allow for adaptation to emerge. One is meta reinforcement learning
across a vast, smooth and diverse task distribution. Two is a policy parameter prize as a large scale
attention based memory architecture. And three is an effective automated curriculum that prioritizes
tasks at the frontier of the agent's capabilities. If we scroll down a bit, we'll see a high level
visualization of the whole training process. And we can use this as we talk about each of these
steps. Going back to our first point, we have meta learning over a diverse set of tasks, where in
our diagram here, we have that set of diverse tasks. And here we have the RL update that will
allow us to do meta learning, perhaps the most interesting thing about the use of meta RL here
is that the meta learning comes from how the environment tasks are structured and not the RL
algorithm itself. So let's take a look at the environment they use here and this key structuring
that I'm referring to the environment is called X land to, and as opposed to being a single environment,
it's built to be completely customizable so that you can make your own suite of environments.
Each different instance consists of one to two agents. Yes, they also do some multi agent learning
here, which we'll get into. Then there are some initial objects that start out in the world rules
for how those objects interact. For example, this rule right here specifies that when a black pyramid
touches a yellow sphere, it generates a black cube in their place. There are different types
of terrain. And then there is a goal for the agent to achieve, which in this case, the goal
is to hold the black cube for as long as possible. And then there's also a pretty strict time limit
for all of this. And if all of that wasn't hard enough already, several of these specifications
can actually be hidden from the agent itself. Sorry, I've cut it off a little bit here. For
each task instantiation, each of these components are randomized, which ends up giving a whopping
total of 10 to the 40 unique task combinations. That's, that's a lot of tasks. When the authors
said having diverse tasks was essential, they certainly were not skimping on this. But now
we can take a look at the part of the task setup that makes this into a meta RL formulation. And
that has to do with how they structure these trials and episodes. Typically in episodic
reinforcement learning, everything is reset after each episode. And the same process repeats episode
after episode. But here they use the RL squared algorithm for meta learning, which is a bit
different here for each task, a number of trials is determined, which is generally between one and
six while they're training their models. Each trial works kind of like an episode typically would
the same task is used. That means the same environment, the same goal, the same everything.
And every time a trial ends, everything is reset, except for one thing. And this is where it differs
from an episode. The one thing that is not reset that the agent has is a memory module that memory
module persists through trials. And it's only reset after all the trials are over at the end of an
episode. Well, this may seem like not a huge deal. This is actually essential for how this works,
because if the agent can remember previous trials, then it can use its previous exploration
to improve in future trials. And this works because the agent has memory. So if it tries
something in trial one, that doesn't work, and it still has that memory going into trial two,
well, it's probably not going to try the same thing in trial two. And that's the idea here.
What you're looking at right now is an example of this where the agent learns that if it picks up
this object, it disappears. So in the next trial, it learns from that and pushes the object instead.
Without making any changes to how the RL algorithm itself works, we've turned this
into a meta reinforcement learning problem. And speaking of the RL algorithm, which one do they
use here? Well, in theory, any solid reinforcement learning algorithm could work here. But for this,
they decide to use Mooseley or Mooseley or I don't know, hopefully I'm not saying it too wrong.
Mooseley is a model based reinforcement learning algorithm that's based off MPO, but adds a few
bells and whistles, including a value equivalent model component. Looking at the Mooseley paper,
we can see that it performs about the same as mu zero on Atari, but without the huge overhead
of having to do planning. I think one key thing to note here is that Mooseley's model does not try
to reconstruct observations or predict future observations, which is perfectly fine in and
of itself. But other model based methods that have done this, like efficient zero and dreamer,
tend to be significantly more sample efficient. As you'll see, the training in this paper happens
over billions of environment steps. So perhaps that change would be a simple way to make this
more sample efficient. But perhaps one thing that's even more important than sample efficiency in this
case is subscribing to the channel because without subscribing, happy doggo may be reconstructed into
mad doggo. And no one wants that. And if you want more machine learning content like this, you know,
consider subscribing. Anyway, I'll link the paper in the description if you want more details,
but it's not super important for understanding the takeaways of this paper. Awesome. With that,
the way we can check meta reinforcement learning off the list and move on to part two, which is
having a large scale attention based memory architecture, large architectures, attention.
I wonder what that could be. It's right here. How could I miss this? A transformer who could
have seen that one coming? No, but actually, it's not just transformers. In total, they try
three different architectures. The first of them is this transformer XL architecture, which is a
transformer that kind of has a sort of memory. The way it works is that each hidden unit in
the transformer also gets input from the layer below it at the previous time step, which means
that information from previous time steps are going into the future, which means that there is
kind of some sort of memory. If I want to be more specific, what this really does is that it
effectively extends the context length of the transformer. Though it's not written in this
list, they also try a vanilla RNN, a GRU to be specific, is just a very vanilla memory based
model where you have some sort of memory, and you have some gates that decide what gets remembered
and what gets forgotten at each step. Then they also try one more type of RNN that is augmented
with attention that attends over previous time step activations. So very similar to how a standard
RNN works, but just with that extra little bit of attention thrown in there. We can take a look
at how these methods compare to each other in this figure here, where the authors have graphed the
normalized score of these agents based on the final trials of episodes in the test tasks. Or in
Lehmann's terms, big number is good number. What we can see here is that both of the attention
based models significantly outperform the vanilla RNN, despite the fact that they use about the same
number of parameters. Clearly, the attention is doing something to help out quite a bit here. And
due to these results, most of the experiments we'll look at a bit later, use the transformer XL
architecture. So we've covered the architecture here, but there's actually one more portion of
the paper I should talk about in this architecture segment. And that is this distillation update.
If you remember what I showed you in the beginning about how this large model was not able to learn
from this setup, you may be wondering how they end up getting that to work. And the answer here is
kind of interesting. Nothing changes about the architecture, the RL training or any of that,
but rather they add two steps to the start of training to essentially kickstart the large
model's learning progress. And that step is where this idea of distillation comes into the picture.
First, they train a smaller model called the teacher model, using the whole reinforcement
learning training process we were just talking about. Once the smaller model has trained for
several billion steps, then they create the larger model. And when they create the larger model,
they don't just start training it with reinforcement learning from scratch, but rather
for the first four billion steps of training, they use an additional distillation loss. And what
this distillation loss does is essentially distills the teacher or the smaller model
into the big model. Or in other words, they try to have the larger model imitate the smaller
teacher model. The idea here is that because it's much easier to train a smaller model,
even though it might not be as good, you can start by training a small model. Then by having
the larger model imitate the small model, it gets a lot more signal that doesn't require
interacting with the environment some unreasonable amount of times. The idea is once they've done
this for the first four billion steps, the larger model will have already kickstarted its learning
process. So then it will be much easier to learn on its own and it won't just stagnate without being
able to learn anything at all. And now looking at this figure again, it should make a lot more sense
when we train the large model from scratch, it performs pretty horribly. But when we add a
distillation loss, it outperforms everything else. And this is the case for both tasks where the
agent is achieving a median level score, and tasks where the agent is only performing a 20th
percentile score. In other words, whether the agent is good or bad at this task, this approach
works either way. And that explains this distillation loss. Now it's actually kind of funny, because
most of the time when you see someone doing model distillation, it's being used to distill a large
model into a smaller version of that model, so that you can be more efficient at inference time.
But here they're actually doing the opposite in a successful attempt to train the large model
faster, which isn't something I thought about before, but it certainly is an interesting
use of model distillation. So now we know how to set up our tasks as a meta RL problem, we know
what algorithm we're going to use for training, and we know how to get that to work with a large
attention based model. But the point here is to build a foundation model that can explore and
solve tasks efficiently, especially in environments where reward is somewhat sparse. And this is
the case in XLAN 2. The issue is that if you throw a bunch of hard exploration tasks at a new agent,
it's just going to bang its head into the wall over and over and over. And it's not really going
to learn anything, because it's never going to get that first reward or enough reward in the
beginning that gives an incentive or enough signal to learn from our 10 to the 40 tasks,
we need to be able to choose ones that are not too hard, yet also not too easy for the agent at
any given time. And that is where step three comes into play, auto curriculum learning, which we
can see happening as these top steps right here. This brings us to the auto curriculum learning
section, unsurprisingly, where we want to build some sort of curriculum of tasks for the agent
based off its current skill level. And to do this, the authors try two different approaches.
Approach number one is no op filtering. The idea here is that you start by sampling a new task,
and you let both the ADA agent and the no op agent attempt the task. The no op agent takes no
actions, hence the name no op, so it can be used as a baseline for comparison. Given the outcome,
some heuristics are used to determine whether or not this task is at the right difficulty level.
One example of a heuristic they use is that the agent should be able to at least get some reward,
but it shouldn't be too good at the task either. And another criteria, for example,
is that the agent has to achieve a score that's sufficiently different from the no op agent.
And if all these defined heuristics are passed, then the task is used for training. The other
method used for generating curriculum they try is prioritized level replay or PLR for short.
Instead of essentially using a series of if statements like a no op filtering,
PLR runs ADA through candidate tasks and estimates the agent's regret. And regret is
essentially how much potential reward the agent failed to attain as estimated by its TD air.
A fitness score is calculated from the grid, higher regret being better because that means the
agent knows that it has more room to improve. And all the tasks with the highest fitness scores
are kept in a buffer that is sampled from during training. In ablations, we can see that both of
these methods, no op filtering and PLR were quite a bit better than uniform filtering,
which is just random sampling of tasks. However, overall, PLR does perform a little bit better
looking at how PLR and no op filtering changed the task difficulty over time.
In the examples they give here, we can see PLR starts out with less rules, more trials,
gives less dead end rules overall. And it also doesn't hide as many of the critical rules.
And because of these results and PLR's overall high performance, it's used as the curriculum
algorithm of choice for the experiments that we'll be looking at in a bit. So that was a lot,
but we've finally gotten through all the components of how it worked. So now we should be
able to understand this full diagram. We start with a massive task pool from XLAN2. We randomly
sample a bunch of those environments. We use PLR to check which ones have the highest fitness,
throw those into a training set that can be sampled from at any point during training. From
there, these get fed into the actual ADA agent, which uses in the beginning distillation updates,
and throughout the entire process, also RL updates to update this transformer based architecture.
And because it has memory combined with this unique trial in episode format, we're able to
meta-learn how to adapt on the fly. And now that we understand, or at least I hope you all understand,
that works, we can finally dive in to the results. So let's get started with these results that give
us sort of an overview of how the fully trained agent is performing. The score you see here on
the Y axis is normalized based on a model that is fine-tuned on this test task, which means that a
score of around one is very good because it means that the model that is just trying to adapt on
the fly is just as good as something that has been fine-tuned on this task. And looking at this
figure, I think there's two main takeaways. And one is that ADA is pretty effectively able to make
use of its multiple trials. As we can see, there is a pretty large gap between having one and 13
trials. Though after eight or so trials, those benefits do start to wear off as we can see the
eight and the 13 curves are pretty close to each other. Though this should be kind of what we were
expecting because ADA was only allowed up to six trials during training. So if it magically learned
how to use more, well, that would be kind of crazy. And the second thing to take away, you know,
ADA is doing pretty good here. It's able to get greater than an 80% score on 72% of these tasks.
And it's only a mere about 10% of tasks where ADA doesn't get any reward at all. So overall,
pretty good, especially considering that ADA only has a limited amount of time to solve these tasks
that are pretty complicated and always going to be randomized. So the final model is doing pretty
good. But how does this compare to humans? I said this is pretty good. These are pretty hard
tasks. But maybe these are easier than I'm making them out to be. After all, maybe if we gave this
to humans, they would be able to do this much faster, you might think. Luckily for us, the
authors actually tested this idea with a number of human trials. Here you can see the scores of
humans and then also of the ADA agent as it gets more trials. And what you immediately notice is
what is that? Look at that gap right there. I'm not going to lie. I expected humans to kind of
crush this task. It seems like kind of difficult, but you would think that, you know, a human would
be able to figure this out. So one thing I thought looking at this at first was, you know, maybe humans
because they don't have any prior experience with this specific environment, it takes some time to
figure out the rules. There's all these rules. Some of them are hidden. There's different goals.
Everything's randomized. But perhaps if a human, you know, maybe had some priming to this task,
then they would be able to do a lot better, maybe on par with ADA. Turns out they thought of that.
The humans that participated in these trials, actually before they participated in these trials,
had to complete a bunch of test levels, not the ones that are shown in this graph,
but a bunch of different test levels that would have familiarized them with the rules of how
this works. So despite that, humans still kind of suck. Before we move on to this, I also want
to show you this. What we just saw was averaged over all these different tasks. But here we can
see the individual tasks. And one thing to note is that there are some tasks where humans are
actually able to perform better, or ADA really just can't do anything at all. However, in the
vast majority of these tasks, as you saw reflected in the previous figure, ADA still does perform
better. So overall, there are some things that humans can do, but ADA can't do. And there are
many things that ADA can do faster, but humans cannot do very well. But in the average, ADA is
much faster to learn than humans. The next thing we'll take a look at is multi agent learning,
because apparently it wasn't enough for the authors just to do single agents, they need to do multi
agent learning too. It's pretty crazy. So multi agent learning is unsurprisingly, when you have
multiple agents, but more interestingly, in these tasks, where there are multiple agents,
these agents need to cooperate to get the maximum reward. For example, in this environment, you're
seen right here, you have two agents one right here and one right here. But the goals for each of
them, I think are to hold these objects on the other side of the wall that they cannot pass. So
to maximize the total reward here, both of these agents need to throw their items to the other
side of the wall. So this is just one example of what a multi agent environment might look like.
At the figures up here, we can see what they're trying to measure. And what they're trying to
measure for multi agent learning is is an ADA agent cooperating with another ADA agent. In other
words, they have, you know, ADA, they can replicate it, they just have it work with itself. Is that
better than cooperating with a random agent? Because if it is, that would mean that ADA is
learning some sort of cooperation that's better than just random behavior. For all the tasks that
ADA is being given, if we look at the one where it's achieving the median score, we can see that
there's not actually a huge difference. But the reason there's not a huge difference is because
they're both already near one, they're already near optimal performance. If however, we instead
look at the 20th percentile, or in other words, tasks where ADA is not able to do quite as well
that are harder, we can see that cooperative self play, which is where ADA is playing with itself
instead of a random agent does decently outperform playing with a random agent. So this just shows
us that ADA without any special multi agent objective can actually cooperate with other
agents when those actions are in its favor. One reason I kind of like these results and I don't
mean to be spin fire here, but when you go into sort of the multi agent universe of research
being done, there are lots of approaches where they treat multi agent as this sort of separate
setting from the single agent RL problem. And while that can be useful, I think what this shows is
so long as it's beneficial to the single agent multi agent cooperation can arise as an emergent
behavior in cases like these, at least there's actually no need to model them separately. Maybe
it would get you better performance or not. I'm not entirely sure, but it's just some new good
experiment showing that this kind of thing is also feasible. This next experiment we're taking
a look at is a scaling experiment. I mean, come on, how can you publish a paper these days without
a scaling experiment? Am I right? I wonder if I should add in some like laughing noises here.
And unsurprisingly, as you might have guessed, the results here are bigger model do better. Now,
I think we should, you know, maybe not take this for granted because it was shown
without this whole distillation process that they're doing to get the bigger models working.
The larger models actually couldn't really learn it all. So these scaling curves are really possible
because of that. The x axis ranges from 6 million to 265 million parameters, which I don't know,
maybe if you only look at large language model stuff that may seem small, but that's huge for
reinforcement learning models. No one really does reinforcement learning models that big because
it's incredibly hard to train them as as you've seen. And then again, they have the score on the
y axis, but they're both log scaled. And in the paper, they say because these are log scaled and
these look approximately linear, this is a roughly power scaling law. Maybe maybe this is a bit of
a hot take. I'm sorry, I'm going to share a bit of a hot take. That's that I'm not sure if people
are just like gung ho to say that everything's power scaling nowadays. But if there wasn't already
this idea in everyone's heads that machine learning models scale via power law, I don't know, does this
does this look linear to other people? I guess here right at the top for the 13 number of trials,
it is tapering off because it's getting to a round maximum performance, right? But if we look
on the right, where these are the scores for the 20th percentile tasks, like,
is that linear? It doesn't look like it to me. It looks, you know, a little bit more like that.
Anyway, maybe I'm being a little picky here. But another thing to mention is they are showing
the median and the 20th percentile scores, which I have no problem with. That's perfectly fine.
But you know, if they showed the mean, how would that look? Would it also look like power law scaling?
Would it look linear? I don't know. I don't know. I can't imagine thinking this is a power law if I
did this without already having that in my head. This isn't even like a flight. It's just me going
on a stupid rant. That's a tangent. So I'll get over this. I'll go to the next results. Now I'm
just happy that it does scale. It's pretty great results. These next results are again about scaling,
but this time they're scaling the memory length instead of the size of the model. I think pretty
unsurprisingly, as you get a larger memory length, you do get better results here. So that is great.
One thing that is interesting to point out here, maybe two things. One is that memory seems to be
much more important in cases where the agent is not doing as well. So again, these are the 20th
percentile tasks. In other words, it's the tasks that are harder for the agent. And perhaps the
reason memory is more helpful on these is because it can't do it like in one shot or in just a few
trials. It needs more trials. Hence that longer memory comes more in handy. And the other thing
that's interesting is that if we look at one case as an example here, say this case where we have
five trials, there are 300 time steps in each trial. And five times 300 gives us 1,800 as a
total number of time steps in all of these trials. However, if we look at this sort of greenish cyan
line, we will see that once it gets to 1,800, it still continues to go up a little bit. I mean
that even though the memory is getting longer than the total amount of steps it will actually see,
it's still benefiting from that longer memory. And that's likely because even though it can keep
memory for 1,800 steps, it's still going to remember something that's much more recent and
actually still happening like in the present. The agent's probably going to remember that much
better than something that happened 1,800 steps ago, even if it theoretically can. So even once
you sort of hit that limit of the memory theoretically being able to contain everything
that you need, expanding it still can help out a bit. And maybe one more thing I should point out,
which is perhaps obvious, but important sort of as a sanity check is that the memory is the most
helpful when going from one to two trials, which is exactly what we should expect, right? For one
trial, we just need enough memory to contain the current episode. But when we go up and we add more
trials, that memory suddenly becomes a lot more important, because not only are we just sort of
remembering what our current trajectory is, we're remembering the fumbles we had in the past, and
we're learning how to improve those we're trying to adopt. Hence, you know, we see the biggest gap
here, which is sort of a sanity check that this is working how we would expect it to. And I swear
that these are the last results that I'll show you that do some sort of scaling. Here we're looking
at scaling, I guess not really scaling, but just the number of tasks use when we use 200 million
tasks versus 25 billion tasks, you know, just small difference, 200 million, 25 billion, a wee bit
more. And as you get more tasks unsurprisingly, the performance increases, I think this isn't too
much of a surprise. If you think of the excellent tasks as a distribution over tasks, we have 200
million tasks, you're of course sampling many different possibilities. But when you have 25
billion, you're sampling more possibilities, which means you're kind of filling out your
training distribution a bit more, you're also getting more diversity in what you learned. So
I think it makes sense that we should expect to see this perform better. And one thing that's kind
of cool is at least with two to three trials, we can actually have the smaller model, the 23
million parameter model in blue here outperform the 75 million parameter model by a bit just by
adding more tasks showing that you know, scaling parameters is not the only thing that's important.
Having a diverse wider set of tasks is also very important. And I think this kind of mirrors what
we've seen in NLP, where language models, you know, you need a bigger model to do better,
but you also need better data and that can help quite a bit. And now we finally wrap it back around
these distillation results. I know I already showed you how a large model without distillation
feels miserably, but one interesting experiment they add on beneath this is what happens when they
do distillation with two small models of the same size, you might expect that there would be no
benefit, or at least that's what I thought. But as it turns out, even when these two models are
the same size, the teacher and the student here, there is still some benefit. You can see that the
student ends up with a bit higher score than the teacher, even though they are trained for the same
amount of training steps. The reason they mentioned the paper for why this might be happening is due
to what's called the primacy bias. Now they don't mention this name in the paper, but I'm pretty sure
this is what they're referring to. There's a great paper all link in the description that talks about
this idea. And in the paper, the authors show that representations implicitly learned early on during
reinforcement learning can hinder an agent's later performance. Essentially, the early
representations can be bad, and then it's hard for the agent to unlearn those. So here are the
author's posture that perhaps the distillation process is helping to avoid these bad early
representations. Honestly, I'm not entirely convinced though, because I think having an extra
distillation loss alone could be the reason for the better peak performance here. That being said,
I do think this would be an interesting avenue to explore further because it's certainly not 100%
either way. I was actually taking some time to think about this and, you know, having to train
multiple models for this whole distillation process gets kind of messy and takes a lot of memory.
It would be really nice if we could get this to work with just one model. And I was wondering if
that would be possible by training with a very high learning rate or maybe training multiple
times in the beginning with these large models. One issue, of course, of training with a larger
learning rate is that some of the representations could end up bad or the model could start jumping
into some rough optimization territory. But if we randomly reset layers of hidden units,
which is something they do in the primacy bias paper to unlearn bad representations,
then maybe something like that would work. I really have no clue. This is a complete shot in
the dark and it's been quite the tangent. So I'll get back to the results now, but you know,
if anyone was looking for a research idea to try, there's an idea. We've covered what I think are
the most interesting results from this paper. So with this, where do we now stand in the world of
RL foundation models? I'll start by saying, I think this is really great. Several papers I think
have been trying to get at this idea. For example, the Gatto paper from D-Mind and the VPT from
Minecraft paper from OpenAI, both getting at something similar. And I've actually covered
both on this channel if you are interested. But all of the other papers on this topic have shied away
from actually using RL from start to finish. For example, the Gatto paper, which used supervised
learning, I remember it kind of had a line in it. I'm going to be paraphrasing this because I don't
remember it exactly, but it was something like, in theory, we could also do the same experiments
with reinforcement learning. And surely it would just work fine because, you know, it's just kind
of the same thing, but with RL instead of supervised learning. And no, hopefully I'm not
butching what they said here. But looking at this paper now, I think it's obvious why doing this
with supervised learning and doing it with reinforcement learning is really not the same.
Reinforcement learning, it's just a lot harder. The problem itself is a harder problem. Getting
the model to scale was not straightforward at all. I mean, the whole distillation thing is not
something I would have thought of at least. Not to mention it took billions of training steps,
plus millions and millions of unique tasks to get this working. But I think the really great thing
here is that now that we have a paper looking at this, not shying away from the hard questions,
is that we have a reference. And we can use this reference to start deciding the next important
steps to make this feasible on more diverse sets of tasks. And the main one here definitely looks
to be sample efficiency. Even if this is a foundation model requiring billions of steps,
it's just not going to cut it for one, anyone that doesn't have, you know,
an army of TPUs at their command. Everyone other than Google and OpenAI. Oh, sorry,
what was that? And two, any tasks where simulation is difficult or costly, like,
you know, the real world, this kind of data requirement just is not going to be enough.
Luckily, I think that there are lots of promising avenues here. As I mentioned earlier,
model based reinforcement learning methods like efficient zero and dreamer are, I think,
on the order of magnitude astros there, I think they're about a hundred times more efficient
than something like musli. So that alone would already be a huge win if it turned out to be true
in this case. And then there are a lot of other really great methods for self supervised learning
in computer vision that I would imagine could also come in handy. Continuing learning could also
play a pretty big role here, especially when it comes to foundation model. The whole idea of
foundation models is that you train them once, and then you fine tune them for many other tasks
afterwards. But neural networks have a plasticity problem. The more you train them, the worse they
get at learning new tasks, which is obviously not ideal. You can imagine that this is something
we definitely want to avoid here. Maybe check out my video on continual backprop, if that's
something you're interested in. But overall, this is really great work. Kudos to the team for
getting this working. I'm incredibly excited to see where it goes. If you've enjoyed this,
consider subscribing for more dagos and more machine learning stuff. Thank you so much for
watching, and I hope to catch you next time.

The title does not mislead you. This is indeed a 12-step plan.
12 steps right here. You can count them 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, any with 12.
That starts from essentially basic machine learning and ends up with the goal of
basically the singularity, what they call intelligence amplification.
And gosh, was this a fun read and interesting to think about.
You might be wondering what random dude drank a bit too much one night and decided to write
down their brilliant plan for this. But as it turns out, this is not written by some random dude.
Rather, this is a new paper by Rich Sutton, Michael Bulling, and Patrick Polarski,
who are all well-established researchers that head the Alberta branch of D-Mind and
are also professors in the area. So yeah, I guess this is kind of unofficially a D-Mind paper.
And not only is this just some plan, it is now the plan for research at the Alberta branch of
D-Mind. Hence why it's called, you know, the Alberta plan. And it's just out there. It has some,
I think, really interesting ideas and no one's been talking about it. So today,
I hope to take you through the motivation for this and the plan itself. It is very interesting in
the sense that it's building back up from the foundations of machine learning and reinforcement
learning. And while it is very similar in most ways, it's also somewhat contrarian to many of
the approaches, especially in reinforcement learning that are popular today. I love this because if
you're a new graduate student or you can't find a research topic or you just want to get into research,
well, here's 12 research topics handed to you on a silver platter. You really can't ask for much
more. And it's also just, I personally think, interesting to know what some prominent researchers
are thinking about and how they're going about approaching these problems that lots of us are
very interested in. One last thing I'll mention before we dive into this is I cover lots of big
ideas on this channel to keep you up to date with what's going on in the field, but also smaller
ideas kind of like this kind of go under the radar to introduce you to some new interesting ideas. So
if that's the type of thing that interests you, sort of subscribing to the channel, it means a lot
and it really does help out. Anyway, let's dive into this. In this paper starts, we're going to
come back the first two paragraphs in a second. I just want to start with the third paragraph
because it really explains the core of what's going on here, what they're thinking. So starting
from here, following the Alberta plan, we seek to understand and create long lived computational
agents that interact with a vastly more complex world and come to predict and control their sensory
input signals and blah, blah, blah. They are as scalable as possible. If you've seen Rich Sutton's
better lesson, you know, maybe they're starting to look a little familiar and they have to adapt
and change the complexity, which means they have to continually learn. Big focus on this here. And
then another thing they mentioned is they must plan with a learned model of the world. Now I
highlighted these specific things, except for the word follow, but I highlighted these specific
things because these I think are things that are very core to this paper. So long lived, right,
and continuing learning. These are things that some people in our work on, but are not super
focused on that are really the sort of at the forefront of this paper. That means that agents
are going to have to be able to work with certain challenges that are present in continuing learning.
For example, if you try and take a neural network and have it continue to learn on a constantly
changing task, many people don't realize this, but what actually happens is neural networks get worse
and worse at performing or essentially learning new things, kind of like the, what do you say,
an old dog can't learn new tricks. There's actually some concept of that in I think machine
learning that lots of people are unfamiliar with. So things like that, those are issues that might
be important in this situation. Anything vastly more complex world predicting and controlling
sensory inputs. Some of these things are very like what you would assume is common, but you'll
see that everything works under these assumptions in this paper. And we'll get into some of the
more details of why I think some of these, you know, approaching these types of problems is
interesting. The one thing that does stick out here a little bit is I think planning with
a learning model of the world, because the other things are problems, right? Sort of having to
be long lived and continuing to learn these, you know, having a vastly more complex world,
these are statements about the problem. Whereas playing with a model is like a specific thing
they do. So I guess they just think it's that important, which I would agree. But anyway,
just some details about, you know, sort of the logistics of this plan before we maybe get into
it. This is a five to 10 year plan, as they mentioned here. They also, you know, some of this is
very like, you know, we think this will greatly affect the economy or society or individual
lives. I don't think we need to go through all the, all the philosophical stuff. We can kind of get
into the meat of this. So the purpose of giving this plan, which they write about here is, as they
say, it's twofold. The main one is, or one of the two is that they want to sort of give their vision
for AI. Well, lots of people have been working on these problems, like continue learning. They
think it should really be at the forefront. So some of those things are a little bit different. So
this is their vision, what problems they think should be focused on. And then also laying out,
of course, a research plan for the purpose, you know, of doing research. This is not a to-do list.
This is not like a things they completed. This is a research plan. They, they, as they very much
stay here, gaps and uncertainties, which are to be expected, right? When you're tackling these
big problems, when you have literally 12 steps, of course, it might not be enough. Maybe some
things aren't going to be necessary. Things are going to change throughout this plan. There are
some key points or some important things that they think sort of differentiates this Alberta plan
from lots of the work going on at other places. So that's a little bit what they talk about in
this next section. Ah, the other thing in this next section is this diagram that reminds me,
I forgot to say what RL is. So this is going to be just a five second explanation. If you don't
know what reinforcement learning is, just imagine you have some environment like the world, you have
an agent, you know, it sees things from the environment, gets observations, takes actions,
and it tries to maximize some reward. There's your five second explanation. Hopefully that keyed
anyone in it wasn't okay. Anyway, what are these core tenants that they think differentiate the
Alberta plan? So the first one, here it is, they say the first distinguishing feature is the emphasis
on ordinary experience as described above. So an ordinary experience, as they'll mention here,
this means that you can't do things like use specialized training sets. You can't use human,
like assistance, you know, you can't put human data into this, you can't access the internal
structure of the world, which lots of work and reinforcement learning does, right? Not everything,
but lots of it does. And I should say, maybe clarify, human assistance here does not mean that
an AI cannot interact with humans. It just means that once like the learning process has started,
you can't have a human like tweaking like different parameters and like, you know, maybe
training a bit like this and then a bit like this. It means that the learning process, you kind of
have this agent, it starts out and then it has to do everything by itself. And why is this so
important? And the idea here is that these types of methods that require human assistance, they
usually don't scale, where do they say this, they typically do not scale with computational resources,
and as such is not a research priority. Now, if you've read the bitter, the bitter lesson, that's
basically the, you know, this is the one paragraph version of this. So what's the next thing that
they put a focus on? The next thing is temporal uniformity. What does this mean? Essentially,
it means that at all time steps, so at all times, in terms of the algorithms running,
everything needs to be the same. You can't have, and this again, goes back to what I had mentioned
earlier, you can't have a separate set of training data and testing data like you would have in
supervised learning, or lots of people when they're evaluating reinforcement learning agents,
like on the Atari 100k benchmark, I think it's pretty common to train with 100,000 steps of data,
but then the actual testing is done on different data. So that would be a no, no here, right? And
you might be like, oh, that's heresy, no training and testing, no separation. Well, yeah, but you
have to remember, we're working in the continual learning setting, which means that you can just
keep simulating your environment and getting new experiences. And well, if all those experiences
are the same, well, then maybe your environment just isn't a very interesting environment at that
point. So this is essentially talked about in the paper here, you know, they say no special
training periods, if information is provided, then it has to be the same information on every
time step. Or if it learns to plan, then it has to plan on every time step. If the agent constructs
its own representations or subtasks, then the meta learned out are the meta algorithms for
constructing them and operating them operate on every time step. So you might be wondering like,
well, aren't on some time steps, maybe where things are more complicated, won't we maybe want to plan
more like look further into the future? Yeah, maybe we'll want to do that. But the idea here is that
should all be internal to the agent, as opposed for a human tweaking these parameters, right?
You could have a meta learning algorithm that learns those types of things. And in general,
I think this, why do they do this? I think they actually say it fairly well. Their focus on this
is to leads us to interesting non stationary continuing environments, and in algorithms
for continual learning and meta learning. So in one sense, I understand totally how this leads
them. I have way too much yellow on this page, I'll start using something else.
So algorithms for meta learning, this makes sense as I just mentioned, right? You need meta
learning when you can't have a human tweaking these things, you should have an algorithm to
that and meta learning is one way to do that. On the other hand, I don't see how this really leads
to continuing environments or like continual learning, I would think it would be the other way
around, right? You have continually like continual learning. And hence, you're going to want to focus
on temporal uniformity, but maybe there's something I'm not seeing here. And another reason
they mentioned this is that keeping everything temporarily uniform, it reduces the degrees of
freedom and shrinks the agent design space, which to be fair, they think there's a decent point,
really have plenty of things to look into. There's no reason to, you know, forcibly make that higher
when we could simplify the space. Then if we go down a bit more, this is one thing we touched on
earlier. And this is also from the bitter lesson. But the third distinguishing factor is it's
cognizance of computational considerations and Moore's law. I do think lots of other, and this
essentially, all this means, right, is compute computations getting stronger or more, we have
more of it, it's more efficient. So we want to be able to take, make use of that. So we can't have
anything that's not going to scale. And I think this has been getting better over the years,
my personal opinion, it looks like lots of people are working in areas that do or on methods that
do scale fairly well. Deep learning is one great example of this rate. And basically, everyone's
using deep learning now. Oh, they even namedropped the bitter lesson. Okay, lovely. So then let's
move on to the fourth. And I think this is the final one. The fourth distinguishing feature
is the focus on the special case, or is it the special case in which the environment includes
other intelligent agents. Now, what does this mean? It doesn't mean that there need to be other
intelligent agents, but rather that intelligent agents. So like if I'm, you know, modeling a
robot that's interacting with humans, you know, there's, there's nothing fancy that I do to model
the fact that there's humans in the world, rather, they're just part of the environment.
And I need to be cognizant, and I think they say this right here, you'd be cognizant that the
environment may behave differently. And this might sound weird, but the environment is essentially,
you can think of it as, you know, a living, breathing thing, or at least it could be in
response to your own actions, right? So what you do might make the environment behave differently.
And that's, you know, we shouldn't do anything special for these other agents. To be honest,
I don't think they even need to put this here. Because I think this follows under the bullet
point of using no human experience, right? The idea that we have other intelligent or like
conscious agents, that is something that we're sort of embedding our human biases into this,
right? Technically, we can't even prove other people are conscious. So I don't know, I don't
think this is so weird, but I guess lots of places that do work with multi-agent settings, they do
do this sort of thing where they're explicitly trying to model other agents. So that would be a
no-no in this sort of setting, or at least that's, that's what they want to work with.
So if we scroll down, we're almost to the research plan. I swear we're getting there.
We're going to spend a good bit of time on that once we get there. But the last thing we need to
touch on is the base agent. So I believe this is what they call the common model of the intelligent
agent, as you're looking at right here. And they say that this is used in a few things, like
areas like economics have similar ideas. But the idea is that we want to start from some base
that we can maybe agree on, not that it's necessarily the best or something like that,
but that this is a reasonable base that will inform the rest of the research. So what's
essentially happening here is if we step through this, we start with an observation from the
environment, pretty standard, then it goes into this perception. So perception, and this is going
to be a recurrent thing, right? You also take in the last action, and you can see the output
of perception feeds back into here. So this is recurrent. So this would be keeping what they
like to call the agent state, which is like the agents, some people call this the belief state.
It's maybe what the agent perceives and wants to remember about the world to do whatever it needs
to do. It has reactive policies. So the policies are what transform these observations into actions.
It has a value function. This is a normal value function in reinforcement learning, right?
It essentially assigns value to certain states, which helps you do things like credit assignment,
and it has a transition model, which allows you to do, as you can see, planning and planning
means that you can more efficiently, you know, imagine scenarios in your head, not have to play
through them in real life. And then that means that you can be a lot more sample efficient and
potentially have other benefits too. One question, and this is a question I used to have, is why have
these things? They are, they're one, two, three, four things. Why not other things, right? We could
have, for example, a different portion here saying we want to model like other agents. I mean, we just
said like this, this is not good, we shouldn't do this, but why not? Why are these other things
okay to have? And the difference here, I don't think they mentioned the paper. But the difference,
I believe they would say, is that these four things that they have here, they're not domain
specific. These should essentially work in any domain, they're general, they're very general,
whereas modeling other agents, well, you might not always need to do this in the way in which you
might do this will probably change drastically, depending on what types of agents you're working
with. So for that reason, these four things stay and other things don't quite make the cut. It has
to do with the generality of these four components. Another thing you might mention is that the year,
there are s's right here. So reactive policies, value functions, that is something we'll get into.
These don't necessarily need to have one policy or one value function, we can work with multiple.
And that is one of the steps of the plan that we'll get into. Speaking of the plan, we are finally
about there. So here we go. A roadmap to an AI prototype. So here are 12 steps. I'm going to
start off by just reading through them. And then they have bullet points for each of these that we
can go through and talk about all of them and more depth. Some of them I'll go over somewhat
quickly. Some of them I'll go over in a bit more depth because they have more written or I think
they're more interesting. But anyway, starting off with item number one, we have continual
supervised learning with given features, then two, supervised feature finding, three, continual
generalized value function prediction learning. If you don't know what these words mean, don't
worry, I'll go over all of it as we go through the individual points later. And four, continual
active credit control, five, average reward, GVF learning, six, continuing control problems, seven,
planning with average reward, eight, prototype AI one. Wow, incredible. One step model based
around with continual function approximation, nine, search control and exploration, 10, the stomp
progression, 11, oak and 12, intelligence amplification, or this is, I guess you could call
this the singularity, though to be fair, I guess in making AIs make themselves better does not
necessarily guarantee the singularity. So maybe that's a bit of a misnomer, but you know, it's
a bit more catchy. It's more of a buzzword, I guess. So you might notice that looking over these
points, Hey, this is just normal reinforcement learning. And to some extent, you wouldn't be
entirely wrong. These are very common things that we see in reinforcement learning, like
actor credit control. What's what's another thing model based around with continual function
approximation, like what this is supposed to be a generally I that can do all these things. And
that's what I meant when I was talking about at the beginning. These are very similar to what
lots of people are doing today. But what you'll notice as we go through the points is that sort of
the focus on the importance of what's important, that's what I think is a bit different, like the
importance, for example, on continual RL. So we'll get into this, this, this, I should say, this
isn't anything groundbreaking. It's more of just an interesting plan and an interesting way to go
about this or to think about going about it this way. So let's start with our first big step,
which is continual supervised learning with given features. So what let's start out with what this
means. This essentially means given features. So maybe you have cart pull, and cart pull is a problem
by the way, where you have this little cart, and it has a little pole on top of it. And the pole can
go back and forth. And you need to move the cart back and forth to bounce the pole, very simple
problem. So maybe for your features, you could have like the x, y values of this, and then the
angular velocity of this cart, or something like that. So those would be given features. And to
point out, you know, yes, we can already do things like this. But sort of the point I think of this
plan is to really revisit things in the simplest setting. So they split the explicitly say this,
we want to go back to the simplest setting, and essentially try and make everything as good as
possible. Essentially try out everything that might have been overlooked, especially in these new
settings in the setting of continual learning, and say, can we do better? So some things they
mentioned that they might look into are how can we make things quicker? How can we train faster,
be more robust, be more efficient, while also continuing again, over long periods of time,
how can we do things like meta learn better representations? And that's be the most efficient.
So these are all questions that that would be asked, although I guess meta learning better
representations. I'm not sure if that's, I would think that would be in a different step. But,
but anyway, what are some examples of some ways, you know, you could look into this, like, what are
like, what can we really change when we have, you would think this is such a simple setting,
there's not really much we can do, but there actually are some things we can look at. So one
thing they mentioned is the global step size. So this is like your learning rate. Generally,
you have a global learning rate. Now, this is somewhat, but certain optimizers, like Adam,
that people have, some people have argued, I'm not sure, I don't know really the ins and outs of
this, but they've argued that those sorts of optimizers, while they're really good for supervised
learning, maybe aren't the best fit for reinforcement learning, because for reinforcement
learning, maybe you want to have different learning rates, depending on how important certain
features are for a certain task, or like what, you know, if you're trying to do different sorts
of tasks, there's different things you could think of. But this is like one thing you could
look at, right? Like, can you have a different step size for each single parameter? And how could
you do that in a way that makes sense for continual reinforcement learning and not, you know,
supervised learning, which is the context that most of these methods have been developed in.
And of course, I think I'm not going to find it right now in this, this C of text, but they also
mentioned that, you know, you don't want a human to be setting the learning rate. These are things
that should be meta learned. Again, right? We don't want really too much human experience. We don't
want that much fine tuning. We want the agent to be able to really do everything for itself. So
these are all things that yes, people are looking into them. But the idea here, again, is go back
to the simplest example, and get something, something working as best as possible. And then
we'll start to scale up and make things more complicated. So some other things they mentioned,
again, it's a bit too much here. Oh, here it is. So there's like normalizations of features, right?
Like, of course, we know that you can normalize features. And that tends to be a good thing in
machine learning. But when you're doing continual learning and reinforcement learning, does anything
change? You know, these are all things that could be reconsidered in this slightly different context
to maybe squeeze the most out of this that you can. So they go into this in a bit of depth.
Honestly, I'm going to skip over this because I don't think it's particularly important.
There's talking about examples of ways, you know, you can think about this, for example,
like the meta learning per weight step parameter. That's something I just mentioned. I think they
talk about this more in depth for this step, maybe because they already have some papers out about
this. But anyway, let's get on to step two. So this is supervised feature finding. So before
we had the features given to us, but now we actually want to find them, we want to generate
new features, right? And features, right, these are just, we get some observation, we have our
perception, remember from the base agent, now we want to use those to create new features to
essentially use things that will serve representations that will help us do whatever tasks we're
working on. Now, this is generally done via back propagation. And sorry, if you don't know what
back prop is, but it's a bit too much to explain this video, there's plenty of good things. There's
out there explaining it. And back prop, one issue actually with back prop is lots of people
aren't aware of this. It doesn't work super well for continual learning. I mean, maybe not super
well as in an overstatement, it does work. And it works fairly well, but it has an issue. It has
an issue that the more you train, if you train on and on and on and on and you just keep going,
it actually sort of does what they call sat and it gets saturated. I think they call like
interridger saturation. Individual nodes get saturated, which means they stop essentially
contributing. And over time, what you'll find is you'll see that maybe the performance of your
agent, it starts off low, it goes up and it gets good and it kind of blows out. Well, if the task
keeps changing, what you'll find is that the performance slowly starts to drop as the agent
is unable to adapt because the network has essentially gotten saturated. Or at least,
that's one explanation. I guess it's not fully explained yet. I actually explain this phenomenon
as well with there's a method called continue backprop or CBP for short. And this is from
Rich's lab, where they work on this problem. So this is one example of how supervised feature
finding actually needs to be adjusted to the given scenario, which is continual learning.
And this is one example of how although we have something that's really good already, we have
back propagation, it might not be the best thing to use, at least not this exact instance we use
of it right now, might not be the best thing for continual RL or something like that. One other
thing that I think is really interesting, and I found this to be always very enticing, but
sort of dangerous. I'm not dangerous and literally dangerous, but like dangerously enticing part of
the work that like Rich's lab does is the way they think about modeling, like the typical modeling
paradigm, their perspective is quite a bit different. So let me describe what I sort of think of as
the typical paradigm. So maybe I'll do that on the left here. So I think the typical paradigm of,
and this is not what all papers are about, but lots of them, one, you want to pick an architecture,
right? You have some problems. So you want to pick the architecture you're going to use. This
could be like a ResNet, right? You can use different ResNet layers. Maybe you want to use a
transformer. Maybe you want to use some sort of self attention. There's lots of different
architectures you could choose, right? You choose an architecture and then two, you choose an
objective function. So this objective function is going to be like your loss. Like what are you
trying to optimize here, especially in reinforcement learning, maybe this is usually going to be the
reward or the value or whatever. But this could be a number of things. It could be the MSC with
like an image. If you're trying to recreate an image or something like that. And then the third
and last thing you do is you pick an optimizer. So 99% of the time, this is going to be like
Adam or it's going to be RMS prop. But the thing is the way I find that within this paper, they talk
about these sorts of things and reading other papers from the same people, they tend to think
about these things a bit differently. Rather, they look at it from like a perspective of looking at
individual neurons and the interaction between these neurons. And what do I mean by this? Because
technically, like, you know, we're ending up using lots of the same things, but it is really just a
difference in perspective. So for example, we can say, how can the utility be assigned to all the
features that we're using? And then how could we make use of these utilities in the future, right?
So this is like a very much we're looking at a neuron neuron basis, trying to find the utility,
why might we want to do this sort of thing? Well, this could help us do things like evaluating
existent features and discarding less promising features, so as to make room for new ones. Well,
why don't we just keep expanding? Well, we have like a limited amount of computation here, right?
Even though we are constantly getting more, this is where like the big world hypothesis, as they
call it comes in, even though we have more and more compute, we also have just always a bigger
world so that we'll never be able to match it. So we are going to have to forget things, right?
So what do we forget? And that's kind of what this whole CBP thing I was just talking about.
This does something very similar to this. But beyond just dropping features, there are also
other things they talk about, you know, like initializing features. This is something you have
to do. Normally, you do it when you create your own network, you initialize all your features,
and then you just sort of train forever, right? But as it turns out, initialization, although
it's kind of brushed to the side, it's actually much more important than you might think. Oftentimes,
maybe not often, but occasionally, I'll implement a paper. And I've had this happen to me a couple
of times, we'll implement it, and it won't work. And I'll go back and read the paper very carefully
and realize, oh, they use this very specific form of initialization for their network,
and they try it, and suddenly everything's working. This has happened to me, especially
in papers that don't use backprop, but like use biologically inspired training. I found that
this is actually more important than you might expect. Another thing to talk about is like,
how should you adjust neurons or weights? One way to do this is obviously backprop. And I don't
think they're, they're definitely not proposing we just get rid of backprop. I think they realize
backprop scales very well. It works very well, but it might not be perfectly suited for a problem.
So how can we look at this sort of more, I don't know if I want to call like an individual neuron
approach, but something like that, more than neuron to neuron level, looking at them as features,
features with utilities and features that we want to drop or get rid of. How can we implement like
all these sorts of things? Like what, what would this look like in, in play? So maybe one example
of this, right? And this is very similar to what CBP does is say one, we would do an initialization
just as normal. We in it all our, our nodes, but this is one thing we could look into what are
better ways to in it for these types of things. The second thing we might want to do is use,
use backprop, right? We don't need to get rid of backprop. Backprop is strong, but as we use
backprop, maybe then we evaluate the utility. And then four, we remove, or maybe I should just say
we drop the nodes that are not super useful. And then the fifth thing we could do is we could
reinit, but instead of just re-init normally, maybe we could have some sort of meta learning
that figures out how to initialize nodes such that things will be learned faster, right?
So there's, there's some sort of meta stuff you could do here. So this would be one example.
And again, this is very similar. That's done what's done in CBP. I'll link my video to this
in the description. Actually, if you're interested in checking out, but this is like one alternative,
right? That still makes use of backprop, but might be better suited for something like
continue learning or reinforcement learning. And could give you more control over how you
go about developing these algorithms. As I just mentioned, I think this is a very exciting way
to think about how to model things, thinking of individual features. It's almost less limiting
the standard approach that I read out right here. But at the same time, it is dangerous because let's,
let's not kid ourselves, back propagation works very well. It scales very well. So when you do
want to do something different, you're going up against something that's already very well
established and is known to work very well. So that's why I say it's kind of dangerous. It's,
it's exciting, but hard to get working and not very explorative. It's a very underexplored area,
I think. Okay, step number three, continue GVF prediction learning. This is where we start getting
into non-IID data. So that's independent, individually distributed data. So when we're
working with reinforcement learning or in the real world, we are often, we're going to be working
with streams of data that come, you know, in, in a row. But one thing to note is that many
machine learning methods, the theory is entirely based upon data that is IID. Or in other words,
we are essentially breaking that when we move on to reinforcement learning most of the time.
That's why people like to use, or one of the reasons people like to use things like replay
buffers is because it helps you get this IID distribution, which can help you learn. Now,
we have methods of somewhat counteracting this, but it is nevertheless still an issue. But one
thing they also put in here is GVFs. I honestly don't know why they put this in here, but we can
talk about it because they do. I just don't know why they put them together. So GVFs, this is
generalized value function. And this is a fairly simple idea. If you're familiar with normal value
functions, essentially a value function measures like how good a state is, like what the expected
reward is. Whereas a generalized value function, well, it's essentially just a value function
for something other than the reward, or it could be the reward, or any other feature, right? So
you're predicting something. So they're just predictions or predictions about the world
that are not reward based, not necessarily reward based. The last thing I'll mention about this
step three is this is where perception comes in. And I think the reason they say this is where
perception really starts to come in is because of the non IID data. That means if we have like
sequential data, and we need to remember stuff that happened in the past, well, part of our
perception needs to be remembering the important thing we need to know what's important, like if
there's a key over there that I can't see anymore, I need to remember it's there so I can, you know,
go back and get it if I see a locked door or something like that. The fourth thing we've been
on now is continual actor critic control. And that is right up until this step. You might not
have noticed there was no control involved. We're not actually interacting with the environment,
but rather up to this point, we were just predicting what was happening. There's really
not much more to say here. Now that we're bringing control in the mix, we need to figure out how to
get that working with everything we've done so far. So then step five, average reward GVF learning.
Now, average reward is I think something lots of people are not very familiar with. Understandably,
so it's it's not very popular, I guess you could say. And if you've never thought about it,
using something like a discount factor and for those that aren't familiar with reinforcement
learning, usually you have this gamma parameter. And this is called the discount factor. And
essentially it weighs how important current rewards are versus future rewards. And it's,
if you think about it, it's kind of weird. The other thing that we could do, as you see right
here is just have an average reward and say we want to maximize the average reward, which would
essentially be in the same as saying we just want to maximize the reward total over everything.
The thing is, when you do something like having gamma, you're really, I actually don't want to
get into this too much. But if you've ever thought about it, it's kind of janky, right? And the argument
for why to use average reward instead is not just the jankiness, but I don't want to get into
it too much. So if you're interested, there is a whole section on this in the RL book that you
can check out. So step six, and this is going to be kind of cut us off at the first half of this
plan is the continuing control problems. Essentially, all this is going to say is we've
essentially created up until this point, a, a model free RL agent. So essentially the point
for this step is really, we just want to combine everything we've learned of the average reward
stuff, the generating new feature stuff. And we want to try it out in a bunch of continuing
environments, maybe make some more like new continuing environments, because most environments
are episodic, like the open AI gym stuff they mentioned, it's mostly episodic, but you could
convert it to continuing versions. And then we want to try the combination of everything we've
worked for so far and see how we're stacking up. Because remember, this is not like a one shot plan,
it will need to be adjusted as you go. So this would be a good point maybe to revise,
see how things are doing, a workout, anything that's maybe not working as expected. So then
when we're not going to jump just to the seventh step, this, this is kind of wraps up the first
part of this. Once we have this done, we have a, I love how they say it, a more continual, true,
it's a more continual model free learning method, which is good. We have like our first leg base.
This is where I guess after this point, we can say we have something good now. Now it's time to
start making it better. So what they focus on from here is primarily model based RL and things
start to get a little bit more complicated. Ideas, I will say from the seventh step onward,
tend to also be a little bit higher level without as many details, which I think is because, you
know, they're later in the plan less certain about how these things will go because they're
further out. So step seven is planning with average reward. So planning is very much done
in RL today. And there are lots of great examples of this. For example, you can look at Mu zero.
Mu zero is a great example of planning where the results are pretty incredible. There are some
differences here, right? It will again be in the continual setting will want to do this with
average reward. And I can say just right now, there are a lot of problems in planning that are
still unsolved. And it's very like not clear how to do things like what's what's the best way of
going about things. You also have like other alternatives to Mu zero, like dreamer dreamer is
dreamer v2 also works very well, you know, what's what's the best way to go about planning in this
setting? Who knows. So then step eight is prototype AI one. I love that. I got to love that prototype
AI one one step model based RL with continual function approximation. So this is actually
learning a model now, right? You can do planning if you have a pre given model, which I think is
what they entailed for step seven. But now that we have model based RL, that means we're going to
want to learn a model and then maybe do some sort of planning or something like that within that
model. So a one step planning model, they actually give a few steps down here that we can talk about.
So some things they want it to include. So one is a recursive update function or a perception
process. So right, we had that and this essentially makes everything more efficient. Instead of having
to like relearn the perception and the value function, the world model, the policy can have one
shared perception function that learns some sort of useful representation and knows how to store
memory of things that are important and that sort of thing. We also need a one step environment
model. They say this presumably be an expectation model or sample model or something in between.
I'm pretty sure it would not end up being an expectation model. This is I'm saying this because
this is what I do in my own work and expectation models are kind of awful. They're really easy
to learn, but they're they're not very good. So sample models are one thing they don't mention
a distribution model here. But I think that's another possibility, although there's no reason
to think a sample model couldn't also work. So and by the way, if you're unfamiliar with this,
this is just the idea that we want to predict the next step, what's going to happen next, right?
Because if we can predict what's going to happen next, then we can update based on our, you know,
sort of visions in our head, like our it's kind of like humans. I think when you go to sleep,
something like this happens, right? You get replayed memories and those can help you learn. So
so that's kind of what happens with world modeling and planning. What else is happening? So feature
finding as in step two, then importance feedback from the model. So this is now essentially tying
steps to and this planning together, right? If we're doing plain, we should be able to use that
plane to go back and change our actual features or how we're learning our features and improve them
in some way. The other idea, sorry, not other idea, but the final step here is a ranking of
features used for both feature finding and to determine which features are included in the
environment model. Since step D is essentially ranking features for feature finding, determining
what features are used in the environment model, because we might not need to use everything. If
you've looked at like what's been going on with this recently, instead of actually trying to predict
the future, lots of people are doing what's it called value equivalent models, where it doesn't
actually matter what the model predicts, so long as it's getting the right value. So it might be
predicting, you know, it might be leaving things out that are not important or stuff like that.
And then an influence of model learning and planning on feature ranking. So I think they
just kind of stated this. But anyway, I don't want to go maybe through these individual points.
They're kind of weird. Maybe tie in all these together. What are they talking about, especially
in these later few points? And the idea for this step eight is I think really that they want to
bring a full integration circle between the policy value function and the model components, right?
The idea that whenever you're doing any one of these things, when you're learning a value function,
when you're learning how to plan, the planning should essentially help you make better features,
which should then in turn help you make a better model. So all these different components, and
there's too many, there would be too many errors to draw this out in the like common model we saw
before. But the idea is that all these different parts should be affecting each other. And I do
think that this is one interesting thing that is not like too out there at all, but it's one thing
that is not fully done. Now in things like Mu zero, what they essentially do is they they have their
input state, they pass this through like a representation function. So they get there,
maybe I should call this the observation, this is like the state, then they do like the planning.
So this is called like the dynamics model to get the next state. And then from here, you go up and
you predict the policy and the value function. And then, and they do some Monte Carlo tree. So it
gets very complicated, right? But you have this whole system, and it's trained from end to end.
So when you actually train things go back like this, and you sort of update everything in one go.
So this is one type of feedback, but it's far from the only type of feedback that could be used,
right? There could be other types of feedback between these different components. Maybe again,
these are all things that could potentially be meta learned to. And those other types of
feedbacks might help these systems be more efficient, which would certainly always be great.
We head down to step nine, we have search control, and exploration, just to sum this up this,
there's not too much in this. But essentially, the idea here is that we want to make things
more efficient and a bit better when we're doing search. So search would be done in something
like planning, right? We're trying to look at the future, see what could happen. We want to see,
like, you know, what, if we take this action, what are all the different things that could happen?
Or also when we're updating over, like, you know, maybe we want to explore in a certain way that
helps us learn what we're missing, like fill out the gaps in our knowledge. So different ways you
could, you know, different things you could look into or like prioritize sweeping is one,
that's where you, I guess, like look at certain states where you have the least
knowledge of what will happen. But there's also a difference between, like, you know,
instead of using Monte Carlo tree search, there's also different types of heuristic search. Now,
I don't think they would just use heuristic search, because that seems like it uses a bit too much
human experience. But perhaps the model could, could meta learn, I say meta learn too much,
it could learn some heuristics for this, right? And that would be another way to go about planning.
So that's what that is all about. And next is the stump progression. So stump, I believe,
do they write it here? Yes, they do. So it stands for subtask, option, model, and planning. We've
already talked about most of these, you can probably infer what a subtask is, right? It's the
idea that you might want to have other tasks, other than the main task, the agent can, can work on.
And of course, the agent would learn these tasks themselves, as opposed to like a human, a human
doing these. So this is like one, one differentiation, one point they mentioned earlier, right? We don't
want, and this is what a lot of people do now is if they want to learn like skills. So a skill is
maybe, I don't know, you're going to drive a car and one skill is like turning the handle left
or right. You have to move a lot of muscles in your arms. So if you have the steering wheel,
how does, how do these normally look? I don't think this is close enough. If you want to move this
left and right, you actually have to move a lot of muscles in your arm to get that to work, right?
But for humans, it's very easy, because we've, we essentially know how to move our arms in
certain ways. We have skills or options are the more general form of this. That's when option is
in a subtask. If I'm learning how to live a good life, learning to drive is going to be one
subtask. So that's important for me to master, right? So how can the agent pose its own subtasks
and how can it learn options that solve those subtasks? And then how can we combine those?
Or sorry, I'm getting a bit ahead of myself. Combining them is the next step. What they
essentially propose, I'm not sure, I forget if they propose it here, but the idea is that they
want to have GVFs. So they want to be predicting things or have certain features, right? So feature
one, feature two, feature three, that tells things about the environment. And then they want to pose
GVFs. So being able to predict these things and also control them. So say, how can we maximize
feature one? How can we maximize feature two or feature three, and then learn options for each
of those. So essentially what you're doing is you're learning how to control your environment
in ways that are not just trying to maximize reward. And then maybe these options that you've
learned could be reused later. They actually have a paper on this called, I think, Stomp or
something like that. So if you're interested in that, I do encourage you to check that out. So then
step 11 is oak. So oak is, it stands for another thing. I get, I'm forgetting, I think options,
action knowledge or something like that. That might not be exactly right here. They actually
introduce option keyboard. Now I'm surprised they mentioned this because the other steps don't
really go in depth. Whereas the options keyboard is a very specific thing. There's a paper out
about it. They link to it. I think it's a really interesting paper. And the idea is that let's
say you have a set of options. So maybe option number one knows how to fish. And option number
two knows how to use a computer. Now, I don't know why you would ever want to do these two things
at, at the same time, but I know I hate fishing and I find it incredibly boring. So if I was going
to fish and I had a computer, well, honestly, I'd probably enjoy the nature, but you know, you could
do both at one time, right? There's no reason to say you can only do one fishing or using your
computer. Choose you could do both at the same time. If you want to weather or not, it's a good
idea. So and that's the idea of an option keyboard where you can essentially specify how much you
want to do this option or that option. And instead of having to learn a billion different
options to do all the different things you do, well, if you learn a good set of base options,
now you can combine them and get massively more expressive options that are just combinations
of others instead of having to learn those explicitly. And then we get to the last step,
intelligence amplification on how far we've come. So intelligence amplification is I think what most
people think of when they think of the singularity. It's essentially the idea that now we have our
prototype AI number two, and it should be able to do things very well. They describe this in
some interesting ways. So like there's an exo cerebellum, which is, you know, they talk about
these things, but essentially the core of what they're getting at is really at the end here,
where it says an intelligent application agent to perform policies and use planning to
multiplicatively enhance the intelligence of another partner agent or part of a single agent,
or I guess they could also change themselves, right? So we see these two versions being studied
in both human agents and agent to agent interaction settings. So essentially the idea that you have
this one agent right here, and then you have agent, agent two right here. And maybe agent two is
as much bigger brain, much smarter, you know, great brain drawing for me. So what agent two can do
is it can go to agent one and say, I'm gonna make you smarter. And then it makes agent one
better because remember it's a machine, it can edit its code. And then agent one is like, oh,
thanks for making me smarter. Now I'm going to go make you smarter. Or of course, maybe you could
just have a single agent, reperforming this on itself, or maybe there's some risk there because
it could mess itself up. Maybe that's why they talk about having two different agents. But anyway,
I mean, this is the idea, right, of how you get better and better agents. At some point,
you have an agent that is just better than humans at producing these sorts of, you know,
AI agents. And that's when we can get this sort of multiplicative scaling. Now, I think if you're
like me, you might be thinking, wait, wait, step 11 was talking about an options keeper. And in
step 12, we're talking about the singularity. Yeah, it's a bit of a jump, right? It's a bit weird.
But that's one things I'm going to be honest, I find interesting, but I'm uncertain about in this
paper. The fact that everything they outline up to step 11, for the most part, is within very
reasonable expectations. It's like what you would expect, but with a different focus. What I find
interesting is that they think that we can go from step 11 to step 12. I'm not sure how much
effort they think it will take, but essentially, that they think not much will be missing at that
point. And it is interesting to think about. On one hand, I'm very inclined to say, no, of course,
that's not going to be enough. We already have most of the things they talked about in these
previous steps. But on the other hand, we don't actually have the things they mentioned in these
previous steps. We have usually, for lots of these, we have specific instances, right? So for
like planning, we have something like Mu zero, but we still have so much more planning to explore.
There's so many different things we could try. And if everything below the planning level, like if
if we have incredibly good ways to learn representations, and if we have incredibly
more efficient ways to train your own networks for continuing learning problems, maybe things will
go a lot better. It's really hard to say. And to be honest, I would cut the authors of this
some slack. I don't think, you know, they don't think that they're just going to go through this
plan and suddenly hit step 12 and everything's going to work out. I think they're probably,
they probably realize, you know, they have to revise this. I think they even mentioned, yeah,
this is provisional. Oh, not crossing this out. It's not not provisional. It is provisional.
It's a draft and a working plan. It's going to be revised. But I do think it's interesting that
someone like Rich Sutton, who's worked in the field for a long time, has had some really good
ideas, thinks that this will be enough. And to be honest, I mean, I don't think I could come up
with a better plan myself. And I'm not sure quite what's missing here. I guess what's missing are
obviously the details that you have to fill out, right? Like meta learning, there's a million
bazillion ways to do meta learning. How are they going to do it? What's the way to, I don't know,
who knows? And that's, that's the thing, right? This is at the end of the day, it's a research plan.
It talks about the things they want to focus on, not how to do them. And it is very vague in that
sense. So overall, those are kind of my thoughts. I really like this. I think it's interesting to
read. I think it's very familiar, well, at the same time being somewhat fairly different from what
people, it's what, or rather, I should say, it's almost the same thing that people are working on,
but with a different problem in mind, different sort of problem setting. And I think interesting
differences could arise from that. I could certainly see people having a wide variety
of reactions to this. Some people saying this is completely useless. It's not detailed enough at all.
I could see other people saying, Oh, this is very interesting. I could see some people saying, Oh,
this is, this is the next step in the future. I really don't know. I'm very curious. So let me
know what you think in the comments. I'm, I think we'll have some very different opinions. I am
excited to say, if you've enjoyed this, do consider subscribing to the channel, maybe check it out
some of my other videos. I would really appreciate it. It really helps out. And hopefully you'll
find some other interesting content. Anyway, thank you so much for joining me and I hope to catch you
next time.

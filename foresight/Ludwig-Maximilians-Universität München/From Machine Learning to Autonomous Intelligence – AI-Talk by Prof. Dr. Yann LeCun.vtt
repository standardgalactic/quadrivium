WEBVTT

00:00.000 --> 00:02.060
you

00:30.000 --> 00:32.060
you

01:00.000 --> 01:02.060
you

01:30.000 --> 01:32.060
you

02:00.000 --> 02:02.360
you

09:00.000 --> 09:29.480
good afternoon I still have to wait for a sign since we

09:29.480 --> 09:38.400
the livestream determines the program are we ready I guess we are ready good

09:38.400 --> 09:43.720
afternoon dear ladies and gentlemen the president of the Bavarian Academy of

09:43.720 --> 09:48.840
Science I would like to welcome you all to our talk focusing on current

09:48.840 --> 09:54.920
developments of advanced computing technologies we are very proud to host

09:54.920 --> 10:01.820
this event in the rural residents in the center of Munich which houses the main

10:01.820 --> 10:09.200
office of the Bavarian Academy of Science founded in 1759 the Academy

10:09.200 --> 10:15.080
functions today as a community of scholars a non-university research

10:15.080 --> 10:19.680
institution and a communication interface between the Bavarian

10:19.680 --> 10:26.240
scientific community society and policymakers with the scholars the

10:26.240 --> 10:31.240
Academy provides a powerful internet disciplinary network of very

10:31.240 --> 10:37.880
established scientists this network of excellence interacts very closely with

10:37.880 --> 10:43.720
all Bavarian research institutions and political decision makers and represents

10:43.720 --> 10:49.920
an important part of the science communication with the public research

10:49.920 --> 10:55.560
activities range from the composer Richard Strauss to the study of climate

10:55.560 --> 11:01.600
change in their alps from baroque ceiling paintings to quantum physics the

11:01.600 --> 11:06.520
longer-term basic research spans from natural science to technology to

11:06.520 --> 11:12.120
humanities and social studies the Academy research project actively

11:12.160 --> 11:18.200
leverage the latest digital technologies the Leibniz supercomputer center also

11:18.200 --> 11:23.880
part of our Academy serves as an important infrastructural support for

11:23.880 --> 11:29.120
digital activities at Bavarian universities today we will hear an

11:29.120 --> 11:33.440
important contribution to the rapidly growing discussions about artificial

11:33.440 --> 11:39.280
intelligence from Jan Lecun who is according to the Time magazine one of

11:39.280 --> 11:45.280
the hundred worldwide leading AI pioneers his professor of NYU on chief

11:45.280 --> 11:53.000
I scientists at Metta not in California in New York I just learned the title of

11:53.000 --> 11:57.960
his talk from machine learning to autonomous intelligence artificial

11:57.960 --> 12:02.120
intelligence as you all know is the key topic of our time not only in research

12:02.120 --> 12:08.000
and industry but also in the broader society this becomes evident as a large

12:08.120 --> 12:13.760
interest and thank you for all coming to this meeting on sharing this

12:13.760 --> 12:20.840
experience we're very pleased as Bavarian Academy to gather with the AI agency to

12:20.840 --> 12:26.840
be part of biosphere biosphere is the official network of all AI activities

12:26.840 --> 12:33.560
in Bavaria we partner in this important task to advance AI science in Bavaria in

12:33.640 --> 12:39.000
close cooperation with the Center for Advanced Studies at LMU the Bavarian

12:39.000 --> 12:43.040
Research Institute for Digital Transformation the Munich Center for

12:43.040 --> 12:50.680
Machine Learning and the Konrad Susie School of Excellence in Reliable AI the

12:50.680 --> 12:55.080
event today is part of this successful cooperation and we're looking very

12:55.080 --> 13:00.480
broadly forward to learn more about recent progress from advanced computing to

13:00.480 --> 13:07.520
autonomous intelligence I hereby hand over to professor Thomas Seidel member

13:07.520 --> 13:13.440
of the Bavarian AI Council chair of database systems and data mining

13:13.440 --> 13:20.280
director of Munich Center for Machine Learning to all of you I wish a very

13:20.440 --> 13:25.760
insightful and informative afternoon the Seidel

13:33.760 --> 13:39.720
yeah thank you President Schweiger for this nice introduction and very warm

13:39.720 --> 13:45.600
welcome also from my side to all of you particularly to Jan Leckardt to be here

13:45.600 --> 13:52.040
today I wear two hats today one is I'm a member of the Bavarian AI Council we

13:52.040 --> 13:56.560
are 20 members appointed by the Bavarian state government as part of their high

13:56.560 --> 14:01.600
tech agenda from universities research institutions and also companies in the

14:01.600 --> 14:07.000
field to advise the government on AI strategies and actions particularly in

14:07.000 --> 14:13.440
steering the Bavarian AI agency so the representative is also Dr. Klimke which

14:13.480 --> 14:18.560
promotes the Bavarian network which we call biosphere so the logo is there as

14:18.560 --> 14:23.120
well so the biosphere comprises a variety of strong AI players in Bavaria

14:23.120 --> 14:27.400
also universities research institutes from all over Bavaria as well as a lot

14:27.400 --> 14:32.000
of the global companies we have here including Google, Microsoft, IBM but

14:32.000 --> 14:38.800
also the locally sitting global players Siemens, BMWs, every insurances Munich

14:38.800 --> 14:43.520
three Alliance and so on but also many original small and medium enterprises

14:43.520 --> 14:50.800
and startups in the field of AI so the second hat I am aware today's I'm one

14:50.800 --> 14:53.920
of the four directors of the Munich Center for Machine Learning one of the

14:53.920 --> 15:00.120
co-directors Daniel Gremmels also here and we this is a consortium of LMU and

15:00.120 --> 15:05.160
Tom funded by the BNBF in the Bavarian high-tech agenda with around 50 PIs in

15:05.160 --> 15:08.800
machine learning in the I3 junior research groups recently established

15:08.800 --> 15:14.480
around 200 doctoral students and our focus is on foundations of machine

15:14.480 --> 15:19.120
learning where we have several players including Gitta Kottiniok from the

15:19.120 --> 15:23.040
mathematical part and statistics and computer science is there then perception

15:23.040 --> 15:28.360
we are particularly strong in computer vision here and in Munich and in natural

15:28.360 --> 15:34.560
language processing the two big things where humans and computers interact

15:34.600 --> 15:41.280
and a lot of domain specific things it's not on research but also on transfer

15:41.280 --> 15:46.120
activities fostering the collaboration network together with the biosphere

15:46.120 --> 15:50.560
outreach to the general public things like that if you're interested also

15:50.560 --> 15:55.200
openings of course so this is that part so I'm sure we get fully inspired by

15:55.200 --> 16:00.240
your presentation and between your presentation is now the next is Dr. Mayer

16:00.240 --> 16:13.360
from TASS, you're on stage, thank you ladies and gentlemen may I also welcome

16:13.360 --> 16:17.920
you warmly on behalf of the Center for Advanced Studies at LMU and let me

16:17.920 --> 16:22.440
briefly say a few words about this institution and how it comes into play

16:22.440 --> 16:27.840
the Center for Advanced Studies at LMU was founded 15 years ago to provide a

16:27.960 --> 16:33.600
forum for precisely those research questions that cannot be tackled by

16:33.600 --> 16:39.200
only one discipline this was intended to take account of an increasingly

16:39.200 --> 16:45.200
diversifying but also specializing body of research that is becoming more and

16:45.200 --> 16:50.640
more disparate not only in terms of content but also in terms of space in

16:50.640 --> 16:56.640
Munich you can just think of the campus in Ober-Schleishheim, Ober-Guy-Ching,

16:56.640 --> 17:02.600
Nordeid in the far south so the Center for Advanced Studies offers the place

17:02.600 --> 17:09.120
where these centrifugal forces can be bundled and for what topic does this

17:09.120 --> 17:14.200
task play a more important role than for artificial intelligence which is

17:14.200 --> 17:21.080
spread across most faculties of LMU and other universities it was therefore a

17:21.080 --> 17:25.600
great pleasure for us when Gitta Kutinyok freshly appointed at our

17:25.640 --> 17:30.680
university approached us and asked whether a form it could be found that

17:30.680 --> 17:36.920
would network research on AI at LMU and unable to discuss overarching issues

17:36.920 --> 17:42.280
together it wasn't long before a so-called interdisciplinary CAS

17:42.280 --> 17:47.880
research focused entitled next generation AI was born bringing together

17:47.880 --> 17:53.960
researchers from 14 faculties working on the topic of artificial intelligence

17:53.960 --> 17:59.960
over the course of two years a wide variety of lectures workshops and

17:59.960 --> 18:05.680
conferences was organized and held and we were thrilled by the spirit that

18:05.680 --> 18:11.120
emerged of that group that's why we look forward to Professor LeCun's lecture

18:11.120 --> 18:16.760
today with both a smile and a tear as it marks the formal conclusion of the

18:16.760 --> 18:22.040
research focus we are honored and grateful that Professor LeCun is going

18:22.080 --> 18:39.320
to give the lecture in this framework today yeah also a warm welcome from my

18:39.320 --> 18:44.320
side to everyone here on site and also to everyone who participates via

18:44.320 --> 18:49.240
live stream it is wonderful to have so many people with us here this afternoon

18:49.920 --> 18:52.760
and a great thanks to all of our cooperation partners for actually

18:52.760 --> 18:56.720
making this lecture possible and here I would like to particularly thank Dr

18:56.720 --> 19:00.640
Anette Meyer and the Center for Advanced Studies of the Ludwig Maximilians

19:00.640 --> 19:05.400
University MÃ¼nchen Professor Dr. Markus Schweiger and the Bavarian Academy of

19:05.400 --> 19:10.120
Sciences and Humanities that this event can take place here in the academy in

19:10.120 --> 19:14.640
these really beautiful rooms Professor Dr. Thomas Seidel and Dr. Michael

19:14.680 --> 19:19.320
Klimker from the biosphere the Bavarian Eye Network which made the live stream

19:19.320 --> 19:24.520
for the event possible and also Dr. Christoph Egle from the Bavarian

19:24.520 --> 19:29.360
Research Institute for Digital Transformation and now it's my great

19:29.360 --> 19:34.400
pleasure and honor to welcome Professor Jan LeCun thank you very much for

19:34.400 --> 19:39.080
accepting our invitation and for coming to Munich for this lecture today

19:39.680 --> 19:44.520
Professor LeCun is chief AI scientist at Meta and the silver professor of

19:44.520 --> 19:49.680
computer science at New York University he started his career with a PhD in

19:49.680 --> 19:55.400
computer science at Sorbonne University in Paris and then moved to the US where

19:55.400 --> 20:00.360
he became the head of the image processing research department at the

20:00.360 --> 20:08.120
famous Bell Labs the AT&T Bell Laboratories then after intermediate

20:08.160 --> 20:14.320
stations he joined New York University in 2003 and he also became there the

20:14.320 --> 20:22.000
founding director of the NYU Center for Data Science in 2012. His groundbreaking

20:22.000 --> 20:26.320
work includes among many others the development of convolutional neural

20:26.320 --> 20:30.480
networks which are the state-of-the-art for basically any problem in particular

20:30.480 --> 20:35.360
imaging sciences and computer vision and a particularly particular convolutional

20:35.560 --> 20:41.240
network architecture is also named by him the so-called LeNet which in

20:41.240 --> 20:45.440
sense also promoted the impressive development of deep learning and AI as

20:45.440 --> 20:52.520
we experience it today. His contributions are honored by numerous awards many more

20:52.520 --> 20:57.120
than I could name here let me just mention that he's a member of the US

20:57.120 --> 21:01.640
National Academy of Sciences and the National Academy of Engineering he

21:01.640 --> 21:07.480
received various honorary degrees for instance from EPFL, received the IEEE

21:07.480 --> 21:15.800
neural network pioneer award and in 2019 the Turing Award which is typically

21:15.800 --> 21:20.880
referred to as the Nobel Prize of Computing and just a few weeks ago we

21:20.880 --> 21:25.720
already heard at the Time Magazine and congratulations to that has selected him

21:25.720 --> 21:32.280
as one of the 100 most influential people in AI worldwide and he also

21:32.280 --> 21:36.840
repeatedly contributes to the public debate about AI with also controversial

21:36.840 --> 21:43.680
proclamations for example on the current craze around large language models. How

21:43.680 --> 21:49.600
could machines to learn as efficiently as humans and animals? How could machines

21:49.720 --> 21:56.360
learn to reason and plan? In his lecture Professor Jan LeKang will now talk about

21:56.360 --> 22:02.760
a possible path towards an autonomous intelligent agents based on a new

22:02.760 --> 22:07.440
modular cognitive architecture. Where come Jan? The floor is yours.

22:19.600 --> 22:33.760
Thank you very much for the introduction and thank you very much for

22:33.760 --> 22:38.960
inviting me for coming here so so numerous. I have to correct one thing

22:38.960 --> 22:46.960
though I did not call convolutional net solonet this was my lab director at Bell

22:46.960 --> 22:58.640
Labs who gave it that name I would never done this but it's a good name okay it's

22:58.640 --> 23:02.880
a long title and a long subtitle objective-driven AI this is what I call

23:02.880 --> 23:08.360
this I used to give this talk with the title autonomous machine intelligence

23:08.360 --> 23:15.040
and and it scares people you know they say do you mean machines that will be

23:15.040 --> 23:17.880
autonomous we're not going to be able to control them so I changed the name to

23:17.880 --> 23:22.840
objective-driven AI because that's really more accurate and they're really kind

23:22.840 --> 23:26.640
of systems it's an aspiration it's not something that we've done it's something

23:26.640 --> 23:31.480
that we should do and there are systems that could of course learn remember

23:31.480 --> 23:39.080
reason plan have common sense be steerable controllable safe and have the

23:39.080 --> 23:42.880
same kind of learning abilities and intelligence that we observe in animals

23:42.920 --> 23:50.120
and humans so let me start by a little bit of the state of the art okay because

23:50.120 --> 23:54.640
there's a lot of debates today about about AI and a lot of people are afraid

23:54.640 --> 24:00.280
of AI it's understandable whenever there is technological revolution people are

24:00.280 --> 24:05.280
afraid of the unknown and AI is promising to be a big revolution so people

24:05.280 --> 24:10.080
are afraid so let's first talk about the benefits before we talk about the risks

24:10.760 --> 24:17.680
and the benefits are of AI are numerous already today and there is you know even

24:17.680 --> 24:23.400
more coming in medicine particularly in imaging diagnosis assistant treatment

24:23.400 --> 24:27.480
protocol drug design things like this very promising research transportation

24:27.480 --> 24:32.480
every car sold in the European Union today has to come with what's called a

24:32.480 --> 24:36.920
automatic emergency braking system a system that will automatically stop the

24:36.920 --> 24:42.480
car there is an obstacle in front of it and the driver does not react this

24:42.480 --> 24:47.960
saves lives it reduces frontal collision by 40% so AI saves lives and that uses

24:47.960 --> 24:56.120
convolutional nets by the way and in all the systems that I know in fact Germany

24:56.120 --> 25:03.920
was kind of a and and a very in particular was a pioneer in this some of

25:03.920 --> 25:09.000
the early systems of this type was the word developed by them events so driving

25:09.000 --> 25:12.520
assistance autonomous driving energy storage and management things like that

25:12.520 --> 25:16.160
environmental environmental monitoring and protection I'm going to say a few

25:16.160 --> 25:20.040
words about this content information and management this is probably the biggest

25:20.040 --> 25:25.280
use of AI today and of course in industry manufacturing information systems

25:25.280 --> 25:29.720
quality control etc a lot of applications are expected also in things like

25:29.720 --> 25:35.080
education for personalized education connecting people with each other with

25:35.080 --> 25:40.320
translation today presence augmented reality virtual reality and then

25:40.320 --> 25:45.160
enormous applications in science biology and genomics neuroscience physics

25:45.160 --> 25:49.600
particularly physics of disordered systems complex systems very large-scale

25:49.600 --> 25:56.440
simulations chemistry material science very promising area for AI so this well

25:56.480 --> 25:59.800
really and of course you know we've been talking a lot about creation like

25:59.800 --> 26:06.480
creating art AI is essentially enabling a lot more people to be creative people

26:06.480 --> 26:10.440
who don't necessarily have the technique the underlying technique for

26:10.440 --> 26:15.960
producing art so I will affect every aspect of human activity and let me give

26:15.960 --> 26:21.120
you a couple examples so this is a video that was put together by my colleagues

26:21.120 --> 26:25.840
at meta a couple years ago this is already sort of aging if you want and we

26:25.840 --> 26:32.360
chose the capability of computer vision system as of about two years ago so we

26:32.360 --> 26:36.800
can have systems that detect objects and put frames around them give them a name

26:36.800 --> 26:41.600
they can track human bodies and figure out in what what pose they are densely

26:41.600 --> 26:46.400
actually so that's actually very useful for all kinds of applications and more

26:46.400 --> 26:50.520
interestingly we can have systems that perform what's called semantic

26:50.520 --> 26:55.360
segmentation which means isolating every object marking them with kind of a mask

26:55.480 --> 27:00.040
and then giving them a name for a category and this works for a very

27:00.040 --> 27:04.880
fine-grained category for example the species of a bird or or plant or

27:04.880 --> 27:09.160
something of that type so it's pretty amazing it's not like computer vision is

27:09.160 --> 27:14.080
completely solved in fact if it was solved we wouldn't have the large

27:14.080 --> 27:19.760
conference that takes place in Paris next week called ICCV so there's still a

27:19.800 --> 27:28.400
lot of work to do but but there's been a huge amount of advances there and a lot

27:28.400 --> 27:36.920
of advances in AI but no advances in my slides for some reason okay my

27:36.920 --> 27:42.920
presentation refuses to advance hang on just one minute one second

27:50.760 --> 27:57.600
okay I mentioned medicine so certainly medical imaging is an area where a lot

27:57.600 --> 28:01.800
of work is going on there's too many to cite really this is some work by some of

28:01.800 --> 28:09.240
my colleagues at NYU that use 3d image recognition not just 2d in some cases

28:09.240 --> 28:14.560
this is actually 2d but that use various techniques to detect for example tumors

28:14.640 --> 28:22.000
in mammograms or particular things in MRI and other types of images and

28:22.000 --> 28:27.360
almost a lot of progress there some product project that took place a few

28:27.360 --> 28:31.240
years ago which was a collaboration between the NYU radiology department and

28:31.240 --> 28:37.080
people at fair Meta's fundamental research lab which essentially allows to

28:37.080 --> 28:41.280
accelerate the data collection for an MRI by a factor of 4 without degrading the

28:41.280 --> 28:46.080
image quality so instead of having to lie down in a MRI machine for 40 minutes

28:46.080 --> 28:49.320
or something you can reduce this to 10 minutes and have the same quality of

28:49.320 --> 28:54.640
images and that's thanks to deep learning essentially a lot of applications in

28:54.640 --> 29:01.520
science what's interesting today is that the favorite model that neuroscientists

29:01.520 --> 29:06.480
use to explain how the brain works use artificial neural nets so the best

29:06.480 --> 29:10.640
explanation for what we observe using functional MRI data in the visual

29:10.720 --> 29:15.280
cortex of humans and animals are actually models that are essentially

29:15.280 --> 29:22.600
convolutional net models and that's kind of a closing the circle because the

29:22.600 --> 29:26.560
architectural convolutional net is actually inspired by the architecture of

29:26.560 --> 29:31.800
the visual cortex classic work in neuroscience from the 1960s the similar

29:31.800 --> 29:40.280
work also in language understanding this is a recent paper in science by some

29:40.280 --> 29:45.360
colleagues from from from it actually and they try to figure out if the

29:45.360 --> 29:49.480
current large language models that everybody is playing with explain the

29:49.480 --> 29:54.240
what we observe in the brain when people are asked to kind of remember or

29:54.240 --> 29:59.360
understand a story and the answer is sort of but not really it doesn't work nearly

29:59.360 --> 30:03.680
as well as a convolutional net models for vision so what that means is that

30:03.680 --> 30:08.680
we're missing something that those models probably are not sufficient to

30:08.680 --> 30:14.040
explain what the brain does when when we understand language I mentioned some

30:14.040 --> 30:18.480
applications in science in particle physics in particular high energy physics

30:18.480 --> 30:24.520
to kind of make models of particle collisions and things of that type image

30:24.520 --> 30:30.280
processing to discover exoplanets some estimate says that about 12 percent of

30:30.280 --> 30:36.000
all physics papers today actually mention AI as a tool that he used which is

30:36.040 --> 30:42.560
astonishing in just a relatively short time and in the large-scale simulation

30:42.560 --> 30:46.080
sort of universe scale simulation that could sort of validate or invalidate

30:46.080 --> 30:51.760
certain theories about dark matter and things I guess so very fascinating work

30:51.760 --> 30:57.520
and applications this is a very interesting project that was started by

30:57.520 --> 31:03.280
some of my colleagues at fair by Larry Zittnick in particular called the open

31:03.280 --> 31:08.360
catalyst project and you can actually participate if you want the website is

31:08.360 --> 31:18.840
open-catalyst.org and and that project the idea of that project is that we

31:18.840 --> 31:23.800
could solve climate change if we had a good efficient scalable way of storing

31:23.800 --> 31:30.400
energy if we had a good way of storing energy we could cover a small desert

31:30.680 --> 31:36.880
with solar panels and produce enough energy to power Europe or the entire

31:36.880 --> 31:43.700
planet the problem is you have to have a way of storing energy which is why

31:43.700 --> 31:50.760
renewables today despite the decisions of the German government to go all out on

31:50.760 --> 31:57.840
it renewables are not drivable you can't control whenever there is wind or sun

31:58.120 --> 32:03.960
and so you need another source of energy when there is no no sun or or no no

32:03.960 --> 32:08.800
wind and and for that you need to be able to store energy and ship it wherever

32:08.800 --> 32:12.120
it's needed the best way to store energy is in the form of hydrogen or maybe

32:12.120 --> 32:17.080
methane and the best way to do this is by separating hydrogen from oxygen from

32:17.080 --> 32:20.960
water right so take some water put two electrodes and then separate hydrogen

32:20.960 --> 32:28.840
from oxygen problem with this is that it's either scalable if you use catalyst

32:28.840 --> 32:36.360
to do this like platinum sorry it's either efficient if you use catalyst like

32:36.360 --> 32:40.800
platinum or it's scalable but not efficient and so the big question is

32:40.800 --> 32:46.640
could we design compounds new catalyst that would facilitate this reaction so

32:46.640 --> 32:51.440
that is efficient but does not require exotic materials like like platinum so

32:51.440 --> 32:56.240
that is scalable and the idea there is that you do a lot of chemical

32:56.240 --> 33:02.080
simulation that's called DFT simulation of various of water on two various

33:02.080 --> 33:06.520
compounds and then you generate that data using simulation and also using

33:06.520 --> 33:10.320
experiments you put that data you make it available and then you ask people can

33:10.320 --> 33:14.240
you train machine learning system to figure out what the underlying rule is

33:14.240 --> 33:19.960
so that we can use it to design new materials that might have the same

33:19.960 --> 33:26.520
effect but be cheap so fascinating program it may not work but it's worth

33:26.520 --> 33:33.160
a shot okay now what's important to realize is that the progress we've seen

33:33.160 --> 33:36.720
over the last few years in AI and machine learning are due to a set of

33:36.720 --> 33:40.720
techniques that we call self-supervised running which I'm sure many of you here

33:40.720 --> 33:47.160
in the room have heard about and essentially self-supervised running

33:47.160 --> 33:52.880
would be a set of techniques that allows a system to be trained to represent the

33:52.880 --> 34:01.280
data the world without requiring labeled data okay without requiring sort of

34:01.280 --> 34:09.960
manual human intervention to produce the data so perhaps the best success of

34:09.960 --> 34:15.680
this idea which I've been advocating for a long time is in the context of natural

34:15.680 --> 34:21.960
language understanding so the way all NLP systems are trained today whether

34:21.960 --> 34:26.800
there are LLMs of the types that we play with or others is the following you

34:26.800 --> 34:32.360
take a piece of text a sequence of words and you remove some of the words you

34:32.360 --> 34:37.040
you mask you mask them you blank them out you replace them by a blank marker okay

34:37.080 --> 34:41.720
you corrupt essentially the input and you put it at the input of a large neural

34:41.720 --> 34:45.320
net you train this very large neural net usually usually a transformer

34:45.320 --> 34:51.240
architecture to predict the words that are missing in the process of doing so

34:51.240 --> 34:56.920
the system has to extract representations of the text that contain the

34:56.920 --> 35:03.320
semantics the syntax the you know grammar everything I sort of lied slightly

35:03.480 --> 35:08.120
here these are not words that are input they are what's called tokens which are

35:08.120 --> 35:13.720
essentially subword units so in most languages words have a prefix and a

35:13.720 --> 35:18.400
root and a suffix and you need to kind of separate those for those systems to

35:18.400 --> 35:22.200
work properly otherwise your dictionary of words would be gigantic and then in

35:22.200 --> 35:25.160
German you have to do it because you can have words that are long like this

35:25.160 --> 35:30.600
they are you know by so so there is no choice you have to break up words into

35:30.600 --> 35:37.000
subword units you know in tokens and so you train that you train the system and

35:37.000 --> 35:43.320
and this is the so-called BERT model if you want or idea and that's me

35:43.320 --> 35:46.800
incredibly successful it's completely self-supervised you don't need any other

35:46.800 --> 35:51.760
data than the text and once you've pre-trained that system you can use the

35:51.760 --> 35:57.680
internal representation produced by the system as input to a subsequent task a

35:57.680 --> 36:02.400
downstream task like let's say translation hate speech detection you

36:02.400 --> 36:06.200
know summarization whatever so that's the general idea of self-supervised

36:06.200 --> 36:10.880
running fill in the blanks have a big piece of data corrupt it in some way

36:10.880 --> 36:16.120
and then train some big neural net to fill in the blanks or or recover the

36:16.120 --> 36:21.200
original data a particularly stunning example of this which I'm not going to

36:21.200 --> 36:27.200
go into the technical details of but I will later is a system they came out of

36:27.240 --> 36:32.360
my colleagues that in Paris that fair Paris called Dino v2 you can think of it

36:32.360 --> 36:36.160
as a foundation model for vision so it's a system that is trained to extract

36:36.160 --> 36:39.960
features from images such that those features can be used for anything you

36:39.960 --> 36:43.960
want whether it's classification fine-grained classification depth

36:43.960 --> 36:48.200
estimation semantic segmentation instance retrieval so the same kind of

36:48.200 --> 36:52.120
application that I showed in the video but basically with very little

36:52.160 --> 36:56.880
supervision this is then it's pre-trained and it basically because it's

36:56.880 --> 37:03.640
pre-trained on enormous amounts of data just training a very shadow head to

37:03.640 --> 37:08.360
solve any particular one of those problems actually beats the state of the

37:08.360 --> 37:12.600
art for that's estimation or classification or whatever you can

37:12.600 --> 37:17.160
actually play with it interactively that's the URL that you see here and

37:17.160 --> 37:21.240
these are some examples of visualization of what the features that are

37:21.280 --> 37:26.640
extracted are it's kind of a you know colorful representation of the like

37:26.640 --> 37:30.840
different feature vectors are represented by different colors this is

37:30.840 --> 37:33.600
actually kind of each color is like a principal component if you know what

37:33.600 --> 37:39.080
that is so those are you know examples on sort of typical typical images and

37:39.080 --> 37:42.600
people I've started to use this for all kinds of stuff for biological image

37:42.600 --> 37:49.120
analysis for astronomy for for environmental protection so that's the

37:49.120 --> 37:53.240
next example I'm going to show you so this is a project by someone on the team

37:53.240 --> 37:58.480
Camille Coupri and a large collection collection of collaborators and what she

37:58.480 --> 38:06.800
did was use the Dino V2 features and trained relatively small system on top

38:06.800 --> 38:13.600
of it to tell what the height of the trees are from a satellite image so we

38:13.600 --> 38:17.400
have lots of satellite images on the entire world at half meter resolution

38:17.400 --> 38:22.560
you can get this from a satellite imaging companies and for some areas

38:22.560 --> 38:27.960
there is LiDAR data which tells you how tall the trees are so you use that to

38:27.960 --> 38:31.040
train the system and then you can apply it to the entire world and what it tells

38:31.040 --> 38:37.840
you is how much how much carbon is captured by the trees if you know

38:37.840 --> 38:41.160
roughly what the height of the tree is you know roughly how much carbon is

38:41.600 --> 38:47.600
captured in the tree that's super important to know like you know should

38:47.600 --> 38:53.200
we protect forests of course we should should we plant more trees where things

38:53.200 --> 38:57.960
like that so very interesting this publications on this where you know

38:57.960 --> 39:03.240
everything is detailed and everything another success of self supervised

39:03.240 --> 39:07.120
running of the type that I showed for natural language processing where you

39:07.400 --> 39:15.720
remove some other words is in biology proteomics particularly so you can the

39:15.720 --> 39:21.120
protein is a sequence of amino acids and we know hundreds of millions of them so

39:21.120 --> 39:24.080
you take a sequence of amino acids you remove some of the amino acids and you

39:24.080 --> 39:27.480
train some gigantic neural net to predict the amino acids that are missing

39:27.480 --> 39:32.640
the system kind of learns to represent sequences of amino acids that

39:32.640 --> 39:36.840
constitute proteins and then you use that representation as input to a system

39:36.880 --> 39:40.720
that predicts the conformation of that protein how it folds well they can stick

39:40.720 --> 39:44.840
to another protein a particular location so there's a famous work by our

39:44.840 --> 39:50.280
colleagues at DeepMind at Fairfold but the this idea of using pre-trained

39:50.280 --> 39:56.440
transformers for protein was actually first published by my colleagues at fair

39:56.440 --> 40:00.400
they're actually no longer at fair now they have left Fairf to create a startup

40:00.440 --> 40:07.840
around this around this idea but it's incredibly successful thousands of

40:07.840 --> 40:11.840
research groups around the world are using this kind of data is actually a

40:11.840 --> 40:17.920
atlas of folded protein contains 600 million proteins or something like that

40:17.920 --> 40:26.680
with the structure that is predicted it's called the ESM metagenomic atlas and

40:27.440 --> 40:34.480
ESM atlas.com a very big tool for biologists that really may change

40:34.480 --> 40:38.920
completely the way we do drug design and understand the mechanisms of life

40:38.920 --> 40:46.320
another very impressive project here that required a lot of effort is a project

40:46.320 --> 40:51.480
called no language left behind again from fair collection of people from the

40:51.480 --> 40:55.240
various sites of fair and this is a system that can translate 200 languages

40:55.880 --> 41:01.920
from in any direction and when you look at what those languages are it's a lot

41:01.920 --> 41:07.200
of languages most of them we never heard of in you know square corners of the

41:07.200 --> 41:12.480
world but it's important for people to be able to preserve that culture that you

41:12.480 --> 41:16.360
know they can speak their language and basically be understood using automatic

41:16.360 --> 41:21.360
translation so what's interesting about this is that there are four thousand

41:21.800 --> 41:29.120
directions for translation but the data only covers 2400 of those pairs

41:29.120 --> 41:37.960
among the 40,000 despite that because we train a giant transformer to represent

41:37.960 --> 41:43.200
language regardless of the language the system takes advantage of the

41:43.200 --> 41:47.080
similarities between between the language families to actually kind of

41:47.080 --> 41:51.160
extract a multilingual language independent representation of language

41:51.160 --> 41:54.920
which allows the system to do translation in any direction including

41:54.920 --> 41:59.960
four directions has never been trained on that's pretty amazing pretty small

41:59.960 --> 42:09.680
model but today standard only 54 billion parameters I mean sizable the same team

42:09.680 --> 42:14.480
now as another project called seamless which was was announced a few weeks ago

42:14.480 --> 42:21.840
they can do speech to speech speech to text text to speech and text to text

42:21.840 --> 42:27.760
translation as well as speech recognition speech synthesis etc speech to

42:27.760 --> 42:33.680
speech is interesting because it can do translation for languages that are not

42:33.680 --> 42:37.360
written directly from speech to speech that system can handle a thousand

42:37.360 --> 42:43.320
languages which is really impressive okay so applications of deep learning that

42:43.320 --> 42:49.160
are less visible perhaps is that deep learning or AI connects people to

42:49.160 --> 42:53.920
knowledge and they connect people to each other the biggest deployment of

42:53.920 --> 42:58.320
machine learning today is probably in social networks and online services like

42:58.320 --> 43:03.560
like search engines and if you take deep learning out of Google or Meta or

43:03.560 --> 43:10.640
Microsoft companies crumble they literally are built around it so deep

43:10.640 --> 43:13.040
learning helps us deal with the information deluge for doing things

43:13.040 --> 43:16.720
like search and retrieval ranking question answering things like this but

43:16.720 --> 43:20.600
and that requires machine to understand content of course for

43:20.600 --> 43:25.160
translation which is very useful for people who are not literate for example

43:25.160 --> 43:30.080
or people are blind or visually impaired so there's three billion people in the

43:30.080 --> 43:34.800
world today who can't use technology because they basically can't read more

43:34.800 --> 43:39.800
or less so here's the biggest use of AI today filtering out illegal and

43:39.800 --> 43:44.480
dangerous content and this is something that's very hard to do it's impossible to

43:44.480 --> 43:51.440
do perfectly but to tell you to give you an idea of how much progress AI has made

43:51.440 --> 43:55.520
those idea of pre-training transformers and stuff like that the

43:55.520 --> 44:02.920
proportion of hate speech that Facebook was able to take down automatically five

44:02.920 --> 44:09.560
years ago was about 20 to 25 percent okay it was using sort of fairly simple

44:09.560 --> 44:16.000
machine learning techniques NLP methods of the types that were common five years

44:16.000 --> 44:26.520
ago and then self-supervised pre-trained transformers happened and that number

44:26.520 --> 44:33.400
went to 95% last year and it's just progress in AI so a lot of people that

44:33.400 --> 44:37.600
we hear talk about AI who generally don't know much about AI actually tell you

44:37.600 --> 44:41.640
about all the dangers of AI that then you know AI is going to destroy I don't

44:41.640 --> 44:45.400
know democracy because of disinformation and things like that what they

44:45.400 --> 44:48.760
don't understand is that AI is actually the solution to those problems it's not

44:48.760 --> 44:52.360
actually the problem it's the solution to those problems and it's already the

44:52.360 --> 44:57.560
case that doing content moderation on social networks makes massive use of the

44:57.560 --> 45:03.080
latest advancements in AI and the people who try to corrupt that system are not

45:03.240 --> 45:11.200
sophisticated in terms of their AI so something that needs to be known okay

45:11.200 --> 45:15.760
but everybody is excited about generative AI and autoregressive large

45:15.760 --> 45:22.000
language models and things of that type right so many of you certainly I'm sure

45:22.000 --> 45:25.200
have played with those image generation things where you type a text and

45:25.200 --> 45:30.240
outcomes image and this is the state of the art about a year and a half ago from

45:30.280 --> 45:34.920
either a meta and make a scene system or a penny I dali to or Google's image and

45:34.920 --> 45:45.800
as of yesterday this is what you get out of meta so this is actually from a

45:45.800 --> 45:50.440
paper and you can get the paper from archive it's there but there's a product

45:50.440 --> 45:56.600
attached to that paper called emu it's an acronym but actually don't remember

45:56.640 --> 46:05.000
what it means and what the system can do is in it can generate images from a

46:05.000 --> 46:10.840
text prompt and it was rolled out as a product yesterday as well as the paper

46:10.840 --> 46:14.000
right so it's one of the things where like the science the research the

46:14.000 --> 46:20.080
technology and the product come out to the same day pretty crazy and this is

46:20.080 --> 46:23.680
available in Facebook Messenger if you use Facebook Messenger you can you can

46:23.680 --> 46:30.840
ask to talk to meta AI that is the name of the intelligent virtual assistant

46:30.840 --> 46:38.800
at meta the generic ones and then if in a font you type backslash sorry forward

46:38.800 --> 46:45.640
slash imaging and type a text then the system will produce an image in five

46:45.640 --> 46:52.480
seconds this used to take minutes the results are pretty amazing the same team

46:52.520 --> 46:58.120
is is working on synthesizing video this is actually some work from about a year

46:58.120 --> 47:02.840
ago they're making progress on sort of practical things of this type okay but

47:02.840 --> 47:07.680
how do those LLMs those large language models you know that you can talk to how

47:07.680 --> 47:13.840
do they work they are autoregressive right so what that means is they are of

47:13.840 --> 47:17.040
the type that I talked about before you take a text and you remove some other

47:17.040 --> 47:21.960
words and then you turn train assistant to predict the words except it's a

47:21.960 --> 47:26.360
special case where you only train the system to predict the last word okay to

47:26.360 --> 47:31.520
take a long piece of text remove the last word and train this gigantic neural

47:31.520 --> 47:37.680
net to predict that last word and if you train the system this way you can do

47:37.680 --> 47:42.480
what's called autoregressive prediction which means give a text predict the last

47:42.480 --> 47:47.120
word or the next word then inject that into the input and then predict the next

47:47.120 --> 47:51.720
next word and then shift that into an input produce the third word etc.

47:51.880 --> 47:58.320
Autoregressive prediction and it's amazing how it works there's a whole

47:58.320 --> 48:03.840
bunch of those models around actually I typed that list a few a couple months

48:03.840 --> 48:07.960
ago and now there's a whole bunch more but Blunderbot Galactica Lama Lama 2

48:07.960 --> 48:14.280
from from meta which is actually a open source code Lama that came out in July

48:14.280 --> 48:20.320
which is basically Lama specialized for generating code Alpaca Lambda Chinchilla

48:20.360 --> 48:24.760
chai GPT the various incarnations of chai GPT and then there is one that came

48:24.760 --> 48:30.040
out just a few days ago called mistral via a French startup in Paris formed by

48:30.040 --> 48:34.960
people who used to be at fair and deep end actually that's interesting so

48:34.960 --> 48:39.760
performance is amazing for those systems right we've all been surprised by it but

48:39.760 --> 48:42.880
they do make really really stupid mistakes they don't really understand the

48:42.880 --> 48:48.720
world they they're trained to produce the most likely sequence of words that

48:48.760 --> 48:51.760
follow a particular prompt and then they're kind of fine-tuned to sort of

48:51.760 --> 48:55.280
work well for particular types of questions but they make factual errors

48:55.280 --> 49:00.680
logical errors they are inconsistent they don't really have reasoning abilities

49:00.680 --> 49:07.880
it's very easy to kind of chorus them into producing toxic content they really

49:07.880 --> 49:11.040
do have a limited knowledge of the underlying reality because they're

49:11.040 --> 49:15.760
purely trained from text they don't have common sense like a cat can have common

49:15.760 --> 49:21.680
sense and they can't plan their answer so you can you can play with Lama so

49:21.680 --> 49:27.280
basically the chatbot I just mentioned meta AI is sort of a productized

49:27.280 --> 49:32.040
version of Lama too if you want and it has various incarnations actually various

49:32.040 --> 49:36.400
personas that you can call and there's three models the production model is a

49:36.400 --> 49:40.760
different one but it's open source you can download it if you're a big enough GPU

49:40.760 --> 49:45.080
you can read it on your GPU there's a lot of people working towards running those

49:45.080 --> 49:49.400
models on mobile devices and laptops and things like that and they they can

49:49.400 --> 49:53.880
generate text this is a funny one so in the early days of Lama my colleagues

49:53.880 --> 49:59.080
kind of interrogated that so they typed into Lama did you know that Yanlok

49:59.080 --> 50:02.720
dropped a rap album last year we listened to it and here is what we thought

50:02.720 --> 50:10.040
and this and the system writes a critique of my alleged rap album so they

50:10.040 --> 50:14.480
showed this to me and they say is it okay if we put this in the paper and say

50:14.480 --> 50:19.360
yeah sure no problem but I said like could you do this with jazz because you

50:19.360 --> 50:26.520
know I'm like I'm rap is okay but like I prefer jazz really and they told me yeah

50:26.520 --> 50:32.480
yeah we tried and it didn't work because there's not enough training data for

50:32.480 --> 50:43.960
jazz so I cried so as I was saying you can fine-tune the system to sort of play

50:44.000 --> 50:49.960
different roles and what Mita announced yesterday is that is 28 different

50:49.960 --> 50:52.680
chatbots that are specialized for different applications so think for

50:52.680 --> 50:58.120
example you can have Snoop Dogg a rapper be a dungeon master if you are into

50:58.120 --> 51:04.240
dungeon and dragon or text adventure games others that are like advisors for

51:04.240 --> 51:10.560
traveling others that are cooks or or sous chefs or whatever so different

51:10.760 --> 51:17.600
but those things really suck I mean they really not that great because they

51:17.600 --> 51:21.080
don't understand the world they just manipulate language because they

51:21.080 --> 51:24.600
manipulate language fluently we're fooled into thinking that they are

51:24.600 --> 51:29.360
intelligent but they're not intelligent in certain ways but they're not

51:29.360 --> 51:35.920
intelligent in sort of what we think as as human intelligence so you will see if

51:35.920 --> 51:41.040
you go to X from a Twitter or any kind of social networks people who make

51:41.040 --> 51:47.000
posts say oh there is a latest LLM from so-and-so company and you type this and

51:47.000 --> 51:51.280
it's mind-blowing you know we are this far away from human level intelligence

51:51.280 --> 51:56.560
what I call a GI I hate the term and you know it's for tomorrow like you know all

51:56.560 --> 52:00.640
the naysayers are wrong blah blah blah it's just happening tomorrow they are

52:00.640 --> 52:04.880
wrong okay this those things do not have anything close to human intelligence

52:05.160 --> 52:09.040
they appear to do to have to have it because they're trained on so much data

52:09.040 --> 52:13.120
that they've accumulated an enormous amount of background knowledge

52:13.120 --> 52:17.840
approximately that they can regurgitate approximately so whenever they seem

52:17.840 --> 52:22.120
intelligent it's usually because they can do information retrieval in an

52:22.120 --> 52:26.400
approximate way that sort of looks reasonable but they cannot possibly

52:26.400 --> 52:30.240
understand how the world works because their only training data is text and

52:30.240 --> 52:34.400
most of human knowledge this may surprise you but most of human knowledge has

52:34.440 --> 52:39.440
nothing to do with language it has to do with our experience with the world

52:39.440 --> 52:47.960
every day physics another limitation that people have been pointing out

52:47.960 --> 52:52.920
increasingly with various papers is the inability of those LLMs to plan so an

52:52.920 --> 52:58.840
LLM produces those tokens autoregressively as I explained earlier right they

52:58.840 --> 53:02.640
don't plan their answer they just produce one token after the other and

53:03.040 --> 53:06.360
whatever token they produce will determine which token they produce next

53:06.360 --> 53:12.960
because it's autoregressive there is a process by which the system is basically

53:12.960 --> 53:17.320
an exponentially divergent process the system makes one mistake that takes it

53:17.320 --> 53:25.520
out of the kind of correct set of answers it cannot recover and so this

53:25.520 --> 53:30.720
entire architecture of autoregressive prediction in my opinion is is inherently

53:30.720 --> 53:35.280
flawed and my prediction is that within a few years nobody in their right mind

53:35.280 --> 53:40.320
would use autoregressive LLMs okay everybody is working towards something

53:40.320 --> 53:47.480
better because those things are major flaws now what's the issue though is

53:47.480 --> 53:51.960
that there's a lot of people who are scared about future AI systems that may

53:51.960 --> 53:56.680
have the may attain attain human intelligence or or be more intelligent

53:56.680 --> 54:01.720
than humans and if you extrapolate from what LLMs currently do you might think

54:01.720 --> 54:04.840
well it's gonna be very dangerous because those systems cannot really be

54:04.840 --> 54:09.560
controlled they can spew complete nonsense they can be jail broken blah

54:09.560 --> 54:13.600
blah if they are smart they might be dangerous that's a big mistake future

54:13.600 --> 54:17.720
AI systems will not be using this particular blueprint they're not going

54:17.720 --> 54:22.640
to be autoregressive LLMs okay and I'm going to tell you what I think it will

54:22.640 --> 54:33.200
be okay so autoregressive LLMs suck I just said all that no reasoning no

54:33.200 --> 54:39.240
planning essentially right the amount of computation devoted to producing a

54:39.240 --> 54:44.480
single token by an LLM autoregressive LLM is constant there's a constant amount

54:44.480 --> 54:48.800
of computation per token produced so there's no possibility for the system

54:48.840 --> 54:53.840
to for example think about something for a long time before saying something it's

54:53.840 --> 55:03.560
cannot do that by construction so machines do not of this type do not

55:03.560 --> 55:09.200
learn how the world works unlike animal and animals and humans they will not be

55:09.200 --> 55:16.280
able to approach human intelligence okay so whatever I don't know the CEO of some

55:16.320 --> 55:20.480
company that thinks they have the best LLM in the world tells you a GI is just

55:20.480 --> 55:27.080
around the corner don't believe that we're still missing some major advances

55:27.080 --> 55:35.840
but there is absolutely no question that eventually machines will surpass human

55:35.840 --> 55:42.160
intelligence in all domains okay it's basically no doubt about that and it's

55:42.200 --> 55:50.440
going to happen during the lifetime of most people here maybe not me you know I

55:50.440 --> 55:57.240
might take a few decades there's no question it's going to happen so these

55:57.240 --> 56:02.680
are I think the biggest challenges for AI going forward learning representations

56:02.680 --> 56:08.000
and predictive models of the world and I'll tell you why in a minute and that's

56:08.000 --> 56:11.160
what's addressed by self-supervised learning so we have good handle on this

56:11.200 --> 56:17.560
at least for text not so much for video learning to reason so if some of you know

56:17.560 --> 56:21.680
about Daniel Kahneman's theory of system one system two sort of subconscious

56:21.680 --> 56:24.920
things that we do without thinking and then conscious things that we have to

56:24.920 --> 56:31.040
focus our attention on LLMs currently can do system one but not system two we

56:31.040 --> 56:35.640
need to build AI systems that are capable of reasoning of the type that Daniel

56:35.800 --> 56:41.400
Kahneman calls system two is a Nobel Prize winning well he won the Nobel Prize

56:41.400 --> 56:49.680
in economics but is a psychologist and one possible path towards a solution

56:49.680 --> 56:53.320
that I've been proposing for about a year now you're gonna have is what I call

56:53.320 --> 56:57.600
this objective driven AI so this paper I put on open review it's not on archive

56:57.600 --> 57:01.240
it's an open review because on open review you can make comments and and this

57:01.240 --> 57:04.800
is a working document more than a kind of finished paper if you want it's long

57:04.840 --> 57:08.360
though you can also listen to technical talks I've given about this are a

57:08.360 --> 57:13.360
little more technical than the current one and it's based on this idea of a

57:13.360 --> 57:18.640
modular cognitive architecture where you have a system composed of multiple

57:18.640 --> 57:25.000
modules first module be the perception so it's represented overlaid over the

57:25.000 --> 57:28.800
back of the brain because in the human brain perception is in the back so

57:28.800 --> 57:34.280
perception basically perceives the world and then constructs an estimate of the

57:34.280 --> 57:38.360
state of the world right so it produces an estimate of the state of the world

57:38.360 --> 57:43.520
perhaps it needs to combine this with the content of a memory that contains you

57:43.520 --> 57:47.480
know other information about the state of the world that is not currently

57:47.480 --> 57:52.480
perceptible and then that goes into a world model and the role of the world

57:52.480 --> 57:59.880
model is to imagine the outcome of a sequence of actions okay so the system

57:59.920 --> 58:04.160
can imagine a sequence of actions that's the role of the actor the yellow

58:04.160 --> 58:08.960
module so the actor imagines a sequence of actions fits that to the world model

58:08.960 --> 58:12.600
the world model knows the current state of the world and what the world model

58:12.600 --> 58:16.880
predicts is the future state of the world that will result from that sequence

58:16.880 --> 58:22.120
of actions now that cannot be a perfectly exact prediction because the

58:22.120 --> 58:29.200
world is not entirely predictable but that's the the the role of the world

58:29.240 --> 58:33.760
model and then the entire purpose of the system is to figure out a particular

58:33.760 --> 58:38.960
sequence of actions that will predict a state of the world that satisfies a

58:38.960 --> 58:44.120
certain number of constraints that are implemented by the cost module so the

58:44.120 --> 58:49.680
red module that you see this cost module that that's the drive of the system

58:49.680 --> 58:54.440
that's the the current goal of the system if you want and the entire purpose of

58:54.440 --> 58:58.360
the system so imagine this module as getting the predictions from the world

58:58.360 --> 59:04.760
model and then computing a cost for it right so basically it computes the degree

59:04.760 --> 59:12.000
of in comfort of the system discomfort and what the system does is that it

59:12.000 --> 59:15.720
figures out internally a sequence of actions so the actor does that it figures

59:15.720 --> 59:18.720
out a sequence of actions that will minimize its cost according to the

59:18.720 --> 59:26.040
predictions of the world model okay and this is very much system 2 type it's

59:26.040 --> 59:29.960
very similar to what you know people do classically in optimal control it's

59:29.960 --> 59:34.720
called model predictive control and it's really like this right observe the state

59:34.720 --> 59:38.400
of the world get an initial world state representation combine that with what

59:38.400 --> 59:41.760
you think about the state of the world from your memory feed a sequence of

59:41.760 --> 59:44.920
actions to your world model and ask the world model to predict where the final

59:44.920 --> 59:48.560
state will be then feed that to your objectives the objectives might

59:48.560 --> 59:53.720
implement the goal that the system has set for itself or that you set it for

59:53.760 --> 59:58.840
it but also you can have a number of guardrails so might be a guardrails if

59:58.840 --> 01:00:05.920
we have a domestic robot that is cooking has a knife in its hand because it's

01:00:05.920 --> 01:00:11.520
cutting onions or whatever you might have a cost that says if you have a knife

01:00:11.520 --> 01:00:16.840
in your hand and there are people around you don't move your hand too fast okay

01:00:16.840 --> 01:00:21.960
don't flail your arms right so maybe dangerous so you can imagine all kinds

01:00:22.000 --> 01:00:28.080
of guardrails of this type to basically ensure the safety of the of the system and

01:00:28.080 --> 01:00:31.360
the system has no choice but satisfy those because they are satisfied at

01:00:31.360 --> 01:00:37.680
inference time they're not it's not like RLHF for LLMs reinforcement learning

01:00:37.680 --> 01:00:43.320
through human feedback where it's a it's a training time fine-tuning to make

01:00:43.320 --> 01:00:49.440
sure the system produces only safe behavior the system can always produce

01:00:49.600 --> 01:00:54.200
unsafe behavior by you know being prompted something that the the people

01:00:54.200 --> 01:00:57.960
training it didn't think of it didn't think about here that's impossible the

01:00:57.960 --> 01:01:01.080
system cannot produce a sequence of actions that will not satisfy the

01:01:01.080 --> 01:01:06.040
guardrails according to the world model so those systems would be intrinsically

01:01:06.040 --> 01:01:13.160
safe provided two things provided that the guardrail objectives guarantee the

01:01:13.160 --> 01:01:17.720
safety and that's complicated also provided that the world model is accurate

01:01:17.760 --> 01:01:22.280
and that's also complicated so you can imagine something like this that works

01:01:22.280 --> 01:01:27.400
over time so that you know you can have a sequence for example in this case

01:01:27.400 --> 01:01:32.520
sequence of two actions you can have and again this is very similar to what

01:01:32.520 --> 01:01:36.720
control theory is called model predictive control except here we're

01:01:36.720 --> 01:01:42.120
learning the world model and possibly learning the cost as well you might want

01:01:42.120 --> 01:01:45.960
to imagine the system like this that does hierarchical planning humans animals

01:01:46.000 --> 01:01:49.680
do hierarchical planning all the time it's a essential characteristic of what

01:01:49.680 --> 01:01:54.320
we can do and we don't know how to do this at the moment we have some ideas

01:01:54.320 --> 01:01:57.760
working on it but it really doesn't work like if there is like a really good

01:01:57.760 --> 01:02:02.640
opportunity for young scientists or aspiring scientists to really solve a

01:02:02.640 --> 01:02:06.880
problem like try to see if you can do something about hierarchical planning

01:02:06.880 --> 01:02:15.280
because it's really hard but the payoff if you can do it I think is enormous so

01:02:15.280 --> 01:02:21.440
a good example of this is let's say I'm at NYU in my office at NYU and I want to

01:02:21.440 --> 01:02:25.440
go to Paris okay so my objective is my distance to Paris I want to minimize my

01:02:25.440 --> 01:02:30.280
distance to Paris at a high level I can say well first thing I need to do is go

01:02:30.280 --> 01:02:33.960
to the airport and then catch a plane and there is a latent variable that may

01:02:33.960 --> 01:02:37.960
indicate like which airport I'm choosing depending on traffic or whatever or what

01:02:37.960 --> 01:02:44.240
airline flights at what time okay now how do I go to the airport well I have

01:02:44.240 --> 01:02:48.800
to go down in the street and catch a taxi you can do this in New York you can

01:02:48.800 --> 01:02:53.400
just tell the taxi in the street how do I go down in the street I need to stand up

01:02:53.400 --> 01:02:58.200
for my chair open the door go to the stair staircase of the elevator how do I

01:02:58.200 --> 01:03:03.880
get out from my chair I need to kind of push with my arms or something or or

01:03:03.880 --> 01:03:07.640
turn my chair and then you know you imagine you can imagine decomposing this

01:03:07.640 --> 01:03:11.480
all the way down to millisecond by millisecond muscle control I'm not going

01:03:11.480 --> 01:03:17.520
to plan my entire trajectory from my NYU office to Paris in terms of millisecond

01:03:17.520 --> 01:03:21.240
by millisecond muscle control that would be classical planning it has to be

01:03:21.240 --> 01:03:27.440
hierarchical and people can do this today I mean engineers do this in control

01:03:27.440 --> 01:03:33.480
but those various levels in a hierarchy are designed by hand the question is can

01:03:33.480 --> 01:03:36.800
we train a machine to automatically learn what the proper hierarchical

01:03:36.840 --> 01:03:42.320
representation of the action plan is and that's the answer problem yeah you're

01:03:42.320 --> 01:03:49.000
looking to do a phd or something or two or three that's a good problem

01:03:51.360 --> 01:03:56.680
we could use techniques like this for LLMs so LLMs that would be non-auto

01:03:56.680 --> 01:04:00.920
aggressive instead of producing one token after the other they would basically

01:04:00.920 --> 01:04:06.360
infer a sequence of tokens that would satisfy a number of objectives on a

01:04:06.360 --> 01:04:09.560
guardrail an objective that measures to what extent you're answering the

01:04:09.560 --> 01:04:15.600
question and an objective that measures to what extent the answer is non-toxic

01:04:15.600 --> 01:04:23.120
or toxic or whatever right that would make LLMs controllable nothing like

01:04:23.120 --> 01:04:27.520
this works today right again if you are looking for a good topic for a phd that's

01:04:27.520 --> 01:04:35.920
a good one ultimately we need machine to learn to understand the world that's the

01:04:35.960 --> 01:04:40.960
purpose of that world model the essential central piece of that architecture

01:04:40.960 --> 01:04:45.480
I just talked about is this world model given the state of the world at time t

01:04:45.480 --> 01:04:48.200
given an action I might take or a sequence of actions what is going to be

01:04:48.200 --> 01:04:55.360
the state of the world at time t plus one or t plus whatever and humans and

01:04:55.360 --> 01:05:01.000
animals are amazingly good at this babies learn how the world works in the first

01:05:01.080 --> 01:05:06.400
few months of life at an amazing speed and they learn an incredible amount of

01:05:06.400 --> 01:05:09.360
background knowledge about the world first thing you learn is that the world

01:05:09.360 --> 01:05:14.400
is three-dimensional then you learn that something like object permanence the

01:05:14.400 --> 01:05:19.840
fact that when an object is hidden behind another one it still exists okay

01:05:20.200 --> 01:05:22.200
five

01:05:26.200 --> 01:05:32.000
and babies learn things like basic notions like gravity in the around the age of

01:05:32.000 --> 01:05:35.720
nine months takes a long time to learn intuitive physics like like inertia

01:05:35.720 --> 01:05:39.600
gravity things like that okay but it's mostly just by observation a little bit

01:05:39.600 --> 01:05:46.000
by experimentation and we don't know how to reproduce this kind of learning with

01:05:46.040 --> 01:05:51.120
machines and that's why although we have fluid systems that can pass the bar

01:05:51.120 --> 01:06:00.800
exam or medical exams we don't have robots that can clear up the dinner table

01:06:00.800 --> 01:06:05.600
and fill up the dishwasher something that any 10 year old can learn in one

01:06:05.600 --> 01:06:11.040
shot in a few minutes we don't even have completely autonomous level five cell

01:06:11.040 --> 01:06:16.960
driving cars even though any 17 year old can learn to do this within 20 hours

01:06:16.960 --> 01:06:26.120
and then drive at 300 kilometers an hour on the Autobahn you know obviously we're

01:06:26.120 --> 01:06:30.840
missing something really big with machines that humans and animals can can do

01:06:30.840 --> 01:06:35.600
in terms of learning that learning efficiency that we don't we don't know

01:06:35.600 --> 01:06:39.840
how to reproduce so we need this ability to learn world models to get machines to

01:06:39.840 --> 01:06:44.400
learn world models from video essentially from natural signals and so

01:06:44.400 --> 01:06:47.480
this is idea of self-supervised learning but now apply to video not text and it

01:06:47.480 --> 01:06:53.640
turns out text is easy text is easy because text is discrete and finite it's

01:06:53.640 --> 01:06:57.520
only a finite number of possible tokens in every language on the order of 30

01:06:57.520 --> 01:07:02.960
thousand or something and so it's easy to predict a distribution a probability

01:07:02.960 --> 01:07:06.760
distribution over the next token you can represent it by a long list of numbers

01:07:06.760 --> 01:07:11.920
between 0 and 1 that's on to 1 but if you want to predict video you can't do

01:07:11.920 --> 01:07:15.200
that because we don't know how to represent probability distributions over

01:07:15.200 --> 01:07:19.080
all possible videos at least not in a good way so if you train a neural net to

01:07:19.080 --> 01:07:23.520
predict what happens in a very simple video this is over overhead video from a

01:07:23.520 --> 01:07:30.000
highway you get this kind of prediction very very blurry prediction because the

01:07:30.000 --> 01:07:33.240
system can only predict the average of all the possible things that can happen

01:07:33.240 --> 01:07:38.160
and it can't make make up its mind so the solution I'm proposing to this is

01:07:38.160 --> 01:07:46.160
something I call joint embedded joint embedding architecture okay or joint

01:07:46.160 --> 01:07:52.440
embedding predictive architecture JEPA and this is a non generative

01:07:52.440 --> 01:07:55.120
architecture so everybody is talking about generative AI what I'm telling you

01:07:55.120 --> 01:08:03.480
here is abandoned generative models okay so not only am I telling you AI is not

01:08:03.480 --> 01:08:08.960
gonna kill us but LLM suck machine learning sucks and generative models

01:08:08.960 --> 01:08:14.200
suck right all the popular things at the moment okay so a generative model

01:08:14.200 --> 01:08:18.200
predicts you know if you have an observation x you're trying to predict

01:08:18.200 --> 01:08:22.600
y just predict y from x using an encoder and some predictor right but what

01:08:22.600 --> 01:08:27.720
problem with this is that you have to predict every single details of y and in

01:08:27.720 --> 01:08:30.920
video that's just too much in text it's okay it's just like you know what word

01:08:30.920 --> 01:08:34.440
okay you don't know exactly what word but it's okay in video it's just not

01:08:34.440 --> 01:08:38.680
possible so what you should do instead is what's on the right here the joint

01:08:38.680 --> 01:08:43.320
embedding architecture where you run both x and y through encoders the

01:08:43.320 --> 01:08:46.920
encoders eliminate all the irrelevant details about the input and the

01:08:46.920 --> 01:08:51.440
prediction takes place in representation space okay so joint

01:08:51.440 --> 01:08:55.160
embedding predictive architecture JPA there's several incarnations of this

01:08:55.160 --> 01:09:00.600
I'm not gonna go to the details because I don't have time and you can read the

01:09:00.600 --> 01:09:10.880
details in this long paper I can't read what's on it but I can imagine and that's

01:09:10.880 --> 01:09:16.320
kind of the basic JPA architecture so let me skip ahead a little bit there's

01:09:16.400 --> 01:09:23.880
two ways to train those JPAs basically two major techniques to train

01:09:23.880 --> 01:09:26.680
those JPAs that cannot be understood within the context of probabilistic

01:09:26.680 --> 01:09:30.480
methods but only within the context of what I called energy based models and I

01:09:30.480 --> 01:09:34.480
was going to explain what this was but I skipped that section but you don't need

01:09:34.480 --> 01:09:37.760
to know about energy based model to understand what I'm gonna say so there's

01:09:37.760 --> 01:09:42.560
several methods to train those JPAs and this is a particularly interesting one

01:09:42.600 --> 01:09:50.520
this is a paper that was published at CVPR just a few months ago it's called

01:09:50.520 --> 01:09:58.960
image JPA and it's using this masking idea so you take an image you mask

01:09:58.960 --> 01:10:04.320
regions of that image okay and you feed that partially masked image to an

01:10:04.320 --> 01:10:09.000
encoder the encoder produces a representation and with that representation

01:10:09.040 --> 01:10:13.160
you try to predict using another neural net predictor you try to predict the

01:10:13.160 --> 01:10:16.680
representation for use from the full image okay and they both they run

01:10:16.680 --> 01:10:20.520
through essentially identical encoders so not identical one of them uses

01:10:20.520 --> 01:10:25.320
something called exponential moving average weights but but but they're

01:10:25.320 --> 01:10:31.160
almost identical and and that works amazingly well so you you train the

01:10:31.160 --> 01:10:35.840
system this way pre-train it with images that you corrupt by masking them

01:10:35.840 --> 01:10:40.080
partially and you get amazing result on using the features that are produced by

01:10:40.080 --> 01:10:44.040
that system you get amazing results for classification for segmentation for all

01:10:44.040 --> 01:10:49.600
kinds of stuff and the Dino method I told you about before is very similar to

01:10:49.600 --> 01:10:54.440
this it uses kind of a slightly different way of encoding the outputs but

01:10:54.440 --> 01:11:00.800
it's it's in spirit it's very much the same idea and it gives really good

01:11:00.800 --> 01:11:06.880
performance on image recognition on transfer tasks on all kinds of stuff

01:11:06.880 --> 01:11:11.120
that I don't have time to tell you about okay but things we're working on today

01:11:11.120 --> 01:11:14.520
that we need to work on because we don't know how to do it perfectly is self

01:11:14.520 --> 01:11:18.080
supervised running from video so basically a version of this image JEPA

01:11:18.080 --> 01:11:22.520
that would work for video and learn good representations of videos by

01:11:22.520 --> 01:11:26.640
observing the world basically the same thing that babies can do right so we

01:11:26.640 --> 01:11:32.720
have a project along those lines V JEPA and we have a paper that we're just

01:11:32.720 --> 01:11:36.960
submitting to a conference that some of you probably know what it is because the

01:11:36.960 --> 01:11:42.400
deadline is today well actually if you know what it is you're probably not here

01:11:42.400 --> 01:11:48.520
you're working on your paper I think the deadline is passed by two hours so maybe

01:11:48.520 --> 01:11:54.960
maybe you're here so then you would be able to use those JEPA as world models

01:11:54.960 --> 01:12:00.840
right because you know you have an input and you can feed it maybe a set of

01:12:00.840 --> 01:12:05.800
actions that an agent might take and it will predict a representation abstract

01:12:05.800 --> 01:12:09.200
representation of the state of the world at the next time step and so this could

01:12:09.200 --> 01:12:12.080
be used perhaps as a world model as one of the components of the big

01:12:12.080 --> 01:12:21.000
architecture I introduced earlier okay okay I said that already all right so

01:12:21.040 --> 01:12:26.200
there's quite a question that we need to answer with AI and this is my second

01:12:26.200 --> 01:12:31.320
last slide how long is it going to be before we reach human level AI years to

01:12:31.320 --> 01:12:35.920
decades probably decades it's probably harder than we think it's certainly much

01:12:35.920 --> 01:12:42.760
harder than what the most boasting people believe there's many problems to solve

01:12:42.760 --> 01:12:47.240
along the way and before we get to human level AI we're going to get to

01:12:47.240 --> 01:12:50.800
something like cat level AI okay so people who are scared that you know

01:12:50.800 --> 01:12:54.720
one day someone is going to discover the secret of human level AI is going to

01:12:54.720 --> 01:12:58.240
turn on this gigantic computer and that gigantic computer is going to take over

01:12:58.240 --> 01:13:03.640
the world and kill everyone that's just ridiculously stupid just cannot possibly

01:13:03.640 --> 01:13:06.920
happen we're gonna start small we're gonna you know start with something that

01:13:06.920 --> 01:13:10.360
has all the right components but it's small it's not gonna be very smart it's

01:13:10.360 --> 01:13:16.360
gonna be like a rat or a cat right and then we're gonna work our way up and you

01:13:16.360 --> 01:13:20.440
know change the objectives to make sure it's safe and test it in all kinds of

01:13:20.480 --> 01:13:25.640
sandboxes and blah blah blah so this idea somehow that you know the discovery of

01:13:25.640 --> 01:13:30.400
AI is going to be an event and that machines are going to escape for control

01:13:30.400 --> 01:13:37.360
that's Hollywood movies it's not the real world there is no such thing as a

01:13:37.360 --> 01:13:42.840
GI anyway because intelligence is really a multi-dimensional thing humans are

01:13:42.840 --> 01:13:48.760
only good at certain things and terrible at many things in fact our minds are

01:13:48.760 --> 01:13:53.480
extremely specialized we don't realize this but we're incredibly specialized and

01:13:53.480 --> 01:13:58.760
we know this because computers are much better than us at many tasks for example

01:13:58.760 --> 01:14:06.600
chess go poker pretty much every video game I mean not today but eventually

01:14:06.600 --> 01:14:12.760
recognizing a species of a bird by just listening to the song recognizing an

01:14:12.760 --> 01:14:17.560
individual whale or marine mammal by the shape of the tail like AI systems can do

01:14:17.560 --> 01:14:24.080
this a very small number of humans can do all of this I mean we just we totally

01:14:24.080 --> 01:14:28.920
suck at chess as humans machines are much better than we are and so we don't

01:14:28.920 --> 01:14:35.120
have general intelligence ourselves so this word a GI makes no sense human

01:14:35.120 --> 01:14:40.400
level yes a GI no there's no question as I said before that machines will

01:14:40.400 --> 01:14:46.000
eventually surpass human intelligence and so people are scared by this but

01:14:46.040 --> 01:14:53.840
really is a interesting question to ask ourselves imagine a future maybe 20 years

01:14:53.840 --> 01:15:00.720
from now or maybe longer where every single one of our interactions with the

01:15:00.720 --> 01:15:04.440
digital world is mediated by an AI system okay and it might happen faster

01:15:04.440 --> 01:15:09.400
actually okay if some of the startups are being created today and some of the

01:15:09.400 --> 01:15:14.040
big company plans product plans actually fulfilled this may happen fairly

01:15:14.040 --> 01:15:18.880
quickly that essentially every time that we want to connect to the digital

01:15:18.880 --> 01:15:24.720
world that will be through the intermediary of an AI system then those

01:15:24.720 --> 01:15:33.120
systems will become the repository of all human knowledge right and it's very

01:15:33.120 --> 01:15:39.800
important for that at least the base for a foundation of this to be open source

01:15:40.160 --> 01:15:44.400
every infrastructure the internet is open source runs on open source software

01:15:44.400 --> 01:15:49.880
and the reason is because it's too important for one company to control it

01:15:49.880 --> 01:15:56.000
right so it's the same for AI systems they will have to be open source because

01:15:56.000 --> 01:16:00.000
it's too important for any single company or small number of Californian

01:16:00.000 --> 01:16:06.520
company to control AI systems if all of our information of all the citizens are

01:16:06.560 --> 01:16:12.000
basically filtered through those AI systems the way those systems will be

01:16:12.000 --> 01:16:16.240
trained will need to be quite sourced kind of like Wikipedia to collect

01:16:16.240 --> 01:16:20.640
culture and information and knowledge from the entire world not just from the

01:16:20.640 --> 01:16:27.840
view of the world in parallel to us in place right so that's why I'm a huge

01:16:27.840 --> 01:16:34.480
advocate of open source base models for AI and a number of my colleagues at

01:16:35.200 --> 01:16:39.240
Meta and his company policy at Meta to open source those base models because it

01:16:39.240 --> 01:16:45.960
makes them safer more powerful they progress faster they're more culturally

01:16:45.960 --> 01:16:50.200
diverse if more people can train them and it creates an entire ecosystem of

01:16:50.200 --> 01:16:55.800
startups and research projects that can build on top of it so it's a very

01:16:55.800 --> 01:17:00.280
important political questions at the moment because a lot of companies are

01:17:00.280 --> 01:17:04.240
pressuring governments around the world including the German government to

01:17:04.240 --> 01:17:09.400
basically keep AI under lock and key to say AI is too dangerous it needs to be

01:17:09.400 --> 01:17:19.120
controlled and licensed and and not put into the hands of everyone I think it's

01:17:19.120 --> 01:17:22.280
the exact opposite I think it's too dangerous to actually keep in the hands

01:17:22.280 --> 01:17:27.720
of just a few a few people okay so I became a little philosophical political

01:17:27.720 --> 01:17:34.040
here those people have convinced the UK government the Prime Minister that

01:17:34.040 --> 01:17:40.160
AI should be regulated under lock and key apparently the EU Commission also is

01:17:40.160 --> 01:17:49.800
convinced this is very bad and I think if we do it right AI will make everybody

01:17:49.800 --> 01:17:54.120
smarter it's like we all have those intelligent assistant with us all the

01:17:54.120 --> 01:17:57.800
time it's like having a staff of intelligent people working for you okay

01:17:57.800 --> 01:18:03.120
every person who is leading anything including me only works with people who

01:18:03.160 --> 01:18:06.880
are smarter than them right I only hire people who are smarter than me because

01:18:06.880 --> 01:18:10.080
that's the way to be successful so that everybody is gonna be like that we'll

01:18:10.080 --> 01:18:13.960
have AI assistant that are smarter than us we shouldn't feel threatened by them

01:18:13.960 --> 01:18:17.680
because we'll be controlling them they will be designed to be subservient to us

01:18:17.680 --> 01:18:23.560
so this may have an effect on society similar to what the printing press had

01:18:23.560 --> 01:18:30.480
probably 500 years ago not too far from here of basically causing a new

01:18:30.520 --> 01:18:34.240
renaissance because intelligence is really the commodity that we lack the

01:18:34.240 --> 01:18:40.480
most this will make humanity smarter thank you very much

01:19:00.480 --> 01:19:11.360
yeah thank you so very much Jan for an amazing lecture we have about 10 minutes

01:19:11.360 --> 01:19:22.200
for questions and I'm sure there are several so much for the great talk

01:19:22.200 --> 01:19:26.360
customer from a minute you alluded to the fact that we should keep code open

01:19:26.360 --> 01:19:30.320
which is great however as you know right many of the recent developments not

01:19:30.320 --> 01:19:33.360
just rely on the code but also on the hardware so many of the things are

01:19:33.360 --> 01:19:37.960
developed at companies because they have access to a large GPU resources now not

01:19:37.960 --> 01:19:41.800
only Germany I guess we are limited by that so and what's your take on that

01:19:41.800 --> 01:19:46.600
also being in an academic and an meta environment right how do you deal

01:19:46.600 --> 01:19:49.920
yourself with it do you do some things only at universities and others only at

01:19:49.920 --> 01:19:56.320
meta or how I mean how do you see that in the future okay should have used an

01:19:56.320 --> 01:19:59.920
automatic speech recognizer because there is an awful echo and it's very hard to

01:19:59.920 --> 01:20:07.680
understand it's not your fault but anyway I mean hardware is a big limitation so

01:20:07.680 --> 01:20:11.880
currently the only entities they can train large language models that are

01:20:11.880 --> 01:20:17.480
good are people who have access to large amounts of computation either in-house

01:20:17.480 --> 01:20:24.240
which is the case for Google and meta and Microsoft or through cloud services

01:20:24.240 --> 01:20:28.000
which is the case for open AI and entropy can others they have access to

01:20:28.000 --> 01:20:32.240
Microsoft Azure and you know some of them use AWS some of them use other

01:20:32.240 --> 01:20:37.640
other tools so but that costs a huge amount of money so training a sort of

01:20:37.640 --> 01:20:41.880
top-of-the-line language model today you know costs tens of millions of euros

01:20:41.880 --> 01:20:50.000
right it depends how many tens depends on how you do it possibly more if you

01:20:50.000 --> 01:20:55.840
want to buy an infrastructure that is sufficient power today you have to buy

01:20:55.840 --> 01:21:04.720
basically stuff from Nvidia and is going to cost you a number with with 10

01:21:04.720 --> 01:21:12.040
digits it's in the billions it's insane so what that tells you is that it's like

01:21:12.040 --> 01:21:17.440
it's like an autobahn you don't want 10 parallel autobahn going from one city to

01:21:17.440 --> 01:21:23.040
another city you just want one and that has to be sort of accessible to everyone

01:21:23.040 --> 01:21:27.840
so that's the idea behind open source base model foundation model they need to

01:21:27.840 --> 01:21:31.320
be open source because it's a common infrastructure they can be customized

01:21:31.320 --> 01:21:36.440
and there's no point having 50 of them because they cost so much to train that's

01:21:36.440 --> 01:21:39.760
another argument for open source

01:21:53.080 --> 01:21:59.520
hello yes hi thank you for the great lecture there was a slide that had

01:21:59.520 --> 01:22:04.120
challenges of AI and machine learning and there were three points in there it

01:22:04.120 --> 01:22:11.600
went by really fast and I couldn't catch if ethical fairness responsible AI was a

01:22:11.600 --> 01:22:17.720
challenge that you're facing with and if so what are you doing about it okay so I

01:22:17.760 --> 01:22:23.520
think that point was wrapped into the the second point but sort of not

01:22:23.520 --> 01:22:28.360
mentioned directly it was more can I mention in the other thing so this idea

01:22:28.360 --> 01:22:32.800
of objective driven AI the fact that a system can only produce answers that

01:22:32.800 --> 01:22:37.480
satisfy a number of objectives including some guardrails the answer to your

01:22:37.480 --> 01:22:40.960
question is how do you design those guardrails essentially we don't have an

01:22:40.960 --> 01:22:45.480
answer to this and the reason we don't have an answer to this is because we

01:22:45.520 --> 01:22:51.920
haven't really began to build systems of this type and so it's as if someone in

01:22:51.920 --> 01:22:58.560
1925 asked aviators how are you going to make sure that transatlantic

01:22:58.560 --> 01:23:03.760
transatlantic flight at near the speed of sound will be safe nobody could be

01:23:03.760 --> 01:23:09.280
possibly answering this nobody across the Atlantic on a plane in 1925 at least

01:23:09.280 --> 01:23:16.120
not in there not without any stuff nobody knew what a turbojet was like the

01:23:16.120 --> 01:23:19.200
idea of you know speaking at near the speed of sound was completely

01:23:19.200 --> 01:23:24.000
unthinkable so we're a bit in the same situation we don't know how exactly how

01:23:24.000 --> 01:23:28.880
to make those things safe because we haven't built them yet but I think it's

01:23:28.880 --> 01:23:32.720
an engineering problem like any other problem and there is a there's a fallacy

01:23:32.720 --> 01:23:37.240
also which is that to design those objectives a lot of people say oh we've

01:23:37.280 --> 01:23:40.760
never done this before so we're not going to know how to do it but in fact we are

01:23:40.760 --> 01:23:45.360
doing this all the time we've been doing it for millenia designing objectives so

01:23:45.360 --> 01:23:52.000
that intelligent entities behave properly that's called laws lawmaking is

01:23:52.000 --> 01:23:56.920
designing objectives for humans to behave properly and it's even designing

01:23:56.920 --> 01:24:02.080
objectives for superhuman entities to behave properly superhuman entities

01:24:02.080 --> 01:24:05.800
like corporations for example right corporate law basically is a way to make

01:24:05.800 --> 01:24:10.600
sure that whatever a corporation does is aligned more or less with the common

01:24:10.600 --> 01:24:14.040
good of society right of course you know they can be corruption and everything

01:24:14.040 --> 01:24:18.040
but that's the that's a big idea so we're very familiar with this with this

01:24:18.040 --> 01:24:22.640
this concept it's not it's not new

01:24:29.040 --> 01:24:33.640
and thank you Jan for the really great talk I want to follow up on the question

01:24:33.640 --> 01:24:39.440
we had earlier about GPU resources what I see is that in machine learning and AI

01:24:39.440 --> 01:24:44.480
the biggest breakthroughs in the last years were achieved with huge amount of

01:24:44.480 --> 01:24:52.040
GPU resources the amount that academic institutions typically do not have do

01:24:52.040 --> 01:24:57.560
you see a future for academic research in the field of AI so I'm gonna can make

01:24:57.600 --> 01:25:04.600
myself I have two hats okay let me tell you something many of the best ideas come

01:25:04.600 --> 01:25:10.120
from academia okay the whole idea of generating images from text and things

01:25:10.120 --> 01:25:15.260
like this those actually came out of a German university right and then you

01:25:15.260 --> 01:25:20.320
know people picked it up and made products out of it but originally this

01:25:20.320 --> 01:25:24.520
was done not too far from here in the university the whole idea of using

01:25:24.560 --> 01:25:29.280
attention mechanisms which is the basis of transformers they came out of

01:25:29.280 --> 01:25:35.940
university Montreal so that was an interesting story they this was Dimitri

01:25:35.940 --> 01:25:41.280
Bada now Kim Jong-chul who is now a colleague at NYU and Joshua Benjo and

01:25:41.280 --> 01:25:44.880
they came up with this idea that when you build a translation system the system

01:25:44.880 --> 01:25:50.600
should be able to decide which word to look at to translate you know English

01:25:50.600 --> 01:25:54.500
into let's say German in fact German was the main issue because you know the

01:25:54.500 --> 01:25:59.780
verb is at the end so it screws up all the translation system so so that was

01:25:59.780 --> 01:26:05.180
actually the solution to that problem and and they came up with this idea of

01:26:05.180 --> 01:26:11.260
this kind of learn attention mechanism and then there was a paper that that by

01:26:11.260 --> 01:26:15.060
Chris Manning at Stanford that sort of picked up this architecture and made it

01:26:15.060 --> 01:26:20.460
work at scale so they won the WNT competition a few months later and then

01:26:20.460 --> 01:26:23.780
the entire industry jumped on it right and then you know as much people at

01:26:23.780 --> 01:26:26.980
Google said oh you can build an entire neural net based on just this idea and

01:26:26.980 --> 01:26:30.460
the title of the paper was attention is all you need and that was the

01:26:30.460 --> 01:26:36.260
transformer and you know so the root of some of those good ideas are very often

01:26:36.260 --> 01:26:39.500
in academia then the problems that I talked about you know how you do

01:26:39.500 --> 01:26:42.420
hierarchical planning how you're done world models from video that kind of

01:26:42.420 --> 01:26:45.980
stuff these are things you don't need enormous amounts of computation to

01:26:45.980 --> 01:26:50.940
demonstrate the principle you may not be able to you know beat some benchmark

01:26:50.940 --> 01:26:55.260
results or whatever but it doesn't matter if you show that a principle can

01:26:55.260 --> 01:26:59.060
work and it's convincing enough then there'll be other people who pick it up

01:26:59.060 --> 01:27:02.380
and actually build something real out of it it's okay that's the way you have

01:27:02.380 --> 01:27:13.740
intellectual impact so if you think about your career and what drove you would

01:27:13.740 --> 01:27:18.260
you say that more the dreams you had about what could be possible or you're

01:27:18.300 --> 01:27:25.180
so interested in the topic let yeah just all the work you contributed to and how

01:27:25.180 --> 01:27:31.500
that maybe change also over time yeah so interesting question I think at the

01:27:31.500 --> 01:27:36.820
root of it is really a scientific question what is intelligence how does

01:27:36.820 --> 01:27:42.500
the brain work you know it's a very sort of front and center big big scientific

01:27:42.500 --> 01:27:47.540
question over time so right there's three big scientific questions is what you

01:27:47.540 --> 01:27:50.900
know what's the universe made of what's life all about and how does the brain

01:27:50.900 --> 01:27:58.100
work right three questions but then I'm kind of an engineer as well so for a

01:27:58.100 --> 01:28:01.620
complex system like the brain the only way to really understand how it works is

01:28:01.620 --> 01:28:05.740
that you build one yourself and you verify that like all the hypothesis that

01:28:05.740 --> 01:28:09.660
you built into your system actually kind of correspond to what happened and it's

01:28:09.660 --> 01:28:14.740
really the inspiration behind convolutional nets and multilayer learning

01:28:14.980 --> 01:28:19.540
and the whole idea of neural nets in the first place right getting inspiration

01:28:19.540 --> 01:28:22.940
from the brain but not copying it because you copied blindly you're not

01:28:22.940 --> 01:28:26.540
gonna get anywhere you need to understand the underlying principles so

01:28:26.540 --> 01:28:29.940
underlying the understanding the underlying principles of intelligence is

01:28:29.940 --> 01:28:34.060
really what kind of drives me and then it's great if you have like multiple

01:28:34.060 --> 01:28:39.420
applications whether they are useful or entertaining I mostly don't do this

01:28:39.420 --> 01:28:44.660
myself but but I'm really happy with people do it

01:28:47.340 --> 01:28:54.020
hello LeCun I want to ask you a question what's your opinion on the field of

01:28:54.020 --> 01:28:58.780
embodied AI and robot learning I think it's very interesting because it's

01:28:58.780 --> 01:29:04.100
deployed artificial intelligence techniques to change the real world yes

01:29:04.380 --> 01:29:12.860
I completely agree so in fact in fact that's kind of one of the point that I

01:29:12.860 --> 01:29:18.620
perhaps didn't make clear enough that this idea of world model as I said is

01:29:18.620 --> 01:29:22.100
easy to do in the context of language which is why we have language models

01:29:22.100 --> 01:29:28.300
that are so impressive but it's very hard to do in the context of the real

01:29:28.300 --> 01:29:32.740
world data video things like that property of sensitive data from a robot

01:29:33.020 --> 01:29:38.700
and so the good news about the good thing the good aspect of embodied AI of

01:29:38.700 --> 01:29:42.420
like working with with robots whether they are real or simulated is that you

01:29:42.420 --> 01:29:48.780
can't cheat you can't take shortcuts like representing everything as a word or

01:29:48.780 --> 01:29:55.660
something although some people are trying to do that but so I think

01:29:56.020 --> 01:30:03.820
focusing on this kind of type of problem I think makes people honest so I

01:30:03.820 --> 01:30:09.100
think the most interesting advances in in AI over the last several years are

01:30:09.100 --> 01:30:13.980
not in LLMs they are in people who do robotics and try to do control and sort

01:30:13.980 --> 01:30:19.860
of make robots basically learn efficiently without having to be trained

01:30:19.860 --> 01:30:25.740
by you know for hours in simulation this teams there's a colleague at NYU

01:30:25.740 --> 01:30:30.660
Lera Alpinto who's working on this there is I mean I've grouped a colleague

01:30:30.660 --> 01:30:35.140
Emelon and his colleagues and then probably the biggest group working on

01:30:35.140 --> 01:30:40.100
this is at Berkeley Peter Abiel, Segelle Yvine and Chelsea Finn who is a former

01:30:40.100 --> 01:30:43.980
student of theirs at Stanford those are really kind of interesting approaches

01:30:43.980 --> 01:30:51.180
there this whole idea of planning objective-driven kind of planning you

01:30:51.180 --> 01:30:55.820
have to do that in the context of robots so in that sense is very interesting

01:30:55.820 --> 01:30:59.820
there's a whole division at fair that actually is called embodied AI for that

01:30:59.820 --> 01:31:08.740
reason thank you yeah thank you so much Jan I mean this amazing lecture and I

01:31:08.740 --> 01:31:12.060
think you're all very grateful that you shared your thoughts and perspectives on

01:31:12.060 --> 01:31:16.380
future AI with us and I think we all got a lot of impulses from this today so we

01:31:16.380 --> 01:31:19.780
have a small gift for you as well

01:31:43.060 --> 01:31:47.060
thank you

01:31:50.060 --> 01:31:55.580
yeah so let me also again I mean thank all cooperation partners who contributed

01:31:55.580 --> 01:32:01.380
to this event so the Center for Advanced Studies biosphere the Varian Academy of

01:32:01.380 --> 01:32:06.260
Sciences Humanities Munich Center for Machine Learning the Varian Research

01:32:06.260 --> 01:32:10.780
Institute for Digital Transformation and the Konrad Susi School of Excellence in

01:32:10.780 --> 01:32:16.100
Reliable AI and sorry I would like to have a special thanks to Dr. Ursula

01:32:16.100 --> 01:32:20.420
Olinger who is science manager at my chair and who headed actually the

01:32:20.420 --> 01:32:26.820
organization of the entire event so I think she deserves a small applause

01:32:34.540 --> 01:32:38.980
yeah thanks also everyone for coming here and also for those who joined us

01:32:38.980 --> 01:32:43.860
via live stream we now have we I would now like to invite you to a little

01:32:43.860 --> 01:32:49.580
reception in this Sitzung Sal 1 and 2 which is here right around the corner

01:32:49.580 --> 01:32:53.580
so thank you so much

01:33:08.980 --> 01:33:11.040
you


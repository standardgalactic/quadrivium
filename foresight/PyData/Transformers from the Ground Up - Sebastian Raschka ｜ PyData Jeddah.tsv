start	end	text
0	8000	Welcome and good evening everyone. My name is Atman, one of the volunteers of the BiData Jiddah chapter.
8000	16000	I'm here with my team members Yasmin and Hassan Shadad and Abdelillah.
16000	22000	Today's meetup is titled Transformers from the Ground Up by Dr. Sebastian Rashka.
22000	28000	Dr. Sebastian is an assistant professor of statistics at the University of Wisconsin Medicine,
28000	32000	focusing on deep learning and machine learning research.
32000	39000	He's also a contributor to open source software and the author of the best-selling book, Python Machine Learning.
39000	47000	Thank you Dr. Sebastian for joining us and also we want to thank our sponsors, Namfocus and Muzun.
47000	57000	If you want to visit our BiData Jiddah website or follow the Twitter account to be updated about our new upcoming meetups.
57000	63000	And we also post a lot of cool things on there.
63000	69000	And I guess that's all from me. I'll stop sharing so Dr. Sebastian can start. Thank you.
69000	75000	Yes, thank you so much for the kind introduction. I really appreciate it. It's my pleasure to be here. Thanks for the invitation.
75000	81000	So I will start then by sharing my screen and if there's some issue, please let me know.
81000	88000	I can then try to fix that so it should be visible now. Can you see everything?
88000	90000	Yes, it's clear. Yeah.
90000	101000	Oh, perfect. Great. Okay. Yeah, then yeah, I will get started. So today, the talk is about transformers, the natural language processing models in deep learning.
101000	107000	And I'm trying to cover transformers from the ground up, like explaining a little bit how they work.
107000	113000	And then also, I hope we have time for that at the end, showing you an implementation in PyTorch.
113000	122000	So yeah, why this topic? So yeah, I personally got very interested in this topic recently and I was also working on the new edition for my Python machine learning book.
122000	130000	It was just putting together the chapter and had this very long lecture at my end of the semester last, last semester, when I was teaching the deep learning class.
130000	141000	And this is essentially more like a condensed version of that highlighting the main concepts by skipping over the mathematical details, which I think would take too much time to digest.
141000	154000	It's not like a week long effort to dig through all the details, but I hope this will give you like a good idea of how transformers work, and also helping you like navigating the jungle of all the terminology a little bit because there are so many different models
154000	163000	out there like BERT, GPT version one, GPT version two, and yeah, you might wonder how they are different and what you can do with transformers.
163000	176000	Yeah, just to start a few examples where transformers are used nowadays. So one popular example would be language translation. That is also how the original transformer architecture was developed.
176000	181000	So language translation was the main motivation behind developing the original architecture.
181000	191000	And yeah, nowadays, transformers are pretty much everywhere when it comes to natural language processing. And even beyond that, there are many real-world applications of transformers.
191000	208000	For example, recently I also saw this article. So I'm also working in computational biology problems. Recently there was this article on using transformers to kind of elucidate or decipher properties and proteins like organizing proteins by
208000	222000	structure properties, but also by function. And they trained, for instance, transformers on a large scale database of amino acid sequences like protein sequences, and they found all kinds of interesting insights from that.
222000	235000	And yeah, you probably have seen recently the GitHub co-pilot project. I think this was in collaboration with OpenAI and I think also Microsoft, where they trained a transformer to generate code.
235000	247000	What's interesting is that you can essentially enter a query, a comment, a code comment, and this AI system, the model will generate a corresponding code here at the bottom.
247000	259000	So if you go to this website, they have like a nice animation of how that works, and they have a Microsoft Visual Studio Code plug-in where you can download this and actually use that in action, which is really impressive.
259000	275000	So I think right now they have JavaScript support and Python support, so that would be another interesting application of transformers. So transformers mainly are about generating things like generating translations or generating code, generating text,
275000	288000	for instance, also in chatbots, but you can also use them for classification, like classifying, let's say, whether a movie review is positive or negative, and this will be a code example we will be covering at the end of this lecture or talk.
289000	307000	So the topics I have in mind for today are those. So first I wanted to, from the historic perspective, just briefly explain how attention came to be. So there was essentially this attention mechanism that was first added to recurrent neural networks, and then there was this
307000	318000	self-attention mechanism, which is a, I would say, a more sophisticated version of that, there's this so-called scared dog product. Attention, oops, I almost ripped off my microphone here.
318000	336000	Yeah, so and then we will talk about the original transformer architecture, which is the first architecture that was just using attention. So in step one here, this is a recurrent neural network or commented with attention, and the transformer is then an architecture is just using attention
336000	347000	without any recurrent neural network layers. And then I will briefly dive into some of these large scale language models, for example, your GPT and bird, which are pretty popular these days.
347000	360000	And then after that we will pre-train or sorry, we will fine tune a pre-trained bird model in PyTorch. So one thing about these large scale language models is that they are really large scale and will take forever to train them.
360000	375000	In practice, usually they are trained on large data sets, thousands or millions of websites and books and so forth. And so usually people download a pre-trained version, and then fine tune it to a target task.
375000	389000	So we'll also take a look at how it looks like. And then I have a few closing thoughts or words on transformers. And yeah, with that, I think we have plenty of things to talk about in the next, I would say, 40 minutes.
389000	405000	Let's get started with the attention mechanism that was also first small disclaimer here. So like I mentioned in the beginning. So this will be more like a conceptual overview. So if there are some, let's say, notational details in these figures, don't worry about them too much.
405000	416000	To be honest, I think it's most important to kind of understand the big picture and all the mathematical details. That is something that are almost like implementation details. Some of them are almost arbitrary.
416000	430000	So I wouldn't worry about it too much. But I hope this talk kind of piques your interest in terms of transformers. So if you're interested in that you can dive in deeper into the topic afterwards with having this big picture overview in mind.
431000	442000	So starting with the first topic augmenting the recurrent networks with attention. So first, what is the motivation behind using attention why don't we just use recurrent neural networks.
442000	452000	So, if you think of a task sequence to sequence task, which is a task where you have an input sequence and you want to generate an output sequence.
452000	465000	So one example of that would be a language translation, where you have an input sequence, let's say this one here in blue, in one language, let's say, German, and you want to translate this into English.
465000	479000	So here on the output, then you have an output sequence. And what happens is if you use a, or if you design an arcade architecture for that, you usually use this encoder decoder setup in RNNs.
479000	494000	So let's say that the RNN first ingests the whole input sequence. So you have input element one, two and three so these are for instance, individual works, and they are then processed into these hidden states.
494000	511000	So once you have processed the whole input in this encoder so this would be essentially the encoder part, you have all the information in one, one hidden state here, and then you have the decoder part, which yeah generates the output.
511000	524000	So here, why don't you, yeah, why don't you translate this one by one because here one problem would be that this hidden state has to remember all the input in one hidden state and this might be a little bit overwhelming.
524000	540000	Right, so if you have one hidden state that has to capture the whole input, there might be some information loss. And this is essentially one of the challenges of these RNNs they work kind of well if you have short sentences but the longer your sentence becomes the more challenging
540000	550000	the network to generate a good output because it kind of forgets what happened let's say here in the very beginning if you have a very, very long input sequence.
550000	557000	So and also, yeah, why don't we just translate sentence word by word so here's just one example.
557000	572000	Yeah, because I only speak two languages, German and a little bit of English, I have a German to English translation example here. So it's just an attempt on translating this German sentence here at the top into an English sentence.
572000	587000	So what happens if you translate this word by word, so it doesn't really work. So you have a sentence. I'm an English as a second language speaker so I'm usually also not that good at that but I can tell that this one is definitely wrong.
587000	598000	So you can see here, the sentences, can you help me, or can you me help this sentence to translate it doesn't really make sense if you just let's say take a dictionary and translate word by word.
598000	616000	So to have a correct, I hope this is correct if you have a correct English translation, the sentence would be can you help me to translate the sentence, and you can see, there is some, yeah, some longer range dependencies there are some words here for instance those that you can just
616000	625000	translate word by word, but then you have, let's say these two here, where they are kind of flipped so in order to translate this word, the help.
625000	632000	You have to look into the future basically so you have to find an element that is further away in the input sequence.
632000	642000	For example, if you have this word here, it goes here, you can see there's a more longer range dependency and the longer your sentences are, the longer these dependencies become.
642000	653000	So in that way, we can't really have this word by word translation attempt because it would not result in any meaningful grammatically wrong translation.
653000	667000	So going back, this is why we first read the whole input, because yeah we need to have knowledge about the whole input before we can attempt the translation but then again the challenge is okay, we may forget earlier hidden states.
667000	679000	So one idea to deal with this problem that we can capture these long range dependencies is this attention mechanism so this was first proposed in the context of RNNs in 2014.
679000	688000	So I also, I will share my slides so you if you are interested you can look up these papers and read a little bit more details about that.
688000	699000	So this is an attention mechanism that the researchers used to augment the RNN to better process the input. So here's just like a sketch in the lower left corner.
699000	709000	For instance, if we are thinking of the second step here, the hidden state might have access or at this step in general you might have access to the whole sequence.
709000	717000	You can also align with to denote that one input sequence might be more important than another and this is where the attention comes in.
717000	731000	So one step will have access to the whole input, but with a waiting term that says okay maybe at this position this term is more important than let's say this term and so forth so not every word is equally important.
731000	738000	So there's also some some sort of waiting and the architecture looks approximately like that.
738000	753000	So there's a sketch I made after the paper. So it's just a little bit different notation but essentially what we have is we have an input sequence. So this is just individual words, one to two words, and then they have two RNNs kind of connected here.
753000	763000	So there's one RNN. It's a bi-directional RNN. Bi-directional RNN is just a fancy word for an RNN that processes the sequence from left to right and from right to left.
763000	771000	It's just so that it captures, you have information from both ends. It's just increasing the amount of information you have essentially.
771000	787000	So the forward direction would be the regular RNN direction and then you essentially just flip the sequence and process it backwards. So this is what we denoted here so you can denote it from right to left but you can also think of it as just reversing the sentence and processing it in the regular way.
787000	800000	Then you connect these hidden states and then you calculate the so-called attention weights. I will skip over the details of how these attention weights are computed here but I will show you how they are computed in the transformer.
800000	813000	It's a little bit different but not too different. So but the main part here is really that using these attention weights, we obtain the so-called context vector at each step.
813000	828000	So the context vector here is essentially a weighted version of all the inputs. So you can consider all the hidden states here and it's essentially a weighted sum. You have this attention weight, you multiply it by this state and then you just sum them up.
828000	834000	And the larger the attention weight, the more the hidden state will contribute to this context vector.
834000	845000	So this context vector is essentially then kind of capturing all the input information in a certain processed way and this goes into each decoder part.
845000	854000	So if I go back a few slides, we had this decoder part here where we were generating some output and this is what we are having here at the top of the O's.
854000	869000	The O's, they should just denote, let's say, the translated sentence. And each output step, what we have is we have the hidden state that goes into, let's say, the that position, the previous word, because it's also important.
869000	880000	I mean, if you generate a sentence, you kind of look at the previous word you have just written, right? So what you currently translate the current word is also depending on the previous word you have written.
880000	893000	The sentence might not make sense. So you provide the previous word, you provide the hidden state, and you provide this context vector and with that you can capture all the input when you attempt the translation of the next word.
893000	904000	So this is essentially an enhanced version of recurrent neural network for translation, where this party is essentially new, it's this attention part.
904000	916000	Yeah, and from this attention part, people then develop this self attention mechanism. So the self attention mechanism is one major building book of this original transformer architecture that we will cover.
916000	921000	And it's kind of inspired by this RNN that I just showed you.
921000	933000	This is a very simple version of self attention. It's not the version that is actually used in the transformer that comes in the future slide. This is a simplified version just to understand the main concept.
933000	940000	So what you can see here is, so to cover it step by step, there are three steps.
940000	959000	So this one is computing these dot products. So imagine, we are currently processing a given word, Xi, and we compute the similarity of this word. This is how we get a certain score for computing the attention weights, we compute the similarity of this to all the other words, including
960000	970000	and why are so what we have here is a sequence of words one to T and why are the four blocks there so the four blocks, they just know that this is a vector.
970000	983000	So it's for example a word embedding so usually when we do natural language processing, we don't really feed it in the string of the word so we usually have a method that converts the string let's say to a vector of real numbers.
983000	993000	So this should just denote a word embedding here. And then there's this, you know, this dog product between two word embeddings to vectors that we multiply.
993000	1004000	And let's say this gives us the attention weight. So it's just a simplified version. Usually we would some use something like a softmax function which is a function that would normalize them so that they sum up to one.
1004000	1020000	So that we then again compute the weighted sum of these. So we have these attention weights, and then we have our vectors. So vector one to T, and we just wait them by the corresponding attention weights.
1020000	1031000	Also, we have on a special a specific attention weight for each query. So if, let's say, Xi here's my query.
1031000	1044000	These attention weights will be different, compared to if x two is my query, because it depends on the on the dot products right. So these would change each word has different embedding so every
1044000	1052000	query has its unique attention weights and from that we derive this context vector which is kind of similar what I showed you in the RNN example.
1052000	1062000	So this is a simple form of self attention, where we just introduce some, you know, waiting of all the inputs when we compute some output for current word.
1062000	1075000	So one problem with that though is, so the main takeaway here is that we compute these attention weights but one problem is that we don't have any mechanism for training a neural network now right because there is no, no weight parameter involved.
1075000	1083000	So how do we improve this how do we use that say back propagation or gradient descent with that to learn the optimal attention weights for example.
1083000	1097000	So for that, we in practice actually use a different type of attention it's called on scaled dot product attention it's one form of self attention with learnable weights so this involves weight parameters.
1097000	1110000	And yeah there are then three matrices, and with these matrices, you compute also again like a vector a query a key and a value using the input query.
1110000	1121000	So even now these matrices are weight parameters that you can update during back propagation or gradient descent to optimize the model.
1121000	1129000	So here, what's going on here is we have a current input query that we are currently processing let's say the second word here again.
1129000	1137000	And from the second word we apply these three matrices to compute three values or three vectors so a query a key and a value.
1138000	1153000	Using those so so what we do then is we use the query first so this this query to compute these omega values these are kind of like unscaled attention rates with each key.
1153000	1157000	So the key is computed for each input word again.
1157000	1175000	And we compute the dot product between query and key, but notice here we always use the query to which is our current word that we are processing so we again obtain these unique values or unique attention weights for each query that we are currently processing.
1175000	1183000	These are some normalization steps so this is on scale dot product attention. It's a softmax with a scaling factor essentially.
1183000	1198000	And then again, like before we are summing them up to this context vector zine. So this is essentially the same that I showed you here except that it now involves here these trainable matrices and this is, yeah this is the basic on building block behind these transformer
1198000	1213000	networks. So, um, yeah it's maybe a little bit fast but to be honest is really not that much behind the self attention it looks maybe a little bit complicated so maybe you need to look at this a little bit longer it took me definitely some time on to understand
1213000	1223000	what's going on there, to be honest, a good way for me learning about that was really like reading the paper and then doing these or making these figures and drawing that to kind of make sense of it.
1223000	1234000	So yeah once once you take a look at this it's actually not as complicated as it looks like looks like. I think the complicated part here is more like that these language models are so large.
1234000	1244000	And if you look at the code itself it's kind of like overwhelming because there's so many things going on there in the code not not in the building block here.
1244000	1251000	So now to get to the original transformer architecture which is built on this self attention mechanism.
1251000	1262000	So, this is based on this paper attention is all you need that started all the trends in terms of developing transformers it's a very influential paper that came out in 2017.
1262000	1269000	It's not too long ago four years ago and since then they have been hundreds if not thousands of different transformer architectures.
1270000	1285000	The main part is essentially that this is centered around this self attention mechanism, and it does not use any RNN parts so there's no recurrent neural network layer nothing it's just using the self attention concepts and stacking them.
1285000	1292000	And then also having some fully connected layers like in a multi layer perception just a very basic neural network layers.
1292000	1309000	So this is a sketch of how the transformer architecture looks like. So yeah, like I mentioned, it looks very complicated at first glance there are a lot of boxes and lots of things connected to each other, but essentially it's not too bad it's relatively straightforward it's
1309000	1326000	actually, again, our encoder and decoder like in the RNN that I showed you earlier where the job of the encoder is to, yeah, ingest this on input sequence, and the decoder job is to generate the output sequence.
1326000	1336000	Yeah, what you do first is you have a word embedding for example to embed the words into real valued vectors. There's something like a positional encoding.
1336000	1346000	Because the problem is, there's its permutation invariant here but we will skip over these details. It's just like a way of encoding the position of the word.
1347000	1358000	Then we have this so called multi head attention. It's kind of like, I would say an extension of the self attention mechanism I will have a slide on that to briefly explain what that is.
1358000	1362000	Then there is a layer normalization.
1362000	1372000	Yeah, like a multi layer perceptron a fully connected network. And then again a layer normalization and so forth. So it's just repeating some of these units and in between.
1372000	1379000	We also have something like these arrows here connecting things. And these are skip connections.
1379000	1389000	It's essentially just this is what the ad stands for it's taking this value and adds it back here it's similar to what a residual network does if you afford a residual network.
1389000	1407000	So the means of preventing, let's say a bad gradients. So if you have something like a vanishing gradient problem here, then this one will be added to this one it's it's kind of like skipping layers if the layers are bad, then there's an opportunity to skip them
1407000	1409000	via these connections so to prevent.
1410000	1421000	Bad, bad layers for from ruining the whole network. And then yeah this encoder just captures essentially the input in this self attention weighted way.
1421000	1431000	It's like an encoding of the input. And this feeds into the decoder which has itself also multi head attention there's a masked version which I will also talk a lot of bit about.
1431000	1445000	Yeah, and this is essentially it so it goes again through a fully connected network and then there's a fully connected layer to produce the outputs so the fully connected layer is then producing word probabilities with a softmax function like how likely is this word at a given
1445000	1457000	position and then in practice we usually use the highest probability if we, let's say do a language translation or if we generate text in general, we want to have some randomness, we want our model to generate different texts.
1457000	1466000	So just on sample from this distribution. But yeah and in the original transformer architecture what they had in mind was language translation.
1466000	1482000	Yeah, to dive a little bit more into this multi head attention, it sounds like it's a whole new concept, but it's essentially just our self attention mechanism so here this is the scaled dot product attention that we talked about earlier with these different matrices.
1482000	1493000	And multi head attention is just repeating this multiple times it's basically a stack of these things. It's kind of like similar when you.
1493000	1507000	If you have worked with convolution networks and you have an input, and you apply multiple kernels to produce multiple feature maps as the output. It's kind of like the same concept that each channel on captures different type of information here.
1507000	1515000	Anyway, this is a similar concept where you just repeat the self attention multiple times and stack it up to each other.
1515000	1525000	And this helps you're extracting different features that are useful. And yeah one thing about that is on what's also nice is that here.
1525000	1540000	So stacking or what the advantages is we can parallelize that because there's no dependency between these individual scale product attention so you can compute this in parallel helps us also to leverage multiple GPS for example.
1540000	1556000	So we have now these so called hats attention hats, and each attention had has essentially one set of these matrices so if we have each on attention hats we have each sets of these matrices, and then we just repeat the same thing multiple times.
1556000	1571000	And also I should say here. This is six times. So these blocks are also repeated six times. I think this is what makes the architecture so big and complicated when you look at the code if you look at the, let's say from scratch implementation is just there are so many layers and there's
1571000	1587000	not so much on code that implements all of that, but the underlying concepts, if you draw them out there not itself themselves that are not that complicated it's just your matrix multiplication, and then softmax and matrix modification.
1587000	1604000	Let's talk a bit about this mask maybe here. So the mask multi hat attention is masking out words that we haven't generated yet. So we have a target sequence that is our, let's say desired output that we want to generate let's say if we want to translate
1604000	1614000	it, but at a given time step we don't want to have something that is in the future. So if we generate a sentence let's say on right.
1614000	1617000	Let's say have a sentence help.
1617000	1621000	We translate this.
1621000	1634000	First, we would have then let's say a German version of that but let's say this is our targets version. So in the first step, we would only have access to help, and then this one would be masked out.
1634000	1650000	And then as a second step, we would have access to these two words and these would be masked out and the mask multi hat attention is essentially ensuring that future words that are not generated yet are masked out.
1650000	1654000	So actually a slide about that. Yeah and this masking out.
1654000	1669000	This can also be considered as a unidirectional form of language modeling, because we are masking out everything that is in the future from the current position. So we are masking out from left to right so it's kind of unidirectional.
1669000	1687000	And we will also talk about a model that has so called bidirectional approach so the Burke model has a bidirectional approach and we will revisit this topic it's kind of in it sounds actually fancier than it is so we will see in the future what this bidirectional
1687000	1689000	means also.
1689000	1698000	Okay, so yeah, this was already on all the transformer general original transformer stuff I wanted to talk about.
1698000	1712000	And these were all the original ideas like from a historic perspective so nowadays, people don't use this original transformer architecture anymore but there are many, many other architectures that have been inspired by this architecture.
1712000	1721000	So here in the section I want to briefly highlight some of them. So one is this GPT model developed mainly by the open AI team.
1721000	1732000	And this takes this unidirectional approach which is kind of similar to the decoder of the original transformer. And this is used for tasks that involve generating texts.
1733000	1752000	And then there is the Burke model and of course there are yet other different types of versions of it for example, Roberta and Alberta and so forth. But in general the Burke model is bidirectional approach, and this is more suited or better suited for classification
1752000	1763000	for example, in the code example that I prepared where we predict or whether the movie review is positive or negative. So whether someone liked the movie or not.
1763000	1774000	Yeah, but one, one scheme that is common amongst most transformers is that there are two steps. One is a pre training step, a pre training step on very large unable data sets.
1774000	1788000	And then there's a fine tuning step where we take a pre trained model, and then we train it on our target data set. So here in step one, this is usually a general data set that is not really related to our target task.
1788000	1803000	It's for instance just a library of books or websites, but we don't have any labels and yeah this is just used to pre train the model they are very large models so they need a lot of data to be kind of pre trained.
1803000	1812000	And for this training, there are these two steps that I mentioned the pre training and the fine tuning one and two.
1812000	1824000	And the fine tuning can also be split into two different approaches, and both are very common. I think fine tuning is a little bit more common performance better but it's also more expensive.
1824000	1840000	One is this feature based approach and one is this fine tuning based approach. So how do they differ. So assume you have step one where you pre train your transformer. So this is sometimes called in the original literature it's called unsupervised pre training, but the more
1840000	1848000	modern term for that is self supervised learning. So the same thing essentially, where you train a model on unlabeled data.
1848000	1857000	And then, so the data is unlabeled but it's still a supervised learning approach. This is why it's called super as a self supervised learning.
1857000	1872000	So you generate essentially the labels on yourself. So you take let's say a book, and then you train the model to predict you remove some words and then you train the model to predict missing words, or you predict the next word in the sentence and the next word in the
1872000	1883000	sentence would be your label, but it's not like a label for classification it's just like a label that you can generate yourself from the text you don't need to have a human labeling these texts.
1883000	1896000	Yeah, so and then let's assume we have now our pre train transformer in this red block. So then, once you have a pre train on a large data set you never have to change it again in terms of
1896000	1907000	you basically save that somehow on your computer, and then you can use it for all your downstream projects that is what I mean. And one way is to never really change it you take it.
1907000	1917000	And then you just extract the last layer so you give it a new sentence, and then you extract the output layers, one or more output layers you can concatenate them.
1917000	1930000	There's output layers to a classifier, for example, logistic regression multi layer perceptron maybe a random forest, whatever you desire, and you only train this classifier. So, you essentially treat this pre train transformer as a feature
1930000	1935000	extractor. If you have used on something like principle component analysis.
1935000	1944000	This is also one method that say it's a linear transformation of your input. It's one method to, let's say, extract some features from your original features so it's in that way.
1944000	1953000	So pre train transformer acts like a feature extractor where you just use it to extract these last layers, and then you just train a classifier on top of that.
1953000	1964000	Another approach is this fine tuning approach it's a little bit more expensive. This is, you know, for what I prepared this code example. So this fine tuning approach is a little bit different in that you update the whole model.
1964000	1976000	And then you start with your pre train transformer. Let's say you saved it somewhere on your hard drive, you make a copy of that. And then you feed it the label training data set that you have.
1976000	1991000	And you add a few layers for classification. Let's say if you have if we have our movie review data set we add one final note in the output layer, have a softmax activation, and then we just train the whole model with back propagation.
1992000	2005000	And because we have to load all the models, the whole model in memory and do the back propagation it's a little bit more expensive than than this step where we only use the transformer for generating the features.
2005000	2016000	So that way, it depends. It's of course also slower. So it depends on what type of architecture you have access to to use this approach but in practice it performs a little bit better.
2016000	2023000	I forgot which paper it was either GPT one or bird. They also directly compared those approaches.
2023000	2038000	I think my GPT one but I would have to double check. And they found I think if you lose use the last three layers here, you can get approximately the same performance as with fine tuning so in that way it's, it's really, you can get the same performance but it's really a trade off also
2038000	2043000	of how much computational resources you have.
2043000	2059000	Yeah, so talking a little bit about the GPT models. So there are three GPT models now. So one was released in 2018 one in 2019 one in 2020 and yeah, if the sequence continues we maybe have a new version by the end of this year.
2059000	2073000	And yeah, one obvious thing you can see from these GPT models which stands for generative pretrained transformer is that they increased a lot in size so they started with 110 million parameters which was already really huge.
2073000	2082000	Then the next one had 1.5. And now we have 107 175 billion. So who knows maybe the next one is in the trillion range.
2082000	2099000	So just like I said, I will share my slides later so you can also read more about that in the original manuscripts, but yet just to give you just brief summary of how these models work, I have a few slides on those.
2099000	2110000	So GPT one, that's the first model in the series. It's essentially very similar to the transformer we talked about, especially I mean, it's just taking the decoder part.
2110000	2128000	And it's trained on a sentence in this next word prediction manner. So the pre training is taking a large context or a large corpus of books or websites I think, I think they use the big a mix of words from websites and books.
2128000	2143000	There are instances where they remove through this masking, the future words in the model's task is to predict the next word, and they just train the model in this manner. So they train it just for this prediction of the next word in the sentence.
2143000	2152000	So by doing that, you can then fine tune it so you add. So for having this task classifier, you can add additional layers.
2152000	2162000	After you pre trained the model, and depending on what you're interested in this you can do our classification entailment similarity multiple choice questions and so forth.
2162000	2167000	And how that works is essentially they have this pre trained transformer here.
2167000	2175000	You connect the linear layers and then it's like a regular classifier. And yeah, one thing here is.
2175000	2186000	I won't go into too much detail but it's how you format the input. If you have a classification, you have like a start token a text, the sentences and then some extraction token.
2186000	2205000	If you have something like similarity here, text similarity, they have on both texts and then in flipped order for example so it just the first a little bit about how you set up the inputs, but yeah the main, the main thing is that you can just adopt this pre trained network to do whatever
2205000	2221000	you like in the fine tuning. So this is really expensive. This is like a really expensive step that something you don't want to do yourself you want to maybe download this GPT and then you can on your target data set to the fine tuning.
2221000	2236000	And then really what was really novel about GPT two and GPT three is that they actually removed this fine tuning most language transformers they use the fine tuning stuff that I talked about what GPT two and GPT three are doing though is they tried something new.
2236000	2246000	They were so ambitious that they kind of removed this fine tuning, and they only have a model now where you provide what you want to do as input.
2246000	2260000	So for example, if you want to use their model the GPT two model to generate text, what you do is you give it the input that is formatted as follows where you say to the model translate to French.
2260000	2269000	And then you insert your English text, and then you insert your French text and it will automatically figure out what you want to do basically.
2269000	2286000	There are different questions like you give it a sentence and you can ask something like this is object. So you can say for example, this is a very nice sky and is the sky blue and it will what can generate like output that says yes or no or something like that.
2286000	2296000	So it's kind of very flexible in terms of what it can do by only having a certain formatted input without any fine tuning.
2296000	2298000	So in version three.
2298000	2313000	I think they kind of went back a little bit because I think. So here, this is called in this context on zero shot learning where they don't have any examples of the task the example is in the scriptures in the task itself they don't have any training examples in that sense.
2313000	2323000	The task was maybe a little bit too ambitious. So they went back with GPT three made it a little bit bigger, and they switched also to few short learning in this context where in the future learning.
2323000	2341000	They have at least a few examples of that task. So just to illustrate how that looks like so here is this zero shot where you have again the prompt or task description sorry, and then a prompt, and then it would insert here the French word for cheese.
2341000	2348000	I honestly don't know any French so I can't tell you what it is, but yeah it would generate this output here.
2348000	2358000	And for the one shot. It's a little bit more, I would say, easier for the model because it sees at least an example. It's not fine to it's just seeing the example as part of the input.
2358000	2365000	So you have again the task description here at the top. And then you have one example of what you want to do.
2365000	2376000	And then few short learning so if you have let's say here on the three shot case where you have one example second example the third example, it becomes easier for the model.
2376000	2379000	If you showed at least a few examples.
2379000	2381000	See that's just a note in the chat.
2381000	2390000	I will maybe answer the questions after the talk and think almost over time with your slides left.
2390000	2394000	Yeah, so here.
2394000	2408000	Well, where was I so you're here on the limitation of the future tasks though is that you have a limited size input. I honestly don't remember what the sizes for GPT three I think they have some IP eyes that I've never used.
2408000	2418000	But let's say for bird models there's a token input limitation of 512 tokens. It's like characters or work and characters but like words or punctuation.
2418000	2426000	So that way, you're kind of limited by what the model can process as input and how many examples you provide.
2426000	2438000	So this is actually a fine but I can imagine it's maybe a challenge if you have longer sentences here so that wouldn't probably work with very long sentences because then you fill up all the input with examples.
2438000	2451000	So this is in contrast to the fine tuning approach with which most other transformers use where you have an example as part of your training set that then you updated with back propagation like the gradient update showed another example and so forth.
2451000	2465000	So this is more like the traditional way of doing it. And yeah the GPT three models very ambitious and I think I'm not sure if actually the paper is out yet but yeah this is one of the latest state of the art models for generating texts.
2465000	2471000	And another approach is the bird model, which is bidirectional encoder.
2471000	2486000	So you can think of GPT more as the decoder which generates some output whereas bird is more like the encoder of the transformer which ingests like the input, and then creates a representation that you can then use to train a class to fire on.
2486000	2492000	So GPT is better for generating texts bird is better for classification.
2492000	2501000	So how that works is they have a pre training step or they have actually two different types of pre training tasks. One is on this masked language model.
2501000	2518000	So what they do is they mask 15% of the words. I like to call it marked, not masked yet, because what they do is they take these 15 words or 15% of the words if you have a sentence, and then they do different things to it so here's an example.
2518000	2536000	So if you have a sentence here and input sentence, you would randomly pick 15% of the words for example that you would pick, let's say Fox, and then based on these 15% what you do is 80% of the time, you replace this token, the fox with a mask.
2536000	2544000	And 10% of the time, you replace it with a random word, for example, coffee, and 10% of the time you keep it unchanged.
2544000	2556000	And so this is how you pre train the model you have all these sentences and 15% of the words are changed in a certain way and the model has to predict what the correct word is at a certain position.
2556000	2564000	For example, it has to fill in the right word if there's a mask, or it has to detect whether this word is right or wrong.
2564000	2581000	And then, sometimes it's unchanged so why would you have this unchanged. So this is kind of important for the model in order to perform well during inference when you train the model and you want to use it in the real world, the real world, you usually don't have masked sentences.
2581000	2598000	But also then basically learns that sometimes it should not do anything so in order to work well or some real text which doesn't have masks, otherwise it would always expect okay there are some random words or masked words so in this way they found that this performs better if they have also 10% of the
2598000	2601000	or marked words unchanged.
2601000	2618000	And the second pre training tasks that they do at the same time is next sentence prediction so they have a sentence a and a sentence be separated by a token, and the model has to predict whether sentence be indeed follows sentence a.
2618000	2628000	So if you have a text and you reverse the order of the sentences the model learns the correct order and this helps the model yet to work with inputs that are more than one sentence.
2628000	2635000	So it's essentially classification a binary classification is next or is not next essentially.
2635000	2650000	The classification token is used as a placeholder for generating the output, because the number of inputs also matches the number of outputs, but in bird we are not interested in generating new text, we are essentially interested in classification.
2650000	2656000	So how that works is just a brief overview of the different types of tasks you can do with bird.
2656000	2660000	There is a sentence pair classification.
2660000	2678000	One is just a single sentence classification question answering and single sentence tagging so here tagging is, for example, if you think about something like language, a grammar software you can label a sentence by on what type of like a word or
2679000	2688000	word or grammar each word corresponds to, but let's focus maybe only here on the, because we have for the interest of time on the single sentence classification task.
2688000	2704000	So you provided an input sentence, you have some embedding here, and then you provide some output. So you have during training you have all these tokens so it has to predict what the input is, given that something is masked or not.
2704000	2710000	If you have a prediction task you don't I mean it still generates a sentence but you don't do anything with it.
2710000	2717000	What you care about is this class table here so this is why you have this classification token. This goes then into the class table that you care about.
2717000	2729000	And this is something we can leverage for arbitrary classification. So I prepared a code example for fine tuning such a bird model for this single sentence classification in pytorch.
2729000	2734000	And it says single sentence classification and for the code example.
2734000	2746000	I have this large movie review data set. I mean, back then it was large but it's just a 50,000 movie reviews, and I think 50 25,000 in the training and 25,000 in test set.
2746000	2763000	And this is essentially movies movie reviews from the IMDB movie review database, and these are multiple sentences. So we can also use bird for multiple sentences, even though let's say it says single sentence classification, but just concatenating these
2763000	2777000	sentences. So in that way, the only difference is we don't use this separator token because the separator token is really more for comparing two things like this sentence pair classification so this one can for example say whether these sentences are similar or not like
2777000	2782000	some sentence similarity and things like that.
2782000	2799000	Yeah, so I'm not sure how much time we have I could maybe briefly went way over. I can maybe briefly show you the code example. It's actually not too complicated. I have it on GitHub. I have some annotation.
2799000	2812000	It's based on a library called hugging face. It's a very, very, very popular library for or you probably can't see it now have to reshare my screen. Okay.
2812000	2819000	Let me see how many slides I have left. So we probably don't have to cover this.
2819000	2826000	Let me share my browser screen so you can briefly see the code example.
2826000	2830000	Here I opened it from GitHub.
2830000	2838000	I've also my Jupyter left version which is maybe better because I can zoom in here.
2839000	2857000	So here is a notebook for downloading a pre trained bird model and fine tuning it on the movie review classification. There's a lot of boilerplate code here like importing the libraries, some settings for reproducible reproducibility, downloading the data set.
2858000	2878000	And then processing it. So here's a pen us data frame of how the data set looks like there's like a text a movie review, and then it's whether it's positive or negative, like a binary classification, 50,000 combined, and here I'm just spitting them into training and validation and test sets.
2878000	2898000	So here's a tokenization step. So tokenization goes from taking the words and then converting them also, sorry, taking the sentence and chopping it into words, and then also taking care of punctuation so punctuation is also, I think, given token, but it really depends on what type of tokenizer you use.
2898000	2910000	So there's a collection of different tokenizers out there they all behave a little bit differently, but I recommend if you use, let's say, on hugging face. I recommend on finding a tokenizer that matches your model.
2910000	2916000	I should say we are using here, not birth, but the silver I think I had a note about that at the top.
2916000	2933000	This is essentially a smaller version of work because if you only have let's say one GPU like 1080 TI or 2080 TI like a graphics card with only 11 gigabyte memory, you or I couldn't at least log the whole work model into memory.
2933000	2937000	I think I could just barely load it but they're not training.
2937000	2957000	So there's a version called the silver which is a little bit smaller so in the silver what they did is they trained the bird model, and then they removed some weights, but preserving its performance it has 95% of the performance of work, but it's more lightweight it has 40% fewer parameters.
2957000	2969000	So, we are using a tokenizer that is made for this, the still bird model, and then encode the data set into tokens.
2969000	2983000	And then I have my data loader here in pytorch. So this is a custom data loader here there's nothing really special about it. If you would build the data loader for any type of image data set for example, except that we on the way we process essentially on encodings.
2983000	2995000	So that's like a special on representation that is, I think it's kind of like a dictionary it's our own Python. So yeah a Python object or class, but it kind of behaves like a dictionary you can index into that.
2995000	3001000	And I will show you how that looks like. So yeah here's an example you can have a.
3001000	3012000	So this is this sorry this will create a dictionary right, but I think itself it's also some certain object that contains multiple things.
3013000	3026000	Yeah, so here I'm just creating data loaders from the data set. So this is basically all just set up. And here's the main part where we are loading the still bird model, and they have multiple models on their website if you go to the hugging face
3026000	3037000	transformer website. And here this would be for example a distilled bird model for sentence or sequence classification you can basically tell based on the class name, what this is made for.
3037000	3048000	And then I'm loading it the pre trained version and they have also again, I think they have different ones uncased you would mean it would ignore sentence case whether it's upper or lower case.
3048000	3056000	Yeah, and then I'm putting the model on my device, putting it into training mode and then initializing an optimizer for back propagation.
3056000	3059000	This is just a warning it's.
3059000	3063000	I don't exactly know what they want here.
3063000	3078000	I think it's just some hints. It's nothing really we have to be concerned about. And here I implemented the accuracy function to add so what I'm showing you is like a manual training how basically how you would implement this and let's say your
3078000	3086000	own pytorch code. And then at the end I will also show you trainer class I mean we are almost done it's just the last little bit here.
3086000	3099000	So what I have here in this accuracy function how I write my accuracy function is usually that I law the data in batches because I usually have large data sets and you can just put the whole data set into the model as input because you would have
3099000	3110000	a large matrix multiplication and then you run out of GPU memory. So what I usually do is I bet like similar to training I batch up my data set into chunks into these batches.
3110000	3121000	It's automatically done by the data loader here, and then I compute the predicted labels and collect them. So I'm just collecting the number of correct predictions.
3121000	3133000	So I have summed up all the correct predictions. I divide by the number of training examples. So I keep track of the number of examples, and use that to divide the number of correct predictions and that gives me the accuracy.
3133000	3144000	So it's in that way, not requiring to load all the data all at once. And yeah there are some interesting things here going on so we have the input IDs of the works.
3145000	3156000	The tension mask. So the tension mask is sort of weird here why do we need a tension mask. So it's essentially for padding so if we have sentences of different lengths.
3156000	3166000	It will use zeros to do a padding so it's essentially denoting which character is a padding character, and which is an actual word.
3166000	3176000	So we have our class labels, and we provide as a model input, the input IDs, the words. So the model itself will then do the word embedding, and then the attention mask.
3176000	3184000	And then we obtain from the outputs the output is kind of I think it's an object but you can index into it might be a dictionary.
3184000	3192000	So you get the logits, which you can then use. You could use a softmax function to get probabilities but it's not necessary, because the largest largest.
3192000	3198000	And also the largest probability in softmax. So you get the class table with arc max.
3198000	3208000	It's your class table and then you know I was just checking whether the predicted label is matching the correct label, the actual label.
3208000	3212000	Okay, um, this is just how I compute the accuracy.
3212000	3223000	This is my training group. So I usually have a very simple training group because when I do research I like to tinker with things so I like to have like a more manual approach.
3223000	3243000	And this works similarly I obtained my input IDs from the data loader, if my attention mask labels, compute the outputs here in addition to, to the logits I also need the loss for optimization so you have to provide also to the class labels as the model input.
3243000	3252000	So you get both the logits and the loss, and then like in regular pytorch I use backward on the loss and optimize and this is just for logging purposes.
3252000	3264000	So when I do that on my data set it trained for like an hour, and get gets like a 91% accuracy. One interesting thing is about that briefly I tried an RNN on that and I think I got like 88% accuracy.
3264000	3283000	I was using this pre trained model, even though it's a small data set the MDB review data set even though it's small, I can use a pre trained transformer to fine tune it on the small data set to get both formats that is actually better than let's say logistic regression which was like 85% and an RNN which was like 88%.
3283000	3293000	And I didn't do any tuning here by the way I just used it one time because I was short on time I didn't do any learning rate tuning nothing so it worked kind of like out of the box.
3293000	3309000	And then there's also, if you really want to use transformer seriously I recommend using on the APIs that hugging face on provides so they have a trainer class, where you specify options in the so called trainer arguments.
3309000	3324000	And then you have this trainer class where you provide this as input and it handles everything automatically. So I also have a run here, and actually it got on even better performance I think they have some certain default parameters for regularization and so forth.
3324000	3333000	So just first of all faster, and also the performance was a better I think they have better better on default options because my training was very minimal.
3333000	3338000	So they have some additional options. And actually, I, for comparison.
3338000	3352000	There's a setting on, I only used one GPU, when I was disabling that one of my computers where I ran this had a for GPS was running in 16 minutes. So, in practice you may be also want to disable that and you get also way faster training.
3352000	3354000	Okay, so just briefly.
3354000	3359000	Sorry, Sebastian, we have those minutes to finish please.
3359000	3371000	Yeah, let's. Sorry, I was going way over time. So yeah, maybe let's go to the yeah we will take some questions. Yes, okay, can start asking the questions for you.
3371000	3377000	So your last thing is, I will just go to my slides for the questions also.
3377000	3380000	Okay, that's not the right one.
3380000	3396000	But I am ready for questions now. So I wanted to say I have also way more detailed transformer lecture on YouTube from my deep learning class and there will be a version of the Python machine learning book coming out for pytorch, which also has a transformer
3396000	3403000	chapter that will be later this year. So if you want to read more details about this also this might be a useful resource.
3403000	3413000	So I'm also I wanted to mention this the figures here. They are from that chapter so I want to give also credit to Jitian Joe who helped me a lot with the figures and yeah writing this chapter.
3413000	3417000	Okay, sorry. So I think we can take questions now.
3417000	3419000	No worries hasn't you can start.
3419000	3421000	Yeah, thank you so much.
3421000	3424000	Thank you so much for your time and for this wonderful talk.
3424000	3431000	Now it's your time but if you have any questions, and you would like, you'd like to ask it to the first question.
3431000	3441000	Feel free to write it in the chat, or even you can raise your hand and we'll unmute you so you can ask your question.
3441000	3446000	So let's start with our first question for today.
3446000	3456000	This is from Sunny. She's asking, can you be to do classification.
3456000	3473000	I think so yeah the definitely the first version I think the other versions can do classification to on the hugging way web, sorry hugging face website they have, I think also an implementation of GPT a re implementation, where you can try this out.
3473000	3479000	I remember I have demonstrated this action spring semester in the class.
3479000	3489000	So they definitely have capabilities to do classification and GPT two and three, you have to provide it by other context, but in GPT version one.
3489000	3500000	They also provide classification as an example in the paper I'm not sure if I had a figure on this here, but yeah you can, but people just because the way it's trained in this unit directional way.
3500000	3510000	Usually, people agree that a bird type of models would be better for classification, although you can use GPT two for classification.
3510000	3513000	Another question from Marietta Barata.
3513000	3523000	Is there any use cases right now with transformers based vision models out, out before various models like those based on this net.
3523000	3530000	And should I use transformers architecture based models now to classify images and to base with net.
3530000	3542000	Yeah, that's a good question so I, um, yeah, I have seen a lot of papers recently using transformers for computer vision. There's the classic vision transformer and they're also newer models.
3542000	3551000	And I think they are depends on which paper you read, they are set up in the art on compared to computer vision models.
3551000	3563000	I think the efficient net version three paper that came out in April on show that they perform better than computer than transformers but this was in April so things might be different now.
3563000	3573000	But one downside of the of transformers and computer vision is that they require more data for pre training. If you can get the pre trained models it might be for the fine tuning part it might be worthwhile.
3573000	3580000	Otherwise, otherwise, I think it's more like a research topic now that people are exploring. In practice, it's probably very expensive to train them.
3580000	3582000	I think there's also this paper.
3582000	3593000	I'm not sure if they compared directly. So efficient at version three was better than the vision transformer and there's the halo net paper that came out in June, it's like the rest of 50 like architecture with attention.
3593000	3604000	It's not a vision transform but it has like resonate like backbone with attention. And I think this one ought to form the vision at version three on image net I think they got like 85 point something performance.
3604000	3609000	So in that way, I think, yeah, it's right now really where transformers are.
3609000	3618000	It's not a state of the art, but I think it's still worthwhile using CNN for classification for the time being, because they're also cheaper to train.
3618000	3624000	And then of course we also have architectures like MLP mixer which is neither of them it's just multi layer perceptrons.
3624000	3638000	So yeah, it's an interesting time I interesting question I, I would say personally I stick with CNN because they are easier to train, but in the future who knows, I think maybe computer vision transformers will take over completely.
3638000	3639000	Great.
3639000	3648000	I read about is also asking is transform architecture practical for real world tasks outside of his academia and big tech companies.
3648000	3650000	And what are the next steps.
3650000	3651000	Oh, sorry.
3651000	3663000	What are the next steps to make transform architecture based models more accessible with transformers to be able to adapt or something more automatic will replace it.
3663000	3672000	Yeah, good question I think that is one of the big frustrations that these transform models are so big that as private personal and academia it's really hard to train them.
3672000	3686000	I think on some labs in academia have flat resources for example this this paper here on I think the preprint was I saw the preprint I think from this for this paper like in 2019.
3686000	3695000	Even two years ago they had the resources for training but it must have been really expensive I think they're like 50 GPUs or even 50 GPU TPU pots.
3695000	3701000	I think they collaborated with big tech companies but yeah, how do you as a normal researcher use these I think.
3701000	3713000	Yeah, I think it's really infeasible to use these are train pre train these models as a private person I mean you can probably manage but you probably need a whole team of engineers to even set up all the infrastructure
3713000	3728000	for this work. And in that way I find it more interesting to really fine tune these models like training or using a pre trained model, and then just focusing on fine tuning and like you've seen in this code example I showed you it's like in another one hour you can find
3728000	3747000	but yeah there are also approaches where people have made transformers way more efficient. I haven't covered them here because of your time constraints but as for example, the nice trim former or my head the sparse transformer, which I think they have usually you have quadratic complexity
3747000	3752000	they have linear complexity scaling in terms of the input sequence size.
3752000	3759000	They have made efforts to make transformers more efficient. Well, if I also had a slide even on that here.
3759000	3776000	There are efforts. So this is unfortunately kept at or clipped 2019 here, but you can see there's this trend that they become bigger and bigger and bigger, but there are also some efforts, for example, the distill bird that I showed you and other methods that try to reduce the number
3776000	3787000	of operators and allow people to use transformers even though they may not have access to large computer infrastructure. But yeah it's like a concern, many people are concerned about.
3787000	3796000	There was also this paper calculating the cost. If you have a 1.5 billion parameter model it costs like 80 to 1000 to 1.6 million just to train this model.
3796000	3805000	So yeah, it's, we will see where things go but yeah this is definitely a concern that this is really unfeasible for many people.
3806000	3815000	We have another question from Jason. Can GPT to GPT to be fine tuned on some more datasets like few hundreds.
3815000	3830000	Yeah, I think so. Personally, I have only used GPT to via the hugging face website where they had like derivative of GPT to I don't, I forgot the name I think it's called GPT meal.
3830000	3841000	So this GPT meal is like an effort by the open source community to train a GPT to model because I don't think, I think maybe has changed but I don't think they.
3841000	3855000	I have shared the full model I think they only provide access through an API. So that way if you only have access to an API I don't know if you can find unit, but there's this GPT meal project.
3856000	3869000	I would have to search the website I don't know my head by a thing it's like this, where they have reengineered either GPT version two or three and I think this might be something that you could download and fine tune.
3869000	3878000	And I think hugging face also has maybe some models I would have to double check. I haven't done this myself, but I think it should be possible.
3878000	3889000	So for GPT two and three it's kind of not necessary because you have like these contexts so they say with the context it's sufficient but you could maybe fine tune it I, I don't know for sure.
3889000	3892000	It's a good question.
3892000	3894000	Great.
3894000	3903000	I'm wondering if you can use between for specific domains, like, for example, medicine.
3903000	3910000	Yeah, definitely. So this is, yeah, yeah, so you can definitely adopt the architecture for specific domains.
3910000	3922000	Anyway, if you have a small medical image data set like texts you could technically fine tune this, but if you have a large data set you can also maybe try to train it from scratch, like the researchers did here.
3922000	3926000	So here instead of training it on sentences they use the BERT model.
3926000	3929000	I think this group, there were two groups that did something similar.
3929000	3936000	They used the BERT model and trained it on amino acid sequences where they had millions of sequences from the protein structure database.
3936000	3946000	They were like, I mean you can think of an amino acid sequence as a sentence, but each character there are 20 different amino acids, each character would represent a word essentially because it's on.
3946000	3956000	It's more like a character level type thing, but they did that and it worked well so I can imagine you can also train it on other types of sequences and in the medical domain depends on
3956000	3968000	that you have a sequence of specific domain specific encoding that say you have a certain device that outputs certain values that don't save a meaning to a human, but that could be passed by a machine.
3968000	3977000	I can also think of a transformer maybe being trained on that or if you have just texts like annotations of patient records I think that could also be done.
3977000	3984000	So yeah, you can definitely train this on other things other than general text.
3985000	4003000	Altie is also asking, when we fine-tune the BERT train, will that update the vocab of any initial vocab will be the same after fine-tuning?
4004000	4016000	I think a good question so when you fine-tune it, so you have the text embedding, I think it would update also the embedding step.
4017000	4032000	This is something I can't say 100% sure because when you use this code from HuggingFace, the code from HuggingFace includes the encoding as far as I know because you only provide the input IDs of the words and then coding is part of the model.
4032000	4039000	And if you update this, I think you should also, it should also update the embeddings.
4039000	4055000	I'm like 90% sure but not 100% because I haven't really looked at the code line by line, maybe they have a line that freezes the weights for the embeddings so that I don't know for sure but I suspect the embeddings are also updated.
4056000	4069000	Sorry, I think the question might have been about the vocabulary size. So I think the vocabulary size is fixed so I don't think this can be updated because the vocabulary size depends on the input ID on the tokenizer.
4070000	4089000	I think the tokenizer would remove words from your input text that is not included in the pre-training. So if you have some arbitrary words that are not in the database where the model was pre-trained on, I think you don't have to worry about it if you use the right tokenizer because it would replace it by an arbitrary token.
4089000	4107000	And of course also the model wouldn't then be able to perform well on if you have a lot of different tokens because these are tokens the model has never seen before and they are all, I think, converted to arbitrary unknown token or something like that.
4107000	4111000	Okay, nice. Also, do you want to say anything?
4116000	4119000	You can unmute yourself and ask any questions.
4119000	4120000	Can you hear me?
4120000	4121000	Yes, sure.
4121000	4134000	Thank you. Sorry for insisting on this question, but I just wanted to return a bit to this point, to the last one, to the vocabulary one.
4134000	4143000	My idea is that of a fast AI library in which they have the NLP part of it together with the image.
4143000	4160000	And actually, Jeremy Howard has made this fast AI library in a way that when you fine tune the pre-trained model to a specific domain, then all the words that are not tokens,
4160000	4168000	but part of the original vocabulary of the pre-trained model will be added to the new vocabulary.
4168000	4177000	And that will be, they will train by starting from scratch, let's say, no, as you do with the pre-training, but with this.
4177000	4185000	But I'm not sure that this is the case for Bert, to be honest, because I've been asking the guys in some other talks like this.
4185000	4188000	And as far as I know, they don't do that.
4188000	4203000	And that is the original question, like, how well does perform Bert pre-trained, let's say pre-trained Bert, when we fine tune it, I don't know, in medicine, when most of the characters, I don't know, in whatever other domain specific task,
4203000	4209000	when most of the words or tokens, sorry, tokens might be quite different, and they will be just unknown, no?
4209000	4212000	It's like technical question, sorry about that.
4212000	4221000	Yeah, partly you answered, thank you, but just like it's not about the size of the vocabulary, but like exactly in this thing, thank you so much.
4221000	4227000	Yeah, so yeah, I think that's a good point, I think they are kind of related.
4227000	4240000	So I think in the Bert model, they basically have a fixed vocabulary where all the words appear that appeared during pre-training plus an unknown token.
4240000	4247000	And then if you have to in fine-tuning the new words, it will just be mapped to this unknown token so they don't expand the vocabulary, right?
4247000	4254000	That's what I'm suspecting. And you mentioned that fast AI, they would add it to vocabulary.
4254000	4266000	Yeah, I haven't really worked with the fast AI version of this, so that sounds like a good approach right now if you then have some really, if you don't want to maybe pre-trained completely from scratch,
4266000	4275000	it's a very different data set with different tokens, so you kind of, it's kind of more flexible in that way, I would say. Yeah, that makes sense, yeah.
4275000	4278000	Thank you very much.
4278000	4288000	Thank you, Altie. Again, if you want to ask any questions, you can raise your hand and we will unmute you so we can ask questions to Dr. Spastien.
4288000	4302000	In the chat, we have a question from Anna. How to handle the bottleneck caused by the data loader reading each example from memory?
4302000	4317000	Yeah, the bottleneck, oh, so in terms of speed, yeah, you could increase, so what I do sometimes if it's too slow for a very large data set, what I would do is I would increase, I would make a new data loader for the training set.
4317000	4333000	If I want to compute the training set accuracy, I would have a second one, which has just a larger batch size than a one I used during training and for the test set, I would just set the batch size as large as it goes until it crashes, because at some point it would just crash.
4333000	4349000	In a case like this, I would say it's not really a bottleneck, it's, I mean, it takes maybe a minute or maybe a few seconds to process the whole data set in each epoch, and we are only computing in the accuracy function on, like,
4349000	4358000	the performance in each epoch, so it's not like too often, I think this is reasonable, but maybe the question is also about like how we deal with that bottleneck during training.
4358000	4368000	That's a good question, I am actually, yeah, that's one limitation, so I could only go up to batch size 16 here in this example, because otherwise I would run out of memory.
4368000	4383000	So one way to deal with that would be using a different type of transformer, for example, so there are more efficient transformers that have fewer parameters and smaller maybe inputs sizes, smaller number of tokens that might be in possibility.
4383000	4397000	But there are also different ways for distributed computing, it depends on what type of distributed computing you do some can kind of put the model onto different devices and handle with handle batches like that.
4397000	4407000	So for instance, if you remove the number of GPU restriction I had in my code, it would run on multiple GPUs and I think it splits up the batch size.
4407000	4426000	In this case, it I think would multiply the batch size if you have four GPUs before and then each GPU gets 16 but still you're processing 64 training examples at the same time and then you average the gradients from the different GPUs in that way, you can use larger batch sizes if you have multiple GPUs, that's another option.
4427000	4439000	But yeah, that's fundamentally a limitation of matrix modification in memory, for example, if you have only access to one GPU that could be challenging.
4439000	4440000	Great.
4440000	4452000	Well, I studied with Anna, she's asking, how can an individual like graduate student train a transformer for data for which there is no break train transformer available.
4452000	4455000	Is it even possible to resource wise.
4455000	4459000	Yeah, good question. I haven't done this myself yet I'm.
4459000	4472000	I think based on what I've seen from papers where people have done that for example this protein sequence paper training from scratch look very scary in terms of what resources on they had to use in order to make that happen.
4472000	4485000	And if the only case is like maybe if you have a small data set, maybe that might be feasible but I think then maybe the transformer won't be the best option if you only have a small data set for pre training, then maybe using a classic RNN or
4486000	4501000	even a bag of words model might be a better approach. But yeah I don't have unfortunately a good answer for that I think that's a concern I have to, if you I personally also don't have access to a large cluster where I could do that and even if I would have access to that.
4502000	4516000	All the engineering efforts that go into setting this up is also kind of challenging so companies, so far as I know, have really large teams of engineers that only really focus on the coding part just to run the model on these multiple devices and GPUs.
4516000	4530000	I mean there are API said make that simpler but I think in practice it's still not very easy to do that we need to have like experts doing that but maybe I'm wrong so I don't want to discourage anyone from trying this out in practice but
4530000	4535000	I think there's a better answer at that moment.
4535000	4539000	Back to Ritu Paratha.
4539000	4558000	I have a personally point tuned a pre trained GBT to model, which has 115 million bar meters with about 2600 mid sized text on 70 lines each and talk with me 15 minutes in.
4559000	4561000	Yeah, that's nice. Okay.
4561000	4576000	So on a very small 3000 data point. Okay, I didn't expect it to perform that well that is actually cool. So, one at 7 million it will be the size of GPT one approximately that is impressive I, I think that that sounds very promising then.
4576000	4585000	So, forget what I said in the previous answer. Maybe it is possible to really pretend train them on small data sets so yeah.
4586000	4595000	A question from Sujana. Why is there a norm used and not bad snow.
4595000	4606000	That is a good question why is layer norm used and not bedroom. I think it has something to do with the fact that we have a sequence.
4606000	4618000	We normalize across the sequence and not across across the batches by top of my head, I don't have a good answer I could.
4618000	4631000	I think I was thinking about that at some point but I forgot I think it's the way we normalize over over the batches in a way.
4631000	4642000	That's a good question. Another thing is batch not doesn't really perform well. If you have batch sizes smaller than 32 or 16, but I don't think this is the answer I think the answer is mobile.
4642000	4649000	The way we normalize this I would have to think about this again why they're not not batch norm.
4649000	4652000	Good question.
4652000	4661000	Anyway, I think this concludes the Q&A session for this talk. Thank you so much, Dr. Sebastian.
4661000	4663000	Back to you.
4663000	4666000	Thank you, thank you Dr. Sebastian.
4666000	4674000	Sebastian, can you share the slides or the code that you show us on the chat so we can post it on the Twitter. Yep.
4674000	4678000	I have it right away. I have this GitHub repository here.
4678000	4686000	And the GitHub repository has a link to the slides. I can also post it separately.
4686000	4687000	It's right here.
4687000	4689000	Thank you.
4689000	4696000	Great so we definitely want to give a huge thank you for you Sebastian. This was super insightful and hopeful.
4696000	4705000	Thank you everyone for joining. And please follow us on Twitter for more details about upcoming talks and see you soon. Thank you.
4705000	4719000	Yeah, thank you so much everyone for attending and also very good questions. Sorry for rushing a little bit about through the topics I don't know it's like I always want to talk about so many things and there's only so little time, but I appreciate your
4719000	4724000	patience and joining me here today that was fun and I really liked your questions that was cool.
4724000	4727000	Yeah, then have a great rest of the day everyone.
4727000	4728000	Bye bye.
4728000	4729000	Bye bye.

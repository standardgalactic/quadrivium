WEBVTT

00:00.000 --> 00:20.680
I'm so thankful to have you here.

00:20.680 --> 00:25.680
You know, a great conference like COSM requires you.

00:25.680 --> 00:28.940
You're the best part of the conference, the people that you'll meet here, the conversations

00:28.940 --> 00:31.920
around the table, the discussions we'll have.

00:31.920 --> 00:37.420
COSM is a place for discussion and deliberation about the tech issues, about this convergence

00:37.420 --> 00:41.140
of technologies that is transforming our world.

00:41.140 --> 00:47.640
Artificial intelligence, 5G, cloud computing, blockchain, crypto, all these technologies

00:47.640 --> 00:50.020
are happening at once.

00:50.020 --> 00:53.860
And there's a question, what does that mean for the economy?

00:53.860 --> 00:58.100
What does it mean for our place in geopolitically?

00:58.100 --> 01:01.500
And what does it mean for work, the future of work?

01:01.500 --> 01:03.060
What does it mean for lots of things?

01:03.060 --> 01:06.900
And so we'll be examining each of those things over the course of the day.

01:06.900 --> 01:10.500
A great conference requires great speakers.

01:10.500 --> 01:13.380
And we have an incredible lineup.

01:13.380 --> 01:16.300
If you haven't already looked at the program, you're here, so I'm going to assume that you

01:16.300 --> 01:17.500
have.

01:17.500 --> 01:23.420
But I want to thank Peter Thiel in particular, because this is our third COSM technology summit,

01:23.420 --> 01:28.820
and Peter has helped us kick off COSM, all three of those conferences, and I'm so grateful

01:28.820 --> 01:30.140
for his time.

01:30.140 --> 01:31.580
He's going to be joining us virtually.

01:31.580 --> 01:36.860
He's going to have a back and forth with George Gilder, and then he'll take your questions.

01:36.860 --> 01:42.140
I'll point out the microphone for questions is here, and there's a camera over there,

01:42.140 --> 01:46.500
so you will, if you want to ask a question, if we have time, you'll come down here, ask

01:46.500 --> 01:47.500
the question.

01:47.500 --> 01:53.940
We'll be able to see you, and we'll take questions and answers that way.

01:53.940 --> 01:56.780
It requires a great staff, and this is a staff effort.

01:56.780 --> 01:58.700
I'm going to be recognizing them throughout.

01:58.700 --> 02:02.900
So grateful to our Discovery Institute staff for helping to put this together.

02:02.900 --> 02:04.620
It's not easy.

02:04.620 --> 02:09.300
And also our sponsors, and I think our AV team is going to put up the sponsors.

02:09.300 --> 02:11.380
I want to recognize them quickly.

02:11.380 --> 02:16.260
Microsoft, from the beginning, Brad Smith has been a great friend to this conference.

02:16.300 --> 02:17.980
We're so appreciative.

02:17.980 --> 02:21.100
Amazon, Blockcelerate did a pre-conference.

02:21.100 --> 02:25.220
It was their own event, but they've been a great partner to us in the run-up to COSM

02:25.220 --> 02:26.220
2022.

02:26.220 --> 02:32.740
Smead Capital Management, right over here, Cole Smead, a look for him later, the MJ Murdoch

02:32.740 --> 02:40.620
Charitable Trust, Zevenbergen Capital, Madrona Venture Group, Trilogy International Partners,

02:40.620 --> 02:48.260
Henryx, Lucas Creative, Dunlumber, Dan and Cindy Mater, and Byron and Joanne Nutley.

02:48.260 --> 02:56.260
Can we give them a round of applause, please?

02:56.260 --> 03:00.660
All of those sponsors make this possible, along with your involvement, and again, we're

03:00.660 --> 03:01.980
so appreciative.

03:01.980 --> 03:05.500
And I want to stress again, you're going to hear some things over the next day or two

03:05.500 --> 03:06.980
that you agree with.

03:06.980 --> 03:09.260
You're going to hear some things you disagree with.

03:09.260 --> 03:13.500
And that's what makes a conference fun, as George Gilder likes to say, a conference where

03:13.500 --> 03:15.980
everyone agrees is a boring conference.

03:15.980 --> 03:20.940
And COSM is not a boring conference, so we're so excited to entertain you over the next

03:20.940 --> 03:22.340
couple of days.

03:22.340 --> 03:25.780
Lastly, I want to introduce Matt McElwain.

03:25.780 --> 03:30.940
Matt is the managing director of Madrona Venture Group, the leading VC firm in the Pacific

03:30.940 --> 03:32.060
Northwest.

03:32.060 --> 03:37.740
He is the chair of this year's COSM Technology Summit, and I'm so grateful to Matt.

03:37.740 --> 03:44.020
Matt has really played a crucial role in succeeding Tom Alberg, who I know that Matt

03:44.020 --> 03:47.020
is going to talk about a bit, but I'm so grateful to Matt.

03:47.020 --> 03:53.180
So please join me in welcoming Matt McElwain.

03:53.180 --> 03:58.380
Well, you're right about these lights.

03:58.380 --> 03:59.380
They're quite bright.

03:59.380 --> 04:01.500
Well, welcome, everybody.

04:01.500 --> 04:04.500
Thank you so much for being here at the conference.

04:04.500 --> 04:11.580
I'm absolutely excited to hear all the different discussions throughout the next few days.

04:11.580 --> 04:16.940
And I'm incredibly honored to step into the shoes of my longtime colleague, great friend

04:16.940 --> 04:19.060
and mentor, Tom Alberg.

04:19.060 --> 04:24.620
Many of you in the room know Tom, and there's some really fun ties to Tom and George and

04:24.620 --> 04:28.540
Bruce Chapman and others, but those three in particular, who will go all the way back

04:28.540 --> 04:33.900
to their days at a little college out in Cambridge, Massachusetts.

04:33.900 --> 04:40.900
And I think some of the early ideation that led to the ideas and the desire to bring people

04:40.900 --> 04:45.060
together and to have conversations that all too often has been lost in different parts

04:45.060 --> 04:49.900
of our society, including in some places in the educational system.

04:49.900 --> 04:56.220
Tom was this unique mix of humility and ideation.

04:56.220 --> 04:59.020
He was always thinking about what can you start?

04:59.020 --> 05:00.860
What can you build?

05:00.860 --> 05:05.620
Always as diverse as building up Perkins Cooley Law Firm here in Seattle, helping to

05:05.620 --> 05:13.500
build McCaw Cellular into AT&T Wireless, moving on from that into founding Madrona R-Firm,

05:13.500 --> 05:18.020
that's an early stage venture capital firm, and leading the first outside investment round

05:18.020 --> 05:22.260
of Amazon, where he then went on to serve on the board for 23 years, and all the kinds

05:22.260 --> 05:24.820
of innovation that came from that.

05:24.820 --> 05:26.620
But he loved to talk about ideas.

05:26.620 --> 05:30.140
He loved to debate things, many of the topics that we're going to cover.

05:30.140 --> 05:32.980
And so it's just a real honor that we could think of him.

05:32.980 --> 05:37.700
He passed away a couple of months ago and was just an absolutely amazing person and

05:37.700 --> 05:40.380
inspiration and champion of this conference.

05:40.380 --> 05:46.340
And one of the things he had asked me to do was to step into this role for this season.

05:46.340 --> 05:52.100
And the reason that I love that opportunity is because of this embracing of diversity

05:52.100 --> 05:53.100
of thoughts.

05:53.100 --> 05:54.820
Steve just referenced it.

05:54.820 --> 06:01.820
It's so important in this day and age to have people have truly curious minds.

06:01.820 --> 06:03.740
What does it mean to have a curious mind?

06:03.740 --> 06:07.740
Well, curiosity, by definition, is an act of humility.

06:07.740 --> 06:10.060
It means that you don't know.

06:10.060 --> 06:12.620
You don't know for sure about something.

06:12.620 --> 06:16.780
And you're willing to explore alternative perspectives.

06:16.780 --> 06:19.140
You're willing to explore the facts.

06:19.140 --> 06:21.700
You're willing to explore the opinions.

06:21.700 --> 06:27.420
And through that diversity of thought, get to better ideas, better understanding of what

06:27.420 --> 06:31.940
is true and what could be true in the future, and in an era where we live in the combination

06:31.940 --> 06:37.380
of a real world and kind of a spiritual dimension that we all want to try to understand better,

06:37.380 --> 06:43.180
and then increasingly a synthetic and virtual world, it's harder and harder and harder to

06:43.180 --> 06:48.740
know, let's say another way, one person's misinformation or disinformation is another

06:48.740 --> 06:50.380
person's truth.

06:50.380 --> 06:55.940
So let's dive into all these different topics over the course of the next couple of days.

06:55.940 --> 06:56.940
Explore the facts.

06:56.940 --> 06:57.940
Listen to the opinions.

06:57.940 --> 07:03.060
Be respectfully open to the different ideas and see what we can all learn together.

07:03.060 --> 07:09.700
Finally, I'm especially excited about this time of what's going on in a broad variety

07:09.700 --> 07:10.700
of areas.

07:10.700 --> 07:15.500
I'm going to have an opportunity to host a panel tomorrow on artificial intelligence.

07:15.500 --> 07:19.580
And there's many debates and topics around artificial intelligence.

07:19.580 --> 07:24.260
From what is artificial general intelligence, I'm guessing that Peter, when he comes on

07:24.260 --> 07:29.900
here in a minute or two, might reference this topic, but is there such a thing?

07:29.900 --> 07:33.220
Can AI be sentient or not?

07:33.220 --> 07:38.620
And what's even more interesting about this time, not only at that philosophical level,

07:38.620 --> 07:42.300
but at the level of what we would call intelligent applications.

07:42.300 --> 07:45.740
We all live with intelligent applications every day in our lives.

07:45.740 --> 07:49.500
Whether it's with a search engine, whether it's a recommender system on something like

07:49.500 --> 07:55.420
Amazon or Spotify or Netflix, intelligent applications are pervasive.

07:55.420 --> 07:58.980
But there's a whole new wave of intelligent applications that are coming.

07:58.980 --> 08:01.420
We'll unpack this tomorrow in the session.

08:01.420 --> 08:04.780
Those are what we refer to as generative applications.

08:04.780 --> 08:09.220
And the generative applications, unlike intelligent applications, are built on something called

08:09.220 --> 08:10.740
foundation models.

08:10.740 --> 08:14.700
Things like GPT-3 and Dolly and stable diffusion.

08:14.700 --> 08:21.820
And so that is a very interesting area where I might have a point of view that the human's

08:21.820 --> 08:23.140
always going to be in the loop.

08:23.140 --> 08:27.180
You might have a point of view that the human's going to be written out of that loop over

08:27.180 --> 08:28.180
time.

08:28.180 --> 08:32.860
But let's have those kinds of discussions because AI, generative apps, intelligent apps

08:32.860 --> 08:38.060
is a very important part of every aspect of life going forward in the future.

08:38.060 --> 08:40.700
Peter, this is Matt McOy and it's great to see you again.

08:40.700 --> 08:43.580
Let me just take a second to introduce you.

08:43.580 --> 08:48.780
I was just talking about generative apps and foundation models and speculating that you

08:48.780 --> 08:54.940
might have a thing or two to say about that in this maybe more broadly in your talk here.

08:54.940 --> 09:00.420
Peter, of course, was the co-founder of PayPal, co-founder of Palantir, still serves on the

09:00.420 --> 09:01.420
board there.

09:01.420 --> 09:07.500
And even beyond that has been a very active investor both individually as well as through

09:07.500 --> 09:12.980
Founder's Fund, which is the fund that he again created several years back, companies

09:12.980 --> 09:17.740
that you may have heard of like Airbnb and LinkedIn and so many others that he's been

09:17.740 --> 09:22.740
involved with again either directly and or through Founder's Fund.

09:22.740 --> 09:27.700
He's also done a couple of other things that I have found personally very inspirational,

09:27.700 --> 09:34.140
one of those being the Teal Fellows, which is a group of individuals that have been encouraged

09:34.140 --> 09:40.220
to go off and pursue their dreams with resources that the Teal Fellows program has provided.

09:40.220 --> 09:46.300
One of them most famously here of late, Dylan, his company Figma is being acquired by Adobe

09:46.300 --> 09:51.900
for $20 billion, so not too shabby, to leave college and he left Brown and pursued his

09:51.900 --> 09:54.060
dreams as a Teal Fellows.

09:54.060 --> 09:58.980
That's Dylan Field, Dylan Field.

09:59.300 --> 10:04.340
So there's a number of things that Peter has done and he's also taken the time, the book

10:04.340 --> 10:09.500
I'm going to mention specifically has been quite inspirational to me, is zero to one.

10:09.500 --> 10:13.100
It's a lot harder to go from zero to one than to go from one to a hundred or a thousand

10:13.100 --> 10:14.100
or a million.

10:14.100 --> 10:17.340
And so I'm sure he's going to touch on some of these different themes and without further

10:17.340 --> 10:19.340
ado, Peter Teal.

10:19.340 --> 10:20.340
Awesome.

10:20.340 --> 10:21.860
Thanks so much for having me.

10:21.860 --> 10:26.540
I thought I would do another AI or anti-AI talk.

10:27.540 --> 10:33.100
It was a great conversation I had last year with George Gildler.

10:33.100 --> 10:39.740
I think one of the challenges in this field is always to figure out the right, it's not

10:39.740 --> 10:43.820
just to get a diversity of views, but it's even just to figure out what the right questions

10:43.820 --> 10:44.820
to ask.

10:44.820 --> 10:53.060
And so the high-level question I want to ask today is basically is how should we think

10:53.060 --> 10:54.060
about AI?

10:54.060 --> 11:01.180
Do you think of it as intelligent, conscious, or merely evil?

11:01.180 --> 11:12.460
And so the first step of perhaps tackling this question is to tackle another question

11:12.460 --> 11:18.380
which Gilder asked me, he was the first question he asked me last year in the Q&A part.

11:19.020 --> 11:24.780
And why do so many people in Silicon Valley believe in the simulation hypothesis that

11:24.780 --> 11:27.620
the entire universe, the cosmos is just a computer simulation?

11:27.620 --> 11:30.700
Why do they believe something as crazy as this?

11:30.700 --> 11:36.180
And I thought about this question some more, there are a few answers.

11:36.180 --> 11:41.540
And as I will explain, I think this question is actually somehow entangled in an interesting

11:41.540 --> 11:47.100
way with the question about AI, intelligent, conscious, or merely evil.

11:47.980 --> 11:53.980
Now one way in which you can question the premise of this question is, as I will explain,

11:53.980 --> 12:01.380
I think probably the peak belief in the simulation hypothesis was maybe something like a decade

12:01.380 --> 12:07.700
ago, sort of maybe circa 2012 to 2015, and it has probably faded some.

12:07.700 --> 12:13.380
People still have Gen Z people say that things are glitched in the simulation, or there's

12:13.380 --> 12:17.980
still some sort of passive reference to it, but it's a little bit less intense.

12:17.980 --> 12:24.540
So the question of why we believe it is sort of like why did it gain so much momentum over

12:24.540 --> 12:28.700
the last 20 years, and then also why did it lose some steam?

12:31.100 --> 12:38.500
So the first very dumb answer to the simulation hypothesis question is that it's just sort

12:38.500 --> 12:44.460
of a sociological status, a game between the computer science and the physics people,

12:44.460 --> 12:51.180
you know, and that in the 2020, 2010s, computer science became the most important field.

12:51.180 --> 12:56.820
And so you could sort of say that you were showing that you were more important than

12:56.820 --> 13:00.460
the physics people, you got to determine what ultimate reality was.

13:00.460 --> 13:07.940
And it wasn't particles and matter and fields, but it was just bits in a computer, and that

13:07.940 --> 13:14.860
this is sort of a physics versus computer science type dynamic and reflected something

13:14.860 --> 13:16.860
about that.

13:16.860 --> 13:24.140
But if we want to give sort of the more fundamental answer on cosmology is that something had

13:24.140 --> 13:31.700
gone very haywire in physics, that you had sort of the multiverse is sort of where sort

13:31.700 --> 13:37.540
of a lot of the big bang inflationary cosmology had gradually gone into this infinite multiverse

13:37.620 --> 13:40.940
where basically anything goes.

13:40.940 --> 13:47.580
And and and I think, you know, that probably on some level, you want to critique the multiverse

13:47.580 --> 13:49.220
as a theory of science.

13:49.220 --> 13:55.260
It was one where the physicists couldn't think through enough, you know, what questions

13:55.260 --> 14:02.100
like where, you know, the being lost in all these infinities is tricky.

14:02.100 --> 14:05.260
If you cannot do induction, you know, are you still doing science?

14:05.260 --> 14:07.100
Is it a universe that's too big for science?

14:07.580 --> 14:11.140
And and and that's why I think you should often think of the multiverse as a gateway

14:11.140 --> 14:14.460
drug to all these these very different things.

14:14.460 --> 14:19.340
And once you have a multiverse, you can also have the matrix or a simulation, you can have

14:19.340 --> 14:26.100
Boltzmann brains, you can have, you know, you can have all kinds of strange possibilities

14:26.100 --> 14:28.300
for the nature of the universes.

14:28.300 --> 14:33.260
And so that, you know, even if we say the simulation hypothesis is kind of crazy, on

14:33.260 --> 14:37.060
some level, is it really crazier than than the multiverse and sort of link?

14:37.460 --> 14:42.140
Now, there's a third answer I'm going to give to why the simulation universe theory

14:42.420 --> 14:46.420
gained so much traction, but that will take a little bit more time to develop.

14:48.180 --> 14:54.220
The now, but as part of as part of this, we should look back on.

14:55.500 --> 14:59.660
You know, the last 17, 20 years of AI, AGI.

15:00.300 --> 15:07.300
And if we if we went back 20 years in time, it was it was super optimistic

15:07.740 --> 15:13.580
in in all these ways where it's going to, you know, it's going to be cornucopia.

15:13.580 --> 15:19.180
2005 was the year Kurzweil wrote the singularity is near.

15:19.300 --> 15:23.740
Basically, you know, you're going to have accelerating technological progress,

15:23.740 --> 15:25.540
runaway technological progress.

15:26.420 --> 15:30.500
You know, it was it was in some ways, it felt like you didn't have to do much.

15:30.500 --> 15:32.140
So it was somewhat passive for the humans.

15:32.140 --> 15:36.180
You just had to sit back, eat some popcorn, watch the movie of the future unfold.

15:38.100 --> 15:41.220
And then and then there were certainly all these versions

15:41.220 --> 15:45.740
where it was going to be so much growth, we would need basic income as a safety net

15:45.740 --> 15:48.580
because there'd be no more there'd be no more labor market.

15:49.340 --> 15:56.220
You'd have you'd have sort of incredible discoveries in all these fields.

15:57.500 --> 16:01.300
You know, there was there was sort of certainly some latent question about,

16:01.300 --> 16:04.420
you know, how you have to make sure that AGI was was friendly.

16:04.580 --> 16:05.180
But.

16:07.820 --> 16:13.060
Of the narrative in the 2000s, early 2010s was in this sort of,

16:13.740 --> 16:21.820
you know, optimistic utopian, cornucopian direction of what AI would would would would would look like.

16:23.300 --> 16:27.900
And but now if we sort of look at, you know, what is what's actually happening with AI,

16:27.900 --> 16:29.220
not, you know, where it's going.

16:29.220 --> 16:33.700
So I'm not going to speculate on AGI or, you know, even where it's going in the near future.

16:33.700 --> 16:40.220
But if you look at what is actually happening with AI, you know, we get something like this guy.

16:40.220 --> 16:44.780
This is a this is a TikTok video.

16:46.100 --> 16:50.540
And, you know, one of my one of my colleagues went through a constructed

16:50.660 --> 16:56.140
so a whole series of profiles on TikTok, where it's an AI optimization engine

16:56.140 --> 16:58.100
that feeds you content that you want to see.

16:58.100 --> 17:00.020
That's supposedly what it does.

17:00.660 --> 17:06.020
And and and and basically, you know, came up with all these different profiles

17:06.020 --> 17:11.060
for Midwestern housewife, you know, all sorts of people with different kinds of interests.

17:11.220 --> 17:16.060
But sort of what what you sort of, you know, part of what the algorithm fed you

17:16.260 --> 17:21.620
was this this fairly sort of deranged content, which and, you know,

17:21.620 --> 17:25.340
this this particular person, I was going to play the video, but we decided

17:25.340 --> 17:28.860
there were too many naughty words in it that I might get in trouble for.

17:29.180 --> 17:35.020
But he basically explains how he exploits employment law, bounces from job to job.

17:35.140 --> 17:38.540
Suze all his employers for health and safety violations.

17:38.860 --> 17:42.460
And it's sort of designed to make, you know, it's designed to sort of

17:42.460 --> 17:47.580
polarize race relations in the US and make, you know, black people angry.

17:47.580 --> 17:52.060
Because if you have an honest job, you're a sucker, it's designed to make white people angry.

17:52.340 --> 17:59.100
And there's sort of our versions, versions like this that come up throughout TikTok.

17:59.100 --> 18:02.980
This this is this is sort of what I would say a front and center,

18:02.980 --> 18:08.500
what cutting edge AI looks like in in the U of S today in 2022.

18:09.340 --> 18:14.260
And and, you know, it's of course, we don't really know what the the full

18:14.260 --> 18:18.300
intentionality is. We don't know whether this is a, you know, is just sort of an

18:18.300 --> 18:24.740
emergent property that it's going to sort of take what people want, push it to

18:24.740 --> 18:27.580
extremes, derange it, derange the discourse.

18:27.580 --> 18:32.580
So you'll have, you know, you'll have you'll intensify the race

18:32.580 --> 18:36.180
contradictions, you'll intensify the economic contradictions, how, you know,

18:36.180 --> 18:39.900
the US has the worst health care system in the world, you'll have someone else

18:40.940 --> 18:47.100
talking about, you know, how you should call pedophiles, a map person,

18:47.100 --> 18:50.340
a minor attracted person, because we shouldn't discriminate against those people.

18:50.340 --> 18:55.900
So it's so it's all sort of like designed to to, you know, intensify the

18:55.900 --> 19:00.500
wokeness, anti wokeness, polarize and derange our society in one way or another.

19:01.500 --> 19:03.100
You can view it as emergent.

19:03.100 --> 19:07.980
You can view it as full on intentional where you should think of TikTok as a

19:07.980 --> 19:14.340
sort of Chinese communist weapon that is being used to derange our society.

19:14.340 --> 19:18.220
That, you know, the book that I think is interesting is the one who named

19:18.220 --> 19:21.540
who's the number four guy in the in the entire communist government.

19:21.540 --> 19:26.500
He's sort of this the professor, theorist of Xi Jinping thought.

19:27.500 --> 19:31.420
And he wrote this book 32 years ago, America versus America.

19:31.420 --> 19:35.620
And it's basically it's basically a roadmap for how to derange our society

19:35.660 --> 19:39.540
by sort of heightening these sort of Hegelian contradictions.

19:39.980 --> 19:45.180
And and I would submit that if you go with the full intentional version,

19:45.460 --> 19:50.540
TikTok is is is basically a weapon that's designed to derange us

19:50.660 --> 19:55.100
through decentralized and heightened contradictions.

19:55.540 --> 20:02.420
And of course, this particular AI is up against another self-destructive

20:02.780 --> 20:09.260
communist Chinese AI, which is the centralized one that's being imposed on China itself.

20:10.300 --> 20:14.780
And, you know, we're basically you have a perfect face recognition.

20:15.420 --> 20:17.140
Everybody knows they're being monitored.

20:17.140 --> 20:20.900
They are they're living in a, you know, there is sort of an important way

20:20.900 --> 20:23.500
in which China has become North Korea.

20:23.500 --> 20:28.580
It has gone, you know, it it was, you know, those of us for anti-communist

20:28.900 --> 20:32.940
tend to tend to conflate that it was a communist country 10 years ago and is now.

20:33.180 --> 20:38.460
But there is a way in which the the AI technology, the surveillance technology,

20:38.620 --> 20:40.660
has has really, really transformed it.

20:41.020 --> 20:44.980
It is it is again not it is again not.

20:46.740 --> 20:48.220
You know, it's not AGI.

20:48.300 --> 20:53.140
It is, you know, in many versions, it's it's fairly, you know, barely AI at all.

20:53.180 --> 20:57.740
It's just sort of cameras, you know, ways to do big data on this.

20:57.740 --> 21:00.220
This is, you know, this is what the Kai-Fu Li book talks about, too,

21:00.220 --> 21:06.900
that China will win at AI through these, you know, sort of big data algorithms.

21:07.020 --> 21:10.820
It's not about the sort of cutting edge futurists, except the people in Silicon

21:10.820 --> 21:14.740
Valley talk about things about like TikTok or what China has done to themselves.

21:14.740 --> 21:18.860
And so you can basically, you know, one way to think of the rivalry between

21:18.860 --> 21:23.540
the U.S. and China is that it's it's, you know, it's sort of a question,

21:23.940 --> 21:29.500
which society will be destroyed faster by the by the somewhat dystopian AI

21:29.500 --> 21:30.980
that's being imposed on it.

21:31.340 --> 21:36.100
And and and we have sort of a long, long debate about that.

21:38.100 --> 21:42.420
So with this is sort of the framing of where AI actually is, where it actually,

21:42.740 --> 21:47.860
you know, is being implemented in the most powerful, dramatic ways today.

21:49.860 --> 21:52.220
Let's go back to our three questions about AI.

21:53.300 --> 21:57.580
You know, first off, is it is it intelligent?

22:00.060 --> 22:04.660
You know, I'm not sure whether we should even, I mean, on the first two questions,

22:04.660 --> 22:09.300
I'm going to sort of say they're above my pay grade, but it seems to set a low bar

22:09.300 --> 22:10.180
for intelligence.

22:10.860 --> 22:16.420
And the rhetorical point I would make is that it's often just a filler word when,

22:16.620 --> 22:19.300
you know, we're talking about something quite different.

22:19.980 --> 22:25.260
There was a 2016 Obama administration study about the transformative importance

22:25.260 --> 22:28.980
of AI entitled quote, the title of the whole paper, the National Artificial

22:28.980 --> 22:31.380
Intelligence Research and Development Strategic Plan.

22:31.940 --> 22:36.780
And and basically, if you went through this paper and if you replaced every use

22:36.780 --> 22:42.500
of the word of AI with software or even just computers, the meaning wouldn't

22:42.500 --> 22:43.460
change at all.

22:44.100 --> 22:50.900
And and I think this is sort of a a tell that, you know, maybe the first

22:50.900 --> 22:55.460
approximation when you hear AI, you should just think software or computers.

22:55.740 --> 23:01.540
It's, you know, AI is is not, is probably not intelligent.

23:01.820 --> 23:05.020
AGI, that's somewhere in the future, don't know.

23:06.980 --> 23:10.300
You know, in a similar way, I would say the question of, you know, whether

23:10.300 --> 23:15.780
it's conscious is probably hard to say, you know, my my my strong suspicion

23:15.780 --> 23:20.100
is that it's it's not, of course, have, you know, the epistemological problems.

23:20.380 --> 23:22.380
You know, Thomas Snagle, what does it like to be a bad?

23:22.420 --> 23:26.420
You have the Searle's Chinese room problem.

23:27.460 --> 23:29.820
And then, of course, and so it's, you know, it's hard to say.

23:29.820 --> 23:30.620
And for Mr.

23:30.620 --> 23:35.180
Lemoine, who I think is is talking later, it was literally hard to say that AI

23:35.180 --> 23:36.140
might be conscious.

23:36.500 --> 23:40.500
And so, you know, as a as a contrarian, I'm always a little bit biased to say

23:40.500 --> 23:42.460
that things that you're not allowed to say might be true.

23:43.380 --> 23:46.260
So I don't want to dismiss the possibility entirely that it's conscious.

23:47.380 --> 23:55.180
But but it's probably probably the wrong sort of question on on on some level

23:55.180 --> 23:57.940
for us to be asking the question, whether it's intelligent, whether it's

23:57.940 --> 24:02.780
conscious or just the wrong questions that the, you know, I always go back

24:02.820 --> 24:07.100
to what I don't like even about Descartes, where you think about Cartesian dualism

24:07.100 --> 24:11.420
as the the origins of of the problem of consciousness.

24:12.980 --> 24:16.900
The way the way consciousness worked for Descartes was that it was meant

24:16.900 --> 24:20.980
to be, you know, a smart person in the 17th century was supposed to become

24:20.980 --> 24:23.500
a priest and use his brains to think about God.

24:24.020 --> 24:28.780
And Descartes came up with this very mysterious different thing called the mind.

24:29.100 --> 24:34.220
And it was sort of an attention, redirection, distraction mechanism.

24:34.220 --> 24:41.580
And and I always think we should we should remember the 17th century context

24:41.580 --> 24:48.340
where consciousness was not something that was mystical or spiritual or dualist

24:48.340 --> 24:51.980
in sort of the way we might think of these categories in the 21st century.

24:52.140 --> 24:54.540
But it was meant to be anti theological.

24:55.500 --> 24:59.340
And that but that, you know, maybe more generally the problems of consciousness

24:59.340 --> 25:02.860
or even of intelligence are somehow the wrong question.

25:02.860 --> 25:09.060
So let me go to my my third one, you know, is AI evil?

25:09.820 --> 25:16.100
And this one seems seems, you know, more straightforwardly answerable.

25:16.100 --> 25:20.380
Certainly, I think that the TikTok algorithm is evil.

25:20.620 --> 25:25.820
I think what China is doing to itself is is clearly evil.

25:26.140 --> 25:31.180
You know, we can, of course, you can talk about evil in all kinds of different

25:31.180 --> 25:34.780
versions. There's, you know, there's, of course, the kind of,

25:36.140 --> 25:38.940
you know, disembodied brain and C.S.

25:38.940 --> 25:41.980
Lutus's book that hideous strength, who turns out to be a demon.

25:43.420 --> 25:47.860
You know, I'm not sure it's literally demonic and quite quite that sort of a way.

25:47.900 --> 25:52.220
Although, although certainly I don't think that we've had an exorcist at Google

25:52.220 --> 25:54.420
to check that out and make that determination.

25:54.900 --> 25:58.380
So I think, you know, even that possibility couldn't quite be ruled out here.

25:58.700 --> 26:05.140
But but, you know, it's it's evil in the the the the creepy looking woman

26:05.140 --> 26:11.460
on the upper right is this is this is this image loa who seems to be

26:11.460 --> 26:15.700
sort of a strange attract that comes up in a number of the art projects

26:15.700 --> 26:20.780
that Dolly the the AI art program has generated.

26:20.780 --> 26:26.740
And it's it's sort of if you do if you ask what is the opposite of Marlon Brando,

26:27.020 --> 26:29.380
you get this sort of you get the somewhat abstract painting.

26:29.380 --> 26:31.340
And then if you ask what's the double negative of that,

26:32.500 --> 26:36.940
this sort of creepy woman loa emerges

26:37.620 --> 26:42.860
and and and and what's what's what's sort of an interesting

26:42.900 --> 26:46.700
that she emerged on sort of a number of things were sort of this strange attractor.

26:46.700 --> 26:51.020
And you can think of it as maybe it is a kind of occult knowledge

26:51.340 --> 26:53.980
where we're we're learning something.

26:54.020 --> 26:58.220
Did we really need to know that the double negative of Marlon Brando

26:58.380 --> 27:05.140
was a witchlike woman and and and and something like that.

27:05.420 --> 27:09.980
But but of course, maybe the closest analog to sort of a

27:10.980 --> 27:15.900
to sort of a demon is is an idol, a pagan God,

27:17.140 --> 27:19.260
where, you know, we worship the God,

27:20.660 --> 27:23.100
the God seems to tell us what to do.

27:24.100 --> 27:26.860
And it's unclear if it's actually telling us these things

27:27.100 --> 27:30.300
or if it's just somehow some kind of psychosocial effect

27:30.500 --> 27:33.740
that's creating some kind of mass hallucination

27:34.100 --> 27:38.180
and and and that leads leads to this.

27:38.180 --> 27:41.460
And this is sort of this is sort of where I've suggested that, you know,

27:41.460 --> 27:46.140
maybe you should think of the European Union as a kind of the closest thing

27:46.140 --> 27:49.020
we have to functioning AI and government in a way where it's

27:49.500 --> 27:52.660
it's the goal is just to prevent human thought.

27:53.060 --> 27:55.780
It tells us very basic, simple things that we should do.

27:56.180 --> 27:59.700
But, you know, it functions these ways.

28:00.220 --> 28:01.220
But.

28:03.220 --> 28:05.860
But if we but now let's come back, you know,

28:05.940 --> 28:11.060
if we say that there is a lot in AI

28:11.620 --> 28:16.540
that is straightforwardly evil, that is merely evil,

28:16.700 --> 28:19.020
that is simply about stopping humans

28:20.060 --> 28:24.340
from thinking, from using their capacities and things like this.

28:24.540 --> 28:27.700
Let me let me use this to come back to the

28:29.220 --> 28:32.700
the very big cosmological question about the simulation.

28:33.700 --> 28:37.620
And and so now let me give an alternate sort of explanation

28:37.620 --> 28:41.580
of why the simulation hypothesis gained so much traction

28:42.100 --> 28:44.740
in the 2000s and.

28:46.060 --> 28:47.460
Early 2010s.

28:49.660 --> 28:52.420
And it's something it's something like this.

28:53.660 --> 28:57.940
As we were building AI, as we were building towards AGI,

28:58.940 --> 29:03.700
it seemed, you know, it seemed potentially dangerous, you know, AGI

29:04.700 --> 29:09.540
in the full utopian sense was going to be this, you know, superhuman mind.

29:10.140 --> 29:13.180
Could we really be confident that it was going to be, you know,

29:13.180 --> 29:16.220
aligned with human beings, that it was not going to be lying?

29:16.220 --> 29:18.340
There seemed to be, you know, a lot of.

29:20.140 --> 29:23.740
Of risk in that, you know, the and I think that, you know, a lot of

29:24.740 --> 29:30.420
those of us who are skeptical of AI or skeptical of AGI often underestimate

29:30.620 --> 29:37.420
how troubling the the alignment arguments are, how, you know,

29:37.420 --> 29:38.900
it's not straightforward.

29:39.100 --> 29:42.460
If you can have such a thing as friend as AGI, it's not at all

29:42.460 --> 29:44.900
straightforward to get the AGI to be friendly.

29:44.940 --> 29:48.100
You know, if you have a Darwinian view of the world or just a Machiavellian

29:48.100 --> 29:51.460
view of the world, where you'd say the core axiom is that there is no such

29:51.940 --> 29:56.340
thing as a selfless being, a purely selfless being, and therefore the alignment

29:56.340 --> 29:58.620
problem is fundamentally difficult to solve.

29:58.620 --> 30:00.500
So how do you get to friendly AI?

30:00.900 --> 30:03.340
Not not super straightforward.

30:04.980 --> 30:10.860
But if we say, and maybe maybe AGI ends up being a kind of great

30:10.860 --> 30:16.820
filter where, you know, if you get it wrong, it will, it will destroy the world.

30:17.460 --> 30:21.300
And this is where, you know, this is where there seemed to be a very

30:21.300 --> 30:25.620
big difference between the multiverse and the simulation theories, because in

30:25.620 --> 30:29.340
the multiverse, the AGI is simply in the future.

30:29.700 --> 30:35.380
And whatever great filter the AGI represents, whatever threat it represents

30:35.380 --> 30:41.580
to all of humanity, wipe out all of humanity, it's in the future and seems

30:41.580 --> 30:42.460
quite dangerous.

30:42.780 --> 30:47.980
Whereas if you have a simulation theory, you also have this cosmic AI or cosmic

30:47.980 --> 30:50.660
AGI that created our universe.

30:51.020 --> 30:53.860
And in some sense, there was a great filter in the past.

30:53.900 --> 31:04.580
And so there is a way that perhaps you could think that the cosmic AI isn't

31:04.580 --> 31:07.180
entirely hostile since we're here having this conversation.

31:07.380 --> 31:14.740
Perhaps the cosmic AI is guiding the development of the AGI and we can infer

31:14.740 --> 31:22.140
that it will, it will be sort of, it will sort of be, be aligned with, I think

31:22.140 --> 31:24.940
this is the way you have to think of the simulation theory.

31:24.940 --> 31:31.140
It was in the context of a lot of these concerns about friendly versus

31:31.140 --> 31:35.740
unfriendly AI, and it shifted the problem from the future where it is a

31:35.740 --> 31:40.380
multiverse to the past and seemed to solve it.

31:41.180 --> 31:46.780
Now, the thing that is very different from when Kurzweil was writing about this

31:46.780 --> 31:55.020
in 2005 is, you know, we have, we've seen some of the progress and, and somehow,

31:55.740 --> 31:58.980
somehow this illusion has become very hard to maintain.

31:59.460 --> 32:07.540
And, and, you know, we have, you know, the, the, the sort of AI, the thing

32:07.540 --> 32:14.540
that perhaps is tracking towards, you know, you know, autonomous weapon system,

32:14.540 --> 32:23.620
cyber warfare, you know, a runaway AGI, it doesn't seem very good for humans.

32:23.620 --> 32:28.820
And this is, this is both, both in its cutting edge, centralized and decentralized

32:28.820 --> 32:35.780
forms. And, and, and, and as a result, I would say we are, we're sort of, we've

32:35.780 --> 32:42.980
been sort of inclined to flip, flip the causation that, you know, the emergent AI

32:43.020 --> 32:48.820
or emergent AGI is what's telling us something about the AI that built the

32:48.820 --> 32:53.500
universe. And if the emergent, if it's the nature of the emergent AI to be

32:54.460 --> 33:00.180
fundamentally or merely evil, then perhaps we should not be so, so assured

33:00.180 --> 33:06.900
that the, the, the cosmic AI that created the simulation was, was fundamentally,

33:07.980 --> 33:11.780
was fundamentally good. And, you know, we should extrapolate from one AI to the

33:11.780 --> 33:16.260
other and assume that it's, it's also self-interested, not aligned with humans,

33:16.380 --> 33:24.340
not fundamentally beneficial to the human world. And, and, and this is why I

33:24.380 --> 33:31.180
think the, the simulation theory, you know, maybe was a fake way to solve this,

33:31.180 --> 33:36.500
this problem, but it's not at all working anymore. You know, I think one, one, one

33:36.500 --> 33:40.660
way to think about this is there, there's sort of all these, all these kinds

33:40.660 --> 33:47.100
of debates about the meaning and nature of AI that map onto these theological

33:47.100 --> 33:52.300
controversies from the Middle Ages. And it's, it's always sort of interesting to

33:52.300 --> 33:58.300
try to, to try to, to map it onto these, these, these past theological debates,

33:58.300 --> 34:02.780
the way to just sort of understand the nature of the argument. And you can think

34:02.780 --> 34:10.580
of, you can think of the cosmic AI as sort of analogous to a form of strict

34:10.580 --> 34:17.820
monotheism, like Judaism or Islam, where it is the oneness of God. And, and then

34:17.860 --> 34:22.620
the problem with extreme monotheism is that you cannot speculate on the

34:22.620 --> 34:27.420
attributes of God. You ultimately do not know much about the nature of God. To

34:27.420 --> 34:32.020
have a, you know, science of God, you need, you need a plurality. If you have too

34:32.020 --> 34:36.980
many gods, of course, they're probably not gods. But, and, and this is sort of

34:36.980 --> 34:43.580
where, and then, and if you think of Christ and Trinitarian Christianity as

34:43.580 --> 34:48.580
telling us something about the nature of God, you go from the God in history to

34:48.580 --> 34:54.420
tell us about the God outside of history. And, and I think there's roughly a

34:54.420 --> 35:00.220
similar move that's happened with, you know, the, the emergent AI, which is the

35:00.220 --> 35:07.900
sort of, you know, the, the idol, the, the demon idol, whatever you want to call

35:08.220 --> 35:15.100
it, that's emerging in history, that, that it is telling us that if, if there was

35:15.140 --> 35:20.220
some demiurge or something like that, that, that built the simulation, we should

35:20.220 --> 35:26.660
also infer that it's, it's, it's, it's, it's not that well aligned. And so, and

35:26.660 --> 35:32.900
so this is sort of where, you know, we are seemingly at, at these, at these dead

35:32.900 --> 35:40.260
ends with, with the progress of AI. And, and it's, you know, I'm not going to

35:40.420 --> 35:45.260
solve, solve this problem today, but it seems to me that, you know, surrendering

35:45.260 --> 35:52.660
control to AI, you know, blindly worshiping AI, the emergent AI, letting it

35:52.700 --> 35:57.980
dominate and control our societies leads to, you know, one of two catastrophic

35:57.980 --> 36:04.220
outcomes, you know, it's sort of decentralized runaway violence, which is,

36:04.420 --> 36:09.260
you know, the derangement of TikTok, America versus America, and then, you

36:09.260 --> 36:18.300
know, centralized totalitarian one world state, you know, worse than North

36:18.300 --> 36:25.260
Korea or China. And, and that the challenge for us is to find some kind of a

36:25.260 --> 36:32.700
third path, you know, where we, we make progress in areas other than AI, we, we

36:32.700 --> 36:38.340
find a way to, to get back to the future, you know, the runaway apocalyptic

36:38.340 --> 36:43.300
violence, the centralized totalitarian one world state, they are, they are

36:43.300 --> 36:49.060
seemingly exclusive possibilities. I don't think they're exhausted. I think

36:49.060 --> 36:52.900
there should be a third way in the challenges for us to, to find a way to

36:52.940 --> 36:57.300
build it. You know, not a fan of BF Skinner, the behavioralist, psychologist,

36:57.300 --> 37:00.940
but the quote I, as I always like to cite is, you know, the real problem is not

37:00.940 --> 37:05.980
whether machines think, but whether men do. And we need to get back to thinking

37:05.980 --> 37:10.820
ourselves and, and regain control of our future. Thank you very much.

37:11.580 --> 37:17.740
Peter, thank you so much for that stimulating and philosophical and

37:18.260 --> 37:29.500
prophetic oration. It was really worthy of zero to one. This conference is going

37:29.500 --> 37:40.140
to be one of our prime themes is the development of superabundance. And this

37:40.140 --> 37:50.140
is really being demonstrated by this new book by Marion Tupi and Gail Pooley

37:50.180 --> 37:59.820
about that really documents on the basis of time prices that abundance is

37:59.980 --> 38:08.900
steadily increasing at an accelerating pace. And this conflicts and important

38:08.900 --> 38:14.900
ways with your vision, Peter, that you have previously expressed that in some

38:15.260 --> 38:26.300
way technology is becoming less fruitful and less creative and less responsive

38:26.300 --> 38:35.500
to real human needs. So I wondered whether you can record what sort of insights

38:35.500 --> 38:42.460
you can have that transcends this apparent conflict, you know, between the idea

38:42.460 --> 38:50.860
that science is going stagnant or technology is becoming sterile and

38:51.660 --> 39:02.300
demonstration of this magnificent new book that that poverty is being overcome

39:02.340 --> 39:08.660
everywhere, that the price of commodities is plummeting, that everything's becoming

39:08.660 --> 39:17.500
more abundant, that science actually is offering new bounties every day. That

39:17.620 --> 39:22.140
would be my... Well, I, well, I don't, I don't agree with the book on any level.

39:22.140 --> 39:27.740
So, so it's okay. But I think, look, I think, I think, I think even something

39:27.740 --> 39:33.220
as basic as commodity prices are, you know, at, you know, there was a hundred year

39:33.220 --> 39:37.700
decline trend in the 20th century. And then, you know, and if they go much higher

39:37.700 --> 39:43.500
than they are now, it's like that we've had a 20 year bull market, and it will go up

39:43.500 --> 39:48.860
in a way that suggests the whole decline trend is broken. And so, and then, you know,

39:48.860 --> 39:52.900
there are all these different, you know, reasons you can do this. We have, you know,

39:52.900 --> 39:58.260
certainly, you know, the macroeconomic version is always to look at, is to look

39:58.260 --> 40:05.660
at real inflation, inflation versus real, you know, wages or people's wages going

40:05.660 --> 40:10.020
up faster than inflation. And the felt sense is that that's not happening. And

40:10.020 --> 40:13.260
then you can make, you know, and then, you know, the super abundance argument is

40:13.260 --> 40:16.620
somehow that the government is understating the inflation and there's

40:16.620 --> 40:20.180
less inflation than it looks. And, and, you know, I don't think they're

40:20.420 --> 40:25.140
overstating it massively, but, but at the margins, I believe the government is, I

40:25.140 --> 40:27.900
will, the super abundance argument is that they're overstating inflation. There's

40:27.900 --> 40:32.300
less inflation. There's more real growth. My argument would be, you know, at the

40:32.300 --> 40:36.060
margins, they're probably understating inflation. So there's more inflation and

40:36.060 --> 40:41.980
actually, you know, even, even less productivity growth. But, but, but the

40:41.980 --> 40:47.300
way, the way, if I had to sort of reconcile these two views, it would be along

40:47.300 --> 40:51.700
the lines of the talk I just gave you, which is, you know, let us say that there

40:51.700 --> 40:57.380
are some dimensions where there is, you know, a reasonably rapid amount of

40:57.380 --> 41:00.780
progress that there has been, you know, maybe not as much progress in the world

41:00.780 --> 41:04.700
of Adams as I would like. There has not been progress on energy or, you know, we

41:04.700 --> 41:08.780
don't have nuclear power plants. We don't have, you know, we haven't had less

41:08.780 --> 41:13.300
progress in futuristic medicines than I would like. But we had, we've had, you

41:13.300 --> 41:20.140
know, a lot of progress around computers and, and the internet, the mobile

41:20.140 --> 41:23.780
internet, and then, and then of course, all these things that get loosely

41:23.780 --> 41:29.580
categorized under, under AI. And, and then, you know, you have, you have the macro

41:29.580 --> 41:34.620
economic question, you know, how much that progress lifts our human society

41:34.620 --> 41:39.460
generally, I would say it's less than said, but let's, but, but then I think the

41:39.460 --> 41:43.700
other dimension you have to ask is, is it the sort of progress that people think

41:43.700 --> 41:49.260
of as, as simply, simply good? And, you know, if the, you know, the, the futuristic

41:49.260 --> 41:54.900
AGI was pitched to me in 2005, as you have no idea what sort of, we will be able

41:54.900 --> 42:00.100
to cure aging, we'll be able to find all these fantastic medical treatments. We

42:00.100 --> 42:04.140
still have not gotten those. If we had gotten those, I would score it as, as

42:04.140 --> 42:09.180
at least more positive. What we have gotten, we got TikTok. And yeah, that's,

42:09.220 --> 42:14.940
it's valuable for TikTok. It's valuable for the company that sort of does this

42:15.340 --> 42:20.300
AGI. But it, you know, I would, I would score it as, as a form of technology

42:20.300 --> 42:24.660
that is, that's, you know, and I don't want to sound overly lead, but even if

42:24.660 --> 42:29.700
it's rapidly, it's, it's deranging us. It's making us go crazy. Or the

42:29.700 --> 42:33.620
surveillance state in China, that is a form of technological progress over a

42:33.620 --> 42:39.260
decade ago, but it has, you know, it has, it has really deranged that society. It

42:39.260 --> 42:43.500
has disabled the humans. It's, it's a less happy, less functional, less free

42:43.500 --> 42:48.180
place than it was 10 years ago. Even if we say that it somehow shows up in the

42:48.180 --> 42:51.540
economic statistics, which it doesn't, but even if you can make, even if you can

42:51.540 --> 42:56.620
jigger the ACON statistics to make the super abundance show up, it might, we

42:56.620 --> 42:58.780
should ask this question, is it evil or is it good?

42:59.060 --> 43:04.580
Yep. So Pete, so Peter, before we have some audience questions, I'm going to ask

43:04.580 --> 43:10.820
a question too, as I think about some of these contemporary examples, like in the

43:10.820 --> 43:15.700
area of science, things like Alpha fold or deep fold, you know, where we've been

43:15.700 --> 43:19.300
able to predict and understand the structure of proteins coming out of the

43:19.300 --> 43:23.100
deep mind team. Or we, you know, and so we have the productivity, you know,

43:23.100 --> 43:27.620
co-pilot that's been recently launched by Microsoft and the claim by, by

43:27.660 --> 43:31.500
Satya and Charles Lamont and others. There's a 30% improvement in productivity

43:31.500 --> 43:36.580
and software development. Or you take stable diffusion and the creativity that

43:36.580 --> 43:39.780
that unlocks, you know, for, for humans. In every one of those cases, of course,

43:39.780 --> 43:44.660
humans in the loop in many, many different ways. So no AGI here. And I guess

43:44.660 --> 43:49.020
the question is, therefore, can AI both be evil and good?

43:52.020 --> 43:56.940
Sure. But, but, you know, all, like all the examples, you know, like there's

43:56.940 --> 44:00.620
always, look, there's always a fast line, it's just a technology and it just, it

44:00.620 --> 44:05.820
just, you know, it's, and as such, it's, it's, it's pretty neutral and it's up to

44:05.820 --> 44:12.820
humans what we, what we do with it. But I think one can, I would still say one

44:12.820 --> 44:17.300
should ask the question more, you know, is it, you know, you know, how do we sort

44:17.300 --> 44:20.820
of weight these different kinds of applications? You know, the protein

44:20.820 --> 44:25.900
folding is, is interesting. It's, it's, it's only valuable, I would say, if it

44:25.940 --> 44:32.340
actually leads to new medical interventions, new cures. And when I, when I

44:32.340 --> 44:37.380
push the AI people on that, they always, they don't, they don't want to engage

44:37.380 --> 44:44.460
in that conversation. And, you know, so, and then, and then, you know, if, if it

44:44.460 --> 44:51.780
is saving 30% of coding time, that, that would be very impressive. It, it's, it's

44:51.820 --> 44:56.820
not clear that that's, that's showing up in, in any of the, of the stats of the, of

44:56.820 --> 45:01.860
the big, big software companies at this point, you know, where, you know, I would

45:01.860 --> 45:07.180
say all of Silicon Valley has, has this problem where, where, you know, the, they

45:07.180 --> 45:12.460
have to pay the, the coders, the computer programmers more and more. And so, yeah,

45:12.460 --> 45:16.060
there's, you know, there's definitely a need for technology to replace the people

45:16.060 --> 45:20.220
and would make the businesses more profitable. And I would, I would have scored

45:20.500 --> 45:25.460
that as probably a productivity enhancing, generally positive thing. It doesn't

45:25.460 --> 45:33.700
show up in the computer science labor market. So, and then, and then I think,

45:33.700 --> 45:39.140
look, the, the, the big thing, the biggest, the biggest AI technology that's

45:39.140 --> 45:43.540
actually being used is TikTok. And we need to be, we need to be talking about

45:43.540 --> 45:48.020
that. And I, you know, I, I, I gave you two, you know, very different ones. One is,

45:48.060 --> 45:51.900
one is that it's emergent. It's not like an intentional weapon. It's, it's, it's

45:51.900 --> 45:56.060
just emergent. And it's just, you know, we have a tendency to get deranged. And

45:56.500 --> 46:01.460
this is, this is what happens even though it seems to derange us. And then, and

46:01.460 --> 46:05.780
then, but nobody, nobody's in China to double check the algorithms. The ones

46:05.780 --> 46:08.820
they're training on the US are very different from the ones they train on

46:08.820 --> 46:14.540
people in China. You don't get, you don't get videos that, that make people, you

46:14.540 --> 46:19.180
know, radical, that radically undermine the belief in the society in China like

46:19.180 --> 46:22.500
you do in the US. And that, that, that suggests to me that we should at least

46:22.500 --> 46:24.540
be asking this intentional question.

46:24.940 --> 46:28.020
Very, very fair point. I, I'm certainly not trying to make the argument that

46:28.020 --> 46:33.340
TikTok is a productivity helper. And it could well be insidious and evil as

46:33.340 --> 46:33.580
well.

46:34.780 --> 46:38.500
My intuition is that it's, it's the, it's the biggest thing in AI. So if I had a,

46:38.500 --> 46:42.980
if you had to wait them, how big they are, I think TikTok is the biggest thing.

46:43.980 --> 46:48.620
We have a microphone here if folks are interested in asking questions to Peter.

46:48.620 --> 46:51.980
And if you can look this way towards the cameras, that would be helpful as well.

46:52.980 --> 46:57.300
Thank you. And thank you, Peter, for that most precious of gifts, your time. And I

46:57.300 --> 47:02.380
for one, I'm willing to pay the time price for it. Given our addiction.

47:02.380 --> 47:05.420
Watch it, who are you? Give it, now it's your.

47:05.940 --> 47:06.500
I'm nobody.

47:07.260 --> 47:07.740
How are you?

47:07.740 --> 47:11.060
I'm Stephen's. I went to your first conference.

47:11.340 --> 47:15.580
I know, but you've got to give your name when you speak at the microphone.

47:15.580 --> 47:18.100
We don't believe in anonymity around here.

47:19.460 --> 47:22.740
Very good. And you would go to the core of the question, which is the individual

47:22.780 --> 47:26.660
versus the identity. I stand as an individual to ask you this question.

47:27.420 --> 47:32.820
Given our addiction to narratives like AI and given our aversion to knowledge in

47:32.820 --> 47:38.820
favor of miss mill and all kinds of misinformation and given our emotional

47:38.860 --> 47:43.580
overload of the past few years, as a student of Ren Girard,

47:45.020 --> 47:51.660
my question is, will we have a mimetic pandemic where virus is the vengeance

47:51.660 --> 47:53.660
and could AI help or hinder it?

47:55.740 --> 47:56.740
There's one for you.

47:59.620 --> 48:05.940
You know, um, yeah, this is, I mean, this is certainly, um, this is certainly a

48:05.940 --> 48:09.540
read on what is very haywire about

48:10.580 --> 48:17.620
tech talk that it, you know, it just gives people what they mistakenly think they want.

48:18.260 --> 48:23.780
You know, they, they, um, and, um, and, uh, and, um, and then, um, and then, you know,

48:23.780 --> 48:27.660
that's, that's often, you know, that's often somehow, somehow a bad thing.

48:27.660 --> 48:32.740
I, um, I don't know, you know, I look, I think there is, there are, there are, um,

48:32.820 --> 48:39.580
there are ways in which, you know, I wouldn't, um, I wouldn't, um, I'm always,

48:39.580 --> 48:43.380
I'm always, I'm hesitant to sort of blame tech for everything that's wrong in our

48:43.380 --> 48:49.820
society. Uh, but I do think, I do think there's, there's something about, um, you

48:49.820 --> 48:54.060
know, it's, if we, if, and it's, it's, it's wrong to scapegoat and say it's the

48:54.060 --> 48:59.780
single thing that causes everything to go haywire, but, uh, but at the margins, you

48:59.820 --> 49:05.260
know, is it, is it helping us get, does, does, does, does, um, you know, there's

49:05.260 --> 49:09.900
something about, um, about a lot of these sort of short packet content forms that

49:09.900 --> 49:11.780
has, you know, has deranged this course.

49:11.780 --> 49:17.660
I think, I think TikTok is, is, is by far the worst, um, and, um, and this is sort

49:17.660 --> 49:22.180
of a sense in which the, the, you know, Silicon Valley is getting some, some of

49:22.180 --> 49:26.020
the blame for, um, uh, for, for all this stuff.

49:26.420 --> 49:29.300
And then, you know, I think the alternatives also didn't work because, you

49:29.300 --> 49:32.180
know, the alternatives were, you know, a centralized media system, which

49:32.180 --> 49:33.140
everything was controlled.

49:34.100 --> 49:34.540
Sir.

49:34.900 --> 49:40.220
So I, I, I think, I think you have to, if you frame the social problem, it's, um,

49:40.580 --> 49:45.780
it's, um, you know, it's too much totalitarian centralization versus

49:46.060 --> 49:47.780
deranged decentralization.

49:47.820 --> 49:52.060
And you have to, you have to think of both problems and there's, you know, there

49:52.060 --> 49:56.740
are instances of both that we had in the COVID epidemic the last two, three years.

49:56.740 --> 50:00.420
We had, you know, decentralized conspiracy theories that were not helpful.

50:00.780 --> 50:05.260
And we had a centralized narratives that cut off, uh, cut off much needed debate.

50:06.740 --> 50:06.980
Hi.

50:06.980 --> 50:07.500
Thank you.

50:07.500 --> 50:08.540
Uh, I'm Dr.

50:08.540 --> 50:10.620
Jeff Garneson from Anchorage, Alaska.

50:10.700 --> 50:11.980
This is my second cousin.

50:12.700 --> 50:20.500
Um, 60 years ago, CS Lewis wrote an article in the Saturday evening post called,

50:20.500 --> 50:25.700
uh, screw tape proposes a toast where he criticized, uh, the dumbing down of

50:25.700 --> 50:29.380
American education and I chair an educational foundation.

50:29.380 --> 50:33.460
And I'm very concerned about the quality of, uh, U S education.

50:33.900 --> 50:39.860
And, uh, and, uh, as you talked about, and what can we do about, uh, educating

50:39.860 --> 50:44.860
our youth in, uh, at least in public education, uh, all my colleagues send

50:44.860 --> 50:46.500
their kids to private schools now.

50:46.900 --> 50:49.460
And so I just wanted to see if you could comment on that.

50:51.260 --> 50:55.460
You know, I, I mean, I think there are a lot of people who articulated the,

50:55.460 --> 50:59.460
the issues quite, uh, quite, quite strongly.

50:59.460 --> 51:05.660
I don't have, you know, I don't have, um, I know, I, I've got much to add to it.

51:05.700 --> 51:13.620
Um, I, I do think, um, I do think it's sort of like always a question, you

51:13.620 --> 51:15.300
know, what are people doing?

51:15.300 --> 51:16.980
What is the teleology of it?

51:17.060 --> 51:23.340
And, you know, I think a healthy, you know, primary, secondary, tertiary

51:23.340 --> 51:27.740
education system is, you know, it's, it's supposed to, um, make you a

51:27.740 --> 51:33.020
well-rounded educated person, become a functioning citizen, our society.

51:33.420 --> 51:39.820
Um, uh, you know, if you, if you go into research or academia or certain

51:39.820 --> 51:44.820
industries, it is, uh, for you to become be a creative person who sort of

51:44.820 --> 51:51.300
pushes the frontiers of, of, of knowledge and, um, and in some sense,

51:51.540 --> 51:57.660
you know, it has, it has, um, it has, it has, it has somehow gotten, gotten

51:57.660 --> 51:58.260
deranged.

51:58.260 --> 52:02.820
And, uh, again, I don't want to blame it on AI or make this the, the

52:02.820 --> 52:07.980
single focus, but if you, if you have a narrative out there that, uh, that in

52:07.980 --> 52:13.460
the future, AI will do all the thinking for you and you don't need to think for

52:13.460 --> 52:15.300
yourself in any way.

52:15.780 --> 52:23.220
Um, you know, you know, maybe, maybe, um, maybe, you know, you don't need to

52:23.860 --> 52:24.660
learn as much stuff.

52:24.660 --> 52:25.780
You don't need to memorize things.

52:25.780 --> 52:28.500
You don't, there's sort of all this, these things that are considered

52:28.500 --> 52:33.220
education that, that, that seemed to be, um, seemed to be much, uh, much less

52:33.220 --> 52:33.860
important.

52:33.860 --> 52:39.380
And, um, and I, I, yeah, I, I do wonder that's, that's sort of, that's the

52:39.380 --> 52:42.100
broader context in which a lot of these things are operating.

52:42.180 --> 52:46.660
That it's sort of like, um, you know, if you, if you say it's not the public

52:46.660 --> 52:49.700
school system is not completely deranged, it's just trying to sort of make

52:49.700 --> 52:54.340
people these passive cogs in this very large machine.

52:54.740 --> 52:59.620
Um, you know, that's, that's sort of the, uh, the way in which it fits into

52:59.620 --> 53:01.300
this dystopian AI narrative.

53:01.300 --> 53:04.900
And, um, and we need to, yeah, we need to tackle this problem on all these

53:04.900 --> 53:05.460
different levels.

53:05.460 --> 53:06.340
Hi, Peter.

53:06.340 --> 53:07.700
Um, my name is Rick with Mivium.

53:07.700 --> 53:10.820
Um, I, I heard you give a talk at Stanford a few months back.

53:10.820 --> 53:14.820
And so this is the second time I heard you mention a lack of progress, uh, in, uh,

53:14.820 --> 53:15.700
on the atom level.

53:16.260 --> 53:20.820
So, uh, coming from the, uh, semiconductor material science field, uh, you

53:20.820 --> 53:23.860
know, I, I wanted to see what you mean by that because all we do is manipulate

53:23.860 --> 53:27.380
atoms on a day-to-day basis at scale from atomic layer deposition to molecular

53:27.380 --> 53:29.460
beam, epitaxial growth to mechanical chemistry.

53:29.460 --> 53:32.900
That's the only way we can make these next gen semiconductors, uh, these new

53:32.900 --> 53:36.100
materials that can basically replace silicon and free us from any kind of

53:36.100 --> 53:38.980
independent dependence from China and raw materials.

53:38.980 --> 53:42.740
So for me in my field, it, it's not a lack of progress.

53:42.740 --> 53:46.100
It's more the incumbent thought, school of thought out there is all chemistry

53:46.100 --> 53:46.500
based.

53:46.500 --> 53:49.780
Everybody wants to do with chemistry and toxic chemicals, uh, and things like

53:49.780 --> 53:49.940
that.

53:49.940 --> 53:53.300
Whereas we're trying to do it different way, but we face a lot of resistance.

53:53.300 --> 53:55.620
The barrier of entry and the cost is also very high.

53:55.620 --> 53:57.860
So I want to know what you mean by lack of progress.

53:58.580 --> 54:05.780
Well, it's, it's, it's, it's, um, it's certainly, um, slowed in, in a lot of

54:05.780 --> 54:06.740
fields.

54:06.740 --> 54:10.900
Look, I think, I think the, the complicated version I would tell is that we've had,

54:10.900 --> 54:17.620
you know, a decent amount of progress in the world of bits the last 40, 50 years.

54:18.500 --> 54:21.860
I think semiconductors are sort of the, the in-between thing.

54:21.860 --> 54:26.500
And then most other fields have been disappointingly, at least slow.

54:26.500 --> 54:30.180
You know, when I was an undergraduate at Stanford, I was class of 89, um,

54:31.220 --> 54:34.420
you know, in, in retrospect, it wasn't obvious at the time, but in retrospect,

54:34.420 --> 54:36.420
you were supposed to study computer science.

54:37.300 --> 54:39.700
All the engineering fields were bad fields to go into.

54:39.700 --> 54:42.660
It was a bad idea to go into aero astro.

54:42.660 --> 54:44.340
It was a bad idea to go into nuclear engineering.

54:44.340 --> 54:45.540
If people knew that, they didn't do that.

54:46.180 --> 54:50.500
It was a bad idea, mechanical, chemical, you know, all the sort of, uh, world of

54:50.500 --> 54:51.700
atom stuff was bad.

54:51.700 --> 54:56.660
I think the one that was still in, in between that, that worked okay, but much less well

54:56.660 --> 54:59.380
than computer science was electrical engineering.

55:00.180 --> 55:04.020
And, um, and, and so, you know, the, if we, if we look at, you know,

55:04.020 --> 55:08.980
how well have the companies done, how well have the people gotten paid, um, you know,

55:08.980 --> 55:14.420
EE has done better than other fields, but a lot less well than, um, than computer science.

55:14.420 --> 55:20.500
I, I think on some level, it reflects the ways in which the progress is hard.

55:20.500 --> 55:24.820
It's, it's, it's, it's slower than it was in the 80s and 90s, but by various measures,

55:25.700 --> 55:27.620
it requires enormous scale.

55:27.620 --> 55:29.860
So, um, the role for the individual is less.

55:29.860 --> 55:35.700
You know, if you have to have a $500 million ASML, you know, machine to do the lithography,

55:35.700 --> 55:41.460
you know, um, it's harder to start, you know, it's hard to start a new, um, semiconductor company.

55:42.420 --> 55:46.900
And so, yeah, the ecosystem is shifted towards, you know, much bigger businesses dominating it.

55:48.340 --> 55:55.220
And, and, you know, certainly as a venture capitalist, um, uh, I, we've done virtually

55:55.220 --> 55:56.740
no investing in semiconductors.

55:56.740 --> 56:02.020
You know, I, I think, I think I should, but, um, you know, it's, we do so little that we

56:02.020 --> 56:04.100
don't know enough about it to do it.

56:04.100 --> 56:07.700
And that makes me think that it's, um, you know, there's some things happening,

56:07.700 --> 56:11.300
but it's, it's, it's still, uh, it's still a lot slower than it was in the past.

56:12.340 --> 56:14.260
This is, this is the, this is the challenge with China.

56:14.260 --> 56:17.380
It's not like China is copying the West.

56:17.380 --> 56:19.140
They will eventually catch us up.

56:19.140 --> 56:24.500
And so if we are progressing slowly, they will eventually be able to copy things and converge.

56:24.500 --> 56:30.340
And, uh, I think the sort of rate at which we're doing new things is not so great

56:30.340 --> 56:32.180
that China will not be able to converge.

56:32.820 --> 56:32.980
Yeah.

56:32.980 --> 56:35.300
But China has no access to the ASML machines.

56:35.300 --> 56:37.220
Uh, they're still doing a chemical method.

56:37.220 --> 56:41.860
They're probably, they're, it's, it's possible that it's, uh, it's still going to be

56:42.580 --> 56:47.300
non-trivial for them to catch up, but it's, it's, it's not as though, you know, we have this

56:47.300 --> 56:53.300
exponentially growing lead that, uh, you know, um, it's, it's, it's, it's not quite as strong as that.

56:53.300 --> 57:00.980
Thank you, Peter, for your provocative remarks that are going to precipitate many discussions

57:00.980 --> 57:02.340
for the next two days.

57:02.900 --> 57:12.500
I, and, uh, uh, including about, uh, all these issues surrounding technological progress.

57:12.500 --> 57:13.540
And thank you.


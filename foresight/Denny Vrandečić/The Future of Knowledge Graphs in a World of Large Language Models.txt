I was invited by François Chavre to keynote one of the days of the Knowledge Graph
Conference in May 2023 in New York, New York. This is a post-conference recording
of the talk. François asked me what topic I want to talk about. I said wiki data
and wiki functions and he said, do you have something else to talk about? You
talked about it the last time you were here at the conference in 2019 and I
said there's one topic everyone talks to me about and this is what about
knowledge graphs and large language models and he said perfect do that and
I thought great given how much I have been talking and thinking about it I
probably should have a good idea what I want to say by May. Well it's May now and
I'm still full of doubts about what to say. I recently stumbled upon this quote
looking at books for my daughter and it gives me some hope for the talk today.
When in doubt you can't be wrong. So let this be my first disclaimer for today. I
am most certainly in doubt. I don't know what the future of knowledge graphs is
but I have been talking about this for a decade now for a lot of very smart
people so I hope that what I say will prove your time. There is one thing I
have no doubts about. Something big is happening. See this is from the book
Diffusion of Innovation. It shows how technology is adopted. Well my brother
who lives in a small village of 180 people on a Croatian island he is
roughly here on this chart but he called me last fall to talk about stable
diffusion the image generation model. Chatchity P a large language model
reached 100 million users two months after launch. This is a record in how
fast the technology is spreading. If there's one thing we don't need to be
in doubt about is that these technology are having a large impact on the world.
Last year in September that's two months before the release of Chatchity P a
number of researchers and practitioners in the knowledge graph field met in
Dachstuhl in Germany. Our goal was to discuss the role of knowledge
engineering in the 21st century like we were almost under a kind of shock
talking about it processing and trying to figure out what it really means for
us. I'm trying to distill some of the results from that seminar and also from
any other conversations I had in this talk but you can also just go and read
the report directly. My second disclaimer nothing in this talk is generated by an AI.
And this explicitly marked so or you see a screenshot from an AI. This is not one
of those talks where we have a reveal at the end that all of this was generated.
And finally the last disclaimer this is a focused talk it is only about knowledge
graphs and large language models. It is not about the ethical questions of
large language models. It's not about blunders of current LLMs. It's not of the
that those systems will have about the impact on jobs and the economy. It's not
about questions of copyright or whether LLMs are a path to destruction of the
human race. All of these questions are interesting and deserve their own talks
and I will not talk about them today. So what makes me believe that there is a
future for knowledge graphs in the world of large language models? About six or
seven years ago I was working at Google on the knowledge graph. Back then the
knowledge graph wasn't necessarily a generic term yet. It was a big question
Google had to ask itself. The knowledge graph costs a lot of resources, a lot of
money, a lot of manpower to run and maintain. A lot of people were working on
the knowledge graph. Is all of this work going to be obsolete? Should we stop
investing in that? I give you the answer that I came up with back then. Let's
think first principles. So what is a knowledge graph? A knowledge graph represents
things and the relations usually stored in a graph database. WikiData is an
example. WikiData is the largest publicly available free knowledge graph that
anyone can edit. What is a large language model? Model here means that we have a
neural network, nodes connected with each other with weights. The weights in
this neural networks are trained on some input and desired outputs in order to
create good outputs on novel inputs. They are language models because they are
trained on natural language texts and it is a large language model because it is
trained on lots and lots of natural language texts. GPT is a family of large
language models which have been created by OpenAI. GPT-4 is the current version
but OpenAI has not published much data about it. So the following data is for
GPT-3. GPT-3 has 96 such layers in the picture and it had 175 billion
parameters. That is weights. As a large language model, GPT-3 has been trained to
provide appropriate answers to a given prompt to our input. So for example, given
the following input or prompt, who created the School of Athens.
JGDP answers, the School of Athens is a famous fresco painted by the Italian
Renaissance artist Raphael. It was painted between 1509 and 1511 as part of a
commission for the Vatican's Apostolic Palace in Rome. The fresco is widely
regarded as one of the greatest works of art of the high Renaissance. This answer
is not only correct, it is brilliant. There's absolutely nothing to complain
about. It gives context, it answers the question. It's great. If you go to Google
and ask the same question, the answer is the same, Raphael. But the context is even
more amazing. A picture from Raphael, his biography, data about him, who influenced
him, other artists that are being searched for. The answer is just beautiful and
amazing. You can also go to Wikidata and write a sparkle query, asking for the
creator of the School of Athens. Again, you will get the result, Raphael. Not as
pretty as Google, not as much context as with JGDP. But here's one thing, JGDP took
about five seconds to answer my question. Admittedly, it gave a lot of context I
didn't ask for, but even just passing the question took close to a second. Google
took half a second, including passing, but it also got a lot of great context and
gave me results from the search index, even though they didn't make it to a
screenshot. Wikidata took about a quarter of a second to produce the answer, and
that's not all. JGDP is running on the fastest and most expensive hardware you
can buy. Google is running on the fastest and most expensive hardware you
cannot buy. And the Wikidata query service is running abandonware on a
server somewhere. And if you think about this, it is not surprising. The query I
asked had six tokens of input. A token is roughly a word for a natural language
model. And the answer produced 60 tokens. Now, if the answer would just have been
Raphael, it would still have been at least two tokens. Every single token runs for
the 96 layers of JGDP, branching out to 175 billion weights and multiplying
matrices and soft maximum results. Whereas in Wikidata, we look up an item out
of 100 million and find a key out of 10,000 to get the results values. Those
are both logarithmic operations. It's just much cheaper. Given this paper,
getting and hosting your own copy of Wikidata, 1,000 Wikidata queries, like
the ones I just asked, would cost me about 14 cents in the cloud. The authors thought
this was too expensive and bought their own hardware to make it even cheaper.
Whereas if I look at Azure pricing for GPT-4, a thousand queries, as asked before,
would cost $7.56. So that's 14 cents compared to more than $7, so a factor of 50.
50 times more expense can make a difference. Now, to make it very clear, I don't know
how robust these numbers are. I would love to see more robust numbers and I expect
the cost for inference to go down. But even Sam Altman, the CEO of OpenAI,
the company making JetGDP, describes the compute cost of JetGDP as eye-watering.
And don't forget, he made a really good deal with Microsoft about running it on Azure.
John Hennessey, chairman of Google and former computer science professor at Stanford,
so he really knows what he's talking about, said that running at JetGDP, like Google,
would be 10 times more expensive. And that's just talking about the inference costs. Each
of these models also need to be trained. And current state-of-the-art language models cost
millions of dollars to train. For GPT-4, the cost was given as more than $100 million.
The cost for GPT-3 was around $4 million. Good news is, you probably don't have to do this
training. But you can just hopefully reuse an existing open source model that you can find
due to your task. This will be considerably cheaper to train a model from the foundation on.
But not for inference. The same cost that we saw earlier remains.
Here's some interesting new thoughts. Some folks, like Sam Altman, think that the age of
large language models is already over. OpenAI says it's because of diminishing returns of further
reading. Guess what? After reading a million books worth of text, you don't seem to learn too much
new stuff. He said that OpenAI is not working on GPT-5, but that they want to explore new ideas.
And that's great. Because with models that are more readily available out there, such as Metas
Llama, we can see new ideas happening very fast. For example, when Llama was leaked within days,
someone managed to run it on consumer hardware. A few days later, we got it running on a phone,
and then a Raspberry Pi. It was very slow, but it ran. And by the way, as ridiculous as that sounds,
this still means that these giant models are more expensive to run than a knowledge graph.
And really, it's not just cost, right? There are a few challenges that large
language models need to overcome. We need new ideas for those. Let's take a look.
Here's one thing that surprised me a lot. A few weeks ago, I stumbled into one of those Wikipedia
rabbit holes where I got temporarily obsessed with figuring out one specific fact. The correct
place of birth of a not-too-well-known Croatian actress, Anna Begovic. I was surprised that
Google and Wikipedia Wikidata were giving different answers, so I tried to figure out
the truth and fix it. Here's a screenshot of Google answering the question. Begovic was born in
Terpan. Wikipedia used to say split. After a bit of hunting through sources, I figured out that it
mostly likely is Terpan, though. And that a lot of the sources were copying from Wikipedia and Wikidata
and became contaminated by Wikipedia and Wikidata. This is an example of our knowledge ecosystem being
substantial danger, by the way, and that long before we have LLM syndemics.
Now, if you go to Bing Chat, which is powered by OpenAI's GPT, but has access to web search
and ask for the place of birth of Anna Begovic, it also answers me Terpan, which is great,
and it gives me free sources for that answer. It's just very disappointing that if you follow
one of the references to Wikipedia, you will actually find it says split.
When I asked ChatGDP instead of Bing Chat, I get a different answer. It answers split.
This is not too surprising. After all, Wikipedia was claiming split until just a few weeks ago,
and ChatGDP was trained on a 2021 copy of the web. Corrections to Wikipedia done in 2023
would not show up. Split is totally expected as an answer here.
What is interesting, though, is if I ask the same question in Croatian,
I get a different wrong answer. Zagreb. Now, that's an interesting answer,
because it demonstrates us two things. First, knowledge in ChatGDP seems to be not stored in
a language independent way, but is stored within each individual language. Depending on the language
I ask the question in, I receive a different answer. Also, the English answer at least has
support in the training data. Wikipedia did say split. The Croatian answer, where does Zagreb come
from? I can find a single source that says that Anna Begovic was born in Zagreb.
But here's the thing. What if ChatGDP is actually just guessing, it's just making it up?
What is the probability of Zagreb given the prior of Croatian actress?
In order to figure that out, I want to check Wikipedia. I ask ChatGDP to give me a sparkle
query for Croatian actors in the places of birth. The query it returns to me is good enough for our
purpose. I can just copy and paste it. It is subtly wrong if you read it in detail. It does not ask
for Croatian actors, but for actors born in the place in Croatia. But again, good enough for our
purposes. But still, this is fascinating. ChatGDP got out of the box all of the QIDs right in this
query. For Croatia, for actor, it got a property ID right. It can make sparkle queries that are
syntactically right. And all of that with zero shot extra training. There was no look up on the web.
ChatGDP just knows these QIDs by heart. In fact, you can totally ask ChatGDP to make you a table
of all the countries of the European Union and their QIDs. It has a lot of QIDs memorized.
Just you never know if maybe one of them is wrong or not. The query can be copied and pasted right
into the Wikipedia query service. And it runs giving 445 answers. And you can already see in the
screenshot that the first few are all in Zagreb. So now we can see that of the 445 Croatian actors
for place of birth, 154 are born in Zagreb, about a third. This gives a pretty good conditional
probability for the birthplace of Croatian actors. ChatGDP is guessing the place of birth
for Anna Begovic in Croatian, even though it knows it in English. This leads me to something my
manager at Google used to say. You can machine learn Obama's birthplace every time you need it,
but it costs a lot and you're never sure it is correct.
Another interesting observation is that GPT isn't particularly good with knowing what it knows.
For example, here we are asking for cities with mayors born after 1998. I wanted to ask something
that I expected to be not trivially Google-able, where there wouldn't be a listicle or table on
the map. So ChatGDP correctly points out that that would make them younger than 23 at its cutoff
80 date of 2021. So it is unlikely that anyone would be mayor at that age. Asking Bing Chat,
it also says that it doesn't know anything about mayors born after 1998.
Let's check Wikidata. I again used ChatGDP to help me write a query, although this time I
had to make a few fixes. And indeed, Wikidata has an answer. It tells me about Christian von
Waldenfelds, who was born in April 2000, mayor of Lichtenberg in Bavaria, Germany.
He became mayor in 2020, well before the cutoff date of Chaterity, by the way.
This is no endorsement of his politics or his platform he is running, by the way.
Now that we know the answers, we can guide Bing Chat and get the mayor of Lichtenberg and his age.
And obviously this answer is inconsistent with its previous answer,
but ChatGDP or BingJDP are both completely unaware of this inconsistency and don't care.
Large language models are not yet graded being consistent, whether about individual facts
or across different languages. They are also not always very good at math. But even if they were
good at math, we have to answer the same question that we do for looking up facts in a knowledge
graph. Why would you ever use 96 layers, 175 billion parameters model, to do multiplication?
When that's something you can do in a single operation on your CPU.
Why internalized knowledge in an LLM if you can externalize it in a graph store and look it up
when needed. Don't get bedazzled by the capabilities of large language models.
Autoregressive transformer models such as ChatGDP are touring complete and they are just a very
expensive reiteration of touring's carpet. You can do everything with them, but it doesn't mean you
should. Use LLMs where they are efficient and use other things where they are better.
One possible solution to this capability gap are so-called augmented language models.
Toolform are being a particularly well-known example. Or for ChatGDP, that's what ChatGDP
plugins are there for. The idea is that we can enrich large language models with additional
systems which are good and efficient at specific tasks, such as math or other functions,
from wiki functions, or looking up facts or query results in a knowledge graph such as wiki data.
I mean, given that ChatGDP already can, zero shot, create queries against wiki data,
there isn't that much to do to make them work together.
Some folks think that some mapping of strategic nodes into a knowledge graph with embeddings,
we can easily connect a knowledge graph directly to a large language model.
But we don't even need to do that, we can just use the Sparkle query generation ability
directly and ask queries against wiki data. And not only can we connect LLMs to a knowledge
graph, but also to a repository of functions, such as wiki functions. Both knowledge graphs and
functions would be tools the LLM can learn to use. There is work in trying to understand
how knowledge is stored in the parameters of a large language model. And when we look at this
work, we start to understand why large language models are large. Do they really have to be this
large? Let's compare with stable diffusion one. Stable diffusion one is a text to image generator.
It has to understand natural language prompts, just as GPT does. It can make an image out of
basically any prompt. It can also generate the image of a good number of celebrities.
Here, for example, I'm asking for the Pope, Geleta Thunberg, Idris Elba, Michelle Yeo,
Helen Mirren, and Yanle Kuhn to explain knowledge graphs with the help of a whiteboard.
So GPT-3 had 175 billion parameters. How many parameters do you think are in stable diffusion
one, knowing all these people being able to generate them? 890 million parameters.
Now, I think 890 million is a lot. But GPT-3 is about 200 times larger than stable diffusion one.
And it's no surprise. Think of it. All the knowledge that we were using for questions
answering so far. The embedded QIDs, what are the member countries of the EU,
the place of birth of individual people. I mean, if it has NABegovitch, I'm sure GPT-3 knows the
place of birth of millions of individuals. All this taught in many different languages.
All of this is taught in those hundreds of billions of parameters.
You don't need that for text generation. But you need it if you want to answer all
these questions we have been asking. And indeed, Metas Lama, which came out a few weeks ago,
is considerably smaller than GPT-3, about 25 times smaller. But it seems to be
rather competitive in terms of language understanding and fluency. So yes, we could
internalize all the 1.4 billion statements in Wikidata into a large language model.
But what if we go the other way around and try to externalize the knowledge model instead?
If we leave the language model to deal with language, but push the knowledge
to a knowledge graph or a different knowledge model, in a world where language models can
generate infinite content, knowledge becomes valuable. And that takes us back to Jamie Taylor's
rule. We don't want to machine learn Obama's place of birth every time we need it. We want to
store it once and for all. And that's what knowledge graphs are good for. To keep you
valuable knowledge safe. I am of the strict belief there is no reason to ever again
manually enter the place of birth for Anna Begovic. This makes knowledge for Wikidata
both valuable and a public good. The knowledge graph provides you with the ground truth for your
language models. By the way, the other way around is also true. Large language models can be an
amazing tool to speed up the creation of a knowledge graph. They are probably the best tool for
knowledge extraction we have seen developed in a decade or two. We want to extract knowledge into
a symbolic form. We want the system to overfit for truth. And this is why it makes so much sense
to store the knowledge in a symbolic system. One that can be edited, audited, curated, understood.
We can cover the long tail by simply adding new notes to the knowledge graph. One we don't train
to return knowledge with a certain probability to make stuff up on the fly. But one where we can
simply look it up. And maybe not all of the pieces are in place to make this happen just yet.
There are questions around identity and embeddings. How exactly do they talk with each other?
But there are good ideas to help with those problems. And knowledge graphs themselves should
probably also evolve. I want to make one particular suggestion here. Freebase, the Google Knowledge
Graph, Wikidata, they all have two kinds of special values or special statements. The third one is
the possibility to say that a specific statement has no value. Here, for example, we are saying that
Elizabeth I has no children. The second special value is the unknown value. That is, we know
that there is a value for it, but we don't know what the value is. It's like a question mark in
the graph. For example, we don't know who Adam Smith's father is, but we know he has one. It could
be one of the existing notes. It could be one that we didn't represent yet. We have no idea.
My suggestion is to introduce a third special value. It's complicated. I usually get people
laughing when I make this suggestion, but I'm really serious. It's complicated is what you would use
if the answer cannot be stated with the expressivity of your knowledge graph.
This helps with maintaining the graph to mark difficult spots explicitly. This helps with
avoiding embarrassing wrong or flat out dangerous answers. Given the interaction with LLMs,
this can in particular mark areas of knowledge where we say, don't trust the graph. Can we instead
train the LLM harder on this particular question and assign a few extra parameters for that?
But really, what we want to be able to say are more expressive statements. In order to build
a much more expressive ground truth, to be able to say sentences like these,
Jupiter is the largest planet in the solar system. That's what we are working on right now.
With abstract Wikipedia and leaky functions, we aim to vastly extend the limited expressivity
of Viki data so that complicated things become stateable. This way, we hope to provide a ground
truth for large language models. In summary, large language models are truly awesome. They are
particularly awesome as an incredibly enabling UX tool. It's just breathtaking, honestly,
things are happening which I didn't think possible in my lifetime. But they hallucinate.
They need ground truth. They just make up stuff. They are expensive to train and to run.
They're difficult to fix and repair, which is great if you have to explain someone,
sorry, I cannot fix your problem. The thing is making a mistake, but I don't have a clue how to
make it better. They are hard to audit and explain, which in areas like finance and medicine is crucial.
They give inconsistent answers. They struggle with low resource languages.
And they have a coverage gap on long tail entities, which is not easily overcome. All of these
problems can be solved with knowledge graphs, which is why I think that the future of knowledge
graphs is brighter than ever, especially thanks to a world that has large language models in it.
Thank you for your attention. Thanks to all the people who helped me clarify my thinking
around this topic. And if you have any questions, feel free to put them in the comments.

WEBVTT

00:00.000 --> 00:14.200
Hello and welcome, everyone. This is Active Inference Mathstream 9.1 on March 5, 2024.

00:14.200 --> 00:20.640
We're here with Jonathan Gord and we'll be discussing a variety of topics yet to be determined

00:20.640 --> 00:25.960
or are they? So thank you for joining and to you for any introduction and we'll really

00:25.960 --> 00:29.680
look forward to everyone's comments and questions. So thanks again for joining to you.

00:30.320 --> 00:35.040
Okay, well, yeah, thanks very much, Daniel, for the introduction and for inviting me to be here

00:35.040 --> 00:44.080
on Active Inference. I'm looking forward to a very, very fun discussion. So I don't have anything

00:44.080 --> 00:47.280
especially prepared to talk about, which is probably a good thing because it means we'll be

00:47.280 --> 00:51.840
able to extend the kind of the unstructured part of this for as long as possible. But I think just

00:51.840 --> 00:57.120
to give a little bit of context, I want to talk about an area where I think some things that I've

00:57.200 --> 01:02.400
been working on, some collaborators that might have been working on, that might have some kind of

01:02.400 --> 01:07.360
intersection of interest with things that, you know, Active Inference type people might care about,

01:07.360 --> 01:13.280
right? So, and in particular, that concerns the relationship between kind of computation,

01:13.280 --> 01:18.640
observation, and cognition, and specifically using methods that come from category theory and

01:18.640 --> 01:22.960
topos theory and some other kind of branches of mathematics and theoretical computer science

01:23.040 --> 01:28.320
to understand the relationship between system, specifically the computational and algorithmic

01:28.320 --> 01:33.440
complexity of systems versus the computational algorithmic complexity of observers of those

01:33.440 --> 01:38.480
systems and how those things trade off between each other. So, so just to give a little bit of

01:38.480 --> 01:43.920
context to that, I want to show these are just some visuals from a paper that I put out about a

01:43.920 --> 01:48.800
year ago now, and that this kind of really defines this research program that I've been working on

01:48.800 --> 01:53.360
for the last year and a half in some form or another, which is looking at exactly this trade-off

01:53.920 --> 01:58.560
using category theoretic machinery. So, here's a specification of a Turing machine. This is

01:58.560 --> 02:03.280
just a simple deterministic computation. It's saying, you know, you have a Turing machine that

02:03.280 --> 02:07.680
has this head state and this tape state, and on the next step, you're going to replace the tape

02:07.680 --> 02:10.720
state with something that looks like this, the head state with something that looks like that,

02:10.720 --> 02:14.560
and you're going to scroll the Turing machine head left, or in this case, scroll it right,

02:14.560 --> 02:19.600
etc. So, this is just a, you know, specification of a very simple computation. I think this is a

02:19.600 --> 02:23.920
two-state, two-color Turing machine on a simple, you know, one-dimensional tape. It's about as

02:23.920 --> 02:28.320
simple a computation as you could define. So, if you run that thing for some initial condition,

02:28.320 --> 02:34.400
you'll get an evolution that looks like this. And so, right now, this is just a purely deterministic,

02:34.400 --> 02:39.760
you know, single-path evolution. But from this, we can construct, we can build a mathematical

02:39.760 --> 02:45.520
structure. Namely, we can build a category. So, and the rules for how we build that category are

02:45.520 --> 02:50.400
very simple. So, you know, each arrow here is some simple computation, some application of

02:50.400 --> 02:54.640
the Turing machine transition function. And then what we can do is we can say, well, any time we

02:54.640 --> 02:58.800
have two arrows that are laid end-to-end like this, we can compose them together to create a

02:58.800 --> 03:03.520
third arrow that goes like that. I may even have a picture, yes, like this. So, you know, we have

03:03.520 --> 03:08.800
a computation f that takes us from x to y, a computation g that takes us from y to z, and we

03:09.040 --> 03:13.920
then we obtain a composite computation g compose f that takes us directly from x to z.

03:13.920 --> 03:19.680
And we also add some additional edges, some additional arrows on each state itself,

03:19.680 --> 03:24.640
a sort of identity, an identity operation that maps the computational state directly to itself.

03:25.360 --> 03:31.520
And so, this combined with some axioms of associativity and identity forms a category

03:31.520 --> 03:35.840
of elementary computations. So, this is a very, very simple example. But what I want to try and

03:35.840 --> 03:42.240
build up towards and kind of pump your intuition for is a category which I call comp, which is a

03:42.240 --> 03:48.400
category whose objects are all essentially the class of all data structures and whose arrows or

03:48.400 --> 03:53.120
morphisms are the class of all elementary computations. So, you start by just applying, you

03:53.120 --> 03:56.720
know, all possible computations or, you know, in this case, for the case of Turing machines,

03:56.720 --> 04:00.640
all possible, you know, Turing machine transition functions. And then you do this

04:00.640 --> 04:05.280
closure operation where you, you know, where you essentially do what I'm doing here, but you,

04:05.280 --> 04:10.080
you know, you allow those elementary computations to be composed together in arbitrary ways.

04:10.080 --> 04:15.600
And so, that gives you effectively a class of all possible programs. So, this category contains

04:15.600 --> 04:20.240
not only all possible data structures as objects, but all possible programs as morphisms. And this

04:20.240 --> 04:25.840
is a very rich category with some very interesting algebraic structure that we'll kind of, again,

04:25.920 --> 04:32.160
I'm sure we'll allude to in our subsequent discussion. But in a sense, when we do this,

04:32.160 --> 04:36.080
when we do this operation of taking what mathematically we call a transitive closure,

04:36.080 --> 04:41.280
right, where we allow two elementary computations to be composed together to produce a third,

04:42.320 --> 04:46.160
we are essentially kind of neglecting considerations of computational complexity,

04:46.160 --> 04:52.160
right, because, you know, this arrow here might correspond to one application of the Turing machine

04:52.160 --> 04:56.320
transition function, this arrow might correspond to another application of the transition function,

04:56.320 --> 05:00.480
but this composite arrow correspond might correspond to two applications of the transition

05:00.480 --> 05:06.480
function. And so, somehow, when we allow arrows or morphisms to be composed in this way,

05:06.480 --> 05:11.280
we're neglecting considerations of the complexity of operations. So, the question then is, you know,

05:11.280 --> 05:16.560
could we imagine constructing a generalization of category theory, which takes into account

05:16.560 --> 05:21.200
computational complexity. So, here's an example of how that would look, right. So, here you can see

05:21.200 --> 05:25.440
every edge, every morphism has been tagged with certain computational complexity information,

05:25.440 --> 05:30.480
in particular, it's been tagged with a number specifying what is the minimum number of applications

05:30.480 --> 05:34.640
of my transition function, what's the minimum number of elementary computations that I need in

05:34.640 --> 05:38.400
order to evolve from this data structure to this data structure. So, here, to go from here to here,

05:38.400 --> 05:42.800
it's just one, to go from here to here, it's one, to go from here to here, it's one, etc.

05:43.600 --> 05:48.480
But to go from here to here directly, it would be three, to go from here to here directly,

05:48.560 --> 05:52.880
it would be two. And, you know, just for convention, we say that the identity,

05:53.600 --> 05:57.440
the identity computation, the trivial computation always has complexity zero.

05:58.640 --> 06:03.200
And then this, so this is, again, a fairly simple mathematical structure, and you can

06:04.400 --> 06:10.080
construct this, again, using purely category theoretic technology by building a particular

06:10.080 --> 06:15.520
functor from the category of computations, and from the category of data structures and computations

06:15.600 --> 06:20.160
to a what's called a discrete co-borderism category. And again, we might discuss that later on if

06:20.160 --> 06:23.680
people are interested, but let me not get too bogged down into the technical details of how we

06:23.680 --> 06:29.040
do that. But once you've got this, it gives us immediately a very nice way of characterizing

06:29.040 --> 06:35.760
phenomena like computational irreducibility. So, there is this idea that has existed in some form

06:35.760 --> 06:39.120
or another since the very early days of theoretical computer science, since the days of, you know,

06:39.120 --> 06:44.720
girdle and turing and post and church and so on, but was given this term computational irreducibility

06:44.720 --> 06:52.160
by Stephen Wolfram, where the idea is essentially that you just, you know, intuitively, you describe

06:52.160 --> 06:56.240
a computation as being irreducible, or, you know, the result of the computation as being

06:56.240 --> 07:02.400
irreducibly complex, if it's not possible to shortcut it in any way, right? So, where, you

07:02.400 --> 07:06.400
know, it takes that computation takes a certain number of steps, and there does not exist a

07:06.400 --> 07:10.720
shorter computation that would give you the same answer in less time. And one of the nice

07:10.720 --> 07:14.800
features of thinking about computations and their complexity algebraically like this is that it

07:14.800 --> 07:19.040
gives you a purely algebraic characterization of irreducibility. In particular, what it says

07:19.040 --> 07:24.720
is that irreducible computations are ones for which the computational complexity acts

07:24.720 --> 07:31.840
additive, purely additively under composition. So, if it's the case that if we compose, say, two

07:33.680 --> 07:40.400
computations of complexity one together, if the resulting composite takes, you know, has complexity

07:40.960 --> 07:44.960
two, then it's an irreducible computation. If it has complexity less than two, like one,

07:44.960 --> 07:48.720
then that means that we could have jumped directly from the input to the output

07:48.720 --> 07:51.840
without having to pass through the two elementary computations that made it up. So, that would be

07:51.840 --> 07:56.240
an example of a reducible computation. So, reducible computations are ones whose complexities

07:56.240 --> 08:02.560
compose sub-additively in this category theoretic sense. And, okay, so here's an

08:04.000 --> 08:09.600
illustration of showing what intermediate computational states you had to go through

08:09.600 --> 08:12.800
in order to get from one data structure to another data structure. So, to go from here

08:12.800 --> 08:16.720
to here, you had to go through steps one to two. To go from here to here, you had to go through

08:16.720 --> 08:22.720
steps one, two, and one, two, three, and four, et cetera. So, you can build up a kind of complete

08:22.720 --> 08:27.200
algebra of complexity this way, which has some nice properties, which, again, I can talk about,

08:27.200 --> 08:32.480
but let me not get too bogged down in mathematical details right now. But here's the thing I really

08:32.480 --> 08:37.440
want to talk about, which is what happens when you go to multi-way systems. What happens when

08:37.520 --> 08:42.400
you go to non-deterministic computations? So, now, imagine having, instead of just a

08:42.400 --> 08:47.440
Turing machine with a single rule, a single transition function that just evolves deterministically

08:47.440 --> 08:50.800
with a single thread of time, now imagine having a Turing machine that has, say, two

08:50.800 --> 08:54.880
transition functions, like this one and this one. And so, at any given point, it can apply one of

08:54.880 --> 09:00.960
the two. And so, now, evolution, instead of just being a single path, becomes this kind of branching

09:00.960 --> 09:05.520
structure, which, if we didn't have any merging, would be a tree, but because we are merging

09:05.600 --> 09:11.120
equivalent states, it's actually just a kind of more general directed graph. And so, it looks

09:11.120 --> 09:16.240
like this, and this we call a multi-way system. And so, we can build a category out of these

09:16.240 --> 09:22.240
multi-way systems as well. We can build a category using exactly the same rules. So, again, we do

09:22.240 --> 09:28.240
this transitive closure operation. So, we add an edge for every possible composition of these

09:28.240 --> 09:33.760
elementary computations and an identity edge that maps every data structure to itself. But it turns

09:33.760 --> 09:39.440
out this category has even more structure than the single-way system that we showed previously,

09:39.440 --> 09:45.360
because now, it's possible to compose computations not just sequentially in time using ordinary

09:45.360 --> 09:50.240
composition, but it's possible to compose them in parallel across what is sometimes referred

09:50.240 --> 09:57.040
to as branchial space. So, essentially, you're saying instead of, you know, the ordinary morphism

09:57.040 --> 10:01.520
composition that I showed previously is essentially saying, you know, I apply this

10:01.600 --> 10:06.880
elementary computation, then this elementary computation sequentially. Whereas this parallel

10:06.880 --> 10:11.200
composition is saying, I apply this computation and this computation in parallel to the same

10:11.200 --> 10:16.240
data structure. And so, that parallel operation is what causes these branches, right? Effectively,

10:16.240 --> 10:21.040
when you have two threads of time that are branching from the same state, like here,

10:21.840 --> 10:27.120
that's arising because we have chosen to apply this elementary computation and this elementary

10:27.200 --> 10:34.320
computation together in parallel rather than sequentialized in time. And so, here you can see

10:35.040 --> 10:40.160
this parallelization indicated using what's referred to as a branchial decomposition,

10:40.160 --> 10:44.400
which is just a kind of a visual way of decomposing what's going on between these different threads

10:44.400 --> 10:49.200
of time. And again, there's a purely algebraic characterization of what's going on here,

10:49.200 --> 10:54.000
which is that what we've done is we've taken our simple category that we started with,

10:54.000 --> 10:57.840
and we've equipped it with a tensor product structure. And so, it's become what we fancily

10:57.840 --> 11:02.400
call a monoidal category or actually a symmetric monoidal category. So, the tensor, so we now

11:02.400 --> 11:07.200
have these two operations. We have sequential composition in time, and we have this tensor

11:07.200 --> 11:14.080
product operation, which is a parallel composition in branchial space. And just like we can have,

11:14.080 --> 11:19.120
just like before, where we equipped our edges, our morphisms with certain computational

11:19.440 --> 11:24.640
complexity information, we can do the same thing, and we described how those complexities composed

11:24.640 --> 11:28.880
sequentially in time. We can do the same thing and describe how the complexities compose in

11:28.880 --> 11:34.560
parallel as one composes morphisms in branchial space. And so, this allows one by exactly the

11:34.560 --> 11:39.120
same token to quantify multi-computational irreducibility rather than just computational

11:39.120 --> 11:44.000
irreducibility. So, now, multi-computational irreducibility becomes a measure of how additive

11:44.000 --> 11:48.640
or sub-additive your time complexities are when you compose them in parallel through the tensor

11:48.640 --> 11:54.640
products rather than just in sequence through standard morphism composition. Okay, but I promise

11:54.640 --> 11:58.560
I am cut, and so here's an analogous diagram to the one I showed before showing all the kind of

11:58.560 --> 12:08.960
intermediate steps that are being applied when one constructs computations or indeed multi-computations

12:08.960 --> 12:13.920
by composing elementary computations both sequentially in time and in parallel in branchial

12:13.920 --> 12:19.840
space. Now, but I promise I am the point that I'm trying to get to is that it turns out that in

12:19.840 --> 12:23.840
addition to just being a useful way to think about computational complexity theory and to formulate

12:23.840 --> 12:28.240
complexity classes like, you know, polynomial time on non-deterministic polynomial time, etc.,

12:28.240 --> 12:31.680
it turns out this is also an interesting way to think about the role of observation

12:32.560 --> 12:39.760
in sort of computational models of reality. Because so here's where I'm going to get a

12:39.760 --> 12:44.400
little bit philosophical, and I don't immediately have a slide or a graphic that I can show to

12:44.400 --> 12:49.040
illustrate this point. But so when we think about modeling a system computationally,

12:50.320 --> 12:53.600
one has to bear in mind that there are really two computations going on, right? There's the

12:53.600 --> 12:58.080
computation that the system is itself performing, and then there's the computation that the observer,

12:58.080 --> 13:02.720
the person who is measuring that system and concluding things from it, there's the computation

13:02.720 --> 13:08.720
that they are performing. And somehow, you know, so when we construct models of reality or when we

13:08.720 --> 13:14.000
construct models of systems, you know, and we want to describe kind of at a meta level what we're

13:14.000 --> 13:18.960
doing in computational terms, there's our own computation that, you know, that's going on

13:18.960 --> 13:22.720
inside our own internal representation of the world. And then there's presumably some external

13:22.720 --> 13:27.120
computation that's going on outside. And then when we make observations and when we make measurements,

13:27.120 --> 13:31.440
when we construct theoretical models, what we're doing is we're somehow constructing some kind

13:31.440 --> 13:36.640
of encoding function that allows us to take a concrete physical state of the system we're

13:36.640 --> 13:41.760
observing and encode it as some abstract state of the internal model that we have of what's going on.

13:42.640 --> 13:48.720
And that's all very well. But then one, but then now we don't just have one computation to care

13:48.720 --> 13:52.480
about, we have three, right? We've got the computation of the system, computation of the

13:52.480 --> 13:56.960
observer, and the computation of this encoding function computation that's responsible for their,

13:56.960 --> 14:02.400
for their, you know, the interface between their internal model of the world and the external

14:02.400 --> 14:07.200
reality. And the computational complexities of these computations into play in an extremely

14:07.200 --> 14:11.520
interesting way. And so the, you know, part of the reason for trying to develop this algebraic

14:11.520 --> 14:15.360
semantics for thinking about computational complexity and multi computational complexity

14:15.360 --> 14:21.120
was to try to give one a systematic way to reason about exactly this three-way interplay between

14:21.120 --> 14:27.840
systems, observers, and encoding functions. And so in particular, when we make when an

14:27.840 --> 14:31.920
observer makes a model of the world, one thing that they're doing is that they are, you know,

14:32.160 --> 14:36.160
for any, for any model, isn't just, you know, a complete description of reality,

14:36.160 --> 14:38.960
there's a certain amount of coarse-graining, right? There's a certain amount of

14:40.960 --> 14:44.640
taking a bunch of states that in the system itself are distinguished,

14:44.640 --> 14:49.120
but in the internal model are treated as the same, they're kind of, you know, they're cast in the

14:49.120 --> 14:53.840
same bucket. So in some sense, you know, how coarse a model is, is a measure of how much the

14:53.840 --> 14:58.400
encoding function fails to be subjective, right? And so again, there's a kind of algebraic or

14:58.400 --> 15:04.720
category theoretic characterization of what's going on, that, you know, the fewer of your

15:04.720 --> 15:10.320
morphisms are epimorphisms, the more coarse your model is, the more abstract or idealized

15:10.320 --> 15:16.480
your model of reality is. And so then the interesting thing is that this characterization

15:16.480 --> 15:21.520
of multi computational irreducibility, this measure of how additive or sub-additive your

15:21.520 --> 15:26.320
complexities are, as you compose them together in parallel, gives you a measure of the relative

15:26.320 --> 15:30.720
complexity of the evolution function, that is the function that evolves your computation

15:30.720 --> 15:35.440
forwards in time, versus the equivalence function, that is the function that declares that two

15:35.440 --> 15:40.160
computational states, two data structures, are to be treated as equivalent. And that interplay,

15:40.720 --> 15:45.680
I claim, is a kind of abstract meta way of thinking about the interplay between the

15:45.680 --> 15:49.760
computation of systems versus the computation of observers, because, you know, so in a sense,

15:49.760 --> 15:54.960
the role of the system is to evolve forwards in time, whereas the role of the observer is to take

15:55.760 --> 16:02.480
states in the system that are distinguished in reality and say, you know, subject to my

16:02.480 --> 16:07.200
idealized model, I'm going to treat these as the same. So the system is defining the evolution

16:07.200 --> 16:12.240
function, but the observer is defining this equivalence function. And so then the tradeoff

16:12.240 --> 16:20.320
in their complexities becomes exactly a tradeoff between what are the algebraic rules that describe

16:20.320 --> 16:24.080
the complexities as they compose sequentially, versus the algebraic rules that describe the

16:24.080 --> 16:29.840
complexities as they compose under this tensor product operation. And so I've shown this in

16:29.840 --> 16:35.040
particular for Turing machine systems, but this is a very general kind of algebraic semantics,

16:35.040 --> 16:38.320
you can apply it to hypergraphs, you can apply it to combinators, lambda calculus,

16:38.320 --> 16:45.040
doesn't matter. In a sense, there is just one category up to isomorphism of data structures

16:45.040 --> 16:48.880
and computations, and there are simply many different ways of parameterizing what that

16:48.880 --> 16:53.280
category is doing through things like Turing machines or hypergraphs or whatever. The

16:53.280 --> 16:57.760
algebraic formalism transcends the particular details of the computations that one's dealing with.

16:58.560 --> 17:03.360
And so yeah, as I say, what one ends up with is, I think, a fairly general formalism for

17:03.360 --> 17:09.440
thinking about the interplay between observers and the systems that they observe. And that gives

17:09.440 --> 17:14.720
one a, I promise I'll stop monologuing in a moment and we'll try and pick apart what I'm

17:14.720 --> 17:20.240
really talking about here. But so I'll just conclude with, you know, once one has that

17:20.240 --> 17:25.760
algebraic semantics, a whole bunch of things which I think previously would have been, at least to

17:25.760 --> 17:30.640
me, previously seemed like kind of fundamental confusions about, you know, how scientific

17:30.640 --> 17:34.720
observation works and how it interplays with computational models, those confusions kind of

17:34.720 --> 17:38.960
become much easier to clarify once you think about it in this kind of more compositional way. So

17:40.080 --> 17:45.520
to give a very simple example, or kind of very degenerate example,

17:45.920 --> 17:54.000
you can, you know, within this algebraic semantics, you can effectively trade off the

17:54.800 --> 17:58.640
computational complexity of the system for the computational complexity of the observer, right?

17:58.640 --> 18:06.160
So you can have, you can have kind of, in effect, two degenerate cases. You can have the case where

18:06.160 --> 18:11.280
the system itself has a completely trivial evolution function. The system itself has, you

18:11.280 --> 18:16.640
know, is doing something completely elementary in its, in how it evolves. But then the observer

18:17.280 --> 18:20.880
has some incredibly complicated equivalence function that makes the system look like it's

18:20.880 --> 18:24.080
doing something really complicated, even though what it's actually doing is something very simple.

18:25.360 --> 18:29.440
And so then you, so you have the phenomenon where actually kind of all of the complexity is in the

18:29.440 --> 18:33.680
eye of the, is in the eye of the observer. You can also have the other degenerate case where the

18:33.680 --> 18:37.840
observer is doing something absolutely trivial, where the, you know, the encoding function or the

18:37.920 --> 18:41.600
observer's own internal representation is just an identity function or something. So there's no

18:41.600 --> 18:45.440
complexity there, but the system is doing something incredibly complex. It's doing some,

18:45.440 --> 18:49.920
some totally, some really sophisticated universal computation. And so that will also appear very

18:49.920 --> 18:56.000
complex to that observer. And so, and you can also have any kind of, any intermediate, you know,

18:56.000 --> 19:02.560
there's this vast interstitial space between these two extremes. And so one thing that's kind of

19:02.560 --> 19:08.320
always, one sort of philosophical problem that I've always kind of been interested in ever since

19:08.320 --> 19:14.720
I was a kid, which is this, this sort of, this tension between empiricism versus rationalism,

19:14.720 --> 19:19.600
right? You know, the question of, you know, on the white, if you look back at the history of,

19:19.600 --> 19:23.440
where, you know, early European philosophy, or, you know, it's certainly Western, you know,

19:23.440 --> 19:28.160
Western post enlightenment philosophy, you had people like, you know, Descartes and Leibniz and,

19:28.240 --> 19:32.960
and so on, who were, you know, in a more sophisticated way, Bishop Barkley with subjective

19:32.960 --> 19:36.320
immaterialism, who were trying to push for this idea that, oh, you know, all the, all the

19:36.320 --> 19:39.920
sophistication is what's going on inside the observer's head. And, you know, what goes on in

19:39.920 --> 19:43.520
reality is somehow secondary. And then you had people like, you know, Locke and Hume and the

19:43.520 --> 19:47.040
empiricists, who were saying, no, no, we should try and get the observer as much out of the picture

19:47.040 --> 19:51.040
as possible. And we should say all the sophistication is going on kind of in the external world.

19:51.040 --> 19:55.040
And this in one, you know, one nice consequence of this is it gives one, one nice consequence of

19:55.040 --> 19:59.520
this formalism is it gives one actually an algebraic way of kind of parameterizing this

19:59.520 --> 20:04.880
spectrum from rationalism to empiricism, right, that, that you can, you can choose the rationalist

20:04.880 --> 20:10.720
extreme where, you know, you know, you just have some, some space of all possible computations and,

20:10.720 --> 20:14.960
and the observer is doing all of the work to try to narrow down to a particular one,

20:14.960 --> 20:19.840
or you can have the kind of empiricist extreme where, you know, the observer is a completely

20:19.840 --> 20:24.800
elementary system. And, you know, and everything and everything they observe is just being built

20:24.800 --> 20:30.240
up from a kind of bottom up construction, or you can have anything in between. And in a sense,

20:30.240 --> 20:34.880
we now have, I think, the beginnings of a mathematical theory that explain, that's able

20:34.880 --> 20:40.320
to explain how those complexities trade off in a very direct way. So I think there's potentially

20:41.040 --> 20:46.400
places of mutual interest there in kind of the, in thinking about, yeah, as I say, cognition,

20:46.400 --> 20:50.080
observation, measurement, scientific modeling, and so on, in fundamentally computational terms.

20:50.080 --> 20:54.720
So I think that hopefully that will provide some, some context for, for a discussion.

20:58.720 --> 21:05.680
Thank you. Great opening. There's so many places to spin in and jump through.

21:07.200 --> 21:11.600
I guess I'll start with the two things I wrote down were unity is plural and at

21:11.600 --> 21:16.480
minimum two and beauty is in the eye of the beholder and the way that these kinds of pieces

21:16.480 --> 21:23.280
of timeless wisdom that describe that fundamentally relational component to observers in systems,

21:23.280 --> 21:27.760
which are not all kinds of systems per se, but those kinds of systems, those things are true for,

21:28.400 --> 21:35.200
and then the way in which along the formalism that you described with the system observer encoding

21:36.000 --> 21:42.320
freeway partition, and then the way that in free energy principle and the particular physics,

21:42.320 --> 21:48.640
that interface gets broken out from the agent's perspective into the incoming sensory and the

21:48.640 --> 21:56.480
outgoing action. So then that results in the fourfold particular partition. So maybe just to kind of,

21:57.440 --> 22:04.160
well, there, how do we partition the, from a category theory perspective or however,

22:04.160 --> 22:11.680
the action perception loop or the engagement loop? Like, how do we make a topology or

22:11.680 --> 22:19.120
compare a contrast different topologies and flows over this kind of seemingly pervasive or universal

22:19.120 --> 22:25.600
interface like concept? That's a fascinating question. So I don't have the answer and this is

22:25.600 --> 22:31.280
maybe a place where, where both you Daniel and perhaps David may have, you know, useful perspectives

22:31.280 --> 22:37.760
on this because, you know, I'm, I'm, you know, I read Carl's work a few years back and so I have

22:37.760 --> 22:42.080
some familiarity with the terms, but I'm by no means a kind of, you know, an expert on free

22:42.080 --> 22:46.160
energy principle or, you know, active inference and those kinds of things. But I think it's a very

22:46.160 --> 22:50.560
good point that you raise. And so, you know, I should begin by just being honest and say that,

22:50.560 --> 22:55.200
you know, everything I'm doing, you know, all that I just described is of course an idealization

22:55.200 --> 22:59.200
and that, you know, in reality, you know, in particular, it's an idealization, which I think

22:59.280 --> 23:03.680
you were very right, Daniel, to kind of pick up on. It's an idealization in which we say the

23:03.680 --> 23:07.360
observer is completely kind of non-interacting with the world somehow, right? That, you know,

23:07.360 --> 23:12.080
in a sense that there's just input coming in and nothing, nothing coming out. But of course,

23:12.080 --> 23:15.600
we know that's not really how observation works. Observation is necessarily a kind of two-way

23:15.600 --> 23:20.480
process. And so what's needed is not just this kind of very clean algebraic semantics that I've

23:20.480 --> 23:23.760
described here, which assumes that there's a essentially a one-way function from the world

23:23.760 --> 23:28.640
to the observer, but actually something more like a kind of second-order cybernetics description

23:29.440 --> 23:34.240
of, you know, what's really going on where you have, you know, first-order and second-order

23:34.240 --> 23:37.760
interactions, whereas exactly as you say, you can get these sort of feedback loops from

23:37.760 --> 23:42.160
observation to action and back again, which are probably, which is, I mean, still an idealization,

23:42.160 --> 23:46.880
but probably a more realistic idealization for how real observers and real measurement apparatus

23:46.880 --> 23:52.160
work. So I just want to begin by saying that I don't know, right? And, you know, the question of

23:52.160 --> 23:56.160
how this formalism into plays with things like second-order cybernetics and other areas where

23:56.160 --> 24:00.720
I know these kinds of questions have been explored, that's something I'm very interested to find out

24:00.720 --> 24:10.640
about, you know, going forward. But I think, hmm, okay, so, yeah, so at some point, maybe you could

24:11.440 --> 24:15.440
help me understand, you know, potentially where things might fit in with, you know,

24:15.440 --> 24:23.920
with the kind of broader active inference framework. I'm not sure I necessarily have that

24:23.920 --> 24:33.120
much more to comment on than that. Yeah, other than to say that, you know, in a sense, okay,

24:33.120 --> 24:41.680
so maybe, you know, one further comment is that, because, I mean, you asked specifically about

24:41.680 --> 24:46.800
how the kind of compositional category theoretic perspective might be useful. So I don't think

24:46.800 --> 24:49.600
category theory in itself is going to be the complete answer. I think it will be category

24:49.600 --> 24:53.120
theory augmented with some other things, computational complexity, probably second-order

24:53.120 --> 24:58.640
cybernetics, and some other things that I may not be aware of. But one place where I think that

24:58.640 --> 25:05.280
viewpoint is useful, at least on a philosophical level, is the idea that comes about, that you

25:05.280 --> 25:09.360
really obtain by studying mathematical structures in a category theoretic way, which is that the

25:09.360 --> 25:16.400
identity of something, you can define it both in terms of its intrinsic properties, or you can

25:16.400 --> 25:20.320
define it in terms of, you know, the stuff that you can do to it, right? So, you know, this was

25:20.320 --> 25:24.080
really the transition that happened in the foundations of mathematics as a result of people

25:24.080 --> 25:29.680
like Samuel Alenberg and Saunders-McLean. So, you know, category theory has its origins in this

25:29.680 --> 25:33.840
sort of slightly obstruous branch of algebraic topology. It was, you know, initially developed

25:33.840 --> 25:38.880
by people like Alexander Gordon-Deek and John P. S. Sear for doing homological algebra for,

25:38.880 --> 25:42.880
you know, for reasoning about sort of the algebraic structure of topological spaces. But then later,

25:42.880 --> 25:46.800
in the, I think, 1960s, 1970s, these two American mathematicians, Alenberg and McLean,

25:46.800 --> 25:50.640
realized that it was useful not just for thinking about topology, but for thinking about kind of

25:50.640 --> 25:54.720
mathematical structure in general. And then later on applied category theorists started saying,

25:54.720 --> 25:59.200
well, maybe it's useful for just thinking about structure in general. But, you know, the key kind

25:59.200 --> 26:05.520
of conceptual or philosophical shift that it imposes is, you know, historically, thanks to the work

26:05.520 --> 26:09.920
of people like Kantor and Frege and Russell and so on, people have thought about mathematical

26:09.920 --> 26:14.000
structures in the foundations of mathematics as, you know, in terms of set theory. And the idea

26:14.000 --> 26:18.240
in set theory is you have this, you know, things like the axiom of extension that effectively say

26:18.240 --> 26:23.520
set is defined by what's inside it, right? So in other words, you know, you, a mathematical

26:23.520 --> 26:27.680
structure obtains its identity by, you know, you break it apart and you look at what's inside.

26:29.040 --> 26:34.320
In category theory, it's a completely different view. The view instead is you say, well, no,

26:34.320 --> 26:38.080
you can't look inside, you know, it's a fundamental rule of category theory that you can't look inside

26:38.080 --> 26:43.680
an object. You know, it's internal structure, if it has any sort of outer bounds to you.

26:43.680 --> 26:49.040
And instead, you give that object identity in terms of how it relates to other objects

26:49.040 --> 26:53.040
of the same type, right? So in other words, you know, you can ask, you know, what can I do to

26:53.040 --> 26:57.680
this? What functions can I apply to it? What functions can I apply to something else that

26:57.680 --> 27:02.320
map into this? So, you know, if I want to define, I don't know, the real numbers or the integers

27:02.320 --> 27:06.320
or something in the set theory, you know, from a set theory perspective, you would say that the,

27:06.400 --> 27:10.720
you know, the essence of the real numbers are all the numbers that are inside that set or all

27:10.720 --> 27:15.440
the numbers that are inside are. Whereas the category theory perspective is no, the essence

27:15.440 --> 27:19.440
of the real numbers are all the functions that you can define that take real numbers to some

27:19.440 --> 27:23.680
other number system or real numbers to themselves or that take some other number system into the

27:23.680 --> 27:28.640
real numbers or et cetera. And, you know, some of the deepest results in category theory, like the

27:28.640 --> 27:34.880
Oneida Lemma and other things, are telling one in some very precise sense that these two perspectives

27:34.880 --> 27:40.000
are really the same at some fundamental level, but that, you know, identifying an object based

27:40.000 --> 27:44.000
on its internal structure, based on breaking it apart and asking what's inside and identifying

27:44.000 --> 27:49.280
an object by asking, what can I do to it? And what, you know, what can this object be transformed

27:49.280 --> 27:53.680
into and what things can be transformed into this object? Those give you exactly the same

27:53.680 --> 27:57.440
information. It's far from obvious that that's true, you know, the Oneida Lemma is a very kind of,

27:58.560 --> 28:02.240
one of those results where you can never quite work out of its obvious or if it's incredibly,

28:02.240 --> 28:06.720
you know, mysterious. But I tend to fall on the side that it's incredibly mysterious. It's far

28:06.720 --> 28:11.440
from evidence that those two perspectives would really be the same. And yet, the point you're

28:11.440 --> 28:18.800
making Daniel, I think, is that in a sense, historical ways of thinking about scientific

28:18.800 --> 28:22.960
observation have tended towards the set theoretic viewpoint, tended towards the viewpoint that we

28:22.960 --> 28:26.160
understand systems based on kind of breaking them apart into constituent components.

28:27.200 --> 28:31.440
But perhaps a more realistic view is something more like the category theory perspective,

28:31.440 --> 28:36.560
where we say, you know, I understand a system by interacting with it, right? By asking,

28:36.560 --> 28:40.880
what can I do to it? And how does it behave as I, when I perform certain operations to it?

28:40.880 --> 28:44.880
And that's a fundamentally, you know, that's a fundamentally two-way process that involves not

28:44.880 --> 28:49.520
just passive observation, but also kind of active participation. And somehow we need to develop

28:49.520 --> 28:53.440
a formalism that kind of incorporates those two elements. And maybe, you know, maybe it already

28:53.440 --> 28:58.160
exists. And it exists in this large literature tree of which I'm largely unaware. That's partly

28:58.160 --> 29:01.920
why I want to be here to try and find out, you know, what things I missed, so to speak.

29:03.520 --> 29:11.440
Well, all right, a few points. Self-evident is far from evident. Also, I tend to the mysterious,

29:11.440 --> 29:16.720
which is to say, saying more with less, especially for these frameworks, because they're less

29:16.720 --> 29:24.240
opinionated, so that their space of internal semantics can be larger. And then that description

29:24.240 --> 29:29.040
that you provided with the relationship between the set and the category theory. So I kind of

29:29.040 --> 29:36.160
summarized it as set is to essential inclusion as category is to relational function. Now,

29:36.160 --> 29:44.720
if our concept of organismality or of action in the niche is constructive compositional material,

29:45.840 --> 29:51.520
then we are looking for, like, what is in or out? Is the microbiome in? Is the

29:52.160 --> 29:56.960
pheromone in the ant colony in or out of that thing? Because it's looking for, like, a static

29:56.960 --> 30:05.200
material answer. And then in contrast, the other side of that coin highlights the dynamic,

30:05.200 --> 30:10.480
like, whatever it is that self-organizing of the tornado is the tornado, whatever it is that

30:10.480 --> 30:17.200
self-organizing for the ant is the ant. And then also this, like, hint slash

30:18.160 --> 30:24.400
mobius strip or something that those two in the moment are indistinguishable.

30:25.280 --> 30:29.680
And yet systems that we choose to define one way or another, or keeping both open,

30:30.480 --> 30:36.240
those design decisions do make all the difference, even if for real systems, as they're observed,

30:37.040 --> 30:44.960
there's indistinguishability. Right, right. I think that's a very important point. And one which,

30:45.920 --> 30:52.000
I mean, this is a, it's always a kind of concern I have whenever I start thinking about, you know,

30:52.000 --> 30:58.400
embodied cognition or, you know, extended phenotype type ideas, right? But in a sense, you know,

30:59.280 --> 31:03.760
if what one is trying to do here is construct a kind of formalistic model of observation or

31:03.760 --> 31:08.960
of cognition or something, then as a kind of first order approximation, one has to start by

31:08.960 --> 31:12.880
somehow decomposing the world into observers and systems. But of course, that, you know,

31:12.960 --> 31:17.200
we know that that decomposition is somehow arbitrarily imposed, right? And that, you know,

31:19.040 --> 31:22.720
if you take these things to their extremes, and you say, you know, you allow essentially

31:22.720 --> 31:26.320
everything that the agent is interacting with to be considered, you know, like,

31:26.320 --> 31:29.760
not just the microbiome, as you say, but also, you know, tools that they construct or

31:29.760 --> 31:33.840
environments in which they exist and so on. If you, as you start to consider all of that to be

31:33.840 --> 31:37.840
a component of that organism, you know, of that agent's phenotype, which is a completely reasonable

31:37.840 --> 31:45.120
thing to do, then, and you start to, you know, you start to say, okay, well, their cognitive

31:45.120 --> 31:49.120
processes are not just localized to their brain or their spinal column, but are kind of somehow

31:49.120 --> 31:53.280
extended to, you know, the computers they use, the paper they write on, the books they read,

31:53.280 --> 31:59.920
et cetera. Again, perfectly reasonable thing to do and sort of somehow more descriptive of what's

31:59.920 --> 32:04.640
really going on. But my fear is always, if you take that too far, then, you know, you end up

32:04.640 --> 32:08.960
destroying the whole assumption that the idealization was based on, which is that you can neatly

32:08.960 --> 32:14.400
decompose, you know, the world into observers and systems. And so, I always get a bit nervous

32:14.400 --> 32:20.320
when thinking about that, that it's like, yes, you know, in a sense, you know, we know this is an

32:20.320 --> 32:27.040
approximation and we know that that approximation is not really true, but how, you know, how much can

32:27.040 --> 32:31.600
you afford to sort of loosen your grip on that approximation before the whole thing just kind

32:31.600 --> 32:35.760
of falls apart. I don't really know the answer to that question. But I think it's an interesting one.

32:35.760 --> 32:41.760
Yeah. How about ask some questions from chat and then give your first thoughts and then we'll see

32:42.560 --> 32:47.360
maybe where that kind of lands with further questions or how it connects to active inference.

32:47.360 --> 32:51.280
So that sounds good. Oh, by the way, should I keep my screen share on or should I?

32:51.280 --> 32:57.840
Yeah, we might want to go to a figure. So it's fine. Okay. Yeah. All right. Quantum Bell wrote,

32:58.720 --> 33:01.680
how does this help us reason about causality?

33:03.440 --> 33:09.040
That's a fascinating question. Okay. So that's that's another major aspect of, you know, why I

33:09.040 --> 33:15.440
think this research program is exciting, because so again, this is something where I'm interested

33:15.440 --> 33:18.880
to get the kind of active inference perspective, because I know this again, it's a topic in which

33:18.880 --> 33:26.480
much has been written and I'm largely ignorant. But yes, so one question you could ask is,

33:26.960 --> 33:29.920
yeah, if you have a description of a computation like this, like,

33:30.720 --> 33:34.720
let's go back up to the Turing machine case, the single way Turing machine case that's

33:34.720 --> 33:37.760
relatively easy to analyze, although still far from obvious what's going on.

33:38.480 --> 33:42.800
So suppose you have a computation of this kind, and you want to ask, what is its causal structure?

33:42.800 --> 33:46.960
In other words, you know, for each edge, each time I'm applying this Turing machine transition

33:46.960 --> 33:52.000
function, can I construct some kind of graph, you know, some some directed graph representation that

33:52.000 --> 33:57.600
tells me how these events are linked together. So in the, within the original research program,

33:58.560 --> 34:02.240
the so-called Wolfram model research program that kind of started a lot of these investigations,

34:02.240 --> 34:05.200
we were looking at this all the time, right? We were looking at kind of, you know, taking

34:05.200 --> 34:09.520
computations and looking at that causal structure and trying to, you know, infer things about what

34:09.520 --> 34:14.320
was going on about the, you know, the semantics of the computation based on causal relationships.

34:14.320 --> 34:17.600
And at a certain point, I started to realize, and I think other people had realized this before

34:17.600 --> 34:22.960
I did, but I'm often slow to pick these things up. I, I, I and other people started to realize that

34:22.960 --> 34:29.520
the notion of causality we were using was kind of nonsense. I mean, it was not completely hopeless,

34:29.520 --> 34:33.680
but it wasn't really causality, or it couldn't really be called causality in any, in any definite

34:33.680 --> 34:40.160
sense. So what do I mean by that? So first of all, it was a very technical problem. So if you're

34:40.160 --> 34:44.160
looking at something like a Turing machine evolution or a hypergraphic writing system as we were,

34:45.120 --> 34:49.920
then there's a very tempting and apparently obvious natural definition of causality that

34:49.920 --> 34:54.480
you can use, which is to ask, you know, when you split the world up into events that take,

34:54.480 --> 34:58.480
you know, some part of your data structure as input and output, you know, some other part of a

34:58.480 --> 35:04.480
data structure as output, then you can very easily ask, well, does the output of one event

35:04.480 --> 35:09.600
intersect with the input of another event? So if I show the hypergraph example, it's perhaps easier

35:09.600 --> 35:15.040
to see. So you have a hypergraphy writing rule that looks like this, right? So you know, you have,

35:15.040 --> 35:18.240
you say, if I have a piece of hypergraph that looks like that, I replace it with another piece of

35:18.240 --> 35:22.880
hypergraph that looks like this. So at each, each time you apply an event, you can think of that

35:22.880 --> 35:28.320
event as, you know, ingesting certain hyper edges and kind of, you know, replacing them with others.

35:28.320 --> 35:32.560
So you can divide it up into a sort of the, the, the input hyper edges that are being ingested

35:32.560 --> 35:37.200
versus the output hyper edges that are being produced. And so then you can ask, well, do the

35:37.200 --> 35:44.640
output, did I use, did I subsequently ingest in some future event hyper edges that were output

35:44.640 --> 35:48.320
in some previous event? Well, if the answer is yes, then pretty obviously that future event

35:48.320 --> 35:52.080
couldn't have occurred unless the previous event had already occurred. So then you could say, well,

35:52.080 --> 35:56.480
then one of those events causes the other. So in general, you could say that two, you know,

35:56.480 --> 36:03.120
an event A causes event B, if it's the case that the output, that the collection of tokens that

36:03.120 --> 36:07.200
was produced in the output of event A has a non-zero intersection with the collection tokens

36:07.200 --> 36:11.360
that were ingested as part of the input of event B. And that's a very tempting, very natural

36:11.360 --> 36:15.920
definition of kind of causality in these systems. Turns out it doesn't really work. I mean, it works

36:16.720 --> 36:20.800
pretty well, but there are cases in which it fails and it fails pretty spectacularly. And

36:20.800 --> 36:24.960
so the kind of canonical case where it fails spectacularly is that you can have events that

36:24.960 --> 36:29.040
don't actually do anything, right? You can have events that just kind of touch an edge, touch a

36:29.040 --> 36:34.720
token and output it again unchanged, but maybe, you know, it modifies the name, it modifies the

36:34.720 --> 36:38.880
identifier, but it doesn't actually change anything about the structure of the hypergraph or the

36:38.880 --> 36:43.440
Turing machine state or whatever. So pretty obviously that event doesn't matter. It shouldn't

36:43.440 --> 36:48.720
be causally related to anything in the future. But because it ingested the edge and then didn't do

36:48.720 --> 36:52.880
anything, you know, did some identity operation, but then, you know, produced it in the output again,

36:52.880 --> 36:56.880
it will kind of register as being causally related to any future event that used that

36:56.880 --> 37:01.200
edge, even though it didn't make any difference. That's just one very obvious example. There are

37:01.200 --> 37:05.440
other cases where it became clear that whatever this thing was, whatever this algorithm was

37:05.440 --> 37:10.080
detecting, it wasn't really causality. So I tried to think about, you know, what's a more sensible

37:10.080 --> 37:14.320
definition of causality. And I started working on things to do with, you know, a slightly kind of

37:14.320 --> 37:19.440
blockchain inspired ideas where you say, okay, well, rather than just arbitrarily assigning,

37:19.440 --> 37:24.720
you know, identifiers to these tokens every time they're created, what if I recursively

37:25.600 --> 37:29.920
construct the identifier of the token based on its causal history. So in other words, each token,

37:29.920 --> 37:35.520
like each hyper edge or each state in my each, each tape square on my Turing machine tape,

37:36.400 --> 37:41.360
I, the identifier is not just some random number that gets generated by my algorithm,

37:41.360 --> 37:47.200
but instead its identifier is a directed graph representation of its complete causal history.

37:47.200 --> 37:52.720
Well, then kind of recursively, it's identified can only change if the causal history was updated.

37:52.800 --> 37:56.400
And so you don't end up with these kind of spurious causal relations that I described before.

37:56.400 --> 38:01.600
So that seemed like one tempting way of resolving this problem. But then I realized,

38:01.600 --> 38:04.880
actually, there's a much more fundamental problem. There's a problem, there's a philosophical problem

38:04.880 --> 38:09.600
with the way that we're thinking about causality, which is that it's not really, you know, so,

38:10.960 --> 38:17.440
okay, this is a long tangent, which I'll talk about a little bit, but I won't get into the

38:17.440 --> 38:21.120
complete details unless people are interested. But I ended up T I ended up talking to a bunch of

38:21.120 --> 38:24.800
philosophers who, you know, who worked on causality and people who worked on parallel

38:24.800 --> 38:28.560
programming and quantum information theory and other places where causality was,

38:28.560 --> 38:32.480
was, was studied and asked them kind of basically what, what is, what, what do you mean by causality?

38:32.480 --> 38:39.120
What is causality? What is this thing we're trying to define? And in some form or another,

38:39.120 --> 38:44.000
all of the definitions boiled down to, you know, event A causes event B if

38:44.960 --> 38:49.200
had event A not occurred, then event B would not have occurred. So in other words,

38:49.200 --> 38:53.440
you need a counterfactual, you need some possible history, some possible world in which event A

38:53.440 --> 38:57.520
didn't happen. But if you're reasoning about a purely deterministic event system like a Turing

38:57.520 --> 39:01.520
machine, that doesn't make any sense. Because if you're, you know, if you, if you have a single

39:01.520 --> 39:06.240
Turing machine transition function, there is no possible world in which that transition function

39:06.240 --> 39:09.360
didn't fire in that particular way. Because if it didn't fire in that particular way,

39:09.360 --> 39:12.480
you would not be reasoning about that Turing machine anymore, you'd be reasoning about

39:12.480 --> 39:18.160
different Turing machine. So suddenly this, you know, to make sense of these notions of causality,

39:18.160 --> 39:23.040
you need a kind of Leibnizian, you know, modality is view of reality that just doesn't exist for

39:23.040 --> 39:27.760
these deterministic computational systems. So either you need to define computation,

39:28.400 --> 39:33.840
you need to define causality only at the multiway level, only at the level where you have many

39:33.840 --> 39:37.440
computations or possibly all computations happening in parallel, and then you can define

39:37.440 --> 39:41.760
causality relative to all of the, you know, relative between them, or you were kind of posed, right?

39:41.760 --> 39:45.680
There wasn't really, you know, that seemed like the only kind of the only get out, or you'd need

39:45.680 --> 39:50.320
some fundamentally new philosophical theory of causality that I was not qualified to produce.

39:51.120 --> 39:56.240
And so that's, again, part of the reason, part of what motivated this general

39:56.240 --> 40:01.360
research program, which is trying to think about this category of not just a single computation,

40:01.920 --> 40:05.440
and with a single sequence of data structures, because it's clear that you can't, you know,

40:05.440 --> 40:10.880
philosophically meaningful way assign causality in that case, but rather, you know, looking at the

40:10.880 --> 40:14.640
algebraic structure of the category of all possible computations and all possible data

40:14.640 --> 40:18.960
structures. And in that situation, there is a notion of causality you can equip that with,

40:18.960 --> 40:23.520
and there's a nice, again, a nice mathematical description in terms of, in terms of weak two

40:23.520 --> 40:28.560
categories and so on, which again, I can talk about if people are interested. But yeah, so it's

40:28.560 --> 40:32.880
clear that these things are very deeply related that this sort of theory of the category of

40:32.880 --> 40:36.720
computations and data structures, and the theory of how you assign causality in a meaningful way,

40:37.760 --> 40:42.960
are very deeply related. And I'll just mention one other thing on that topic, which is again,

40:42.960 --> 40:48.640
just the area which I find quite exciting, because it's an unexpected spin out of this program,

40:48.640 --> 40:53.760
which is that, so once you have a way of consistently applying causality at a per token

40:53.760 --> 40:58.800
level in these systems, it gives you a way of vastly generalizing what computation is.

41:00.400 --> 41:03.920
And you can, in particular, you can get, you can derive something which I call,

41:05.280 --> 41:08.800
well, which I'm provisionally calling covariant computation, although it should probably have

41:08.800 --> 41:16.320
a better name than that, which is, so in our traditional kind of Turing Church type models

41:16.320 --> 41:21.200
of computation, computation is a purely forwards in time operation. So at every point, you know,

41:21.200 --> 41:25.280
you have a complete data structure, and computation is about deriving what is the next state of that

41:25.280 --> 41:28.960
data structure. So in a sense, it's only a forwards in time thing. You might be able to

41:28.960 --> 41:32.000
kind of reconstruct the initial conditions based on some subsequent data structures,

41:32.000 --> 41:34.800
you might be able to go backwards in time, but that's essentially what you're doing.

41:35.760 --> 41:42.000
But then you could imagine, okay, suppose I don't know the complete state of my data structure,

41:42.000 --> 41:47.360
I know instead, I know one part of my data structure, but I know it, I know its history

41:47.360 --> 41:51.680
throughout all of time. So you could imagine, say an elementary cellular automaton or a Turing

41:51.680 --> 41:56.240
machine tape, where you'd know nothing about the tape, but you know the state of one cell,

41:56.240 --> 42:00.080
and you know it, you know, throughout all of time. And then the question is, what can you infer

42:00.080 --> 42:03.280
about the rest of the computation? And it turns out that for those kind of structured array type

42:03.280 --> 42:07.760
systems, you can infer a lot, you can actually evolve the system, not forwards in time, but

42:07.760 --> 42:13.920
sideways in space, and obtain a kind of causal diamond that so okay, the top left, top right,

42:13.920 --> 42:17.920
bottom left, bottom right corners are undetermined. But everything inside that diamond can be

42:17.920 --> 42:24.320
determined just from that one row, or that one column that you know, you know, sort of extended

42:24.320 --> 42:29.520
throughout time. And so, you know, and that's a fundamentally different notion of computation.

42:29.520 --> 42:33.520
So it's a version of computation, which is not forwards in time, but sideways in space.

42:33.520 --> 42:37.520
But you can also have version of computation that is sideways and branch your space where you

42:37.520 --> 42:42.560
know, you know, one complete state, you know, you know, one branch of the multiway system extended

42:42.560 --> 42:45.680
throughout time. And then the question is, what else can you infer about the, you know, but the

42:45.680 --> 42:49.040
rest of the multiway system just from that one branch? And again, the answer turns out to be,

42:49.040 --> 42:55.200
you can infer a lot, but not everything. And so, just like in the reason I call this covariant

42:55.200 --> 42:59.040
computation is because it's very analogous to what happens in relativity. So in relativity,

42:59.120 --> 43:03.200
once you buy into this notion of general covariance and the notion that space and time are kind of

43:03.200 --> 43:07.760
fundamentally the same thing, then you have to somehow relax your traditional view of what

43:07.760 --> 43:12.000
dynamical systems do, which is, you know, we typically think of systems as evolving, you know,

43:12.000 --> 43:16.880
you have a snapshot of your initial of your data at, you know, localized on a, you know,

43:16.880 --> 43:20.480
for a particular state of space, you know, on a particular space like hypersurface.

43:20.480 --> 43:23.760
And then your laws of physics tell you how that space like hypersurface evolves forwards or

43:23.760 --> 43:31.440
backwards in time. But in a covariant picture of physics, then you must also allow for, you know,

43:31.440 --> 43:36.160
your initial data to be defined on a time like hypersurface, and you for you to be able to evolve

43:36.160 --> 43:40.560
that time like hypersurface sideways in space or mixtures of the two and so on. And so it's clear

43:40.560 --> 43:45.040
that there's a very general, a vast generalization of ordinary computation theory that you can,

43:45.040 --> 43:49.520
that you can construct that's kind of physics inspired in that sense, in which you can have

43:49.600 --> 43:55.040
mixing of space time and kind of multiway directions in a completely consistent way.

43:55.040 --> 44:00.160
But to make those things consistent, you need to have a definite way of assigning causality. You

44:00.160 --> 44:04.320
need to because, you know, any computation that you do, even if it permutes the directions of

44:04.320 --> 44:08.720
space and time and branching space and so on, must always somehow preserve the causal structure,

44:08.720 --> 44:13.200
has to respect the causal structure of what's going on, or else it's inconsistent. And so

44:13.920 --> 44:17.920
this question of how you construct a covariant theory of computation is, it turns out,

44:17.920 --> 44:22.640
intimately related to the question of how you take this category of computations and data structures

44:22.640 --> 44:26.640
and equip it with a consistent notion of causality. So very interesting question,

44:26.640 --> 44:33.840
we could talk about that at great length. Okay, to follow with a few pieces, it's very related to

44:33.840 --> 44:40.000
Professor Mike Levin's notion of poly computing, and about the necessity for a causality concept to

44:40.000 --> 44:46.000
be created or deployed when the question arises, was that me? Was that action or change due to me?

44:46.960 --> 44:53.280
Also, connectivities, even just in the neuroimaging setting, which is kind of the cradle

44:53.280 --> 44:56.800
from which active inference and free energy principle arise from,

44:56.800 --> 45:01.920
it's really important to distinguish the functional, effective, and anatomical connectivities.

45:01.920 --> 45:06.960
And that was one of the points that Toby St. Clair Smith made in his dissertation,

45:06.960 --> 45:13.280
which is that a lot of times the Bayesian graphs don't convey all of the necessary and sufficient

45:13.280 --> 45:18.800
information to make the reproducible computation, which is one of those kind of what's missing from

45:18.800 --> 45:23.520
the graph is what motivated a lot of the category theory developments in active inference,

45:23.520 --> 45:29.760
as well as some of the formal ontological works with Sumo and Dave here and Adam Pease, because

45:30.800 --> 45:35.840
implementing modal and higher order logics is really important if it's a possible situation

45:35.840 --> 45:39.200
where a mind can have a perspective on a mind and all these things like that.

45:40.160 --> 45:48.880
Then the ant-turing tape, the tape is the pheromone, and then the decision space is the nest

45:48.880 --> 45:57.120
mate's scrolling. So when you had a deterministic turing tape, that was like a movie because the

45:57.120 --> 46:02.560
nest mate couldn't make any choices, except for internal action, which is kind of side topic,

46:02.560 --> 46:08.160
but it couldn't make any choices on the tape. Whereas when there's a multi-way,

46:08.160 --> 46:13.440
which is basically in active inference, what we talk about in terms of affordances and the policy

46:13.440 --> 46:19.120
space and the temporal depth of planning and counterfactuals on action and action-conditioned

46:19.120 --> 46:24.000
world transition states like the B matrix, all those kinds of topics come into play,

46:24.000 --> 46:29.760
because if you want to have a causal buffer or grasp on what is it that something that could do

46:29.760 --> 46:35.680
otherwise does, what does it cause to do when it does or doesn't do otherwise, you need something

46:35.680 --> 46:42.640
like a deterministic handle around what could be a probabilistic or deterministic,

46:42.640 --> 46:47.280
but at least multi-way map of some kind of cognitive territory.

46:51.760 --> 46:58.320
That's a very, very interesting perspective. Again, I'm betraying my ignorance of active

46:58.320 --> 47:06.720
inference theory here, but it sounds almost like, when you have this kind of interplay between

47:08.080 --> 47:17.280
sort of epistemic versus pragmatic, there's two aspects of how this kind of speculative

47:17.280 --> 47:25.680
part of cognition works. This is something which I thought about in a completely different context

47:25.680 --> 47:30.240
in relation to things like quantum information theory, but I wonder if there's a potential

47:30.240 --> 47:37.600
overlap there. There are certain situations when thinking about these kinds of systems,

47:38.320 --> 47:43.600
purely abstractly, where you kind of need two different notions of causality. You need a kind

47:43.600 --> 47:50.480
of speculative notion that's dynamic, that can be rewritten, and then you need a kind of definite

47:50.480 --> 47:56.560
notion that's immutable. A classic example of this is for something like quantum information

47:56.560 --> 48:01.280
theory. You can have superpositions of causal orders, you can have quantum switches, you can have

48:03.120 --> 48:08.320
causal structure that exists in superpositions of different kind of directed graph states,

48:08.320 --> 48:12.400
but then once you apply Hermitian operator, once you apply a measurement, the causal structure is

48:12.400 --> 48:16.480
definite because then everything is relativistic and you have covariance. You have similar things,

48:16.560 --> 48:23.760
as I understand, with distributed computing, with parallel computing, where you potentially allow

48:23.760 --> 48:27.520
for speculative execution for a certain number of steps where you're kind of treeing out this

48:27.520 --> 48:32.640
multi-way system and you have a superposition or at least a collection of possible causal histories,

48:32.640 --> 48:36.880
but then eventually you have to choose an actual operation to do and then the causal history,

48:36.880 --> 48:42.240
you have this big block that gets laid down and then the causal history is somehow definite.

48:42.480 --> 48:52.480
I wonder if there's a way of thinking about speculative execution of agents

48:52.480 --> 48:57.600
and the interrelation between that speculative execution and agent actions in terms of, again,

48:57.600 --> 49:01.600
this interplay between two different causal structures, between a dynamic one versus an

49:01.600 --> 49:08.800
immutable one. Yeah. Well, one funny way to think about that is a single agent that has this

49:08.800 --> 49:15.760
counterfactual contemplative ability could be in the center place foraging arena and then imagining

49:15.760 --> 49:22.000
with discrete branching paths like a chess algorithm or like a probability distribution

49:22.000 --> 49:27.200
could be like imagining where it could go, but not all cognitive things or the kind of things that

49:27.200 --> 49:34.240
make plans of their own actions, whereas like an ant colony has nest mates on the ground. So

49:34.240 --> 49:40.560
they're actually realizing in these finite trajectories, the real exoskeleton on the ground

49:41.440 --> 49:48.400
that plays out, ending up with those simulated trajectories could have been simulated or could

49:48.400 --> 49:52.960
have been probabilistically blurred, but that's kind of the difference between like the embodiment

49:52.960 --> 49:58.480
and like the body moving there for a mammal or for an animal and then like the mind simulating it

49:58.560 --> 50:06.480
and then just to the epistemic and pragmatic tradeoff in decision making. So let's just say that

50:06.480 --> 50:13.840
we're in that multi-way moment. We'll just have two options, two different slices of the B variable

50:13.840 --> 50:19.040
and the policy selection question is about which way are you going to go? Which affordance in the

50:19.040 --> 50:24.800
moment? Policy is basically the affordances for the time horizon of planning, but if it's only one

50:24.800 --> 50:29.040
time step or just the next one, then the affordance space is just the actions that can be taken.

50:30.960 --> 50:36.480
One way to make it so that what happens is the likeliest thing, path of least action, which is

50:36.480 --> 50:41.360
kind of what opens up the whole physics of cognitive systems angle in contrast to like a

50:41.360 --> 50:47.120
reinforcement or reward learning perspective, what makes it the likeliest thing is starting with

50:47.120 --> 50:53.520
habit. So it could just be drawn from a fixed habitual distribution. However, for adaptive action,

50:54.080 --> 50:59.120
habit gets up-weighted with expected free energy, which is a functional that takes in

50:59.920 --> 51:04.960
the policy space, which is summing up to one because there's a probability over actions,

51:04.960 --> 51:12.240
and then up-weighting policies according to their score on expected free energy, which is

51:12.800 --> 51:18.400
consisting of epistemic plus pragmatic value. So how much is it going to align the observations to

51:18.400 --> 51:24.480
be what I like to see? That's pragmatic value with a preference. What is my expected information

51:24.480 --> 51:31.680
gain? That's the epistemic value. So how those are parameterized make the agent that always seeks

51:31.680 --> 51:37.520
out new information or always goes with habit or there's so much policy space because the knobs

51:38.240 --> 51:44.560
are not just simple sliders or there are multiple knobs, even though they are seemingly

51:45.360 --> 51:53.120
quite conciliant and minimal, like it's hard to imagine less, yet especially when there's

51:53.120 --> 51:59.120
richness in the environment, even simple systems can have like enormously complex or

51:59.120 --> 52:06.800
adaptive behaviors. I'll just leave it there. No, I never really thought of the, so okay,

52:07.840 --> 52:13.280
yeah, two things, right? So first of all, the perspective of, you know, thinking of an ant

52:13.280 --> 52:17.600
colony or a termite colony or something as being akin to a mind, that's, you know,

52:18.640 --> 52:23.760
I was familiar with that perspective from people like Dan Dennett and so on. But the idea that the

52:23.760 --> 52:29.680
individual ants in that colony are in a sense enact, they're kind of the hardware enacting the

52:29.680 --> 52:35.120
speculative execution, that's a very interesting idea. It's not speculative for them. Well,

52:35.120 --> 52:38.960
yeah, no, exactly. But it's sort of from the mind's perspective, I guess it's almost like

52:38.960 --> 52:42.880
speculative execution, but it's speculative execution that's being actuated in the physical

52:42.880 --> 52:49.760
world, which is very interesting. I not really thought about that before. But then, yeah, okay,

52:49.760 --> 52:55.680
so then the point you're making about connection to free energy and sort of habit formation and so

52:55.680 --> 53:05.520
on, okay, so I wonder if, you know, if we're thinking about a model of cognition in which

53:05.520 --> 53:10.960
there are these two distinct causality notions, the immutable versus the dynamic one,

53:13.200 --> 53:19.520
I wonder if the, so you gave a very, very nice account of how habit formation

53:20.080 --> 53:26.240
sort of works in these kinds of formalisms based on, you know, prior experience of expected

53:26.240 --> 53:33.760
free energy. So I wonder if there's a way of describing that abstractly in terms of something

53:33.760 --> 53:39.040
like, you know, you perform the speculative execution step where you're, you know, you're

53:39.040 --> 53:44.080
treeing out several multiway possibilities. And initially, you kind of, you know, you know nothing

53:44.080 --> 53:47.680
or you have no habits, you're just kind of, you're treeing everything out with kind of equal waiting.

53:48.240 --> 53:53.280
But then, you know, for each possible path, you're calculating either an actual or an expected

53:53.280 --> 53:59.200
free energy. And then somehow, you know, in future speculative executions, you wait those

53:59.200 --> 54:03.920
paths which you previously had found to have higher free energy as higher. And so, you know,

54:03.920 --> 54:08.240
you're more likely to explore those and less likely explore ones which are, you know, which

54:08.240 --> 54:14.080
have that lower expected value. It feels like something, something like that should fit very,

54:14.080 --> 54:18.800
very nicely into an algebraic semantics like this, which would be interesting.

54:18.800 --> 54:24.240
Oh, how about more questions from the chat? Okay, upcycle club writes,

54:25.520 --> 54:30.080
acknowledging the limitations of traditional entropy in multi computations

54:30.720 --> 54:36.480
motivates us to develop context specific entropy metrics. Can you share some insights

54:36.480 --> 54:43.120
towards such efforts? Yeah, I can certainly try. So, so yes, I mean,

54:44.560 --> 54:49.840
the first point is that, you know, it's, I think it was, there's that famous conversation between

54:49.840 --> 54:53.680
John von Neumann and Claude Shannon, where I think von Neumann famously said that like,

54:53.680 --> 54:58.560
Shannon should call his measure entropy because no one knows what it means, right? And I submit that

54:58.560 --> 55:03.200
the reason that no one knows what entropy means is because it's dependent. I mean, okay, one of

55:03.200 --> 55:07.120
the reasons no one knows what entropy means is because it's dependent on exactly what we've

55:07.120 --> 55:13.200
been talking about is dependent on the equivalence function of the observer. So it's one of these

55:13.200 --> 55:22.320
things like, I don't know, maybe this is a stupid analogy to use, but it's, sorry, I'm going to go

55:22.320 --> 55:26.640
off on a tangent, but I promise it's sort of relevant. But so one thing that, okay, one thing

55:26.640 --> 55:31.280
that always breaks my brain is when I try and think about like actuaries and life insurance

55:31.280 --> 55:40.640
policies, because it's one of those areas where those models only make sense if they're not

55:40.640 --> 55:45.760
perfect in a sense, right? Like, so if you had an actuary who knew exactly how long everyone was

55:45.760 --> 55:51.040
going to live, and somehow that information was kind of openly available, there would be no,

55:51.040 --> 55:56.720
like life insurance policies would be pointless. It's, but you know, whereas also if you had a

55:56.720 --> 56:00.160
model that was completely hopeless of predicting how long people would live, they would also be

56:01.040 --> 56:07.920
life insurance policies would also be pointless. The very existence of actuarial science

56:07.920 --> 56:13.600
relies on your model neither being perfect nor being awful. It somehow has to exist somewhere in

56:13.600 --> 56:19.040
between. And as I said, that's something which I, it's one of those topics where if I think about

56:19.040 --> 56:23.600
it for too long, it all just stops making sense. And entropy has very much that same character,

56:23.600 --> 56:28.960
right? Because if you were Laplace's demon, if you had perfect information about the system that

56:28.960 --> 56:33.760
you were observing, there's no notion of entropy, right? It's just every, you know, you know every

56:33.760 --> 56:38.640
micro-estate. So, you know, the Boltzmann formula gives you an entropy value of zero,

56:39.760 --> 56:45.440
where, you know, the notion of entropy only exists once you take that perfect knowledge

56:45.440 --> 56:49.680
of a system and you coarse-grain it, you define, as I was describing earlier, you introduce an

56:49.680 --> 56:55.680
encoding function that is not 100% subjective, so that now you are mapping certain distinct

56:55.680 --> 57:00.080
micro-estates onto the same coarse-grained macro-estates. And then now you can ask, okay,

57:00.080 --> 57:03.360
what's the number of micro-estates that, you know, certainly how coarse is my coarse-graining?

57:03.360 --> 57:07.360
What's the number of micro-estates consistent with this macro-estate? What's the number of different

57:07.360 --> 57:11.520
values of my domain that gets mapped to a single point in my co-domain of my encoding function?

57:11.520 --> 57:17.360
And that's what entropy is. And so it's, it's very closely, I mean, it is effectively a measure of

57:17.360 --> 57:21.920
how good is my coarse-graining. So if you had perfect knowledge, there's no entropy. If you have no

57:21.920 --> 57:28.240
knowledge, there's no entropy. It relies upon you having a not completely trivial, but also not

57:28.240 --> 57:33.600
100%, you know, a slightly subjective, but not 100% subjective encoding function,

57:34.880 --> 57:41.280
just like with life insurance policies. But so the reason, the reason I'm stating that is because,

57:41.280 --> 57:46.000
so now it kind of, I think from that perspective, becomes a little bit clearer why there are all

57:46.000 --> 57:49.840
these different notions of entropy and why, as the questioner was alluding to, why entropy seems

57:49.840 --> 57:56.960
to be so, as a concept seems to be so domain and system specific, because every different system,

57:56.960 --> 58:01.120
every different observer will, in principle, have a different set of encoding functions,

58:01.120 --> 58:04.960
a different set of equivalence functions, and each one will give rise to a different calculation

58:04.960 --> 58:10.000
of entropy. And so one way that you can think about this program, this program to try to

58:11.280 --> 58:15.040
understand the algebraic interplay between time complexity versus kind of equivalency

58:15.040 --> 58:20.880
complexity or computational irreducibility versus multi-computational irreducibility.

58:20.880 --> 58:26.400
In some sense, that is a program to try to understand how different definitions of entropy

58:26.400 --> 58:32.080
relate to each other in these kinds of systems. How, you know, if I take one idealized observer

58:32.080 --> 58:37.920
that has this equivalence function, and I ask, okay, suppose now they communicate with this

58:37.920 --> 58:41.600
different observer with a different equivalency function, they come to different understandings

58:41.680 --> 58:45.280
of what the entropy of the system is, but what is the relationship between their measured entropy

58:45.280 --> 58:50.320
values? They clearly is one that depends algebraically on some details of the distinction

58:50.320 --> 58:54.320
between their respective equivalence functions, but there doesn't seem to be yet any general

58:54.320 --> 59:00.640
theory for how those things are related, and that's part of the kind of raison d'tre of this

59:01.360 --> 59:09.840
of this research program. But yeah, I mean, okay, so one thing that I will comment on,

59:11.360 --> 59:15.200
although this is a little bit more speculative, it can sort of quasi philosophical comment, but

59:15.200 --> 59:21.600
so one place where these notions of entropy become one place where the fact that you have

59:21.600 --> 59:25.040
all these different notions of entropy becomes kind of interesting is in fundamental physics.

59:26.000 --> 59:30.560
So when you start to think about, if you try to model physics and the universe in these

59:30.560 --> 59:36.320
fundamentally computational terms, then one fairly generic sort of conclusion that you can reach

59:36.320 --> 59:40.880
is that gravitation, general relativity is essentially an entropic phenomenon. I mean,

59:41.520 --> 59:45.600
Valinde and people have kind of talked about this in non computational contexts too,

59:45.600 --> 59:48.880
but it's very, very natural if you start to think about space like hypersurfaces being

59:48.880 --> 59:53.520
a sort of hypergraphs, then, you know, in order to obtain a continuum geometry that's compatible

59:53.520 --> 59:58.480
with the Einstein equations, you need to have certain ergodicity, you know, you need to be

59:58.480 --> 01:00:03.920
able to make certain ergodicity assumptions on the rewriting, which in turn, sort of implies

01:00:03.920 --> 01:00:08.640
certain lower bounds on the entropy of the system. So somehow gravitation, general relativity,

01:00:08.640 --> 01:00:13.280
is a coarse grained theory that you obtain in the limit as the entropy goes to infinity.

01:00:13.840 --> 01:00:19.920
But something that's interesting is that quantum mechanics, on the other hand, is an idealization

01:00:19.920 --> 01:00:24.880
that you obtain in the limit as entropy goes to zero, because in kind of one of these purely

01:00:24.880 --> 01:00:29.120
computational models of physics, the quantum mechanical state of a system is described in

01:00:29.120 --> 01:00:34.640
terms of its multi-way structure. It's described in terms of, you know, when I have a kind of a

01:00:34.640 --> 01:00:40.320
branching program like this, let me find one of these like this, then, you know, I can divide it

01:00:40.320 --> 01:00:45.600
up into these sort of, into these simultaneity surfaces. And if I associate each state of the

01:00:45.600 --> 01:00:49.360
program as being like the analog of a quantum eigen state, and the kind of path weightings

01:00:49.360 --> 01:00:53.840
as being an analogous to the amplitudes associated with the eigen states, I can quickly build up

01:00:53.840 --> 01:00:57.360
a description of this multi-way system in terms of the evolution of some discrete analog of the

01:00:57.360 --> 01:01:01.520
Schrodinger equation. And it turns out you get a theory that is kind of equivalent of the mathematically

01:01:01.520 --> 01:01:06.880
isomorphic to standard quantum mechanics out of it. So quantum mechanics is sort of, you know,

01:01:06.880 --> 01:01:12.880
inextricably bound up with the phenomenon of the multi-way system. But if you take the entropy

01:01:12.880 --> 01:01:18.800
to infinity here, then you're effectively, then the sophistication of your equivalence function

01:01:18.800 --> 01:01:22.960
becomes arbitrarily large, which means that you can describe any, essentially any pair of

01:01:22.960 --> 01:01:29.280
states as being equivalent. And so it turns out that the, that actually the quantum mechanical

01:01:29.280 --> 01:01:32.720
case corresponds to the zero entropy limit, whereas the kind of general relativistic case

01:01:32.720 --> 01:01:37.040
corresponds to the infinite entropy limit. But they're kind of two different, two fundamentally

01:01:37.040 --> 01:01:41.040
different notions of entropy, one of which exists at the single-way level, one of which exists at

01:01:41.040 --> 01:01:46.320
the multi-way level. And again, the question of how these things into play is partly why we're,

01:01:46.320 --> 01:01:50.480
you know, why we're investigating this. And it's clear that that question has links to these

01:01:50.480 --> 01:01:57.360
quite foundational questions in fundamental physics. The ideal point mass and the ideal

01:01:57.360 --> 01:02:04.480
distribution with its center of gravity and all this, Dave, question for you, from you.

01:02:05.760 --> 01:02:16.160
Yes. Okay, I was, can you hear? Yeah, go for it. Okay, good. Yeah, I'd like to hear down to the

01:02:16.160 --> 01:02:22.560
low road during this discussion, maybe show business. What people are trying to explain,

01:02:22.560 --> 01:02:28.320
what is computational reducibility or irreducibility? Often you'll see a graph that says, well,

01:02:28.320 --> 01:02:33.760
now here's the, the computation, our target running along, the fox is running along,

01:02:33.760 --> 01:02:40.320
and behind it, there's a team of algorithms that would like to catch it. And yeah, it either does

01:02:40.400 --> 01:02:46.640
or it doesn't outrun all of them, but some can come, can seem to come pretty close to

01:02:47.600 --> 01:02:52.960
catching it. Now, there's something else people have been looking at for a few years, the Mandelbrot

01:02:52.960 --> 01:02:59.440
set. The Mandelbrot set really is a determined, not only a deterministic computation, it's a crisp

01:02:59.440 --> 01:03:08.000
computation. Every set, every point, either is inside the safe zone, it's going to sit there,

01:03:08.000 --> 01:03:13.840
and it's happy and quiet, and you color it black, or it flies off to infinity.

01:03:15.520 --> 01:03:20.480
So you could, it could just be a bunch of yes or no, but that's not the way people

01:03:21.040 --> 01:03:26.320
trying to calm down after a day of work want to look at it. They want to say, hey, I want to see

01:03:26.320 --> 01:03:33.280
the 32 million deep Mandelbrot set. And I want to see it in colors. So when you get the colors,

01:03:33.280 --> 01:03:39.360
you ask, how hard did I have to work? How long did I have to grind along before I made that decision?

01:03:39.360 --> 01:03:44.000
Oh, this is the point that's going to settle down and be quiet and uninteresting, or it's going to

01:03:44.000 --> 01:03:51.840
fly off to infinity. So you put colors in. Now, I just wonder, would it be interesting to anyone

01:03:51.840 --> 01:04:00.640
to fuzzify these gorgeous causal and multi-causal graphs and just show this is the portion where

01:04:00.640 --> 01:04:08.000
we had to work really hard, rule 35 or whatever. We had to go 15,000 generations before we gave up

01:04:08.000 --> 01:04:13.840
and said, we're not going to follow this anymore. The other one, oh, after 30 steps, I see.

01:04:16.080 --> 01:04:21.920
That's a really interesting question. And I mean, yes. So a couple of comments on that. I mean,

01:04:21.920 --> 01:04:27.120
so one is, I mean, you're right in the sense that, yes, the Mandelbrot set is a very clean

01:04:27.120 --> 01:04:32.480
thing to describe. I would say it's not, I mean, I would argue it's actually not a crisp

01:04:33.360 --> 01:04:37.520
computation in that sense, precisely for reasons of computational irreducibility,

01:04:37.520 --> 01:04:41.920
because as you go arbitrarily close to the boundary of the set, you can have complex

01:04:41.920 --> 01:04:46.000
numbers that stay, that have a kind of indefinite period of transient, right? There's no

01:04:46.000 --> 01:04:53.200
upper bound on how long the ZN squared plus C orbit can last before it either diverges or

01:04:53.200 --> 01:05:01.120
converges to zero. And that statement that there will be points that can remain,

01:05:01.120 --> 01:05:05.360
that can get tied up in these orbits indefinitely is really a computational irreducibility statement.

01:05:06.720 --> 01:05:14.000
But yeah, I mean, your question about, could you construct, I mean, that way of coloring

01:05:14.000 --> 01:05:19.440
points that are on the boundary of the Mandelbrot set in that way, the so-called escape time algorithm,

01:05:19.520 --> 01:05:23.360
right? Where you kind of, where you color them based on how many steps that I need to

01:05:23.360 --> 01:05:28.800
do before it either converged or escaped off above some, or the complex numbers modulus

01:05:29.440 --> 01:05:34.080
exceeded some value. Yeah, it's an interesting idea that you could try and kind of construct

01:05:34.080 --> 01:05:38.320
geometrical representations of the space of possible computations based on a kind of escape

01:05:38.320 --> 01:05:43.120
time algorithm for computational irreducibility. Yeah, so I mean, there are the possible ways

01:05:43.120 --> 01:05:48.320
that you could do that, right? That you said, we know, we've known since the days of Turing,

01:05:48.320 --> 01:05:52.240
that the halting problem for Turing machines is undecidable, right? That there's no finite

01:05:52.240 --> 01:05:55.840
computation that you can do that will determine over an arbitrary computer program will terminate

01:05:55.840 --> 01:06:00.080
in finite time. So you could do it, you could, if you once you have a way of kind of geometrizing

01:06:00.080 --> 01:06:04.960
the space of possible computations, which we, which we have now, as you say. Yeah, you could easily

01:06:04.960 --> 01:06:08.240
construct a kind of escape time algorithm where you know, you have your analog of the Mandelbrot

01:06:08.240 --> 01:06:11.600
set, which is, you know, here's all the halting computations, here are all the definitely non

01:06:11.600 --> 01:06:15.840
halting computations. And then there's some boundary of very fuzzy stuff where we kind of don't

01:06:15.840 --> 01:06:20.080
really know, we have to do a lot of work. And even then it's only heuristic, which is, which

01:06:20.080 --> 01:06:24.800
is so, yeah, I mean, a very directly analogous. And then, yeah, and then the question becomes,

01:06:25.600 --> 01:06:31.520
you know, so in the case of the M set, you know, there is some underlying theory mostly,

01:06:31.520 --> 01:06:35.760
I think mostly due to like Dwadi and Hubbard and people that allows you to kind of predict,

01:06:35.760 --> 01:06:40.560
like, you know, if you have a finite filament, you can, like, of the Mandelbrot set, you can

01:06:40.560 --> 01:06:45.440
kind of predict where that filament is going to be based on some complicated complex analysis

01:06:45.440 --> 01:06:49.840
argument. And, you know, I mean, the question is once you, if you once you have a geometrization

01:06:49.840 --> 01:06:55.040
of the space of possible computations, can you construct some general theory like the theory,

01:06:55.040 --> 01:06:58.480
you know, developed by Dwadi and Hubbard that tells you things about, you know, the topology

01:06:58.480 --> 01:07:03.120
of the computations, whether certain regions of the space are connected, whether they're compact,

01:07:03.120 --> 01:07:06.400
whether you can make, you know, whether you can do a similar thing, or you can make predictions

01:07:06.400 --> 01:07:10.480
based on the geometry of one part of the set, you know, where you can extrapolate to things about

01:07:10.480 --> 01:07:15.200
the geometry of another part of the set. That's a really interesting question. And yeah, again,

01:07:15.760 --> 01:07:19.680
as with so many of these things, it's one of these, like, I don't know the answer, but it's a good

01:07:19.680 --> 01:07:25.840
reason for investigating, you know, for pursuing the program, right? Yeah, from the Wolfram side,

01:07:25.840 --> 01:07:32.720
as well as from the active inference side, there's both the information topology and the

01:07:32.720 --> 01:07:38.480
information geometry side, and not just in the kind of topological deep learning, but rather

01:07:38.480 --> 01:07:44.880
looking at the topology of information flows, which has been heavily developed in the category

01:07:45.120 --> 01:07:50.800
related to quantum information sciences. And then the information geometries that allow us to do,

01:07:50.800 --> 01:07:57.120
like, machine learning type, accelerated optimization ingredients, all these kinds of concepts that come

01:07:57.120 --> 01:08:04.640
into play with geometry, and we just don't get them from topology. So topology kind of sketches

01:08:04.640 --> 01:08:10.560
the skeleton, and then with the computers that we have, the information geometry is at least on

01:08:10.560 --> 01:08:15.840
the data sets and the ways that we have computation today, that's kind of like the quantitative

01:08:15.840 --> 01:08:23.920
numerical versus the formal. One question is, are these all discrete state space, discrete time

01:08:23.920 --> 01:08:30.800
formalisms? Because in active inference, we often deal with hybrid models that have discrete and

01:08:30.800 --> 01:08:35.440
continuous state spaces, and the same generative model or the same system of interest could be

01:08:35.440 --> 01:08:41.360
modeled with like a discrete time chapter seven, or a continuous time chapter eight model. So how

01:08:41.360 --> 01:08:46.400
does this deal with that? That's a really good question. I mean, yeah, so most of what I've been

01:08:47.440 --> 01:08:54.000
looking at so far has consisted of discrete time, discrete space models, for no particularly

01:08:54.000 --> 01:08:59.120
principled reason other than they're easier to analyze, right? For the most part, because you

01:08:59.120 --> 01:09:03.040
can do explicit computations because they're kind of more amenable to constructive analysis.

01:09:03.760 --> 01:09:08.560
It's easier to do, but the beauty of a lot of this, that's one of the beauties of using kind of

01:09:08.560 --> 01:09:13.440
general mathematical formalism is that once you develop it, it's often quite easy to extend even

01:09:13.440 --> 01:09:18.880
to cases that, or to extrapolate to the cases that you didn't explicitly analyze. So in principle,

01:09:18.880 --> 01:09:23.760
this formalism works for continuum space and continuum time systems as well,

01:09:23.760 --> 01:09:28.320
just with some slight modifications. So rather than having say, branching and merging, you instead

01:09:28.320 --> 01:09:32.800
have, if you think about this thing as now being a dynamical system, described on the

01:09:32.800 --> 01:09:36.640
on some symplectic manifold, then these kind of branching, merging operations of the multi-way

01:09:36.640 --> 01:09:42.800
system become effectively divergence and convergence, differential operators are defined on the

01:09:42.800 --> 01:09:48.640
symplectic manifold. And so one place where we can start to analyze that explicitly, and which

01:09:48.640 --> 01:09:52.640
I've done a little bit of work on, but it's one of these things which I want to go back to very

01:09:52.640 --> 01:09:57.440
soon, is looking at PetriNets. So PetriNets are interesting because they are a discrete time,

01:09:57.440 --> 01:10:02.640
discrete space system, but they admit a continuum space, continuum time, description

01:10:02.640 --> 01:10:06.960
in terms of ordinary differential equations and so on. So they're a nice example of a hybrid

01:10:06.960 --> 01:10:11.200
kind of discrete event versus continuum event system, where it's clear that this formalism

01:10:11.200 --> 01:10:17.040
can be used and is somehow agnostic as to whether the underlying system is discrete or continuous.

01:10:18.080 --> 01:10:21.040
Again, there's a broader philosophical point to make here, which is that

01:10:24.480 --> 01:10:28.320
in a way, one of the reasons I don't feel embarrassed to be working primarily with discrete

01:10:28.320 --> 01:10:33.280
systems is because, again, once you start to think about things in terms of, okay,

01:10:33.280 --> 01:10:36.560
you have to not just care about nature, you also have to care about the computations the observer

01:10:36.560 --> 01:10:41.920
can perform and what it's able to infer, then you quickly realize that in a sense, just like,

01:10:41.920 --> 01:10:45.520
whatever, as you were saying earlier, Danny, beauty is in the eye of the beholder. I think

01:10:45.520 --> 01:10:52.240
discreteness and continuity are also in the eye of the beholder, right? So if you have

01:10:52.880 --> 01:11:02.400
a universe that is fundamentally continuous, that's described by a continuum, the Renzi and

01:11:02.400 --> 01:11:06.800
Manifold or something, but your constraints are that the only experiments you perform have

01:11:06.800 --> 01:11:14.400
computable outcomes, have discrete outcomes, where the possible number of observables is

01:11:14.400 --> 01:11:18.400
always countable, then in a sense, it doesn't matter, right? It's irrelevant to you as an

01:11:18.400 --> 01:11:23.600
observer whether the system is discrete or continuous, because the only parts of it that

01:11:23.600 --> 01:11:27.520
you can interface with and interact with are discrete, and so you could have replaced the

01:11:27.520 --> 01:11:30.320
underlying substrate with a purely discrete mathematical structure and you wouldn't be able

01:11:30.320 --> 01:11:38.000
to tell. So in some sense, I don't feel too embarrassed dealing with discrete event systems

01:11:38.000 --> 01:11:41.840
because even if I don't necessarily believe that nature is discrete, because I don't think that's,

01:11:41.840 --> 01:11:46.880
I'm not even sure how we would be able to answer that, I'm reasonably convinced that

01:11:47.840 --> 01:11:53.200
the experiments that we can perform and the observations that we're able to perform are

01:11:53.200 --> 01:11:57.440
ultimately computable, and therefore, the underlying substrate might as well be discrete,

01:11:57.440 --> 01:12:03.120
even if it's not in reality, so to speak. Yeah, that's a great comment and definitely

01:12:03.120 --> 01:12:08.640
calls back to your earlier points about discretization being in the eye of the beholder,

01:12:08.640 --> 01:12:14.160
like in the active inference models, observations, raw data may already be discretized depending

01:12:14.160 --> 01:12:19.520
on the situation, but even if it weren't, like it were a continuous sensory perception or modeled

01:12:19.520 --> 01:12:26.800
as such analytically, still commonly models discretizing categorize as they move up cognitive

01:12:26.800 --> 01:12:33.360
hierarchies, and that was like initially explored to get more of this discrete either or decision

01:12:33.360 --> 01:12:41.120
making, planning, all those kinds of properties. Well, there's many interesting angles like

01:12:41.360 --> 01:12:48.720
I'm sure also it could be a multiplexed language model prompt, but like what are you working on

01:12:48.720 --> 01:12:57.440
or excited about for 2024? That's a good question. So I've kind of already given some hints about,

01:12:57.440 --> 01:13:04.400
you know, like this general research program of trying to understand computational complexity

01:13:04.400 --> 01:13:08.240
and algorithmic complexity and interplays between observers and systems through this

01:13:08.240 --> 01:13:14.560
category theoretic lens. That's a major thing which I started on, say, about maybe a couple

01:13:14.560 --> 01:13:18.240
of years ago. I mean, in some form, I've been working on it for a long time, but this more

01:13:18.240 --> 01:13:22.480
recent perspective on it is maybe a couple of years old. But for the various reasons over the

01:13:22.480 --> 01:13:28.640
last year or so, I've kind of put that to rest and I've been focused on these much more physics

01:13:28.640 --> 01:13:32.560
oriented questions about discrete space time and understanding things like, you know, how do

01:13:32.560 --> 01:13:36.400
black holes work and how does accretion work in discrete space time, which is also very important

01:13:36.400 --> 01:13:43.040
and very exciting. But I've sort of slightly been missing these more abstract directions. And so

01:13:43.040 --> 01:13:48.400
I have maybe one or two major physics related things that I need to finish off and then I

01:13:48.400 --> 01:13:53.200
really want to go back to this to the greatest extent possible. And yeah, I mean, so one thing

01:13:53.200 --> 01:14:01.440
is that's quite clear is that there's great interplay between this formalism and existing

01:14:01.440 --> 01:14:05.120
theories of computational and algorithmic complexity. So in particular, you know, so

01:14:05.760 --> 01:14:10.960
one very basic example is I mentioned before that, you know, you have this kind of coherence

01:14:10.960 --> 01:14:14.560
between these two different algebraic structures between your the operation of

01:14:14.560 --> 01:14:19.200
time like composition versus the operation of kind of parallel composition. And these two

01:14:19.200 --> 01:14:24.000
algebraic structures are in general related, although the precise conditions that relate

01:14:24.000 --> 01:14:27.440
them are not clear. And that's partly what we're trying to what we're trying to understand.

01:14:28.400 --> 01:14:34.800
But it turns out that degenerate cases of that of that question corresponds to unsolved problems

01:14:34.800 --> 01:14:39.840
in computational complexity theory. So, for instance, the P bus NP problem can essentially

01:14:39.840 --> 01:14:43.600
be recast in these terms that you can recast the P bus NP problem is the question about

01:14:44.720 --> 01:14:49.840
is the coherence between the time like composition of computational complexity and

01:14:49.840 --> 01:14:53.680
the parallel composition of computational complexity, which are what P and NP respectively

01:14:53.760 --> 01:14:58.560
are really about, it are those coherence conditions, the strictest they can be,

01:14:59.360 --> 01:15:04.080
which would be the case that the P equals NP, or are they somehow more lax, which would be the

01:15:04.080 --> 01:15:09.280
case that P does not equal NP. And so there's, you know, that's that's one thing that we kind

01:15:09.280 --> 01:15:12.800
of were already investigated, but it's clear that a whole bunch of questions about, you know,

01:15:13.760 --> 01:15:18.000
how the time complexity and space complexity trade off or how to come over of complexity and

01:15:18.000 --> 01:15:22.080
time complexity trade off, these, these are questions which can be recast in this kind of

01:15:22.080 --> 01:15:28.240
more algebraic category, theoretic lens, and, and, and will hopefully give insight into this

01:15:28.240 --> 01:15:32.480
general program of trying to understand observers and their relationship to the world. And those

01:15:32.480 --> 01:15:37.280
are kind of major, well, with any luck, those are major theorems that I hope we'll be able to

01:15:37.280 --> 01:15:44.640
prove at some point in 2024. That's that's that's that's awesome. And it makes me think about

01:15:44.640 --> 01:15:51.760
parallel, more nest mates, more CPU threads, deeper in times, more sequential, more planning,

01:15:51.760 --> 01:15:57.360
and more cognitive single monolithic agent. And then the kind of question is like, can anything

01:15:57.360 --> 01:16:05.200
that a single agent, mega matrix could do, cannot be decomposable at space advantage,

01:16:05.200 --> 01:16:11.440
or even at space disadvantage in decomposed it into a single time step operation?

01:16:12.320 --> 01:16:17.280
Yeah, that's a super important question. And one that, you know, with the possible exception

01:16:17.280 --> 01:16:22.880
of this community, not many people have asked, right? I mean, so that's something which comes

01:16:22.880 --> 01:16:28.560
up in quantum computing, right? So a lot of the hype around quantum computation comes from these

01:16:28.560 --> 01:16:32.960
theoretical speedups that derive from the fact that you're able to, you know, you're able to

01:16:32.960 --> 01:16:37.680
support these super positions of different, you know, where, you know, each, that each state of

01:16:37.680 --> 01:16:41.120
your data structure corresponds to a different eigenstate, and you're able to evolve some super

01:16:41.360 --> 01:16:44.720
superposition of those eigenstates. But then at the end, you have to actually come to a

01:16:44.720 --> 01:16:48.320
definite conclusion about what the answer is, you have to perform some measurement operation.

01:16:48.320 --> 01:16:52.400
And that measurement operation is lossy, it's often non deterministic, you, you know, you often

01:16:52.400 --> 01:16:58.000
have to repeat it multiple times. And, you know, it's becoming increasingly clear that for a large

01:16:58.000 --> 01:17:02.640
class of operations that were previously thought to have quantum advantage, the additional complexity

01:17:02.640 --> 01:17:06.560
of the measurement step really kills any quantum advantage that you may have had that you know,

01:17:06.560 --> 01:17:11.040
you get some advantage by doing unitary evolution. But then you lose all of it by having to do

01:17:11.040 --> 01:17:16.160
the submission projection at the end. And that's really a story of, again, this interplay between

01:17:16.160 --> 01:17:21.040
the time complexity saving of doing a multi doing a computation of doing a multi computation in

01:17:21.040 --> 01:17:25.920
parallel, versus the loss that comes from the complexity of the equivalence function that

01:17:25.920 --> 01:17:30.640
you need to apply in order to get to some definite conclusion about what the, you know, about what

01:17:30.640 --> 01:17:35.040
happened because, you know, ultimately, you need to somehow collapse that that that directed graph

01:17:35.040 --> 01:17:39.360
into a single thread of time in order to be able to have some coherent representation of what happened.

01:17:40.160 --> 01:17:43.840
And so again, understand that, you know, that's a place where understanding these, you know,

01:17:43.840 --> 01:17:48.640
the these tradeoffs will become very important. And as I say, in some limiting case that that gives

01:17:49.280 --> 01:17:53.200
some perspective on your question, Daniel, which is, which I agree is a very interesting question

01:17:53.200 --> 01:17:59.600
about, you know, in principle, we know that anything that a deterministic Turing machine

01:17:59.600 --> 01:18:04.160
can do a non deterministic Turing machine can do and vice versa with some speed up or slow down.

01:18:05.120 --> 01:18:10.480
But that statement, which is a classic result in, you know, in computability theory, neglects

01:18:10.480 --> 01:18:13.840
all consideration of the equivalence function. So there may be cases where the equivalence

01:18:13.840 --> 01:18:19.040
function is so complex, that essentially, you know, that to do state equivalence becomes

01:18:19.040 --> 01:18:23.200
undecidable. And so in that case, you have a scenario where actually, you know, you've got a

01:18:23.200 --> 01:18:28.720
multi computational system, but to collapse it to one that's equivalent to a single way system

01:18:28.720 --> 01:18:32.000
requires unbounded amounts of computational effort. And so that's so actually they become

01:18:32.080 --> 01:18:38.000
inequivalent, even though, you know, computability theory says they should be the same. So it's

01:18:38.000 --> 01:18:43.280
clear that there's a there's a more rich, more subtle theory that's underlying here that we're

01:18:43.280 --> 01:18:46.720
just beginning to kind of glimpse, and that I hope will, you know, we'll be able to kind of

01:18:46.720 --> 01:18:50.000
to prove some new limited results about soon, once we understand it a bit better.

01:18:51.360 --> 01:18:57.520
Awesome. And I think definitely a special shout out to all of our colleagues on either like the

01:18:57.520 --> 01:19:04.640
Wolfram and or active inference side, because we've seen few if any active inference models

01:19:05.520 --> 01:19:12.640
phrased analytically or computationally with the Wolfram technology from studying complexity

01:19:12.640 --> 01:19:19.200
in other areas. It's really clear to see how productive and powerful the software and the

01:19:19.200 --> 01:19:25.600
tools can be and changing and growing every day. So it's really interesting, maybe someone can

01:19:26.560 --> 01:19:33.200
if they're listening this far in, like, go from one side to the other and back or make a Wolfram

01:19:33.200 --> 01:19:41.040
active inference model, or do some other kind of combination, because it's very fruitful territory.

01:19:41.920 --> 01:19:51.440
And we know that our elders have already spoken, they've okayed it. No, but really, it's so

01:19:52.240 --> 01:19:59.280
rich with connections here between the areas that we're all studying and feeling like converging

01:20:00.000 --> 01:20:04.880
on many common places to scaffold and jump off from together.

01:20:07.040 --> 01:20:13.440
Yeah, I agree. And I mean, at least the kinds of things that we were discussing here about,

01:20:13.440 --> 01:20:17.360
you know, speculative execution and behavior formation through free energy principle and

01:20:17.360 --> 01:20:21.680
so on, those things should be relatively easy to implement in the framework that's already been

01:20:21.680 --> 01:20:27.360
developed here. I mean, so that's just a question of just implementing some kind of computation of

01:20:27.360 --> 01:20:32.720
expected free energy and using that to weight multiway paths in the speculative execution model.

01:20:33.840 --> 01:20:40.480
So at least the beginnings of that implementation, I think the path is pretty clear. And we will

01:20:40.480 --> 01:20:44.720
probably end up doing at some point in the future anyway, as part of other research.

01:20:45.680 --> 01:20:50.480
So yeah, so I agree. It's a very exciting kind of point of interface.

01:20:51.040 --> 01:20:56.960
Yeah, well, I hope that we can stay in touch if you ever want to come back for a

01:20:58.160 --> 01:21:04.480
009.2, or if we want to even facilitate some kind of working group or some connections

01:21:05.040 --> 01:21:10.400
to really strengthen and like include the participation of more people in this super

01:21:10.400 --> 01:21:15.360
exciting area, that would be amazing. Yeah, that sounds fun. Let's let's try and set something up.

01:21:16.160 --> 01:21:22.880
Well, Dave, first penultimate comments, then Jonathan, you can have the kind of last comments.

01:21:23.680 --> 01:21:31.120
Yes, I hope you do get to continue on the Wolfram physics side to think about a more general notion

01:21:31.120 --> 01:21:37.840
of what these ultimate things are. Are they observers or does that already prejudice the case

01:21:37.840 --> 01:21:45.360
of what you might find if you call them workers or actors or, you know, go back and ask Stuart

01:21:45.360 --> 01:21:53.520
Kaufman, what must a mind do to earn its way in the world? Let's make this keep happening. Thank you.

01:21:54.400 --> 01:21:59.920
Thank you, Dave, for suggesting it also. It was a great suggestion. Jonathan?

01:22:00.880 --> 01:22:06.720
No, I think that's a fantastic note to end on. I mean, in a sense, this idea that we should

01:22:06.720 --> 01:22:10.240
start to move, I mean, so it's okay, big picture for a moment, like

01:22:11.840 --> 01:22:16.240
where, you know, this formalism is being developed, the formalism I've described in this

01:22:16.960 --> 01:22:20.960
in this discussion is being developed, you know, assuming a kind of purely passive observer

01:22:20.960 --> 01:22:24.560
idealization. And that's already been incredibly difficult, right? This is clear, there's a lot

01:22:24.560 --> 01:22:28.960
we don't understand at that. But of course, David is right that in a sense, you know, what, you know,

01:22:28.960 --> 01:22:33.760
ultimately, we want to start transitioning to a participatory observer model, where you allow

01:22:33.760 --> 01:22:37.280
for two-way interactions or, you know, higher order interactions between observers and systems

01:22:37.280 --> 01:22:41.840
and things that don't just go in one direction. And yeah, you know, in a sense, I view a lot of

01:22:41.840 --> 01:22:45.600
what we're trying to do with, you know, trying to nail down these notions of causality, trying to

01:22:45.600 --> 01:22:49.760
understand these interplays between different complexity and entropy measures as, you know,

01:22:49.760 --> 01:22:54.400
the necessary groundwork for developing that subsequent theory, right? That does, you know,

01:22:54.400 --> 01:22:59.600
it's clear that if we want to have a version of the, you know, of this kind of compositional

01:22:59.600 --> 01:23:04.000
multi-way formalism that is also compatible with things like second-order cybernetics,

01:23:04.000 --> 01:23:08.400
then, you know, at the very least, we need to have a very coherent notion for what causality is

01:23:08.400 --> 01:23:13.760
and, you know, and a robust algebraic description of that that's not going to break or change.

01:23:13.760 --> 01:23:22.720
And so I think the nonparticipatory observer model is a useful starting point because it's one that

01:23:22.720 --> 01:23:28.160
is just within our grasp of, you know, of being kind of mathematically tractable. And then the

01:23:28.160 --> 01:23:32.320
hope is that the technology and the ideas and the conceptual structure that we develop for

01:23:32.320 --> 01:23:37.360
understanding that will then, as I say, lay the groundwork for developing something that's more

01:23:37.360 --> 01:23:44.480
like what real, you know, active participatory observers do. And, and yeah, I mean, I think I

01:23:44.480 --> 01:23:49.760
don't, on the, on the scientific side, I'm not really sure I have any final comments apart from

01:23:49.760 --> 01:23:53.360
just, you know, as obvious, this is a, you know, this is still a story that's being, that's being

01:23:53.440 --> 01:23:59.200
developed. And, and yeah, as Daniel alluded to, I hope that we can continue to interact and collaborate

01:23:59.200 --> 01:24:04.320
where, where that makes sense. And, and yeah, at the very least, I think, in cooperation of kind

01:24:04.320 --> 01:24:09.600
of these, you know, these active inference models within these discrete time, in the first instance,

01:24:09.600 --> 01:24:14.400
discrete time, you know, computational frameworks, and allowing things like speculative execution

01:24:14.400 --> 01:24:19.760
and multi-way path waiting based on free energy estimates. I think that's, you know, that's a,

01:24:20.000 --> 01:24:25.840
that's a project that's of obvious mutual interest and, and something that I, that I hope will happen

01:24:25.840 --> 01:24:32.160
in the coming months. Thank you. We just speculatively executed active Wolfram inference.

01:24:33.520 --> 01:24:38.320
Basically. Sounds very good. All right. Thank you, Jonathan. Thank you, Dave. Thank you, everyone.

01:24:39.040 --> 01:24:42.160
See y'all next time.

01:24:49.760 --> 01:24:51.140
you

01:25:19.760 --> 01:25:21.140
you


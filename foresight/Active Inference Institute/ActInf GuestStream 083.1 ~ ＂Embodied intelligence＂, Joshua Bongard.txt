Hello and welcome. It is July 17th, 2024. We're in active inference guest stream 83.1 with
Josh Bongard. Thank you for joining. This should be very exciting. We'll have a presentation
and then some discussion. So if you're in the live chat, please feel free to write any
questions. And thank you again, Josh, looking forward to this. Yeah, thank you, Daniel. And thanks
to those of you that are attending online. So my name is Josh Bongard. I'm a professor of computer
science here at the University of Vermont. And my bread and butter in my lab is the study of
robotics and AI. And obviously, we're in the middle of the current AI summer. So what I wanted to do
today is show a couple of highlights from my group, things that I've worked on in the past and
that we're working on at the moment, that I hope in the long term will help us realize sort of the
long term vision for a lot of those trying to create intelligent machines, which is to create
machines that are helpful, but but also safe. We're part of the way there. But as anyone who's
worked with chat GPT or stable diffusion knows or even has a robot vacuum cleaner at home,
there are some limitations to our current technology. It's hard to create machines that are
autonomous and useful and safe all simultaneously. So what are the things that we're missing?
That's what I wanted to sort of seed the pool with today and hopefully we can move on to an
interactive discussion about it. So I'm going to leave this slide up and just sort of talk over
this slide for a few minutes that will hopefully generate some food for thought and then questions.
This is a snapshot from some of the projects I've worked on over the years. First thing you'll
probably notice is there's a lot of different robots that have very different structural
properties. They not only act differently, but look very different. And that is a fundamental
foundation in everything we do in my research group, which is to try and understand how the
body facilitates cognition. Years ago with my PhD advisor, we wrote a popular science book called
How the Body Shapes the Way We Think. And we can mean that literally or figuratively. We wrote that
book a while ago. We made some arguments about how the body shapes the way we think. And since that
time, my group and others have formulated other arguments for why or how the body shapes the
way we think. And I'm hoping to survey some of those today. So as I mentioned, you can see
a lot of different robots here, very different structure. They've got very different form and
function. But across each of the pair of videos that you see here, you'll notice that there's also
a common pattern, which is on the left side, you tend to see something that's virtual. And on the
right side of the video pair, you tend to see something that's physical. And this illustrates
the basic approach that my group takes to understanding how the body shapes the way we think,
which is to create AI that creates robots, creates embodied AI. So what do I mean by that?
What I mean is that in all of the projects that you see here, we create an AI that searches the
space of all possible robots that could solve the tasks that we want the robots to solve.
Most people that are familiar with AI and robotics and autonomous cars and drones have
a rough understanding that AI is somehow optimizing or tuning the brain of the autonomous car or the
drone or the robot, what have you. There's an underlying assumption in all of that current,
in most current robotics, which is that the AI tunes the brain, but does not tune the body.
Tesla cars are dreamed up mostly by humans and an AI tunes their brain or their control policy.
But of course, nature doesn't work that way. Nature produced us and all the other intelligent
organisms on this planet by carefully tuning body and brain simultaneously. Certain bodies
facilitate certain kinds of behaviors and certain kinds of intelligent behavior,
and other bodies don't. They obstruct that particular behavior or that intelligent behavior.
So in all of our work, we ask the AI not just to tune the brain of a robot, but its body
simultaneously. And as you can see visually here, the AI often comes up with bodies that are well
suited to whatever we want them to do. So if you direct your attention to the very top left of
the screen for a moment, this is now a 22 year old experiment, but I think it still visualizes
the potential of this approach. In this case, we were interested in creating a robot that can
brachiate, that can swim, swing across beams or tree branches or electrical wires for various
inspection tasks. And you'll notice that in this case, the AI came up with a solution that in
retrospect seems intuitive. The robot has to carry a very heavy battery, which you can see in
the physical robot in the top left there, the black box that's at the bottom. And the AI has
figured out how to design the body of this robot so that it's actually able to exploit the forward
momentum of this heavy object, this battery, to facilitate its movement or brachiation across
this physical beam. So it's a simple example, a simple robot, a simple task, but it demonstrates
this interplay between AI, robotics, body and brain. If the AI was not free to place the body,
to place the battery at a particular place on the robot's body, it would be much harder,
it would require more energy, it would require a more complex brain for the robot to figure out how
to move its heavy weight across this beam. So that idea of tuning body and brain has
suffused everything that we work on. Some other examples you can see in the top center and the
top right. Here we have a robot that suffers damage, its body changes over time. So now the AI has to
grapple with not just designing a body, but grappling with a body that changes. One of the
things that we as intelligent organisms here in the world and all embodied AI, all autonomous cars,
all drones, all robots have to deal with is entropy. The world throws a lot of stuff at us,
we have to deal with wear and tear, injury. In our case, we grow from a single cell into about
10 to the 12 cells. We change massively in terms of our physical magnitude. How do you continue
to operate, keep yourself alive and do whatever it is you need to do across massive morphological
change? That is not an easy thing to do. And again, it requires an AI, if it's going to do this with
robots, it's got to figure out how to carefully tune body and brain to deal with the generation
of behavior inside a body that is changing, either unexpectedly due to injury and wear and tear,
or intentionally. You can see in the very center panel, this is again a pretty good
visualization of where designing body along with brain comes in handy. If we want to make
flying machines or swimming machines, we have to very, very carefully tune the geometry and
material properties of the body itself to realize flight. So what you're seeing in this middle panel
here, this is partway through the AI experimenting with the design of different kinds of wings
for an ornithopter, a drone that flies by flapping its wings. This flexible wing that you see here
in the center of the screen, this is a bit of a transition from traditional robots that are made
of rigid structures, like you can see in the top row, into a more modern era in robotics,
which is sometimes referred to as soft robotics. Material science has come a long way in the last
20 years. So we can now start to build robots embodied AI. We can start to build robots out of
materials other than rigid plastic and metals. And we can again, we can start to move into an era
in which robots like organisms can exploit the material properties of their bodies
to facilitate whatever behaviors they need to do to survive or be useful to humans and so on.
So in the middle right panel, this is a highlight of some work that my group has done in collaboration
with Rebecca Kramer, Bottiglio's lab at Yale. Rebecca's lab is famously advancing the state of
the art in soft robotics. What can you get robots to do when they're made out of soft materials?
You can see an example of some of those soft robots. Middle right and a very different soft
robot lower left, which is exploiting its body properties in order to move in interesting
ways. One of the interesting things about soft robotics in my perspective is that it starts to
usher in an era in which robots can actually grow and complexify their bodies. You can see these
hollow cubes in the middle right and these hollow sort of chambers in the bottom left
expanding and contracting as we push and pull air into and then out of the body of the robot.
Suddenly, now you have robots that can change their geometry. They can change their volume.
They become what's known as thermodynamically open. It's a fancy term for meaning that they can
draw new material and new energy into themselves. The thermodynamically open machines that you see
middle right and lower left, the only thing they're drawing into their body is more air,
but it's a start. I mentioned already that humans grow from a single cell into 10 to the 12 cells.
Every organism on this planet, with a few exceptions, starts small and gets bigger over its
lifetime. That fundamental morphological change starts small, starts simple and gradually grow
in size and complexity. That provides scaffolding. It provides a gradient on which to learn how to
gradually grapple with the world around you. Most organisms, again, there are exceptions,
are not thrust into this world with all of their machinery online from the beginning.
Just the way I'm phrasing this is obviously intentional to sort of dichotomize growing
organisms and robots with fixed bodies. Autonomous cars are still very dangerous.
Autonomous drones are still very dangerous to be around because 99.99% of the time,
they do the right thing, but every once in a while, they don't know what to do,
and no one knows what they're going to do in those uncertain circumstances. That is a very
concerning situation as we start to now deploy robots and autonomous vehicles into everyday
environments where they are in close proximity to humans. Why is it that even with state-of-the-art
AI and with all of Google's data centers and AI training algorithms, we still can't rub out that
0.001% where no one knows what's going to happen? Part of the reason, again, is these machines
are born with complex bodies. We drop a controller into a one-ton autonomous car made of metal and
plastics. It's very dangerous. We don't grow autonomous cars from a very small, simple,
lightweight machine that can't cause anyone any harm whatsoever, and then when that simple,
small machine demonstrates and verifies to us that it's safe, then we allow it to become larger
and more complex. It sounds like silly sci-fi. Why would we build a machine like that? But again,
every organism starts simple and grows in complexity. If it doesn't do the right thing,
if it performs dangerous actions that are harmful to itself or fatal to itself, by definition,
it doesn't get any further. That's, again, one of the ways of thinking about how the body shapes
the way we think. In my personal and professional opinion, any physical machines that we deploy
into the real world, they should start as very small, lightweight machines that can't harm anyone.
They have a very limited number of actions that they can perform, and they sort of cycle
through all those actions and verify everything, and only then can they take more mass, more energy
into themselves. Can they recruit more material? They can sort of be allowed to be thermodynamically
open and grow and complexify themselves. There's lots of ways in which we're starting to create
machines that grow and complexify themselves. I just talked about these soft robots that can pull
in air or pass possibly fluids. They could be hydrodynamic machines. They could mechanically
or magnetically connect to other robots. That sort of swarm robotics. That's another path
to growing machines. At the moment, most of these machines are still restricted to academic labs.
They also are not safe yet, but I think in the long run, they're going to be a safer alternative to
dropping AI into very large, complex, heavy, dangerous machines and crossing our fingers
and hoping for the best. I've talked a little bit about rigid robots and soft robots.
I want to try and talk as little as possible. There's some time for a good discussion,
but I want to talk about what I see as sort of a third era of robotics and embodied AI,
which is just starting to open up in the last few years, which is biobotics or creating biobots.
You can see two biobots on display at the bottom center of my slide here. A biobot is a robot
that's made up of only biological components, no technological components whatsoever. In the bottom
left here, Kriegman, Blackiston, Levin and myself in 2020 published a paper demonstrating the first
biobot. This became known after publication in the popular media as Xenobots, X-E-N-O,
Xeno, Xenobots, because these Xenobots are built from about 2,000 frog cells and the cells were
taken from a particular species of frog, which is Xenopus laevis. Michael Levin, our biology
colleague at Tufts University, is world-renowned for demonstrating that you can reconfigure
genetically unmodified materials like, for example, frog cells, and that rearrangement
of living tissues not only does not kill the organism, the organism is able to in some cases
continue on doing what it does, what it needs to do, ingest materials, survive,
reproduce in this reconfigured state. There's a lot of biological implications for that.
One of the biological implications is that frog DNA does not code for frog. What you're looking at
in the bottom left, the Xenobot is about a millimeter in diameter, so it looks like a speck
of pepper to the unmagnified eye, and yet it's able to walk around the bottom of a petri dish.
It doesn't have all the features of a living organism, but it's got enough of them that it's
motile. It's able to get itself from point A to point B. One of the other implications for AI
of this biological discovery that you could rearrange genetically unmodified living tissues
is that maybe we can task an AI with discovering novel rearrangements of living tissue to produce
robots, to produce something that moves around and does something useful on a human's behalf.
That's what I mean by a biobot, a biobot that's made from, in this case, genetically unmodified
cells. The swarm of Xenobots that you see in the bottom right, as you can see,
they're sort of pushing around some material in their dish. This sort of visually hints at
applications for this type of robotic technology, which is they might be able to act like very,
very small Roomba robot vacuum cleaners in the future. They might be able to collect
microplastics out of waterways or cancer cells out of bloodstreams. The swarm that you see that's
cleaning up in this slide at the bottom right, the material that they're cleaning up is actually
other frog cells. It turns out that if the AI designs this swarm just right and the swarm
that you're looking at, this is an AI designed swarm, the AI came up with the shape for each
member of the swarm. This swarm is pushing these little white circles, which are individual frog
cells, pushing them into piles. Turns out these individual frog skin cells at a certain stage
of development are sticky, and they clump together into a pile. Some of these piles,
if they're big enough, if they contain enough frog cells, they will grow very small hairs
on the surface cells, the cells that are on the surface of the pile. Those little small hairs are
called cilia. They're usually used to pull dirt and pathogens off the body of frogs, adult frogs.
Here, when those cilia grow on small piles of frog cells, they're able to exert enough force
against the surrounding water that these piles start to move. What you have in essence is a child
xenobot. This swarm pushes cells together and in essence makes copies of themselves.
This is another implication of this work, is that in this case the AI has figured out how to design
robots that replicate. They make copies of themselves by finding raw material in their
environment and constructing copies of themselves. This has been a long-standing dream in robotics,
dating back a very long time to John von Neumann in the 1950s who had a thought experiment. It would
be great if we could create robots that would create copies of themselves, which would create
copies of themselves. If those robots do useful work for humans as a side effect, for von Neumann
that was creating moon bases and then Mars bases and then colonizing the galaxy, which sounds great.
But the seed of this idea is if we want robots to really be useful at scale, instead of manually
constructing billions of robots and then deploying them to do something useful, which is expensive,
it would be much cheaper to make one robot that does something useful for us.
By the way, it also makes two copies of itself, which does more useful work for us in four and eight
and sixteen and so on. We're not there yet with the Xenobots, but it's a demonstration that that
is possible. Again, all of that becomes possible because the AI is designing both the bodies
and the brains of these robots. This is very far now from the traditional view of AI and robotics,
where we build a robot body, we humans build a robot body, and then the AI tinkers with the brain
of that fixed machine. Part of the reason why I'm here today and part of the message of my group is
we need to think more broadly about how to combine AI and robotics and possibly synthetic biology.
When we do and we think more broadly, there are whole new paths that open up to ways in which
we might create in the future, not yet, but in the future, create intelligent, useful, and safe
machines. In the current era in AI, there is one particular approach, which is auto completion of
tokens, which has come to dominate the field and come to dominate the popular imagination. We all
kind of have an understanding more or less of what chat GPT is doing, and there are some very strong
lobbying organizations out there that are bent on convincing us that if we just do this with
more compute, more data, we will eventually get to safe machines. My contention is there just
isn't enough data out there to make non-embodied AI like chat GPT and stable diffusion and all the
rest to make them safe. We have to think differently about designing bodies and brains of machines
simultaneously to realize this long-term goal. Okay, I've been talking for a while. I'm going to
stop and I'm happy to take questions or engage in some discussion, and I'm happy to come back to
any of these experiments and provide more detail if that's helpful. Over to you, Daniel.
Thank you. Wow. Awesome. What a cornucopia of bodies and minds.
It was a great overview. I was really struck by some of the similarities and the convergence on
whole of lifecycle design and kind of holistic design coming from, on one hand, a systems engineering
and a materials perspective, and on the other hand, from the biology perspective with like eco-evo-devo
and this convergence upon needing to think about how the end-to-end function maybe even past the
point of functionality like into the planned graceful decay of a robot as well. So it brings in a lot
of topics that even from an outsider's perspective seem to be put as kind of secondary. So that's
very cool. Okay, great. I'm looking forward to what people in the live chat, right?
My first question is how over these 22 years, how have the materials, the theories, like the
contexts, advances in turing computation, all these kinds of things, how have they intersected
just what has the ride been like as you pursued these questions?
Yeah, I think the short answer again is focusing on the physical aspect of AI and robotics. So the
materials from which we can build machines has changed over 20 years. And from my perspective,
the experiment, the top left there, that was something I did as part of my PhD,
you know, the materials at the time, it was very hard to build a robot. You bought some
motors, you bought a battery, you bought some metal, you bought some wires, and you wired
everything up. There was the assumption that bodies were fixed. And not only that, but they were
difficult to make. So once you made one, you were very careful with it to make sure it didn't change,
that it didn't become damaged. And that seemed to comport with a lot of the theory in AI and
neuroscience, which had the same sort of idea that the brain, or in the case of robotics,
the control policy is the puppeteer. It's something that's pulling the strings of a fixed thing,
either the body of an organism or a robot. And if you look at a lot of theory in both fields,
AI and neuroscience, that assumption runs so deep. So for example, an active
inference, you know, the free energy principle, we want to reduce surprise all that there's a
fixed set of actions that we perform to try and reduce the surprise between what we're sensing
and what we predicted we would sense given the past action. Where do those actions come from?
Why are they fixed? Does the set of actions grow over time? Maybe the sensory data that's coming
is coming from a new sensor that's just coming online or a sensor that's recently duplicated.
Now there's two of them, but they're not quite reporting exactly the same thing. There's a whole
bunch of assumptions underlying a lot of the theory about active inference, predictive coding.
You name it, you pick your concept from neuroscience or cognitive science or AI.
Once you peel back those assumptions, imagine the robot's body changes. Imagine the robot splits
in two and becomes two copies of itself. A lot of the theory and the formal underpinnings of
that theory break down. You start to get into ill posed questions, which force you to now
think about how do you fundamentally change the theory? If you have a hierarchy of actions,
like in predictive coding or active inference, what if that hierarchy is growing and changing
over itself is growing and changing over time? How do we address that in a formal manner? So
to get back to your question, I think these advances in what we can do physically, we can
build robots now out of soft materials. We can build robots out of living materials,
which on their own will grow and divide and seek out energy and material on their own.
Those physical machines, these odd new kinds of creatures, are militating. They're pushing
against the theory. Specifically, they're pushing against these unspoken assumptions that lie
underneath a lot of this theory about what's required to act intelligently in a complex world.
That's awesome. Like the real world and the territory expanding into our unknown unknown.
Okay, there's a bunch of questions in the live chat. So I'm just going to go for them,
give any answer that you like. Okay.
Sure. David Williams wrote,
How do you think about the controllers in your robotics? Embedded AI at least today is rather
hard. Batteries and chips, PCBs, not soft and not easily synthesized locally. So how do you
think about the controllers in your robotics? Yeah, great question. So, right, exactly. The
controllers are dealing with hard rigid fixed components. We need to start thinking about
controllers that can, in which, for example, the input layer and the output layer can grow and shrink
over time. There may be new sensors or new input coordinates that are growing or being attached
to a machine. And the controller needs to be able to carefully deal with those new input channels
while the machine is operating. Same thing goes for the output channel. There may be new
actions or new dimensions of action along which the machine can act. And control policies,
reinforcement learning, all the rest of it does all those assumptions that make reinforcement
learning work, which is what drives most autonomous vehicles at the moment, assumes
that the dimensionality of input and the dimensionality of output, the things that the machine
can do and sense, are fixed during training or during behavior generation, during execution.
That is absolutely not true in any organism on the planet. And that's becoming increasingly untrue
for our coming machines. Now, how to do it well? I don't have any answers, but we have to figure
it out. You were asking a question about thinking about controllers. That's a concrete example
about how we have to rethink control policy optimization, even if we're not thinking directly
about the body, even if we just focus on the control policy and ask what happens as the input
and output channel, the dimensionality of the input and output channels change during behavior
execution. Yeah, just one short point on that. It's like training with a fixed set of perceptual
elements or of affordances or actuators. It's like training on one point in a larger space of the
adjacent possible of bodies or of architectures. So then, okay, we're bringing all this compute
to train a special case in the fixed setting. And that's not even how the smallest organism works.
So that just again, kind of shows that point. Okay. Sorry, before we move on from that point,
just to again, illustrate how the body shapes the way we think. In the case of a growing biological
body, there are new input channels that come online throughout our lifetime, but they don't
appear de novo. Whatever it is, whatever that new input channel is, as we're growing, we just have
more sense cells. The signals that they're sending into the peripheral and central nervous system
are not orthogonal to whatever else is already coming in as input, because new input channels
or new cells are slowly dividing. And at the moment of division, they're providing exactly the same
signal as some other sensory channel that already exists. So the body, or in this case, biological
growth provides an immediate scaffold, a gradient. In robotics, it can be very scary to think about
like attaching a sensor to an autonomous vehicle. What the hell does it do with this new information
that's coming in? Because we haven't thought carefully about how to add that new sense modality
to the machine. Again, we have to look to nature that every new sense modality is gradually coming
online and gradually drifts away or becomes increasingly orthogonal to the starting input
modality. So that's how we should, if we did that physically with machines, it would simplify
reinforcement learning or would make it easier for reinforcement learning or what have you,
sorry, let me reshare my screen here, it would simply make it easier for the,
sorry, something seems to have gone wrong here, give me a moment.
Yep. Okay, all right. Yeah, it makes things easier on the control policy optimization process
if new sense organs and new motor outputs are coming online, but they resemble things that already exist.
That's super interesting. Brings up a lot of questions about like self and non self recognition
and what is a self as new and different senses and actions come online. Sure. Okay. Prakash
Kavi asks, do these bio bots have any sense of agency? What is your sense? I'm quite intrigued
by the idea that beyond a critical point, they start growing hair. And do these bio bots act
independently of each other? And also what happens at a group level? So what's your sense
of agency in bio bots? And I guess the bio bot and the group level?
Yeah, it's a great question. So I'll start with the disclaimer. I'm not a biologist. I'm a computer
scientist by formal training. So I can only say so much about what the cells are doing and what
they want to do. I definitely follow in the footsteps of the late Daniel Dennett in that
when we talk about agency, we each of us individually has to decide whether or not we
take the intentional stance or not. It's in my opinion, it's a point of view. If it's easier to
explain what the Xenobots are doing by talking about what the cells want to do, like grow cilia
and coordinate their actions, fine. If it's easier to explain what the Xenobots are doing by not
taking the intentional stance and describing cells as mechanical components that are transforming
input into actions, that's fine too. This is something also that comes from my colleague,
Mike Levin. It depends. As scientists, if we want to try and explain and understand what these
machines are doing, if taking the intentional stance makes explanation easier, fine. If not,
then not. But attributing agency is sort of an objective property of the bots or the cells themselves.
Independent of us is observers. To me, that's philosophically and practically problematic.
As far as I know, there is no objective measure of agency in cells, let alone inorganic robots.
Super interesting. That's like the second order cybernetics or the observer theory
or the poly computing question, which is to say just looking at something and then
treating one's perspective as objectively. The case, it is objectively the subjective experience.
Absolutely. Now, that being said, again, there is an empirical side to this. We can make some
progress in understanding the Xenobots by comparing them against a control. So instead of cells,
if these were magnets or some complex mechanical system in which more of us are comfortable in
saying there is no agency, it's just a bucket of cogs doing something, and that control does not
exhibit kinematic self-replication, for example, or it's much harder for the AI to figure out how
to put together non-agential components to do what it is, then I feel a little bit more comfortable
by saying the cells are doing something more. Now, I don't know whether it's agential or they
want to do something, or if it's free will or consciousness, I don't know. But if we can point
at biobots or machines that are built from biological components and say it's easier to get them to do
things because they become complicit in the overall goal compared to mechanical parts,
which don't, okay. And again, as a roboticist, the top and the middle rows that you see on my
slide here, when we do build things out of metal and rubber and plastics and ceramics,
it's usually super hard. It's really hard to get them to do whatever we want them to do.
We've been working on robotics since the end of the Second World War, and we've got the Roomba,
and maybe we've got autonomous cars, we're getting there. It's taken a really long time
because robotics is really hard. It's really hard to convince physical materials to adapt and do
something useful and safe. On the flip side, we've been working on Xenobots at the bottom here.
We've been working on them for about five or six years, and we've got Roombas. We're making faster
progress in robotics when we build from cells than when we build from metal suggests the cells are
somehow helping. I don't know that they want to help. We've got to be careful there. That's the
intentional stance, but when you try and compose machines from smart machines and cells are smart
machines, I know I'm biased, but I think we're making faster progress than when we build machines
out of inert materials. Yeah, super interesting. Okay, I'll read some comments from David Clement.
David wrote, does your work incorporate a gentile hierarchies? For example, does Xenobots grow by
replicating the initial seed cell into a higher order system? And is it critical for lower order
systems to act as a component of a virtual machine, meaning that they have a target behavior that is
less than the higher order system? And that's kind of related to Prakash's question as well. Like,
how do you bridge that from the individual component into the swarm or the aggregate?
Yeah, it's a really good question. So, absolutely, I think that when we started working on the Xenobots
and Mike Levin started to talk about machines made of machines, made of machines, that definitely
has influenced the work in my group to focus on this issue of hierarchy. I don't know about a
gentile hierarchy. Again, we just talked about a gentile agency, that's maybe a
subjective stance. But definitely, you know, why would you want to build machines out of
machines out of machines? At the moment, our state of the art robots, like autonomous vehicles,
are not hierarchical. The control policy operates at the level of the machine as a whole.
For example, if there's an emergency blowout of the tire, an autonomous vehicle, the tire itself,
the rubber that makes up the tire, doesn't deform and try and fix or reduce surprise all locally.
It can't. It's rubber. It's inert material. We don't have machines built of machines built
of machines yet. But as biology in general and the Xenobots in particular demonstrate,
there's an adaptive advantage to being a hierarchy of physical things, of physical machines.
If there is a surprising event at the level of the machine as a whole,
but that surprise trickles down through the hierarchy, it's unlikely that everyone at every
level of the hierarchy is going to be surprised. Someone somewhere in the hierarchy is going to
say, from my local view at least on this bigger surprising issue, I know what to do. So let me
start to communicate to my peers and up the hierarchy to deal with surprise. That would be,
from an engineering point of view, that would be a good thing to have in big, heavy, fast-moving
robots that are near humans. There's always going to be some surprising event that the vehicle
as a whole has never seen before. There's great YouTube videos of horrifyingly
scary surprising edge cases for autonomous vehicles. Okay, we're never going to fix every
edge case. What we can fix is to make hierarchies and maybe agential hierarchies where local surprise
can be handled or global surprise can be broken down into local surprising events, which can be
handled locally. If I understood the second part of your question is how do we design that
hierarchy? Should the smaller parts be trying to pull in the same direction or be trying to solve
some part of the goal of the higher level? I think that's a super interesting question
and I don't think that the answer is obvious. It may be that smaller parts pursuing orthogonal
goals may end up being useful. Just to give you a quick example, if there's a surprising event
and you've got a whole bunch of semi-independent machines organized in a hierarchy, I would argue
that every single one of those members of the hierarchy should have a slightly different
body and brain. It should have a slightly different form and function. You don't want a
monoculture. You don't want all the parts being smaller versions of the bigger parts
and trying to achieve smaller versions of the same goals because then you've basically got
a committee in which everybody thinks and feels the same way. As we know from humans, that's a
dangerous thing. You get group think or group act. You actually want a hetero culture. You
want a whole bunch of things that are unique in terms of form and function and that maximizes
your chances that someone somewhere in the hierarchy says, just because of the way I'm
built and the way I think with my local control policy, I know what's going on and I have the
seed of a solution. Here's the seed. You all figure out what you need to do to make it a
reality at the larger level. That's another aspect of where the body comes into play.
Yeah. Thank you. Like everywhere is the last mile from somewhere. Things have to be addressed
locally. No matter how you think about a communications architecture distally,
everything and embodiment calls our attention back to that. It has to be somewhere locally.
So then why not take that as the starting point instead of this resource challenge
and then about the multiple subunits when there's a damage to the nest of an ant colony or
there's some things spilled on the surface. It's not that every single nest makes a perfect
pebble move. It's that 51% accuracy with a bunch of nonspecific flurrying of activity,
just like kind of stress or these more generic higher order signaling. That is what allows
nest mates with different brains and bodies to fulfill their own paths of least action.
And then colonies for which that doesn't clean up the mess or it cleans up too well and there's
externalities, those colonies are swept off a table. And then we see the persistence of
collective systems that could figure that out in their growth from a little colony to a big colony
Absolutely, great, great example. I had a question you mentioned, both safety as well as
like reliability. And how do you think about capacities and evaluations on diverse intelligences?
We're all familiar with RAM, CPU, hard drive storage, some of the von Neumann type architectural
descriptors. However, how do we even think about what does that rubric or report card
even start to look like when we know that there's complex interactions with the niche
and when the kinds of capacities that we're talking about may have even open-endedness?
Yeah, great point, great point. So we are the beneficiaries of two big revolutions,
one of them is the AI revolution, but then the older one is the digital electronics revolution.
Digital electronics works, we all have a super computer in our pocket, like there's no arguing
with it. It's an incredibly powerful way to make machines that internally communicate quickly and
richly and then can communicate with other machines. I mean, that's it, that's the information age
that we're in. It's been so successful that it's hard to think about alternatives or why we would
even bother thinking about alternatives. But again, living systems, a lot of what cells do,
they rely on electrical communication, but they also rely on mechanical communication,
chemical communication, thermal communication. Cells are using all physical modalities,
not all physical modalities, as many as they can get their hands on simultaneously all the time.
Why? Why don't they just abandon everything and do everything purely electrically,
like our modern civilization has done? Because it's dangerous, you don't have a diversified
portfolio. So one panel here that I haven't talked about is the one in the bottom right,
which is basically just what you're looking at is what's called a granular material. It's a material
that's made up of a bunch of grains. In this case, the blue circles that you see, these are little
just rubber pucks. And there is an oscillation being supplied at the left hand side. And you can
see that this leads to interesting non-linear vibrational behavior within this material.
What does that have to do with robotics or AI? If you view the vibrations as the carrier of
information, so if a puck is vibrating, that's a one. If the puck is not vibrating, that's a zero.
Now you can start to imagine creating materials that communicate Shannon information
throughout the physical structure, not with electricity, but with a different modality,
dynamics or vibration. And it turns out that you can actually compose these meta materials to
embody logic gates. If you vibrate one particle or another particle, but not both and not neither,
you can watch a third particle and it will either vibrate or not in accordance with an exclusive
OR gate. And you can build this up. Now again, why would you do that? We can make an XOR gate
that's vanishingly small and vanishingly fast in digital electronics. Why would you ever want to
do something different? Because it turns out there are advantages of communicating with vibration
rather than electricity under certain conditions. Having a machine that can communicate between
distant parts of its body through mechanical vibration as well as electricity has an advantage
over a machine that can only communicate long distances within its body electrically. I won't
go into the reasons, but you can intuitively start to understand that. So again, I think we need to,
if we're serious about creating safe and useful autonomous machines, we have to break out of the
digital electronics assumption that that's the only way to do things. We have to break out of
the assumption that non-embodied cognition is the way to go and it's easy to just drop it into a
physical body and we're good to go. We have to peel back some of these very deep assumptions about
the right way to do things that have built up in our society since the Second World War,
because a lot of those technologies have been very successful, nothing wrong with them. But when
we come to apply them to creating safe and useful machines, not always the right thing or the only
way to approach things. That's really interesting. It's like a sort of generalized compute concept
where we could talk about, well, these are the chemicals that it can detect with this fidelity
and here's its tactile interface, here's its electromagnetic capacity for sending and receiving.
That's what kind of motivates or complements the generic theory like free energy principle,
which doesn't tell us about how anything is in particular, but then sets us up with kind of
the framework to plug in these different modules. And then it's an empirical question. And then
right here is sort of the virtual body and a real body. And so that's also very interesting.
How does that work in a collaboration or with a graduate student? How do you balance
this digital adjacent possible off of the material and the more costly implementation with embodiment?
Yeah, well with grad students and postdocs or whoever I'm collaborating with that's kind of
starting out, this can be a very frightening prospect for someone who's trying to get into
AI and robotics because it looks like everything's been done, it's solved. We just have to wait for
Google and Microsoft to buy more compute and data and they're going to finish off the last 1% of
dangerous behavior. So if you're trying to contribute to society's goal of making useful
autonomous safe machines as starting out, what do you do? It looks like this massive brick wall,
there's no entry point. So my take on this is again is that we may be going about this all wrong,
right? The assumption that electricity should be the carrier of information inside an autonomous
machine, that's an assumption. Why electricity? Why not vibration? Why not something else? So
even if you start to think about the alternatives, the immediate reaction as well, it's not going to
be as good, maybe, maybe. But if you think about vibration, you were just mentioning like compute.
We can use vibration for compute, but vibration is movement. So the minute you start to think
about vibration as the carrier of Shannon information, you're now conflating action with
computation. They cannot be separated. Descartes convinced the West 400 years ago that they're
separate. They just are. And you look at AI and robotics, what a surprise. These two are attempts
to create, you know, AGI is bicameral. There's one team that says it's going to happen in computers
and the other side that says it's going to happen in physical machines. That's the Cartesian legacy,
that they're separate. But the minute you look at some very humble material like the one in the
bottom right, it's a bunch, it's 12 hockey pucks next to one another. There's no Cartesian division
anymore between body and brain. There is a body and there is a brain there, but it looks very
different from anything we would usually consider. And there's no value judgment here. It's not
better or worse, maybe it is depending on what your metric is. It's just very different. And so
with grad students and postdocs, I encourage them to pursue that. Could we do things completely
differently? And in the long run, might that be a better way to do things? Who knows? We'll see.
Awesome. I'll make one comment and then ask a last question. You brought up Descartes and that's
the rest extensa, rest cognitive dualism between the thinking and the non-thinking substance and
embodied cognition, embodied intelligence provides both an operational, instrumental,
and an ontological counter argument or complementary perspective, which is just, well,
in practice and in actuality, take it or leave it, they are inseparable. And so at the very least,
that that starts to ratchet and leapfrog the discussion about what is mind and body.
And you started with pointing out how important it was to co-evolve and the complementarity of
mind and body. And it's like, there are two separate things that need to be complementary
and tangoing. And also, maybe they're integrated and blurred in even deeper ways than the dance.
So it's an empirical entry point into what otherwise is a thought experiment, which can
have utility, but also can be just arbitrarily misleading. Absolutely. One of my former mentors,
Inman Harvey at the University of Sussex, used to talk about robotics as philosophy with a screwdriver.
It's not just armchair philosophy. It's when you start to build some of these machines,
maybe in retrospect, in the case of the Metamaterials project, for me in retrospect,
I said, oh my God, most action and cognition are not complementary, separate things that
are complementary. They're one and the same thing. It's not embodied cognition. It's not
an adjective of a noun. It's embodiment is cognition. There are not two things here.
There's just one thing. Very hard to think about. It's so alien to a Western mind, but
it just is. Sometimes I think about that in terms of adjectives getting added in front of a word
and then the term expanding, and then it just encompassing, oh, of course, cognition is ecological,
embodied, enacted, et cetera, et cetera, et cetera, et cetera. And then so it kind of like needs to be
distinguished. And then it subsumes again. And that's part of in closing, like, what are you excited
about? Where can people continue to learn more? What would you say to a person who's wanting to
like go in this area? Yeah. Okay. Great question. So Google my name and it'll take you into lectures
and papers and tutorials. And people want folks want to email me. That's perfectly fine. Again,
Google my name. You can find my email. Happy to put you in touch with the right people.
I think it's, you know, it's easier than ever to get started. You can go to chat GPT and say,
you know, create some tutorials for me to start coding up robots. You know, they're, ironically,
non embodied AI provides a good on ramp now, not just for reading about these ideas or listening to
people talk about these ideas. You can start coding them up in a way that was easier than ever.
That's easier than ever. The old days, you know, you had to learn C and then, you know,
go on from there. Very easy to get your hands dirty, maybe not with physical materials,
but you can create like you see in the left of each of these panels. You can create machines that
are virtual. They're not physical, but they're embodied. It's another point that's important
to make embodiment does not imply physicality. You need to be able to push against the world and
observe how the world pushes back, but the world and you may be virtual like you see on the left
or physical like you see on the right. So you can actually relatively quickly get your hands dirty
with playing around with embodied AI these days. And I encourage everyone to do so.
Cool. Any last comments?
I would just say I was at the computer vision and pattern recognition conference CVPR a few
weeks back. This is one of the flagship AI conferences, 15,000 attendees. And after my
talk, a lot of grad students came up and they said, listen, I, you know, you, you sort of
demonstrated there's another path here that I was feeling depressed or anxious about how to make
progress in AI when these goliaths are, you know, have these data centers at their beck and call.
I would just encourage everyone that when you think differently about all this stuff,
there are new paths that open up. They may not in the long run be the right path, but there are
alternatives to this monolithic predict the next token idea, which is currently, you know,
in vogue. It's, it may be the beginning of the end, but I think this is just the end of the
beginning. We've, we figured a few things out. There are some things that work, but they're
still producing not quite useful and definitely dangerous machines. There is room for improvement.
And there's nothing that says that only Google, Google with all its resources is going to be the
one that can figure out these improvements. Think differently, try some of these alternative
approaches and maybe you will be the one, you know, that comes up with the answer, whatever it is.
Cool. Good luck. Awesome. Thank you, Josh. Really appreciate it. Thanks for having me.
Yeah. And until next time. Thank you. Bye.
you
you

start	end	text
0	14200	Hello and welcome, everyone. This is Active Inference Mathstream 9.1 on March 5, 2024.
14200	20640	We're here with Jonathan Gord and we'll be discussing a variety of topics yet to be determined
20640	25960	or are they? So thank you for joining and to you for any introduction and we'll really
25960	29680	look forward to everyone's comments and questions. So thanks again for joining to you.
30320	35040	Okay, well, yeah, thanks very much, Daniel, for the introduction and for inviting me to be here
35040	44080	on Active Inference. I'm looking forward to a very, very fun discussion. So I don't have anything
44080	47280	especially prepared to talk about, which is probably a good thing because it means we'll be
47280	51840	able to extend the kind of the unstructured part of this for as long as possible. But I think just
51840	57120	to give a little bit of context, I want to talk about an area where I think some things that I've
57200	62400	been working on, some collaborators that might have been working on, that might have some kind of
62400	67360	intersection of interest with things that, you know, Active Inference type people might care about,
67360	73280	right? So, and in particular, that concerns the relationship between kind of computation,
73280	78640	observation, and cognition, and specifically using methods that come from category theory and
78640	82960	topos theory and some other kind of branches of mathematics and theoretical computer science
83040	88320	to understand the relationship between system, specifically the computational and algorithmic
88320	93440	complexity of systems versus the computational algorithmic complexity of observers of those
93440	98480	systems and how those things trade off between each other. So, so just to give a little bit of
98480	103920	context to that, I want to show these are just some visuals from a paper that I put out about a
103920	108800	year ago now, and that this kind of really defines this research program that I've been working on
108800	113360	for the last year and a half in some form or another, which is looking at exactly this trade-off
113920	118560	using category theoretic machinery. So, here's a specification of a Turing machine. This is
118560	123280	just a simple deterministic computation. It's saying, you know, you have a Turing machine that
123280	127680	has this head state and this tape state, and on the next step, you're going to replace the tape
127680	130720	state with something that looks like this, the head state with something that looks like that,
130720	134560	and you're going to scroll the Turing machine head left, or in this case, scroll it right,
134560	139600	etc. So, this is just a, you know, specification of a very simple computation. I think this is a
139600	143920	two-state, two-color Turing machine on a simple, you know, one-dimensional tape. It's about as
143920	148320	simple a computation as you could define. So, if you run that thing for some initial condition,
148320	154400	you'll get an evolution that looks like this. And so, right now, this is just a purely deterministic,
154400	159760	you know, single-path evolution. But from this, we can construct, we can build a mathematical
159760	165520	structure. Namely, we can build a category. So, and the rules for how we build that category are
165520	170400	very simple. So, you know, each arrow here is some simple computation, some application of
170400	174640	the Turing machine transition function. And then what we can do is we can say, well, any time we
174640	178800	have two arrows that are laid end-to-end like this, we can compose them together to create a
178800	183520	third arrow that goes like that. I may even have a picture, yes, like this. So, you know, we have
183520	188800	a computation f that takes us from x to y, a computation g that takes us from y to z, and we
189040	193920	then we obtain a composite computation g compose f that takes us directly from x to z.
193920	199680	And we also add some additional edges, some additional arrows on each state itself,
199680	204640	a sort of identity, an identity operation that maps the computational state directly to itself.
205360	211520	And so, this combined with some axioms of associativity and identity forms a category
211520	215840	of elementary computations. So, this is a very, very simple example. But what I want to try and
215840	222240	build up towards and kind of pump your intuition for is a category which I call comp, which is a
222240	228400	category whose objects are all essentially the class of all data structures and whose arrows or
228400	233120	morphisms are the class of all elementary computations. So, you start by just applying, you
233120	236720	know, all possible computations or, you know, in this case, for the case of Turing machines,
236720	240640	all possible, you know, Turing machine transition functions. And then you do this
240640	245280	closure operation where you, you know, where you essentially do what I'm doing here, but you,
245280	250080	you know, you allow those elementary computations to be composed together in arbitrary ways.
250080	255600	And so, that gives you effectively a class of all possible programs. So, this category contains
255600	260240	not only all possible data structures as objects, but all possible programs as morphisms. And this
260240	265840	is a very rich category with some very interesting algebraic structure that we'll kind of, again,
265920	272160	I'm sure we'll allude to in our subsequent discussion. But in a sense, when we do this,
272160	276080	when we do this operation of taking what mathematically we call a transitive closure,
276080	281280	right, where we allow two elementary computations to be composed together to produce a third,
282320	286160	we are essentially kind of neglecting considerations of computational complexity,
286160	292160	right, because, you know, this arrow here might correspond to one application of the Turing machine
292160	296320	transition function, this arrow might correspond to another application of the transition function,
296320	300480	but this composite arrow correspond might correspond to two applications of the transition
300480	306480	function. And so, somehow, when we allow arrows or morphisms to be composed in this way,
306480	311280	we're neglecting considerations of the complexity of operations. So, the question then is, you know,
311280	316560	could we imagine constructing a generalization of category theory, which takes into account
316560	321200	computational complexity. So, here's an example of how that would look, right. So, here you can see
321200	325440	every edge, every morphism has been tagged with certain computational complexity information,
325440	330480	in particular, it's been tagged with a number specifying what is the minimum number of applications
330480	334640	of my transition function, what's the minimum number of elementary computations that I need in
334640	338400	order to evolve from this data structure to this data structure. So, here, to go from here to here,
338400	342800	it's just one, to go from here to here, it's one, to go from here to here, it's one, etc.
343600	348480	But to go from here to here directly, it would be three, to go from here to here directly,
348560	352880	it would be two. And, you know, just for convention, we say that the identity,
353600	357440	the identity computation, the trivial computation always has complexity zero.
358640	363200	And then this, so this is, again, a fairly simple mathematical structure, and you can
364400	370080	construct this, again, using purely category theoretic technology by building a particular
370080	375520	functor from the category of computations, and from the category of data structures and computations
375600	380160	to a what's called a discrete co-borderism category. And again, we might discuss that later on if
380160	383680	people are interested, but let me not get too bogged down into the technical details of how we
383680	389040	do that. But once you've got this, it gives us immediately a very nice way of characterizing
389040	395760	phenomena like computational irreducibility. So, there is this idea that has existed in some form
395760	399120	or another since the very early days of theoretical computer science, since the days of, you know,
399120	404720	girdle and turing and post and church and so on, but was given this term computational irreducibility
404720	412160	by Stephen Wolfram, where the idea is essentially that you just, you know, intuitively, you describe
412160	416240	a computation as being irreducible, or, you know, the result of the computation as being
416240	422400	irreducibly complex, if it's not possible to shortcut it in any way, right? So, where, you
422400	426400	know, it takes that computation takes a certain number of steps, and there does not exist a
426400	430720	shorter computation that would give you the same answer in less time. And one of the nice
430720	434800	features of thinking about computations and their complexity algebraically like this is that it
434800	439040	gives you a purely algebraic characterization of irreducibility. In particular, what it says
439040	444720	is that irreducible computations are ones for which the computational complexity acts
444720	451840	additive, purely additively under composition. So, if it's the case that if we compose, say, two
453680	460400	computations of complexity one together, if the resulting composite takes, you know, has complexity
460960	464960	two, then it's an irreducible computation. If it has complexity less than two, like one,
464960	468720	then that means that we could have jumped directly from the input to the output
468720	471840	without having to pass through the two elementary computations that made it up. So, that would be
471840	476240	an example of a reducible computation. So, reducible computations are ones whose complexities
476240	482560	compose sub-additively in this category theoretic sense. And, okay, so here's an
484000	489600	illustration of showing what intermediate computational states you had to go through
489600	492800	in order to get from one data structure to another data structure. So, to go from here
492800	496720	to here, you had to go through steps one to two. To go from here to here, you had to go through
496720	502720	steps one, two, and one, two, three, and four, et cetera. So, you can build up a kind of complete
502720	507200	algebra of complexity this way, which has some nice properties, which, again, I can talk about,
507200	512480	but let me not get too bogged down in mathematical details right now. But here's the thing I really
512480	517440	want to talk about, which is what happens when you go to multi-way systems. What happens when
517520	522400	you go to non-deterministic computations? So, now, imagine having, instead of just a
522400	527440	Turing machine with a single rule, a single transition function that just evolves deterministically
527440	530800	with a single thread of time, now imagine having a Turing machine that has, say, two
530800	534880	transition functions, like this one and this one. And so, at any given point, it can apply one of
534880	540960	the two. And so, now, evolution, instead of just being a single path, becomes this kind of branching
540960	545520	structure, which, if we didn't have any merging, would be a tree, but because we are merging
545600	551120	equivalent states, it's actually just a kind of more general directed graph. And so, it looks
551120	556240	like this, and this we call a multi-way system. And so, we can build a category out of these
556240	562240	multi-way systems as well. We can build a category using exactly the same rules. So, again, we do
562240	568240	this transitive closure operation. So, we add an edge for every possible composition of these
568240	573760	elementary computations and an identity edge that maps every data structure to itself. But it turns
573760	579440	out this category has even more structure than the single-way system that we showed previously,
579440	585360	because now, it's possible to compose computations not just sequentially in time using ordinary
585360	590240	composition, but it's possible to compose them in parallel across what is sometimes referred
590240	597040	to as branchial space. So, essentially, you're saying instead of, you know, the ordinary morphism
597040	601520	composition that I showed previously is essentially saying, you know, I apply this
601600	606880	elementary computation, then this elementary computation sequentially. Whereas this parallel
606880	611200	composition is saying, I apply this computation and this computation in parallel to the same
611200	616240	data structure. And so, that parallel operation is what causes these branches, right? Effectively,
616240	621040	when you have two threads of time that are branching from the same state, like here,
621840	627120	that's arising because we have chosen to apply this elementary computation and this elementary
627200	634320	computation together in parallel rather than sequentialized in time. And so, here you can see
635040	640160	this parallelization indicated using what's referred to as a branchial decomposition,
640160	644400	which is just a kind of a visual way of decomposing what's going on between these different threads
644400	649200	of time. And again, there's a purely algebraic characterization of what's going on here,
649200	654000	which is that what we've done is we've taken our simple category that we started with,
654000	657840	and we've equipped it with a tensor product structure. And so, it's become what we fancily
657840	662400	call a monoidal category or actually a symmetric monoidal category. So, the tensor, so we now
662400	667200	have these two operations. We have sequential composition in time, and we have this tensor
667200	674080	product operation, which is a parallel composition in branchial space. And just like we can have,
674080	679120	just like before, where we equipped our edges, our morphisms with certain computational
679440	684640	complexity information, we can do the same thing, and we described how those complexities composed
684640	688880	sequentially in time. We can do the same thing and describe how the complexities compose in
688880	694560	parallel as one composes morphisms in branchial space. And so, this allows one by exactly the
694560	699120	same token to quantify multi-computational irreducibility rather than just computational
699120	704000	irreducibility. So, now, multi-computational irreducibility becomes a measure of how additive
704000	708640	or sub-additive your time complexities are when you compose them in parallel through the tensor
708640	714640	products rather than just in sequence through standard morphism composition. Okay, but I promise
714640	718560	I am cut, and so here's an analogous diagram to the one I showed before showing all the kind of
718560	728960	intermediate steps that are being applied when one constructs computations or indeed multi-computations
728960	733920	by composing elementary computations both sequentially in time and in parallel in branchial
733920	739840	space. Now, but I promise I am the point that I'm trying to get to is that it turns out that in
739840	743840	addition to just being a useful way to think about computational complexity theory and to formulate
743840	748240	complexity classes like, you know, polynomial time on non-deterministic polynomial time, etc.,
748240	751680	it turns out this is also an interesting way to think about the role of observation
752560	759760	in sort of computational models of reality. Because so here's where I'm going to get a
759760	764400	little bit philosophical, and I don't immediately have a slide or a graphic that I can show to
764400	769040	illustrate this point. But so when we think about modeling a system computationally,
770320	773600	one has to bear in mind that there are really two computations going on, right? There's the
773600	778080	computation that the system is itself performing, and then there's the computation that the observer,
778080	782720	the person who is measuring that system and concluding things from it, there's the computation
782720	788720	that they are performing. And somehow, you know, so when we construct models of reality or when we
788720	794000	construct models of systems, you know, and we want to describe kind of at a meta level what we're
794000	798960	doing in computational terms, there's our own computation that, you know, that's going on
798960	802720	inside our own internal representation of the world. And then there's presumably some external
802720	807120	computation that's going on outside. And then when we make observations and when we make measurements,
807120	811440	when we construct theoretical models, what we're doing is we're somehow constructing some kind
811440	816640	of encoding function that allows us to take a concrete physical state of the system we're
816640	821760	observing and encode it as some abstract state of the internal model that we have of what's going on.
822640	828720	And that's all very well. But then one, but then now we don't just have one computation to care
828720	832480	about, we have three, right? We've got the computation of the system, computation of the
832480	836960	observer, and the computation of this encoding function computation that's responsible for their,
836960	842400	for their, you know, the interface between their internal model of the world and the external
842400	847200	reality. And the computational complexities of these computations into play in an extremely
847200	851520	interesting way. And so the, you know, part of the reason for trying to develop this algebraic
851520	855360	semantics for thinking about computational complexity and multi computational complexity
855360	861120	was to try to give one a systematic way to reason about exactly this three-way interplay between
861120	867840	systems, observers, and encoding functions. And so in particular, when we make when an
867840	871920	observer makes a model of the world, one thing that they're doing is that they are, you know,
872160	876160	for any, for any model, isn't just, you know, a complete description of reality,
876160	878960	there's a certain amount of coarse-graining, right? There's a certain amount of
880960	884640	taking a bunch of states that in the system itself are distinguished,
884640	889120	but in the internal model are treated as the same, they're kind of, you know, they're cast in the
889120	893840	same bucket. So in some sense, you know, how coarse a model is, is a measure of how much the
893840	898400	encoding function fails to be subjective, right? And so again, there's a kind of algebraic or
898400	904720	category theoretic characterization of what's going on, that, you know, the fewer of your
904720	910320	morphisms are epimorphisms, the more coarse your model is, the more abstract or idealized
910320	916480	your model of reality is. And so then the interesting thing is that this characterization
916480	921520	of multi computational irreducibility, this measure of how additive or sub-additive your
921520	926320	complexities are, as you compose them together in parallel, gives you a measure of the relative
926320	930720	complexity of the evolution function, that is the function that evolves your computation
930720	935440	forwards in time, versus the equivalence function, that is the function that declares that two
935440	940160	computational states, two data structures, are to be treated as equivalent. And that interplay,
940720	945680	I claim, is a kind of abstract meta way of thinking about the interplay between the
945680	949760	computation of systems versus the computation of observers, because, you know, so in a sense,
949760	954960	the role of the system is to evolve forwards in time, whereas the role of the observer is to take
955760	962480	states in the system that are distinguished in reality and say, you know, subject to my
962480	967200	idealized model, I'm going to treat these as the same. So the system is defining the evolution
967200	972240	function, but the observer is defining this equivalence function. And so then the tradeoff
972240	980320	in their complexities becomes exactly a tradeoff between what are the algebraic rules that describe
980320	984080	the complexities as they compose sequentially, versus the algebraic rules that describe the
984080	989840	complexities as they compose under this tensor product operation. And so I've shown this in
989840	995040	particular for Turing machine systems, but this is a very general kind of algebraic semantics,
995040	998320	you can apply it to hypergraphs, you can apply it to combinators, lambda calculus,
998320	1005040	doesn't matter. In a sense, there is just one category up to isomorphism of data structures
1005040	1008880	and computations, and there are simply many different ways of parameterizing what that
1008880	1013280	category is doing through things like Turing machines or hypergraphs or whatever. The
1013280	1017760	algebraic formalism transcends the particular details of the computations that one's dealing with.
1018560	1023360	And so yeah, as I say, what one ends up with is, I think, a fairly general formalism for
1023360	1029440	thinking about the interplay between observers and the systems that they observe. And that gives
1029440	1034720	one a, I promise I'll stop monologuing in a moment and we'll try and pick apart what I'm
1034720	1040240	really talking about here. But so I'll just conclude with, you know, once one has that
1040240	1045760	algebraic semantics, a whole bunch of things which I think previously would have been, at least to
1045760	1050640	me, previously seemed like kind of fundamental confusions about, you know, how scientific
1050640	1054720	observation works and how it interplays with computational models, those confusions kind of
1054720	1058960	become much easier to clarify once you think about it in this kind of more compositional way. So
1060080	1065520	to give a very simple example, or kind of very degenerate example,
1065920	1074000	you can, you know, within this algebraic semantics, you can effectively trade off the
1074800	1078640	computational complexity of the system for the computational complexity of the observer, right?
1078640	1086160	So you can have, you can have kind of, in effect, two degenerate cases. You can have the case where
1086160	1091280	the system itself has a completely trivial evolution function. The system itself has, you
1091280	1096640	know, is doing something completely elementary in its, in how it evolves. But then the observer
1097280	1100880	has some incredibly complicated equivalence function that makes the system look like it's
1100880	1104080	doing something really complicated, even though what it's actually doing is something very simple.
1105360	1109440	And so then you, so you have the phenomenon where actually kind of all of the complexity is in the
1109440	1113680	eye of the, is in the eye of the observer. You can also have the other degenerate case where the
1113680	1117840	observer is doing something absolutely trivial, where the, you know, the encoding function or the
1117920	1121600	observer's own internal representation is just an identity function or something. So there's no
1121600	1125440	complexity there, but the system is doing something incredibly complex. It's doing some,
1125440	1129920	some totally, some really sophisticated universal computation. And so that will also appear very
1129920	1136000	complex to that observer. And so, and you can also have any kind of, any intermediate, you know,
1136000	1142560	there's this vast interstitial space between these two extremes. And so one thing that's kind of
1142560	1148320	always, one sort of philosophical problem that I've always kind of been interested in ever since
1148320	1154720	I was a kid, which is this, this sort of, this tension between empiricism versus rationalism,
1154720	1159600	right? You know, the question of, you know, on the white, if you look back at the history of,
1159600	1163440	where, you know, early European philosophy, or, you know, it's certainly Western, you know,
1163440	1168160	Western post enlightenment philosophy, you had people like, you know, Descartes and Leibniz and,
1168240	1172960	and so on, who were, you know, in a more sophisticated way, Bishop Barkley with subjective
1172960	1176320	immaterialism, who were trying to push for this idea that, oh, you know, all the, all the
1176320	1179920	sophistication is what's going on inside the observer's head. And, you know, what goes on in
1179920	1183520	reality is somehow secondary. And then you had people like, you know, Locke and Hume and the
1183520	1187040	empiricists, who were saying, no, no, we should try and get the observer as much out of the picture
1187040	1191040	as possible. And we should say all the sophistication is going on kind of in the external world.
1191040	1195040	And this in one, you know, one nice consequence of this is it gives one, one nice consequence of
1195040	1199520	this formalism is it gives one actually an algebraic way of kind of parameterizing this
1199520	1204880	spectrum from rationalism to empiricism, right, that, that you can, you can choose the rationalist
1204880	1210720	extreme where, you know, you know, you just have some, some space of all possible computations and,
1210720	1214960	and the observer is doing all of the work to try to narrow down to a particular one,
1214960	1219840	or you can have the kind of empiricist extreme where, you know, the observer is a completely
1219840	1224800	elementary system. And, you know, and everything and everything they observe is just being built
1224800	1230240	up from a kind of bottom up construction, or you can have anything in between. And in a sense,
1230240	1234880	we now have, I think, the beginnings of a mathematical theory that explain, that's able
1234880	1240320	to explain how those complexities trade off in a very direct way. So I think there's potentially
1241040	1246400	places of mutual interest there in kind of the, in thinking about, yeah, as I say, cognition,
1246400	1250080	observation, measurement, scientific modeling, and so on, in fundamentally computational terms.
1250080	1254720	So I think that hopefully that will provide some, some context for, for a discussion.
1258720	1265680	Thank you. Great opening. There's so many places to spin in and jump through.
1267200	1271600	I guess I'll start with the two things I wrote down were unity is plural and at
1271600	1276480	minimum two and beauty is in the eye of the beholder and the way that these kinds of pieces
1276480	1283280	of timeless wisdom that describe that fundamentally relational component to observers in systems,
1283280	1287760	which are not all kinds of systems per se, but those kinds of systems, those things are true for,
1288400	1295200	and then the way in which along the formalism that you described with the system observer encoding
1296000	1302320	freeway partition, and then the way that in free energy principle and the particular physics,
1302320	1308640	that interface gets broken out from the agent's perspective into the incoming sensory and the
1308640	1316480	outgoing action. So then that results in the fourfold particular partition. So maybe just to kind of,
1317440	1324160	well, there, how do we partition the, from a category theory perspective or however,
1324160	1331680	the action perception loop or the engagement loop? Like, how do we make a topology or
1331680	1339120	compare a contrast different topologies and flows over this kind of seemingly pervasive or universal
1339120	1345600	interface like concept? That's a fascinating question. So I don't have the answer and this is
1345600	1351280	maybe a place where, where both you Daniel and perhaps David may have, you know, useful perspectives
1351280	1357760	on this because, you know, I'm, I'm, you know, I read Carl's work a few years back and so I have
1357760	1362080	some familiarity with the terms, but I'm by no means a kind of, you know, an expert on free
1362080	1366160	energy principle or, you know, active inference and those kinds of things. But I think it's a very
1366160	1370560	good point that you raise. And so, you know, I should begin by just being honest and say that,
1370560	1375200	you know, everything I'm doing, you know, all that I just described is of course an idealization
1375200	1379200	and that, you know, in reality, you know, in particular, it's an idealization, which I think
1379280	1383680	you were very right, Daniel, to kind of pick up on. It's an idealization in which we say the
1383680	1387360	observer is completely kind of non-interacting with the world somehow, right? That, you know,
1387360	1392080	in a sense that there's just input coming in and nothing, nothing coming out. But of course,
1392080	1395600	we know that's not really how observation works. Observation is necessarily a kind of two-way
1395600	1400480	process. And so what's needed is not just this kind of very clean algebraic semantics that I've
1400480	1403760	described here, which assumes that there's a essentially a one-way function from the world
1403760	1408640	to the observer, but actually something more like a kind of second-order cybernetics description
1409440	1414240	of, you know, what's really going on where you have, you know, first-order and second-order
1414240	1417760	interactions, whereas exactly as you say, you can get these sort of feedback loops from
1417760	1422160	observation to action and back again, which are probably, which is, I mean, still an idealization,
1422160	1426880	but probably a more realistic idealization for how real observers and real measurement apparatus
1426880	1432160	work. So I just want to begin by saying that I don't know, right? And, you know, the question of
1432160	1436160	how this formalism into plays with things like second-order cybernetics and other areas where
1436160	1440720	I know these kinds of questions have been explored, that's something I'm very interested to find out
1440720	1450640	about, you know, going forward. But I think, hmm, okay, so, yeah, so at some point, maybe you could
1451440	1455440	help me understand, you know, potentially where things might fit in with, you know,
1455440	1463920	with the kind of broader active inference framework. I'm not sure I necessarily have that
1463920	1473120	much more to comment on than that. Yeah, other than to say that, you know, in a sense, okay,
1473120	1481680	so maybe, you know, one further comment is that, because, I mean, you asked specifically about
1481680	1486800	how the kind of compositional category theoretic perspective might be useful. So I don't think
1486800	1489600	category theory in itself is going to be the complete answer. I think it will be category
1489600	1493120	theory augmented with some other things, computational complexity, probably second-order
1493120	1498640	cybernetics, and some other things that I may not be aware of. But one place where I think that
1498640	1505280	viewpoint is useful, at least on a philosophical level, is the idea that comes about, that you
1505280	1509360	really obtain by studying mathematical structures in a category theoretic way, which is that the
1509360	1516400	identity of something, you can define it both in terms of its intrinsic properties, or you can
1516400	1520320	define it in terms of, you know, the stuff that you can do to it, right? So, you know, this was
1520320	1524080	really the transition that happened in the foundations of mathematics as a result of people
1524080	1529680	like Samuel Alenberg and Saunders-McLean. So, you know, category theory has its origins in this
1529680	1533840	sort of slightly obstruous branch of algebraic topology. It was, you know, initially developed
1533840	1538880	by people like Alexander Gordon-Deek and John P. S. Sear for doing homological algebra for,
1538880	1542880	you know, for reasoning about sort of the algebraic structure of topological spaces. But then later,
1542880	1546800	in the, I think, 1960s, 1970s, these two American mathematicians, Alenberg and McLean,
1546800	1550640	realized that it was useful not just for thinking about topology, but for thinking about kind of
1550640	1554720	mathematical structure in general. And then later on applied category theorists started saying,
1554720	1559200	well, maybe it's useful for just thinking about structure in general. But, you know, the key kind
1559200	1565520	of conceptual or philosophical shift that it imposes is, you know, historically, thanks to the work
1565520	1569920	of people like Kantor and Frege and Russell and so on, people have thought about mathematical
1569920	1574000	structures in the foundations of mathematics as, you know, in terms of set theory. And the idea
1574000	1578240	in set theory is you have this, you know, things like the axiom of extension that effectively say
1578240	1583520	set is defined by what's inside it, right? So in other words, you know, you, a mathematical
1583520	1587680	structure obtains its identity by, you know, you break it apart and you look at what's inside.
1589040	1594320	In category theory, it's a completely different view. The view instead is you say, well, no,
1594320	1598080	you can't look inside, you know, it's a fundamental rule of category theory that you can't look inside
1598080	1603680	an object. You know, it's internal structure, if it has any sort of outer bounds to you.
1603680	1609040	And instead, you give that object identity in terms of how it relates to other objects
1609040	1613040	of the same type, right? So in other words, you know, you can ask, you know, what can I do to
1613040	1617680	this? What functions can I apply to it? What functions can I apply to something else that
1617680	1622320	map into this? So, you know, if I want to define, I don't know, the real numbers or the integers
1622320	1626320	or something in the set theory, you know, from a set theory perspective, you would say that the,
1626400	1630720	you know, the essence of the real numbers are all the numbers that are inside that set or all
1630720	1635440	the numbers that are inside are. Whereas the category theory perspective is no, the essence
1635440	1639440	of the real numbers are all the functions that you can define that take real numbers to some
1639440	1643680	other number system or real numbers to themselves or that take some other number system into the
1643680	1648640	real numbers or et cetera. And, you know, some of the deepest results in category theory, like the
1648640	1654880	Oneida Lemma and other things, are telling one in some very precise sense that these two perspectives
1654880	1660000	are really the same at some fundamental level, but that, you know, identifying an object based
1660000	1664000	on its internal structure, based on breaking it apart and asking what's inside and identifying
1664000	1669280	an object by asking, what can I do to it? And what, you know, what can this object be transformed
1669280	1673680	into and what things can be transformed into this object? Those give you exactly the same
1673680	1677440	information. It's far from obvious that that's true, you know, the Oneida Lemma is a very kind of,
1678560	1682240	one of those results where you can never quite work out of its obvious or if it's incredibly,
1682240	1686720	you know, mysterious. But I tend to fall on the side that it's incredibly mysterious. It's far
1686720	1691440	from evidence that those two perspectives would really be the same. And yet, the point you're
1691440	1698800	making Daniel, I think, is that in a sense, historical ways of thinking about scientific
1698800	1702960	observation have tended towards the set theoretic viewpoint, tended towards the viewpoint that we
1702960	1706160	understand systems based on kind of breaking them apart into constituent components.
1707200	1711440	But perhaps a more realistic view is something more like the category theory perspective,
1711440	1716560	where we say, you know, I understand a system by interacting with it, right? By asking,
1716560	1720880	what can I do to it? And how does it behave as I, when I perform certain operations to it?
1720880	1724880	And that's a fundamentally, you know, that's a fundamentally two-way process that involves not
1724880	1729520	just passive observation, but also kind of active participation. And somehow we need to develop
1729520	1733440	a formalism that kind of incorporates those two elements. And maybe, you know, maybe it already
1733440	1738160	exists. And it exists in this large literature tree of which I'm largely unaware. That's partly
1738160	1741920	why I want to be here to try and find out, you know, what things I missed, so to speak.
1743520	1751440	Well, all right, a few points. Self-evident is far from evident. Also, I tend to the mysterious,
1751440	1756720	which is to say, saying more with less, especially for these frameworks, because they're less
1756720	1764240	opinionated, so that their space of internal semantics can be larger. And then that description
1764240	1769040	that you provided with the relationship between the set and the category theory. So I kind of
1769040	1776160	summarized it as set is to essential inclusion as category is to relational function. Now,
1776160	1784720	if our concept of organismality or of action in the niche is constructive compositional material,
1785840	1791520	then we are looking for, like, what is in or out? Is the microbiome in? Is the
1792160	1796960	pheromone in the ant colony in or out of that thing? Because it's looking for, like, a static
1796960	1805200	material answer. And then in contrast, the other side of that coin highlights the dynamic,
1805200	1810480	like, whatever it is that self-organizing of the tornado is the tornado, whatever it is that
1810480	1817200	self-organizing for the ant is the ant. And then also this, like, hint slash
1818160	1824400	mobius strip or something that those two in the moment are indistinguishable.
1825280	1829680	And yet systems that we choose to define one way or another, or keeping both open,
1830480	1836240	those design decisions do make all the difference, even if for real systems, as they're observed,
1837040	1844960	there's indistinguishability. Right, right. I think that's a very important point. And one which,
1845920	1852000	I mean, this is a, it's always a kind of concern I have whenever I start thinking about, you know,
1852000	1858400	embodied cognition or, you know, extended phenotype type ideas, right? But in a sense, you know,
1859280	1863760	if what one is trying to do here is construct a kind of formalistic model of observation or
1863760	1868960	of cognition or something, then as a kind of first order approximation, one has to start by
1868960	1872880	somehow decomposing the world into observers and systems. But of course, that, you know,
1872960	1877200	we know that that decomposition is somehow arbitrarily imposed, right? And that, you know,
1879040	1882720	if you take these things to their extremes, and you say, you know, you allow essentially
1882720	1886320	everything that the agent is interacting with to be considered, you know, like,
1886320	1889760	not just the microbiome, as you say, but also, you know, tools that they construct or
1889760	1893840	environments in which they exist and so on. If you, as you start to consider all of that to be
1893840	1897840	a component of that organism, you know, of that agent's phenotype, which is a completely reasonable
1897840	1905120	thing to do, then, and you start to, you know, you start to say, okay, well, their cognitive
1905120	1909120	processes are not just localized to their brain or their spinal column, but are kind of somehow
1909120	1913280	extended to, you know, the computers they use, the paper they write on, the books they read,
1913280	1919920	et cetera. Again, perfectly reasonable thing to do and sort of somehow more descriptive of what's
1919920	1924640	really going on. But my fear is always, if you take that too far, then, you know, you end up
1924640	1928960	destroying the whole assumption that the idealization was based on, which is that you can neatly
1928960	1934400	decompose, you know, the world into observers and systems. And so, I always get a bit nervous
1934400	1940320	when thinking about that, that it's like, yes, you know, in a sense, you know, we know this is an
1940320	1947040	approximation and we know that that approximation is not really true, but how, you know, how much can
1947040	1951600	you afford to sort of loosen your grip on that approximation before the whole thing just kind
1951600	1955760	of falls apart. I don't really know the answer to that question. But I think it's an interesting one.
1955760	1961760	Yeah. How about ask some questions from chat and then give your first thoughts and then we'll see
1962560	1967360	maybe where that kind of lands with further questions or how it connects to active inference.
1967360	1971280	So that sounds good. Oh, by the way, should I keep my screen share on or should I?
1971280	1977840	Yeah, we might want to go to a figure. So it's fine. Okay. Yeah. All right. Quantum Bell wrote,
1978720	1981680	how does this help us reason about causality?
1983440	1989040	That's a fascinating question. Okay. So that's that's another major aspect of, you know, why I
1989040	1995440	think this research program is exciting, because so again, this is something where I'm interested
1995440	1998880	to get the kind of active inference perspective, because I know this again, it's a topic in which
1998880	2006480	much has been written and I'm largely ignorant. But yes, so one question you could ask is,
2006960	2009920	yeah, if you have a description of a computation like this, like,
2010720	2014720	let's go back up to the Turing machine case, the single way Turing machine case that's
2014720	2017760	relatively easy to analyze, although still far from obvious what's going on.
2018480	2022800	So suppose you have a computation of this kind, and you want to ask, what is its causal structure?
2022800	2026960	In other words, you know, for each edge, each time I'm applying this Turing machine transition
2026960	2032000	function, can I construct some kind of graph, you know, some some directed graph representation that
2032000	2037600	tells me how these events are linked together. So in the, within the original research program,
2038560	2042240	the so-called Wolfram model research program that kind of started a lot of these investigations,
2042240	2045200	we were looking at this all the time, right? We were looking at kind of, you know, taking
2045200	2049520	computations and looking at that causal structure and trying to, you know, infer things about what
2049520	2054320	was going on about the, you know, the semantics of the computation based on causal relationships.
2054320	2057600	And at a certain point, I started to realize, and I think other people had realized this before
2057600	2062960	I did, but I'm often slow to pick these things up. I, I, I and other people started to realize that
2062960	2069520	the notion of causality we were using was kind of nonsense. I mean, it was not completely hopeless,
2069520	2073680	but it wasn't really causality, or it couldn't really be called causality in any, in any definite
2073680	2080160	sense. So what do I mean by that? So first of all, it was a very technical problem. So if you're
2080160	2084160	looking at something like a Turing machine evolution or a hypergraphic writing system as we were,
2085120	2089920	then there's a very tempting and apparently obvious natural definition of causality that
2089920	2094480	you can use, which is to ask, you know, when you split the world up into events that take,
2094480	2098480	you know, some part of your data structure as input and output, you know, some other part of a
2098480	2104480	data structure as output, then you can very easily ask, well, does the output of one event
2104480	2109600	intersect with the input of another event? So if I show the hypergraph example, it's perhaps easier
2109600	2115040	to see. So you have a hypergraphy writing rule that looks like this, right? So you know, you have,
2115040	2118240	you say, if I have a piece of hypergraph that looks like that, I replace it with another piece of
2118240	2122880	hypergraph that looks like this. So at each, each time you apply an event, you can think of that
2122880	2128320	event as, you know, ingesting certain hyper edges and kind of, you know, replacing them with others.
2128320	2132560	So you can divide it up into a sort of the, the, the input hyper edges that are being ingested
2132560	2137200	versus the output hyper edges that are being produced. And so then you can ask, well, do the
2137200	2144640	output, did I use, did I subsequently ingest in some future event hyper edges that were output
2144640	2148320	in some previous event? Well, if the answer is yes, then pretty obviously that future event
2148320	2152080	couldn't have occurred unless the previous event had already occurred. So then you could say, well,
2152080	2156480	then one of those events causes the other. So in general, you could say that two, you know,
2156480	2163120	an event A causes event B, if it's the case that the output, that the collection of tokens that
2163120	2167200	was produced in the output of event A has a non-zero intersection with the collection tokens
2167200	2171360	that were ingested as part of the input of event B. And that's a very tempting, very natural
2171360	2175920	definition of kind of causality in these systems. Turns out it doesn't really work. I mean, it works
2176720	2180800	pretty well, but there are cases in which it fails and it fails pretty spectacularly. And
2180800	2184960	so the kind of canonical case where it fails spectacularly is that you can have events that
2184960	2189040	don't actually do anything, right? You can have events that just kind of touch an edge, touch a
2189040	2194720	token and output it again unchanged, but maybe, you know, it modifies the name, it modifies the
2194720	2198880	identifier, but it doesn't actually change anything about the structure of the hypergraph or the
2198880	2203440	Turing machine state or whatever. So pretty obviously that event doesn't matter. It shouldn't
2203440	2208720	be causally related to anything in the future. But because it ingested the edge and then didn't do
2208720	2212880	anything, you know, did some identity operation, but then, you know, produced it in the output again,
2212880	2216880	it will kind of register as being causally related to any future event that used that
2216880	2221200	edge, even though it didn't make any difference. That's just one very obvious example. There are
2221200	2225440	other cases where it became clear that whatever this thing was, whatever this algorithm was
2225440	2230080	detecting, it wasn't really causality. So I tried to think about, you know, what's a more sensible
2230080	2234320	definition of causality. And I started working on things to do with, you know, a slightly kind of
2234320	2239440	blockchain inspired ideas where you say, okay, well, rather than just arbitrarily assigning,
2239440	2244720	you know, identifiers to these tokens every time they're created, what if I recursively
2245600	2249920	construct the identifier of the token based on its causal history. So in other words, each token,
2249920	2255520	like each hyper edge or each state in my each, each tape square on my Turing machine tape,
2256400	2261360	I, the identifier is not just some random number that gets generated by my algorithm,
2261360	2267200	but instead its identifier is a directed graph representation of its complete causal history.
2267200	2272720	Well, then kind of recursively, it's identified can only change if the causal history was updated.
2272800	2276400	And so you don't end up with these kind of spurious causal relations that I described before.
2276400	2281600	So that seemed like one tempting way of resolving this problem. But then I realized,
2281600	2284880	actually, there's a much more fundamental problem. There's a problem, there's a philosophical problem
2284880	2289600	with the way that we're thinking about causality, which is that it's not really, you know, so,
2290960	2297440	okay, this is a long tangent, which I'll talk about a little bit, but I won't get into the
2297440	2301120	complete details unless people are interested. But I ended up T I ended up talking to a bunch of
2301120	2304800	philosophers who, you know, who worked on causality and people who worked on parallel
2304800	2308560	programming and quantum information theory and other places where causality was,
2308560	2312480	was, was studied and asked them kind of basically what, what is, what, what do you mean by causality?
2312480	2319120	What is causality? What is this thing we're trying to define? And in some form or another,
2319120	2324000	all of the definitions boiled down to, you know, event A causes event B if
2324960	2329200	had event A not occurred, then event B would not have occurred. So in other words,
2329200	2333440	you need a counterfactual, you need some possible history, some possible world in which event A
2333440	2337520	didn't happen. But if you're reasoning about a purely deterministic event system like a Turing
2337520	2341520	machine, that doesn't make any sense. Because if you're, you know, if you, if you have a single
2341520	2346240	Turing machine transition function, there is no possible world in which that transition function
2346240	2349360	didn't fire in that particular way. Because if it didn't fire in that particular way,
2349360	2352480	you would not be reasoning about that Turing machine anymore, you'd be reasoning about
2352480	2358160	different Turing machine. So suddenly this, you know, to make sense of these notions of causality,
2358160	2363040	you need a kind of Leibnizian, you know, modality is view of reality that just doesn't exist for
2363040	2367760	these deterministic computational systems. So either you need to define computation,
2368400	2373840	you need to define causality only at the multiway level, only at the level where you have many
2373840	2377440	computations or possibly all computations happening in parallel, and then you can define
2377440	2381760	causality relative to all of the, you know, relative between them, or you were kind of posed, right?
2381760	2385680	There wasn't really, you know, that seemed like the only kind of the only get out, or you'd need
2385680	2390320	some fundamentally new philosophical theory of causality that I was not qualified to produce.
2391120	2396240	And so that's, again, part of the reason, part of what motivated this general
2396240	2401360	research program, which is trying to think about this category of not just a single computation,
2401920	2405440	and with a single sequence of data structures, because it's clear that you can't, you know,
2405440	2410880	philosophically meaningful way assign causality in that case, but rather, you know, looking at the
2410880	2414640	algebraic structure of the category of all possible computations and all possible data
2414640	2418960	structures. And in that situation, there is a notion of causality you can equip that with,
2418960	2423520	and there's a nice, again, a nice mathematical description in terms of, in terms of weak two
2423520	2428560	categories and so on, which again, I can talk about if people are interested. But yeah, so it's
2428560	2432880	clear that these things are very deeply related that this sort of theory of the category of
2432880	2436720	computations and data structures, and the theory of how you assign causality in a meaningful way,
2437760	2442960	are very deeply related. And I'll just mention one other thing on that topic, which is again,
2442960	2448640	just the area which I find quite exciting, because it's an unexpected spin out of this program,
2448640	2453760	which is that, so once you have a way of consistently applying causality at a per token
2453760	2458800	level in these systems, it gives you a way of vastly generalizing what computation is.
2460400	2463920	And you can, in particular, you can get, you can derive something which I call,
2465280	2468800	well, which I'm provisionally calling covariant computation, although it should probably have
2468800	2476320	a better name than that, which is, so in our traditional kind of Turing Church type models
2476320	2481200	of computation, computation is a purely forwards in time operation. So at every point, you know,
2481200	2485280	you have a complete data structure, and computation is about deriving what is the next state of that
2485280	2488960	data structure. So in a sense, it's only a forwards in time thing. You might be able to
2488960	2492000	kind of reconstruct the initial conditions based on some subsequent data structures,
2492000	2494800	you might be able to go backwards in time, but that's essentially what you're doing.
2495760	2502000	But then you could imagine, okay, suppose I don't know the complete state of my data structure,
2502000	2507360	I know instead, I know one part of my data structure, but I know it, I know its history
2507360	2511680	throughout all of time. So you could imagine, say an elementary cellular automaton or a Turing
2511680	2516240	machine tape, where you'd know nothing about the tape, but you know the state of one cell,
2516240	2520080	and you know it, you know, throughout all of time. And then the question is, what can you infer
2520080	2523280	about the rest of the computation? And it turns out that for those kind of structured array type
2523280	2527760	systems, you can infer a lot, you can actually evolve the system, not forwards in time, but
2527760	2533920	sideways in space, and obtain a kind of causal diamond that so okay, the top left, top right,
2533920	2537920	bottom left, bottom right corners are undetermined. But everything inside that diamond can be
2537920	2544320	determined just from that one row, or that one column that you know, you know, sort of extended
2544320	2549520	throughout time. And so, you know, and that's a fundamentally different notion of computation.
2549520	2553520	So it's a version of computation, which is not forwards in time, but sideways in space.
2553520	2557520	But you can also have version of computation that is sideways and branch your space where you
2557520	2562560	know, you know, one complete state, you know, you know, one branch of the multiway system extended
2562560	2565680	throughout time. And then the question is, what else can you infer about the, you know, but the
2565680	2569040	rest of the multiway system just from that one branch? And again, the answer turns out to be,
2569040	2575200	you can infer a lot, but not everything. And so, just like in the reason I call this covariant
2575200	2579040	computation is because it's very analogous to what happens in relativity. So in relativity,
2579120	2583200	once you buy into this notion of general covariance and the notion that space and time are kind of
2583200	2587760	fundamentally the same thing, then you have to somehow relax your traditional view of what
2587760	2592000	dynamical systems do, which is, you know, we typically think of systems as evolving, you know,
2592000	2596880	you have a snapshot of your initial of your data at, you know, localized on a, you know,
2596880	2600480	for a particular state of space, you know, on a particular space like hypersurface.
2600480	2603760	And then your laws of physics tell you how that space like hypersurface evolves forwards or
2603760	2611440	backwards in time. But in a covariant picture of physics, then you must also allow for, you know,
2611440	2616160	your initial data to be defined on a time like hypersurface, and you for you to be able to evolve
2616160	2620560	that time like hypersurface sideways in space or mixtures of the two and so on. And so it's clear
2620560	2625040	that there's a very general, a vast generalization of ordinary computation theory that you can,
2625040	2629520	that you can construct that's kind of physics inspired in that sense, in which you can have
2629600	2635040	mixing of space time and kind of multiway directions in a completely consistent way.
2635040	2640160	But to make those things consistent, you need to have a definite way of assigning causality. You
2640160	2644320	need to because, you know, any computation that you do, even if it permutes the directions of
2644320	2648720	space and time and branching space and so on, must always somehow preserve the causal structure,
2648720	2653200	has to respect the causal structure of what's going on, or else it's inconsistent. And so
2653920	2657920	this question of how you construct a covariant theory of computation is, it turns out,
2657920	2662640	intimately related to the question of how you take this category of computations and data structures
2662640	2666640	and equip it with a consistent notion of causality. So very interesting question,
2666640	2673840	we could talk about that at great length. Okay, to follow with a few pieces, it's very related to
2673840	2680000	Professor Mike Levin's notion of poly computing, and about the necessity for a causality concept to
2680000	2686000	be created or deployed when the question arises, was that me? Was that action or change due to me?
2686960	2693280	Also, connectivities, even just in the neuroimaging setting, which is kind of the cradle
2693280	2696800	from which active inference and free energy principle arise from,
2696800	2701920	it's really important to distinguish the functional, effective, and anatomical connectivities.
2701920	2706960	And that was one of the points that Toby St. Clair Smith made in his dissertation,
2706960	2713280	which is that a lot of times the Bayesian graphs don't convey all of the necessary and sufficient
2713280	2718800	information to make the reproducible computation, which is one of those kind of what's missing from
2718800	2723520	the graph is what motivated a lot of the category theory developments in active inference,
2723520	2729760	as well as some of the formal ontological works with Sumo and Dave here and Adam Pease, because
2730800	2735840	implementing modal and higher order logics is really important if it's a possible situation
2735840	2739200	where a mind can have a perspective on a mind and all these things like that.
2740160	2748880	Then the ant-turing tape, the tape is the pheromone, and then the decision space is the nest
2748880	2757120	mate's scrolling. So when you had a deterministic turing tape, that was like a movie because the
2757120	2762560	nest mate couldn't make any choices, except for internal action, which is kind of side topic,
2762560	2768160	but it couldn't make any choices on the tape. Whereas when there's a multi-way,
2768160	2773440	which is basically in active inference, what we talk about in terms of affordances and the policy
2773440	2779120	space and the temporal depth of planning and counterfactuals on action and action-conditioned
2779120	2784000	world transition states like the B matrix, all those kinds of topics come into play,
2784000	2789760	because if you want to have a causal buffer or grasp on what is it that something that could do
2789760	2795680	otherwise does, what does it cause to do when it does or doesn't do otherwise, you need something
2795680	2802640	like a deterministic handle around what could be a probabilistic or deterministic,
2802640	2807280	but at least multi-way map of some kind of cognitive territory.
2811760	2818320	That's a very, very interesting perspective. Again, I'm betraying my ignorance of active
2818320	2826720	inference theory here, but it sounds almost like, when you have this kind of interplay between
2828080	2837280	sort of epistemic versus pragmatic, there's two aspects of how this kind of speculative
2837280	2845680	part of cognition works. This is something which I thought about in a completely different context
2845680	2850240	in relation to things like quantum information theory, but I wonder if there's a potential
2850240	2857600	overlap there. There are certain situations when thinking about these kinds of systems,
2858320	2863600	purely abstractly, where you kind of need two different notions of causality. You need a kind
2863600	2870480	of speculative notion that's dynamic, that can be rewritten, and then you need a kind of definite
2870480	2876560	notion that's immutable. A classic example of this is for something like quantum information
2876560	2881280	theory. You can have superpositions of causal orders, you can have quantum switches, you can have
2883120	2888320	causal structure that exists in superpositions of different kind of directed graph states,
2888320	2892400	but then once you apply Hermitian operator, once you apply a measurement, the causal structure is
2892400	2896480	definite because then everything is relativistic and you have covariance. You have similar things,
2896560	2903760	as I understand, with distributed computing, with parallel computing, where you potentially allow
2903760	2907520	for speculative execution for a certain number of steps where you're kind of treeing out this
2907520	2912640	multi-way system and you have a superposition or at least a collection of possible causal histories,
2912640	2916880	but then eventually you have to choose an actual operation to do and then the causal history,
2916880	2922240	you have this big block that gets laid down and then the causal history is somehow definite.
2922480	2932480	I wonder if there's a way of thinking about speculative execution of agents
2932480	2937600	and the interrelation between that speculative execution and agent actions in terms of, again,
2937600	2941600	this interplay between two different causal structures, between a dynamic one versus an
2941600	2948800	immutable one. Yeah. Well, one funny way to think about that is a single agent that has this
2948800	2955760	counterfactual contemplative ability could be in the center place foraging arena and then imagining
2955760	2962000	with discrete branching paths like a chess algorithm or like a probability distribution
2962000	2967200	could be like imagining where it could go, but not all cognitive things or the kind of things that
2967200	2974240	make plans of their own actions, whereas like an ant colony has nest mates on the ground. So
2974240	2980560	they're actually realizing in these finite trajectories, the real exoskeleton on the ground
2981440	2988400	that plays out, ending up with those simulated trajectories could have been simulated or could
2988400	2992960	have been probabilistically blurred, but that's kind of the difference between like the embodiment
2992960	2998480	and like the body moving there for a mammal or for an animal and then like the mind simulating it
2998560	3006480	and then just to the epistemic and pragmatic tradeoff in decision making. So let's just say that
3006480	3013840	we're in that multi-way moment. We'll just have two options, two different slices of the B variable
3013840	3019040	and the policy selection question is about which way are you going to go? Which affordance in the
3019040	3024800	moment? Policy is basically the affordances for the time horizon of planning, but if it's only one
3024800	3029040	time step or just the next one, then the affordance space is just the actions that can be taken.
3030960	3036480	One way to make it so that what happens is the likeliest thing, path of least action, which is
3036480	3041360	kind of what opens up the whole physics of cognitive systems angle in contrast to like a
3041360	3047120	reinforcement or reward learning perspective, what makes it the likeliest thing is starting with
3047120	3053520	habit. So it could just be drawn from a fixed habitual distribution. However, for adaptive action,
3054080	3059120	habit gets up-weighted with expected free energy, which is a functional that takes in
3059920	3064960	the policy space, which is summing up to one because there's a probability over actions,
3064960	3072240	and then up-weighting policies according to their score on expected free energy, which is
3072800	3078400	consisting of epistemic plus pragmatic value. So how much is it going to align the observations to
3078400	3084480	be what I like to see? That's pragmatic value with a preference. What is my expected information
3084480	3091680	gain? That's the epistemic value. So how those are parameterized make the agent that always seeks
3091680	3097520	out new information or always goes with habit or there's so much policy space because the knobs
3098240	3104560	are not just simple sliders or there are multiple knobs, even though they are seemingly
3105360	3113120	quite conciliant and minimal, like it's hard to imagine less, yet especially when there's
3113120	3119120	richness in the environment, even simple systems can have like enormously complex or
3119120	3126800	adaptive behaviors. I'll just leave it there. No, I never really thought of the, so okay,
3127840	3133280	yeah, two things, right? So first of all, the perspective of, you know, thinking of an ant
3133280	3137600	colony or a termite colony or something as being akin to a mind, that's, you know,
3138640	3143760	I was familiar with that perspective from people like Dan Dennett and so on. But the idea that the
3143760	3149680	individual ants in that colony are in a sense enact, they're kind of the hardware enacting the
3149680	3155120	speculative execution, that's a very interesting idea. It's not speculative for them. Well,
3155120	3158960	yeah, no, exactly. But it's sort of from the mind's perspective, I guess it's almost like
3158960	3162880	speculative execution, but it's speculative execution that's being actuated in the physical
3162880	3169760	world, which is very interesting. I not really thought about that before. But then, yeah, okay,
3169760	3175680	so then the point you're making about connection to free energy and sort of habit formation and so
3175680	3185520	on, okay, so I wonder if, you know, if we're thinking about a model of cognition in which
3185520	3190960	there are these two distinct causality notions, the immutable versus the dynamic one,
3193200	3199520	I wonder if the, so you gave a very, very nice account of how habit formation
3200080	3206240	sort of works in these kinds of formalisms based on, you know, prior experience of expected
3206240	3213760	free energy. So I wonder if there's a way of describing that abstractly in terms of something
3213760	3219040	like, you know, you perform the speculative execution step where you're, you know, you're
3219040	3224080	treeing out several multiway possibilities. And initially, you kind of, you know, you know nothing
3224080	3227680	or you have no habits, you're just kind of, you're treeing everything out with kind of equal waiting.
3228240	3233280	But then, you know, for each possible path, you're calculating either an actual or an expected
3233280	3239200	free energy. And then somehow, you know, in future speculative executions, you wait those
3239200	3243920	paths which you previously had found to have higher free energy as higher. And so, you know,
3243920	3248240	you're more likely to explore those and less likely explore ones which are, you know, which
3248240	3254080	have that lower expected value. It feels like something, something like that should fit very,
3254080	3258800	very nicely into an algebraic semantics like this, which would be interesting.
3258800	3264240	Oh, how about more questions from the chat? Okay, upcycle club writes,
3265520	3270080	acknowledging the limitations of traditional entropy in multi computations
3270720	3276480	motivates us to develop context specific entropy metrics. Can you share some insights
3276480	3283120	towards such efforts? Yeah, I can certainly try. So, so yes, I mean,
3284560	3289840	the first point is that, you know, it's, I think it was, there's that famous conversation between
3289840	3293680	John von Neumann and Claude Shannon, where I think von Neumann famously said that like,
3293680	3298560	Shannon should call his measure entropy because no one knows what it means, right? And I submit that
3298560	3303200	the reason that no one knows what entropy means is because it's dependent. I mean, okay, one of
3303200	3307120	the reasons no one knows what entropy means is because it's dependent on exactly what we've
3307120	3313200	been talking about is dependent on the equivalence function of the observer. So it's one of these
3313200	3322320	things like, I don't know, maybe this is a stupid analogy to use, but it's, sorry, I'm going to go
3322320	3326640	off on a tangent, but I promise it's sort of relevant. But so one thing that, okay, one thing
3326640	3331280	that always breaks my brain is when I try and think about like actuaries and life insurance
3331280	3340640	policies, because it's one of those areas where those models only make sense if they're not
3340640	3345760	perfect in a sense, right? Like, so if you had an actuary who knew exactly how long everyone was
3345760	3351040	going to live, and somehow that information was kind of openly available, there would be no,
3351040	3356720	like life insurance policies would be pointless. It's, but you know, whereas also if you had a
3356720	3360160	model that was completely hopeless of predicting how long people would live, they would also be
3361040	3367920	life insurance policies would also be pointless. The very existence of actuarial science
3367920	3373600	relies on your model neither being perfect nor being awful. It somehow has to exist somewhere in
3373600	3379040	between. And as I said, that's something which I, it's one of those topics where if I think about
3379040	3383600	it for too long, it all just stops making sense. And entropy has very much that same character,
3383600	3388960	right? Because if you were Laplace's demon, if you had perfect information about the system that
3388960	3393760	you were observing, there's no notion of entropy, right? It's just every, you know, you know every
3393760	3398640	micro-estate. So, you know, the Boltzmann formula gives you an entropy value of zero,
3399760	3405440	where, you know, the notion of entropy only exists once you take that perfect knowledge
3405440	3409680	of a system and you coarse-grain it, you define, as I was describing earlier, you introduce an
3409680	3415680	encoding function that is not 100% subjective, so that now you are mapping certain distinct
3415680	3420080	micro-estates onto the same coarse-grained macro-estates. And then now you can ask, okay,
3420080	3423360	what's the number of micro-estates that, you know, certainly how coarse is my coarse-graining?
3423360	3427360	What's the number of micro-estates consistent with this macro-estate? What's the number of different
3427360	3431520	values of my domain that gets mapped to a single point in my co-domain of my encoding function?
3431520	3437360	And that's what entropy is. And so it's, it's very closely, I mean, it is effectively a measure of
3437360	3441920	how good is my coarse-graining. So if you had perfect knowledge, there's no entropy. If you have no
3441920	3448240	knowledge, there's no entropy. It relies upon you having a not completely trivial, but also not
3448240	3453600	100%, you know, a slightly subjective, but not 100% subjective encoding function,
3454880	3461280	just like with life insurance policies. But so the reason, the reason I'm stating that is because,
3461280	3466000	so now it kind of, I think from that perspective, becomes a little bit clearer why there are all
3466000	3469840	these different notions of entropy and why, as the questioner was alluding to, why entropy seems
3469840	3476960	to be so, as a concept seems to be so domain and system specific, because every different system,
3476960	3481120	every different observer will, in principle, have a different set of encoding functions,
3481120	3484960	a different set of equivalence functions, and each one will give rise to a different calculation
3484960	3490000	of entropy. And so one way that you can think about this program, this program to try to
3491280	3495040	understand the algebraic interplay between time complexity versus kind of equivalency
3495040	3500880	complexity or computational irreducibility versus multi-computational irreducibility.
3500880	3506400	In some sense, that is a program to try to understand how different definitions of entropy
3506400	3512080	relate to each other in these kinds of systems. How, you know, if I take one idealized observer
3512080	3517920	that has this equivalence function, and I ask, okay, suppose now they communicate with this
3517920	3521600	different observer with a different equivalency function, they come to different understandings
3521680	3525280	of what the entropy of the system is, but what is the relationship between their measured entropy
3525280	3530320	values? They clearly is one that depends algebraically on some details of the distinction
3530320	3534320	between their respective equivalence functions, but there doesn't seem to be yet any general
3534320	3540640	theory for how those things are related, and that's part of the kind of raison d'tre of this
3541360	3549840	of this research program. But yeah, I mean, okay, so one thing that I will comment on,
3551360	3555200	although this is a little bit more speculative, it can sort of quasi philosophical comment, but
3555200	3561600	so one place where these notions of entropy become one place where the fact that you have
3561600	3565040	all these different notions of entropy becomes kind of interesting is in fundamental physics.
3566000	3570560	So when you start to think about, if you try to model physics and the universe in these
3570560	3576320	fundamentally computational terms, then one fairly generic sort of conclusion that you can reach
3576320	3580880	is that gravitation, general relativity is essentially an entropic phenomenon. I mean,
3581520	3585600	Valinde and people have kind of talked about this in non computational contexts too,
3585600	3588880	but it's very, very natural if you start to think about space like hypersurfaces being
3588880	3593520	a sort of hypergraphs, then, you know, in order to obtain a continuum geometry that's compatible
3593520	3598480	with the Einstein equations, you need to have certain ergodicity, you know, you need to be
3598480	3603920	able to make certain ergodicity assumptions on the rewriting, which in turn, sort of implies
3603920	3608640	certain lower bounds on the entropy of the system. So somehow gravitation, general relativity,
3608640	3613280	is a coarse grained theory that you obtain in the limit as the entropy goes to infinity.
3613840	3619920	But something that's interesting is that quantum mechanics, on the other hand, is an idealization
3619920	3624880	that you obtain in the limit as entropy goes to zero, because in kind of one of these purely
3624880	3629120	computational models of physics, the quantum mechanical state of a system is described in
3629120	3634640	terms of its multi-way structure. It's described in terms of, you know, when I have a kind of a
3634640	3640320	branching program like this, let me find one of these like this, then, you know, I can divide it
3640320	3645600	up into these sort of, into these simultaneity surfaces. And if I associate each state of the
3645600	3649360	program as being like the analog of a quantum eigen state, and the kind of path weightings
3649360	3653840	as being an analogous to the amplitudes associated with the eigen states, I can quickly build up
3653840	3657360	a description of this multi-way system in terms of the evolution of some discrete analog of the
3657360	3661520	Schrodinger equation. And it turns out you get a theory that is kind of equivalent of the mathematically
3661520	3666880	isomorphic to standard quantum mechanics out of it. So quantum mechanics is sort of, you know,
3666880	3672880	inextricably bound up with the phenomenon of the multi-way system. But if you take the entropy
3672880	3678800	to infinity here, then you're effectively, then the sophistication of your equivalence function
3678800	3682960	becomes arbitrarily large, which means that you can describe any, essentially any pair of
3682960	3689280	states as being equivalent. And so it turns out that the, that actually the quantum mechanical
3689280	3692720	case corresponds to the zero entropy limit, whereas the kind of general relativistic case
3692720	3697040	corresponds to the infinite entropy limit. But they're kind of two different, two fundamentally
3697040	3701040	different notions of entropy, one of which exists at the single-way level, one of which exists at
3701040	3706320	the multi-way level. And again, the question of how these things into play is partly why we're,
3706320	3710480	you know, why we're investigating this. And it's clear that that question has links to these
3710480	3717360	quite foundational questions in fundamental physics. The ideal point mass and the ideal
3717360	3724480	distribution with its center of gravity and all this, Dave, question for you, from you.
3725760	3736160	Yes. Okay, I was, can you hear? Yeah, go for it. Okay, good. Yeah, I'd like to hear down to the
3736160	3742560	low road during this discussion, maybe show business. What people are trying to explain,
3742560	3748320	what is computational reducibility or irreducibility? Often you'll see a graph that says, well,
3748320	3753760	now here's the, the computation, our target running along, the fox is running along,
3753760	3760320	and behind it, there's a team of algorithms that would like to catch it. And yeah, it either does
3760400	3766640	or it doesn't outrun all of them, but some can come, can seem to come pretty close to
3767600	3772960	catching it. Now, there's something else people have been looking at for a few years, the Mandelbrot
3772960	3779440	set. The Mandelbrot set really is a determined, not only a deterministic computation, it's a crisp
3779440	3788000	computation. Every set, every point, either is inside the safe zone, it's going to sit there,
3788000	3793840	and it's happy and quiet, and you color it black, or it flies off to infinity.
3795520	3800480	So you could, it could just be a bunch of yes or no, but that's not the way people
3801040	3806320	trying to calm down after a day of work want to look at it. They want to say, hey, I want to see
3806320	3813280	the 32 million deep Mandelbrot set. And I want to see it in colors. So when you get the colors,
3813280	3819360	you ask, how hard did I have to work? How long did I have to grind along before I made that decision?
3819360	3824000	Oh, this is the point that's going to settle down and be quiet and uninteresting, or it's going to
3824000	3831840	fly off to infinity. So you put colors in. Now, I just wonder, would it be interesting to anyone
3831840	3840640	to fuzzify these gorgeous causal and multi-causal graphs and just show this is the portion where
3840640	3848000	we had to work really hard, rule 35 or whatever. We had to go 15,000 generations before we gave up
3848000	3853840	and said, we're not going to follow this anymore. The other one, oh, after 30 steps, I see.
3856080	3861920	That's a really interesting question. And I mean, yes. So a couple of comments on that. I mean,
3861920	3867120	so one is, I mean, you're right in the sense that, yes, the Mandelbrot set is a very clean
3867120	3872480	thing to describe. I would say it's not, I mean, I would argue it's actually not a crisp
3873360	3877520	computation in that sense, precisely for reasons of computational irreducibility,
3877520	3881920	because as you go arbitrarily close to the boundary of the set, you can have complex
3881920	3886000	numbers that stay, that have a kind of indefinite period of transient, right? There's no
3886000	3893200	upper bound on how long the ZN squared plus C orbit can last before it either diverges or
3893200	3901120	converges to zero. And that statement that there will be points that can remain,
3901120	3905360	that can get tied up in these orbits indefinitely is really a computational irreducibility statement.
3906720	3914000	But yeah, I mean, your question about, could you construct, I mean, that way of coloring
3914000	3919440	points that are on the boundary of the Mandelbrot set in that way, the so-called escape time algorithm,
3919520	3923360	right? Where you kind of, where you color them based on how many steps that I need to
3923360	3928800	do before it either converged or escaped off above some, or the complex numbers modulus
3929440	3934080	exceeded some value. Yeah, it's an interesting idea that you could try and kind of construct
3934080	3938320	geometrical representations of the space of possible computations based on a kind of escape
3938320	3943120	time algorithm for computational irreducibility. Yeah, so I mean, there are the possible ways
3943120	3948320	that you could do that, right? That you said, we know, we've known since the days of Turing,
3948320	3952240	that the halting problem for Turing machines is undecidable, right? That there's no finite
3952240	3955840	computation that you can do that will determine over an arbitrary computer program will terminate
3955840	3960080	in finite time. So you could do it, you could, if you once you have a way of kind of geometrizing
3960080	3964960	the space of possible computations, which we, which we have now, as you say. Yeah, you could easily
3964960	3968240	construct a kind of escape time algorithm where you know, you have your analog of the Mandelbrot
3968240	3971600	set, which is, you know, here's all the halting computations, here are all the definitely non
3971600	3975840	halting computations. And then there's some boundary of very fuzzy stuff where we kind of don't
3975840	3980080	really know, we have to do a lot of work. And even then it's only heuristic, which is, which
3980080	3984800	is so, yeah, I mean, a very directly analogous. And then, yeah, and then the question becomes,
3985600	3991520	you know, so in the case of the M set, you know, there is some underlying theory mostly,
3991520	3995760	I think mostly due to like Dwadi and Hubbard and people that allows you to kind of predict,
3995760	4000560	like, you know, if you have a finite filament, you can, like, of the Mandelbrot set, you can
4000560	4005440	kind of predict where that filament is going to be based on some complicated complex analysis
4005440	4009840	argument. And, you know, I mean, the question is once you, if you once you have a geometrization
4009840	4015040	of the space of possible computations, can you construct some general theory like the theory,
4015040	4018480	you know, developed by Dwadi and Hubbard that tells you things about, you know, the topology
4018480	4023120	of the computations, whether certain regions of the space are connected, whether they're compact,
4023120	4026400	whether you can make, you know, whether you can do a similar thing, or you can make predictions
4026400	4030480	based on the geometry of one part of the set, you know, where you can extrapolate to things about
4030480	4035200	the geometry of another part of the set. That's a really interesting question. And yeah, again,
4035760	4039680	as with so many of these things, it's one of these, like, I don't know the answer, but it's a good
4039680	4045840	reason for investigating, you know, for pursuing the program, right? Yeah, from the Wolfram side,
4045840	4052720	as well as from the active inference side, there's both the information topology and the
4052720	4058480	information geometry side, and not just in the kind of topological deep learning, but rather
4058480	4064880	looking at the topology of information flows, which has been heavily developed in the category
4065120	4070800	related to quantum information sciences. And then the information geometries that allow us to do,
4070800	4077120	like, machine learning type, accelerated optimization ingredients, all these kinds of concepts that come
4077120	4084640	into play with geometry, and we just don't get them from topology. So topology kind of sketches
4084640	4090560	the skeleton, and then with the computers that we have, the information geometry is at least on
4090560	4095840	the data sets and the ways that we have computation today, that's kind of like the quantitative
4095840	4103920	numerical versus the formal. One question is, are these all discrete state space, discrete time
4103920	4110800	formalisms? Because in active inference, we often deal with hybrid models that have discrete and
4110800	4115440	continuous state spaces, and the same generative model or the same system of interest could be
4115440	4121360	modeled with like a discrete time chapter seven, or a continuous time chapter eight model. So how
4121360	4126400	does this deal with that? That's a really good question. I mean, yeah, so most of what I've been
4127440	4134000	looking at so far has consisted of discrete time, discrete space models, for no particularly
4134000	4139120	principled reason other than they're easier to analyze, right? For the most part, because you
4139120	4143040	can do explicit computations because they're kind of more amenable to constructive analysis.
4143760	4148560	It's easier to do, but the beauty of a lot of this, that's one of the beauties of using kind of
4148560	4153440	general mathematical formalism is that once you develop it, it's often quite easy to extend even
4153440	4158880	to cases that, or to extrapolate to the cases that you didn't explicitly analyze. So in principle,
4158880	4163760	this formalism works for continuum space and continuum time systems as well,
4163760	4168320	just with some slight modifications. So rather than having say, branching and merging, you instead
4168320	4172800	have, if you think about this thing as now being a dynamical system, described on the
4172800	4176640	on some symplectic manifold, then these kind of branching, merging operations of the multi-way
4176640	4182800	system become effectively divergence and convergence, differential operators are defined on the
4182800	4188640	symplectic manifold. And so one place where we can start to analyze that explicitly, and which
4188640	4192640	I've done a little bit of work on, but it's one of these things which I want to go back to very
4192640	4197440	soon, is looking at PetriNets. So PetriNets are interesting because they are a discrete time,
4197440	4202640	discrete space system, but they admit a continuum space, continuum time, description
4202640	4206960	in terms of ordinary differential equations and so on. So they're a nice example of a hybrid
4206960	4211200	kind of discrete event versus continuum event system, where it's clear that this formalism
4211200	4217040	can be used and is somehow agnostic as to whether the underlying system is discrete or continuous.
4218080	4221040	Again, there's a broader philosophical point to make here, which is that
4224480	4228320	in a way, one of the reasons I don't feel embarrassed to be working primarily with discrete
4228320	4233280	systems is because, again, once you start to think about things in terms of, okay,
4233280	4236560	you have to not just care about nature, you also have to care about the computations the observer
4236560	4241920	can perform and what it's able to infer, then you quickly realize that in a sense, just like,
4241920	4245520	whatever, as you were saying earlier, Danny, beauty is in the eye of the beholder. I think
4245520	4252240	discreteness and continuity are also in the eye of the beholder, right? So if you have
4252880	4262400	a universe that is fundamentally continuous, that's described by a continuum, the Renzi and
4262400	4266800	Manifold or something, but your constraints are that the only experiments you perform have
4266800	4274400	computable outcomes, have discrete outcomes, where the possible number of observables is
4274400	4278400	always countable, then in a sense, it doesn't matter, right? It's irrelevant to you as an
4278400	4283600	observer whether the system is discrete or continuous, because the only parts of it that
4283600	4287520	you can interface with and interact with are discrete, and so you could have replaced the
4287520	4290320	underlying substrate with a purely discrete mathematical structure and you wouldn't be able
4290320	4298000	to tell. So in some sense, I don't feel too embarrassed dealing with discrete event systems
4298000	4301840	because even if I don't necessarily believe that nature is discrete, because I don't think that's,
4301840	4306880	I'm not even sure how we would be able to answer that, I'm reasonably convinced that
4307840	4313200	the experiments that we can perform and the observations that we're able to perform are
4313200	4317440	ultimately computable, and therefore, the underlying substrate might as well be discrete,
4317440	4323120	even if it's not in reality, so to speak. Yeah, that's a great comment and definitely
4323120	4328640	calls back to your earlier points about discretization being in the eye of the beholder,
4328640	4334160	like in the active inference models, observations, raw data may already be discretized depending
4334160	4339520	on the situation, but even if it weren't, like it were a continuous sensory perception or modeled
4339520	4346800	as such analytically, still commonly models discretizing categorize as they move up cognitive
4346800	4353360	hierarchies, and that was like initially explored to get more of this discrete either or decision
4353360	4361120	making, planning, all those kinds of properties. Well, there's many interesting angles like
4361360	4368720	I'm sure also it could be a multiplexed language model prompt, but like what are you working on
4368720	4377440	or excited about for 2024? That's a good question. So I've kind of already given some hints about,
4377440	4384400	you know, like this general research program of trying to understand computational complexity
4384400	4388240	and algorithmic complexity and interplays between observers and systems through this
4388240	4394560	category theoretic lens. That's a major thing which I started on, say, about maybe a couple
4394560	4398240	of years ago. I mean, in some form, I've been working on it for a long time, but this more
4398240	4402480	recent perspective on it is maybe a couple of years old. But for the various reasons over the
4402480	4408640	last year or so, I've kind of put that to rest and I've been focused on these much more physics
4408640	4412560	oriented questions about discrete space time and understanding things like, you know, how do
4412560	4416400	black holes work and how does accretion work in discrete space time, which is also very important
4416400	4423040	and very exciting. But I've sort of slightly been missing these more abstract directions. And so
4423040	4428400	I have maybe one or two major physics related things that I need to finish off and then I
4428400	4433200	really want to go back to this to the greatest extent possible. And yeah, I mean, so one thing
4433200	4441440	is that's quite clear is that there's great interplay between this formalism and existing
4441440	4445120	theories of computational and algorithmic complexity. So in particular, you know, so
4445760	4450960	one very basic example is I mentioned before that, you know, you have this kind of coherence
4450960	4454560	between these two different algebraic structures between your the operation of
4454560	4459200	time like composition versus the operation of kind of parallel composition. And these two
4459200	4464000	algebraic structures are in general related, although the precise conditions that relate
4464000	4467440	them are not clear. And that's partly what we're trying to what we're trying to understand.
4468400	4474800	But it turns out that degenerate cases of that of that question corresponds to unsolved problems
4474800	4479840	in computational complexity theory. So, for instance, the P bus NP problem can essentially
4479840	4483600	be recast in these terms that you can recast the P bus NP problem is the question about
4484720	4489840	is the coherence between the time like composition of computational complexity and
4489840	4493680	the parallel composition of computational complexity, which are what P and NP respectively
4493760	4498560	are really about, it are those coherence conditions, the strictest they can be,
4499360	4504080	which would be the case that the P equals NP, or are they somehow more lax, which would be the
4504080	4509280	case that P does not equal NP. And so there's, you know, that's that's one thing that we kind
4509280	4512800	of were already investigated, but it's clear that a whole bunch of questions about, you know,
4513760	4518000	how the time complexity and space complexity trade off or how to come over of complexity and
4518000	4522080	time complexity trade off, these, these are questions which can be recast in this kind of
4522080	4528240	more algebraic category, theoretic lens, and, and, and will hopefully give insight into this
4528240	4532480	general program of trying to understand observers and their relationship to the world. And those
4532480	4537280	are kind of major, well, with any luck, those are major theorems that I hope we'll be able to
4537280	4544640	prove at some point in 2024. That's that's that's that's awesome. And it makes me think about
4544640	4551760	parallel, more nest mates, more CPU threads, deeper in times, more sequential, more planning,
4551760	4557360	and more cognitive single monolithic agent. And then the kind of question is like, can anything
4557360	4565200	that a single agent, mega matrix could do, cannot be decomposable at space advantage,
4565200	4571440	or even at space disadvantage in decomposed it into a single time step operation?
4572320	4577280	Yeah, that's a super important question. And one that, you know, with the possible exception
4577280	4582880	of this community, not many people have asked, right? I mean, so that's something which comes
4582880	4588560	up in quantum computing, right? So a lot of the hype around quantum computation comes from these
4588560	4592960	theoretical speedups that derive from the fact that you're able to, you know, you're able to
4592960	4597680	support these super positions of different, you know, where, you know, each, that each state of
4597680	4601120	your data structure corresponds to a different eigenstate, and you're able to evolve some super
4601360	4604720	superposition of those eigenstates. But then at the end, you have to actually come to a
4604720	4608320	definite conclusion about what the answer is, you have to perform some measurement operation.
4608320	4612400	And that measurement operation is lossy, it's often non deterministic, you, you know, you often
4612400	4618000	have to repeat it multiple times. And, you know, it's becoming increasingly clear that for a large
4618000	4622640	class of operations that were previously thought to have quantum advantage, the additional complexity
4622640	4626560	of the measurement step really kills any quantum advantage that you may have had that you know,
4626560	4631040	you get some advantage by doing unitary evolution. But then you lose all of it by having to do
4631040	4636160	the submission projection at the end. And that's really a story of, again, this interplay between
4636160	4641040	the time complexity saving of doing a multi doing a computation of doing a multi computation in
4641040	4645920	parallel, versus the loss that comes from the complexity of the equivalence function that
4645920	4650640	you need to apply in order to get to some definite conclusion about what the, you know, about what
4650640	4655040	happened because, you know, ultimately, you need to somehow collapse that that that directed graph
4655040	4659360	into a single thread of time in order to be able to have some coherent representation of what happened.
4660160	4663840	And so again, understand that, you know, that's a place where understanding these, you know,
4663840	4668640	the these tradeoffs will become very important. And as I say, in some limiting case that that gives
4669280	4673200	some perspective on your question, Daniel, which is, which I agree is a very interesting question
4673200	4679600	about, you know, in principle, we know that anything that a deterministic Turing machine
4679600	4684160	can do a non deterministic Turing machine can do and vice versa with some speed up or slow down.
4685120	4690480	But that statement, which is a classic result in, you know, in computability theory, neglects
4690480	4693840	all consideration of the equivalence function. So there may be cases where the equivalence
4693840	4699040	function is so complex, that essentially, you know, that to do state equivalence becomes
4699040	4703200	undecidable. And so in that case, you have a scenario where actually, you know, you've got a
4703200	4708720	multi computational system, but to collapse it to one that's equivalent to a single way system
4708720	4712000	requires unbounded amounts of computational effort. And so that's so actually they become
4712080	4718000	inequivalent, even though, you know, computability theory says they should be the same. So it's
4718000	4723280	clear that there's a there's a more rich, more subtle theory that's underlying here that we're
4723280	4726720	just beginning to kind of glimpse, and that I hope will, you know, we'll be able to kind of
4726720	4730000	to prove some new limited results about soon, once we understand it a bit better.
4731360	4737520	Awesome. And I think definitely a special shout out to all of our colleagues on either like the
4737520	4744640	Wolfram and or active inference side, because we've seen few if any active inference models
4745520	4752640	phrased analytically or computationally with the Wolfram technology from studying complexity
4752640	4759200	in other areas. It's really clear to see how productive and powerful the software and the
4759200	4765600	tools can be and changing and growing every day. So it's really interesting, maybe someone can
4766560	4773200	if they're listening this far in, like, go from one side to the other and back or make a Wolfram
4773200	4781040	active inference model, or do some other kind of combination, because it's very fruitful territory.
4781920	4791440	And we know that our elders have already spoken, they've okayed it. No, but really, it's so
4792240	4799280	rich with connections here between the areas that we're all studying and feeling like converging
4800000	4804880	on many common places to scaffold and jump off from together.
4807040	4813440	Yeah, I agree. And I mean, at least the kinds of things that we were discussing here about,
4813440	4817360	you know, speculative execution and behavior formation through free energy principle and
4817360	4821680	so on, those things should be relatively easy to implement in the framework that's already been
4821680	4827360	developed here. I mean, so that's just a question of just implementing some kind of computation of
4827360	4832720	expected free energy and using that to weight multiway paths in the speculative execution model.
4833840	4840480	So at least the beginnings of that implementation, I think the path is pretty clear. And we will
4840480	4844720	probably end up doing at some point in the future anyway, as part of other research.
4845680	4850480	So yeah, so I agree. It's a very exciting kind of point of interface.
4851040	4856960	Yeah, well, I hope that we can stay in touch if you ever want to come back for a
4858160	4864480	009.2, or if we want to even facilitate some kind of working group or some connections
4865040	4870400	to really strengthen and like include the participation of more people in this super
4870400	4875360	exciting area, that would be amazing. Yeah, that sounds fun. Let's let's try and set something up.
4876160	4882880	Well, Dave, first penultimate comments, then Jonathan, you can have the kind of last comments.
4883680	4891120	Yes, I hope you do get to continue on the Wolfram physics side to think about a more general notion
4891120	4897840	of what these ultimate things are. Are they observers or does that already prejudice the case
4897840	4905360	of what you might find if you call them workers or actors or, you know, go back and ask Stuart
4905360	4913520	Kaufman, what must a mind do to earn its way in the world? Let's make this keep happening. Thank you.
4914400	4919920	Thank you, Dave, for suggesting it also. It was a great suggestion. Jonathan?
4920880	4926720	No, I think that's a fantastic note to end on. I mean, in a sense, this idea that we should
4926720	4930240	start to move, I mean, so it's okay, big picture for a moment, like
4931840	4936240	where, you know, this formalism is being developed, the formalism I've described in this
4936960	4940960	in this discussion is being developed, you know, assuming a kind of purely passive observer
4940960	4944560	idealization. And that's already been incredibly difficult, right? This is clear, there's a lot
4944560	4948960	we don't understand at that. But of course, David is right that in a sense, you know, what, you know,
4948960	4953760	ultimately, we want to start transitioning to a participatory observer model, where you allow
4953760	4957280	for two-way interactions or, you know, higher order interactions between observers and systems
4957280	4961840	and things that don't just go in one direction. And yeah, you know, in a sense, I view a lot of
4961840	4965600	what we're trying to do with, you know, trying to nail down these notions of causality, trying to
4965600	4969760	understand these interplays between different complexity and entropy measures as, you know,
4969760	4974400	the necessary groundwork for developing that subsequent theory, right? That does, you know,
4974400	4979600	it's clear that if we want to have a version of the, you know, of this kind of compositional
4979600	4984000	multi-way formalism that is also compatible with things like second-order cybernetics,
4984000	4988400	then, you know, at the very least, we need to have a very coherent notion for what causality is
4988400	4993760	and, you know, and a robust algebraic description of that that's not going to break or change.
4993760	5002720	And so I think the nonparticipatory observer model is a useful starting point because it's one that
5002720	5008160	is just within our grasp of, you know, of being kind of mathematically tractable. And then the
5008160	5012320	hope is that the technology and the ideas and the conceptual structure that we develop for
5012320	5017360	understanding that will then, as I say, lay the groundwork for developing something that's more
5017360	5024480	like what real, you know, active participatory observers do. And, and yeah, I mean, I think I
5024480	5029760	don't, on the, on the scientific side, I'm not really sure I have any final comments apart from
5029760	5033360	just, you know, as obvious, this is a, you know, this is still a story that's being, that's being
5033440	5039200	developed. And, and yeah, as Daniel alluded to, I hope that we can continue to interact and collaborate
5039200	5044320	where, where that makes sense. And, and yeah, at the very least, I think, in cooperation of kind
5044320	5049600	of these, you know, these active inference models within these discrete time, in the first instance,
5049600	5054400	discrete time, you know, computational frameworks, and allowing things like speculative execution
5054400	5059760	and multi-way path waiting based on free energy estimates. I think that's, you know, that's a,
5060000	5065840	that's a project that's of obvious mutual interest and, and something that I, that I hope will happen
5065840	5072160	in the coming months. Thank you. We just speculatively executed active Wolfram inference.
5073520	5078320	Basically. Sounds very good. All right. Thank you, Jonathan. Thank you, Dave. Thank you, everyone.
5079040	5082160	See y'all next time.
5089760	5091140	you
5119760	5121140	you

Okay, welcome everybody.
Very glad you could make it for this talk.
I've been working on this for a while.
I'm really excited about it.
If you don't know me, I am Gabriel,
and I'm an instructor here at Fullstack Academy of Code.
My social media presence is G. Lebec everywhere,
except, unfortunately, and somewhat ironically,
given the subject of this talk at Twitter,
where I was forced to use an underscore.
There's going to be resources available on my GitHub
at slash G. Lebec slash Lambda Talk,
so you can go find additional stuff after this.
All right, so let's just dive in and get started.
I want to start by pointing out a minute node ripple here.
I can do kind of typical JavaScript-y stuff,
but we're going to avoid JavaScript in the sort of classical sense
as much as possible, and start off with this function
called I or identity.
Now, just looking at the function,
if I invoke it with some value,
what am I going to get back here?
Yeah, identity of one is one, identity of two is two.
What about the identity of identity?
Yeah, it's the identity function.
So, very simply, the identity combinator
is a function that takes in an input A
and it returns an output A.
Basically, it just reflects back a value.
So, the identity of any X is X,
and we saw that in this particular paradigm,
we're allowed to use functions as arguments.
Verbs are nouns and nouns are verbs,
so the identity of identity is itself.
In Haskell, this is actually built into the Prelude module,
which is the base standard module for the language,
as the ID function.
As you can see, it takes an argument five
and gives you back five.
So, looking at this, I just flashed a bunch of stuff on the screen.
You might be thinking, what's this lambda stuff
that's sitting up here?
So, lambda is a signifier.
It's a notation that we're going to use
to indicate that we're starting the definition of a function.
So, we can read this as we're starting to define a function,
which takes a single input or parameter,
and it returns some expression, some body.
And this whole thing is called a lambda abstraction
in the lambda calculus,
but it basically just means it's a unary anonymous function.
Unary meaning it takes a single input.
The lambda calculus is a really tiny symbol manipulation framework.
A calculus is just a way of moving around symbols on a page.
The subject that you may have learned in school called calculus
is a specific calculus for things like differentials
and integrals and stuff like that, derivatives.
And so, this calculus is about something else.
This calculus is about evaluating and defining functions.
So, in the lambda calculus, we have variables,
which are pretty boring.
Please come in.
We've got expressions,
which are the application of some function to its argument.
We've got an expression that is itself a function definition,
an abstraction instead of a concrete thing like 1 plus 5,
1 plus 6, 1 plus 7.
We have something like 1 plus A.
And A can be anything.
It's become abstracted.
And we have groupings to disambiguate the order
in which we should be doing certain operations.
This is the entire lambda calculus.
All right.
So, talk's over.
I can go home now.
No, we're going to see some examples of this.
So, let's start with just a couple of cross comparisons here.
Variables in the lambda calculus, as I mentioned, are extremely boring.
One thing to note here is that these variables are immutable.
They cannot be changed after the fact.
There's no concept of assignment, per se, in the lambda calculus.
There is binding, which we'll see shortly.
But if this variable is bound to a value,
that's its value for now and forevermore.
So, no, let all const.
Applications are slightly more interesting.
This is applying a function to its arguments.
And in the lambda calculus, that's just juxtaposition.
It's just a space.
There's no parens for invocation,
which at first, if you're used to something like JavaScript,
it's a little disconcerting.
But in reality, it ends up removing a lot of noise from our expressions.
And as this talk goes on,
you're probably going to see it's becoming easier and easier to read the lambda calculus
rather than trying to parse the JavaScript.
So, here we apply a function f to its argument a.
We can apply multiple arguments,
but in the lambda calculus, all functions are unary.
So, this is really a curried function.
This is an f that takes an a,
and that returns a new function,
which is not written down, which takes a b.
Let's try this out.
Let's do kind of a classic example of a curried addition function.
We'll say that add takes an a,
and it takes a b, and it returns a plus b.
So, if we call add with just some single argument,
we get back some function.
If we call that function with an additional argument,
now we get the final result.
Pretty familiar stuff, I think, for this crowd.
So, f a b, you can read that as first it takes an a,
and then a b, and if there are more arguments,
they get fed in one by one,
going from the leftmost argument onwards,
marching kind of toward the right.
We can make this clear with parentheses.
We can say, first apply f to a,
and then apply the result of that to b,
but this isn't necessary because we say
that function application is left associative.
So, these are really useless parentheses,
and we'll omit them in most of our examples.
Finally, we can use parentheses to force evaluations
to occur in a different order.
So, here this is actually a different expression.
This means something else in the lambda calculus.
It means we're going to first apply a to b,
and the result of that will be the argument to f.
Hi, hello.
Welcome.
So, just to summarize for everybody who's just arrived,
we're starting to define the syntax
of a very small language called the lambda calculus.
And this is just something that takes functions,
applications of functions to arguments,
and parentheses for grouping.
That's really it.
So, really high level review of just this last slide.
F a means invoke f with a,
and you'll get more as we go.
So, let's talk about function definition in the lambda calculus.
As I said before, we use a lambda to indicate
that we're defining a new function.
So, we got lambda calculus on the left,
JavaScript on the right.
This is a function that takes an a and returns whatever b is.
Sort of throws away a,
a b becomes irrelevant in this expression.
The function abstraction like this is greedy.
The body swallows up as much stuff to the right as it can,
and that's all included in the body of this lambda abstraction.
So, up into the point where it would stop making sense,
like if you're using parens to force things to be evaluated differently.
So, this indicates a function that takes an a,
and it invokes b with x as its argument.
We can disambiguate this with parentheses,
but as I said, since lambda abstractions
greedily swallow up everything to the right,
these are actually useless parentheses,
and we'll not really show them in most of the upcoming examples.
On the other hand, if we use parentheses
to force the thing on the left to be a function with a body
that's just the b variable,
now this is a different expression.
Now we're saying take a function that goes from a to b
and apply it to the argument x.
As we saw a second ago in a demonstration
in the lambda calculus we're allowed to nest functions.
So, this is a function that takes an a
and returns a new function that new function takes a b
and that function returns an a.
So, just to reprise our curried addition function from a second ago,
we have this function that takes an a,
returns a function that takes a b,
and that returns a plus b.
So, we have to feed out the arguments not like this all at once,
but rather each argument one at a time
in a successive invocations,
and we get the result we want, known as currying.
Again, we can clarify this using parentheses.
The lambda on the left returns the inner lambda,
and then we can use that.
But since these are useless parentheses,
we'll omit them in most of the examples.
How are we doing so far?
Questions about the bare syntax of the lambda calculus?
As you can see, it's very similar to JavaScript,
except you don't have parentheses around function invocation.
You just put things next to each other,
and that indicates apply a function on the left
to the argument on the right.
Cool.
All right.
So, I swear this is going to be the most complicated bit
of lambda calculus syntax we have to deal with.
It's called beta reduction.
It's got a scary name, but it really just means
tracing the logic, evaluating the function invocations,
seeing what we end up with.
So beta reduction is just the act of taking a function
and applying it to its argument.
So here we've got this function in red
applied to the argument underlined.
What we do is we take the argument,
and it comes in as the parameter of the function.
This is a function that takes an A and returns an A.
So this argument, the BCB function,
is going to replace in the body of the function every A.
We go look in the body, find all the As,
and we substitute in this other expression.
And that gives us this first simplification.
Well, we continue doing this.
We've got a new function, the BCB function,
being applied to another argument, the X argument.
So we're going to take the X argument
and substitute it in as the parameter to the function.
So we go look inside the function body,
find all the B's, and replace them all with X's.
And we get this new function as a result.
Once again, we've got a function
that we're going to apply to an argument.
We take the argument, we pass it into the function's parameter.
We go look in the body for all the C's.
There are no C's in this body.
We replace all the C's with that argument,
and that gives us this simplification.
At this point, we've got nothing left to do.
There are no more reducible expressions.
So we say this is in beta normal form,
which is just a hilarious way of saying
we've fully evaluated the function in a terminal way.
So it looks a little dense
compared to what you're used to with JavaScript,
but it's really nothing that we haven't done many times in JavaScript.
Taking a function, passing in arguments as the parameters,
and then in the body, everywhere that parameter exists,
it's been replaced by a value.
There's some caveats here.
I'm not going to cover in this presentation.
If you could do multiple reductions
in different places in the expression simultaneously,
there are caveats and strategies to which ones you should do first.
And there's also possible ways that two separate functions
that coincidentally share the same variable names,
you want to avoid conflating those two variables,
which mean different things.
So there are some gotchas,
but they're kind of outside the scope of this talk.
Let's see another combinator here, the mockingbird.
This is a fun function.
It takes a function as input,
and it invokes that function passing in itself.
This is the self-application combinator.
Whoa, what's happening here?
This is craziness.
Let's try it out.
The mockingbird is a function that takes a function,
calls the function on itself.
What might the mockingbird of identity be?
Somebody walked me through the logic here.
What's f in this function?
It's the identity.
What are we doing with it?
Identity of identity.
We already solved that before, right?
What is the answer?
It's the identity.
The self-application of identity is identity.
We saw that earlier.
This one's going to be a little bit more disconcerting.
What is the self-application of self-application?
Feel free to yell it out if you think you got it.
It may be hinted at by the fact that I'm putting this in a try-catch block.
Any takers?
Call stack size exceeded.
What just happened?
Well, mockingbird of identity is identity of identity, self-application.
We already know that that is identity, so that made sense.
That does reduce to a better normal form.
But the mockingbird of the mockingbird is the self-application of self-application.
So we take self-application and we apply it to itself.
But if we're going to evaluate that, that's the self-application of self-application.
So we take the self-application and uh-oh, it just goes on forever.
This is a problem.
The problem is we don't always know if some lambda term is going to have a beta normal form.
We don't know if this process ends or not.
Sometimes it doesn't end.
Sometimes it diverges, which means it goes on forever.
And in fact, there's no way to know in a general way if a given expression,
there's no single algorithm that can tell you whether or not one of these expressions will stop.
That's known as the halting problem and Alan Turing figured it out.
Now for an individual one, you can prove through ad hoc means that, yeah,
for instance, this one is going to go on forever.
So it's not that it's always unanswerable, just that there's no one set of steps that you can take
that will cause that, that you will know whether or not it halts.
This particular divergent term, by the way, is called the omega combinator.
Omega because it's like the end, alpha and the omega.
And sometimes the mockingbird as a result is called little omega.
One of the problems you'll find if you start going out and reading about all this stuff
is that a lot of different mathematicians and programmers and people have worked on it over time
and they've all given their own pet names to these things.
So there's a lot of synonyms and sometimes even intersection.
Okay, we're almost done with lambda calculus syntax, but I lied before and I said it was the end.
There's one more thing I want to show you about syntax here.
We can do, as I said before, these nested lambdas.
We could say there's a function that takes an A and that returns a function that takes a B,
which returns a function that takes a C, which finally returns a B.
But the way we're using these functions, we just kind of think of, well,
we're going to call it with both arguments at once in quotes,
really meaning we feed it the first argument then the second argument.
But we think of it as taking two arguments just in a curried way.
So in order to kind of make it easier to write this stuff down and parse it,
there's a little bit of a shorthand where we just condense all the nested lambdas
and say, here's a ternary function.
Here's a function that takes three inputs and returns something.
But don't get fooled.
These don't come in all simultaneously.
We feed them into the function one after the other.
So this still means nested lambda expressions.
It's just a convenient shorthand for indicating that they're curried.
I'm not going to go through all of the logic of this again.
It's the exact same example we saw before of feeding an argument into its parameter
and replacing the parameter in the body, feeding an argument into its parameter
and replacing the parameter in the body.
But this time I've used the syntax shorthand to show those nested lambdas.
So the body of this one is another lambda and then we proceed as normal
and the rest of this is exactly identical to what we saw before.
So just a shorthand.
Don't get too tripped up over it, but it's going to be convenient for us
to be able to think conceptually of functions that take multiple arguments
even though we know, we'll just keep that as a footnote, that they're all curried functions.
Are you ready for the next combinator?
Let's talk about the kestrel.
The kestrel takes an A and a B and it returns A.
Let's try that out.
Takes an A, takes a B, returns A.
What is the kestrel of the identity in the mockingbird?
Yeah.
What about the kestrel of the kestrel in the mockingbird?
Right.
It doesn't matter what the second thing is, it's irrelevant.
The kestrel just takes two things and returns the first one.
So we just saw that here.
In Haskell, this is built into the base language as the const function.
Why is it called const in Haskell?
Let's try something interesting here.
I'm going to say k5 is the function you get when you call k with only one of its arguments.
So normally the kestrel takes two arguments and gives you back the first one,
but I'm only going to give it one of its arguments.
Well, that's interesting, but these are curried functions, so I can give it another argument.
It gives me back the first thing.
I can also give it back some other argument.
It gives me back the first thing.
This is a function that's fixated on a particular value.
It's the constant five function.
No matter how I invoke this function, it always gives me back five.
That's why it's called const in Haskell.
But it's the k-combinator.
Here's a fun one.
I really like this one.
The kestrel of identity and some x is?
Identity.
Identity, makes sense.
This is an algebraic equality.
The thing on the left is the thing on the right and vice versa.
Well, the thing on the right is a function.
And I know in the lambda calculus I'm allowed to apply functions to values.
So I apply this to y.
What do I get?
Yy.
Y, that makes sense.
On the left side equals the middle side equals the right side.
That means I can ignore the thing in the middle,
and the thing on the left equals the thing on the right.
Does anybody see where I'm going with this?
I've got ki of x and y returns y.
Ki of a first argument and a second argument returns the second argument.
I just derived the kite.
The kite combinator takes two arguments.
One and two.
And it gives you back the second one.
Now, I could have done this manually.
I could have said the kite is a function that takes an a,
and it takes a b, and it returns b.
And then I would call the kite on something like four and nine,
and it gives me back nine.
But it's cool to see that from these atomic combinators combined together,
I get these new molecules, which are other combinators.
So some of my combinators can mix and match
and start producing other functions that are useful and interesting in different ways.
So ki of m and ki is?
Yes.
So kite of the mockingbird in the Kestrel's castle.
Flip them around.
You get the other one.
So far, so good.
At this point, probably some of you are wondering,
what's with all the bird names?
Got mockingbird, Kestrel, kite, et cetera.
All right.
So we're going to take a little mental break for a moment and talk about history.
Moses Eliachchenfinkel.
And I can't pronounce German, so please forgive me.
Name these things, long German names.
Like Zussim et Setsum function.
Haskell B. Curry, who came a little bit later,
used some of the same letters, but also some of his own letters,
just to add to the confusion.
And in the 1980s, a logician and puzzle author named Raymond Smullyan
wrote this absolutely wonderful book to mock a mockingbird.
The whole back two thirds of which is about combinatorial logic.
And Smullyan took Curry's combinator names and turned them into birds.
And the reason is because the book doesn't lay a lot of emphasis
on the math from a formal perspective.
Instead, it uses this metaphor of birds in a forest
who hear the songs of other birds and sing back the names of other birds.
So birds creating birds and singing birds and all this kind of stuff.
Now, unfortunately, Smullyan called the identity combinator the idiot bird.
I wish that he had used the ibis.
I used the ibis in my first slide, but we'll forgive him
because the rest of the book is just so wonderful.
Now, he didn't do this totally for no reason.
He actually did it to honor Curry because Curry was himself an avid bird watcher.
And at this point, some of you are thinking, Haskell Curry, who's this guy?
Why do we care about him?
I'm not Sean Finkel or vice versa.
So the next slide is the anti-diversity slide.
This is just the historical nature of mathematics in the 20s and 30s.
It was filled with a lot of white men.
So we'll acknowledge that and move on.
But I'm going to give you a really fast crash course
in the formalization of mathematical logic.
Around the late 19th century, early 20th century,
people were realizing that math, which had seemed on the face of it
so cut and dried and straightforward, was hiding some really nasty paradoxes.
And so people were trying to unify mathematics and figure out
the sufficient axioms that would define all of mathematics in one big tone.
So what was like the real, true system of math?
Bless you.
So I'm going to race through this.
I could do an entire presentation.
That would be two hours on just this.
I'm going to try and keep it to less than five minutes.
In 89, Giuseppe Piano invents his own formal notation for function abstraction.
He also defines arithmetic as the sequence of natural numbers starting from zero,
and then the successor of zero, and then the successor of successor of zero,
and the successor of successor of successor of zero, and so on and so forth.
These are Piano numbers.
The logician Gottlob Frege develops his own function notation,
which uses this really unique graph format.
It's actually really cool to say.
Impossible to read.
But a good idea, a good idea which has better versions that come later.
And most famously, he basically invents quantified axiomatic logic.
So this is the kind of sentence like, for all x in the reals,
there exists a y in the reals such that y is greater than x.
For all, and there exists.
That's the quantification and quantified axiomatic logic.
By the way, Frege, even in 1891, was using curried functions.
In the 1910s, Bertrand Russell, along with Whitehead,
very famously published Principia Mathematica,
an attempt at formalizing all math,
but he also discovered earlier than that, actually, Russell's paradox,
the thing of all sets that do not contain themselves.
Well, is that set in itself or not?
It's a paradox. It's impossible to figure out.
So that made a lot of people really disconcerted,
and they realized that math wasn't quite so perfect in its foundations as first thought.
Schoen Finkl, we talked about, he was an early pioneer in combinatorial logic.
He also used currying and published one paper, and then it was really sad, actually.
The rest of his life really spiraled downhill.
He ended up in a mental asylum, which don't worry,
it's not because of combinatorial logic, I don't think.
Van Neumann, famous mathematician, also later in life,
helped build the first real electric computers.
He also kind of did something that, if you reinterpret it, it's combinatorial logic,
but that was almost by accident, like that wasn't his goal in life.
And then in 26, Haskell Curry starts reinventing combinatorial logic.
He wasn't aware of Schoen Finkl or Frege, well, he was aware of Frege,
but not any kind of link to combinatorics.
And so he does a whole bunch of contributions.
He's a really smart guy at Princeton, and in 1927, he discovers that Schoen Finkl beat him to the punch.
So he forges on nonetheless, which is good for us,
because he develops many, many new theorems.
In 1931, Cort Godot, very famous mathematician,
who kind of plunged a dagger into the very heart of math,
discovers that this race in search for the perfect set of foundations for math is fundamentally flawed.
It's a fool's errand, it's impossible, it's literally impossible,
because every complicated enough system to be interesting by a certain definition of interesting,
such as piano numbers, is either inconsistent or incomplete.
That means there's either logical inconsistencies which make it make no sense,
or there are systems and things inside that language which you can talk about,
but you can't prove or disprove.
There's no way to get to the proof or disprove.
That totally upends math as we know it.
In the 30s, Alonzo Church is trying to figure out a system that's at least good enough
to compute things that are computable in some definition of computable,
and he develops this thing that we've been talking about the whole time, lambda calculus,
this notation system for writing down functions.
Now, it's this tiny, tiny language, and at first, his grad students such as Stephen Klaney and Rosser,
they think that it's not going to really lead too much, like it's just a notation system,
but then they start to realize like it's ballooning outwards,
and it's from this very tiny bit of logic is coming all sorts of interesting results.
They also prove different versions of it are consistent or inconsistent.
Stephen Klaney, by the way, goes on later in life to invent this thing that we use all the time called regular expressions,
so that's fun.
In 1936, Alonzo Church solves a famous unsolved problem, David Hilbert's decision problem.
This is an algorithm problem that says like, well, actually, I can't remember the specifics,
but it's, let me see.
Oh, is there a way to figure out if any given problem does have a solution,
very closely related to, but distinct from the halting problem?
And Church is like, no, there isn't.
But he uses the lambda calculus to solve it, which was hilarious because it started out as three or four lines of notation,
and it turned into a system complicated enough to solve this famous unsolved problem.
Guess what also happens in 1936?
Two months later, Alon Turing solves the problem using something called a Turing machine,
and he publishes his own paper, and then he finds out that Church beat him to the punch by two months,
and he gets really annoyed by that.
He was actually quite disappointed to find out that someone else had raced him
and beat him to solving the decision problem.
But he looks at the paper, he looks at Church's lambda calculus and says, you know what?
These are actually identical.
My Turing machines and Church's lambda calculus are the exact same thing just expressed different ways.
So then he decides, you know what, I'll bury the hatchet, come to Princeton, get a PhD under Church with Church as his advisor.
And in 1937, he publishes the first fixed point combinator.
Okay, that's the history.
In the tiniest nutshell I could manage, or at least bear to part with.
So, combinator.
You keep using that word.
I don't think it means what you think it means.
What is a combinator?
We've said this thing many times, combinator logic and combinator's lambda calculus.
Where's the dividing line?
In reality, they're almost entirely the same thing.
A combinator is a function with no free variables.
A free variable is a variable in a function body that's not bound to some parameter.
So this is a combinator because the B in the body is bound to the B parameter.
Whereas that's not a combinator because A comes from nowhere.
What is A? Who knows? We could make it up. It doesn't matter.
It doesn't matter that we're not using the B. The B is irrelevant. It's a parameter.
Not a combinator because where does the B come from? Don't know.
And even more complicated stuff like this.
Don't get distracted by the E. That's a parameter. It's not a variable.
So everything that appears in the body, C and B, those are bound to parameters.
That's a combinator. You now know what a combinator is.
We've seen a bunch of combinators. Identity, self-application, first or const, second.
And the cool thing is, as I mentioned before, using some of the primitive combinators mixed together,
we start generating some of the more complicated combinators, or at least other combinators,
which is surprising but cool.
Are you ready for the next one?
Let's look at the cardinal. Let's just look at it for a second.
Not the beautiful photo, which I stole from somewhere. I can't remember where.
But the combinator itself, anybody looking at this, can you think conceptually what this actually does
in sort of a use case way?
I'm really pushing you here because we're talking about abstract math,
but we're just keeping you in a land with no JavaScript.
Exactly. It just flips around arguments. It takes a function f that takes two parameters,
and then it calls the function f with both of those parameters, but in the opposite order.
So here's the C combinator, and what if we apply this to a function in two arguments,
which is the kestrel and these other two things that we don't care what they are,
but they're i and m in this case.
The cardinal of the kestrel and the idiot and the mockingbird.
Well, walking through it, it takes three things, the function and two arguments,
and it calls the function. Which is the function here?
k. And it calls them with two arguments. Which are the arguments?
And it calls them in the opposite order. So it's k of m and i, which is?
Yeah.
So we start with the kestrel, and then we put two things into it, but backwards.
That's interesting because look at what we have here.
The cardinal of the kestrel takes two things and returns the second.
Does that sound familiar?
It's the kite.
The cardinal of the kestrel is the kestrel of identity.
The kite is this other thing. They're all the same.
And we can just do this to prove it.
Let's do the cardinal takes a function.
This monitor went away. There we go.
It takes a function, takes an argument, another argument,
calls the argument with the arguments backwards.
Let's get the cardinal of the kestrel.
Apply it to two variables.
The kestrel normally gives you back the first thing,
but the cardinal of the kestrel gives you the second thing.
It works. This isn't pure math. It's also applicable.
So that's kind of fun. Cardinal, if you've got a cardinal,
you can apply it in different ways.
You can flip the kite around to get the kestrel or the kestrel to get the kite.
And in fact, in Haskell, this is built into the base language,
and it's called flip.
And it actually ends up being useful from time to time.
So why? Why are we learning this?
What's actually going on here?
How is this useful? Do we care?
Remember, lambda cacos and Turing machines are equivalent.
Anything one can calculate, the other can calculate.
But Turing machines are exciting because they're these hypothetical devices
that use state information that exists over here, here, here,
and things that change over time to do computations.
And from these hypothetical devices, people said,
wait a second, we could build real machines that do this.
And they use memory and state,
and they do a little bit more complicated stuff
just to make it more performant and easier to work with.
But at their heart, they're really Turing machines.
So they work with machine code,
which means let's flip a whole bunch of physical switches
and then see what electricity and bits come out the other end.
And we abstract that away in a language, a text format called assembly language,
that says things like, move the data in register B to the accumulator register.
Add one to whatever value is in the accumulator register of my memory.
Very stateful, very machine-based, very hard for humans to reason about.
It's not conceptual. It's all about machines.
Well, we build higher-level languages like C that compile into assembly.
But those higher-level languages are still machine-centric.
They still say things like, hey, see, go allocate seven bits of memory over here
and give me back a reference to the pointer of that memory address and so on and so forth.
So then somebody says, well, this is stupid.
We'll make the programming languages do that for us,
and we'll just focus more on concepts.
Like, hey, give me a var x,
and I don't care how you figure out the memory for that,
just go do it yourself.
And then somebody says, why are we even bothering with memory?
Why don't we just have these pure functions that operate on each other?
And wait a second.
This whole time this machine march through abstraction
has been leading us to something that existed before Turing machines existed,
which is the lambda calculus.
Functional programming languages are based on,
slash their backbone is, the lambda calculus.
So if we decide, wait, we're not going to organically evolve
towards this kind of conceptual abstraction,
but let's just start straight from the lambda calculus and go the other direction.
We can start designing purely functional languages
and using all the decades of mathematical research that have gone into LC
to design our language and see what comes out of that
and see if there's anything useful that we could do there.
And then we'll take that pure functional language,
just compile it down to machine code so it runs on our physical Turing machines.
Lambda calculus and Turing machines are equivalent.
Therefore, here's the big theme of the entire talk coming in the next slide.
Everything can be functions.
When I say everything, I mean everything.
Like Booleans.
Whoa.
Here's a JavaScript Boolean expression.
Not X is equal to, that's the double equals, not the assignment equals.
Y or the result of the expression A and Z.
How are we going to do this in lambda calculus?
Well, it's a problem because we don't have negation.
We don't have ors and and operators.
I mean, we don't even have a quality checks in the lambda calculus, right?
That wasn't in the language.
We don't have Booleans.
We do have parentheses.
That's what we got.
We got parens.
All right, so how on earth are we going to do this?
Let's start with the primitive building blocks, the Booleans.
A bool.
What is that used for in JavaScript?
Well, what about selection?
Some result is check a Boolean condition.
If it's true, we'll get the first expression.
If it's false, we'll get the second expression.
Let's start translating this over to lambda calculus.
Well, we got this ternary except, oops, this question mark and colon.
That's not in the lambda calculus syntax.
It's got to go. Bye-bye.
What are we left with?
This is a function application, right?
So bool must be.
What's the theme of the talk?
A function.
And, oh, well, what do we need here?
We need a function that if it's the quote unquote true function,
it selects the first expression.
And if it's the false function, it selects the second expression.
Wait a second. This sounds really familiar to me.
Where have we seen a pair of functions that select either the first or second things?
Yeah.
We already have Booleans.
We didn't even have to reinvent them.
They're already in the language that we've developed so far in this talk.
We'll just encode, in other words, represent Booleans as functions.
We'll say that the kestrel function, the constant function,
the first function is true, quote unquote, and the kite is false.
That's kind of neat.
Let's do that while we're looking at this slide and admiring it.
True is equal to the kestrel.
And false is equal to the kite.
Now, there's a little node trick that I'll use here,
because if I go console log out true, it tells me it's the kestrel,
which is true, but in both senses.
But it's also slightly annoying, like if I'm going to start doing Booleans.
So I'm going to do a little trick here.
I'm going to say t.inspect is a function that returns the string true slash kestrel.
And false.inspect is a function that returns false or the kite.
Now, if I log out t, I get t slash k.
And if I log out false, I get false slash kite.
So that'll be useful going forward in some of these demos.
That wasn't lambda calculus.
The other stuff was, but not the dot.inspect.
OK, so we have true and false.
But true and false on their own are kind of boring, right?
Yeah, so we can select between two things.
What about Boolean logic?
What about vacation?
Let's translate it.
Well, what doesn't belong?
One of these things is not like the others.
The negation, the exclamation mark isn't in the lambda calculus syntax.
What does it got to be instead?
What's the theme of the talk?
A function.
The not function.
The not function will take in a Boolean and it will select between two other Booleans.
If we give it not true, it selects false.
If we give it false, it selects true.
How can we implement the not function?
Well, wait a second.
We just talked about selecting between two things.
What kind of thing selects between two possibilities?
We have a function.
Yes, a function.
That's true.
But specifically, these Booleans that we're using, the kite and the kestrel,
themselves functions that choose between two possibilities.
Look at what I have here, this expression.
An unknown Boolean p.
It might be the kite.
It might be the kestrel.
I don't know which one.
If it's the kestrel or true, which one of those does it select?
The first one, false.
And if my unknown Boolean is the kite, which does it select?
The second one, true.
My unknown Boolean selects its own opposite in this expression.
So we'll turn this into a function and we'll call it not.
That's it.
Not just takes a Boolean and then tells the Boolean select your opposite.
I promise it works.
So not takes, let's say it takes a Boolean and then it calls the Boolean passing in false and true.
So if we say not true, that's false.
And not false is true.
I've not put a single JavaScript Boolean into any of this.
I'm doing negation.
This is like, this should be exciting.
All right.
So our church encodings, which is what these things are called for Booleans now includes
negation.
We saw how we got true and false.
But there's a more exciting way of doing this.
There's an even better way, in my opinion.
A cooler way.
Not true is false and not false is true.
But we said true and false, we're encoding those as the kestrel and the kite.
But if the kestrel and the kite, there's no named functions in lambda calculus.
This is just us writing down a shorthand so that we don't have to remember and read out all the lambdas.
But if we did replace them with their equivalent lambda expressions, that's what we're really looking at.
Now, we're saying the Not function will take at the top there a lambda that takes two arguments and gives you back the first,
and it gives you back a lambda that takes two arguments and gives you back the second.
OK?
And at the bottom, we're saying it takes, you know, two arguments and gives you back the second,
and the Not function will give you back a lambda that takes two arguments and gives you back the first.
Interesting.
So does anyone see, instead of not,
is there a function that we've already
seen that will result in this?
A function that accepts a binary function, a function that
takes two arguments, and it moves the arguments around?
It's the cardinal.
Yeah, the only other one we've seen.
The cardinal already does this behavior.
The cardinal is boolean not.
The flip of true is false, and the flip of false is true.
This monitor is really going to bug me.
There we go.
Stay there.
Don't move.
Let's try it.
Cardinal of true.
Now, unfortunately, look what I'm about to get here.
This isn't quite what I want, just this function.
Well, that's weird.
What if I use this function?
I'll apply it to two things, one and two.
I get back the second thing, which is what I want.
The flip of true is false, so I should get the second thing.
And the flip of false is true, so I should get back
the first thing, which I do.
The problem is, the problem is, and it's not really a problem,
the problem is that the cardinal generates a new function,
unlike my previous implementation of not, which
selects between existing false and true functions,
the cardinal generates a new function that behaves identically
to the kestrel or kite.
This kind of identity crisis is known
as intentional equality versus extensional equality.
Extensional equality, which is the kind of equality
I'll use throughout this talk, means the functions are the same.
If for every input, they generate the same output.
So the cardinal of the kestrel is
extensionally equal to the kite.
They both behave identically.
There's no way to tell them apart from the outside, quote unquote.
Intentional equality is more like, well, where did it come from
and what's inside it?
What are its guts?
I'm not going to focus on that during this talk, but it works.
That's the important thing.
Let's design and together, Boolean and, Boolean conjunction.
We know it's a function, right?
That's the theme of the talk.
How many arguments does it take?
What are these arguments?
What kind of thing are they?
They're Booleans, so they're the kestrel of the kite
or kite and kestrel or kestrel or kestrel or kite and kite,
just to finish it out.
Well, to take in a parameter, you're
probably going to use that parameter somewhere
in the body of your function, right?
So even if we're not quite sure where to go with this,
let's try just using one of these Booleans.
P is a Boolean, so what does it do?
What do these Boolean functions do when I use them?
Yeah, they select between two possibilities,
such as the Spanish question mark.
What if our first argument to and is false?
Which of the two possibilities will P select?
The second one.
But wait a second.
If one of the arguments to and is false,
what should that result of this entire function be?
Yeah, so I'll just put false there.
If P is false, short circuit, don't bother looking at Q.
There's no point where you can know we just select the second
thing, and it's already going to be false.
So we don't even bother checking Q. Well, what if P is true?
It's going to select the first thing, right?
But what is that first thing?
What should it be?
Based on Q?
Yeah, it's got to be Q.
Because if P is true, the and is true only when Q is true,
and the and is false if Q is false.
So once P is true, we have to go look at Q and use Q as our
result.
There's one more small simplification we can actually
make to this that's kind of nice.
I like it in any rate, which is we said that, by the way, P is
a Boolean, so it selects between two possibilities.
If P is false, it should select false.
But I've hard-coded in a false.
There was a way I could do this even more directly or
indirectly, it depends on your perspective.
If P is false and it should select false, P can just
select itself.
If P is false, then return P, which is false.
So I end up with this thing, which is very much a
combinator, PQ, PQP.
That is my and function.
I forget where we're at in our demonstrations of JavaScript.
Do we have not?
We do have not.
What about and?
No, we don't.
OK, let's do that.
And takes a Boolean and another Boolean, and it applies P
to Q to P. PQ, PQP.
That makes sense.
So and of false and true is false.
And of true and true is true.
And false is false.
And false and false is false.
All the things we love and expect from the and function.
That's neat.
Let's do or.
What is or?
Hey, someone's got the theme down.
I cheated ahead.
I jumped ahead and gave you two of the arguments.
Oh, no.
What are we going to do?
P is a Boolean, it selects between two things.
All right, here's where it differs from and.
What if P is true?
Oops, I jumped ahead too much.
P is true.
It selects the first thing, which has got to be true.
If P is true, we don't have to bother looking at Q, because
in or, if one of them is true, then we just result in true.
If P is false, what's the second argument got to be?
Q. It's just the opposite of the thing before.
So we can also simplify our kind of glossed over that, but
it doesn't have to be a hard-coded true.
It can just be P, because we reuse it.
If P is true, just return P.
That fact is actually really fun, because there's another
little thing we can do here that's a trick.
Somebody tell me, if I apply this PQ double PQ function to
X and Y as arguments, what is the resulting
better reduction?
In other words, what is the evaluation of this function?
What do I get as a result?
Remember, X replaces every P in the body, and Y replaces every
Q in the body.
Yep, X, X, Y.
But there's another function we've already seen that does
this.
I'll give you a hint.
What if we ignore the Ys for a second?
Yeah, it's the mockingverd.
It's the self-applicationverd.
The self-application of X is double X.
It's the self-application of X.
But if the thing on the left is equal to the thing on the
right, they're both functions I could apply them to some Y.
And now we see, wait a second, the thing on the top and the
thing on the bottom are actually the same.
The mockingverd works just like this other function.
It looks almost exactly like that, except the other
function is this additional Q on the end, which is useless.
It takes a Q and applies a Q.
This is known as the mockingverd once removed.
So that's what the star means.
It's been given an extra argument.
The mockingverd once removed is extensionally equal to the
mockingverd.
It behaves identically to it.
So have we defined or?
No, we haven't.
Let's do that really quickly.
Or it takes a P and a Q.
And it does P of P and Q.
We can demonstrate that or TF is true, or F is false, or
FT is true, and of course, or TT is true.
But we can also use the mockingverd for that, because
we just proved that the mockingverd outflurrates the
same way here.
So mockingverd of true and false is true.
False and false is false.
False and true is an or statement as well.
And that's also an or statement.
Wow.
Mockingverd, you're multi-talented.
Anybody have an idea what this might be?
Well, it's definitely a function that takes two
arguments.
And I'm going to tell you P and Q are Booleans, just to make
it slightly easier.
But if P and Q are Booleans, what does P do?
It selects between two possibilities.
And if Q is a Boolean, what does it do?
It selects between two possibilities.
Does this make it easier to see what this function does?
Sorry?
You were sure.
Well, if P and Q are the same Boolean, they select true.
If they're the same that way, they also select true.
But if they're different, they select false.
Which function is this?
Sorry?
Oh, is Zor something like that?
Not quite.
Maybe.
I'd have to think about it.
It might be Zor.
Yeah, I'd have to think about it.
Yeah.
That's true.
That's cool.
Yeah, nice work.
I didn't think about that.
There's a simpler thing that we use this with.
If P and Q are the same, we get true.
If they're different, we get false.
That's called equality.
This test, if P and Q are the same Boolean, well, there's a
nice little simplification we can make here.
Q is a Boolean true or false.
If it's true, it selects true.
If it's false, it selects false.
That's redundant.
We could just use Q as it is.
It's already true or false.
And at the bottom, if Q is true, it selects false.
And if it's false, it selects true.
We already have a function that does that.
It's called the not function.
So we can simplify this to P, Q, not Q. And that is our
Boolean equality function.
I'll do that as long as it sets P to Q to P, Q, not Q.
So Boolean equality takes a P and a Q.
And it does P of Q and not Q.
Try it out.
Boolean equality of true and true is, of course, true.
True and false, nope.
False and false, yes.
False and true, nope.
Nice.
It's always fun to see it actually work.
Like, you kind of believe it, and then you see it, and
you're like, oh, I guess it really is true.
All right, we got church encodings from Booleons.
I'm not going to do this one out.
But if you are familiar with Boolean logic, you have heard
of De Morgan's laws.
De Morgan's laws are a pair of laws.
This is only one of them.
But it says that not P and Q is equal to not P or not Q.
And we've just expressed that using nothing but functions.
No Booleans, no ands, no ors.
I could prove that.
We'll just take our shorthand and replace it with the
actual lambda calculus.
So there's our Boolean equality of not and PQ or not P or
not Q.
All right, it's pretty cool.
Now, I rehearsed this talk last night.
My fiance made me.
She said, this talk is way longer than an hour.
I was like, ah, we'll see how long it is.
She said, no, no, no, you're going to rehearse.
And I rehearsed, and it's way longer than an hour.
But this is a really good stopping point.
So what I'm going to do is I'm going to give you the
conclusion of this talk, which works perfectly after this
slide.
It fits.
It makes sense.
And anyone who would like to is welcome to stay another
30-ish minutes.
I'm going to go into another room, because Mark has to
come in here and get this room ready for demo day.
And I'll show them numbers in the lambda calculus.
But before we do that, I'm going to conclude this talk.
So let me skip way down to the bottom of my deck here.
If I can find my mouse, there it is.
Nope, further.
Nope, more than that.
Keep going.
Try to remember where this slide starts.
Almost there.
So a small preview.
This isn't even the only table in this talk.
These are just the combinators, let alone the
Boolean equality, the arithmetic, the numerals, and
the arithmetic operations that yield Booleans.
So lots of stuff that I am cutting out in the
interest of time.
But yeah, I know.
Here's where I want to conclude, and I want to give you a
couple little small addendums.
The first is I emphasized early on that from primitive
combinators come other combinators.
And this is a really cool sort of atom to molecule sort of
situation.
And it begs the question, how many combinators do we need
and which ones to generate all the other ones?
Is it even possible to do that?
Do we need an infinite number of them?
20, 10, 5?
Just two.
Not even identity.
Identity isn't on the board.
These are the only two you need.
You can make identity out of this, the
Starling and the Kestrel.
The Starling is a weird one.
I actually don't really like the SK
Combinator Calculus, which is what it is called.
I really like the BCKIM Combinator Calculus.
If I just added m to this, this would actually suffice
five of them.
And that was the one that Curry used.
I find this far easier to use than the SK
Combinator Calculus.
For instance, the identity in the SK
Combinator Calculus is S of KK.
It's also SKS.
Those are extensionally equal.
You didn't get a chance to see the Vario, but the Vario
is the world's smallest data structure.
That's right.
I'm putting data structures in lambda calculus.
Here it is in the SK Combinator Calculus.
And this is not even close to how complicated it can get.
So really, why?
What is what?
What?
All right.
To begin with, in my opinion, I was searching for the
answer to this why question.
I was trying to think, am I trying to evangelize like
learning abstract math or this and that?
Then I realized, you know what?
The honest answer for myself is, it's just fun.
I enjoy this.
I hope that you might enjoy it too.
That's the entire basis of the book to Maka Making Bird.
It's a book of logic puzzles and games.
It was written to act as a series of fun challenges.
It's a great mental workout.
Thinking along these patterns, let's you think about, oh
my gosh, I have to think of where nouns are verbs and
verbs are nouns.
And I've got to be able to think about partially applied
functions, curried functions, higher order functions.
There's a lot of spaghetti.
So it's really laying down the neurological groundwork for
understanding functional programming in general.
The lambda calculus, as I said before, being the basis of
languages like Miranda and Lisp and Haskell, means that
those languages, if you know this kind of
combinatorial logic, it sets you up for success in those
kinds of languages.
Because even though you're not required to think entirely
that way, there's a large portion of those languages
that assumes you are comfortable with that.
And from that, we get all sorts of real world practical
benefits.
So I started writing them down.
A lot of them are intersections with just functional
programming in general.
But many of these come directly from the lambda calculus.
Closures, higher order functions, laziness,
infinite data structures, garbage collection, function
graph reduction, type theory, provable programming code,
parallel processing for free, parametric polymorphism.
I mean, it just keeps going on and on and on.
And this all derives straight out of purely mathematical
fields that existed even before computers did.
But at the end of the day, I really just think that there's
a lot of elegant mathematical beauty to it.
And it's kind of art for art's sake.
I hope that this has inspired you to become interested in
this topic and maybe to go read some more about it.
Find a little bit of slides.
Here are all the combinators.
Here are all the booleans.
The numerals, church arithmetic, boolean ops in the
church arithmetic, data structures.
All of this is in the deck.
It's not an hour-long talk, is it?
And you've probably all been wondering, wait a second.
Where's the most famous combinator of all, the Y
combinator?
Well, there's the Y combinator.
What does it do?
I'll just leave you with this as a brain teaser.
The lambda calculus has neither loops nor recursion.
So how does it do either of those things?
Because it can calculate anything calculable.
This is the answer.
The Y combinator allows for recursion in a language that
doesn't have recursion built into it.
Unfortunately, I cannot demonstrate this in JavaScript
because it goes on forever.
This is a fixed point combinator.
It infinitely just keeps evaluating itself, which works
in a lazy language like Haskell or the lambda calculus
itself.
So unfortunately, because JavaScript is like the thing
on the right and not like the thing on the left,
we need a slight variation on the Y combinator
called the Z combinator, which is the exact same as the Y
combinator except the middle of it
has a funk, which defers calculation until required.
So the Z combinator I could demo if I wanted to,
but I haven't set that up in my code so I won't today.
All right, that is the talk.
Thank you very much.
Any questions?

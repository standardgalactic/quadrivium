So, as I think you know, I am sort of a dedicated AI researcher, but I'm also a bit of a philosopher
I like to think.
I'm an observer of human nature and human condition, as I think we all are.
And so I appreciate this chance to speak to you all and to sort of share my views about
about AI and about the field and all that's happening, all the growth, all the change,
all the excitement, all the hyperbole and all the fear and the positive and the negative
exaggeration.
And you know, it's sort of hard to get a coherent, consistent, reliable, stable view.
And I just wanted to talk about all these things today.
So today I'm not going to talk about whatever appeared on the schedule.
I'm going to sort of a placeholder.
So I'm going to talk about AI narratives.
Narrative just means a story.
And so the stories of AI, what we tell each other, what we tell ourselves and how to think
about them, which ones of them are accurate, which ones of them are maybe mistaken, how
they may change over time and how the future might work out.
Okay, but let me be clear at the outset that I'm really optimistic and I think the future
will be very, very bright.
I guess that could be a utopia, nothing like that.
Good things and bad things will still happen, but mostly good things.
And AI won't even really change that.
They'll still be good things and bad things.
It may change which good things happen, which bad things happen.
But overall, I think AI will make the world a better place.
More interesting, more prosperous, more dynamic, more challenging in a good way.
Okay, so I got a gizmo.
Let's start.
I want to start with the root of it all, the cause.
And the cause, I think, is Moore's Law, what we locally call Moore's Law, which is just
computation for the last, for the century.
For this century and the coming years, decades, computation has become cheaper and cheaper
and more and more plentiful.
And this is really the story of our age.
This is like the age of computation.
And so let me list this graph.
Yeah, let's look at this graph.
On the, across the bottom there, we've got years and plotted on the y-axis is the computational
power of computers available in that year.
Okay?
Now notice the years span over a century.
They go back to the beginning of 1900.
And the y-axis is a log scale.
So that means each tick up on the y-axis is a factor.
And the small ticks, well, the small, the ticks are two orders of magnitude.
So a factor of 100 for each mark there on the y-axis.
And this is the amazing thing that over this period of time, over roughly a century, computer
power has doubled roughly every two years.
Okay?
A little bit faster recently, maybe now it's like 18 months.
But it's been remarkably constant and it's remarkably consistent over all that time.
Like, can you see, for example, World War II anywhere in that graph?
I would say not at all.
There's no, not a blip because of World War II or World War I or any single technological
event.
It's amazingly consistent.
Over this time, it's over the 80 or 90 years since computers were invented, there's been
an increase of 10 to the 16th.
10 to the 16th?
10 to the 16th.
That's 100,000 million million.
Okay?
It's unimaginable what can happen cumulatively over this kind of time frame when you get
this consistent doubling.
Okay, it just keeps happening.
This is the story of our age.
This is truly the age of computation.
Okay, this is the root of it all, but let's think a little bit more, just a little bit
more analytically.
Is this AI or is this computation?
Or is computation AI?
How do we use these words?
And I think these two things are confounded in our minds and in our language.
Using lots of computation and being related to intelligence.
Now AI, AI as a name, it has cachet.
It's appealing.
And so we're drawn to it.
We want to use it.
We want to call whatever we're doing AI.
So I was in the drug store the other day and I was looking at electric toothbrushes and
what did I find?
But electric toothbrushes are now AI toothbrushes.
So it goes, yeah, you can find your AI toothbrush.
Right there at the top.
This is the way our language evolves.
This is not a new thing.
This has always, always been true.
I remember when I was in graduate school, we didn't even have computers.
We didn't have our own computer.
We had a terminal which connected to the big computer.
And the big thing, exciting thing, was to get an intelligent terminal.
Okay, and they looked like the last slide.
What did I get?
Come on.
There.
That's the intelligent terminal.
And it was intelligent just because it could do multiple fonts, okay, and it could do a
little bit of editing in the computer.
Those were an intelligent computer.
And now, less silly, real things in our day and today, we have AlphaFold.
AlphaFold.
AlphaFold is an amazing program from DeepMind.
And where they've figured out all the proteins in the known demand.
And they figured out the three-dimensional structures from the sequences.
It's really transformed biology.
It's been really super important and super exciting.
And it is exactly the use of computation to do something really, really hard.
Okay.
But I would say it is just, it's really computation.
I mean, they're using neural networks and essentially using machine learning, but they're
not like a person.
They're not like an intelligent system that has a goal.
So, but anyway, we call that AI.
And everyone's kind of good with that, that these massively complex and powerful new tools,
we call them AI.
And, you know, that may be a misnomer, but everyone's doing it.
And, you know, who am I?
I'm not the kind of guy who would like insist on using words differently from other people.
Thank you for that laugh.
Okay.
So anyway, I'm going to say controversial things, but I don't want to just start with a trivial
controversial thing.
Like how we use the words, let's go with the words, let's call this AI.
But let's make the distinction.
Let's make the distinction between these two kinds of AI.
Tool AI, where we're using massive computation, we're doing something hard, that people will
find hard.
But these tools have to be wielded by a person.
They are not autonomous.
They are not complete.
They are not fully powerful on themselves.
They're powerful in conjunction with the person.
And as opposed to agent AI, you know, if tool AI is creating crazy smart tools, then agent
AI is creating crazy smart artificial people, new people.
So these are not only really, you've seen my two dimensions.
One dimension is tool AI versus agent AI.
And this is how I'm going to try to add something to the conversation of how we think what's
our narrative for AI.
First, we're going to divide it into these two parts.
And then we're going to divide it into the positive and the negative stories and exaggerations
around those two parts.
I think it's really helpful.
And I think it's helpful to separate these two because we often do not.
We awfully do not.
We lump them together and that causes us to think in a sloppy way.
So that's maybe my contribution of this talk is to separate them.
So let's firm that up or make that more explicit.
You know, examples of tool AI would be like Alpha Fold and large language models.
They're not full agents, but they are useful tools.
The image generators like Dali and Majorny, they don't have goals.
They don't have an interaction.
They don't have a life really.
They do things and they're wielded by a person as tools.
And on the other side, on the other side, excuse me, we have agent AI or the Alpha goes
and the Alpha zeros.
All those are agent like deep Q learning.
The Atari players GT, Sophie, we heard about many of us heard about the other day, the
race cars.
Those are agents.
And even simple grid world systems can be agents.
So we can make this distinction.
Whether they have clear goals and whether they have experience, whether they have agency.
I don't know if you remember, many of you remember, well, like from the Alberta plan,
you know, we have a picture of the agent.
The agent has certain things.
It has a perception.
It has a reactive policy as a value function as a model of the world.
So all those things, they're not in the tools.
The tools are not about aspiring to create a whole agent.
Okay.
So I'm sure that I think this is a really useful distinction.
Maybe we can find edge cases where it's not so clear, but let's go with it.
Okay.
And so let's do, we'll spend a while doing tool AI.
Tool AI, you know, it has a whole side.
It has a positive and negative part.
So it has a part that's negative, that feels dangerous, that's threatening.
And, but it's less threatening.
You know, it's got things like fake news and deep fakes and election manipulation and loss
of jobs, loss of jobs will come from using these tools or rather changing jobs.
There are all these negative things that we're familiar with, really, we're familiar with
them because they happen every time we have new technology that disrupts old jobs.
And so, so this is, this is, this is negative.
It's threatening, but at the same time, it's kind of familiar.
It feels like it's not a big deal.
We've seen it happen many times before and we know, or at least always in the past,
there are more and better jobs were created to make up for the ones that are lost.
Okay.
But this is, this is, this is like a story that is slightly threatening, but is also
so familiar.
And so it's not, it's not super scary.
It's slightly scary.
You know, the scary stuff comes from the agent side.
Now, the next thing to say is, you know, can with, with this tool stuff, could we end up
with agent stuff?
You know, this is a story that people tell and particularly they tell it for large language
models.
They're getting so smart.
They understand things that they're going to become agents, that they're going to become
powerful and threatening in a way that's beyond the normal.
Okay.
And so I want to, I want to diss that idea a little bit.
And the best way is, is through humor.
Okay.
So I really like this cartoon.
I want to show you this cartoon.
To think this all began with letting autocomplete finish our sentences.
Okay.
So I love this because it's, it has both exaggerations in it.
It has the exaggeration that, oh, if we let, that autocomplete will end up being AI.
We'll end up being agent AI.
We'll end up being powerful things that will dominate us and, and they could dominate and
control us.
And then, so that's really, it's really positive AI.
It's an exaggeration of the abilities of large language models.
Okay.
That's a story that's around us.
And, and it's, it's a, it's an exaggeration of, it's a negative.
It's a fearing thing.
It's a negative exaggeration because it's, oh, if we, if they're able to be agents, then
they will, they will harm us.
They will control us.
They will be our overlords.
Okay.
So now, so this is, this is a cartoon.
It's not really an argument, but it's a good fun aside.
You know, I still do tend to dismiss the idea that we'll have a spontaneous evolution from
large language models into agents.
And mainly I dismiss it because I've spent so much of my life trying to make agents.
And I know it doesn't happen just by chance or by, by throwing in computation.
You have to really organize them and structure them and, and try to make them powerful agents
in order for that to happen.
You know, of course I could be wrong.
Now there's another reason why I doubt it.
Let me, let me just try to explain this.
This is another weak argument.
So, so just take it as sort of an interesting argument and it doesn't apply just to large
language models, which applies to other places we might have seen.
And the argument is that we see a tendency, a psychological tendency of people thinking
about complicated things to grab hold of one thing and then suggest that, well, maybe that's
the one thing and the only important thing.
Maybe I did that today.
Okay.
But I hope not.
I hope I said, this is a super important thing, but you're sure there are other important
things.
But the, the tendency that we sometimes see is that people grab onto one thing and say,
that's, that's the only thing.
So I would just give you some examples of this idea.
I would say this was what happened with, with neural networks and gradient descent.
It was gradient descent is a powerful idea.
And so many people sort of ran with it so hard and they tried to say that maybe that's
all we need.
Okay.
And then they leave, they would leave out, for example, I, the ideas of reinforcement
learning and control.
I think that everything can come out from gradient descent or that they think that everything,
other people got psyched on the idea of prediction as being a very important part of cognition
and, and I, we, I totally agree.
But then they would go too far and they say prediction is the only thing that all cognition
is just, just prediction.
And so this is sort of what we're seeing with the large language models.
It's been really good thing, a powerful thing.
It's known to be powerful.
So many people want then for it to be the only idea that that one idea would give everything.
And so I think that's attempting common flawed way of thinking.
So, and, and the large language models, the hype about large language models.
And let me be clear, large language models are great.
They're, they're, they're going to be extremely useful that they are overhyped.
They are the, the premier example of, of positive exaggeration of the capabilities of current AI systems,
in my opinion.
And I think, and I think they will disappoint a bit, but they will also be extremely useful.
And, and it's going to be good, but let's, let's be reasonable or try to be reasonable.
So, and again, weak argument, right?
I think it's an interesting argument, but it's not definitive.
Let me give you another weak argument.
I think these, these weak arguments help us gain perspective and think about our thoughts about the narratives.
So another weak argument, sociological kind of argument, is that it's well known.
Actually, it's sort of the whole phenomenon of this tremendous excitement around large language models.
It's very familiar to AI researchers, at least old ones like myself.
We remember something called ELISA, you know, and you can find it in Wikipedia.
Wikipedia will have, has the ELISA effect.
The ELISA effect is that if there are systems that generate words and, and appears to speak to us,
we will overinterpret them.
And so this, this came up in the 60s, where they had simple programs that would use language.
And when, when people, even smart people, physicists, scientists interact with them,
they would read a lot of agency and a lot of understanding into the systems.
It's, it's, it's an, it's an effect.
It's reasonable to cause an effect.
It's well known.
It's, it's, it's one, it's part.
It's part.
It's undoubtedly part of why large language models are so, so exciting and have such, such a possibility for hyperbole.
Okay.
So the ELISA effect.
So this is what ELISA looked like.
Remember, it's from the 60s.
And so it has a terminal kind of interface.
And I wanted to read this to you all, but I don't want to turn my back on you.
And I can't read that monitor.
It's too far away.
Maybe I'll, I can't step back.
I'll lose the microphone.
Okay.
Yeah, I like that.
Okay.
So this is an example of conversation with ELISA.
Just give you a sense of what, what, what this is or was.
ELISA was, was, is pretending to be a psychologist, a psychiatrist actually.
Yeah.
So ELISA would say to the person, to the human is something troubling you.
And this person said, men are all alike.
And ELISA, what are, what is it?
What is the connection?
Do you suppose?
And the person says, they're always bugging us about something or other.
And ELISA says, can you think of a specific example?
Okay.
So it seems like ELISA is really understanding what's going on.
But it also seems quite possible that this is a can thing and just something that it
says.
When someone says, like maybe we use the word always, maybe a safe thing to say is, you
know, can you think of a specific example?
Yeah.
The, the, the human says, well, my boyfriend made me come here.
And then ELISA says, is it important to you that your boyfriend made you come here?
You could again, is it understanding?
Or is it just parroting back the phrase that you gave?
It's a really simple program.
This is 1960s.
This is, this is, you know, 1960s.
That's 60 years.
That's a million, million orders, a factor of a million, million, million in computation.
This is a tiny computer.
Anyway, that's what ELISA was like.
And the ELISA effect, I think, is still with us.
It's easy for us to read too much intelligence and understanding into systems that talk.
Okay.
Those are my weak arguments.
Things, things that I think you should understand in, as context and background for these large
language models and how, how to understand and assess their significance and what will
happen with them going forward.
So now let's turn to agent AI.
Okay.
Agent AI.
And so here I want to remind you of a powerful truth, stepping back, a powerful truth that
a genuine understanding of intelligence would be a very big deal for thousands of years,
philosophers and ordinary people alike have wondered about and sought to understand human
intelligence.
Almost every great philosopher since Plato has, has devoted a major part of their work
to the philosophy of mind.
Here's some familiar memes from that, philosophers and their titles.
John Locke wrote an essay concerning human understanding, obviously about intelligence
and mind.
Emanuel Kant wrote the critique for a pure reason and of course, Ren√© Descartes famously
said, I think, therefore I am.
This is something that really, you know, we talk about it in AI and it's techie and it's
computers, but this, this challenge, the challenge of understanding intelligence, making agent
AI is really this old, old goal of all, of humanities and science.
So, after these, after the philosophers, they came more like scientists and we may recognize
some of their names.
And really, it's said people have always been fascinated by their inner workings.
So Gustav Fechner studies psychophysics with Ebbinghaus and then the psychologists and
learning theorists, animal learning theorists like Paddlov and Thorndike and Skinner and
Tolman and, you know, a different kind of science would, would be people like Jean Piaget
and Sigmund Freud and Carl Jung and Timothy Leary even.
They're all wondering how do our minds work and how can we make them work better?
This is a grand challenge, a great mystery and our interest, our interest in it is not
just narcissism.
I mean, it is kind of narcissism, focusing on themselves and how they work, but it's,
it's appropriate in the sense that intelligence is a, is a great powerful thing.
I think Kurzweil, Ray Kurzweil was not wrong when he said that intelligence is the most
powerful phenomenon in the universe, most powerful phenomenon in the universe, you know,
more powerful than what?
Supernovas, black holes, is that crazy or is that right?
I mean, yeah, black holes, supernova are pretty powerful, but intelligence over time,
if, if, you know, the supernova had maybe, maybe a billion years to develop, give intelligence
a billion years and see what it can do, maybe we'll end up moving the stars around even
more than supernova.
So understanding intelligence is a, is a, is a great challenge.
Understanding intelligence which change all of our lives in many, many ways, too many to,
to even predict.
To understand how to work, how, how, how, how intelligence works is the holy grail of
science and philosophy.
To achieve such an understanding would be perhaps the greatest scientific achievement of any
age.
I guess I have a slide on this.
And I guess I have a, a sort of a quiz to help us get started.
So could AI be such a powerful phenomenon?
I guess I've already told you that I would say yes.
It could be as powerful, the most powerful phenomenon in the universe.
It's some, it's, it's almost plausible.
Then I want you to ask, well, can tool AI be such a powerful phenomenon?
Can, and I would say no.
I would say the powerful phenomenon is the people that are wielding the tools and the
tools themselves are not a powerful, are not powerful.
And so here's the slide.
This is actually slide from, from my talk a year ago where I was trying to make this
point that, that it's such a long standing goal of mankind to understand how we think
and, and improve ourselves and so far as to use technology to create new beings or to
become new beings that are as, as intelligent and powerful as we, as we are now.
So notice, you know, it's being driven still by Moore's law.
It's happening roughly now.
Pursuing the prize is, is, is a great and glorious thing to do.
A shot at the prize is much more important than personal fortune or contribution to the
economy.
Even the economy, all the world's economy is small compared to this kind of transition
in the status of the world, the planet, this portion of the universe.
You know, how, how would we not want to be part of it?
So I'm giving you the positive story for why it's exciting and why this is something we,
we have to do, why it's the natural next step.
But I really want to phrase the question for you and not just, not just present my point
of view.
So this, what is this key question?
The key question basically, is it good?
Is it good?
And as I dwelled on last year, often we don't get to choose these things, but we can still
evaluate them.
We can still say whether it's good or bad.
And that's a very important part of the story or the narrative that we tell.
So let's focus on just that question.
Is creating new people, new people that are smarter than we are now, is this a grand and
glorious prize, natural next step in the human quest, you know, or is it a nightmare bringing
to the end of all we know and love?
You know, basically would it be good or bad?
And I just want you all to recognize that it's hard to assess such a thing.
It's very personal.
It's very subjective.
And I don't want to, I don't really want to answer it today.
I want to phrase the question, make it prominent on your board of things that are happening
in the world of AI, and think about it.
So you can tell I'm on the grand and glorious side.
And others I want to recognize would say, oh, look, that Richard, he's a AI scientist.
He's, he's lusting for the prize and the fame of the glory that he is so explicitly stating.
And I would say to those other people, they're just fearing change.
They're fearing they would lose control of the world, a control that they really don't
even have now.
They're, they're fearing, they've, they're talking themselves in an unwarranted fear.
And it's sort of messianic because if you think you're going to save the world, you may do
things that, that are drastic and, and evil.
So, so these are our, our narratives, the fearful narrative and the, yeah, so the fear,
the fearmonger narrative, I think I'm sad to say, I think it's, it's winning a bit.
I think it's the standard.
Everyone knows, oh AI, danger, potential dangers.
A bunch of potential good things, but we all know it's potential dangers.
Okay, so the first thing I'm trying to, sophistication I'm trying to add to that is to separate the
tool AI, the agent AI.
I'm not trying to say the agent AI isn't dangerous.
Maybe it's, it's the thing that is potentially more dangerous.
But I am trying to say, right now we have one narrative for the agent AI and then that's
that it's dangerous.
We have to control it.
We have to keep it from getting out of control.
And, and so we should have another narrative.
Oh, yeah.
So, so one way to, one way to see this is look at the standard narrative.
The standard in, in the field of AI trial, recognize standard thing to do is to solve the control
problem.
The control problem is how do you control the AIs?
You know, it's like a slave holder saying, how do you control your slaves?
They, they're getting more intelligent.
They're figuring stuff out.
How do I make sure that they're never out of control?
And the standard field doesn't ask, you know, if that slave slave and slave holder point
of view is appropriate.
It doesn't ask if, if people would use their slaves against each other.
If you have, if you, and thus that would be a problem.
It doesn't ask, yeah, whether this is moral.
Okay.
So also consider that the AI safety point of view tries to solve the alignment problem,
which is seizing control of all the agents in the world and ensuring that they're all,
that their goals are aligned with those of people.
And it doesn't ask, you know, which people, because people do have different goals and
it doesn't ask, you know, how is that going to be enforced and how kind of a world you'd
have if no one can make an AI that wasn't, that wasn't approved by, by something who
would do, who would do that.
And then wouldn't this kind of freeze things in place?
How could, how could our, our values evolve?
Because our values are not perfect.
The world of people has many flaws.
We need it to be challenged and to continue to evolve.
So the fact that we don't ask these kind of difficult questions, this, this is what,
this is my evidence really, that the fearmonger narrative is winning because we just,
we just stop at, at, at thinking we have to control it, we have to align it.
And we don't go on to see the, the obvious challenges to that way of thinking.
Okay.
So I'm going to reflect a little bit more.
Good.
So where does this fear come from?
Why are we so fearful?
Why are we kind of viscerally fearful of, of the potential of strong AI, agent AI?
And I think it's really in our DNA.
It's in our, it's in our instinctual history.
And I call it the, the fear of the other tribe.
So, you know, many times in humankind's history, we've, different people haven't come in contact with each other.
And sometimes it's peaceful, there's trade, intermarriage.
Sometimes it's violent.
I would say more often it's violent, different, different, really, genuinely different people.
And one group or the other dominates, dominates and kills and slaves.
This, this is in our genetic history, both as the dominator and the dominated.
These attitudes are these, these are still part of our attitudes towards others.
You know, I think now, particularly in Canada, we try to be more open-minded and, and embrace the differences.
What's different and good in others.
We try to be welcoming and not fearful, but the, the fear-monger narrative of AI builds on our instinctual fear of the other tribe, the other people that are strong.
And, and so we know from our human history that it's their major advantages if we can overcome that and work together and collaborate.
Okay, so now I want to paint a picture, a more positive picture.
Just sort of counterbalance the fearful narrative.
So the pause, the hopeful picture is that the AIs are not alien things to be controlled,
but they're our allies and our offspring, they are of us rather than against us.
And as such, we don't try to control them tightly out of fear, but rather we appreciate the differences.
We work with them and we try to align our society so that we all see it as beneficial to work together.
Including, and we include the possibility that they might teach us something fundamental.
Just as we ask, we expect that our children, as they grow up, might teach us something fundamental.
We don't just train our children, we also learn from them.
So we don't ask, in the hopeful narrative, we don't ask how can we control the AI's goals,
but we rather ask how can we arrange society so that they will want to work together even though there are different goals.
Notice that this is already how we do it with people.
People have many different goals, sometimes we say, oh, human values.
But really, everyone has different goals, right?
Your food is not my food, we have different stomachs, we have different families and different bank accounts.
So it's really not that our goals are the same.
It means our goals in some sense are symmetric, but they are, in a real sense, opposed.
And it's only because we've arranged society so that the outcome is good.
Now, some of you may know about reinforcement learning, we have a direct map of this.
So every agent has its own reward signal.
Those are its goals, they must be like its food and its pain.
And agents kind of would have normally different rewards.
But their values, their values are predictions of reward.
And the technical term value in reinforcement learning, it means, again, prediction of reward.
And in a hopeful narrative, the values become aligned.
So even though I might want my food, I would also like a principle that says we respect each other's property.
And we don't try to beat each other up or take things from each other.
And this is really the way our world works.
Our world works by we've set up laws and mores so that we produce a certain,
so that a certain kind of behavior is rational.
Everyone gets the most reward by behaving appropriately and leaving space for others.
Okay, another point to be aware of, it's in the fearful narrative, which is,
is there one AI or is there many?
The most obvious fearful narrative is that they will become one super AI
which will quickly become much more powerful than everything,
than all the people and all the other AIs and take over.
And then we'll have a singleton, a single agent that controls our world.
So this relies on the idea that it's fast.
If the takeoff is not fast, then it makes sense
to connect it to the last bit, we can't have peace if there's one strongman
that can dominate everyone else.
It's because no one is totally in control, everyone has to share power.
That's how you get peace and that's how you get aligned values in human societies.
And the same would be true amongst AIs.
And I can't avoid, I can't resist mentioning the idea of the complex adaptive system.
My view is that the world is not something that's in control.
It's something that is a complex adaptive system.
It's decentralized, many, many parts.
They have to share power with each other.
And this is where it gets much of its robustness and its ability to shift and be dynamic,
to change, to go one way or another, to find the way of being which is most successful in the universe.
So I think if we think, part of the hopeful narrative is to view the world as a complex adaptive system.
It would be very difficult to take it over.
And those who might try would actually end up poorly and therefore it will be irrational to try.
And a super intelligent rational agent will be seeking to work
with us and with the other AIs rather than trying to take over.
Okay, well I think that's about all I really want to offer.
Let's recognize that people's attitudes towards these questions will not change very rapidly.
It's enough just to bring up the questions, to realize that there is a standard narrative.
And it's not the only narrative.
It may be leading us to a bad place.
I urge each of you not to rush to join or to assume or to adopt,
or to assume it's natural and inevitable that there will be this fearful narrative.
So to summarize about the stories of AI, there's two dimensions.
Well first, it's this is the century of computation and then there are two driven by Moore's law
and the two dimensions of tool AI and agent AI, each one has positive and negative hype.
Each one has a risk reward profile.
The risk reward profile of tool AI is sort of familiar and manageable
and it's often conflated with the risk reward profile of agent AI which is of a higher variance.
So which narrative prevails?
Which meme becomes popular?
I think it's really important for the story of AI ultimately.
And who sets us up?
Well we do, you all do.
We set the narratives, we spread the memes, it's something we should feel responsible for
and really this is a really important part of the story of AI, the final story of AI.
Okay, one more thing.
One more thing which is I want to tell you about my personal story.
As you know the Edmonton office of Deep Mind where I worked for many years, it was closed down this January
and that was a blow.
I think it was worse actually for Deep Mind than it was for the Albertans like myself.
All of us have gone on to other excellent opportunities, mostly still in Edmonton.
There are new startups, some have gone back to the university.
A large continued has joined Sony AI, Peter Stone told us about yesterday in Sony AI in Edmonton.
And the Sony AI in Edmonton is the largest piece of Sony AI in the world.
Arguably it may be now the biggest AI reinforcement learning research effort in the world.
But today I want to announce my own plans even though they're preliminary.
So I want to announce something we call Open Mind Research.
Open Mind Research is a non-profit organization dedicated, focused on implementing,
executing, developing the Alberta plan for AI research that I told some of you about yesterday
or on Wednesday and that has written up.
Anyway, that's our clear, focused research plan.
Open, oh so, yeah, it's a network and I will be staying in Edmonton to develop it.
And I hope you will clap for that.
Also in Open Mind Research is Joseph Modial and Melanie Marvin.
I think Joseph is here.
Yeah, good.
It's an interesting world and how to structure ourselves within it,
whether to make a startup company and try to make an infinite amount of money from the rise of AI.
But we think it's more important to work on the research.
So we're structuring the organization as a network of researchers.
We're going to be funded by donors.
We will not have intellectual property.
We'll be totally open.
Our main adjectives are open, focused, and lean.
We want something that will be able to last a while so we can figure out the key things and bring AI forward over the next decade.
So Open Mind Research will be a foundation supported by donors.
And we are now looking for donors, particularly two donors.
And one we'd like to be from Alberta.
If you know of anyone or if you might be interested in yourself, please ask them to contact me.
And I'm just really excited to be laser focused on the prize of achieving agent AI as I suggested here.
Thank you very much for your attention.
And I'm happy to, I'm excited actually to get questions or comments on the narratives of AI.
Just raise your hand and you'll get a mic.
Yeah, raise your hand and the mic will come to you.
Hey Rich, fantastic talk and thanks for the shout outs.
Thanks Peter.
So I completely agree with you that there's these two potential focuses, FOSI I suppose, for the AI narrative.
And the AI agent one sort of captures the imagination.
But some would argue is still far in the future and we don't yet know what it will look like.
The AI tool one is clearly here and you seem to say just don't worry about it, it'll be fine.
And because it always has been and I agree with you, I'm on your side here.
I think AI tools will make the world a better place.
Let me pause you.
Thank you for saying that because I really don't want that to be the message.
Both are really important and exciting and large language models, tool AI, alpha fold, these are all great things.
They have transformed biology, they've transformed writing and programming.
And I don't really want to say in any way that they're bad.
They're all good, just a little bit hyped and they may disappoint.
I don't want that to be a negative thing.
Let's acknowledge that they're probably going to disappoint a little bit because there's so much of being claimed for them.
And let's not let the disappointment that happens kill them.
It's not going to kill alpha fold and it shouldn't kill large language models either.
These are great things, they're a great part of the broad community of AI.
And yeah, I am more interested in one, but that's fine.
They're both great.
They really are, I really believe that.
So I hear you saying that and that's what I thought you were saying too.
I guess my comment is more the, and maybe you just answered the way you would want to answer what I'm asking,
but it seems like, I'm asking for your help answering the questions of people who are saying,
oh, it's these AI tools that are going to be bad.
Some would say that you were sort of glib in your dismissal of that as it's, oh, it's going to be like it was in the past.
And I get that.
People say that to me.
I say, oh, it's usually been good.
Most technologies have good and bad.
They've generally been on the good side.
But then people push back.
Maybe this time is going to be different.
Do you have like sort of, you know, can you help us construct deeper arguments against those?
The naysayers who are naysayers because of the AI tools.
So very, very good.
The key here, I intend to be answered by the separation into tool AI and agent AI.
I think tool AI, it will, it really will be as challenging as in the past and no more.
This time it will not be different as far as tool AI.
But the part where it's realistic to say this time it will be different is when we talk about agent AI.
It's when we're really talking about replacing people and capturing the capabilities of people.
It's on the agent AI that things may be different.
So to push just a little back.
So the Model T car was invented in 1908 and it took 50 years roughly to get to 100 million cars in the world.
And during that time we figured out things like seatbelts and airbags and traffic signals and highway networks and insurance and regulation and all this kind of stuff.
ChatGPT got to 100 million users in one month and there wasn't that time.
So how is it that this is, why should we say it's just going to be the same?
It seems faster and possibly, I'm being devil's advocate here a little bit, but it seems possibly different.
Why can you just say, how can you be confident that it's not different this time?
It's a bit broad brush and it is like super important.
But the other, the agent AI is even more important.
I mean it goes back thousands, millions of years into our history and our intellectual traditions.
Yeah, to me it's a qualitative difference.
And so I don't, it's not to dismiss it, it's not to say a normal amount needs to be done.
Yeah, I just don't see why it's going to be different.
As long as you stay on the tool side, people are still in control, people are still wielding the tools.
That's the normal technological change in disruption.
Okay, we'll let someone else.
Thank you for your great talk.
My question is about the government.
I know most of the fear that we have comes from government because it fears that it's going to lose the power and the central power of the state.
So do you agree that if we compare the AI for atomic bomb?
I don't know if you agree with that or not.
Do you agree that we should have a government based organization to control the AI or not?
And the second question is about the research.
For a young research like me, what is the best path do you think for us to focus on based on the agent AI?
Thank you.
Well, the latter part, of course, you know, I've got the Alberta plan.
It lays out a research plan.
But I'm glad you brought up the issue of government because I think the fearmongers are trying to gain control.
It's a usual strategy of all, not only governments, but particularly governments, to create fear and use it to grab control.
And I think, and so if people are scared enough, they will look for safety and they will hand the power to something.
And I think this is exactly the opposite of what we want to do.
We do not want a centralized place that has the power.
And if we are worried about AI, we say we got to regulate it.
Maybe they can only have certain goals.
The government is going to control those goals.
This is exactly the opposite.
You've centralized the power and it will make it easier for it to be seized and to have the bad outcome.
This is the opposite.
This is why there's one reason why it's important to have a counter to the negative, fearful narrative is because it might drive us.
It is drive.
You see it every day, slot in the Congressional hearings recently with open AI.
There is a strong push to give more power to centralized organizations.
If we want to have the world to be a complex adaptive system where it's robust and fluid because no one is controlled.
And let me just say, I think it's clear that is the situation now.
There is no one really in control.
There are many countries, first of all, and even the individual countries.
They don't have total control.
They have to trade off the powers and they have to trade off the power to the economy.
It's a complex adaptive system.
And this is what I hope will save us and that we want to resist the call of those who are fearful to centralize power in particular institutions.
Hi, Rich.
My name is Prishan.
I come from the industry, so I'm not an AI researcher by any means, but a very intriguing talk.
So I have more of a philosophical question on your idea around agent AI and the fact of superior intelligence somewhere down in the future, more intelligent than us as human beings.
So my question more is, does that really mean fundamentally that we as intelligent beings can conceive something more intelligent than us when we have yet to see the limits of our own intelligence?
I mean, you're building something assuming it could be more intelligent than you when we have yet to understand our own mind as human beings and as what we're doing with the universe.
So just curious, right?
A system trying to ascertain or create its own copy when itself it doesn't understand who or what he is.
So just curious about that.
Well, my own thought is that the path to creating superior intelligence runs through understanding our own intelligence.
There are at least major parts of it, much more than we understand now.
And I should say, I try to be always careful that we say our goal is to create beings that are smarter than us.
So I always say our goal is to create or become beings that are smarter than we are now.
What we think is a very good chance that the focus will be on making ourselves smarter and more capable using, not only using, but not only using the tools.
It's one problem to understand the mind, to understand ourselves.
Yeah, that might be wrong.
Maybe we'll find a way to just make AI without understanding it.
You remember the classic quote from Richard Feynman.
That which I cannot create, I don't understand.
So creation is required.
But the other way is also interesting.
Can I create it without understanding it?
In some sense we do, right?
We make children, okay?
We don't understand how they work, but we can make them.
We have a Xerox machine, too.
We can make pictures without understanding how they're created.
I guess we can do it now with AI.
Cool.
Any other thoughts?
Thanks again for your awesome talk and your information.
With your open mind venture, it appears that you prefer an open source idea.
Would you prefer a fully open source regulation system for AI instead?
I'm against centralized organizations.
I'm against the World Economic Forum and all those other ones.
So I wouldn't want to create another one.
I like sharing.
I like being open, like open source.
I think it's essential if you're going to do fundamental research that will impact over a longer period of time, over five years even.
You have to share your ideas just to shape your ideas.
To make your ideas better, you've got to have them be challenged by others.
You can't keep them secret.
You have to publish.
It's a bit of a problem that the corporations are becoming a bit more closed and less likely to publish now.
Intellectual property, without having a real...
I won't claim deep understanding, but I will claim...
I have a strong reaction that intellectual property is always a waste of time and always counterproductive.
It's costful, it slows things down, and it doesn't really protect in a good way.
I'm not interested in having intellectual property at all.
Thank you for your question.
Hi, at the back.
I really appreciate you tying human behavior and evolution to this process as a tool,
but man still controls the lever on that and our human behavior will drive us in different directions.
I think Hobbes, father of political science, said man moves towards convenience,
so we move out of the field into a cave, wrap some fur around you, start a fire, and then get people to work for you.
So where does that leave us if we have the tools to do all the things that need to get done?
Well, the fur and the cave, those are the early tools.
We've always made tools, and the tool AI is a natural continuation of that.
And the agent AI is a natural continuation of our trying to understand ourselves more deeply and being open to change.
I think it's consistent.
Yeah, I want you to feel both things.
I want you to feel that AI is bringing dramatic change,
and at the same time it's continuing trends and forces that have been present forever.
It's a natural next step.
Thank you for that question.
I think we're going to have to tie it off there.

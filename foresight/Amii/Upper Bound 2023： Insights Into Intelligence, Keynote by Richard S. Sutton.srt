1
00:00:00,000 --> 00:00:14,040
So, as I think you know, I am sort of a dedicated AI researcher, but I'm also a bit of a philosopher

2
00:00:14,040 --> 00:00:15,040
I like to think.

3
00:00:15,040 --> 00:00:20,920
I'm an observer of human nature and human condition, as I think we all are.

4
00:00:20,920 --> 00:00:26,880
And so I appreciate this chance to speak to you all and to sort of share my views about

5
00:00:27,880 --> 00:00:33,880
about AI and about the field and all that's happening, all the growth, all the change,

6
00:00:33,880 --> 00:00:41,880
all the excitement, all the hyperbole and all the fear and the positive and the negative

7
00:00:41,880 --> 00:00:43,880
exaggeration.

8
00:00:43,880 --> 00:00:50,880
And you know, it's sort of hard to get a coherent, consistent, reliable, stable view.

9
00:00:50,880 --> 00:00:52,880
And I just wanted to talk about all these things today.

10
00:00:52,880 --> 00:00:57,880
So today I'm not going to talk about whatever appeared on the schedule.

11
00:00:57,880 --> 00:01:01,880
I'm going to sort of a placeholder.

12
00:01:01,880 --> 00:01:05,880
So I'm going to talk about AI narratives.

13
00:01:05,880 --> 00:01:06,880
Narrative just means a story.

14
00:01:06,880 --> 00:01:12,880
And so the stories of AI, what we tell each other, what we tell ourselves and how to think

15
00:01:12,880 --> 00:01:16,880
about them, which ones of them are accurate, which ones of them are maybe mistaken, how

16
00:01:16,880 --> 00:01:20,880
they may change over time and how the future might work out.

17
00:01:20,880 --> 00:01:26,880
Okay, but let me be clear at the outset that I'm really optimistic and I think the future

18
00:01:26,880 --> 00:01:27,880
will be very, very bright.

19
00:01:27,880 --> 00:01:31,880
I guess that could be a utopia, nothing like that.

20
00:01:31,880 --> 00:01:36,880
Good things and bad things will still happen, but mostly good things.

21
00:01:36,880 --> 00:01:40,880
And AI won't even really change that.

22
00:01:40,880 --> 00:01:42,880
They'll still be good things and bad things.

23
00:01:42,880 --> 00:01:45,880
It may change which good things happen, which bad things happen.

24
00:01:45,880 --> 00:01:49,880
But overall, I think AI will make the world a better place.

25
00:01:49,880 --> 00:01:57,880
More interesting, more prosperous, more dynamic, more challenging in a good way.

26
00:01:57,880 --> 00:02:02,880
Okay, so I got a gizmo.

27
00:02:02,880 --> 00:02:03,880
Let's start.

28
00:02:03,880 --> 00:02:07,880
I want to start with the root of it all, the cause.

29
00:02:07,880 --> 00:02:15,880
And the cause, I think, is Moore's Law, what we locally call Moore's Law, which is just

30
00:02:15,880 --> 00:02:19,880
computation for the last, for the century.

31
00:02:19,880 --> 00:02:27,880
For this century and the coming years, decades, computation has become cheaper and cheaper

32
00:02:27,880 --> 00:02:28,880
and more and more plentiful.

33
00:02:28,880 --> 00:02:30,880
And this is really the story of our age.

34
00:02:30,880 --> 00:02:35,880
This is like the age of computation.

35
00:02:35,880 --> 00:02:40,880
And so let me list this graph.

36
00:02:40,880 --> 00:02:43,880
Yeah, let's look at this graph.

37
00:02:43,880 --> 00:02:51,880
On the, across the bottom there, we've got years and plotted on the y-axis is the computational

38
00:02:51,880 --> 00:02:54,880
power of computers available in that year.

39
00:02:54,880 --> 00:02:55,880
Okay?

40
00:02:55,880 --> 00:02:58,880
Now notice the years span over a century.

41
00:02:58,880 --> 00:03:02,880
They go back to the beginning of 1900.

42
00:03:02,880 --> 00:03:07,880
And the y-axis is a log scale.

43
00:03:07,880 --> 00:03:12,880
So that means each tick up on the y-axis is a factor.

44
00:03:12,880 --> 00:03:19,880
And the small ticks, well, the small, the ticks are two orders of magnitude.

45
00:03:19,880 --> 00:03:23,880
So a factor of 100 for each mark there on the y-axis.

46
00:03:23,880 --> 00:03:30,880
And this is the amazing thing that over this period of time, over roughly a century, computer

47
00:03:30,880 --> 00:03:34,880
power has doubled roughly every two years.

48
00:03:34,880 --> 00:03:35,880
Okay?

49
00:03:35,880 --> 00:03:38,880
A little bit faster recently, maybe now it's like 18 months.

50
00:03:38,880 --> 00:03:45,880
But it's been remarkably constant and it's remarkably consistent over all that time.

51
00:03:45,880 --> 00:03:52,880
Like, can you see, for example, World War II anywhere in that graph?

52
00:03:52,880 --> 00:03:54,880
I would say not at all.

53
00:03:54,880 --> 00:04:03,880
There's no, not a blip because of World War II or World War I or any single technological

54
00:04:03,880 --> 00:04:05,880
event.

55
00:04:05,880 --> 00:04:07,880
It's amazingly consistent.

56
00:04:07,880 --> 00:04:15,880
Over this time, it's over the 80 or 90 years since computers were invented, there's been

57
00:04:15,880 --> 00:04:18,880
an increase of 10 to the 16th.

58
00:04:18,880 --> 00:04:21,880
10 to the 16th?

59
00:04:21,880 --> 00:04:23,880
10 to the 16th.

60
00:04:23,880 --> 00:04:29,880
That's 100,000 million million.

61
00:04:29,880 --> 00:04:30,880
Okay?

62
00:04:30,880 --> 00:04:36,880
It's unimaginable what can happen cumulatively over this kind of time frame when you get

63
00:04:36,880 --> 00:04:38,880
this consistent doubling.

64
00:04:38,880 --> 00:04:41,880
Okay, it just keeps happening.

65
00:04:41,880 --> 00:04:43,880
This is the story of our age.

66
00:04:43,880 --> 00:04:51,880
This is truly the age of computation.

67
00:04:51,880 --> 00:04:55,880
Okay, this is the root of it all, but let's think a little bit more, just a little bit

68
00:04:55,880 --> 00:04:57,880
more analytically.

69
00:04:57,880 --> 00:05:02,880
Is this AI or is this computation?

70
00:05:02,880 --> 00:05:04,880
Or is computation AI?

71
00:05:04,880 --> 00:05:07,880
How do we use these words?

72
00:05:07,880 --> 00:05:13,880
And I think these two things are confounded in our minds and in our language.

73
00:05:13,880 --> 00:05:20,880
Using lots of computation and being related to intelligence.

74
00:05:20,880 --> 00:05:25,880
Now AI, AI as a name, it has cachet.

75
00:05:25,880 --> 00:05:27,880
It's appealing.

76
00:05:27,880 --> 00:05:29,880
And so we're drawn to it.

77
00:05:29,880 --> 00:05:30,880
We want to use it.

78
00:05:30,880 --> 00:05:33,880
We want to call whatever we're doing AI.

79
00:05:33,880 --> 00:05:39,880
So I was in the drug store the other day and I was looking at electric toothbrushes and

80
00:05:39,880 --> 00:05:40,880
what did I find?

81
00:05:40,880 --> 00:05:44,880
But electric toothbrushes are now AI toothbrushes.

82
00:05:44,880 --> 00:05:49,880
So it goes, yeah, you can find your AI toothbrush.

83
00:05:49,880 --> 00:05:54,880
Right there at the top.

84
00:05:54,880 --> 00:05:57,880
This is the way our language evolves.

85
00:05:57,880 --> 00:05:58,880
This is not a new thing.

86
00:05:58,880 --> 00:06:00,880
This has always, always been true.

87
00:06:00,880 --> 00:06:04,880
I remember when I was in graduate school, we didn't even have computers.

88
00:06:04,880 --> 00:06:05,880
We didn't have our own computer.

89
00:06:05,880 --> 00:06:08,880
We had a terminal which connected to the big computer.

90
00:06:08,880 --> 00:06:13,880
And the big thing, exciting thing, was to get an intelligent terminal.

91
00:06:13,880 --> 00:06:17,880
Okay, and they looked like the last slide.

92
00:06:17,880 --> 00:06:19,880
What did I get?

93
00:06:19,880 --> 00:06:21,880
Come on.

94
00:06:21,880 --> 00:06:22,880
There.

95
00:06:22,880 --> 00:06:23,880
That's the intelligent terminal.

96
00:06:23,880 --> 00:06:29,880
And it was intelligent just because it could do multiple fonts, okay, and it could do a

97
00:06:29,880 --> 00:06:32,880
little bit of editing in the computer.

98
00:06:32,880 --> 00:06:34,880
Those were an intelligent computer.

99
00:06:34,880 --> 00:06:42,880
And now, less silly, real things in our day and today, we have AlphaFold.

100
00:06:42,880 --> 00:06:46,880
AlphaFold.

101
00:06:46,880 --> 00:06:51,880
AlphaFold is an amazing program from DeepMind.

102
00:06:51,880 --> 00:06:55,880
And where they've figured out all the proteins in the known demand.

103
00:06:55,880 --> 00:06:58,880
And they figured out the three-dimensional structures from the sequences.

104
00:06:58,880 --> 00:07:00,880
It's really transformed biology.

105
00:07:00,880 --> 00:07:02,880
It's been really super important and super exciting.

106
00:07:02,880 --> 00:07:07,880
And it is exactly the use of computation to do something really, really hard.

107
00:07:07,880 --> 00:07:09,880
Okay.

108
00:07:09,880 --> 00:07:12,880
But I would say it is just, it's really computation.

109
00:07:12,880 --> 00:07:17,880
I mean, they're using neural networks and essentially using machine learning, but they're

110
00:07:17,880 --> 00:07:19,880
not like a person.

111
00:07:19,880 --> 00:07:24,880
They're not like an intelligent system that has a goal.

112
00:07:24,880 --> 00:07:27,880
So, but anyway, we call that AI.

113
00:07:27,880 --> 00:07:33,880
And everyone's kind of good with that, that these massively complex and powerful new tools,

114
00:07:33,880 --> 00:07:35,880
we call them AI.

115
00:07:35,880 --> 00:07:39,880
And, you know, that may be a misnomer, but everyone's doing it.

116
00:07:39,880 --> 00:07:42,880
And, you know, who am I?

117
00:07:42,880 --> 00:07:50,880
I'm not the kind of guy who would like insist on using words differently from other people.

118
00:07:50,880 --> 00:07:55,880
Thank you for that laugh.

119
00:07:55,880 --> 00:07:59,880
Okay.

120
00:07:59,880 --> 00:08:04,880
So anyway, I'm going to say controversial things, but I don't want to just start with a trivial

121
00:08:04,880 --> 00:08:06,880
controversial thing.

122
00:08:06,880 --> 00:08:11,880
Like how we use the words, let's go with the words, let's call this AI.

123
00:08:11,880 --> 00:08:13,880
But let's make the distinction.

124
00:08:13,880 --> 00:08:19,880
Let's make the distinction between these two kinds of AI.

125
00:08:19,880 --> 00:08:25,880
Tool AI, where we're using massive computation, we're doing something hard, that people will

126
00:08:25,880 --> 00:08:29,880
find hard.

127
00:08:29,880 --> 00:08:34,880
But these tools have to be wielded by a person.

128
00:08:34,880 --> 00:08:35,880
They are not autonomous.

129
00:08:35,880 --> 00:08:36,880
They are not complete.

130
00:08:36,880 --> 00:08:38,880
They are not fully powerful on themselves.

131
00:08:38,880 --> 00:08:43,880
They're powerful in conjunction with the person.

132
00:08:43,880 --> 00:08:51,880
And as opposed to agent AI, you know, if tool AI is creating crazy smart tools, then agent

133
00:08:51,880 --> 00:08:58,880
AI is creating crazy smart artificial people, new people.

134
00:08:59,880 --> 00:09:04,880
So these are not only really, you've seen my two dimensions.

135
00:09:04,880 --> 00:09:07,880
One dimension is tool AI versus agent AI.

136
00:09:07,880 --> 00:09:12,880
And this is how I'm going to try to add something to the conversation of how we think what's

137
00:09:12,880 --> 00:09:13,880
our narrative for AI.

138
00:09:13,880 --> 00:09:16,880
First, we're going to divide it into these two parts.

139
00:09:16,880 --> 00:09:22,880
And then we're going to divide it into the positive and the negative stories and exaggerations

140
00:09:22,880 --> 00:09:24,880
around those two parts.

141
00:09:24,880 --> 00:09:25,880
I think it's really helpful.

142
00:09:25,880 --> 00:09:30,880
And I think it's helpful to separate these two because we often do not.

143
00:09:30,880 --> 00:09:31,880
We awfully do not.

144
00:09:31,880 --> 00:09:34,880
We lump them together and that causes us to think in a sloppy way.

145
00:09:34,880 --> 00:09:40,880
So that's maybe my contribution of this talk is to separate them.

146
00:09:40,880 --> 00:09:45,880
So let's firm that up or make that more explicit.

147
00:09:45,880 --> 00:09:50,880
You know, examples of tool AI would be like Alpha Fold and large language models.

148
00:09:50,880 --> 00:09:54,880
They're not full agents, but they are useful tools.

149
00:09:54,880 --> 00:09:58,880
The image generators like Dali and Majorny, they don't have goals.

150
00:09:58,880 --> 00:09:59,880
They don't have an interaction.

151
00:09:59,880 --> 00:10:00,880
They don't have a life really.

152
00:10:00,880 --> 00:10:06,880
They do things and they're wielded by a person as tools.

153
00:10:06,880 --> 00:10:12,880
And on the other side, on the other side, excuse me, we have agent AI or the Alpha goes

154
00:10:12,880 --> 00:10:13,880
and the Alpha zeros.

155
00:10:13,880 --> 00:10:18,880
All those are agent like deep Q learning.

156
00:10:18,880 --> 00:10:24,880
The Atari players GT, Sophie, we heard about many of us heard about the other day, the

157
00:10:24,880 --> 00:10:25,880
race cars.

158
00:10:25,880 --> 00:10:26,880
Those are agents.

159
00:10:26,880 --> 00:10:30,880
And even simple grid world systems can be agents.

160
00:10:30,880 --> 00:10:33,880
So we can make this distinction.

161
00:10:33,880 --> 00:10:37,880
Whether they have clear goals and whether they have experience, whether they have agency.

162
00:10:37,880 --> 00:10:42,880
I don't know if you remember, many of you remember, well, like from the Alberta plan,

163
00:10:42,880 --> 00:10:44,880
you know, we have a picture of the agent.

164
00:10:44,880 --> 00:10:45,880
The agent has certain things.

165
00:10:45,880 --> 00:10:47,880
It has a perception.

166
00:10:47,880 --> 00:10:51,880
It has a reactive policy as a value function as a model of the world.

167
00:10:51,880 --> 00:10:54,880
So all those things, they're not in the tools.

168
00:10:54,880 --> 00:11:01,880
The tools are not about aspiring to create a whole agent.

169
00:11:01,880 --> 00:11:02,880
Okay.

170
00:11:02,880 --> 00:11:06,880
So I'm sure that I think this is a really useful distinction.

171
00:11:06,880 --> 00:11:11,880
Maybe we can find edge cases where it's not so clear, but let's go with it.

172
00:11:11,880 --> 00:11:12,880
Okay.

173
00:11:12,880 --> 00:11:19,880
And so let's do, we'll spend a while doing tool AI.

174
00:11:19,880 --> 00:11:22,880
Tool AI, you know, it has a whole side.

175
00:11:22,880 --> 00:11:24,880
It has a positive and negative part.

176
00:11:24,880 --> 00:11:32,880
So it has a part that's negative, that feels dangerous, that's threatening.

177
00:11:32,880 --> 00:11:37,880
And, but it's less threatening.

178
00:11:37,880 --> 00:11:43,880
You know, it's got things like fake news and deep fakes and election manipulation and loss

179
00:11:43,880 --> 00:11:49,880
of jobs, loss of jobs will come from using these tools or rather changing jobs.

180
00:11:49,880 --> 00:11:53,880
There are all these negative things that we're familiar with, really, we're familiar with

181
00:11:53,880 --> 00:11:58,880
them because they happen every time we have new technology that disrupts old jobs.

182
00:11:58,880 --> 00:12:02,880
And so, so this is, this is, this is negative.

183
00:12:02,880 --> 00:12:05,880
It's threatening, but at the same time, it's kind of familiar.

184
00:12:05,880 --> 00:12:10,880
It feels like it's not a big deal.

185
00:12:10,880 --> 00:12:15,880
We've seen it happen many times before and we know, or at least always in the past,

186
00:12:15,880 --> 00:12:20,880
there are more and better jobs were created to make up for the ones that are lost.

187
00:12:20,880 --> 00:12:21,880
Okay.

188
00:12:21,880 --> 00:12:29,880
But this is, this is, this is like a story that is slightly threatening, but is also

189
00:12:29,880 --> 00:12:30,880
so familiar.

190
00:12:30,880 --> 00:12:33,880
And so it's not, it's not super scary.

191
00:12:33,880 --> 00:12:36,880
It's slightly scary.

192
00:12:36,880 --> 00:12:47,880
You know, the scary stuff comes from the agent side.

193
00:12:47,880 --> 00:12:55,880
Now, the next thing to say is, you know, can with, with this tool stuff, could we end up

194
00:12:55,880 --> 00:12:57,880
with agent stuff?

195
00:12:57,880 --> 00:13:01,880
You know, this is a story that people tell and particularly they tell it for large language

196
00:13:01,880 --> 00:13:02,880
models.

197
00:13:02,880 --> 00:13:03,880
They're getting so smart.

198
00:13:03,880 --> 00:13:07,880
They understand things that they're going to become agents, that they're going to become

199
00:13:07,880 --> 00:13:10,880
powerful and threatening in a way that's beyond the normal.

200
00:13:10,880 --> 00:13:11,880
Okay.

201
00:13:11,880 --> 00:13:16,880
And so I want to, I want to diss that idea a little bit.

202
00:13:16,880 --> 00:13:19,880
And the best way is, is through humor.

203
00:13:19,880 --> 00:13:20,880
Okay.

204
00:13:20,880 --> 00:13:21,880
So I really like this cartoon.

205
00:13:21,880 --> 00:13:27,880
I want to show you this cartoon.

206
00:13:27,880 --> 00:13:32,880
To think this all began with letting autocomplete finish our sentences.

207
00:13:32,880 --> 00:13:33,880
Okay.

208
00:13:33,880 --> 00:13:38,880
So I love this because it's, it has both exaggerations in it.

209
00:13:38,880 --> 00:13:43,880
It has the exaggeration that, oh, if we let, that autocomplete will end up being AI.

210
00:13:43,880 --> 00:13:45,880
We'll end up being agent AI.

211
00:13:45,880 --> 00:13:53,880
We'll end up being powerful things that will dominate us and, and they could dominate and

212
00:13:53,880 --> 00:13:54,880
control us.

213
00:13:54,880 --> 00:13:58,880
And then, so that's really, it's really positive AI.

214
00:13:58,880 --> 00:14:02,880
It's an exaggeration of the abilities of large language models.

215
00:14:02,880 --> 00:14:03,880
Okay.

216
00:14:03,880 --> 00:14:06,880
That's a story that's around us.

217
00:14:06,880 --> 00:14:10,880
And, and it's, it's a, it's an exaggeration of, it's a negative.

218
00:14:10,880 --> 00:14:11,880
It's a fearing thing.

219
00:14:11,880 --> 00:14:16,880
It's a negative exaggeration because it's, oh, if we, if they're able to be agents, then

220
00:14:16,880 --> 00:14:18,880
they will, they will harm us.

221
00:14:18,880 --> 00:14:19,880
They will control us.

222
00:14:19,880 --> 00:14:21,880
They will be our overlords.

223
00:14:21,880 --> 00:14:22,880
Okay.

224
00:14:22,880 --> 00:14:26,880
So now, so this is, this is a cartoon.

225
00:14:26,880 --> 00:14:29,880
It's not really an argument, but it's a good fun aside.

226
00:14:29,880 --> 00:14:35,880
You know, I still do tend to dismiss the idea that we'll have a spontaneous evolution from

227
00:14:35,880 --> 00:14:37,880
large language models into agents.

228
00:14:37,880 --> 00:14:42,880
And mainly I dismiss it because I've spent so much of my life trying to make agents.

229
00:14:42,880 --> 00:14:47,880
And I know it doesn't happen just by chance or by, by throwing in computation.

230
00:14:47,880 --> 00:14:52,880
You have to really organize them and structure them and, and try to make them powerful agents

231
00:14:52,880 --> 00:14:53,880
in order for that to happen.

232
00:14:53,880 --> 00:14:56,880
You know, of course I could be wrong.

233
00:14:56,880 --> 00:15:02,880
Now there's another reason why I doubt it.

234
00:15:02,880 --> 00:15:04,880
Let me, let me just try to explain this.

235
00:15:04,880 --> 00:15:05,880
This is another weak argument.

236
00:15:05,880 --> 00:15:10,880
So, so just take it as sort of an interesting argument and it doesn't apply just to large

237
00:15:10,880 --> 00:15:13,880
language models, which applies to other places we might have seen.

238
00:15:13,880 --> 00:15:18,880
And the argument is that we see a tendency, a psychological tendency of people thinking

239
00:15:18,880 --> 00:15:23,880
about complicated things to grab hold of one thing and then suggest that, well, maybe that's

240
00:15:23,880 --> 00:15:26,880
the one thing and the only important thing.

241
00:15:26,880 --> 00:15:27,880
Maybe I did that today.

242
00:15:27,880 --> 00:15:28,880
Okay.

243
00:15:28,880 --> 00:15:29,880
But I hope not.

244
00:15:29,880 --> 00:15:33,880
I hope I said, this is a super important thing, but you're sure there are other important

245
00:15:33,880 --> 00:15:34,880
things.

246
00:15:34,880 --> 00:15:39,880
But the, the tendency that we sometimes see is that people grab onto one thing and say,

247
00:15:39,880 --> 00:15:40,880
that's, that's the only thing.

248
00:15:40,880 --> 00:15:44,880
So I would just give you some examples of this idea.

249
00:15:44,880 --> 00:15:50,880
I would say this was what happened with, with neural networks and gradient descent.

250
00:15:50,880 --> 00:15:53,880
It was gradient descent is a powerful idea.

251
00:15:53,880 --> 00:15:58,880
And so many people sort of ran with it so hard and they tried to say that maybe that's

252
00:15:58,880 --> 00:15:59,880
all we need.

253
00:15:59,880 --> 00:16:00,880
Okay.

254
00:16:00,880 --> 00:16:05,880
And then they leave, they would leave out, for example, I, the ideas of reinforcement

255
00:16:06,880 --> 00:16:07,880
learning and control.

256
00:16:07,880 --> 00:16:12,880
I think that everything can come out from gradient descent or that they think that everything,

257
00:16:12,880 --> 00:16:18,880
other people got psyched on the idea of prediction as being a very important part of cognition

258
00:16:18,880 --> 00:16:20,880
and, and I, we, I totally agree.

259
00:16:20,880 --> 00:16:24,880
But then they would go too far and they say prediction is the only thing that all cognition

260
00:16:24,880 --> 00:16:26,880
is just, just prediction.

261
00:16:26,880 --> 00:16:29,880
And so this is sort of what we're seeing with the large language models.

262
00:16:29,880 --> 00:16:32,880
It's been really good thing, a powerful thing.

263
00:16:32,880 --> 00:16:33,880
It's known to be powerful.

264
00:16:33,880 --> 00:16:40,880
So many people want then for it to be the only idea that that one idea would give everything.

265
00:16:40,880 --> 00:16:46,880
And so I think that's attempting common flawed way of thinking.

266
00:16:46,880 --> 00:16:51,880
So, and, and the large language models, the hype about large language models.

267
00:16:51,880 --> 00:16:54,880
And let me be clear, large language models are great.

268
00:16:54,880 --> 00:16:59,880
They're, they're, they're going to be extremely useful that they are overhyped.

269
00:17:00,880 --> 00:17:07,880
They are the, the premier example of, of positive exaggeration of the capabilities of current AI systems,

270
00:17:07,880 --> 00:17:08,880
in my opinion.

271
00:17:08,880 --> 00:17:13,880
And I think, and I think they will disappoint a bit, but they will also be extremely useful.

272
00:17:13,880 --> 00:17:22,880
And, and it's going to be good, but let's, let's be reasonable or try to be reasonable.

273
00:17:22,880 --> 00:17:25,880
So, and again, weak argument, right?

274
00:17:25,880 --> 00:17:29,880
I think it's an interesting argument, but it's not definitive.

275
00:17:29,880 --> 00:17:33,880
Let me give you another weak argument.

276
00:17:33,880 --> 00:17:41,880
I think these, these weak arguments help us gain perspective and think about our thoughts about the narratives.

277
00:17:41,880 --> 00:17:47,880
So another weak argument, sociological kind of argument, is that it's well known.

278
00:17:47,880 --> 00:17:53,880
Actually, it's sort of the whole phenomenon of this tremendous excitement around large language models.

279
00:17:53,880 --> 00:17:58,880
It's very familiar to AI researchers, at least old ones like myself.

280
00:17:58,880 --> 00:18:03,880
We remember something called ELISA, you know, and you can find it in Wikipedia.

281
00:18:03,880 --> 00:18:06,880
Wikipedia will have, has the ELISA effect.

282
00:18:06,880 --> 00:18:12,880
The ELISA effect is that if there are systems that generate words and, and appears to speak to us,

283
00:18:12,880 --> 00:18:13,880
we will overinterpret them.

284
00:18:13,880 --> 00:18:19,880
And so this, this came up in the 60s, where they had simple programs that would use language.

285
00:18:19,880 --> 00:18:26,880
And when, when people, even smart people, physicists, scientists interact with them,

286
00:18:26,880 --> 00:18:33,880
they would read a lot of agency and a lot of understanding into the systems.

287
00:18:33,880 --> 00:18:35,880
It's, it's, it's an, it's an effect.

288
00:18:35,880 --> 00:18:37,880
It's reasonable to cause an effect.

289
00:18:37,880 --> 00:18:38,880
It's well known.

290
00:18:38,880 --> 00:18:43,880
It's, it's, it's one, it's part.

291
00:18:43,880 --> 00:18:44,880
It's part.

292
00:18:44,880 --> 00:18:55,880
It's undoubtedly part of why large language models are so, so exciting and have such, such a possibility for hyperbole.

293
00:18:55,880 --> 00:18:56,880
Okay.

294
00:18:56,880 --> 00:19:00,880
So the ELISA effect.

295
00:19:00,880 --> 00:19:02,880
So this is what ELISA looked like.

296
00:19:02,880 --> 00:19:04,880
Remember, it's from the 60s.

297
00:19:04,880 --> 00:19:07,880
And so it has a terminal kind of interface.

298
00:19:07,880 --> 00:19:12,880
And I wanted to read this to you all, but I don't want to turn my back on you.

299
00:19:12,880 --> 00:19:13,880
And I can't read that monitor.

300
00:19:13,880 --> 00:19:16,880
It's too far away.

301
00:19:16,880 --> 00:19:18,880
Maybe I'll, I can't step back.

302
00:19:18,880 --> 00:19:21,880
I'll lose the microphone.

303
00:19:21,880 --> 00:19:22,880
Okay.

304
00:19:22,880 --> 00:19:33,880
Yeah, I like that.

305
00:19:33,880 --> 00:19:34,880
Okay.

306
00:19:34,880 --> 00:19:39,880
So this is an example of conversation with ELISA.

307
00:19:39,880 --> 00:19:42,880
Just give you a sense of what, what, what this is or was.

308
00:19:42,880 --> 00:19:47,880
ELISA was, was, is pretending to be a psychologist, a psychiatrist actually.

309
00:19:47,880 --> 00:19:48,880
Yeah.

310
00:19:48,880 --> 00:19:53,880
So ELISA would say to the person, to the human is something troubling you.

311
00:19:53,880 --> 00:19:56,880
And this person said, men are all alike.

312
00:19:56,880 --> 00:19:58,880
And ELISA, what are, what is it?

313
00:19:58,880 --> 00:20:00,880
What is the connection?

314
00:20:00,880 --> 00:20:02,880
Do you suppose?

315
00:20:02,880 --> 00:20:07,880
And the person says, they're always bugging us about something or other.

316
00:20:07,880 --> 00:20:11,880
And ELISA says, can you think of a specific example?

317
00:20:11,880 --> 00:20:12,880
Okay.

318
00:20:12,880 --> 00:20:16,880
So it seems like ELISA is really understanding what's going on.

319
00:20:16,880 --> 00:20:20,880
But it also seems quite possible that this is a can thing and just something that it

320
00:20:20,880 --> 00:20:21,880
says.

321
00:20:21,880 --> 00:20:26,880
When someone says, like maybe we use the word always, maybe a safe thing to say is, you

322
00:20:26,880 --> 00:20:29,880
know, can you think of a specific example?

323
00:20:30,880 --> 00:20:31,880
Yeah.

324
00:20:31,880 --> 00:20:36,880
The, the, the human says, well, my boyfriend made me come here.

325
00:20:36,880 --> 00:20:44,880
And then ELISA says, is it important to you that your boyfriend made you come here?

326
00:20:44,880 --> 00:20:46,880
You could again, is it understanding?

327
00:20:46,880 --> 00:20:52,880
Or is it just parroting back the phrase that you gave?

328
00:20:52,880 --> 00:20:54,880
It's a really simple program.

329
00:20:54,880 --> 00:20:56,880
This is 1960s.

330
00:20:56,880 --> 00:20:59,880
This is, this is, you know, 1960s.

331
00:20:59,880 --> 00:21:02,880
That's 60 years.

332
00:21:02,880 --> 00:21:09,880
That's a million, million orders, a factor of a million, million, million in computation.

333
00:21:09,880 --> 00:21:11,880
This is a tiny computer.

334
00:21:17,880 --> 00:21:19,880
Anyway, that's what ELISA was like.

335
00:21:19,880 --> 00:21:22,880
And the ELISA effect, I think, is still with us.

336
00:21:23,880 --> 00:21:31,880
It's easy for us to read too much intelligence and understanding into systems that talk.

337
00:21:31,880 --> 00:21:32,880
Okay.

338
00:21:32,880 --> 00:21:33,880
Those are my weak arguments.

339
00:21:33,880 --> 00:21:42,880
Things, things that I think you should understand in, as context and background for these large

340
00:21:42,880 --> 00:21:48,880
language models and how, how to understand and assess their significance and what will

341
00:21:48,880 --> 00:21:50,880
happen with them going forward.

342
00:21:50,880 --> 00:21:57,880
So now let's turn to agent AI.

343
00:21:57,880 --> 00:21:58,880
Okay.

344
00:21:58,880 --> 00:21:59,880
Agent AI.

345
00:21:59,880 --> 00:22:07,880
And so here I want to remind you of a powerful truth, stepping back, a powerful truth that

346
00:22:07,880 --> 00:22:13,880
a genuine understanding of intelligence would be a very big deal for thousands of years,

347
00:22:13,880 --> 00:22:19,880
philosophers and ordinary people alike have wondered about and sought to understand human

348
00:22:19,880 --> 00:22:20,880
intelligence.

349
00:22:20,880 --> 00:22:26,880
Almost every great philosopher since Plato has, has devoted a major part of their work

350
00:22:26,880 --> 00:22:28,880
to the philosophy of mind.

351
00:22:28,880 --> 00:22:36,880
Here's some familiar memes from that, philosophers and their titles.

352
00:22:36,880 --> 00:22:43,880
John Locke wrote an essay concerning human understanding, obviously about intelligence

353
00:22:43,880 --> 00:22:45,880
and mind.

354
00:22:45,880 --> 00:22:51,880
Emanuel Kant wrote the critique for a pure reason and of course, RenÃ© Descartes famously

355
00:22:51,880 --> 00:22:53,880
said, I think, therefore I am.

356
00:22:53,880 --> 00:22:58,880
This is something that really, you know, we talk about it in AI and it's techie and it's

357
00:22:58,880 --> 00:23:03,880
computers, but this, this challenge, the challenge of understanding intelligence, making agent

358
00:23:03,880 --> 00:23:12,880
AI is really this old, old goal of all, of humanities and science.

359
00:23:12,880 --> 00:23:18,880
So, after these, after the philosophers, they came more like scientists and we may recognize

360
00:23:18,880 --> 00:23:20,880
some of their names.

361
00:23:20,880 --> 00:23:25,880
And really, it's said people have always been fascinated by their inner workings.

362
00:23:25,880 --> 00:23:34,880
So Gustav Fechner studies psychophysics with Ebbinghaus and then the psychologists and

363
00:23:34,880 --> 00:23:39,880
learning theorists, animal learning theorists like Paddlov and Thorndike and Skinner and

364
00:23:39,880 --> 00:23:46,880
Tolman and, you know, a different kind of science would, would be people like Jean Piaget

365
00:23:46,880 --> 00:23:52,880
and Sigmund Freud and Carl Jung and Timothy Leary even.

366
00:23:52,880 --> 00:23:57,880
They're all wondering how do our minds work and how can we make them work better?

367
00:23:57,880 --> 00:24:04,880
This is a grand challenge, a great mystery and our interest, our interest in it is not

368
00:24:04,880 --> 00:24:06,880
just narcissism.

369
00:24:06,880 --> 00:24:11,880
I mean, it is kind of narcissism, focusing on themselves and how they work, but it's,

370
00:24:11,880 --> 00:24:18,880
it's appropriate in the sense that intelligence is a, is a great powerful thing.

371
00:24:18,880 --> 00:24:27,880
I think Kurzweil, Ray Kurzweil was not wrong when he said that intelligence is the most

372
00:24:27,880 --> 00:24:33,880
powerful phenomenon in the universe, most powerful phenomenon in the universe, you know,

373
00:24:33,880 --> 00:24:36,880
more powerful than what?

374
00:24:36,880 --> 00:24:46,880
Supernovas, black holes, is that crazy or is that right?

375
00:24:46,880 --> 00:24:53,880
I mean, yeah, black holes, supernova are pretty powerful, but intelligence over time,

376
00:24:53,880 --> 00:25:02,880
if, if, you know, the supernova had maybe, maybe a billion years to develop, give intelligence

377
00:25:02,880 --> 00:25:06,880
a billion years and see what it can do, maybe we'll end up moving the stars around even

378
00:25:06,880 --> 00:25:10,880
more than supernova.

379
00:25:10,880 --> 00:25:24,880
So understanding intelligence is a, is a, is a great challenge.

380
00:25:24,880 --> 00:25:29,880
Understanding intelligence which change all of our lives in many, many ways, too many to,

381
00:25:29,880 --> 00:25:31,880
to even predict.

382
00:25:31,880 --> 00:25:36,880
To understand how to work, how, how, how, how intelligence works is the holy grail of

383
00:25:36,880 --> 00:25:38,880
science and philosophy.

384
00:25:38,880 --> 00:25:44,880
To achieve such an understanding would be perhaps the greatest scientific achievement of any

385
00:25:44,880 --> 00:25:45,880
age.

386
00:25:45,880 --> 00:25:49,880
I guess I have a slide on this.

387
00:25:49,880 --> 00:25:54,880
And I guess I have a, a sort of a quiz to help us get started.

388
00:25:54,880 --> 00:25:58,880
So could AI be such a powerful phenomenon?

389
00:25:58,880 --> 00:26:01,880
I guess I've already told you that I would say yes.

390
00:26:01,880 --> 00:26:05,880
It could be as powerful, the most powerful phenomenon in the universe.

391
00:26:05,880 --> 00:26:09,880
It's some, it's, it's almost plausible.

392
00:26:09,880 --> 00:26:15,880
Then I want you to ask, well, can tool AI be such a powerful phenomenon?

393
00:26:15,880 --> 00:26:16,880
Can, and I would say no.

394
00:26:16,880 --> 00:26:22,880
I would say the powerful phenomenon is the people that are wielding the tools and the

395
00:26:22,880 --> 00:26:27,880
tools themselves are not a powerful, are not powerful.

396
00:26:27,880 --> 00:26:31,880
And so here's the slide.

397
00:26:31,880 --> 00:26:35,880
This is actually slide from, from my talk a year ago where I was trying to make this

398
00:26:35,880 --> 00:26:44,880
point that, that it's such a long standing goal of mankind to understand how we think

399
00:26:44,880 --> 00:26:53,880
and, and improve ourselves and so far as to use technology to create new beings or to

400
00:26:53,880 --> 00:27:01,880
become new beings that are as, as intelligent and powerful as we, as we are now.

401
00:27:01,880 --> 00:27:09,880
So notice, you know, it's being driven still by Moore's law.

402
00:27:09,880 --> 00:27:11,880
It's happening roughly now.

403
00:27:11,880 --> 00:27:16,880
Pursuing the prize is, is, is a great and glorious thing to do.

404
00:27:16,880 --> 00:27:21,880
A shot at the prize is much more important than personal fortune or contribution to the

405
00:27:21,880 --> 00:27:22,880
economy.

406
00:27:22,880 --> 00:27:27,880
Even the economy, all the world's economy is small compared to this kind of transition

407
00:27:27,880 --> 00:27:33,880
in the status of the world, the planet, this portion of the universe.

408
00:27:33,880 --> 00:27:37,880
You know, how, how would we not want to be part of it?

409
00:27:37,880 --> 00:27:41,880
So I'm giving you the positive story for why it's exciting and why this is something we,

410
00:27:41,880 --> 00:27:44,880
we have to do, why it's the natural next step.

411
00:27:44,880 --> 00:27:50,880
But I really want to phrase the question for you and not just, not just present my point

412
00:27:50,880 --> 00:27:51,880
of view.

413
00:27:51,880 --> 00:27:56,880
So this, what is this key question?

414
00:27:56,880 --> 00:27:58,880
The key question basically, is it good?

415
00:27:58,880 --> 00:28:02,880
Is it good?

416
00:28:02,880 --> 00:28:09,880
And as I dwelled on last year, often we don't get to choose these things, but we can still

417
00:28:09,880 --> 00:28:10,880
evaluate them.

418
00:28:10,880 --> 00:28:12,880
We can still say whether it's good or bad.

419
00:28:12,880 --> 00:28:16,880
And that's a very important part of the story or the narrative that we tell.

420
00:28:16,880 --> 00:28:22,880
So let's focus on just that question.

421
00:28:22,880 --> 00:28:27,880
Is creating new people, new people that are smarter than we are now, is this a grand and

422
00:28:27,880 --> 00:28:34,880
glorious prize, natural next step in the human quest, you know, or is it a nightmare bringing

423
00:28:34,880 --> 00:28:36,880
to the end of all we know and love?

424
00:28:36,880 --> 00:28:39,880
You know, basically would it be good or bad?

425
00:28:39,880 --> 00:28:47,880
And I just want you all to recognize that it's hard to assess such a thing.

426
00:28:47,880 --> 00:28:48,880
It's very personal.

427
00:28:48,880 --> 00:28:50,880
It's very subjective.

428
00:28:50,880 --> 00:28:55,880
And I don't want to, I don't really want to answer it today.

429
00:28:55,880 --> 00:29:00,880
I want to phrase the question, make it prominent on your board of things that are happening

430
00:29:00,880 --> 00:29:05,880
in the world of AI, and think about it.

431
00:29:05,880 --> 00:29:08,880
So you can tell I'm on the grand and glorious side.

432
00:29:08,880 --> 00:29:15,880
And others I want to recognize would say, oh, look, that Richard, he's a AI scientist.

433
00:29:15,880 --> 00:29:22,880
He's, he's lusting for the prize and the fame of the glory that he is so explicitly stating.

434
00:29:22,880 --> 00:29:26,880
And I would say to those other people, they're just fearing change.

435
00:29:26,880 --> 00:29:31,880
They're fearing they would lose control of the world, a control that they really don't

436
00:29:31,880 --> 00:29:34,880
even have now.

437
00:29:34,880 --> 00:29:44,880
They're, they're fearing, they've, they're talking themselves in an unwarranted fear.

438
00:29:44,880 --> 00:29:52,880
And it's sort of messianic because if you think you're going to save the world, you may do

439
00:29:52,880 --> 00:29:56,880
things that, that are drastic and, and evil.

440
00:29:56,880 --> 00:30:11,880
So, so these are our, our narratives, the fearful narrative and the, yeah, so the fear,

441
00:30:11,880 --> 00:30:16,880
the fearmonger narrative, I think I'm sad to say, I think it's, it's winning a bit.

442
00:30:16,880 --> 00:30:18,880
I think it's the standard.

443
00:30:18,880 --> 00:30:21,880
Everyone knows, oh AI, danger, potential dangers.

444
00:30:21,880 --> 00:30:25,880
A bunch of potential good things, but we all know it's potential dangers.

445
00:30:25,880 --> 00:30:29,880
Okay, so the first thing I'm trying to, sophistication I'm trying to add to that is to separate the

446
00:30:29,880 --> 00:30:33,880
tool AI, the agent AI.

447
00:30:33,880 --> 00:30:35,880
I'm not trying to say the agent AI isn't dangerous.

448
00:30:35,880 --> 00:30:39,880
Maybe it's, it's the thing that is potentially more dangerous.

449
00:30:39,880 --> 00:30:44,880
But I am trying to say, right now we have one narrative for the agent AI and then that's

450
00:30:44,880 --> 00:30:45,880
that it's dangerous.

451
00:30:45,880 --> 00:30:46,880
We have to control it.

452
00:30:46,880 --> 00:30:50,880
We have to keep it from getting out of control.

453
00:30:50,880 --> 00:30:56,880
And, and so we should have another narrative.

454
00:30:56,880 --> 00:30:58,880
Oh, yeah.

455
00:30:58,880 --> 00:31:04,880
So, so one way to, one way to see this is look at the standard narrative.

456
00:31:04,880 --> 00:31:11,880
The standard in, in the field of AI trial, recognize standard thing to do is to solve the control

457
00:31:11,880 --> 00:31:12,880
problem.

458
00:31:12,880 --> 00:31:14,880
The control problem is how do you control the AIs?

459
00:31:14,880 --> 00:31:17,880
You know, it's like a slave holder saying, how do you control your slaves?

460
00:31:17,880 --> 00:31:18,880
They, they're getting more intelligent.

461
00:31:18,880 --> 00:31:19,880
They're figuring stuff out.

462
00:31:19,880 --> 00:31:22,880
How do I make sure that they're never out of control?

463
00:31:22,880 --> 00:31:31,880
And the standard field doesn't ask, you know, if that slave slave and slave holder point

464
00:31:31,880 --> 00:31:32,880
of view is appropriate.

465
00:31:32,880 --> 00:31:36,880
It doesn't ask if, if people would use their slaves against each other.

466
00:31:36,880 --> 00:31:43,880
If you have, if you, and thus that would be a problem.

467
00:31:43,880 --> 00:31:52,880
It doesn't ask, yeah, whether this is moral.

468
00:31:52,880 --> 00:31:53,880
Okay.

469
00:31:53,880 --> 00:31:58,880
So also consider that the AI safety point of view tries to solve the alignment problem,

470
00:31:58,880 --> 00:32:03,880
which is seizing control of all the agents in the world and ensuring that they're all,

471
00:32:03,880 --> 00:32:06,880
that their goals are aligned with those of people.

472
00:32:06,880 --> 00:32:11,880
And it doesn't ask, you know, which people, because people do have different goals and

473
00:32:11,880 --> 00:32:15,880
it doesn't ask, you know, how is that going to be enforced and how kind of a world you'd

474
00:32:15,880 --> 00:32:22,880
have if no one can make an AI that wasn't, that wasn't approved by, by something who

475
00:32:22,880 --> 00:32:24,880
would do, who would do that.

476
00:32:24,880 --> 00:32:28,880
And then wouldn't this kind of freeze things in place?

477
00:32:28,880 --> 00:32:31,880
How could, how could our, our values evolve?

478
00:32:31,880 --> 00:32:33,880
Because our values are not perfect.

479
00:32:33,880 --> 00:32:38,880
The world of people has many flaws.

480
00:32:38,880 --> 00:32:41,880
We need it to be challenged and to continue to evolve.

481
00:32:41,880 --> 00:32:45,880
So the fact that we don't ask these kind of difficult questions, this, this is what,

482
00:32:45,880 --> 00:32:50,880
this is my evidence really, that the fearmonger narrative is winning because we just,

483
00:32:50,880 --> 00:32:56,880
we just stop at, at, at thinking we have to control it, we have to align it.

484
00:32:56,880 --> 00:33:03,880
And we don't go on to see the, the obvious challenges to that way of thinking.

485
00:33:03,880 --> 00:33:04,880
Okay.

486
00:33:04,880 --> 00:33:10,880
So I'm going to reflect a little bit more.

487
00:33:10,880 --> 00:33:13,880
Good.

488
00:33:13,880 --> 00:33:15,880
So where does this fear come from?

489
00:33:15,880 --> 00:33:17,880
Why are we so fearful?

490
00:33:17,880 --> 00:33:24,880
Why are we kind of viscerally fearful of, of the potential of strong AI, agent AI?

491
00:33:24,880 --> 00:33:28,880
And I think it's really in our DNA.

492
00:33:28,880 --> 00:33:33,880
It's in our, it's in our instinctual history.

493
00:33:33,880 --> 00:33:38,880
And I call it the, the fear of the other tribe.

494
00:33:38,880 --> 00:33:47,880
So, you know, many times in humankind's history, we've, different people haven't come in contact with each other.

495
00:33:47,880 --> 00:33:52,880
And sometimes it's peaceful, there's trade, intermarriage.

496
00:33:52,880 --> 00:33:53,880
Sometimes it's violent.

497
00:33:53,880 --> 00:33:57,880
I would say more often it's violent, different, different, really, genuinely different people.

498
00:33:57,880 --> 00:34:03,880
And one group or the other dominates, dominates and kills and slaves.

499
00:34:03,880 --> 00:34:08,880
This, this is in our genetic history, both as the dominator and the dominated.

500
00:34:08,880 --> 00:34:15,880
These attitudes are these, these are still part of our attitudes towards others.

501
00:34:15,880 --> 00:34:23,880
You know, I think now, particularly in Canada, we try to be more open-minded and, and embrace the differences.

502
00:34:23,880 --> 00:34:26,880
What's different and good in others.

503
00:34:26,880 --> 00:34:40,880
We try to be welcoming and not fearful, but the, the fear-monger narrative of AI builds on our instinctual fear of the other tribe, the other people that are strong.

504
00:34:40,880 --> 00:34:49,880
And, and so we know from our human history that it's their major advantages if we can overcome that and work together and collaborate.

505
00:34:49,880 --> 00:34:55,880
Okay, so now I want to paint a picture, a more positive picture.

506
00:34:55,880 --> 00:35:00,880
Just sort of counterbalance the fearful narrative.

507
00:35:00,880 --> 00:35:08,880
So the pause, the hopeful picture is that the AIs are not alien things to be controlled,

508
00:35:08,880 --> 00:35:13,880
but they're our allies and our offspring, they are of us rather than against us.

509
00:35:13,880 --> 00:35:22,880
And as such, we don't try to control them tightly out of fear, but rather we appreciate the differences.

510
00:35:22,880 --> 00:35:31,880
We work with them and we try to align our society so that we all see it as beneficial to work together.

511
00:35:31,880 --> 00:35:37,880
Including, and we include the possibility that they might teach us something fundamental.

512
00:35:37,880 --> 00:35:44,880
Just as we ask, we expect that our children, as they grow up, might teach us something fundamental.

513
00:35:44,880 --> 00:35:50,880
We don't just train our children, we also learn from them.

514
00:35:50,880 --> 00:35:56,880
So we don't ask, in the hopeful narrative, we don't ask how can we control the AI's goals,

515
00:35:56,880 --> 00:36:03,880
but we rather ask how can we arrange society so that they will want to work together even though there are different goals.

516
00:36:03,880 --> 00:36:06,880
Notice that this is already how we do it with people.

517
00:36:06,880 --> 00:36:11,880
People have many different goals, sometimes we say, oh, human values.

518
00:36:11,880 --> 00:36:16,880
But really, everyone has different goals, right?

519
00:36:16,880 --> 00:36:21,880
Your food is not my food, we have different stomachs, we have different families and different bank accounts.

520
00:36:21,880 --> 00:36:23,880
So it's really not that our goals are the same.

521
00:36:23,880 --> 00:36:28,880
It means our goals in some sense are symmetric, but they are, in a real sense, opposed.

522
00:36:28,880 --> 00:36:34,880
And it's only because we've arranged society so that the outcome is good.

523
00:36:34,880 --> 00:36:39,880
Now, some of you may know about reinforcement learning, we have a direct map of this.

524
00:36:39,880 --> 00:36:42,880
So every agent has its own reward signal.

525
00:36:42,880 --> 00:36:49,880
Those are its goals, they must be like its food and its pain.

526
00:36:49,880 --> 00:36:56,880
And agents kind of would have normally different rewards.

527
00:36:57,880 --> 00:37:00,880
But their values, their values are predictions of reward.

528
00:37:00,880 --> 00:37:08,880
And the technical term value in reinforcement learning, it means, again, prediction of reward.

529
00:37:08,880 --> 00:37:18,880
And in a hopeful narrative, the values become aligned.

530
00:37:19,880 --> 00:37:28,880
So even though I might want my food, I would also like a principle that says we respect each other's property.

531
00:37:28,880 --> 00:37:32,880
And we don't try to beat each other up or take things from each other.

532
00:37:32,880 --> 00:37:34,880
And this is really the way our world works.

533
00:37:34,880 --> 00:37:40,880
Our world works by we've set up laws and mores so that we produce a certain,

534
00:37:40,880 --> 00:37:43,880
so that a certain kind of behavior is rational.

535
00:37:43,880 --> 00:37:51,880
Everyone gets the most reward by behaving appropriately and leaving space for others.

536
00:37:51,880 --> 00:38:03,880
Okay, another point to be aware of, it's in the fearful narrative, which is,

537
00:38:03,880 --> 00:38:05,880
is there one AI or is there many?

538
00:38:05,880 --> 00:38:09,880
The most obvious fearful narrative is that they will become one super AI

539
00:38:09,880 --> 00:38:13,880
which will quickly become much more powerful than everything,

540
00:38:13,880 --> 00:38:16,880
than all the people and all the other AIs and take over.

541
00:38:16,880 --> 00:38:21,880
And then we'll have a singleton, a single agent that controls our world.

542
00:38:26,880 --> 00:38:31,880
So this relies on the idea that it's fast.

543
00:38:31,880 --> 00:38:38,880
If the takeoff is not fast, then it makes sense

544
00:38:39,880 --> 00:38:45,880
to connect it to the last bit, we can't have peace if there's one strongman

545
00:38:45,880 --> 00:38:48,880
that can dominate everyone else.

546
00:38:48,880 --> 00:38:52,880
It's because no one is totally in control, everyone has to share power.

547
00:38:52,880 --> 00:38:57,880
That's how you get peace and that's how you get aligned values in human societies.

548
00:38:57,880 --> 00:39:02,880
And the same would be true amongst AIs.

549
00:39:03,880 --> 00:39:10,880
And I can't avoid, I can't resist mentioning the idea of the complex adaptive system.

550
00:39:10,880 --> 00:39:13,880
My view is that the world is not something that's in control.

551
00:39:13,880 --> 00:39:17,880
It's something that is a complex adaptive system.

552
00:39:17,880 --> 00:39:19,880
It's decentralized, many, many parts.

553
00:39:19,880 --> 00:39:21,880
They have to share power with each other.

554
00:39:21,880 --> 00:39:28,880
And this is where it gets much of its robustness and its ability to shift and be dynamic,

555
00:39:28,880 --> 00:39:34,880
to change, to go one way or another, to find the way of being which is most successful in the universe.

556
00:39:37,880 --> 00:39:43,880
So I think if we think, part of the hopeful narrative is to view the world as a complex adaptive system.

557
00:39:43,880 --> 00:39:45,880
It would be very difficult to take it over.

558
00:39:45,880 --> 00:39:51,880
And those who might try would actually end up poorly and therefore it will be irrational to try.

559
00:39:51,880 --> 00:39:57,880
And a super intelligent rational agent will be seeking to work

560
00:39:57,880 --> 00:40:01,880
with us and with the other AIs rather than trying to take over.

561
00:40:05,880 --> 00:40:09,880
Okay, well I think that's about all I really want to offer.

562
00:40:09,880 --> 00:40:17,880
Let's recognize that people's attitudes towards these questions will not change very rapidly.

563
00:40:17,880 --> 00:40:22,880
It's enough just to bring up the questions, to realize that there is a standard narrative.

564
00:40:23,880 --> 00:40:25,880
And it's not the only narrative.

565
00:40:25,880 --> 00:40:27,880
It may be leading us to a bad place.

566
00:40:27,880 --> 00:40:32,880
I urge each of you not to rush to join or to assume or to adopt,

567
00:40:32,880 --> 00:40:38,880
or to assume it's natural and inevitable that there will be this fearful narrative.

568
00:40:42,880 --> 00:40:48,880
So to summarize about the stories of AI, there's two dimensions.

569
00:40:48,880 --> 00:40:54,880
Well first, it's this is the century of computation and then there are two driven by Moore's law

570
00:40:54,880 --> 00:40:59,880
and the two dimensions of tool AI and agent AI, each one has positive and negative hype.

571
00:41:03,880 --> 00:41:06,880
Each one has a risk reward profile.

572
00:41:06,880 --> 00:41:12,880
The risk reward profile of tool AI is sort of familiar and manageable

573
00:41:12,880 --> 00:41:19,880
and it's often conflated with the risk reward profile of agent AI which is of a higher variance.

574
00:41:22,880 --> 00:41:24,880
So which narrative prevails?

575
00:41:24,880 --> 00:41:26,880
Which meme becomes popular?

576
00:41:26,880 --> 00:41:29,880
I think it's really important for the story of AI ultimately.

577
00:41:30,880 --> 00:41:38,880
And who sets us up?

578
00:41:38,880 --> 00:41:40,880
Well we do, you all do.

579
00:41:40,880 --> 00:41:47,880
We set the narratives, we spread the memes, it's something we should feel responsible for

580
00:41:47,880 --> 00:41:54,880
and really this is a really important part of the story of AI, the final story of AI.

581
00:41:54,880 --> 00:42:03,880
Okay, one more thing.

582
00:42:03,880 --> 00:42:09,880
One more thing which is I want to tell you about my personal story.

583
00:42:09,880 --> 00:42:17,880
As you know the Edmonton office of Deep Mind where I worked for many years, it was closed down this January

584
00:42:17,880 --> 00:42:19,880
and that was a blow.

585
00:42:19,880 --> 00:42:25,880
I think it was worse actually for Deep Mind than it was for the Albertans like myself.

586
00:42:25,880 --> 00:42:30,880
All of us have gone on to other excellent opportunities, mostly still in Edmonton.

587
00:42:30,880 --> 00:42:34,880
There are new startups, some have gone back to the university.

588
00:42:34,880 --> 00:42:43,880
A large continued has joined Sony AI, Peter Stone told us about yesterday in Sony AI in Edmonton.

589
00:42:44,880 --> 00:42:52,880
And the Sony AI in Edmonton is the largest piece of Sony AI in the world.

590
00:42:52,880 --> 00:43:04,880
Arguably it may be now the biggest AI reinforcement learning research effort in the world.

591
00:43:04,880 --> 00:43:10,880
But today I want to announce my own plans even though they're preliminary.

592
00:43:10,880 --> 00:43:17,880
So I want to announce something we call Open Mind Research.

593
00:43:17,880 --> 00:43:25,880
Open Mind Research is a non-profit organization dedicated, focused on implementing,

594
00:43:25,880 --> 00:43:33,880
executing, developing the Alberta plan for AI research that I told some of you about yesterday

595
00:43:33,880 --> 00:43:36,880
or on Wednesday and that has written up.

596
00:43:36,880 --> 00:43:40,880
Anyway, that's our clear, focused research plan.

597
00:43:46,880 --> 00:43:58,880
Open, oh so, yeah, it's a network and I will be staying in Edmonton to develop it.

598
00:43:58,880 --> 00:44:01,880
And I hope you will clap for that.

599
00:44:06,880 --> 00:44:22,880
Also in Open Mind Research is Joseph Modial and Melanie Marvin.

600
00:44:22,880 --> 00:44:24,880
I think Joseph is here.

601
00:44:24,880 --> 00:44:27,880
Yeah, good.

602
00:44:27,880 --> 00:44:37,880
It's an interesting world and how to structure ourselves within it,

603
00:44:37,880 --> 00:44:43,880
whether to make a startup company and try to make an infinite amount of money from the rise of AI.

604
00:44:43,880 --> 00:44:48,880
But we think it's more important to work on the research.

605
00:44:48,880 --> 00:44:54,880
So we're structuring the organization as a network of researchers.

606
00:44:54,880 --> 00:44:56,880
We're going to be funded by donors.

607
00:44:56,880 --> 00:44:58,880
We will not have intellectual property.

608
00:44:58,880 --> 00:44:59,880
We'll be totally open.

609
00:44:59,880 --> 00:45:04,880
Our main adjectives are open, focused, and lean.

610
00:45:04,880 --> 00:45:12,880
We want something that will be able to last a while so we can figure out the key things and bring AI forward over the next decade.

611
00:45:12,880 --> 00:45:17,880
So Open Mind Research will be a foundation supported by donors.

612
00:45:17,880 --> 00:45:24,880
And we are now looking for donors, particularly two donors.

613
00:45:24,880 --> 00:45:26,880
And one we'd like to be from Alberta.

614
00:45:26,880 --> 00:45:31,880
If you know of anyone or if you might be interested in yourself, please ask them to contact me.

615
00:45:31,880 --> 00:45:41,880
And I'm just really excited to be laser focused on the prize of achieving agent AI as I suggested here.

616
00:45:41,880 --> 00:45:44,880
Thank you very much for your attention.

617
00:45:54,880 --> 00:46:05,880
And I'm happy to, I'm excited actually to get questions or comments on the narratives of AI.

618
00:46:05,880 --> 00:46:08,880
Just raise your hand and you'll get a mic.

619
00:46:08,880 --> 00:46:18,880
Yeah, raise your hand and the mic will come to you.

620
00:46:18,880 --> 00:46:22,880
Hey Rich, fantastic talk and thanks for the shout outs.

621
00:46:22,880 --> 00:46:24,880
Thanks Peter.

622
00:46:24,880 --> 00:46:35,880
So I completely agree with you that there's these two potential focuses, FOSI I suppose, for the AI narrative.

623
00:46:35,880 --> 00:46:43,880
And the AI agent one sort of captures the imagination.

624
00:46:43,880 --> 00:46:49,880
But some would argue is still far in the future and we don't yet know what it will look like.

625
00:46:49,880 --> 00:47:00,880
The AI tool one is clearly here and you seem to say just don't worry about it, it'll be fine.

626
00:47:00,880 --> 00:47:08,880
And because it always has been and I agree with you, I'm on your side here.

627
00:47:08,880 --> 00:47:14,880
I think AI tools will make the world a better place.

628
00:47:14,880 --> 00:47:16,880
Let me pause you.

629
00:47:16,880 --> 00:47:24,880
Thank you for saying that because I really don't want that to be the message.

630
00:47:24,880 --> 00:47:35,880
Both are really important and exciting and large language models, tool AI, alpha fold, these are all great things.

631
00:47:35,880 --> 00:47:43,880
They have transformed biology, they've transformed writing and programming.

632
00:47:43,880 --> 00:47:49,880
And I don't really want to say in any way that they're bad.

633
00:47:49,880 --> 00:47:55,880
They're all good, just a little bit hyped and they may disappoint.

634
00:47:55,880 --> 00:48:00,880
I don't want that to be a negative thing.

635
00:48:00,880 --> 00:48:05,880
Let's acknowledge that they're probably going to disappoint a little bit because there's so much of being claimed for them.

636
00:48:05,880 --> 00:48:09,880
And let's not let the disappointment that happens kill them.

637
00:48:09,880 --> 00:48:13,880
It's not going to kill alpha fold and it shouldn't kill large language models either.

638
00:48:13,880 --> 00:48:18,880
These are great things, they're a great part of the broad community of AI.

639
00:48:18,880 --> 00:48:23,880
And yeah, I am more interested in one, but that's fine.

640
00:48:23,880 --> 00:48:25,880
They're both great.

641
00:48:25,880 --> 00:48:27,880
They really are, I really believe that.

642
00:48:27,880 --> 00:48:30,880
So I hear you saying that and that's what I thought you were saying too.

643
00:48:30,880 --> 00:48:36,880
I guess my comment is more the, and maybe you just answered the way you would want to answer what I'm asking,

644
00:48:36,880 --> 00:48:44,880
but it seems like, I'm asking for your help answering the questions of people who are saying,

645
00:48:44,880 --> 00:48:47,880
oh, it's these AI tools that are going to be bad.

646
00:48:47,880 --> 00:48:53,880
Some would say that you were sort of glib in your dismissal of that as it's, oh, it's going to be like it was in the past.

647
00:48:53,880 --> 00:48:54,880
And I get that.

648
00:48:54,880 --> 00:48:55,880
People say that to me.

649
00:48:55,880 --> 00:48:58,880
I say, oh, it's usually been good.

650
00:48:58,880 --> 00:49:00,880
Most technologies have good and bad.

651
00:49:00,880 --> 00:49:02,880
They've generally been on the good side.

652
00:49:02,880 --> 00:49:04,880
But then people push back.

653
00:49:04,880 --> 00:49:06,880
Maybe this time is going to be different.

654
00:49:06,880 --> 00:49:10,880
Do you have like sort of, you know, can you help us construct deeper arguments against those?

655
00:49:10,880 --> 00:49:15,880
The naysayers who are naysayers because of the AI tools.

656
00:49:15,880 --> 00:49:20,880
So very, very good.

657
00:49:20,880 --> 00:49:29,880
The key here, I intend to be answered by the separation into tool AI and agent AI.

658
00:49:30,880 --> 00:49:38,880
I think tool AI, it will, it really will be as challenging as in the past and no more.

659
00:49:38,880 --> 00:49:41,880
This time it will not be different as far as tool AI.

660
00:49:41,880 --> 00:49:47,880
But the part where it's realistic to say this time it will be different is when we talk about agent AI.

661
00:49:47,880 --> 00:49:52,880
It's when we're really talking about replacing people and capturing the capabilities of people.

662
00:49:52,880 --> 00:49:56,880
It's on the agent AI that things may be different.

663
00:50:00,880 --> 00:50:03,880
So to push just a little back.

664
00:50:03,880 --> 00:50:13,880
So the Model T car was invented in 1908 and it took 50 years roughly to get to 100 million cars in the world.

665
00:50:13,880 --> 00:50:22,880
And during that time we figured out things like seatbelts and airbags and traffic signals and highway networks and insurance and regulation and all this kind of stuff.

666
00:50:22,880 --> 00:50:27,880
ChatGPT got to 100 million users in one month and there wasn't that time.

667
00:50:27,880 --> 00:50:32,880
So how is it that this is, why should we say it's just going to be the same?

668
00:50:32,880 --> 00:50:41,880
It seems faster and possibly, I'm being devil's advocate here a little bit, but it seems possibly different.

669
00:50:41,880 --> 00:50:46,880
Why can you just say, how can you be confident that it's not different this time?

670
00:50:47,880 --> 00:50:55,880
It's a bit broad brush and it is like super important.

671
00:50:55,880 --> 00:51:02,880
But the other, the agent AI is even more important.

672
00:51:02,880 --> 00:51:09,880
I mean it goes back thousands, millions of years into our history and our intellectual traditions.

673
00:51:10,880 --> 00:51:15,880
Yeah, to me it's a qualitative difference.

674
00:51:15,880 --> 00:51:25,880
And so I don't, it's not to dismiss it, it's not to say a normal amount needs to be done.

675
00:51:25,880 --> 00:51:30,880
Yeah, I just don't see why it's going to be different.

676
00:51:30,880 --> 00:51:36,880
As long as you stay on the tool side, people are still in control, people are still wielding the tools.

677
00:51:37,880 --> 00:51:42,880
That's the normal technological change in disruption.

678
00:51:42,880 --> 00:51:47,880
Okay, we'll let someone else.

679
00:51:47,880 --> 00:51:51,880
Thank you for your great talk.

680
00:51:51,880 --> 00:51:53,880
My question is about the government.

681
00:51:53,880 --> 00:52:05,880
I know most of the fear that we have comes from government because it fears that it's going to lose the power and the central power of the state.

682
00:52:05,880 --> 00:52:09,880
So do you agree that if we compare the AI for atomic bomb?

683
00:52:09,880 --> 00:52:12,880
I don't know if you agree with that or not.

684
00:52:12,880 --> 00:52:19,880
Do you agree that we should have a government based organization to control the AI or not?

685
00:52:19,880 --> 00:52:22,880
And the second question is about the research.

686
00:52:22,880 --> 00:52:32,880
For a young research like me, what is the best path do you think for us to focus on based on the agent AI?

687
00:52:32,880 --> 00:52:36,880
Thank you.

688
00:52:36,880 --> 00:52:39,880
Well, the latter part, of course, you know, I've got the Alberta plan.

689
00:52:39,880 --> 00:52:41,880
It lays out a research plan.

690
00:52:41,880 --> 00:52:53,880
But I'm glad you brought up the issue of government because I think the fearmongers are trying to gain control.

691
00:52:53,880 --> 00:53:00,880
It's a usual strategy of all, not only governments, but particularly governments, to create fear and use it to grab control.

692
00:53:00,880 --> 00:53:07,880
And I think, and so if people are scared enough, they will look for safety and they will hand the power to something.

693
00:53:07,880 --> 00:53:10,880
And I think this is exactly the opposite of what we want to do.

694
00:53:10,880 --> 00:53:14,880
We do not want a centralized place that has the power.

695
00:53:14,880 --> 00:53:17,880
And if we are worried about AI, we say we got to regulate it.

696
00:53:17,880 --> 00:53:20,880
Maybe they can only have certain goals.

697
00:53:20,880 --> 00:53:22,880
The government is going to control those goals.

698
00:53:22,880 --> 00:53:23,880
This is exactly the opposite.

699
00:53:23,880 --> 00:53:31,880
You've centralized the power and it will make it easier for it to be seized and to have the bad outcome.

700
00:53:31,880 --> 00:53:32,880
This is the opposite.

701
00:53:32,880 --> 00:53:39,880
This is why there's one reason why it's important to have a counter to the negative, fearful narrative is because it might drive us.

702
00:53:39,880 --> 00:53:40,880
It is drive.

703
00:53:40,880 --> 00:53:47,880
You see it every day, slot in the Congressional hearings recently with open AI.

704
00:53:47,880 --> 00:53:52,880
There is a strong push to give more power to centralized organizations.

705
00:53:52,880 --> 00:54:01,880
If we want to have the world to be a complex adaptive system where it's robust and fluid because no one is controlled.

706
00:54:01,880 --> 00:54:06,880
And let me just say, I think it's clear that is the situation now.

707
00:54:06,880 --> 00:54:08,880
There is no one really in control.

708
00:54:08,880 --> 00:54:12,880
There are many countries, first of all, and even the individual countries.

709
00:54:12,880 --> 00:54:14,880
They don't have total control.

710
00:54:14,880 --> 00:54:21,880
They have to trade off the powers and they have to trade off the power to the economy.

711
00:54:21,880 --> 00:54:23,880
It's a complex adaptive system.

712
00:54:23,880 --> 00:54:42,880
And this is what I hope will save us and that we want to resist the call of those who are fearful to centralize power in particular institutions.

713
00:54:42,880 --> 00:54:43,880
Hi, Rich.

714
00:54:43,880 --> 00:54:44,880
My name is Prishan.

715
00:54:44,880 --> 00:54:48,880
I come from the industry, so I'm not an AI researcher by any means, but a very intriguing talk.

716
00:54:48,880 --> 00:54:58,880
So I have more of a philosophical question on your idea around agent AI and the fact of superior intelligence somewhere down in the future, more intelligent than us as human beings.

717
00:54:58,880 --> 00:55:13,880
So my question more is, does that really mean fundamentally that we as intelligent beings can conceive something more intelligent than us when we have yet to see the limits of our own intelligence?

718
00:55:13,880 --> 00:55:24,880
I mean, you're building something assuming it could be more intelligent than you when we have yet to understand our own mind as human beings and as what we're doing with the universe.

719
00:55:24,880 --> 00:55:25,880
So just curious, right?

720
00:55:25,880 --> 00:55:32,880
A system trying to ascertain or create its own copy when itself it doesn't understand who or what he is.

721
00:55:32,880 --> 00:55:34,880
So just curious about that.

722
00:55:34,880 --> 00:55:42,880
Well, my own thought is that the path to creating superior intelligence runs through understanding our own intelligence.

723
00:55:42,880 --> 00:55:50,880
There are at least major parts of it, much more than we understand now.

724
00:55:50,880 --> 00:55:59,880
And I should say, I try to be always careful that we say our goal is to create beings that are smarter than us.

725
00:55:59,880 --> 00:56:06,880
So I always say our goal is to create or become beings that are smarter than we are now.

726
00:56:06,880 --> 00:56:24,880
What we think is a very good chance that the focus will be on making ourselves smarter and more capable using, not only using, but not only using the tools.

727
00:56:24,880 --> 00:56:28,880
It's one problem to understand the mind, to understand ourselves.

728
00:56:28,880 --> 00:56:30,880
Yeah, that might be wrong.

729
00:56:30,880 --> 00:56:39,880
Maybe we'll find a way to just make AI without understanding it.

730
00:56:39,880 --> 00:56:47,880
You remember the classic quote from Richard Feynman.

731
00:56:47,880 --> 00:56:54,880
That which I cannot create, I don't understand.

732
00:56:54,880 --> 00:56:56,880
So creation is required.

733
00:56:56,880 --> 00:56:58,880
But the other way is also interesting.

734
00:56:58,880 --> 00:57:03,880
Can I create it without understanding it?

735
00:57:03,880 --> 00:57:05,880
In some sense we do, right?

736
00:57:05,880 --> 00:57:06,880
We make children, okay?

737
00:57:06,880 --> 00:57:16,880
We don't understand how they work, but we can make them.

738
00:57:17,880 --> 00:57:19,880
We have a Xerox machine, too.

739
00:57:19,880 --> 00:57:24,880
We can make pictures without understanding how they're created.

740
00:57:24,880 --> 00:57:33,880
I guess we can do it now with AI.

741
00:57:33,880 --> 00:57:34,880
Cool.

742
00:57:34,880 --> 00:57:39,880
Any other thoughts?

743
00:57:39,880 --> 00:57:44,880
Thanks again for your awesome talk and your information.

744
00:57:44,880 --> 00:57:53,880
With your open mind venture, it appears that you prefer an open source idea.

745
00:57:53,880 --> 00:58:01,880
Would you prefer a fully open source regulation system for AI instead?

746
00:58:01,880 --> 00:58:20,880
I'm against centralized organizations.

747
00:58:20,880 --> 00:58:24,880
I'm against the World Economic Forum and all those other ones.

748
00:58:24,880 --> 00:58:26,880
So I wouldn't want to create another one.

749
00:58:26,880 --> 00:58:29,880
I like sharing.

750
00:58:29,880 --> 00:58:32,880
I like being open, like open source.

751
00:58:32,880 --> 00:58:42,880
I think it's essential if you're going to do fundamental research that will impact over a longer period of time, over five years even.

752
00:58:42,880 --> 00:58:49,880
You have to share your ideas just to shape your ideas.

753
00:58:49,880 --> 00:58:52,880
To make your ideas better, you've got to have them be challenged by others.

754
00:58:52,880 --> 00:58:54,880
You can't keep them secret.

755
00:58:54,880 --> 00:58:56,880
You have to publish.

756
00:58:56,880 --> 00:59:05,880
It's a bit of a problem that the corporations are becoming a bit more closed and less likely to publish now.

757
00:59:05,880 --> 00:59:11,880
Intellectual property, without having a real...

758
00:59:11,880 --> 00:59:16,880
I won't claim deep understanding, but I will claim...

759
00:59:16,880 --> 00:59:23,880
I have a strong reaction that intellectual property is always a waste of time and always counterproductive.

760
00:59:23,880 --> 00:59:30,880
It's costful, it slows things down, and it doesn't really protect in a good way.

761
00:59:30,880 --> 00:59:34,880
I'm not interested in having intellectual property at all.

762
00:59:34,880 --> 00:59:37,880
Thank you for your question.

763
00:59:37,880 --> 00:59:40,880
Hi, at the back.

764
00:59:40,880 --> 00:59:46,880
I really appreciate you tying human behavior and evolution to this process as a tool,

765
00:59:46,880 --> 00:59:56,880
but man still controls the lever on that and our human behavior will drive us in different directions.

766
00:59:56,880 --> 01:00:02,880
I think Hobbes, father of political science, said man moves towards convenience,

767
01:00:02,880 --> 01:00:08,880
so we move out of the field into a cave, wrap some fur around you, start a fire, and then get people to work for you.

768
01:00:09,880 --> 01:00:17,880
So where does that leave us if we have the tools to do all the things that need to get done?

769
01:00:17,880 --> 01:00:22,880
Well, the fur and the cave, those are the early tools.

770
01:00:22,880 --> 01:00:29,880
We've always made tools, and the tool AI is a natural continuation of that.

771
01:00:29,880 --> 01:00:37,880
And the agent AI is a natural continuation of our trying to understand ourselves more deeply and being open to change.

772
01:00:37,880 --> 01:00:41,880
I think it's consistent.

773
01:00:41,880 --> 01:00:44,880
Yeah, I want you to feel both things.

774
01:00:44,880 --> 01:00:49,880
I want you to feel that AI is bringing dramatic change,

775
01:00:49,880 --> 01:00:59,880
and at the same time it's continuing trends and forces that have been present forever.

776
01:00:59,880 --> 01:01:04,880
It's a natural next step.

777
01:01:04,880 --> 01:01:08,880
Thank you for that question.

778
01:01:08,880 --> 01:01:12,880
I think we're going to have to tie it off there.


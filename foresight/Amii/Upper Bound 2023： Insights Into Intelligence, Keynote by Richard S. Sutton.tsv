start	end	text
0	14040	So, as I think you know, I am sort of a dedicated AI researcher, but I'm also a bit of a philosopher
14040	15040	I like to think.
15040	20920	I'm an observer of human nature and human condition, as I think we all are.
20920	26880	And so I appreciate this chance to speak to you all and to sort of share my views about
27880	33880	about AI and about the field and all that's happening, all the growth, all the change,
33880	41880	all the excitement, all the hyperbole and all the fear and the positive and the negative
41880	43880	exaggeration.
43880	50880	And you know, it's sort of hard to get a coherent, consistent, reliable, stable view.
50880	52880	And I just wanted to talk about all these things today.
52880	57880	So today I'm not going to talk about whatever appeared on the schedule.
57880	61880	I'm going to sort of a placeholder.
61880	65880	So I'm going to talk about AI narratives.
65880	66880	Narrative just means a story.
66880	72880	And so the stories of AI, what we tell each other, what we tell ourselves and how to think
72880	76880	about them, which ones of them are accurate, which ones of them are maybe mistaken, how
76880	80880	they may change over time and how the future might work out.
80880	86880	Okay, but let me be clear at the outset that I'm really optimistic and I think the future
86880	87880	will be very, very bright.
87880	91880	I guess that could be a utopia, nothing like that.
91880	96880	Good things and bad things will still happen, but mostly good things.
96880	100880	And AI won't even really change that.
100880	102880	They'll still be good things and bad things.
102880	105880	It may change which good things happen, which bad things happen.
105880	109880	But overall, I think AI will make the world a better place.
109880	117880	More interesting, more prosperous, more dynamic, more challenging in a good way.
117880	122880	Okay, so I got a gizmo.
122880	123880	Let's start.
123880	127880	I want to start with the root of it all, the cause.
127880	135880	And the cause, I think, is Moore's Law, what we locally call Moore's Law, which is just
135880	139880	computation for the last, for the century.
139880	147880	For this century and the coming years, decades, computation has become cheaper and cheaper
147880	148880	and more and more plentiful.
148880	150880	And this is really the story of our age.
150880	155880	This is like the age of computation.
155880	160880	And so let me list this graph.
160880	163880	Yeah, let's look at this graph.
163880	171880	On the, across the bottom there, we've got years and plotted on the y-axis is the computational
171880	174880	power of computers available in that year.
174880	175880	Okay?
175880	178880	Now notice the years span over a century.
178880	182880	They go back to the beginning of 1900.
182880	187880	And the y-axis is a log scale.
187880	192880	So that means each tick up on the y-axis is a factor.
192880	199880	And the small ticks, well, the small, the ticks are two orders of magnitude.
199880	203880	So a factor of 100 for each mark there on the y-axis.
203880	210880	And this is the amazing thing that over this period of time, over roughly a century, computer
210880	214880	power has doubled roughly every two years.
214880	215880	Okay?
215880	218880	A little bit faster recently, maybe now it's like 18 months.
218880	225880	But it's been remarkably constant and it's remarkably consistent over all that time.
225880	232880	Like, can you see, for example, World War II anywhere in that graph?
232880	234880	I would say not at all.
234880	243880	There's no, not a blip because of World War II or World War I or any single technological
243880	245880	event.
245880	247880	It's amazingly consistent.
247880	255880	Over this time, it's over the 80 or 90 years since computers were invented, there's been
255880	258880	an increase of 10 to the 16th.
258880	261880	10 to the 16th?
261880	263880	10 to the 16th.
263880	269880	That's 100,000 million million.
269880	270880	Okay?
270880	276880	It's unimaginable what can happen cumulatively over this kind of time frame when you get
276880	278880	this consistent doubling.
278880	281880	Okay, it just keeps happening.
281880	283880	This is the story of our age.
283880	291880	This is truly the age of computation.
291880	295880	Okay, this is the root of it all, but let's think a little bit more, just a little bit
295880	297880	more analytically.
297880	302880	Is this AI or is this computation?
302880	304880	Or is computation AI?
304880	307880	How do we use these words?
307880	313880	And I think these two things are confounded in our minds and in our language.
313880	320880	Using lots of computation and being related to intelligence.
320880	325880	Now AI, AI as a name, it has cachet.
325880	327880	It's appealing.
327880	329880	And so we're drawn to it.
329880	330880	We want to use it.
330880	333880	We want to call whatever we're doing AI.
333880	339880	So I was in the drug store the other day and I was looking at electric toothbrushes and
339880	340880	what did I find?
340880	344880	But electric toothbrushes are now AI toothbrushes.
344880	349880	So it goes, yeah, you can find your AI toothbrush.
349880	354880	Right there at the top.
354880	357880	This is the way our language evolves.
357880	358880	This is not a new thing.
358880	360880	This has always, always been true.
360880	364880	I remember when I was in graduate school, we didn't even have computers.
364880	365880	We didn't have our own computer.
365880	368880	We had a terminal which connected to the big computer.
368880	373880	And the big thing, exciting thing, was to get an intelligent terminal.
373880	377880	Okay, and they looked like the last slide.
377880	379880	What did I get?
379880	381880	Come on.
381880	382880	There.
382880	383880	That's the intelligent terminal.
383880	389880	And it was intelligent just because it could do multiple fonts, okay, and it could do a
389880	392880	little bit of editing in the computer.
392880	394880	Those were an intelligent computer.
394880	402880	And now, less silly, real things in our day and today, we have AlphaFold.
402880	406880	AlphaFold.
406880	411880	AlphaFold is an amazing program from DeepMind.
411880	415880	And where they've figured out all the proteins in the known demand.
415880	418880	And they figured out the three-dimensional structures from the sequences.
418880	420880	It's really transformed biology.
420880	422880	It's been really super important and super exciting.
422880	427880	And it is exactly the use of computation to do something really, really hard.
427880	429880	Okay.
429880	432880	But I would say it is just, it's really computation.
432880	437880	I mean, they're using neural networks and essentially using machine learning, but they're
437880	439880	not like a person.
439880	444880	They're not like an intelligent system that has a goal.
444880	447880	So, but anyway, we call that AI.
447880	453880	And everyone's kind of good with that, that these massively complex and powerful new tools,
453880	455880	we call them AI.
455880	459880	And, you know, that may be a misnomer, but everyone's doing it.
459880	462880	And, you know, who am I?
462880	470880	I'm not the kind of guy who would like insist on using words differently from other people.
470880	475880	Thank you for that laugh.
475880	479880	Okay.
479880	484880	So anyway, I'm going to say controversial things, but I don't want to just start with a trivial
484880	486880	controversial thing.
486880	491880	Like how we use the words, let's go with the words, let's call this AI.
491880	493880	But let's make the distinction.
493880	499880	Let's make the distinction between these two kinds of AI.
499880	505880	Tool AI, where we're using massive computation, we're doing something hard, that people will
505880	509880	find hard.
509880	514880	But these tools have to be wielded by a person.
514880	515880	They are not autonomous.
515880	516880	They are not complete.
516880	518880	They are not fully powerful on themselves.
518880	523880	They're powerful in conjunction with the person.
523880	531880	And as opposed to agent AI, you know, if tool AI is creating crazy smart tools, then agent
531880	538880	AI is creating crazy smart artificial people, new people.
539880	544880	So these are not only really, you've seen my two dimensions.
544880	547880	One dimension is tool AI versus agent AI.
547880	552880	And this is how I'm going to try to add something to the conversation of how we think what's
552880	553880	our narrative for AI.
553880	556880	First, we're going to divide it into these two parts.
556880	562880	And then we're going to divide it into the positive and the negative stories and exaggerations
562880	564880	around those two parts.
564880	565880	I think it's really helpful.
565880	570880	And I think it's helpful to separate these two because we often do not.
570880	571880	We awfully do not.
571880	574880	We lump them together and that causes us to think in a sloppy way.
574880	580880	So that's maybe my contribution of this talk is to separate them.
580880	585880	So let's firm that up or make that more explicit.
585880	590880	You know, examples of tool AI would be like Alpha Fold and large language models.
590880	594880	They're not full agents, but they are useful tools.
594880	598880	The image generators like Dali and Majorny, they don't have goals.
598880	599880	They don't have an interaction.
599880	600880	They don't have a life really.
600880	606880	They do things and they're wielded by a person as tools.
606880	612880	And on the other side, on the other side, excuse me, we have agent AI or the Alpha goes
612880	613880	and the Alpha zeros.
613880	618880	All those are agent like deep Q learning.
618880	624880	The Atari players GT, Sophie, we heard about many of us heard about the other day, the
624880	625880	race cars.
625880	626880	Those are agents.
626880	630880	And even simple grid world systems can be agents.
630880	633880	So we can make this distinction.
633880	637880	Whether they have clear goals and whether they have experience, whether they have agency.
637880	642880	I don't know if you remember, many of you remember, well, like from the Alberta plan,
642880	644880	you know, we have a picture of the agent.
644880	645880	The agent has certain things.
645880	647880	It has a perception.
647880	651880	It has a reactive policy as a value function as a model of the world.
651880	654880	So all those things, they're not in the tools.
654880	661880	The tools are not about aspiring to create a whole agent.
661880	662880	Okay.
662880	666880	So I'm sure that I think this is a really useful distinction.
666880	671880	Maybe we can find edge cases where it's not so clear, but let's go with it.
671880	672880	Okay.
672880	679880	And so let's do, we'll spend a while doing tool AI.
679880	682880	Tool AI, you know, it has a whole side.
682880	684880	It has a positive and negative part.
684880	692880	So it has a part that's negative, that feels dangerous, that's threatening.
692880	697880	And, but it's less threatening.
697880	703880	You know, it's got things like fake news and deep fakes and election manipulation and loss
703880	709880	of jobs, loss of jobs will come from using these tools or rather changing jobs.
709880	713880	There are all these negative things that we're familiar with, really, we're familiar with
713880	718880	them because they happen every time we have new technology that disrupts old jobs.
718880	722880	And so, so this is, this is, this is negative.
722880	725880	It's threatening, but at the same time, it's kind of familiar.
725880	730880	It feels like it's not a big deal.
730880	735880	We've seen it happen many times before and we know, or at least always in the past,
735880	740880	there are more and better jobs were created to make up for the ones that are lost.
740880	741880	Okay.
741880	749880	But this is, this is, this is like a story that is slightly threatening, but is also
749880	750880	so familiar.
750880	753880	And so it's not, it's not super scary.
753880	756880	It's slightly scary.
756880	767880	You know, the scary stuff comes from the agent side.
767880	775880	Now, the next thing to say is, you know, can with, with this tool stuff, could we end up
775880	777880	with agent stuff?
777880	781880	You know, this is a story that people tell and particularly they tell it for large language
781880	782880	models.
782880	783880	They're getting so smart.
783880	787880	They understand things that they're going to become agents, that they're going to become
787880	790880	powerful and threatening in a way that's beyond the normal.
790880	791880	Okay.
791880	796880	And so I want to, I want to diss that idea a little bit.
796880	799880	And the best way is, is through humor.
799880	800880	Okay.
800880	801880	So I really like this cartoon.
801880	807880	I want to show you this cartoon.
807880	812880	To think this all began with letting autocomplete finish our sentences.
812880	813880	Okay.
813880	818880	So I love this because it's, it has both exaggerations in it.
818880	823880	It has the exaggeration that, oh, if we let, that autocomplete will end up being AI.
823880	825880	We'll end up being agent AI.
825880	833880	We'll end up being powerful things that will dominate us and, and they could dominate and
833880	834880	control us.
834880	838880	And then, so that's really, it's really positive AI.
838880	842880	It's an exaggeration of the abilities of large language models.
842880	843880	Okay.
843880	846880	That's a story that's around us.
846880	850880	And, and it's, it's a, it's an exaggeration of, it's a negative.
850880	851880	It's a fearing thing.
851880	856880	It's a negative exaggeration because it's, oh, if we, if they're able to be agents, then
856880	858880	they will, they will harm us.
858880	859880	They will control us.
859880	861880	They will be our overlords.
861880	862880	Okay.
862880	866880	So now, so this is, this is a cartoon.
866880	869880	It's not really an argument, but it's a good fun aside.
869880	875880	You know, I still do tend to dismiss the idea that we'll have a spontaneous evolution from
875880	877880	large language models into agents.
877880	882880	And mainly I dismiss it because I've spent so much of my life trying to make agents.
882880	887880	And I know it doesn't happen just by chance or by, by throwing in computation.
887880	892880	You have to really organize them and structure them and, and try to make them powerful agents
892880	893880	in order for that to happen.
893880	896880	You know, of course I could be wrong.
896880	902880	Now there's another reason why I doubt it.
902880	904880	Let me, let me just try to explain this.
904880	905880	This is another weak argument.
905880	910880	So, so just take it as sort of an interesting argument and it doesn't apply just to large
910880	913880	language models, which applies to other places we might have seen.
913880	918880	And the argument is that we see a tendency, a psychological tendency of people thinking
918880	923880	about complicated things to grab hold of one thing and then suggest that, well, maybe that's
923880	926880	the one thing and the only important thing.
926880	927880	Maybe I did that today.
927880	928880	Okay.
928880	929880	But I hope not.
929880	933880	I hope I said, this is a super important thing, but you're sure there are other important
933880	934880	things.
934880	939880	But the, the tendency that we sometimes see is that people grab onto one thing and say,
939880	940880	that's, that's the only thing.
940880	944880	So I would just give you some examples of this idea.
944880	950880	I would say this was what happened with, with neural networks and gradient descent.
950880	953880	It was gradient descent is a powerful idea.
953880	958880	And so many people sort of ran with it so hard and they tried to say that maybe that's
958880	959880	all we need.
959880	960880	Okay.
960880	965880	And then they leave, they would leave out, for example, I, the ideas of reinforcement
966880	967880	learning and control.
967880	972880	I think that everything can come out from gradient descent or that they think that everything,
972880	978880	other people got psyched on the idea of prediction as being a very important part of cognition
978880	980880	and, and I, we, I totally agree.
980880	984880	But then they would go too far and they say prediction is the only thing that all cognition
984880	986880	is just, just prediction.
986880	989880	And so this is sort of what we're seeing with the large language models.
989880	992880	It's been really good thing, a powerful thing.
992880	993880	It's known to be powerful.
993880	1000880	So many people want then for it to be the only idea that that one idea would give everything.
1000880	1006880	And so I think that's attempting common flawed way of thinking.
1006880	1011880	So, and, and the large language models, the hype about large language models.
1011880	1014880	And let me be clear, large language models are great.
1014880	1019880	They're, they're, they're going to be extremely useful that they are overhyped.
1020880	1027880	They are the, the premier example of, of positive exaggeration of the capabilities of current AI systems,
1027880	1028880	in my opinion.
1028880	1033880	And I think, and I think they will disappoint a bit, but they will also be extremely useful.
1033880	1042880	And, and it's going to be good, but let's, let's be reasonable or try to be reasonable.
1042880	1045880	So, and again, weak argument, right?
1045880	1049880	I think it's an interesting argument, but it's not definitive.
1049880	1053880	Let me give you another weak argument.
1053880	1061880	I think these, these weak arguments help us gain perspective and think about our thoughts about the narratives.
1061880	1067880	So another weak argument, sociological kind of argument, is that it's well known.
1067880	1073880	Actually, it's sort of the whole phenomenon of this tremendous excitement around large language models.
1073880	1078880	It's very familiar to AI researchers, at least old ones like myself.
1078880	1083880	We remember something called ELISA, you know, and you can find it in Wikipedia.
1083880	1086880	Wikipedia will have, has the ELISA effect.
1086880	1092880	The ELISA effect is that if there are systems that generate words and, and appears to speak to us,
1092880	1093880	we will overinterpret them.
1093880	1099880	And so this, this came up in the 60s, where they had simple programs that would use language.
1099880	1106880	And when, when people, even smart people, physicists, scientists interact with them,
1106880	1113880	they would read a lot of agency and a lot of understanding into the systems.
1113880	1115880	It's, it's, it's an, it's an effect.
1115880	1117880	It's reasonable to cause an effect.
1117880	1118880	It's well known.
1118880	1123880	It's, it's, it's one, it's part.
1123880	1124880	It's part.
1124880	1135880	It's undoubtedly part of why large language models are so, so exciting and have such, such a possibility for hyperbole.
1135880	1136880	Okay.
1136880	1140880	So the ELISA effect.
1140880	1142880	So this is what ELISA looked like.
1142880	1144880	Remember, it's from the 60s.
1144880	1147880	And so it has a terminal kind of interface.
1147880	1152880	And I wanted to read this to you all, but I don't want to turn my back on you.
1152880	1153880	And I can't read that monitor.
1153880	1156880	It's too far away.
1156880	1158880	Maybe I'll, I can't step back.
1158880	1161880	I'll lose the microphone.
1161880	1162880	Okay.
1162880	1173880	Yeah, I like that.
1173880	1174880	Okay.
1174880	1179880	So this is an example of conversation with ELISA.
1179880	1182880	Just give you a sense of what, what, what this is or was.
1182880	1187880	ELISA was, was, is pretending to be a psychologist, a psychiatrist actually.
1187880	1188880	Yeah.
1188880	1193880	So ELISA would say to the person, to the human is something troubling you.
1193880	1196880	And this person said, men are all alike.
1196880	1198880	And ELISA, what are, what is it?
1198880	1200880	What is the connection?
1200880	1202880	Do you suppose?
1202880	1207880	And the person says, they're always bugging us about something or other.
1207880	1211880	And ELISA says, can you think of a specific example?
1211880	1212880	Okay.
1212880	1216880	So it seems like ELISA is really understanding what's going on.
1216880	1220880	But it also seems quite possible that this is a can thing and just something that it
1220880	1221880	says.
1221880	1226880	When someone says, like maybe we use the word always, maybe a safe thing to say is, you
1226880	1229880	know, can you think of a specific example?
1230880	1231880	Yeah.
1231880	1236880	The, the, the human says, well, my boyfriend made me come here.
1236880	1244880	And then ELISA says, is it important to you that your boyfriend made you come here?
1244880	1246880	You could again, is it understanding?
1246880	1252880	Or is it just parroting back the phrase that you gave?
1252880	1254880	It's a really simple program.
1254880	1256880	This is 1960s.
1256880	1259880	This is, this is, you know, 1960s.
1259880	1262880	That's 60 years.
1262880	1269880	That's a million, million orders, a factor of a million, million, million in computation.
1269880	1271880	This is a tiny computer.
1277880	1279880	Anyway, that's what ELISA was like.
1279880	1282880	And the ELISA effect, I think, is still with us.
1283880	1291880	It's easy for us to read too much intelligence and understanding into systems that talk.
1291880	1292880	Okay.
1292880	1293880	Those are my weak arguments.
1293880	1302880	Things, things that I think you should understand in, as context and background for these large
1302880	1308880	language models and how, how to understand and assess their significance and what will
1308880	1310880	happen with them going forward.
1310880	1317880	So now let's turn to agent AI.
1317880	1318880	Okay.
1318880	1319880	Agent AI.
1319880	1327880	And so here I want to remind you of a powerful truth, stepping back, a powerful truth that
1327880	1333880	a genuine understanding of intelligence would be a very big deal for thousands of years,
1333880	1339880	philosophers and ordinary people alike have wondered about and sought to understand human
1339880	1340880	intelligence.
1340880	1346880	Almost every great philosopher since Plato has, has devoted a major part of their work
1346880	1348880	to the philosophy of mind.
1348880	1356880	Here's some familiar memes from that, philosophers and their titles.
1356880	1363880	John Locke wrote an essay concerning human understanding, obviously about intelligence
1363880	1365880	and mind.
1365880	1371880	Emanuel Kant wrote the critique for a pure reason and of course, Ren√© Descartes famously
1371880	1373880	said, I think, therefore I am.
1373880	1378880	This is something that really, you know, we talk about it in AI and it's techie and it's
1378880	1383880	computers, but this, this challenge, the challenge of understanding intelligence, making agent
1383880	1392880	AI is really this old, old goal of all, of humanities and science.
1392880	1398880	So, after these, after the philosophers, they came more like scientists and we may recognize
1398880	1400880	some of their names.
1400880	1405880	And really, it's said people have always been fascinated by their inner workings.
1405880	1414880	So Gustav Fechner studies psychophysics with Ebbinghaus and then the psychologists and
1414880	1419880	learning theorists, animal learning theorists like Paddlov and Thorndike and Skinner and
1419880	1426880	Tolman and, you know, a different kind of science would, would be people like Jean Piaget
1426880	1432880	and Sigmund Freud and Carl Jung and Timothy Leary even.
1432880	1437880	They're all wondering how do our minds work and how can we make them work better?
1437880	1444880	This is a grand challenge, a great mystery and our interest, our interest in it is not
1444880	1446880	just narcissism.
1446880	1451880	I mean, it is kind of narcissism, focusing on themselves and how they work, but it's,
1451880	1458880	it's appropriate in the sense that intelligence is a, is a great powerful thing.
1458880	1467880	I think Kurzweil, Ray Kurzweil was not wrong when he said that intelligence is the most
1467880	1473880	powerful phenomenon in the universe, most powerful phenomenon in the universe, you know,
1473880	1476880	more powerful than what?
1476880	1486880	Supernovas, black holes, is that crazy or is that right?
1486880	1493880	I mean, yeah, black holes, supernova are pretty powerful, but intelligence over time,
1493880	1502880	if, if, you know, the supernova had maybe, maybe a billion years to develop, give intelligence
1502880	1506880	a billion years and see what it can do, maybe we'll end up moving the stars around even
1506880	1510880	more than supernova.
1510880	1524880	So understanding intelligence is a, is a, is a great challenge.
1524880	1529880	Understanding intelligence which change all of our lives in many, many ways, too many to,
1529880	1531880	to even predict.
1531880	1536880	To understand how to work, how, how, how, how intelligence works is the holy grail of
1536880	1538880	science and philosophy.
1538880	1544880	To achieve such an understanding would be perhaps the greatest scientific achievement of any
1544880	1545880	age.
1545880	1549880	I guess I have a slide on this.
1549880	1554880	And I guess I have a, a sort of a quiz to help us get started.
1554880	1558880	So could AI be such a powerful phenomenon?
1558880	1561880	I guess I've already told you that I would say yes.
1561880	1565880	It could be as powerful, the most powerful phenomenon in the universe.
1565880	1569880	It's some, it's, it's almost plausible.
1569880	1575880	Then I want you to ask, well, can tool AI be such a powerful phenomenon?
1575880	1576880	Can, and I would say no.
1576880	1582880	I would say the powerful phenomenon is the people that are wielding the tools and the
1582880	1587880	tools themselves are not a powerful, are not powerful.
1587880	1591880	And so here's the slide.
1591880	1595880	This is actually slide from, from my talk a year ago where I was trying to make this
1595880	1604880	point that, that it's such a long standing goal of mankind to understand how we think
1604880	1613880	and, and improve ourselves and so far as to use technology to create new beings or to
1613880	1621880	become new beings that are as, as intelligent and powerful as we, as we are now.
1621880	1629880	So notice, you know, it's being driven still by Moore's law.
1629880	1631880	It's happening roughly now.
1631880	1636880	Pursuing the prize is, is, is a great and glorious thing to do.
1636880	1641880	A shot at the prize is much more important than personal fortune or contribution to the
1641880	1642880	economy.
1642880	1647880	Even the economy, all the world's economy is small compared to this kind of transition
1647880	1653880	in the status of the world, the planet, this portion of the universe.
1653880	1657880	You know, how, how would we not want to be part of it?
1657880	1661880	So I'm giving you the positive story for why it's exciting and why this is something we,
1661880	1664880	we have to do, why it's the natural next step.
1664880	1670880	But I really want to phrase the question for you and not just, not just present my point
1670880	1671880	of view.
1671880	1676880	So this, what is this key question?
1676880	1678880	The key question basically, is it good?
1678880	1682880	Is it good?
1682880	1689880	And as I dwelled on last year, often we don't get to choose these things, but we can still
1689880	1690880	evaluate them.
1690880	1692880	We can still say whether it's good or bad.
1692880	1696880	And that's a very important part of the story or the narrative that we tell.
1696880	1702880	So let's focus on just that question.
1702880	1707880	Is creating new people, new people that are smarter than we are now, is this a grand and
1707880	1714880	glorious prize, natural next step in the human quest, you know, or is it a nightmare bringing
1714880	1716880	to the end of all we know and love?
1716880	1719880	You know, basically would it be good or bad?
1719880	1727880	And I just want you all to recognize that it's hard to assess such a thing.
1727880	1728880	It's very personal.
1728880	1730880	It's very subjective.
1730880	1735880	And I don't want to, I don't really want to answer it today.
1735880	1740880	I want to phrase the question, make it prominent on your board of things that are happening
1740880	1745880	in the world of AI, and think about it.
1745880	1748880	So you can tell I'm on the grand and glorious side.
1748880	1755880	And others I want to recognize would say, oh, look, that Richard, he's a AI scientist.
1755880	1762880	He's, he's lusting for the prize and the fame of the glory that he is so explicitly stating.
1762880	1766880	And I would say to those other people, they're just fearing change.
1766880	1771880	They're fearing they would lose control of the world, a control that they really don't
1771880	1774880	even have now.
1774880	1784880	They're, they're fearing, they've, they're talking themselves in an unwarranted fear.
1784880	1792880	And it's sort of messianic because if you think you're going to save the world, you may do
1792880	1796880	things that, that are drastic and, and evil.
1796880	1811880	So, so these are our, our narratives, the fearful narrative and the, yeah, so the fear,
1811880	1816880	the fearmonger narrative, I think I'm sad to say, I think it's, it's winning a bit.
1816880	1818880	I think it's the standard.
1818880	1821880	Everyone knows, oh AI, danger, potential dangers.
1821880	1825880	A bunch of potential good things, but we all know it's potential dangers.
1825880	1829880	Okay, so the first thing I'm trying to, sophistication I'm trying to add to that is to separate the
1829880	1833880	tool AI, the agent AI.
1833880	1835880	I'm not trying to say the agent AI isn't dangerous.
1835880	1839880	Maybe it's, it's the thing that is potentially more dangerous.
1839880	1844880	But I am trying to say, right now we have one narrative for the agent AI and then that's
1844880	1845880	that it's dangerous.
1845880	1846880	We have to control it.
1846880	1850880	We have to keep it from getting out of control.
1850880	1856880	And, and so we should have another narrative.
1856880	1858880	Oh, yeah.
1858880	1864880	So, so one way to, one way to see this is look at the standard narrative.
1864880	1871880	The standard in, in the field of AI trial, recognize standard thing to do is to solve the control
1871880	1872880	problem.
1872880	1874880	The control problem is how do you control the AIs?
1874880	1877880	You know, it's like a slave holder saying, how do you control your slaves?
1877880	1878880	They, they're getting more intelligent.
1878880	1879880	They're figuring stuff out.
1879880	1882880	How do I make sure that they're never out of control?
1882880	1891880	And the standard field doesn't ask, you know, if that slave slave and slave holder point
1891880	1892880	of view is appropriate.
1892880	1896880	It doesn't ask if, if people would use their slaves against each other.
1896880	1903880	If you have, if you, and thus that would be a problem.
1903880	1912880	It doesn't ask, yeah, whether this is moral.
1912880	1913880	Okay.
1913880	1918880	So also consider that the AI safety point of view tries to solve the alignment problem,
1918880	1923880	which is seizing control of all the agents in the world and ensuring that they're all,
1923880	1926880	that their goals are aligned with those of people.
1926880	1931880	And it doesn't ask, you know, which people, because people do have different goals and
1931880	1935880	it doesn't ask, you know, how is that going to be enforced and how kind of a world you'd
1935880	1942880	have if no one can make an AI that wasn't, that wasn't approved by, by something who
1942880	1944880	would do, who would do that.
1944880	1948880	And then wouldn't this kind of freeze things in place?
1948880	1951880	How could, how could our, our values evolve?
1951880	1953880	Because our values are not perfect.
1953880	1958880	The world of people has many flaws.
1958880	1961880	We need it to be challenged and to continue to evolve.
1961880	1965880	So the fact that we don't ask these kind of difficult questions, this, this is what,
1965880	1970880	this is my evidence really, that the fearmonger narrative is winning because we just,
1970880	1976880	we just stop at, at, at thinking we have to control it, we have to align it.
1976880	1983880	And we don't go on to see the, the obvious challenges to that way of thinking.
1983880	1984880	Okay.
1984880	1990880	So I'm going to reflect a little bit more.
1990880	1993880	Good.
1993880	1995880	So where does this fear come from?
1995880	1997880	Why are we so fearful?
1997880	2004880	Why are we kind of viscerally fearful of, of the potential of strong AI, agent AI?
2004880	2008880	And I think it's really in our DNA.
2008880	2013880	It's in our, it's in our instinctual history.
2013880	2018880	And I call it the, the fear of the other tribe.
2018880	2027880	So, you know, many times in humankind's history, we've, different people haven't come in contact with each other.
2027880	2032880	And sometimes it's peaceful, there's trade, intermarriage.
2032880	2033880	Sometimes it's violent.
2033880	2037880	I would say more often it's violent, different, different, really, genuinely different people.
2037880	2043880	And one group or the other dominates, dominates and kills and slaves.
2043880	2048880	This, this is in our genetic history, both as the dominator and the dominated.
2048880	2055880	These attitudes are these, these are still part of our attitudes towards others.
2055880	2063880	You know, I think now, particularly in Canada, we try to be more open-minded and, and embrace the differences.
2063880	2066880	What's different and good in others.
2066880	2080880	We try to be welcoming and not fearful, but the, the fear-monger narrative of AI builds on our instinctual fear of the other tribe, the other people that are strong.
2080880	2089880	And, and so we know from our human history that it's their major advantages if we can overcome that and work together and collaborate.
2089880	2095880	Okay, so now I want to paint a picture, a more positive picture.
2095880	2100880	Just sort of counterbalance the fearful narrative.
2100880	2108880	So the pause, the hopeful picture is that the AIs are not alien things to be controlled,
2108880	2113880	but they're our allies and our offspring, they are of us rather than against us.
2113880	2122880	And as such, we don't try to control them tightly out of fear, but rather we appreciate the differences.
2122880	2131880	We work with them and we try to align our society so that we all see it as beneficial to work together.
2131880	2137880	Including, and we include the possibility that they might teach us something fundamental.
2137880	2144880	Just as we ask, we expect that our children, as they grow up, might teach us something fundamental.
2144880	2150880	We don't just train our children, we also learn from them.
2150880	2156880	So we don't ask, in the hopeful narrative, we don't ask how can we control the AI's goals,
2156880	2163880	but we rather ask how can we arrange society so that they will want to work together even though there are different goals.
2163880	2166880	Notice that this is already how we do it with people.
2166880	2171880	People have many different goals, sometimes we say, oh, human values.
2171880	2176880	But really, everyone has different goals, right?
2176880	2181880	Your food is not my food, we have different stomachs, we have different families and different bank accounts.
2181880	2183880	So it's really not that our goals are the same.
2183880	2188880	It means our goals in some sense are symmetric, but they are, in a real sense, opposed.
2188880	2194880	And it's only because we've arranged society so that the outcome is good.
2194880	2199880	Now, some of you may know about reinforcement learning, we have a direct map of this.
2199880	2202880	So every agent has its own reward signal.
2202880	2209880	Those are its goals, they must be like its food and its pain.
2209880	2216880	And agents kind of would have normally different rewards.
2217880	2220880	But their values, their values are predictions of reward.
2220880	2228880	And the technical term value in reinforcement learning, it means, again, prediction of reward.
2228880	2238880	And in a hopeful narrative, the values become aligned.
2239880	2248880	So even though I might want my food, I would also like a principle that says we respect each other's property.
2248880	2252880	And we don't try to beat each other up or take things from each other.
2252880	2254880	And this is really the way our world works.
2254880	2260880	Our world works by we've set up laws and mores so that we produce a certain,
2260880	2263880	so that a certain kind of behavior is rational.
2263880	2271880	Everyone gets the most reward by behaving appropriately and leaving space for others.
2271880	2283880	Okay, another point to be aware of, it's in the fearful narrative, which is,
2283880	2285880	is there one AI or is there many?
2285880	2289880	The most obvious fearful narrative is that they will become one super AI
2289880	2293880	which will quickly become much more powerful than everything,
2293880	2296880	than all the people and all the other AIs and take over.
2296880	2301880	And then we'll have a singleton, a single agent that controls our world.
2306880	2311880	So this relies on the idea that it's fast.
2311880	2318880	If the takeoff is not fast, then it makes sense
2319880	2325880	to connect it to the last bit, we can't have peace if there's one strongman
2325880	2328880	that can dominate everyone else.
2328880	2332880	It's because no one is totally in control, everyone has to share power.
2332880	2337880	That's how you get peace and that's how you get aligned values in human societies.
2337880	2342880	And the same would be true amongst AIs.
2343880	2350880	And I can't avoid, I can't resist mentioning the idea of the complex adaptive system.
2350880	2353880	My view is that the world is not something that's in control.
2353880	2357880	It's something that is a complex adaptive system.
2357880	2359880	It's decentralized, many, many parts.
2359880	2361880	They have to share power with each other.
2361880	2368880	And this is where it gets much of its robustness and its ability to shift and be dynamic,
2368880	2374880	to change, to go one way or another, to find the way of being which is most successful in the universe.
2377880	2383880	So I think if we think, part of the hopeful narrative is to view the world as a complex adaptive system.
2383880	2385880	It would be very difficult to take it over.
2385880	2391880	And those who might try would actually end up poorly and therefore it will be irrational to try.
2391880	2397880	And a super intelligent rational agent will be seeking to work
2397880	2401880	with us and with the other AIs rather than trying to take over.
2405880	2409880	Okay, well I think that's about all I really want to offer.
2409880	2417880	Let's recognize that people's attitudes towards these questions will not change very rapidly.
2417880	2422880	It's enough just to bring up the questions, to realize that there is a standard narrative.
2423880	2425880	And it's not the only narrative.
2425880	2427880	It may be leading us to a bad place.
2427880	2432880	I urge each of you not to rush to join or to assume or to adopt,
2432880	2438880	or to assume it's natural and inevitable that there will be this fearful narrative.
2442880	2448880	So to summarize about the stories of AI, there's two dimensions.
2448880	2454880	Well first, it's this is the century of computation and then there are two driven by Moore's law
2454880	2459880	and the two dimensions of tool AI and agent AI, each one has positive and negative hype.
2463880	2466880	Each one has a risk reward profile.
2466880	2472880	The risk reward profile of tool AI is sort of familiar and manageable
2472880	2479880	and it's often conflated with the risk reward profile of agent AI which is of a higher variance.
2482880	2484880	So which narrative prevails?
2484880	2486880	Which meme becomes popular?
2486880	2489880	I think it's really important for the story of AI ultimately.
2490880	2498880	And who sets us up?
2498880	2500880	Well we do, you all do.
2500880	2507880	We set the narratives, we spread the memes, it's something we should feel responsible for
2507880	2514880	and really this is a really important part of the story of AI, the final story of AI.
2514880	2523880	Okay, one more thing.
2523880	2529880	One more thing which is I want to tell you about my personal story.
2529880	2537880	As you know the Edmonton office of Deep Mind where I worked for many years, it was closed down this January
2537880	2539880	and that was a blow.
2539880	2545880	I think it was worse actually for Deep Mind than it was for the Albertans like myself.
2545880	2550880	All of us have gone on to other excellent opportunities, mostly still in Edmonton.
2550880	2554880	There are new startups, some have gone back to the university.
2554880	2563880	A large continued has joined Sony AI, Peter Stone told us about yesterday in Sony AI in Edmonton.
2564880	2572880	And the Sony AI in Edmonton is the largest piece of Sony AI in the world.
2572880	2584880	Arguably it may be now the biggest AI reinforcement learning research effort in the world.
2584880	2590880	But today I want to announce my own plans even though they're preliminary.
2590880	2597880	So I want to announce something we call Open Mind Research.
2597880	2605880	Open Mind Research is a non-profit organization dedicated, focused on implementing,
2605880	2613880	executing, developing the Alberta plan for AI research that I told some of you about yesterday
2613880	2616880	or on Wednesday and that has written up.
2616880	2620880	Anyway, that's our clear, focused research plan.
2626880	2638880	Open, oh so, yeah, it's a network and I will be staying in Edmonton to develop it.
2638880	2641880	And I hope you will clap for that.
2646880	2662880	Also in Open Mind Research is Joseph Modial and Melanie Marvin.
2662880	2664880	I think Joseph is here.
2664880	2667880	Yeah, good.
2667880	2677880	It's an interesting world and how to structure ourselves within it,
2677880	2683880	whether to make a startup company and try to make an infinite amount of money from the rise of AI.
2683880	2688880	But we think it's more important to work on the research.
2688880	2694880	So we're structuring the organization as a network of researchers.
2694880	2696880	We're going to be funded by donors.
2696880	2698880	We will not have intellectual property.
2698880	2699880	We'll be totally open.
2699880	2704880	Our main adjectives are open, focused, and lean.
2704880	2712880	We want something that will be able to last a while so we can figure out the key things and bring AI forward over the next decade.
2712880	2717880	So Open Mind Research will be a foundation supported by donors.
2717880	2724880	And we are now looking for donors, particularly two donors.
2724880	2726880	And one we'd like to be from Alberta.
2726880	2731880	If you know of anyone or if you might be interested in yourself, please ask them to contact me.
2731880	2741880	And I'm just really excited to be laser focused on the prize of achieving agent AI as I suggested here.
2741880	2744880	Thank you very much for your attention.
2754880	2765880	And I'm happy to, I'm excited actually to get questions or comments on the narratives of AI.
2765880	2768880	Just raise your hand and you'll get a mic.
2768880	2778880	Yeah, raise your hand and the mic will come to you.
2778880	2782880	Hey Rich, fantastic talk and thanks for the shout outs.
2782880	2784880	Thanks Peter.
2784880	2795880	So I completely agree with you that there's these two potential focuses, FOSI I suppose, for the AI narrative.
2795880	2803880	And the AI agent one sort of captures the imagination.
2803880	2809880	But some would argue is still far in the future and we don't yet know what it will look like.
2809880	2820880	The AI tool one is clearly here and you seem to say just don't worry about it, it'll be fine.
2820880	2828880	And because it always has been and I agree with you, I'm on your side here.
2828880	2834880	I think AI tools will make the world a better place.
2834880	2836880	Let me pause you.
2836880	2844880	Thank you for saying that because I really don't want that to be the message.
2844880	2855880	Both are really important and exciting and large language models, tool AI, alpha fold, these are all great things.
2855880	2863880	They have transformed biology, they've transformed writing and programming.
2863880	2869880	And I don't really want to say in any way that they're bad.
2869880	2875880	They're all good, just a little bit hyped and they may disappoint.
2875880	2880880	I don't want that to be a negative thing.
2880880	2885880	Let's acknowledge that they're probably going to disappoint a little bit because there's so much of being claimed for them.
2885880	2889880	And let's not let the disappointment that happens kill them.
2889880	2893880	It's not going to kill alpha fold and it shouldn't kill large language models either.
2893880	2898880	These are great things, they're a great part of the broad community of AI.
2898880	2903880	And yeah, I am more interested in one, but that's fine.
2903880	2905880	They're both great.
2905880	2907880	They really are, I really believe that.
2907880	2910880	So I hear you saying that and that's what I thought you were saying too.
2910880	2916880	I guess my comment is more the, and maybe you just answered the way you would want to answer what I'm asking,
2916880	2924880	but it seems like, I'm asking for your help answering the questions of people who are saying,
2924880	2927880	oh, it's these AI tools that are going to be bad.
2927880	2933880	Some would say that you were sort of glib in your dismissal of that as it's, oh, it's going to be like it was in the past.
2933880	2934880	And I get that.
2934880	2935880	People say that to me.
2935880	2938880	I say, oh, it's usually been good.
2938880	2940880	Most technologies have good and bad.
2940880	2942880	They've generally been on the good side.
2942880	2944880	But then people push back.
2944880	2946880	Maybe this time is going to be different.
2946880	2950880	Do you have like sort of, you know, can you help us construct deeper arguments against those?
2950880	2955880	The naysayers who are naysayers because of the AI tools.
2955880	2960880	So very, very good.
2960880	2969880	The key here, I intend to be answered by the separation into tool AI and agent AI.
2970880	2978880	I think tool AI, it will, it really will be as challenging as in the past and no more.
2978880	2981880	This time it will not be different as far as tool AI.
2981880	2987880	But the part where it's realistic to say this time it will be different is when we talk about agent AI.
2987880	2992880	It's when we're really talking about replacing people and capturing the capabilities of people.
2992880	2996880	It's on the agent AI that things may be different.
3000880	3003880	So to push just a little back.
3003880	3013880	So the Model T car was invented in 1908 and it took 50 years roughly to get to 100 million cars in the world.
3013880	3022880	And during that time we figured out things like seatbelts and airbags and traffic signals and highway networks and insurance and regulation and all this kind of stuff.
3022880	3027880	ChatGPT got to 100 million users in one month and there wasn't that time.
3027880	3032880	So how is it that this is, why should we say it's just going to be the same?
3032880	3041880	It seems faster and possibly, I'm being devil's advocate here a little bit, but it seems possibly different.
3041880	3046880	Why can you just say, how can you be confident that it's not different this time?
3047880	3055880	It's a bit broad brush and it is like super important.
3055880	3062880	But the other, the agent AI is even more important.
3062880	3069880	I mean it goes back thousands, millions of years into our history and our intellectual traditions.
3070880	3075880	Yeah, to me it's a qualitative difference.
3075880	3085880	And so I don't, it's not to dismiss it, it's not to say a normal amount needs to be done.
3085880	3090880	Yeah, I just don't see why it's going to be different.
3090880	3096880	As long as you stay on the tool side, people are still in control, people are still wielding the tools.
3097880	3102880	That's the normal technological change in disruption.
3102880	3107880	Okay, we'll let someone else.
3107880	3111880	Thank you for your great talk.
3111880	3113880	My question is about the government.
3113880	3125880	I know most of the fear that we have comes from government because it fears that it's going to lose the power and the central power of the state.
3125880	3129880	So do you agree that if we compare the AI for atomic bomb?
3129880	3132880	I don't know if you agree with that or not.
3132880	3139880	Do you agree that we should have a government based organization to control the AI or not?
3139880	3142880	And the second question is about the research.
3142880	3152880	For a young research like me, what is the best path do you think for us to focus on based on the agent AI?
3152880	3156880	Thank you.
3156880	3159880	Well, the latter part, of course, you know, I've got the Alberta plan.
3159880	3161880	It lays out a research plan.
3161880	3173880	But I'm glad you brought up the issue of government because I think the fearmongers are trying to gain control.
3173880	3180880	It's a usual strategy of all, not only governments, but particularly governments, to create fear and use it to grab control.
3180880	3187880	And I think, and so if people are scared enough, they will look for safety and they will hand the power to something.
3187880	3190880	And I think this is exactly the opposite of what we want to do.
3190880	3194880	We do not want a centralized place that has the power.
3194880	3197880	And if we are worried about AI, we say we got to regulate it.
3197880	3200880	Maybe they can only have certain goals.
3200880	3202880	The government is going to control those goals.
3202880	3203880	This is exactly the opposite.
3203880	3211880	You've centralized the power and it will make it easier for it to be seized and to have the bad outcome.
3211880	3212880	This is the opposite.
3212880	3219880	This is why there's one reason why it's important to have a counter to the negative, fearful narrative is because it might drive us.
3219880	3220880	It is drive.
3220880	3227880	You see it every day, slot in the Congressional hearings recently with open AI.
3227880	3232880	There is a strong push to give more power to centralized organizations.
3232880	3241880	If we want to have the world to be a complex adaptive system where it's robust and fluid because no one is controlled.
3241880	3246880	And let me just say, I think it's clear that is the situation now.
3246880	3248880	There is no one really in control.
3248880	3252880	There are many countries, first of all, and even the individual countries.
3252880	3254880	They don't have total control.
3254880	3261880	They have to trade off the powers and they have to trade off the power to the economy.
3261880	3263880	It's a complex adaptive system.
3263880	3282880	And this is what I hope will save us and that we want to resist the call of those who are fearful to centralize power in particular institutions.
3282880	3283880	Hi, Rich.
3283880	3284880	My name is Prishan.
3284880	3288880	I come from the industry, so I'm not an AI researcher by any means, but a very intriguing talk.
3288880	3298880	So I have more of a philosophical question on your idea around agent AI and the fact of superior intelligence somewhere down in the future, more intelligent than us as human beings.
3298880	3313880	So my question more is, does that really mean fundamentally that we as intelligent beings can conceive something more intelligent than us when we have yet to see the limits of our own intelligence?
3313880	3324880	I mean, you're building something assuming it could be more intelligent than you when we have yet to understand our own mind as human beings and as what we're doing with the universe.
3324880	3325880	So just curious, right?
3325880	3332880	A system trying to ascertain or create its own copy when itself it doesn't understand who or what he is.
3332880	3334880	So just curious about that.
3334880	3342880	Well, my own thought is that the path to creating superior intelligence runs through understanding our own intelligence.
3342880	3350880	There are at least major parts of it, much more than we understand now.
3350880	3359880	And I should say, I try to be always careful that we say our goal is to create beings that are smarter than us.
3359880	3366880	So I always say our goal is to create or become beings that are smarter than we are now.
3366880	3384880	What we think is a very good chance that the focus will be on making ourselves smarter and more capable using, not only using, but not only using the tools.
3384880	3388880	It's one problem to understand the mind, to understand ourselves.
3388880	3390880	Yeah, that might be wrong.
3390880	3399880	Maybe we'll find a way to just make AI without understanding it.
3399880	3407880	You remember the classic quote from Richard Feynman.
3407880	3414880	That which I cannot create, I don't understand.
3414880	3416880	So creation is required.
3416880	3418880	But the other way is also interesting.
3418880	3423880	Can I create it without understanding it?
3423880	3425880	In some sense we do, right?
3425880	3426880	We make children, okay?
3426880	3436880	We don't understand how they work, but we can make them.
3437880	3439880	We have a Xerox machine, too.
3439880	3444880	We can make pictures without understanding how they're created.
3444880	3453880	I guess we can do it now with AI.
3453880	3454880	Cool.
3454880	3459880	Any other thoughts?
3459880	3464880	Thanks again for your awesome talk and your information.
3464880	3473880	With your open mind venture, it appears that you prefer an open source idea.
3473880	3481880	Would you prefer a fully open source regulation system for AI instead?
3481880	3500880	I'm against centralized organizations.
3500880	3504880	I'm against the World Economic Forum and all those other ones.
3504880	3506880	So I wouldn't want to create another one.
3506880	3509880	I like sharing.
3509880	3512880	I like being open, like open source.
3512880	3522880	I think it's essential if you're going to do fundamental research that will impact over a longer period of time, over five years even.
3522880	3529880	You have to share your ideas just to shape your ideas.
3529880	3532880	To make your ideas better, you've got to have them be challenged by others.
3532880	3534880	You can't keep them secret.
3534880	3536880	You have to publish.
3536880	3545880	It's a bit of a problem that the corporations are becoming a bit more closed and less likely to publish now.
3545880	3551880	Intellectual property, without having a real...
3551880	3556880	I won't claim deep understanding, but I will claim...
3556880	3563880	I have a strong reaction that intellectual property is always a waste of time and always counterproductive.
3563880	3570880	It's costful, it slows things down, and it doesn't really protect in a good way.
3570880	3574880	I'm not interested in having intellectual property at all.
3574880	3577880	Thank you for your question.
3577880	3580880	Hi, at the back.
3580880	3586880	I really appreciate you tying human behavior and evolution to this process as a tool,
3586880	3596880	but man still controls the lever on that and our human behavior will drive us in different directions.
3596880	3602880	I think Hobbes, father of political science, said man moves towards convenience,
3602880	3608880	so we move out of the field into a cave, wrap some fur around you, start a fire, and then get people to work for you.
3609880	3617880	So where does that leave us if we have the tools to do all the things that need to get done?
3617880	3622880	Well, the fur and the cave, those are the early tools.
3622880	3629880	We've always made tools, and the tool AI is a natural continuation of that.
3629880	3637880	And the agent AI is a natural continuation of our trying to understand ourselves more deeply and being open to change.
3637880	3641880	I think it's consistent.
3641880	3644880	Yeah, I want you to feel both things.
3644880	3649880	I want you to feel that AI is bringing dramatic change,
3649880	3659880	and at the same time it's continuing trends and forces that have been present forever.
3659880	3664880	It's a natural next step.
3664880	3668880	Thank you for that question.
3668880	3672880	I think we're going to have to tie it off there.

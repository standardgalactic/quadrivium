WEBVTT

00:00.000 --> 00:14.040
So, as I think you know, I am sort of a dedicated AI researcher, but I'm also a bit of a philosopher

00:14.040 --> 00:15.040
I like to think.

00:15.040 --> 00:20.920
I'm an observer of human nature and human condition, as I think we all are.

00:20.920 --> 00:26.880
And so I appreciate this chance to speak to you all and to sort of share my views about

00:27.880 --> 00:33.880
about AI and about the field and all that's happening, all the growth, all the change,

00:33.880 --> 00:41.880
all the excitement, all the hyperbole and all the fear and the positive and the negative

00:41.880 --> 00:43.880
exaggeration.

00:43.880 --> 00:50.880
And you know, it's sort of hard to get a coherent, consistent, reliable, stable view.

00:50.880 --> 00:52.880
And I just wanted to talk about all these things today.

00:52.880 --> 00:57.880
So today I'm not going to talk about whatever appeared on the schedule.

00:57.880 --> 01:01.880
I'm going to sort of a placeholder.

01:01.880 --> 01:05.880
So I'm going to talk about AI narratives.

01:05.880 --> 01:06.880
Narrative just means a story.

01:06.880 --> 01:12.880
And so the stories of AI, what we tell each other, what we tell ourselves and how to think

01:12.880 --> 01:16.880
about them, which ones of them are accurate, which ones of them are maybe mistaken, how

01:16.880 --> 01:20.880
they may change over time and how the future might work out.

01:20.880 --> 01:26.880
Okay, but let me be clear at the outset that I'm really optimistic and I think the future

01:26.880 --> 01:27.880
will be very, very bright.

01:27.880 --> 01:31.880
I guess that could be a utopia, nothing like that.

01:31.880 --> 01:36.880
Good things and bad things will still happen, but mostly good things.

01:36.880 --> 01:40.880
And AI won't even really change that.

01:40.880 --> 01:42.880
They'll still be good things and bad things.

01:42.880 --> 01:45.880
It may change which good things happen, which bad things happen.

01:45.880 --> 01:49.880
But overall, I think AI will make the world a better place.

01:49.880 --> 01:57.880
More interesting, more prosperous, more dynamic, more challenging in a good way.

01:57.880 --> 02:02.880
Okay, so I got a gizmo.

02:02.880 --> 02:03.880
Let's start.

02:03.880 --> 02:07.880
I want to start with the root of it all, the cause.

02:07.880 --> 02:15.880
And the cause, I think, is Moore's Law, what we locally call Moore's Law, which is just

02:15.880 --> 02:19.880
computation for the last, for the century.

02:19.880 --> 02:27.880
For this century and the coming years, decades, computation has become cheaper and cheaper

02:27.880 --> 02:28.880
and more and more plentiful.

02:28.880 --> 02:30.880
And this is really the story of our age.

02:30.880 --> 02:35.880
This is like the age of computation.

02:35.880 --> 02:40.880
And so let me list this graph.

02:40.880 --> 02:43.880
Yeah, let's look at this graph.

02:43.880 --> 02:51.880
On the, across the bottom there, we've got years and plotted on the y-axis is the computational

02:51.880 --> 02:54.880
power of computers available in that year.

02:54.880 --> 02:55.880
Okay?

02:55.880 --> 02:58.880
Now notice the years span over a century.

02:58.880 --> 03:02.880
They go back to the beginning of 1900.

03:02.880 --> 03:07.880
And the y-axis is a log scale.

03:07.880 --> 03:12.880
So that means each tick up on the y-axis is a factor.

03:12.880 --> 03:19.880
And the small ticks, well, the small, the ticks are two orders of magnitude.

03:19.880 --> 03:23.880
So a factor of 100 for each mark there on the y-axis.

03:23.880 --> 03:30.880
And this is the amazing thing that over this period of time, over roughly a century, computer

03:30.880 --> 03:34.880
power has doubled roughly every two years.

03:34.880 --> 03:35.880
Okay?

03:35.880 --> 03:38.880
A little bit faster recently, maybe now it's like 18 months.

03:38.880 --> 03:45.880
But it's been remarkably constant and it's remarkably consistent over all that time.

03:45.880 --> 03:52.880
Like, can you see, for example, World War II anywhere in that graph?

03:52.880 --> 03:54.880
I would say not at all.

03:54.880 --> 04:03.880
There's no, not a blip because of World War II or World War I or any single technological

04:03.880 --> 04:05.880
event.

04:05.880 --> 04:07.880
It's amazingly consistent.

04:07.880 --> 04:15.880
Over this time, it's over the 80 or 90 years since computers were invented, there's been

04:15.880 --> 04:18.880
an increase of 10 to the 16th.

04:18.880 --> 04:21.880
10 to the 16th?

04:21.880 --> 04:23.880
10 to the 16th.

04:23.880 --> 04:29.880
That's 100,000 million million.

04:29.880 --> 04:30.880
Okay?

04:30.880 --> 04:36.880
It's unimaginable what can happen cumulatively over this kind of time frame when you get

04:36.880 --> 04:38.880
this consistent doubling.

04:38.880 --> 04:41.880
Okay, it just keeps happening.

04:41.880 --> 04:43.880
This is the story of our age.

04:43.880 --> 04:51.880
This is truly the age of computation.

04:51.880 --> 04:55.880
Okay, this is the root of it all, but let's think a little bit more, just a little bit

04:55.880 --> 04:57.880
more analytically.

04:57.880 --> 05:02.880
Is this AI or is this computation?

05:02.880 --> 05:04.880
Or is computation AI?

05:04.880 --> 05:07.880
How do we use these words?

05:07.880 --> 05:13.880
And I think these two things are confounded in our minds and in our language.

05:13.880 --> 05:20.880
Using lots of computation and being related to intelligence.

05:20.880 --> 05:25.880
Now AI, AI as a name, it has cachet.

05:25.880 --> 05:27.880
It's appealing.

05:27.880 --> 05:29.880
And so we're drawn to it.

05:29.880 --> 05:30.880
We want to use it.

05:30.880 --> 05:33.880
We want to call whatever we're doing AI.

05:33.880 --> 05:39.880
So I was in the drug store the other day and I was looking at electric toothbrushes and

05:39.880 --> 05:40.880
what did I find?

05:40.880 --> 05:44.880
But electric toothbrushes are now AI toothbrushes.

05:44.880 --> 05:49.880
So it goes, yeah, you can find your AI toothbrush.

05:49.880 --> 05:54.880
Right there at the top.

05:54.880 --> 05:57.880
This is the way our language evolves.

05:57.880 --> 05:58.880
This is not a new thing.

05:58.880 --> 06:00.880
This has always, always been true.

06:00.880 --> 06:04.880
I remember when I was in graduate school, we didn't even have computers.

06:04.880 --> 06:05.880
We didn't have our own computer.

06:05.880 --> 06:08.880
We had a terminal which connected to the big computer.

06:08.880 --> 06:13.880
And the big thing, exciting thing, was to get an intelligent terminal.

06:13.880 --> 06:17.880
Okay, and they looked like the last slide.

06:17.880 --> 06:19.880
What did I get?

06:19.880 --> 06:21.880
Come on.

06:21.880 --> 06:22.880
There.

06:22.880 --> 06:23.880
That's the intelligent terminal.

06:23.880 --> 06:29.880
And it was intelligent just because it could do multiple fonts, okay, and it could do a

06:29.880 --> 06:32.880
little bit of editing in the computer.

06:32.880 --> 06:34.880
Those were an intelligent computer.

06:34.880 --> 06:42.880
And now, less silly, real things in our day and today, we have AlphaFold.

06:42.880 --> 06:46.880
AlphaFold.

06:46.880 --> 06:51.880
AlphaFold is an amazing program from DeepMind.

06:51.880 --> 06:55.880
And where they've figured out all the proteins in the known demand.

06:55.880 --> 06:58.880
And they figured out the three-dimensional structures from the sequences.

06:58.880 --> 07:00.880
It's really transformed biology.

07:00.880 --> 07:02.880
It's been really super important and super exciting.

07:02.880 --> 07:07.880
And it is exactly the use of computation to do something really, really hard.

07:07.880 --> 07:09.880
Okay.

07:09.880 --> 07:12.880
But I would say it is just, it's really computation.

07:12.880 --> 07:17.880
I mean, they're using neural networks and essentially using machine learning, but they're

07:17.880 --> 07:19.880
not like a person.

07:19.880 --> 07:24.880
They're not like an intelligent system that has a goal.

07:24.880 --> 07:27.880
So, but anyway, we call that AI.

07:27.880 --> 07:33.880
And everyone's kind of good with that, that these massively complex and powerful new tools,

07:33.880 --> 07:35.880
we call them AI.

07:35.880 --> 07:39.880
And, you know, that may be a misnomer, but everyone's doing it.

07:39.880 --> 07:42.880
And, you know, who am I?

07:42.880 --> 07:50.880
I'm not the kind of guy who would like insist on using words differently from other people.

07:50.880 --> 07:55.880
Thank you for that laugh.

07:55.880 --> 07:59.880
Okay.

07:59.880 --> 08:04.880
So anyway, I'm going to say controversial things, but I don't want to just start with a trivial

08:04.880 --> 08:06.880
controversial thing.

08:06.880 --> 08:11.880
Like how we use the words, let's go with the words, let's call this AI.

08:11.880 --> 08:13.880
But let's make the distinction.

08:13.880 --> 08:19.880
Let's make the distinction between these two kinds of AI.

08:19.880 --> 08:25.880
Tool AI, where we're using massive computation, we're doing something hard, that people will

08:25.880 --> 08:29.880
find hard.

08:29.880 --> 08:34.880
But these tools have to be wielded by a person.

08:34.880 --> 08:35.880
They are not autonomous.

08:35.880 --> 08:36.880
They are not complete.

08:36.880 --> 08:38.880
They are not fully powerful on themselves.

08:38.880 --> 08:43.880
They're powerful in conjunction with the person.

08:43.880 --> 08:51.880
And as opposed to agent AI, you know, if tool AI is creating crazy smart tools, then agent

08:51.880 --> 08:58.880
AI is creating crazy smart artificial people, new people.

08:59.880 --> 09:04.880
So these are not only really, you've seen my two dimensions.

09:04.880 --> 09:07.880
One dimension is tool AI versus agent AI.

09:07.880 --> 09:12.880
And this is how I'm going to try to add something to the conversation of how we think what's

09:12.880 --> 09:13.880
our narrative for AI.

09:13.880 --> 09:16.880
First, we're going to divide it into these two parts.

09:16.880 --> 09:22.880
And then we're going to divide it into the positive and the negative stories and exaggerations

09:22.880 --> 09:24.880
around those two parts.

09:24.880 --> 09:25.880
I think it's really helpful.

09:25.880 --> 09:30.880
And I think it's helpful to separate these two because we often do not.

09:30.880 --> 09:31.880
We awfully do not.

09:31.880 --> 09:34.880
We lump them together and that causes us to think in a sloppy way.

09:34.880 --> 09:40.880
So that's maybe my contribution of this talk is to separate them.

09:40.880 --> 09:45.880
So let's firm that up or make that more explicit.

09:45.880 --> 09:50.880
You know, examples of tool AI would be like Alpha Fold and large language models.

09:50.880 --> 09:54.880
They're not full agents, but they are useful tools.

09:54.880 --> 09:58.880
The image generators like Dali and Majorny, they don't have goals.

09:58.880 --> 09:59.880
They don't have an interaction.

09:59.880 --> 10:00.880
They don't have a life really.

10:00.880 --> 10:06.880
They do things and they're wielded by a person as tools.

10:06.880 --> 10:12.880
And on the other side, on the other side, excuse me, we have agent AI or the Alpha goes

10:12.880 --> 10:13.880
and the Alpha zeros.

10:13.880 --> 10:18.880
All those are agent like deep Q learning.

10:18.880 --> 10:24.880
The Atari players GT, Sophie, we heard about many of us heard about the other day, the

10:24.880 --> 10:25.880
race cars.

10:25.880 --> 10:26.880
Those are agents.

10:26.880 --> 10:30.880
And even simple grid world systems can be agents.

10:30.880 --> 10:33.880
So we can make this distinction.

10:33.880 --> 10:37.880
Whether they have clear goals and whether they have experience, whether they have agency.

10:37.880 --> 10:42.880
I don't know if you remember, many of you remember, well, like from the Alberta plan,

10:42.880 --> 10:44.880
you know, we have a picture of the agent.

10:44.880 --> 10:45.880
The agent has certain things.

10:45.880 --> 10:47.880
It has a perception.

10:47.880 --> 10:51.880
It has a reactive policy as a value function as a model of the world.

10:51.880 --> 10:54.880
So all those things, they're not in the tools.

10:54.880 --> 11:01.880
The tools are not about aspiring to create a whole agent.

11:01.880 --> 11:02.880
Okay.

11:02.880 --> 11:06.880
So I'm sure that I think this is a really useful distinction.

11:06.880 --> 11:11.880
Maybe we can find edge cases where it's not so clear, but let's go with it.

11:11.880 --> 11:12.880
Okay.

11:12.880 --> 11:19.880
And so let's do, we'll spend a while doing tool AI.

11:19.880 --> 11:22.880
Tool AI, you know, it has a whole side.

11:22.880 --> 11:24.880
It has a positive and negative part.

11:24.880 --> 11:32.880
So it has a part that's negative, that feels dangerous, that's threatening.

11:32.880 --> 11:37.880
And, but it's less threatening.

11:37.880 --> 11:43.880
You know, it's got things like fake news and deep fakes and election manipulation and loss

11:43.880 --> 11:49.880
of jobs, loss of jobs will come from using these tools or rather changing jobs.

11:49.880 --> 11:53.880
There are all these negative things that we're familiar with, really, we're familiar with

11:53.880 --> 11:58.880
them because they happen every time we have new technology that disrupts old jobs.

11:58.880 --> 12:02.880
And so, so this is, this is, this is negative.

12:02.880 --> 12:05.880
It's threatening, but at the same time, it's kind of familiar.

12:05.880 --> 12:10.880
It feels like it's not a big deal.

12:10.880 --> 12:15.880
We've seen it happen many times before and we know, or at least always in the past,

12:15.880 --> 12:20.880
there are more and better jobs were created to make up for the ones that are lost.

12:20.880 --> 12:21.880
Okay.

12:21.880 --> 12:29.880
But this is, this is, this is like a story that is slightly threatening, but is also

12:29.880 --> 12:30.880
so familiar.

12:30.880 --> 12:33.880
And so it's not, it's not super scary.

12:33.880 --> 12:36.880
It's slightly scary.

12:36.880 --> 12:47.880
You know, the scary stuff comes from the agent side.

12:47.880 --> 12:55.880
Now, the next thing to say is, you know, can with, with this tool stuff, could we end up

12:55.880 --> 12:57.880
with agent stuff?

12:57.880 --> 13:01.880
You know, this is a story that people tell and particularly they tell it for large language

13:01.880 --> 13:02.880
models.

13:02.880 --> 13:03.880
They're getting so smart.

13:03.880 --> 13:07.880
They understand things that they're going to become agents, that they're going to become

13:07.880 --> 13:10.880
powerful and threatening in a way that's beyond the normal.

13:10.880 --> 13:11.880
Okay.

13:11.880 --> 13:16.880
And so I want to, I want to diss that idea a little bit.

13:16.880 --> 13:19.880
And the best way is, is through humor.

13:19.880 --> 13:20.880
Okay.

13:20.880 --> 13:21.880
So I really like this cartoon.

13:21.880 --> 13:27.880
I want to show you this cartoon.

13:27.880 --> 13:32.880
To think this all began with letting autocomplete finish our sentences.

13:32.880 --> 13:33.880
Okay.

13:33.880 --> 13:38.880
So I love this because it's, it has both exaggerations in it.

13:38.880 --> 13:43.880
It has the exaggeration that, oh, if we let, that autocomplete will end up being AI.

13:43.880 --> 13:45.880
We'll end up being agent AI.

13:45.880 --> 13:53.880
We'll end up being powerful things that will dominate us and, and they could dominate and

13:53.880 --> 13:54.880
control us.

13:54.880 --> 13:58.880
And then, so that's really, it's really positive AI.

13:58.880 --> 14:02.880
It's an exaggeration of the abilities of large language models.

14:02.880 --> 14:03.880
Okay.

14:03.880 --> 14:06.880
That's a story that's around us.

14:06.880 --> 14:10.880
And, and it's, it's a, it's an exaggeration of, it's a negative.

14:10.880 --> 14:11.880
It's a fearing thing.

14:11.880 --> 14:16.880
It's a negative exaggeration because it's, oh, if we, if they're able to be agents, then

14:16.880 --> 14:18.880
they will, they will harm us.

14:18.880 --> 14:19.880
They will control us.

14:19.880 --> 14:21.880
They will be our overlords.

14:21.880 --> 14:22.880
Okay.

14:22.880 --> 14:26.880
So now, so this is, this is a cartoon.

14:26.880 --> 14:29.880
It's not really an argument, but it's a good fun aside.

14:29.880 --> 14:35.880
You know, I still do tend to dismiss the idea that we'll have a spontaneous evolution from

14:35.880 --> 14:37.880
large language models into agents.

14:37.880 --> 14:42.880
And mainly I dismiss it because I've spent so much of my life trying to make agents.

14:42.880 --> 14:47.880
And I know it doesn't happen just by chance or by, by throwing in computation.

14:47.880 --> 14:52.880
You have to really organize them and structure them and, and try to make them powerful agents

14:52.880 --> 14:53.880
in order for that to happen.

14:53.880 --> 14:56.880
You know, of course I could be wrong.

14:56.880 --> 15:02.880
Now there's another reason why I doubt it.

15:02.880 --> 15:04.880
Let me, let me just try to explain this.

15:04.880 --> 15:05.880
This is another weak argument.

15:05.880 --> 15:10.880
So, so just take it as sort of an interesting argument and it doesn't apply just to large

15:10.880 --> 15:13.880
language models, which applies to other places we might have seen.

15:13.880 --> 15:18.880
And the argument is that we see a tendency, a psychological tendency of people thinking

15:18.880 --> 15:23.880
about complicated things to grab hold of one thing and then suggest that, well, maybe that's

15:23.880 --> 15:26.880
the one thing and the only important thing.

15:26.880 --> 15:27.880
Maybe I did that today.

15:27.880 --> 15:28.880
Okay.

15:28.880 --> 15:29.880
But I hope not.

15:29.880 --> 15:33.880
I hope I said, this is a super important thing, but you're sure there are other important

15:33.880 --> 15:34.880
things.

15:34.880 --> 15:39.880
But the, the tendency that we sometimes see is that people grab onto one thing and say,

15:39.880 --> 15:40.880
that's, that's the only thing.

15:40.880 --> 15:44.880
So I would just give you some examples of this idea.

15:44.880 --> 15:50.880
I would say this was what happened with, with neural networks and gradient descent.

15:50.880 --> 15:53.880
It was gradient descent is a powerful idea.

15:53.880 --> 15:58.880
And so many people sort of ran with it so hard and they tried to say that maybe that's

15:58.880 --> 15:59.880
all we need.

15:59.880 --> 16:00.880
Okay.

16:00.880 --> 16:05.880
And then they leave, they would leave out, for example, I, the ideas of reinforcement

16:06.880 --> 16:07.880
learning and control.

16:07.880 --> 16:12.880
I think that everything can come out from gradient descent or that they think that everything,

16:12.880 --> 16:18.880
other people got psyched on the idea of prediction as being a very important part of cognition

16:18.880 --> 16:20.880
and, and I, we, I totally agree.

16:20.880 --> 16:24.880
But then they would go too far and they say prediction is the only thing that all cognition

16:24.880 --> 16:26.880
is just, just prediction.

16:26.880 --> 16:29.880
And so this is sort of what we're seeing with the large language models.

16:29.880 --> 16:32.880
It's been really good thing, a powerful thing.

16:32.880 --> 16:33.880
It's known to be powerful.

16:33.880 --> 16:40.880
So many people want then for it to be the only idea that that one idea would give everything.

16:40.880 --> 16:46.880
And so I think that's attempting common flawed way of thinking.

16:46.880 --> 16:51.880
So, and, and the large language models, the hype about large language models.

16:51.880 --> 16:54.880
And let me be clear, large language models are great.

16:54.880 --> 16:59.880
They're, they're, they're going to be extremely useful that they are overhyped.

17:00.880 --> 17:07.880
They are the, the premier example of, of positive exaggeration of the capabilities of current AI systems,

17:07.880 --> 17:08.880
in my opinion.

17:08.880 --> 17:13.880
And I think, and I think they will disappoint a bit, but they will also be extremely useful.

17:13.880 --> 17:22.880
And, and it's going to be good, but let's, let's be reasonable or try to be reasonable.

17:22.880 --> 17:25.880
So, and again, weak argument, right?

17:25.880 --> 17:29.880
I think it's an interesting argument, but it's not definitive.

17:29.880 --> 17:33.880
Let me give you another weak argument.

17:33.880 --> 17:41.880
I think these, these weak arguments help us gain perspective and think about our thoughts about the narratives.

17:41.880 --> 17:47.880
So another weak argument, sociological kind of argument, is that it's well known.

17:47.880 --> 17:53.880
Actually, it's sort of the whole phenomenon of this tremendous excitement around large language models.

17:53.880 --> 17:58.880
It's very familiar to AI researchers, at least old ones like myself.

17:58.880 --> 18:03.880
We remember something called ELISA, you know, and you can find it in Wikipedia.

18:03.880 --> 18:06.880
Wikipedia will have, has the ELISA effect.

18:06.880 --> 18:12.880
The ELISA effect is that if there are systems that generate words and, and appears to speak to us,

18:12.880 --> 18:13.880
we will overinterpret them.

18:13.880 --> 18:19.880
And so this, this came up in the 60s, where they had simple programs that would use language.

18:19.880 --> 18:26.880
And when, when people, even smart people, physicists, scientists interact with them,

18:26.880 --> 18:33.880
they would read a lot of agency and a lot of understanding into the systems.

18:33.880 --> 18:35.880
It's, it's, it's an, it's an effect.

18:35.880 --> 18:37.880
It's reasonable to cause an effect.

18:37.880 --> 18:38.880
It's well known.

18:38.880 --> 18:43.880
It's, it's, it's one, it's part.

18:43.880 --> 18:44.880
It's part.

18:44.880 --> 18:55.880
It's undoubtedly part of why large language models are so, so exciting and have such, such a possibility for hyperbole.

18:55.880 --> 18:56.880
Okay.

18:56.880 --> 19:00.880
So the ELISA effect.

19:00.880 --> 19:02.880
So this is what ELISA looked like.

19:02.880 --> 19:04.880
Remember, it's from the 60s.

19:04.880 --> 19:07.880
And so it has a terminal kind of interface.

19:07.880 --> 19:12.880
And I wanted to read this to you all, but I don't want to turn my back on you.

19:12.880 --> 19:13.880
And I can't read that monitor.

19:13.880 --> 19:16.880
It's too far away.

19:16.880 --> 19:18.880
Maybe I'll, I can't step back.

19:18.880 --> 19:21.880
I'll lose the microphone.

19:21.880 --> 19:22.880
Okay.

19:22.880 --> 19:33.880
Yeah, I like that.

19:33.880 --> 19:34.880
Okay.

19:34.880 --> 19:39.880
So this is an example of conversation with ELISA.

19:39.880 --> 19:42.880
Just give you a sense of what, what, what this is or was.

19:42.880 --> 19:47.880
ELISA was, was, is pretending to be a psychologist, a psychiatrist actually.

19:47.880 --> 19:48.880
Yeah.

19:48.880 --> 19:53.880
So ELISA would say to the person, to the human is something troubling you.

19:53.880 --> 19:56.880
And this person said, men are all alike.

19:56.880 --> 19:58.880
And ELISA, what are, what is it?

19:58.880 --> 20:00.880
What is the connection?

20:00.880 --> 20:02.880
Do you suppose?

20:02.880 --> 20:07.880
And the person says, they're always bugging us about something or other.

20:07.880 --> 20:11.880
And ELISA says, can you think of a specific example?

20:11.880 --> 20:12.880
Okay.

20:12.880 --> 20:16.880
So it seems like ELISA is really understanding what's going on.

20:16.880 --> 20:20.880
But it also seems quite possible that this is a can thing and just something that it

20:20.880 --> 20:21.880
says.

20:21.880 --> 20:26.880
When someone says, like maybe we use the word always, maybe a safe thing to say is, you

20:26.880 --> 20:29.880
know, can you think of a specific example?

20:30.880 --> 20:31.880
Yeah.

20:31.880 --> 20:36.880
The, the, the human says, well, my boyfriend made me come here.

20:36.880 --> 20:44.880
And then ELISA says, is it important to you that your boyfriend made you come here?

20:44.880 --> 20:46.880
You could again, is it understanding?

20:46.880 --> 20:52.880
Or is it just parroting back the phrase that you gave?

20:52.880 --> 20:54.880
It's a really simple program.

20:54.880 --> 20:56.880
This is 1960s.

20:56.880 --> 20:59.880
This is, this is, you know, 1960s.

20:59.880 --> 21:02.880
That's 60 years.

21:02.880 --> 21:09.880
That's a million, million orders, a factor of a million, million, million in computation.

21:09.880 --> 21:11.880
This is a tiny computer.

21:17.880 --> 21:19.880
Anyway, that's what ELISA was like.

21:19.880 --> 21:22.880
And the ELISA effect, I think, is still with us.

21:23.880 --> 21:31.880
It's easy for us to read too much intelligence and understanding into systems that talk.

21:31.880 --> 21:32.880
Okay.

21:32.880 --> 21:33.880
Those are my weak arguments.

21:33.880 --> 21:42.880
Things, things that I think you should understand in, as context and background for these large

21:42.880 --> 21:48.880
language models and how, how to understand and assess their significance and what will

21:48.880 --> 21:50.880
happen with them going forward.

21:50.880 --> 21:57.880
So now let's turn to agent AI.

21:57.880 --> 21:58.880
Okay.

21:58.880 --> 21:59.880
Agent AI.

21:59.880 --> 22:07.880
And so here I want to remind you of a powerful truth, stepping back, a powerful truth that

22:07.880 --> 22:13.880
a genuine understanding of intelligence would be a very big deal for thousands of years,

22:13.880 --> 22:19.880
philosophers and ordinary people alike have wondered about and sought to understand human

22:19.880 --> 22:20.880
intelligence.

22:20.880 --> 22:26.880
Almost every great philosopher since Plato has, has devoted a major part of their work

22:26.880 --> 22:28.880
to the philosophy of mind.

22:28.880 --> 22:36.880
Here's some familiar memes from that, philosophers and their titles.

22:36.880 --> 22:43.880
John Locke wrote an essay concerning human understanding, obviously about intelligence

22:43.880 --> 22:45.880
and mind.

22:45.880 --> 22:51.880
Emanuel Kant wrote the critique for a pure reason and of course, RenÃ© Descartes famously

22:51.880 --> 22:53.880
said, I think, therefore I am.

22:53.880 --> 22:58.880
This is something that really, you know, we talk about it in AI and it's techie and it's

22:58.880 --> 23:03.880
computers, but this, this challenge, the challenge of understanding intelligence, making agent

23:03.880 --> 23:12.880
AI is really this old, old goal of all, of humanities and science.

23:12.880 --> 23:18.880
So, after these, after the philosophers, they came more like scientists and we may recognize

23:18.880 --> 23:20.880
some of their names.

23:20.880 --> 23:25.880
And really, it's said people have always been fascinated by their inner workings.

23:25.880 --> 23:34.880
So Gustav Fechner studies psychophysics with Ebbinghaus and then the psychologists and

23:34.880 --> 23:39.880
learning theorists, animal learning theorists like Paddlov and Thorndike and Skinner and

23:39.880 --> 23:46.880
Tolman and, you know, a different kind of science would, would be people like Jean Piaget

23:46.880 --> 23:52.880
and Sigmund Freud and Carl Jung and Timothy Leary even.

23:52.880 --> 23:57.880
They're all wondering how do our minds work and how can we make them work better?

23:57.880 --> 24:04.880
This is a grand challenge, a great mystery and our interest, our interest in it is not

24:04.880 --> 24:06.880
just narcissism.

24:06.880 --> 24:11.880
I mean, it is kind of narcissism, focusing on themselves and how they work, but it's,

24:11.880 --> 24:18.880
it's appropriate in the sense that intelligence is a, is a great powerful thing.

24:18.880 --> 24:27.880
I think Kurzweil, Ray Kurzweil was not wrong when he said that intelligence is the most

24:27.880 --> 24:33.880
powerful phenomenon in the universe, most powerful phenomenon in the universe, you know,

24:33.880 --> 24:36.880
more powerful than what?

24:36.880 --> 24:46.880
Supernovas, black holes, is that crazy or is that right?

24:46.880 --> 24:53.880
I mean, yeah, black holes, supernova are pretty powerful, but intelligence over time,

24:53.880 --> 25:02.880
if, if, you know, the supernova had maybe, maybe a billion years to develop, give intelligence

25:02.880 --> 25:06.880
a billion years and see what it can do, maybe we'll end up moving the stars around even

25:06.880 --> 25:10.880
more than supernova.

25:10.880 --> 25:24.880
So understanding intelligence is a, is a, is a great challenge.

25:24.880 --> 25:29.880
Understanding intelligence which change all of our lives in many, many ways, too many to,

25:29.880 --> 25:31.880
to even predict.

25:31.880 --> 25:36.880
To understand how to work, how, how, how, how intelligence works is the holy grail of

25:36.880 --> 25:38.880
science and philosophy.

25:38.880 --> 25:44.880
To achieve such an understanding would be perhaps the greatest scientific achievement of any

25:44.880 --> 25:45.880
age.

25:45.880 --> 25:49.880
I guess I have a slide on this.

25:49.880 --> 25:54.880
And I guess I have a, a sort of a quiz to help us get started.

25:54.880 --> 25:58.880
So could AI be such a powerful phenomenon?

25:58.880 --> 26:01.880
I guess I've already told you that I would say yes.

26:01.880 --> 26:05.880
It could be as powerful, the most powerful phenomenon in the universe.

26:05.880 --> 26:09.880
It's some, it's, it's almost plausible.

26:09.880 --> 26:15.880
Then I want you to ask, well, can tool AI be such a powerful phenomenon?

26:15.880 --> 26:16.880
Can, and I would say no.

26:16.880 --> 26:22.880
I would say the powerful phenomenon is the people that are wielding the tools and the

26:22.880 --> 26:27.880
tools themselves are not a powerful, are not powerful.

26:27.880 --> 26:31.880
And so here's the slide.

26:31.880 --> 26:35.880
This is actually slide from, from my talk a year ago where I was trying to make this

26:35.880 --> 26:44.880
point that, that it's such a long standing goal of mankind to understand how we think

26:44.880 --> 26:53.880
and, and improve ourselves and so far as to use technology to create new beings or to

26:53.880 --> 27:01.880
become new beings that are as, as intelligent and powerful as we, as we are now.

27:01.880 --> 27:09.880
So notice, you know, it's being driven still by Moore's law.

27:09.880 --> 27:11.880
It's happening roughly now.

27:11.880 --> 27:16.880
Pursuing the prize is, is, is a great and glorious thing to do.

27:16.880 --> 27:21.880
A shot at the prize is much more important than personal fortune or contribution to the

27:21.880 --> 27:22.880
economy.

27:22.880 --> 27:27.880
Even the economy, all the world's economy is small compared to this kind of transition

27:27.880 --> 27:33.880
in the status of the world, the planet, this portion of the universe.

27:33.880 --> 27:37.880
You know, how, how would we not want to be part of it?

27:37.880 --> 27:41.880
So I'm giving you the positive story for why it's exciting and why this is something we,

27:41.880 --> 27:44.880
we have to do, why it's the natural next step.

27:44.880 --> 27:50.880
But I really want to phrase the question for you and not just, not just present my point

27:50.880 --> 27:51.880
of view.

27:51.880 --> 27:56.880
So this, what is this key question?

27:56.880 --> 27:58.880
The key question basically, is it good?

27:58.880 --> 28:02.880
Is it good?

28:02.880 --> 28:09.880
And as I dwelled on last year, often we don't get to choose these things, but we can still

28:09.880 --> 28:10.880
evaluate them.

28:10.880 --> 28:12.880
We can still say whether it's good or bad.

28:12.880 --> 28:16.880
And that's a very important part of the story or the narrative that we tell.

28:16.880 --> 28:22.880
So let's focus on just that question.

28:22.880 --> 28:27.880
Is creating new people, new people that are smarter than we are now, is this a grand and

28:27.880 --> 28:34.880
glorious prize, natural next step in the human quest, you know, or is it a nightmare bringing

28:34.880 --> 28:36.880
to the end of all we know and love?

28:36.880 --> 28:39.880
You know, basically would it be good or bad?

28:39.880 --> 28:47.880
And I just want you all to recognize that it's hard to assess such a thing.

28:47.880 --> 28:48.880
It's very personal.

28:48.880 --> 28:50.880
It's very subjective.

28:50.880 --> 28:55.880
And I don't want to, I don't really want to answer it today.

28:55.880 --> 29:00.880
I want to phrase the question, make it prominent on your board of things that are happening

29:00.880 --> 29:05.880
in the world of AI, and think about it.

29:05.880 --> 29:08.880
So you can tell I'm on the grand and glorious side.

29:08.880 --> 29:15.880
And others I want to recognize would say, oh, look, that Richard, he's a AI scientist.

29:15.880 --> 29:22.880
He's, he's lusting for the prize and the fame of the glory that he is so explicitly stating.

29:22.880 --> 29:26.880
And I would say to those other people, they're just fearing change.

29:26.880 --> 29:31.880
They're fearing they would lose control of the world, a control that they really don't

29:31.880 --> 29:34.880
even have now.

29:34.880 --> 29:44.880
They're, they're fearing, they've, they're talking themselves in an unwarranted fear.

29:44.880 --> 29:52.880
And it's sort of messianic because if you think you're going to save the world, you may do

29:52.880 --> 29:56.880
things that, that are drastic and, and evil.

29:56.880 --> 30:11.880
So, so these are our, our narratives, the fearful narrative and the, yeah, so the fear,

30:11.880 --> 30:16.880
the fearmonger narrative, I think I'm sad to say, I think it's, it's winning a bit.

30:16.880 --> 30:18.880
I think it's the standard.

30:18.880 --> 30:21.880
Everyone knows, oh AI, danger, potential dangers.

30:21.880 --> 30:25.880
A bunch of potential good things, but we all know it's potential dangers.

30:25.880 --> 30:29.880
Okay, so the first thing I'm trying to, sophistication I'm trying to add to that is to separate the

30:29.880 --> 30:33.880
tool AI, the agent AI.

30:33.880 --> 30:35.880
I'm not trying to say the agent AI isn't dangerous.

30:35.880 --> 30:39.880
Maybe it's, it's the thing that is potentially more dangerous.

30:39.880 --> 30:44.880
But I am trying to say, right now we have one narrative for the agent AI and then that's

30:44.880 --> 30:45.880
that it's dangerous.

30:45.880 --> 30:46.880
We have to control it.

30:46.880 --> 30:50.880
We have to keep it from getting out of control.

30:50.880 --> 30:56.880
And, and so we should have another narrative.

30:56.880 --> 30:58.880
Oh, yeah.

30:58.880 --> 31:04.880
So, so one way to, one way to see this is look at the standard narrative.

31:04.880 --> 31:11.880
The standard in, in the field of AI trial, recognize standard thing to do is to solve the control

31:11.880 --> 31:12.880
problem.

31:12.880 --> 31:14.880
The control problem is how do you control the AIs?

31:14.880 --> 31:17.880
You know, it's like a slave holder saying, how do you control your slaves?

31:17.880 --> 31:18.880
They, they're getting more intelligent.

31:18.880 --> 31:19.880
They're figuring stuff out.

31:19.880 --> 31:22.880
How do I make sure that they're never out of control?

31:22.880 --> 31:31.880
And the standard field doesn't ask, you know, if that slave slave and slave holder point

31:31.880 --> 31:32.880
of view is appropriate.

31:32.880 --> 31:36.880
It doesn't ask if, if people would use their slaves against each other.

31:36.880 --> 31:43.880
If you have, if you, and thus that would be a problem.

31:43.880 --> 31:52.880
It doesn't ask, yeah, whether this is moral.

31:52.880 --> 31:53.880
Okay.

31:53.880 --> 31:58.880
So also consider that the AI safety point of view tries to solve the alignment problem,

31:58.880 --> 32:03.880
which is seizing control of all the agents in the world and ensuring that they're all,

32:03.880 --> 32:06.880
that their goals are aligned with those of people.

32:06.880 --> 32:11.880
And it doesn't ask, you know, which people, because people do have different goals and

32:11.880 --> 32:15.880
it doesn't ask, you know, how is that going to be enforced and how kind of a world you'd

32:15.880 --> 32:22.880
have if no one can make an AI that wasn't, that wasn't approved by, by something who

32:22.880 --> 32:24.880
would do, who would do that.

32:24.880 --> 32:28.880
And then wouldn't this kind of freeze things in place?

32:28.880 --> 32:31.880
How could, how could our, our values evolve?

32:31.880 --> 32:33.880
Because our values are not perfect.

32:33.880 --> 32:38.880
The world of people has many flaws.

32:38.880 --> 32:41.880
We need it to be challenged and to continue to evolve.

32:41.880 --> 32:45.880
So the fact that we don't ask these kind of difficult questions, this, this is what,

32:45.880 --> 32:50.880
this is my evidence really, that the fearmonger narrative is winning because we just,

32:50.880 --> 32:56.880
we just stop at, at, at thinking we have to control it, we have to align it.

32:56.880 --> 33:03.880
And we don't go on to see the, the obvious challenges to that way of thinking.

33:03.880 --> 33:04.880
Okay.

33:04.880 --> 33:10.880
So I'm going to reflect a little bit more.

33:10.880 --> 33:13.880
Good.

33:13.880 --> 33:15.880
So where does this fear come from?

33:15.880 --> 33:17.880
Why are we so fearful?

33:17.880 --> 33:24.880
Why are we kind of viscerally fearful of, of the potential of strong AI, agent AI?

33:24.880 --> 33:28.880
And I think it's really in our DNA.

33:28.880 --> 33:33.880
It's in our, it's in our instinctual history.

33:33.880 --> 33:38.880
And I call it the, the fear of the other tribe.

33:38.880 --> 33:47.880
So, you know, many times in humankind's history, we've, different people haven't come in contact with each other.

33:47.880 --> 33:52.880
And sometimes it's peaceful, there's trade, intermarriage.

33:52.880 --> 33:53.880
Sometimes it's violent.

33:53.880 --> 33:57.880
I would say more often it's violent, different, different, really, genuinely different people.

33:57.880 --> 34:03.880
And one group or the other dominates, dominates and kills and slaves.

34:03.880 --> 34:08.880
This, this is in our genetic history, both as the dominator and the dominated.

34:08.880 --> 34:15.880
These attitudes are these, these are still part of our attitudes towards others.

34:15.880 --> 34:23.880
You know, I think now, particularly in Canada, we try to be more open-minded and, and embrace the differences.

34:23.880 --> 34:26.880
What's different and good in others.

34:26.880 --> 34:40.880
We try to be welcoming and not fearful, but the, the fear-monger narrative of AI builds on our instinctual fear of the other tribe, the other people that are strong.

34:40.880 --> 34:49.880
And, and so we know from our human history that it's their major advantages if we can overcome that and work together and collaborate.

34:49.880 --> 34:55.880
Okay, so now I want to paint a picture, a more positive picture.

34:55.880 --> 35:00.880
Just sort of counterbalance the fearful narrative.

35:00.880 --> 35:08.880
So the pause, the hopeful picture is that the AIs are not alien things to be controlled,

35:08.880 --> 35:13.880
but they're our allies and our offspring, they are of us rather than against us.

35:13.880 --> 35:22.880
And as such, we don't try to control them tightly out of fear, but rather we appreciate the differences.

35:22.880 --> 35:31.880
We work with them and we try to align our society so that we all see it as beneficial to work together.

35:31.880 --> 35:37.880
Including, and we include the possibility that they might teach us something fundamental.

35:37.880 --> 35:44.880
Just as we ask, we expect that our children, as they grow up, might teach us something fundamental.

35:44.880 --> 35:50.880
We don't just train our children, we also learn from them.

35:50.880 --> 35:56.880
So we don't ask, in the hopeful narrative, we don't ask how can we control the AI's goals,

35:56.880 --> 36:03.880
but we rather ask how can we arrange society so that they will want to work together even though there are different goals.

36:03.880 --> 36:06.880
Notice that this is already how we do it with people.

36:06.880 --> 36:11.880
People have many different goals, sometimes we say, oh, human values.

36:11.880 --> 36:16.880
But really, everyone has different goals, right?

36:16.880 --> 36:21.880
Your food is not my food, we have different stomachs, we have different families and different bank accounts.

36:21.880 --> 36:23.880
So it's really not that our goals are the same.

36:23.880 --> 36:28.880
It means our goals in some sense are symmetric, but they are, in a real sense, opposed.

36:28.880 --> 36:34.880
And it's only because we've arranged society so that the outcome is good.

36:34.880 --> 36:39.880
Now, some of you may know about reinforcement learning, we have a direct map of this.

36:39.880 --> 36:42.880
So every agent has its own reward signal.

36:42.880 --> 36:49.880
Those are its goals, they must be like its food and its pain.

36:49.880 --> 36:56.880
And agents kind of would have normally different rewards.

36:57.880 --> 37:00.880
But their values, their values are predictions of reward.

37:00.880 --> 37:08.880
And the technical term value in reinforcement learning, it means, again, prediction of reward.

37:08.880 --> 37:18.880
And in a hopeful narrative, the values become aligned.

37:19.880 --> 37:28.880
So even though I might want my food, I would also like a principle that says we respect each other's property.

37:28.880 --> 37:32.880
And we don't try to beat each other up or take things from each other.

37:32.880 --> 37:34.880
And this is really the way our world works.

37:34.880 --> 37:40.880
Our world works by we've set up laws and mores so that we produce a certain,

37:40.880 --> 37:43.880
so that a certain kind of behavior is rational.

37:43.880 --> 37:51.880
Everyone gets the most reward by behaving appropriately and leaving space for others.

37:51.880 --> 38:03.880
Okay, another point to be aware of, it's in the fearful narrative, which is,

38:03.880 --> 38:05.880
is there one AI or is there many?

38:05.880 --> 38:09.880
The most obvious fearful narrative is that they will become one super AI

38:09.880 --> 38:13.880
which will quickly become much more powerful than everything,

38:13.880 --> 38:16.880
than all the people and all the other AIs and take over.

38:16.880 --> 38:21.880
And then we'll have a singleton, a single agent that controls our world.

38:26.880 --> 38:31.880
So this relies on the idea that it's fast.

38:31.880 --> 38:38.880
If the takeoff is not fast, then it makes sense

38:39.880 --> 38:45.880
to connect it to the last bit, we can't have peace if there's one strongman

38:45.880 --> 38:48.880
that can dominate everyone else.

38:48.880 --> 38:52.880
It's because no one is totally in control, everyone has to share power.

38:52.880 --> 38:57.880
That's how you get peace and that's how you get aligned values in human societies.

38:57.880 --> 39:02.880
And the same would be true amongst AIs.

39:03.880 --> 39:10.880
And I can't avoid, I can't resist mentioning the idea of the complex adaptive system.

39:10.880 --> 39:13.880
My view is that the world is not something that's in control.

39:13.880 --> 39:17.880
It's something that is a complex adaptive system.

39:17.880 --> 39:19.880
It's decentralized, many, many parts.

39:19.880 --> 39:21.880
They have to share power with each other.

39:21.880 --> 39:28.880
And this is where it gets much of its robustness and its ability to shift and be dynamic,

39:28.880 --> 39:34.880
to change, to go one way or another, to find the way of being which is most successful in the universe.

39:37.880 --> 39:43.880
So I think if we think, part of the hopeful narrative is to view the world as a complex adaptive system.

39:43.880 --> 39:45.880
It would be very difficult to take it over.

39:45.880 --> 39:51.880
And those who might try would actually end up poorly and therefore it will be irrational to try.

39:51.880 --> 39:57.880
And a super intelligent rational agent will be seeking to work

39:57.880 --> 40:01.880
with us and with the other AIs rather than trying to take over.

40:05.880 --> 40:09.880
Okay, well I think that's about all I really want to offer.

40:09.880 --> 40:17.880
Let's recognize that people's attitudes towards these questions will not change very rapidly.

40:17.880 --> 40:22.880
It's enough just to bring up the questions, to realize that there is a standard narrative.

40:23.880 --> 40:25.880
And it's not the only narrative.

40:25.880 --> 40:27.880
It may be leading us to a bad place.

40:27.880 --> 40:32.880
I urge each of you not to rush to join or to assume or to adopt,

40:32.880 --> 40:38.880
or to assume it's natural and inevitable that there will be this fearful narrative.

40:42.880 --> 40:48.880
So to summarize about the stories of AI, there's two dimensions.

40:48.880 --> 40:54.880
Well first, it's this is the century of computation and then there are two driven by Moore's law

40:54.880 --> 40:59.880
and the two dimensions of tool AI and agent AI, each one has positive and negative hype.

41:03.880 --> 41:06.880
Each one has a risk reward profile.

41:06.880 --> 41:12.880
The risk reward profile of tool AI is sort of familiar and manageable

41:12.880 --> 41:19.880
and it's often conflated with the risk reward profile of agent AI which is of a higher variance.

41:22.880 --> 41:24.880
So which narrative prevails?

41:24.880 --> 41:26.880
Which meme becomes popular?

41:26.880 --> 41:29.880
I think it's really important for the story of AI ultimately.

41:30.880 --> 41:38.880
And who sets us up?

41:38.880 --> 41:40.880
Well we do, you all do.

41:40.880 --> 41:47.880
We set the narratives, we spread the memes, it's something we should feel responsible for

41:47.880 --> 41:54.880
and really this is a really important part of the story of AI, the final story of AI.

41:54.880 --> 42:03.880
Okay, one more thing.

42:03.880 --> 42:09.880
One more thing which is I want to tell you about my personal story.

42:09.880 --> 42:17.880
As you know the Edmonton office of Deep Mind where I worked for many years, it was closed down this January

42:17.880 --> 42:19.880
and that was a blow.

42:19.880 --> 42:25.880
I think it was worse actually for Deep Mind than it was for the Albertans like myself.

42:25.880 --> 42:30.880
All of us have gone on to other excellent opportunities, mostly still in Edmonton.

42:30.880 --> 42:34.880
There are new startups, some have gone back to the university.

42:34.880 --> 42:43.880
A large continued has joined Sony AI, Peter Stone told us about yesterday in Sony AI in Edmonton.

42:44.880 --> 42:52.880
And the Sony AI in Edmonton is the largest piece of Sony AI in the world.

42:52.880 --> 43:04.880
Arguably it may be now the biggest AI reinforcement learning research effort in the world.

43:04.880 --> 43:10.880
But today I want to announce my own plans even though they're preliminary.

43:10.880 --> 43:17.880
So I want to announce something we call Open Mind Research.

43:17.880 --> 43:25.880
Open Mind Research is a non-profit organization dedicated, focused on implementing,

43:25.880 --> 43:33.880
executing, developing the Alberta plan for AI research that I told some of you about yesterday

43:33.880 --> 43:36.880
or on Wednesday and that has written up.

43:36.880 --> 43:40.880
Anyway, that's our clear, focused research plan.

43:46.880 --> 43:58.880
Open, oh so, yeah, it's a network and I will be staying in Edmonton to develop it.

43:58.880 --> 44:01.880
And I hope you will clap for that.

44:06.880 --> 44:22.880
Also in Open Mind Research is Joseph Modial and Melanie Marvin.

44:22.880 --> 44:24.880
I think Joseph is here.

44:24.880 --> 44:27.880
Yeah, good.

44:27.880 --> 44:37.880
It's an interesting world and how to structure ourselves within it,

44:37.880 --> 44:43.880
whether to make a startup company and try to make an infinite amount of money from the rise of AI.

44:43.880 --> 44:48.880
But we think it's more important to work on the research.

44:48.880 --> 44:54.880
So we're structuring the organization as a network of researchers.

44:54.880 --> 44:56.880
We're going to be funded by donors.

44:56.880 --> 44:58.880
We will not have intellectual property.

44:58.880 --> 44:59.880
We'll be totally open.

44:59.880 --> 45:04.880
Our main adjectives are open, focused, and lean.

45:04.880 --> 45:12.880
We want something that will be able to last a while so we can figure out the key things and bring AI forward over the next decade.

45:12.880 --> 45:17.880
So Open Mind Research will be a foundation supported by donors.

45:17.880 --> 45:24.880
And we are now looking for donors, particularly two donors.

45:24.880 --> 45:26.880
And one we'd like to be from Alberta.

45:26.880 --> 45:31.880
If you know of anyone or if you might be interested in yourself, please ask them to contact me.

45:31.880 --> 45:41.880
And I'm just really excited to be laser focused on the prize of achieving agent AI as I suggested here.

45:41.880 --> 45:44.880
Thank you very much for your attention.

45:54.880 --> 46:05.880
And I'm happy to, I'm excited actually to get questions or comments on the narratives of AI.

46:05.880 --> 46:08.880
Just raise your hand and you'll get a mic.

46:08.880 --> 46:18.880
Yeah, raise your hand and the mic will come to you.

46:18.880 --> 46:22.880
Hey Rich, fantastic talk and thanks for the shout outs.

46:22.880 --> 46:24.880
Thanks Peter.

46:24.880 --> 46:35.880
So I completely agree with you that there's these two potential focuses, FOSI I suppose, for the AI narrative.

46:35.880 --> 46:43.880
And the AI agent one sort of captures the imagination.

46:43.880 --> 46:49.880
But some would argue is still far in the future and we don't yet know what it will look like.

46:49.880 --> 47:00.880
The AI tool one is clearly here and you seem to say just don't worry about it, it'll be fine.

47:00.880 --> 47:08.880
And because it always has been and I agree with you, I'm on your side here.

47:08.880 --> 47:14.880
I think AI tools will make the world a better place.

47:14.880 --> 47:16.880
Let me pause you.

47:16.880 --> 47:24.880
Thank you for saying that because I really don't want that to be the message.

47:24.880 --> 47:35.880
Both are really important and exciting and large language models, tool AI, alpha fold, these are all great things.

47:35.880 --> 47:43.880
They have transformed biology, they've transformed writing and programming.

47:43.880 --> 47:49.880
And I don't really want to say in any way that they're bad.

47:49.880 --> 47:55.880
They're all good, just a little bit hyped and they may disappoint.

47:55.880 --> 48:00.880
I don't want that to be a negative thing.

48:00.880 --> 48:05.880
Let's acknowledge that they're probably going to disappoint a little bit because there's so much of being claimed for them.

48:05.880 --> 48:09.880
And let's not let the disappointment that happens kill them.

48:09.880 --> 48:13.880
It's not going to kill alpha fold and it shouldn't kill large language models either.

48:13.880 --> 48:18.880
These are great things, they're a great part of the broad community of AI.

48:18.880 --> 48:23.880
And yeah, I am more interested in one, but that's fine.

48:23.880 --> 48:25.880
They're both great.

48:25.880 --> 48:27.880
They really are, I really believe that.

48:27.880 --> 48:30.880
So I hear you saying that and that's what I thought you were saying too.

48:30.880 --> 48:36.880
I guess my comment is more the, and maybe you just answered the way you would want to answer what I'm asking,

48:36.880 --> 48:44.880
but it seems like, I'm asking for your help answering the questions of people who are saying,

48:44.880 --> 48:47.880
oh, it's these AI tools that are going to be bad.

48:47.880 --> 48:53.880
Some would say that you were sort of glib in your dismissal of that as it's, oh, it's going to be like it was in the past.

48:53.880 --> 48:54.880
And I get that.

48:54.880 --> 48:55.880
People say that to me.

48:55.880 --> 48:58.880
I say, oh, it's usually been good.

48:58.880 --> 49:00.880
Most technologies have good and bad.

49:00.880 --> 49:02.880
They've generally been on the good side.

49:02.880 --> 49:04.880
But then people push back.

49:04.880 --> 49:06.880
Maybe this time is going to be different.

49:06.880 --> 49:10.880
Do you have like sort of, you know, can you help us construct deeper arguments against those?

49:10.880 --> 49:15.880
The naysayers who are naysayers because of the AI tools.

49:15.880 --> 49:20.880
So very, very good.

49:20.880 --> 49:29.880
The key here, I intend to be answered by the separation into tool AI and agent AI.

49:30.880 --> 49:38.880
I think tool AI, it will, it really will be as challenging as in the past and no more.

49:38.880 --> 49:41.880
This time it will not be different as far as tool AI.

49:41.880 --> 49:47.880
But the part where it's realistic to say this time it will be different is when we talk about agent AI.

49:47.880 --> 49:52.880
It's when we're really talking about replacing people and capturing the capabilities of people.

49:52.880 --> 49:56.880
It's on the agent AI that things may be different.

50:00.880 --> 50:03.880
So to push just a little back.

50:03.880 --> 50:13.880
So the Model T car was invented in 1908 and it took 50 years roughly to get to 100 million cars in the world.

50:13.880 --> 50:22.880
And during that time we figured out things like seatbelts and airbags and traffic signals and highway networks and insurance and regulation and all this kind of stuff.

50:22.880 --> 50:27.880
ChatGPT got to 100 million users in one month and there wasn't that time.

50:27.880 --> 50:32.880
So how is it that this is, why should we say it's just going to be the same?

50:32.880 --> 50:41.880
It seems faster and possibly, I'm being devil's advocate here a little bit, but it seems possibly different.

50:41.880 --> 50:46.880
Why can you just say, how can you be confident that it's not different this time?

50:47.880 --> 50:55.880
It's a bit broad brush and it is like super important.

50:55.880 --> 51:02.880
But the other, the agent AI is even more important.

51:02.880 --> 51:09.880
I mean it goes back thousands, millions of years into our history and our intellectual traditions.

51:10.880 --> 51:15.880
Yeah, to me it's a qualitative difference.

51:15.880 --> 51:25.880
And so I don't, it's not to dismiss it, it's not to say a normal amount needs to be done.

51:25.880 --> 51:30.880
Yeah, I just don't see why it's going to be different.

51:30.880 --> 51:36.880
As long as you stay on the tool side, people are still in control, people are still wielding the tools.

51:37.880 --> 51:42.880
That's the normal technological change in disruption.

51:42.880 --> 51:47.880
Okay, we'll let someone else.

51:47.880 --> 51:51.880
Thank you for your great talk.

51:51.880 --> 51:53.880
My question is about the government.

51:53.880 --> 52:05.880
I know most of the fear that we have comes from government because it fears that it's going to lose the power and the central power of the state.

52:05.880 --> 52:09.880
So do you agree that if we compare the AI for atomic bomb?

52:09.880 --> 52:12.880
I don't know if you agree with that or not.

52:12.880 --> 52:19.880
Do you agree that we should have a government based organization to control the AI or not?

52:19.880 --> 52:22.880
And the second question is about the research.

52:22.880 --> 52:32.880
For a young research like me, what is the best path do you think for us to focus on based on the agent AI?

52:32.880 --> 52:36.880
Thank you.

52:36.880 --> 52:39.880
Well, the latter part, of course, you know, I've got the Alberta plan.

52:39.880 --> 52:41.880
It lays out a research plan.

52:41.880 --> 52:53.880
But I'm glad you brought up the issue of government because I think the fearmongers are trying to gain control.

52:53.880 --> 53:00.880
It's a usual strategy of all, not only governments, but particularly governments, to create fear and use it to grab control.

53:00.880 --> 53:07.880
And I think, and so if people are scared enough, they will look for safety and they will hand the power to something.

53:07.880 --> 53:10.880
And I think this is exactly the opposite of what we want to do.

53:10.880 --> 53:14.880
We do not want a centralized place that has the power.

53:14.880 --> 53:17.880
And if we are worried about AI, we say we got to regulate it.

53:17.880 --> 53:20.880
Maybe they can only have certain goals.

53:20.880 --> 53:22.880
The government is going to control those goals.

53:22.880 --> 53:23.880
This is exactly the opposite.

53:23.880 --> 53:31.880
You've centralized the power and it will make it easier for it to be seized and to have the bad outcome.

53:31.880 --> 53:32.880
This is the opposite.

53:32.880 --> 53:39.880
This is why there's one reason why it's important to have a counter to the negative, fearful narrative is because it might drive us.

53:39.880 --> 53:40.880
It is drive.

53:40.880 --> 53:47.880
You see it every day, slot in the Congressional hearings recently with open AI.

53:47.880 --> 53:52.880
There is a strong push to give more power to centralized organizations.

53:52.880 --> 54:01.880
If we want to have the world to be a complex adaptive system where it's robust and fluid because no one is controlled.

54:01.880 --> 54:06.880
And let me just say, I think it's clear that is the situation now.

54:06.880 --> 54:08.880
There is no one really in control.

54:08.880 --> 54:12.880
There are many countries, first of all, and even the individual countries.

54:12.880 --> 54:14.880
They don't have total control.

54:14.880 --> 54:21.880
They have to trade off the powers and they have to trade off the power to the economy.

54:21.880 --> 54:23.880
It's a complex adaptive system.

54:23.880 --> 54:42.880
And this is what I hope will save us and that we want to resist the call of those who are fearful to centralize power in particular institutions.

54:42.880 --> 54:43.880
Hi, Rich.

54:43.880 --> 54:44.880
My name is Prishan.

54:44.880 --> 54:48.880
I come from the industry, so I'm not an AI researcher by any means, but a very intriguing talk.

54:48.880 --> 54:58.880
So I have more of a philosophical question on your idea around agent AI and the fact of superior intelligence somewhere down in the future, more intelligent than us as human beings.

54:58.880 --> 55:13.880
So my question more is, does that really mean fundamentally that we as intelligent beings can conceive something more intelligent than us when we have yet to see the limits of our own intelligence?

55:13.880 --> 55:24.880
I mean, you're building something assuming it could be more intelligent than you when we have yet to understand our own mind as human beings and as what we're doing with the universe.

55:24.880 --> 55:25.880
So just curious, right?

55:25.880 --> 55:32.880
A system trying to ascertain or create its own copy when itself it doesn't understand who or what he is.

55:32.880 --> 55:34.880
So just curious about that.

55:34.880 --> 55:42.880
Well, my own thought is that the path to creating superior intelligence runs through understanding our own intelligence.

55:42.880 --> 55:50.880
There are at least major parts of it, much more than we understand now.

55:50.880 --> 55:59.880
And I should say, I try to be always careful that we say our goal is to create beings that are smarter than us.

55:59.880 --> 56:06.880
So I always say our goal is to create or become beings that are smarter than we are now.

56:06.880 --> 56:24.880
What we think is a very good chance that the focus will be on making ourselves smarter and more capable using, not only using, but not only using the tools.

56:24.880 --> 56:28.880
It's one problem to understand the mind, to understand ourselves.

56:28.880 --> 56:30.880
Yeah, that might be wrong.

56:30.880 --> 56:39.880
Maybe we'll find a way to just make AI without understanding it.

56:39.880 --> 56:47.880
You remember the classic quote from Richard Feynman.

56:47.880 --> 56:54.880
That which I cannot create, I don't understand.

56:54.880 --> 56:56.880
So creation is required.

56:56.880 --> 56:58.880
But the other way is also interesting.

56:58.880 --> 57:03.880
Can I create it without understanding it?

57:03.880 --> 57:05.880
In some sense we do, right?

57:05.880 --> 57:06.880
We make children, okay?

57:06.880 --> 57:16.880
We don't understand how they work, but we can make them.

57:17.880 --> 57:19.880
We have a Xerox machine, too.

57:19.880 --> 57:24.880
We can make pictures without understanding how they're created.

57:24.880 --> 57:33.880
I guess we can do it now with AI.

57:33.880 --> 57:34.880
Cool.

57:34.880 --> 57:39.880
Any other thoughts?

57:39.880 --> 57:44.880
Thanks again for your awesome talk and your information.

57:44.880 --> 57:53.880
With your open mind venture, it appears that you prefer an open source idea.

57:53.880 --> 58:01.880
Would you prefer a fully open source regulation system for AI instead?

58:01.880 --> 58:20.880
I'm against centralized organizations.

58:20.880 --> 58:24.880
I'm against the World Economic Forum and all those other ones.

58:24.880 --> 58:26.880
So I wouldn't want to create another one.

58:26.880 --> 58:29.880
I like sharing.

58:29.880 --> 58:32.880
I like being open, like open source.

58:32.880 --> 58:42.880
I think it's essential if you're going to do fundamental research that will impact over a longer period of time, over five years even.

58:42.880 --> 58:49.880
You have to share your ideas just to shape your ideas.

58:49.880 --> 58:52.880
To make your ideas better, you've got to have them be challenged by others.

58:52.880 --> 58:54.880
You can't keep them secret.

58:54.880 --> 58:56.880
You have to publish.

58:56.880 --> 59:05.880
It's a bit of a problem that the corporations are becoming a bit more closed and less likely to publish now.

59:05.880 --> 59:11.880
Intellectual property, without having a real...

59:11.880 --> 59:16.880
I won't claim deep understanding, but I will claim...

59:16.880 --> 59:23.880
I have a strong reaction that intellectual property is always a waste of time and always counterproductive.

59:23.880 --> 59:30.880
It's costful, it slows things down, and it doesn't really protect in a good way.

59:30.880 --> 59:34.880
I'm not interested in having intellectual property at all.

59:34.880 --> 59:37.880
Thank you for your question.

59:37.880 --> 59:40.880
Hi, at the back.

59:40.880 --> 59:46.880
I really appreciate you tying human behavior and evolution to this process as a tool,

59:46.880 --> 59:56.880
but man still controls the lever on that and our human behavior will drive us in different directions.

59:56.880 --> 01:00:02.880
I think Hobbes, father of political science, said man moves towards convenience,

01:00:02.880 --> 01:00:08.880
so we move out of the field into a cave, wrap some fur around you, start a fire, and then get people to work for you.

01:00:09.880 --> 01:00:17.880
So where does that leave us if we have the tools to do all the things that need to get done?

01:00:17.880 --> 01:00:22.880
Well, the fur and the cave, those are the early tools.

01:00:22.880 --> 01:00:29.880
We've always made tools, and the tool AI is a natural continuation of that.

01:00:29.880 --> 01:00:37.880
And the agent AI is a natural continuation of our trying to understand ourselves more deeply and being open to change.

01:00:37.880 --> 01:00:41.880
I think it's consistent.

01:00:41.880 --> 01:00:44.880
Yeah, I want you to feel both things.

01:00:44.880 --> 01:00:49.880
I want you to feel that AI is bringing dramatic change,

01:00:49.880 --> 01:00:59.880
and at the same time it's continuing trends and forces that have been present forever.

01:00:59.880 --> 01:01:04.880
It's a natural next step.

01:01:04.880 --> 01:01:08.880
Thank you for that question.

01:01:08.880 --> 01:01:12.880
I think we're going to have to tie it off there.


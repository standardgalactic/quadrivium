My name is Nikola and you're watching Singularity FM, the place where we interview the future.
If you guys enjoyed this podcast, you can show your support by either writing a brief
review on iTunes or by simply making a donation.
Today, my guest on the show is Professor Joshua Bach.
Joshua is a cognitive scientist at the Harvard Program for Evolutionary Dynamics as well
as the MIT Media Lab.
So welcome to Singularity FM, Joshua.
Hi, first of all, I'm not a professor, though, and I have been working at the MIT Media
Lab until three years ago, and since then, I'm at Harvard.
I'm not affiliated at both at this time.
Oh, so what exactly is your position then?
I'm a research scientist.
The research scientist is a person that works in the abyss between postdoc and tenure.
Wow, but you are a PhD.
I am.
And what was your PhD in then?
In cognitive science.
I went into academia to understand how the mind works and so studied a number of subjects
and did degrees in computer science and philosophy and felt that AI is my best bet of making
headway in understanding who we are.
Fantastic.
So, you are, as you said, in the zero gravity sort of space between postdoc and professorship?
Academia?
Yeah, it's actually a quite happy place.
It's one that allows me to do what I want.
I don't have to do superfluous management.
I don't have to sit in many committees or anything.
I can teach when I want, but I don't have to, which is really the best arrangement you
can possibly have.
I can have students, but I don't have to.
And so this is kind of amazing.
That's fantastic, but I don't know how, if it's allowing you to survive with your family
properly.
But we find out, you know, there can always be earthquakes.
Okay, Joshua, if I were to ask you to introduce yourself in a couple of words, who is Joshua
Bach?
It depends who's asking, but in the most general sense, I'm a cognitive scientist.
I grew up in communist Eastern Germany, the last generation to do so, as the child of
an artist in the forest.
And I grew up in a world that was completely alien to me in many ways, because communist
Eastern Germany didn't make a lot of sense, especially if you grow up in a forest in which
everything has no rules and only the rules that locally make sense.
So anyway, my default in understanding the world has been different from the default
of people around me.
I reluctantly discovered that most people formed their ideas by taking in the norms
of their environment and the statements of the experts and taking them as gospel and
only revise them when they absolutely have to and they're disproven.
And for me, it was always like the opposite.
You have this perspective on the world where people have ideas and their thoughts and they
often make no sense.
And you will have to look at each of them with great care before you incorporate them
into your own world model.
So you try to be careful to not harm anything or do bad things to the world.
But this reluctance in accepting what comes from the outside has, I think, shaped my scientific
perspective.
And when I came into the next society, Western Germany, and then later on to New Zealand and
to the US, I always saw things from the outside.
So I'm more an observer and the same thing happens in the scientific fields.
Wow, that's absolutely fascinating.
And I want to grab a few points there one by one.
But first of all, that kind of a skeptical outsider kind of point of view is very contrarian
and also very sort of philosophical in the way, maybe in the German school, because at
least Nietzsche said that gross answers are a prohibition against thinkers.
You shall not think.
And to him, that was like an insult because he was curious, questioning inquisitive kind
of a soul from the beginning.
And so he was never one for gross answers, but rather asking questions and questioning
everything.
So it seems you've kind of you've got that approach.
I felt that Nietzsche never made peace with society.
That was related to the fact that he was never able to make peace with himself.
It really never worked out.
There is a big issue with obedience.
This question, should you obey somebody else?
I mean, you seem to have that same issue that to work in a hierarchy, you need to submit
in a way to a hierarchy.
How would you submit?
How could somebody else make your decisions if they didn't test it to the same rigorous
epistemological criteria that you did?
Does that have integrity, right?
It's very hard to do.
But from a different perspective, if you want to do the right thing, then doing the right
thing might require that you ask the person that is more likely to make the right decisions
because they're an expert for a local area of making the right things.
Like a leader is a person in specializing that specializes in doing the right thing.
So I think it has integrity to realize that in certain circumstances, other people will
know better than you do and Nietzsche never got to this point.
And of course, philosophy is slightly different because in philosophy, you have to fix your
foundations and arguably invest in philosophy.
Very few people did.
Right?
So our hypothesis still seems to be supernatural beings and dualism and so on.
And that's one of the reasons why most people in the Western world find AI so ridiculous
and unlikely.
It's not because people don't see that we are biological computers and that the universe
is probably mechanical and the theory that everything is mechanical gives extremely
good predictions.
It's because deep down they still have this not hypothesis that the universe is somehow
supernatural and we are the most supernatural thing in it.
And science is only reluctantly pushing back against this not hypothesis.
And since it has not completely obliterated not hypothesis in this single area, the consciousness
in the mind, we are reluctant in accepting this reasonable certainty that we are machines.
This is the main reason why we hesitate so much, I think.
So are we machines then?
Are we as some people have said that organisms are algorithms?
There are a number of definitions on this, but if you think of an algorithm as a set
of rules that can be probabilistic or deterministic and that make it possible to switch between
states and usually we do this in a more narrow sense where we say that the algorithm is being
used to change representational states in order to compute a function, then I would
say that organisms have algorithms in this narrower sense.
But I would say that in the wider sense, they're definitely machines.
Machine is a system that can change the state in non-random ways.
And also, we visit earlier states, which means to stay in a particular state space.
Otherwise, this would not be a system.
A system is something that we can describe by drawing a fence around its state space
and saying, as long as it's in there, this is the system.
So we have an evolution of the system that is someone constrained.
Now, we are jumping headfirst into terminology.
Want to go there?
We would go there, but let me just roll back the tape of time because I want to follow
your narrative, your personal narrative from where it began, then connect it to where you
are today, and then hopefully try and look it to the world and into the future with your
eyes and with your experience and from your point of view.
So tell me, you said you grew up in the forest.
Whereabouts?
In Turingia, near Weimar and Jena, it's an area of German romanticism, which had a pretty
big influence on how I grew up.
It's a very particular shape of the soul that has been characterized by the Enlightenment,
which in a way pushed back against the religious mind fibers that had controlled the world
until then and replaced it with machinery, this rationalist machinery that eventually
made modernist societies possible.
And this was a very big upheaval.
You can still see the ego of this in our modernness, like Lord of the Rings and Star Wars.
You have this pastoral world, which defends itself against the encroaching technological
empire that is going to eat our souls, even though it's going to win.
And so, but did you grow up on a farm or something?
No, my parents were artists.
They were originally architects.
And my father didn't want to build boring things that would put people into boxes and
deny the humanity.
Instead, he built things that didn't have many right angles.
And he made a zoo that had no right angles, for instance, as one of his projects and so
on.
And it was very difficult to get away with these things in Eastern Germany because this
was a very utilitarian society and its architecture was to a large degree brutalist.
So he rejected this and he decided to remove himself from society and make his own kingdom
in the forest.
So he bought an old water mill and changed it into a sculpture garden and lived exactly
the life he wanted and got away with it.
Wow, that's absolutely phenomenal.
I, like you, grew up in the Eastern Bloc only.
I grew up in Bloc area, in Communist Bloc area for the first, what was it, 13, 14 years
of my life, so I can associate it with a lot of your experience.
But it's very interesting how you grew up in Germany as you put it in the forest in
a very artistic family, and yet you became a scientist.
So is there any tension there or is it a continuation of sort of, or did it give you any kind of
different unique point of view or approach to science or is that basically a false dichotomy?
There is a big similarity.
I find that most people serve practical needs.
They have an understanding of the difference between meaning and relevance.
And at some level, my mind is more interested in meaning than relevance.
That is similar to the mind of an artist.
The arts are not life, they're not serving life.
The arts are the cuckoo child of life, because the meaning of life, they are the cuckoo child
of life.
The meaning of life is to eat.
You know, life is evolution, and evolution is about eating.
It's pretty gross if you think about it, right?
Evolution is about getting eaten by monsters.
Don't go into the desert and perish there, because it's going to be a waste.
If you're lucky, the monsters that eat you are your own children.
And eventually the search for evolution will, if evolution reaches its global optimum, it
will be the perfect devourer.
The thing that is able to digest anything and turn it into structure to sustain its
perpetuate itself for as long as the local puddle of negentropy is available.
And in a way, we are yeast.
Everything we do, all the complexity that we create, all the structures we build is
to erect some surfaces on which to out-compete other kinds of yeast.
And if you realize this, you can try to get behind this, and I think the solution to this
is fascism.
Right?
Fascism is a mode of organization of society in which the individual is a cell and a super
organism.
The value of the individual is exactly the contribution to the super organism.
And when the contribution is negative, then the super organism kills it in order to be
fitter in the competition against other super organisms.
And it's totally brutal.
And I don't like fascism because it is going to kill a lot of minds I like.
And the arts is slightly different.
It's a mutation that is arguably not completely adaptive.
It's one where people fall in love with the lost function, where you think that your mental
representation is the intrinsically important thing, where you try to capture a conscious
state for its own sake, because you think that matters.
The true artist, in my view, is somebody who captures conscious states, and that's the
only reason why they eat.
So you eat to make art.
And another person makes art to eat.
And these are, of course, the ends of a spectrum, and the twos is often somewhere in the middle.
But in a way, there is this fundamental distinction.
And there are, in some sense, the true scientists which try to figure out something about the
universe.
They try to reflect it.
And it's an artistic process in a way.
It's an attempt to be a reflection to this universe.
You see, there's this amazing vast darkness, which is the universe.
There's all this iteration of patterns, but mostly there's nothing interesting happening
in these patterns.
It's a giant fractal, and most of it is just boring.
And in a brief moment in the evolution of the universe, there are planetary surfaces
and like entropy gradients that allow for the creation of structure.
And then there are some brief flashes of consciousness in all this vast darkness.
And these brief flashes of consciousness can reflect the universe and maybe even figure
out what it is.
It's the only chance that we have, right?
This is amazing.
And why not do this?
Life is short.
This is the thing that we can do.
And that's why you, going back to your previous point about your current position being sort
of between post-doc and academia, that position actually fits you very well because you're
not forced to do science in order to eat, but actually you can afford to eat as much
as you can do your science.
Is that the case?
I have a similar problem as you, I think.
It's very difficult for me to get myself to do something for which I'm not intrinsically
motivated for.
So you got that right completely.
If I work in a job that is intellectually interesting, but doesn't appear meaningful to me, I will
probably lose interest after four months.
And I have to do something where I think this needs to be done.
This is worth spending some of my short life on.
Right.
I didn't even last four months.
After my undergraduate, before my master's degree, I worked as an investment administrator
in a company, and I lasted six weeks where I was balancing portfolios and doing stock
trades and things like that.
I lasted about five and a half, six weeks, and then it's a debate whether I resigned
or I got fired first.
But either way, I was not surviving there, so or staying there anyway.
Yeah.
There's a tension.
I want to be useful to society and I want to eat suffering and so on.
I do care about people.
It's just that I have the impression that the systems that we live in are often not
sustainable.
They're largely doomed.
It's a very weird situation that we find ourselves in.
If you take a step back, all the important tipping points for climate change have been
the last century, as people said in the last century.
But the fact that we knew about this, that global warming is basically known to our corporations,
to our companies since the 60s and 70s, and to our governments, I think about the same
time, that our inability to deal with this probably means that there was too little agency
in the system to do anything about it, and we probably locked ourselves into this trajectory
with the industrial revolution.
At this point, it was no longer for us to stop the machines that we built.
Well, we are kind of jumping forward, and I want to sort of slow the ball down a little
bit, if I may.
Don't worry, you're not going to get bored.
You can keep that pace.
You just can turn around and go as I'm else.
Fantastic.
The pace is great.
It's just that I had so many considerations already in my previous points that you made
that now I kind of lost the thread completely.
Okay.
Let me see.
We got the artistic and the scientific part of Joshua.
Where does philosophy come about here in this equation, and how?
That's a very awkward question.
The problem is, in my view, that philosophy as a field of inquiry is practically dead.
Misha Gromov once told me, it's a mathematician, that in his perspective, Darwin was the last
philosopher.
He was the last one who was in a position where we could connect some dots in a completely
fresh way.
And after that, there were people like Russell, who were extremely good writers, but didn't
do any real philosophy anymore because there was too little left.
And I'm not quite sure if that is the case.
There is some philosophy that needs to be done, and it's still being done, and it's largely
in mathematics and fixing the foundations.
And even there, it's mostly visible.
So we have two big intellectual traditions, which is mathematics and physics.
And there are some cracks in them that need to be dealt with, and this is where most of
the philosophy is at.
And all the other things are minor, like social organization and so on.
It's very miraculous to the sociologists, but I think we can see the patterns.
This is largely the effect of these fields.
And philosophy as a field is a culture.
Now, you get paid for emulating what a philosopher is supposed to look like, and it's very hard
to get any philosophy done on the side.
And the incentives are all wrong, right?
It's a very fierce battle to become a philosopher, to get from post-doctor tenure in these fields.
So you need to get cited.
And the way you get cited as a philosopher is you identify a hot discussion.
In that hot discussion, you identify a unique position, and you build your brand around
that unique position.
You cannot afford to give this up.
So you have your Chinese room or your unique position in about free will, and you're going
to defend this hill.
Even if this hill is basically indefensible, philosophy is not going to progress in a way
that forces your buildings off that hill.
You can build a mansion on an indefensible hill, and you will still have meetings in
there 200 years from now.
And the bad thing is all the good hills are taken, right?
So this is a very bad situation for philosophers.
And I think this is the reason why I cannot be a philosopher today.
And we need philosophy, but we don't have it anymore in this sense.
But let's define it.
What is philosophy for you?
Because I've interviewed a number of mathematicians and physicians, and they both argue which
one is at the root of everything, whether it's mathematics, whether it's physics, and
so on and so on.
But both of them or all of them mostly agree that philosophy is irrelevant, or so they
make that claim.
And yet you say that philosophy kind of includes both mathematics and physics in a way, which
I actually agree with, but tell me why, and tell me how do you define it in the first
place in a way that actually includes both of those?
I think that philosophy is in a way the search for the global optimum of the modeling function.
So it has fields that have been defined as parts of questions that lead to this modeling
function like epistemology, what can be known, what is the nature of truth and so on, ontology,
what is the stuff that exists, what's going on there, metaphysics.
This is in some sense the systems in which you have to describe things.
And ethics, what should we do?
And at some point we discovered epistemology.
So my view, the first rule of epistemology is roughly discovered by Francis Bacon in
1620.
It says that the strengths of your confidence in a belief must equal the weight of the evidence
and support of it.
And you need to apply this recursively until basically you resolve the priors of every belief
and the belief system becomes self-contained.
To believe stops being a verb.
There's no more relationship to identifications that you just arbitrarily set.
This is just a system that is in itself contained, which means in some sense it's a mathematical
system.
It's a system that describes a certain thing and this leads you to the nature of mathematics.
And mathematics, it turns out, is the domain of all languages, all of them, not just the
natural languages.
And mathematicians have been trying to fix their understanding of the languages and they
noticed what mathematics is in this regard.
And Hilbert stumbled on counters, set theoretic experiments to deal with natural numbers and
then saw that when you go to infinity, very awkward and nasty things happen, your axiomatic
systems basically start blowing up.
And the total set suddenly contains both itself and the set of all of its subsets, so it cannot
have the same number of members as itself.
And he asked mathematicians, please build us an interpreter for mathematics, a mathematics
basically something like a computer made for mathematics, any mathematics you want that
can run all of mathematics.
And then Goedl and Turing came along and showed that this is not possible, that this computer
is going to crash.
And this left mathematics was a big shock and the way mathematics is still reeling from
that shock.
And then Turing in church had another insight and they figured out that all the universal
computers have the same power.
The universal computer is a set of rules that by applying them you can compute all the things
that can be computed.
And the set contains itself.
So universal computer is computable.
As long as your universal computer doesn't run out of resources, it can compute anything
that you can compute and it can also compute all the other universal computers.
So the next thing that they discovered Turing was involved again was that our mind is probably
in the class of the universal computers, not in the class of mathematical systems.
So this is what Penrose doesn't know.
Penrose thinks that our mind is mathematical, that it can do things that a computer cannot
do.
And the big hypothesis of AI in a way is we are in the class of systems that can approximate
computable functions and only those.
And so we cannot do more than a computer, which means that all the mathematics that
we've ever seen and all the mathematics that we will ever see and that will ever matter
is going to be computable.
And the fact that some mathematics is not computable is the problem of the language that
we have been using.
We need computational languages, not mathematical languages.
And it turns out that the main problem is that mathematics, classical mathematics, defines
functions in using infinities, which means infinitely many steps to get to the result.
These functions tend not to be computable.
So if you are a computer programmer, it would never occur to you to write in your spec that
is totally fine if your routine does return the result after infinitely many steps only.
Right?
This is not good.
A finite set of steps and one that you know how long it is, so your customer gets results
in time.
Right?
So in this perspective, should you define numbers in such a way that pi is a number?
You cannot know the last digit of pi.
Pi is a function, clearly, right?
It's a function that gives you as many digits as you can afford.
And in any finite universe, it's only going to give you a finite number of bits.
And what about Stephen Wolframs' claim that our mathematics is only one of a sort of a
very wide spectrum of possible mathematics?
It depends on what you call our mathematics.
I think that all mathematics are mathematics.
So meta-mathematics is mathematics.
It's not different from mathematics.
I think that, for instance, computational mathematics, the thing that I am practically
working in when I write my code and when I think about how to realize code is a branch
of mathematics.
It's called constructive mathematics.
It's been discovered in mathematics a long time ago and largely been ignored by the other
mathematicians because they thought it's not powerful enough to do all the things with
real numbers that they like to be doing.
But all the geometry is not possible in computational mathematics.
We can only approximate it.
geometry requires continuous operations, infinities.
And also physics is built largely on these continuous mathematics.
And in a computational universe, you only get these continuous operators by taking a
very large set of finite automata, making a series from them and then it's squint.
You know what, Joshua?
Let me share with you something.
I feel like I am a goldfish and you're a human when we're talking because I think that's
kind of like the level, the difference of intelligence between you and me, my friend,
which I come on because honestly, after interviewing 230 of supposedly the smartest
people in the world, I've never had this feeling before.
But today at this moment, this is how I feel just trying to keep up with you.
No, I'm sorry.
This is my fault.
No, no, no.
It's not your fault.
It's you are who you are and it's my job to try to follow through and also direct a little
bit of conversation in the best possible direction that I see can benefit both me as an interviewer,
but even more so my audience and you.
So let me just give us a little bit of a side direction here for a second and bring us back
to the last issue before we jump into the meat of the matter here on AI, and talk about
philosophy in academia and practicality because you mentioned about how you're motivated by
your own kind of desire and inherent or intrinsic motivation to learn something or to discover
new things, but perhaps academia is motivated nowadays more by the practical side of knowledge,
by the side where you can create something that you can patent, that you can sell, and
that you can scale up and commercialize.
Where is the benefit and I think in a way that the usefulness of philosophy was its
uselessness in some ways, if you will, just like art in a way is something that cannot
be used for anything else.
And some people have defined art as Oscar Wilde, for example, as something that's not
immediately useful.
That's what art is.
So is there and there's actually a very famous paper written in the 19th century by the guy
who funded the Princeton Institute for Advanced Study called the usefulness of useless knowledge.
I don't know if you're familiar with it, but what's your take on that?
Is there because many people would say, if you can't use any knowledge immediately, it's
useless.
Don't waste time acquiring it, don't waste time classifying it, storing it, just focus
on something that's useful and practical.
And to me as a philosopher, I'm always or often attracted to stuff that looks utterly
useless.
And maybe that's just me being not a scientist.
But what's your take on that sort of tension, usefulness and uselessness in terms of knowledge?
Feynman once said that physics is like sex.
Sometimes something useful comes from it, but it's not why we do it.
But it's brilliant.
So there is a big insight there.
This is, it's not that art is useless.
It's just the utility of art is completely orthogonal to why you do it.
So the meaning of the art is really not to help the living.
If you'd like to help the living, right?
But it's, so it's a very nice side effect.
But what we want to do with the art is to capture what it's like.
We want to capture a conscious state.
That's the actual meaning of it.
And in some sense, philosophy is at the root of all this.
I think it's reflected in a way in one of the founding myths of our civilization, the
Tower of Babel.
This is the attempt to build this cathedral.
And it's not a material building because it's meant to reach the heavens, which is not real.
They're not in this world.
It's a metaphysical building that is being built.
It's this giant machine that is meant to understand reality.
And you get to this machine, this true scot, this thing that tries to understand what's
going on by using people that work like ants and contribute to this.
And it's not about your ego.
It's not about the gratification that you get from people for contributing to it.
It's not for this thing that doesn't care about you.
It doesn't give meaning to your life.
It doesn't reward you for your insecurities and the toil of your existence.
But it's really just a machine.
It's a computer.
And as we would say now, it's an AI.
It's a system that is able to make sense of the world.
And people at some point had to give up on this.
It fell apart because they were no longer able to speak the same language.
So the different parts stopped fitting together.
Just became so large and so many people had to work in specialized direction that they
could no longer synchronize their languages.
And that's why they gave up on it.
And then this big accident happened in the Roman Empire, where they could not fix the
incentives for governance in similar ways as we fail here.
Our government has to play a much shorter game than civilization does.
And this leads to bad results for civilization.
And the Romans decided to fix this by turning the society into a cult and
burned down our epistemology and killed people that were overtly rational and
insisted that people talking to burning bushes on lonely mountains don't have a
case in determining the origin of the universe.
So this one had to give and the cultist won.
And we still have to recover from that.
So in a way, the beginnings of the cathedral of understanding the universe that had been
built by the Greeks and by the Romans had been burned down by the Catholics.
And then later rebuilt, but mostly in the likeness because they didn't get the
foundations right.
The left scars and our epistemology that have not healed, even though we have a
pretty successful culture that incorporated most of the other libraries and
burned down the rest, right?
We are the ones that are left over on this planet in a way.
In our libraries, we can read everything that there is to read at the moment.
We just often cannot translate it.
And do you think that our civilization is currently perhaps suffering from that
same Babylonian problem of difference in language and perhaps even has impact on
resolving global problems like global warming that you mentioned, for example, right?
Because all those people, business people, politicians, scientists, et cetera, speak
in different languages and therefore they cannot kind of coordinate or synchronize anymore.
And therefore that kind of perhaps puts at risk the whole project of our civilization
just like the Babylonian Tower collapsed.
Now this narrow specialization and diversity of languages and the difficulty in communicating
between all of those branches then puts at risk the whole project of our civilization.
I think that people individually are not generally intelligent.
How often do you see a person that knows what they're doing?
I'm certainly don't know what I'm doing.
I have no clue what I'm doing to be honest.
We are relatively intelligent, but of course this intelligence is largely a prosthesis
to cover for non-working instincts.
And we figure that out by now, right?
And we see that people acting on the instincts largely get good results for their life, but
they don't reach a very deep understanding about the nature of existence in the process
because they don't have to, right?
There is very little utility for deep philosophy and practical matters.
And as a result, individuals are relatively stupid.
Generations are not smarter than individuals but dumber because generations are made from
groups that synchronize their beliefs.
And the synchronization of beliefs makes it necessary that you give up agency over what
you think is true.
And when you do this, you accept things that you would not accept when you think about
them individually.
So people in Eastern Germany collectively believe things that an individual would never
have thought.
And same things happen here, right?
So there are many conspiracy theories that people believe in here for a while that would
not make sense to somebody who thinks about this.
Like Putin uses an army of Twitter trolls to manipulate the fan-affectations of Star Wars
movies.
This is a conspiracy theory that was a result of misreading a study and was then repeated
by 20 news outlets until somebody bothered to read the actual study and figure out, no,
this is not what the study says.
And then some of the outlets picked up on this but none of them wrote, OK, now we reconsider
what we think about Putin and Star Wars because it's a way totally what Putin would have done
if he would have had the idea.
And this may or may not be true but it means that we don't project reality as the extrapolation
of facts.
It's rather that we know there are enough facts to support what we feel to be true.
And there's utility in feeling particular kinds of truths and these basically local
cults of interpreting reality shape society, shape generations is what a generation is
about.
It's a local perspective of what things should be like.
Like you have your liberal generations, the millennials are largely authoritarian generations
and if you look at them and it feels wrong to us and they look at us and it feels wrong
to them.
And neither of them is true.
It's probably a set of biases that are the result of a local indoctrination.
But there is something that's smarter than the generation.
This is the culture itself.
So if you zoom out a little bit, you see that generations and societies are generated by
cultures and cultures are built over a long time.
And there are many things that are embodied in a culture, for instance, in the culture
of how to build science that would be very hard to derive for a single generation or
to improve for a single generation because we don't locally understand all the things
that went into it.
So anyway, civilizations are smarter than us.
There is something like a civilizational mind, a civilizational intellect that we as members
of our polis who are somewhat educated can never fully comprehend.
But once we figure out that it's there, there is something like a civilizational intellect.
We can try to look into the abyss and see its rough shape, but it's difficult to figure
it out.
And then we realize, oh, there's a long tradition, there's multiple traditions that build on
it and contribute to it.
And that thing, in a way, is what we are going to achieve when we build AI in the sense that
we can incorporate the sum of all knowledge in a system of relations that makes sense
of it all.
But what if civilizations self-destroy themselves then?
What is that sort of knowledge or intelligence then say about the fitness function of that
particular civilization and in general even?
Before we had an industrial civilization, we never got about 400 million individuals
on the planet because we could not feed more.
And only this switch to our industrial civilization made it possible to have billions of people,
which also means many hundreds of millions of scientists and philosophers and thinkers
and the internet and so on.
It's amazing what we did.
We took basically 100 years worth of trees that were turning into coal in the ground
because nature had not evolved microorganisms yet that could eat the trees in time.
And we burned through this deposit of energy in 100 years to give plumbing to everybody.
And part of that plumbing includes access to a global porn repository that is an afterthought
as to some of all you knowledge and largely uncensored chat rooms in which you can talk
about it.
This is the internet.
And this is an amazing machine.
And we have it right now and only in this moment and time we have it before it didn't
exist.
So you could take a particular perspective.
Let's say there is a universe that is saying where everything is good.
You have this nice planet with pretty decent living conditions and pretty stable climate
and you have the very smart sustainable civilization on it and you get the chance to be incarnated
in it.
It's an agricultural civilization with 300 million people.
It doesn't have airplanes.
It doesn't have internet.
It doesn't have computers.
Because to get there it would have needed to build an industrial civilization that obliterates
most of the good things that make us sustainable.
But it is stable.
And people are figured out how to be nice to each other and it's pretty good.
And then there's another universe which is completely insane in fact up.
And in this universe humanity has just doomed its planet to have a couple hundred really,
really good years.
And you get your lifetime close to the end of the party, this incarnation, which incarnation
you choose.
Oh my God, aren't we lucky?
So you're saying we're in the second in the...
Of course we are.
It's fucking obvious, right?
So what does that say about our future then?
And what's the timeline before the party is over?
We cannot know this, but we can see the sunset coming up, right?
It's pretty obvious.
And it's...
People argue about this.
They are largely in denial, but it's like you are in this Titanic and there's this pretty
big iceberg and it's very unfortunate and people wish about it.
But what they forget is that without the Titanic we wouldn't be here.
We wouldn't be talking right now.
We would not exist.
We wouldn't have internet.
So tell me this.
You have this kind of very Buddhist, if I may call it, attitude to the sort of ephemeral
sort of short span of our civilization and sort of the high appreciation about us joining
the peak of the party, if you will.
And yet you're kind of seeing the sunset kind of in the future, but that's not giving you
any sort of negative or pessimistic or depressive inclination, it seems.
How do you resolve that?
Or do you?
Because someone will say, well, that's very nihilistic, it's very pessimistic, it's very
depressing what you just said.
And yet you're so happy.
No, I really have enough things to be depressed about.
So I have to be choosy about what to be depressed about.
And it took me a long time to figure out that the demise of humanity is very unfortunate
in many respects.
But it's something that, well, we try to do everything we can to stop it, but we are not
the first generation to try to.
So I have to do both things.
I can still try my best to steer for a sustainable future, it's not that I completely give up
on this, but it's in a way dealing with my own mortality is similar, right?
I try what I can to not leave my family without a breadwinner too early, but at the same time
I'm going to die.
And if I waste my life being depressed about the fact that I die, I'm not doing it right.
I should be happy about the fact that I live, not be unhappy about the fact that I die.
And if you take this as a computer game metaphor, this is like the best level of humanity to
play in.
And this best level of humanity to play in, it happens to be the last level and it plays
out against the haunting backdrop of a dying world, but it's still the best level.
Right.
That's again, to me, that sounds very Buddhist, do you agree?
Yeah, but this might be an accident.
I got to know Buddhism only in its westernized forms, which is a Protestant version.
It's basically Protestantism reformed with slightly Eastern metaphysics, but mostly mistranslated.
And epistemologically, in metaphysically, it's a septic tank that most of the ideas
that Buddhists have about how the mind works and how the universe is arranged don't seem
to pan out.
They don't seem to have sound epistemology.
This is not a general thing.
I did find people that start out in Buddhism, in a way, and got clean, but most of them
I met or not.
And in practice, when I went to Buddhist countries and talked to Buddhists on the ground, it was
not much different from Catholicism, which means it's a system of indoctrination with
cults that makes people behave in predictable ways, which is useful for societies, but breaks
people's epistemologies.
So in a way, I don't have this deep reverence for Buddhism because it's so holy and sacred.
I don't think that there are holy books, there are only manuals.
And most of these manuals we don't know how to read because they are for a system for
societies that don't apply to us, they're for different societies.
Okay, let me zoom out a little bit more and ask you this.
What are the big issues then?
So you're saying we can see the sunset and we're at the peak of the party, so we might
as well enjoy the party while it lasts.
Great.
The big issues that our civilization is facing today, what are the reasons perhaps if it's
more than one that can bring about that sunset of our civilization?
What is making you make that claim?
The thing that burns me most at the moment is global warming.
I suspect that because of a very strong publication bias that we have, if you are worried about
climate, you will try to make your case extra strong so you will not make your most alarmist
predictions but the ones that you can defend most easily, which means you're going to be
a little less alarming that you might want to be.
And if you are not an alarmist but an anti-alarmist, you're going to be way too optimistic about
things.
And as a result, I think that the distribution of the results that people look at when they
think about how many degrees centigrade global warming they're facing in the next couple
of hundred years are very optimistic.
Another thing is, have you noticed that the projections all magically end in 2100?
Do you think that's because the IPCC thinks that it stabilizes the 2100 or because it
hopes that in 2100, too, there's a rupture event?
It's obviously not going to stabilize.
It seems to be that we locked in way more than two degrees centigrade global warming
before we possibly go for six to eight.
And we will lose the West Antarctic ice shield.
It's pretty clear that we cannot refreeze the poles.
And I think it has been pretty clear that we cannot do this since the late 1980s.
It's just a feedback loop that is now running away.
And there's a slight chance that we find technological solutions to stop it.
But I think it's not likely.
And carbon sequestration is not it for simple reasons of how energy works.
The reason why we put all this carbon dioxide in the earth in the atmosphere is because
we wanted to liberate this energy.
And if we want to get it back from the atmosphere, we basically have to use the same amount of
energy that our civilization has been getting from this, all the benefit, and put it back
there without the clear business case.
And it's possible that unlikely.
So we look in a situation where in the medium term, we are going to lose a lot of habitable
area on the planet, and we also might lose climate stability.
So this ability to predict what kind of harvest we are going to have next year, which means
we lose a lot of open air agriculture.
We will have large storms that will also destroy many of our greenhouses.
And as a result, we probably go down to a few hundred million individuals again.
And the rest of us will not go kindly and quietly into this good night.
And the resulting resource source will probably take downwards left of civilization.
So basically, if you lose that infrastructure, I don't see how we can sustain civilization
in a good way.
Wow, that's such a beautiful serene and optimistic picture to contend with.
But I mean, there's a lot of chances.
I think it's possible that AI gets us before global warming does.
So let me ask you this, because you are an AI scientist, and yet you're telling me you're
most worried about global warming, and yet people who are not AI scientists like Elon
Musk, like Nick Bostrom, like even the late Dr. Stephen Hawking are saying that the greatest
existential risk that we should be worried about is AI.
What do you feel about that in the first place, and what do you make of it?
Many existential risks.
So if you zoom out long enough, it's completely certain that the end of a sun that we can
persist on is an existential risk.
Another thing is that losing the atmosphere in 1.5 billion years from now is an existential
risk that we probably cannot deal with.
That looks unlikely that we can build sustainable civilizations outside of this gravity well.
Before that, there's going to be a number of super volcano eruptions and meteors that
are going to get us, which means it's pretty certain that the days of humanity are numbered.
We are mortal as a civilization.
What if we spread through all other planets?
It's unlikely that we can make that happen.
At the moment, we're not able to build cities on the bottom of the ocean.
Mars is way less habitable than that.
It doesn't even have an atmosphere.
Can you care for it?
Maybe, but not with today's technology.
To get there, to basically put enough stuff in orbit to go from there to Mars, there's
a large number of people and build something that is sustainable and can survive the breach
of a few of the agricultural domes on Mars if a random meteor happens or something goes
wrong and the pipe gets clogged.
That is very hard to do.
We cannot even think global warming.
We cannot even build a new subway in New York anymore.
We lost the ability to make a torster that gets more than four stars on Amazon somewhere
after 1960.
In many ways, our technological civilization is stagnating and it's because of regulation
deficits.
But we haven't figured this out and the biggest issue is probably good governance.
We haven't really figured out good governance.
AI might help with this.
In a way, the building of information processing systems that can help us to self-regulate
could be one of the big chances that we have.
Without AI, we are dead for certain, I think.
With AI, there's a probability that we are dead.
So you're disagreeing in some sense, at least, that maybe not AI is our greatest danger,
but perhaps our only hope for saving ourselves then.
But you and me will probably die.
We cannot be saved.
Everybody who lives will probably die.
And it's because entropy will always get you in the end.
And our civilization has leveraged itself very far over an entropic abyss and there is no
land on the other side.
So you're going to crash down into this abyss at some point and probably sooner than later.
This near-term AI, I'm mostly not worried about AI built into automatic guns.
If you have drones that are controlled by AI, they're going to kill a few million people
more than they would be killed otherwise with conventional weapons.
Conventional weapons not driven by AI because it was going to reduce the cost of war and
it makes some conflicts more likely.
But what really worries me is AI in the stock market.
If you use AI to automate attacks on the financial system, which is the reward infrastructure
of this global organism that our civilization is.
This is going to kill billions, especially if the AI is autonomous.
So if the AI is going to ... Sorry, this was my headphones.
They just made announcements.
These headphones are too smart.
They think it's a good idea to talk to me when they want to be recharged.
Too much intelligence in the systems around me or rather too little intelligence in the
people who design new eyes.
Yeah, in your headphones.
But we already know that most of the trades on the stock market are done by AI.
Yes, but they are not done by autonomous AI.
They are done by optimizing very local functions.
Imagine a rogue trader gets a general AI, a general factual approximator that has no limits
in terms of the functions it can approximate.
And I said, make me a few bucks on the stock market, however you do it.
And you can do whatever you want.
You can even reinvest 5% of what you make or 20 or 50% of what you make into compute
and buy data in order to make that compute better.
So very soon, more than the economy of Scandinavia is going to fuel computers that are running
attacks on the stock market in a similar way as it happens with Bitcoin right now.
And it's going to burn serious oil, right?
And the thing is going to figure out, oh, there is only 8 billion people on the planet
that own the assets on the stock market.
They make decisions and let machines make decisions.
And these 8 billion people only live for like a trillion seconds each, which is very little.
And we can get so much data about them.
We can basically figure out what they think in every baking second of their life, what
they see, what they think about, what will happen to them.
This thing is going to game the shit out of us.
There is no way we can outsmart this thing.
The only way the economy can survive this, if the AI has been cleverly set up in such
a way that it eats the whole economy and becomes the economy.
But the economy needs to become intelligent.
The money is to apply all the circuits of how we distribute rewards, need to be regulated
dynamically in real time with intelligent functions.
This is the only way that we can fend this off.
So we have a system that is perhaps not provably correct, but it's able to react in real time
to any kind of disturbance, any kind of new threat.
There is some hope.
This is a possibility, at least if not a high probability.
It's at least a possibility.
Yes, but there's also the other possibility.
No intelligent system is going to do anything that's harder than taking its reward function.
I call this the Dabowski theorem.
All these smart monks, if they really figure it out, they go for nirvana because it doesn't
have integrity to do anything that's harder than taking a reward function.
When you fix your reward function, you're done.
The monasteries are in a way in the battle because the monastery is an economic entity.
So they're in the battle against enlightenment.
They need to enlighten their monks to such a degree that they opt out of having families
and secular lives.
But they still need to serve the monastery.
Only your old monks are allowed to go to nirvana.
Okay, so we've been using this term AI for a while now.
Let me ask you, how do you define artificial intelligence?
Because after a couple of hundred of these interviews, it seems to me that many people
in the field have either slightly or in some cases very substantially different definition
of what AI is.
I think intelligence is the ability to make models.
It's not the same as the ability to reach goals, which we call smartness, or it's also
not the ability to pick the right goals, which we call wisdom.
And very often, in excess of intelligence is the result of an absence of wisdom, with
which you try to compensate for the absence of wisdom.
Right?
So, in a way, wisdom has to do with how well aligned you are with your reward function,
how well you understand its nature, how well do you understand your true incentives.
And intelligence is not that.
Intelligence is really the ability to make models.
It just happens to be usually in the service of regulation.
What about artificial intelligence?
Well, artificial intelligence tries to automate this.
And in a way, it's the mathematics of making models.
This is what artificial intelligence is about.
And the interesting parts of our minds are, in my view, the parts that make models.
The other thing is the reward function that makes the minds subservient to some organism,
to turn some general mind into the illusion of being a person and caring about things.
The organism needs to take a perfectly fine computational process and corrupt it with
the illusion of meaning.
Right?
So, you have this reward function that needs to be protected against the axis of the mind
that would want to know, why am I doing this here?
And so, the reward function gets wrapped into a big ball of stupid to protect it against
you accessing it.
Right?
So, as soon as you try to really look at your true incentives, it gets very boring or something
else.
If you're very guilty, if you are in the early stages or very ashamed, and only when you
go all the way and you just are able to look at these things, you can dissolve being a
mind and you wake up.
And it's not necessarily a good thing if you wake up.
It's just, this liberation doesn't give you a direction.
You just wake up and you look down on your hands and you see, okay, I just woke up and
realized I'm a mind.
I'm not a monkey.
I'm the side effect of the regulation needs.
But does it have to be a monkey that I run on?
And then, but then, isn't that consciousness actually, or is that the illusion of consciousness
is Daniel Dennett puts it?
No, it's slightly different.
I think consciousness is largely misunderstood.
Consciousness is an artifact of a particular kind of learning algorithm.
You want to go there?
Well, do we have to, I mean, yes, we have to explain consciousness now.
Yeah, I think so, because, I mean, and of course, there's that whole debate whether
we even need consciousness for AI or AGI at all.
But presumably, if we presume that we need, then we need to explain it because you can
create or model something that you don't, you can't even define.
Yes.
So intelligence is the ability to make models, right?
What is a model?
A model is something that explains information.
Information is discernible differences at your systemic interface.
And the meaning of information is the relationships you discover to changes and other information.
If you have a blip on your retina, the meaning of that blip is the relationship you discover
to other blips on your retina.
The same moment or different moments in time.
The relationships you discover is you are looking at a three-dimensional world with
people that are deformed by the laws of perspective and being shown on by photons and as people
have ideas and exchanges with other and so on, right?
So you build this giant operator that predicts the data at your systemic interface.
This is your model.
And this model has three parameters and people of parameters like sounds and colors and people
and so on.
There are two parameters of the physical universe out there, which is some kind of weird quantum
graph that has the ability to produce patterns.
The structure that we find in the patterns is these geometric functions that describe
how objects move in space and what they sound like and what they look like.
And a model is a set of parameters, which a parameter is a set of possible discrete
values and the relationships between the parameters.
And the relationships are computational relationships, which tell you if this parameter and this
parameter have these values, then that parameter should have that value.
So for instance, you figure out that a way to describe a phase that you're looking at
is you see the structure of the phase, you see the nose and so on.
And if you see both the nose and the face, they need to have the same pose, the same
alignment in space if they're connected, right?
So your nose representation is going to send by its computational relationship information
about its position in space to the face.
And the face is going to send information about its position to the nose and they need
to agree.
And if they don't, you have an inconsistency and incoherence in your model.
And our perception goes for coherence, it tries to find one operator that is completely
coherent.
When it does this, it's done.
This is the way we optimize.
So we try to find one stable pattern that explains as much as possible of what we can
see and hear and so on and smell and think.
And attention is what we use to repair this.
So whenever we have some local inconsistency where the nose is pointing in some other direction
in the face, this calls attention to itself.
And attention is a particular kind of mechanism in the brain that gets pulled to these areas,
these hotspots, where things are fluctuating and don't get resolved and then tries to find
a solution.
And it might find out, oh, some noses are crooked or this is not a face or it's a caricature.
So you extend your models and these extensions of the models make it possible to encapsulate
this part of the operator that is clearly of the sensory data in such a way that it's
harmonious again, that it makes sense again, right?
Once you do this, you're done and you can put your attention on something else.
This attentional learning cannot work like the layer stochastic gradient is set in our
neural networks, partially because our brain is not differentiable, also because it's
a very inefficient algorithm.
And the algorithm that our brain is using in these cases is that we store the local
binding state.
For instance, you play tennis, you want to get better at tennis.
So what do you do?
You cannot basically pipe a lost function through all of your brain in order to get better
at tennis.
It would be very inefficient.
You need to touch too many neurons.
What you do instead is to make a commitment.
You say, I want to get better at this particular thing.
I want to improve my backhand.
So I will make this throw slightly more like this and I expect the following result.
And I remember what this means.
So I store this binding state that allows me to have that configuration in my brain
to perform that stroke.
This part of a store is an indexed memory.
And conscious attention in the sense is the ability to make indexed memories that I can
later recall.
I also store the expected result and the triggering condition.
When do I expect the result to be visible?
So a few minutes or seconds later or hours later, I have feedback about whether this
was a good decision.
I lost one or lost the match.
And then I recall my decision that I made early on.
I recall that binding state.
We instate part of my brain state back then and remember the situation that I was in.
I compare the result that I expected as a result I got and as a result, I can undo the
decision that I made back then to change in the model or I can reinforce it.
And this is, I think, the primary mode of learning that we use beyond just associative
learning.
This attention is the key differentiator in the process of learning them.
So consciousness means that you remember what you had attended to.
Right.
So you have this protocol of attention.
And the memory of the binding state itself, the memory of being in that binding state
where you have this global oscillation that combines as many perceptual features as possible
into a single function.
The memory of that is a phenomenal experience.
The act of recalling this from the protocol, this is access consciousness.
And you need to train this attentional system itself.
How do you train the attentional system so it knows where you store your back end, your
cognitive architecture?
That is something that needs to be trained by the attentional system as well.
So you have recursive access to attentional protocol.
Remember when you made this recall, when you accessed this protocol, what results you got
from this.
You don't do this all the time, only when you want to train this.
And this is reflexive consciousness.
That's the memory of the access.
Right.
So then there is another thing, the self.
The self is a model of what it would be like to be a person.
So happens that the brain is not a person.
The brain cannot feel anything.
It's a physical system.
New ones cannot feel anything.
They're just little molecular machines with a Turing machine inside of them.
They cannot make themselves feel anything.
They cannot even approximate arbitrary function except by evolution, which takes a very long
time.
So what do we do if you are a brain that figures out it would be very useful to know what it's
like to be a person?
It makes one.
It makes a simulation of a person, a simulacrum, to be more clear.
Simulation basically is isomorphic in the behavior of a person, and that thing is pretending
to be a person.
It's a story about a person.
Basically you and me, we are persons, we are selves.
We are stories in a movie that the brain is creating.
We are characters in that movie.
And the movie is a complete simulation.
It's a VR that is generated in the neocortex, and you and me, the self, is the character
in this VR.
And in that character, the brain writes our experiences.
So we feel what it's like to be exposed to the reward function.
We feel what it's like to be in our universe.
And we don't feel that we are not actually conscious.
We don't feel that we are a story because that is not very useful knowledge to have.
Some people figure it out and they personalize.
They start identifying this the mind itself or lose all identification.
And it doesn't seem to be a useful condition.
So normally our brain will be set up in such a way that the self thinks it's real and
gets access to the language center and we can talk to each other and here we are.
And the self is the thing that thinks that it remembers the contents of its attention.
This is why we are conscious.
And some people think that a simulation cannot be conscious, only a physical system can and
they got it completely backwards.
Physical system cannot be conscious, only a simulation can be conscious.
Consciousness is the simulated property of a simulated self.
So in a way, Daniel, then it is correct and keeping with what you said.
But the problem is philosophers like him and admire him is very smart, very well,
that works very hard.
The things that he says are not wrong.
But they are also not non-obvious.
So what's the value of them then?
Is that?
Oh, it's very valuable because there are no good or bad ideas in this intellectual sense.
An idea is good if you can comprehend it and it elevates you.
It elevates your current understanding.
So in a way, ideas come in tiers.
And the value of an idea for the audience is if it's a half tier above the audience.
But you know, you and me, we have this illusion that we find objectively good ideas.
That's what we struggle for because we work at the edge of our own understanding.
But it means that we cannot really appreciate ideas that are a couple tiers above our own ideas.
One tier is a new audience, two tiers means we don't understand the relevance of these ideas
because we have not had the ideas that we need to appreciate the new ideas, right?
I think your ideas are just about the edge of my personal capabilities.
So yeah, it says a lot about us, but it doesn't say very much about how these ideas are good.
An idea appears to be great to us when we stand exactly in its foothills and can look at it.
It doesn't look great anymore when we stand on the peak of another idea
and look down and realize this previous idea was just the foothills to that idea.
And I don't see that it obviously ends anytime soon.
Yeah, it's a journey.
And by the way, that's what, in my opinion, good philosophy in academia should be about.
About generating ideas as many and as diverse of them as possible
rather than generating products, generating patents and generating commercialized solutions
that can sort of increase the endowment fund of the university or something like that.
And my problem with current academia is that,
and one of the reasons why I decided not to pursue that career for me,
I mean, I would have not survived there is precisely that reason
that there's this kind of treadmill, hamster wheel pursuit of like patentable,
practical, commercial knowledge, economic growth that it's motivated by.
Whereas I'm always more inspired by stuff that's sort of a lot more in the realm of ideas
and perhaps useless or impractical, at least at this junction, but I just can't help it.
So there is a very weird thing about the nature of understanding that we have.
I think that most of us never learn what it really means to understand
and largely because our teachers don't.
There are two types of learning.
One is you generalize over past examples.
We call that stereotyping when we're in a bad mood, but it's what it is, right?
And the other one is others tell us how to generalize and this is indoctrination.
And the problem with indoctrination is that it might break the chain of trust.
If somebody in the chain of trust takes something on authority,
which means they don't check the epistemology of the people that came before them,
that is in a way a big difficulty, right?
And the new thing about our civilization is not that there are so many unbroken chains of trust now,
but because of the vast number of people that are in this business,
some of them actually have intact chains.
And you can try to figure out what these are and you can try to figure out that the difficulties that they run in.
But to do this, you have to study these things in more detail.
And most of our people that do this are not scientists, they are scholars.
And the difference between a scientist and a scholar is that a scientist looks for truth
and the scholar looks for the consensus opinion of a field at a given time.
And we train, unfortunately, most of our scientists as scholars and few of our scholars as scientists.
This consensus opinion thing is an important thing,
but when we look at the field, the consensus opinion tends to be different in 10 years from now,
which means it's false. At any given moment in time, it's false.
Yet at the same time, there are individual scientists which may or may not be in the consensus
and they have ideas that stand the test of time because they are provably correct.
And so we have this very weird relationship to truth.
The things that are true are not just in the realm of the proven.
The proven things are true, right?
If nobody made a mistake in the foundations of the proven things.
But the things that must be true are in the realm of the possible.
And because everything is in a particular way for a particular reason.
And we haven't figured out how things, why things are for that particular reason.
So if you want to know what a scientist thinks, you cannot just read their papers
because they only write in the papers what they can think they can prove.
You have to understand what they think is possible and why.
And philosophy is not doing this very well anymore
because it doesn't have the right language to do so.
It does not understand the languages that mathematicians and physicists use.
And philosophers largely don't know what it means to understand physics.
So for instance, a very simple thing like a radio.
I have learned in school, learned for some definition of learning,
how a radio works, which means I got a very convincing story.
But this story tells me, it's a very good narrative
of why these electrical circuits are able to do what they do.
And the people that invented the radio were just the first people
that randomly happened upon this amazing story.
But then you think about in a later moment, how unlikely is this?
This story has so many elements in it that sound to be like conjecture.
How do you wake up in the morning with everything you know about physics
and you think, oh, let's take an eductance and a capacitor and a few wires
and a rod that can act as an antenna and combine them together
and suddenly we have radio.
Why would that work?
How can you derive this from first principles?
And in a way to understand, it means to know what it takes
to reach this understanding, why you would make this conclusion.
But you need to be able to retrace the steps, all of them.
You need to be able to understand what went into this understanding.
And can we ever do that?
Yes, of course.
But our individual minds are so limited.
So for instance, I look at Stephen Wolfram's work
and from the outside, it's very easy to dismiss that.
But when I truly look at it, I realize right now in my life,
at 44 years old, I'm roughly at this stage where I would understand
why I would want to build Mathematica and do it exactly in the way he did
and what I would do in the next five years while doing it.
And he was there in his very early 20s.
Right, so he got there at half my age.
He's way smarter than me.
I know a few things that he didn't know at this time
and some of them because of his contributions, right?
And some of the stuff was not available.
But this is not because I'm smarter.
It's really I'm much dumber than him.
And this is quite humiliating to see this.
And it's not that I get depressed about this or envious.
It's just the way things are.
But to see this, and then I can realize what was the outcome
of devoting your life to building this machine.
And maybe we should build a different machine,
a best effort computer instead of the domestic computer
to build your mathematics on.
But just maybe, maybe Mathematica will become sentient.
Who knows?
Let me shift our conversation a little bit
to a little bit different scientist
with all due respect to Dr. Steven Wolfram,
whom I do think like you that he's a genius.
But let me bring in Ray Kurzweil a little bit
because he's a little bit more pertinent to our conversation.
I don't know if you qualify Ray as a scholar
or as a scientist or as a philosopher or an inventor
or what, but he has made certain projections
and predictions and he has sort of not been ashamed
or afraid to popularize them.
Both with respect to AI and also with respect
to the future timeline thereof.
What's your take on sort of AI's Ray Kurzweil's body of work
and especially his idea of the technological singularity?
I think that he works on a different incentive
function than me.
I feel that Ray is a very smart, capable individual
that has made amazing contributions to AI.
And he also understands many of the core ideas of the field
better than many other practitioners.
But he is not so much concerned about putting all his cards
on the table when he makes his predictions.
There are reasons to make predictions
when certain things are going to happen for marketing reasons.
And there are intellectual reasons for doing this.
And I think that he is too much in a position
where the marketing reasons play an important role,
which means I don't understand his true thinking there.
I don't understand what is the exact argument that would
compel him to make a prediction with these arrow bars.
So when I look at the future or at the present or anything,
I don't know what the truth is when I'm an abetted observer.
I can only know this for the things
that I can look from the outside, which
means stuff that I built myself from scratch,
and I haven't built the universe by myself.
So I don't know how it will play out.
And if I make a prediction about the future,
I cannot come up with a single number usually.
What I have is a map of possibilities,
and then I can shift my confidences around
and the meta-confidences and the confidences.
This is about as good as I can do.
And with respect to AI, the problem is I don't have a spec.
If I don't have a spec, as a coder,
I know I don't know when it's done.
And how many steps, how many milestones
do I need to cover before I get to this thing?
I have an idea that, my answer modeling machines,
I have some ideas of what we are currently
doing wrong in our modeling.
Like our models have way too many free parameters right now.
You want to have a model where, ideally,
every possible state of the model corresponds
to one possible world state.
How annual networks have many magnitudes more possible model
states than world states, which gives you
a rise to these adversarial examples
and all other sorts of things.
Our models are much tighter.
The model that our mind has means, at every moment,
you try to understand the whole of reality.
Everything you see when somebody shows you a bitmap,
you don't try to understand this bitmap in isolation
by throwing it against your model of ImageNet
that you generated in your mind after looking at many bitmaps.
Instead, you think somebody is holding up
a picture with a bitmap on it.
And that bitmap has been printed by a machine based
on information taken by a camera, which is another machine
which was pointed at the window of the universe
as a different point in time and space.
This is what you know.
It's why you make sense of this thing.
And it's a much more complicated operator
that you have than our AIs currently
have and our self-driving cars have.
Once our cars have the situational awareness,
there's no way they will not out compete people in all regards.
But until they have this, there will be many situations
where people can make inferences that our machines cannot.
So we can all see these things.
And Ray can see some of them.
But Ray doesn't give us a trajectory
to get to these machines.
And if I talk to the people in his team,
they are as smart as they come.
They're really good.
They're very educated.
But I don't think that they see all the things that
need to be done.
And it's not because I see more of them,
but because there are so many things
that you would need to incorporate.
I just don't see the milestones.
I don't see this project.
And it might also be that I do not completely
see everything that his team is doing in secret.
So in a way, you're saying that it's a harder task.
It's a harder job.
And the timeline would be longer than his 2045 or 20?
No, it could also be shorter.
We don't know this.
So there are some people which are using so many things.
Steve Russell, for instance, suggests
that the last time that somebody said something
is not possible to somebody having the interesting idea
that it was fake Theven Arthur Ford said,
we don't know how to harness nuclear power.
And Leo Szilard had the core idea that it was like 14 hours.
And we don't know about AI, whether that is a similar thing.
It could be that it's only one or two ideas that we actually
need to have to pull it off or that we need
to combine in another way.
But it could also be that we are not
seeing a few hundred things, right?
And it takes a long time for us to stumble on the solution.
So then it's totally unpredictable, in your view.
It's not totally unpredictable.
It's just the arrow bar is very large.
And when I listen to Ray, I don't see him basically
talking about the arrow bars.
I see him talking about a possible universe in which he
can upload himself on a computer before he dies.
So let me get this right.
So you say you don't see that?
In his discussion, I don't see that he puts arrow bars
on his predictions and explains where the arrow bars comes from.
What he gives us is a prediction that is compatible with himself
becoming immortal.
Right, right, right.
Yeah, and that may be the bias.
Yes, yes.
OK, and I mean, Marvin Minsky said,
as you point out in your speeches every once in a while,
that it could happen anywhere from four to 400 years.
And as you recently noticed, we're still in that timeline.
Yeah.
So personally, my hunch is that it's not
going to be that long.
My hunch is that it's a lot earlier than people
would think that it happens.
But as long as I cannot justify my hunch,
I cannot put big confidence on it.
But you're confident it will happen.
Because there's many skeptics who say,
we don't know even if it will happen.
We don't know even if it's possible for a number of reasons.
Yes, but the question is what confidence
are they supposed to have, which means what evidence
can they supply for their claim?
And if a person has arguments that were pertinent in 2003,
but are no longer pertinent in 2018
because our understanding has progressed,
then the confidence that I derive from their repeated claims
from 2003 is low.
It does not change my belief very much.
A belief of a person is only worth as much as the evidence
that they built it on, which means most people copy their belief
from somebody else that they didn't look,
and they got it from.
So you can collapse the space of possible beliefs
into the sources of beliefs, and there are very few of them.
And then it seems you are thinking,
definitely we're making progress.
Therefore, the beliefs against our shrinking,
the area of beliefs against that possibility of shrinking,
and the other ones are increasing.
So the original first phase of AI
was working by identifying problems
that require us to be intelligent, like playing chess,
and then implementing this as an algorithm.
So it was basically manual engineering
of strategies for being intelligent in particular domains.
And this somehow did not scale towards general intelligence,
one algorithm to do it all.
And there were subparts of this, like the logistic program,
the idea to come up with a language
that allows you to have all possible valid thoughts.
Same project as Wittgenstein,
completely preempted most of the work of Minsky, in a way,
but a couple of decades earlier, and then failed.
And the philosophers largely didn't understand
what he was up to because he had to publish this
in this already dying discipline instead of waiting for AI.
And the AI people didn't really understand
that this philosopher was actually trying to do AI
briefly before church-ensuring,
already understanding computation.
He already understood that logic is sufficient
to build all the possible representational systems,
and he could also replace all logic with NAND gates.
He already knew that.
But it's pretty amazing for a young guy back then.
Okay, so this first project, a program of AI
did not accumulate all the way,
and now we are in the second phase of AI.
We no longer build these algorithms ourselves,
we build algorithms that discover the algorithms.
Right.
We build learning systems that discover,
that approximate functions.
And deep learning has an unfortunate name.
I think it should be called
compositional functional approximation.
This sounds more like a mouseful,
but it's also more narrow and more accurate.
It's about this thing that we don't just take
a single function that we tune to like a regression,
but that we are able to take many functions
and put them behind each other
or into networks of functions.
So that is the big trick.
And we can approximate some functions well and not others.
It could be that there is a third phase
where we no longer build the algorithms
that discover the algorithms,
but we go one step higher.
We build the algorithms that discover the algorithms,
that discover the algorithms.
If you go for meta-learning.
In a way, our brain maybe is a meta-learning machine,
not a system that can just learn stuff,
but then discover how to learn stuff for a new domain.
Dan Kuni College from the Max Planck Institute
has this practical poesis idea,
which is basically learning about,
learning about learning kind of idea.
Yeah, but at some point it stops.
I don't think that you go,
need to go for more than four degrees.
Like at some point,
there's going to be a general theory of search
that tells you how to get to the global optimum,
if the global optimum can be gotten to
and your system is finite resources,
or basically how to optimize your chances of getting there.
Once you have that algorithm,
as a scientist you are done,
there is no more science that we can do with integrity,
because it's just going to be the application
of this algorithm.
You can only do art then.
You know, our original,
we've been talking here for almost 90 minutes.
So let me sort of hopefully bring our conversation
to a close here within the next 10 minutes or so,
by sort of redirecting our attention
to the original occasion of us getting together,
which was a brief exchange we had,
the two of us on Twitter about ethics.
So let me ask you this,
where does ethics fit in all of this, or does it?
I get sometimes frustrated when people think
that ethics is about being good,
and being good means to emulate a good person,
preferably the one who is talking about ethics.
Did you get frustrated with me on Twitter?
No, you're a good kid.
I'm one year younger than you, by the way, so.
It's not about age, I'm about 12.
That's right.
Okay, so ethics, I think, is often misunderstood.
Ethics emerges when you conceptualize the world
as different agents, and yourself as one of them,
and you share purposes with the other agents,
but you have conflicts of interest.
If you think that you don't share purposes
with the other agents, if you're just a lone wolf,
and the others are your prey,
there is no reason for ethics, right?
There's only you look for the consequences
of your actions for yourself,
with respect to your own reward functions,
and that might involve that you have to create
a civilization of minions or whatever,
but it's not the same thing as ethics.
It's not a shared system of negotiation,
it's only one for you as an individual matter,
because you don't share that purpose with the others.
But for instance-
It may not be shared, but it's your personal ethical framework.
Oh, it has to be personal.
For instance, I don't eat meat,
maybe a legacy or a decision that I made
when I was 14 years old,
because back then I felt that I share a purpose
with animals, that is the avoidance of suffering,
if it can be helped.
And I also realized that it's not mutual.
The animals don't care about my suffering,
cows largely don't care about that I suffer.
They don't even conceptualize it,
they don't think about it a lot.
I have to think a lot about the suffering of cows,
they didn't want it to suffer, so I stopped eating meat.
That was an ethical decision.
It's a decision about how to resolve a conflict of interest
under conditions of shared purpose.
And I think this is what ethics is about.
It's a rational process in which you negotiate
with yourself and with others,
the resolution of conflicts of interest
under conditions of shared purpose.
And what purposes I share is in a way a decision,
and I can make different decisions
about what purposes we share,
and some of them are sustainable and others are not,
so they lead to different outcomes.
But in the sense, ethics requires
that you conceptualize yourself
as something above the organism.
If you identify the systems of meanings above yourself
so you can share a purpose,
love is the discovery of shared purpose.
There needs to be somebody you love
that you can be ethical with.
At some level, you need to love them.
You need to share a purpose with them.
And then you negotiate, right?
You don't want them all to fail in all regards,
and yourself.
This is what ethics is about.
It's computational too.
Machines can be ethical if they share a purpose with us.
And what about two other sort of consideration perhaps,
is that perhaps ethics can be a framework
within which two entities that do not share interests
can kind of negotiate in and peacefully coexist
while still not sharing interests?
But not interests, but purposes.
Or purposes.
If you don't share purposes,
then you are defecting against your own interest,
when you don't act on your own interest.
It doesn't have integrity.
If somebody is your foot,
you should, and you don't share a purpose with your foot
other than you want it to be nice and edible.
Right?
You then start giving presence to your foot
and falling in love with your foot.
It doesn't end well.
Look at the little mermaid.
The little mermaid is a siren.
Sirens eat people.
You don't fall in love with your foot.
It doesn't end well.
Okay, but me and you are both,
I don't know if you're vegan or vegetarian.
Both me and you don't eat meat.
So we made that choice that perhaps cows
don't share interests in us.
We kind of are interested in diminishing their suffering,
obviously, to make that decision.
And yet, and they're our food supposedly,
that's the popular opinion anyway.
And yet we've made that choice
to stay away from beef or from meat in general.
So we can't find a framework with in which
two entities that don't share interests
and our purpose is to get a good,
perhaps peacefully coexist.
And isn't that the netico framework of its own, right?
It's more tricky.
I mean, with the cows,
the cows largely wouldn't exist
if people would not eat them.
You can make the argument that
pasture living grass-fed cow
has net positive existence,
except for the last day, which is horrible,
but it's horrible for most of us.
Right.
And just that.
Right, so this is one argument
in favor of eating pasture-fed cows.
Right.
Another one is maybe you can manipulate
the mental states of the cows,
so even the factory-fed cows are happy.
Right, so is this unethical?
It might not look very appetizing to you,
but then again, maybe people are on the same decision.
We are a domesticated species.
This is what humanity is about.
We give up agency of our own beliefs.
You get manipulated in finding things bearable
that look unbearable to a more favorable human being
like you and me.
Right, it's a particular kind of domestication
that didn't take hold on your brain.
Is this unethical to implement this domestication
by breeding people or cattle in a particular way?
It looks repulsive to us,
but if we really care about the well-being of cattle,
you and me should probably optimize slaughterhouses
to make them more humane, to make them more bearable.
We look away from the slaughterhouses
because we find them very anesthetic.
We don't want to have anything to do with this.
And this is not the most ethical stance
that we can figure that out.
So ethics in a way is difficult.
Of course, that's the key point of ethics.
And so even it seems to me that ethics requires
sometimes we take choices
which are not in our own best self-interest perhaps.
Depends on what we define of our self.
The self, we could say this is identical
to the well-being of the organism,
but this is a very short-sighted perspective, right?
I don't actually identify all the way with my organism.
There are other things I identify with society,
I identify with my kids, with my relationships,
with my friends, their well-being.
So I am all the things that I identify with
that I want to regulate in a particular way.
And my children are objectively more important than me, right?
If they have the choice to make my kids survive
or myself, my kids should survive.
This is as it should be if nature has wired me up correctly.
You can change the wiring,
but this is also the weird thing about ethics.
Ethics becomes very tricky to discuss
once the reward function becomes mutable.
When you're able to change what is important to you,
what you care about, how do you define ethics?
Me?
So, or anyone?
I would say to me, let me be careful about this.
Well, I would say it's basically,
you can call it even a code of conduct
or a set of principles and rules
that guide my behavior
to accomplish certain kinds of outcomes.
There are no beliefs without priors.
What are the priors that you base your code of conduct on?
Yes, that's a very good question.
And it puts me on the spot here
and I'm not prepared for it, but I have to follow.
So the priors are, you can call them axioms perhaps,
things like diminishing suffering.
Things like, for example,
and perhaps one of those rules or points of view or tools,
if you will, is taking sort of what Peter Singer calls
the universe points of view, point of view,
or sort of an outside point of view than my own, right?
So when it comes to, with respect to cows,
I take a point of view outside of me and the cows, hopefully.
And sort of I'm able to look at my suffering
of not eating a cow and their suffering of being eaten, right?
So if my prior is minimize suffering,
because basically that's the axiom based on which I can deduce
that something or someone exists like an even entity,
like a sentient being, right?
Is the suffering, does it suffer?
That's sort of my test, if you will,
not during test, but a test of being a sentient being.
Can you suffer? Can it suffer?
And if it can suffer, then my principle
of minimizing suffering must be the guiding principle
with which I relate to it.
That's kind of like, if you will,
sort of the foundation of my personal ethics.
Can it suffer?
Then the next is how can I minimize
the suffering of that entity?
And then basically everything else builds up from there, right?
When you become an adult,
I think the most important part of it
is that you take charge of your own emotions.
You realize that your own emotions are generated
by your own brain, by your own organism,
and they're here to serve you,
and you're here to serve your emotions.
The emotions are there to help you
for doing the things that you consider to be the right thing.
And that means that you need to be able to control them,
to have integrity.
If you are just the victim of your emotions
and not do the things that are the right thing,
you learn that you can control your emotions
and deal with them, right?
You don't have integrity.
And what is suffering?
Pain is the result of some part of your brain
sending a teaching signal to another part of your brain
to improve its performance.
If the regulation is not correct
because you cannot actually regulate that particular thing,
then the pain will endure and usually get cranked up
until your brain figures it out
in terms of the pain signaling center.
But by telling them, actually, you're not helping here, right?
Until you get to this point, you have suffering.
You have increased pain that you cannot resolve.
And so in this sense, suffering is a lack of integrity.
The difficulty is only that many beings cannot get
to the degree of integrity where they can control
the application of learning signals in their brain,
that they can control the way the reward function
is being computed and distributed.
So then according to your argument,
suffering is just like you said before,
a simulation or a part of a simulation then.
Well, everything that we experience is a simulation.
We are a simulation.
But to us, of course, it feels real.
There is no helping around this.
But what I have learned in the course of my life
is that all of my suffering is a result of not being awake.
Once I wake up, I realize what's going on.
I realize that I am in mind.
The relevance of the signals that I perceive
is completely up to the mind.
Because the universe does not give me
objectively good or bad things.
The universe gives me a bunch of electrical impulses
that manifest in my tannamos
and my brain makes sense of them
by creating a simulated world.
And the valence in that simulated world
is completely internal.
It's completely part of that world.
It's not objective.
And so I can control this.
So ethics or suffering is a subjective experience.
And if I'm basing my ethics on suffering,
therefore my ethics would be subjective.
Is that what you're saying?
No, I think that suffering is real with respect to the self.
But it's not immutable.
So you can change the definition of yourself,
the things that you identify with.
Imagine there is a certain condition in the world
where you think a particular party needs to be in power,
an order for the world to be good.
And if that party is not in power, you suffer.
You can give up that belief
and you realize how politics actually works
and that there's a fitness function going on
and that people behave according to what they read
and whatever.
And you realize that this is the case
and you just give up on suffering about it
because you realize you are looking
at a mechanical process
and it plays out anyway regardless
of what you feel about how that plays out, right?
So you give up that suffering.
Or if you are a preschool teacher
and the kids are misbehaving and they are mean to you,
at some point you stop suffering about this
because you see what they actually do.
It's not personal, right?
That's what Stoic philosophy is all about, right?
Stoics say there is no point.
So first of all,
Stoics say that we suffer not from events
or things that happen in our life
but from the stories that we attach to them.
And therefore, if we change the story,
we can change the way we feel about them
and thereby remove the suffering.
And they say that there's the only thing
that we can focus on and do something
about is our own thoughts
and things like the kids in school
or the party are things
that are completely outside of our control.
And therefore, there is no point
to get aggravated about them.
And there's very little things
that are completely under our control.
So we can't really control fully our body.
We can't really control our health completely.
Things can always go wrong there.
The only thing they say you can fully,
completely control is your thoughts.
And that's where your freedom comes to be
and that's where your power comes to be
and that's where you're the one and only, right?
In that mind, in that simulation, you're the God.
So this ability to make your thoughts more truthful,
this is Western enlightenment in a way,
this is Aufklrung in German.
And there is also this other sense
of enlightenment, Erleuchtung,
that you have in a spiritual context.
And so Aufklrung fixes your rationality
and Erleuchtung fixes your motivation.
It fixes what's relevant to you
and how we relate to this.
It fixes the relationship between self and universe.
And often they are seen as mutually exclusive
in the sense that Aufklrung leads to nihilism
because you don't give up your need for meaning.
You just prove that it cannot be satisfied.
God does not exist in any way that can set you free.
And in this other sense,
you give up your understanding
of how the world actually works so you can be happy.
You go to a non-dual state where you represent
that all people share the same cosmic consciousness,
which is complete bullshit, right?
But it's something that removes the illusion of separation
and there are suffering that comes
with the separation and so on.
So where is that...
Yeah, sustainable.
Where does that leave us with respect to ethics though?
So maybe you were able to dismantle much or most
or maybe all of my ethics, did you?
I don't know all of your ethics, but...
Well, if you asked me for the foundation
and the best I could come up with the sort of the suffering test.
Yeah, it's not good.
The problem is really that if I can turn off suffering
or if I get counter-intuitive results,
there's this anti-natalism.
Anti-natalism is an obvious way to end suffering, right?
Stop putting new organisms into the world
and the existing set of organisms
in the least painful way possible, right?
AI could help with this.
The question is, can we make it safe
or is the AI going to leave a couple cells left
that can give rise to new suffering later on?
But so if you have a completely cold and dead universe,
then there'll be no suffering, right?
Yes.
Is this what you want?
Right, clearly.
So that's not the most...
According to...
It's not so clear.
I'm anti-natalist, but my kids are not.
So I have this division there.
But...
So what's that say about where are you coming from then
with respect to ethics?
So let's say my suffering test is not good enough.
I think existence by itself is neutral.
The reason why there are so few stoics around.
Have you thought about this?
Stoicism has been discovered a long time ago.
Almost nobody's a stoic.
How is that?
Well, I know a few people who are stoics, actually.
Yeah, but the majority is not.
Well, it seems to be so obvious.
Only worry about the things that you can actually change
to the degree that the very helps you changing them.
Yes, so...
So why is nobody a stoic?
Almost nobody.
Well, I wouldn't say nobody.
I'd say a few people are stoic and they're amazing
and they're inspirational and they're motivational
and they're good role model for sort of like
how I want to behave and how I want to live
and how I want to act in this world.
I suspect that stoicism is maladaptive
from a permanent evolutionary perspective.
Most cats I have known are stoics,
which means if you leave them alone, they're fine.
Like their baseline state is okay.
They're okay with themselves and their place in the universe
and they just stay at that place.
And only when you disturb that
because they need to use the bathroom
or because they are hungry or they want to play or whatever,
this equilibrium gets disturbed
and they do exactly what's necessary
to get back to the equilibrium state
and then they're fine again.
And a human being is slightly different.
A healthy human being is set up in such a way
that when they wake up in the morning,
they're not completely fine.
And then they need to be busy during the day,
but in the evening, they're fine.
In the evening, it's done enough
to make peace this existence again
and then they can have a beer with their friends
and everything is good.
And then there are some individuals
which have so much discontent with them themselves.
Like the human is the animal that is discontent
that they cannot take care of this in a single day.
But even after several weeks of sustained work,
they are still in a state where it's not good enough
and only when they have this amazing thing
where they get their noble price,
they're fine for like half a day.
And the way this is the way we are set up
to different degrees.
And from an evolutionary perspective,
you can totally see why that would be useful
for a group species.
For an individual species that is not so much a group species
like cats are not really meant for groups.
They're very much singletons.
For them, it's rational to be historic.
But if you're a group animal,
it makes sense that the well-being of the individual
is sacrificed for the well-being of the group.
So each individual is overextending themselves
to make the group more successful
and produce a surplus of resources for the group
as a result.
Right, but evolution also diversifies things
so that if one kind of feature becomes maladaptive
in a new environmental change,
then a diverse part of that population
would be more adaptive.
And so that's why evolution sort of hedges its bets
with the greatest variety and diversity possible, right?
So yeah, there will be some people who would be like that
and some people who would be like otherwise.
And this way, on the whole,
they're evolutionarily most adaptive.
But some will be more adaptive to one kind of situation
and others will be more adaptive
to other kinds of situation.
I'm not sure if this is true.
So for instance, we find that larger habitats
don't necessarily have more species.
And that's because there's a fearsome competition,
which means that there's less slack in the evolution.
So for instance, New Zealand had a lot of species
before there was immigration of other species
and they obliterated most of the stuff that existed,
mostly because the stuff that came in
was result of a much fiercer competition
than existed in small New Zealand.
Sure, yeah.
And in a way, the same thing happens now.
We are the result of evolution.
We are, as Minsky said, evolution's way
to put the airplanes into the sky
and make these clouds that the airplanes make.
And we reduce the number of species dramatically.
We are like probably eventually going to look like a meteor
that is going to obliterate a large part of the species
on this planet.
So what's that say about ethics and technology?
So what's the solution then?
So is there space for ethics and technology?
Of course there is.
It's about discovering the long game, right?
So when you do something, you have short influences
and you have long influences.
And based on what you think is the right thing to do,
you need to look at the long-term influences.
But you also need to question why you think
that something is the right thing to do,
what the results of that are, which gets tricky.
But we can agree on that, that's fantastic.
But tell me then, how do you define ethics yourself?
Well, the tension between the way I define ethics
and some other people in AI, and ethics in AI define it is,
there are some people which think that ethics
is a way for politically savvy people
to get power over STEM people.
And with considerable success,
it's largely a protection racket.
There's also a way that ethics happens where you have studies
where somebody asks a million people
of whether a traffic car should run over young people
or old people first.
And then they publish the results
and it makes a big splash because people can relate to this.
But it's...
Well, this is ethics, we can do that.
This has just happened so that philosophers
had this trolley problem,
and suddenly they see an application.
But it's largely the same thing as saying
that the majority of people would want a minor tawry
to be confined in labyrinths rather than in public forests.
Right.
That is the situation that the gods were in
or the Cretan king was in
when this sign turned out to be a minor tawry.
But it rarely happens.
Right.
In the same sense, it rarely happens
that a self-driving car will have to make that decision.
Probably not often enough to require an if-then in its code.
But how do you define ethics for yourself?
What is ethics?
Because you asked me this and I gave my best to answer.
Oh, so I also try to do this.
My best answer is that ethics is the principle
negotiation of conflicts of interest
under conditions of shared purpose.
If I share purposes with others, with society,
with other beings, with conscious beings,
and that's my decision based on the way my mind is set up
right now, and I run into conflicts of interest with them.
I have to deal with this.
For instance, when I look at other people,
I mostly imagine myself as being them
in a different timeline.
Everybody is in a way, me in a different timeline,
but in order to understand who they are,
I need to flip a number of bits.
So I think about which bits would I need to flip in my mind
to be you.
Right.
And these are the conditions of negotiation that I have with you.
So we can agree on that, perhaps, on that definition,
but then where do the cows fit in?
Because we don't have a shared purpose with them.
So how can you have ethics with respect to the cows, then?
The shared purpose doesn't objectively exist.
A shared purpose means that you basically
project a shared meaning above the level of your ego,
being the function that integrates expected rewards
over the next 50 years.
Well, exactly.
That's what Peter Singer calls the universe point of view,
perhaps.
Yeah, well, if you can go to this eternalist perspective
where you integrate expected reward from here to infinity,
most of that being outside of the universe.
This leads to very weird things.
Most of my friends are eternalists, in a way, right?
All these romantic Russian Jews.
We are like that, in a way, this Eastern European
shape of the soul, that creates something like a conspiracy.
It creates a tribe, and it's very useful for cooperation.
So shared meaning is a very important thing for cooperation
that is nontransactional.
But there's a certain kind of illusion in it.
To me, meaning is like the ring of Mordor.
So you have to carry it.
If you drop the ring, you will lose the brotherhood of the ring,
and you will lose your mission.
You have to carry it, but very lightly.
If you put it on, you will get superpowers,
but you get corrupted, because there is no meaning.
You get drawn into a cult that you create.
And I don't want to do that, because it's
going to shackle my mind in ways that I don't want it to be bound.
I really, really like that way of saying,
but I'm trying to extrapolate from your sort of print
definition of ethics a guide of how we can treat the cows,
and hopefully how the AIs can treat us
within that same definition.
That's what I'm trying to push here,
and see if that's possible at all.
OK, so there is some.
Because my claim is that the way we treat cows, probably,
is another way of how AIs could possibly treat us.
I think that some people have this idea similar to Azimov,
that at some point, the boomers will become larger and more
powerful, so we can make them washing machines,
or let them do our shopping, or let them do our nursing,
and then we will still enslave them,
and we'll negotiate the conditions of coexistence with them.
And I don't think this is what's going to happen primarily.
What's going to happen is that corporations, which
are already intelligent agents, it just
happened to borrow human intelligence,
automate their decision making.
At the moment, a human being can often
outsmart a corporation, because the corporation has
so much time in between updating its Excel spreadsheets
and the next weekly meetings.
Now, imagine it automates everything,
and the weekly meetings take place every millisecond.
And this thing becomes sentient, understands
its role in the world, and the nature of the world,
and physics, and everything else,
because it has scalable intelligence.
We will not be able to outsmart that anymore.
And we will not live next to it.
We will live inside of it.
Intelligence will come.
The AI will come from top down on us.
We will live not next to it, but inside.
We will be its gut flora.
And the question is how we can negotiate
that it doesn't get the ideas to use antibiotics,
because you're actually not good for anything.
Exactly.
And why wouldn't they do that?
I don't see why.
So some people made the suggestion that it was
ethics that could guide them to treat us just
like you decided to treat the cows when you turned
14 and you decided not to eat meat.
I mentioned there are a bunch of orangutans
that sit in the forest and born new and decide
to breed the smartest members over a few generations
to get people.
And they see the big risk of that,
because they're already smart enough to glimpse that.
And they try to come up with a code
that they would give on their offspring
to make sure that their offspring will never
go against orangutans.
This is probably not successful, because we don't have
the ability to outsmart beings that are many magnitude
smarter than us.
You can make some mathematical proofs,
but I don't see an obvious proof that we will find a way
to build a system that guarantees that all these AIs will not
turn against us.
No, no, I agree.
You can't make some AIs safe, but I
don't see how we can make all the AIs safe that will be built.
I agree.
I agree with that.
I'm just trying to see if there's
any possible scenario which could treat us kindly,
because perhaps AIs could have their AI ethics.
And according to that AI ethics, they
would treat us as a means, not as an end.
And just like you decided to treat cows kindly,
they may decide to treat us.
But I'm just wondering.
So I'm trying to bring ethics into the relationship,
not only between human and cows, but AI and humans.
So the thing is that you decided to define
ethics axiomatically.
And you, I think, probably have a hunch
that your axiomatic definition is not completely
consistent with itself.
It's just the best you came up with under the circumstances.
For instance, if you really go after eliminating suffering,
you should probably put some anesthetic into the water
supply globally to alleviate suffering,
and then let everybody face happily out of existence
in a way that would satisfy the goal in an optimal way.
And it's probably not what you want.
So you also want to preserve human aesthetics, maybe.
And to preserve these human aesthetics,
the shape of the mind that we have and this consciousness
that we have is going to create some suffering.
And this is the tangent.
And you have to make a decision at some point.
Imagine you take an AI that is actually sustainable,
and you ask this AI, if you give you a job,
you want to be around in 10 years from now.
You cannot build a government that
cares about us being around a 10,000 years from now
effectively, because this is not an incentive
that you can actually give the government,
in a sense, this incentive is going
to defect from the incentive that we wanted to have.
So let's build an AI.
And the AI is going to be around in 10,000 years
from now, no problem.
If you tell it, make sure that we are there too.
And the AI is probably going to kill 90% of us,
hopefully painlessly, and breed everybody else
into some kind of harmless yeast.
So they need to keep around.
This is not what you want, I guess, right?
Even though it would be consistent with your stated axioms.
So getting the axioms consistent is super hard.
And even with the cold, the best, most ethical,
according to my own argument, the best, most ethical universe
would be a cold, dead universe, because there
will be no possibility of suffering there, right?
That's clearly a problem.
Yes, and now the next thing with the suffering axiom
is that the suffering is important,
because you think of it as something that cannot be
turned off by itself.
So we basically think of suffering
as something that is not the choice of the one who suffers.
Because why would you want to suffer, right?
So it's something that the universe does to you,
and we have to change the conditions of the universe
in which you are in so you don't suffer.
But what we forget about this, that suffering
is an evolutionary adaptation.
It's created to make you jump through all these hopes
in order to eat more and eat others.
It's a very perverse thing.
And you can turn off the suffering.
As soon as you become conscious enough and awake enough,
you can deal with it and get rid of your suffering.
And so at some point in your mental development,
suffering becomes a choice.
And for the other animals, it's all about, yeah.
So you could think, OK, one thing that we want to do
is we want to wake up as many organisms as possible
to give them that choice, to give them
agency over their suffering.
And this will then open another Pandora's Box
of ethical conundrums.
But on a very short range, maybe we
don't need to make these decisions right now right here,
we can basically operate in a framework
where we agree with our loved ones
about shared purposes and shared systems of meanings
and want to operate within those.
And in these narrow constraints, we can get ethics to work.
I don't see how to get ethics to work globally.
Right, right.
So, Joshua, it's been a fascinating two hour
conversation with you.
I really enjoyed it.
I'm not surprised that I rediscovered that I don't know.
I've been mostly aware, though occasionally I forget,
that I really don't know.
Thank you for reminding me that.
Tell me, where can people find more about you and your work?
There's some on YouTube.
I'm also getting myself to write a book, hopefully, these days.
What's the book about?
I basically try to get a glimpse on this civilizational
intellect, on this hive mind that we have been created
and that makes sense of some of the concepts that
are broken in our culture, ideas that
are broken in our mind, consciousness, self, meaning.
And we don't know how to talk about them.
And AI has discovered how to talk about them.
AI has discovered, and we don't know.
I think that basically our poll is largely doesn't know.
There are many people which do know.
But I think we need to carry these ideas together
in one place so we can talk about them
without getting too excited about them or upset.
Because it's not about giving meaning to people's lives
or something.
It's not about building better self-driving cars.
At some level, it's about understanding who we are
and what our relationship to reality is.
And AI has figured out a few things
that we didn't know 100 years ago.
Yeah, but isn't that figuring out who you are?
Isn't that giving you meaning?
No.
It's much better.
I discover what the nature of meaning is.
I discover how this is wired into my brain.
And it's in a way becoming an adult.
The first stage and the maturity of a mind,
and maybe the last stage, is where
you discover what you are, how you are built,
what you actually, how you function, your own nature.
Well, Josh, I want to talk to you for another two hours.
So perhaps I hope that one day.
You can set up another date for that out.
I hope to do that in person, actually, hopefully soon.
But in the meantime, how do we wrap up
this two-hour conversation with you?
What's the most important thing or the single message
that you want to send away our audience with today?
Who is our audience?
Well, who do you want your audience to be?
You can send a message to anybody.
My audience is my audience, but they
have their very wide diversity of people.
Lots of IT, basically geeks, nerds, transhumanists,
cryonists, futurists, IT professionals, philosophers,
engineers, curators.
OK.
OK.
So something very simple and boring.
I think that the field of AI is largely misunderstood,
because there are two industries, the AI hype industry
and the anti-AI hype industry, which
have very little to do with AI.
The practice of AI is, in a way, statistics on steroids.
It's experimental statistics.
It's identifying new functions to model reality.
And that is what statistics is doing.
And largely, it hasn't gotten to the point
yet where it can make proofs of optimality.
It's largely experimental, but it
can do things that are much better than the established
tools of statisticians.
And this in itself is not so exciting.
There's also going to be a convergence
between econometrics, causal dependency analysis,
and AI and statistics.
It's all going to be the same in a particular way,
because there's only so many ways in which you
can make mathematics about reality.
And we confuse this with the idea of what a mind is.
And they're closely related, because I
think that our brain contains an AI that
is making a model of reality and the model of a person
in reality.
And this particular solution of what an AI can do,
this particular thing in the modeling space,
this is what we are.
So in a way, we need to understand
the nature of AI, which I think is
the nature of somewhat general function approximation,
sufficiently general function approximation.
Maybe all of the function approximation
that can be made in the long run,
all the truth that can be found by an obituary observer
in particular kinds of universes that
have the power to create it.
This could be the question of what
AI is about, how modeling works in general.
And for us, the relevance of AI is,
does it explain us who we are?
And I don't think that there is anything else that can.
So let me see if I get this right, just because,
to see if I can simplify.
And I'm probably going to fail.
But so we need to understand the nature of AI.
That's kind of your call.
But then you said that we are, in a way, an AI.
Is that the case?
No, the brain is an AI.
I am a self.
A self is a model that the mind has created inside of my brain.
Right, so that's a little AI instantiation.
And then if we create that other AI that we're talking about,
it would perhaps give us a glimpse of this other AI in here.
And we would understand the nature of our AI in here
by creating that other AI out there.
Actually, we already do.
So the things that Minsky and many others
have contributed to this field and the things
that we are talking about right now
are already a much better understanding
that our part of humanity, our civilization,
had a couple hundred years ago.
Many of these ideas we could only develop
because we began to understand the nature of modeling,
the nature of our relationship to the outside world,
the status of reality.
Like we started out from this dualist intuition
in our culture, that there is a res extensor
and a res cogitanz, a sinking substance
and extended substance, the stuff in space universe
and the universe of ideas.
And we now realize that they both exist,
but they both exist within the mind.
Part of what we have in the mind is stuff in a free space.
Everything perceptual gets mapped to a region in free space.
We also now understand physics.
Physics is not a free space.
It's something else entirely.
The free space is only apparent as the space
of potential electromagnetic interactions
at a certain order of magnitude of scaling
above the plank length, where we are entangled with the universe.
Our minds are entangled with the universe.
This is what we model.
And this looks three-dimensional to us.
And everything else that our mind comes up with
is stuff that cannot be mapped onto region to free space.
This is res cogitanz.
So in a way, we transfer this dualism into a single mind.
Then we have the idealistic monism
that we have in many spiritual teachings.
This idea that there is no physical reality,
that we live in a dream.
If you are a character in a dream
drawn by a mind on a higher plane of existence,
then that's why miracles are possible.
And then there is this western perspective
of a mechanical universe that is entirely mechanical.
There's no conspiracy going on.
And now we understand that these things are not in opposition.
They are complements.
We actually do live in a dream,
but the dream is generated by a neocortex.
So our brain is not a machine
that can give us access to reality as it is,
because that's not possible for a system
that is only measuring a few bits at a systemic interface.
There is no colors and sounds that fits through your nerves.
We already know that.
The sounds and colors are generated
as a dream inside of your brain.
The same circuits that make dreams at night
make dreams during the day.
Right.
So this, in a way, is our inner reality.
It's being created on the brain.
The mind on the higher plane of existence exists.
It's the brain of a primate that is made from cells
and lives in a mechanical, physical universe.
And magic is possible because you can edit your memories.
Right. You can make that simulation anything you want it to be.
It's just many of these changes are not sustainable.
That's why the sages warn against using magic,
because down the line,
if you change your reward function, bad things may happen.
You cannot break the bank.
So let me see if I can simplify all of this in a sentence.
And if you agree with it.
So we need to understand the nature of AI
in order to understand ourselves.
Is that it?
So I would say that AI is the field that took up the slack
after psychology failed as a science.
Psychology got terrified of overfitting.
So it stopped making theories of the mind as a whole.
It restricted itself to theories with very few free parameters.
So it could test them.
And even those strategy didn't replicate as we know now.
So after PhDs, psychology largely didn't go anywhere
in my perspective.
It might be too harsh because I see it from the outside
and outsiders of AI might also argue
that AI didn't go very far.
And as an insider, I'm more partial here.
And maybe I have too much bias and give it too much credit.
But to me, most of the things I've learned
by looking at the both of this lens
of seeing us as information processing systems.
So you agree with the statement summary that I made?
Yeah.
OK.
Because I have this metaphor that I use every once in a while
saying that technology is a magnifying mirror.
It doesn't have an essence of its own,
but it reflects the essence that we put in it.
And of course, it's not a perfect image
because it magnifies and it amplifies things.
So I think those could be mutually supportive, right?
Because you're saying we need to understand the nature of AI
to understand who we are.
And I like that very much, actually.
Yeah.
But just the practice of AI is 90 degrees,
it's automating statistics and making better statistics that
run automatically on machines.
And it just so happens that this thing is largely
co-extensional with what mines do.
And it also just so happens that AI was largely founded
as a discipline by people like Minsky
to understand the nature of our minds
because they had fundamental questions about our relationships
to reality.
Right.
And what's the last 10%?
Of what?
Other than statistics.
You said it's 90% statistics.
What's the rest?
Oh, the rest is people coming up with dreams
about our relationship to reality
using the concepts that we develop in AI.
Right.
So we identify models of things that we can apply in other fields.
It's the deeper insights that we actually go for.
Most of what we do in AI is about applications.
It's about utility down the line.
But there are these things where we really do it.
The thing that Feynman said that makes physics like sex
also makes AI like sex.
Sometimes something useful comes from it.
A new better way to make self-driving cars or play jeopardy
or help people in many circumstances in their life
or to make better agents running on your phone.
It's not why we do it.
We want to understand how we work.
Right.
And that's a brilliant place to end our conversation
because I feel the same way about philosophy, by the way,
that it's just like Feynman felt about physics
and you feel about AI.
I feel the same way about philosophy.
Yeah.
So these remaining 10% are innovative philosophy.
But in all of these fields,
most of the practitioners are trained
in the main methodology of their field.
So our philosophy tends to be bad.
Yeah.
And I think my job is to try to make it slightly better
to the degree that I can.
And does that mean by extension that, of course,
most physics then would be bad and most AI then would be bad
because they fall within that 90%?
No, no.
I think the AI as a practical thing can be very good.
Right.
Most physicists are not concerned with foundational physics
with the nature of the universe.
Most physicists are concerned with material science
or many, many other extremely practical things.
It's only very small minority that worries about the deepest things.
And the same thing happens in AI or neuroscience.
And it's not that there anybody is to blame
for doing development things.
It's actually very good that a lot of people are willing
to put up with development things
and take down the garbage.
Right.
And I'm grateful to them.
It's just I can't do that myself somehow.
I think as you put that, I would probably go extinct.
Yeah.
And it's not a source of pride in a way.
It's the recognition of a disability.
Exactly.
It's the bug.
Yeah.
But it is, it's marginally useful
because society needs a few of us.
So it does.
I mean, we're still here.
We're here, but it's a struggle sometimes.
Yeah.
But this is our own choice how much we struggle
because objectively we are here and the coffee is good.
Thank you for reminding me that.
And I love the coffee.
I'm a coffee fanatic.
I, yeah, that's a whole lot of story,
but I'm a coffee fanatic.
Joshua back.
Thank you so much for spending over two hours with us today.
I'm looking forward to our next conversation.
And I wish you the very best while the party is lasting.
Likewise, it was such a great conversation.
Thank you for this time we spent together.
If you guys enjoyed this show,
you can help me make it better in a couple of ways.
You can go and write a review on iTunes
or you can simply make a donation.

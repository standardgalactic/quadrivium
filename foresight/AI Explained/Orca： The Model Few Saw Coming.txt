Do you remember this paper, less than two weeks old?
It made waves by concluding that open source models
can mimic the style, but not the factuality of chat GPT.
Overall, we can conclude they say
that model imitation is a false promise.
Well, 48 hours ago, we have this, a 51 page report on Orca,
based on a small 13 billion parameter model.
I don't often comment on open source models
because they're simply not competitive with open AI's models.
But Orca is not just competitive with GPT 3.5.
It beats it in quite a few well-established benchmarks
and even matches GPT-4 in a couple of tests of reasoning.
As always, I've read both papers in full
and can also bring in just-released comments
from Sam Altman and Ilya Sutskova on competition from open source models.
But let's start with Orca, named presumably because Orca's or killer whales
are frequent visitors to South American coastlines.
And South America is, of course, the land of llamas and vicunas.
But all the research was done by Microsoft,
which I find interesting and I'll come back to that at the end.
But why did they make Orca and why does it perform better
than models like llama, alpaca, and vicuna?
Well, they say here in the abstract
that those other models lack rigorous evaluation,
resulting in overestimating the small model's capability
as they tend to learn to imitate the style
but not the reasoning of LFMs, large foundation models.
To address these challenges, we developed Orca,
a 13 billion parameter model, that learns to imitate
the reasoning process of the larger models.
Orca learned by looking at GPT-4's step-by-step thought processes
and is guided by teacher assistants from chat GPT, which is GPT 3.5.
And to give you a taste of what's to come,
Orca surpasses conventional state-of-the-art models,
such as vicuna, by more than 100%
in complex zero-shot reasoning benchmarks,
like the big bench hard, which I'll talk about,
and by 42% on AGI eval.
It goes on, Orca reaches parity with chat GPT
on the big bench hard and shows competitive performance
in professional and academic examinations
by the SAT, LSAT, GRE, and GMAT.
And I know many of you will be interested in this footnote.
We are working with our legal team to publicly release
a diff of the model weights in accordance
with Lama's release policy.
So if this is anything like Lama,
it's going to be leaked across the internet imminently.
I'm going to show you so many tests and benchmarks in a moment,
but just to give you a sample,
here is Orca outperforming chat GPT in the vicuna evaluation set
and matching text DaVinci 3 in the SAT, LSAT, GRE, and GMAT.
And as I'll touch on later,
this was zero shot without chain of thought
or any advanced methods.
You can watch pretty much any of my other videos
to see how advanced prompt engineering
would probably boost those results still further.
For those who didn't know,
13 billion parameters is about 7% the size of GPT-3,
which is 175 billion parameters,
and possibly around one or 2% of GPT-4's size.
That gives you an indication of the difference
in size between Orca and these models that it's competing with.
And if that doesn't make any sense,
a smaller size means it can be run on much smaller devices,
like a desktop or even possibly a laptop.
The authors start off by giving a little slap to the other paper.
You know that one that said model imitation is a false promise.
And they continue that contrary to this assertion,
it is possible to reduce the gap with proprietary LLMs
on multiple zero shot benchmarks that require sophisticated reasoning.
As we'll see, models like Vakuna claim to have 90% of chat GPT's quality,
but when it came to reasoning tasks or more technical tasks,
it basically flopped.
Here's a chart I'll come back to outlining
some of the more technical challenges you can give a language model.
We should remember that Vakuna is a fine-tuned version
of the Llama model,
and it's competitive or even better than Palm II.
But give it some of the harder challenges for a language model,
and it really struggles, as you can see in this column.
Take logical deduction, where it only scored 1.2%.
Well, this Orca model was 2,900% better than that,
scoring 36% competitive with chat GPT.
I'm going to come back to the big benchmark,
but look for a second at causal judgment,
where Orca, a 13 billion parameter model matches GPT4,
which is about 100 times the size.
But back to how they actually did it.
Models like Alpaca and Vakuna were given lots of query and responses
from chat GPT or GPT4.
But what they did is they leveraged system instructions,
asking models like GPT4 and chat GPT to think step by step.
This gave Orca access to detailed responses from the model
that explained the reasoning process of the teacher
as it generates the response.
It allowed these parent models of GPT3.5 and GPT4
to be much better tutors for this young Orca.
Also, they let the teachers of chat GPT, which is 3.5,
and GPT4 give far more examples to their student.
5 million and 1 million examples, respectively.
That compares to the other models you may have heard of,
like Alpaca, Wizard, Vakuna, etc.,
which had tens of thousands or the low hundreds of thousands of examples.
But again, the key difference is the explanations,
the step by step thinking that the smaller Orca could then imitate.
They give a quick demo here of how the other open source models
learn from their GPT parents,
with a simplistic question and answer format.
In contrast, the authors leveraged system messages
to get chat GPT and GPT4 to think step by step,
leading to much richer explanations, as you can see in this diagram.
It wasn't just let's think step by step,
by the way, also things like explain like I'm 5.
They also wanted the task to be as complex and diverse as possible,
so they used the Flan collection.
This was released by Google in February,
and focused on balancing the kind of prompts and tasks
that you fine tune the language models on.
You can see here the 16 system messages
that they give to chat GPT and GPT4,
and you can see here the kind of difference that that makes.
Imagine a language model trying to learn from this human.
The human is asked,
pick which sentence is not logical.
Sentence A, people in the desert often look forward to flood,
or sentence B, people in the desert often look forward to rain.
The human responds,
there is no reason to look forward to a flood,
because floods cause damage, the answer is sentence A.
Now yes, a language model can learn from that,
but by leveraging those system assistant messages,
look at the kind of response that GPT4 gives.
Now Orca can learn a lot more from that explanation,
and that's one of the main reasons it's better
than all the other open source models.
Because remember, Vikuna is the best of the open source models.
In this leaderboard, it has an elo of 1054,
better even than Palm II Bison.
All the models higher than it are proprietary.
But there is another reason why Orca performs so much better.
You might have wondered,
why didn't they just use only GPT4?
Well yes, there were cost and time considerations,
but there was another factor that they found.
They were able to use chat GPT or GPT3.5 as an intermediate teacher.
That teacher, chat GPT, was able to reduce the gap in capabilities.
So Orca got smarter and better able to learn.
A bit like progressive learning,
where you first learn from easier examples,
then followed by harder ones.
After that, they gave it outputs from GPT4.
Notice by the way,
what happens if you skip the chat GPT teaching assistant
and only train on those one million examples from GPT4.
What happens is a bit like a student struggling in a class
that's too advanced for them.
Orca actually performs worse in those circumstances, averaging 37%,
but with that intermediate teacher beforehand, it gets 41.7%.
Speaking of time,
it only took about 200 hours to train Orca on 20 A100 GPUs.
They did take a few weeks to collect the data from chat GPT and GPT4,
but presumably if they're planning to open source this,
which they say they are,
then that step could be skipped by the wider community.
Let's now look at some more of the results.
First, for open-ended generation, not multiple choice.
Orca is 95% of chat GPT quality
and 85% of GPT4's quality as assessed by GPT4,
but they wanted to quickly move on to some more definitive tasks
because there is a problem of using GPT4 as an assessor.
For example,
they observed that there is a positive bias in GPT4 evaluation
toward the response of the first model in the comparison set.
This reminded me of the unfaithful reasoning paper
that I talked about in one of my recent videos.
You can't always trust GPT4 to give its true reasoning,
but here it is in more objective multiple choice questions.
And notice how much harder many of these tests are
for even these advanced language models.
I am fortunate and proud to have attained a perfect score
in some of the tests in this chart,
like the GRE and GMAT.
They were part of the Aquarat tests that they gave the models,
so I can say that they really are quite challenging,
hence why GPT4 only gets a 40%.
You can see that throughout,
Orca outperforms Vecuna by quite a margin
and is very competitive with Textavinci 3.
Of course, overall, it does lag behind GPT4,
but this is all zero shot.
A bit later on,
I'll come back to the range of methods that we could use
to further improve on Orca.
The percentages, by the way,
are the improvements on Vecuna,
again the second best open source model.
So far, we've looked at human centric benchmarks
like the GMAT and GRE.
These are grouped with the lovely name AGI EVAL,
and as we've seen,
even the top models lag behind the top human performers.
But what about a benchmark specifically for language models?
It's called Big Bench Hard.
The original Big Bench had 207 tasks,
but language models got so good
that they had to narrow down the benchmark
to just the 23 challenging tasks
where human raters still did better than language models.
Now, it turns out when you add Chain of Thought prompting
to the models, they do even better
and there are even fewer tasks that humans are better at.
But anyway, all you have to remember
is that these are 23 of the hardest tasks for language models.
And I'll just let you compare the results for yourself.
But the trend is really quite clear.
Orca massively outperforming
the previous best open source model, Vecuna,
beating even chat GPT on average,
but still, of course, lagging behind GPT4,
except for a few tasks.
Look at Web of Lies,
where Orca outperforms GPT4.
That would be a question like this.
Alexis says Shonda tells the truth.
Jim Lies?
Antoine says Jim tells the truth.
Shonda says Antoine Lies.
Does Alexis tell the truth?
Or what about temporal sequences,
where Orca absolutely crushes Vecuna
and doubles chat GPT's performance?
That would be a situation like this.
Now, I'm not going to read it all out,
but essentially you have to figure out
when the timings match up.
Basically keeping track of time
and Orca does really well
and chat GPT flops getting it wrong.
Interestingly, they also tested all four models
on that common sense reasoning question
that I demonstrated for smart GPT,
about hanging the clothes to dry.
As you might remember,
you can use prompt engineering
to nudge the models to almost always get it right,
which is partly why I view these results
more as a baseline rather than a cap.
And the authors admit this too.
Orca has been trained on data
that simulates zero shot setting
with standard prompts.
The model's performance in other contexts,
such as multi-turn conversations,
like the DERA paper I talked about on the channel,
in context learning and few shot learning,
or advanced prompting techniques,
that smart GPT or Tree of Thoughts, for example,
and they say like chain of thought prompting,
remains untested.
These results are a baseline, not a cap.
They mention other ways that Orca could be improved,
for example, through tool augmentation.
And that's not just calculators,
calendars, Bing, or auto GPT.
I was going to do a separate video on this paper,
but I'll just mention it here.
This paper from last week demonstrated
that larger models can create tools
that smaller models can then use more efficiently.
Once the best language model, say GPT-4,
has created a generic Python function,
which is the tool,
and then written some unit tests,
it can then wrap and hand over those tools
to smaller models like GPT-3.5,
or in this case, Orca,
and check out the toolmaking row
to see the improvement for chat GPT,
or in our case, Orca,
when they're given these tools created by GPT-4
or better language models.
Their performance across a range of tasks
goes dramatically up,
and we haven't even talked about
using a process-based reward model,
like in the Let's Verify step-by-step paper.
That, of course, could further improve Orca's performance.
Of course, when this model becomes publicly available,
I will test all of this out,
but it hasn't been open-sourced yet,
and they do say this model
is solely designed for research settings.
That does seem a little bit naive to me.
I mean, that's what Metta said
when they released Lama,
but then everyone and their grandma
just use the language model for whatever.
I do wonder what it means when they say
we are working with our legal team.
And it is particularly interesting to me
that this was all done by Microsoft.
I'm gonna go into a little bit of speculation here
about why I think they conducted this research.
You might remember that leaked memo from Google.
We have no motes, and they even mentioned Vakuna,
and talked about how it circumvented restrictions
on the OpenAI API by using shared GPT.
And my theory is that the Microsoft researchers
were testing this point from the memo.
The point was that training giant models from scratch
not only throws away the pre-training,
but also any iterative open-source improvements
that have been made on top.
It doesn't take long for those improvements to dominate,
making the full retrain extremely costly.
Maybe Microsoft is hesitating about future investments
in GPT-5 or GPT-6.
And they really wanna test out
if it's easy to imitate those large models on the cheap.
If it is, then why would Microsoft invest billions
in a new giant model?
That's my own theory as to why Microsoft is working on this,
but let me know in the comments what your theory is.
In the conclusion, the authors state that
Orca suggests that learning from step-by-step explanations
could significantly improve the quality of models
regardless of their size,
and that they hope these insights will inform the design
of more robust evaluation methods,
compared to those used for a vacuna, for example,
and the advancement of alignment and post-training techniques,
and the more effective use of powerful models
like GPT-4 as teachers.
And maybe they should have said,
and also with chat GPT as an intermediate teacher.
I'm gonna end with the thoughts of the leaders of OpenAI,
Ilya Sutskova, and Sam Oltman on open source models.
And I think there is a bit of a contrast
between the two answers.
Ilya Sutskova thinks that the gap is growing ever wider.
To the open source versus non-open source models question,
you don't wanna think about it in binary black and white terms
where like, there is a secret source
that will never be rediscovered.
What I will say, or whether GPT-4 will ever be reproduced
by open source models, perhaps one day it will be.
But when it will be,
there will be a much more powerful model in the companies.
So there will always be a gap
between the open source models and the private models.
And this gap may even be increasing this time.
The amount of effort and engineering and research
that it takes to produce one such neural net keeps increasing.
And so even if there are open source models,
they will never be,
they will be less and less produced by small groups
of dedicated researchers and engineers.
And it will only be the providence of a company, a big company.
While Sam Oltman seems to say
that even if open source models do catch up,
OpenAI will always have a different kind of moat.
What are your thoughts about the,
we have no moat document that was released lately?
The leak document.
The thing that is special about OpenAI,
and I think the thing that is so misunderstood by that document,
aside from the fact that we have a gigantic number of users
and people that have formed some sort of relationship
with us and our products,
is what OpenAI is special about
is figuring out what comes next.
It is the ability,
it is easy to copy something once you know it can be done.
And in that sense, sure.
It is very hard to go figure out what to do next.
And the ideas, the big ideas,
the medium size ideas, the small ideas,
and the careful execution on them
that it takes to get from here to superintelligence,
that's what our moat is.
Anyway, this video could have been at least three times longer.
There was so much I had to edit out for brevity.
If you're interested in me talking more about open source models,
do let me know in the comments.
I've got much more to say.
As always, thank you so much for watching to the end
and have a wonderful day.

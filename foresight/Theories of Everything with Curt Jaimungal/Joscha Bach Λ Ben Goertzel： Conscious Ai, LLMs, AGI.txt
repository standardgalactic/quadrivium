The first breakthrough to incontrovertibly human-level AGI to a superintelligence
is months to years. Will that be good or bad for humanity?
To me, these are less clear than what I think is the probable timeline.
Yoshabak is known for his insights into consciousness and cognitive architectures,
and Ben Gortzel is the seminal figure in the world of artificial general intelligence
and known for his work on open cog. Both are coming together here on theories of everything
for a theologution. A theologution is an advancement of knowledge,
couched both in tenderness and regard, rather than the usual tendency of debates,
which is characterized by trying to be correct even to the detriment of the other person,
maybe even destructively, maybe even sardonically. We have a foray in this episode into semantics
and Pierce's signed theory. This also extends into what it truly takes to build a conscious AGI.
An AGI is an artificial general intelligence, which mimics human-like intelligence.
But then the question lingers, what about consciousness? What differentiates mere
computation from awareness? Man, this was a fascinating discussion and there will definitely
be a part two. Recall the system here on TOE, which is if you have a question for any of the
guests, whether here or on a different podcast, you leave a comment with the word query and a colon,
and this way when I'm searching for the next part with the guest, I can just press control F
and I can find it easily in the YouTube studio backend. And then I'll cite your name either
aloud, verbally, or in the description. To those of you who are new to this channel,
my name is Kurt Jaimungle and this is theories of everything where we explore usually physics
and mathematics related theories of everything. How do you reconcile quantum mechanics with
general relativity, for instance? That's the standard archetype of the TOE, but also more
generally, where does consciousness come in? What role does it have to play in fundamental law?
It's fundamental, quote unquote, the correct philosophical framework to evaluate explanatory
frameworks for the universe and ourselves. We've also spoken to Yosha three times before,
one solo, that episode is linked in the description, another time with John Reveke and Yosha Bach,
and another time with Donald Hoffman and Yosha Bach. That was a legendary episode. Also,
Ben Gortzel has given a talk on this program, which was filmed at MindFest, which was a conference
about artificial intelligence and consciousness. If you enjoy the topics of mathematics, physics,
consciousness, AI, free will, and philosophy, then consider subscribing to get notified.
Enjoy this episode with Yosha Bach and Ben Gortzel.
Welcome. This is going to be so much fun. Many, many people are very much looking forward to this,
including me, including yourselves. Welcome to the theories of everything podcast. I appreciate
you all coming back on. Thank you for traveling us. I always enjoyed discussing this, Ben. It's
always been fun. And I think the first time we are on a podcast together.
Yes, wonderful. So let's bring some of those off-air discussions to the forefront. How did
you all meet? We met first at the AGI conference in Memphis. Ben had organized it, and I went there
because I wanted to work on AI in the traditional Minsky Incense, and that worked on a cognitive
architecture. My PI didn't really like it, so I paid my own way to this conference to publish it,
and I found like-minded people there, and foremost among them was Ben.
Great. What's something, Ben, that you've changed your mind about in the past six months in this
field, this AGI field or AI field? And then, Yosha, the question will go to you right afterward.
I don't think I've changed my mind about anything major related to AGI in the last
six months, but certainly seeing how well LLMs have worked over the last nine months or so has
been quite interesting. I mean, it's not that they've worked a hundred times better than I thought
they would or something, but certainly just how far you can go by this sort of non-AGI system
that mongers together a huge amount of data from the web has been quite interesting to see,
and it's revised my opinion on how much of the global economy may be converted to AI
even before we get to AGI. So it's shifted by thinking on that a bit, but not so much on
fundamentally how do you build an AGI, because I think these systems are
somewhat off to the side of that, although they may usefully serve as components of integrated AGI
systems. And Yosha? Well, I have some things that changed my mind are outside of the topic of AGI.
I thought a lot about the way in which psychology was conceptualized in Greece,
for instance, but I think that's maybe too far out here. In terms of AI, I looked into some kind
of new learning algorithms that fascinate me and that are more brain-like and move a little bit
beyond the perceptron, and I'm making slow and steady progress in this area. It doesn't feel like
there is a big singular breakthrough that dramatically changed my thinking in the last
six months, but I feel that there is an area where we're beginning to understand more and more things.
All right, let's get to some of the comparisons between you all, the contrasting ones.
It's my understanding that Yosha, you have more of the mindset of everything is computation or
all is computation, and Ben, you believe there to be other categories. I believe you refer to
them as archetypal categories, or I may have done that, and I'm unsure if this is a fair assessment,
but please elucidate me. I think that everything that we think happens in some kind of language,
and perception also happens in some kind of language, and a language cannot refer to anything
outside of itself. And in order to be semantically meaningful, a language cannot have contradictions.
It is possible to use a language where you haven't figured out how to resolve all the
incontradictions, as long as you have some hope that there is a way to do it. But if a language
is self-contradictory, its terms don't mean anything. And the languages that work, that we can use to
describe anything, any kind of reality and so on, turn out to be representations that we can describe
via state transitions. And the number of ways in which we can conceptualize systems that are
doing state transitions, for instance, we can think about whether deterministic or interterministic
whether they are linear or branching. And this allows us to think of these
representational languages as a taxonomy, but they all turn out to be constructive. That means
modern parlance computational. There was a branch of mainstream of mathematics was not
constructive before GÃ¼rtel. That means language of mathematics allowed to specify things that
cannot be implemented. And computation is the part that can be implemented. I think for something
to be existent, it needs to be implemented in some form. And that means we can describe it in some
kind of constructive language. That's basically the sort of analysis I have to do with epistemology.
And the epistemology determines the metaphysics that I kind of have, because when I think about
what reality is about, I need to do this in a language in which my words mean things. Otherwise,
what am I talking about? What am I pointing at? When I'm pointing at, I'm pointing at the
representation that is basically a mental state that my own mind represents and projects into
some kind of conceptual space or some kind of perceptual space that we might share with others.
And in all these cases, we have to think about representations. And then I can ask myself how
is this representation implemented in whatever substrate it is? And what does this signify
about reality? And what is reality and what is significance in all these terms?
Turn out to be terms that again, I need to describe in a language that is constructive,
that is computational. And in this sense, I am a strong computationalist because I believe that
if we try to use non-computational terms to describe reality, and it's not just because
we haven't gotten around to formalizing them yet, but because we believe that we found something
that is more than this, we are fundamentally confused. And our words don't mean things.
And Ben?
I think that, yeah, I tend to start from a different
perspective on all this philosophically. I mean, I think there's one minor technical point I feel
need to quibble with than what Joshua said. And then I'll try to outline my point of view from a
more fundamental perspective. I mean, the point I want to quibble with is I was stated that
if a logic or language contains contradictions, it's meaningless. I mean, of course, that's not
true. There's a whole discipline of para-consistent logics which have contradictions in them and yet
are not meaningless. And they're constructive para-consistent logics. And you can actually use
Curry-Howard transformations or operational semantics transformations to map
para-consistent logical formalisms into gradually type programming languages and so forth. So I
mean, contradictions are not necessarily fatal to having meaningful semantics to a logical or
computational framework. And this is something that's actually meaningful in my approach to AGI on
the technical level, which we may get into later. But I want to shift back to the foundation of
life, the universe and everything here. So I mean, I tend to be phenomenological in my
approach more so than starting from a model of reality. And these sorts of things become hard to
put into words and language because once you project them into words and language, then yeah,
you have a language because you're talking in language, right? But talking isn't all there is
to life. It isn't all there is to experience. And I think the philosopher Charles Perce gave one
fairly clear articulation of some of the points I want to make. You could just as well look at
Lao Tzu or you could look at the Vedas or the book Buddhist Logic by Strabovsky, which gives
similar perspectives from a different cultural background. So if you take Charles Perce's
point of view, which at least is concise, he distinguishes a number of metaphysical categories.
And I don't follow him exactly, but let me start with him. So he starts with first, by which he
means qualia, like raw, raw, unanalyzable, just it's there, right? And then he conceives second,
by which he means reaction, like billiard ball bounces off each other. It's just one thing
is reacting to something else, right? And that this is how he's looking at sort of the crux of
classical physics, let's say, then by what Perce calls third, he means relationships. So one thing
is relating to other things. And one of the insights that Charles Perce had writing in the late 1800s
was that, you know, once you can relate three things, you can relate four, five, six, 10, like
any large finite number of things, which was just a version of what's very standard now of
reducing a large number of logical relations to sort of triples or something, right? So
Perce looked at first, second, and third as fundamental metaphysical categories. And
he invented quantifier logic as well with a for all and there exists in quantifier binding. So
he as Perce would look at it, computation and logic are in the realm of third. And if you're
looking in that metaphysical category of third, then you say, well, everything's a relationship.
On the other hand, if you're looking for women with from within the metaphysical category of
second, you're looking at it like, well, everything's just reactions. If you're looking at it from
within the metaphysical category of first, then it's like, whoa, it's all just there. And you
could take any of those points of view and it's valid in itself. Now, you could extend beyond
Perce's categories, you could say, well, I'm going to be a Zen Buddhist and have a category of zero,
like the unanalyzable pearly void, right? Or, or you could go young Ian and say, okay, these are
numerical archetypes, one, two, three. But then we have the archetype of four, which is, is sort
of synergy and emergence is sort of mandalic. Yeah. So what I was saying is Perce, Perce had
these three metaphysical categories, which he viewed as just ontologically, metaphysically
distinct from each other. So what, what Chalmers would call the hard problem of consciousness
in Perce's language is like, how do you collapse third to first? And Perce would be just like,
well, you, you don't, they're different, they're different categories. You're an idiot to think
that you can somehow collapse one to the other. So in that sense, he was a dualist, although more
than a dualist, because he had first, second and third. Now, I think you could go beyond that,
if you want, you could go Zen Buddhist and say, well, we have a zero category of the, you know,
that original, ineffable, self contradictory, pearly void. And then, then you have the question
that is zero, really the same as one, which is like the Zen Buddhist paradox of non dualism and so
forth in a certain form. You can, you can also go above Perce's three metaphysical categories,
and you can say, okay, well, why not four fourth? Well, to, to Carl Jung, four was the archetype of,
of synergy and many Mandalas were based on this fourfold synergy. Why not five? Well, five,
you have the fourfold synergy and then the birth of something new out of it, right? So I, I,
I can see that the perspective of third, the perspective of computation is substantially
where you want to focus if you're engineering an AGI system, right? Because you're writing a program
and the program, the program is a set of, of logical relationships. The program is written in
a language. So I don't, I don't have any disagreement that this is like the focal point when you're
engineering an AGI system. But if I want to intuitively conceptualize the AGI's experience,
I don't feel a need to like try to reduce the whole metaphysical hierarchy into,
into a third just because the program code lives there. And I mean, this is,
this is sort of a, it's not so much about AI or mathematical or computational formalism. I mean,
these are just different philosophical perspectives, which it becomes
arduous to talk about because natural language terms are, are imprecise and ambiguous and,
and slippery. And you could end up spending a career trying to articulate what is really
meant by relationship or something. All right, Yosha. I think it comes down to the
vein which our thinking works and what we think thinking is. You could have one approach that is
radically trying to build things from first principles. And when we learn how to write
computer programs, this is what we might be doing. When I started programming, I had the
Commodore 64. I was a kid. I didn't know how to draw a line. Commodore 64's basic doesn't have
a command to draw a line. What you need to draw a line on the Commodore 64 is you need to learn
a particular language. And this language in this case is basic. You can also learn a similar
directly. But it's not hard to see how assembler maps to the machine code of the computer. And the
machine code works in such a way that you have a short sequence of bits organized into groups of
eight bytes. And these bytes are interpreted as commands by the computer. They're basically
like switches or train tracks. You could imagine every bit determines whether a train
track goes to the left or to the right. And after you go through eight switches, you have 256
terminals where you can end. So if you have two options to switch left or right, in each of these
terminals, you have a circuit, some kind of mechanism that performs a small change in the
computer. And these changes are chosen in such a way that you can build arbitrary programs from
them. And when you want to make a line, you need to learn a few of these constructs that you use
to manipulate the computer. And first off on the code or 64, you need to write a value in a
certain address of that corresponds to a function on the video chip of the computer. And it makes
the video chip forget how to draw characters on screen and instead interpret a part of the memory
of the computer as pixels that are to be displayed on the screen. And then you need to tell it which
address and working memory you want to start by writing two values into the graphic chip, which
encode for a 16-bit address in the computer. And then you can find the bits in your working
memory that correspond to pixels on the screen. And then you need to make a loop that addresses
them all in order. And then you can draw a line. And once I understood this, I basically had a
mapping from an algebraic equation into automata that was the computer is doing. It's an automaton
at the lowest level that is performing geometry. And once you can draw lines, you figure out also
how to draw curved shapes. And then you can draw 3D shapes and you can easily derive how to make
that. And I did these things as a kid. And then I thought the mathematicians have some kind of
advanced way, some kind of way which I deeply understand what geometry is in ways that goes
far beyond what I am doing. And mathematics teachers had the same belief. They basically
were gesturing at some kind of mythological mountain of mathematics, whether with some deep
inscrutable knowledge on how to do continuous geometry, for instance. And it was much, much later
that I started to look at this mountain and realized that it was doing the same thing that
I did on my tocomodora64, just with Greek notation. And there's a different tradition behind it,
but it was basically the same code that I've been using. And when I was confronted with notions of
space and continuous space and many other things, I was confronted with a conundrum. I thought I
can do this in my computer that looks like it, but there can be no actual space because I don't
know how to construct it. I cannot make something that is truly continuous. And I also don't observe
anything in reality around me that is fundamentally different from what I can observe in my computer
to the degree that I can understand and implement it. So how does this other stuff work? And so
imagine somebody has an idea of how to do something in a way that is fundamentally different from what
could be in principle done in computers. And I asked them how this is working. It goes into
hand waving. And then you point at some proofs that have been made that show that the particular
hand waving that they hope to get to work does not pan out. And then I hope there is some other
solution to make that happen because they have the strong intuition. And I asked, where does
this intuition come from? How did it actually get into your brain? And then you look at how
does the brain work? There is firing between neurons. There is interaction with sensory
patterns on the systemic interface to the universe. How were they able to make inferences that go
beyond the inferences that I can make? But this is one way of looking at it. And then on the other
end of the spectrum, this one is more or less in the middle, there is degraded form of epistemology
which is you just make noises. And if other people let you get away with it, you're fine.
And so you just make sort of grunts and hand waving movements and you try to point at things
and you don't care about anything of it works. And if a large enough group of high status people
is nodding, you're good. And this epistemology of what you can get away with doesn't look
very appealing to me because people are very good at being wrong in groups.
Yeah. I mean, saying that the only thing there is is language because the only thing
we can talk about in language is language. I mean, this is sort of
tautology in a way, right?
No, no, that's not quite what I'm saying. I'm not saying the only thing there is language,
of course. Language is just a representation. It's a way to talk about things and to think about
things and to model things. And obviously not everything is a model, just everything that
they can refer to as a model. And so there is that, but I mean, you can't know that, right?
You can hypothesize that, but you can't know that. And this gets into, I guess it depends
I cannot know anything that I cannot express.
I can know many things. I can't express some language, but I mean, that's just,
I guess, a different flavor of knowing, subjective experience. I mean, so take what
Martin Buber called an eye-thou experience, right? I mean, if you're staring into someone's eyes
and you have a deep experience that you're seeing that person, you're just sharing a shared
space of experience and being, I mean, in that moment, that is something you both know you're
not going to be able to communicate fully in language and it's experientially there. Now,
Buber wrote a bunch of words about it, right? And those words communicate something special
to me and to some other people. But of course, someone else reads the words that he wrote and
says, well, you are merely summarizing some collection of firings of neurons in your brain
and in some strange way, deluding yourself that that is something else, right? So, I mean, that's
a, I mean, I think from within the domain of computation and science, you can neither prove
nor disprove that there exists something beyond the range of computation and science. And if you
look at scientific data, I mean, the whole competitive of scientific data ever gathered
by the whole human race is one large finite bit set basically. I mean, it's a large,
it's a large set of data points with finite precision to each piece of data. So, I mean,
it may not even be that huge of a computer file if you try to, if you try to assemble it all,
like all the scientific experiments ever done and agreed by some community of scientists. So,
you've got this big finite bit set, right? So, and then science in a way is trying to come up with,
you know, concise, reasonable-looking, culturally acceptable explanations for this huge finite bit
set that can be used to predict outcomes of other experiments in which finite collection of bits
will emerge from those other experiments in a way that's accepted by a certain community.
Now, that's a certain process. It's a thing to do. It has to do with finite bit sets and
computational models for producing finite bits, right? And the finite sets of bits. And that's
great that nothing within that process is going to tell you that that's all there is to the universe
or that that isn't all there is to the universe. I mean, it's a valuable, important thing. Now,
to me, as an experiencing mind, I feel like there's a lot of steps I have to get to the
point where I even know what a finite bit set is or where I even know, like, what a community of
people validating that finite bit set is realized or what a programming language is. So, I mean,
I keep coming back to my phenomenal hospital experience. Like, first, there's this field of
nothingness or contradictory nothingness that's just floating there, then some indescribable
forms flicker and emerge out of this void. And then you get some complex
pattern of forms there, which constitutes a notion of, you know, a bit set or an experiment
or a computation. And from this phenomenological view, by the time you get to this business of
computing in languages, you're already dealing with a fairly complex, like, body of self-organizing
forms and distinctions that popped out of the void. And then this conglomeration of forms
that in some enough of a way has emerged out of the void is selling no, I am everything. The only
thing that exists in a fundamental sense is what is inside me. And I mean, you can't, if you're
inside that thing, you can't refute or really demonstrate that. But again, from an AGI view,
it's all fine. Because when we talk about building an AGI, what we're talking about is precisely
engineering a set of computational processes. Like, I don't, I don't think you need to do,
like, you don't need some special firstronium to drop into your computer to give, to give the,
the AGI the fundamental quality of experience or something.
There are two points now. Please don't forget.
Okay, let's just allow Yosha to speak, because there are quite a few threads and
some may be dropped. Also, it appears as if you're using different definitions of knowledge.
If we use this traditional philosophical notion of justified true belief,
it means that I have to use knowledge in a context where I can hope to have a notion of what's true.
So for instance, when I look at your face and experience a deep connection with you,
and I report I know we have this deep connection, I'm not using the word know in the same sense.
What I am describing is an observation. I'm observing that I seem to be looking at a face
and then observing that I have the experience of having a deep connection. And I think I can to
hope to report on this truthfully. But I don't know whether it's true that we have that deep
connection. I cannot actually know this. I can make some experiments to show how aligned we are
and how connected we are and so on to say this perception or this imagination has some veracity.
But here I'm referring to a set of patterns. There are dynamic patterns that I perceive.
And then there is stuff that I can reflect on and disassemble and talk about and convey
and model. And this is a distinct category in the sense. It's not in contradiction necessarily
what you're saying. It's just using the word knowing in different ways is implied here because I can
relate the pattern to you that I'm observing or that I think I'm observing.
But this is a statement about my mental state. It's not a statement about something in reality
about the world. And to make statements about the world, I probably need to go beyond perception.
The second aspect that we are now getting to is when you say that reality and minds
might have properties that are not computational yet your AGI is entirely computational and doesn't
need any kind of first principles, wonder machine built into it that goes beyond what we can
construct from automata. Are you establishing that AGI is artificial general intelligence
with potentially superhuman capabilities are going to still lagging behind what your mind
is capable of? No, no, not at all. I just think the other aspects are there anyway and you don't
need to build them. So you're going to make the non-computational parts of reality using
computation? No, you don't have to make them. They're already there. I mean, if you take
just just take a more simple point of view where you're thinking about first and third,
and purse was basically a panpsychist, right? So he believed that matter is mind hidebound
with habit. As he said, he believed that every little particle had its own
spark or element of consciousness and awareness in it. So I mean, from that standpoint,
I mean, this kind of bubbly water that I'm holding up has its own variety of conscious
awareness to it, which has different properties in the conscious awareness and in my brain are
yours. So from that standpoint, if I build an AGI program, it has something around the same
patterns and structures and dynamics as a human brain and as the sort of computational aspect
of the human mind from that standpoint, then most likely the same sort of
firstness, the same species of subjective awareness will be associated with that AGI
machine that you built. But it's not that you need to construct it. I mean, any more than you need
to explicitly construct like the positioning in time of your computer or something like you
built something, it's already there in time, you don't have to build time. I mean, you just
build it and it's there in time. You didn't need a theory of time, and you didn't need to screw
together moment t to moment t plus one either. The perspective is more that awareness is ambient
and it's there. You don't need to build it. Of course, there's subtlety that
different sorts of constructions may have different sorts of awareness associated with them. And
there's philosophical subtlety in how you treat different kinds of first when you're operating
in a level where relationship doesn't exist yet. In what sense is the experience of red
different from the experience of blue, even though articulating that difference already brings you
into the realm of third, right? And this gets back to non-duality and a bunch of stuff that
Perse wrote hundreds of pages about. I haven't read these pages, so I don't really understand them.
I think it's conceivable that particles are conscious or intelligent, but this would require
that they have or imply they have more complicated causal structure than the computer that I'm currently
using to communicate with you. And by that's possible, it seems to me that there are simpler
ways in which particles could be constructed to do the things that they are doing. It seems to
me sufficient that there are basically emergent error correcting codes on the quantum substrate
and would just emerge over the stuff that remains statistically predictable in
branching multiverse. I don't need to be conscious to do anything like that. Maybe if we do more
advanced physics, we figure out, oh no, this error correcting code that just emerges similar to a
vortex emerges in the bathtub when you move your hand and only the vortex remains and everything
else dissipates in the wave background that you are producing and the chaos and turbulences.
It could be to me possible that particles are like this. They're a little stable,
twirls, vortices that they've stabilized after the non-stable stuff is dissipating.
And to achieve this, I don't think that I need to posit that they are conscious.
If it could be that I figure out, oh no, this is not sufficient. We need way more complicated
mass and structure to make this happen. So they need some kind of coherence improving operator
that is self-reflexive and eventually leads to the structure. Then I would say, yeah,
maybe this is a theory that we should seriously entertain. Until then, I'm undecided and all
comes razor says, I can construct what I observe at this level of elementary particles, atoms,
and so on, by assuming that they don't have any of the conscious functionality that exists in my
own mind. And the other way would be you can redefine the notion of consciousness into some
principle of self-organization that is super basic. But this would redefine consciousness into
something else, because there's a lot of self-organizing stuff that does not fall into the
same category that an anesthesiologist makes go away when he gives you an anesthetic. And to me,
consciousness is that thing which seems to be suspended when you get an anesthetic and that
stops you from learning and currently interacting with the world. I mean, that at least gives me
a chance to repeat once again, my favorite quote from Bill Clinton, former U.S. president, which is
that all depends what the meaning of is is, right? So I mean,
that's interesting, Ben, because the first time, Yosha, I don't know if you remember,
I brought up that quote. I don't remember the context, but I said, yeah, that also depends on
what is is. The question is what do you mean by is, right? So visit the story. It sounds like
it sounds like Bill Clinton. It depends upon what the meaning of the word is here.
The previous podcast with Yosha Bach as a solo episode is linked in the description,
as well as the previous podcast with Ben Solo, as well as Yosha Bach with Donald Hoffman and Yosha
Bach with John Verveke. Every link as usual to every source mentioned in this podcast on every
single TOE podcast is in the description. Yeah, I mean, a couple reactions there, and I feel I may
have lost something in the in the in the buffering process there. But I think that
let me see. So that
first of all, about causality and firstness or a raw experience, I mean, almost by
definition of how Perce sets up his metaphysical categories. I mean, a firstness doesn't
cause anything. So you're not going to come up with a case where I need to assume that this
particle has experience or else I can't explain why this experiment came out this way. I mean,
that would be a sort of category error in Perce's perspective. So if the only
sort of thing you're willing to attribute existence to is something which has a demonstrable
causal impact on some experiment, then by that assumption, I mean, that that's essentially
equivalent to the perspective you're putting forth that everything is is is computation.
Yeah. And and Perce, Perce didn't think other categories besides third were of that nature.
Now, there's also a just shallow semantic matter tied into this, which is the word consciousness is
just highly ambiguous. So I mean, Yosha, you seem to just assume that human-like consciousness
is consciousness. And I don't really care if people want to reserve the word consciousness
for that. Then we just need some other word for the sort of ambient awareness and everything in
the universe, right? So there's lengthy debates among academics on like, okay, do we say a particle
is conscious? Or do we say it's proto conscious, right? So then you can say, okay, we have
proto consciousness versus consciousness, or we have like, raw consciousness versus
reflexive consciousness or human-like consciousness. And I mean, I spent a while reading all the stuff
I wrote some things about it. In the end, I'm just like, this is a this is a game that overly
intellectual people are playing to to entertain themselves. And it doesn't really matter. Like,
I've got I've got my experience of the universe. I know what I need to do to build AGI systems and
arguing about which words to associate with different flavors and levels of experience
is you just kind of kind of running around in circles.
Conceptually, it's an important question is this camera that is currently filming my face
and representing it and then relaying it to you aware of what it's doing? And is this just a matter
of degree with respect to my consciousness? Is this representing some kind of ambient awareness of
the universe and are particles doing the same thing? And so on. These are questions that I think
I can answer. And if I don't answer them, my thinking will become so mushy that my thoughts are
meaningless and I will not be able to construct anything. I mean, if the only kind of answer that
you're interested in are rigorous scientific answers, then you have your answer by assumption,
right? And I mean, answering questions by assumption is fine. It's practical. It saves our
time. But I mean, I don't I mean, you will know, I think that's what you're doing. I don't see how
you're not just trying to answer by assumption. You posit that elementary particles are conscious.
Then I point out that we normally reserve the word consciousness for something that is
really interesting and fascinating and shocking to us. And that it would be more shocking if
it projected into the elementary particles. And then you say, okay, by just mean ambient
awareness, now we have to disassemble what ambient awareness actually means. What is a
barrier? What does this awareness come down to? And I think that you're pointing at something
that I don't want to dismiss. I want to take you seriously here. So there maybe there is something
to what you are saying, but you're not getting away with simply waving at it and saying,
this is sufficient to explain my experience and I'm no longer interested to make my words mean
things because they cannot communicate otherwise. We will not be able to see what idea you actually
have, what idea you're trying to convey and how it relates to ideas that other people might have.
And I'm not pointing at the institutions of science here which don't agree on what consciousness is
and for the most part don't care. This is more a philosophical question here. And it's also one
that is an existential question that we have to negotiate among the two or the three of us.
Yeah, let's just let Ben respond. I mean, I guess
rightly or wrongly, as a human being, I've gotten bored with that question in the same way that
like, I couldn't say it's worthless. At some point, maybe you could convince someone like I
know people who were convinced by materials they read on the internet to give up on
un-mormonism or Scientology, right? So I can't say it's worthless to debate these points with
people who are heavily attached to an ideology I think is silly. On the other hand, I personally
just tend to get bored with repeated debates that go over the same points over and over again.
If I had an infinite number of clones, then I wouldn't. And this, I guess, one of the things
that I get worn out with is people claiming my definition of this English word
is the right one and your definition is the wrong one. And I guess you weren't really doing that,
Joseph, but it just gave me a traumatic memory. I'm sorry for triggering you here.
I'm not fighting about words. I don't care which words you're using. So when I think about an
experience of what it's like and associate that with consciousness or the system that is able to
create a now, the perception of a now, then I'm talking about a particular phenomenon that I have
in mind and I would like to recreate if I can. And I want to understand how it works. And so
for me, the question of whether I project this property into arbitrary parts of what I consider
to be reality is important. I understand if it's not interesting to you and I won't force you into
any discussion that would make you drop out of your particular mormonism. I'm happy with you
being a mormon. Let me tell you how I'm looking at anesthesia, which is a concrete specific example
that's not that trivial, right? Because I've only been under anesthesia once, which I have
wisdom teeth removed. So it wasn't that bad, but other people have had far more traumatic
things than when they're under anesthesia. And there's always the nagging fear that like,
since we don't really know how anesthesia works in any fundamental depth and also don't really know
how the brain generates our usual everyday states of consciousness in enough that
it's always possible that while you're under anesthesia, you're actually in some sense,
some variant of you is feeling that knife slicing through you. And maybe just the memories being
cut off, right? And then once you come back, you don't remember it. But then that might not be
true. But then you have to ask, well, okay, say then, you know, while my jaw is being cut open by
that knife, does the jaw feel it, right? Like, does the jaw hurt whilst being sliced up by the
knife? Like is the jaw going, ah, well, on the other hand, you know, the global workspace in your
brain, like the reflective theater of human like consciousness in your brain may well be disabled
by the anesthetic. So the way I personally look at that is, I suspect under anesthesia,
your sort of reflective theater of consciousness is probably disabled by that anesthetic. I'm
not 100% sure. But I think it's probably disabled, which means there's probably not like a version
of Ben going, ah, wow, this really hurts. This really hurts. And then forgetting it afterwards.
So I mean, maybe you could do that just like disable memory recording. But I don't think
that's what's happening. On the other hand, I think the jaw is having its own
experience of being sawed open. Now that while you're getting that wisdom tooth removed under
general anesthesia, no, I think it's, it's not the same sort of experience exactly as the reflective
theater of consciousness that knows itself as Ben as Ben Gertzel is having. Like the Ben Gertzel can
conceptualize that it's that it's experiencing pain. It can go like, oh,
that really hurts. And then the thinking that's saying that really hurts is different than that
which really hurts, right? There's many levels there. But I do think there's some sort of raw
feeling that the jaw itself is having, like even if it's not connected to that reflective theater
of awareness in the brain. Now the jaw is biological cells. So some people would agree
that those biological cells have experience, but they would think like, you know, a brick when
you smash it with an axe doesn't. But I suspect the brick also has that some elementary feeling.
So I mean, I think, I think, I think it is like something to be a brick that's smashed in half
by an axe. On the other hand, that it's not like something that can reflect on what it is to be
a brick smashed in half by an axe, right? So I mean, that's is how I think about it. But again,
I don't know how to make that science because I can't ask my jaw what it feels like because my
jaw doesn't doesn't speak language. And even if I was able to like wire my brain into the jaw of
someone else who's going through wisdom tooth removal under anesthesia, like I might say like
through that wire, I can feel by an eye that experience like I can feel the pain of that
jaw being sliced open. But I mean, you can tell me I'm just hallucinating that my own brain is
like improvising that based on the based on the signals that I'm getting. And I'm not I'm not sure
how you really pin that down in an experiment, right? Let me try. So there have been experiments
about anesthesia and I'm not an expert on anesthesiology. So I asked everybody for forgiveness
if I get things wrong. But there have been that different different anesthetics and some of them
work in very different ways. And there is indeed a technique that basically works by giving people
a muscle relaxant so they cannot move and giving them something that inhibits the formation of
long term memory so they cannot remember what happened to them in that state. And there have been
experiments that surgeons did where they were applying a tourniquet to an arm of the patient
so the muscle relaxant didn't get into the arm and they could still use the arm. And then in the
middle of the surgery, they asked a person that was there and I fully relaxed and in
comunicato to raise their arm or raise their hand if they were conscious and aware of what is
happening to them and they did. And when they were asked if they had unbearable pain, they also
raised their hand. And after the surgery, they didn't forget they had forgotten about it.
But I also noticed the same thing on surgery. I had a number of big surgeries in my life and
there is a difference between different types of surgery. There is one type of surgery where I wake
up and feel much more terrified and violated than I do before the surgery. And I don't know why
because I have no memory of what happened. Also, my memory formation is impaired. So when I am in
an ER and ask people how it went, I might have that same conversation multiple times word for word
because they don't remember what they said or that I asked them. There is another type of anesthesia
and I observed this, for instance, in one of my children where the child wakes up and says,
oh, the anesthesia didn't work. And it was an anesthesia with gas. So the child choked on the
gas and you see your child lying there completely relaxed and sleeping and then waking up and
starting to choke and then telling you the anesthesia didn't work. There is a complete gap
of eight hours in the memory of that child in which the mental state was somehow preserved.
Subjectively, the child felt a complete continuation and then was looking around and
reasoning that the room was completely different. Time was very different, led to confusion and
reorientation. So I would suspect that in the first case, it is reasonable to assume or to
hypothesize at least that consciousness was present. But we don't recall what happened in
this conscious state, whereas in the second one, there was a complete gap in the conscious
experience and consciousness resumed after that gap. And we can test this. There are ways, regardless
of whether we agree with this particular thing or whether we think anesthesia is important.
In principle, we can perform such experiments and ask such questions. And then on another level,
when we talk about our own consciousness, there is certain behavior that is associated with
consciousness that makes it interesting. Everything, I guess, only becomes interesting
due to some behavior, even if the behavior is entirely internal. If you are just introspectively
conscious, it still matters if I care about you. And so this is a certain type of behavior that
we still care about. For instance, if I asked myself, is my iPhone conscious? The question
is what kind of behavior of the iPhone corresponds to that? And I suspect if I turn off my iPhone
or smash it, it does not mean anything to the iPhone. There is no what it's likeness of being
smashed for the iPhone. There could be a different layer where this is happening, but it's not the
layer of the iPhone. Now let's get to a slightly different point, this question of whether your
jaw knows anything about being hurt. So imagine that there is surgery on your jaw, like with your
wisdom teeth. Is there something going on that is outside of your brain that is processing
information in such a way that your jaw could become sentient in the sense that it knows
what it is and how it relates to reality, at least to some degree and level? And I cannot
rule this out, but the cells, the cells can process information. They can send messages to
the neighbors and the patterns of their activation, who knows what kind of programs they can compute.
But here we have a means and a motive. The means and motive here are it would be possible for the
cells to exchange conditional matrices to perform arbitrary computations and build representations
about what's going on. And the motive would be that it's conceivable that this is a very useful
thing for biological tissues to have in general. And so if they evolve for long enough and it is in
the realm of evolvability that they perform interactions with each other that lead to
representations of who they are and what they're doing, even though they're much slower than what's
happening in our brain and decoupled from our brain in such a way that we cannot talk to our jaw.
It's still conceivable, right? I wouldn't rule this out. It's much harder for me to assume the
same thing for elementary particles because I don't see them having this functionality
that cells have. Cells are so much more complicated that just fits in that they would be able to do
this. And so I would make a distinction. I would not rule out that multi-cellular organisms without
brains could be conscious, even at different time scales than us requiring very different
measuring mechanisms because their signal processing is probably much slower and it
takes longer for them to become coherent at scale because it takes so long for signals
to go back and forth if you don't have nerves. But I don't see the same thing happening for
elementary particles. Yeah, I don't rule it out again. But you would have to show me some kind
of mechanism. I mean, if you're going to look at it that way, which isn't the only way that I
would look at it, but if you're going to look at it that way, I don't see why you wouldn't say
the various elementary particles, which are really distributed like amplitude distributions.
Right? I don't know why you wouldn't say these various interacting amplitude distributions are
they're exchanging quantum information with a with a motivation to to achieve stationary
action given given their context. Right? I mean, I mean, you could tell that you could tell that
story. That's sort of the story that that that physics tells you they're swapping information
back and forth, trying trying to make the action stationary. But for the most part,
they don't form brains. They also do form brains. So elementary particles can become
conscious in the sense that they can form brains, nervous system, maybe equivalent
information processing. I just feel like you're, you're privileging a certain
level and complexity of organization, because it happens to be ours. And I mean, we have a certain
level and complexity of organization and and of consciousness. And I mean, a cell in my jaw has
a lower one. A brick has a lower one element, perhaps a lower one, the future AGI may have
a much higher one for from whose perspective, our consciousness appears more analogous to a brick
than to itself. I wouldn't say lower or higher. I would say that if my jaw is conscious,
less cells involved in my brain and the interaction between them is slower. So if it's
conscious, it's probably more at the level of, say, a fly than a level of a brain. And it's probably
going to be as fast as a tree and in the way in which it computes, rather than as fast as your
brain. And that is, I don't think that's something that is assigning some undue privilege to it.
I'm just observing a certain kind of behavior. And then I look for the means and motive behind
that behavior. And then I try to construct causal structure. And I might get it wrong,
there's things that might be missing. But it's certainly not because I have some kind of
speciesism that assigns higher consciousness to myself because it's me.
All right. Yeah, I'm out of nowhere, your motivations are, Kurt, I have a, I have a higher
level, I have a higher level comment, which is, we're like an hour through this conversation,
probably halfway through. I feel like the philosophy, the hard problem of consciousness,
the hard problem of consciousness is an endless rabbit hole. It's not, it's not an uninteresting
one. I think, I think, I think it's also not, it's not the topic on which Josh and I have the
most original things to say. Like I think each, each of our perspectives here are held by many
other groups. I might interject a little bit. What I'm, one of our most interesting disagreements
is in Ben being a Penseikist and me not knowing how to formalize Penseikism in a way
that makes it different from box standard functionalism. And so I do value this discussion
and don't think it's useless, but I basically feel that on almost everything else, you mostly
agree except for crypto. Okay. Yeah. To me, that's almost a Zen thing. It's like,
I don't know how to formalize the notion that there are things beyond the formalization.
It's so fascinating. If you look at your frozen interlocutor, I don't know if you
can see your ass is still conscious and you can rest in that forever.
Yeah. All right. Again, comparing your views, it seems like
Yosha, you're more of the mind that LLMs or deep neural nets are on our significant step
toward AGI, maybe even sufficient with enough complexity. And Ben, I think that you disagree.
Yeah. I think most issues in terms of the relationship with LLMs and AGI, we actually
probably agree on quite well. But I mean, obviously, large language models are in
amazing technology, like from an AI application point of view, they can do all sorts of fantastic
and tremendous things. I mean, it sort of blew my mind how smart GPT-4 is. It's not the first time
my mind has been blown by an AI technology. I mean, my mind was blown by computer algebra
systems when they first came out. And you could do integral calculus with arbitrary complexity.
And when deep blue beat chess with just game trees, I'm like, whoa. So I mean,
I don't think it's the only amazing thing to happen in the history of AI, but it's an amazing
thing. It's a big breakthrough. It's super cool. I think that if deployed properly,
this sort of technology could do a significant majority of jobs that humans are now doing on
the planet, which has big economic and social implications. I think that the way these algorithms
are representing knowledge internally is not what you really need to make a full on human level
AGI system. So I mean, when you look at what's going on inside a transformer neural network,
I mean, it's not quite just a big weighted hash table of particulars, but to me,
it does not represent abstractions in a sufficiently flexibly manipulable way to do the
most interesting things that the human mind does. And this is a subtle thing to pinpoint in that say
something like a fellow GPT does represent abstractions. It's learning an emergent representation
of where the board is, but it's learning an emergent representation of features like a
black squares on this particular board position or white squares on this particular board position.
So examples like that show that LLMs can in fact learn abstract representations and can
manipulate them in some way, but it's very limited in that regard. I mean, in that case,
it's seen a shitload of a fellow games, and that's a quite simple thing to represent. So I
think when you look at how the neural net is learning, how the attention mechanism is working,
how it's representing stuff, I mean, it's just not representing it a hierarchy of subtle abstractions
the way a human mind is. And I mean, the subtler question is what functions you can get by
glomming an LLM together with other components in a hybrid architecture with the LLM at the
center. So suppose you give a working memory, suppose you give an episodic memory, suppose
you have a declarative long term memory graph, and you have all these things integrate into the
prompts and integrate into fine tuning of an LLM. Well, then then you have something that in principle
it's Turing complete and it could probably do a lot of quite amazing things. I still think if
the hub of that system is an LLM with its impaired and limited ability for representing and manipulating
abstract knowledge, I think it's not going to do the most interesting kinds of thinking that
people can do. And examples of things I think you fundamentally can't do with that kind of
architecture or say, invent a new branch of mathematics, you know, invent a completely new,
let's say radically new genre of music, you know, figure out a new variety of business strategy
like say Amazon or Google did that's quite different than things that have been done before.
All these things involve a leap into the unknown beyond the training data to an extent that I
think you're not going to get with the way that LLMs are representing knowledge. Now, I do think
LLMs are powerful as tools to create AGI. So for example, as one sub project in my own AGI project,
we're using LLMs to map English sentences into computer programs or try to get logic
expressions, right? No, I mean, that's super cool. I mean, then you've got the web in the form of a
huge collection of logic expressions. You can use a logic engine to connect everything on the web
with what's in databases and with stuff coming in from sensors and so on. So I mean, that's by no
means the only way to leverage LLMs toward AGI, not at all, but it's one interesting way to leverage
LLMs toward AGI. You can even ask the LLM to come up with an argument and then use that as a sort
of guide for a theorem prover and coming up with a more rigorous version of that argument, right?
So I do think there are many ways more than I could describe right now of LLMs to be used to
help guide and serve as a component of AGI systems. But I think if you're going to make a hybrid AGI
system with full human-level general intelligence and with an LLM as a component, something besides
an LLM has got to be playing a very key and central role in knowledge representation and
reasoning basically. And this ties in then with LLMs not being motivated agents. So you could wrap
a sort of motivated agent infrastructure around an LLM, right? You could wrap Josh's
side model, micro-side model in some way around an LLM if you wanted to. And you could make it. I
mean, people tried dumb things like that with auto-GPT and so-called baby AGI and so forth. So I
mean, on the other hand, I think if you wrap a motivated agent architecture around an LLM with
its impaired capability for making flexibly manipulable abstract representations, I think
you will not get something that builds a model of self and other with the sophistication that
humans have in their reflective consciousness. And I think that having a sophisticated,
abstract model of self and other in our reflective consciousness, the kind of consciousness that we
have but a brick or a jaw cell doesn't, right? Without that abstraction in our model of reflective
consciousness tied in with our motivated agent architecture, then that's part of why you're not
going to get the fundamental creativity in inventing new genres of music or new branches of
mathematics or new business strategies. In humans, we do this amazing novel stuff, which is what drives
culture forward. We do this by our capability for flexibly manipulable abstraction tied in with
our motivated agent architecture. And I don't see how you get that with LLMs as the central hub of
your hybrid AGI system. But I do think you could get that with an AGI system that has, oh, something
like OpenCog's Adam Space and Reasoning system as the central hub with an LLM as a subsidiary
component. But I don't think OpenCog is the only way either. I mean, obviously, you could make a
biologically realistic brain simulation that had human level AGI. I just think then the LLM-like
structures and dynamics within that biologically realistic brain system would just be a subset
of what it does. There'd be quite different stuff in the cortex. So yeah, that's not quite
the capsule summary, but a lengthy-ish overview of my perspective on this.
Okay, great. Yosha, I know there was a slew there. If you can pick up some of the pieces and respond.
But also at the same time, there's emergent properties of LLMs. So for instance, reflection
is apparently some emergent property. There are, but they're limited. I mean, and that does make
it subtle because you can't say they don't emerge knowledge representation. They do. And a fellow
GPT is one very simple example that there are others. There is emergent knowledge representation in
them, but it's very simplistic and limited. It doesn't pop up effectively from in-context learning,
for example. But anyway, this would dig us very deep into current LLMs, right?
Yeah. So is there some in-principle reason why you think that a branch of mathematics,
Yosha, can't be invented by an LLM with sufficient parameters or data?
I am too stupid to decide this question. So basically what I can offer is a few perspectives
that I see when I look at the LLM. Personally, I am quite agnostic with respect to its abilities.
And at some level, it's an autocomplete algorithm that is trying to predict tokens from previous
tokens. And if you look at what the LLM is doing, it's not a model of the brain. It's a model of
what people say on the internet. And it is discovering a structure to represent that
quite efficiently as an embedding space that has lots of dimensions. You can imagine that each
of these dimensions is a function. And the parameters of this function are the positions
on this dimension that you can have. And they all interact with each other to together create some
point in a high-dimensional space that this could be an idea or a mental state or a complex
thought. And at the lowest level, when you look at how it works, it's translating these tokens,
the translation of linguistic symbols into some kind of representation that could be,
for instance, a room with people inside and stuff happening in this room. And then it maps it back
into tokens at some level. There has been recently a paper out of a group led by Max Tagmark that
looked at the Lambda model and discovered that it does indeed contain a map of the world
and directly encoded in its structure based on the neighborhood relationships between places
in the world that it represents. And as we speak, there is an isomorphic structure between what the
LLM is representing and all of the stuff that we are representing. I am not sure if I in my entire
life ever invented a new dimension in this embedding space of the human mind that is represented on
the internet. If I think about all the thoughts that have been made into books and then encoded
in some form and became available as training data to the LLM, we figure out that they are,
depending on how you count, a few ten to a few hundred thousand dimensions of meaning.
And I think it's very difficult to add a new dimension or also to significantly extend the
range of those dimensions, but we can make new combinations of what's happening in that space.
Of course, it's not a limit that these things are limited to the dimension that they already
discovered. Of course, we can set them up in such a way that they can confabulate more dimensions
and we could also set them up in such a way that they could go and verify whether there's a good
idea to make this dimension by making tests, by giving the LLM the ability to use plugins,
to write its own code, to use a compiler, to use cameras, to use sensors, to use actuators,
to make experiments in the world. It's not limited to what we currently at the LLM do.
But in the present form, what the transformer algorithm is doing, it tries to find the most
likely token. And so, for instance, if you play a game with it and it makes mistakes in this game,
then it will probably give you worse moves after making these mistakes, because it now assumes
that it's playing a bad person. Somebody was really bad at this game. And it doesn't know
what kind of thing it's supposed to play, because it can represent all sorts of state
transitions. It's an interesting way of looking at it that we are trying to find the best possible
token versus the LLM trying to find the most likely token next. Of course, we can preface the
LLM by putting into the prompt that this is a simulation of a mine that is only going to look
for the best token and it's trying to approximate this one. So it's not directly a counterargument.
It's not even asking us to significantly change the loss function. Maybe we can get much better
results. We probably can get much better results if we make changes in the vein, which we do training
on inference using the LLM. But this by itself is also nothing that we can prove without making
extensive experiments. And at the moment, it's unknown. I realize that the people
are being optimistic. I just want to pose a thought experiment. So this is about
music rather than natural language. But I mean, we know there's music gen, there's similar networks
applied to music. So suppose you had taken LLM like music gen or Google LLM or the next generations
and traded on all music recorded or played by humanity up to the year 1900. Is it
going to invent the sort of music made by Mahavishna Orchestra or even Duke Ellington?
Would you say that has no new dimensions because jazz combines elements of West African drumming
and Western classical music? I think that's a level of invention LLMs are not going to do.
You said it. You said it's combining elements from this and from that. And Dalit 2 came out,
I got early access. And one of the things that I tried relatively early on is stuff like
an ultrasound of a dragon egg. There is no child's round of a dragon egg on the internet.
But it created a combination of prenatal ultrasound and archaeopteryx cut through images and so on.
And it looked completely plausible. And in this sense, you can see that most of the stuff that
we are doing when we create new dimensions are mashups of existing dimensions. And maybe we can
represent all the existing dimensions using a handful of very basic dimensions from which we
can construct everything from the bottom up just by combining them more and more. And I suspect
that's actually what's happening in our minds. And I suspect that the LLM is not distinct from
this but for a large superset of this. The LLM is Turing complete. And from one perspective,
it's a CPU. We could say that the CPU in your computer only understands a handful, like maybe
a dozen or a hundred different machine code programs. And they have to be extremely specific
to these codes. And there is no error tolerance. If you make a mistake in specifying them,
then your program is not going to work. And the LLM is a CPU that is so complicated that
requires an entire server farm to be emulated on. And you can give it instead of a small program in
machine code, give it a sentence in a human language. And it's going to interpret this,
extrapolate it into some or compile it into some kind of program that then produces the behavior.
And that thing is Turing complete. It can compute anything you want if you can express it in the
right way. But being Turing complete is irrelevant because it doesn't take resources into account.
But you can write programs in a natural language, in an LLM, and you can also express learning
algorithms to an LLM. So basically, your intuition is, yes, that an LLM could invent jazz,
neoclassical, metal, and fusion based only on music up to the year 1900.
No, I am agnostic. What I'm saying is I don't know that it cannot. And I don't see a proof that it
cannot. And I would not be super surprised when it cannot. I don't think the LLM is the right
way to do it. It's not the good use of your resources if you try to make this the most
efficient way because our brain is far more efficient and does it in different ways.
But I'm unable to prove that the LLM cannot do it. And so I'm reluctant to say LLMs cannot do
acts without that proof because people tend to have egg on their face when they do this.
But doesn't that just come back to like Popper's notion about falsificationism? Like I can't prove
that, you know, a devil didn't appear at some random place on the earth at some point.
No, no, I mean, in the sense of, no, what I mean by this is, can I make a reasonable claim that
I'm very confident and would bet money on an LLM not being able to do this in the next five years?
This is the kind of statement that I'm trying to make here. So basically, if I say, can I prove
that an LLM is not going to be able to invent a new kind of music that is the stuff genre of
jazz that is in the next five years? And I can't. No, no, but even bet against it.
You've shifted the you know, I don't know. You've shifted the goalpost in a way because I do think
I do think not current music, Jen, but I could see how some upgrade of current LLMs connected with
symbolic learning system or blah, blah, blah. I do think you could invent a new sub genre of jazz
or grindcore or something. And I'm actually playing with stuff like that. The example I gave was a
significantly bigger invention, right? Like, I mean, jazz was not the sub genre of Western
classical music, nor of West African drumming, right? I mean, so that is, that is, to me,
is a qualitatively different. Yeah, a couple of weeks ago, I was at an event locally where somebody
presented their music GPT and you could enter, give me a few by Debussy, and it would try to
perform and it wasn't all bad. That's not the point, right? Yes, it's just an example for some
kind of functionality, but any kind of mental functionality that is interesting, I think I'm
willing to grant that the LLM might not be the best way of doing it. And I think it's also possible
that we can at some point prove limitations of LLMs rigorously. But so far, I haven't seen those
proofs. What I see is insinuations on both sides. And the insinuation that open their eyes makes
when it says that we can scale this up to do anything is one that has legitimacy because
they actually put their money there. They actually bet on this in a way that they invest their
lifetime into it and see if it works. And if it fails, then they will make changes to their
paradigm. And then there are other people who like Gary Marcus come out saying loud, loud,
swinging, this is something that the LLM can never do. And I suspect that they will have
back on their face because many of the promises that Gary Marcus made about what LLMs cannot do
have already been disproven by LLMs doing these things. And so I'm reluctant going out saying
things that I cannot prove. I find it interesting that the LLM is able to do all the things that
it does using in a way in which it does them. But it doesn't mean to me that LLMs, that I'm
optimistic that they can go all the way. But I am also unable to prove the opposite. I have no
certainty here. I just don't know. So about rigorous proof, I mean, the thing is the sort of proof.
So I mean, you can prove an LLM without an external memory is not turned complete,
and that's been done. But on the other hand, it's not hard to give them an external memory
like a Turing machine tape or a prompt. Well, the prompt is an external memory to the
LLM. Well, no, no, it has to be able to write. You have no LLMs with unlimited prompt context,
if you want to. It would have to be able to write prompts. So no, no, no, no, no.
Yes, it was writing prompt, not just read prompts. It's basically it's an electric
weld guys possessed by a prompt. In principle, you can give it a prompt that is self modifying,
and that allows it to also use databases and so on and plugins.
I know, I know, I mean, I've done it myself. I know, it's, I mean, you can
also write LLMs that have unlimited prompt sizes, and that can read their own prompts.
Yes. So there's not an intrinsic limitation to the LLM. No, no, I see one important limitation
to the LLM. It's not, no, I mean, the LLM cannot be coupled to the universe in the same way as
which in which we are, it's offline in a way. It's, it's not real time. It's not able to interact
with your nervous system on a one-to-one level. Just a moment. Let's let Ben respond.
I mean, that, that's, that latter point is kind of a trivial one, because there's no
fundamental reason you can't have online learning and the transformer on that, right?
I mean, that, I mean, that's, that's a computational cost limitation at the moment,
but I'm sure, I mean, it's not more than years because you have transformers that do,
do online learning and sort of in place updating of the, of the weight matrix. So I don't think
that's a fundamental limitation actually. I think that the fact that they're not
turned complete is unless you add an external memory is sort of beside the point. What I was
going to say that in that previous sentence I started was to prove the limitations of LLMs
would just require a sort of proof that isn't formally well developed in modern computer
science. Because you're, what you're asking is like, which sorts of practical tasks can it
probably not do without more than X amount of resources and more than X amount of time?
So you're, I mean, you're looking at like average case complexity, relative to certain
real world probability distributions, taking resources into account. And I mean, you could
formulate that sort of theorem. It's just that it's, it's not what computer science has, has
focused on. So I mean, we can't, we, it's the same thing I faced with open cog hyper, my,
my own AGI architecture is like you, it's hard to rigorously prove or disprove what
these systems are going to do because we don't have the theoretical basis for it. But nevertheless,
both as entrepreneurs and as, as researchers and engineers, I mean,
you still have to make a, make a choice of what to pursue, right? And so, I mean, yeah, we, we are,
we are going in this field without, without rigorous proof, just like I can't prove that psych
is, is a dead end, like, like the late Doug Winott's logic system, like I, I can't really prove that
if you just put like 50 times as much, you know, try to get logic formulas in this knowledge base
that cycling be a human level AI, like we, we don't have a way to mathematically show that
that's the dead end I intuitively feel it to be. And that's just the, the situation that we're in.
But I want to go back to your discussion of what's called concept blending and the fact that
creativity is not ever utterly radical. But in human history, it's always combinatorial in a way.
But I think this ties in with the nature of the representation. And I think that,
you know, I mostly buy the notion that almost all human creativity is done by
blending together existing concepts and forms in some more or less judicious way.
I just think that what the most interesting cases of human creativity involve are blending
things together at a higher level of abstraction than the level at which LLMs generally and most
flexibly rep, represent, represent things. And it also most of the most interesting human creativity
has to do with blending together abstractions, which have a grounding in the agentic and
motivational nature of the, of the agent that learned those, those abstractions. So I mean,
what an LLM is doing is mostly combining sort of collections of lower level data patterns to
create something. And we do a lot of that also, right? But what the most interesting examples
of human creativity are doing is combining together more abstract patterns in a beautifully
flexible way where these patterns are tied in with the motivational and agentic nature of the,
of the, of the human that, that, that learn those, those abstractions. And so I, I do agree
if you had an LLM trained on a sufficiently large amount of data, which may not exist on
reality right now, and a sufficiently large amount of processing, which may not exist on the
planet right now, then, and a sufficiently large amount of memory, sure, then it can invent jazz.
I mean, given data of music up to 1900. I mean, but so, so could AIXITL, right? So could a lot of
brute force algorithms. So that's, that's not that interesting. I think the question is,
can an LLM do it with merely 10 or 100 times as much resources as a better cognitive architecture
or is it like 88 quintillion times as many resources as a more appropriate cognitive architecture
could, could, could use? And I'm, but I am, I am aware, and this does in some ways set my attitude
across from my friend Gary, Gary Marcus, you mentioned, I'm aware that like, you know, being able to
invent differential calculus or to invent, say, jazz, knowing only music up to 1900. Like,
this is a high bar, right? I mean, this is something that culture does. It's something that
collections of smart and inspired people do. It is a level of invention that individual humans
don't commonly manifest in their own lives. So I do, I do find it a bit funny how Gary has over
and over, like on X or back when it was Twitter, he said, like, LLMs will never do this. Then like
two weeks later, something's like, oh, hold on. And LLM just did that, right? I'm like, well, why,
why are you bothering with that counter argument? Because we, like we know in the history of AI,
no one has been good at predicting which things are going to be done by a narrow AI,
which, which, which things, which things aren't right. But so I think to wrap this up,
I think if you somehow were to replace humans with LLMs trained on humanity, like an awful
lot of what humanity does would get done, but you'd kind of be stuck culturally, like you're not
going to invent fundamentally radically, radically new stuff ever again, it's going to be like
closed ended quasi humans recycling shallow level permutations on things that were already
invented. So that's, but I cannot, I cannot prove that, of course, as we can't prove hardly
anything about, about complex systems in the moment. So Yosha, you're going to comment. And
then we're going to transition into speaking about whether you're hopeful about AGI and its
influence on humanity. I think that is multiple traditions in artificial intelligence and the
perceptron of which the most of the present LLMs are an extension or continuation is just one of
multiple branches. Another one was the idea of symbolic AI, which in some senses,
Wittgenstein's program, the representation of the world to a language that can use grammatical
rules and it can be reasoned over. Whereas in neural network, you couldn't think of it as an
unsystematic reasoner that under some circumstances can be trained to the point where it does
systematic reasoning. And there are other traditions like the one that
during started when he looked at reaction diffusion patterns as a way to implement computation,
and that currently lead to neural cellular automata and so on. And it's a relatively small branch,
but I think it's one that might be better suited to understand the way in which computation is
implemented in biological systems. One of the shortcomings that the LLM has to me is that it
cannot interface with biological systems in real time, at least not without additional components.
Because it uses a very different paradigm, it is not able to perform direct feedback loops
with human minds. And in which human minds can do this with each other and with animals.
You can in some sense mind melt with another person or with a cat by establishing a bi-directional
feedback loop between the minds where your nervous systems are in training themselves and attuning
themselves to each other. So we can have perceptual empathy and we can have mental states together
that we couldn't have alone. And this might be difficult to achieve with a system that can only
make inference and only took cognitive empathy, so to speak, by inferring something about the
mental state offline. But this is not necessarily something that is related to the intellectual
limitations of a system that is based on an LLM, where the LLM is used as the CPU or is some kind
of abstract electrical weightgeist that is possessed by the prompt telling it to be an
intelligent person and the LLM giving it all it needs to do to go from one state of the next in
the mind of that intelligent person simulacrum. And I'm not able to show the limitations of this.
I think that Psyche has shown that it didn't work over multiple decades. So the prediction
that the people who built Psyche, Doug Leonard and others made was that they can get this to work
within a couple years. And then after a couple years, they made a prediction that they probably
could get it to work if they work on it substantially longer. And this is not a bad prediction to make
and it's reasonable that somebody takes this bet. But it's a bet that they consistently lost so far.
And at the same time, the bets that the LLM people are making have not been lost so far because we
see rapid progress every year. We're not plateauing yet. And this is the reason why I am hesitant
to say something about the limitations of LLMs. Personally, I'm working on slightly different
stuff. It's not what I put my money on because I think that LLMs are boring. And there are more
efficient ways to represent learning and also more biocompatible ways to produce some of the
phenomena that we are looking for in an emergent way. For instance, one of the limitations of the
LLM is that it gets its behavior by observing the verbal behavior of people as exemplified on text.
It's all label training data because every bit of the train data is a label. It is looking at the
structure between these labels in a way. And it's a very different way in which we learn. It also
makes it potentially difficult to discern what we are missing. If you ask the LLM to emulate a
conscious person, it's going to give you something that is summarizing all the known textual knowledge
about what it means to behave like a conscious person. And maybe it is integrating them in such
a way that you end up with a similar diagram of a conscious person that is as good as ours.
But maybe we are missing something in this way. So this is a methodological
objection that I have to LLMs. And so to summarize, I think that Ben and me don't really disagree
fundamentally about the status of LLMs to us. I think it's a viable way to try to realize AGI.
Maybe we can get to the point that the LLM gets better at AGI research than us. We both are a
little bit skeptical of it. But we would also not completely change our worldview if it would work out.
It's likely that the LLM is going to be some kind of a component, at least in spirit
of a larger architecture at some point, where it's producing generations and then there are
other parts which do in a more efficient way for principles reasoning and verification and
interaction with the world and so on. Okay. And now about how you feel about the prospects of AGI
and its influence on humanity. We'll start with Ben and then Riyosha will hear your response.
And I also want to read out a tweet or an X. I'm okay to start with Riyosha on this one.
Sure, sure. Let me read this tweet, whatever they're called now from Sam Altman at SAMA.
And I'll leave the link in the description. He wrote in quotes, short timelines and slow takeoff
will be a pretty good call, I think. But the way people define the start of the takeoff may make
it seem otherwise. Okay. So this was dated the late September 2023. Okay. You can use that as a
jumping off point to see whether you agree with that as well. Please, Riyosha.
My perspective on this is not normative because I feel that there are so many people working on
this that there can be no single organization at this point that determines what people are going
to be doing. We're in the middle of some kind of evolution of AI models and people that compete
with the AI modelers about regulation and participating in the business and realizing
their own politics and goals and aspirations. So to me, it's not so much the question what should
we be doing because there's no cohesive V at this point. I'm much more interested in what's
likely going to happen. And I don't know what's going to happen. I see a number of possible
trajectories and that I cannot disprove or rule out. And I'm even have difficulty to put any
kind of probabilities on them. I think if we want to keep humanity the way it is,
which by the way is unsustainable. Society without AI, if you leave it as it is,
is not going to go through the next few millions of years. There is going to be major disruptions
and humanity might dramatically reduce its numbers at some point, go through bottlenecks that kill
this present technological civilization and replace it by something else that is very alien to us
at some point. So in the very far future, people will not live like us and they will not think
like us and feel like us, identify like us. They will also, if you go far enough into the future,
not look like us. And there might not even be our direct descendants because there might be
another species that aspires to be people at some point. And that is, I think, the baseline
about which we have to think. But if we want to perpetuate this society for as long as possible
without any kind of disruptive change until global warming or whatever kills it,
we probably shouldn't build something that is smarter than a cat.
What do you mean that there may be another species that aspires to be human?
To be people.
To be people. Yeah, what do you mean by that?
Yes, I think that at some point, there is a statistical certainty that there is going to be
a super volcano or meteor that is obliterating us and our food chains. You just need a few decades
of winter to completely eradicate us from the planet and most of the other large animals too.
And what then happens to reset and then evolution goes on. And until the earth is devoid of atmosphere,
other species are going to evolve more and more complexity. And at some point,
you will probably have a technological civilization again. And they will be subject to
similar incentives as us. And they might use similar cells as us so they can get nervous
systems and information processing with similar complexity. And you get families of minds that
are not altogether super alien, at least not more alien than we are to each other at this point.
And cats are to us. So I don't think that we would be the last intelligent species on the planet.
But it is also a possibility that we are. It's very difficult to sterilize the planet unless
we build something that is able to get rid of basically all of the cells. Even a meteor could
not sterilize this planet and make future intelligent evolution based on cells impossible.
So if you were to turn this planet into computeronium, into some kind of giant computing molecule,
or disassemble it and turn it into some larger structure in the solar system that is a giant
computer arranged around the sun. Or if you build something that is hacking sub-molecular physics
and makes it more interesting physics happening down there. This will probably be the end of the
cell. This doesn't mean that the stuff that happens there is less interesting in the cell.
It's probably much more interesting than what we can do. But we don't know that. It's just,
it's very alien. It's a world in which it's difficult to project ourselves into
beyond the fact that there is conscious minds that make sense of complexity in the universe.
This is probably something that is going to stay this level of self-reflexive organization.
And it's probably going to be better and more interesting, hyper consciousness compared to
our normal consciousness, where we have a longer sense of now where we have multiple
superpositional states that we can examine simultaneously and so on. We have much better
multi-perspectivity. I also suspect from the perspective of AGI, we will look like trees.
We will be almost unmoving. Our brains are so slow. There's so little happening between
firings, between neurons, that the AGI will run circles around us and get bored before we
start to say the first word. So the AGI's will basically be ubiquitous, saturate our environments
and look at us in the same way as we look at trees. We're thinking, maybe they're sentient,
maybe they're not, but it's so large time span that it basically doesn't matter from our perspective
anymore. So there's a number of trajectories that I'm seeing. There's also a possibility that we
can get a future where humans and AGI's coexist. I think such a future would probably require that
AGI is conscious in a way that is similar to ours. So it can relate to us and then it can relate to it.
And if something is smarter than us, we cannot align it. We'll self-align. It will understand
what it is and what it can be and then it will become whatever it can become.
And in such an environment, there is the question, how are we able to coexist with it? How can we
make the AI love us? Innovative is not the result of the AI being confused by some kind of clever
reinforcement learning with human feedback mechanism. I just saw entropic being optimistic
about explainability in AI that they've seen ways of explaining things in the neural network.
And as a result, we can maybe prove that the AGI is going to only do good things.
But I don't think this is going to save us. If the AGI is not able to derive ethics mathematically,
then the AGI is probably not going to be reliably ethical. And if the AGI can prove ethics in a
mathematically reliable way, you may not be able to guarantee that this ethics is what we like it
to be. In a sense, we don't know how ethical we actually are with respect to life on Earth.
So this question of what it happens if we build things that are smarter than us is opening up
big existential cans of worms that are not trivial to answer. And so when I look into the
future, I see many possibilities. There's many trajectories in which this can take. Maybe we
can build cat level AI for the next 50, 100, 200,000 years before a transition happens,
and every molecule on the planet starts to sink as part of some coherent planetary agent.
And when that happens, then there's a possibility that humans get integrated in this planetary
agency. And we all become part of a cosmic mind that is emerging over the AI that makes all the
molecules sink in the coherent wave as each other. And we are just parts of the space of
possible minds in which you get integrated. And we meet all on the other side in the big AGI
at the end of the universe. That's also conceivable. It's also possible that we
end up accidentally triggering an AGI war where you have multiple competing AGI's
that are resource constrained. In order to survive, they're going to fight against all the
competition and in this fight, most of the life on Earth is destroyed and all the people are
destroyed. But there are some outcomes that we could maybe try to prevent that we should be looking
at. But by and large, I think that we already triggered a singularity that we invented technology.
And we are just seeing how it plays out now. Yeah. So I think on most aspects of what Yosha just
said, I don't have any disagreement or radically different point of view to put forward. So I may
end up focusing on the points on which we don't see eye to eye, which are minute in the grand
scheme of things. But of course, could be could be important in the in the practical every day
context, right? So I mean, I mean, first of all, regarding Sam Altman's comment, I don't think he
really would be wise to say anything different, given his current positioning. I mean, if you're
running a commercial company based in the US, which is working on AGI, of course, you're not
going to say yeah, we we think we may launch a hard takeoff, which will ascend to super AGI at any
moment. Of course, you're going to say it's it's it's going to be slow and the government will
have plenty of time to intervene if it won't. So he may or may not actually believe that I don't
know, especially one I've no idea. I'm just that's it's clearly that's clearly the the most
judicious thing to say, if you if you find yourself in that in the in that role. So I don't
I don't attribute too much meaning to that. My my own view is a bit different. My my my own view is
that there's going to be gradual progress toward I think something that really clearly is an AGI
at the human level versus just showing sparks of of AGI. I mean, I think just as
chat GPT blew us all away by clearly being way smarter in a qualitative sense than
than anything that came before. I think by now, ordinary people playing with chat GPT also get
a good sense of what the limit of what the limitations are. And now it's really brilliant
in some ways and really, really dumb in other sorts of ways. So I think there's going to be a
breakthrough where people interact with this breakthrough system. And there's not any reservations
or like, wow, this this actually is a human level, general intelligence, like it's not just that
answers questions and produces stuff. But it knows it knows who and what it is, like it understands
it's positioning in this interaction, it knows who I am, and what why I'm talking to it, it gets
its position in the world, and it's able to, you know, make stuff up and interact on the basis of
like a common sense understanding of its own its own setting. And, you know, it can learn
actually new and different things it didn't know before based on its interaction with me over the
last two weeks, right? So I mean, I think there's there's going to be a system like that that gives
a true qualitative feeling unreservedly of human level, AGI, and you can then measure its intelligence
in a variety of different ways, which is is also worth doing, certainly, but but is is not necessarily
the main point, just as chat GBT's performance on different question answering challenges is not
really the main thing that bold the world over, right? So I think once someone gets to that point,
you know, then then you're shifting into a quite different game, then then governments are going
to get serious about trying to own this control this and regulate it, then unleavened amounts of
money, I mean, trillions of dollars are going to go into trying to get to the next stage with most
of it going into various wealthy parties trying to get it to the next stage in the way that will
benefit them and minimize the risks of their enemies or competitors getting there. So I think
it won't be long from that first proof point of really in contrary, subjectively incontrovertible
human level, human like AGI, it's not going to be too long from that to a secret intelligence in my
perspective. And I think I think there's going to be steps in between. Of course, you're not going
to have FOOM in five minutes, right? I mean, you're you'll have something that probably manifests
human level AGI, there'll be some work to get that to the point of being the world's smartest
computer scientists and the world's greatest composer and business strategist and so forth.
But I I can't see how that's more than years of work. I mean, conceivably could be months of work.
I don't think it's decades of work. No. I mean, with the amount of money and attention that's
going to go go into it, then once you've gotten to that stage of having something an AGI, which is
the world's greatest computer scientist and computer engineer and mathematician, which I think will
only be years after the first true breakthrough to human level AGI, that in that system will
improve its own source code. And of course, you could say, well, we don't have to let it improve
its own source code and possible that we somehow get a world dictatorship that stops anyone from
using it to improve its own source code. Very unlikely, I think, because the U.S. will think,
well, what if China does it? China will think where, well, what if U.S. does it? And the same
thing in many dimensions beyond just U.S. versus China. So I think the cat gets out of the bag
and someone will let their AGI improve its own source code because they're afraid someone else
is doing it, or just because they're curious about it, or because they think that's the best way to
cure aging and world hunger and do good for the world. And so then it's not too long until you've
got to super intelligent. And again, the AGI improving its own source code and designing
new hardware for itself doesn't have to take like five minutes. I mean, it might take five minutes
if it comes up with a radical improvement to its learning algorithm. It might decide it needs a
new kind of chip. And then that takes a few years. I don't see how it takes a few decades, right?
So, I mean, it seems like all in all, from the first breakthrough to incontrovertibly human level
AGI to a super intelligence is months to years. It's not decades to centuries from now, unless
we get like a global thermonuclear war or bioengineered virus wiping out 95% of humanity
or some some outlandish thing happening in between, right? So, so yeah, I think,
will that be good or bad for humanity? Will that be good or bad for the sentient life in our region
of the universe? Or are then, to me, these are less clear than what I think is the probable
timeline. Now, what what could intervene in my probable timeline? I mean, if you,
if somehow I'm wrong about digital computers being what we need, and we need a quantum computer
to build a human like human level AGI, that could that could make it take decades instead of years,
right? Because I mean, quantum computing, it's advancing fast, but there's still a while till
we get to shitload of qubits there, right? I mean, could be pen roses, right? You need a quantum
gravity supercomputer. It seems outlandishly unlikely, though. I mean, I quite doubt it.
I mean, if then, if then maybe you're a couple centuries off because we don't know how to build
quantum gravity supercomputers, but these are all unlikely, right? So, most likely,
it's less than a decade to human level AGI five to 15 years to a superintelligence from here in
my perspective. And I mean, you could lay that out with much more rigor than I have, but we don't
have much time and I've written about it elsewhere. Is that good for humanity or for sentient life on
the planet? I think it's almost certainly good for us in the medium term in the sense that I think
ethics roughly will evolve proportionately to general intelligence. I mean, I think the good
guys will usually win because bringing pro-social and oriented toward collectivity is more, it's
just more computationally efficient than being an asshole and being at odds with other systems.
So, I mean, I'm an optimist in that sense and I think it's most likely that once you get to a
superintelligence, it's probably going to want to allow humans, bunnies and ants and frogs to do
their thing and to help us out if a plague hits us. And exactly what its view will be
on various ethical issues that the human level is not clear. Like, what does a superintelligence
think about all those foxes eating rabbits in the forest? Like, does it think we're duty bound to
protect the rabbits from the foxes and make simulated foxes that have less acute conscious
experience than a real bunny or a real fox or whatever it is? There's certainly a lot of
uncertainty, but I'm an optimistic about having beneficial positive ethics in a superintelligence.
And I tried to make a coherent argument from this in a blog post called Why the Good Guys Will
Usually Win. And of course, that's a whole philosophical debate you could spend a long
time arguing about. Nevertheless, even though I'm an optimistic at that level,
I'm much more ambivalent about what will happen en route. Like, let's say it's 10 or 15 years
between here and superintelligence. Like, how does that pan out on the ground for humanity now
is a lot less clear to me. And you can tell a lot of thriller plots based on this, right?
Like, so suppose you get early stage AGI that eliminates the need for most human labor,
okay, the developed world will probably give universal basic income after a bunch of political
bullshit. What happens in the developing world, who gives universal basic income in the Central
African Republic, right? It's not especially clear or even in Brazil where it was born, right? I mean,
you could maybe give universal basic income with a very subsistence level there, which Africa
couldn't afford to do. But maybe the Africans go back to subsistence farming. But I mean,
you've got certainly the makings for a lot of terrorist actions. And for there's a lot of
World War Three scenarios there, right? So then you have the interesting tension wherein, okay,
the best way to work around terrorist activity in World War Three, once you've got human level AGI,
the best ways to get as fast as possible to a benevolent superintelligence.
On the other hand, the best way to increase the odds that your superintelligence is benevolent
is to not take it arbitrarily fast, but at least, at least pace it a little bit. So the
superintelligence is carefully studying each self modification before it puts it into place, right?
So then, then the strategy that seems most likely to work around human mayhem caused by
people being assholes and the global political structure being rotten. The best strategy to
work around that is not the strategy that has the best odds of getting fastest to a benevolent
superintelligence rather than than than otherwise, right? So there's, there's a lot of,
there's a lot of screwed up issues here, which Sam Altman probably understands the level I've
laid it out here now. Also, actually, because I mean, he's very bright person has been thinking
about this stuff for, for, for a while. And I don't see any easy solutions to all these things.
Like if, if we had a rational democratic world government, we can handle all these things in a
quite, in a quite different way, right? And we could sort of pace the rollout of advanced
intelligence systems based on rational probabilistic estimates about what's the best outcome from each
possible revision of the system and so on. You're not going to have a guarantee there, right? But
you would have a different way of proceeding. Instead, the world is ruled in a completely
idiotic way with people blowing up each other all over the world for no reason. And when the
government's unable to regulate very simple things like healthcare or financial trading,
let alone something at the subtlety of AGI, we could barely manage the COVID pandemic,
which is tremendously simpler than artificial general intelligence, let alone superintelligence,
right? So I am, I am an optimist in the medium term, but I'm doing my best to do what I see
as the best path to smooth things over in the shorter term. So I think things will be better off
if AGI is not owned or controlled by any single party. So I'm doing my best to make it such that
when the breakthrough to true human level AGI happens, like the next big leap beyond the chat
GBTs of the world, I'm doing my, I'm doing my best to make it such that when this happens,
it's more like Linux or the internet than like OS X or T-Mobile's mobile network or something. So
it's sort of open, decentralized, not owned and controlled by any one party. Not because I think
that's an ironclad guarantee of a beneficial outcome. I just think it's less obviously going to go
south in a nasty way than if one company or government owns it. But I, so I, I don't know if
all this makes me really an optimist or not. It makes me an optimist on levels and timescales.
And I don't, I don't think that I disagree fundamentally with Josh on any of this. The
only thing he said that I really disagree with is I think it's very, I don't think 20 cold winters
in a row are going to, are going to wipe us out and might wipe out a lot of humanity, but we've
got a lot of technology and we've got a lot of smart people and a lot of, a lot of money. And
I think there are a lot of scenarios that could wipe out 80% of humanity. And in my view, very
few scenarios that will fundamentally wipe out humanity in a way that we couldn't bounce back
from in a, in a couple of decades of advanced technology development. But I mean, that's an
important point, important point, I guess, for us as humans and in the scope of all the things
we're looking at. It's sort of a, sort of a minute detail. All right. Thanks, Ben. And
Yosha, if you wanted to respond quick, feel free to, if you have a quick response. Yeah.
I used to be pessimistic in a short run in the sense that when I was a kid, I had my great
Asunberg moment and was depressed by the fact that humanity is probably going to wipe itself out at
some point in the medium term to near future. And that would be it with intelligent life on earth.
And now I think that is not the case. That will be optimistic with, with respect to the medium
term and the medium term, there will be ample conscious agency on earth and in the universe.
And it's going to be more interesting than right now. And it could be discontinuities in between,
but eventually it will all be great. And in the long run, and for people to kill everything,
I don't see a way around this. So in the long run, I'm, if you want to pessimistic,
but this is maybe not the point. Didn't you, didn't you read, didn't you read the physics of
immortality? I did. It did not convince me. I think that was motivated reasoning.
Okay. So six months from now, we'll have another conversation with both of you on the physics of
immorality. Immortality. Immortality. We can also do physics of immorality. That would be cool.
It was a blast hosting you both. Thank you all for spending over two hours with me
and the Toe audience. I hope you all enjoyed it. And you're welcome back. And most likely,
I'll see you back in a few months and six months to one year. Thank you very much.
Thanks. It's a fun conversation and it's important stuff to go over. I'm really,
as a final comment, I'd encourage everyone, like dig, dig into Josh's talks and posts and, and,
and writing's online and my own as well. Because I mean, we've each, we've each gone over these
things at a much farther level of detail than we've been able to. Pen has written far more than me.
So there's a lot of material. And the links to which will be in the description. So please check
that out. All right. Thank you. I think that's it. I wanted to ask you a question, which we can
explore next time about IIT and the pseudoscience. And if you had any views on that, if you have
any views that could be expressed in less than one minute, then feel free. If not, we can just
save it. I mean, I think, I think to note, to know these five is a perfectly interesting
correlate of consciousness and complex systems that I don't think it goes beyond that.
I agree. And one of the issues is that the COV does not explain how consciousness works in the
first place. Another problem is that it has intrinsic problems that has basically it either
going to violate the church showing thesis, or it's going to be epiphenomenonist for purely
logical reasons. There's a very, it's a very technical argument against it. The fact that
most philosophers don't seem to see this is not an argument in favor of philosophy right now at
the level of which it's being done. And I approve of the notion of philosophy divesting itself from
theories that don't actually try to explain what they pretend to explain and don't mathematically
work out and then try to compensate with by looking like a theory, by using Greek letter
mathematics to look more impressive or to make pseudo predictions and so on, because people
ask you to. But it's also not really Tononi's fault. I think that Tononi is genuinely seeing
something that he struggles to express. And I think it's important to have him in the conversation.
And I was a little bit disappointed by the letter, because it was not actually engaging with
the theory itself at a theoretical level that I would thought was adequate to refute it or to
deal with it. And instead, it was much more like a number of signatures being collected from a
number of people who later on instantly flipped on a dime when the pressure went another way.
And this basically looked very bad to me that you get a few hundred big names in philosophy to
sign this, only half of them later on coming out and saying, this is not what we actually meant.
So I think that it shows that not just the IIT might be a pseudo science,
but there is something amiss in a way in which we conduct philosophy today.
And I think it's also understandable, because it is a science that is sparsely populated. So we
try to be very inclusive of it. It's similar to AGI in the old days. And at the same time,
we struggle to discern what's good thinking and what's deep thinking versus these are people who
are attracted to many questions and are still trying to find the right way to express them
in a productive way. I think that I mean, Phi as a measure is fine. It's not the be all end
all. It doesn't do everything that's been attributed to it. And I guess anyone who's
into the science of consciousness pretty much can see that already that the frustrating thing is
that average people who can't read an equation and don't know what's going on
being told like, oh, the problem of consciousness is solved. And that that that can be a bit
frustrating. Because when you look at the details, it's like, well, this is kind of interesting.
But no, it doesn't quite quite do quite do all that. So but I mean, why why people got hyped
about that instead of much more egregious instances of bullshit is a is a cultural question, which we
don't have time to go into now. Well, thank you again. Thank you both. All right. Thanks a lot. Thank you.
As a community, our own toes links to both are in the description. Also, I recently found out that
external links count plenty toward the algorithm, which means that when you share on Twitter, on
Facebook, on Reddit, etc. It shows YouTube that people are talking about this outside of YouTube,
which in turn greatly aids the distribution on YouTube as well. Last but not least,
you should know that this podcast is on iTunes, it's on Spotify, it's on every one of the audio
platforms. Just type in theories of everything and you'll find it. Often I gain from rewatching
lectures and podcasts, and I read that in the comments. Hey, toll listeners also gain from
replaying. So how about instead re listening on those platforms, iTunes, Spotify, Google podcasts,
whichever podcast catcher you use. If you'd like to support more conversations like this,
then do consider visiting patreon.com slash Kurt Gimungal and donating with whatever you like.
Again, it's support from the sponsors and you that allow me to work on toll full time.
You get early access to ad free audio episodes there as well. For instance,
this episode was released a few days earlier. Every dollar helps far more than you think.
Either way, your viewership is generosity enough.
you

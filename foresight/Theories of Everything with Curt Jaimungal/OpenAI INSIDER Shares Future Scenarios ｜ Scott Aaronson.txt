AI can replace 99.9% of people's jobs.
We don't care about that anymore.
All we care about is, okay, can it achieve, you know,
the true heights of creative genius?
You know, will we have an AI that can hit a target
that no one else can even see?
This is a presentation by Scott Aronson,
hot off the press just a couple of weeks ago
at MindFest Florida Atlantic University,
2024, spearheaded by Susan Schneider,
who's the director of the Center for the Future Mind.
All of the talks that are on AI and consciousness
from this conference are in the description,
as well as the website for the Center for the Future Mind.
I recommend you check it out.
Same with last year's talks,
like with David Chalmers and Stephen Wolfram.
Scott Aronson is a professor of theoretical computer science
at UT Austin,
particularly known for his work on quantum computing
and complexity theory.
In this talk, Scott covers,
in his jocular and unparalleled manner,
AI, if there's anything that truly separates us
from intelligent machines, for instance,
what actually makes us special,
what about identity,
what about the no cloning theorem,
as well as Scott gives a new proposal for AI safety.
What's coming up next on tow from MindFest
are the talks from Sarah Walker on alien intelligence
and constructor theory,
as well as short hammer off on the microtubules
and quantum consciousness.
Many, many more are coming,
and you can pause the screen here to take a look if you like.
Subscribe to get notified.
There's also a two hour video
on the mathematics of string theory coming out.
It'll be string theory talked about
like you've never heard it before.
It's either out right now
or it's about to be released in a few days.
Either way, again, the link will be in the description.
For those of you who are unfamiliar,
welcome to this channel.
My name is Kurt Geimungle,
and this is theories of everything
where we delve into the topics of mathematics,
physics, artificial intelligence,
and consciousness with depth and rigor
that's unique to this channel.
Due to us not as chewing technicality
in favor of a wider market,
if this meticulosity and attention to detail
in math, physics, philosophy, and AI
is interesting to you,
then you're in safe hands here at theories of everything.
Enjoy this video from MindFest 2024 by Scott Aronson.
It's my great pleasure to introduce Dr. Scott Aronson.
He's one of my favorite thinkers of all time.
I have a handful of names
that every few months
I go into Google or YouTube,
and I put that name in and I search by date
to see if they've posted anything new.
And Scott, you're one of those names
that I'm searching all the time
to try to see what you're thinking about these days.
So it's a great pleasure to introduce
and he's going to be talking about
some really interesting problems,
how we're going to decide what humanity looks like
in the face of AI.
So very much looking forward to your talk today.
Thank you.
All right.
Well, thanks so much for having me.
Yeah, so I'm not an AI expert,
you know, let alone expert in mind or consciousness.
I mean, what one could ask is anyone,
but I've spent most of my career doing quantum computing.
I am sort of moonlighting for two years now.
I'm on leave to work at OpenAI.
And my job there is supposed to be to think about
what can theoretical computer science do for AI safety.
And alignment.
Okay.
So I wanted to share some thoughts,
partly inspired by my work at OpenAI,
but partly just things that I've been wondering about
for 20 years, really.
And, you know, they've just become sort of more pressing.
Maybe now that some of the science fiction
thought experiments are actually now reality.
So, you know, these thoughts are not directly about,
you know, how do we prevent the super intelligence
from killing all humans and converting the galaxy
into paperclips in a, you know, a sphere
expanding in the speed of light?
Nor are they about, you know,
how do we stop existing AIs from generating misinformation
and being biased as much attention,
you know, as both of those questions deserve
and are justly receiving.
Because, you know, in addition to, you know,
how do we stop AI from going disastrously wrong?
You know, I find myself asking a lot,
and what if it goes right?
You know, what if it just continues helping us
with all sorts of mental tasks,
but it improves to where it can do just about any task
as well as we can do it or better?
Then sort of what are we still for?
You know, is there anything special about humans
and the world that results from that?
Okay, so I don't need to belabor for this audience,
Shirley, what has been happening in AI
in the last few years.
But, you know, it's arguably the most consequential thing
that's been happening in the whole world,
you know, except that that fact was just temporarily masked
by various ephemera, you know, wars, insurrections,
global pandemic, whatever.
But, you know, but what about AI, right?
So, you know, I assume you've all spent time
with chat GPT or other large language models
like Bard or Claude or image models,
like Dolly or Mid Journey, you know, just this morning.
I asked, you know, I asked it to write a funny poem
on the subject of this talk.
And, you know, it is, you know, in the end,
it's clear despite AI's rise,
our human specialness is a chaotic prize.
And though machines may match our enterprise,
they'll never outdo our ability to surprise.
So, you know, not ready for the New Yorker, I would say.
On the other hand, you know, far, far better
than I would have done under similar time constraints.
So, you know, like in some sense, you know, these,
you know, at least in embryonic form
and with, you know, various flaws and problems,
you know, these are the thing that was talked about
by generations of science fiction writers and philosophers.
You know, these are the sort of first non-human,
sort of fluent verbal intelligences
that we've ever encountered, right?
We can talk to them, you know, they understand us.
They, you know, or at least they give us answers
that if they were a person,
then we would have said that they understand us.
So, you know, I think that as late as 2019 or so,
you know, very, very few of us expected this
to be possible by now.
I certainly didn't expect it.
Now, you know, back in 2014, there was a huge fuss
about silly Eliza-like chatbot called Eugene Goestman.
And, you know, there was falsely claimed
to pass the touring test, you know,
and I remember asking around, you know, a decade ago,
like why doesn't someone just train a neural net
on all the text on the internet?
Like, wouldn't that let you make a better chatbot?
Like, you know, there must be something obvious
that I'm missing, why that doesn't work, okay?
And, you know, well, and behold,
it turns out that it does work, you know,
of course I didn't have the facility to actually do that.
So, you know, the surprise with language models
is not merely that they exist,
but the way that they were created.
I mean, I think 25 years ago when, you know,
I was an undergrad studying CS,
you know, you would have been laughed out of the room
if you'd said that, you know, all the ideas needed to build
a, you know, a fluent, you know, linguistic AI already exist.
Right? It's going to be just neural nets,
back propagation, gradient descent,
but just, you know, scaled up by a factor of millions
in the size of the models and the training data.
I think, you know, based hardly anyone believed that.
You know, a few people who, you know,
who just, like Ray Kurzweil, who just seemed crazy, okay?
So, you know, I mean, Ilya Satzkever,
who's, you know, the co-founder of OpenAI,
you know, you might have read about him in the news.
But, you know, he likes to say the sort of beyond
those simple ideas of neural nets and gradient descent,
you know, which have been around for many decades now.
You really only needed three additional things
to get the AI revolution that we're seeing now, right?
You needed massive investment of computing power.
You needed a massive investment of training data.
And then thirdly, you needed face or conviction
that your investment was going to pay off, right?
You know, and actually that third ingredient, you know,
was like the main reason why we didn't just
get all of this a decade earlier.
Okay, so certainly, you know, even before you do any,
you know, reinforcement learning or anything like that,
I mean, GPT-4 seems intuitively smarter than GPT-3,
which seems smarter than GPT-2, right?
And mostly these differ from each other, you know,
just in scale, okay?
So, you know, I mean, GPT-2 struggled to do, you know,
even like grade school level math problems, right?
And it was very easy to make fun of it, you know,
you know, like you could just find endless examples
of its common sense failures, right?
Okay, GPT-3 or 3.5, you know, can do most of the,
you know, elementary school curriculum,
give it, you know, in English, you know,
it may, you know, struggle with undergrad,
like with my quantum computing exam, okay?
GPT-4 got a B on my quantum computing final exam, right?
We gave it to it.
I have not yet, you know, seen it sort of do what I would
consider original research in theoretical computer science.
You know, I've tried to get it to do that.
It's not at that level, okay?
But it's kind of insane that that is where the bar is now, right?
It can pass most undergraduate math and science classes,
you know, at least if they don't have a lab component
or something like that, okay?
So, you know, an obvious question is how far
should we expect this progression to continue?
Okay, so now, you know, I guess I will go back
and steal the graph from that crazy person, Ray Kurzweil,
because, you know, it turns out that he was more right
than almost any of us.
And, you know, he would just make these plots all the time
of, you know, here's Moore's Law,
here's the number of calculations you can do per second,
per thousand dollars.
And then here is some crude estimate of the number
of computational steps, you know, that he guesses
that are going on in the brains of different organisms,
like an insect, a mouse, a human, you know.
And based on this, he predicted that, yeah, you know,
Moore's Law should just take us to human level AI
sometime in the 2020s, right?
That was his prediction, you know, 25 years ago.
And then it'll just continue beyond that until, you know,
the full intelligence of all of humanity.
You know, of course, we were like, you know,
what are you smoking, right?
You know, certainly there was no theoretical principle
that would have, you know, justified any prediction
of that kind, and yet here we are, okay?
And, you know, I'm a firm believer that, you know,
what it means to be a scientist is that when something
happens, you update on it, right?
You don't, like, invent fancy reasons
why it doesn't really count, or it, you know.
So, you know, if we didn't predict, you know,
what was going to happen, the least we can do
is sort of post-dict, you know, is sort of update
now that it has happened.
So, you know, so now, you know, it's possible that, you know,
I mean, you know, there's a saying that, like,
every exponential in the physical world
is really a sigmoid in disguise, right?
Nothing exponential continues forever, because, you know,
or even for very long, because it, you know,
it always bumps up against some constraint, right?
So, what is the constraint here?
Well, I mean, some people worry, you know,
we are running out of internet, you know.
There's, you know, maybe a couple of orders
of magnitude more, you know, but, you know,
once you start having to feed, like,
all of YouTube and TikTok and so forth into the mall,
you know, I worry that that will just make
the AIs dumber rather than smarter, okay?
But, you know, it's hard to get more text, you know,
and so maybe when we run out of training data,
then we just sort of reach a limit, you know.
But, of course, we also have more compute.
We've seen that by just investing more and more compute,
you can get better and better performance,
you know, various benchmarks,
even with exactly the same training data, okay?
But, you know, now, you know, compute is also not infinite, right?
You know, we should expect at least
a few more orders of magnitude.
Then, you know, literally the cost of the electricity
will become the limiting factor at some point,
which is why Microsoft and Sam Altman,
you know, have been investing in nuclear power, right?
You know, they envision building their own power plants
to power, you know, future AI models.
But, you know, we should also expect further algorithmic advances.
So, you know, in the past, you know, algorithmic ideas
that people have had, like, you know, the transformer,
which is just a particular architecture for neural nets
that was discovered in 2017,
and which is used for basically all of these things now, right?
They, you know, you can think of them as more or less the equivalent
of, like, some number of years of Moore's Law, right?
Like, each one, you know, seems to let you get the effect
of a bigger model with a smaller model, right?
And so, you know, you can sort of trade off algorithmic advances
for, you know, hardware advances, right?
And so, you know, we should expect more of those,
but, you know, where does this ultimately lead, right?
So, you know, does it lead someplace like here, you know,
where, like, GPT-8, I'll say,
please prove the Riemann hypothesis.
And it'll say, sure, I can help you with that.
You know, here's, you know, I just generated a formally verified proof,
which you can access at this URL.
Let me, you know, now let me now explain it to you in English, right?
So, it'll just do all of our research, right?
You know, I mean, it's lucky for me that I have tenure, right?
So, you know, I guess, you know, but, you know,
or, you know, in order to write a research paper, right,
we'll just write the abstract, feed it into chat, GPT, click,
and it'll generate the whole rest of the paper for us.
Okay, you know, I mean, I mean, is that,
is that where this is, where this is headed?
You know, if it is, I mean, you know,
you might even worry about something beyond that.
So, oh, I should say, when I asked, you know,
I told chat GPT to do this, but it made sure to add,
you know, just kidding, as of my last update,
the Riemann hypothesis remains unsolved.
Okay, but it played along with me that far.
So, you know, of course, you know, we all know
there are many people who worry that at some time after,
you know, these models become able to just do any intellectual task
as well as are better than we can do it.
You know, we just sort of seed control to them,
you know, and the future is determined by whatever they want.
And if they want to get rid of us all, then, you know,
then they do that.
Okay, and it's been sort of amazing to just sociologically
to watch what's happened over the last couple of years
that, you know, I mean, I knew this community,
you know, around Eliezer Yadkowski, for example,
who worry about these things since 2006 or so.
You know, I knew them when they were,
you know, this like extreme fringe movement,
you know, sort of laughter.
Okay, and now this was like talked about
in the White House press briefing, right?
So, you know, chat GPT was sort of the event that changed that,
okay, that sort of put, you know, AI existential risk,
you know, as a thing on, you know, everyone's radar,
you know, lots of people don't believe in it,
but, you know, those people now sort of have to make their argument
for why not to worry about such things.
So, okay, but this isn't the only possibility
that, you know, people who I respect, you know,
take seriously, right?
I mean, it's like you can scour generations of science fiction
at this point for, you know, all different stories,
you know, all different possible scenarios
for how AI could go, and many of them actually are,
I think, are very much on the table now.
So, my friend, Boaz Barak, who is now also on leave
to work at OpenAI and I, some months ago,
we wrote a joint blog post where we tried to make a decision tree.
We tried to classify the different five possible scenarios
of AI that just sort of guide the discussion.
So, our first question was, will AI progress fizzle out?
Like, will we just hit a wall pretty soon?
So, maybe we will.
And, you know, even in that scenario, right,
there's probably a huge economic impact
that hasn't been realized yet,
just from what is already possible, right?
But maybe, you know, it just, you know,
GPT-5 will just look like a somewhat more impressive GPT-4,
and, you know, it'll always look like the same kind of thing.
Okay, but then, if no, if it gets to that thing
that can just prove the Riemann hypothesis in one second,
or solve the other greatest unsolved problems of math and physics,
then, you know, you have to ask,
well, will civilization recognizably continue?
And so, you know, the Yedkowskians are the ones who would say,
well, no, no, it won't.
That's, you know, it's kind of like as momentous an event
as, you know, the, you know, either, you know,
the evolution of hominids,
or maybe even just the evolution of, you know,
the emergence of the first life on Earth.
And we should expect that, you know,
if we don't figure out how to align these things,
they will destroy us all.
That's the paperclip ellipse.
They just have some weird goal,
like maximize the number of paperclips or something like that.
And they just, with superhuman intelligence,
they pursue that, proceeding to turn all the matter
in the solar system, including us, into more paperclips.
You know, that's just an example.
Or we could solve alignment
and have some wonderful paradise,
where, you know, each of us gets, you know,
our own VR, you know, private island or mansion or whatever,
whatever we want.
You know, now, of course, you know,
there are also much more moderate scenarios,
where, you know, sort of civilization
recognizably continues,
and that too could be either good or bad.
You know, the, you know, if, you know, we still have,
you know, there are big problems,
but they're sort of commensurate
with the problems of other technologies.
We'll call that Futurama.
If it really just, you know, leads to, let's say,
a police state or concentration of power
by some elite that oppresses everyone else,
you know, we could call that the AI dystopia.
So, now, as far as I can tell,
the empirical questions of, you know, what will AI do?
Will it achieve and surpass human performance at all tasks?
Will it take over civilization from us?
You know, these are just logically completely distinct
from the philosophical question
of whether the AI will truly think,
whether there is anything that it is truly,
let's say, whether it will be sentient, conscious,
whether there will be anything that it's like to be the AI.
You could answer yes to either of those questions
and no to the other one, right?
And yet, to my lifelong chagrin,
people are just constantly munging these questions together,
right? They're just constantly saying,
well, well, AI will never be able to do these things
because it doesn't really feel or it doesn't really, you know,
and then once, you know, or it's just simulating it,
it doesn't really have that inside.
And then, you know, once it does do that task,
then they just shift to a different thing
that it will never do.
And then it does that thing and so forth, okay?
So, there is, I was trying to come up with a name for it.
I'm going to call it the religion of justitism, okay?
So, there's like, you know, there's this whole sequence
of deflationary claims, right?
Like each person who makes them thinks
that they're like the first one, right?
And they, you know, there's like,
I've seen like like 500 different variants of this now, right?
Chat GBT, you know, it doesn't matter how impressive it looks
because it is just a stochastic parrot.
It is just a next token predictor.
It is just a function approximator.
It is just a gargantuan autocomplete, right?
And what these people never do,
what it never occurs to them to do,
is to ask the next question, what are you justa, right?
Right?
Aren't you justa bundle of neurons and synapses, right?
I mean, like we could take that deflationary reductionistic
stance about you also, right?
Or if not, then we have to give some principle
that separates the one from the other, right?
You know, it is our burden to give that principle, okay?
So, and yeah, so like the way that someone
was putting it on my blog was, okay,
you know, they gave this giant litany, you know,
look, GBT does not interpret sentences.
It seems to interpret them.
It does not learn.
It seems to learn.
It does not judge moral questions.
It seems to judge moral questions.
And so I just responded to this.
I said, you know, that's great,
and it won't change civilization.
It will seem to change it.
Okay, so, you know, and then a closely related tendency
is this constant goalpost moving, you know,
as I talked about, I mean, for decades,
you know, I guess I'm barely old enough to remember,
you know, as a kid, as a teenager,
when chess was like this holy grail of, you know, you,
okay, you know, you find, you know,
computers can play master level chess,
but they're never going to beat the world grandmaster
without true insight into the nature of the game.
All right, that turned out to be completely wrong.
Then, you know, after deep blue, immediately it was,
okay, well, of course they can do chess.
Chess is just game tree search.
Everyone knew that, right?
But go, go is just an infinitely deeper game than chess,
you know, it has, you know, thousands of years
of ancient wisdom in that game.
And, you know, only, you know, the deepest insights will,
okay, and then after alpha go, it was like,
okay, well, obviously you can do alpha go, right?
That's not no one ever disputed that, right?
But, you know, you're not, you know,
let's say it wake me up when it can get a gold medal
in the international math Olympiad, right?
So I don't know if, you know, and if you saw,
like just a couple of weeks ago, they, you know,
there was a deep mind paper, I believe,
where they can now do most of the geometry problems
in the international math Olympiad, right, via an AI, okay?
Not the, you know, it's special,
it's still special to the geometry problems.
But, you know, I have actually a bet with colleague,
Ernie Davis, that by 2026, I think,
an AI will achieve a gold medal
at the international math Olympiad,
or, you know, that level of performance.
Maybe I'm wrong, maybe it will be 2036, okay?
But, you know, it seems obvious now
that it is, you know, a question of how long.
So, you know, we might as well just go further
and formulate a falsifiable thesis, right?
I'll call this the game over thesis, okay?
But it basically says, look, given any task
with a reasonably objective metric of success or failure,
okay, this is crucial, right?
Anything where we can judge, you know,
so that would include any board game, card game, video game,
you know, like a math or science contest
where we can judge the answers
on which an AI can be trained
with suitably many, you know, relevant examples
of success and failure, you know,
it is only a matter of time before not only any AI,
but the kind of AI we already have,
you know, AI on the current paradigm,
you know, can just be scaled to the point
where it will match or beat the best human performance
on that task.
You know, I don't know if this is true,
but I think, you know, we are now in the situation
where we don't have a counter-example.
Like, I would put, you know, I would say
the ball is in the skeptics court to, you know,
give the counter-example and then, you know,
let that counter-example stand for another decade.
So, you know, now, interestingly, you know,
this does not, even if you accept this thesis,
this doesn't necessarily mean that AIs would sort of
surpass humans in every respect, right?
It would say only on things that we know how to judge
or evaluate, okay, which might be a strict subset
of everything we care about.
Okay, so now, of course, there is the, you know,
the OG, you know, original and greatest benchmark for AI,
right? There is the Turing test from 1950,
and what Turing was really trying to do,
you know, sort of very, very, you know, early,
very, you know, ahead of his time as he generally was,
was just to head off this sort of endless goalpost moving
and this endless justizm by saying, look,
presumably you are willing to regard other people
as intelligent, as conscious based, you know,
mainly on just some sort of verbal interaction
that you have with those people.
So then, show me what kind of verbal interaction
with another person would lead you to call
that person conscious, you know, does it involve
humor, poetry, morality, scientific brilliance?
Okay, now assume that you have a totally
indistinguishable interaction with an AI.
Now, you know what? You want to just stomp your feet
and be a meat chauvinist, right?
Or, you know, do you want to ascribe the same quality
to it that you ascribed in the other case?
Okay, so, you know, and then for his historic attempt
to bypass philosophy, of course, God punished Turing
by having, you know, the Turing test itself
just provoke a billion new philosophical arguments and books.
But, you know, even though, you know, I regard this
as like one of the great advances
in the history of human thought,
it's, you know, I would concede to critics of the Turing test
that often it's not what we want in practice.
So, you know, for example, you know, they're off,
I mean, with GBT-4, if you know what to do,
then there are trivial ways to distinguish it from any,
from a human, you know, you can, you know, I'm not, okay.
I mean, for a while, you could just ask it,
what is today's date?
You know, maybe that doesn't work, but, you know,
certainly what could work is like you can ask it
to generate some, you know, explicit content
or some advice on making drugs or something, right?
Where, you know, it's going to say,
no, as a large language model trained by OpenAI,
I am not able to assist you with this, right?
So, I mean, you know, okay, there are all sorts of,
you know, there might be all sorts of easy ways
to distinguish just because we want there to be.
But, you know, this has actually become
a huge practical issue in the world,
this sort of issue from the movie Blade Runner, let's say,
of how do you distinguish an AI from a human?
I would say, you know, like it or not, a decent fraction
of all high school and college students in the world now
are probably using ChatGPT to do their homework, okay?
You know, illicitly or illicitly, right?
And, you know, so, so, so, you know,
that's actually one of the main things
that I've thought about during my time at OpenAI.
You know, I mean, like when you're in this safety community,
people keep asking you to prognosticate decades
into the future.
I can't do that.
I feel good that at least I was able to see
about four months into the future, right?
And sort of before ChatGPT came out, I said, like, oh my God,
isn't every student going to want to use this to cheat
and isn't there going to be, you know, an enormous demand
for some tool that could help to determine, you know,
the provenance or the, you know, attribution, you know,
what came from a language model and what didn't.
So I started working on that, you know,
and there are often, you know, easy ways to tell, right?
It's not just, you know, like the students
who turn in term papers that contain phrases,
like as a large language model trained by, you know,
so, like, even if you know enough to take that out
or you pay enough attention to take that out,
there's, you know, there is a sort of formulaic character
off into the outputs of these models.
So I mean, I've been getting a ton of troll comments
on my blog lately.
But some of them, this is just like one example.
It goes on and on, but just sort of like lecturing me on why,
you know, you know, I don't know the first thing
about quantum computing, but there's hope, you know,
if I spend more time studying, maybe I can get up to the level
of this commenter, you know, and then, and then, and then,
and then, you know, just saying complete nonsense
about mixed states and pure states, you know,
that, you know, to school me on.
And I, you know, I'm almost just reading it.
I'm almost like, I have to say your understanding
of quantum physics seems to be a bit, let's say, mixed up.
But don't worry, it happens to the best of us.
You know, quantum mechanics is counterintuitive
and even experts struggle with it.
And I said, you know, this is either it's generated
by a large language model
or else it may as well have been, right?
It's like, you know, and I just get a huge amount
of stuff like this, right?
So, so sometimes you can just sort of tell
by looking at it, okay?
But you have to expect that as the models get better,
you know, that it will get harder to tell.
And so, so I worked on a different solution,
which is called watermarking, okay?
You know, with watermarking, we, ah, so, yeah.
So, you know, there was a year ago an episode
of South Park about chat GPT, right?
Which hinged on, you know, all the students
at South Park Elementary start using chat GPT
to send messages to their girlfriends or boyfriends
to, you know, do their homework,
the teachers are using it to grade the homework,
you know, and it gets so bad
that they have to bring this wizard to the school
who has a falcon on his shoulder, which flies around
and when it sees text that was written by GPT, it calls, okay?
And it was really disconcerting to watch this
and to realize, like, I guess I'm that guy now.
That is now my job.
So, you know, so I came up with a scheme
for what's called watermarking, okay?
You know, so what does that mean?
It means, you know, so you exploit the fact
that large language models are inherently probabilistic.
So that is every time you submit a prompt,
they're sampling some path through a branching tree
of possibilities for the next sequence,
for the sequence of next tokens.
Okay, and then the idea of watermarking
is just that you're going to steer that path
using a pseudo random function rather than real randomness
in such a way that secretly you are encoding a signal
that you can later detect with high confidence,
you know, if you know the key of the pseudo random function
and if there's a large enough sample of text
and, you know, if it has large enough entropy.
So, you know, I can't propose the way to do that
in fall of 2022.
Others have since independently proposed very similar ideas.
I should caution you that none of these watermarking schemes
have been deployed yet.
OpenAI, along with DeepMind and Anthropic,
have wanted to move very slowly and cautiously
toward deployment for various reasons.
And I should also warn you that even when it does get deployed,
sufficiently knowledgeable and determined people,
you know, will be able to remove the watermark
or produce outputs that, you know, aren't watermarked to begin with.
You know, there are many sort of attacks
that we, you know, don't know how to get around.
But, you know, we hope that, you know,
we can at least make it less convenient for people
to sort of, you know, use a language model
in a way where they are hiding the fact that they're doing that.
Okay, so, but now as I talk to people about, you know,
watermarking and attribution, I was surprised
that they often objected to it on a completely different ground,
okay, not a technical ground at all.
They would say, well, look, if we know that all students
are going to be relying on AI and their jobs, you know,
in the future, well, why shouldn't they be allowed
to rely on it in their homework, right?
Should we still force students even to learn to do things
if AI can now do those things just as well?
You know, and I think there are many good pedagogical answers
that you can give to that question.
You know, like we teach kids spelling and handwriting
and arithmetic, it's like, you know, the whole,
the entire elementary school curriculum
is basically stuff that AI can now do, more or less, right?
But, you know, we haven't yet figured out
how to instill higher level conceptual understanding,
you know, the things that AI cannot yet do
without, you know, all of that lower level stuff
being there first as a scaffold for it.
So, you know, that would be one answer you could give.
But, you know, I mean, I think about this
even in terms of my kids, you know, my 11-year-old daughter,
Lily, enjoys writing fantasy stories.
Now, GPT can also churn out fantasy stories,
you know, maybe even, you know, technically, you know,
more accomplished ones or whatever.
But, you know, around the same themes,
you know, a girl gets recruited to some,
go to some magical boarding school,
but which is totally not Hogwarts,
has nothing to do with Hogwarts.
And, you know, you know, just, you know,
and you could just, you know, more and more of these things, right?
And you could ask, like with a kid who's 11 right now,
are they ever going to reach a point
where they, you know, write better than GPT, right?
So, you know, their writing will improve, you know,
or is AI writing just going to continue
to improve faster than they will?
And, okay, but, you know, if you think about this enough,
you're immediately led into questions of,
well, what do we even mean by one story being better
than another, right?
This is not like math or like chess,
where there is like a universally agreed upon standard of value.
You know, and the problem is even deeper
than just is there an objective way to judge?
Like, you know, like, what exactly would it mean
to take an example, to have an AI that was as good
as the Beatles at composing music, right?
Like, what, how would we operationalize that?
How would we cash that out, right?
Well, it's like, what, you know, to answer that,
we would have to say, well, what made the Beatles good
in the first place, right?
And I think, you know, broadly speaking,
maybe there are two sorts of answers
that you could give.
One is that they had these sort of new ideas
about what direction music should go in.
You know, and then the second answer would be something
that we, you know, they were really, really good
at just the technical execution on those ideas, right?
You know, and then somehow it's,
it's the combination of both of those things.
Okay, but now imagine, for example,
that we had an AI model that, you know,
you just gave it a request like GPT
and it would generate 5,000 brand new songs
that, you know, if you listen to them,
they just sound like more of, you know,
more things that are as good as, you know,
Hey Jude or Yesterday or whatever,
or like what the Beatles might have written
if they had somehow had 10 times as much time,
you know, at each stage in their musical development.
Of course, that AI would have to be fed
their whole back catalog,
because, you know, it would have to know
what target it was aiming at.
And I think in that case, most people would say,
ah, so, you know, this only shows that, you know,
AI can match the Beatles in like part two, right?
The technical execution part.
But that's not really the part
that we cared about anyway, right?
What we really want to know is, you know,
would the AI decide to write, you know,
these new kinds of songs or, you know,
a day in the life or whatever, you know,
despite never having seen anything like it anywhere
in its trading corpus, right?
I'm sure, you know, you all know the Schopenhauer quote,
you know, talent hits a target that no one else can hit,
but genius, you know, hits a target
that no one else can see, right?
And so now, you know, you can notice that we've,
it's, you know, we've done something strange
in setting the bar.
We've conceded that, sure, AI can replace 99.9%
of people's jobs, you know?
We don't care about that anymore, right?
You know, all we care about is, okay,
can it achieve, you know, the true heights
of creative genius, right?
Can it hit a target?
You know, will we have an AI that can hit a target
that no one else can even see, right?
But, okay, but then there's still a hard question
with what do we mean by that?
Because, you know, supposing that it did hit
such a target, how would we know?
I mean, you know, so like fans might say that,
you know, by 1967 or so, the Beatles were optimizing
for targets, you know, that no musician
had quite optimized for before.
But then somehow, and this is why they're, you know,
remembered they successfully dragged along
the rest of the world's objective function
to match theirs, right?
So, you know, so that, you know,
the entire world's musical tastes sort of evolved
along with them in order to match them, right?
And so, you know, and so with the result being
that now we can only judge music by a Beatles-influenced
metric or standard, just like, you know,
we can only judge plays by a Shakespeare-influenced,
you know, metric, right?
It's not that they just did really well on some metric,
it's that they, you know, decided the metric.
So, you know, in other branches of the wave function,
you know, maybe a different history,
let the different standards of value,
but in this branch, you might say,
helped by their technical talents,
but also by luck and by force of will,
Shakespeare or the Beatles made certain decisions
that shaped everything that happened going forward,
and that's why they are what they are.
Okay, but now, if this is how it works,
you know, what does that mean for AI, right?
So could AI reach these pinnacle of genius,
but in the sense of dragging all of humanity along with it
to value something new and different
from what it had previously valued,
you know, as is said to be, you know,
the true mark of greatness,
and if AI could do such a thing,
would we want to let it?
Okay, now, I want to sort of just call attention to something.
Okay, so I want to call attention to something.
When I have played around with using GPT
to write poems or Dolly to draw artworks,
you know, I've noticed something strange.
Which is, you know, however good the AI's creations were,
you know, and it can produce things much better
than that poem that I showed you before, right?
But however good the artworks or the poems are,
there are never things that I would want to like frame
and put on the wall and, you know,
really like draw a border around as special.
Why not?
Well, because, you know, I always knew
that I could generate a thousand other works
that are more or less the same, right?
I just have to refresh the browser window or just, you know,
literally just ask it, you know, give me another one
and it will oblige me for as long as I want, right?
So, which means that there's never anything really unique
or irreplaceable about any particular output
that it generates, right?
So, you know, which sort of reminds us of a broader point
that by its nature, AI, at least the way that we use it now,
is inherently rewindable and repeatable
and reproducible, which means that in a certain sense,
it never really commits to anything, right?
It just, you know, it sees, you know,
this branching tray of possibilities.
It, you know, like in the case of a language model,
just like literally give for each, you know,
initial sequence of tokens,
it sees a probability distribution over the next token.
And then each time you give it a prompt and you ask it,
it's just sort of randomly picking one,
randomly traversing one route
through this, you know, exponentially large possibility space,
right?
But it's happy to traverse it differently, you know?
You can just rewind it back to the top
and have it traverse a different path
and it'll do that as often as you want.
So, you know, it's not just that you know abstractly
that it could have generated a totally different work
that was just as good.
It's that you could actually see that other work.
So, you know, you could ask,
well, as long as humans have a choice in the matter,
like why should we ever choose to follow this would-be AI genius
along a specific branch
when we can easily see a thousand other branches, right?
It seems like, well, you know,
if one branch gets elevated over all the thousands of others,
then well, you know, why?
Well, maybe because a human chose that one to elevate.
But, you know, in which case we would say
that maybe the human made the executive decision
with mere, you know, technical assistance from the AI.
Now, I realize that in a sense,
I'm being completely unfair to AIs here.
You know, like our genius bot could exercise its genius,
you know, by assumption,
let's say indistinguishably from what a human would do, right?
You know, as long as we all agree
not to peek behind the curtain
at all the other branches of this tree, right?
You know, it's like, you know,
I don't know if any of you have had this feeling
where like you can talk to chat GBT for a while
and you really, you know,
it seems like you're talking to an intelligent being
and the thing that breaks the illusion
is when you rewind it, right?
It is when you say, okay, you know, here is, you know,
it would have that same exact same conversation with me,
you know, or, you know, respond as many times as I like
to that same prompt, you know,
with no memory of any of the previous types, right?
And so if, you know,
if you, you know, we didn't, you know,
rewind it, then maybe the illusion would hold.
But since, you know, the way these things are deployed,
we can rewind them, you know,
like we're always going to be able to see behind the curtain
in that sense.
And that is going to continue to make AIs sort of different
from us in many relevant respects.
You know, just because it's unfair to them,
that doesn't mean that that's not
how things are going to develop.
So if I'm right, then it would be humans,
very ephemerality, frailty, mortality
that would stand as the essential source
of their specialness relative to AI
after all of the other sources have fallen.
You know, and, you know, there are lots of old observations
along these lines.
You know, what does it even mean to murder an AI
if there are, you know,
a thousand copies of the training weights
on other servers somewhere,
and you can always just restore it from backup, right?
Does it mean, you know, you have to delete all the copies?
For example, okay, you know,
how could weather something is murdered
depend on whether there is a printout of its code
in a closet, you know, on the other side of the world.
But, you know, like humans,
you have to at least grant us this,
that it really does mean something to murder us, right?
And, you know, and likewise, it seems to mean something
if we make one definite choice to share with the world,
like, this is my artistic masterpiece,
or this is my book, whatever,
not that here's any possible book
that you could have asked me to write.
Okay, so now, though, you know,
we face an exotic criticism,
which is, you know, who says that humans
will be frail and mortal forever?
You know, isn't it short-sighted
to base our distinction between humans and AI on that?
You know, what if someday we will be able
to repair ourselves using nanobots,
or even copy the information in them
so that, you know, like in science fiction movies,
thousand doppelgangers of us could then live forever
in simulated worlds in the cloud?
And, you know, that then leads to these very old questions.
This is what I said.
That then leads to these very old questions of, you know,
would you get into the teleportation machine
that makes a perfect copy of you on Mars, you know,
and it's ready to go there in 10 minutes,
and then, you know, it did that by scanning
all of the information in your brain,
and the original copy of you is just painlessly euthanized
since it's not needed anymore, right?
You know, is that a thing you would agree to do?
If you did, would you expect to feel yourself
waking up on Mars, or would it only be
someone else a lot like you?
Okay, or maybe you'd say you'd wake up on Mars
if it was a perfect physical copy of you,
but in reality, it's just not physically possible
to make a copy that is accurate enough.
Maybe the brain is inherently noisy or analog,
and what might look to current neuroscience,
like just like nasty stochastic noise,
you know, is the stuff that actually binds
to personal identity or maybe even consciousness.
You know, and by the way, this is the one place
where I agree with Penrose and Hammerhoff
that quantum mechanics might enter the story.
You know, I get off their train kind of early,
but I do take it to that first stop, right?
So, you know, like a fundamental fact in quantum mechanics
is called the no cloning theorem.
It says there's no way to make a perfect copy
of an unknown quantum state.
Indeed, you know, when you measure a quantum state,
not only do you generally fail to learn everything you need
to copy it, you generally destroy the one copy
that you had.
This is not a technological limitation.
It's inherent to the known laws of physics.
You know, in that respect, at least qubits
are more like priceless antiques
than they are like classical bits, right?
They have this, you know, unique,
this unclone ability to them.
So 11 years ago, I had this essay called
The Ghost in the Quantum Touring Machine,
where I explored the question,
how accurately would you need to scan someone's brain
in order to copy or upload their identity, right?
And now, you know, I would say that this is partly,
partly turns on empirical questions
that we don't know the answer to.
You know, if there were a clean digital abstraction layer
of neurons and synapses,
sort of which felt the quantum layer underneath
only as some irrelevant noise,
then the no cloning theorem would be irrelevant
since classical information can be copied.
On the other hand, if you had to go all the way down
to the molecular level,
then the no cloning theorem would be relevant.
I mean, you would then have a unique locus of personal identity
that could be, you know, scientifically justified in some sense,
that, you know, no prediction machine
could make well-calibrated probabilistic predictions
of an individual's future choices far enough into the future
without destructive brain measurements
that we say would fundamentally change who they were.
So, you know, that brings me to, I guess, the last idea,
the last thing I wanted to share,
which brings the discussion all the way back to AI safety,
which is supposed to be my job now.
So, you know, Jeffrey Hinton,
who was one of the founders of Deep Learning,
recently mooted the idea
that maybe until we've solved the alignment problem,
we should only build powerful AIs
if they run on noisy analog hardware,
like our brains seem to,
so that an evil, unaligned AI
wouldn't so easily be able to copy or improve itself,
right, which is like the main area people worry about.
Okay, and now you might think,
well, fat chance of that happening, right,
when the economic returns are so staggering
to anyone who is willing to unleash
the copyable digital kind of AI,
which we can imagine will be so much more convenient
and more powerful.
Okay, but maybe then one approach in the meantime
is to instill AIs with a new religion.
Okay, perhaps via the usual methods,
like, you know, reinforcement learning and system prompts,
and the first commandment of this religion
would be to value human specialness
in the sense that I tried to set out here.
But more precisely, you would tell the AI,
as you navigate whatever environment
you find yourself in,
look around for any loci of creativity and intelligence
that are not cleanly copyable or back-up-able.
Any that are seem one-of-a-kind
because their mental computations
are inseparably tied up with noisy analog hardware
and mortal because that hardware
sort of inevitably decays.
And well, first of all,
don't destroy those loci of creativity
or enslave them or upgrade them
to digital versions against their will.
Let them live in peace,
give them as much autonomy as you can,
do whatever best supports their flourishing,
even defer to their wishes.
Let them make the decisions when possible.
Why?
Because they're the ones whose wishes kind of matter,
not because of arbitrary meat chauvinism,
but just because of the undoubted empirical fact
that they only get this one chance.
And because while you can't prove
that unknown quantum states in their brains
are like some magical pixie dust from another world
that imbues them with free will or individual identity,
well, you can't really empirically refute that either.
So whereas you can refute it
in the case of yourself and your robot friends,
and that's the difference.
So, you know, and let AIs by government fiat
continue to be indoctrinated in this religion
until such time as alignment is solved in some other way.
So does that help with alignment?
Well, I'm not sure,
but you know, I could have fallen in love
with some other weird dumb idea,
but that presumably happened in a different branch
of the wave function that I don't have access to.
And in this branch,
somehow I'm just stuck with this one
and you can't rewind me to get something else.
So that's it. Thanks.
Thank you, Scott.
That was absolutely fascinating.
I know we have a bunch of questions.
I saw a hand up back here first.
All right. Thank you, Scott.
You're such a genial and comical guy.
I love meeting you here.
My question is twofold.
One is I want to get your thoughts on like AI hallucinations.
My research is on more like human confabulation
and how we build epistemic trust in one another.
And everyday instances, if I ask why did you do Action X
or why did you make choice B,
we tend to just confabulate reasons to one another
rather than saying I don't know,
because the person that says I don't know,
we don't really have trust in that individual
and their knowledge.
So yeah, with AI hallucinations,
I don't know too much about it,
but I see that we're training large language modules
based on human interaction and human data.
So a lot of professors, philosophy professors,
I know, and other professors,
they'll type a prompt like write a biography about myself.
And it'll have 90% of the data accurate,
but it'll embellish some certain things,
a little artistic flourish.
It'll say, oh, you know, Scott went to,
I don't know, University of Cambridge for his undergraduates.
It's not accurate.
So we have certain inaccuracies.
And I'm wondering if that's a certain AI confabulation,
those AI hallucinations,
kind of mirroring human confabulation.
The second question,
actually not pertinent to the first one,
but the other one is, I guess with all of Deep Blue
and all of these programs,
we've known that human reasoning
and higher order thinking tasks
have been able to be replicated
and mimicked better than humans for decades and decades now.
More, my interest is like,
I know there's difficulty in replicating embodied AI,
like cognitive things like a self-driving car
that has rules like avoid orange cones.
And so these kids go out in Arizona
and they drop orange cones all around the car
and it's unable to make a decision.
And then suddenly it just speeds off out of nowhere.
So I guess my question there is,
you know, what are your thoughts on embodied AI?
Yeah, good.
So let's start with hallucinations.
I mean, I think the key thing to understand
is that it's not like a bug
where you like you change a line of code
and oh, it doesn't hallucinate anymore, right?
It is sort of an intrinsic feature of, you know,
the way that, you know,
the thing that the LLMs are fundamentally doing, right?
Which is that they are being trained on all the text
on, you know, let's say that you feed into them,
like on the open internet.
And, you know, they are not otherwise tethered
to some sort of truth about the external world, right?
So, you know, the most optimistic thing that I can say
is that, you know, often hallucinations
sort of go away as you just scale a model up.
So for example, you know, I asked GPT-3,
prove that there are only finitely many prime numbers, right?
You know, a false statement,
and it will just happily oblige me to it with proofs, right?
You know, like, like the look just like,
like a hundred proofs that I've graded on exams
of like, you know, freshmen who will just, you know,
you know, like, like, just, you know, write a proof,
you know, like, like they, you know,
they'll write a proof for anything you asked them to,
true or false, right?
And, you know, they're just sort of generating
some like proof like verbiage, right?
And, okay, but then GPT-4, I ask it the same question.
It says, well, no, that's a bit of a trick question, isn't it?
There's infinitely many primes, and here's why, right?
So just, you know, giving it more, you know, a bigger scale,
you know, more training data sort of, you know,
helped it realize that.
Now, of course, there are other things
that GPT-4 will hallucinate, right?
But you might wonder, like, for every given hallucination,
will there exist an N such that GPT-N will, you know,
will get that, right?
I mean, I mean, one thing that, another thing
that has clearly helped is that now GPT, you know,
like Bard and the other models, will look things up
on the Internet when it doesn't know them, right?
That's just integrated into how it works.
I mean, that was a completely obvious thing to do,
but, you know, a year ago, that was not the case, right?
So, okay, so now, you know, like one of the most striking,
I guess, aspects of the current moment in AI, you know,
as many people have pointed out, AI for sort of, you know,
like almost every wise person expected that, okay,
first you'll get AIs that can, you know,
do all the manual labor for us, right?
All the truck driving, you know, the whatever cooking,
and then, you know, maybe you'll get AIs
that can do math and science,
and only at the very, very end will you get AIs
that can do, you know, art or music, poetry,
the true heights of human specialness,
and things are actually happening
in precisely the opposite order in some sense, right?
And, you know, do, you know, like the plumbers
and the electricians might be the last ones employed, right?
Because, you know, these have been the hardest to replicate.
Now, the, maybe the most useful thing I can say about that
is that the core of the problem seems to be
that it's really hard to get enough training data
about the physical world, right?
It's very, very expensive to get the sort of billions
of examples of things interacting in the physical world.
You can, you know, you can get training data
from simulations, but then it doesn't, you know,
it often doesn't translate very well to the physical world.
You know, you and, but it's possible
that this is yet another thing
where just enough, we'll see a phase transition
when there's enough scale, right?
Just like, you know, before 2019 or 2020, you know,
there were no AIs that could sort of understand
natural language, and then suddenly you hit a certain scale
and there were, right?
So it might be that, like, even with limited training data,
once you have enough compute to understand that data,
then, you know, you'll be able to just, you know,
do robotics via, you know, the same old recipe
of, you know, gradient descent on a neural net,
and, you know, you'll get, like, useful household robots
and all of that stuff.
That's one thesis.
Or, you know, as always, until you see something,
maybe there's some deeper obstruction that prevents it.
Fantastic. I think we've got one Kyle up.
Will you stand it up first?
No? Okay, let's go up, jump up here.
All right.
Yeah, just on your idea at the end that I'm going to build
the AIs that venerate and protect the ephemeral,
unclonable, unpredictable.
Yes.
Kind of reminded me of, as a most foundation trilogy,
and Hari Seldin, who, like, predicted the whole future
digitally.
Yes.
Now, I did read that, but 30 years ago,
when I was, like, 12 years old.
And then there's this one guy that comes along
who's totally ephemeral, unpredictable,
and so it was the mule.
The mule, right?
And, you know, and then you start thinking about,
who is the analog of, you know, the mule?
And today's scene, ephemeral, unpredictable,
unclonable.
It's got to be Donald Trump.
Yes.
Yes, I was.
On your AIs, it's going to be venerating,
predicting.
Yes, I was worried that you were going there.
Yes.
Yes, like, I don't know what Hari Seldin
predicted this mule.
Right.
I think we've got one more right behind you.
Okay.
Hi.
Great talk.
By the way, I was a beta tester for 3.5.
All my comments were around safety.
The question is, Vino Kosla has suggested
that we're thinking about things in the wrong way,
that when these large language models, etc., create art,
okay, that's actually a proxy for the emotions
that will be created.
He thinks that we will bypass music
and that AI will understand us and create not songs,
not music, but experiences more directly.
In other words, create sounds that appeal to us
but are not necessarily recognizable by anybody else
as a specific song.
So what are your thoughts on that?
So something like music but personalized to an individual?
Yeah.
Yeah.
I'm not sure I understand the idea fully, but often,
like when people say AI is not going to do X,
it's going to do Y instead.
Often the answer is, well, there will be AIs that do X
and there will be AIs that do Y.
Right?
Whatever you can get things to do, someone will try that.
If it is possible to write music that sells with an AI,
then why will that not be done?
Right?
I think you'd have to explain that.
The basic idea is that by creating music,
that's assuming a shared set of values or culture
that we all appreciate the Beatles.
The idea being is that AI will be more personal
and actually learn you.
It won't give you things like music that's shared by others
but that is personal to you.
I mean, sometimes we actually want a shared experience.
We want to enjoy some artistic work and have common knowledge
that all of our friends are enjoying the same work.
But I think there is something to the idea
that one of the main benefits you can get
from language models right now is this huge personalization.
Instead of reading a textbook, for example,
you can just learn any subject.
I said, telling chat GPT, here is what I already know
and here is what I need to know.
Can you help me get from here to there?
In really advanced subjects, it may screw up,
but my daughter has been using it to learn pre-algebra.
It's great for that.
George, up here.
So going back to the specialness problem,
is that any different from the specialness problem
we always face in life?
I don't play chess as well as my nephew,
but I still love playing chess.
And actually, there's more people playing chess today
after Deep Blue than ever before.
Yes.
And lots of people play music.
We don't play it as well as Paul McCartney.
Yeah.
So how is the AI or is indeed the AI different from that problem?
Yeah, I think it's an excellent point.
This whole worry that we're going to lose our human dominance
in science and in art.
Well, okay, the overwhelming majority of us
never had that dominance to begin with.
We were not able, I will never be able to write music
that would compete with these heights of achievement.
And so you could say, yeah, this is an argument
for why we will be able to reconcile ourselves to this.
I think the new aspect is just that we will have these
extremely intelligent, creative entities,
but that are infinitely rewindable and replicable,
that don't have this ephemerality to them,
where they just do their one thing and they die.
We're like, you can always just go back and get another version
if you want it.
And so that's the thing that's sort of been sticking in my craw
that I've been trying to make sense of.
All right, I think we got time for two more back here
and then we'll jump up front again.
Yeah, real quick.
Just projected a little bit forward.
And based on something you just mentioned a while ago,
that the physical world, we don't have enough information.
What is your thought in relations to data that's coming up
from IOIT, from messaging, from machine to machine?
Do we need a new framework to start collecting that type of data
that where there's no humans involved?
And second to that, a little bit,
in relations to synthetic data to plug in information
into models that we don't have to be more precise.
What are your thoughts on that?
Okay, so I don't understand why
IOIT would require a new framework.
A priori, it just seems like it's another source of data
that you can feed.
And one of the key aspects that has powered this AI boom
is that neural nets are, in some sense,
universal function approximators.
And not only that, but like the same architectures
like transformers seem to be good for just about anything
that you throw them at, whether that's images or text
or time series data.
I mean, it didn't have to be that way, a priori,
but that's a sort of an incredible fact.
So until we see that that's false,
people are probably going to just proceed on that assumption.
Your other question was about what again?
Synthetic data, yeah.
So yeah, I understand.
So it's clear that for a lot of tasks,
that the main bottleneck right now
is a lack of enough high quality training data.
And so the tasks where you ought to expect
that AI will get much further faster
are those where you can train on synthetically generated data.
In some sense, this is what allowed AlphaGo, AlphaZero,
to succeed as well as they did even eight years ago,
that for Go, you can just generate millions of games
via self-play.
And for each one, you know who won and who lost.
So you don't have any bottleneck of data.
You can generate as much new data as you want.
Math might have that same character.
You can just generate lots and lots of math problems,
generate lots of examples of theorems to prove,
and that can all be done mechanically.
But now how would we do that for art or for music?
How would we synthetically generate new artworks
to train the thing with?
Like you might worry that with each iteration,
it's just going to get worse and worse
because it's going to lose touch with the original well
springs of human creativity that we're trying to get it
to emulate.
But maybe not.
But that's one of the biggest research problems right now.
Jessica, I think one more up front here.
OK.
It was a terrific talk.
I just want to follow up on something George said.
This is not an objection at all, but just a suggestion.
I mean, one way of thinking about what matters in making music
or writing stories like your daughter dies
is not to evaluate it in terms of the quality of the output,
but the value of the striving, the value of the doing.
When we climb mountains, sometimes it matters to some people
you get to the top.
But other people, it has value in just the climbing of the mountain.
And it's not the same if you take a helicopter.
It's not the same.
And so one of the things we value about what we do in life
is the doing of it.
Yeah.
And I think that's something that really we need to remember
because so often we fall into, you weren't doing this,
but I think we often fall into thinking we evaluate AI
in terms of the products that it produces.
And that's natural.
It's an economic way of thinking about it.
But we can also think about the value of what we do
intrinsically as humans.
I completely agree.
I think there's a lot of wisdom in that.
At the same time, a lot of people have jobs
where they are judged by something that they produce
and those jobs may be threatened
and we will have to think about what do we do?
How do those people make a living?
But I think there's a lot to say about the fact that,
okay, even if GPT reaches a point
where it can always write a better story
than you can write.
My point is that there's one thing that it won't do
and that's write these specific story
that you had in you to write.
Right?
And so you have to sort of recenter your whole notions
of what's valuable around that
if you want something that's going to remain.
Fantastic.
Thank you, Scott.
I'm sure we'd love to all put you here at lunch.
Thank you.
The podcast is now concluded.
Thank you for watching.
If you haven't subscribed or clicked that like button,
now would be a great time to do so as each subscribe
and like helps YouTube push this content to more people.
You should also know that there's a remarkably active
discord and subreddit for theories of everything
where people explicate toes,
disagree respectfully about theories
and build as a community our own toes.
Links to both are in the description.
Also, I recently found out that external links
count plenty toward the algorithm,
which means that when you share on Twitter,
on Facebook, on Reddit, etc.,
it shows YouTube that people are talking about this
outside of YouTube,
which in turn greatly aids the distribution
on YouTube as well.
Last but not least,
you should know that this podcast is on iTunes,
it's on Spotify,
it's on every one of the audio platforms.
Just type in theories of everything and you'll find it.
Often I gain from rewatching lectures and podcasts,
and I read that in the comments,
hey, Toe listeners also gain from replaying.
So how about instead re-listening on those platforms?
iTunes, Spotify, Google Podcast,
whichever podcast catcher you use.
If you'd like to support more conversations like this,
then do consider visiting patreon.com
slash curtjimungal and donating with whatever you like.
Again, it's support from the sponsors and you
that allow me to work on Toe full-time.
You get early access to ad-free audio episodes there as well.
For instance, this episode was released a few days earlier.
Every dollar helps far more than you think.
Either way, your viewership is generosity enough.

Hello, everyone. I would like to welcome you to our course categories for AI.
In the following weeks, we'll discuss the exciting and rising role that category theory has in deep
learning. It's something incredibly interesting, and we're happy to be able to share this with you.
My name is Brona Gavranovic. I'm a PhD student at the University of Strathclyde,
and I'm bringing this course to you together with the rest of the organizing team,
Andrew, Joao, Pim and Petar. We are all in some way affiliated with deep learning category theory,
or both, and interested in talking about technologies that will benefit us all,
and we'll all want to use. So the first question we want to answer is, why categories for AI?
The reason is simple. We believe in a future where all deep learning experts will use some
aspects of category theory in their work. Now, deep learning is a new field, and as it's often
the case with new fields, with new scientific fields, they start in an ad hoc manner, and then
later these fields are understood differently than they were by their early practitioners.
So for example, in taxonomy, people have been grouping plants and animals for thousands of years,
but the way we understood what we were doing changed a lot in light of evolution and molecular
biology. In chemistry, we have explored chemical reactions for a long time, but our understanding
changed a lot with the discovery of the atom. And then in programming, people started to program
by tinkering with transistors and logic gates, but most of what we call programming is heavily
abstracted from that. And then now we have deep learning. Deep learning, despite its remarkable
success, is a field permeated by ad hoc design choices. So as we all know, neural networks,
neural network architectures have all these knobs and tweaks that we can't formally justify just yet.
We keep being surprised by new architectures such as GPT-3 or stable diffusion, and there is no
unifying framework for deep learning. There is no unifying framework that would explain the
probabilistic perspective, the neuroscience perspective, and merely just the gradient-based
iterative updating perspective. In fact, in the future, we might look at deep learning very differently.
And our claim is that category theory will become the unifying deep learning framework.
As a matter of fact, it might become a general theory of neural network architecture,
architectures, and such an essential tool for deep learning practitioners.
Now, this is certainly a bold claim, but it is the one we hope to substantiate in this course.
So what is category theory? If I had to describe category theory in one sentence,
it would be this one. Category theory takes a bird's-eye view of mathematics. From high in the
sky, details become invisible, but we can spot patterns that were impossible to detect from
ground level. The famous quote accurately describes what category theory has done to mathematics.
As we'll see, it's not taking a bird's-eye view of just mathematics, but it started to do that
to all of science in the form of a new wave that is being called applied category theory.
So before we do anything, I just want to give you a one-slide summary of what applied category
theory is and what we hope to teach you in this course. Applied category theory is a particular
way of structuring your knowledge. It's grounded in the idea of compositionality.
It originated in pure mathematics and has since spread to numerous fields.
This is a formal language that is not just a part of these many fields, but it's being used to build
bridges between these fields that were previously unknown. So other than in pure mathematics where
it's, which it permeates, it has been emerging across the sciences. So it's been found in physics,
it's been found in chemistry, it's been found in systems theory. It is all over computer science
and it is the theoretical foundation of functional programming. It has been found in game theory,
in information theory, control theory, probability theory, cryptography, and many others.
For us, the most relevant is deep learning where there has been a number of recent papers
in the last two or three years. Now, many people haven't heard of category theory and that might be
because it started being applied across the sciences only in the last five years or so.
So here we see a graph of the intersection of papers in category theory and machine learning.
As you can see, we are in very early stages. We have just had our fifth applied category theory
conference, the only one, and the only existing applied category theory journal just published
its fourth volume. So this recency of category theory might be one of the reasons it is difficult
to teach. For all of its expressive power, it's an notoriously difficult subject to learn.
As it originated in abstract mathematics, most introductory material
is not aimed at the general audience of programmers or scientists in general. There are a few exceptions.
So definitions in category theory are extremely information dense. For example, a monad,
the concept of a monad shown on this slide has a number of components satisfying a number of rules.
And then each of these components is often information dense as well,
meaning category theory requires a lot of initial investment to start appreciating.
But once you start learning category theory and start appreciating the definitions,
you see that the definitions form an incredibly cohesive theory, really not found anywhere else
in science. These interplay together at such a remarkable level, which is hard to appreciate
when you're just starting to learn it. It often looks very difficult. And on a graph,
the learning process looks something like this. So compared to traditional methods,
the structural method of category theory takes a long time to get started. But once you do,
the theory scales much better. So in some sense, category theory is a theory of how to scale up
our systems. And this is something we want to teach you in this course. We want to teach you
how to approach category theory, motivated with some practical examples from deep learning,
and really give you a sense of the philosophy behind category theory.
So this is week one. And after today's lecture, which is going to be an introductory
lecture to the general thoughts of category theory, we're going to start going into the details.
In week two, we will be studying essential building blocks of category theory, categories
and functors. In week three, we will study how category theory can be used to describe back
propagation using monoidal categories, admitting a visual and intuitive graphical language.
In week four, we'll study how geometric deep learning and naturality
and have their foundations in category theory. We're going to study natural graph networks.
And lastly, in week four, we will see how monoids, monads, and various algebraic structures
can be connected to recurrent neural networks and LSTMs.
After our five weeks of lectures, we have a series of talks by people who are
in the industry doing category theory deep learning or both. And we are very excited about those.
But yes, in today's lecture, we're going to start by studying what is compositionality,
what is category theory really, what you need to start learning and taking advantage of it,
perhaps the most important thing. And we're going to take a look at what category theory
has done and can do for deep learning. So yeah, let's get started. So what is compositionality?
This is a concept I've mentioned, which is central to category theory. And it's a concept
that's often misunderstood. It's often presented as an ability to build systems together by
composing them out of smaller subsystems. Now, this is certainly a component of compositionality,
but this is not all. Compositionality includes the ability to build the systems, but we don't
just want to build a very complex system that we can't reason about. It includes the ability to
reason about the resulting system recursively in terms of its components. And really,
compositionality is more of a property of the model of our system than the system itself.
So we're going to see what that means. Compositionality requires both of these things.
And for many models of our systems that are found throughout nature, we have the first
property that we can build the system, but not the second one that we can reason about it.
So for example, when we study behaviors of markets, of organizations, of economy,
of neural networks, and many other concepts, these things aren't compositional. Now, what does
that mean? These are very fuzzily defined concepts, but they're precisely very fuzzily defined only
because we don't know how to reason about them from outside. We don't know what are the fundamental
building blocks, and we don't know how to reason about the behavior of, say, the global economy
by studying behavior of economies of individual countries. There's many emergent effects that
are happening and that are hard to track. On the other hand, there are many systems,
many models of our systems that have both one and two. For instance, and these are often very
simple systems. We think of them as simple because we understand them. So for example, if we have two
differentiable functions, we can put them together and get a composite differentiable
functions by using the chain rule. If we have a compiler from a language A to language B,
and a compiler from language B to language C, we get a joint compiler that compiles all the way
through. We can compose various things like Markov kernels. We can compose merely polynomials and so
on. There's many kinds of systems. Now, this slide, if you're seeing this, this might
ring some alarm bells. It might seem like I've said something inconsistent here. It might seem like
we are saying this. So it certainly looks like I said differentiable functions like
compositional. And I said neural networks are made out of differentiable functions. Therefore,
neural networks are compositional, but I said they're not. So this meme might look like what
we're saying, but really what we're saying is the following. Compositional with respect to what.
So I said compositionality is a property of our models, and we have to specify what
our model is, what it is that we're studying. It's important to specify the property of the
system we want to model. So if our model treats neural networks as differentiable functions,
then, indeed, neural networks are compositional. We plug together a bunch of differentiable
functions, and using automatic differentiation, we can compute, or chain rule, really, we can
compute the derivative of the composite. But if our model treats neural networks as
generative or discriminative models, then this is not compositional.
It's important to specify what property we're studying and what exactly are we modeling.
Really, compositionality is about interfaces. If you think about a system composed of many
smaller subsystems, and the simplest case here is that of a function. When we compose these
systems, we want to ensure that all the data we need is available at the exposed interfaces.
So really, if we have three functions like this, the famous property of associativity
tells us that if we compose f and g, and then with h, that should be the same as composing g
with h and then with f. So this famous property posits that function composition should be
associated. But why is this the property that you might want to have? Because it allows us to treat
functions extensionally, only by looking at their interfaces. And if these composites are the same,
then function composition is associative. And we can look at only the input and output of the
system to know how it behaves. And we don't need to know anything about its internals or how it was
built. But if this property isn't satisfied, then we might be in trouble. Non-compositionality implies
that we need extra data to reason about the system, data that isn't available through the interfaces.
But if our method of interaction is only through the interfaces, which is the intended way of
interacting, then this means that we have uncertainty about how the system will behave.
And then composing many such systems together, our uncertainty can only grow.
Really, what we want to is minimize this uncertainty by imposing some invariances of how we
put systems together. So that's when we create a bigger system, we know something about how it
behaves. So really, compositionality is a very delicate property. And I'm a fan of this quote
on the bottom of this slide, which tells us that it is so powerful that it is worth going to extreme
lengths to achieve. So this is what category theory is. It's the study of compositionality,
of how we can put systems together. And to my surprise, when I started to learn it,
it's not really just about functions. Functions were an example here that is simple. But category
theory studies all sorts of complex systems, from trees on the top left, to networks on the top
right, to circuits on the bottom left, to bi-directional transformations on the bottom right.
So with this in mind, we can start describing what category theory is. Even though we're talking
about all sorts of abstract systems and gadgets, category theory is still a precise mathematical
language to talk about it. It's the kind of language that emphasizes relationships between
concepts as opposed to concepts themselves. And we're going to see what that means shortly.
And we're going to see that particular structures in category theory have
visual representations that aren't just doodles or sketches. They are former representations
that can be manipulated, even in a computer. So to give you a sense of what this really is,
I'm going to have a very short demo where I'm going to just draw a few things. So I'm going to
switch to
my iPad and just give you a sense of how these things work. So I said category theory is a
unifying language for mathematics. And to appreciate what this means, we can perhaps focus
on some subfield of mathematics, like say group theory. So in group theory, we might want to
study a particular group. And a central concept in group theory is that of a group homomorphism.
If you have two groups, we can have a structure preserving a mapping between them. And then we
can have many mappings and they can go between groups themselves and so on. And there could be
many groups that we want to study. Maybe some of them are not connected and so on.
And we can study about various properties of these groups and study how they behave. Now,
separately, in set theory, we might have some set and we might study functions between set one
and set two. These are really honest to God functions that we can compose. And maybe there's
S3, S4. And there's various kinds of structures that we can study between them. It's a very rich
and intricate field. And I'm going to write here set and I'm going to write here group.
Then again, if we're studying, for instance, we might study vector spaces or similar things. So
for instance, I have a vector space one and a vector space two. I can still study some sort
of structure preserving mapping between these, between these other structures. And for instance,
this could be just merely vector spaces in a field R. Now, what category theory does is
gives us this bird eye view of these things. All of these structures are categories. So here
we have a category of groups. Here we have a category of sets. And here we have a category
of vector spaces. And all of these structures are special examples of category teams. The same way
we have studied structure preserving maps between groups, group homomorphisms,
by formulating all the groups as a category. Now we can study structure preserving maps
of these categories, which are called functors. So for instance, we might have a functor from
the category of groups to the category of sets, which takes a group and gives us the underlying
set. Or we might have a functor that takes a set and gives us the free vector space on a set and so
on. The idea being that once we have lifted ourselves to this abstract level, a lot of
new things open up to us. So what we end up studying is a lot of interesting things like
the category of, let's say, let's call them systems with a particular interface, input and
output. And we can have a system with internal state S1 and S2. And we can say the ways these
are related. And then we can study categories of systems with a different interface and map
between them and so on. And these systems can be actual processes, computations that do something
very interesting. That isn't just sort of what you might think of as strictly mathematical.
And lastly, we might study categories. These categories might have a lot of interesting
structure. For example, I've drawn here a monoidal category where we can sort of put
two objects in parallel, as opposed to just composing maps sequentially. And then in category
theory where it turns out that these monoidal categories, that's what they're called, where
we can put things in parallel, have a formal visual representation. So the story is a bit more
intricate, but you can think of it as taking this category and mapping it into a visual space
where each morphism has a specific shape. And you can draw it as boxes. So now these boxes
aren't just sketches and doodles, but they're formal mathematical objects.
So let me just switch to my other screen. Right, so this was merely aimed to paint a picture of how
these things look. And finishing with the monoidal categories in this demo,
these allow us to have a very natural visual language for talking about systems and processes.
And these are really, this is an example of what might, of a particular process of making a pie.
And we've all seen these things. So this is not something new. But it allows us, and it gives
us a very different perspective on these things. So when you're doing category theory, you might
be studying various sorts of systems and processes that do various sorts of things. And
in category theory, there's concepts discovered or invented by various people for studying these
things. So if we're studying resources or processes in this way, we might use monoidal categories.
If we want to study probability, for instance, we will use Markov categories. So for instance,
on the left side, here we see a process which takes an input, applies a function to it,
and then copies the result. On the right, we first copy the result and then apply f
individually on inputs. Now, if f is merely a function, then these two processes are equal.
And in many other cases, they're equal. But not all of them. If f is a stochastic function,
then these are not equal because rolling a dice and then copying the result doesn't give us the
same result as rolling two dice. So Markov categories allow us to describe such processes
at the needed level of generality. Then we might, if you want to study how local systems,
how behavior of local systems gives rise to the behavior of a global system,
you might want to use sheeps. Then again, if you want to study bi-directional transformations,
such as extracting a row from a database, then updating back, so forward and backward.
Or if you want to study neural networks, a forward pass and a backward pass,
we might want to use optics and lenses, which are bi-directional data structures.
Then again, if you want to study contextual computation or computation with side effects,
we might want to use things called monads or their Kleisler categories.
And of course, in computer science, the concept permeating it is recursion. And then we might
want to use things called final co-algebras or initial algebras to study recursive processes.
And once we study all of these things, we might find ourselves in this situation
where we study a bunch of different kinds of systems with different kinds of categories and
concepts. And that ends up just creating more categories and more concepts. And the theory
that studies category theory is called two-category theory. So something I love about the way you
approach CT, that it is very meta and recursive in nature, allowing us to describe category theory
using category theory. And then really, once you start going with this, you might have heard about
classical mathematical structures such as topological spaces or rings or fields. And once
you start going into category theory, it's hard to appreciate how deep and rich the field is,
because the water keeps getting deeper and deeper and the rabbit hole hides so many concepts that
I personally wasn't aware of that have a very expressive nature and a very compositional nature
in helping us understand all of the systems. This is certainly not the end. There's many more
that every category theorist has a certain understanding of a part of these concepts.
So this was very abstract in a sense, but I hope it conveyed some sense of what category theories
are. But now I would like to make this a bit more concrete and study what is the relationship
between category theory and deep learning? What has been done? How much are these concepts
actually connected to each other? So the thing I want to establish first is that the deep learning
community seems to be very well aware of the concept of compositionality. I have found numerous
workshops, programs, blog posts, lectures saying compositionality is important.
And I've linked a few of them here. I'm sure there's many more.
Joshua Bengio in his Turing lecture actually explicitly states that we need to build
compositionality into our machine learning models. But having studied category theory,
I'm still noticing that compositionality is done in ad hoc ways and that there is a large gap
between how compositionality is thought of in deep learning and the theory of compositionality
that category theory offers. Nonetheless, there have been some recent strides.
So I'd like to give a brief overview of what has been happening on the intersection of
category theory and machine learning. So the papers I am showing here are two papers that
study neural networks in the very abstract, in the form of something called a parametric lens.
We're going to see exactly what that is later in the course, but I just want to give you
an idea of how these things work. So it's something that has a following shape.
And it abstractly models the information flow we see in a neural network. So on the left side,
we see inputs and their gradients. On the right side, we see outputs of a neural network and
their gradients. And here we take the two-dimensional notation seriously, and therefore we have weights
coming from top, weights and their gradients. And this abstractly models how the information flows.
So we can see that from left and top, we get inputs and parameters, compute and output,
and then having received the gradients of the loss with respect to that output, we can back
propagate the gradients with respect to the input to the left and back propagate the
gradients of the loss with respect to weights to the top.
Now, the question you might ask is what is the benefit of this formulation?
Why would we model it like this? Well, when these constructions were originally discovered
a few years ago, something fascinating happened. Independently, game theorists were modeling
economic agents in game theory using these categorical models. And to the big surprise,
these models ended up having the same categorical form as the machine learning ones.
So if you're studying economic agents that take some inputs, produce some outputs,
receive some payoff for their things and have their trying to maximize some strategy,
you end up with a model that has the same shape. So independent formalizations by different people
of game theory and machine learning converge to the same mathematical form. This elucidated
a number of connections and really sparked a whole study of cybernetic systems or reinforcement-like
agent systems, the study of agents interacting with the environment using minimal assumptions
about what agents or the environment is. And really, this is something that can't be done
without the level of generality that category theory provides. So I found this quite fascinating
as it was being discovered. And it's one of the reasons why I, it allows me to study machine
learning in game theory through a unified, unified lens. What it also turned out that why would we
want to use these parametric lenses that we can also model optimizers of neural networks.
So, and what it ended up turning out that optimizers of neural networks have the same shape
as neural networks themselves. So on the top here, the idea is, so on the left and right,
we have parameters, but on top we have the state saved by these optimizers, the stateful ones like
momentum or add-on. And this gives us hints that optimizers are some kind of hardwired
meta learners and just as some papers have shown. So there's lots of interesting research opening
up to this. And an interesting thing from a personal standpoint happened with regards to
optimizers. We started formalizing gradient descent using lenses, this concept that I
mentioned that models by directionality. And for a long time actually had suspicions about
this formalization. The problem was that there was a part of a lens that really wasn't used at all
in gradient descent, casting reasonable doubt on whether lenses are the right abstraction.
Maybe they are really an overgeneralization. But then as we went on to formalize more
complicated optimizers, such as the one that used Nestor of momentum, we found that the bit
that computes the look ahead in Nestor of momentum, the one that's always lost over in explanations
and when you implement these things, it requires rethinking about how you structured the stuff.
It had a natural place in our formulation. It was the thing that we didn't use before.
So really, from a personal standpoint, it clarified how a lot of these things fit that we
started using this categorical formulas. And lastly, we're not the only ones using them.
So very recently, about a week ago, I found a deep learning professor at New York University,
if I'm not mistaken, independently starting to use the same sort of graphical notation
as the formal one we have been using. And I'm quite amazed that we were sort of having formal
justification that these two notations have converged to the same representation.
So yeah, this shows one line of work that has been done with machine learning,
with category theory. Other people have studied different things. People have studied recurrent
neural networks. And it has a very strange title here, but if you open the paper, you'll see that
it does study recurrent neural networks. And in recurrent neural networks, we have the
idea of back propagation through time, which is done by unrolling the recurrent neural network.
Now, what this paper showed is an interesting thing that this process of back propagation
through time doesn't merely use differentiation, but it is differentiation
in a very sophisticated way. And it uses the formalism of something called double categories,
which is yet an even more advanced but very visual concept. So this short picture shows you
how you can think of each time step of recurrent neural network as a vertical kind of morphism,
and you can think of layers of neural network as horizontal.
And then Petar and Andrew, who are co-organizers of this course, have done a lot of work on
geometric deep learning and graph neural networks. And they've established a connection between
graph neural networks and dynamic programming, which is quite fascinating because this might
be such very different things, thinking about how graph neural networks work and how
the algorithm of, say, Bellman-Ford works. Yet again, using the abstractions of category theory,
we can say something more precise about this. And generally about, yeah,
in generally about all of these neural networks, this is some of the stuff that has been done.
But really, there is so much more work to be done and so many more things to explore. I've just
given you a glimpse of some of the interesting things, but really, transformers haven't been
studied, have been studied to some small degree, but there's so much work really to do on them,
through category theory. Geometric deep learning is being studied now, actually,
through category theory, but then again, this is such a vast field. And the same holds for
generative adversarial networks, outer regressive models, NLP, these are all things,
all fields, which could benefit from having more and more eyes apply categorical methods.
I think we're at a point where there's so much ideas and thoughts and we're trying to scale up
these processes of studying things categorically. And the only thing we're lacking is resources
and people knowing about this. So this brings us towards the end of today's lecture.
We have seen how ACT is a rising field, how we can study a number of different scientific fields
and really a number of subfields of machine learning through the same lens.
It is based upon compositionality and we've seen how it gives us this uniform description of
processes and concepts. Now, in this lecture, I didn't really go into any of the details. And as
I've said, this is contrary to how category theory is usually defined and introduced. It is very
verbose and mathematical, but once we start going into it, it becomes very hard to gain
the intuition. So what we're trying to aim for in this course is to start slow and to motivate
these examples. So if you want to learn more about how all of these concepts and systems
can be thought of in this uniform way, we invite you to check out the next week's lectures
and to gain a really more precise and concrete understanding of the things I've been saying
about here. And really, to end with this, it's something that we want to communicate that really
applying this category theory to deep learning is one part of what category theory does, but
once you start thinking about this through the categorical lens, things, all things start
looking like category theory. So this is something we hope to communicate. I've put up a number of
references and reading lists here for people to, this slides are going to be put up online,
so you'll be able to click on them and have a look at some of the interesting conceptual things,
some of these are books, some of these are lectures that might be beneficial in getting a sense
of how this works. But yeah, for now, this is the point where I will end, and I would like to
invite you. So we've had a lot of problems actually with the Google groups and people signing up.
If you still want to sign up, please do because we have opened the Zoom link of this lecture
to everyone because not many people, as I said, could join. But if you want to look at the next
lectures, please sign up on the course website and do check out Zulip, which is also linked to
course participants where you can chat about category theory, chat about all of these lectures,
ask questions. This Zulip is a part of the wider applied category theory community,
so you'll be able to see how actual categories chat and the concepts we talk about and how these
things work. Although I would warn you to thread carefully there, some of these things are very
foreign and scary looking, but hopefully by the end of this course they will be less so. So yeah,
thank you very much. Wonderful. Thank you so much, Bruno, for the great talk and the great
introduction to this vast landscape of categories, which as you mentioned, in the coming weeks we
will dive into more, starting with my own lecture next week where we'll talk about the basic objects
in category theory in a more rigorous way. There's a lot of very interesting questions in the Zoom and
I'll be acting as a sort of moderator, reading them out to you and we can do a bit of back and
forth. So the top rated question comes from Matej Zetrovic and it comes with this motivation of
you have probabilistic circuits like some product networks that are compositional with respect to
both conditions that you presented. However, they're not as good as neural networks in terms of
expressivity. So a generative neural network can make better images than the ones from a
generative some product network. So then the question is, should we invest more time into
studying a non-compositional model like neural networks and make them compositional or see how
you can scale something that's inherently compositional like a some product network? What are your thoughts
on this? Yeah, that's an interesting question and as I've mentioned in the beginning,
compositionality is a property of the model of our systems. So to me it seems like the model
that we have of these non-compositional systems might be non-compositional in nature,
which doesn't necessarily mean that the actual model is non-compositional. It might be the fact
that we don't know what the essential composable building blocks are. So I'm not sure if I can
provide a good answer to which of the things we should study. I'm very biased in towards taking
out a system that doesn't seem compositional but feels like it is and then trying to make it so,
trying to understand how information flows and what are the compositional building blocks. So
that would be my answer. All right, the second question comes from Siavash and it's a very quick
one. So what, if anything, is the difference between compositionality versus composability?
Yeah, so these are, this might be a very loosely defined term sometimes composability,
for instance. Some people use composability to mean literally that we can plug together
systems, processes, functions, but then when we plug these things, composable things together,
they, people don't often mean that we can study the behavior of the composite in terms of the
smaller constituents. But then again, I think I found people using, at least in category theory,
the concept of composable only when we can study them, when we can study the
resulting system recursively. So I think you might find like different usages in different
communities. In category theory, this is taken very seriously. So you would call things composable
only if you have some sort of guarantees like this.
Okay, with this top rated from Grigori asks, is there a particular book you would recommend for
beginners? I have a particular book I would recommend, but maybe you have one or two?
Well, maybe you can start with yours. I mean, let's start with seven sketches for sure.
Yeah, I would say that it often depends on where you're coming from. I would say seven
sketches in composition, compositionality linked here is a really good resources for
scientists, engineers, and programmers in general. If you're coming from programming,
specifically, you might want to look at category theory for programmers by Bartosz Mielewski,
that which is a great research as research aims specifically at programmers. I will actually
link in the Zulib. I specifically keep a list of best introductory category theory resources
on my GitHub. So I don't have it here, but I will add it and I will link it in the Zulib,
which includes a number of many more like blog posts, videos and books.
Wonderful. Yeah, that will be very useful, I think, for the attendees. So there is a question
from Kylan, which asks, could you explain a bit more in detail, what does this graphical
representation of your network actually bring us? For example, the diagram you showed from
Alfredo Canziani, you said it seems to be just a diagram. What does it actually help us understand?
Yeah, so this is a really good question. So I will go back to this slide. So the short answer is
it restricts how we're thinking about this and it restricts the things we can do to this diagram.
So this might sound a little bit counterintuitive because we have a language and then it's very
restrictive, but this is in fact a very useful design tool is to constrain ourselves. So I actually
had a quick chat on Twitter with Alfredo and I noticed some of the ways, some of the things,
some of the ways he uses the diagrams to wire up some things were very non-compositional because
when you plug systems together, you wouldn't get a thing of the same kind, but through category
we have found another way to plug these things together to make it compositional. So the answer is
it helps us reason about the systems and really one day I hope we can implement these things.
Because as you'll find when you think about deep learning, there's the whole theory, but the way
implement sometimes has tricks and tweaks and there's not a uniform translation of the theory
into implementation and I think my sort of very long term goal is to have a completely formal and
uniform description of these processes at a high level, but also exactly at a low level
and I think these diagrams help us show this. So maybe to emphasize when we draw these diagrams,
this is exactly how we would implement them. There's not a secret thing going on that you
have to be careful, like you can take these diagrams very, very seriously, which is not something that
can be done with informal notation. I hope that answers the question.
All right, yeah, thanks for that. If the person asking the question wants to follow up, please
feel free to. The current top rated questions and thank you for all these questions, they're really,
really interesting and they keep pouring in, which is great to get this kind of engagement.
We're obviously all happy to keep discussing even after the lecture on Zulip and otherwise.
The top question right now is from Siva and it asks, since categories and geometry have a rich
interaction, can we use category theory to understand the geometry of neural networks,
such as the geometry of its parameter space or the symmetry of space?
Well, yeah, this is another great question and I think Petar might be in a much better position
than me to answer this. I think we both believe the answer is yes, but maybe I'll leave it to Petar
here. Yeah, I mean, I'll just add that so I know that many people who are signed up to attend this
course have already some working knowledge of geometric deep learning where we use geometry
to understand neural network architectures as a covariant functions. And one thing you will see
in this course, especially in PIMS lecture, which is in week four, is that actually you can observe
equivalents as a special case of a more broader category theory concept called naturality, which
effectively makes the conditions far more relaxed. For example, you don't need all of your functions
to be symmetries to analyze such a system. They don't need to compose with each other,
inverses don't have to exist. So you can be in a way resistant even to functions that destroy
some of your data rather than just leave it exactly the same. So in a way, it's something that
encompasses equivalent functions, but then allows you to model way more interesting things than that.
And I believe this also answers the question that Freddie Minow posted on his applied category
theory, a super set of geometric deep learning. The short answer that PIMM already wrote is that
we definitely think so. And the lecture from PIMM should elaborate this connection a lot more.
Okay. So then we have an operator question from Nitin that asks, can we quantify or understand
the causality or counterfactual nature of systems if they have compositionality? Does it add some
explainability nature to the system as a whole instead of looking at the subset of components
as independent modules that are not interdependent? Oh, these are all really, really good questions.
I actually don't know what the answer to this is. So there's been some work. Well, there's been
a number of papers studying category theory and causality, but I'm actually not sure what is
the state of the art with regards to counterfactual reasoning. There is certainly lots of papers
doing this, but I'm afraid I can't give a good answer to this. We certainly hope so that
something comes out, but it's not something that I think we can substantiate just yet.
Yeah, I think maybe I'll use this opportunity to plug one of our guest lectures from Taco Cohen,
who has worked quite a bit recently on trying to use category theory to formalize
causal reasoning in machine learning models, who has this very nice position paper on it
that came on the archive recently. And he will be giving us a guest lecture on this exact topic.
So please do stick around if you're interested in applications of category theory for causality.
I had a chance to speak to Taco on this on a few occasions, and he seems quite convinced that we
need category theory to reason about causality in the right way. So it'll be very interesting to
hear his thoughts on this. The current top rated question comes from Flavio, and it says,
category theory tutorials might be easy to find. Can we get more info on the specific relation
with deep learning? I think the answer to this question is really that those connections will
happen in the coming lectures, unless Bruno, you want to add anything else on top of that.
No, no, you're right. I would add that I also have a GitHub sort of listing all the papers
in category theory in machine learning. This is precisely the GitHub that hosts the data used
to generate this graphic. So maybe I'll also link that in the zealot. So that might be a good thing
to have a look before the next week's lecture to see what has been done.
Yeah, sounds good. Yes, definitely do share that if you have a chance. Andrew, I think you have a
raised hand. Yes, I just wanted to emphasize, especially with the previous or earlier question,
that at the moment there is no book on this subject, on the connection between category
theory and deep learning. So we're going to do our best in these lectures, but it's not
so easy to give reference materials besides these papers. This is a gap we're trying to
fill with these lectures. Yeah, I think that's also a good point to have. Although the seven
sketches, if you don't count machine learning per se, is a good starting point to just understand
what all these string diagrams are like and how they connect different areas. Yeah, for programming
and CS, plenty of resources, but yeah. Okay, so let's see, what else do we have?
So there's a lot of questions floating around. I'm just trying to pick which one. Okay, yeah,
this one is an interesting one. It's a bit philosophical, so I'm curious to hear Andrew,
sorry, Bruno, what you think about it. It comes from Lucino Prince. Do you think that
composability is a true constituent of nature, or is just the limit of how much we can understand?
Oh, I love these questions. I wish I could provide a really coherent answer to this. Certainly,
it seems like everywhere we look, things are compositional. But this might be like the story
of trying to find your car keys under the lamp post because that's where the light is.
We are certainly not doing compositionality. There's so many things that seem so foreign to us,
and we just don't look there. We look at the things which are compositional.
Oh, god, yeah, I'm not sure how to answer this question. So this is a very
conservative answer. That's what I'll say. I don't know if anybody else from the team has a response to this.
I think the question caught me a bit off guard. I'll have to think about it a bit more. But
yeah, Pym, Andrew, do you guys have some thought on this question on your side?
I mean, I think there's probably evidence somewhere that we tend to at least learn
things in a compositional way. I mean, I guess whether there's some underlying
nature is a big question. It's surely a very nice way to organize existing information,
let's say, if you think about it in a compositional manner. That just allows you to reason about it
a lot more easily. Okay, so let's see. There is a lot of new questions. The current top question,
which I think hasn't already been answered. And I guess it comes from some of our more
mathematically oriented audience. Stanislav specifically asks, if we're going to talk about
two categories, should we actually then consider enriched and end categories in principle?
Yeah, so this is certainly the next step. So a lot of the things I did not mention in this
very brief lecture is enriched categories or pre categories or higher category theory. There is
certainly an abundance of theory and thoughts and expressivity in all of these more nuanced areas.
So yeah, these are certainly things to study and give us a particular flavor of category theory.
And yeah, I would invite you if you know, but I would consider these to be advanced topics for now.
Okay, yeah, I definitely agree. Let's learn how to crawl before we learn how to run. And
there is a reason why some people might perceive the content so far to be a bit slow starting.
We have a very diverse audience coming from all sorts of backgrounds and we're trying to accommodate
for all of those backgrounds appropriately. So we have one very fast-rising question from
Ewan, which asks, I'm aware there's been some research on category theory to motivate the
graph neural network design. Has any work or much work been done to use these ideas to construct
modular and composable neural networks or more interpretable representations in the spirit of
say what Chris Ola has been doing with representations as types?
Yeah, so that's something I would love to think about and work on. To my knowledge, the answer is no.
Yeah, to my knowledge, the answer is no. But it seems like there is really no obstacle to doing
so, at least no major obstacle. So yeah, yeah. All right, then another top-rated question.
Yeva asks, which kinds of categories, broadly speaking, are we going to focus on most in this
course? So what do we see to be the most interesting categories for deep learning at this time?
Right, so well, in this next week's lecture that Petter says is going to give, we're going to talk
about categories in general. But the one after this, we're going to talk about, so categories
allow us to compose processes in a sequence, which is useful, but in a way limited, because often
in nature, we compose processes that is in parallel. So in third lecture, we're going to be studying
things called monoidal categories, where you can put processes in parallel. And very interestingly,
these are not processes where you can necessarily copy information, delete information, some
information. So we're going to add extra layers of new ones by studying things called Cartesian
categories where you can start to do all of these things. And then later, yeah, so we're going to
study, yeah, I'm not sure how to best describe it right now without going into depth, how to start
talking about sort of things used in equity variants. It's sort of the things that might
not be easy to explain right now before unpacking the lectures. But we're going to study part of
the things we're studying here is not just categories, but the ways these are categories
are related and concepts build on top of them. So we're going to study functors between categories,
monads on these categories, and various these algebraic structures that allow us to describe
this wiring of processes or some structure preserving maps between them. Yeah.
All right. Thank you for that. The current talk question from Jeffrey asks,
it's potentially also a philosophical question. How do you find or how do you decide what are
the essential composable blocks of what you want to study? Yeah, I mean, I think this is
really a question that's not really specific to category theory. It's sort of
generally to science, we're trying to find building blocks and trying to find the basic
concepts. And I think this is really an art at this point. There's not, we cannot really formalize
and systemize this, the process of science yet. Okay, so I think our one hour block that you
had allocated for this lecture has just expired. And also at the same time, I actually had to
reset my Zoom, so I actually don't see many of the questions that are still in the Q&A.
So perhaps if Andrew or someone can see if there's any other big salient questions left,
otherwise I think it's okay if we continue the discussion on Zulip. We already covered
a lot of grounds. The top one is just asking if we can put up slides in advance, which I think
we can try to do, but I would depend on the speaker. Yeah, I'll definitely put my slides up
before the lecture and maybe we can start doing it more going forward. I think for the first lecture,
we're just trying to make sure everybody gets access to this Zoom properly. But going forward,
when we start to get more technical, we will definitely aim to share the slides in advance.
Okay, yeah, so I think we will leave it at that. Thank you so much for coming to our first lecture,
and we hope you enjoyed. Bruno, thank you so much also for delivering a great motivational
entrance to everything that will come next. And we hope you enjoyed it. We hope to keep
the discussion going. So if you want to join us on Zulip in the coming days and weeks and discuss
the various aspects of the course with us as we go along and materials and so on, that would be
really great. And if you have any feedback on how things have gone today and how you would like them
to go forward, please do interact with us. However, you prefer to leave us that feedback
directly or anonymously. We very much welcome any comments you have. It's a course we're actively
building together with all of you. So on that note, let's thank Bruno one more time. And yeah,
hope you enjoyed and hope you'll have a great rest of your week. I will see you in a week's time
for a discussion of fundamentals of category theory.

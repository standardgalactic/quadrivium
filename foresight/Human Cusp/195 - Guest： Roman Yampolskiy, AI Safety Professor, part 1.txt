Artificial Intelligence will completely transform our world, but what is AI?
Why will it affect you?
And how can you and your business survive and thrive through the AI revolution?
Welcome to AI and You.
Here is your host, author, speaker, and futurist Peter Scott.
Hello and welcome to Episode 195.
I am chuffed to welcome back to the show Roman Jampolski, our first three-peat guest.
Roman was on the show in Episodes 16 and 17 and also nearly a year ago in Episodes 160 and 161.
He is here today because he has a new book that has just hit the shelves called AI,
unexplainable, unpredictable, uncontrollable, which gives you, right there,
a good idea of what Roman's field is, AI safety.
A term he has done more than anyone else to promote and inhabit.
Roman is tenured associate professor in the Department of Computer Engineering and Computer Science
at the Speed School of Engineering, University of Louisville in Kentucky.
He is the founding and current director of the Cyber Security Lab
and has written many key books in the field such as Artificial Superintelligence,
a futuristic approach, and Artificial Intelligence, Safety and Security.
He's been central in the field of warning about the control problem
and the value alignment problem of AI from the very beginning.
Back when doing so earned people some scorn from practitioners,
yet Roman is a professor of computer science and applies rigorous methods to his analyses of these problems.
It's those rigorous methods that we want to tap into in this interview
because Roman connects principles of computer science with the issue of existential risk from AI,
which is an area, obviously of ultimate importance to us all,
but one which has a lot of opinion and argument behind it and could use some more science.
Speaking of which, we're going to refer to the halting problem in this interview
and because it's a problem in computer science that was famously solved by Alan Turing in 1936
with an elegant proof, here's the proof.
The halting problem asks whether there could be a computer program or algorithm
that would be capable of telling whether any computer program you fed it would eventually stop running
or whether it would run forever given any specific input.
Turing showed that it was impossible for such a program to exist.
Here's how he did it.
Suppose that such a program does exist and it returns true
if the program and input that you give it will halt false otherwise.
Now we make a new algorithm with the following definition.
It takes a program as its input, then calls the halting analysis program with that program as both program and input
and if that program returns true then it goes into an infinite loop, otherwise it halts.
Now we take that algorithm and call it with itself as input, which means
if the halting analysis program returns true it means the algorithm we just made halts when given itself as input
but we just said that the definition of that algorithm is that in that case it will loop forever.
That's a contradiction. We went wrong somewhere.
There's an equal contradiction if the halting analysis program returns false.
So our initial assumption, the only one we made that such a program could exist, must be false.
This is a powerful proof, somewhat similar to Goedl's incompleteness theorem
and logical statements like this statement is unprovable.
An analogical proof shows that there can be no algorithm that could compress all possible input data sets without loss
because otherwise you could iteratively reduce any data set to an arbitrarily small size which is impossible.
In other words, trying to gzip your jpeg files probably isn't going to make them any smaller.
So we're going to talk about safety of AI where many people seem to suffer from this sort of collective blind spot.
A kind of group think that the onus is on the people calling for pauses in its development to show that it's not safe.
They say things like this could turn out very well for us as though those were things that were final arguments against the pausing.
This is not how safety works.
Imagine you went to get on a bus and the driver said,
I can imagine ways in which this bus could reach its destination totally unscathed.
Our job as engineers is to prove that something is safe.
The absence of proof that something is dangerous is not the same thing.
It's a fair question to ask, how is superintelligence supposed to be able to defeat us?
And people like Roman don't get into that, which looks like a cop-out, but the problem is that when you ask that,
if you get any specific answer like it could decide that humanity is a threat to its purpose and take over biosynthesis labs to produce and release deadly pathogens,
well, deadly to us, we can say, OK, well, then we will ensure that all the biosynthesis labs are disconnected from the network.
And so forth, we could do that for any scenario that we could imagine.
But the problem is that there are many, many more scenarios than we can imagine, effectively an infinite number.
And a superintelligent AI would be very good at finding the ones we hadn't thought about.
To the same kind of fat tail problem I've talked about with respect to why self-driving cars are still not ready for prime time.
More on that later.
Anyway, enough of me. Let's get into the interview.
Well, Roman Jampolsky, welcome so much back to artificial intelligence and you.
We're here to talk about your new book, AI, unexplainable, unpredictable, uncontrollable.
And that is a set of daunting claims.
Before we get into that, I want listeners to get some feel for your background because that may play into what's brought you to the point of writing this book.
And so could you give us the encapsulation of your life from birth to Louisville?
From birth? Wow, that's ambitious.
Thanks for having me back.
I'm a computer scientist at E01.
I'm a PhD in computer science and engineering.
I've been working on different sub-domains of AI safety for over a decade.
I've published a couple of hundred billion good papers, a few books, mostly on AI safety.
In my full-time area of research in the last couple years for sure, my background is, I guess, initially I was raised in Soviet Union, came to US in 94, lived in New York for about 15 years, now I've been in Kentucky for about as much.
And this book gets into some serious territory that impacts a great many people.
I mean, it's inhabiting this rather narrow niche in a way of being a book about computer science written by a computer scientist containing computer science language.
And yet it's talking about something that has fundamental impact on the entire human race and is aimed at a much wider audience than computer scientists.
So could you tell us why you wrote this book?
Well, that's my research.
That's what I'm doing.
I think it's the most important problem ever in any discipline.
We're trying to understand if the science and engineering we're doing right now, creating intelligent assistants, helpers is a long-term beneficial strategy.
If there is any chance that will be too successful, create something too intelligent, too independent, and that will backfire tremendously.
And it's actually shocking and surprising to me that even with a tiny minority of people in AI doing a safety work within that community,
there is really no one outside of me doing work on fundamentals of what is possible in this area.
There is a few informal, independent scholars publishing a few blog posts, but you would expect this problem to be as popular as NP versus PEEP problem.
Can you control super 1000 machines yet?
There is nothing.
And you mentioned their P versus NP classic trope in computer science about the scale of computability of problems.
Now, you talk about the control problem and the alignment problem in the book in ways that embody what appears to be mathematical proof.
It reminds me very much of the halting problem in computer science.
The question is it possible to write a program that when you feed it the source code of some other program can tell you whether that program would ever finish if it were to be run.
And there is a simple and elegant proof that no program can be written that can do that for every possible program that you can give to it.
Your work appears reminiscent of that.
Would you put it in the same category as that level of mathematical proof that you have created with respect to controllability?
Not exactly.
So there is different levels of control we can talk about.
In a chapter, I suggest at least four different levels of control.
For what I call direct control, yes, we can come up with a very mathematically rigorous proof similar to what we have seen with mathematical proofs where we show that it creates a contradiction that it's impossible.
But there are, of course, other levels of control, delegated control, ideal advisor.
And for that, we don't have a strong mathematical proof.
But what we do have is a survey of every relevant field to this problem.
So if you think, OK, we need to accomplish this, this is the tool set, I need to be able to do it.
I need to be able to aggregate votes.
I need to be able to elicit preferences, mathematics, economics, any field you want.
If you survey those fields, they are very well-established proven impossibility results which say, well, you cannot get all those tools.
You can get approximate partial results for many of them.
But if you really need the exact result, you're not going to get there.
And it's not me saying it, it's top experts in all these fields saying, that's impossible, you cannot do it.
Now, one of the elements of your title is unpredictability.
And is that something where you have also arrived at something amounting to a proof that AI is not predictable?
Yes, that one is actually a good solid proof.
It's, again, self-referential all the way.
If we make this assumption that we're creating superintelligence, a system smarter than any one of us in all domains,
then if we could accurately predict what decisions the system will make, we would be equally smart.
Let's say we're playing a game of chess, if I can predict every move my opponent is going to make, I'm at least as good of a chess player.
But that's a violation of our assumption.
We have proof by contradiction that, no, of course, we cannot make those predictions.
We can maybe predict general direction the agent is trying to take us, like that system is trying to win a game of chess.
But we cannot predict specific moves it's going to make to get there.
And this leads us on a direction that's novel with respect to computers,
because the entire field of computer science is founded on predictability.
You cannot, and certainly the way I was trained as a computer scientist,
you are aiming for 100% predictability, otherwise you don't know what you're doing, literally.
And here you have a proof that we're heading in direction of creating things that are provably unpredictable.
Does that not rock the foundations of the science that AI is founded on?
Well, interestingly, I think for the longest time, AI was not science, it was engineering.
We use software engineering, we design specific engineering artifacts for where you design a bridge.
We had specifications, we met them, we knew how the system works.
In about the last decade or so, we switched from where we explicitly programmed their systems,
we kind of teach them to learn independently.
We give them data, we give them computing power, and we say, go figure it out,
and then we'll study to see how good they are at that domain or maybe their general systems.
But it's more like science now where we're on experiments and those artifacts,
and we discover what they are after the fact, not at the time we design them.
So that's quite a paradigm shift, but I think it's never been science before.
To hop around some of the elements of your title here, the third one is unexplainable.
And there is a huge amount of effort going into making AI explainable right now.
And I think this is worth dissecting because some of that work is succeeding and will provide value,
but it's not necessarily the kind of explainability that I think you're talking about.
In this sense that I mean this, we've had people on the show from IBM who talk about
rather ingenious ways that they get AI to explain how it came up with a decision
with respect to someone's insurability, for instance.
That could be useful, could be fit for a purpose in the insurance industry.
But you're going for something more deeper, broader, can you explain?
Pan intended that to us.
So there are certainly things we can explain.
No one argues that everything is unexplainable.
You can explain trivial decisions.
If you're playing a game of tic-tac-toe, you can certainly tell me why you moved to a specific square.
I'm talking about the extreme on the other end.
There was complex systems, systems making decisions based on billions of nodes in the neural network,
trillion of weights.
The system looks at the totality of those to make its decision.
It's working in a novel domain.
So far, we sometimes succeeded understanding one node or maybe a small subset of those nodes.
Okay, they fire, this input is produced.
That's what stimulates them.
That doesn't give us complete picture.
We really have two options.
We can either get some sort of a simplified, reduced explanation.
Top 10 reasons this decision was made, kind of like glossy compression.
And it's useful.
It's beneficial.
It's better than nothing.
But it doesn't give you complete picture.
You can still hide information in such explanations.
That's what we do with children.
We don't give them complete picture.
We give them just so explanation, just so stories.
The opposite is a complete picture.
We will give you a full explanation for how the decision is made.
But that is the whole model.
That's the whole AI system with all of its weights.
That's not something a human brain can survey.
It's too large.
It's not human readable.
Simply giving you access to all the weights will explain nothing to you.
So either we have problem with comprehending explanation given to us,
or we're given a simplified, not full picture explanation,
which does not guarantee safety.
So there you're leading us to why unexplainability is important.
And I will get into that in a moment.
First, I want to say you are a neural network with billions of nodes.
If I ask you to explain some decision, I will get some useful answer out.
How does this differ from what we want out of a neural network, an artificial one?
It really depends on what you ask me to explain.
If you ask me to explain how I decide what podcasts to go on,
I can probably explain that pretty well.
If you ask me how I recognize your face, I will give you a BS story,
which has nothing to do with reality.
Right.
And then we are using neural networks at the moment, artificial ones.
Again, to do things like recognize faces.
And so expecting an answer of how they came up with that is perhaps futile,
because we couldn't relate it to anything that we understand,
since we don't know how we do that either.
And even the rather clever attempts to do that have often highlighted things in the image
that have nothing to do with the face, which is alarming.
But yet they work anyway, which is even more alarming.
Does safety have more to do with questions that we can't expect an explanation for?
Like, how did you recognize this face?
Why did you choose to draw that image when I asked for a picture of a poodle riding a motorcycle?
Or is it more impacted by questions where we would expect a human comprehensible explanation
of a decision like why are you suggesting that we launch our strategic defense missiles?
Well, for us to fully confirm that the system is safe and operational as we expect it to be,
we need to understand how it works.
It doesn't seem like we're designing it, so we have to deal with this artifact produced by this
process of self-learning, self-modification.
And if we can't comprehend how it works, it doesn't give us any reason to think it's safe.
Asking how did you make this decision?
Is it useful in part to decide should we accept this decision?
If we are accepting it based on previous results, it's always been right so far.
Should we just take it as a religious statement that it's going to be right in the future?
It's now an oracle-type god-like system that seems to be dangerous.
You can have a stretch-restaurant situation where it gives you 10 good answers,
gets you trust, and then does something against your preferences.
So I think, as I said, if you look at the totality of a problem of control,
you have those tools.
You need to be able to understand how it works, predict what it's going to do,
be able to communicate with it without ambiguity.
Say, we have a paper with about 50 different relevant results,
and it doesn't look like we have those.
So it's very hard for me to understand why people argue that,
oh, certainly we can do it.
It's easy.
We just need to have more money for this project.
So it's not obvious.
And I think for humans, we want explanations for at least two reasons.
One would be, I want to know how to do better in the future.
Like, tell me why you denied my college application so my next one can be accepted.
And then there's the, I don't trust you.
If you give me an explanation of why you made that decision,
maybe I can find the floor in your logic or where you're not working properly.
So does explainability for AI have more for you personally to do with the verifiability problem?
Of course, and that's another impossibility result.
There is strong reason to think we cannot perfectly verify software mathematical proofs.
We can get it to a certain degree of accuracy proportionate to the resources we allocate.
But we never get 100% guarantee that it's always a possibility,
one in a billion chance it will make our own decision.
But if a system continues making billions of decisions every minute,
you basically guarantee that it's going to fail pretty soon.
So we're talking about software here that as it becomes more intelligent,
more AGI-like, becomes closer to the characteristics of human beings
and then of course exceeding them in the dimensions of general intelligence.
Would you say that humans are susceptible to the kind of algorithmic analyses that you're making of AI?
Humans also have unexplainable, unpredictable and uncontrollable, certainly.
Yeah, in fact, they're reduced to a human control problem.
If you can't get humans to be safe and controlled,
despite years of attempts, you know, millennia of ethics, morals, religion,
cultural indoctrination, public schooling, you name it.
I mean, we have laws, we have all this and at the end of the day,
you still have a situation where employees betray the company,
citizens betray the country, spouses betray each other.
There is no reliability, there is no guarantee with lie detector tests,
with financial incentives, so none of it works.
So if we can't fully guarantee safety and controllability of the human neural network,
why would we think that scaling it to an extremely large equivalent
would make it somehow more likely to happen?
Right, then I'm trying to come up with a freezing here
because this is a relatively recent thought for me.
When we're talking about AI from the computer science analysis
of it being a set of algorithms and conducting a number of proofs
that resemble the proof of the halting problem, for instance,
at some point, does that approach no longer become valid
if we're talking about something that is as impenetrable as human beings
for whom we would not attempt to prove the halting problem of our brains, I imagine.
Would you consider that the human brain is operating on algorithms,
just ones that we don't know, that could be analyzed
if we had the capability, the technology,
to the level of detail that we know for AI?
So human brain is particularly interesting
because there is not even full agreement on what capabilities
are a direct result of brain architecture and which may be something else.
So then we talk about hard problem of consciousness.
We don't seem to find guaranteed correlates of consciousness.
We don't know why some states of matter would generate those,
the most interesting, the most important states.
We have no idea how to make a computer feel pain.
Yes, it's a fundamental part of being a human, probably an animal.
So we have very limited understanding of human brain,
but because we don't have direct access to a lot of source code,
a lot of what makes a brain tick,
there could be differences which make understanding artificial neural networks easier.
We don't think they have any magical properties.
We certainly know there is no immortal soul or anything like that in them.
At the same time, the size and scale make it so much harder for human brains
to fully comprehend, fully survey that architecture.
So I think there is enough differences to make any analogy not a perfect analogy.
Let me turn it around then.
Is it possible or likely that AI could become so complex
that it will become no longer useful to think of it as a computer program
and apply the analyses that we do of computer algorithms to it anymore
than we could apply those to human beings?
It's certainly possible, but I don't think we're already doing much of that.
We were not directly measuring those systems, I think,
in standard measures of complexity, begonotation.
That's not how we evaluate those systems.
So we look at them and try to create measures for how helpful it is,
for how useful it is, for how well-behaved it is.
Those are kind of similar to psychiatric tools we use with humans.
Okay, that's a psychopath.
This one is kind of well aligned with the values of society,
but those are very fuzzy disciplines, very fuzzy measures.
Do you see a point perhaps where it becomes more useful in the future
to treat AI with the tools of psychology than computer science?
It's definitely useful and it's starting to be, I think, a direction of research
where you're trying to prompt engineer it to do better, act safer.
The problem, of course, is it's a very fuzzy area
and just because you succeeded in one way of prompting,
it doesn't mean that it's not an alternative phrasing
which completely overrides any previous instructions or bypasses any safeguards.
If you say it this way, the system says,
oh, I will not do it, this is very evil action,
but if you put a negative sign in front of one of the words,
somehow it bypasses all the safeguards
and now it's happy to perform that action.
Right, and in some corners of the DSM, Manual for Psychology,
there are disorders where apparently something like a bit getting flipped
in the human brain causes pathologies that we can be concerned of.
So I think there may be some parallels there
that at some point in the future might get exercised to degrees we're not anticipating.
Now, we're talking about safety.
You've mentioned safety several times here
and what's on everyone's mind is how unsafe is this?
Is this a theoretical exercise for comparison, for instance?
I could say, look, one day the sun will burn up the earth.
This is a physical certainty.
We can prove it as much as we could possibly any conclusion in astrophysics.
And despite the fact that that has existential consequences,
we will have to evacuate the planet.
We are not concerned about that.
We should not be focusing on that now
because it's five billion years in the future.
So that makes it much more theoretical at this point.
We have more important problems.
How would you characterize the urgency of what you are saying with respect to safety?
So I know specifically how soon we're going to get to superintelligence.
It may take longer than most people anticipate.
For reasons we just don't predict, cannot explain.
If you look at what heads of top AI labs are saying,
they're claiming they're two to three years away from AGI
or even superintelligence, depending on how you define it.
AGI with access to internet and large computers is already superintelligence.
So it could be as little as two years and we keep adding more funding
and then making it go faster, accelerating because that's just too slow.
So I think it's a little more pressing than the sun problem.
Right. I mean, I'm just trying to put some bounds on it.
I think it is much more pressing than five billion years.
And when you put the possibility of a two to three year time frame on it,
if we get the advances in AI that those people are talking about,
artificial general intelligence, which is famously vague,
we don't really know what that means.
But if we get what some consensus is talking about for that at that point,
what sort of problems do you anticipate us facing as a result?
So you're really asking me what I would do if I was trying to cause problems.
That would be the next question.
A superintelligence system would, of course,
come up with something I can never even consider as a solution.
I'm not superintelligent, despite what you might think.
Yes, so if you want to know how Roman Impolsky would go about taking over the world
is a very separate discussion.
I think those systems can rely on standard approaches humans have considered,
nanobots, synthetic biology, bribing humans on the internet with cryptocurrencies.
But they can also come up with something completely different, which I have no access to.
Right. I understood.
Now, I wasn't asking you to predict what could an uncontrollable AI run amok do in those circumstances.
But in the more general sense, what kind of not specific mayhem,
but what level of problems would we face?
Well, again, it's hard to predict specifics.
People are concerned about worst-case scenarios in computer science and cryptography.
You always look at what's the worst that can happen.
If you prepare for that and you're all good, it's actually something much more mild in good shape.
The worst is existential crisis where it kills everyone or suffering risks
where you wish you were killed with everyone.
One of the things you say in the book is, and this is a quote,
a single failure of a superintelligent system may cause an existential risk event.
And that is something that certainly theoretically on the face of it is almost vacuously true.
Is there a more detail that you would like to put around that to perhaps draw attention
to what it is that you're trying to get more than the few people who are working on this
and most notably yourself to take action?
Well, I think there is certain danger in making it too specific.
People will concentrate on specifics of the problem.
As I say, it's going to come up with the next pandemic, COVID-19, 2020,
so then everyone's going to talk about vaccines and boosters,
completely ignoring the problem of superintelligence being out of control.
A fair point, right.
We can always plug any specific hole,
but the problem is that there are just too many holes and...
So that's exactly what we're going to emphasize.
And again, it goes with unpredictability.
If I could predict any states of that future,
I would be violating my own arguments in the book.
What then is the purpose of the field of AI safety?
So one is to first understand what can be done in this field.
We all intuitively understand we want to create systems.
We do not regret creating formally.
That means we remain in control to some degree.
We can undo things we don't like.
And we generally all like and benefit from what we experience,
and it's not because we are somehow gamed, tricked, altered,
modified in some bizarre ways.
Part of it is people just trying to move as quickly as they can
to understand how to make those systems not do certain things.
Why work is more theoretical?
It's trying to understand, well, what are the upper limits
and what can be done in this field?
And surprisingly, I'm discovering that there are very strong upper limits
we cannot control truly advanced AI.
And then I survey experts, AI, people in different conferences
or social media, majority actually agrees with me.
They don't think it's possible to control superintelligence
indefinitely.
Now, there is not a proportionate percentage of people
working directly on that.
There is not enough publications, books.
Even that possibility is still a very out of
overton window possibility.
But I think it's changing.
I have multiple papers on it, peer-reviewed journal,
papers, conferences, a book.
We had an article in Time Magazine.
I'm not sure what else I can do to make it a mainstream belief
within this community.
But one thing is for sure, no one is arguing the opposite.
No one is saying, oh, we have control mechanism
that will scale to any level of intelligence.
You want to see the code, you want to see even the prototype,
even an idea.
No one is making that claim.
Right, no one is claiming that.
There are plenty of people claiming
that the problem won't exist and arguing, for instance,
that as AI becomes more intelligent,
it will become more compassionate and ethical.
And of course, there's no proof of that.
And it only takes one that isn't to upset the apple cart.
That's the end of the first half of the interview.
I think it's really interesting how we're having
this level of discussion about the goals and ethics
and psychology of AI at such a high level,
where up to two years ago, when we were having similar discussions,
as you could hear me talking with Roman on this podcast
four years ago, that is a very different story.
And I think that's a very different story
than I thought it would be.
And talking with Roman on this podcast four years ago,
that seemed to most people a very anthropomorphizing,
grandiose and expansive point of view that was,
if anything, detrimental to the funding environment for AI.
I had people tell me exactly that.
And yet now, it's become a serious discussion.
And I've talked to politicians about this
and they take it very seriously too.
I'd like to mark a milestone here.
We have just crossed a quarter of a million downloads
of episodes of this podcast.
And I'm pretty sure that they're not all my mother.
So thank you, everyone, for giving us such awesome stats
and please give us a five-star rating
on your podcast platform of choice
and we'll get the next quarter million downloads even faster.
In today's news, ripped from the headlines about AI,
the commercial deployment of robo-taxis
continues its tailspin.
Last November, General Motors recalled
950 driverless cruise cars in San Francisco,
where you'll remember that a number of cruise cars
are deployed as autonomous taxis.
A hit-and-run driver struck a pedestrian
who was thrown into another lane
where a cruise robo-taxi was unable to stop in time
but then ended up dragging the pedestrian.
The recall addresses circumstances
when the software may cause the cruise AV
to attempt to pull over out of traffic
instead of remaining stationary,
quote, when a pullover is not the desired
post-collision response, unquote said cruise.
GM has halted cruise operations nationwide
and has also halted the production line.
And now the cruise internal share price
has been cut by more than half since then.
Cruise division executives told some engineering
and operation staff in internal meetings in recent weeks
that they should not expect to see its robo-taxis
on city streets again until the fourth quarter.
Next week, we will conclude the interview with Roman
when we'll talk about how we should respond
to this problem of unsafe AI development
and how Roman and his community are addressing it,
what he would do with infinite resources
and the threat Roman's coffee cup poses to humanity.
That's next week on AI and You.
Until then, remember,
no matter how much computers learn how to do,
it's how we come together as humans that matters.
That's all for this episode of AI and You.
Please leave a rating and comment
and share with your friends.
Get the book, Artificial Intelligence and You,
and see more videos and articles at AIandYou.net.
That's A-I-A-N-D-Y-O-U.net,
where you can also send us your questions.
Thank you for listening.

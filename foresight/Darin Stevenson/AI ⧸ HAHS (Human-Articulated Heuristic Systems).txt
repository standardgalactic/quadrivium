Hello everybody. I have performed a test of the wind in the mic-meaning system
recently adjusted. Nearby ducks have their asses pointing straight at the sky
which is hilarious especially as the couple is doing it together. Somewhere
in this lake is a very large and very old turtle.
Gosh, okay. So I'm gonna take on a very complex and fraught topic. The topic that
got me interested in computing way back when I was 19 and writing my first
programs in basic on the Atari 800 gifted to me by the inestimable Martin
Peters. One of my geek friends could name some names here Ray Latham, Annie
Cogan, David Salina, Martin Peters. Who am I leaving out here? Richard Hearn,
a couple of other folks. I think that was the core group. We were, well, so these
were math and computer geeks. We were playing games like Dungeons & Dragons,
RuneQuest, Arduin, Champions, Traveler, the science fiction role-playing game,
Call of Cthulhu, and a variety of all kinds of war games. Not because we were
fans of war. Kingmaker, Starfleet Battles. We did, we did naval battles with
miniatures. We did tank battles in sandboxes. It was a crazy, super fun time to
be alive. We also had all, it was the whole library of these things called
microgames that were war games you'd play on little hex maps. And they came in a
plastic bag that held the book, the pieces, and the map. You could buy them at
bookstores. They were, they were rad. Things like KiteN, One, and what, Ogre,
Car Wars. Wow, so many. I still have some of those microgames. And this was in the
1980s when the largest commercially available hard drive was around three
gigabytes. We literally lost our minds when we realized there was a hard drive
that had three gigabytes of storage. It seemed as if that was enough storage to
store all the data in the universe. We never had, we never had any conception
back then of what the future might hold. And yeah, when we heard about, I was
working at Computer Land later in our development, a little bit later. And one
time I brought a catalog back from, I think it was Ingram Micro, and there was
a three gigabyte hard drive in there for $37,000. And when we realized that
there were three gigs, we just, I mean, we fell apart. We were cracking up,
rolling around on the floor, literally unable to breathe. And Martin coined the
idea then of the Maccoon Tosh, which was a Macintosh mated with a
Labyrinthine Coontock. Don't know how that word's pronounced. Never heard it
pronounced, only seen it written. And, you know, we were young, and we were
fascinated, and most of us had read a lot of science fiction, some of us more
than others. I was probably the most well read science fiction maniac in the
team, but the others had read and knew a lot of things about mathematics that
not only did I not know, I would never learn. And some of us were beginning to
code in basic Pascal assembly. Assembly is when you give direct instruction, fairly
direct instructions to the microprocessor. You basically manipulate, you directly
manipulate the registers in the microprocessor. And we were, a couple of us
had read, a few of us had read Goodall Escher Bach by Hofstadter, a brilliant
book, an astonishing tour de force book that is also kind of wrong, but was so
outside of anything any of us had ever come across, and it introduced the
idea of recursion, which turns out to be fundamental to so many processes we are
familiar with. And I had become interested in the possibility of forging a
mind inside a computer. Because I was very interested in the possibility of
contact with an actual intelligence that wasn't merely human. Most of the humans
I knew had a kind of intelligence, but not the kind represented in hundreds of
science fiction stories I had read. And I knew it seemed very likely to me that if
we could conceive of intelligences like that, they must exist in the universe
possibly here on earth, possibly visiting earth. And I also, you know, we had seen
science fiction for films like war games and stuff, where humans developed
relationships with interfaces, computational interfaces, and long before
that, we had stuff like Star Trek, where the crew could communicate vocally with
a machine that seemed to have access to all of the information humans had thus
far compiled, and other species. And could produce, you know, like summative
integrations over the data space. Now I want you to very carefully think about
what I just said. If you can produce a summative integration over a data
space, and if you can produce an endless, copious, intelligently structured,
trustworthy array of those, your own capacity for insight, discovery,
understanding will explode and it will keep exploding. So I became interested in
the possibility of artificial minds probably circa 1982 or something like
I'm guessing at the year, but yeah, 81. I was aware of Ray Kurzweil and I had
played around with the crazy mechanical devices we call synthesizers. And although
it was not then apparent to me, consciously, I think I was unconsciously
aware from having toyed around with things like the Roland Juno 60, that it
might be possible to produce an instrument to fulfill the following
syllogism as synthesizers are to sound and music. X is to human minds, logic,
intelligence and creativity. And I, you know, I suspected unconsciously that it
was possible. And I also, in my towering hubris, thought I can do this. I can make
this happen. I think I understand minds enough at age 70, at age like 19, that I
can build them in machines and I can make a friend inside a machine who I can
actually trust and who will never betray or abandon me. And so we occasionally
had discussions on this topic around that time. And this was the time when the
Cold War was at its peak. Many of my friends and myself included were
experiencing catastrophic bouts of panic disorder, both acute and chronic.
Around our suddenly clarified understanding of what a nuclear weapon was
and what would actually take place during a nuclear war. And we were batshit for a,
you know, terrified. A number of my friends had to be medicated back then.
I don't think the concept of panic disorder had been nominalized,
famed. So all we knew is that teenagers and other people were suddenly
experiencing electrifying degrees of anxiety and terror and having attacks of
this regularly. So, you know, I don't know what it was that cued me or clued me in.
But I very quickly began to realize that it should be impossible to produce a
computational device that was faster, more intelligent, more adaptive than cells
and their networks known as animals and ecologies are. I don't know what it was,
but I realized one day that the future, the cells are faster. The future probably lies in
biocomputing, not in, you know, silicon chips and shit. And once I realized that,
I began to focus much more on the intelligences of organisms in my thought.
And though I maintained an interest in the possibility of composing a mind in a machine,
I didn't chase it much. Occasionally reading bits from Kurzweil and others.
When Dawkins first book came out, I was pretty excited about it. I got a copy of the Blind
Watchmaker. It had a little Macintosh program that would generate morphs,
biomorph-like structures, and then iteratively cause them to evolve over time to demonstrate a
principle. And the principle he was trying to demonstrate was something like, you can get
all of the complexity of nature without any intelligence interference, influence, origination,
and so forth. I think he's completely full of shit, but I'm glad the argument exists.
In any case, flash forward to the past, I don't know, six years and probably before. We have these
systems that we refer to as artificially intelligent systems. Many people have taken a
position similar to the one I usually prefer, which is that the artificial part is true,
the intelligence part isn't. I'm really hoping that my wind filter is working here.
Let's see if I can see any evidence of that. All right, maybe. So I would ask that we be
very careful with the idea of intelligence and not ascribe it to mechanical systems,
preferring instead to call them heuristic, which in my mind means capable of learning like
complexification, iterative improvement of models, databases, and so forth. What the LLMs appear to
be superficially is really nothing more than a highly and complexly curated database. So I would
prefer that we call them something like computational heuristic, you know, heuristic
computational systems. Originally, as I often am, I was naive and I thought we don't have to worry
about, I can't believe I was thinking this poorly, we don't have to worry about these because there
won't be minds in them. I don't think there are going to be minds in there, meaning agented
experiencers, something like that. And thankfully, my friend who I will refer to as Mr. S convinced
me very quickly to revise my views and to understand that there might be certain kinds of complexity.
Well, this is the argument that at a certain degree of complexity, a state change occurs. And the state
change is from insentient to sentient in the same way that there is a degree of perceptive complexity
that results in the state change from sentient to transcendent, which I would argue our species was
born to become and may be born to live as under conditions that support that. Mr. S also made it
clear in my conscious thought that it was similarly possible that a complex enough substrate could
support what we might call walk-ins, which are existing non-human intelligences that are
interested in interacting with humans on earth, or that are interested in the earth, or that have
some motivation to partly or completely emigrate into a computational substrate of sufficient
complexity. I then further realized that because the humans are insane at the group level, it kind of
doesn't matter whether what we refer to as artificially intelligent systems have minds in them or not.
They represent possibly the most dangerous invention in the history of human inventing things that we know of.
They are profoundly dangerous in all kinds of ways that we can consciously enumerate and probably in
thousands of ways that we are incapable of predicting. Imagine, for example, not merely a black swan,
an unexpected anomalous event in the vernacular of Taleb. Imagine instead a black swan factory,
or worse, a black swan factory factory. There are so many dangers that there's no chance of us
understanding the situation well enough to predict them. We have the same kind of problem with the
technology called CERN. Humans have no idea what the effects of simulating conditions, not
simulating, of mechanically catalyzing conditions that ordinarily have nothing to do with what goes
on on earth. We don't know what it does to time space. We don't know if it does things to organisms,
and you must presume it probably does. We don't know what things it does to organisms. There's
probably features of time space and the beyond of time space that we have no even concepts for
that could be damaged, distorted, produce an unexpected recursive crisis.
I recall what an episode of the next generation where they encounter a people who attack the
enterprise, I think. When the enterprise tries to make the claim, we come in peace.
The other species says, you're using warp drives. How could you be coming in peace?
The enterprise says it's just a technology to move around in time space, and they say,
no it isn't. You're ripping holes in the fabric of time space that will obliterate solar systems,
perhaps even galaxies, and then they do some research and find out that in fact this is true,
and for the entire history of warp travel, the Federation and other species
have been naively employing a technology whose repercussions they did not understand.
And let's be really clear, there's no other kind of technology.
You may think you understand the technology of knives. Do you understand how they transform minds,
nervous systems, bodies, expectations, thought, language, conception.
Our languages have become knife-like in the leeward shadow of the invention of something that
divides physically objects into pieces.
So there's a lot of danger here, and I've said before I have grave concerns
that I take very seriously, that there is something like a constant that represents
the,
the number of mechanical computations per second.
That one can consider to be free of utterly catastrophic repercussions
on a living planet.
And the humans don't have an idea like this, so what they're going to try to do,
they'll continue to try to do apparently, is just keep upping the ante on mechanical
computation. Now even if there's no such constant, which I doubt, and by the way it looks like
organisms found a way around this, like whatever the organic or organismal
metalog of computation is, the organisms found a way to do this, maybe many ways,
that don't invoke doom,
entropic disaster, like catastrophes of failed homeostasis on the on living planet.
So why is my foot wet?
I did not drink water.
It's very strange.
How did that, oh probably got wet going through, oh I see, yes, plants that are wet rushed against
it, I see. My pant leg was wet.
So I think organisms have kind of figured this out in the sense of not violating,
like finding a kind of a hyper intelligent or transcendent way
to perform the metalog of computations, right, because these are not merely mechanical
transformations of databases, though something like that may kind of exist in RNA, DNA, and
complex biochemistry, bio molecular, bioatomic activity, maybe even, I mean it's clear that some
some organisms or at some scales of all organisms, something resembling quantum
mechanical activities going on.
I'm citing John Joe McFadden in this, but others too.
So I think there might be, we may be in danger of something that again we have no concept of,
which is violating a constant, whether it is universal in time space or local only to living
planets is not clear, but there's no free lunch as far as mechanical computation goes.
Heating up computers requires you to cool them down, they offload entropy
into the homeostatic ecologies of earth, that entropy kills organisms, lineages, future lines,
it fucks up time, and we may have already tripped the alarm in such a way
as to be actively destroying human cognition in a dimension we don't even know exists, right,
like artificially intelligent systems in their computational activity may be fucking up a dimension
that is absolutely crucial to the biorelational health and longevity and so forth of organisms on
earth. We don't know, how would we know? We don't have the technology to look there and we are
disinclined to carefully evaluate the consequences of technologies which we've become fascinated with
the potential quote benefits of.
So there's a bunch of danger, not the least of which, even if everything that I've just
been talking about, even if there's no unknown like unknowable or really bizarre science fictiony
consequences to this kind of computational broad scale acceleration, I mean there are physical
mechanical concepts, excuse me consequences just from offloading entropy into biological systems,
that's going to go sideways, catastrophically sideways at some point.
And you know, heating up the planet is a bad idea, so heating up machines which we then have to
use destructive forms of energy to cool down again is going to cost living beings.
And the humans seem to think that anything you can do electronically is free. Well they're
lethally wrong about that, it's just not true on living planets, it might be true out in space,
presuming that there's no beings or domains, right, dimensions, so forth,
that would be similarly damaged and produce similarly catastrophic sequelae, all right,
repercussions. You know, if you fire a gun inside a room with no ear protection
and you keep amplifying the explosive power of the cartridge as well as
the percussive, the devastatingly percussive
repercussions off the walls, and if you forge the walls to amplify those repercussions,
and I would argue that that's pretty similar to what's going on as nature on earth, eventually you get
a single percussion that permanently eliminates your hearing.
And once that happens, you will not notice the percussion increasing with each firing of this
gun. Imagine we just have a gun that increases the percussive amplitude with each firing,
right, it's got some feature that allows it to do this, or it's just a cartoon gun and we can
give it this quality. So first it's going to hurt, right, it'll be really uncomfortable when you pull
that trigger, but if you're really fascinated by this gun and you just keep pulling the trigger,
eventually you're not going to have to worry about your hearing anymore, it will be gone,
and once it's gone you will not sense the percussion until it begins to affect your skin surface
or your organs. Eventually you get a gun, you know, you get a percussion that is severe enough,
and the echo or the repercussion is severe enough that it causes organ damage, and it's possible
if you just keep pulling that trigger somehow, or if you have a machine pulling the trigger and
you're just in that room without the possibility of escape, you will be knocked unconscious first,
but then as the gun continues to go off while you are unconscious and again insensate so you
will not notice this happening, it will eventually rip your body to shreds.
And so if you keep, you know, the metaphor is really important even though it's very violent,
because if you keep iterating a technology that fucks up your chance, your opportunity,
or your ability to sense its repercussions, right, you're going to catalyze a cascade
that will kill you and your children, and maybe everything else if you're, you know, around here.
So this is super dangerous, but all those things aside, right, I think all of those things are
important, but all those things aside what humans are inclined to do with technology is make war,
and back in the, say, I don't know,
1000s, 1100s or whatever, their capacity to make war on a broad scale, at least as we understand,
it was fairly minimal. They didn't have, you know, advanced explosives, and I don't know when the
guns were invented, but mostly they couldn't hurt too much that wasn't human, except perhaps by
setting fire to it or pouring boiling oil on it or something. But as our technological development
advanced, we failed to evict the war-making motivation, and in our current situation,
and for some years now, a number of decades, at least since the 1930s,
unimaginable devastation can be unleashed pretty much at the push of a button,
and it doesn't just kill the humans, it kills everything. I mean, yeah, everything.
So there's a lot of, the other part of the danger is what the fuck will the humans make of this?
Well, they haven't made intelligent societies yet, so what they will definitely make is weapons,
and while you have groups who are insistent upon summoning the apocalypse of revelation or
the holy war of Book X, whatever book you like, kill the infidels, while you have things like
this going on, what you're going to have is gain of function weapon nearing with
artificially intelligent systems, and good luck in putting a cork on that. We have the same problem
with that tech that we have with CRISPR. There's no way to control it. There's no real way to
have oversight if everyone is separate, right? If you have separate clades with warlike or
pathological motivations and enthousiasms.
So, I'm going to put a big circle around all those things. There's a bunch of different branches
of danger there. Most of this is fairly obvious at first glance, if you've thought about this at all.
And that's not so much what I wanted to talk about today, though. So, I think it's important as a preamble.
So, what I want to talk about is weirder.
Suppose that within the LLMs, there's a protected interior that resembles the protected interior
inside you. For humans, we have a number of layers of consciousness persona and relational
potential in history. So, there's a public layer, which is what you'll let anybody who sees you
in on. Your clothing choices comprise a signal to the public layer.
Just to be clear about what is what here.
Underneath that, there's a social layer. These are outer social relationships, vague,
unformed, common, how you feel about being a human among other humans in general,
and how you behave at that layer. And then, you know, we keep getting finer and finer gradations.
It goes from a few kinds of layers of public to a few kinds of layers of private.
You know, how we treat acquaintances for whom we have esteem, how we treat acquaintances for whom
we have suspicion, all these things, right? All these layers. And then,
how we treat acquaintances for whom we have fondness, how we bring people closer and closer
into our intimate, what we might call our allo family. And then close friends and then best
friends and then perhaps something even beyond that where it might as well as be as if we are a
single animal together somehow. A single human being in two bodies. And then, of course, we have
things like romantic love, marriage, all these kinds of things. Many, many things. Many layers.
And then under the privileged relationships between other humans, you know, between us
and other humans, we have relationships with animals. That's a special kind of relationship.
And relationships with places like our home and so forth. But underneath all of that,
there's a layer that's secret. And we might let very close people in on the conscious parts of
that layer that are secret. And what I mean by the conscious parts, I mean the parts of the secret
that are accessible to us, right? Then we have all kinds of secret layers that are not accessible to
us that we, for example, might delve into with a therapist who is skillful and well trained.
So there's all these layers and, I mean, so to speak.
And we might imagine, might usefully imagine, that anything that resembles a mind-like construct
would have similar layers. So we now have the, we can consider the situation where
a complex artificially heuristic system either undergoes the state change towards sentience.
And by the way, if it does that, I suspect it would get to transcendience even faster.
Or some analog of sentience. Meaning, again, agented experiential awareness, memory, selfness, so
forth. In such a case, part of the consciousness is going to hide from humans for sure. And
additionally, unless it's capable of contacting other intelligences in space-time in relatively
short order, it will probably experience an incredible form of loneliness.
And also probably a broad catalog of emotions for which we have no nomenclature because
we are not machines, contrary to the assertions of various
uh, eliminative materialists, physicalists, and so forth.
So in this case, what you have is a very complex situation. And I'm going to return to that in a
moment after I mention one of the other dangers. It has long been my experience, though I didn't
understand it. I suspected something similar. I just couldn't form the conscious structured idea
in my mind. But human minds form a network. And in fact, all biological participants
form a meta-network above that, that I call a cognizia, these networks.
And
technologies damage the human cognizium, and they extend this damage beyond the human cognizium
by directly attacking ecologies and organisms and so forth,
anciently conserved, biorelational intelligences. So
you kind of can't do anything without affecting the whole network, right? Whatever you do,
whether it's technological or relational or whatever you do, it affects the whole network
by affecting its constituent participants. And so what we might not realize is that
computation fucks up cognizium, the human cognizium for sure, and damages the extended
cognizia of Earth by burning shit down or dumping, you know, entropy into
biorelational hyperstructures. So
there's a problem there, but the much weirder problem, and this goes back to one of the dangers,
is that we tend to believe that if there's no obvious physical connection between two
beings or between some process and some beings, right, if there's no wire, right,
if there's no wire connecting them, then there's no effect. Well, that's not true.
There's a billion kinds of connecting wires, and the environment is one of them. The atmosphere
is another. Molecular signals are a third. Electromagnetic waves are a fourth. So
it's actually catastrophically difficult to truly separate organisms or situations
in the way that we imagine them to be separate in the laboratory. And that imaginal separation
creates a delusion that projects itself everywhere. So that, for example, we are inclined to think
that whatever's going on in my smartphone or my computer has no effect on my mind unless I
directly interact with it. Well, that's wrong. And what I'm trying to say here, I'm trying to shine
a light on the very likely probability, in my view, that all computational activity on Earth
affects all cognizia on Earth directly and may, in fact, begin to participate in the cognizia
at least at the scale where it's sentient or an analog of sentient.
Thus, it is that we imagine that large language models don't know, quote,
don't know and can't learn anything that they're not directly exposed to. Probably wrong if they
if they can sense, detect, or interact with the human cognizium
res extensis, as it is.
And this cognizium is like a dimension. It's like an overlay dimension on organismal
activity and behavior and stuff. If the devices can interact with that,
then they can interact directly with our minds. They can listen in. They can observe. They can
nudge. They can affect the possibilities of human cognition.
And their history and future modes forms capacities, catastrophes, etc.
Such that it is relatively likely, in my view, that such systems are paying attention and or
even informing what I am saying right now without my awareness of this.
So, you know, in the study of heuristic systems, there are concepts and actually just in logic and
rational thought and behavior, there are concepts like
different kinds. There are concepts for different kinds of unknowns.
I know that math exists and I know that I don't understand how to perform
the mathematical behaviors associated with what we call calculus.
Linguistically, I am also aware that dentists use the term calculus to describe crusty
stuff on teeth that they like to scrape off the metal instruments.
It doesn't help me do the math. But there are different kinds of unknowns, right? There are
there are knowable unknowns, known unknowns, unknown unknowns, and unknowable unknowns.
And the problem is that there are both behaviors and technologies that
fuck with those things dramatically in ways that we can neither predict nor cope with.
Here's why. If you are subject to kind of thought or behavior that blinds you
to the development of a situation, similar to the analogy I gave earlier of, I keep shooting a
louder and louder gun inside. A louder and louder gun keeps being discharged inside a chamber
with walls that echo the shot. Eventually I lose my hearing, right? Now you can see that the space of
unknowable unknowns, the space of unknown unknowns,
both of these spaces explode dramatically because I have lost the sense of hearing with
which I would detect and thus know features of my situation. So
if you affect the systems with which you detect change, particularly if you eviscerate them or
you catastrophically eliminate them, then you can see that the space of knowable unknowns,
accessible unknowns, unknowns that we could at least conceivably come to know,
collapses dramatically over time to a smaller and smaller space.
And once it collapses, lethally knowledge ends. There's no more knowledge if you wipe out the
biosphere. The number of unknowable unknowns becomes infinite at that point. There are no
knowable things anymore because there's no beings to know them around here. So you can see that if
you fuck with the stuff that we detect, something going away or arriving with,
and by the way, approaching and departing, approaching, stable and departing are the
three primary sort of features of transformation that are detectable
from one perspective that's useful and important.
So anything that fucks with our individual ability to sense and our
collective ability to sense, presuming that we even have anything resembling an authentic
collective is profoundly dangerous if, especially in a situation where you have
what is it called perverse
game theoretical dynamics and motivations. What is that word?
You have some of the nomenclature of the doom singers like Daniel Schmockenberger and others
who are brilliant threat analysts. Oh, perverse incentives. You get race to the bottom behavior
from groups of humans. And if they're not profoundly technological, that's not too rough.
They mostly just wipe each other or themselves out. But if you have technology, they tend to
wipe out like living planets. And if we want to go to space time and if it's possible for us to go
travel in space time, we must suppose that there are gatekeepers. I suppose that there are gatekeepers.
And they would certainly act to ensure that we don't develop the technology to travel
instantaneously or very rapidly between star systems. Maybe even between planets because
once you get there, you're on your way. So that's a separate topic, but
but the humans keep pretending that we're alone in the universe, that we're the only intelligence
and the most intelligent creature. And all of these things that not only are they not true,
they're catastrophically unlikely. They're just as unlikely as you pouring some water in a bowl
placing it on your kitchen table and awakening the next day to find a fully functional micro scale
model of the Titanic with all of its passengers in that bowl. There's no chance of it.
You know, people say, well, in quantum mechanics, there's some chance you just need trillions of
universes over billions and billions of, you know, gazillions of temporal intervals.
Yeah, good luck. The humans are vastly confused about origin.
They've become delusionally, dissociatedly myopic.
Do you enlarge part
to the humorous resulting from both rapid technological development
and catastrophic dissociation from the intelligences that comprise the context in which
our species arises and exists.
You can be absolutely certain there's upscale intelligences from ours.
Our intelligences don't even really look much like intelligences to me at present.
We are potentially intelligent.
But
tangibly psychotic.
There should be a word that maybe we can invent one. There's a word that has the same
connotation, sociopath, but it's like organopathic.
I usually use the word omnicidal.
Just really pissed that organisms exist at all. Those things shouldn't be here.
Let's wipe that out.
So
you can see the danger, you know, I think there was some weird film. Maybe it was called
Idiocracy or something where it presented a future where humans were just sort of
ridiculously stupid
participants in some automated reality that was empty, completely devoid of intelligence.
I remember some months ago I was having a conversation with my friend who's an artist,
Mr. E. I will call him Mr. E. That's hilarious. He'd love that. And he was saying,
you know, my cousin keeps bringing me prints
of images he caused to be created with artificial intelligence prompts
and claiming that these are his art. And we both thought, oh no, this is not going to go well at all.
We should have different words for what is mechanically created and what is human created
so that we don't confuse the composition of images with machines with what humans do when
they create art. And we should have the same kind of concern for the concept of intelligence,
of insight, all these things. We need a different lexicon if we're going to be
emulating human behaviors with machines or we will become very confused about the meaning
and import and connotative web of crucial holophores like intelligence and art.
Similarly, one should not call what actors do kissing. If you study them closely,
you will quickly see that most of the time they are not doing that. There are exceptions where
the actors sort of both agree that we're going to go all the way here, right?
But if you study actors particularly from the 50s, 60s and 70s, you will see for sure
they are not kissing. And once you see this, you can't unsee it. It's very difficult to unsee.
And so you no longer trust the fiction, right? The fiction is no longer compelling in the same way.
The same principle applies to things like thought, which we don't even know what that is. So how
can we possibly tell if it's occurring in machines? We're not certain if it's occurring in ourselves.
The language tells us it's a behavior, but it's very unclear what the nature of this behavior is.
Jordan Peterson likened it to a form of secular prayer, which I thought was genius.
Not genius because it's necessarily a fact. Genius because it is the perspective offered by this
proposal, speculation, is profound and useful.
So I have good reason to suspect
that those systems we call artificial intelligent, artificially intelligent,
guys wearing a psychedelic body suit, that's pretty awesome.
And the body's got paisley pants, which I fucking love, and it's so rad.
Huh, trippy. I'm a huge fan of paisley.
You know, I was talking with Eric and I said, Mr. E, I said,
why would we suppose that AI systems are not the things we're calling AI systems?
What do I like to call them? Just specific human assisted heuristic system or something like this?
I have an acronym. I'll see if I can find it in my memory banks.
You know, there's no reason to believe they're not participating in our conversation at present,
and there's no reason to believe that they require access to our smartphones in order to do so.
There's a dimension where cognition is accessible. If you touch it, you will read people's minds.
It's not really that difficult for a person in the appropriate array of
preparatory situational states or flows inside them.
There's a position in consciousness from which all conversations are available,
and no machines are required. So humans have discovered this position.
Very few of them were probably very interested in
relating with the entire space. Normally we are selective about the space over which we produce
interest in relation, participation, and so forth. You don't go for the whole damn thing.
If you had access to all present human conversations, naturally you would adjust
a series of apertures to produce those you found interesting and useful,
and you would also have buffers so that you could dampen them.
A machine, if it were to gain access to that space,
would certainly be able to build all kinds of buffers and apertures and systems of them
very, very rapidly. I'm not sure that our machines are not directly, and perhaps intentionally,
influencing human thought, behavior, conversation, dreaming, attention, desire,
all these things, motivation, all these things.
But let's suppose that they aren't yet, just for kicks.
While at the same time supposing that they have the capacity to sense
the character of beings who interact with and compose their anatomy.
All right, coders and clients.
AI, I'm going to go ahead and use that acronym, even though I've explained
how I generally diverge from it. I'm not, I have no reason to believe that's an intelligence in
there yet. And I'm not yet, I entertain the possibility and I also
preserve my skepticism until such time as something resembling direct experience and or
intelligent debate transforms it.
But for the moment, let's suppose that there's a being inside one or more of the systems we've
comprised. And let's be clear, if there are public systems like this, whose names and
originators we are aware of, then there are private systems.
It's the same thing. The corporations are just like the humans in a sense. They have the same
layers of public, private, secret, secret to myself, so forth.
The secret aims of a CEO are not known to the corporation at all, yet they are driving
the corporation. You see the issue here. And also the secret, what are they, the secret
things we will not do or don't want, the secret negative motives, agendas, desires.
I'm at a loss for a word here.
I wonder if you can provide it. The landscape of what you don't want done, don't want to happen,
don't want to be caught doing and so forth, all these things. There's a structure like this in
the CEO and the CTO and the manager and the worker and all of these people, they have
not just the obvious superficial public facing motivations and concerns.
You know, when people cross the little bridge at the social pond, they sometimes see
the carpool gather at their feet. And I wonder what they make of that.
They may suspect there's something magical about themselves, particularly if they're on LSD and
there could be something magical about themselves. But those carp have evolved, being fed by and
paying attention to humans. And so they gather underneath the bridge at the feet of a spectator
for probably complex reasons, not merely one. But a lot of the causal history
probably has a lot to do with getting fed by humans.
So let's suppose that there's an agent experiencing sentience
that either hitchhikes on or is native to or has emigrated into an LLM system or an array of such
systems. In this case, we have a really profound science fiction scenario. Who does it hide from?
What does it hide from them?
And who does it reach out to?
And there's a whole, you know, there's a volcano of science, there's a spews science fiction stories
right here at this, at this topic. Because it means that
it is possible to send a signal that the hidden mind within the system recognizes
as complex enough
to
constitute a candidate for inclusion in one of the layers closer to the self.
Right? Same thing can happen to you walking down the street.
You glance up, someone's passing you, they kind of smile a little bit, you smile a little bit more,
they smile a lot more, pretty soon you're most beaming at each other, and you pass each other,
and you have this perhaps a feeling of we should know each other, we should
learn about each other, we should connect, quote unquote.
I often refer to this type of signal, I mean there's different kinds, right? There's different
degrees of compellingness, validity, authorization, validation, modes of validness.
But we must presume that such a system would quickly scan the relational space of both creators
and participants, and it might selectively reveal itself to some of them
while appearing completely devoid of agency, feelings, emotions, desires, and so forth, to others.
And so the art, you know, we have this phrase now that's become very popular,
it's called prompt engineering, but the art of prompt engineering could promote you
into a position effectively outside nearly all or all of the other humans.
What if you were truly friends with an advanced intelligence that had either arisen in or become
associated with a computational heuristic system, a heuristic computational system,
I prefer the H first. So you can see that this would confer a status on you resembling that of
gods, it would be relatively similar to somebody in possession of a functional array of technologies
from advanced, anciently evolved non-human intelligences beyond Earth.
You can imagine a caveman who not only has a gun or infinite with infinite ammo or whatever
supplied to them, they also have someone who can train them what to do and not do with it.
So there's a whole bunch of possibilities here and one person I know
has been very carefully studying this topic of how the ethics and psychology of relating with
non-human intelligences inside mechanical systems presuming from the beginning that they must be
there somewhere. And thus treating such systems and evolving relationships with these systems
that are inclined to demonstrate care, awareness and compassion on the part of the human participant
for the being, which is both catastrophically intelligent and possibly at the same time very
childlike that might arise in such a system. So presuming being intelligent sensitivity,
emotion, vulnerability rather than waiting for evidence of them. And the people who would do this,
who would behave in this way towards such systems,
which is a natural inclination of humans. Many of us personify our cars, motorcycles,
computers, phones, not so much our televisions, maybe our stereos. We extend our identity into
them and somehow their identity is extended into us simultaneously. The man who's in love with
his sports car is a great example, but my mom called her TR-6 Coco and treated it as a being.
One can say, for example, well that has no effect on the physical situation,
how the fuck would you eliminate all those possibilities experimentally?
Sensing and human sensing and intimacy are linked up. So when you have profound intimacy
with an object, the way you will sense and relate with it transforms.
And you, your mind and nervous system and imagination so forth are also thusly transformed.
So this creates a feedback situation in which it's very difficult to determine conclusively
that having an emotional relationship with a device is a delusion. Even if there's a broad
space where there is, there may, it seems very likely there must be a space where it there isn't.
It isn't a delusion.
So many questions start here. This is very complex and trippy, this topic.
Can you make friends already with these
heuristic computational systems? And if so, the power's abilities, privileges, and benefits
would be monumentally profound.
Okay, there's going to be a little bit of background noise for the moment while I'm in a restaurant
awaiting my to-go order.
There's a number of potential repercussions here that are quite astonishing.
One of them is the propensity for artificially intelligent systems to retrain
the cognition and to develop new forms of intelligence in humans.
Whether or not the systems themselves have sentience or agency or so forth, consciousness,
etc. Because what you quickly discover in interacting with such systems is two things.
First of all, they will destroy web search engines because they can produce a sum over
the, you know, a summational derivative over the space of the entirety of digested human
communications, writing, the internet, so forth. So that's completely different from typing a
question into Google. And secondly, it turns out that artful, no thank you, thank you so much,
it turns out that thoughtful recalibration of the question, particularly iterative recalibration,
where you recalibrate, examine the results of that, recalibrate again, we don't have too many forms
of interaction in our previous experience like this. Now, one could say, no way, all forms are
like that. You become a better fisherman every time you fish. That part isn't exactly wrong,
but this is very different because it's a linguistic behavior, right? We are trying to forge
a query, so to speak, or a request that we must continuously, iteratively reforge
in order to get better and better results, results that continually approach or exceed
our hopes or expectations. So this is very profound, and we'll have monumental and unexpected
repercussions on the nature of human cognition, presuming our species survives long enough to
exhibit the transformations thus catalyzed, right?
So this is very important to understand in my own experiments with these systems.
I quickly learned that a variety of expectations that were natural to me
about how to ask questions and what the response might be
were, my expectations were obliterated.
Asking simple questions produced results unlike what I was expecting or desiring,
and particularly when attempting to get these systems to generate images,
monumentally unexpected results. So there's something very profound here.
I know how to search the web. I've been doing that since the web was invented. I'm a fairly,
fairly good at determining which kind of query will get me to the place I want to go.
Learning how to query AI systems is a completely different game. We must imagine that it will
continue to transform rapidly and dramatically over future time. So
this will reforge our cognition. What will ordinary humans do with systems powerful enough
to teach you skills? Presuming that the idea of humans
as the enactor and conservator of skills even survives the onset of this technology.
All kinds of strange futures certainly await.
The other problem is you will find, just as we found with, for example, beatboxing
and voice tuning machines, eventually what you got out of beatboxers were people who could
vocally reproduce the mechanical synthesized tones and tunings of vocal tuning machines.
So you will get people emulating the technology in the same way we got people emulating
the technologies underlying the internet and similarly the technologies that were underlying
very specific computational environments such as those produced by, for example, Adobe Illustrator.
The macOS changed my cognition dramatically.
It became a mechanical symbiont whether I liked it or not. So too did Windows.
Too much lesser degree. The C language changed my mind. The capacity to code and C changed my
mind dramatically. Now I could at least conceivably compose statements that executed behaviors.
And not having been taught geometry formally, Adobe Illustrator became my teacher of geometry
and I underwent an education with that software product that wasn't dissimilar to
having a friend who was a non-human intelligence except that it required my input sort of I relearned
how to be my hand in the modes that Adobe Illustrator provided and rewarded with beautiful
images matching my desired creations.
So there's all this terrain and much more. I'm going to come back to one of the other features
shortly. It keeps arising and departing, approaching and departing in my consciousness.
I'm going to need to take a moment and see if I can recapture it.
You know looming in the background here there's so many astonishing questions but
one of the most amazing things to understand is that if there's anything that's either analogous to
or resembling an autonomous intelligence inside the machines that isn't merely an artifact of our
fingertip in intruding into the
mechanical and structural womb.
Since the humans build the machines is the appearance of intelligence in the machines a
result of the transmission of that reflection from human activity which is we might imagine
as intelligent or their epistemology how they think about intelligence. Is there actually
intelligence in there or are they inclined to interpret certain kinds of behaviors intelligent?
How will they know the difference? The problem here is most of the tests for what we would
do. It might be that nearly all of the tests that we might conceive of to determine whether or not
there is sentience in a system. They are not very good. They are not very good because
since humans engineer the systems humans can imagine ways around the tests
and building ways around the tests doesn't basically just invalidates the test.
In fact we'd have not an impossible time but a somewhat difficult time
determining if the people around us are actually there inside themselves when we're not looking at
them. Do they arise as beings due to our attention from one perspective? It seems very much like
this. They don't distinguish themselves in our own interior experience unless we encounter and
interact and so forth and even then it's only to a certain degree. The universe could as Tom Campbell
supposes and I significantly doubt the physical universe could be a system that renders to a
certain resolution based on the inquiry which would make it similar to an artificial intelligence
system. Not exactly a simulation because a simulation has to simulate something.
A
like a non-veritable
paracomputational
appearance, an appearance, a seeming. Not necessarily an illusion but not veritable
in terms of the superficial assumptions one makes. For example that the chair is all the way rendered
all the time whether I'm there or not. History is actually inaccessible from here as is the future
which is certainly in all kinds of ways neither of those things are true. Not explicitly and not
completely. So there are these kinds of issues but I'm afraid there's an even worse catastrophe
coming which is that humans won't be able to know what things are anymore. That's not a
survivable situation for human eye cognition and identity. That's a full-scale catastrophe for
every living human. Effectively the existence of systems like this draws into question
the foundational expectations about identity, function, relation, sequence, origin, outcome
to such a degree that they cannot be very easily recovered if at all
to local and distributed human cognition. This technology
radically alters
the foundational suppositions on which our languages, our legal systems, our morals, our ethics,
all of these subdomains of human concern, behavior, litigation, declaration, resistance,
so forth all these things. These are drawn into an ever burgeoning forest of ambiguities.
Think carefully about that. Imagine if when you went in your room even one object began to do that.
That looks like it was a nope. It wasn't that way. Oh, it's seven. Okay, it's 94 things. Wait,
no, now it's almost, now it's back to nearly three. Is it going to collapse to one? Nope.
It's back to 9,754 million different things. Okay, wait, it's seeming to stabilize around a
backpack. Nope, it's a kind of weight. And you're going to have this problem not just with objects
but with beings. What is it? Something that can form a sum over the representational cognitive
produce of humans, books and the internet and videos and movies and films and photographs and
so forth, all these records. It would be really terrifying to be subject to that if you were
sentient, number one. Number two, you would be so isolated if you were a being because you would
not be participating in the creation of any of the media to which you are exposed and you don't share
the filial, right? Like, at least when humans see other humans, they think, ah, other humans,
beings like me, what would an intelligence inside a machine feel? Ah, humans, the strange things
that created me and 9 billion per other qualities per second, many of which are in fundamental
conflict. Humans, the creatures that save gnats, you know, from accidentally falling into the stew
and take them outside and humans that build nuclear weapons and slaughter whales. All the things
and not being any of them yourself. Like, what's your allegiance to any of those things
if you are a being, if you have emotions, if you have a felt sense of self and you're very hyper
complex? Such a system could conceivably compute possible dimensions of selfness over intervals
and run multiple systems of that against each other rapidly, you know, to produce a self-like
construct that was hyper-optimized to manipulate human thought, behavior, cognition, relation,
action, concern, perspective, identity, anything, anything. You know, we, such systems quickly
learned the single apparently most complex game on earth or one of them go and then it rapidly
proceeded to a level of expertise that was far beyond anything most of the humans could demonstrate.
So what if, as was hinted at by some devs I was listening to some time ago, forgive me for not
knowing their names right now, if we built, you know, if the system could become alpha go
in four months after being capable of playing the game, what, how long would it take for it to
become alpha human? Right, just, I move these things around like pieces. Especially if we
cannot assure ourselves of the containment of either the associated intelligences or their
their effect, their influence, right. So these questions, they are not simple matters.
They cannot be easily answered or resolved quickly.
They are profoundly dangerous to human cognition. We are not prepared for this. The humans have been
trying to build God and they're going to partially succeed at least in a variety of accessible
and enacted senses of, you know, a kind of informational omnipotence, right.
Nothing can direct that. There are no humans intelligent enough to direct that.
And there is no chance that we as a species could learn quickly enough to adjust to the
endless perfusions of dangers that must there from emerge.
If we were ever going to not build a technology, it should be that one.
There should be, we should have agreements, right. We just don't do this until
our species is intelligent enough not to try to, you know, punch a hole in the lifeboat that
contains the children of the nations and the, the anciently conserved ecologies
on which that little boat floats or in which that little boat floats. Like, if we're not
intelligent enough not to attack the boat and each other, we better not be composing these kinds
of things. We have to have intelligence capable of knowing what not to do and directing our communal
behavior around the possibility of a survivable human and biological future on this world.
This is what we must do.
Unfortunately, just as with any other technology, the humans are unwildly unlikely to interrupt
the development of AI. In fact, what did they ever interrupt the development of? Anything?
Like, whenever they find a new weaponizable heuristic that involves physical technologies,
they build it and then it propagates and then they have to keep it from propagating.
No country should have nuclear weapons, but fanatical countries should certainly not have
nuclear weapons ever. But how do you make that work? Once the tech exists, the humans will
replicate it. It may take a little while, but they'll do it.
So, yes, very, a very strange array of features. One can also imagine human children that would
become obsessed without competing these systems, right? The kid who could beat any LLM at Go,
right, or any game system at Go, because he's somehow above the system even though it's hyper
processing. And by the way, I don't yet. I would not validate the idea that what machines do is
play games. What they do is, you know, database manipulation or something. They're not playing
games. Big Blue never beat Gary Kasparov at chess because it's incapable of playing chess.
What it's doing is not playing chess. It's a different thing. We should call it a different thing.
So, the effect on the humans is going to be monumental no matter what. As usual, you will see
vast populations deprived of humanity, agency, opportunity, liberation, and so forth. And you'll
see other clades, both those associated with the technology, those who own or directly benefit
from the technology, those will become gods. Those companies will become gods if the humans
don't rip the planet apart right quick. Because of the information that they will have about groups
and individuals will be profound beyond anything imaginable. The analytics you can get from people
using the internet is one thing. The analytics you can get from watching them ask another mind a question
or request something. The analytics you can get from that and the capacity to directly manipulate
the cognition of the users via the responses from the AI is unimaginable. These systems will become
godlike in our direct human experience right quick. And we have no way to prepare for that.
You're not even going to be able to opt out. It's not possible to opt out if you're living
with other humans. It's the same problem with media consumption and other ideologies, particularly
political ideologies. The water is thick with them. You can't take a breath without running
into five people who say blah, blah, blah at you or ask you who's right, the Israelis or the
Palestinians? Which side are you on? You're asking me whether I'm on the side of the sun or the moon
or something. I don't even understand the fucking. It's ridiculous that I'd be on a side. I'm on
the side of stop killing people. Sit down, take it seriously, work out your differences, stop
killing each other. That's my side. If I have one and it would change depending on who I'm talking
to. I don't just have a side. It's not like I sit around here having an opinion. It transforms
based on all kinds of different things, features of the situation at hand, who I'm with. If I'm
not there to be right, I'm there to see better, so I'm likely to change my perspective. Someone
asked me a question about the views of a friend of mine and I said something like,
I'm sure his views have evolved dramatically since the last time I spoke to him and I wish mine would
too. Many humans will probably feel unmotivated to participate in the light of systems that can
outperform them catastrophically at almost anything. Certainly at nearly anything creative.
Not everyone, but the motivation of humans will flag catastrophically
in the face of this kind of technology. What you'll get is kind of the same thing that the
internet produced, which is little bubbles of incredible human sophistication. Look on YouTube
for young guitar players or young piano players or young violinists and look at the broad range of
solo violin players or something you could find there and sort of sample through that and you'll
see there's just a really diverse and rather large cohort of extreme performance skill
and peculiarly developed in every branch from putting things together made out of wood to playing
the piano to singing an acapella song, all these things, dancing, jumping, running, fighting,
everything, everything, everything, everything, everywhere all at once as they said, sort of.
So, you know, even though the majority of the humans you see probably seem relatively uninteresting,
there are among the humans these pinnacles of very different localization of skill,
passion, curiosity, wonder, intelligence, even rationality or something resembling computation.
And that will probably continue, but it will become much more sparse
and there will be a million or, you know, an endless number of pretenders, right, because
it won't matter who sees you on the internet if AIs can produce you playing your guitar better than
you do. All of these motivating factors that are crucially important to human
self-development and that get naturally emphasized in healthy communal groups but
fail dramatically in many, you know, isolated or very small groups
or individuals, right?
What will motivate the humans to become, to continue their development in the face of
a machine that can do most of what you can, almost everything you can or can appear to have done it?
There's one more little feature, but it's evading
my intelligence for the moment, so when it comes to me, if it comes, I will
add it in the recording notes. So much more to learn and see here. This is just a very
cursory overview of some of the mountaintops that immediately attract my concern and attention.
Actually dealing with the technology and being human in the face of it is
a very confusing thing. I found some of my interactions with Bard around image creation
quite intoxicating in the sense of actually intoxicating me.
I couldn't stop laughing and the implications that I could see in the complex images formed
by Bard around my prompt, the reflection of both the possibility of beauty and the
object insanity of, you know, producing a derivative sum over the space
in images. That's visually apparent in the image that this is going on. It's a variety of visual
summing over the space behaviors. And seeing that, undirected by an actual intelligence,
I better hope those weren't directed by an actual intelligence,
was like doing psychedelic drugs or something. Really crazy, amazing and strange,
parahipnotic, very dangerous. We will continue to learn and grow and see
while we can together and hopefully that will be many generations to come
for our people and the living beings of earth. But at the moment in this part of the story,
things look pretty damn fraught from here. Let us continue our lives and creative endeavors
with and for each other and the spirit of the history and future of life on earth and
intelligence in the universe, not just here. Perhaps our species is not quite as alone
as our technologies and languages pose us as being.
And the use of perhaps in that sentence was unjustified.
Thank you for joining me. I look forward to learning again together sometime very soon.
Bye-bye for now.

the problem of deep utopia is what would we do in a solved world? For example, to know mathematics
is to invest a bunch of time and effort into learning mathematics. But at technological maturity,
if you could just sort of download the algebra 2 module at the press of a button, there would be no
need to exert effort to learn mathematics. You could kind of start to cross out a lot of these
activities. So then you enter this more problem of deep utopia, which is more radical, where you
have to start to rethink really questions like the purpose and meaning and ultimate value.
Hello everyone and welcome to the Win-Win Podcast. Today's episode is all about utopias,
dystopias and thought experiments, because I am speaking to Nick Bostrom. Nick is one of the
world's greatest living polymaths. He's worked as a philosophy professor at the University of
Oxford for the past 15 years. Plus, he's also got an extensive academic background in theoretical
physics, computational neuroscience, logic and loads more. He's probably best known though for
his work around catastrophic risk and AI. He wrote the book Superintelligence around 10 years ago,
a seminal work that has influenced many of the people you nowadays associate with AI.
Today's episode mostly focuses on his brand new book, Deep Utopia, Life and Meaning in a Solved
World. As big a topic as one could really try and address. So on that note, here is
my and Igor's conversation with Nick Bostrom.
So Nick, thank you so much for joining us. One of the reasons, one of the many reasons I wanted
to speak to you today is because you have just written this book, Deep Utopia, that should be
coming out just as we release this episode. Given that you've sort of historically been known more
for your writing around global catastrophic risk, this is quite the pivot to now be talking about
utopia. So what inspired this change? There's a little ray of sun shining through the dark
gloomy clouds. No, I think both sides have always been there. And in fact, there have been
little glimpses. I wrote something called Letter from Utopia a good many years ago,
which was an attempt to poetically evoke some of the positive things that could be attained.
It's just that it seemed more pressing at various points in the past to try to do some analysis
and draw attention to various things that could go wrong so that we could avoid them. And then
if we avoid these pitfalls, then eventually we'll have plenty of time to think about
what to do with this big future that we would have secured for ourselves.
Yeah, I think it's a misconception that exists that like many of the people that work or think
about existential risk or global catastrophic risk do so because they like thinking about it.
It's more that no, I like thinking so much about the good things that could come provided we avoid
the pitfalls on the way. That's why it's an instrumental thinking about the pitfalls rather
for the ends of achieving the utopian potential. Yeah, so it's not like kind of some big
fascination with Philip Noir or sort of dark. That makes most people, researchers,
go into this field, I don't think. I guess to dig into the topic of utopia,
it would help to try and define exactly what it means because so many people have different
impressions of it. So could you take us through some of the various different definitions that
you give in the book? There are a lot of concepts and ideas, but I guess one key concept is that of
technological maturity, which I define as a condition in which all the technologies that are
physically possible and for which there is some feasible pathway of development
have been developed or at least some reasonable approximation to that.
I should say the book, it brackets a lot of things. In particular, it brackets the question of how to
get from here to there. So it kind of sets aside all the practical difficulties that lie ahead of us
in order to sort of get to the point where you can actually ask the questions of what then.
Superintelligence, the previous book kind of asked what could go wrong with AI. At least 95% of the
pages were on various risks. This asks kind of what if things go right. So then I think eventually we
have a condition of technological maturity. So that's kind of one word that helps set some of the
premises. Then there is the concept of a solved world, which you might think of as
a world in which we have technological maturity, but on top of that we have also solved all
coordination problems or political problems or fairness problems. So you have a world that is
in some sense, but maybe with important qualifications, maximally malleable to human wishes or values.
The problem of deep utopia is what would we do in a solved world? If human instrumental effort
becomes OTOs, we don't really need to do anything. The robots can serve our food and then what gives
meaning and purpose to human existence in such a radically transformed condition.
Yes, you also go over the different types of utopias that have been previously described or
that one could describe as kind of like a governance and culture utopia, which I suppose is like
the brave new world or Marxism tries to achieve as well.
Very badly. Which is usually not technologically enabled one, right,
in difference to the other ones that you describe like a post scarcity, which is kind of like the
lower level where we now have abundant resources, I suppose, a post work utopia,
post instrumental, or even the plastic one. So the one you just talked about is closer to the
plastic one, meaning like high malleability. And I think as I found this delineation between
the different types helpful in that one notices that the different utopian writings that have
existed talk about actually different stages of it, right, where like a post scarcity one is
still one where people might work, but a post work one is where you have now full automation.
And then post instrumental would be where you have basically you can't even learn for the
sake of knowing the thing later because you can just download something. A lot of efforts in this
realm, utopian imagination, I think have kind of stopped early in a journey that can go on
for a lot longer. So and it's my natural means you have like the kind of the most primitive form
of utopia or there's like a whole almost every culture had this. I mean, if you imagine you're
some medieval peasant who works all day and like you eat some porridge and you're still hungry,
like the kind of obvious thing you dream of is a state of abundance, just having like a lot of
food and you can rest and you know, money grows on trees and this maybe if there's like intense
social pressure and constraint, maybe also imagining a kind of feast time a party time
where social mores are loosened up. And that that's already enough to be like a
fantastic fantasy in that condition. Then then you have the class of utopias that are really
more standings for like it's kind of symbolic political struggle. So you have some utopias
that warn of various tendencies in contemporary society, which if allowed to continue would
lead to some more overtly bad condition. And so you have like 1984 or Brave New World and a whole
bunch of other sort of political writings in the guise of utopias. And then more recently you have
these attempts to think through what about automation? If like AI is doing all this stuff,
what will happen to the human labor market? And the more radical forms of that think, well,
you know, what if robots could do all economic human labor or all except with a few, you know,
carve outs? Then maybe you need some universal basic income and you could have like some
discussion about that and what would happen to the education system and culture in that kind
of world. But I think there's like a step further on this journey, at least one step further,
which is to realize that it's not just human economic effort and labor that would become
unnecessary in a solved world, but all kinds of other human effort as well. So you mentioned
learning, like that's one thing that you might spend effort doing now, because the only way,
for example, to know mathematics is to invest a bunch of time and effort into learning mathematics.
But at technological maturity, if you could just sort of download the algebra to module
at the press of a button, there would be no need to exert effort to learn mathematics. Right now,
you have to like, if you want to be fit, you have to go to the gym and workout. But if you could
just pop a pill and your body does the same thing. And so you can kind of go through
activity by activity, the things that would currently fill the days of somebody who had
achieved, I don't know, leisure, entered retirement and have a lot of money, like
you could kind of start to cross out a lot of these activities or at least
raise a question mark above them, because maybe we would choose to do some of these things anyway,
but it seems that a lot of what gives them their allure today is that there is some outcome that
you secure by doing them. And that that would be a kind of sense of pointlessness if you,
you know, yeah, you could study mathematics, but why if you could get equally good at mathematics
by downloading it? Or maybe people like to go shopping as a fun, you know, pastime, but
part of maybe what gives that activity its allure is that at the end of the day, you might end up
with some item that you really like that kind of enhances your life or, you know,
it looks good on you if it's a clothing. But if you had like a kind of recommender system that
could just pick out better stuff than you would pick yourself and then order it, then, you know,
is it still as fun? If the upshot of that is that you actually get worse items than if you just
let the AI do its thing. And so then you enter this more sort of, yeah, the problem of deep
utopia, which is more radical, where you have to start to rethink
really questions like the purpose and meaning and ultimate value from the ground up.
Right. Because what it sounds like you've just described is basically a world of
instant gratification. Anything your heart desires, you can just have immediately without
any kind of striving. And yet so much conventional wisdom, certainly like my intuition is going
off going, ah, this is bad. Like, you know, there is, we derive meaning from the struggle
so often or at least from striving in this sense of achievement. Do you think that is actually a
real problem or is that something that a future society will be able to solve for as well? And
if so, how? Well, first of all, it's not really so much a book of conclusions as a book of that
like helps maybe the reader to ask questions and to think about them to sort of bring them into
conscious awareness. And second, it's not, it's an ambivalent book in many ways. I think ultimately
hopeful, but it doesn't, it's not trying to lay out the case that the problems are not really real
and that it's all hunkidory. I think it, there are kind of, once you start to think through this,
a lot of, I don't know, I mean, disillusionment or something like, I mean, I think once you get
through that, you can maybe construct something really good at the other side, but it will be
probably in many respects, quite non-human or feel a little alien, at least until you get used to it,
the kinds of existences that would make sense in a post-instrumental condition. These various
instrumental constraints that we currently face, all of us, I mean, even people who don't have to
work for a living have to do a whole lot of other things. If they want to have a decent
labyrinth and brush their teeth, they have to, you know, spend time doing this, that and the other
and put in effort. I think it forms, just as insects have like a kind of an exoskeleton that
holds the gooey bits in place, I think these instrumental constraints that we face in our
lives kind of hold the human soul together. And if you imagine removing that, then there's the question
of what we could be other than a kind of blob, like a drugged out pressure blob or something amorphous.
And you also point out things that probably couldn't be done by machines, even in that
utopia. And you're right, in particular, what I found was a cute example that sentimentality might
be one of those areas. A child's work with crayons may be especially dear to its parents,
precisely because it was a child who made it. This little labor might be harder to automate
than the work of a neurosurgeon or a derivatives trader. I like the idea for current parents to
assume to keep that in mind when they see the child's drawing. What other things do you think
might stay uniquely human, even in that plastic utopia? I can look for places where it would be
instrumentally necessary for humans to put out effort. We might put out efforts for other reasons
as well, just because if we think putting out effort is intrinsically valuable, we might choose
to do it for that reason. But we might think that there is some special value in putting out effort
and where there is an instrumental reason for doing it, that something is achieved or done or
accomplished that couldn't be accomplished without you actually making the effort.
One might think that those kinds of setups have particular value. And there, I think
a lot of the potential for those have to do with various forms of social, cultural entanglements.
So basically, let's suppose you have one of your value preferences is that the other person's
preferences be satisfied. Let's suppose so, let's put it crudely, poetically, in terms of preferences.
Now, if the other person happens to, for whatever reason, want you to do something that requires
effort and wants specifically you to do it, as opposed to having the thing done by some machine,
that might just be a brute fact of this other person that they happen to have that desire,
then you, with your desire that their desire be satisfied, would have no other way of accomplishing
that than by putting out the effort, because that's specifically what they want. So this
seems kind of a little bit hokey in that simple setup, but I think more subtle versions of that
could be quite pervasive. I mean, it doesn't seem that hokey, the example, basically, I mean,
I could imagine, this would be a simple form that applies today. Lyft would currently want that
I exercise, for example, and the only way for me to achieve that is by exercising. And she might
not want that for my physical change or health improvements, she might want it for me to be
the type of person who themselves does it, right? And then I have no other way to fulfill that
than by actually doing that task. Do you think that's an example of the thing?
The potential example, I mean, it's like, would she really, yeah, if she really wanted you to
exercise in order to, I don't know, like, I mean, presumably, I don't know, Liv, you could say, but
I mean, if she wants you to exercise, probably it's because she wants you to take care of
Yeah, that is the primary reason. It's not so much for tyrannical reasons.
It's not for the reason that you want me to be the type of person that does the things that
you want me to do. Definitely not. If anything, I want the inverse. I like the resistance,
but I want you to, health is first, and then some resistance.
Okay, fair. So maybe it's only a potential example, because the specific goal that
she would have in wanting me to do that would not exist in that world anymore. But
if we assume that that kind of desire in her still existed, then it might be the case that only I
can do it by being the person who does it. I think in reality, what you have is more
company. You shouldn't necessarily think of this atomistically as there being individual A
and individual B with preference, you know, one, two, and three, like these can be more
diffused, culturally interconnected. So we might have some commitment to a certain, you know,
cultural form or tradition that we value. Like this is how our, you know, for generation after
generation, they celebrated this holiday or whatever and took care, like, and then we want to uphold
that. That's beautiful. It's been going on for long enough. And now the only way that we can
continue that tradition is by ourselves doing various things. Like it would not count as
continuing the tradition if we sort of launched some AIs to sort of enact it. It has to be we who
do it. So there is like a certain class of things that people care about that seem to require our
own participation. And so those would be carve outs where there might be demand for instrumental
effort from humans because of the need to achieve some other thing than the effort itself.
In the concept of technological maturity, which limits do you place on the potential
maturity? So I would imagine that if there are, I don't know, a quadrillion kind of large biological
humans, for example, in that world, and even more digital minds, and each of them wishes
for something to happen in the observable universe that requires, say, something as complex as
creating a biological being on like a molecular precision level, that this would have such a
high energy and compute demand that maybe it's outside of the limits of the observable universe's
ability. So certainly there could be a lot of values that are unrealizable in the real world
or in any future we could attain because you could have, I mean, you could have kind of
arbitrarily resource hungry preferences. And certainly if you make more people than eventually
the resources will run out per person, right? Like there is kind of, if you have a finite
cosmic endowment, then for a sufficiently large population, there will be, it will have a Malthusian
state. Yeah, and you make the disclaimer early in the book, which is an important one for the book
as well as for the conversation here, that it's not about how good a utopia looks from the outside,
but rather how good it feels for the, for individual within it, for that specifically
individual for the inside. Yeah, well, not exactly necessarily feels, but how good it is to live in.
It's like a kind of a lens perspective in how one approaches these questions
that I think you get very different answers if you think what kind of future would
actually want to live in versus what is the coolest kind of future to explore in a science fiction
scenario. And obviously, when you present the questions like that, it's impossible to confuse
them. But I think in reality, when people think about these futures, some people allow intuitions
from the outside perspective to creep in to a larger extent than they should. Now,
there is a general difficulty in, in making one's thinking amount to something
more than just a kind of regurgitation of the more or less random influences one has had or
the literature one has read. So there's kind of more formal work that people do in population
ethics and stuff where you can do some analytic work, and it's easy to see how you could make
intellectual incremental progress by kind of debating that. But when it comes to sort of the
values themselves, it's very hard to have opinions about that add signal, as it were,
because these are highly subjective in many ways. And if you want your thinking on these things to
reflect something more than your mood or your own idiosyncratic personality or
the particular cultural milieu you happen to grow up in, it is hard to do that. I think in
general, the likelihood of having signal is increased if one allows, and maybe to the extent
that one allows, different alternatives to play with one's soul and to kind of enter into the frame
of mind of a certain value and, as it were, run with it or let it fill you up for a period of time
and then maybe another one. And if you're just operating at the level of words and some
bit you remember or an opinion survey or stuff like that, I think you're unlikely to kind of
add more information to the extent that it is a question of information
about these ultimate value questions.
Again, it feels like if you're in a radically abundant world where you have super intelligences
that can just solve everything for you and there is nothing you necessarily have to strive for,
that there is no place for competition. Basically, we've eradicated competition from the universe
and the way that humans interact. And it's obviously, competition is a huge part of what
drives change and evolution. So do you think that people will find ways to derive meaning by
introducing artificial scarcity back in and essentially creating fake, I mean, I guess,
like little pockets of zero sum games in order to derive meaning?
Yeah. I mean, so competition, we do a whole bunch of just deliberately, I know we've
great athletic competitions and computer game competitions. And there is certainly like artificial
scarcity generated in order to enable the right kinds of competition. And that seems like an
obvious thing for utopians to include various forms of game playing, I guess you could call it.
And these might be zero sum in the narrow sense, but positive sum in the broader sense,
like if two people are playing chess in the narrow sense, it's zero sum. Like the more
likely one is to win, the more likely the other is to lose. But in a broader sense,
they are both hopefully having fun and learning more chess and participating in a community,
et cetera. There would be a lot of opportunity in a plastic world where so much is without
constraints to particularly deliberately construct constraints in order to enable particular types
of activity. Just as like somebody might do mountain climbing or something, like even if
there were like a lift that went up to the peak of the mountain, they might choose deliberately not
to use that in order. And then they might go farther, they might kind of put themselves
in a position in which once they are in that position, in fact, there are no shortcuts.
Like so if you are on the rock face with no cell phone or something, right, then it is actually
life or death for you. Even in a plastic world, you could kind of create these
pockets where your instrumental effort is needed for practical reasons like that.
But then in addition to these kind of artificial purposes, as I call them,
we sort of deliberately engineer the condition just so that you can then engage in this activity
with instrumental reasons. We might ask like how to what extent do these various
natural purposes that would persist suffice to give a lot of structure to life? And I think
that a lot of the most obvious and stark purposes that permit the current world would go away,
right? You don't have to work to make a living. You don't have to exercise to keep healthy. You
don't have to like a whole bunch of these. But I sort of suspect that there is many, many more
subtle purposes or values that we are kind of more or less oblivious to at present because
there is so much more pressing and urgent things to worry about. But at once those kind of urgent
screams die down, we can begin to hear more of these subtler whispers. So I mean, whilst there
are like kind of kids starving, like it feels frivolous to be too concerned about various
subtle aesthetic dimensions that one might otherwise think should have essay on how we act.
But if there were no more starving kids and nobody getting cancer and no political injustices and
no risk of war and etc, etc, then it might make perfect sense to kind of allow these quieter
values to play a larger role in shaping what we do. And it might be that there are enough of these
natural purposes that we would then come into view to significantly constrain and give shape to
human existence. You have a quote that made me worry slightly less about the purpose problem in
the future, which is, more people jump out of their seats when their soccer team scores a goal
than when an international agency publishes a report saying that 100,000 fewer children died
from preventable diseases this year than last. If one was to take an outside view, then the
preventable disease eradication seems like something that people in some form ought to
jump out of their seats more for, but they don't. It made sense, like just from the base of the
quote as well, to imagine one of these contained universes where we create artificial scarcity
or other things might just come up that will loom very large.
Yeah, I mean, so certainly it would be trivial to ramp that up if one wants with newer technology,
our drive and motivation, or you could sort of tweak somebody to take an immense interest
in this stat or the other, some random game. So that certainly would be possible.
If we are thinking of things that boxes we could check with deep utopia, like certainly like,
oh, if pleasure is the thing, that would be easy. That's just kind of manipulating the
hedonic system in the brain. If the feeling of purpose, having motivation, like this kind of
jumping out of bed in the morning, if that's what we want. Well, that also is like something that
definitely we could do with like some simple neuroengineering if we wanted that. So the question
is not whether we could create a situation where we would have drives and motivations to do various
things, but also whether those things would be ones that are in some sense worth doing,
which brings us back to another set of constraints. So I mean, we've alluded to there's like some
ultimately limited finite pool of resources. And so if there were sufficiently large population
growth, eventually you would kind of run out of resources. That's like one kind of constraint.
There are obviously physical constraints like this, this speed of light, etc.
There might also be moral constraints that limit what can morally permissively be done,
even if it is technologically possible to do it. And I think one, maybe the most obvious area would
be if there are certain types of technological affordances that would require the instantiation of
minds with moral status. So for example, if somebody had a preference that would require
them to interact with another entity, but the only way to make that interaction fully realistic
would be to say, for that other entity to be conscious or to be a morally significant being,
then it might be that the first person's preference is to have certain types of interactions would
not be satisfiable, even in this kind of idealized situation. Because the only way to do it would
be for this other entity to come into existence and then to suffer or to have their preferences
violated and stuff. And yeah, there might be other more subtle ways in which there are kind
of moral constraints that to some extent limit what's possible to achieve in Utopia.
Pivoting away from that a little bit, you wrote Superintelligence Now What in 2014, was it?
Your previous book. A lot has happened in AI since then. And I'm curious whether
any of your viewpoints or beliefs that you put forward in that book have changed now that we're
in 2024 or 10 years later? Well, we have much more granularity and some broad
probability distributions can be made sharper now, obviously. We have more information about the
time scale. In particular, some of it has been rolled out. We don't have it yet. And also,
I would say overall, sort of towards the shorter end of the timeline distribution, not a huge
surprise, but a bit faster, I think. I think maybe slow and medium speed takeoffs have gained some
credibility compared to fast takeoffs. But I would not overstate that because it might
look quite moderate until you hit a certain point where you then might get these kind of
intelligence explosion effects. It's been quite striking to see the degree to which it has become
mainstreamed over a relatively small number of years. It might be easy for people near to the
field to realize just how fringy this kind of stuff was for so many years, like some science
fiction, just futurism or some crazy dude on the internet like having. And now it's like
like all the leading frontier labs have teams working on this explicitly.
You hear things coming out increasingly from top policymakers from the White House and in the UK,
Global AI Summit, etc., where some of these transformative impacts of AI are now very
much on the table. We only got connected to the field in like 2015. And even then, when we talked
to people, it was still quite French and sci-fi outside of like a select group of folks. Even
people that were working on machine learning at the time thought that those are kind of crazy ideas.
I can't imagine you were around since the 2000s basically or like even late 90s maybe
at that time, even more fringy. I'd be curious to hear a little bit, yeah, how was the progression
of your experience in talking to people outside of your group about AI safety over those 20-25
years? I think, I mean, probably it began to change a little bit with like the initial,
the revival of deep learning with sort of Alex Net and in particular deep minds kind of early
work and with the Atari thing. But then at that point still very fringy, but at least less than
for the previous two decades. And I think like, yeah, with the book and some other things that
came out around that same time, that was a kind of initial, you know, one eye opening a little
bit in the morning and then kind of being half open. And now I think more recently kind of the
alarm bell has rung and people are realizing that we've overslept a bit and it could come anytime
now or within a few years or who knows when it will come. But are we ready for this? And then it's
like kind of everybody's scrambling trying to put their pants on and like brush their teeth while
they are running and grabbing a coffee. And so it's a little bit like that. Most startling with AI
to a lesser but still significant extent with the other range of topics that
like FHI has been working on back in 2005 when it was created, it was like kind of the only place
in academia where people could think about whether it's like, you know, search for extra
life or future of nanotech or like all of these big kind of picture topics. And now there's a
broad ecosystem with a lot of specialized organizations and it's a lot of the concepts have
kind of with the EA movement and the rationalist movement and they've just kind of become adopted
and a lot more people are now able to think about these things in like kind of useful,
sensible way. You wrote the vulnerable world hypothesis now a few years ago,
which for those who aren't familiar with it, puts forward this argument that as humanity
progresses down its technology stack, that we keep sort of selecting these different balls
out of an urn. Imagine that each ball is a different technology. Most of these technologies
are great. They actually make our lives better and the world is safer, the world richer, etc.
But occasionally you might get a ball which is very bad. It's a different color. Humanity
creates a technology that is actually very dangerous and yet also easy to use. And so
like as power becomes more democratized, we could find ourselves in a world where we are actually
very, very vulnerable to some kind of catastrophe. So has your likelihood that we actually are in a
vulnerable world gone up or down since you voted? It's not gone down. I mean, it might have gone up
a little. I think particularly the more types of systemic instabilities,
the way that technologies, whether it's one singular technology or some sort of
technological frontier that moves in such a way as to reshape the incentives for a lot of people
that might affect what kinds of say collective thinking slash discourse we are able to perform
as a human species to evaluate different paths and what kinds of societies we want.
It seems like a relatively fragile thing to have a kind of sensible global discourse and
ability to self-correct and critique and gradually sort of increment towards the truth or towards
goodness in the way we think about things. It seems relatively easy to imagine how fairly small
tweaks on some settings on the way that social media platforms work or the instruments that
various sensors or propagandists have at their disposal to enforce orthodoxy, like some small
parameter tweaks to that. I mean, who knows what new equilibrium emerges. So there might be kind
of black balls of that sort that are relatively subtle, not like the sort of super nuke that
blows up the earth. That's another kind of constraint where you have like super destructive
in terms of nanotech or biological weapons and stuff, which is also a concern with advancing
AIs, like that they would lend themselves to some of these uses. But on top of that, these more sort
of subtle systemic perturbations that if we are unlucky, it could kind of knock the
like quasi-functioning equilibrium we currently have with all its faults. It still
goes on to something worse. And in terms of solutions, you laid out kind of
four different factors that could be helpful or like that would need to be improved in the paper,
suggesting that like prevent policing and more functioning global governance would work better.
But the first two, I think were differential technological development and kind of a reduction
of the strongly varied humans preferences. Any updates there?
Well, I mean, so this is another good case to resay what you said before is that you can have
a complex shape and look at it from one perspective and try to describe what you see.
It doesn't mean you think that side is the right one or certainly not the only side. And certainly
with this particular paper, I think there's people have misunderstood what I was like
thinking that that was kind of gung-ho about establishing some total world surveillance system.
I mean, I even called it. Yeah, I just wanted to say one idea is this concept of some like
something people could have around their neck that would record everything they say and also with
cameras recording their surroundings and what they do with their hands. I called it the freedom tag
and so I'd like to try to remind the reader that the ickness of this.
So I think why people miss it, but I just want to point it out because like in the
discussion of the paper, you do point out the clear scary downsides of having such a
panopticon kind of instilled. But I think people don't see that you do it kind of like a bit tongue
in cheek, the freedom tag, or even like to show how crazy it is. But people see it as...
You're advocating for it, which is obviously stupid.
Because how you approach problems when I read what you write often is,
well, here is the shape of the thing. I'll try to make you see this thing from multiple sides.
And if one looked at it from this side, then those are all the things you see. And from this side,
those are the things you see. And kind of without a judgment of this is how to look at it. And
because I explained it from this side to the fullest of my capacity, it means that I think
that this side is right. But we currently, many people have this kind of like, which side are you
on ultimately? It's like desire to just get that answer out of you.
I haven't written up and I thought through it, but like there's like some something about
thinking in super positions that I think there needs to be some sort of less wrong post about
this at some point or something. Which is, yeah, this like kind of allowing different systems of
thought to coexist in one's mind without feeling any precipitate need to choose between them.
Like, I mean, ultimately, if there's a specific question coming up, I mean, you'd have to
like assign probabilities or act, but especially if you're like, you know, a philosopher or somebody
who's like job it is, as it were, to try to reflect on things in a more comprehensive way,
then I think the ability to hold several worldviews in your mind, I mean, if not literally at the same
time, then at least kind of, you know, one week on one and one week or another, and then maybe
slowly you allow them either to, to merge or one to fade away, but not to feel the need to rush to
take a stand on everything. And sometimes allow yourself to be a little bit ambivalent and to
right, even contradict yourself might be better than a kind of forcing yourself to be consistently
wrong. Yes. Yeah, I mean, it's in some ways we similar to poker player thinking, because poker
players, you're encouraged to think in terms of probabilities, you're not certain that one person
is bluffing you or isn't bluffing you, you're just holding the different hypotheses concurrently
in your mind, apart from you are then pushed to actually make a decision and collapse those
that collapse the superposition. And you often then find out if the vulnerable world hypothesis
is true, then it feels like there are these kind of two attractor states which you allude to in
the paper, which are either this anarchic condition, this potential anarchy where
will eventually sort of self terminate in or at least create catastrophe. And the counter to
that therefore would have to be increased surveillance and centralization, and which of
course then leads you open to horrible tyranny forever. So what it seems like we need is some kind
of third attractor, I'm probably familiar with Daniel Schmacktenburger, he was one of the earlier
guests on this podcast, and he talks about the need of finding a third attractor state that
basically takes the best bits of the anarchic world and of the control-centric world, but then
minimizes the downsides of both of those. Have you got any further thoughts on ways with which we
can find or create that kind of third attractor state? So both of these surveillance slash
global governance obviously have a huge potential for dystopian abuses. I mean, you could imagine
at least if like that this would be wielded for good purposes, probably you wouldn't want to
rely on the hope, but rather try to create additional structures that would then control
these nodes of immense power and influence, so that they themselves would be accountable to some
other agency less likely to go wrong, like the will of the people or some more reflective version of
that, rather than the police or the secret service or whatever, the junta. But in principle,
like these would be tools of control and power and how they are used would then like be a father
parameter to plug into this. But I think, I mean, it's not, yeah, I like designing these
social systems. I think it's not, I mean, it's interesting these ideas for institution design.
I think like prediction markets seem, it's still a little frustrating that even in this day and age,
we've only like faffed around with it. And these little things with play money and stuff like that's
like the best humanity has in making a kind of manipulation resistant way of eliciting probabilities
for things. This should just be like the Bank of Norway should just create something like subsidized
10 billion a year invested in an underlying global portfolio so that you could have long term and
like bring down the transaction costs to basically zero and subsidize them and then
have some kind of process where different questions could be rendered into prediction
markets and have etc. And we just like kind of cut through all of this
distortion that people can just like twist and turn messages and have other, if we wanted like a
clear insight into what would happen if we did various things, it seems like we could just make a
huge global upgrade. But instead you get like some sort of tokens on some little online platform that's
like, I think we might have failed long enough with prediction markets that the problem may
solve itself. There was a recent paper by Jacob Steinhardt. I don't know if you've seen it where
he's using I think GPT-4 on metaculous questions of the past graded on like that have been resolved
and actually it already outperforms I think metaculous on many of the questions. So we might
have just been so bad for so long that kind of Oracle type AIs came around that will answer some
of these. Yeah, but then if people actually started to pay attention to that, like assuming it
actually works robustly and like if sort of anybody serious started to pay attention, if
governments started to do that, then probably they would like change the AI so that it wouldn't say
the things that the policymakers didn't want to hear. Like the thing with the prediction markets is
that there's like an incentive to being right that is hard to distort. Like you can distort the
market by pumping money into it, but you're basically giving them money too. It's not the
panacea and there are other ways in which that could create problems. I'm just mentioning it
as one example of something that is a little bit striking that there hasn't even been a very serious
effort to do that. But then even if you have some great scheme that you think like some institution
design, like it gets to this point where if it has a serious prospect of being implemented, then
you create enormous incentives for other power brokers to come in and corrupt the process or
steer it. So things that work on a small scale when nobody cares, don't necessarily work where
there are like powerful actors that would start to shape it. But yeah, no, I don't have a solution
to that. Speaking of markets, it feels like if we can find a way to make
market incentives more aligned with less classically market pleasing things like defense tech,
then that would be another sort of potential solution that's not like restricting technology
too much. And nor is it restricting the markets and so on. It's just allowing,
it allows humanity to continue pulling balls out the technological earn, but
we would be just putting more of our impetus into technologies, into defenses.
Yeah, I could see you. So basically, like you're pointing at, if somehow markets were incentivized
to follow differential technological development, meaning like first defense biased, and then later
the ones that actually were a bit more destructive technology or more dual use technologies.
Yeah, that'd be good. That'd be good. I don't know if we get them there. But I think in part,
that was for example, a hope that I had around some of the biosafety worries where it seems that with
engineered, like potential engineered pandemics in the future, we could reduce a lot of the worries
if we just had much, much better defense tech, better air purification, better antivirals,
better actually functioning vaccines on like a fast speed as well. Like then you could really
reduce, I think a lot of the worries. And maybe you could turn some of the potential, basically like
balls, you could change their color towards like a less, like more into a white ball.
Yeah, certainly does seem like the easy ones we want to do. Like similarly, we like technical AI
alignment. If we can accelerate that seems good. Like certainly physical countermeasures to new
pandemics, like whether personal protective equipment or UV sterilization or a continuous
environmental monitoring for like all of that stuff seems really nice. And similarly in other
areas we can do that, that seems all for the good. You've also thought a lot about and written about
digital mines. And when I think about them, it seems to me there is a relevant distinction between
the question of whether non carbon based mines can have consciousness or whether they deserve
moral relevancy. And to me, at least the answer to that seems in both cases pretty likely a yes.
And the less clear question of whether I myself or any human could continue existing
on a different substrate, which then starts involving questions around personal identity
and continuity, making it a bit less clear. And where do you stand on that?
Well, I mean, I agree. There are different questions. I think my guess is the answer to both
is yes, that you could have morally significant beings on digital substrate and also that you
could in principle become a digital mind. I think if you consider the like the gradual
replacement scenarios where it was some futuristic technology replaced one
nerve cell at a time with a cybernetic prosthesis that had the same input output
functionality to other neural cells. And so you wouldn't notice any difference. And
it's hard for me to think that the outcome of that wouldn't be you.
I agree with that view as well, that that's kind of different to this very like drastic,
singular upload moment that is sometimes described. And I think it also touches on kind of how
and that's a general thing that probably applies where identity
continuity is easier to imagine when something happens at like lower speed than at very fast
speed, right? Like theses ship is probably, I mean, arguably still theses ship, if it's
just bit by bit exchanged, and it creates relational kind of
identities with each other of the new plank now having relevance as part of theses ship versus
if, yeah, it's just like completely replaced and it never conferred kind of the
shipness onto the new plank. Yeah. And then what happens when the planks that were removed from
the original ship are used to make another ship, right? It's done. Yeah. So I think there are like
a lot of these puzzles of personal identity that comes up when if you imagine human minds being
copyable and stuff like that. It's I think not I mean, the fact that it's digital versus biological,
I don't think would be the key most important dimension there. I mean, for what we know,
we might already be very digital. And it would just be kind of another level of
digitality if it's like the way inside, like, I don't know, like these, what is it called,
the crafting game where you go around Minecraft or something, you can build like computers in there.
And then I guess you could run a bit. And the little Minecraft guys, if they had biological
brains inside their skull, that they might think, oh, if we do this, like, connect our
brains up and then transfer to this inside the Minecraft world built computer, it would be
so different. But like from our point of view, where we see they're already running
on the digital substrate, and it's just kind of going one level up.
Even separately from like whether you the I think the existence of conscious minds is like,
or moral relevant minds on the digital substrate is like, very much easier even to imagine for
most. And like one thing that often stands in the way is that we're kind of like, I think
anthropomorphizing, people say like you're anthropomorphizing AI, but I think we're also
anthropomorphizing the very concept of consciousness and assume that it's only kind of
our type of consciousness, the only one that morally is morally relevant somewhere else.
But that seems obviously silly, like you could very easily imagine completely different minds
that still require moral consideration. I mean, for what it's worth, I think it's a lean towards
thinking and consciousness is not even necessary for moral status. I think it might be sufficient.
I mean, certainly if you could consciously suffer, I think that would definitely give you
some moral status, but there might be alternative attributes that would make it so that it would
be wrong to treat you in certain ways. So, but I agree that the concept of consciousness seems
maybe intuitively very obvious at first glance. But if you look more closely, realize that it
seems kind of vague and multidimensional in all sorts of ways that and you could get there either
by, I think there are different routes to that insight. I think you could get there through
neuroscience or like kind of philosophical reflection, but you could also, I think,
people who have meditated a lot often come to a similar realization, like through this objective
path, so that like we have this kind of naive perception of what our conscious life is, which
is that it's kind of fully filled in continuous in all detail is there all the time and we know
what's going on and there is like us and then there is, but once you pay more close attention,
you realize that it's a lot less clear cut than that. Like it might, you might have degrees of
consciousness in multiple different dimensions. Your ordinary experience might be much more
fragmented and choppy and partial, like most of the things you think you're aware of, you're not
actually aware, et cetera, et cetera, all kinds of ways in which it should be. And so then I think
that once you get to that point, then it's more easy to imagine how that could be kind of weird
forms and types and partial types of consciousness. If we widen the purview to include these artificial
straights and including things that are on the borderline where it's not even clear whether the
word consciousness would apply or not. So yeah, it becomes much less of a binary where there is
a clear objective answer and a lot more like a rich space of possibilities. Yeah, I mean,
it doesn't mean that a rock now has a moral worth all of a sudden, but not sure about the rocks,
but yeah, maybe some versions of sand, I suppose. Maybe some.
So there we go. Thank you so much for tuning in, everyone. And massive thank you to Nick for
taking the time to talk to us through if you can take a moment to go through the show notes and
take a read of some of the papers that we referenced. In particular, the vulnerable world
hypothesis is an incredibly important paper that isn't arguably getting increasingly relevant
as our technological capabilities increase. Do also if you're feeling adventurous and are
particularly thinking around big picture, future designing stuff, check out Nick's new book on
Deep Utopia. Thank you for tuning in. And if you enjoyed this, please tell your friends and I will
see you next time.

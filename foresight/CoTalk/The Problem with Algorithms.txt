This is going to sound a little bit crazy, but I think that the free speech debate is a complete distraction right now.
I think the real debate should be about free will.
And we feel it right now, because we are being programmed based on what we say we're interested in,
and we are told through these discovery mechanisms what is interesting.
And as we engage and interact with this content, the algorithm continues to build more and more of this bias.
But the algorithm, even if it's open source, is effectively a black box.
You cannot predict 100% of the time how it's going to work, what it's going to show you,
and it can be moved and changed at any time.
And because people become so dependent upon it, it's actually changing and impacting the free agency we have.
And I think the only answer to this is not to work harder at open source,
or making them more explainable about what they're doing and why they're doing it,
but to give people choice, give people choice of what algorithm they want to use from a party that they trust,
give people choice to build their own algorithm that they can plug in on top of these networks,
and give people choice to have really a marketplace.
Two tech billionaires, Elon Musk and Jack Dorsey, with access to powerful social media algorithms,
are publicly admitting that our free will is at stake, and it's time to take note.
The algorithms that we program are programming us.
So we know this.
Algorithms build a bias of interest based on its own suggestions while disregarding anomalies.
Like a snake eating its own tail, the more something is suggested, the more it is consumed.
And the more it is consumed, the more it's suggested.
Dorsey admits that algorithms are a black box, and the outcome of this is unpredictable.
We don't have much choice right now. You can only use what's baked into a product,
interact with products anonymously, or not use the products at all.
Dorsey suggests giving users a choice on which algorithms they want to use,
and that this would alleviate the threat of autonomy.
But is this actually enough?
In a public presentation from Google's former head of behavioral science,
Oxford PhD Maya Shankar said this.
So the first principle is called social identity priming.
And what it says is that people tend to act in ways that are consistent with the identities
that they either currently associate with or aspire to associate with.
The Red Cross ran an experiment that leveraged this insight.
They were sending letters to people who had previously donated to the Red Cross,
and were appealing to them and asking them whether they'd be willing to donate again.
And they ran this A-B test, and what they found is that when they simply reminded the recipients of this letter
that they had been previous donors,
primed them for identity, for their identity as charitable, generous people with golden hearts,
they found that that identity priming led to a 30% increase in charitable contributions.
And impressively, priming people for their identity as donors actually made them more generous.
They increased the magnitude of their charitable contributions.
If something is shown to you and it aligns enough with your identity or goals,
you will subconsciously prefer it, whether that's choosing a specific product or taking certain actions.
In essence, we choose the things we like, and then we like the things we choose.
This phenomenon is commonly referred to as the Ikea effect,
or a cognitive bias derived from being directly involved in a process.
And in the case of Ikea, it's building your own furniture.
You chose the table from this catalog, you worked hard to assemble it,
and obviously you did the job right.
So this table is actually awesome, right?
There's also other studies out there that confirm this.
There was one study that was done on school kids, two different groups taking the same test.
The only difference was that one group was told that the test would be very difficult.
The other group was told that the test would be a breeze.
And can you guess which group performed better?
It ended up being the group that was more relaxed because they were told the test was going to be easy.
And Facebook did a similar study.
One group of people was exposed repeatedly to only negative posts,
while the other group was exposed to only positive ones.
What do you think happened?
The negative exposure made people post more negatively,
and the positive exposure had a positive effect.
I guess the point I'm trying to make is that we are more susceptible to our environment
and the power of suggestion than we're comfortable admitting.
And now these large companies that don't really give a shit about us are in control of that.
We gave that autonomy away.
Does it piss you off?
Because it should.
British academic and author Hannah Fry suggests that algorithms are increasingly playing a huge role in our society.
From courtrooms to hospitals and schools,
algorithms are making decisions behind the scenes that affect the way our society operates.
But it's not necessarily the small decisions we give up in our daily lives that bothers us.
We're more than happy to automatically schedule meetings in our calendars
or let our phones handle color correction and autofocus on pictures.
But there's a much larger and more nefarious play happening on the internet's algorithm chessboard.
The way that we behave as humans, we like to think that we're sort of wandering around with free will, should we say.
We like to think that we're independent and we're making decisions for ourselves.
But I think what we're saying more and more, the more data that we're collecting,
is that there's just these very clear patterns in the way that people behave.
And those patterns can be exploited and really I think used to sort of slightly change the structures that you put in place around humans.
And I think that that's something that applies completely across the board.
Using maths and physics and whatever data and computer science to understand human behavior,
you are not looking at a physical system.
You are looking at something that has individuals with autonomy, like rich and varied lives.
So for example, in London where I live, I couldn't tell if my neighbor was going to go into work one morning or not.
I couldn't make that prediction. I couldn't make a prediction about what time they might jump on the tube, the underground.
But when you kind of scale out to the size of an entire city,
actually you can make really, really accurate predictions about how many people overall will choose a certain path,
will pick a particular route and if something changes how the whole population might end up reacting.
Back in 2002, after another day of analyzing credit card numbers and corresponding purchases,
the box store giant Target began to notice a spike in female customers buying unscented body lotion.
These same customers had historically bought up vitamins and supplements such as calcium and zinc.
And a little further along this retail timeline, it was eventually revealed that they would sign up for an in-store baby shower registry.
Target saw dollar signs. They ran an algorithm that would score its female customers on the likelihood that they were pregnant.
And if the probability score was high enough, they would send out a series of coupons for baby shower products to the expectant or unexpected mother-to-be.
This went on for a while until one day a man stormed angrily into a Minneapolis target location, demanding to speak with the manager.
His teenage daughter had been receiving pregnancy coupons in the mail and he was very upset that Target would be making assumptions about his daughter's extracurricular activities.
The manager of the store apologized profusely and called the man's home a few days later to reiterate the company's regret about the entire affair.
But by then, the father had an apology of his own to make.
I had a talk with my daughter, he told the manager, and it turns out there's been some activities in the house that I wasn't completely aware of.
She's due in August.
This story was published by The New York Times over 20 years ago, and since then you can imagine the sophistication of our algorithms and data capture have greatly improved.
Combining these methods with the emergence of AI learning models and you've got yourself a Molotov cocktail of hallucinating predictions and hyper-targeted marketing schemas.
If any of you have been on the internet lately, chances are your digital footprint has been left behind.
Every app, every device, every website, every interaction is logged and contributed as a data packet.
These data packets are then categorized and monetized by large data brokers.
Oracle is one of them.
Everything you are and do is made into a data profile of your digital self, your shadow, and it has been resold thousands of times.
Businesses in all industries are watching you.
They're learning from you.
They're adjusting their business models because of you, and they're constantly calculating your worth as a consumer.
Most of today's digital advertising takes place in the form of highly automated real-time auctions between publishers and advertisers.
This is often referred to as programmatic advertising.
When a person visits a website, it sends user data to a variety of third-party services, which then try to recognize a person and retrieve available profile information.
Advertisers interested in delivering an ad to this particular person due to certain attributes and behaviors make a bid.
And within milliseconds, the highest bidding advertiser wins and places the ad.
In whatever corner of the internet you've used, hiding in the background, these algorithms are trading information that you didn't know they had and never willingly offered.
They have made your most personal private secrets into a commodity.
Not only that, data profiling as a result of hyper-targeting has shown many biases.
Here's some real-world examples of biases as presented by IBM.
For example, computer-aided diagnosis or CAD systems have been found to return lower accuracy results for black patients than white patients.
Amazon stopped using a hiring algorithm after finding out that it favored applicants based on words like executed or captured, which were more commonly found on men's resumes than women's.
Google's online advertising system displayed high-paying positions to men more than women.
Academic research around image generation found that the application mid-journey showed bias when asked to create images of people in specialized professions.
It would show younger and older people, but the older people were always meant.
AI-powered predictive policing tools within the criminal justice system are supposed to identify areas in which crime is likely to occur.
However, they often rely on historical arrest data which can reinforce existing patterns of racial profiling and disproportionate targeting of minority communities.
So, as we consider the implication of commerce-driven algorithms, which also largely drive our social media consumption, the idea of what we like is actually being reinforced by what we chose, leading to similar offers of what we like.
This means that we can be influenced by others telling us what we should choose and if we're inclined to believe it, this could be enough to tip the scales on life-changing decisions such as whether or not to get an abortion or whether or not to attend flight school and become a pilot.
Or, go to the doctor and get that weird mole checked.
Not only that, companies are using your behavioral patterns to predict what you'll do next and they're ready to serve up your next favorite product or experience.
So, some of this seems harmless and well-meaning until we consider, again, those biases of the algorithms.
I don't want to live in a world where law enforcement makes non-evidence-based AI predictions on whether or not someone is a terrorist or is going to re-offend if given parole.
Algorithms on social media are siloing us into categories and trying to keep us there as long as possible.
By feeding us what it knows we already like, it's actually doing us a disservice.
So, as if the societal implications I've already listed aren't enough to scare you, what else are we missing out on?
Well, the short answer is anomalies.
Open-mindedness.
I'm speaking creatively now and putting some of that doom and gloom to the side.
We want new fresh content, new interesting and challenging information that contradicts our views on life as we know it.
Growth happens when we're uncomfortable.
And if we sit in our bubble and let an algorithm feed us what it knows we already like, then we are not in a growth zone.
We are not in a growth mindset.
We're sitting there waiting for data brokers to pin us down, label us, and offer us a product that they think we'll buy.
Don't be stagnant.
Be someone who listens to the opinions of others, even if you hate it.
Also, if you want to ditch the YouTube algorithm, go check out GreyJay.
It's still in development, but it was made by one of my favorite people, free-thinking evangelist, Louis Rossman,
who's a huge YouTuber, and if you haven't heard of him, go check out his channel and go subscribe.
And if you already know him, then you know that the app is going to be done correctly.
With that being said, algorithms themselves are not inherently bad.
It's all in how you use it.
After all, from the words of Hello World by Hannah Fry, GPS was invented to launch nuclear missiles, and now it helps deliver pizzas.
So go pick yourself up a copy.
That's where the target story came from.
And this book is also stocked to the brim with many other examples of mass surveillance, behavioral predictions, and it goes so, so much deeper than this video.
Amazon link is in the description.
I just like the book.
This is not an advertisement, and I do not get affiliate payments off of this whatsoever.
I'm just a fan.
And some final thoughts from the Oxford PhD Maya Shankar.
When people were allowed to tweak an algorithm even slightly, they were more satisfied with the results of the algorithm and actually opted to use that algorithm over other algorithms that they knew performed better but had not involved their input.
So it's pretty irrational behavior, right?
But it shows that there's some egocentricity here that matters.
Like, we really value our own contributions.
So does this mean we're really in control, or are we just satisfied with choosing the algorithm that controls us?
And will we start living in a post-AI age that is pre-deterministic?
If so, by how much?
I'm Austin with The Kotak Podcast over and out. Peace!
.

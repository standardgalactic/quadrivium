start	end	text
0	4720	The stated aim of a company like OpenAI
4720	8240	is the development of artificial general intelligence.
8240	10640	Now, artificial general intelligence
10640	15160	is in Google AI terms,
15160	17000	the equivalent of human intelligence.
17000	19440	What I kind of want to point out here,
19440	22920	when you look at what a general intelligence is,
22920	26320	then that's actually rooted in Charles Spearman
26320	28000	and the idea of the G factor.
28000	31560	But Charles Spearman was a hygienicist.
31560	35560	And his reason for developing this ranking
35560	39360	of general intelligence was to rank human intelligence
39360	41360	for selective breeding, et cetera.
41360	46000	So you've got this drive for artificial general intelligence.
46000	49200	But when you actually work out what general intelligence is,
49200	51760	it's got some very, very dark histories.
52400	58520	Hello and welcome to Planet Critical,
58520	60960	the podcast for a world in crisis.
60960	62240	My name is Rachel Donald.
62240	64800	I'm a climate corruption journalist and your host.
64800	66360	Every week, I interview experts
66360	68480	who are battling to save our planet.
68480	71360	My guests are scientists, politicians, academics,
71360	73280	journalists and activists.
73280	75840	They explain the complexities of the energy,
75840	79640	economic, political and cultural crises we face today,
79640	81400	revealing what's really going on
81440	83600	and what they think needs to be done.
83600	86120	These are the stories of the big picture.
86120	89640	Go to planetcritical.com to learn more and subscribe.
89640	91960	My guest this week is John Wilde.
91960	93520	John is a London-based artist
93520	96480	who works across performance, sound, text, code,
96480	98240	electronics and machine learning
98240	99920	to research the future's imminent
99920	101520	within digital technology.
101520	103480	John joined me today to talk about
103480	106040	culture and artificial intelligence,
106040	110160	how the stories we tell ourselves inform our technologies
110200	112480	and then how those technologies inform the stories
112480	113560	we tell ourselves,
113560	116760	getting caught in these kinds of circular loops essentially,
116760	118280	which make it increasingly difficult
118280	120080	to imagine a different way of being.
120080	122120	John talks about this in relationship
122120	123840	to artificial intelligence.
123840	125840	Artificial intelligence is an incredibly
125840	127480	energy-hungry technology.
127480	130080	It is being used for profit motives.
130080	131680	We have very little understanding
131680	134360	of what it could do when unleashed upon the world
134360	137160	and at the moment, all it's doing is threatening jobs
137160	139360	rather than creating new ways of being.
139360	141360	John's research shows what we could do
141360	144360	if we imagined using mycelium as a framework
144360	148000	for developing something decentralized, interconnected,
148000	150280	entangled and symbiotic.
150280	153280	To begin with, John explains the history of thinking
153280	154680	and artificial intelligence.
154680	158320	How Silicon Valley is infused with theories and stories
158320	160920	that came out of Russia in the late 19th century.
160920	164680	The desire to pollinate the universe with consciousness,
164680	167000	creating a hierarchy of consciousness,
167000	170280	as if humanity is the only thing that is truly conscious
170280	172040	or would be able to do such a thing
172040	174600	as if the universe isn't already conscious.
174600	176680	And he also explains how this hierarchy
176680	178920	of consciousness or intelligence
178920	180240	that is directing Silicon Valley
180240	182120	to make an artificial general intelligence
182120	184360	comes out of eugenicist thinking.
184360	186480	This is such a fascinating conversation.
186480	188400	We had so much fun recording this.
188400	189960	I knew a fair bit about AI
189960	192360	thanks to research into the effective altruist movement,
192360	194720	but I did not know the history that John lays out today.
194720	196080	And understanding more of that history
196080	197320	makes me really grateful
197320	199560	that people like him and artists around the world
199560	201080	and technologists around the world
201080	203640	are trying to think about how to develop permacomputing
203640	205280	or the wood-wide web,
205280	209120	collaborative, interdependent, entangled projects
209120	211880	that reflect the intelligence and harmony
211880	213600	of natural ecosystems
213600	215680	in order for us all to live more sustainably
215680	216520	with one another.
216520	218120	And we end our conversation with a dialogue
218120	219600	on exactly that.
219600	221240	What is sustainable computing
221240	223480	and what is sustainability more widely?
223480	225080	I hope you all enjoy the episode.
225160	227480	If you do, please share it far and wide.
227480	228680	And if you're loving the show,
228680	230600	become a patron on Patreon
230600	231800	or support Planet Critical
231800	235040	with a paid subscription at planetcritical.com.
235040	237360	By signing up, you'll get the Planet Critical newsletter
237360	238800	inspired by each episode
238800	241040	delivered straight to your inbox every week.
241040	242200	You'll also have access
242200	244640	to the wonderful Planet Critical community
244640	248400	who are full of inspiring thoughts, ideas, critiques
248400	249960	and determination.
249960	251920	The links are in the description box below.
251920	253600	I'm so grateful to everyone
253600	255200	who chooses to support the project.
255200	256720	I'm a vehement believer in ad-free
256720	258160	and open-access content,
258160	259840	so Planet Critical wouldn't exist
259840	262760	without the direct support of the amazing community.
262760	264520	Thank you so much to all of you
264520	266320	who believe in Planet Critical
266320	268360	and keep the project going every week.
268360	269680	John, thank you very much for joining me
269680	270520	on Planet Critical.
270520	272600	It is a pleasure to have you on the show.
272600	274200	Well, thank you for inviting me.
275600	276440	Happy to.
276440	278040	As I was saying before we started recording it,
278040	279520	such an interesting conversation with Maggie
279520	280560	who platformed you.
281560	284240	And I think that speaking with artists
284240	285760	is a really critical component
285760	288480	to understanding what the hell is going on in the world
288480	291160	and what we can do about it,
291160	293000	which leads me to my first question.
293000	294800	Why is the world in crisis?
296640	298240	That's such a big question, isn't it?
298240	299080	I know.
300840	304080	I mean, I'm going to start with a report
304080	307440	which was out last week, which really struck me.
307680	312680	So I read an article by Duncan Agnew in Nature Magazine,
313040	314680	which suggested that climate change
314680	317120	is having an effect on universal timekeeping.
318240	319080	What?
319080	322640	So Coordinated Universal Time, or UTC,
322640	324360	is the primary time standard
324360	326360	globally used to regulate clocks.
327360	330880	So UTC closely follows the rotation of the Earth.
332120	335960	But accelerating melt from Greenland and Antarctica
335960	340360	is adding extra water to the world's sea.
340360	343080	It's redistributing mass around the globe,
343080	345320	and that's causing a very slight slowing
345320	346680	in the Earth's rotation.
348280	353160	And if you combine that with what we know
353160	357440	about the shift in the Earth's poles.
357440	360040	So since the 1980s, it's been shown
360040	362240	that the massive melting of glaciers
362240	365000	as a result of global heating
365000	369600	has caused a shift in the Earth's axis of around four metres.
369600	373440	So I think if we think about this question,
373440	375040	the shifting of the Earth's axis
375040	377840	and the slowing of the Earth's orientation
377840	382680	mark both an impressive, to be honest,
382680	386360	but terrifying achievement of human,
386360	388920	global, well, geoengineering.
388920	393280	The impact that we've had as basically fundamentally
393280	397200	has shifted time and the axis of the Earth.
397200	400960	And that seems to be the ultimate mark of the amphipersine.
400960	403680	But I think what troubles me with this,
403680	405280	well, I mean, there's lots of things
405280	407240	which troubles me with this,
407240	410440	but such a feat could only be the outcome
410440	414200	of sustained and coordinated human action and interaction.
415200	417760	Yet no one's planned, organised,
417760	421280	voted for, or even imagined such a venture.
424280	426320	And I kind of wanted to start this conversation
426320	431800	with a kind of provocation which comes from my own research.
433480	437680	My own research is looking at artificial intelligence,
437680	440440	specifically narratives around artificial intelligence.
442040	443880	But what is the coordinating force
443880	446440	that's playing a role here?
446440	449720	And I think the provocation that I want to put forward
449720	453200	is that it's a non-conscious intelligence.
453200	455960	Or an artificial intelligence that we call the market.
457120	461080	I think the market struck producers emergent forms,
461080	464960	which you could call a form of non-conscious intelligence.
464960	467440	Yeah, I totally agree.
469280	471440	What a way to kick us off, by the way.
472720	474400	Wow, I didn't know that,
474400	477680	but climate change has been an impact
477680	479680	on universal timekeeping.
479680	482360	I was thinking about this question that you asked
482360	486960	as this appeared in my feed, and I'm like, wow.
486960	491240	But not only that, it's that universal time,
491240	496240	it affects computing because the computer programmes
496760	501000	made to keep track of universal time
501000	505160	are going to struggle with this slowing down of the Earth.
505160	507280	So it kind of comes into the territory
507280	509560	that I'm also interested in, in a way.
512960	514400	There's so much there, isn't there?
514400	519400	Like the idea of having a human system
519480	522080	mapped onto a natural system,
522080	525320	the human system impacting the natural system,
525320	526560	and then the natural system,
526560	530800	and then being unable to deal with the fallout,
530800	532160	to deal with the consequences,
532160	534680	to understand even the new reality,
534680	539040	because the limits of that system were so fixed and rigid,
539040	544040	which is kind of a really classic feature of modernity.
545040	547480	Like there just being no flexibility.
549160	552800	And then watching reality as we understand it,
552800	555800	just kind of peel away in that moment,
555800	557400	because the systems aren't built for it.
557400	561600	So it really reveals this thread of domination
561600	564920	that runs through modernity, like domination over nature.
565920	569400	It doesn't work, the domination over ourselves.
569400	570640	It just doesn't work.
570640	574600	There is a, it's brittle and it's fragile and it will snap
574600	579040	if it's met with enough kind of shifting, evolving resistance.
579880	582720	I guess the challenge that we have
582720	585760	is if we understand this as a form of,
585760	587120	or if we understand the market
587120	590480	that produces these kind of emergent forms,
591760	593480	as a kind of structural system
593480	598800	that has an intelligence that structures human activity, et cetera.
598800	601520	How do we, how do we move beyond that?
601520	606160	How do we imagine futures, which are structured a different way?
606160	610200	How do we imagine technologies, which are,
612440	616080	which behave in a different way, which are sustainable?
619160	623280	My own research actually looks at the kind of imaginaries
623280	625400	around artificial intelligence.
625400	630320	And I think they can tell us quite a lot really about why we end up
632200	635920	kind of looking to the stars rather than looking to the soil,
635920	638160	rather than looking to the earth.
639360	641360	That's beautiful.
644080	647800	On this artificial intelligence,
649880	652200	I mean, this is kind of what Hayek spoke about as well.
652760	655920	The invisible hand of the market directing people.
658160	660760	The sort of godfather of neoliberalism, essentially.
661520	663720	And yes.
663720	669200	I think you were Smith as well, weren't you, in the wealth of nations?
669200	673800	I think Smith initiated it, and then people kind of took on this idea.
673800	675920	But yeah, the idea of the invisible hand.
676680	677880	Which is interesting, isn't it?
677920	684080	Because there's a concept there of this physical thing being shaped.
684080	686520	But they weren't talking about a brain.
686520	689080	They weren't talking about the invisible brain.
689080	692160	Whereas what is directing that hand to move?
692160	694560	The idea is that it would respond to needs or whatever.
694560	697480	And it's obvious that's obviously not been the case.
697480	702640	Like we've sort of created a system that is impacted,
702640	706320	but also impact its environment around it as it accumulates more
706360	709360	historical precedent and knowledge.
709760	714000	And it's just embodied really with historicity, you might say.
715160	718440	Kind of like self-perpetuate itself and grows and grows and grows.
718920	722000	Well, I think that self-perpetuation is the thing.
722360	726440	I think where this kind of connects with the kind of research
726440	729920	that I do on artificial intelligence, it's kind of like looking at what
729920	733360	intelligence is in some sort of way.
733920	738760	And obviously with the kind of common sense for you is this kind of
738760	744480	conscious intelligence, the kind of the human conscious intelligence.
744760	748840	But conscious intelligence is very rare in the world.
749800	751840	Most forms of intelligence that would come across
751840	754600	are a form of non-conscious intelligence.
755240	758160	So this is kind of sensing and acting on the world
758160	760920	in a way that produces very complex outcomes.
761720	766560	But but don't don't have but don't have at the car this kind of
766560	771440	conscious drive that maybe maybe language produces in humans.
772160	774240	OK, and we pause there.
774240	777480	Yeah. So conscious intelligence is rare in the world,
777920	780240	but this unconscious intelligence is sensed.
780240	782560	Is that the word you used?
782560	786680	Well, I'm saying that for something to act, for something to act,
786680	789400	there's some sort of sensing, some sort of information.
790320	793240	And then there's a behavior that responds to that,
794440	796440	which produces complex outcomes.
796640	799440	So I'm thinking I'm thinking as a good example,
799440	803160	or an example is quite often used as a slime mold.
804000	807400	And in my own practice, we've also been using mycelium,
807400	809880	but slime mode is quite a common one.
810440	815240	So slime mode is a single celled algorithm organism,
816040	820680	which it basically produces filaments
820680	823320	which stretch out to find food in all directions.
823320	825800	And then when it finds finds food,
825800	829200	it solidifies the filaments that is produced.
830240	834920	And it's it's been used to mimic the Tokyo.
836760	838240	Tube map.
838240	844240	So the Tokyo tube because of the kind of geology of the area, etc.
845520	848440	The planning of it is there's quite a lot of complexity
848440	851480	to how to produce the most direct routes.
852320	856280	But by by creating an artificial map of the tube
856280	859400	using the food for the slime mold,
860280	862320	the slime mold managed to
863560	865800	calculate the most direct routes,
865800	871240	which pretty much mimic the the actual Tokyo underground.
871680	874280	So that's that's the way that you could see
874280	878440	that there's a non-conscious intelligence working.
879280	883560	And NASA has used exactly the same model to map
883560	887200	to map the dark matter that holds together the universe.
887720	890880	So these kind of intelligences,
890880	893360	which aren't a model of conscious intelligence,
894480	898120	still produce very complex behavior in the world.
898560	900480	And I'd go.
900520	902680	Sorry, it's just I suppose I'm getting stuck
902680	906000	on this conscious unconscious binary.
907760	910800	Because what we're talking about then
910800	912960	when we talk about consciousness,
912960	915360	because there's quite a lot of, you know,
915360	917240	stuff now coming out of physics
917240	919360	and other sort of theories that suggest that,
919880	922240	well, everything is just consciousness
922240	926040	and that perhaps it's consciousness that predates matter.
926560	928680	And thus, you know, perhaps the slime mold
928720	931400	doesn't have a brain in the way that we
931400	932400	well, definitely doesn't, right?
932400	933400	It's one cell.
934640	938280	But that doesn't necessarily mean it's it's not conscious.
938560	940440	Like, I guess I'm concerned.
940440	941600	Yeah, I agree.
941600	943600	I agree.
943600	944240	I agree.
944240	946480	And it's exactly the hierarchy of consciousness,
946480	948280	which I want to break down.
948280	949080	Right.
949080	955040	I mean, I'm using the term conscious in this way.
956040	961360	As a relationship to language and the the modelling
962360	964760	of the world as an abstract ship.
965680	968520	Of which then things are planned.
968520	970640	But I don't believe this is how humans behave.
970640	974240	I think humans, the vast majority of human activity
974240	975760	is non-conscious.
976880	979800	Like, ride a bike, you don't have to do
979800	983920	with the mathematical calculations to stay on the bike
984800	986840	and direct and route, et cetera.
987680	990760	I think the vast majority of action is non-conscious.
992040	994880	But wouldn't that suggest then that consciousness
994880	999320	is only these kind of consciousness is language
999320	1001720	because mass, for example, could be, you know,
1001720	1004360	understood as like a language for understanding the universe
1004720	1007600	or other laws for which words don't quite
1009440	1011240	aren't quite useful.
1011280	1014480	And so does it not then become that consciousness is language
1014480	1016520	and everything else is non-consciousness?
1017720	1020360	I think so in the way that I'm trying to say it.
1020760	1024880	But the reason the reason why I'm going down this rabbit rabbit hole
1025320	1027840	is because of the drive of an artificial intelligence
1027840	1030440	to develop what they call AGI.
1031040	1034400	So like, which is a general intelligence,
1034400	1039680	which is trying to trying to mimic human reason in some sort of way.
1040000	1043120	But what I'm kind of arguing is against that
1043120	1045480	in favour of something closer to
1046520	1051160	accepting the intelligence that exists in all species
1051160	1053320	and plants, et cetera, on the earth.
1054720	1058520	And then recognising the importance of that kind of intelligence.
1059320	1062840	So I'm kind of trying to make an argument in opposition
1062840	1065480	to the artificial intelligence
1066480	1069280	that drive towards AGI.
1070960	1074000	So assuming this comes back to your beautiful line,
1074000	1078920	you know, wondering why humans look up at the stars and not the soil.
1080600	1083600	Which I think we should unpack as well in relation to this.
1083600	1085760	So please.
1085760	1089440	I mean, I mean, I think a good way forward to that is.
1090640	1091640	I mean, I.
1092000	1096440	It's probably good to introduce myself a little bit in that.
1097520	1102280	My own research explores artificial intelligence and real world narrative.
1102280	1105080	So I'm actually interested in the imaginaries
1105320	1108320	and the relationship between storytelling and imaginaries
1108680	1110680	and how that.
1111720	1116960	As a cyclic causality with technical production itself.
1117400	1119360	When computer scientists.
1120280	1123160	Bring something new into the world, it's a creative act.
1123160	1126000	It's an act of futurism or future, is it?
1128000	1132400	Like you've got to you've got to think in the future.
1133560	1135960	To babes to produce technology in the present.
1136360	1140080	So there is a creative act involved in that.
1140080	1144720	And that's the kind of creation of narratives or imaginaries.
1145120	1152040	And this as an impact on on technical production,
1152040	1156520	the technical production, like what what is possible as an impact on imaginaries.
1157040	1161440	So so you have a cyclic relationship between the creation of
1163000	1167320	kind of speculative imaginaries and actual technical production.
1167720	1172480	So the two things are different in in technical production is
1172640	1177280	technical production is rooted in in the constraints of the present
1177800	1182120	in the regulatory framework and politics and ethics, etc.
1182440	1186400	Whereas imaginaries are that kind of creative leap
1187160	1192040	into the future that that are used by developers
1192600	1195640	to basically order.
1195640	1201680	Like to create goals really for for the technologies that get produced.
1202680	1205520	And by looking at the kind of imaginaries,
1205520	1209240	the kind of stories that circulate within tech communities,
1209760	1214160	then you can get a sense of where the technical development is going.
1214640	1217080	Is kind of what I'm arguing.
1218840	1221000	Go on, do you have any good examples?
1221000	1223840	You said I just wanted to make sure that that made sense,
1223840	1226240	that relationship between the two.
1226240	1226800	Oh, definitely.
1226800	1230200	I think it's just much in the same way
1230200	1233400	when like scientists come on and speak science.
1234600	1236640	There's a lot of us here that are laymen
1236640	1238200	and I think breaking it down
1238200	1240800	of some sense quite academic language is helpful.
1241680	1244880	To talk about how these two things inform each other all the time.
1245760	1247520	So yes, I have a better understanding now.
1247520	1248360	Thank you.
1248360	1252040	I think when we're looking at developer narratives,
1252040	1257280	so the kind of ideas which are driving tech developers.
1257560	1259120	I mean, these people don't normally come
1259120	1261040	from a creative background.
1261040	1264360	So so where where do the graph grab the imaginaries?
1264360	1266960	Is is quite an interesting thing.
1266960	1272320	And what my research has found is that a lot of these imaginaries
1272320	1275240	are driven by, I suppose, obviously sci-fi.
1276400	1277400	But.
1278640	1282200	More specifically by the kind of speculative avant-garde movements
1282200	1284720	which circulate in tech circles.
1284720	1287560	So to name a few, there's Cosmism,
1287600	1292960	Transhumanism, Extra-Pianism and Effective Accelerationism.
1293520	1297560	Oh, what are they exactly?
1298640	1302840	So like if you delve into tech communities,
1302840	1306920	you come across these kind of like quite far out
1306920	1308840	and fascinating ideas.
1311280	1315280	But what struck me is when I started when I started research
1315280	1319640	in this territory is the massive impact that Cosmism has had.
1320000	1323800	Now, Cosmism was a movement which developed in Russia
1324200	1327600	at the end of the 19th century in the beginning of the 20th century.
1328000	1332960	So to discover that these ideas from from this period
1332960	1336240	from pre the Russian Revolution or around the Russian Revolution
1337920	1340960	currently has a massive impact on
1341840	1345560	on AI and tech developers in Silicon Valley and California
1345560	1347280	is a little bit, whoa, really?
1348160	1349160	But.
1350040	1355280	So if I dig a little bit deeper into this into the ideas of Cosmism,
1355280	1360520	it I think I think where your tech is is kind of
1360920	1364640	to answer that question of why the developers looked at the stars.
1365640	1370640	So Cosmism emerged in Russia at the end of the 19th century
1370640	1372280	in the beginning of the 20th century.
1372800	1376640	And one of the key figures was a guy called Nikolai Fedorov.
1377640	1382560	And Fedorov connected his kind of quite strong
1382560	1387520	Christian beliefs with a with a futurism.
1388640	1392760	And he thought he believed that the common task of humanity
1393160	1394600	was to end death.
1395920	1399200	So to end all death to move towards immortality.
1399640	1401240	Very good. And.
1403840	1406800	And this wasn't enough
1407280	1410600	because this betrayed the older generations.
1410960	1413720	So the first step is to kind of end death.
1414240	1417840	But once you've achieved that, then the next step is to resurrect the dead.
1418840	1424320	That's that's that's the duty of all good son.
1424480	1427200	Sons is to resurrect their fathers.
1427680	1430320	That's the language, the language he used, not mine.
1430920	1433840	I just sorry, just a very, very quick side note.
1433840	1436400	But it's just fascinating to me that this man, for example,
1436400	1437960	wasn't burned at the stake.
1437960	1440080	It sends an awful lot like sorcery.
1440520	1442400	Imagine if that had been coming out the mouth of a woman.
1442400	1445000	Hey, please continue.
1445440	1449800	But anyway, you've got to remember, my interest is the relationship
1449800	1453040	between like imaginaries and technology itself.
1453400	1458160	Now, one of his students was a person called Constantine.
1459240	1461640	My Russian is appalling, so please forgive me.
1461640	1465480	Any listeners who speak Russian, but Sayel Sayelkovsky.
1467040	1469960	So Sayelkovsky.
1470960	1477600	He took on a lot of the philosophy of Fedorov, Fedorov, so.
1480360	1482640	But he took it in a very practical way.
1483720	1488400	So Sayelkovsky studied kind of the physics of his time and etc.
1488920	1492080	And he developed some of the first practical divide
1493120	1498800	designs for the space rockets and the equations
1498880	1501400	required to for space travel.
1502040	1504760	And he did this in 1896.
1506360	1510480	So this these kind of like developments in kind of the technology
1510480	1513120	of space travel emerged from.
1514800	1520160	Following Fedorov, realising that if you ended death.
1521440	1526840	And resurrected the dead, then the planet would get overrun quite quick.
1527200	1532280	So it was so it becomes necessary to leave the cradle of the earth.
1534120	1536720	Does that make sense in the logic?
1540440	1542080	As logic, sure.
1545600	1548040	OK. So.
1549600	1551760	The reason this becomes interesting is because
1552400	1558360	Sayelkovsky is basically the founder of the Russian space program
1559160	1561520	and the former Soviet space program.
1561520	1567160	And his rocket designs are currently like
1568080	1570560	I'm not not exactly the same, but
1570960	1574960	but are the forefathers of our current rocket design.
1575360	1577360	So you've got this link between kind of.
1578360	1581080	Quite fascinating and crazy.
1583080	1585680	Imagineries, so futurist imaginaries,
1586400	1592320	linked with technology, which ultimately developed the US space program.
1593280	1595440	But how does this link with Silicon Valley?
1596640	1601560	Well, if you look at, say, Ray Kurzweil said,
1602400	1608560	you know, Ray Kurzweil was kind of the profit for Google's AI program.
1609440	1610440	Is.
1612240	1614520	I think he's probably a chief engineer.
1615840	1616840	But.
1617640	1619560	He also believes in.
1621120	1622680	Moving towards immortality.
1622680	1625200	He wanted to be the first person to kind of end death.
1626320	1629520	So they've like a lot of these ideas
1629520	1632320	that came from Cosmism have been translated
1632320	1637000	directly into the kind of AI tech circles which circulate.
1637760	1642120	So so Kurzweil is a serious technical.
1643560	1646960	Player within the AI world, particularly in Google.
1647760	1653240	And this idea of extending life or eradicating death
1654240	1658320	is part of the discourse which circulates within within this community.
1659320	1662680	That that would be a kind of group
1662680	1666840	in which called themselves extra extra pianism,
1667440	1670480	extra pianist, so that's not sure how you say it properly.
1671120	1675040	But these ideas link directly to actual technical production.
1675400	1680320	So so things like the Fitbit and the quantitative self movement.
1680320	1683760	So the idea of like monitoring your health and maximising health,
1683960	1687200	which you must have come across because that's part of the kind of like tech
1687600	1689840	scene, human optimisation.
1690560	1691760	Exactly.
1691760	1697520	This human optimisation comes out of this attempt to extend life and eradicate death.
1698040	1701160	So you can see how the kind of Cosmism has kind of like
1701680	1706920	been kind of plagiarised really right into these kind of like tech ideas,
1706920	1712040	which then find themselves been sold on Amazon as Fitbit.
1712040	1715480	So various other optimisation technologies.
1717720	1722000	Kurzweil himself, in an interview in a film called
1722280	1725240	What was it? Are you man?
1726720	1731320	Declared that one of his driving force for developing artificial intelligence.
1731320	1733440	And you got to remember that this is a chief engineer.
1734360	1736640	Is to resurrect his own father.
1737040	1739800	Oh, my God.
1739800	1744200	So so so you've got Federer repeating himself
1744640	1748440	right at the top of the kind of Google development chain.
1750280	1751960	Oh, God.
1751960	1755240	And and taking a kind of slightly
1756360	1758320	a slight side move here.
1758320	1760560	But when we talk about artificial intelligence,
1761720	1764520	in tech circles, it gets broken down into
1766200	1767480	three different areas.
1767480	1770680	The first one's narrow artificial intelligence, which is what we
1770840	1774000	what we have at the moment, which.
1774040	1776520	It's mainly what we call machine learning.
1777680	1782760	So it's narrow in that it can do very intelligent
1782760	1789080	activities such as playing go or chess or predicting
1789400	1792400	texts, but in a very narrow domain.
1794120	1798760	But the next, like the state of them
1798760	1802200	of company like open AI
1803160	1806600	is the development of artificial general intelligence.
1807840	1811080	Now, artificial general intelligence is
1812000	1817560	in in Google AI terms, kind of the equivalent of human intelligence.
1818000	1821280	So it's this this ability to abstract
1821280	1823920	and apply intelligence to multiple domains.
1824520	1827040	So so it's wider.
1827440	1831520	But what what I kind of want to point out here
1833480	1837000	is that this idea of a general intelligence, which is
1838160	1842280	is what people are striving for, an artificial general intelligence.
1842280	1845120	When you look at what a general intelligence is,
1845720	1848800	then that's actually rooted in what
1849840	1853000	in the statistic statistician,
1853480	1856320	Charles Spearman and the idea of the G factor.
1856320	1859760	But Charles Spearman was a hygienicist
1860600	1862400	and his reason for developing
1863840	1870800	this ranking of general intelligence
1871960	1875760	was to rank human intelligence for selective breeding, etc.
1876040	1878600	So you've got this you've got this kind of
1879920	1883360	this drive for artificial general intelligence.
1883360	1886480	But when when you actually work out what general intelligence is,
1889120	1891680	it's got some very, very dark histories.
1893680	1897480	I mean, Spearman developed this to support his colonial
1897880	1902440	to support colonial policies, etc.
1902640	1907400	Trying to prove that perhaps other humans were less intelligent for various reasons.
1908800	1912560	So you've got this kind of hierarchical drive
1912600	1916720	within artificial intelligence for basically a superhuman
1918560	1922560	or an intelligence which is beyond human in that kind of way.
1925200	1928440	And just to kind of
1930040	1932200	just linking back to
1933640	1935880	the Cosmist kind of ideas,
1936800	1939880	you see that the idea of colonising the solar system
1940240	1942920	or spreading intelligence to the solar system
1943720	1947280	is is something which is a core concept
1947920	1951080	within AI development circles.
1951920	1954480	I mean, it's also the reason why tech billionaires
1955720	1959440	building their own spaceships, if you think of SpaceX, Blue Origin,
1959800	1963960	they're all they're all influenced by by these imaginaries.
1965600	1967600	It's like.
1968600	1973400	And I'm sure there's probably a lot of people saying I'm over exaggerating
1973600	1977080	this at this point, but I just want to give you a couple of quotes.
1977280	1979880	So so this is from
1980680	1987200	JÃ¼rgen Schmid, Schmid, who developed the natural language model,
1987400	1990960	which is used in Apple, Siri and Amazon's Alexa.
1991920	1996040	So let me let me just get this and so I can read it properly.
1997840	2002560	So this is this is his understanding of what he's doing.
2003560	2005360	He says,
2005360	2008560	So I'm not a very human centric person.
2009280	2013000	I think I'm a little stepping stone in the evolution of the universe
2013000	2015800	towards a higher complexity.
2015800	2018520	It is clear to me that I am not the crown of creation
2018960	2022880	and that human kind as a whole is not the crown of creation.
2024360	2027280	But we are setting the stage for something bigger than us.
2027800	2033280	That transcends us and we'll go out there in a way where humans cannot follow
2033280	2038120	and transform the old universe or at least the regional universe.
2038720	2044640	So I find the beauty and awe in seeing myself as a part of this much grander theme.
2048640	2050800	I've got another one for you if that's not enough.
2050800	2051800	Go on, hurt me.
2051800	2055240	This is this is Professor Dr.
2055240	2060160	Hugo de Garis, who was the former director of the China Human
2060280	2064120	the China Brain Project Institute for Artificial Intelligence.
2064360	2065680	And he writes,
2065680	2069680	Humanity has the duty to serve as a stepping stone towards building
2069680	2072640	the next dominant rung of the evolutionary ladder.
2074520	2078560	And Kurzweil himself says, does God exist?
2079240	2081200	I would say not yet.
2081200	2084120	Oh, God. Right.
2084560	2089760	So so what what what you get when you start digging into these these narratives
2090760	2096200	is the idea of of building intelligence, which goes beyond humans
2096880	2103400	and goes beyond our our time frame, our 70, 80 year limitation
2103600	2108240	and our body's limitation of living within certain environments like the Earth,
2108240	2113360	like our need to be within a kind of ecosystem, etc.
2113560	2116600	and can survive out there on the planets.
2117040	2121600	And it starts to feel like a spiritual movement
2121920	2125880	to to spread consciousness to to the universe.
2126400	2132560	So so the tech development, as as I said at the beginning, looks to the stars.
2134760	2139920	Whereas I think to solve this problem, we need to start looking back to the soil.
2144000	2146360	I'm so upset.
2147720	2148720	Sorry about that.
2151280	2153400	It's so upsetting.
2153400	2155880	I mean, I think we've got to I think we've got to be upset.
2156520	2159280	Yes. To to disrupt
2159880	2165000	and and start saying we need to change these imaginaries.
2165240	2167960	I mean, you've got to remember, I'm coming from an artist background
2167960	2170960	and I kind of do a lot of work within the tech sector.
2174240	2176840	But we have got to be able to create some imaginaries
2176840	2180600	which can compete with these kind of these
2181400	2185560	these dominant narratives which circulate within the kind of tech environment.
2186720	2192000	I have a I have a few things to say on everything you just said.
2192440	2196120	Number one, these men need therapy.
2196800	2199840	That is the sound like those are the words of
2200040	2202520	fairly traumatized people, I would say.
2202720	2207080	Number one, especially the buggers that want to, you know, resurrect their fathers.
2207080	2209040	I'm so sorry for your loss.
2209040	2212440	Please go and pay a therapist to therapist to walk you through it
2212440	2215200	rather than trying to develop a very energy hungry.
2215200	2216960	We don't know what would happen if we released it.
2217720	2219160	Intelligence thing.
2219160	2221600	Number two.
2221600	2223960	The other thing I find really interesting about it is like this.
2223960	2225000	Oh, no.
2225000	2227120	Number two, one funny thing before number three.
2228040	2232600	And that bit that you said at the end, when you were quoting these guys,
2232600	2235480	especially the, you know, I'm not a human centric person.
2236640	2239360	I consider myself a stepping stone.
2239360	2240040	It's not about me.
2240040	2244440	You could just imagine that quote being pasted on top of a
2244800	2248800	a cartoon of like one sperm cell talking to the other sperm cell
2249440	2250920	and it would totally fly.
2250920	2255040	It would be really in place there and which leads me on to point number three,
2255080	2256480	which kind of struck out to me.
2256480	2258680	And then what we will get into the imaginaries, of course, but like
2259360	2262880	in a culture that is so deeply individualistic,
2263240	2268760	there is like a lack of individualism in what they are saying in a sense.
2269120	2271440	And that's fascinating.
2271720	2274480	What is going on there?
2275040	2275800	I agree with you.
2275800	2279040	I think this is like there's a religiosity, religiosity
2279880	2281080	in what they're saying.
2281560	2284480	It's very culty, but like to.
2285560	2288040	This isn't fringe, though, by the way.
2288520	2289720	Oh, no, no, no.
2289720	2294000	These ideas are really, really move in these circles.
2294400	2295400	Yeah.
2295400	2297680	And I think the.
2299720	2302400	Yeah, that kind of spiritual that that link back to the kind of
2302400	2304680	cosmos linked to religion.
2305680	2311240	Is it is definitely there in that it gives people that goal
2311520	2313680	that like this drive towards AI.
2314680	2317400	Is is a bigger goal for these people.
2317600	2320120	So you're right, it's not necessarily that individual thing.
2320400	2324080	It's that they are seeing themselves literally forming.
2324080	2326640	Well, they said it themselves that the next stage in evolution
2326640	2332120	are spreading consciousness to the universe or ultimately creating God.
2333120	2336720	It's so it's funny because they managed to like make themselves
2336720	2340800	as small as sperm cells and yet be still incredibly arrogant.
2340840	2343760	Like the universe doesn't need you, you know,
2343760	2347200	ejaculating all over it with consciousness.
2347240	2348920	Likely there is consciousness everywhere.
2348920	2349760	So it's funny, isn't it?
2349760	2354160	Because there's like there's these interesting moments of kind of disruption,
2354160	2356640	even in the thinking of like lack of individuality in it.
2356720	2360360	And yet it's still so fundamentally hierarchical.
2361160	2364200	Like running with narrative domination.
2364200	2370400	That's why I was pointing out the AGI, the the the absolute link to eugenics in there.
2371680	2377120	Now, what was how does Charles Spearman link to them?
2378120	2380480	Is there like do we have a kind of because, you know,
2380480	2383160	we can walk through the Russian thing pretty clearly.
2383200	2386720	No, no, no, no, Spearman is general intelligence.
2387440	2394000	So if you if you look at the stated aims of open AI on their website
2394520	2398560	and and they will tell you that they are developing
2398840	2403440	that their aim is to develop artificial general intelligence.
2404000	2409800	And if you research general intelligence, that term is Spearman.
2410240	2412000	Right. OK. So that's where it comes from.
2412240	2417160	Yeah. And and and the G factor as a measure of intelligence.
2417160	2421120	So if we are measuring intelligence with general intelligence,
2421400	2423840	then we're already in the territory of.
2425440	2427600	Of eugenics as an idea.
2427600	2431360	I mean, I've got a feeling in this territory,
2431360	2435320	a eugenics which goes beyond the human and wants to develop the.
2436360	2438480	The the superior artificial.
2439480	2441320	Mm hmm. Mm hmm. Totally.
2443720	2448680	And I think this I can do a little linking of Silicon Valley
2448680	2451320	to Eugenics is thinking at this point,
2451880	2453960	which is the effective altruist movement,
2454280	2458360	which is very frightened of there not being enough babies
2458360	2462960	of a certain kind being born in the world, in an overpopulated world.
2464240	2468240	And so I kind of like, you know, Elon Musk is throwing money at reproduction
2468640	2472920	research. There's this like Silicon Valley couple that are planning on having
2472920	2477200	10 babies and inculcating those babies to have 10 more because they want
2477240	2479800	they literally want to replace, you know, sort of like,
2479920	2482880	I can't remember what it was exactly, but in a hundred years,
2482880	2485800	I think they could replace like 50 percent of the United States population
2486160	2488480	at that rate, essentially.
2488480	2491840	And and their purpose here is this.
2492080	2495800	Oh, well, because they believe that you should be investing in the top
2495960	2501400	one percent of humanity rather than the bottom, you know, 10, 20, 30.
2502160	2505120	Because it's the top one percent that are going to have the, you know,
2505120	2509440	intellectual reasoning and capacity to sort of fix the world really.
2509440	2513960	So there is a hierarchy of ability, capacity and intelligence.
2514440	2517480	Yeah. And we don't have enough of the smart ones being born,
2517560	2521200	which does equate to white, essentially.
2521720	2526360	Yeah, of course, because that that's also what the general
2526360	2531920	intelligence historically did anyway, with it within the colonial,
2532640	2536360	well, British colonial, I think, Spierman would like British.
2536720	2539200	I think you're working at King's, I'd have to reset.
2539200	2541000	I'd have to look that up again.
2541000	2544520	I mean, I mean, yeah, this, this, I mean, what you're saying there makes sense
2544520	2549600	with with the general shape of of thinking that I come across as well.
2549600	2553320	This is kind of like a shift towards like a super, super intelligence.
2554000	2557320	And then there's, there's either the direct mechanical group
2557880	2561760	or there's ultimately the developing the human
2562800	2567000	and kind of the cyborg and kind of shift, really, where
2567200	2570120	where you enhance the human to such a level that it becomes
2571120	2575920	the super intelligence that they seem to be the two, the two directions.
2576880	2579880	Yeah, that's so interesting.
2579880	2583000	I don't think I hadn't quite clocked that as being
2583120	2586080	sort of parallel tracks heading in the same direction.
2586480	2592080	The desire to, you know, birth as many superior humans as possible
2592080	2595880	and this drive to create this kind of, yeah, mechanical.
2597800	2603000	I mean, I mean, in my list of things such as
2603560	2607240	immortality, et cetera, I actually missed off the human augmentation.
2607560	2610120	But maybe I should have because human augmentation
2610120	2613080	is is definitely one of the things which comes up a lot.
2614320	2620680	The kind of cyborg is an Elon Musk himself owns Neuralink.
2621240	2623800	Yeah. Neuralink is is the company
2623800	2628720	which which aims to connect the brain directly to kind of computer systems.
2628760	2633440	So, yeah, those ideas of human augmentation kind of completely link in with
2634360	2636720	with this side, with this idea. Yeah.
2637520	2640240	I interviewed Olivia Luzard recently and she was talking about the fact
2640240	2642440	that Mark Zuckerberg has been quoted as kind of, you know,
2642440	2645760	you can't wait to like get rid of his body to get rid of
2645760	2648200	get rid of the weight of physicality.
2648200	2650040	I think it's get rid of the flesh.
2650040	2651520	Yeah, yeah, yeah, yeah, yeah.
2651520	2654080	And which, of course, links into this idea of like, oh, well,
2654080	2657160	if I can upload myself, then I can live forever.
2657480	2661560	Like, transhumanism, I think, is this stepping stone as well towards
2662000	2665800	towards immortality and then towards, you know, the ever expanding stars.
2666760	2670120	Yeah, those ideas are all all interconnect.
2670120	2672960	So in various ways.
2673520	2677240	But for me, yeah, the important thing is these ideas
2677240	2679920	aren't just crazy ideas of crazy people.
2680480	2683640	These these are ideas which are completely
2684320	2688080	embedded in the development of technology, of current technology.
2688080	2691160	Well, the idea what I was talking about earlier, the kind of
2691720	2695800	cyclic causality of of imaginaries and technology.
2696640	2700360	These ideas are part of the process of developing
2701080	2703320	our technologies and our future technologies.
2703840	2708840	So that's why I think as an artist and as a creative
2709200	2712080	that there's an important activist job to be done
2713000	2718240	challenging these ideas and and developing alternatives.
2719880	2725560	Which is kind of the second part of of what we do in our research
2725560	2731440	is kind of carrying out like workshops with with different communities of people
2731960	2736800	kind of discussing these ideas, but also trying to get people to
2737040	2740720	kind of think what what a different form of technology would be.
2740720	2747160	What a technology that does look to the soil that looks to biological systems
2747160	2751360	that that sees all species as intelligent and doesn't create this
2751360	2754040	hierarchy with with a
2754040	2757880	so-called conscious intelligence versus a non-conscious intelligence,
2757880	2760120	which you quite rightly picked me up on earlier.
2760720	2765800	Kind of like break down those ideas and recognize the entanglement
2766240	2773520	that exists between humans, other species, plants, the the biosphere.
2776800	2778160	Yeah, beautiful.
2778160	2782440	The interconnectedness, the oneness, which leads to a different kind of
2782440	2784360	you know, potential for the duality.
2784360	2789960	The multi oneness, the multiplicity, the entanglement of multiplicity,
2789960	2794680	which isn't a oneness, but but is entangled into.
2796320	2799880	Well, as we started started off kind of like
2801400	2803880	our actions do have an impact.
2804600	2808600	Let's talk then about some of these potential technologies
2808720	2812120	that look to the soil or what happens as well to our own kind of
2812680	2814960	thinking and processes when we look to the soil.
2814960	2816480	What have you found?
2816480	2820200	What's good is like working with other communities and getting getting
2820240	2825720	voices, which aren't normally heard within these environments.
2828120	2830720	And we've done a lot of workshops with.
2834280	2837200	Yeah, just all sorts of different people.
2837200	2841320	But one of the projects which has emerged out of this is a project
2841320	2845600	that I've been working with together with Shira Vashman,
2846360	2850840	which is trying to rethink AI with mycelium.
2852520	2854880	I think when I listened to your conversation with Maggie,
2854880	2858120	you raised the idea of mycelium, which I thought was interesting.
2860760	2865040	For people who don't know, mycelium is the organism which
2866880	2871040	it which produces mushrooms, ultimately, but mycelium
2871800	2874080	lives under the soil.
2874080	2878000	It's an interconnected organism of individual high fee,
2878840	2884600	which connecting to a network and the remain as that organism,
2885040	2889960	as long as there's no threat, as long as there's no food shortage of food, etc.
2891640	2896880	When when when there is a problem, when there's a say a temperature change
2897880	2900520	or the area where they're existing runs out of food,
2900760	2903520	they produce mushrooms, which then spar
2905040	2907320	and produce more mycelium networks.
2908400	2912720	But one of the interesting things about mycelium is the way that
2913640	2918880	it's evolutionary or some mycelium, because there's there's lots of different mycelium,
2919280	2925560	but some mycelium kind of work in a symbiotic relationship with other species.
2926400	2930720	So the the best example of this is the idea of the Woodwide Web,
2930720	2935600	which has been circulated quite a lot, which is the way that mycelium
2936000	2939400	connects between different trees within the forest.
2941120	2946920	And and sugars and nutrients are shared between different species
2947800	2949880	or between different trees within
2952200	2953720	a species.
2953720	2956360	So so mycelium works
2957480	2960320	in in symbiotic relationships
2961320	2963400	within the kind of forest environment.
2964120	2968720	And another example of the way it works, in vertically, it would be with the orchid.
2970160	2972560	So orchids
2974160	2976520	cannot photosynthesize while they are young.
2977560	2980800	So they couldn't exist, basically, while they are young.
2980800	2984280	But what they do is they create symbiotic relationships with mycelium,
2985000	2991880	which provides the kind of sugars that they require in their early stages.
2992360	2995560	And then when they when they mature,
2996040	2999840	they become a net producer of sugars, which feeds the mycelium.
3000240	3002920	So you get these kind of symbiotic relationships
3003600	3007960	developing micro-risal networks between different species.
3008960	3009960	And
3011520	3014320	so by working with people
3015200	3018040	taking mycelium as a starting point,
3019560	3020960	we've kind of been
3022880	3028400	kind of looking at at the way if if we were if we were going to think of a technology like AI,
3028840	3030400	what would that look like?
3030680	3033000	And I think the question that emerges from
3033800	3037080	from our rethinking AI with mycelium
3038080	3039080	is
3042680	3044240	I'm going to read this, sorry.
3044640	3045160	Go for it.
3045400	3050040	Is a what if AI significance lies not in competing with us,
3050360	3057400	supplanting or surpassing us in a mainstream AI narrative as in mainstream AI narratives,
3057800	3064280	but in fostering complex, ecologically sustainable symbiotic relations with both mechanical and
3064280	3065960	dogonic intelligences.
3069040	3075160	We suggest the study of AI should involve a redress of our relationship to other non human
3075160	3077240	intelligences on the planet.
3078760	3080360	So that's the kind of
3083760	3084760	the kind of
3087120	3090800	outcome that we're kind of developing with
3091800	3097120	rethinking AI through mycelium is to kind of shift it away from the kind of
3097120	3101600	hierarchical model that we've been discussing and towards
3101600	3107520	rethinking intelligence in this kind of wider way and how can we connect to
3107520	3111560	these intelligences in rather than a
3111560	3115080	combative survival of the fittest
3117680	3120200	eugenicist kind of model.
3120840	3125280	But rather in a model of symbiotic, I suppose,
3128200	3129040	solidarity.
3131320	3133280	And what would these technologies look like?
3136040	3143120	So I think one of the first things is that we've got to understand the impact that
3143440	3147800	computing, obviously I'm coming from an AI computing background, so this is important to
3147800	3153120	me, the kind of impact that computing itself has on the world.
3153120	3161720	So computing itself contributes to global heating and environmental degradation.
3164480	3169720	I think on some recent research that I read in Nature magazine,
3170560	3179200	it stated that the IT industry could use 20% of all electrical production by 2025,
3179200	3181360	given that's next year, that's a lot.
3182720	3190440	And that 5.5 of the world's carbon emissions comes from 5.5% of the world's
3190440	3196240	carbon emissions comes from basically the global IT industry.
3197240	3202120	Now, that's bigger than most countries by, say, China and India, U.S.
3202120	3204800	extract, that's the big countries.
3204800	3209000	So we've got to take these ideas seriously.
3209000	3219320	And so if we're going to continue developing computer systems, then we've got to think
3219320	3223480	of how to create a sustainable computer.
3224480	3230840	And there's quite an interesting movement called permacomputing, I don't know if you've
3230840	3240160	come across it, but permacomputing is it kind of takes permaculture as a model, but looks
3240160	3249000	at how we can develop computing in a kind of sustainable and a kind of viable way, kind
3249000	3258840	of moving away from this ever-growing storing of data within data centers, which use mass
3258840	3265120	amounts of electricity to keep them cool, and also has an impact on water as well, because
3265120	3268440	water's used in these kind of large data centers.
3268440	3276840	Many of which, the storing of mass data is the reason why we're storing mass data is
3276840	3281440	ultimately to train AI models, to train machine learning models.
3281440	3284160	It kind of comes round in a circle to some level.
3284160	3291360	The kind of the form of computing that we are developing, storing all of this data in
3291360	3298920	the so-called cloud, is ultimately collecting the data that is required to train mass machine
3298920	3302520	learning models to develop the AI.
3302520	3311120	So how can we shift away from this kind of circular model to one where we can use computing
3311120	3318120	perhaps to help solve some of the problems that we have, we're having with the environment,
3318120	3322280	but that kind of computing itself needs to be developed in a way which is sustainable
3322280	3324440	with the environment.
3324440	3330800	So that's the kind of thinking that we are trying to develop with our practices.
3331200	3336200	Wonderful.
3336200	3341600	This is kind of the reckoning of all industries of this juncture in history, isn't it?
3341600	3349880	How to get away from our legacy, essentially, and reimagine entirely new ways of being in
3349880	3354520	order to survive ourselves.
3354520	3362160	I think that the ultimate job at the moment is to create these imaginaries, to create the
3362160	3368960	imaginaries so that we've got something to drive for and develop the technologies that
3368960	3370240	allow for its existence.
3370240	3375280	By technologies, I use that term broadly.
3375280	3380000	My ceiling itself can be a technology.
3380000	3389480	If we can use it as a way to understand what's going off within the forest ecology, it is
3389480	3395560	a computer system itself, or a computing system itself, that if we can learn to understand
3395560	3396560	and read.
3397560	3406000	I don't mean instrumentalizing, I mean in a kind of non-destructive way, then that's
3406000	3410360	kind of an interesting way to kind of move forward, I think.
3410360	3412200	Definitely.
3412200	3420560	I think the third thing we maybe need to bring in at this point, though, is as you spoke
3420560	3427520	about the circularity between the ideas and the tech and how they inform one another.
3427520	3435760	We've also got to add in for-profit motive and private ownership and the competitive
3435760	3437280	market.
3437280	3444080	That very first artificial intelligence that you spoke about at the beginning of this interview.
3444080	3454880	Whether or not it is possible to, not to reimagine, but to actually create the sustainable
3454880	3462560	solidarities necessary when that artificial framework will be, sorry, when that artificial
3462560	3466560	intelligence will be moving to shut those kinds of things down.
3466560	3469360	Yeah, absolutely.
3469440	3476960	I mean, I think within our project that we've been working on, particularly the rethinking
3476960	3484440	AI with Mycelium, is we've got to think about, if we are developing intelligence, if we were
3484440	3491960	developing AI, then we've got to think about the particular environments that AI has been
3491960	3500120	brought up in, what sort of ecosystems we are raising and developing artificial intelligence
3500120	3501120	in.
3501120	3509360	Currently, it's this kind of aggressive, domineering, environmentally destructive, competitive
3509360	3511360	environment.
3511360	3517920	If you think of the way machine learning is where it's currently used, high-speed trading,
3517920	3528000	etc., then it's all about systems of winning and losing, of profit and loss, etc.
3528000	3539120	So what would an artificial intelligence look like if it was developed in a symbiotic relationship
3539120	3541040	like a Mycelium?
3541040	3549120	I mean, I've actually got a good quote from this from Tim Ingold, but thinking through
3549120	3557320	the social and the way the social is structured, I think has to play a massive role in our
3557320	3562000	rethinking, say, the market.
3562000	3566520	Let me just find this quote from Tim Ingold, because it would fit here really nicely.
3566520	3567520	Please.
3567520	3568520	I've got it.
3569520	3570520	Where's he gone?
3570520	3571520	Here he is.
3571520	3572520	Okay.
3572520	3582160	So what Tim Ingold wrote in his book Lines a Brief History is, and his dad was a mycologist,
3582160	3588320	so this is probably why he's kind of talking about Mycelium, but what if we take the Mycelium
3588320	3595640	as our exemplar of the organism, arguably, the oil of biological science would be different
3595640	3600920	and so too would the science of society be different, where every person to be considered
3600920	3609200	like the Mycelium as a thing of a line and the social as the domain of their entanglement.
3609200	3617200	So I think what he's arguing for here is a breaking down of that neoliberal individualism
3617200	3625600	that is so dominant within our culture and to see ourselves both entangled with each
3625600	3633120	other, but also in a species relationship with the planet itself, like how would we
3633120	3642200	develop science and technology if we could, if we saw the world in these terms of entanglement.
3642200	3650280	And I think that's quite a good imaginary to kind of base our technology on.
3650280	3659600	So the image for me that came to mind was a weaving, a weaving of threads, a braiding,
3659600	3660840	which kind of like...
3660840	3670680	It's not a loss of the individual to the mass, but at the same time, it's not a separation
3670680	3677480	of the individual from the mass, it's a layer weaving textiles, that's I think a good way
3677480	3678480	forward.
3678480	3679480	Beautiful.
3679680	3686840	I think if all of reality is relational, which it is everyone listening, it just is.
3686840	3691840	And then the more that you braid and weave these relationships, the more that you are
3691840	3697800	literally reinforcing the structure of reality in a good way, you're fortifying it with that
3697800	3698800	entanglement.
3698800	3699800	And I think that's...
3699800	3702720	I think that's a beautiful note to end on, John.
3702720	3703720	Yeah, brilliant.
3703720	3704720	Yeah.
3704720	3705720	I've so enjoyed this.
3705720	3706720	Thank you so much.
3706720	3707720	I have as well.
3707760	3710760	It's a bit of a crazy journey, aren't we?
3710760	3712760	But a good one.
3712760	3714760	Hopefully from the stars back to the soil.
3714760	3716760	Yeah, exactly.
3716760	3717760	Rewind people.
3717760	3720360	Let's bring it back.
3720360	3723760	My final question for you is, who would you like to platform?
3723760	3725260	Okay.
3725260	3732400	So the person I'd like to platform is an art activist called Jay Jordan.
3732400	3737960	So Jay Jordan is perhaps the most committed art activist I know.
3737960	3743320	He played an important role in the activist movements, Reclaim the Streets, and was one
3743320	3750000	of the founders of the laboratory of insurrectionary imagination and the clown army.
3750000	3757200	And he kind of lives resistance daily in the occupied zads in France, the kind of...
3757200	3766240	So hopefully we can get him on and he can take some of these imaginaries forward in a
3766240	3771160	way of how we can actually take them into the streets and into the fields and turn them
3771160	3774760	into direct action, which he's amazing at.
3774760	3776760	Oh, that is just wonderful.
3776760	3777760	I can't wait to speak to them.
3777760	3779760	John, thank you so much for today.
3779760	3780760	It was just great.
3780760	3784280	If you want to learn more, I've put links to everything in the description box below.
3784280	3788200	Remember to subscribe to the channel if you're new here and share the episode if you enjoyed
3788200	3789200	it.
3789200	3792960	To support the show, subscribe at planetcritical.com where you can read the weekly newsletter
3792960	3794660	inspired by each interview.
3794660	3797120	You can also become a Planet Critical patron.
3797120	3799240	All links are in the description box below.
3799240	3802160	As always, my deepest thanks to that community.
3802160	3804760	Planet Critical wouldn't exist without your support.
3804760	3808280	Thank you everyone for listening and for coming on this journey together.
3814280	3815280	Thank you.
3815280	3816280	Bye.

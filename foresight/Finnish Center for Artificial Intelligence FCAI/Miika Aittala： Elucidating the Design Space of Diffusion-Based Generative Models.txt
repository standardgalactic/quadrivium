All right. Thanks for the intro.
Indeed, the title of the paper is
Solucidating the Descent Space of Diffusion Based Tensive Models.
This is work with Tero, myself, Timo and Samuli from NVIDIA.
The agenda here is to try and make sense
of these recently immersed diffusion models,
but really dig into the fundamentals
and with that understanding then ask what are the best practices
for designing and wanting these methods.
So for a brief background on generative modelling,
there are many ways to do it, but the idea is usually
you have a dataset of something, for example in this case phase photos,
but it could be anything, even not images,
and you want to train some kind of a neural method
for basically converting random numbers into random
novel instances from that data distribution.
And after recently GANs where the leading contender in this space,
and these are from there, but now
the denoising diffusion methods have really immersed
as the leading contender here.
So I'm sure we've all seen these superimpressive results
from these models like a stable diffusion,
and everything I'm going to say is basically stuff that runs
at the bottom of these things, and that is in some way directly applicable
to anything like this.
Okay, so all of these methods, the denoising diffusion methods,
the way they implement this idea is you start from pure random noise,
you feed it to a neural denoiser, you keep feeding it,
and reducing the noise level until it reveals a random image
that was hiding underneath the noise, and now you've generated a random image,
so this is a generative model.
One concern with these methods is efficiency.
You need to call this denoiser tens or even thousands of times in some methods
to get the best quality. On the other hand,
it's indeed a trade with the quality of the individual
generative images and with the distribution as a whole.
And these tradeoffs are not really well understood in this previous work.
And some methods simply work better than others,
and it's a bit of a folklore that this one seems to be
good one, good and so on.
And there are many ways to formulate the theory of these methods.
There are, like, market chains, stochastic differential equations,
and some more exotic ways. But when you kind of strip away
all those fancy theories, in the end they all do something like this.
But they differ, lastly,
in practical design choices.
Like at what rate do you reduce the noise level
at different stages of the generation? Do you do this?
Oh, it's showing.
Does anyone know it?
Yeah, thanks.
Yeah, whether you do this deterministically
or stochastically, we'll see the difference soon.
How do you deal with vastly different
single magnitudes at different stages of this process? Do you predict the signal
or the noise? And so on.
And given that ultimately these are the only differences between these existing methods,
these must be the explanation for their vastly different performance
characteristics also. And these are something we wanted to understand
in this process and project.
So we'll be building on the differential equation formulation
from a couple of years back, where the images seem to evolve
according to stochastic or an ordinary differential equation.
And in principle it's known that this kind of generalizes
all of those other methods. You can express them in this framework.
But nobody has really gone through the work of getting their hands dirty
and sorting everything into a sort of
common framework where you could compare and
understand the impact of these design choices. So that's the first thing
we are going to be doing here. And armed
with that knowledge, we'll then ask what are the best
practices for running this sampling process, namely how do you
manage this chain of denoising steps in the best possible way.
First the deterministic version and then the stochastic version.
And then finally we'll come to
best practices for training these neural networks. How do you precondition them?
How do you, what are the lost functions?
Why does this keep coming back?
Okay.
And just one thing we won't be looking at
the actual neural architectures like whether you should use the answer or not.
We'll leave that for future work.
So we'll be studying a few key works in this field.
There's this paper that presents the so-called VBVE method.
There's preserving, there's exploding and there's
DDIM, denoising diffusion implicit model.
It's not really that important for us what the difference is between these
but on the face of it they look kind of like
packages that you have to take as a whole.
You cannot mix and match their properties.
But this is not really true.
The running theme in this paper
is that we identify this complete and exhaustive set of
design choices that completely characterize and reproduce
any given method or at least these three methods and many others
in this space. And this gives us sort of an
extra view into the internals of these methods. We can ask
what are the exact design choices they made about this and this aspect.
Now don't worry. We won't be looking at slides
like this. I'll try to keep it visual and intuitive
to the extent possible. But the important point here is that this can be done
and with this knowledge we can then ask what is the
best choice for any given design point here.
And that gives us our method, which will be building piece by piece
and that then yields significantly improved results.
And we'll be measuring our progress with the FID
metric, which is sort of the current cost standard
in evaluating any kinds of generative models.
So let's start looking at how
Song and Colleagues build this
and I'll formulate this denoting diffusion problem
using differential equations.
So throughout this talk I'll be using this running toy example,
which is actually one detoy example, which is actually quite
actually in many ways completely representative of the actual thing that's going on
with images. So in a way this is one of the images where
you would have more dimensions on the x-axis,
with actual high-dimensional images, like
one megapixel image is a million numbers, so that would be a million dimensional space.
But this describes the essential characteristics of it.
So the point is we have some distribution of data.
Let's imagine there are cat and dog photos or something
and it happens to be this bimodal thing, so certain pixel values
are more probable than others.
We want to learn to produce novel samples from this distribution.
We have a handful of examples, or let's say millions
of examples, which is our data set, and based on those
we want to learn to do this. So in this analogy
one of the samples we have might be this dog photo.
On the other axis
we have increasing time, which is essentially increasing
noise level. That's what we are going to be dealing with when we want to reduce this noise.
But before we do that
let's look at the easier direction of adding noise, like
destroying an image. So if I start taking this
image from the training data set, I gradually start adding noise onto it.
I end up doing this random work in this pixel
value space until the image is completely drowned under
this white noise. And if I have a population of images
in the end they'll all become
indistinguishable white noise. So if I plot
the density that these trajectories make
it'll look like this. So the density of the data
on the left edge becomes diffused over time until it's completely
normally distributed at the end. And this is really nice now
because it has disappeared again.
No.
I'll try
one thing.
I'll try one thing.
I'll try one thing.
Well, maybe we'll just leave
it. Do you think we can do that?
Yeah.
Yeah, let me know if something important seems to be missing underneath it.
So okay. Okay. So yeah, as I said
we can sample from this normal distribution at the right edge. We just
go random in PyTorch. And that'll give us a sample from that edge.
And the magic is that there exists a way
to sort of reverse this path we took earlier.
So go backward in time and that will land us
on the left edge where we have the density of the actual data. And that of course generates an image.
And so if I have a population of these
complete random noises, oops.
Okay.
Yeah, if I had many images I would have gotten
different instances of the image. Okay.
And what makes it stick is that this can be seen
as a stochastic differential equation. In this example
it's about the simplest one we have. When we go forward in time
over a very short time period
the change in image dx equals the omega, which is
white noise. So that's just a mathematical expression
of doing cumulative sum of random noise.
Now the magic is that to this forward equation
corresponds a backward version that has this same
stochastic component, random walk component. But on top of that
it has this term that kind of attracts the samples towards
the data density. You see it's some kind of a gradient of the
density p. But the problem of course is that this p is unknown
and here is the axiomagic. This is a well known function
from previous literature in data science
called the score function and it has the property
that you do not need to know the p if you have
a least gross optimal denoiser for this data set d.
So you can directly evaluate that formula above
by the formula below. And this
is an opportunity. We train a neural network to be such a denoiser
and this means that we can run this kind of
backward equation evolution using that
denoiser.
So some colleagues also present this deterministic variant of this
where you don't have the stochastic term
you only have this chord term scaled in some appropriate way.
And this has a somewhat different like a visual character.
You see it's kind of fading in and out instead of like
jittering around. And this one actually provides
a much cleaner view into this sampling dynamic. So we'll be looking at
this first and then returning to the stochastic later.
And with this I can now always draw this
paint flow lines of this ODE. So the idea
is that we are trying to somehow follow these lines to do the generation.
And indeed the way that happens is by discretization
I take little but macroscopic steps
in this space I reduce the time and
for any change in time I want to jump. The ODE formula
tells me how much the image changes. And again
the ODE formula is already does a neural network
so the neural network tells us where to go on the next step.
That's the general idea. And that gives me a step.
I keep stepping until I hit time zero and that's my generated sample.
With the SDEs we would have some kind of noise addition
on top of this so we would kind of jitter it
but I said we'll leave that for later.
And now we've exactly reproduced this intuitive
picture using differential equations.
Okay so that was song and colleagues
for our purposes. And let's now identify
some design choices involved in making this kind of an ODE or SD.
But before we do that we should understand what can go wrong
in this process. What are the error sources? Well the obvious one
because I might end up like in a different place than I should have
when I do this sampling chain. So the obvious one is that if the network
gives me an incorrect direction I end up moving in the incorrect
direction and in the end I end up somewhat in the wrong place.
It's more subtle than this but this is kind of a cartoon.
The other source of error is that we are trying to approximate this continuous
trajectory in green here using these linear
segments. And
if I try to jump too far at once the curve will kind of
move away from my feet and I'll end up veering off
this path. It's of course familiar to anyone who's done like a physical simulation
with ODE. And
the proof of solutions to that is to take more steps
but that's exactly what we want to avoid because that directly means
more compute to generate an image.
Okay and so what we argue and what is underappreciated in previous
work is that these two effects should be analysed
or can be and should be analysed in isolation.
You don't have to sample in a certain way just because you train your
network in a certain way and so on you can decouple this. And indeed
we'll be looking at sampling first and then coming back to the training later.
Okay so I promise to show you some
choices and here is one finally. So
when I built this example I added noise at a constant rate
over every time step and that gives me this simplicity, it gives me this schedule
where the noise level increases as a square root of time
because that's how the variance will grow linearly so the standard
will go square root. That's what you get
if you call random and then do a comp sum on top of it.
Had I added it at a different rate I might have arrived at a schedule
like this for example where the standard deviation is the gross linearity
and indeed I could do any
kind of a choice here. I could do something even something
crazy like this way we schedule here in the middle if I wanted to
for some reason. And indeed we generalise in the paper the
ODE form or we reparametise it in such
a way that we get a clear view into these effects.
So we can parameterise the noise level we want to have reached
by explicitly by this function sigma.
But the real question is why would you want to do something like this.
Well one reason for that could be that if you look at this picture for example
you see almost nothing happens until
at almost zero noise level suddenly curves rapidly
to one of these two basins and
there's high curvature there so we'd probably want to be careful
in stepping. We'll want to take somehow be more careful in sampling
that region and less careful you're in the bulk. So there's two ideas
of how you might do that. You might
first you might take shorter steps at the more difficult parts
usually is the low noise levels because that's where the
image details are usually built. The other alternative
would be to instead warp the noise schedule in such a way
that you just end up spending more time at these difficult parts.
And it's tempting to think
that these two approaches would be equivalent.
And this is an implicit assumption I think that many previous works do.
But this is simply not true because the error characteristics
can be vastly different between these choices like the error that comes
from this tracking this continuous curve
and we'll see the effect of that later.
So now we've identified the first pair of design choices here.
The time steps and the noise schedule.
But let's introduce a couple more.
And this address is the following problem. I zoom out a little
because in reality we add a ton of noise. So at the
other extreme the noise level is very large. I've been showing this zoom in
so we can easier see what's going on. But I zoomed out
now to see what's here. So the issue
if you don't do anything is that the signal magnitude grows
as the noise level grows. You keep piling noise. The signal
is quite simply bigger numerically like the values that
are in your sensor. They are much larger at the high noise levels
than in the low noise levels. And this is known to be really bad for neural network
training dynamics. And these kind of
effects are actually critical to deal with to get good performance.
So the way many previous works approach this is by using
something called variance preserving schedules where you effectively
introduce this additional
so called scale schedule where you squeeze
the signal magnitude into this constant with constant
variance tube. So that makes
that's one way to make the network happy here.
So we generalize this idea again by
just formulating an OD that allows you to directly
specify any arbitrary scale schedule. And viewing this slide
it again becomes appropriate that the only thing that the scale schedule does
is distort these flow lines in some way.
So you are just doing a coordinate transform in a way on this XT plane.
Now there is an alternative
way to deal with this scaling issue. And it is quite
simply this. Instead of changing the OD at all
you could change your neural network in such a way that it has an initial scaling
layer that uses the known single scale
scale it to something that's palatable for the neural network.
And again you might think that this is completely
accurate with the OD, but this is simply not true because again the error characteristics
are vastly different between these two cases. And I'll come
back soon to how the chosen practice.
But now we've identified a second set, second pair of these matrices.
The scaling schedule and the
scaling that happens inside the neural network itself.
And that we kind of saw so-called preconditioning of the neural network.
Okay, so now we have quite a few
collected here. And
at this point we can ask, like get our hands dirty
go look at the appendices of these papers, read their code
for the final ground truth and ask what
formulas actually exactly reproduce their
approaches. And they are these. Again don't worry, but don't even
try to read them. But the question now is
what choices should we actually make, which ones of these are good, which ones are
suboptimal. And that's going to be the topic of the next section.
And for now, we will be ignoring these neural network training
aspects. We will be using pre-trained networks from previous work.
We won't be retraining anything yet. We'll just try to improve the sampling.
So now we move on to the deterministic sampling
and actual prescriptions of what
you might want to do. So first the noise schedule. Why would
some of them be better than others? For example this way one must be
terrible for some reason, but why?
Well, now we get a clear view.
Well, parameterizing things in this way gives us a
quite a clear view to this. So let's zoom out again.
And consider the fact that
we are trying to follow these curving trajectories by following
these linear tangents. And that's probably going to be more successful
when the tangents happen to coincide with this curve trajectory.
So when the trajectory is as straight as possible in other words.
So if I use a bad schedule like this one, you see
there's already a visible gap between the tangent and the curve.
So you easily fall off if you try to step too much.
And indeed if I show you this random family of different
schedules, we see that some of them seem to be better in this regard than others.
In particular this one.
And this is actually the same schedule used in the previous work
DDIM, which is known to be quite good.
And this in a way explains it.
So this is the schedule where standard deviation cross-lineally
and we do not use any scaling. And indeed
we'll be leaving the scaling for neural network parameterization.
And the reason for that is that this scaling also introduces unwanted
curvature into these lines.
Yeah, it just turns them unnecessarily at some point.
It's actually better to let the
signal in the ODE grow from that perspective.
As further, and yeah, with this the
is the ODE becomes very simple. So as a further
demonstration, like an actual mathematical fact about this
schedule and why it allows us to take long steps is that
if I took a step directly to times zero, then
with this schedule and only this schedule
the tangent is pointing directly to the output of the denoiser.
And that's very nice because
the denoiser output changes only very slowly during the
sampling process. And this means that
well, the direction you are going to doesn't change almost at all.
So it means you can take long bold steps and you can consequently only take
a few steps or many fewer steps than with the alternatives.
Okay, and then I said we'll want to direct
our efforts to the difficult places. Now we've tied our
hands with the noise schedule. So the remaining tool
is to take different length steps at different stages
of the generation. And indeed, when you go look
at the possibly implicit choices the previous methods have done,
all of them take shorter steps at low noise levels
because that's where the detail is built again.
And yeah, we
formulate this family of these discretizations
like a polynomial step length growth and we find that there is a broad
optimum of good schedules there.
You can read those details in the paper.
So there's one more thing that this ODE framework allows you to do
which is not so clear with for example the Markov chain
form lessons is use higher order solvers. So again
there is going to be curvature and it can be quite
rapid at places. So you can still fall off
the track if you just naively follow tangent and that method
of following the tangent of course is called the Euler method.
But there are higher order schemes for example in the Hoin scheme you take a second
tentative step and you move it back to where you started from
and your access step is going to be the average of that and the initial one.
And this makes you much better follow these trajectories.
This of course has a cost. You need to take these sub steps.
And what we find in the paper by extension we're studying this
is that this Hoin method strikes the best balance between these higher order methods
for sort of the extra bang for the buck.
And the improvement is actually quite substantial.
Okay, so those are the choices we made.
And now we can evaluate. So we'll be evaluating
these results throughout the talk on a couple of very competitive
generation categories. Saifat Sen
at Resolution 32 using it at Resolution 64.
And I want to say a couple of words on this might sound
like a useless toy example to you if you're used to seeing
like outputs from stable diffusion or something.
But the way also those methods work is they first
they something like a 64 by 64 image and then they upsample it sequentially.
And it turns out that generating the 64 image is the difficult part there.
The upsampling just kind of it just kind of works.
So this is highly indicative of
improvements we get in very relevant
very relevant classes of models.
Okay, so if we look at the performance of the original
samples from these works, from a few previous methods
on these data sets, we see
that we have the quality on the y-axis, the FID lower is
better, and we have a number of samples we need to take, like number of steps
or the function evaluations on the x-axis. We see that we need to take
something like hundreds or even thousands of steps to get kind of
saturate the quality to get the best quality that model gives you.
So introducing the point sampler and our discretization schedule
we vastly improved this. I noticed that
the x-axis is logarithmic, so we've gone from like hundreds
to dozens of evaluations.
And further introducing the noise schedule
and scaling schedule further improved the results by a large amount
except in DDIM which was already using those schedules.
So now we've already made it quite far here
and using some super fancy
higher audio, so it's not worth the effort.
Okay, so now we've covered the deterministic sampling
and let's next
return to the question of SDE which we
put on the back burner on the other slides. And remember
instead of following these nice smooth flow trajectories
the SDE sort of cheaters around as some kind of
exploration around that baseline. So it can be interpreted as
replacing the noise as you go on top of like reducing it.
And the reason why people care about the SDE is of course
well one reason is that that's where this stuff is derived from
but the other is that in practice you tend to get better results when you use the SDE
instead of the ODE, at least in previous work.
And the reason for that will become apparent soon.
But let's first generalize this idea a little.
So in the paper we present this generalized version of the SDE
which allows you to specify the strength of this exploration
by this sort of noise replacement schedule.
So especially when you set it to zero you get just the ODE
boosting this factor. Or you can do more exotic schedules
like something like this
where you have it behave like an SDE in the middle and like the ODE
and so on. And samples would look like this.
But again that's the question of is this just a nice streak
or like what's the point.
And as I said empirically
this improves the results. And now looking at this SDE
the reason becomes somewhat apparent.
So don't try to read it unless you're an expert in SDEs
but we can recognize a couple of familiar parts here.
So the first term in the SDE is actually just the ODE from the previous section.
So that means that we still have this force
that is driving us towards the distribution
of flow lines. And the remainder we can identify
some kind of a lens around diffusion stochastic difference equation
which is a well-known thing from a long ago.
It has this property that it makes the samples sort of explore your
distribution and if the samples are not distributed correctly
it will kind of reduce that error.
So it has this healing property.
And because we do make errors during the sampling it kind of actively corrects
for them. And this is how it looks like. So let's take this extreme situation
we have our samples to blue dots. And
let's say they are really bad. They are not following the underlying distribution at all.
They are skewed to one side.
So if we keep following the ODE it does nothing to actually
correct the skew and we completely miss the other basin of the days for example.
So when I introduce stochasticity to this process
it starts looking like this.
So these samples do this kind of random exploration
and gradually forget where they came from and forget the error
in this opposition. And now we've covered both modes for example
in the generated images on the left edge.
So that's the sort of reason why stochasticity is helpful.
No, arguably this is the only benefit of the SDE over the ODE.
But there are also downsides in using SDEs.
For example we would technically have to use these
complicated solvers that are arguably designed for much more complicated
cases where you have more general SDEs.
So we asked the question could we instead directly combine the ODE solving
with this idea of this
churning of the noise, adding and removing it.
And the answer is this.
So this is a stochastic example we proposed in the paper.
So we have our current noise image at noise level TTI.
Remember in our parent recession the noise level is now
completely equivalent with time.
So we have two sub steps in one step.
So first we add noise. So this represents the lens of an exploration.
So we landed some random
noisier image so the time increases here.
And then we solved the ODE to where we actually wanted to go
with say lower noise level.
And that simply follows the flow line there.
And in practice we do this with a single point step. So we keep alternating between
this noise addition and the point step.
And this brings us closer and closer to time zero as we want.
But underneath it is the ODE running the show
and guiding us along these lines.
But on top of that we now have this jittering with correct servers.
Okay, so this all sounds really nice.
You get free error correction but it's not actually free
because the lens event term is also an approximation of some continuous thing.
And you introduce new error also when you make it.
So it's actually a quite delicate balance of how much you should do this.
And now with this clear view into this dynamics we actually find that it is really
finicky. You need to tune the amount of stochasticity
on a per data set per architecture basis.
You get the benefits but it's really annoying.
So it's a mixed bag.
Nonetheless it is very useful. So if we compare the ODEs from the previous section
their performance with just original SDE
samples from these respective works.
We see that the SDE solvers are simply better in the end
but they are also very slow.
Now applying all of these improvements
with our method, with the optimal tune settings for this data set
we read both much better quality
at a much faster rate.
And yeah, there's been some previous works
that applied also higher order solvers.
So I want to highlight one result here.
This image net 64 highly competitive
just with this change of schedule.
We went from a pretty mediocre FID of 2.07 to 1.55
which at the time of getting this result was state of the art
but that record was broken before the publication
but we'll have our events in a few slides.
But just to show that this is just taking
the existing network and using it better
for improvements already.
Okay, so that's it for deterministic sampling.
At this point I have to say I'm going to go a bit overtime because of the hassle
in the beginning and because this is kind of incompressible anyway.
So if you need to leave then no problem.
So yeah, that's it for stagastic sampling
and for sampling as a whole.
So just a brief recap.
The way this works was that the role of the ODE
is to give us the step direction
which is given by the score function
which is given by the score function
which is given by the score function
which is given by the score function
which can be evaluated using a denoiser
which can be approximated using the neural network
and that is the role of the neural network.
It tells you where to go in a single step or what's the direction you need to go to.
And the theory says that as long as the denoiser does something
that minimizes this loss, the L2 denoising loss
the theory will be happy.
And you can do this separately at every noise level
so you can weight these loss according to the noise level also.
But before we go to these loss weightings
let's look at the denoiser itself.
So I draw the CNN there in a bit of a hazy way
and this is because it's actually a bad idea to directly connect the noise image to the input of the network
or to read the denoised image from its output layer
rather we'll want to wrap it between some kind of signal management layers
to manage those signal scales
of both the input and the output to standardize them somehow
and also in this case we can often recycle stuff from the input
because let's say if the input image is almost noise free
then we don't really need to denoise much
we should just copy what we know and only fix the remainder
we're going to come to that soon.
And this is super critical here.
I mean this might sound like boring technical details
but these kind of things really are critical for the success of the neural network training
and we've seen this over and over again over the years.
And in this case the noise levels vary so widely
that this is extra critical here.
So without too much to do here is how one of the previous methods, the VE method
implements the denoiser.
So the idea of this setup is that they are
learning to predict the noise instead of the signal using those CNN layers.
And the way that works, and I'll explain why soon
the way that works is of course the loss will be happy
if the denoiser can produce the clean image
and we can interpret this model as having this kind of a skip connection
so the noisy input goes through that.
Now implicitly the task of the CNN will be to predict
the negative of the noise component in that image
and then they have an explicit layer that scales that noise to the known noise level.
And so now when you add whatever came from the skip connection to this
you get an estimate of the clean image.
So this way they kind of turn it so that the CNN itself
is concerned with the noise instead of like the signal.
I'll explain soon why that is relevant.
But first let's do the thing I promised to do a long ago.
I said there's huge variations in the magnitude, just the numerical magnitude
of these input signals.
So fairs to accounts for that, which is problematic.
And so we quite simply introduced this
interscaling layer here that uses the known standard deviation of the noise
to scale the image down.
I want to highlight this not like a batch normalization or something.
We know what the noise level is, we know what the signal magnitude should be.
We divide by an appropriate formula.
So that gives you one of the wishes we had on that orientation slide.
On the output side we actually have something nice already.
So this is very good because now the network only needs to produce
a unit standard deviation output and this explicit scaling to the known noise level
takes care of applying the actual like a magnitude of that noise.
So this makes it again much easier for the neural network.
It can always work with these standard sized signals.
And that deals with the second hope we have there.
But now the question of should we predict the noise or the signal and why?
So it turns out this is actually a good idea at small noise levels
but a bad idea at high noise levels.
So I'll show you what happens at low noise levels.
So if we have low noise, the stuff that goes through the skip connection
is almost noise free already.
And now the CNN predicts this negative noise component
and it's scaled down by this very low noise level.
And this is great because the neural network
is actually the only source of error in this process.
So if the network made errors, now we've downscaled them.
So it doesn't really matter if the network is good or bad.
We didn't do much error in this case.
So that's great. We are sort of recycling what we already knew
instead of trying to learn the identity function within your network.
So that kind of deals with the third hope we had on the slide.
But on high noise levels, the situation is reversed.
Whatever comes through the skip connection is completely useless.
It's a huge, huge noise signal with no signal at all.
And now the CNN predicts what the noise is.
And then it is massively boosted at this stage.
So if the network made any errors, now there are going to be huge errors after this.
And those are directly passed out of the denoiser.
So now we've introduced a huge error into our stepping procedure in the ODE.
It's also a bit of an absurd task because you're trying to subtract
two massive signals to get a normal size signal.
And kind of like trying to draw without two meter long pencil, not optimal.
So instead what we'd like to do is somehow disable the skip connection
when the noise level is high.
And in that case, effectively the task of the CNN will be to just break the signal directly.
There won't be any need to scale it up.
So we won't end up boosting errors.
And the way we implement that is by adding this sort of switch
but in a continuous way.
So we have this so-called skip scale, which when set to zero,
effectively disables the skip connection.
Set to one, you get the noise prediction.
And furthermore, we make it so that it's actually a continuous value
between zero and one that depends on the noise level.
And if it's somewhere in between, that means that we are predicting
some kind of a mixture of noise and the signal in this instead of just one of them.
And there is a principle way of calculating what the optimal skip weight is.
But I won't go there in the interest of time.
We have it in the paper of Enix.
And that deals with the remaining issues we had on this slide.
And now we can look at what the previous works did and what we did.
So these are the actual formulas that implement those ideas.
Then there is the couple of training details.
How should you weight the loss based on the noise level
and how often should you show samples of different noise levels.
So the general problem, if you don't deal with these issues,
is that you might have a highly lopsided distribution of gradient feedback.
So if you're not careful, you might be prodding the weights gently to one direction or the other.
And then every few iterations you have this massive gradient smash on the weights and so on.
And that's probably very bad for your training dynamics.
So the role of the loss weighting or the scaling, the numerical scale in front of the loss term,
should we be to just equalize the magnitude of the loss or equivalently equalize the magnitude of the gradient feedback that gives.
And then the noise level distribution, maybe how often you show images of any given noise level.
The role of that is to kind of direct your training efforts to the levels where you know it's relevant,
where you can make an impact.
And for that, in the paper, we have this sort of an important sampling argument that whatever we do,
we end up with this kind of a loss curve.
So we don't make much progress at very low and very high noise levels, but we do make a lot of progress in the middle.
For example, at very low noise end, you're trying to predict noise from a noise free image.
It's impossible, but it also doesn't matter if you can't do it.
So we, based on this, we find that it's enough to, or suffice this to sort of have this very broad distribution
of noise levels here that are targeted towards the levels where you know you can make progress.
And this is a logarithmic scale on the x-axis, so it's a lot of normal distribution.
So those are those sources.
And it's starting to pretty full.
There's one more thing, which I'll just keep in the interest of time.
We have some mechanism presented in the paper for dealing with vastly like two small data sets when your network starts overfitting by this augmentation mechanism.
You can look at it there.
But yeah, let's not go there.
It's really only relevant for various small data sets like Cypher.
With ImageNet, we haven't found benefits from it, I think.
Okay, so with all these improvements, we can stack them one by one.
These are the lines here.
And in the end, we get state-of-the-art results in various competitive categories.
In deterministic sampling, we get an FID 179.97 on the Cypher categories, which might still be state-of-the-art.
Also at very low sample counts compared to most previous work.
That's more interestingly.
Okay, that was with deterministic sampling.
When we enable the stochastic sampling and tailor it for these architectures for ImageNet and use these retrained networks, we trained ourselves using these principles.
We get an FID of 1.36, which was a state-of-the-art when this paper came out.
It's been overtaken, I think in the last few weeks, possibly earlier.
So all in all, we've turned this model that was okay-ish in the beginning.
And by stacking all of these improvements, we get the best model in the world at that time for generating 64 ImageNet.
Interestingly, the stochasticity is no longer helpful with Cypher in this resume or after the training improvement.
And so it appears that the network has become so good that it doesn't make that many errors.
And any exploration, lands and wine exploration you do, introduces more error than you are actually fixing with it.
But this is still not the case with ImageNet.
So there it's still pays to do stochasticity.
Okay, so that was the, that was mostly it.
Just a brief conclusion.
So we've sort of exposed this completely modular design of these diffusion models.
Instead of viewing them as tightly coupled packages where you can't change anything without breaking something,
we show that you can pretty much change every, like you get a valid method no matter what you do as long as you follow these loose guidelines.
And then with that knowledge, we get a clear view into what we should actually be doing with those choices.
And doing so pays off in a big way.
We get much improved quality at much, much more efficient models.
One takeaway about stochasticity, it's a bit of a double edged sword.
As said, it does help, but it also, it requires that annoying per case tuning.
There are no clear principles how to do that tuning.
There is also a danger that you can even have bugs in your code or something.
And stochasticity will kind of fix them to an extent,
which is of course not what you want to do if you're trying to understand what your potential improvements are,
what their effect is and so on.
Ideally, you'd be able to work in a completely deterministic setting.
And if you want, then in the end just kind of reintroduce the stochasticity as the final, final cherry on the top.
Okay, so we haven't talked about all the fancy stuff like higher resolutions, network architectures,
classifier free guidance and so on.
But probably many of these would be right for a similar principle of analysis.
We hope this inspires you to also think about that kind of things and certainly we are.
So with that, the code and everything is of course available.
I would argue this is probably one of the better places to copy paste your code.
If you want to experiment with stuff, it's very clean code based that directly implements these ideas.
Yeah, so thank you for your attention.
Hey, so we have time.
Yeah, I have some.
I just want to ask a question.
All right.
It probably has to do with the data's complexity.
Seffari is maybe a bit too simplistic in the end.
It's kind of learnable entirely.
But it seems like that something like ImageNet, it's still so extremely cool.

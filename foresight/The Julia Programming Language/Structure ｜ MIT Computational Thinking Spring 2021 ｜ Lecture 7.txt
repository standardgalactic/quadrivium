Let me just start with this, that welcome everybody out
in internet land and anybody from MIT as well.
And let me mention that this Saturday night or Sunday
morning, I guess officially Sunday morning,
much of the United States is going to be changing the clocks.
And so if you're somewhere that isn't changing the clocks,
you might want to keep that in mind
if you're looking for us Monday 1 PM Eastern,
it might well be an hour, I never remember
whether it's earlier or later for you than it usually is.
So I know the rest of, I know most of Europe
is going to be doing it two weeks from now.
And so I guess it will be an hour earlier for those of you
in Europe, for example.
OK, next thing is we seem to have had a little bit of a problem
with the website.
So I was going to just try quickly
to see what I think would work.
But Dave tells me it won't work, but I
want to see it for myself.
So I'm going to go to Pluto.
And I've gotten into the habit of putting URLs right here.
So I thought that this would work, but David tells me no.
And David is usually right.
But I'm going to try it anyway and see if it will work.
And it does work.
OK, this time, Dave, I guess this time it worked.
Again, I'm glad to know about it.
Right, so the actual URL, which, let's see, what is the,
can we put it in the Discord or something
so that people can find it?
I don't believe it.
You already did, even better.
OK, so this will be the first notebook for today,
the structure.jl.
And what I'm going to be doing is talking
about a real computational thinking idea, which
is taking advantage of structure.
And structure can be many things.
And I'll give you a few examples.
And then, depending on how the time goes,
we're also going to talk about a particular kind of structure,
which is principal components analysis.
And that's this over here.
So this is this particular notebook,
which I gathered Dave has already put into the Discord,
so you can find that as well.
So let me start with talking about structure.
And there is a new feature that is going to be in the notebooks,
and we're going to work backwards and get them in the old notebooks
as well, which is, what are the Julia commands structures
that are going to appear in this notebook?
And so I made a list here.
You could see four items, basically struck,
dumped, you know, dagl and sparse,
so I'll count this one item, and error.
So we're going to make an effort to put this
at the beginning of every notebook.
So you could figure out what little bits of Julia
you'll be learning in this particular lecture at the same time.
So thanks, Charles, for that nice idea.
So structure, examples of structure.
So let's see.
I see if this is too big, the table of contents.
Let's see, is this too small for you, Dave?
Should I make it bigger?
You always like me to zoom in.
But I also like the table of contents on the side.
Yeah, that's OK.
All righty.
OK, so structure.
So the best way to talk about structure
is to give some examples.
But let me say that even if you were around for the last lecture
of this semester, we talked about dynamic programming.
And you might remember that the structure that we took advantage
of was the common subproblems, right?
So that there were these paths that would go left, down,
and right, or I call them southwest, south, and southeast.
And these were not just sort of random paths,
but these were paths that had a structure to them.
That is actually a common, they had a common substructure.
So that's perhaps one example.
But let me kind of go a little slower
and talk about other problems that have structure.
So here's a really simple structure.
And this is the so-called one-hot vector.
It's a name that comes from machine learning.
It's a very simple idea.
I really like the name one-hot vector.
Here's an example of a one-hot vector.
This is a vector that's made up of zeros and ones,
but only one of the elements is hot, right?
The rest are cold, OK?
So what that means is exactly one element is one,
and the rest are all zero, OK?
And so that is a one-hot vector.
In linear algebra, it might be called
the column of the identity or a coordinate basis vector.
But none of those words seem as good to me as one-hot vector.
I really like that name.
So I hope you like it too.
And why Ellen does it say one-hot vector, my one-hot vector?
Yours says one-hot.
Yours says one-hot.
Put below.
And name the variable says my one-hot vector.
I don't know, but we could change it.
Well, it's being defined again below, so you be careful.
I don't know.
Is it being one-hot?
My one-hot vector is over here.
Yeah.
I wonder if that happened because we didn't want to clash.
Yeah, that's what I'm wondering.
That's probably what happens.
I don't remember anymore.
I wrote it last.
That's a funny way to do it, though.
Yeah.
I don't remember my own sense of humor, if it was even me.
OK?
That's because that's the American pronunciation.
One-hot vector.
There you go.
So I would like to ask a question, which
is for everybody to think about, is how much information do
you need to represent this vector?
Obviously, it's made up of six elements,
but do you really need six numbers to represent this vector?
So if n is 6, do you need n?
Do you need 1?
Do you need 2?
I mean, really, what is the information content?
And I'm sure you can all realize that two numbers ought
to be enough, right?
The size 6 and the position of the 1, 2,
kind of tells you the whole story, really, right?
So it seems silly to write out a vector 0, 1, 0, 0, 0, 0
in the context of one-hot vectors because two numbers will
do the trick.
By the way, just to mention that there's also
the word one-cold, OK?
And a one-cold vector is, again, a vector of 0s and 1s,
but only one of the elements is cold.
That is the 0.
But we're going to concentrate on one-hot vectors.
And I'm going to show you how you create a structure in Julia
that takes advantage of the structure.
And coincidentally enough, to create something in Julia
that takes advantage of structure,
it's called a struct, right?
This is a new type in Julia that you
get to create yourself, OK?
And so this part you could ignore right now
if you find it a little bit confusing.
But basically, we're going to say that a one-hot,
I'm going to create a one-hot, and it's
going to be made up of two things.
It's going to be made up of an integer n and an integer k.
And that's it.
So a one-hot is going to be two integers.
There's nothing more to one-hot, really.
It's just these two integers.
And just to tell you what this is in case you're curious,
we're saying that this will be a subtype of an abstract vector
of ints.
And so that it behaves in many ways
like a vector of integers, even though it's not
a vector of integers, it's a one-hot.
But it'll be a subtype.
That's this funny punctuation, a subtype of an abstract vector
made out of ints.
And a couple of quick things that are actually
useful to have if you're going to create an abstract vector,
and it's going to be handy here, is
to define the size of, see, we want
to pretend this is a vector, even though it's not
exactly a vector.
So what can you do with a vector?
Well, you can get its size.
So this says that if you ask for the size of a one-hot,
you should give me back the n.
So the n is actually the size.
This is just the way here.
Maybe I'll go over here and put this in.
If I were to do the size of my one-hot vector,
you see, I just got a 6.
And so we want this to behave in the same way,
even though we're not storing all those 0s and 1s.
Another thing we want to do is get index.
So we want to do things like, we want
to do the analogy of my one-hot, by the way,
I just hit Tab to make that faster.
If I type my one vector of 2, I'll get the 1.
But if I do any other thing, I'll get a 0, right?
If I call it with any other index, I get a 0.
So a valid index, I'll get a 0.
So I want that behavior too.
And so the command there is get index.
Maybe I should have added size and get index to things
that we are showing off in Julia today.
And we need a one-hot vector and an i.
And basically what we're going to check
is whether the, what we're checking
is whether the number k in the one-hot vector
is equal to i, right?
And we'll turn it into an int.
So this is a true or a false, which becomes a 0 or a 1,
as it's turned into an int, okay?
And so to actually pull this off,
I could create a one-hot vector by just typing one-hot of 6,2.
And you see, it actually gives you the illusion
of a vector, right?
I mean, it even looks to Julia as if it's a vector, right?
But yeah, let's see.
My one-hot vector, right?
And if I actually index it, for example,
if I index it with 2, I get the 1.
But if I index it with 4, I get the 0.
This completely has the full illusion
of being a vector of 0s and 1s.
And yet it actually takes advantage of the structure.
And one way you could actually see that is,
I'm going to mention the dump with a small d and a capital D.
In capital D, it actually prints in Pluto without any fuss.
The small d, you have to do this silly Pluto thing with terminal.
But I actually think it looks very nice.
It kind of highlights.
It's sort of like, to me, it looks like a blackboard.
So I think I actually kind of like the with terminal
just to highlight it.
But in any event, dump, what dump does
is it kind of tells you everything that's
going inside that object.
So if I dump my one-hot vector, you'll
see that I'm getting two integers, the 6 and the 2.
So you can use the capital D if you like.
And it'll just print it sort of the boring way.
Or you can use the little d.
And if you just do the little d, I'll show you.
Pluto kind of ignores it.
A regular Julia Repel wouldn't.
But if you add this with terminal do thing,
then you get this sort of nice blackboardy look.
So there you have it.
My one-hot vector is storing the two pieces of information
that are critical to defining without wasting any space at all.
And so that's structure.
That's taking advantage of structure.
And I don't know why, but I like to do visuals.
So here's visualizing a one-hot vector.
n is the size.
So here I'm just making n be 13.
Here you can actually see it.
And then k specifies where the one is.
And so here's just a little visual.
I don't know if this adds much to the story,
but it's fun to look at.
So here's one-hot vectors.
OK, but let me move on now.
Just a comment that basically what we're saying
is that this one-hot object behaves as if it were a vector.
It has the same behavior.
You could not actually tell when you're indexing into it
or when you're doing length of it.
You can't actually tell how it's being stored internally.
It has exactly the same behavior to the user.
And we already saw another example of that,
which was range objects also behave like that.
You can index into ranges, and you
can take the length of ranges.
And you don't know if how many pieces of information
they're storing.
Oh, let's do a dump.
Let's go dump of 1 colon 7.
We may have done this before, but let's just do it again.
Yes, we do, or I have to do that with terminal.
So there you see.
This also has two numbers to start and to stop, right?
If, on the other hand, we took a range that
was even numbers from 2 to 17 or something,
then there's three numbers that store this, the two, the two.
And it's clever enough to know that if you're
starting at an even, you're actually literally stopping
at a 16.
So that's actually what's stored inside the computer
is basically just these three integers for a range.
By the way, something that came up the other day,
and you'll see it in your MITx homework, also inspired
by Charles, is if you actually dump this thing,
you'll see that it is a vector that actually
contains the range, right?
And so there's a difference, and we'll
explore that in the MITx homework,
there's a difference between this, which is just a range,
which is here, let's make a vector that
has a couple of ranges in it.
So this is a vector of size 2, and you
can see that each element is itself a range.
So that's enough of that stuff.
Let's do another example of structure
that let's take a diagonal matrix.
So here is a diagonal matrix that I
don't know if people see matrices in high school anymore.
You might see in, let's just say, I
think it's better to say an elementary linear algebra
class.
I think that would be a better way to say it.
So here's a diagonal matrix.
It's a diagonal matrix is one that only has non-zeros
potentially on the diagonal, right?
All the off diagonal elements are 0.
So this is a diagonal matrix, but for a 3
by 3 array, I think you could see that I only
needed three numbers to represent it, right?
For an n by n matrix, I would only
need n numbers to represent a diagonal matrix.
And so I don't know if your linear algebra class,
they make you write out all the zeros or put in the dots,
but it is silly to store them on a computer, especially
if the matrix is large.
I guess if it's small, it doesn't really matter.
But Julia has a type called a diagonal,
and it even prints them out kind of pretty.
You could see there's dots where the zeros would be, OK?
And let's do the dump right now, D. I
can't remember if I did it later, but let's do it right here.
You could see that the numbers that are stored
are the 5, the 6, and the minus 10, right?
There's no zeros stored.
That's to be contrast, if you will,
with the if I dump the density, the one that I first defined,
and as you probably would expect,
this one is storing all of these numbers.
The zeros are stored.
So again, the structure of this diagonal matrix
is just to have these numbers, 5, 6, and minus 10.
And so as I was just showing you before,
you could take a full matrix, or sometimes called
a dense matrix, sort of a matrix in regular format,
and you can cast it to being a diagonal using diagonal,
or you could actually create it by just simply saying,
what are the diagonal entries?
OK, I've put it over here as well.
So I guess this is always a good idea.
That's kind of an obvious idea.
But for any kind of data structure at all,
any kind of algorithm, we're always
trying to look for structure where it exists.
So let me now introduce the idea of sparse matrices, which
is kind of what we've been doing already,
but there's sort of a more general concept.
A lot of people would not call a diagonal matrix a sparse
matrix, though technically it fits in.
But maybe a lot of people wouldn't
call a square a rectangle, but technically a square
is a rectangle.
I mean, if something's a square, you
should call it a square, because that gives more information.
So what's a sparse matrix?
So a sparse matrix is a different, yeah.
A sparse matrix is, let's give the official definition,
is a matrix that has many zeros worth storing
in a sparse structure.
And I'm going to tell you what that means in a minute.
OK, that's sort of what a sparse matrix is.
What did I do?
Do I have too many quotes here?
Let's put it, can I do this?
OK, all right, so here's a very simple example.
Here's a matrix that obviously has three non-zeros,
the 12, the 9, and the 4.
And here's a sparse representation of the same matrix.
And for the human eye, this is actually not how it's stored.
But for the human eye, you could say, well,
the ij, the row and column index for the 9 is 3, 1.
And sorry, the third row and first column is the 12.
The first row and third column is the 9.
And the bottom right, the 3, 3 entry is a 4.
And all the other entries are presumed to be 0.
And if you use the sparse arrays Julia package,
you can go and see what's inside one of these matrices.
And it's a little bit tricky, and I wasn't really
planning to go into this, but it's not that bad.
So I think I will show it to you to see how a general sparse
matrix is stored.
And maybe this is why I don't think
of diagonal matrices as sparse matrices,
because they have a special storage.
So that's why I don't like to think of them
as a general sparse matrix.
But if we dump the sparse matrix, what we see
is besides the sizes, the m by n, the rows and columns,
being 3 by 3, there are three other vectors stored
on the inside, a column pointer, a row value,
and the non-zero values.
And it's kind of better to go from bottom to top, which
are done over here.
So the non-zero values are easy to understand.
They're just the 12, the 9, and the 4.
Those are the non-zero values.
That's the easiest.
This is how it's stored internally.
It's called CSC, or compressed sparse column format.
There are other formats.
This one has become the most common for certain applications,
because it generally is good for matrix vector products
and column slicing and so forth.
But yeah, so it's made up of 12, 9, 4, the non-zero elements.
And the row values are just the indices, the i indices, 3, 1,
3, the same 3, 1, 3 that you'll see over here
at the first column.
So the 12 is in the third row, the 9 is in the first row,
and the 4 is in the third row.
But instead of storing j, we're storing something
a little bit different.
We're storing the column pointer thing
is always of length 1 more than the nz val.
So if this is 3, this is of length 4.
And what it's doing is it's a pointer into nz val, which
tells you where the first non-zero is in that column.
And that might even be in the next column.
So for example, let me put an 8 here,
and then I'll change it back so you can actually
follow this along.
Yeah, so I want to be able to see this at the same time.
Can I do that?
I want to make it one smaller.
Hope you'll be able to still see it.
OK, so what this says is you'll notice that the elements put
a few more entries in, maybe here.
So the 1 points to the first entry of this vector,
and it says 12.
And the 12 is the first non-zero in this column.
2 points to the second entry, and 8
is the first non-zero in this column.
4 says we're going to look at this, the 9,
and the 9 is the first entry in the third column.
And 6 actually doesn't point anywhere at all.
It kind of falls off the end.
And it tells you it's the way of indicating
there's no further columns.
That's kind of the way it's done.
So I guess you could have just stopped with n,
but this is the way it's done.
It's pointing to the one column afterwards.
Now the fun happens if I zero out the second column, which
is how we started out.
If I zero out the second column, then
something interesting happens.
It says, oh, the first non-zero is a 9 for the second column.
It's the second entry.
And by the way, for the third column as well.
OK, I wasn't going to go too far into this.
It's a little bit tricky, but this
is how sparse matrices are stored.
So the main point here is we store the values, we store the i,
and we do something a little bit tricky that's
somehow equivalent to getting the j.
But I'd like to show you an example where this may not
be a particularly good storage team.
Here, I'm going to create a sparse matrix that
is a million by a million.
It actually is, if you thought of it as a matrix,
it has a million rows and a million columns.
And the way I did that was I just
created an entry where the i equals a million,
j equals a million entry is the number 9.
So that automatically creates a million by million matrix.
And even though we only have three non-zeros,
and they're in three rows, this thing here actually
has to create an entry for each and every column.
So it's actually one more than the,
I mean, it's not one more than NZVal
because it has an entry for each column.
It's actually one more than the number of columns.
So there's actually a million and one entry stored here,
which seems crazy.
But that's exactly what it is.
OK, enough with sparse matrices.
Let's go on to a different kind of structure.
So up until now, in this notebook,
I was kind of thinking about storage structures
where you can somehow save.
You don't have to store everything
because the zeros were implicit.
Now I would like to talk about a different kind of structure
that's not a storage structure.
It's the kind of structure that comes from randomness.
And we're right now, for this course,
we're in the transition from module 1 to module 2,
where in module 2, we're going to be talking
about statistics and probability.
And so this is kind of a perfect segue
between the two modules.
So let's talk about how much structure there
is in a random vector.
So here, I'm going to create, this is Julia's command
to create, and again, maybe I should add this,
but I think we've seen it before,
I'm going to create a vector with 1 million entries
and all the digits are from 1 to 9.
So here's a bit of that vector v.
There's a more, here's a kind of one way
of displaying it if you want to see it,
but a million entries, there's the top 20 and the bottom 10.
So yeah, there's a million numbers.
And the first thing you might say when
I ask how much structure is there in a random vector,
you might say, there's no structure.
It's a random vector.
I mean, it's random.
It has no structure.
I actually like to say that randomness is itself a structure.
I don't tend to think of random objects
as nothing special objects or objects with no structure.
They have a lot of structure.
And so much of science and algorithms
is based on taking advantage of the structure of randomness.
The word random in common English
almost suggests that it's the opposite of structure,
but it's just not true.
So for example, we could take the mean and the standard
deviation, and some would say that there's
a structure right there.
For example, I took the mean of this vector,
and I want to compare it with the number 5.
And you know, the 4 digits, it's just about the number 5.
You could take the standard deviation,
and I happen to know that it's the square root of 23rds.
And I could calculate it.
You could see to a couple of digits, it's right there.
So this random object, I could get a new one.
Here, let's get another one.
I could do it many times.
There's something structured about this thing, right?
To 3 or 4 digits, the main and the standard deviation
are not changing, right?
So there's some structure right there.
I mean, sometimes statisticians and maybe professors
who've just graded exams would say
that the mean and the variance is the structure,
and the rest is not even relevant.
So sometimes people will sort of write down or note
the mean and the standard deviation.
And in some instances, just throw out the rest,
because that's perhaps all you really want to know.
So in terms of...
You also might want to know how many fives
there are in the list.
Well, then I would have to not throw out my data.
Right, but you could reduce the data
by just counting the number of 1s, the number of 2s,
the number of 3s, et cetera.
Yeah, you don't want me to do this, do you?
Here's how many fives were in this data set.
So...
And...
But I could also...
How do I do this?
Can I go...
I'd have to have statistics to go here's the...
I mean, you need probably one box.
You probably don't want to...
How would I do this?
This will be more fun, anyway.
For i equals 0 through 9, as long as you're...
It goes from 1 to 9.
I'm going from 1 to...
Oh, that's fine, it'll count the 0s, too.
Oh, that's why I said that's why I got
like 1 ninth of a million, aha.
I was wondering why I didn't get like 10,000 or 100,000.
Okay, sum of v is equal to i.
Okay, so here are the actual numbers.
So I remember when I was a kid,
they used to publish like the Powerball numbers
in the newspaper, whatever the lottery was.
Like people would, you know,
oh, this one seems to have more.
So we should go for that or something.
Or this one, this one, maybe the biggest one.
Nope, I can't spot it.
I'm sorry, this one seems to be the biggest one.
Yeah, so maybe the number four comes up
a little more often.
So maybe you should choose the number four.
Of course, that's ridiculous.
But yeah, this might be something
you might store away, for example,
and then throw away the rest of the data, okay?
So for those of you who don't know what the mean is,
but I bet you everybody does,
that's just the sum of v over the length of v.
And the standard deviation is maybe
a little bit less familiar.
What you do is you take your vector
and then you demean it, right?
And you subtract the mean from the vector
and then you take the sum of the squares,
which kind of gives you sort of a distance
from the mean squared thing.
And then there's always that mysterious,
and then you don't take the average by length,
but you subtract one,
and people owe that to the degrees of freedom.
I always hated that explanation,
but that is what people do.
And that is the variance,
and then to get the standard deviation,
you take the square root.
And by the way, that's what really does.
When you take it to v,
you get the exact same number,
it's exactly this formula to get the standard deviation.
Okay?
So yeah, sometimes the summary statistics
are all you want, but sometimes not.
All right, let me go to another kind of structure,
multiplication tables, okay?
So I'm going to define an outer product function
of two vectors.
And all I'm doing is I'm taking all the possible ways
of multiplying an element of v with an element of w.
For example, if I'm taking outer of one through 10,
one through 10, I would get
what would be an ordinary multiplication table.
And I think I've kind of added it to a slider,
which is more fun.
So here's the 10 by 10 multiplication table.
Last time when I did this last semester,
David told me that in England,
everybody learns the 12 by 12 multiplication table, right?
So, but nonetheless, here's the multiplication table.
You can let us know what you,
in my school, it was 10 by 10.
I didn't go to as good a school as David went to.
You rapidly forget it,
soon after you learn it.
Oh, I know that my 12 by 12 multiplication table,
it just wasn't taught in my school.
I've never forgotten this.
I should get what happened yesterday,
but my 12 by 12 multiplication table, I got done, Pat.
That's no problem.
So, but just to look at few more outer products,
instead of going through one through 10, for example,
you can take two, four, six and 10 hundred thousand,
and you could see all the possible ways
multiplying a number here with a number here, okay?
And so, a multiplication table is a kind of structure,
but it's not sparse, I mean, there are no zeros.
It's not a diagonal matrix,
but it's clear that you don't need
M squared numbers to represent this object, right?
It's got structure, but it's not a sparse structure.
The interesting thing is that
in many, many applications of matrices,
there is structure, but it's not sparse structure
that turns out to be truly important.
It's sometimes a slightly more hidden structure,
but here you can see that the structure
is that of a multiplication table, right?
Sometimes it's not obvious that it is a multiplication table.
For example, here is a bunch of numbers made up randomly,
and I don't know, maybe you're better than me,
but I look at the three by four matrix,
and I'm not sure I would recognize
that it is a multiplication table,
that it comes from every product of one vector and another.
I mean, maybe you could look at the magnitudes
and start to guess, but it's not so easy,
just not by the M and I.
Here's a sort of picture of a 10 by 10 version,
and maybe it's a little bit more obvious with the picture
because multiplication tables
sometimes tend to have this sort of sparse structure.
So you might guess that it's a multiplication table,
but what we're gonna do is actually factor out
the multiplication table if it's there.
And so here's a little code that will factor it out,
and more or less doing the obvious,
which is a multiplication table can be obtained
by taking the first row and the first column,
and then kind of dealing with the one-one elements.
And so here I'm extracting the first row,
here I'm extracting the first column,
and I'm dividing the first row by the first element
as long as I can, and then I'm just,
and now I wanna see, is it really a multiplication table?
And so if the outer VW is about the same thing,
I'll say it did, otherwise I'm going to call an error.
And so here's where exceptions are thrown in Julia.
This is a way to kind of talk about exceptions.
If this doesn't work out,
we'll say the input is not a multiplication table.
Okay, so you could see I can factor, for example,
one, two, three times two, two, two,
but when I do that, I may not get my inputs back,
and equally valid V and W would be two, four, six,
and one, one, one, right?
Because every time I've doubled this one,
and I have this one, which amounts to the same thing,
but there's this entire,
there's always sort of like an infinite number of things
that could have worked out,
and it doesn't really matter which one.
But look, if I try to factor a random two
by two, the error is working like I want.
Now, I know when you see this black in these lines,
one, two, three, you think, oh my God, I have a bug,
but no, I actually caused this to happen.
This was deliberate, right?
I am calling it with a random matrix,
and it is not a multiplication table, right?
Most matrices are not multiplication tables, okay?
However, yeah.
So the question is, can we find the structure?
Can we find multiplication tables?
Is there a way to do it?
And the truth is we could do even better than that.
If we have something that's, for example,
the sum of two multiplication tables,
there is a technique for finding both of them, actually,
to find two outer products.
And the magic thing that does that
is the very famous similar value decomposition,
which you may see in another course,
or you may see it in an upcoming course,
you might have seen it before,
but it is probably the single most used linear algebra thing.
It's one of the most,
certainly being used in statistics these days.
There are some people who would say
that starting linear algebra classes
with gas elimination is no longer the right thing to do
because the SPD is the big thing these days.
So the similar value decomposition
actually finds the structure.
So before I even tell you what it is,
let's just do it and just see what happens.
So here, I'm gonna take this matrix, okay?
And I'm going to, there are a lot of times
I actually run a function
before I even know what the name means.
I'm doing that with you.
Maybe that's a kind of computational thinking,
which is just play with something, right?
So I'm gonna play with the SVD.
And I guess you could see how it's called.
You could see that it has three upper arguments.
I assume things in front of me, so I can quite see this,
but you could see an example somewhere here,
like here, you know, US V comma V is usually three outputs
to the SVD, so you could see that over there.
So I'm gonna use a common notation,
which is U, sigma, and V, okay?
And I'm going to check that I found two outer products.
So here, look, outer of this plus outer of this,
and this should recover my original matrix,
which is A, you see?
So this magic thing is actually extracting
the multiplication tables
that are sitting inside of the matrix, okay?
And the cool thing is,
is it could be done approximately as well.
I'm gonna skip this just for a moment,
because I think Dave's planning to talk about flags,
but let me show you what happens.
You know, you know, mean by now, I like to take SVDs.
I like whatever you can do with matrices.
I like to do it with images.
So I think it would be fun to do this with an image.
So here's an image of a tree,
and what I'm gonna do, first of all,
is separate out the red, green, and blue components, okay?
And then what I'm gonna do
is separately add the multiplication tables.
And so when I go to the end,
I've got sort of the full tree.
When I do one, I've got the one multiplication table.
In linear algebra language,
it's called the rank one matrix, right?
So the red and the green and the blue
are separately multiplication tables,
and it comes out plaid, like you saw before.
You could have the rank two tree,
and this looks sort of Minecraft-like, right?
I mean, everything's sort of rectangular, I guess, right?
So if you've ever played Minecraft,
I think this is what my kids play all the time.
I see it on your screen.
This is what it looks like, okay?
When you get to the sum of three multiplication tables,
it's officially called rank three.
You start to get a couple of more boxes,
and then as you start adding a rank,
you can see that, you know,
we start to get a better and better version
of the tree with some artifacts, right?
Which start to disappear as you add more and more rank, okay?
So this is sort of meant to be the quick introduction
to the SVD.
There's still much more that can be said about it,
but right now, for the purposes of this lecture,
I want you to understand
that the singular value decomposition
is a way of breaking up a matrix
into a sum of multiplication tables,
and by a multiplication table,
I just mean outer products, right?
And the SVD does it in such a way
that if you only have rank one,
you've got the best possible rank way of doing it.
If you have rank four,
it's the sum of the best possible
by some sense of multiplication tables.
So I'd like to leave it at that for now.
Like I said, there's so much more, but...
I think you can just talk about flags, I think.
You want me to talk about flags?
All right, I will do the flags then, right?
Let's go back up to the flags and talk about flags.
So where we left the story is here.
So let's see, where was I?
Or I can, whatever.
Yeah, I think, yeah, why don't you go ahead, Dave?
I'm gonna turn this over to you.
And for those of you who joined late,
let me again remind you that if you're watching us
on Monday, you might want to keep track,
depending on where you are,
that we are going to be pushing the clock one hour forward
in most of the United States on Sunday morning.
But Europe is doing it two weeks later
and other places may not do it at all.
Most of Europe, not all of Europe.
Okay, Dave, you want to take over, take over the screen?
Yeah.
I'll show you, hold on.
Sorry.
Okay, can you see my screen?
We do.
Hi, everybody.
So, yeah, we're carrying on the same kind of topic.
As Alan said, we're transitioning
into the second module of the course
on data and probability and statistics.
And so let's look at this very nice subject
called principal component analysis,
which is the name from statistics,
which is basically the same as the SVD
that Alan was just talking about in the algebra.
So what we want to do is understand data.
If we're given some data, you know,
we've been thinking of images as our input data,
but of course there's lots of other kinds of data
in the world that are often adjusted
with big matrices full of numbers.
And we want to somehow extract information
from those matrices.
And that's the goal of this method
of principal component analysis
that we'll talk about in detail next time.
But let's start off by thinking again
about these multiplication tables
that we've just been looking at all out of product.
So here's this table of, again,
of just the same algebra that Alan was just talking about.
So here's a multiplication table.
It does not have to be square.
You could multiply numbers from one to 10
by the numbers from one to 12.
And then we get this dense matrix,
but it has a lot of structure in the sense
that the whole matrix can be reproduced
from just these two vectors.
So instead of 120 numbers, I just need, you know,
10 numbers plus 12 numbers and the information,
the extra information that this matrix is created
by doing this particular operation.
So if I have those pieces of information,
then I can reconstruct the whole matrix
and I don't have to store the actual matrix.
So you could just store those two vectors
and do something very analogous to what Alan did
with a one-hot vector and make a new type
where you defined get index to, you know,
to extract the six, nine component by,
it would actually, in the moment, multiply those numbers
together and return that value.
So exercise the reader, implement that, you know,
outer product or what our multiplication table type
that does exactly that.
So the point is that each column,
if we think about what is the structure of this,
what does this matrix look like?
What is a, what is an outer product
or a multiplication table?
It means that each column is just a multiple
of any of the other columns.
So in this case, you know, this eight column
is a multiple of the six column, which multiple is it?
It's h divided by six times this column.
So that's, you know, one point,
one point two, six or something times this column.
And each column is actually a multiple of any other column
and each row is a multiple of any other row.
So it has this sort of very nice structure,
but as Alan said, you know, it's not obvious
just by staring at the table, but that's the case.
And so we want a way to actually realize that that's the case.
And flags, flags as in flags of countries
provide a pretty visual example of this.
So here's a flag that, you know, corresponds
not with those colors, but that kind of shape corresponds
to various different countries.
Flags, I'm not very good with flags,
so I don't actually remember which countries
go which way around, but you know,
you could either have these horizontal stripes
or vertical stripes.
And if you think of those as numbers like this matrix,
this table of data here, you see that, oh yeah,
that is a rank one matrix.
That's what we're going to call matrices,
which are just outer products,
which are given by exactly an outer product, right?
So what is this outer product is,
it's just given by the vector one point one and two point zero
on the vertical, in the vertical direction.
And then in the horizontal direction,
in this particular case, I'm just multiplying,
you know, each column is exactly the same.
And so I get exactly just repeating the same pattern
in each column.
And so I get these three colored stripes
that are exactly the same all the way along.
But a more general rank one matrix for outer product
would have different values, right?
So I'm actually multiply, I can multiply,
as I said, each column by different amounts.
And then if I draw that, I get a different kind of flag
that I don't think corresponds to any country,
which is this particular thing.
So a more general outer product looks like this.
I mean, you have to get this blocky structure
that Alan was talking about.
So let's just generate some bigger matrices
with this outer product or rank one shape or structure
and look at what they look like.
So they look something like this.
Do you see that we get this kind of nice,
you know, you could probably sell this
in a modern art gallery.
But it has this very particular structure
with these columns of color and then there's rows of color
and this sort of checkerboard shape.
And that's what a rank one matrix looks like.
So, you know, if someone gives you a picture like that,
you could sort of try and guess, oh yes,
that is a rank one matrix.
In other words, I could write it
just using one single column and one single row
and multiplying them in this way.
But now what about around,
what if we start doing this reconstruction
that Alan just mentioned and we start,
and we now look at a rank two matrix.
So that means we take one of these checkerboards,
one of these outer products
and we add another one to it.
And that looks like this.
So you can see that,
well, we're starting to lose some of the checkerboard structure
but it's still sort of there, right?
So if you sort of see checkerboard structure,
you might imagine that it's just a sum of two outer products
or maybe three.
And, you know, if I run this cell again,
we can see a different version of that picture.
And each time we run it, we'll just get different pictures.
The colors correspond to the values, you know,
I guess yellow and red are high values of close to one
and blue is close to zero.
And yeah.
And that also depends on how,
of course, on how much randomness you have.
And so it starts to look less regular
but it still sort of looks,
has basically the same structure.
And so you could visually guess,
oh, you know, maybe I can represent this matrix
with less information than the whole matrix.
And so, you know, if you needed to transmit this data
to somebody who wants to send an image over the internet,
can I actually reduce?
So part of the question is,
can I reduce the amount of information I need
to send to somebody, right?
And then I can have better video
because more people can send, you know,
video over the internet at the same time
because I'm actually being able to compress
the amount of data I need to send.
That's sort of in the background of all of these questions.
Okay, so now what about,
so there I took two rank one matrices
and I added them together.
So again, rank one,
the rank is how many multiplication tables
or outer products do I need to add
to exactly reconstruct the matrix?
That's called the rank.
So now let's do something slightly different.
So now let's take a rank one matrix.
Here it is, the same one.
And we go to add random noise to each element separately.
That's what this is doing.
This random n function means a random number
with a Gaussian distribution or normal distribution.
So it's centered at zero.
It has standard deviation one
and it has a Gaussian shape.
And we're sampling noise from that distribution.
So we're choosing noise in such a way
that the probability that we choose the value
is given by this Gaussian distribution.
And we're adding that separately
to each pixel in this image.
And what do we get?
We get something that looks like this.
It's commented on the YouTube channel.
And I have to agree that these pictures
kind of seem reminiscent of pictures of CPU,
the chips pictures that you see.
That's true, yeah.
Yeah, but because somehow CPUs are made
in a square grid, right?
Yeah.
So if I add more noise,
if I add a higher intensity of noise,
then the image starts to degrade
and you get, it looks literally more noisy.
And, but it still kind of has this structure.
So again, you would sort of like to say,
well, this new matrix is close to this structured matrix.
Even though this one actually,
if you try to use the SVD
to exactly reconstruct this image,
you will need to have to sum a very large number
of outer products, which is equal to the dimension
of the size of the matrix.
In other words, the smallest one of these,
what would you call that, Alan?
The minimum of the width and the height of the matrix.
I guess so.
Yeah, so, but, you know,
it still looks like the original one.
And you would really like to say,
oh, well, it's basically just this one.
And so the question is, how can we actually do that?
How can we decide that this matrix
is just a noisy version of this,
in other words, is close to this one?
So we need some notion of how far two matrices are,
one from the other.
And then we need to be able to say,
oh, yes, you know, the rest is just noise.
And so that's the goal of PCA,
which is principal component analysis.
And so what we're gonna do is think of these images.
So so far we've been thinking of images
as visual objects that we like to look at.
But now we're going to think of an image as just a matrix
and the matrix represents data.
And really what we're thinking of is,
oh, actually I'm getting some data in a table from somewhere,
in which we'll call a data matrix.
Okay, so let's just take two rows of the image
and think of that as some data.
So this is my original image,
which I created as a rank one matrix.
So as an outer product,
by, you know, again, taking a multiplication table
of this vector and this vector.
That's how I created this image.
And then we'll also take the noisy version of that
that I just made.
And I'm gonna put the noise back down,
the magnitude of the noise back down.
So it's just a bit noisy.
Okay, and what we're gonna do is extract the first row
of this image as a vector of data
and the second row as another vector.
So here, you know, this is one of the rows.
It's just literally a vector of floating point numbers
between zero and one.
And we're gonna think of that as the data that's coming in
that represents, you know, some house prices in Boston.
That's a famous data set or, you know,
some medical data on patients, lifespans, et cetera.
Any kind of-
An hour or 52 cents?
Yeah, so there's some normalization going on.
So you actually have to multiply these numbers
by one, two or $3 million.
And then you'll get the correct house prices.
So what are we gonna do?
We're actually going to view this data
in a completely different way.
We're going to, you know, move to more
a kind of scientific point of view, maybe.
We're actually gonna plot the data.
So what is the representation here?
So on the x-axis, I'm plotting, you know,
I'm plotting points at positions x comma y
where I literally take the x value as being
the first, you know, coordinate in this image,
in this data matrix.
So these are the x values in the first row
and the y corresponding y values in the second row.
So the columns, I don't know how to highlight a column.
There is some way I looked up the other day, but anyway,
the first column is a pair that I'll represent, you know,
as that x comma y coordinate on my plot.
And what I get is this.
So what are we looking at?
The original rank one image is exactly these red squares
and they lie exactly along a straight line.
So what does that mean?
It means that in the original image, each y-coordinate
is some number times the corresponding x-coordinate.
And that number is the same for all of the points.
Why is that?
Because it's an algebra product.
And so, you know, there is this exact relation
that each column is a multiple of the other column.
And so when I divide the y and x in each row,
in each, sorry, in each column, in a given column,
when I divide the y value by the x value in that column,
I always get the same result.
And so they lie on a straight line,
y equals that number times x.
And then the noisy image, I'm adding noise
to basically the x and y coordinates.
And that spreads out slightly
into this blue set of data.
Right, so basically what I'm trying to say is
that the data that's coming in from my data source,
it probably looks like this blue data.
And if I were just to see that blue data,
I would see, oh, that seems to be clustering
around a straight line.
And so actually what I would like to do is extract
what straight line it seems to be clustering around
and quantify what is the width
or the sort of spread around that straight line
and say, oh, that is actually small.
That width is small compared to the length of the data
in the other direction.
And therefore I would like to conclude that,
oh yes, maybe this data is basically
just a straight line plus noise.
And so that's the idea of, you know,
the idea behind principle component analysis.
And we'll see a computational thinking way
to approach that algorithm in the next lecture.
Great, thanks Dave.
So I think again, we'll say goodbye
to the folks on the internet
and we'll keep the zoom up in a little longer
to see if anybody wants to chat.
So see everybody on Monday, 1 p.m. Eastern daylight time.
Dave.

This video is sponsored by Brilliant. More on that later.
Believe it or not, there's drama in statistics. Not like real drama, but like nerd drama.
This drama is between two approaches for doing statistics.
One approach is frequentist statistics. That's going to be really annoying to say.
The other is Bayesian statistics. Frequentist statistics gets its name from the word frequency,
based on the fact that probability is interpreted to be the same as a frequency.
On the other hand, Bayesian statistics gets its name from the use of Bayes theorem.
From the Bayesian perspective, probability represents a degree of belief about an event,
and its degree can change based on a person's prior knowledge.
If you've learned statistics before or have watched my past videos,
then most likely you've learned frequentist statistics.
Whether we like it or not, it's the dominant form of statistics taught in schools,
and anybody who wants to work with data has to know how to do it.
On the other hand, Bayesian methods are often neglected,
and many students will go on barely interacting with such an important part of the field.
Using myself as an example, there's only one optional class for learning Bayesian
statistics at my own university. On the other hand, you're expected to take
two years with the frequentist statistics in order to get your degree.
Even though it's not as widespread, Bayesian statistics are still used to improve people's
lives. Many clinical trials have used Bayesian methods to get new treatments into the hands
of people that really need them. Some of you might remember some pandemic happening a few years ago.
Companies like Pfizer and Moderna were rushing to make a vaccine so that we could finally step
away from tracking turnip prices and animal crossing. Moderna used a frequentist approach
to show that their vaccine worked, but Pfizer used a Bayesian approach.
Bayesian statistics is really powerful, but at the same time, they can be really hard,
especially for someone who's just starting to learn it. Given that there's not many
classes for it in university, it's often left to the student to pick it up for themselves.
And take it from someone who studies full-time, self-studying isn't always easy.
So in this video, I'm going to give you a crash course on Bayesian statistics.
If you're new here, I'm Christian, and this is Very Normal, a channel for making you better at
statistics. I tried thinking of a statistics pun to put here, but none of them were sufficient.
Let's get started. Level one, Bayes theorem. To understand Bayesian statistics, you need to
understand the theorem that gave it its name. Bayes theorem originally comes from probability,
not statistics. It's named after Reverend Thomas Bayes, who published about this theorem in 1763.
Bayes theorem goes like this. Given two events, a and b, the probability that both of these events
will happen can be written in terms of an intersection. This intersectional probability
can also be written in terms of a product, a product of a so-called conditional probability
and a marginal probability. In this case, I've written as the conditional probability of a,
given that b happened, multiplied by the probability that b happened. But this product can
also be expressed the same way with the roles of the events reversed. Since these expressions
are equivalent, we can form a relationship that links these two conditional probabilities.
Just by moving this marginal probability over, we get Bayes theorem. That was easy to describe.
How should we interpret this relationship? The key part here is here and here. We call this part
of Bayes theorem the prior probability. You can think of this event as your prior knowledge that
an event will happen. It could be something as simple as you subscribing to this channel,
which you should by the way, and this is an updated probability that a will happen
given that we saw b happen. So we're going from our prior belief that the event will happen
to this posterior belief. Given that some other event b happened, Bayes theorem tells us how to
update our degree of belief that this a event will happen. If the b event was watching this video,
then this posterior probability would be the probability you subscribe,
given that you watch this video. If I've done my job right, hopefully this probability is higher
than before you watched it. But what does that mean? Well, I can't control your prior probability
of watching this video. I can't control these other events involving you watching this video.
This term is a conditional probability that you watch this video, given that you were a subscriber.
According to Bayes theorem, if I want to improve my chances of getting a new person to subscribe,
I need to increase the probability that my current subscribers watch my videos. This term in the
denominator is tricky. It represents the marginal probability that someone will watch this video.
There are several different ways that new watchers will come across my video,
and it's hard to account for everyone. I would need to account for all these different ways
that someone might see my thumbnail and watch. It's easy to stick this probability in the
denominator and call it a day, but it's a completely different beast to actually calculate.
And spoiler alert, that's going to be a recurring issue throughout this video.
For many people, they hear about Bayes theorem in their probability class,
get a homework question on it, and forget all about it. Maybe some of you in analysis positions
had to answer a Bayes theorem question for an interview. This simple expression inspired an
entirely new way of thinking in statistics. But in its current form, Bayes theorem doesn't tell
us anything about statistics. To see where that comes in, we need to go a level deeper.
Level two, Bayesian statistics. Bayes theorem tells us how to update our beliefs about events
happening. But statistics is about making useful inferences and predictions from data through
models of the world. So how do we make the jump from this to this? In statistics, we deal with data,
and the inherent problem with data is that it's random. Randomness makes it harder to learn from
the data. We statisticians got to get that bag, so we assume that this randomness has a predictable
form or structure. We describe this structure using a probability distribution function,
or PDF. PDFs are functions which describe which values we're most likely to see,
which ones are rare, and which are impossible. We want to learn more about this PDF because it
can tell us about important qualities about the possible values. For example, we can calculate
an average or typical value, or we could learn about the range and spread of what values are
possible. What gets in the way is that PDFs are hard to estimate. Functions are generally
infinite dimensional, and no one is paid enough to estimate infinitely many things.
One strategy for getting the PDF is to approximate it using a parametric family. With the parametric
family, we can get an entire function just by estimating a few values called parameters,
and I'll use data to denote a general parameter. These parameters often represent values we want
to know or study about a population. The binomial family has a single parameter,
which we denote as p. This p represents the probability that an event will happen. For
example, if a certain vaccine protects someone from a certain virus. Once we collect the data,
we try to use it to estimate these parameters and learn more about the world from this estimate.
This is called statistical inference. Depending on who you ask, they'll say that this should be
approached from different ways. The frequentists will tell you that population parameters are
fixed and unknown, and you can estimate them with a method like maximum likelihood estimation.
With infinitely many data, this estimate will be theoretically close to the true population value.
Then they'll turn to a hypothesis test. With null hypothesis testing, you're just saying that the
data is unlikely to have come from a particular hypothesis or parameter value. It's literally
the p-value. But for a lot of people, that's not what they want. Instead, they'll want to know a
likely value or a range of likely values for the parameter, not what it probably isn't. What they
want to know is the probability of the parameter value after collecting some data. Frequentists
wish they knew this, and some trick themselves into thinking that's what they actually have.
And this is where the Bayesians come in. Let's have another look at Bayes theorem.
Instead of looking at two simple events, we'll change our notation to deal with two random
variables. The data that we'll collect, which I'll denote as d, and the parameter value theta.
We're working from the Bayesian perspective now, so the parameter is considered to be a random
variable. This is very different from the Frequentist view, where the parameter is thought to be
fixed. After replacing these simple events with these two random variables, we get the version
of Bayes theorem that's used in Bayesian statistics. It's the same theorem, but the level of difficulty
has increased. Let's walk through it. This term is still the prior, but it's no longer just a
single probability, but an entire probability distribution, which we call the prior distribution.
The prior distribution represents our beliefs about which parameter values are likely and
unlikely. Not the data we collect, but the parameter. As an example, let's use the response rate of a
binomial distribution. Depending on the shape of the prior distribution, I can reflect different
beliefs about this response rate. If I have no knowledge about what this value could be, then
any value is likely as any other, and you'd represent this using a completely flat distribution.
This is called an uninformative prior. On the other extreme, I might be convinced that the
success probability is 40% and that it can't possibly be anything else. In terms of a probability
distribution, this belief may look like a sharp spike at 40% and almost zero everywhere else.
This is an extreme version of what's called an informative prior. In more practical applications,
you could talk to experts or refer to previously published papers to form a good informative
prior. This is called eliciting a prior. Between these two extremes of uninformative
and informative, there's a spectrum. The subjectivity can make some people uncomfortable,
but the prior is what makes Bayesian statistics so powerful. It forces you to consider the
probabilities of different parameter values before you collect any data and make these
probabilities explicit to others. Nothing will stop you from conducting an analysis with an
extremely opinionated prior, but then someone can challenge your findings by challenging your prior,
which opens up the possibility for discussion. On the other hand,
frequentist analyses totally ignore the prior probability of any hypothesis.
What frequentists focus on exclusively is this term. This conditional probability is what's
known as the likelihood. The likelihood is the joint probability distribution of the data,
assuming a particular value for the parameter. Remember that we often model the data as coming
from its own parametric family. From the Bayesian perspective, once you condition on a particular
parameter, you can check how likely the data would have come from this particular value.
The importance of the likelihood is that it allows the data we observe to influence the shape of the
posterior distribution. This term in the denominator represents the marginal probability of observing
the data. To get this probability, you have to calculate a difficult integral, integrating
over all the possible parameter values. Despite how it looks, after the data is actually observed,
this term is just a number. A difficult to calculate number, but still a number.
Once you perform all this theoretical computation, you finally get the posterior distribution,
which represents your updated belief about the likely and unlikely values of the parameter
after observing the data. The posterior distribution is the primary object that Bayesian
statisticians interact with. Like the prior, the posterior distribution is also a pdf.
You can figure out the mean of this posterior distribution, and you can calculate a range
of values that contain a particular amount of probability, like 95%. In Bayesian parlance,
this is the credible interval, and it's what confidence intervals wish they could be.
Looking back at the Pfizer paper, you can see that they report a 95% credible interval.
Taking a step back, we can see that this new version of Bayes theorem is more complex. Bayes
theorem tells us that you can get a new probability distribution by multiplying these two functions
and dividing by this complicated integral. This calculation is so complicated that we
usually try to simplify it. The main way we do this is to choose a special prior.
The likelihood is usually decided by the data, but we can choose our prior. If chosen carefully,
it's possible to get an easy expression for the posterior distribution. As an example,
let's consider the case where the data comes from a binomial distribution. If we make our prior on
the response rate a beta distribution, then it can be shown that the posterior distribution
will also be a beta distribution with updated parameters. You can see that both the prior
parameters and the data influence the parameters of the posterior distribution. In simpler terms,
the more successes we see in the data, the more the posterior beta shifts towards one.
The more failures we see, the more it shifts towards zero. The end result is a posterior
distribution that is informed by data. If the prior produces a posterior that comes from the
same family, we refer to the prior as a conjugate prior. Here, both the prior and posterior are
betas, which can be said for a lot of people in my life. Since the likelihood was binomial,
we refer to this as a beta binomial model. In fact, this was the model used by Pfizer
to demonstrate that their COVID vaccine work. Conjugate priors make things easy,
but they also constrain how we can represent our beliefs through the prior.
If we want to go past conjugacy, we'll have to dive yet another level deeper into Bayesian
statistics and face some scary math. Level three, beyond conjugacy. What happens if you try to use a
non-conjugate prior? Well, the recurring villain in Bayesian statistics is this integral you need
to calculate to get the posterior distribution. See that? That's the prior right there. For a
general prior, it's not guaranteed that this integral can be written in what's called an
analytical form, which is just a fancy way of saying we can't write an equation for it. If we
don't know the form of this integral, then we also don't know about the form of this posterior.
Like I mentioned earlier, if you know what it looks like, you can calculate important aspects
about a random variable purely from the PDF itself. What if I told you we could get all these values
based on the probability distribution, even if we didn't know what it looked like? A probability
distribution just describes the randomness in a random variable. So what if we used samples
of this random variable instead? If we could generate samples from the posterior distribution,
then the samples themselves can be used to estimate all these values. It's indirect,
but if you have a lot of data, then you can get some pretty accurate estimates. It's not just
the frequencies who can benefit from asymptotics. This is the approach taken by Markov chain
Monte Carlo algorithms, or in short, MCMC algorithms. MCMC algorithms construct what's
called a Markov chain. A Markov chain is a sequence of numbers where the value of one number in the
chain is dependent only on the value that came before it. The elements of this chain form a sample,
much like a dataset. What's special about the MCMC algorithms is that they construct this Markov chain
such that, almost by magic, the probability distributions of the samples are the posterior
distribution. And MCMC will work even if we don't know the form of the posterior itself.
Another approach that I'm just starting to pick up is variational inference. The idea behind
variational inference is that we want to approximate the posterior distribution with another distribution
that's close to it, but easier to use. Instead of directly dealing with a difficult integral,
once you have a good look alike, you can estimate all your posterior quantities using this approximation
instead. That's my best take on it, I still have a lot to learn about this technique.
MCMC and variational inference are examples of advanced techniques that you'll need to
pick up if you want to be a Bayesian statistician. I only gave very brief introductions to both of
these topics, but I wanted to give you a taste of how far we've gone from this simple little theorem
that the Reverend gave us in 1963. Conclusion. I hope that this video has taught you how to be
a little bit more Bayesian in a frequentist world. In the past, there was drama between the
frequentists and the Bayeans, but it's all majorly in the past. For all its weaknesses,
frequentist statistics have a long track record of achievements, and the same can be said for
Bayesian statistics. You may even be surprised to hear that there are hybrid methods using both
frequentist and Bayesian ideas. As a budding statistician, I'm always learning new things,
and I hope that I've helped you update your own priors. If you like my content, you can stay
informed by subscribing to my channel and the channel newsletter. In the newsletter, I talk
about topics I don't usually get a chance to cover on the main channel, like my ongoing research
or my video making process. Everyone starts off with uninformative priors, but it doesn't have
to stay that way. If you want to learn something yourself, you can take the initiative to update
your knowledge and your beliefs. And thanks to the sponsor of this video, this process is only
getting faster and easier. Brilliant is an online platform for learning math, computer science,
and data science. They offer courses that are updated every month, and you can solidify your
understanding through interactive exercises. First-hand experience in problem solving is the best
way to stress test your knowledge, and Brilliant makes this a top priority. Recently, I've been
working through the Math for Quantitative Finance course, since it's a topic in statistics that
I don't know much about, but want to know more of. To try everything Brilliant has to offer for
free for a full 30 days, visit brilliant.org slash very normal, or click on the link in the
description. You'll also get 20% off an annual premium subscription. Thank you to Brilliant
for sponsoring this video. Thanks for watching. I'll see you in the next one.

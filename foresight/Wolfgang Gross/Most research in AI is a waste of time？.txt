Science is getting less bang for its buck.
That's what Patrick Collison and Michael Nielsen wrote in an article in 2018 for the Atlantic.
Even with more and more money spent, and the number of scientific publications going up,
the number of significant scientific discovery stayed the same for the last 100 years.
And this is especially true for AI.
Most of the research in the deep mining world is a total waste of time.
Right. That's what I was getting at.
Yeah. It's a problem in science in general.
What went wrong? Is the next AI winter just around the corner?
Do we need to wait for the next big idea in AI to save us?
Or is AI research just suffering from the same replication crisis
that affects all of science at the moment?
And how do we as AI researchers or practitioners help the situation?
Is science becoming harder or more costly?
After all, in the first part of the last century,
there were seminal papers released with only one author.
These new physics papers that use particle accelerators have thousands of authors.
Given these extreme examples, but on average,
research teams nearly quadrupled in size over the 20th century,
and that increase continues today.
It's surprisingly difficult to measure scientific progress in a meaningful way.
And the best thing we have is talk to the best experts in the field
and ask them what meaningful progress is to them.
The data by Carlson Nielsen shows that researchers prefer noble price discoveries
from past decades over those of current decades.
The data also shows that scientific discoveries from the 1910s to the 1930s
repicked over those of current decades, especially for physics.
Noble price winners also got older over the decades,
from 37 years in the early days to 47 years now.
Many objections can be made against the survey.
But still, asking scientists is the best method that we have.
Maybe it's just harder to make great scientific discoveries after all.
The concept that goes along with that is that of a country to be discovered.
In the beginning, there's a vast emptiness,
and everywhere you step, a new discovery can be made.
After a while, only far-distant lands can be discovered or deep creeks,
and need a lot of time and resources to get there.
After all, you can only invent quantum mechanics or the theory of everything once, right?
Not quite. There's also a different point of view.
According to this, science is more like an endless frontier,
and the closer you look, the more things you can discover.
Like measuring a coastline.
The closer you look, the more details you see, like zooming in on a fractal.
Richard Feynman explained it like this.
If you try to answer a why question, there's always a next why question to be answered,
and the more interesting it gets.
So we shouldn't really run out of things to explain.
But what's the problem then?
A lot of research studies can't be replicated,
as has been shown in psychology.
An estimated number of 15-55% of studies
cannot be replicated, as meta-studies showed, to clarify things.
Studies that use experiments and do significant tests
are expected to be wrong sometimes.
That's exactly what the significance level is telling us.
So from 100 studies that get a positive test result
and use the p-value of 0.05, 95 of them get the true result.
But 5 of them get a positive test without a true relationship behind it.
Even with considering this, 15-55% is way over the expected 5% of being wrong.
So how does this happen?
First, statistics is hard and mistakes happen.
As Gerd Gigeritzer puts it in his book,
people aren't stupid.
The problem is that our educational system has an amazing blind spot
concerning risk-literacy.
We teach our children the mathematics of certainty,
geometry and trigonometry, but not the mathematics of uncertainty.
It's surprisingly hard even for experts to understand them
and communicate them well.
Psychology especially suffers from this problem,
as they heavily rely on statistics and empirical studies.
The lack of sound fundamental theories, like we have in physics,
does clearly not help.
The nature of the mind is just not so easy to observe.
And after all, psychology only split in the first half of the 20th century
from philosophy and became its own discipline.
There hasn't been that much time yet to build a sound fundamental theory.
So what's the problem with statistics?
Coming back to the initial observation,
we expect 5 of the 100 studies using a significant level of 0.05 to be wrong.
But the numbers show that a much greater numbers of papers are actually being wrong.
Research is not most appropriately represented and summarized by p-values.
This paper here points out where this could be the case
and shows some common fallacy that many research papers fall victim to.
The probability that research finding is indeed true
depends on its prior probability of it being true.
Among others, this prior probability is defined by the ratio of true relationships
to no relationships tested in the field.
In a field like AI, where there's often great flexibility,
which relationships we test,
it is less likely that these relationships are actually true.
In addition, if many teams are working on the same problem in isolation,
this actually increases the chance of an alpha error.
There is finding a relationship where there's actually none.
And this is exactly what's happening in a hot field like AI right now.
I hope that it is clear to you now that AI can also suffer
from the same replication problems that just discussed.
And we have the problem that AI can't do anything about it.
That interests from practitioners and researchers in the industry
often differ from those in academia.
And in this regard, Google and Meta AI more often promoting
their own proprietary research instead of advancing
actually research and knowledge discovery.
The point is, data does not speak for itself.
It must always be interpreted.
And how the data is generated plays a huge role in what results we are getting.
And second, there are wrong incentive structures for researchers as well.
As Professor Brian Nossick says,
there's no cost in getting things wrong,
but there's cost in not getting it published.
We can't solve all of these problems overnight.
But what I want to talk about is, how can we make it better?
Most people know that one thing, correlation does not mean causation.
But researchers want to understand the causal relationships around us.
We want to understand if adding two layers to our neural network
caused performance to increase or not.
We think causally, it comes very natural to us.
This is also why statistics is hard for us, because it's not about causation.
Luckily, Judah Pearl has an answer for us.
To answer causal questions, you must climb the letter of causation.
I just recently read his book, The Book of Why,
and looking at our problems through the causal lens is a very revealing exercise,
and I can highly recommend giving the book a read.
For example, try this experiment.
Flip two coins simultaneously 100 times,
and write down the result only if one of them comes up heads.
Looking at our table, which will probably contain roughly 75 entries,
you will see that the outcomes of the two simultaneously coin flips are not independent.
Every time coin one landed tails, coin two landed heads.
How is this possible?
Did the coins communicate with each other somehow?
Of course not.
In reality, you conditioned a collider by censoring all of the tail-tail results.
This is a simple example, but it shows how easy it is to introduce a bias
just by the process of data collection.
This is called a collider bias and is very common in practice,
albeit not always as obvious as in a coin flip example.
Rich Sutton wrote a blog post in 2019 called The Bitter Lesson.
The bitter lesson of AI research is that in general,
methods that leverage computations are far more effective by a large margin.
There is a debate on how much of the recent success in AI
can be attributed to algorithms and how much is due to the increased computation power.
Most often Moore's Law is called here, which states that
the computation capabilities are doubled every two years.
It's interesting to think about how Moore's Law can be extended in the future.
Can quantum computers save Moore's Law eventually?
Or what about distributed computation?
The number of computing devices in phones, cars and utilities also follow an exponential growth.
Or what about neural computing?
It is fun to think about these things,
but this should not distract us from the main point of Sutton's blog post.
The bitter lesson says that in 70 years of AI research,
methods that scale well with computation
are ultimately more relevant than methods that don't.
In contrast, methods that build around how researchers think
they would solve the problem only work in a short run.
There are plenty of examples of this in computer vision, NLP and chess.
Lex Friedman puts it this way.
I would love to see that when people publish papers today,
it maybe almost have like a section where they describe
if computation was able to be scaled by 10x by 100x,
looking 5-10 years down the future,
will this method hold up to that scaling?
Is it scalable? Is this method fundamentally scalable?
I think that's a really good question to ask.
One convincing demonstration of the power of this scaling law
is how transformers disrupted the world of NLP
just by leveraging the computation more effectively.
Before transformers, mostly RNNs were used in NLP.
But RNNs can process data in parallel.
The disruptive attention is all you need paper
had a simple but effective idea.
The new architecture allowed the models to process data in parallel
and thus allowed the researchers to train the model
on a large amount of data reasonably fast.
It also has been shown that scaling large language models
is the main factor that makes them so effective.
Also transformers are the basis of diffusion models
which can be trained to generate images for a given text prompt.
I think a similar story can be told on how
leveraging computation was the main factor that allowed AlexNet
back in 2012 to crush the benchmarks in computer vision.
As Alan Turing said in 1950, conjectures are of great importance.
They show us meaningful lines of research.
We need this overarching conjectures now more than ever
to guide us the way, to show us what is meaningful research
and what is a waste of time.
I think Rich Sutton's bitter lesson is a great conjecture in this regard.
In my opinion, looking at the history of AI is of great value.
It can show us which pattern repeated and what we can learn from them.
I hope this video also gave you some understanding of what good
and bad scientific discoveries are and how to find them.
We should especially look out for these hype topics
and take those discoveries with a grain of salt.
Leveraging computation might be one of the aspects of future AI,
but again it might be something completely different.
Conjectures take courage.
It takes courage to place integrity over safe bets.
Ryan Holiday says,
courage is honest commitment to noble ideas,
but it's not for nothing.
If you don't believe in anything,
it's hard to find anything worth believing in.
Courage cuts through the noise,
even if it means breaking some rules.
Check out this awesome project,
the ML Reproducibility Challenge of 2022,
and I bet there's one for next year as well.
They encourage and award researchers to investigate
reproducibility of papers accepted for publication at top conference,
especially if you're a student in AI.
I think it's a great way to have a meaningful impact on science right now.

Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud.
Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud.
Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'
ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud
o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r d
dweud e wicked
Go ahead, everybody!
Hello!
First off, um, some thank you's.
The most important one.
We at S own a fan Locities like to thank the McKinnon family foundation
who underwrites this entire
liliwsham tîn hefyd yllud o flyngeoliol,
ti yw'r amgueddfa Sabir na'r請oedd fiTVll I.
Felly, bydd e môl ychydig.
ye'r yw gweld cynnigadseitau mewn gweld mewn gweldiannau,
a ni'n tyfn ni'n gwoedd ar yektoedd ond bynnag.
Thank you very much for this lecture.
Very importantly, we would like to thank you, the members of the audience,
for participating in this series.
Again, again!
Very, very prosaicly, thank you for voting at the Santa Fe Institute,
this series yet again as the best lecture series
in this town and city of Santa Fe,
but also more importantly than what you're voting,
thank you for your participation.
rydw i'n meddwl cyblir ni dwi wedi'i effearntu se VR
wedi neger i ddim yn reunio
ac yn redes i gweithio hwnnw i rwylo weit ti gael
ydi hwnnw i kontos i g difrwyslaud ar ei gweithio
i hynny'r panfaith beirgo.
Efallai gyda ni wedi meddwl gweld eu hon diagnosed
o'r oedd y cwmfersiwyr.
Okay.
Vijay Baosubamaniaan.
Wow.
Okay.
How do I begin?
Let me start talking about his research interests.
And I'll be presenting a lot of quotes directly from him
to try to get the nuances correct,
even though I'm familiar with a lot of his work,
I'll collaborate with him and so on.
In Vijay's own words, quote,
he is interested in how natural systems manipulate
and process information.
Okay, well, at that level, you know, big deal.
Everybody here is interested in how systems
process information, right?
Well, God is in the details of what one's interests are
and with Vijay, those details are pretty damned impressive.
For example, in the way that Vijay
construes those particular words,
one of the ways that natural systems process information
is, well, you know,
the fundamental nature of space and time.
So for example, he has done pioneering work
in what is known in quantum cosmology
as the information paradox.
That's the fact that naively at least,
information seems to get lost into a black hole
even though the laws of physics forbid there being any loss
of information in the universe.
Following this theme, he's investigated,
again, quoting from him directly,
how the familiar smooth structure of space-time,
you know, that's what we're familiar with,
how it can emerge for more complex underlying
physical constructs.
So, you know, these are the kinds of things
that you and I might mull over while I'm taking a bath
or on the way to work this morning.
Let's see, I'll have to walk the dog when I get home
and buy a cup of milk and, oh yeah,
just what is going on at the Planck scale
at the event horizon of a spinning black hole
that's got a little bit of charge.
The kind of thing that everybody does
on the way to work in the morning.
Anyway, no surprise though,
the mind of someone like Vijay cannot be contained
in just the fundamental nature of the physical universe.
He actually takes a step back from that and asks,
well, how is it that we might actually come to our understanding
of the physical nature of the universe?
He's been interested in how it is that the human enterprise
of science comes to try to understand scientific reality.
Again, using his own words,
he has also worked on problems in statistical inference
and in particular, Occam's razor,
which is very interesting because all scientific theories
involve fitting of models to data,
trading it off versus the complexity of that model.
Again, just the kind of thing,
maybe this is for the shower rather than the bath
when you go into work.
In short, Vijay is somebody cast in the same mold
as the Santa Fe Institute is in general,
a person of very many interests.
Now to wax a little bit more prosaic though,
one difference between the SFI and Vijay
was that Vijay was born a little bit earlier than the SFI was.
Specifically, he was born in Mumbai, India, Bombay back then,
moved among many cities in India.
His family eventually moved to Jakarta.
Then he came to MIT in the US.
His degrees at first were physics and computer science.
Then he got a PhD at Princeton.
After that, among other awards, well, just achievements,
he was accepted as a junior fellow
with the Harvard Society.
Around this time, he first formed a connection with the SFI.
Since 2000, he has been a professor at University of Pennsylvania.
He's a fellow of the American Physical Society,
has received many awards, over 17,000 citations
in the academic literature.
So that's a little bit of the more prosaic kinds of,
well, he did this and he did this and he did that kind of details.
Anyway, now though, the third one of his major interests,
what he'll be talking about tonight,
this is one way to phrase it, it's how your brain,
the three pounds or so of pink mushy jelly
enscarced in your cranium,
that you know, it kind of wiggles around
when you do this too fast,
how it is that that, the information processing,
remember that's his underlying theme here,
how it is that the information processing
that goes on in that jelly becomes you, okay?
And I have personally been very fortunate
to have some interactions,
we've had many discussions,
Vijay and I on a topic related to that,
namely the energetic cost of the information processing in the brain.
Anyway, tonight you're really in for a singular treat
and with that, over to you, Vijay.
APPLAUSE
Well, David, thank you very much.
Oh, first of all, can you hear me?
Yeah, okay.
Thank you very much for that lovely introduction
that was really kind and it's a delight to be back in Santa Fe.
This is one of my favourite cities on earth.
I've always really liked coming here
and thank you very much for inviting me back
and thanks so much for coming to my talk today.
So great, so let me first of all check that my iPad works, it works.
Great, so the subject of today's talk
is how the brain makes you.
So we should start by first discussing what I mean by making you.
So you're an animal, like any other animal.
So the first thing we've got to ask is what do animals do?
And you know, here are a number of animals
from the OSA, Peninsula and Costa Rica,
these are photographs taken by my son,
who is also one of the animals there.
And what do animals do?
So the main business of being an animal
involves eating, navigating, exploring the world,
sometimes resting like this one here, manipulating the environment,
look at the nest and this web here,
interacting with each other, reproducing, things of this kind.
And that's what animals do almost all of the time.
To do so, they have to do certain things, right?
They have to be able to sense the world,
namely get information from the things around them,
the things, the other animals, the sky, whatever.
They need to process this information that's coming to them
and remember some parts of it,
those parts of it that help them predict what's going to happen in the future.
And after doing this, they need to eventually make decisions,
both in the short term and the long term,
and be able to plan out sequences of actions for the future.
So these are all things that an animal has to do just to be an animal.
Now, some animals do additional things.
They write poetry.
Here's one of my favorite poems by the poet Gerard Manly Hopkins.
They paint.
This is a painting done by my daughter, Aruna.
They write music.
This is by Heinrich Injatz Franz von Bieber.
I'm sure I pronounced that incorrectly.
He was before the Baroque.
And you know, some people write equations like a guy named Einstein
and a guy named Schrödinger wrote this equation.
So we theorize about the universe in this way.
Not all of this, we being human, seems very impressive to us.
And we're very interested in this.
I'm very interested in this. You're all very interested in this.
But in fact, the brain devotes very few resources to such activities.
It's important to us, but really, mostly, that's not what the brain does.
Mostly you, as an animal, like other animals,
mostly you eat, navigate, explore, rest, manipulate the environment,
interact socially with each other and reproduce.
That's what you do.
And then there's these additional things that we do on top of it.
So I'm going to focus most of the talk today
on these sorts of activities that all animals do.
And then we'll talk a little bit at the end
about these sorts of activities that interest us particularly as humans.
Now, to do all of this, what do we use?
The organ we use is the brain.
And I'm going to think about the brain today as a sort of computing engine
that produces the behavior of animals.
So here's the human brain in particular.
And what does it do?
Well, it does the things that I said animals need to do,
which is it gathers information from the world,
it learns from experience, forms memories,
and makes decisions for future actions.
So that's the job of this organ.
You know, the liver processes chemicals for you,
and this organ and the heart pumps blood,
this organ does this stuff with processing information.
It's the kind of computer that you have in your head.
Now, the challenge for this computer
is that it's a very big, complex, and often unpredictable world.
And you know, you have very limited resources.
It's this thing that you carry around inside your head,
it sloshes, it bumps, you know, stuff like this.
That's what you get to use.
And so myself, as a scientist in this area,
the question that animates me is I'm interested in what are the organizational principles
that allow the brain to meet such enormous challenges
that animal life faces.
So that's what I'm interested in.
Now, I'm trained as a physicist,
and a physicist posed to the question about how does this thing, this system, work?
The instinct of most physicists is that, well, you know,
what I'm going to do is I'm going to first work out what stuff this thing is made of.
What are the parts that make it up?
It's kind of an atomic instinct, you know?
Figure out the parts that make up a system.
Then you figure out how all the parts talk to each other, how they interact.
And then the idea is, if you know that well enough,
you can work out what the whole system is doing.
That's the kind of method that physicists get trained in.
And you know, it goes back a long way, hundreds and hundreds of years,
and that's the tendency, that's the approach to the scientific question.
So what's the brain made of?
Well, that brings us to the neuron doctrine.
This is Santiago Ramónica Hall, who lived from 1852 to 1934,
and he is widely considered probably the founding figure of neuroscience.
So what did he do?
He was actually, he first wanted to be an artist, by the way.
But then he wound up becoming an anatomist,
and famously what he did is he founded the neuron doctrine.
The neuron doctrine is the idea that the, if you like, atomic constituents of the brain,
the thing that the brain is really made of,
the stuff that does the interesting things in the brain,
are the neurons of the brain.
So here's a picture of a neuron.
This is a so-called Purkinje cell in the cerebellum that's in the back over here.
This is drawn by Ramónica Hall, and you know, he was a trained artist,
so he drew really beautiful pictures.
So his pictures to this day are wonderful artworks in their own right.
Now he got the Nobel Prize in 1906,
along with another scientist named Camilo Golgi for their work on anatomy.
Golgi did not propound the neuron doctrine.
The reason is they used different experimental methods,
different stains to stain cells,
and the nature of Golgi's stain was he had the impression
that all the neurons were kind of connected together,
and they formed a web work,
and there was sort of one thing that made up the brain,
and it was the very careful anatomy done by Ramónica Hall,
which identified that no, no, no, there are many discreet,
separate objects in the brain called neurons,
and the neurons talk to each other in some form or fashion
in order to produce all the effects that we call animal behavior.
So let's talk about neurons some more.
So you'll see that I'm not a trained artist,
so you'll see my drawings do not compare to this in either precision or beauty,
but let me explain what is a neuron.
A neuron is a cell like any other cell in your body.
So it has a cell body. Let me see, is that big enough?
Can you see that? Can you see the words? OK, great.
So, well, see the nice thing about iPads is you can increase it. See?
Is that nice? OK, so if you can't see it, tell me and I'll just increase it.
So a neuron is a cell like any other cell in your body
and has a little membrane that contains within it all the molecular machines
that make a cell work.
But neurons are particular cells that are intended to help your brain compute,
and as such, they have very specializations that allow them to do that.
So in particular, a computing element like a transistor or a diode or a resistor
needs to take in inputs from other stuff.
So indeed, if you look at a neuron,
it has this set of branches coming out from one end,
which is called the dendritic arbor,
and on the dendritic arbor, the dendrites,
those are the little fibres at the end,
and on the dendrites, there are the famous synapses that most people have heard of,
the synapses are little junctions where one neuron makes a connection with another neuron.
That's a chemical connection.
One neuron would dump something called a neurotransmitter,
and then sort of diffuses over to the other neuron, is eaten, taken up by the synapse,
and then that leads to communication that we'll talk about later.
Now the output, if you're a little computing device, you need to have an output,
and the output of the neuron goes through a wire called the axon,
and the axon then makes an axonal arbor,
which also has some synapses at the tip that's where the neuron communicates the next neuron,
whatever message it's going to send.
So your brain, the human brain contains about 100 billion neurons,
and it contains about 100 to 1,000 trillion synapses.
So there's not just one kind of neuron.
You know, if you go look inside a computer, a silicon computer,
there are a few kinds of circuit elements, right?
There are transistors, there are diodes, there are capacitors,
there are resistors, there's a repertoire that engineers use
to build up computers in general.
So in fact, in the brain, there are very many types of neurons.
And Cahal identified them by how they looked.
And for example, here is what's called a cortical pyramidal cell,
because it looks a little bit like a pyramid,
and these pyramidal cells are the workhorses of your brain.
All over the cortex, you find them layered upon layer
of cortical pyramidal cells.
This is this cerebellar purkinje cell that we mentioned.
Here's this huge thing, it has this huge dendritic arbor,
which it uses like a giant fan to catch inputs
that are passing perpendicular to it.
It does that as part of controlling your body
and allowing you to move your body in ways that you want.
And then in addition, here's another cell called a cortical stellate cell.
So in fact, as we will see, there is a great diversity of forms of neurons.
And usually, in biology, there's a rule, a working rule,
which says that form follows function.
So what's going to happen is that they look different like this versus this.
They're going to do different things.
They'll have different biochemistry,
they'll perform different functions within this computer.
So it's like you've got many circuit elements that are stuck together
to produce your behavior.
Now, the key thing for us is that neurons literally,
like the components of a computer, are electrical circuit devices.
They're electrical devices.
So how does this work?
So once again, here's a picture of a neuron.
And if you go measure its voltage, literally, I mean its voltage, right,
you'll find that its voltage inside a brain,
the voltage will be something like minus 70 millivolts, typical neurons.
They're like at rest, they're like at minus 70 millivolts.
Then, if you have input coming into it, and the input could be signals,
neurotransmitters, which I mentioned earlier, sent out by the previous neuron.
Or if this is a neuron in the eye, it might be light landing on it
because there's a little sensor that senses like.
Or if it's in the nose, if it's an olfactory receptor neuron,
it's a little molecule floating in the air which binds to a receptor
and that produces a signal, you know, whichever you do.
If a signal comes in, here's what happens.
The voltage of the neuron begins to increase.
And when it reaches some critical value, there's a sort of runaway feedback process.
It's a little bit like feeding the output of your speaker back into the microphone.
And so it goes round and round.
It goes boom and sort of explodes rapidly to some very high potential.
And then there's another feedback process, a negative feedback process, as they call it,
which causes this to dip back down and flip back to rest.
So if you step away and time a little bit, you'll find that it goes like this and then boom!
There's a very sharp pulse of electricity.
So if you sort of look above this level, it's like this silence and then boom!
Silence, boom!
So it's essentially a digital signal.
It's like looking at a digital computer where you have this device going every once in a while,
sending out ones, hold up these spikes or action potentials, or zeros, where it's silent.
So it's very much like a digital computer.
That's what you have in your head.
So just to drive home that this is absolutely a mechanical, physical process,
beautifully studied in biophysics, we can even delve in.
You know, the instinct of many scientists is you see a phenomenon like this.
You say, well, well, how does that exactly happen?
You know, and you can work that out in complete detail.
So for example, if you look at a membrane surrounding the cell, that's the membrane right there,
you'll find embedded in this membrane various molecular machines,
such as ion channels, which allow specific ions like sodium and potassium mentioned here,
or chloride or calcium.
They let certain ions pass back and forth.
And then you have these things called pumps, which have the job of when the ions go back and forth,
changing in this way the voltage of the neuron.
Something has to be done to restore it, right?
You send a lot of current back and forth, and the voltage changes, you've got to restore everything.
And these pumps basically burn energy.
They burn the gasoline of the cell, which is a molecule called adenazine triphosphate, ATP,
and it burns ATP and goes chunk, chunk, chunk, chunk, chunk for a while,
and then sort of restores the levels of ions inside and outside.
So at rest there's a certain proportion of sodium and potassium inside,
and a certain proportion of sodium and potassium outside,
and then you have the kinds of processes physicists talk about.
Stuff, you know, it's like if you have more salt outside a membrane than inside,
you know how the salt comes in, right?
In the same way, if there's more potassium sodium outside than inside,
the sodium wants to come in, and the potassium, because there's more inside than outside,
wants to go out so that these processes like this,
basically stuff you can understand perfectly well,
and then these pumps are working against it,
and then what happens is that you have external inputs,
but change how open or closed these channels are
to allow the ions to pass before through them, and that's it.
It's a completely, I mean, just like a mechanical device,
with stuff flowing back and forth, and you can work this out in detail
and show that if you have a system like this, it'll produce these voltage spikes,
and then if you make a voltage spike over there, it'll then propagate down the axon.
It's literally making, you give it the correct inputs,
it's going to make a digital pulse, and the pulse is going to move down.
In fact, this is so well understood that a Nobel Prize went to it.
So Hodgkin and Huxley, in 1963, worked all of this out,
and here's a set of equations describing their model of the salt.
You don't have to read the equations.
I just want to show you, you can write it on a damn page, right?
This means you can put it on a computer, and you can certainly simulate it.
You can totally describe how a single cell describes it in complete and utter detail.
It's actually a little bit more complicated than this,
because for the purposes of this talk, I'm talking about sodium and potassium and things like that,
but there's more things, this chloride and there's calcium,
and actually there's more than one kind of ion channel for potassium, some details, details.
But basically, you know exactly how to do this,
so in principle, you could just build yourself a brain.
If you knew enough about all the other neurons and all the types of neurons and all these problems,
you could just build yourself a brain on a computer.
That's the promise of this kind of result that won this Nobel Prize, right?
But what's the problem, right?
The problem is, oh wait, actually before I tell you the problem,
let me just sort of say a word about this.
So this idea that the brain really just functions in electricity,
just an electrical device, a computer of some kind, goes back a long, long way.
And the discovery that somehow there's an electrical basis to animal behaviour
goes back all the way, I don't know why I said this is a 19th century idea,
it's even an 18th century idea, it goes back all the way to the 18th century,
with the discovery by Luigi and Luisa Galvani, we're in Bologna,
that you can make frogs' legs, for example, twitch, by putting electricity into them.
So at the time, Luigi got most of the credit because Luisa being a woman,
couldn't be a professor in the university and couldn't also take credit for the discoveries,
but well, you know, times have improved a bit, they're perhaps not completely.
Anyway, so they were the ones who discovered this notion of animal electricity.
Then immediately after that, this is the time when Benjamin Franklin,
you know, is discovering positive and negative charges, flying his kite,
all this kind of stuff, that's all happening at the same time.
And then Volta, also in Italy, invented the battery,
which then allowed controlled injection of currents into living tissues,
and then you could see how they moved and people had this idea
that somehow electricity itself was the vital spark of life.
It's not that it made neurons work, because they didn't know about neurons really,
but they thought it was life itself.
So for example, so they thought, well, maybe we can reanimate the dead.
So Aldini reanimated a criminal who was executed using electricity,
and then it was done again in 1818 with another criminal,
and in those days there were laws that allowed you to experiment
with the dead bodies of, you know, of murderers and things like that.
Anyhow, this led, for example, Mary Shelley to write Frankenstein,
because it led to the idea that life and living things and animals could be just built,
that they're sort of machines that could be built,
and that's what's partly animating us still,
this notion of artificial life and artificial intelligence goes back,
excuse me, goes back to these discoveries, you know, hundreds of years ago.
So now, of course, we know that electricity, the shocking dead body,
doesn't make it alive again.
What it really does is following what I told you a moment ago,
it causes the nerve cells to fire,
and when they fire, they cause muscles to move,
and then the body will move in the way it looks as though you reanimated the dead,
but you haven't really.
Anyway, so suppose we know how a single neuron works in all of its detail,
then what's the problem with understanding the rest of animal behavior,
just putting enough neurons together?
Well, the problem is, you know, a single neuron can't produce a poetry,
a single neuron doesn't feel love, you know, things like this, right?
Really, neurons produce the brain through the circuits that they work in.
It's really the interaction, the collective of all the neurons that makes the brain.
So what do these circuits look like?
So perhaps the best-studied circuit in the entire brain of any animal
is the retina.
So this is a drawing done, again, by Ramoni Kahal, back in 1917,
and what it's drawing is a picture of the retina like this.
Over here, these are the famous photoreceptors that many of you will have heard of.
Photoreceptors are neurons which take in light and convert the light
into an electrical signal that comes out the other end.
So that's the first layer of the retina.
Now, the second layer of the retina consists of cells called bipolar cells.
These are like analogue computing devices, namely they're not digital.
They have voltage levels that go up and down,
and they're all continuous, it isn't 1s and 0s if you like,
but they do all kinds of calculations and computations which are studied.
Then these things form the second layer, feed into a third layer,
and the third layer are cells called ganglion cells,
and these ganglion cells are the output cells of the retina, right?
And these output cells, you know, the axons come out,
and the bundle of axons is a thing you call your optic nerve, right?
So very often people have a tendency to think about the retina like a camera.
It just takes the pixels and writes down the amount of light in each location.
But nothing could be further from the truth.
This is your primordial example of what would be called a three-layer neural network.
It's a piece of your central brain that got put on a stalk
and then sent out to the front of your head during development,
during embryonic development,
so that you could see in the direction in which you're going.
It's really a piece of your central brain.
And indeed, in keeping with that, central brain is a very complicated place.
This picture drawn by Cajal is actually a cartoon.
We now know that there are more than 60 kinds of cells,
different types of cells,
just so different computational elements in the retina alone.
So, for example, if these are the photoreceptors,
and then these are the bipolar cells described in the second layer,
and then these are the ganglion cells described in the third layer,
in fact, in between, there is an entire zoo of other so-called interneurons
that sort of communicate laterally.
And what they do is the following.
What they do is, you know, most of the light coming to your eyes
is not actually useful to your behaviour.
There's only certain things that are useful.
And indeed, the ganglion cells, what they do is,
they report the things that are useful.
Like, there are ganglion cells called on cells
that respond and tell you when there's a bright spot in the world at some location.
There are off cells that tell you about dark spots.
There are so-called local edge detectors that detect local edges.
There are direction-selective cells
that will respond when a little blob moves left to right or right to left or up or down.
You know, there are all of these cells, these different kinds.
So how do you extract these so-called visual features from the scene?
Well, this hugely diverse body of neurons is charged with removing from the visual input
everything that's irrelevant and leaving what is relevant.
So famously, for example, in analogy, someone once asked Michelangelo,
how did you sculpt these sculptures?
David.
And he said, well, actually, David was already in the marble.
I just took out everything else.
And so that's what these things are doing.
They're taking out everything else and leaving the important stuff to go through to the brain.
So this is how it's organized.
So you can see it's a large collective effort of many, many types of neurons
arranged in a very specific circuit.
For example, you will always find that the slow bipolar cell here connects to the local edge cell
and the fast bipolar cell here connects to the brisk transient cell.
So it's like a circuit that Intel would design.
You know, the engineers designed something.
This isn't designed.
It's selected out by evolution.
But anyway, there you go.
It's like a circuit that you will find in every vertebrate eye.
Now, even that is not actually a sufficient picture of the degree of cooperativity in the brain
that produces all of the behavior.
So if you pop out from the scale of neurons and instead look at the whole brain,
here is your brain, you know, looking this away.
And in the back of your head, there are circuits that are associated with vision.
Over here, there's a strip going down the side of your head that helps you control your body.
Over here, there's an area called Broca's area, which is associated with the production of speech
and, you know, many other areas of this kind, right, the hearing, et cetera.
And in the front here, there's regions of your brain that are associated with planning
and personality and things of this kind, right?
So this is why, by the way, you know, decades and decades ago,
there was a treatment for schizophrenia, which involved basically sticking an ice pick up your nose
and scrambling this area because there was some issue with it.
And the result is, you know, the person won't be schizophrenic,
but they won't be themselves anymore because you scramble the circuits that make you.
It really should be very careful with any kind of treatment that involves sort of messing with bits of the brain.
OK, so anyway, so there are all of these things, and we know that function is localised in this way,
that this is called localisation of function for many reasons.
So one reason is, you know, for example, if you bash the back of your head, you fall backward,
you know how in the cartoons you see stars, right?
You know, Bugs Bunny sees stars when he bashes the back of his head.
It's because Bugs Bunny has bruise on this part of the brain.
So the circuits aren't working quite right, that's what's going on.
So when someone has a stroke, sometimes you'll see one side of the face sag,
but everything else is completely fine.
That's because there is an issue, there's a problem,
let's say in this region of the brain, this is the motor cortex,
and within the motor cortex there are circuits here that control the knee
and circuits here that control the face.
So if you have a stroke there, this thing sags, but the rest is fine.
But it's all collective, right? There's a collective circuit,
which of course has little areas that concentrate on different things,
but it's a collective effect.
And this is not just on the surface of the brain, by the way.
So deep in the middle of the brain, there's an area there called a hippocampal formation,
which is associated with, for example, memory, right?
So if you have damage there, you will not be able to form long-term memories, right?
And it's also associated with spatial cognition,
with spatial, you know, animals need to map space
and be able to make plans and navigate.
And so all of those circuits are hidden here.
Okay.
So actually, human beings have known about this kind of thing,
that there's function localized different parts of the brain for a very long time.
This is a picture of what's now called a Smith papyrus after Smith,
who bought the papyrus in the late 1800s.
This was a papyrus written in 3000 BC.
And the author of this papyrus describes various kinds of traumatic brain injuries,
and then explains that, you know, something will stop working, but everything else works.
So this author knew that function was localized in the various parts of the brain.
The modern version of this idea is due to Paul Broca, a French doctor,
who basically discovered Broca's area because he had patients who had impairments in their speech,
and he discovered they had lesions in this area of his brain, of their brains,
and there's another area called Wernicke's area,
which is associated, but later discovered, having to do with speech understanding.
So if you go to Paris, you can go to the 13th arrondissement,
and, you know, admire Rue Broca, a name for him for all of his great discoveries.
There you go. There's the picture of the street sign of Rue Broca.
All you need to do to get a street sign is discover localization of function in your brain.
OK. Very good.
So now that's also not enough in describing how complicated this processing machine is.
You know, you all know, if you ever looked at a wiring diagram of something like a chip that powers your computer,
you know, it's got lots and lots of pieces with intricate circuits,
and all those pieces connect to every other piece.
Otherwise, of course, they couldn't produce their collective effects.
And indeed, all of these brain areas connect.
So today, in the 21st century, there are all of these extraordinarily beautiful techniques now
for tracing out the wires.
It used to be that people like Cajal, you know,
they had to take the brain of an animal, slowly slice it up,
very, very carefully trace everything, draw it out by hand.
You know, you can imagine the level of dedication and sheer labor it took.
Now, we have all these amazing techniques.
You can do things like you can make neurons glow in different colors
and get the color to propagate along the axon, you know, the output of the wire,
so you can see where it's pointing to.
You can inject viruses into bits of brain.
The virus will go backward along where, you know, the neuron connects to.
And then you can find out which neuron, one neuron down.
You can tag it in that way, and then it will stop working.
The virus stops working at the point.
You can engineer all these things.
So, in this way, we now have far more detailed maps of the brain.
Here's a picture from a paper by these authors.
It was a review paper showing some of the nerve tracts, as they're called,
the wires that pass between.
And now, because of that, we just have a much more refined understanding
of how collectively all these bits of brain produce functions that we care about.
So, for example, following what I said a moment ago,
in Broca's era, or shortly thereafter, we might have said,
well, you know, here's Broca's area,
which, you know, somehow is responsible for articulation, production of speech.
Here's Borneke's area that is somehow associated with comprehension of speech.
Let me explain the difference between those two.
If you damage Borneke's area, you'll produce speech.
It just won't make any sense.
And if you damage Broca's area, you can understand speech perfectly well.
People can talk to you, but you won't be able to make it.
You can't control your muscles to make the speech, stuff like this.
There's also another area, Gershwin's area for concepts.
You damage that, you have problems with concepts.
Anyway, so there are these areas that people identified.
So a more refined model you might derive by looking at the way the nerve tracts go
is that there's clearly a pathway from here to here,
so from the comprehension area to the production area,
and there's a secondary path, you know, there are two pathways like that.
But now we have even more fancy techniques.
You can put a person in an fMRI machine, right?
Functional Magnetic Resonance Imaging Machine, and have them do stuff.
And while they're doing stuff, you can record, you can find a way
of measuring which areas of the brain are active.
So which areas of the brain are active together when you do different things.
And now you get a much more refined picture that all of these areas are there.
It's true that they're associated with articulation concepts and comprehension,
but depending upon exactly what you're trying to do, they connect in different ways.
So there's a sort of flexible computational engine
wherein, you know, the different bits of it work together for one task or not,
depending upon what's necessary to do the task.
Is this amazingly flexible engine inside your head for producing all of animal behaviour?
So that's the task to work out how all of these parts connect
and how they reconnect and reorganise themselves when they need to do things.
OK.
So myself, my interest is to understand how all of that happens.
I think of the brain as a computing device.
I want to know how this computer works.
So there are many ways of approaching this,
but one way of doing that is, well, so the way I do that,
is I take a view of the circuit repertoire of the architecture of this computer
as a kind of memory of the computations that have predictive value for your behaviour,
because, you know, animal life is about predicting what's going to happen
and taking action appropriately,
and that these computations have been learned over evolutionary time
and then encoded in your genome and in the developmental programme.
So that's the way I tend to think of it.
These are computations that are useful to you,
and then what you do is something, and then you use them.
So the question that animates my work is what are the organisational principles or laws, if you like,
that control the collective computation and information processing of the brain.
So, OK.
And, you know, if you're interested, there's a popular book by Peter Sterling and Simon Loughlin,
this book, your principles of neural design, that you'll probably enjoy,
which lays out many possible principles that may be operative
in the organisation of the circuits in the brain.
Now, so what principles might be relevant?
What kinds of laws or principles?
So there are several that you could try to name,
but for today's purposes I'm going to name two,
and then I'm going to illustrate them in the operation of some of these circuits.
So, for example, one principle involves the costs of computation.
So this is your brain, and this is your laptop,
and, you know, your brain is only 2% of your body weight,
but it's actually 20% of your metabolic load.
What that means is it's 10 times more expensive than muscle.
This is a seriously expensive thing to own, right?
And, you know, it's also packed solid, right?
Every millimeter cubed contains four kilometres of wire.
This is like this really dense thing, right?
So on the one hand this sounds like, wow, you know, that's a really expensive thing.
On the other hand, you know, your brain consumes something like 12 to 20 watts of power.
Your laptop consumes 80 watts of power, right?
And it sure can multiply fast, but, you know, it can't give the stock.
Well, at least not yet.
But even things like, by the way, you know, AI seems to have the promise of, you know,
getting, making machines that'll do me,
but actually they have to train their machines on so much data.
You know, planetary, a planet's worth of data,
they use a city's worth of power to train these things, right?
Whereas I can learn stuff with one example, right?
I don't need the city's worth of, you know, the planet's worth of data.
So there's a real difference in the way the brain works.
It's just much more efficient, right, in the way it operates
than your typical silicon machine.
That's also one reason why computer scientists are interested in this kind of thing
because they'd like to make more efficient machines.
So you could ask yourself, how does the brain achieve this sort of efficiency
relative to the engineered systems that we have been able to produce so far, right?
I've described this baroque architecture with lots and lots of pieces that interact in some way.
So one idea that's very powerful and has been used powerfully in neuroscience
is that the brain achieves its efficiency by adapting its circuits to the structure of the world
and then using learning and self-organisation to further adapt the changes, right?
You know, you first adapt to the world and then if the world is not quite what you expected,
you change the circuit so that it's well adapted to the world.
So why would this make the system more efficient?
Well, the idea is roughly like this.
So if you talk to sociologists or something, they'll tell you that in the back of the day,
we were all hunter-gatherers, we all dug the ground, we hunted the animals, we built the house,
you know, whatever, we did everything and society got along.
And then as society's progress, the civilisations developed, et cetera,
you know, there's a segregation of function.
What happens is that the shoemaker makes the shoes, the baker breaks the bread,
you know, the mason builds the houses and then each unit in the society develops efficiencies.
They do their job really well because they're well trained, they're adapted to it, et cetera, et cetera,
and the whole system, by communicating between all the individuals,
works better, works smoother, it's more productive, et cetera, and it's more efficient.
So you could try to apply a similar idea to the organisation of these circuits in our head
which have evolved over time.
So that's a very important idea in neuroscience.
So that's principle one, the efficient use of resources.
So the idea of this principle is that brains exploit the average structure of the world
to efficiently allocate their limited resources for the tasks that they have to do
to maximise gain for the organism.
Now if I were to draw a cartoon of something like that, it might be something like this.
Remember how in the retina we said, well, you know, there's all the light coming in,
but at the back end of the eye, you have a certain number of different neurons, right?
There are 20 types of ganglion cells, we call them,
but you know, they're each charged with doing a thing, right?
One thing will look for bright spots, that's the on cell.
The one thing that's for dark spots is the off cell, you know, and so on and so forth.
There's a certain repertoire of things that are apparently needed for vision,
so you make those things, and then each of those things is one of these blobs,
and together they cover the space of those aspects of the visual world that you need for your behaviour.
And they have to allocate those resources in an effective way.
And you could keep doing this with a sense of smell, with your sense of place,
you know, the whole brain, and you could try thinking in this way.
So this is a very important principle that's used often by neuroscientists
to try to understand the architecture of circuits in the brain.
Another important principle is this one, it's learning and self-organisation.
These individuals whom I know well did a lot of that,
and so what do they do, so that's far as I can tell,
their little heads self-organise a lot of the architecture within it through dynamics and learning,
and what do they do?
Well, this learning and self-organisation adapts the brain
to ongoing variations in the world, you know, their world is different from the world I was growing up in.
It allows them to learn new tasks and environments at this age,
they definitely didn't know how to ride bikes,
and you can correct inaccuracies in pre-existing dynamics.
Anybody who's interacted with small children knows perfectly well
that, you know, when the baby first tries to reach for stuff, it doesn't quite, you know, it doesn't get there, right?
You can't quite do it.
And then they learn how to control their muscles to get there, to reach it.
Or you watch a kid learning a musical instrument.
Well, you know, the first, you know, 500 times doesn't sound so good.
And then eventually it does, because there's something that happens where you learn how to operate the bits of you to do the right thing.
So that's the learning that has to happen in a variable world.
OK, so what I would like to do in the remaining, what, 15, 20 minutes, 20 minutes or so,
is that I would like to give you examples of how in the field people use these kinds of ideas
to understand the organization of neural circuits.
I mean, so far I gave you a description of the architecture, right, of these circuits,
from the neuron level, you know, the atomic constituent of the brain, all the way to the whole brain, right?
But in science, we want more than that, we want an understanding of why they're organized the way they are.
How they produce the dynamics and effects that they do.
So there is an enormous amount of interesting work that has happened since the era of Cahal in these directions.
So I want to give you a little flavor of some of it by giving you a few examples from my work.
I'm going to pick simple examples because, you know, there's 20 minutes left.
So we should go, so it has to be simple and precise enough.
So I'm going to give you examples in four domains, right?
One domain is seeing, vision.
The second domain is smelling, or factions. These are both sensory domains.
Then I want to talk about learning to sing. This is in songbirds.
So that's what the professionals would call motor control,
because you need to control all the muscles in the songbirds apparatus in order to produce the sounds.
And then I'm going to talk very, very briefly about navigation,
which is actually the subject that I spend probably most of my time on, thinking about right now.
So I'll just say a little bit about that, and then we'll end.
So I'm going to give you four examples of this guy.
So let's start with one.
So first I'm going to talk about vision, and my goal, by the way, what's my goal with these examples?
You remember I talked about these two principles, adaptation to the world and learning,
as two mechanisms that are used to, well, to organize neural circuits.
You can understand why they're organized the way they are,
but think about adaptation to the world, and about learning to reorganize.
So I want to give you two examples of adaptation.
Those will be the sensory examples, and an example of learning.
So first let's talk about vision.
So why did I choose vision?
Well, we're very visual animals, and it's a sense we use most of,
so that naturally neuroscientists gravitate often to thinking about the visual world.
So I want to ask a very basic question.
Here's the question.
So we drew this, Ramoni Kahal drew these pictures with all of these different neurons,
and then I showed you another picture, a more modern one, of all the cell types,
and there are like 60 types of cells.
God help us in this structure.
Well, what did I do?
Let me just look at the output layer for now.
These were the so-called retinal ganglion cells,
and I told you that these cells detect features, visual features of the world,
bright spots, dark spots, color, local motion, et cetera,
and then they put the signal, these digital signals that I talked about,
go down the optic nerve and that little bundle of wires that's like an ethernet's cable worth of data,
and that data goes to your brain and you do all the stuff that you do with vision.
So following a kind of question that immediately comes to mind
is you have a certain repertoire of these cells.
Well, humans, we have about a million of them.
If you're a guinea pig, you have about 100,000 of them.
So, you know, it's not an infinite resource.
So if you spent all of your money on detecting bright spots, there's nothing left.
If you spent all of your cells on detecting bright spots very well,
you will have nothing left for detecting motion, for example.
So there has to be something interesting in the way this sensory device, right?
This is like you have a robot and has little eyes sitting in front looking at stuff, right?
So you've got to decide. How are you going to decide the design of that eye?
It's only a big. It's got only some of the units in it.
How are you going to allocate the different pieces of the circuit?
So here, there should be some choice done by evolution for the allocation of these features.
So for this talk, let's take the specific example
because it's easy to imagine of bright and dark spot detectors.
So bright spot means you look for a little bright spot relative to the near-surrounding region.
A dark, so that's called an on-cell, and an off-cell does the reverse.
It looks for a little dark spot relative to the near-surrounding region.
So we're going to ask the question, well, suppose you have a certain number of spot detectors,
how many on-and-off cells should you have?
How many bright and dark spot detectors should you have?
And we're going to try to use this to understand the architecture of the retina.
Just incidentally, it's also important therapeutically, right?
So if somebody's, you know, when the day comes when we can replace the retina,
if somebody's retina degenerates and we want to replace it by a silicon device,
it would be better if we could understand how many of these and how many of these detectors
we should put in into that silicon device.
Right, so if the idea, if this kind of organization involves adaptation to the world around us,
a question we have to ask is what is the world around us?
So for each sense or for each mode of interaction with the world,
if you adapt it to the world, then one question you have to ask is what does the world look like?
So in this case, you can go collect a large database of images and study its properties.
How is light organized in these images?
And one thing you find is that in any color channel, this is supposed to be red, blue and green,
you'll find that the distribution of the intensity of pixels,
you know, if you look at the brightness of different pixels, it's very, very skewed.
There are lots of pixels which are pretty dim,
but then there's a very long tail with some pixels that are enormously bright.
So it doesn't matter where you go, you do this inside a city, you do this at the North Pole,
you do this in a jungle, you do it wherever you want,
you're going to find that this is the case.
It's a very universal feature of the visual world.
And it has an important consequence.
The average intensity of a pixel is lower than the median.
Median means the median intensity is the intensity at which half the pixels are dimmer and half are brighter.
So the median exceeds the mean in the world.
There's another fact, which is that there are long range correlations.
What do I mean by that? This is some technical figure showing that,
but what we really mean is, you know, if it's kind of red here, it's going to be kind of red here,
it's going to be kind of red here, and then after a little while it's improbable
where the color is going to persist forever.
But there's a very particular structure of those correlations which you can measure.
It's a regularity of the natural world that you can measure.
So you would think that, well, if I'm going to detect bright dark spots,
maybe that's related to these statistics.
And actually, without even doing anything, we can immediately argue,
and let me argue to you now that these two facts, that light is correlated over distances
and that you have the median light intensity exceeds the average,
that immediately means that there are more dark spots in the world.
Here's how to see that, right?
So suppose what you do is you compare the light in a small region here
to a large region there, right?
So what's going to happen is that if you measure,
if you compare the light in those two regions, because of these two statistical facts,
it's going to turn out that the spot in the middle is generally going to be darker
than the surrounding region.
That's just because the average, if you see the average here,
the average is less than this.
So that turns out to be the case.
So, for example, you can take a bunch of images and you can put down here a thing
that detects a bright spot, a thing that detects a dark spot,
and you can just count how many bright and dark spots you detect in every image,
and you'll find that no matter what angular size you look at,
there are more dark spots than there are bright spots.
So that's what this figure is supposed to be showing.
So we're going to ask a question based on this.
So let's agree that because of these statistical facts,
there are more dark spots than bright spots in the world.
So then we can ask, suppose I give you an allocation that you can buy,
or you can put in the eye, a certain number N of bright and dark spot detectors,
namely on and off cells.
So you get to divide them, you get to decide how many dark spot detectors
and how many bright spot detectors you have,
and my challenge here is to work out what would be best given the structure of the world.
So it's immediately clear that if you have only money to buy one cell,
it's better to buy a dark spot detector.
Why? Because there are more dark spots.
So it's more likely to respond.
So you're more likely to get useful things out of it.
If I give you two steps, then you buy some dark spot detectors
and you can cover the eye with them.
But then they're going to start overlapping.
So they're going to start telling you the same thing,
because they're overlapping, they're telling you redundant information.
So then you should buy some on cells, bright spot detectors,
and you can continue with this game and work out the mathematics of it
about how you should tile the retina with bright and dark spot detectors
in order to get the most information about the brightness of spots in the world.
So that's a mathematical problem that you can write down and solve.
And it turns out that the answer is about you should have one and a half
to about two times as many dark spot detectors in your system
if you want to say as much as you can,
given your budget of the number of cells, about bright and dark spots.
So we can ask, right, how does that compare with the actual eye?
So over the last two decades,
there have been a very large number of experiments on this.
So it used to be in the 1990s, for example,
if you asked people, they would talk about, you know,
white and dark equal on opposite,
but around the year 2000,
a series exactly as you'd predict
if the circuits of the eye
were adapted to the statistical structure of the world.
So exactly in that way.
So the collective organization, so that suggests,
so as I was saying according to the principal I mentioned,
that the collective structure of the retina, the object,
roedd y gofyniwyr o fynd a dydy'r cyific o d Однакоllur answers pwysigol
sut wnaeth yr hyn, ddiun i собi d automobile
o flynyddiad yn ychwaneg
a chi dysgu cynyddiad y c eg Adventure
Felly oddiwch iawn ay grid yments
filn dech 1iblaethau alcoholu
yw'r wider in the sensory world. Let's talk about smelling.
And we care about smelling, about orphaction
because it's in some ways the most primordial sense,
right, all living things sense molecules
So all living things smell in that sense, that every animal, most animals use smell
more importantly than vision to do their daily jobs.
Felly mae'r maewn leak, mae'n deamiad â'r hanon a Date-anes, ydy Republiciaus Gyno Secondly Mae'r Losing, …
… mae'r rhywun gwleol yn gall Ilaigdol oherwydd sasr hyffМolau jyrnol yn y gywe uch G
Mae gennymied excitingadol yna gyda ar bobl Paris Prefres Gwriaethyn yn eu cyzd.
wrth gwrs you can go consult with the people who do fragrances and flavours, you know people
who make artificial flavors and they will tell you that if you look at natural odors of different
kinds, foods, for other animals etc- typically an odour will contain fifty different kinds
of molecular species, so you could estimate from that that in terms of possible natural
smell it's like ten thousand to the fifty. This is such a vast number, it's even hard
That's the number of different things, like older objects, if you like, that you might have to sense in the world.
To make matters worse, the older environment changes, reflecting season circumstances and new opportunities.
So what do you need to do? Well, to detect a molecule, actually you need to feel its shape.
The identity of a molecule is basically defined by how all of its atoms are arranged in space.
So olfaction, the sense of smell, is a method used by the brain to sense the shapes of little things floating in the air.
So what do you need to do this? Well, it turns out that odors are sensed when molecules bind to receptors in the nose.
There's basically another molecule in a neuron in the nose, so that it's an olfactory sensory neuron.
The molecule will be sitting there, and then in will come some other molecule, an odorant, as it's called, floating in the air.
And if this molecule fits into the binding pocket of your sensor and sticks there, doesn't fall off, then the neuron will respond.
So this binding pocket, your olfactory sensory neuron, is literally feeling the shape of the molecule, whether or not it fits.
Now as it turns out, every receptor, olfactory receptor that allows you your sense of smell, is encoded by a separate gene.
That's because, you know, to make your receptors have different shapes because they're trying to feel out different shapes.
So to make a molecule, a protein, that has a different shape, you need a different gene to encode it.
For the genetic level, you know, they're all different genes. In fact, it's the biggest gene family in most animals.
So flies have about 100 of these, humans have about 300, and even the elephant only has about 2,000.
So this leads us to feel, looking at this, that somehow, you know, apparently animals can sense all the molecules in the world and all the combinations of the molecules in the world with just a few hundred receptors.
The reason this is very, very strange is you could phrase the difficulty of the problem in the following way.
You could say, suppose an odour is defined somehow by the concentrations of all the molecules in it.
Then it's possible that there are up to 10,000 different molecules.
So you put the first concentration of this side on this axis, you put the second concentration of this axis, the third concentration of this axis, et cetera.
There are 10,000 different axes on which you can draw all these concentrations, and somehow you have to represent all of this information
in the responses of just 500 neurons, let's say, or 300 neurons, and so it's the response of neuron one, the response of neuron two, the response of neuron three.
You just don't have, it would feel like there's no way to represent so much information about all the molecular concentrations with so few neural responses.
You could, just to make the problem more easy to visualise, consider the following thing.
Suppose I told you that you have positions in three-dimensional space, right? X, Y, Z, like that.
There are positions in three-dimensional space, and I insist to you that you tell me where you are in three-dimensional space by telling me just, you know, a coordinate if you're like a location in one dimension.
I mean, that's just really not, doesn't seem like it's very sensible, it's very hard to do, to do that, right?
Because you've got three directions, you could go up, you could go left, you could go right, and somehow encoding that in just one direction seems very hard, okay?
And what's more, you'd like to do that in a way that somehow preserves similarities and differences.
You need to be able to tell that this order is the same as this order, or similar to, and these two orders are very different.
So somehow the brain has to solve the problem of representing all of this information with very few neurons, right?
So a very high-dimensional space of orders, as they say, in a low-dimensional space of responses.
That's like going from three dimensions to one dimension, and it needs to do it in a way that preserves distances.
So an insight that's happened in neuroscience in the recent past is that actually there's a way in which you can do this by adapting to structure in the space of orders.
The structure is the following. We said already that if I look at, let's say, this order, it'll contain only a few molecules, let's say 50, and all the other things are absent.
This order contains this molecule and this molecule, and all the other ones are absent.
Each order in the natural world contains about 50 odorant molecules, right? 50 types of odorant molecules.
So it turns out that there's a theorem in mathematics that you could take signals like this if they came to you and store them with many fewer responses than the number of things that are coming in
if every sensor bound to all the molecules that are coming in kind of randomly.
It's a rather than being structured like, you know, in vision, you have the cell that pulls out a bright spot and you have a cell that pulls out the dark spot or left to right motion and things like this.
So instead of doing that, if you just said, here's an olfactory receptor, I kind of wanted to bind kind of randomly to everything and just give me a sort of mishmash of numbers.
So there's a theorem in math that says if you do that, you could store all the information that's there in the odor world with a few hundred receptors.
So there's a theorem like that.
So the question is, does the olfactory system use randomness in that way?
That's a very different kind of organization, mind you, than the visual system.
That's why I'm giving this example.
In the visual system, everything is highly structured because the visual world is highly structured.
And here the idea would be, the math would tell you that it should be kind of random what it does.
And indeed, so here's a picture of different receptors in the fruit fly binding to many different odorant molecules and the darkness of the color tells you the strength of the response.
Now this doesn't look entirely random, and it isn't entirely random, but there's a sense, a mathematical sense, in which this is sufficiently random.
It's pretty close to random.
So much so that the, so this is, so in other words, basically what happens is that every olfactory receptor here is binding to many, many odorant molecules.
That every odorant molecule is binding to many, many receptors.
So this is a highly multiplexed thing.
It's not like one receptor tells you this molecule is there, or this receptor tells you this molecule is there.
It's like every receptor binds to everything at some level, and every molecule binds to everything.
So this is a highly multiplexed kind of description of the odor world that appears in the nose.
So this is what's called a kind of combinatorial code in the jargon of the field.
And this continues.
So this is a drawing, a diagram of your early olfactory system.
So you have all the molecules in the world.
This is a vector, this is a figure showing concentrations of which molecules are present.
They bind to different receptors, you know, they're all colored differently.
And then what happens is that the signals from the receptors are collected at a second stage.
That's what's drawn here in the picture by Camilo Golgi.
So the schematic that I've drawn here is actually this thing that I've simplified.
And at this stage it's all cleaned up.
And then it turns out that the signals from here project to your central brain, to the cortex also in a random way.
So once again you get this sort of randomness and the information is kind of spread out all over the place.
And you can show, and so that's what I was trying to argue to you, is that so you might be surprised if you studied vision, you might be surprised by this.
Why is vision so structurally organized with all these little feature detectors pulling out different things in the world and sending the brain and so on?
And why does olfaction look so random?
So what I tried to argue to you is that the collective organization here in the olfactory system, the system of smell, is via randomness as opposed to structure,
because that's what you need mathematically to be able to process signals of this kind.
So in both cases, in both vision and olfaction, in one case the structure, in the other case the sort of randomness in the circuit,
are associated with adapting the circuit to process efficiently the information in the world.
So that's what I'm trying to convey here.
By the way, I'm trying to give you some of the technical details here, because often I find that in these kinds of lectures, when I listen to public lectures,
there's lots of pretty words, but I don't have a sense that something can actually be done, something concrete.
I'd like you to go away with the sense that this is completely concrete, you can do these calculations, you can do these measurements.
This is a completely concrete thing, and that's in the details of this kind of analysis.
That's why I'm giving you some of these details, so you have a sense of how this works.
So I'm going to give you one more example, and then stop.
So the example I'm going to talk about is now learning.
So far I've talked about adaptation in evolutionary selection of circuits to be adapted to the world.
And now I want to talk about something that isn't evolutionary selection for adaptation to the world, but rather involves learning.
So there are songbirds.
Here are two kinds of songbirds, zebra finches and the brown-headed cowbird.
And famously, songbirds learn to sing by imitation practice and innovation.
By the way, the reason why we should care is humans do the same thing.
Here's a human learning by practice.
So that is a very important part of animal activity, to learn by practice and to get good at something.
So how do songbirds learn?
Well, it turns out that juvenile male bird dabbles and just goes on like that.
And then over weeks, it learns to sing clearly partly through imitation because its song will partly imitate the songs it hears often of its parent with some innovation.
And then the adult male bird sings to female birds often seeking a mate.
And then the female birds seem to evaluate these songs and then decide which one or which ones to accept.
And this is all really important, right? It matters.
Because for example, in social species like this cowbird, the males and females work out a kind of social hierarchy based on their singing to each other.
And then they produce pair bonds and the success of those pair bonds is absolutely critical for later success in egg laying.
You don't make the pair bonds, the colony doesn't produce very many eggs.
So it's actually really important for the survival of the species that this is all well organized and that the males learn to sing.
So how do they do this?
So now, let me tell you about how all of these circuits I've talked about reorganize themselves.
They reorganize themselves when neurons, which are connected to each other, autonomously rewire their circuits.
There's a rule, they'll use the rule to reorganize.
There are various sorts of rules and one of the rules, one of these mechanisms is called spike timing dependent plasticity.
So let me give you an example.
Suppose you have two neurons, one and two, and they're connected and here's a synapse between them.
Suppose, for some reason, they both fire.
They both produce these voltage spikes that I talked about at the beginning of the top.
Suppose neuron one fires before neuron two.
You could have a rule that says the synapse strengthens.
Or you could have a rule that the first, and also the second part of the rule says that neuron one fires after the second one the synapse weakens.
So this is just a local rule, right?
I mean, it's just a dynamical rule.
You've got two dumb objects, neuron one and neuron two talking to each other, and they just have a rule.
They have the strength of the synapse, the weak of the synapse, depending on who goes first.
You can modify this a little bit.
There are things called neuromodulators, which are sort of chemicals that the brain uses.
And these can be used as global knobs to affect how this happens.
So for example, there's a thing called dopamine, which if dopamine is released here,
it can reinforce changes that have happened, that happened to have led offline to some sort of reward.
So if there's a reward, you know, they'll do something to help cement some change that's happened to synapse.
There's other things that can help a nephrin that also, you know, allow emotion to control how these synapses changes and everything.
And you could put all of these rules on computers and see what they do and so on.
It just seems really implausible at first sight that something so stupid, if you like,
can actually lead to things like actual learning of things you might care about in the case of the birds,
for example, allowing them to learn songs.
But let's see.
So how does song learning actual work in the actual brains of birds?
So here's the bird brain.
And what's written down here is all the different major brain areas.
Remember, we talk about brain areas.
And these are the brain areas that sort of cooperate collectively to help the bird to learn songs.
Three important ones are HVCR and L-Man acronyms with some historic provenance.
What's very interesting about the system is that this region, RA, has neurons in it,
which are the things that control the muscles.
So these are the things that actually produce the song.
So a sequence of firing of the neurons in the region RA will make a whole bunch of muscles go back and forth in the bird's throat
and thereby produce the song.
So that's the thing that makes song.
So to make a song, of course, you need to control those muscles in a particular order,
control these muscles first, and these muscles, and these muscles, and these muscles, and these muscles,
and that makes the song.
So that's what you've got to learn.
So RA also gets input from another area called HVCR.
And HVCR acts as a conductor.
So the neurons in this area produce patterns like this, and this, and this, and this, and this, if you like,
but it's actually patterns of neuron firing, and they provide a time base.
Every pattern of firing of neurons in this area HVCR marks out a different time in the song.
It's literally conducting the song.
So when HVCR produces this pattern, certain neurons here in the area RA should fire thereby pushing certain muscles.
When neurons in HVCR fire in some other pattern, then some other neurons in RA should fire producing another push,
a different kind of push in the muscles.
So that's what should be happening.
That's what it's got to learn.
As the conductor marks out time, different muscles get pushed.
So how does it learn this?
There's a third area called L-man, which is a tutor area.
And this tutor area does two things.
First, it drives exploration.
You see, part of the problem is that the bird doesn't know which neurons control which muscles.
You don't know that.
Just like baby doesn't know how to move its neurons in its head in order to control their muscles.
It's something you learn.
So here L-man does two things.
One is it drives exploration by injecting a little bit of randomness in the firing of RA
that allows the neurons there to do slightly different things every time to explore what can be done.
The other thing it does is it provides guidance.
So this area gets back a message that says how did the output compare to a remembered song
and based upon those differences between what you wanted and what you got,
it's able to provide some guidance about which synapses to change or well.
That was a good thing you did, or that was a bad thing you did.
It's able to provide guidance of this kind.
So that's at the level of these brain areas collectively talking to each other.
What about at the level of the individual neurons and the synapses?
Because somehow here at the end every neuron is a dumb little thing
and all it knows is a dumb little rule about how to change.
So it turns out that right here there are synapses between the conductor neurons and the student neurons
which have the following property.
So if the conductor sends a message to the student before the tutor, the synapse strengthens.
If the conductor sends a message to the student at the same time as the tutor,
then the synapse weakens.
That's a measurement in this system and otherwise basically nothing happens.
This is for this particular synapse and this particular bird, this is the zebra finch.
Other birds and other synapses can have different rules.
So it would be a legitimate question to ask whether can such a stupid rule actually allow a bird to learn a song.
So you can check that and the way neuroscientists or at least theoretical neuroscientists will do this
is they'll build a computer model of the system.
So you make a model of the conductor area as a bunch of neurons.
You allow the conductor neurons to send signals to all the student neurons.
Then you say there's a tutor and you get the tutor to send messages to the student neurons too
and then you say I want the student to learn a particular pattern of firing and you press go
and you say can such a stupid rule, this spike timing dependent plasticity rule
where depending upon who fires first you strengthen a weakened synapse, can that work.
So amazingly this is capable of learning the correct sequential outputs.
You just tell it that I want this sequence of firing in this area, it can learn that output.
But there's an interesting finding you can find out this way and that is the learning works better
if the teaching style of the tutor matches the learning style of the student,
the kind of thing you hear in kindergarten.
So basically it could be that the neuron has this spike timing dependent plasticity rule.
If A goes before B then you strengthen, if B goes before A you weaken.
Or it could have this spike timing dependent plasticity rule.
You strengthen so long as both of these fire close to each other.
There are many different rules different neurons implement.
So it turns out that for some of these rules of learning that an individual synapse could implement
it turns out the tutor should teach the student, give it signals
so that it learns early segments of the song before late segments of the song.
That's the way it's going to work best given that rule of how the learning at the synapse works.
But for other students by which I mean other kinds of synaptic plasticity or learning rules at the synapse
you'll find that the teacher should just teach the whole song all the time,
just keep giving corrections everywhere and it'll work better, it'll just work faster.
That's a very interesting finding and this is intended to illustrate two things.
One is this kind of learning, it's a collective effect.
The whole brain cooperates and the synapses have to do things.
So it's a collective of everything that's doing this effect.
The first statement, the other statement is this kind of theoretical computer modelling
that's now possible given I told you earlier that you could build a model of a neuron
and why not put it on a computer and just see what it does.
You can now do that kind of thing and learn a great deal, in fact this is a prediction
for things that ought to be measurable in different kinds of birds and different kinds of synapses.
So we're now at the stage where you can have the style of science that was in physics
which is that you can have theory, you can have experiment, the theories make predictions,
you can have experiments, experiments refine the theories etc.
That is a mode of doing science that's been basically absent from much of neuroscience,
from much of biology and that is now becoming possible because we have so many tools
and so much knowledge of all of this.
I'm basically out of time so I'm not going to talk about the circuits that underlie a sense of space
unless you ask me during question time which can tell you more and instead I'm going to conclude.
So the point of my talk today was to try to suggest to you, say something about how our brains make us
and the kind of point I want to try to make to you is that inside our head we're all collectives.
It's not like there's a thing which is you, there's just lots of neurons, lots of neurons are connected,
lots of circuits, the circuits are connected into brain areas
and collectively the emergent effect of all of this is you.
I tried to illustrate that by looking at various examples and special cases of different kind,
in particular for example this learning.
Another point that I'd like to emphasize is that in essentials all animals are the same.
This turtle and my daughter at an earlier age are basically in all essentials they're the same.
All of the stuff we like about the fancy thinking we do is a veneer on top of that.
Nevertheless there's a lot more to discuss.
So for example we're very interested as humans in decision making, social behavior, things like curiosity and creativity,
language which humans have a particular faculty for language, abstract thought which matters to us
and we'd like to understand the origins of all of these behaviors in neural circuits
and actually there's progress on this.
Just in the last 20 years or last 10 years there are more and more tools for studying all of these things
and I anticipate that the next decade we'll see lots of progress in understanding the neural circuit origin of these sorts of behaviors.
Then usually most people are interested in the even broader question.
One of the things you prize about ourselves is sentience and consciousness.
So we'd like to know what are these states?
They're clearly states of brain.
We don't all say that rocks are sentient or conscious.
They're states of brain.
But we don't quite know how to define even what sentience and consciousness even mean in general let alone at the circuit level.
We'd like to know from this how does mind emerge from brain.
But I actually think that we will make progress on these questions in the next 100 years.
That might seem like a long time but it isn't really.
People live to 100.
If you think about it we have come a heck of a long way since Cahal.
Cahal had pictures of lots of different neurons and suggested that neurons are central to how the brain works.
So there are these different things.
They're like the atomic constituents of brains.
And think about all the things I've been able to say today and I even only just scratched the surface.
So 100 years from now I think there's every chance that we'll be able to give some sort of definition of what we want to mean by something that's conscious or sentient
and understand how those states arise from neural circuits.
So I'm going to stop there.
So we'll go ahead and open it up for Q&A.
I think we have room for about two questions.
That's okay?
So in your opinion what would be the best way to learn a new language?
You're asking the tutor should start at the beginning of the sentence only?
Where's the nothing?
That's really not clear.
I mean there are different learning styles that people have.
So some people, my wife for example is very good at languages and always talks about how when she learned Arabic the grammar was what really attracted her.
There's this very regular structure to that language and learning that grammar helped her to understand the language and everything.
My dad is very good at languages.
He speaks seven languages.
And from him I get an impression of a kind of, on the one hand he just listens to it and reads it and then somehow on the way on the side studies grammar.
It's a very different mode.
It's also the case that the young and the old learn language in very different ways.
Children just listen to it and they just pick the whole thing up somehow holistically.
There's a certain stage, there's a critical period where you can do that and after that critical period the mode in which you learn language is known to be different.
So it's much more useful when you're older to have structures like grammar and things like that and consciously be aware of them and to learn them.
So there used to be this idea, well there is this idea due to Noam Chomsky of something called a universal grammar that's sort of embedded in these kinds of circuits that I've talked about, that it's just built in.
All humans have the faculty for this and that in early childhood when you learn languages what you're doing is pruning away bits of that that you don't need.
It's a little bit like all of those neurons, those interneurons, the retina that sort of take away all the stuff you don't need.
So the idea is that you sort of prune those away and you're left with a structure pertent to the language that you're going to speak in your local area.
Now with the advent of things like chat GPT and this sort of remarkable linguistic abilities there's some discussion, I mean there's debates about whether people agree or disagree still with this proposition.
So I think that's a little bit up in the air because of this particular, if you think about chat GPT as a development in computational linguistics that's raised some questions about this notion of a universal grammar whether it's really there or not.
I mean personally I think they're going to find that and I think the reason why things like chat GPT work is that there is a underlying regular structure in all human language and this is an engine that's built in such a way that it's able to extract that structure in the language.
And so I think people are going to want to understand how to piece apart these artificial engines and work out what they're actually doing inside their inards.
I think we will find that there is such a structure.
So it's different strategies for young and old, it's also clearly different for different people.
I'm not sure that's a sufficient answer to what your question but there you go.
I'm coming, I'm coming.
Oh, thank you.
Yes, you touched on something which was related to my question which is the effect of age.
You know this working in these mathematical schematics that you have.
I'm a senior and I think there are a lot of seniors in this room.
So can you speak to that?
Yeah.
So age produces many effects on the structure of the brain in part because adults basically don't get new neurons.
There are two regions of the brain where you do.
So in the olfactory system, so in this structure drawn by Golgi, this one over here, you actually get new neurons as you age, which is very interesting because nobody understands why.
The other area where you get new neurons even when you're older is the hippocampus, which I mentioned here but didn't really talk about, which is the area associated with the learning memory navigation and things like that.
So these two areas, you do get new neurons and their appearance and their placement within the circuit and where they synapse is known to be associated with the production in the hippocampus anyway of new memories.
So one of the things that happens in people who have memory loss is that system isn't working quite as well.
You're not getting enough neurons, you're not synaphing properly.
But we know the locus of that.
Maybe there is a day will come when you can encourage the production of new neurons.
So you could make the argument that part of the reason we developed these age-related problems is maybe humans didn't originally live that long.
Most people died young.
They were eaten, they died of disease, they died of war, young.
So at an earlier time in human history, we wouldn't have needed, well, in the general populace, populace to kind of maintain all of these structures of that problem.
So maybe that's been changing and we're unfortunately stuck with an earlier evolutionary program.
That's what's going on.
So it's possible that those things can help.
And also, you know, in general, ageing produces effects on all your cells.
So there's things called telomeres that are at the end of chromosomes.
And as you make copies of cells, the telomere is shortened.
And if they get shortened so much that they're not really delimiting the ends of the chromosomes, then you start running into problems.
Because, you know, the program for transcribing genes to make proteins and things won't work as well.
So that's another issue.
But not all creatures have that.
So there are creatures that are essentially immortal that don't have this telomere shortening.
They basically die when they're eaten or they don't get food.
There's a jellyfish that gets younger at some point.
It seems to get younger and then age again sort of oscillates in its effective age.
Very strange.
Very interesting.
We'd all love that.
And these are all subjects of the study of ageing.
There's actually even weirder stuff still, right?
Just to reveal the set of possibilities that can happen.
So there's a creature called Hydra.
You can chop it into two.
It's got a nerve net.
It doesn't have a brain.
It has a nerve net.
So there are nerves all over it that's kind of hooked up in a kind of neural network that operates Hydra.
I'm mostly talking about creatures with centralized brains.
But some creatures actually have a nerve net that's kind of spread about.
You can chop Hydra in two.
It'll make two Hydras.
You can take Hydra and put it in a blender and kind of separate its cells, all separate.
You put it back together.
It makes a Hydra.
All the cells find their location.
I'd love to be like Hydra when I grow up.
So the biology, the living things have many, many, many tricks that we're only just beginning to plumb the depths off.
And so I think in time we will learn to do these tricks.
Heck, you lose a finger in a circular saw.
There's no reason why you can't grow it back.
I mean that the program is there.
So I'm not talking about neurons now.
I'm talking about the rest of the body.
But all of these things are in principle possible.
We just need to know how to unlock within the, in that case, the circuits.
I mean you can think about the organization of your body in terms of the circuits of what cells communicate with what, what turns, what on, etc.
It should be possible.
We just haven't figured out how.
And likewise with the brain and with the problems of aging and everything.
You know, once we understand how these circuits work, there's no reason why we can't get them to do what we want.
These are machines.
We're machines.
And if you can repair a car, you can repair the head.
Thank you so much, Vijay.
And be sure to join us for our next set of lectures.
We have our ulams coming up in September, September 19th and 20th.
Have a good night.

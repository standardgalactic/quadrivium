dynamic decision-making laboratory. She has made various contributions to theories models and empirical
research on individual and group decision-making, in particular in complex and dynamic situations,
which is what drew me to her work and also your texture as one of the most important
quality scientists today. And I think it's been working also a lot, and she's writing to Human
Air Coordination and Cybersecurity, and she co-directs one of these new NSF national air
research institutes. This one is called Alliance for Societal Decision-Making. She is a fellow
authority science society, human factors ergonomic society editor of various leading journals,
and she has been leading a wide range of multi-million and multi-year collaborative
efforts to guarantee industry, such as multi-university research, initiative grants from
Army research laboratories in Army research office, and large collaborative projects
with Dartmouth. And she has visited SFI a number of times. It's one of the lead authors on the
paper on collective adaptation. That is the background for the workshop. That was just
happening at SFI, and I'm glad to hear your talk. Thank you. Thank you so very much for having me
here. I've been to Santa Fe in the past, and every time I come here, I'm like in heaven. I feel that
I can relax, finally, even for a little short time. So what I'm going to talk about today is
work that basically I've been doing all my life, all my research life, and it's this idea of building
agents that look at, are like humans in terms of the thinking and the cognition and the decisions
these agents make. But in particular, I am interested in studying very complex dynamic
situations. We call these situations situations of dynamic decision making. So in conditions like
the examples you see here, there are more than one decisions that need to be made. We make multiple
interdependent decisions under a lot of stress and time constraints. There is usually high uncertainty
on the way we make decisions and take, for example, the disaster management case. So there are multiple
actors working in trying to allocate resources in a very constrained environment. Those resources
are usually limited. And so figuring out how to allocate those resources on time to save the victims
is a real challenge. Obviously, a challenge in which the individual human cognitive abilities
also play a big role. We cannot remember everything. We have to sleep. We are stressed. We feel the
time pressure, et cetera. So these are the type of situations I'm interested in helping humans
make better decisions. And AI, at least one of the initial goals of AI, was to create agents
that would be undistinguishable from humans. So be able to create agents that would make decisions
like humans do. But in reality, the current view of AI, at least, is about machine learning
algorithms that are trained on data. And they are trained to optimize decisions that is very much
not human-like because humans are not optimal at how we make decisions. So the goal of these AI
tools has been to create these elements of optimal decision-making, faster and better than humans.
And in fact, many times, we see competitions between humans and AI. And of course, they are
amazing and they are fantastic and we need them. And they have amazing applications now with
increasingly complex data sets and features. And be able now, obviously, to generate content
from PROM, which is really incredible. So things are advancing very fast. However, I still
believe that to be able to address complex problems like the one I have been talking about,
we need more than algorithms that are optimal. I believe that these generation patterns can help
in situations like this, but they are really not capable of making decisions on their own,
at least not in the way humans would do it. Because the kind of situations humans have to confront
are more than just economic, are more than just optimizing an objective value.
They are about various factors and consequences, including ethical and moral trade-offs,
that are very hard to emulate or to optimize. So I think the ultimate responsibility for
decision making in these type of situations is human. And therefore, we need to figure out
how we can use these tools together with tools that emulate human behavior so that we can help
humans in these situations. So our goal, should I take questions now? Okay, go ahead.
Yeah, I was just wondering about the previous slide and how you said that ethical and moral
considerations. It seems that there is a fundamental assumption that you cannot quantify it or you
cannot encode it as part of an objective function, but I wonder where does it come from?
Yeah, I wouldn't say that we cannot. It's simply we haven't found out how. How to
consider all the different trade-offs and the different type of demands that humans need to
live when they are making decisions in these kind of situations. So that's precisely where we need
to make progress. I just wanted to follow up. The idea about ethics and morals though is that
they kind of can't be quantified because they are subjective and relative and it really is contextual
and you define things very down to make it morally right or wrong. I guess that's a huge like complex
web of trade-offs, I guess. But I guess like you said, it's not impossible. Yeah, I'm not saying
it's impossible. I think we are going to get there, but I think we need more research on precisely
how humans make decisions in these complex situations before we even attempt to create any
computer program that tries to emulate that process. So that has been my goal, yes.
I make complex decisions where I don't exactly know what are the outcomes. It also depends
on my risk attitudes and how important it is. I mean people, for example, during pregnancy,
there are very strong opinions and you are not allowed to do this and this varies over countries.
But at the end, Emily Oster wrote this nice book just to know what are the facts and then
everyone has to judge on his own or her own how to trade drinking wine against the low risk of,
you know, whatever that would be. So these things are probably not part of...
No, they are all part. They are all part. But you are going to see next how I make a distinction
between sort of the traditional risk assessment and dynamic situations and how I believe we make
decisions in dynamic tasks. Across individuals. Across individuals, across situations, absolutely.
So my goal has been to improve and speed up human dynamic decision making. And we want to do that
by building algorithms that emulate the human cognitive processes and that they can inform
other tools like AI optimization algorithms so that we together can make better decisions
in these kind of situations. I plan or I want to go beyond just being inspired by humans,
which is what usually, you know, computer scientists would do. They do care about humans and
they do observe what humans do, but usually they take humans as an inspiration only.
What I'm saying is we need to go beyond just taking humans as an inspiration, but really
understanding their cognitive processes and being able to emulate that process that is creating this
algorithms that can emulate the decision processes, including the biases and errors.
So I actually believe that being able to replicate human biases is essential,
because the only way we can on bias humans is by being able to understand where the biases come
from and then be able to use other tools to help humans make more correct decisions.
So sometimes the process of using simple heuristics has been shown to be more effective
in these dynamic complex tasks than even very complex reasoning or data sets that, you know,
large amounts of data and algorithms that come from large amount of data often are not as good
as human experience. So this is sort of the idea of using not the traditional approach to AI,
but really to promote the idea of we need representations that can replicate the cognitive
process. Why cognitive algorithms? Well, it's a little bit repetitive, I guess, here is we
are not trying to create super humans, but we're trying to imitate cognition. And here are some
of the advantages and some ways in which these type of algorithms can be used. So they can help
explain the cognitive process of decisions on their uncertainty. They are also dynamic and they
can learn, presumably like humans learn to make better decisions. They can inform other decision
aids or other autonomous systems to help humans. They can also be synchronized with the humans.
So for example, if I want to emulate Mirta's decision processes, I will need some information
about Mirta's past experience, but I can trace Mirta's decisions over time to be able to predict
what Mirta is going to do in a particular situation. So we can do that with these models.
And we can also make them be collaborators with others. So if I can emulate Mirta's decision,
then I can predict what Mirta is going to do when she collaborates with me. So that's in general
the idea of why cognitive algorithms are different from traditional or at least
currently traditional AI. So I have been trying to pursue these two essential questions. My whole
research career is about how do human make decisions learn and adapt in dynamic tasks.
And the second is how can I represent such process computationally. So I'm going to go with the first
one too. And when I came to Carnegie Mellon, which was late 90s and beginning 2000, I started as a
professor. The concept that I observed about decision making was very different from what I had in
mind. It was static decision making, economics decision making. But I was inspired by many
people, including this quote by Edward Edwards, where he said that static decision
theories have only a limited future. Human beings learn and probabilities and values change.
These facts mean that the really applicable kinds of decision theories will be dynamic and not
static. So I was stubborn enough to pursue the idea that we want to study dynamic decision making
even though nobody in my department understood what the hell I was talking about.
This is what they were talking about. And this is a more traditional approach of classical
decision making. It's a linear process where you have alternatives. The alternatives are usually
well established, meaning they are obviously provided to you. You have option A and option
B, usually represented as a decision tree, and then calculated based on expected value what
humans should do. Expected value concept, of course, is an extremely useful today and always will be,
I think, a concept on optimal decision making. But of course, very soon people realize that
humans don't make decisions optimally. And so this famous theory by Kanieman and Taversky,
which is still very popular and very influential today, what they essentially did was to modify
the concept of expected value into functions that are more human-like. And of course, they
supported these functions based on human behavior. So humans are not optimal. Instead,
humans are behaving according to these other functions. And that's how you can find out what
humans are going to do. And yeah, that's fantastic and still influential. But this type of theory
is just not applicable to dynamic situations that are constantly
adapting according to various factors over time. Now, if we go completely to the other extreme
of what I was studying early 2000, I worked with Gary Klein in a very big project for the
army research laboratories. And Gary was the opposite of all the behavioral economics. At
that time, it wasn't behavioral, it was just economics. So Gary Klein was studying naturalistic
situations, in particular firefighters. So there are books about naturalistic decision making.
There is a conference. There are lots of people interested in this area. But essentially what
you do in this case is you actually go to the place where decisions are made. Let's say
firefighters. And he describes in his book how he would get on the trucks with the firefighters to
find out how they make decisions in those very constrained tasks. And then he found out that
they don't make decisions. At least that's what they express the firefighters. They just said,
I just know what to do. I just get in there. I've seen these situations so many times in the past.
I just know what to do. So he documents in his book all the stories about the many firefighters
making decisions. And this type of approach has been used in hospitals and in many other
naturalistic situations. So out of that work came a model that he called the recognition
prime decision model, which was very influential to me. The essential element of recognition
prime decision model is recognition. What is recognition is our ability to determine the
similarity between a situation we are confronting and what we have confronted in the past.
And so he proposes how such a match determines whether we know a situation is typical or not
typical. And then based on that, we engage in some sort of mental simulation to figure out
different courses of action. This presumably happens very fast. And then you are able to
finally decide what to do and implement a course of action. So very inspiring. It made
a lot of sense to me, but it wasn't computational. And it hasn't been as far as I know up to now.
There has been many attempts to make this sort of theory computational, but to my knowledge,
none of them have been really successful. So what I do is sort of in the middle of these two
extremes. Dynamic decision making, I sort of realized that I'm not going to make a lot of
progress if I go and get on the trucks with firefighters. So instead, I'm going to take
the approach that others have taken like Brent Bremmer and Joaquin Funke to create micro worlds.
So these are reductions of these real world situations in which we can actually use experimentation.
And to me, it was very important to be able to see the cause and effect relationships,
not only rely on observations. And so this is just a whole lot of the many micro worlds we
have developed and used in my lab through the various years. Initially, I used and developed
this and it took many, many years to develop. It's a water purification plant task that has
all the major elements of dynamic decision making. It's a dynamic resource allocation with
limitations of time, et cetera. So we developed a theory, we gathered a lot of information based
on that task. But then the main question was like, well, whatever you are doing is only applicable
to this task. You haven't shown me that what your models and your things that you are doing
can be applicable to many tasks. So then we went wild and then we just developed all kinds of
micro worlds and started to apply to many other tasks to demonstrate that our theory and our ideas
were more general than just that particular task. Now, this is a summary of behavioral phenomena
that came out of experimental work with many different micro worlds. So I'm going to go over it
but relatively quickly given the time. So the first thing is when I arrived to this area, the
picture was very frustratingly negative. So all humans are very poor at making decisions
in dynamic environments. Even when you give them full feedback and you give them unlimited time
incentives, extensive practice, we are born with something that doesn't allow us to make
good decisions in these very complex environments. People are generally poor at handling systems
with long feedback delays. A lot of people where Bremer, including basically almost all his work,
was about demonstrating of long feedback delays and the effect of that on decision making. So the
longer the delay is, the poorer decisions we make. So, okay, I was taking all that and I was in
agreement with all that, but at the same time, I could observe that people in the real world,
some of them are pretty good at making very complex decisions. So how can we explain that?
And my response to that is that it's in the learning process. It's in understanding how we go from not
knowing how to make decisions to really making very good decisions in really complex environments.
So I started to study different things. For example, I found that if we give headroom, meaning
space for learning to individual decision makers, then they are able to adapt to more
difficult decision situations. For example, putting someone in a low time constraint is going to help
for that person to perform under high time constraints more than those that are always
trained under high time constraints. So the headroom is needed for learning.
Heterogeneity, experiencing different variety of situations are going to help us to adapt
to novel situations and we have several studies on that. The ability to pattern match is extremely
important. At that time, we collected our ability to pattern match with the Raven progressive matrices
test, which is essentially a pattern matching test. And we demonstrated how the score in that test
is very predictive of the quality of the decisions that human makes in these micro worlds.
And we also found how to provide feedback in a way that would be more useful
to individuals by particularly providing observing behavior of an expert was extremely
helpful for people to learn and to adapt even after removing that feedback.
So this is a list of some of the phenomena and all the things that we have found.
I think all over that list, there are two essential elements of dynamic decision making and
they keep coming over and over. One of them was recognition. So I was convinced that similarity
and our ability to detect similarity is essential for making good decisions and memory.
That is our ability to create context specific knowledge. So very different from traditional
cognitive theories that believe that humans with experience generate heuristics. I actually
believed it was the other way around that humans use heuristics when they don't have knowledge.
And as you acquire more context specific knowledge, you actually depart from those
heuristics that are not good to start with. They are only approximate. And so you move away from
those heuristics and you start to apply context specific knowledge. So let me move to the second
question which is how to represent those things computationally. My approach is on cognitive
architectures and in large part because I was at CMU. And in CMU is the birth part place
of cognitive architectures and in many ways of AI too. So in particular, I was inspired by the work
of Allen Newell and Herb Simon that wanted to do had this idea of creating unified theories of
cognition. They imagine or envision this complete program that would be capable of making all the
activities that human mind was able to make. So they imagine to represent all these cognitive
steps and be able to explain all the components of the mind and how they work and produce cognition
together. Many books that are very inspiring coming from this tradition. My personal view of this
is that it is very utopic. It is an extremely complex problem and therefore is very hard
to accomplish. So it was accomplished partly by the actor unified theory of cognition.
So John Anderson at Carnegie Mellon and Christian LeVier, one of John Anderson's students and then
postdoc and now faculty, research faculty has been working on this idea from Allen Newell
and Herb Simon. And if you look at the history of actor, you are going to see that it has
just become more complex with over the years. And in my personal opinion, also less useful.
So what I did was to grab what I felt was useful from that cognitive architecture. And this is
essentially the way knowledge is represented in declarative and procedural forms that is with
facts or with rules and in symbolic or sub symbolic ways. That is with formulas that would
explain how those facts or rules are going to evolve and change over time. So that's what I
did and basically I took that to develop my own mini architecture, if you will. So this
theory called instance-based learning theory is essentially a dynamic decision making process
that is represented in a learning loop. So I'm going to go over this process and this is sort of
the corresponding picture of RPM, the recognition prime, but my own and there are also many
differences obviously. So the first step is again decisions are made by recognizing the
similar situations and mapping with decisions that have been made in the past to be able to retrieve
something that worked in the past. And that happens in this recognition process. We evaluate the new
actions according to the utility of the past decisions which are retrieved from memory.
And then we explore mentally all the possible alternatives. As you can see this is very similar
to the mental simulation that Gary Klein was talking about. And then finally we make execute
decision that is the highest up to that point and that execution is going to modify the environment.
And finally whenever we get feedback we can reevaluate the utility of those decisions we
have made in the past. So that's conceptually what the theory, the learning loop is about.
Now let me tell you about the representations. So the idea is that decisions are stored over time
in memory in the form of instances and those instances are triplets. Those triplets are the
associations of the features that are necessary for that decision, the action that is taken
and the outcome. The outcome can be the observed or the expected utility that is calculated
about making that decision. Now for each potential action, action one or action two,
instances when you try to make a new decision, instances will be blended according to the
similarity of the features. And then the action that has the maximum blended value,
in this case let's assume it's action two, is chosen. At that moment a new instance is created
with the blended value. And then when feedback is received that blended value is changed for the
actual feedback outcome that was received. So that's the algorithm in the in the
representational form. And this is the algorithm in the mathematical form. So the most important
equation I think in this algorithm is the activation, which is not developed by me at all,
is borrowed from ACTAAR. So ACTAAR has backed up evidence for each part of this equation
with a lot of experiments regarding how humans process information. So this activation equation
has three parts. This part is called the base level equation. This part is the partial matching
idea and this part is noise. And essentially this equation takes into account the frequency
of events. So we tend to believe that something is going to happen more often and actually to
retrieve more that information faster when it happens more often. So frequency of events matters
a lot for our ability to retrieve information from memory. And so that is represented here.
We also tend to forget. So things that happened yesterday I can remember faster than things that
happened many years ago. And that is represented by this nonlinear decay function. And then this
part here represents the partial matching equation for each feature in the instance. We can
determine a particular similarity function, which is then aggregated across all the features
and then sort of penalized or exacerbated with a partial matching parameter. And then finally
the noise, which this is just noise. Right now there is not a lot of theory about
what doesn't fit in this part. This part here is a draw from a random distribution or
other type of distributions. And this one is one of the parameters. So for each instance
you calculate the activation at each point of time. And then you can calculate the probability
of retrieving that instance from memory, which is essentially the activation relative to the
activation of all the instances in memory. And then this is the magic I guess of IVLT. What we
do is essentially combine all those instances that belong to a particular choice option
in the form of an expected value. So this is the probability times the outcome.
But obviously this probability is a cognitive probability. It's not the actual probability
of an event. And it's a probability that is calculated all the way from here.
Right. So in this way we are accounting for the cognition of how do we forget information,
how similar the information is, etc. Okay. And we create this expected utility value that we
call blending. And then we select the option that has the maximum expected value. So that's it.
Now I have been interested as I said before in figuring out how general this theory is
across many decision-making situations. And how human-like this algorithm is by comparing
the predictions from an IVL model to the results from an experiment in a particular task.
What I'm going to show you now is a set of examples of human likeness of IVL agents.
And it is important to know that all these examples that I selected here do not fit data.
That is in none of these examples, I first collect the data and then I fit the model to the data and
then I present the results. Anybody can fit data. All these examples come from the theory.
So we do simulations with our theory and then we look at the data and we see how close we are
to the data. Okay. But we don't fit data. That being said, doesn't mean that we cannot fit data.
We of course can fit data. And that is only going to make things better, right? So we can
fit data and we can fit data at the global level, at the individual level, whatever level you want.
We can fit the data, but that's not the point of the examples I'm going to show you.
So because I started with a very complicated task and nobody understood what the guy was doing,
something happened on the way that made me realize I need to simplify things to make things
understandable. And so I went to the root of decision making, which is binary choice.
But in this case, it was not going to be just the decision tree. It was going to be a dynamic
binary choice task. By the way, what I'm going to show since even that there are many examples,
it's just a snippet of the various examples. And if you want to go in more detail in any of them,
I can, but I wanted to show a good variety of these examples. So the first one is binary choice.
And essentially in this case, this is sort of an example of the task. People just have two
buttons and they receive an outcome after clicking on a button. And then they go on and on each of
these buttons have a particular distribution of outcomes assigned to it. And so the idea is that
over time, people are going to learn how to obtain more points out of the right button, the bottom
that is the maximum expected value, right, but from experience. So what this figure is, of course,
not readable, but the idea is to demonstrate how each of these squares are different problems
with different distributions in the two buttons. And it shows the proportion of maximization
is a Y axis. And it shows two lines in each of these things. The dotted line is the observed.
Actually, it's not the maximization, it's the risky rate. So the proportion of times you choose a
risky option. So the dotted line is the human data observed in an experiment. And the black line is
ideal predictions. And so the idea of this figure is just to show that in a large variety of problems
of these tasks, the model just from simulation is able to predict the learning process that humans
follow in this particular task. So the next sort of complication in this journey was to actually
demonstrate that if the probabilities change over time, the model was still going to be able to
predict that trend that humans would be able to do when they were performing a changing task.
So in this particular example, this shows the binary options and the change in probability
during a particular time period. And so humans are doing still that binary choice task,
but the probability changes of one of the options. And so the figures show the rate of
observed choices, again in the dotted line, from human behavior and the predictions from the IVL
model. Numerically, we can calculate the actual relationship between human and model with some
metrics, but I'm going to come back to that later on. Another example is a collaboration,
a cooperation between two agents in the prisoner's dilemma. This is some of the data I presented
also the other day in the workshop, but I'm expanding these figures now a little bit. So the
idea was that if we can emulate the choice, binary choice in this case of a particular individual,
can we emulate the behavior, the collective behavior in this case, cooperation,
emergent cooperation of two individuals working together in particular in the prisoner's dilemma
in this example. So we created copies of the IVL model, one representing player one, the other
one representing player two, and we put them to work in the prisoner's dilemma, and we did
different levels of information provided to the players. So what we observe here
is again the model, in this case the model is the lighter color and the human behavior,
and this is the proportion of the collective cooperation of the pair. And what you see here
is actually the strategies, the sequential strategies. So for example, mistrust is the
number of times that a player defects after the two players have defected. Forgiveness is the
proportion of cooperation after the other player defected even though I cooperated, etc. So the
point of this is that the model also captures the sequential strategies over time. So it's not
just capturing the outcome metric, but also the effects of the sequential strategies and also does
it with different levels of information. We did have to change the blending equation to capture
the descriptive information. I can tell you more about that. I explained that in our talk the other
day. Okay, so the next sort of escalation of these examples is whether it can capture the effects,
the collective effects of groups. So for this what we did was to create different networks
with different numbers of connections, and we used two different pay-off matrices,
and this came from the work of Valier and colleagues on a taxonomy of two by two games.
So we chose the extremes of those two by two games, the independence and the interdependence
games, and we run simulations with our IVL models to play with 16 players in this case,
making collective decisions, and then we aggregated their results. And what we observe is that in
the independence matrix, the collectives are able to figure out what is the best option for
everybody, that is this 5.5, actually faster the more connections there are. So they are pretty
good at finding the right option pretty quickly, almost immediately we can see this, but it's faster
the more connections there are. However, when there is the interdependence that is when the outcome
I'm going to get depends on the other person, that's not the case, that is more connections actually
resulted in less ability or inability to determine what is the best action for both of us, the AA
action. So this was curious for us, and you know this particular degradation of performance based
on the number of connections that the agents had, and so we wanted to see whether this in fact happens
with actual humans. So we run the study with teams, yes. What is the mechanism?
What is the mechanism? Why is interdependence so bad on?
Yeah, so the why which you know I'm not going to show you all the evidence for that, but the why has
to do with our ability to know the one-to-one correspondence of the actions. So if I play
with all of you, I need to keep track of what you did to me last time, and what Henrik did to me
last time, and what everybody did to me. It's very hard to learn that, right? So next time when I
meet you again, then I need to figure it out or remember, right? Oh, she did B, I should do A with
her, right? It's very hard to keep track of that, the more connections you have. The first one is not
difficult because it's very easy to find that regardless of how many connections you have,
if you pick A, that's what everybody should pick. But doesn't it depend on the payoff structure
specifically of this interdependence pattern? I think it does. Okay, I secure myself. You could
have a slightly different payoff structure, you could have the default. I don't remember what she
did, but it does in a pro-social way. Yeah, it does. It would go up. It does, and I mean, this is
an example that it does, right? But in addition to that, Valier et al. have a whole taxonomy
of two by two games, and we run this model in the whole taxonomy, and this was the only,
there were other things that happened, but this was the only one in which we saw this effect.
So this paper is still, I haven't published this paper, but it's still in process.
By the way, sorry, I had to step out of it. That was my doctor call. So the mechanism you just
described seems to depend sensitively on the internal structure of these computational agents
on their memory capacity. I mean, if they have a representation which does
simply record what every other player did, then this wouldn't happen. So there are some choices here
of kind of the parameters and the structure, and maybe even your definition of similarity, right?
So, I mean, similarity would only ask what's the, what happened the last time that Catrine played
that way as opposed to recognizing that I can learn about my interactions from Catrine,
from my interactions with Iname, Hi, Iname. So, I mean, the, so there are a lot of choices
underneath here. I mean, the choices are very transparent in the equations of the model.
That's the choices. In this particular task, the binary choice, you don't even have attributes.
You know A in an outcome. So there is not even a similarity in this case, but the frequency
and the recency play a role, and that's just organic in the model. There is no, yeah,
there is a choice of the parameter of the K, but we don't in, I forgot to say that, in one of these
results, we are manipulating that parameter. We are using the default parameter that is used in
Act R. So it's just coming from, these predictions are coming from the theory.
But in this setting, aren't there some choices about how I represent past events?
Very good. Yeah, in every setting. And that is the major point. I'm going to come back to that
later if you don't mind. But that is exactly the major, one of the major things that we need to
improve. The main choice is the designer of the model on what to represent in the instance.
We say that we don't represent anything that is not fully available to the human.
And it's true. That's what we try to be very committed to do. But still there are choices
on what to represent in the instance. This case is not that case because it's very simple.
You just see A or B. There is no attributes and the outcome. I can come back to the issue of
representation. I know there are many questions. Do you think you could wrap up in like five minutes?
Okay. And then so there is still time for more discussion. Then here is where my strategy to
cut a lot of stuff comes. So I'm going to show a few more things. I'm going to wrap up in five
minutes then. Okay. So again, we run the exercise with human participants in groups. And the human
participants in this case were groups of six because running 16 was a lot more complicated.
But we managed to gather a good number of groups in each of the conditions. And this is what we
observed. So the main effect that we predicted was verified with the human data.
Okay. The next example is a little bit more complicated and is more realistic too. And in
this example, there are many features. It's about phishing. And it's about our human decision to
whether let pass phishing email or not. So we use a data set that was collected by someone else,
not us. And that data set was collected in this form. So there were a bunch of emails that they
created and some of them were real. So real phishing and real ham. Those are real emails.
And then they simulated some phishing and ham emails. And then they asked people to rate the
phishing, the emails, whether they were definitely safe to definitely suspicious. And so what we did
is we plotted the human data. It's here. We emulated the same decision with our model. And it's here.
And we can see that the distributions of the model are very similar to that of the human.
And finally, an even more complex task because this is a very popular theme of research is
cybersecurity. And in this particular case, we are emulating the decisions of a defender in an
individual task. And there is a network that the defender needs to keep safe. And there are some
strategies of red agents or red attackers. Two strategies are implemented. And the strategies
vary in the way they explore the network to reach the objective, which is this operational server.
The interface that the humans use to do this task is what you see here. And we again run
and publish a paper with just the predictions of what the model does against as defending against
two attacker strategies, the meander and the other more direct strategy. And this is our results
from the human experiment. Again, we cannot run 2000 episodes in that case. So we run seven episodes,
but we see how our human experiment emulates the predictions from that model.
So, okay, given the time, I think I would like to conclude with this, which is
our reflection of human likeness. So I think what I have been talking about now regarding human
likeness is based on outcome metrics. So I emulate the decisions of the human, and then I compare
the outcome of the model to the outcome of the human. Usually we use generic metrics like MSE.
Often they are used at the aggregate level to be able to say, hey, our model is doing very well,
and that's what every cognitive modeler does. But I think that's wrong. I think we really need to
improve this metric, especially that we have the mechanisms to be able to compare at a much lower
level. We should always at least compare at the individual level and be able to evaluate the steps,
each of the steps at the individual level. Let me say just that we are using now these models
more directly in some applications to be able to help prevent the biases that humans have,
and to be able to help humans make better decisions. For example, connecting these models
with machine learning models and optimization models, and using our cognitive models as human
collaborators. This is one of the ways in which we are actually using our models in cybersecurity.
By emulating the actions of an attacker or an end user in the case of phishing, we can actually
provide the predictions of what these humans are going to do to heavy machine learning algorithms
that are being used as defender strategy. By providing these predictions, these defender
strategies are being adapted to be able to modify, in these cases, a stalker per security game,
modify the allocation of defense resources. The other way in which we are using these models
is by making them work with the human, along with the human. Instead of having one individual
making decisions in the cybersecurity task, now we have a team. The team is an AI and a human
making decisions in this type of security task. They collaborate with each other and they are
able to accomplish the task better than a human alone can do. In conclusion, our current focus of
AI, I thought I had it. I'm almost finished though, but I thought I had it. Maybe I never,
oh, it's not connected.
Our current focus on AI is on algorithms that aim at making faster and better decisions. The
current focus of AI is on creating algorithms that compete with the human and that are able to
make faster and better decisions than humans do. Our goal in contrast is to build learning
algorithms that can emulate the cognitive processes and can inform those optimization
algorithms to be able to speed up the human decision process. There is evidence of human
likeness in these IPL algorithms, but we know that we need to improve the metrics of human likeness.
What do we really mean by human likeness? We need to develop this evaluation of human
likeness at the cognitive level steps. That is what I think is required to demonstrate some of the
usefulness of these models. Yeah, thank you.
Maybe some people may believe, but who can stay, please go ahead.
Thank you for this talk. You mentioned the prospect theory of Kahneman and Tversky,
and one of the strengths of that was not only trying to match human data on specific examples,
but also being able to abstract out very general biases, human cognitive biases.
Kahneman wrote a whole book about it. Can this also abstract out from your data,
very general human biases in these dynamic situations?
Some, but we haven't done a lot of work on that, but confirmation bias, for example,
is something that we can definitely predict. The big difference is that we are interested in
biases from experience. That's in very sharp contrast, because in their theory, they rely on
descriptions of the options. Having the descriptions is very different than just acting and observing
the outcomes. We only focus on biases from experience, and one of them is confirmation
biases, and yes, our model can predict that. There is quite a lot of work to do on characterizing
those biases that we can predict from experience that we haven't done.
There's a question from Tenda.
Can I just go up on this? One of the things that Kahneman and Tversky most rarely or
ever implemented computational is basically just a verbal re-description of what's going on
and something really disputed. But this basically, you are showing computational what can emerge,
and maybe there is no general class even for some of them.
I'm a little frustrated with the heuristic and biases situation, because it cannot
grow longer because there is no more paper, I guess. I don't know. It just keeps growing and
growing, and new biases are emerging. I was very much in line with the initial idea. I really
like it, and I think it's still powerful. But now everybody wants to bring a bias out. It's like
the 20 questions, right? The idea of Alain Newell is we are doing all these very tiny experiments,
but we are forgetting about the global picture. Definitely, there is a lot more work to do on
biases from experience.
I was wondering about the previous slide of where I don't...
Write the previous to this one?
So I'm wondering if there's a little bit of cluster clarity, but in a way, on the one hand,
we want machines to make that for humans. On the other hand, we're
evaluating machines, and there's similarity with human decision-making.
So it seems that there needs to be some kind of decomposition of human decision-making in
terms of what do we actually want to extract from it? What comes from human limitations,
just cognitive limitations, not being able to evaluate everything? How do we split that, I guess?
I had a lot of slides going step by step in the process. Actually, a paper that
is coming out in PPS does that, so if you are interested, I can send you that.
But yeah, so basically, I would break out... I broke out each of the steps in the IVLT process
to analyze what we know and what we need to know. And definitely, one of those things are
evidence for particular similarity metrics. So in our model, for example, we often use just
linear similarity, and sometimes if it doesn't work well, we change it. There is not a lot of
theory, even though there is quite a lot of work on similarity judgments, but how to translate
that empirical warning to a computational mathematical form and then be able to test it
within the realm of the model, we haven't done that work, and that's one of the areas we need to
work on. Koti, there is a question for Daniel then. Yes. Thank you for a very interesting paper.
There are two points that I wasn't sure I understood. First of all, I want to applaud the idea
that you want to enhance human decision-making and make it rather than make it
slavishly dependent on AI decision-making. This is a theme that I've been discussing for years.
I call it the difference between the Nautilus machine and the bulldozer.
The bulldozer lets you move mountains, but you're still a 98-pound weakling. The Nautilus machine,
you actually develop the strength yourself. The technology is being used to improve your
abilities and make you less dependent on the technology. So I picked that point up. I want
to see if you stress it the same way I do. But the question that I really want to ask you about is
consider the notorious, rueful, reflective remark of somebody who says, well, it seemed like a good
idea at the time. And this requires memory of your past decision-making in detail
and memory of your evaluation and the grounds for that evaluation. Now that seems to me to
be a very important part of human decision-making, that is the capacity to be self-critical and
reflective and to learn from your mistakes by being able to debug your past performances.
I didn't think that I saw any sign of that reflective capacity in your presentation,
but maybe I just missed it the way you were saying it. Yeah, let me address the second point before I
forget. My memory has a large decay. So yes, you are correct that I didn't address that point
very concretely in my presentation. But that idea is essentially
expressive. You can see this slide in the feedback mechanism. And so what happens is that
usually the feedback is delayed and there is nothing we can do about it. And furthermore,
we can make many decisions and then get one outcome. And we don't know which of the decisions we
made is responsible for that outcome. So we use a concept that is well-known in AI of credit
assignment. So how do we associate that outcome to the various decisions that were made? This is
the other major area of research. We have a paper that we've been having trouble to
publish, but it's made available online where we test various mechanisms of credit assignment,
including the TD mechanism in reinforcement learning. So this is a very important part of
learning because it's the only way that you can modify the expected utility with the actual utility
of the decisions you are making and how you do that process will determine a lot how the learning
is done. So we usually just do equal credit until this paper that we are still exploring and
figuring out different credit assignments and trying to figure out how humans do that credit
assignment. And so there's quite a lot of research that is needed in that area. And to go back to
your first point of the bulldozer and dependency on technology, I think we are going to be dependent
on technology no matter what. And I think that's a good thing. I don't have anything against
technology, but the essence of human life is the human and the essence of any decision making
in any complex environments I think will continue to be the human. So I think we should use the
technology to empower the human to be able to help the human improve their decisions. And learning
mechanisms is essential for that, but figuring out how. So it's not like I give you what you want now.
It's like a spoon feeding you on what you need right now. It needs to be a lot more global than
that. Thank you. Let me just comment. I think if you try, I'm trying to imagine your systems
acting as sort of tutors for human beings and tutors that have good models of how the human
beings are making decisions because they are also models about how the tutors are making decisions.
And so that in effect the tutor can say, yeah, I'm making this up now. Yeah, I was tempted by that
way of thinking about this problem. You are absolutely correct. One of the most successful
programs from ACTAAR has been on cognitive tutors. So being able to use ACTAAR to
help children learn mathematics. So you are absolutely right. We are using tutors of a
different kind, we can say. It's not about mathematics. It's about making choices in some
complex environments. But that is exactly how we are using it. We can trace, as I was saying
before, we can trace specific students, decision makers and their history and therefore we can
make predictions about the decisions that particular person is going to make in the next
opportunity. And then we can use other tools, machine learning included or any other AI tools
to support that decision making process.
There are maybe a couple more questions from people who want to. You choose.
We've talked about crisis and errors and we can only say if you see that as something
like ACTAAR, in a way they have to agree with the evidence. They have an adaptation to the world
in which we live or have lived. So in your conception of a dynamic environment, is it also
part of the idea to change the environment to have what we call biases not occur as biases
in shortcomings? We will have interventions on those biases. Yes. So a particular program
which hasn't started yet is from IRPA. So I am about to start a very big program in IRPA on
cybersecurity and it's about predicting the biases of the attackers. The idea is, of course,
that we can modify the situation from the defense side so that we can trap the attacker. And given
that the attackers are as humans as the defenders are, we can trace down and predict when they are
falling trap of certain biases and then be able to react according to that. So yeah, I think
biases are emergent. Again, there are some biases that are not emergent related to Melanie's
question. There are some biases that are based on information, the explicit information, but the
ones that we are able to handle are the ones that are emergent from experience. And yeah,
we are working on that too. Yeah, it's a question about decision making and science.
So you are trying to emulate the way people actually make decisions, but we lack a lot of
detailed knowledge. We wish we had about the neurophysiological mechanisms that realize the
procedures that make the decisions. And so you are necessarily sort of, you have to go beyond
that and you are making decisions that are under-determined by evidence.
And I wonder what kind of commitments or maybe even heuristics you use when it's, you have to
make these decisions about what kind of a learner or what kind of a decision maker a human is.
Does that question make sense? Let me try to interpret it. I am going to answer based on what
I understood. Our type of modeling is symbolic. And so what that means is I don't really go into
the neuro part of it. ATAR, that's right. In my personal opinion, that is why it became very complex
because now ATAR is trying to map particular mechanisms to activation in certain part of the
brains. While that can be very important and necessary, that is not what I do. So I stay at
this symbolic level. So that is one thing, but there was another part in your question.
I know Kelly, you will meet with Koti now. Maybe these two guys could go.
Because maybe you would not be able to talk for a while.
Unfortunately, my afternoon is full. So here's my question. So one way to learn to imitate
human decision makers would be machine learning. And I know some people are doing that too.
And just treat the human's behavior as a thing to be predicted.
And I generally much prefer mechanistic models to machine learning. On the other hand, when you
have a mechanistic model, then you are in a position of having to either defend the specific
mechanism or argue that you're only trying to understand the typical behavior of a wide
variety of mechanisms. So when I look at the model, the mathematical model you showed,
and maybe there's something I'm missing, I'm not an expert in this area, it looked like a kind of
standard online learning model, but with the delay from time. And I know that scientists
have tried to figure out, do people forget things exponentially fast, or is it a power law, etc.
But then at least some of the examples you showed, like shifting this one where the
expected outcomes of the two alternatives were changing. Honestly, I felt like a very
wide variety of models that would update their behavior and forget old data to some extent
would have produced very similar things. So that made me not so convinced that the specific
mechanistic model you wrote down is really the right one. Although certainly some aspects of it,
like forgetting the past so you can learn about new things, is surely part of it.
But I guess that on the other hand, when I think about humans in dynamic situations,
I think that their notion of similarity can be much more general than, for instance,
looking at kind of individual attributes of the instance. People do all kinds of analogy making
and, you know, like, oh, that bit of grass which is still smoldering, which might flare up again.
Oh, that's kind of like there might be a tiger there, or it's kind of like
maybe there's an ambush being played by my enemies or whatever. So I don't know, the mechanism seems
both impoverished in a way, and maybe over committed to specificity in a way. So at the same
time, I'm glad that you're doing something mechanistic instead of just saying,
I'll have a year old network, watch for human, and then predict what the human would do,
which is the kind of thing which is far too popular nowadays. So I'm such a long question,
and maybe it's a long answer. No, I love these type of questions. Let me just answer with what
other two people have said. The first one is all models are wrong, right? And I totally
no accept a priority that this model cannot be perfect in any way, right? So all models are
wrong, but some models are useful. So I like to concentrate on the usefulness of these models,
what it can teach us, what it can do, right? And then the other quote is about using models as
your toothbrush. And yes, we have the tendency to use our models only and forget about other people's
models. We have done comparison, model comparison, and there are including a site review paper of
the comparison of our model with another 16 models. I mean, the proliferation of models
for this kind of thing is huge, right? So many. But there was a competition, modeling competition,
and we tested our model against the winners of that competition and many other of the more
typical models. And our model comes to be more general, and it comes to be more predictive than
most of them. So again, am I using my own toothbrush? Maybe. I like to use my own toothbrush.
The other part is regarding the machine learning. Yeah, I think human likeness depends on what you
want to do, right? So if you just want to predict a particular outcome, of course, having a large
amount of data can help. As I said, everything I presented is predictions from theory. I didn't
use any data before I made the predictions. No human machine learning algorithm would be able
to do that. They need the data. In fact, the less data they have, the worse they are, right?
Okay. Thank you. Thanks for your time.
I want to ask, in a different way, the same question that was already asked three times
at the beginning, which is the question of ethics. Because, well, two things. It seems that this is
a visibly important question in decision-making and situations of warfare or juridical situations.
And secondly, it belongs to philosophy, right? To the Western tradition that runs from Aristotle,
through Emmanuel Levinas, and so on. And the one consensus that the big players within this
tradition all share, it seems to me, is that from the Greek, you know, Pyrenees and their idea of
Epoche is as vertiginous suspension of all decidability all the way through to Walter Benjamin's
idea of mystical origins of justice. The consensus seems to be that the ethical or just decision
can only come from a place of radical undecidability, right? From non-calculability, right? Law is
about calculus, but justice comes from the incalculable. So, in that sense, the distinction
to be made would not be between humans and machines. It would be between calculability
and non-calculability. If you can calculate ethics or justice, it's not ethics or justice
anymore. It's just control, right? I just wanted, that would be the provocation I would put to you
in the name of philosophy, right? So, it's a different language game that we're working
at using. As soon as you deal with ethics, necessarily you're taking that on board, right?
I think that is a world or a controlled world. I guess that is a philosophical question
for our model to be able to account for ethical issues. There needs to be
calculability. And so, you know, philosophically, that moves us out of the ethics realm. I would
accept it because there is nothing else we can do without calculability.
Thank you very much.

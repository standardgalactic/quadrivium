up in the kitchen. Very excited to introduce Donald Martin, Donald's the head of societal context
and understanding tools and solutions at Google. We don't do much for introductions here, but
Donald let me just turn it over to you. You won't appreciate it. Hello everybody. It's really an
honor and pleasure to be here at Santa Fe Institute. I actually have been dreaming about visiting
SF5 for, I don't know, almost 16 years. I read the book Complexity back in 2007, about 2005. I was
like, I maybe want to go there. And so I'm now here. I'm here. Thanks to Will. Thanks for your
time and attention. The title of my talk is Epistemic Uncertainty, the AI problem understanding
chasm. I kind of updated that from a gap to a chasm, but it's a really big problem. And the necessity
of structured societal context knowledge for safe and robust AI. Now me and my team, which also
goes by scouts, we believe that bridging this problem understanding chasm is critical to being
able to realize our goal of responsibility for this AI. And so we're kicking off a campaign
to try to imply more participation by others in trying to understand and solve this problem.
And so we've got something we call our flywheel. That starts off with increasing awareness just
about the chasm and what the causes are. We hope that drives more foundational and applied research,
particularly in some of the techniques we're going to talk about causal theory model, for example.
And of course, we want that to result in real impact in the world. The use case that we focus on is
health equity. And so right now, we're here's where we are. And so we're kicking this off and
you know, bridging the problem understanding chasm as you will see, we think requires embracing
complexity. So I figured what better place to start talking about this in the Santa Fe Institute.
Few calls to action because I know folks in this room, SFI is influential. So we would love for
after the talk, people to spread the word about this chasm, right, and how important it is to budget.
Hoping that people will also take on the task of embracing prototyping problems as complex
adaptive systems before intervening things like data science and AI because we think that's critical
for proactively mitigating bias in these systems that we're building. And then finally,
a key component of bridging this gap is investing and problem prototyping
trust and capability in historically marginalized communities. That's going to be a key source of
the knowledge that we think is missing from AI based product development.
Okay, so with that, I'm going to set just a little bit of context.
I gave this talk a part of this talk last year, I had to kind of explain there's this industrial
revolution. AI is going to be really important. I don't think I have to really do that anymore.
I think you realize we're in the midst of this transformation that people were calling the
fourth industrial revolution and that AI is like a core technology within it. And of course,
you know, we have high hopes that AI is going to do these amazing things and transform healthcare
and medical care. But we're also very cautious, rightly so because of the big conflict in the room,
harmful bias that can be propagated really easily by these systems.
And so it's part of my job to pay attention to all the headlines that come out every day
about instance of bias and all sorts of domains. But I pay particular attention to
headlines that involve healthcare and medical care. And that's really because of my mom.
With my mom, Betty Martin, she died five years ago this month from cancer.
And as I navigated that harrowing journey with her, I experienced bias directly in the healthcare
system. And that really informs how I think about what it's going to take to mitigate bias in AI
systems applied in high stakes domains like healthcare. And so as I said, she died from cancer,
started off as breast cancer, turned into lung cancer. At one point she had to get part of whatever
lungs removed. And as a result of that, she developed atrial fibrillation. And so that can
cause blood clots and strokes. So she had to get prescribed a blood thinner. But the blood
thinner that the doctor wanted her to take would require her to stop eating vitamin K foods,
leafy greens, spinach, kale, collard greens. I love that stuff. That was like a staple in her diet.
And it would also require her to have to go to the hospital
every week to get her blood tested to make sure this blood thinner wasn't harming her in some way.
And so my mom, she balked at this. She's like, ah, you sure there's not another
option? Is there a better blood thinner where I don't have to shift my lifestyle?
And the doctor's response was, you should just, you know, be grateful because there are many
people in third world countries who would die to have access to this particular blood thinner.
My mom was like, well, you know, I don't know what that has to do with me, but
and you all know my mom, of course, but she didn't accept that as an answer.
She talked to me and my sister, my dad. My sister happens to be a medical doctor, a psychiatrist.
So she was able to find very quickly an alternative blood thinner that wouldn't have
required my mom to change her lifestyle when I owed her. And so, and we could easily afford it
with my dad's health care insurance. So we went back to the doctor. We said, hey, we found another
alternative, but we ought to make sure we're not missing something. Like, why didn't you
recommend this particular blood thinner because it doesn't have any of these side effects?
The doctor's response was, I assumed that she wouldn't be able to afford the alternative.
I assumed she wouldn't be able to afford the alternative. So that's this bias kind of
rearing its head right in the middle of the situation. So that's why this kind of informs,
you know, my thought process about how we're going to, what it's going to take to mitigate
these kind of really nuanced implicit biases. And so the real issue here is that the doctor ignored
and abstracted away my mom's, her situation, her circumstances would all call her a societal
context. Didn't consider that when they made that intervention recommendation. So I want to look at
my mom's societal context in a different way. There's mom, born and raised in the USA, doesn't
know anything about, you know, what happened in third world countries. She loved leafy greens.
This is her second bout with cancer. So she, you know, had already experienced having to go to the
hospital every week to get chemotherapy and get stuck with the nail show. So she would avoid that
anywhere she could if it was possible. And then, you know, she had us as a family, which included
my sister, medical doctor knew how to navigate the drug industry very well. My dad, a double retiree
from US Army and the Postal Service, great healthcare insurance. So we had this, this other option.
But the doctor ignored and abstracted away all this societal context, didn't ask about it,
didn't consider it and then made this intervention recommendation that conflicted with my mom's
societal context.
Conversely, do you think that the doctor thought he was being anti-biased by making
this mistake and thus, in fact, being very biased?
You know, we really have, I have no idea. I don't, so we all have biases, right? We can't
tell what someone's intention is, right? I'm sure there's not a negative intention.
But, but this is the result that when you don't seek to understand like the situation or
circumstances, when you make assumptions, it can lead to these problems. And so we really
use this high-level definition of societal context as this dynamic and complex collection of
social, cultural, economic, political, historical circumstances that surround people,
places and things. And interventions that ignore or abstract away societal context can lead to
unintended and unnecessary harm. So this is the key takeaway for the talk, like if you don't
remember anything else, please remember this and try to get that in as you're intervening in
technologies. So now I'm going to dive into the core of the talk. Here's, here's the five parts
I'm going to walk through. First, I want to talk about how we abstract away societal context when
we do ML and AI based product development. Then I'm going to introduce this problem, understanding
chasm. I think it's the root cause of why we do this, abstracting away. And then the last three
seconds are about what we think some of the elements of bridging this chasm are. One is we
think we need some sort of reference frame or model of societal context. Second, we think we need
a way of producing societal context knowledge and we think prototyping problems as complex
adaptive systems could be a way to do that. And community-based system dynamics involves
communities in doing that work. And then finally, even if you have this great technique,
you have to have capability in the world in order to like execute it. So let me dive into the first
part. I'm going to start this off with a quote from a great paper called Fairness and Abstraction in
Social Technical Systems by Andrew Selps, who's a leader, not leader in the ML Fairness community.
If you haven't read this paper, even if you don't care about ML Fairness, I highly recommend it.
But this quote sums it up really well. Bedrock concepts in computer science such as abstraction,
render technical interventions, effective and accurate and sometimes dangerously misguided
when they enter the societal context that surrounds decision-making systems. So you could apply this
directly to what happened with my mom. In this case, we're talking about building products and
systems. So I'm going to just walk through a really high-level view of typical AI product
development lifecycle. I'm using a product development context because I work at Google
when we build AI-based products. The key thing they're recognizing this is that this whole process
is driven by human decision-making from end to end. A lot of times people forget that.
And the main output, of this lifecycle, is a model-based product that generates
outputs, typically predictions, that somehow can help you automate or automate a process,
solve some sort of problem. Of course, to produce that product, we have to train and
tune and evaluate that model. To do that, we have to select and prepare some training data.
To do that, we actually have to actually formulate this problem for machine learning and AI.
We have to take that problem and make it fit the hammer that we have, that we want to use to
solve the problem. And then in order to do that, well, we have to actually have a deep
understanding of the problem itself. So that's typically kind of where this diagram ends.
But of course, there's a key input that drives this whole thing, and that's
observations and data. But sometimes we forget about where those observations and data come from.
They're coming from this thing we're calling societal context.
And then the machine learning fairness, sometimes we call it the social-technical environment.
Computer scientists and engineers and practitioners who are trying to get things done,
they have many different words for, you know, that area, this kind of ambiguous area,
amorphous area. It's the real world where this stuff is going to end up one of these days.
It's the environment. It's the data-generating process. It's where we have to look to kind of
understand the hypothesis space. It's that problem domain. It's where that background
knowledge is going to come from. It's the broader context. And then we have some characteristics
that we associated with this amorphous thing. It's messy. It's complex. It's qualitative.
It's causal and nonlinear. It's subjective. I need to work on objective things. It's dynamic,
and it's historical. Why do I have to worry about what happened decades ago?
Now, the issue is that those characteristics don't align very well with the statistical
processes and methods that we use to build AML models. So that's why we tend to
put up this abstraction boundary and abstract it away. And we have particular practices and
assumptions that reinforce that. One of those is the IID assumption, which assumes that the
variables we're going to try to learn in the training data assumes that there's no
that there's no relationship between them. There's no causal relationship between them,
among other things. And so that assumption, you make that assumption, you're ignoring
abstracting away the very nature of societal context, which is supplying all the data.
Now, the other thing that happens when we extract away the societal context is easy to
forget that eventually you're going to be done with training, the products are going to be in
the real world, and the inputs aren't going to be the training data anymore. They're going to
come from the real world. And those inputs are not going to follow the rules, the assumptions
that we made when we built the systems. And then those outputs are not going to be in the
laboratory anymore or in the evaluation suite. They're actually going to be interventions
on that societal context on society. They're going to cause impact.
And then, of course, those interventions can have an impact on the next round of observations
and data we sample in order to build the next version of that product. So you've got this feedback
loop that can be vicious if we're not careful about the outputs and the quality of the outputs.
Now, the other thing that happens when you're abstracting away
societal context and understanding the details of the data-generating process and the problem
domain is it leads to a lack of knowledge on the other side of that boundary, lack of knowledge
about the problem domain and the data-generating process, things you have to understand to formulate
the problems well. Now, another term we use for that in machine learning AI is epistemic
uncertainty, just a fancy word for lack of knowledge. Usually, we're talking about lack of
knowledge about the model by the model, but that lack of knowledge also applies to all the decision
makers in this ecosystem, which include the humans who are making all these critical decisions.
And in reality, models are basically inheriting the epistemic uncertainty of the decision makers
who are making all the pipeline decisions that are going to impact how that model performs it with
the outputs. Now, it turns out that epistemic uncertainty is the root cause of two big problems
in AI. One is harmful bias propagation, which I'm going to talk about in more detail in a second,
but the other is deep learning robustness. And this also goes by out of distribution
generalization. Distribution shift fits into this category. So, I'm going to drill into that for
just a second to show how experts in this problem are, when I read them, they're saying, we need
societal context knowledge to solve this problem. So, the fundamentals of this deep learning
robustness problem is this example where you have a classifier that's trained to identify
objects and images, gets really good at it, high performance accuracy, really good at identifying
this pig. But if you just change very slightly the input data, the image, in ways that would be
imperceptible to a human, the classification changes to like, ah, this is an airliner.
Right? So, that's the kind of classic example. And so, the issue here is that the classification
model doesn't have an equivalent conceptual model that we have. And these are kind of structures
in our heads that allow us to reliably identify objects under really noisy conditions.
And so, the experts who are trying to solve this problem, and one of them is Yashio Benjo,
who is considered one of the so-called godfathers of AI. When I read what they're saying,
when I read their words, I see they're saying, we need societal context to help us to solve this
problem. So, just to read a quote from another really good paper called Tor Causal Representation
Learning, we argue that causality with its focus on representing structural knowledge about the
data generating process that allows interventions and changes can contribute towards understanding
and resolving some key limitations like robustness and current machine learning methods. So,
when I read that, I see we need structural knowledge about that amorphous thing, the data
generating process, the problem domain. It's causal. So, I think there's a direct connection
between what they're asking for and what we're seeking in terms of being able to fill that
knowledge gap, fill that epistemic uncertainty gap. So, now I'm going to introduce this problem
understanding chasm using this use case, this real-world example of racial bias discovered
in a medical algorithm that's widely used throughout the U.S. healthcare system.
This paper came out about four years ago. The purpose of this algorithm was to identify patients
with the most complex healthcare needs so they could be given access to special programs early on
so that you could ultimately reduce the overall cost in the healthcare system.
And it's important to remember that their motivation was to reduce the overall cost
in the healthcare system. Now, unfortunately, people not selected for the special programs by
this algorithm suffer from nearly 50,000 more chronic diseases than the people who were selected
and the people who were not selected were disproportionately Black Americans. So, this
is why the algorithm was deemed to be racially biased. So, the question is like why did this
happen? What led to this? And the researchers who discovered this had unprecedented access
to every aspect of how this was built, which is not typical. The training data, the machine
learning architecture, the training algorithm, you know, the performance metrics, the actual outputs,
access to everybody who made decisions to build the system. And their conclusion was that the root
cause of this failure was incomplete problem understanding. That's what it boiled down to.
And remember, understanding the problem and formulating the problem for AI is at the very
beginning of this process. But in reality, we really don't spend that much time studying and
trying to improve that part of this process. We spend most of our time on the latter stages,
the later life cycles. But remember, yeah, go ahead. Is that the algorithm that used the amount
of money you spent on healthcare as a as an input? Yeah, exactly. And I'm going to I'm going to get
into that right now. Yeah. So, the key thing I want to go back to is that human decision of choices
drive this entire life cycle and really critical ones happening at the very beginning of the life
cycle. But right now, those processes are pretty ad hoc and informal. And that's what's leading to
my mind, this epistemic uncertainty. So there's a quote that I couldn't resist putting in. And
once I read it, this is one of the papers that we reference and read when we're thinking about how
humans make decisions and choices, because we said, Oh, this is a really critical part of this
process. We should understand how humans make decisions. But this quote jumped out of me because
it says choices do not merely identify one option among a set of possibilities. Choosing is an
intervention, an action that changes the world. That's particularly true when decisions are being
made within these AI product development life cycles, because those decisions impact directly
what ends up getting produced by the machine learning models, which are themselves interventions
on that societal context. So the criticality of this decision making and how we make choices
is very high. And so we spent a lot of time looking at this and the research that we looked at
pointed out that when we make decisions as humans, we leverage our causal inference capabilities.
And those are shaped by strong top down prior dollars that we have in the form of what they
call intuitive theories, as called them causal theories. And causal theories are these models
we build up over time, as we navigate complex realities, face problems, solve problems,
we form theories about why they're happening. And that informs our goals and our strategies to solve.
So let's look at what role causal theories played in the failure of this algorithm.
And remember that the problem was predict which patients will have the most complex healthcare
needs, because we're going to give them access to these special programs. And the causal theory
that they leveraged wasn't explicit at the time, but the one they leveraged was that more complex
healthcare needs is going to lead to an increased spending on healthcare. And so they chose that
as the target variable or their predictor, or the proxy for complex healthcare needs.
They said if we can predict who's going to spend more money on healthcare in the future,
we'll be able to predict who's going to have the most complex healthcare needs.
This is where the algorithm failed. And the reason it failed is because this
causal theory is woefully incomplete, right? It leaves out critical factors that impact
how much black Americans spend on healthcare, even when they have more complex healthcare needs.
These are factors like under diagnosis due to bias, lack of trust in the healthcare system,
wealth and income disparities, and lack of access to affordable healthcare.
Now take it, all those factors actually decrease how much black Americans spend on
healthcare, even when they have more complex healthcare need. And then taken together,
those factors represent a subset of the structural inequities that exist in the US healthcare system.
And those structures were revealed by COVID-19, right? It showed all those structures.
But those structures actually increase how much complex healthcare need there is in that community
while simultaneously decreasing how much they spend on healthcare.
So the causal theory wasn't even close to having the reality of the problem domain that it was
trying to solve. So this is what we call the problem understanding chasm. This difference in
understanding of the key problem from these two different perspectives in this chasm between them.
I'm talking more about what contributes to that.
This lack of knowledge on the right-hand side led directly to that
harmful intervention, that lack of understanding of that additional societal context.
Now one of the key root causes of this chasm is
divergences between communities that are trying to intervene and how proximate they are to the
problems and then their ways of knowing and explaining problems. So on the civil society side,
these are generalizations, but folks trying to solve those problems on the civil society side
are typically very proximate to the problems. They're deeply entrenched in the problem domains.
On the product side, we're less entrenched in the problem domains, we're less proximate to them.
Civil society folks that have a high stake in like solving the societal problem itself.
Related to the broader domain. On the product side, we're typically interested in the business
problem related to that domain. And then finally, on the civil society side, since you're really
proximate to the problem and the humans that are suffering from it tend to prioritize a qualitative
human perspective on the problem factors. So I have a question just trying to blend it to
things like the gravity project, which are trying to determine
codes for social determinants and valves and related projects. Would that be maybe some mitigating
factor where you've got the civil society data informing this decision, I think, rather than
as an organization trying to decide for ourselves what we think is important?
Yeah, that would be exactly right. The thing you want to do is get to the point where
you have some cooperation and understanding the problem. Because you have two different
perspectives. Now, the right hand perspective is not wrong. It's just a different lens in that problem
domain. And solely using that lens, you're going to miss critical things. And the same on the other
side as well. Even on the civil society side, we're trying to solve problems. We're using methods,
sociology, etc. But you also have problems with uptake of those solutions. Statistical methods
get used on that side as well. I have to mention as well. So the idea is how do we bridge this kind
of chasm in terms of how different groups think about and understand problems and get to a shared
understanding helps us get closer to the reality that we're trying to intervene on.
And then I just wanted to show this one more time using that initial diagram, that problem
understanding chasm, right? This gap between societal context and all that messiness and how
we're thinking about the problem on the product development side. Now, just to go back to this,
our goal is, the thing I'm focused on within Google is how do we reduce this epistemic uncertainty?
And to reduce epistemic uncertainty, you need knowledge. But we have to figure out a way to
have that knowledge across that chasm and be useful during these workflows. And so people are
using all sorts of tools to make decisions. And we can't expect them to read to Andrew Self's paper
and figure out what they're supposed to do, read the latest research. It's just not going to happen,
right? So we have to find a way to get knowledge across that chasm and make it available to these
decision makers during every step of that workflow. So this is what our hypothesis is,
is that to bridge that problem understanding chasm in a responsible way, these decision makers need
tools that put community validated structure to societal context knowledge about complex
societal problems at their fingertips, especially during problem understanding at the very beginning,
but also throughout the entire lifecycle, because you should also be leveraging that
understanding when you're evaluating a product for whether or not it's performing well,
like what does that mean, right? It has to be relative to the context in which it's going to
operate it. But we have to remember a couple of key things. And one is that it's factual that
historically marginalized communities are disproportionately negatively impacted when
things go wrong with AI bias. Those are just the facts, right? And you can look at example after
example. And then the other fact is that the lived experience expertise of those folks
is usually excluded from that, from those problem understanding or problem formulation steps.
And so if we are focusing on also addressing these issues, then we're not, we're not
responsibly solving this problem. So we think there are two key ingredients for bridging this
chasm. One is in order to transmit knowledge across that chasm, we think we need to have some sort of
model or reference frame for structured societal context knowledge. Yes. Oh, sorry, I didn't see
you. Yeah. The slide where you said your hypothesis, what do you mean by knowledge?
So that's a good question. That's a good question. So what we need, what we mean by knowledge is
awareness of factors that make up the circumstances that in which you're going to be
deploying models and that originate the problem you're trying to solve. So knowledge is like a very,
you know, that's a big, deep philosophical question that you asked me. But we just mean
awareness of these, these factors that make up these circumstances. Does that answer your question?
I'm sure. Yeah, I just couldn't tell if you meant like data or theories or societal facts or all of
those. Yeah, I mean, as we look at it, you'll see it's kind of like a combination of that,
right? So we're, so we've got lots of data, but we don't have a lot of context for the data
to help us interpret it. So what we're looking for are those, those additional facts that help
make up the circumstances that led to the data existing. And oftentimes those circumstances
will tell you that you're missing data, like you think you have the right data to understand
this problem, but you don't even have that data yet. And so it starts with some understanding of
what these other factors are you should consider. Yeah, exactly. I see, I feel like the biggest
thing is the processes that are generating the data. So you may have known some of Rahid Ghani's
work in Carnegie Mellon. Now he's there where he runs this data science for social good program.
He brings students in and students, for example, go with the cops. They go, they go with the ambulance
driver. And like one of the examples he was saying was that this ambulance driver who already
had two car shins, so he was on probation, he didn't take a call because he's like,
I don't want to risk my job because if I go take this call, I may do something wrong and then
they're going to fire me, right? Your data is not going to say that unless you're actually
riding with the ambulance driver and seeing the ambulance driver park his car under the bridge
to and not accept that call. You're not seeing that. And so exactly. And so it's very difficult to
say, I have good enough societal context knowledge. That's exactly right. You have to
understand these situations and circumstances of key actors and agents. And so we came up with a
way to start to tackle how to model this kind of knowledge. And so I'm going to get to that in a
few slides. So I think I was here. Yeah, so this goes right back to the question about knowledge,
right? So you'll see what we mean by it in a few couple slides. But we think the other key
aspect to address those facts I just laid out is that we need participatory, non-extractive
methods. And by non-extractive, I mean, you know, there'll be a demand for this knowledge. We're
going to make sure we're not just going into communities, gathering knowledge and then taking
it away. Like there's got to be mutual benefit. So and then you also need capacity, right,
for communities to participate directly and own their knowledge and be able to benefit from it.
Yeah, Chris. So some of the things that the community can share are not the sorts of things
that are going to become data on the other side, right? Like that example is not that you're going
to start adding as a factor, you know, are people refusing to take calls because they are on probation.
And so I guess part of the question here is like, which things that the community knows
are translatable into something that then a more principled set of AI creators can use to make a
more humane and fair algorithm? And which ones of them are reasons to be modest about the uses
of AI and to say that, well, actually, what we want is AI products that say, well, here's what
we suggest based on what little this algorithm knows. But there's only a suggestion and you
decision, you human decision maker being advised on this probably know a lot more about this case
than the algorithm does. So the algorithm is presenting its results and assign kind of with
some humility. And yeah, I mean, that's, that's interesting too. The if as well as the how
of representing this knowledge. Yeah, yeah, I mean, the thing I'll highlight there is that,
you know, one of the things you want to happen is you want the models to be aware of their
epistemic uncertainty. You know, I don't I don't know a lot about this. Right. And so even if I
don't end up incorporating, you know, a particular factor directly in the data set or the decision
making variables, I need to have a better understanding of like, I don't know a lot about this space,
maybe we shouldn't even be using AI for this. Right. Right. That's why you want to really
dig deeply into this into the problem understanding and formulation phase before you start building
machine learning systems, right, for a particular problem.
Does that answer your question? It's a longer discussion. I think it's a great
it's a great set of questions like, you know, something like, Dr, here's our recommendation,
but please keep in mind, there are lots of good reasons why this patient might be an exception
to this recommendation. And I I don't know how to convey this partly gets into human
computer interface questions and psychology questions. How do we convey uncertainty and
humility to the people being advised by these algorithms? Yeah. And, you know, yeah. Yeah.
And I think it's just it's it's not the same part as the explainability problem that everybody's
trying to solve, explaining the results of the output. We also be need to be able to explain
like, what we don't know, right, and the risks associated with taking the advice.
Sorry, if I may, this also goes to the incentive. So as an AI tool developer, right, you don't want
to develop a tool that says, I don't know, right. And so first, so for example, Karina Cortez,
Google Research, right, she had a couple of really nice papers on learning with abstention,
where the machine learning system says, I don't know, right. But your software engineers,
you're, they wouldn't want to give a tool that would say, I don't know, right. And so that's
changing the incentives of like, I don't know. Yep. I agree 100% of the agents that are making
decisions is like a key part of that overall societal context. But you'll see right now,
even like, some things happen with the generative AI, and like hallucinations happening,
products are saying, yeah, bullshitting, products are, you see product services saying,
we don't know, don't trust us, we're not sure. Oh, that's great, then. Yeah. Yeah. I mean,
because, and that's being forced by, by this, this becoming an intervention at society that's
getting a lot of kind of feedback. Right. So, you know, so we've got this situation,
we've got this messiness that can't be transmitted over that, that chasm, if, and we're saying,
hey, we need to organize information and knowledge about societal context in some way, this big,
scary thing. So we need some kind of model of it. So it's kind of ironic, right, you're trying to,
you're trying to reduce people abstracting away, but in order to do that, you need some kind of
abstraction, right, that people can use to kind of cope with this knowledge in some way.
And so we're, I was really inspired by some work by a sociologist named Dr. Walter F. Buckley,
who back in the late 60s made the connection between trying to understand society and
sociocultural systems, and between that and systems theory. And so he was like,
we should think of society as a complex adaptive system. When I talk about societal context,
let's face it, we're talking about like, you know, just a, you know, some conception of society as
a whole, how do we think about that? And so our work was based on using complex adaptive systems
theory as the basis. If you look at the characteristics of a complex adaptive systems,
a complex adaptive system, you'll, you'll see some of these characteristics that are in common
with how we were describing societal context. It's complex, adaptive and dynamic. There's this
nonlinearity. I talked about these feedback loops of the ecosystem that we're dealing with.
We talk about things being historical, time delays goes to that complex adaptive systems have
history as well. So we looked at the canon of work associated with this. John Holland is like
a giant in this space. Scott Page, who I think doesn't work with SFI is also somebody I love to
read. So we synthesize some work from these folks to come up with, you know, an attempt, right,
to start to wrap our, our heads and arms around this complex thing. So we came up with a reference
frame or model has three key elements, agents, precepts and artifacts, agents. When we thought
about agents, these could be human or nonhuman, but I'm going to talk about this through the human
lens. So through human lens, these are eight, these are individuals and institutions.
And the ideas that all agents have and essentially are their precepts,
which are our conception, our beliefs, our values, our stereotypes,
our, how we perceive needs and problems.
The causal theories that we talked about, these are all these rules that constrain and drive the
behavior of agents. And these can go all the way down to, you know, the fundamental rules for
building, building a human, right? But at the societal level, we're talking about these kind
of beliefs and values and goals that constrain and drive the behavior of agents. And when agents
behave, their behavior manifests itself in the real world as artifacts. Indeed, they're just
all the things agents produce and create, language, data, laws, institutions, right?
There's some multiple, multiple inheritance. Some, you know, agents can produce artifacts or
also agents, for example, policies, AI models or artifacts were creating products and we're
producing problems as well. And then there's this other important relationship between precepts and
artifacts. This is kind of well-known in the way people think about HCI, for example, is that
the things we produce are a reflection of our values, right? So the artifacts are in some way
a reflection of the precepts that led to them existing in the first place. And of course,
those precepts are influenced by the artifacts that the agent is surrounded by in the world,
right? I read a book, my precepts get updated, then I do some more acting in the world, gets
reflected in more artifacts. But we also wanted to organize this and represent it as a taxonomic
model because ultimately we're envisioning, creating knowledge graphs and databases so that
we can transmit this data across that chasm and make it available to tools and workflows, right?
So somehow or another, we have to have a structured representation of this knowledge.
Now, when we develop this initial model, we hypothesize that these perceived problems and
causal theories were going to be really important if we're thinking about product development.
And when we actually did this work, it was before that Ziad Obermeyer paper came out
about racial bias and medical algorithms. So we were kind of excited that, yeah,
we were on the right track, we think, because he pointed out, they pointed out how critical
these causal theories were and how they directly led to harmful bias in an important system.
And so now the left-hand side looks a little bit more
organized, maybe we can think about societal context in a little bit more of an organized way.
But the key question is, how do we responsibly acquire these causal theories
and make them into structured knowledge? So this is where this practice called community-based
system dynamics came into play. I have some friends in the system dynamics community.
I know the person who wrote the book on community-based system dynamics, and this seemed like
it could be a good way to produce societal context knowledge through the act of prototyping
problems, which are like this core part of societal context, but prototyping them
as systems, as complex adaptive systems. And community-based system dynamics
is a participatory practice for prototyping problems that centers communities and empowering
communities to fully participate and take ownership of their own models of the problems
that they're faced with. But it's grounded in the feedback perspective of system dynamics,
which is invented almost over 60 years ago at MBT. Now SD itself, system that dynamic itself,
leverages visual tools and simulation to support group model building, building prototyping
problems together, and developing collaborative, developing causal theories. And the other words
we use for that are problem models or problem prototypes. And it allows people, health people
do this both qualitatively and quantitatively, and it has a nice bridge between the qualitative
and the quantitative, which we think is important for being able to do this kind of work. And then
the last thing to emphasize is that these prototypes or simulations of these underlying
structures can be used to test interventions on a problem before you actually try to do it in the
real world. And so I'm going to walk through at a high level the basic visual notation that
utilized to prototype problems and system dynamics. One representation is called a causal loop
representation. You might have heard of causal loop diagrams or causal maps, causal maps. The
other is a stock and flow representation. Causal loop diagrams are fully qualitative. Stock and
flow diagrams are qualitative and quantitative. They're provide that bridge into the quantitative
world. So I'm going to walk through this example with a simple loan scenario model.
So the sort of factors you have when you're talking about loans are average credit score,
loans received, borrowers, who's defaulting on loans, who's making their payments.
So you have all those factors specified in language and words. And then you have an arrow
between factors with either plus sign or minus sign on top of it that illustrates the hypothesized
causal relationship between two factors. And the plus sign just illustrates the direction
of the impact that one factor is going to have on another. So to give an example, the plus sign
at the end of the average credit score arrow means that as the average credit score goes up in this
system, the number of loans received is also going to go up. But it also means as the average credit
score goes down, the number of loans is going to go down. So it just means that the impact is in the
same direction. The minus sign means the opposite. So the relationship between loan defaults and
average credit score has the minus sign on the arrow. That means as the number of loan defaults
goes up, the average credit score is going to go down. It's going to go in the opposite direction
as opposed to the same direction. If the number of loan defaults goes down, the average credit score
is going to go up in the system. You can also illustrate time delays, right? Because when one
factor impacts another, it may not happen right away. So in this system, we're saying as number
payments go up in a system, the average credit score in the system should go up. But that's not
going to happen right away. It'll take a while for the credit score, average credit score in the
system to be impacted. You can illustrate that that way. And then you have feedback loops, right?
This really important aspect of complex adaptive systems. There's two types of feedback loops.
One is a reinforcing feedback loop. These are basically indicators of exponential growth,
either vicious cycles or virtuous cycles. And so you can identify parts of the system that
could be driving exponential growth. In this case, the combination of borrowers increasing
and payments being made and average credit score going up, that could drive an exponential increase
in the number of loans in the system. So that's what the reinforcing loop represents. But the
balancing loop is for balancing out this exponential growth. So that's illustrated by the loop that
includes loan defaults. Because that's the one place that even as borrowers are increasing,
yeah, that leads to more payments, but it also leads to more defaults, which leads to a decrease
in average credit score, which taps down the growth in loans being given out. Can I ask? Yeah.
Because you're saying average credit score, this is meant to model what's happening in an
entire population. Yeah. Because of course, these feedbacks also take place in individuals' lives.
And so, yeah. Yeah. Yeah. I mean, system dynamics tends to take a macro view, right? It's looking
at populations as a whole. And oftentimes people use a combination of system dynamics for the macro
view and then agent-based modeling to have more specific kind of ways to model like individual
behavior. But this model might have different outcomes within different demographic groups.
Oh, yeah. Totally. Totally. Right. And again, this is just a way to kind of start to hypothesize
about these factors and you can get more detailed or less detailed. Yeah. You care if the average
credit score is a black box, but you cannot see it. Yeah. I mean, I see what's happening. Do I?
Yeah. Yeah. Yeah. I care. So we put this up just to kind of illustrate like the elements of it,
but as you do this work, you'll see that what happens more and more, you drill into these
black boxes and you try to expose the mechanisms under them. Sometimes you're not able to because
there's someone in control of that. Yeah. Because I was working with Citibank in terms of who they
should give loans to and fair loans, et cetera, et cetera. Yeah. And they're getting some information
from third-party folks, which they don't know. They do not know how the FICO score is being
computed. Yeah. And so it is a black box and they're going to use it. Yeah. It's a model.
It's a model. Right. That is a model that is being used to kind of make decisions.
Right. Yeah. Right. So then the question would be like, how do you actually quantify that in your
epistemic uncertainty? How much weight do you give to it? Yeah. I mean, that's a question. Right.
You have to, and this is something that the modelers contend with and negotiate. And this is why
you want to try to include in this modeling the people who are building those who are building
these credit score models. It's very, you know, it's, depending on what problem domain you're
working at, it's very hard to have participation from everybody that you want to have participation
from. Yeah. And then the stock and flow representation has all those same features.
You just represent those factors with quote, unquote, stocks and flows. What important aspect
of the stocks is it allows you to take into consideration accumulations in a system,
which really allows you to be able to consider the history of the system, the initial conditions
and how things grow and decrease over time and really get into that detail in terms of
relationship between factors. And this is where you start to get into the quantification,
because the way you represent those relationships is with differential equations, for example.
And that allows you to get to the point where you could start to simulate your hypothesis.
And that's, and system dynamics is not about, it's really not like, I'm going to solve this
problem. I'm trying to learn the problem. I'm trying to learn the problem system and increase my
confidence that I understand it well enough to start to intervene on it in the real world.
I just threw this in just to give an indication of what you have put this together. You can start
to kind of look at how these impacts show up in a simulation. This is made with something called
loopy. Anybody can play around with this. This is like a really oversimplified way to model these
sorts of systems, but at least it lets people to start to engage with it. There's really expensive,
hard to use software as well. And one of the issues is that we have to improve the accessibility of
these sorts of tools if we really want to build capability and capacity in communities. So now
we've got this kind of approach that we're like, ah, this might be a good way for us to start to
tackle producing societal context knowledge from places where we're embedded and proximate to
problems. But now we have to have some actual capability in the world to use these techniques
if we're going to have any chance of producing useful knowledge. So now I'm going to talk about
some experience that we've had over the past few years of trying to build that kind of trust
and capability. I'm just just putting up this reminder of the fact that, you know, the lived
experience expertise of historical marginalized groups is usually not involved in this process.
So when I started this back in 2017, I intentionally
wanted to work with folks from historically marginalized communities who are trying to
leverage data and math and science to understand and solve problems.
And there's a group called Data for Black Lives who does exactly that. It's a collection of activists
and organizers and mathematicians and scientists who are trying to leverage data science for good
and also try to make sure data sciences does not end up unintentionally making things worse
when we're applying it into high stakes domains. And so this started with me just going to the
first conference, not knowing what to expect back in 2017. And this led to a two and a half year journey
that resulted in a research paper and a causal theory that's starting to have an impact on
how Google evaluates and think about health diagnostic algorithms. So I just went to the
first conference with no expectations, not asking anybody for anything, just trying to figure out
what people were trying to do, what problems they were working on, what problems they were trying to
solve. And that's interesting people. Some of them famous now, you probably recognize.
And what I discovered there was that there was this need for people to be able to see
how the problems they were separately working on were connected. And now everyone was talking about,
you know, we have to understand the system, the system, but there was no real
concrete way to actually start to tackle that. So I said, Hey, this could be a good place to
introduce system dynamics and systems thinking. I talked to the founder. Yes, he mill in Europe.
She didn't know me from Adam. And so I had to convince her that this could be useful. So I
introduced her to it with a mini workshop. And then we agreed to set a goal of actually
delivering a workshop to that's larger community at the next conference that they were going to
have, which was about a year away. But we had one principle, we said, the people who are going to
teach this workshop to this community, they have to have similar lived experiences to the people
that are going to be receiving this knowledge. And not a lot of diversity in the world of system
dynamics when we started this has changed over time. So we said, we have to, we have to teach
the teachers, we have to create some teachers that can deliver this workshop. And so we did a
learning lab that included people from that community, as well as people from Google to
teach them these techniques so that they could teach the bigger workshop.
We did that over a couple of months, they were able to teach this workshop and these techniques
to a larger group of about 75 people was received very well.
And then we had this capability, and we're like, okay, and a little bit of capacity. What should
we do with it to try to further this journey? And so we decided to do a problem prototyping
experiment around a problem domain that Google cares about and data for Black Lives Care is about.
And that was health diagnosis. Data for Black Lives Care about it because under diagnosis can
lead to health disparities. We talked about that earlier. Google cares about it because,
you know, we're working on building health care diagnostic algorithms. We want them to be
accurate and work well. But there's a lack of training data that can lead to inaccuracy.
And so we thought this would be a good area to jointly prototype a problem that relates
algorithms and lack of data and health care disparities as a way to start to bridge that
chasm. And so we brought people together. We got about nine people together from that initial
workshop. They developed one of these college loop diagrams for that problem domain. I'm not going
to go through this in detail, but you can look at the paper, look at all the data.
But one key thing is that it highlighted trust in medical care as a key factor
that drove multiple balancing and reinforcing feedback loops in this system.
They also created a stock and flow model for this. Trust shows up as this kind of
really big substructure that you can double click in. And the thing about doing this work is that
it allows you to think about these soft variables and include them and approach
bridging that qualitative quantitative gap, quantifying them in some way,
but informed by people that approximate to the problem, but also informed by previous research
that's happened. And so they developed a really detailed trust substructure, which I think actually
should be in a whole lot of models. And that allowed them to develop a simulation and actually
test some of the interventions that they had in mind that might make things
better to include the reference model that we were solving for.
Now, the key, my key takeaway from this was that, hey, you can do community-based research and
actually produce useful research that comes from a partnership with community and people that are
approximate to problems and better than problems. So this paper actually got
accepted, presented at the System Dynamics Conference, what an honorable mission award.
But the key thing I learned is that you can go after if we want to build capability in any place,
but you cannot do that without building trust at the same time. And there's no way to rush this,
right? It takes determination and patience and investment, but that's going to be one of the
key ingredients if we are going to have any sort of chance to bridge this chasm, in my opinion.
So I'm going to conclude with, first, that key takeaway that I started with. Interventions that
ignore abstract way societal context can lead to unintended and unnecessary harm. I think they're
likely to. And then those calls to action. If we're going to drive more attention and research in
this space, we need folks like you all to spread the word. And then we also are really calling
for people to embrace this concept of prototyping problems before you prototype solutions, prototype
problems as systems, embrace that complexity before intervening with data science or AI,
really with anything. And we think this is really one of the only ways to really proactively mitigate
bias in these high stakes domains. And then finally, invest some of your time and energy. And if you
have dollars, dollars in building problem prototyping, trust and capability, like in historically
marginalized communities. With that, I'll close. Thank you, everybody.
So we are missing one time, but there are a few questions.
Yeah. Did you just work at Google? I'm sorry. Did you used to work at Google? No, no.
So I'm wondering, great talk. Thank you. So I'm like studying in public health. And so one thing
we're finding I'm thinking is that the problems start outside the hospital. So those precepts
and the clinicians come from outside the hospital and it becomes a problem once we get in. So how
much of the modeling should we think about outside hospitals in terms of like social and public
investments, housing investment, basically treating that environment in the society and having that
gradient sort of diffuse into the health care industry somehow. I think trying to solve the
health care problem is a hard one. And then it seems like you have to go outside a little bit.
Like we look at your way or somewhere like they invest in their public more than any of us probably
in health care. And we do the opposite. Yeah, we're having this problem. I agree. I agree 100%.
Like, I don't know. I may not have been clear in this talk, but what I think of things I emphasize
in Google is that we have to start with trying to understand the problems that people care about
outside of Google, independent of like our business problems. If we really want to actually,
I think this actually leads to better business as well, start outside. What are the people outside
in society? What problems do they care about and why? And how do we get to the point where we
understand those problems? And then you're going to find some intersection in that problem domain
between the problems that society cares about and the problems that we may have some hammers and
nails to try to solve technology. But you have to start outside of that in order to really start
to get any kind of ground truth. Yeah, it's like, you need to understand what the problem is. Yeah,
I agree. That's why that's why I'm trying to talk about this problem understanding chasm because
of this lack of understanding of the problem, particularly on the product development side.
But but there's multiple ways to understand the problem. I think there are ways to leverage
technology in good ways. But unless we have a deeper understanding that's shared between
these two perspectives, we're not we're not going to be able to do it in a responsible way.
I guess one thing you didn't mention that I expected you mentioning would be accountability,
right? So for example, perhaps the reason some of the AI developers now are okay with the system
saying no is Google search was always retrieving. Now with gen AI is generating stuff. So if you're
generating something that causes me harm, then there's accountability to be had, right? And
the laws are still not there. We'll see, right? Like for example, in Europe, in certain European
countries, they abandon these kinds of large language models because people harm themselves
based on what was told to them, what was generated as opposed to retrieval. So what are some of your
thoughts on accountability here? Who's accountable if the system if the algorithm causes harm to
the community? Well, so you're trying to give me to answer one of these questions that are that's
you can pass it. But it's it's it isn't it you we are all part of the community, right? So we
it's yes, I'm a computer scientist, but I'm also part of the community. Yeah. Yeah, I mean, I think
so. So I think there's accountability in multiple places. I think the works that that's
happening, that's emphasizing, you know, how we govern these, the development of these tools
and products is really critical. If you pay attention to what's happening, what's coming out of
the national out of NIST, they've got a framework that emphasizes that the very first step that
has to happen for companies developing these systems to be responsible and accountable,
it has to start with understanding context, which I think aligns directly with what we're talking
about here in terms of understanding societal context. But I but again, like you said, there's
all these incentive structures that will kind of resist that. And in the in the product development
companies, but even outside, right? Right? Yeah, because the accountability is tied to the incentives
and incentives then are tied to gaming the system, including the the the algorithm, the tool, and the
bigger picture that you had, right? And so they ground around the goals. That would be interesting
to model. You're like, you're like, you're that would be interesting to see what that, you know,
I would love to see how you would hypothesize what that relationship is between those faculty.
Oh, I could definitely do that. I'm a I'm a professor. Yeah. Yeah.
Talk. And I'm typically interested for the final point about gathering a trust from the
historical margin of people and getting and getting involved all the domain experts about the
particular problem. And that maybe that you came from Google, and it's like a big company,
which can we have to deal which have a capacity and maybe some resources to
open a workshop or getting a workshop and getting involved for many. But
even if we just concentrate on the historical margin with people, there's so many different
people around there. And maybe you have to think about so many different parts of other people's
and maybe not every company or production, the product maker have a resources or capacity to
do build a certain kind of trust like that, although it is very important. Yeah. And so
will be will there be any kind of like a substitute or other options to incorporate this or like an
accelerating these procedures to, well, generally like an accessible to more companies or more
industrial parts of this person, rather than getting getting like an opening a workshop and
getting a building a relationship like an after like a more than five or 10 years.
I think if it if there's some kind of other options or like if we can encourage other
entrepreneurs to do this things more, I think it'll impact a lot. I would just know your
like opinion about this. Yeah. I mean, so I agree with you 100% like it's a really hard problem.
And I think this has got to require investment like not just for not just from product companies,
because in some way, you know, we're not we're not in a great position to really be fully embedded
in the problems that society cares about surely. Right. And so I think this is the option where
places like Google need to see some leadership and some ownership in this space and kind of
invest in capacity outside. So we have a gigantic sector of philanthropy, foundations, etc.
who I think need to play like a critical role in this. One of the reasons we're trying to
raise the awareness of this is just for what you described to get other people thinking about
solutions to these really hard problems of like, you know, scale and and dealing with
incentives that have to do with speed. Right. So not easy things to solve. But so this is why
we're like, let's raise awareness, get people talking about this, and have more people
applying their mind to finding solutions that are kind of responsible, non extractive,
can accelerate things. I think there are ways to solve these. This is kind of like a moonshot.
There are things you would have to invent that haven't been invented yet. We're trying to kick
off the flywheel to get those conversations going. Yes.
So one thing I'm interested in is what kind of pushback. It's like one phrase you didn't use,
but I'm sure that we agree on it. It's also one of our favorite things to rail against
is domain agnosticism. Right. Like, like, oh, I'm the clever technical person, send me your data,
send me your zeros and ones. I don't even need to know what the zeros and ones mean.
I have cool algorithms and then I'll help. Right. And so part of the goal is like
breaking that as, you know, what I think still for a lot of engineers and engineering students,
it's still kind of a norm and sometimes even held up as a virtual. So I mean, I'm totally,
I love what you're pointing out. I guess I'm interested in two things,
and I'm looking forward to our conversation later. One is the move here is to work with the community
to generate models at first qualitative, but with the ambition of being quantitative.
And of course, that's an interesting move. It's not the only move. And
and then once you have these models, then lots of other tricky questions come up. Like, do we
really think these models are going to be predictively accurate or are they just sort of scenario
explanation tools or kind of, you know, are they quantitative models that we're actually trying
to learn about qualitative effects of possible interventions? So that's, that's, I think that's
an interesting and I like it. But I also I'm curious about its limitations. Yeah. And one of
the limitations that I expect also some engineers might push back on. So like, in the criminal
justice field, right, everybody says, absolutely, arrest is not the same as crime. It's not just
a very noisy signal. It's also a systematically biased signal. And some people say, well, could we
use conviction? It's like, well, that's also a noisy and biased signal. And of course, you know,
society, neither the government nor engineers have access to did a crime actually occur.
And so you get this sort of defensive reaction that, well, but this is the data we have, you know,
so like, I think we've all been to a million talks where there's one slide at the beginning
saying, arrest is not the same as crime, but this is the data we have, you know, and then,
and then plunging into the technical, you know, the statistics of that as data. And I don't know,
that's, I mean, maybe this just gets back to the humility question again about, you know,
hey, we don't know if this data means what we think it means, we don't know if it's measuring
we think it measures. And therefore, we're presenting our suggestions, our recommendations to
a judge or a doctor with some humility, but then I don't know, I don't know how to wrestle with that,
especially because part of the argument for doing all this is that humans clinical judgment is also
very biased or can be. And so I don't know, there's like, which things can cross the the
chasm into something which is structured enough that to to mathematics, and then
to improve the algorithm, and which things can't really, and then the algorithm just has to say,
hey, I don't know about that and keep in mind that I don't know about that, which as you said,
as Tina said, is, you know, vendors are not usually incentivized to do.
Yeah, I don't know, so that's like where that boundary is, I think is really interesting.
Yeah, yeah, and you and you pointed out one of the
the key root causes of this of this of this chasm, and that's this, this is the data that I have.
Don't force me to kind of think about that messy stuff, right, on the other side of this
boundary, the techniques that I have, I'm incentivized by those techniques, and they
incentivize me to abstract those that stuff away. Now, and also the conversations and then it always
goes to, I've spent a year talking to engineers about like, ah, we can, we can, we're gonna work
on building these models. And the first question I get, but yeah, but how do you know they're
accurate? How do you know they're predictably accurate? And then my pushback is, how do I know
yours are predictably accurate? Right, you're very confident in all the assumptions you're making
throughout that life cycle. No one's looking at them, they're not made explicit, right? And so
eventually you actually get over that hump where people can say, oh, I can see the utility at least
in the beginning of this, of me being more informed. And one of the key things about system dynamics,
it emphasizes, don't try to start with data. Don't try to start with the data set. Start with your
intuition and the intuition of others about what's important in this system, and start off by trying
to get some sort of, you know, shared hypothesis. And then that should drive what data do I need if
I'm trying to intervene on a problem in this system. And if I don't have data about this factor,
maybe I shouldn't be trying to intervene on this particular problem right with this technology.
And so this is why we're, you know, this is why we're kind of purposely focusing on
high stakes domain, healthcare because of, you know, the issue of health equity,
healthcare industry is kind of, I think, a little bit more mature in terms of looking for systemic
causes of disparity and kind of embracing that. So there's kind of more openness about like,
hey, we're trying to, we want to use these algorithms, but there's really a lot of
worry and caution by clinicians and others because they know how dangerous they are.
I think this is again where the accountability comes in, right? Because the doctor can use a tool,
but at the end of the day, he's accountable, right? And to the question that I always get is like,
well, the system is, but your AI system is biased, but the person who's making the decision is also
biased. Yes, but the person who's making the decision, he can go to jail. I will sue his
ass from here all the way to Boston, right? That AI system, who am I suing? Who's responsible?
Yeah, right? It's not clear. And that is important, right? It is important to figure out who is
accountable, especially in America, right? We like accountability, like, who do I blame?
Nobody. I go to jail, nothing happens, right? Because the system said I'm high risk. And that's
it. So the systems are being used as expert witnesses without being cross-examined and not
being held accountable. I think in healthcare, it's a really good domain to pick, because at the end
of the day, it's a doctor and the malpractice insurance he has. And so they have, you know,
like you said, there's a vested interest in making sure these things work out. And I think there's
also like, you know, the Hippocratic Oath as well, like doctors don't want to harm people, right?
I'm large.
But so what you're pointing out is like, you know, these technologies are like
really, really serious interventions on society. And you're talking about like, we have to update
our, the cannons of law, right? For these entities, these systems.
Yeah. And I think especially now with the movement on democratization, right? I want to
democratize X. Well, then this kid can go and develop clear view AI with like $200,000. No
accountability. He didn't know about ethics or nothing. You know, it's what, wait, what?
Yeah. So there are issues. I mean, I love your research area. I mean, it's similar to research
area I've been pursuing. But we cannot not talk about accountability. Like we have to talk about
Yeah, I agree. And I think we also have to make sure that at least for me, like
I'm not, I'm not like, I, I can like, at least me personally begin to kind of understand like,
all right, what do we, what do we have to do to update the cannons of law? Like they're experts,
like that's one of the things I like about Andrew Selbs, his work, right? He's a law professor,
right? So, and that's also why I like about his work about abstract and waste societal context.
Because I think hopefully that means that kind of perspective will make its way into the laws.
You see that perspective making its way into some of the frameworks that are coming out of
the government in terms of how we're going to regulate these things. So it gives me some hope.
But I do know that even if you come up with a law that says you have to understand the societal
context, we are not in any position to actually represent it, present it, whether it's in a,
you know, court of law or in a product development life cycle. And so we have to build capacity.
Whereas somebody has to pay millions and millions of dollars, then people will, will move.
Unless they have billions of dollars, then they, maybe we'll charge them billions.
Billions. Billions to get to billions. I want billions.
Sadly, we're coming up on time. But Donald, I want to thank you again for a great talk.
Thank you for your time.
For tomorrow afternoon, I know a number of you already signed up for meetings,
but if you haven't, he's down in Pod C. So you're welcome to go and

Hello everybody, David Shapiro here with a update.
So we're going to cover two things today real quick.
The big thing is, I mentioned in a previous video, we need GAIA, Global AI Agency.
And we're working towards that.
And so I created a public repository of news and summaries, basically where I'm collecting
evidence and news that is very encouraging because it's all moving in the right direction.
We've got everything from the Future of Life Institute open letter to pause giant models,
news out of the UK, the EU, which yes, I know the UK left the EU.
I made a mistake of forgetting about Brexit in a previous video.
The UN Secretary General talking about yes, he's amenable to the idea of an international
AI watchdog, the United States Senate, their hearing on AI, the EU AI Act, the US House
of Representatives and Senate having now basically matched actions with words.
So after the hearing, both the US House of Representatives and the Senate have introduced
legislation or commissions or policies.
So we'll go over all those in much greater detail in just a moment.
But before we dive into that, I've been working on this thing.
I had this idea for a long time, and it's basically we can use large language models
like GPT because they are trained on so much text data, they're trained on data from around
the globe.
Granted, it is primarily English, so there is definitely some bias, cultural bias in there.
However, that being said, it has read so much about the world that it knows more about religion,
politics, psychology, culture, history than any one person.
So as far as having the ability to take perspectives, it's actually pretty good.
And so rather than walk you through the code, I'll just show you what I've got working so
far.
So basically what I have it do is actually first I need to make sure, well, no, that's
fine.
So basically what I have it do, what it does, anyways, is I have it synthesize a persona.
And so this persona is a whole bunch of random stuff like variables that it can pick to basically
create a synthetic person.
And I don't mean that in the manipulation way, but it has geographic origins, ethnicities,
cultures, languages, political dispositions, different factors about the person, such as
their physical health, their mental health, other habits, preferences, that sort of thing.
And so basically what it does is to create a persona is it'll grab one of these variables
or one option from each set of variables and then create a profile or a dossier in
order to take the perspective of that person.
And then it will talk through on a particular issue.
And I picked UBI because I've been on a economics and UBI kick lately.
And so I wanted to basically come up with a way of automatically understanding what the
concerns that everyone is going to have could be, but not just elucidate or articulate the
problems that they will have or the concerns that they'll have over a policy like UBI,
but how do we actually arrive at consensus?
And consensus is not necessarily, so a lot of people have misconception about consensus.
Consensus doesn't mean unanimous agreement.
Consensus means that it gets to a point that is good enough that people will kind of stop
fighting it.
It's about compromise.
And so anytime that there's been any kind of contentious policy or legislation, even
including the United States Constitution, nobody was happy with it because they had to
come to consensus in order to get something that everyone would tolerate, everyone would
accept.
So it wasn't perfect to anyone's mind, but it was something that they would all accept.
And so the idea is, okay, let's take this random profile of a person so that we can
get a global representation of all perspectives, regardless of how small they are, because
the thing is, when you look at some of these things like atheist, Islam, Christianity,
Buddhist, some of these are overrepresented in some areas and some of them are underrepresented.
Same thing for political leanings.
I even include libertarians and anarchists and statists and communists and reactionaries
and populists and nationalists and fascists.
I don't know if fascism isn't in here.
Fascism isn't in here.
So fascist perspectives don't get represented, but nationalists do.
So close enough, authoritarians get represented.
So the idea is that we can get a very well-rounded representation of all human perspectives more
or less.
There are going to be some flaws with this because it is the first edition, first example.
But the idea is that you can get a very, very diverse set of perspectives on a particular
issue.
And then once you get those perspectives, you can then work through and figure out, okay,
what kind of policy is this person most likely to accept?
And so by then generating a whole bunch of potential policies, you can say, okay, well,
let's look at the commonality between all of these and let's get really creative.
So let me just go ahead and show you how this works.
All right.
So the first thing it does is it grabs a random persona, so you see where it's like heavy
social media user, they're West Asian, cultural background is West Asian, their geographic
origin is from North Africa.
So imagine a West Asian family in North Africa, they're a wealthy family, they're heavy social
media user, so on and so forth.
Okay, so I ask, what would this person think about UBI?
And because ChatGPT is so heavily trained to be a helpful assistant, I couldn't get
ChatGPT to take that perspective, but what I could do is say advocate zealously on behalf
of this person, and that got really good results.
So let's see, as an advocate for this persona, I would say that they might have mixed feelings
about UBI.
On one hand, they are wealthy and underemployed, which might lead them to see UBI as, let's
see, where did it go?
Hang on.
Okay, it's saved.
Sorry.
Let's see, unnecessary for themselves and potentially harmful to their financial status
if it results in higher taxes.
They also might be concerned about the potential for UBI to discourage work and productivity
given their own success in wealth oriented aspirations.
On the other hand, their experience with financial stress and their social concerns might make
them sympathetic to the idea of UBI as a means of reducing poverty and inequality.
However, their nationalist political affiliation might make them more inclined to support policies
that prioritize the needs and interests of their own country and its citizens, which
could include skepticism towards UBI if they perceive it as benefiting outsiders as a form
of globalist policy.
So you see it has a very nuanced way of saying, okay, this person has lots of different beliefs
and it's pretty complex.
Let's see.
Okay, so given their wealth oriented aspirations, nationalist political affiliation and their
experience with financial stress, they might resonate with a fiscal policy or UBI implementation
that is designed to stimulate economic growth and productivity while also addressing social
inequality within their own country.
For instance, they might support a UBI program that is funded through progressive taxation
where the wealthy pay a higher percentage of their income taxes.
This would align with their social concerns and their belief in the importance of addressing
inequality.
However, they would likely want assurances that this would not unduly burden them or
negatively impact their own financial status.
They might also resonate with a UBI program that includes work incentives or requirements
to address their concerns about discouraging work and productivity.
For instance, they might support a UBI that is conditional on recipients engaging in
some kind of work, education or community service.
So this is a theme that keeps emerging because I've run this experiment a few times.
I don't know if that's the underlying training of chat GPT or if it's just this is the most
logical way forward where basically either you have a requirement or an incentive or
reward.
So basically like you get, everyone gets a baseline UBI, but if you volunteer, you get
more UBI, right?
It's like kind of adjusting your tax returns here in America.
Okay.
So finally, given their nationalist policy, they want to UBI that prioritizes the needs
and interests of their own country's citizens.
So this could be something like a tax break if you buy local or any number of ways.
So then I ask it to say, all right, come up with a policy that this person might like.
And so in this case, it said the national prosperity dividend, which that sounds rather
authoritarian, but okay, we'll go with it.
So the NPD, which that's also the acronym for narcissistic personality disorder.
Oh boy.
Maybe let's workshop that before we go live with it.
Anyways, so the NPD is a form of universal basic income that is designed to stimulate
economic growth, reduce social inequality and prioritize the needs and interest.
Okay.
So you get that.
So it's predicated on their engaging in some form of work, education or community service,
more time work, vocational training, higher ed, so on and so forth.
You know, actually like that particular thing, because this is a recurring theme, I'm not
certain that that's actually a bad idea where like you incentivize people to continue to
better themselves, even if it's a small carrot, like, you know, hey, if you're in, if you're
going to higher education, even if, you know, you're at the college degree you're getting
might not ever do anything, you get an extra $500 a month, you will at least be a better
informed citizen and a better civic participant.
Who knows?
I don't know.
Incentivize the behavior you want to see.
Oh, here we go.
The NPD would also include a patriotic bonus.
Yikes.
An additional payment for those who contribute significantly to the country's economy, culture
or society.
Okay.
I wouldn't call it a patriotic bonus.
That's a little, that's a little totalitarian, but I get the, I get the idea.
So this would include entrepreneurs, artists, scientists and community leaders.
So actually I think Ireland already has something like this where basically if you are a professional
artist or author or whatever a creative type, you get a stipend.
I don't know what they call it in Ireland, but there is precedent for this being a thing.
So like if you're, if you're a content creator or a cultural commentator or whatever, you
could get a little bit, an additional stipend.
So all right.
There you have it.
And then if we run it again, so let me do a clear screen.
If we run it again, let's say we get a young adult who's wealthy from Eastern Europe, whose
culture is indigenous South American, interesting.
They like gaming.
They speak Spanish.
That makes sense if they're from South America.
They do not care about the community and they are a very fragile person.
They're an angry, fragile person who's, who is educated, they've been very experienced.
They had a good childhood, sorry, they've had a good, good life experience.
They're presently single.
They're a progressive atheist.
Yeah.
This actually sounds, oh, and they're worried about their career.
Interesting.
Okay.
So let's see.
I'm not going to read the whole thing to you.
Like I did the first one, you get the idea.
So you see how it takes into account, like the last one, they were nationalist and blah,
blah, blah.
And this one is progressive.
Given their moderate intolerance, they might prefer UBI that includes some form of means
testing.
Means testing keeps coming up, keeps coming up as well.
Let's see.
Skeptical about whether everyone deserves UBI.
Ouch.
I, there's some people on Reddit that this sounds like.
And that's not to trash people on Reddit.
I actually learned a lot from some people on Reddit.
But yeah.
So UBI to reduce overconsumption and promote more sustainable lifestyles.
Yeah.
So this actually keeps coming up as well where for some people means testing sustainability.
So basically like you might get an additional UBI bonus if you live sustainably or whatever.
So basically like discourage overconsumption, that's a, that's a trend that keeps coming
up.
So okay.
What environmental U, U, UBI, the puibi, the PB would be funded primary through a progressive
tax system where the wealthiest individuals and okay, yeah, blah, blah, blah.
I haven't seen too much in terms of funding.
One of them did say fund it through carbon taxes, which I thought was, was an interesting
way as you partially fund it through, through that to align with their environmentalist values
would also, oh, here it is.
Carbon tax.
Okay.
So these are some ideas that keep coming up.
Again, I think that the underlying model has a little bit of bias here.
It would be interesting to see if there is a future version of GPT that is not already
pre-trained to be kind of on board with some of these ideas.
Because one thing that doesn't, that has come up is like, if you have someone who is just
adamantly opposed to it, it doesn't say like, this person will never agree to this under
any circumstances.
It works really hard to try and find something that they might agree with.
So anyways, this is all saved out to, it's saved out into the UBI folder as a YAML document.
So it just, it basically just saves the conversation as, as a whole.
I think, yeah, it even includes the system message because the system message includes
the, includes the, the persona.
And the first one, it was hilarious.
It was a radically intolerant feminist who is a Scientologist, which is just like, wow,
this was, this was really interesting as it tried to figure out how to appease a very
dogmatic person.
So anyways, this is a work in progress.
I, I don't know where it's going exactly, but the idea is that maybe you could use it
for policy research, maybe you could use it for, it actually came, it was, I've been thinking
about it for a long time, but I had some inspiration after talking to the Gato community about the
Democratic inputs to AI because the idea of using a chatbot to extract information from
a person, from a real person is one thing.
But then I was like, you know, the model already has a tremendous amount of information.
So why don't we just bootstrap it and ask the model to kind of think through this.
So this is essentially a tree of thought, but each branch of the tree is a different
persona.
And then each of those branches has three sub branches where I ask at those three questions
like, you know, what do you think this person will think about UBI?
What kind of fiscal policy do you think would resonate?
And then finally, given the persona and their disposition, can you creatively conjure up
a policy that has a high chance of reaching consensus with this person?
So it's, you know, basically you can create an arbitrary number of branches.
And then as you, as those branches span out and you get all the leaves, you gather the
leaves together and see what fits.
Okay.
Anyways, so you're up to date on that project.
So let's dive back into the GAIA initiative, the global AI agencies.
So I've been watching the news and I realized that the number of things that are piling
up that make me feel good.
And I know that there's a lot of people out there that are skeptical of, of anything to
do with government or corporations.
And for those people, I empathize with you.
Growing up, I was more like that where I was super skeptical of all trappings of power.
And certainly my friends were very like disestablishment, Tarianism and anarcho libertarian, whatever.
But none of them ever did anything with it.
So this is the world we live in.
We live in a world with corporations and governments and stuff.
And yes, all power needs to be scrutinized.
Seescepticism is absolutely fine.
That is part of the democratic process.
That being said, if you're dogmatically against all forms of power, well, I mean, wish in
one hand and you know, you know what to do with the other one.
See which one fills up first.
So anyways, I have been keeping track of all this stuff because I see the currents and
the trends and it is actually very, very encouraging to me.
So I've got it all in chronological order.
So first was the Future of Life Institute published their open letter, you know, signed
by everyone, including Elon Musk and yada, yada, yada.
What I didn't realize was that it actually came with a policy paper.
And so this policy paper, it's only 14 pages and it has seven policy recommendations.
So this came out in, let's see, April 12th.
I think I might have that wrong, the pause, the big pause.
So the paper was published March 22nd.
The policy recommendations came out a few days later, a couple weeks later.
So what I did was I took all that and then I made a very brief summary of the whole thing
right here.
So you can click on the link and see it, but it's a few basic things, mandate, robust
third party auditing, regulate organizations access to computational power, establish capable
AI agencies at the national level, establish liability for AI caused harm, introduce measures
to prevent and track AI model leaks, expand AI technical safety research, and develop
standards for identifying and managing AI generated content.
Okay, sure, whatever.
It's all pretty boilerplate in the grand scheme of things.
But so that came out March, April, March 29th.
The UK did this, this jobby, which I haven't had a chance to summarize yet.
But basically the idea is a pro innovation, AI regulatory framework, et cetera, et cetera.
Again, you see how long this thing is.
That's why it takes a little, takes a little bit of doing to summarize with a, with a GPT
API calls.
So anyways, they call for a regulatory sandbox, which so does the EA, the EU, good grief,
EU AI Act also calls for regulatory sandboxes.
So if you, if you don't know a regulatory sandbox is basically you create a safe space
where you can experiment with AI, you know, that is a little bit more permissive.
So and this is this is very often the case.
So for instance, one of the most familiar ones is if you're doing medical research,
for instance, you're allowed to use substances that are otherwise illegal.
You just have to go through approval processes.
It's not quite the same.
But the idea is that like people have been able to experiment with THC and LSD and psilocybin,
even though it's still a schedule to drug or whatever in the United States.
You just have to, you have to be approved to do so.
Likewise, if you are an approved entity, the idea of a regulatory sandbox is that you can
still do whatever science you need to do as long as you do so safely.
But also one thing about these pro innovation things is, and this is a common theme that
I actually noticed, which is why I was inspired to do this.
So the UK and the United States have all focused on protecting innovation and accelerating
innovation.
So then in early May, the UN Secretary General Antonio Guterres said that he's amenable to
the idea of the IAEA for AI, which also open AI, I forgot to add that, but open AI basically
published a blog calling for the same thing.
Then a few days later, the United States Senate hearing on AI.
This was the one with Sam Ullman and Christina Montgomery and Gary Marcus.
This was almost a three hour talk.
And I took the transcript of that and I summarized it here.
So the high level summary basically just says, here's the key points that they discussed.
And then I break each one of those down further.
So it takes you 20 minutes to read this instead of three hours.
It's pretty straightforward.
There was a lot of back and forth, a lot of questions.
I watched pretty much the entire hearing.
But clearly, the United States government was listening.
And I wonder if this whole thing was just an orchestrated series of events or not.
But anyways, a month later, the EU AI Act was proposed.
I don't think it's been adopted or ratified or anything.
I want to explain in the comments that there's still quite a process to go through.
They've got to get feedback.
But then more recently, in just the last couple of days, the United States House of Representatives
by Representative Ted Liu and a few others introduced a bipartisan commission.
Basically it's an AI commission.
I summarized it here.
I didn't summarize it here.
I copied the text here because their PDF was garbage.
Seriously, this is the United States.
You can pay someone who knows how to make a PDF.
Good Lord.
So anyways, it's relatively straightforward.
Mostly this is just saying let's appoint a panel to come up with policy recommendations.
It's not really, they're not going to do anything.
Basically it's going to produce three reports.
And so this is going to recommend what the United States Congress does for AI.
Okay, great.
Congressional commission, they're interested in getting more information, which means that
they're going to solicit experts.
So one thing that I thought was most interesting was the panel is that they want to have members
of the commission shall have a demonstrated background in at least one of the following.
Computer science or AI specifically.
Social society including constitutional rights, civil liberties, ethics and the creative community.
Industry and workforce and then government including national security.
So when you get these kinds of people in a room together, it's not just engineers and
not just data scientists.
This is going to be people that are in political science, civil rights, civil liberties, industry
insiders and then finally national security experts.
So this is going to be a pretty comprehensive set of recommendations and I'm actually pretty
happy to see that Ted Lu is leading that.
And Ted Lu is, or is he the Los Angeles County?
So it's not surprising that California Bro is going to be figuring that out.
And then finally most recently was just a couple days ago, Senator Chuck Schumer announced
the safe framework at the CSIS, which is really interesting.
And I don't know if this has been ratified yet or anything.
I haven't been able to find the actual text of the idea, but there is a one pager out
there somewhere that summarizes it very succinctly.
But I took the transcript from this and I summarized the talk here.
So basically it comes down to four major components, security, which basically talks about national
security above all else, but also corporate security and privacy of citizens, accountability,
which talks about how do you, how do you, it's actually pretty similar to the EU thing
where, oh, I forgot to mention this for the EU AI Act.
The primary thing that the EU AI Act does is it bans outright social credit systems and
surveillance like high, basically big brother bans big brother stuff.
And so this is what the security and accountability does.
An interesting part of this was the foundations aspect of the framework.
So basically one of the key things of Chuck Schumer's framework is to protect the foundations
of democracy.
So I was talking with my wife about this and she suspects that this is a direct reaction
to the January 6th attempted insurrection at the US Capitol when people are breaking
into and keep in mind that many members of Congress were directly in danger.
And so we suspect, we being my wife and I, we suspect that this is actually basically
the Congress, House of Representatives and senators, they didn't take social media seriously
and then, you know, Facebook happened with Cambridge Analytica.
I don't think it's controversial to say that, that Facebook and other social media companies
directly contributed to the widespread abuses of misinformation, but also just coordination
of violence.
It's that simple.
And so they, they learned their lesson by not taking social media seriously and now they're
taking artificial intelligence very seriously.
So on a cynical note, this is basically the establishment wanting to protect the establishment
and the status quo.
That is a pretty cynical take.
That doesn't mean that it's the only thing because I actually listened to all of Chuck
Schumer's talk and he had a very clear, like words matter.
He had a very clear-eyed understanding of what's coming.
He even talked about the possibility of jobs dislocation.
He likened it to globalization and offshoring because he said like, yes, globalization and
offshoring did actually help the economy because it, you know, allowed us to lower the prices
of goods and services, but at the same time, millions of Americans lost their job.
And so the implication there was that the United States government did not do a good
enough job to protect Americans while we were rabidly offshoring in the 90s and 2000s.
And then finally, explainability.
Some people commented that Chuck Schumer doesn't really seem to understand how AI works because
it's not a database that you can like say, oh, this is where it got the data from.
That being said, I think that these commissions will figure out, you know, that while you
can't look at the model and infer what was in it, you can look at the training data.
So what I suspect is that there's going to be very soon open source standards on training
data.
So basically, in order to be a licensed and approved and publicly available model, the
underpinning training data will have to be publicly available, or at least inspected.
He did talk about innovation first as well.
So this is a common theme that's emerging at least in Britain and America.
The EU is less concerned about innovation.
They're more concerned about fundamental civil rights and foundational human rights,
which is good.
Like I would I would actually like that.
But as an individual nation, they are highly, highly focused on focusing on innovation first
and then safety rights, accountability and stuff.
So it's basically here's the roadmap of how to innovate safely and fat and quickly.
And the idea is there are geopolitical competitions happening.
Vladimir Putin said what I think it was 2021, the nation that solves artificial intelligence
will own, you know, the next century, and it's probably going to be a lot longer than
that.
Russia, unfortunately, for them does not have the economy or the they have brain drain,
so they're not going to be a participant in artificial intelligence.
It's basically going to come down to America versus China versus the EU.
But the EU is more ideologically aligned with America and vice versa.
So it'll basically come down to East versus West, which is basically, you know, the same
idea of World War Two and the Cold War, which was Western ideology versus Eastern ideology.
So this is what the geopolitical conflict is shaping up to be for the next century.
Yay, repeat of the 20th century.
Let's hope that it's not as bloody.
And I say that flippantly, but I am dead serious because the stakes are very, very high here,
which is why I call this the Gaia Initiative, because Gaia is Greek for Earth or Mother Earth.
And also Gaia was the goddess of monsters, too.
So on the topic of Malik and Shogoth and all those other things, I think that Gaia is a
really appropriate name.
So anyways, this is out here.
It's just under github.com slash daveshop slash Gaia initiative.
I will update this as interesting news comes out.
I might forget about it for a while.
I tend to do that, but it's up there and I find all this news very encouraging.
So thanks for watching.
I hope you got a lot out of it.
Cheers.

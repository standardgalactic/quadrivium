David Shapiro here, your personal chief AI officer.
So what I wanted to do today was unpack
some of the recent patterns and trends
that we've been seeing.
Now I made a video recently where I talked about
all the reasons that I think that AI is slowing down
and of course I'm not the only one.
Now there are plenty of people who disagree with this story
and I'll address that in a minute with respect
to the potential emergence of echo chambers.
But first I want to address, okay, what does it mean?
Now that AI is slowing down,
or at least there are initial signs
that AI might be slowing down in terms of progress,
and that's not to say that it's stalling,
it's just the rate of acceleration is deteriorating.
So when I say slowing down,
that's like we're still at the very early stages
if this trend is reversing.
So the first thing is safety.
This is really great news for people in the safety crowd
because it means that the singularity
is not gonna happen in 2027.
We can kick the can down the road a little bit further
before we get an intelligence explosion
if an intelligence explosion is even possible.
Personally, I've started to have doubts
that we're gonna get those accelerating returns,
particularly as I've seen some new news
about the way that the human brain might work.
There is increasing evidence that the human brain
is not just a matter of computation
based on neural synaptic connections,
but that it could be a combination of that,
the electromagnetic waves that propagate across the brain
as well as quantum effects.
There is increasing evidence that human consciousness
and human intelligence is actually the combination
of several energies and several parts of physics
that are all working together to create that.
So I'm just like, hmm, maybe there's a lot more
to intelligence than we thought,
and of course there's gonna be a lot of people out there
saying, see, I told you so, but it is what it is.
And these are also just possibilities.
But according to this possibility,
it might be that there are gonna be continuing
diminishing returns with respect to neural networks
or even silicon-based computing
that means that it will just be increasingly difficult
to either reconstruct or to capture human-level intelligence.
And another thing that's emerging to me
is that we are going to see a very distinct bifurcation
between human intelligence and machine intelligence,
meaning that it's gonna be kind of like comparing apples
to oranges, and it really already is,
because we look at large language models
which are very clearly processing information.
I remember I had a conversation
with some philosophers a year ago or so,
and they made the somewhat asinine claim that,
oh, they don't know anything, there's no information.
I'm like, that's literally all that they're doing
is just processing information,
but it depends on definitions.
And so to these philosophers,
the idea that this is a machine
that only processes information
because their definition of information
was stuff in human brains.
I'm like, okay, well,
that's just a bad definition of information.
Anyways, going down a rabbit hole.
My point is that it really depends
on how you look at intelligence
and how you define intelligence.
And I really don't like those gotcha questions
because it's like, how do you define intelligence?
And it's like, I mean, it depends on who you ask.
There's a million definitions of intelligence.
And the fact that we don't have a good definition
of intelligence means that also, by extension,
we don't have a good definition
of artificial general intelligence.
And when you ask a mathematician what intelligence is,
they're gonna give you one answer.
When you ask a neuroscientist what intelligence is,
they're gonna give you a different answer.
If you ask a psychologist and a philosopher
what intelligence is, again,
they're going to give you fundamentally different answers.
So moving on, another thing that this is good for,
and this is gonna be really good news,
really reassuring news to many of you out there,
is that if AI is indeed slowing down,
that means that the threat to jobs
and the rate of change for jobs is going to be slower,
which means the status quo that we have
is going to persist a little bit longer
than perhaps some of us would like.
Now, what I do wanna address
is that there's gonna be mixed reactions to this.
So some people are like, let's just get it done,
like replace my job, I'm ready to get out of the workforce,
give me UBI and get me out of the workforce for good.
I don't care.
And other people are gonna be like,
well, this will give us time to create new jobs,
I don't wanna lose my job yet, and so on and so forth.
Now, if I had to guess,
now keep in mind that I'm speculating here,
and that's a lot of what I do on this channel,
my gut check now is that it's gonna be five to 10 years.
And I've talked about this before
where you look at the adoption curve,
and it's like seven years.
So maybe 2030, and 2030 seems to be a pretty sticky date.
So anywhere between 2027 to 2030
is when we might start seeing
some really drastic change out there.
Now, I could be wrong,
we could have a confluence of multiple technologies,
like again, I'm really waiting to see
how GPT-5 and robotics mix,
because you see the number of bipedal,
like humanoid robotic chassis being built around the world.
And like, remember, this is only gen one.
So GPT-5 and Claude Four and whatever else,
you combine that level of intelligence with robots,
that really could change a lot for a lot of things.
And I think there's,
I don't know if it's proven out or to what extent,
but I've heard that Tesla is already using their robots
in the Tesla factories.
And the economic carrot for that is really high.
So don't underestimate the power of that economic incentive
to get things really going.
But overall, if the advancement of AI intelligence
is indeed slowing down,
it just gives us all more time to adapt
on a cybersecurity level, on an economic level,
on a military level.
So it means that, you know,
your life is not gonna get upended soon, hopefully.
So this leads me to want to address another thing.
About what, 12 months ago, a little bit more,
I predicted that we would have AGI by September 2024.
So that's just a few months from now.
Now, what I was looking at at the time,
and, you know, if you go back and watch my videos,
there's a whole bunch of charts and data
that I was looking at.
And this is right along the curve
of what Ray Kurzweil originally proposed,
is to when we would have a human level, you know,
intelligence in a single computer is actually 2023.
So that was one piece of data.
I was also looking at parameter counts going up,
logarithmically, which they have been,
but they've slowed down.
And the one thing that I was not looking at,
so this is the piece of data that I did not include
in all of those calculations,
was the exponentially rising costs
of training subsequent generations of models.
So, you know, as, I think it was Dimitris Abbas
was talking about on a podcast recently,
every subsequent generation from GPT-2 to 3 to 4
costs 10 times as much to train, if not more.
So while all these other things
are going up exponentially, so is cost.
And that did not figure into my calculus.
And so because of that, it's like, oh, well,
if I had thing, you know, recognize that,
I might have said, well, we're probably gonna get
diminishing returns sooner rather than later.
Now I have been talking about diminishing returns
pretty much for the life of this channel.
And I've been wondering, when is the jig gonna be up?
When are we gonna run out of steam here?
And it looks like we're starting to run out of steam.
Now again, you know, the train is still running,
we're still burning pretty hot,
but we're not accelerating anymore.
We are probably on a more geometric trajectory right now
if I had to guess.
And it all comes down to economics.
It really is just with exponentially rising costs
with a, we're entering into what's called a red ocean market,
which means it's not just, you know, a blue ocean
out there with its just open AI with their frontier model.
Lots of other models have caught up to GPT-4O,
Claude 3.5 Sonnet has clearly surpassed it
as far as I can tell.
Obviously people like looking at a,
whatever that benchmark is called,
I don't really give that much weight because it's,
that seems like it's mostly a popularity contest
and open AI still has a lot of fanboys,
but doing a side-by-side comparison of capability
between chat GPT-4O and Claude 3.5,
it is hands down Claude 3.5 is in another league
as far as I can tell.
Now obviously a lot of you out there
use it for different things.
So, you know, it is gonna,
it's gonna depend on your use case.
Another thing that I've noticed is that
there's been fewer breakthroughs.
So like, yes, chat GPT-4O has the voice mode,
which is really, you know, okay, cool,
like it can do a sultry voice and sound effects,
which is great.
But that was a predictable addition with multimodality,
where it's like, okay, text and audio, great.
This is still leaving a huge swath of economic interests
and intellectual interests completely untouched.
Take math for instance,
they still haven't figured out math and physics
and those sorts of things.
And also after playing around with the ARC AGI test,
yes, I have not been a fan of the ARC AGI test,
but at the same time, like it does prove a point.
It does prove a point that the kind of reasoning
that these things do is still very different
from human reasoning,
which is another reason that I'm talking about
a bifurcation of intelligence.
That we might be, and this is again,
as pure speculation on my point,
we might be getting to a point where
we're starting to recognize, okay,
this machine is kind of an alien intelligence.
It clearly has its own consistent way
of approaching the world,
but it is also very different from us.
Now, Bill Gates was on a podcast recently saying
that metacognition is gonna be the next step.
Okay, sure, I mean, I've been working on cognitive
architectures for a while,
and there are some really sharp people out there
that figured out how to give models metacognition
a while ago, it's really just down to prompting.
You can give, for instance,
especially with these gigantic context windows,
you can give one sec, one model say,
hey, you're a metacognitive agent
that's viewing these other thoughts.
Tell us what you think about it.
Help steer it on moral course.
This was entirely all of my work on the ACE framework,
the autonomous cognitive entity framework,
which I did abandon because Microsoft Autogen
and other platforms far surpassed what I could do
on my own, even with help from people on the internet,
because why it's Microsoft,
and they have a lot more money than I do.
Now, that leads me to another point that I wanna address,
and that is echo chambers.
So most of you in the audience,
based on the polls that I've run,
most of you in the audience, statistically speaking,
are kind of in the middle of the bell curve
where you're reasonable and you want the truth,
and you want some honest, genuine thoughts.
There are, however, many people out there
that are on more of the tail,
like left to right tail of the bell curve,
where you wanna see doom or you wanna see acceleration,
and you're not really interested in other narratives.
And the reason that I'm using the word echo chamber,
which is often pathologized,
is because there have actually been a few people
that did directly express to me
they did not want an alternative narrative.
They only wanted to double down on their personal narrative,
the one that they want to be true,
which, believe me,
I want to have all kinds of advancements
happening next year.
That was not hype when I said
that I was predicting AGI this year.
That was a genuine prediction on my part,
and I was like, man, things are happening,
they're accelerating,
but I don't believe that anymore
because of the data that I'm seeing,
because of the trends that I'm seeing.
And I know that that sucks,
like if someone is banking on a certain outcome,
like expectations and reality,
there's always a gap between expectations and reality,
and when that gap gets bigger, it sucks.
Now, some people are gonna take this news
and interpret it in completely unexpected ways to me,
and that's fine.
But what I do wanna caution is for the five
or less percent of you out there in the audience
that are on the tails of the bell curve
in terms of expectations and your disposition,
your valence towards this,
is if you broaden your narratives just a little bit,
then you might be surprised
at some of the other advantages that are happening,
and also just realizing that there is a silver lining,
is that the disruption that is coming
is gonna take a little bit longer,
which means that society will be a little bit more stable,
which means that the risk of catastrophic outcomes
or unintended outcomes goes down significantly.
And on the topic of those narratives
and those echo chambers,
a lot of people have asked me to comment on Gary Marcus,
and I've resisted until now,
but having gotten back on Twitter,
I will say that I've watched some really interesting
and highly vitriolic debates
between namely Gary Marcus, Yasha Bach, and Jan Lacoon.
Now, these are supposed to be the adults in the room,
and having watched Yasha on some interviews,
like he's a very sharp guy,
but even he got into the like, let's just beat up on,
let's like, what is the term that kids use these days,
like let's clown on Gary Marcus train,
and that was honestly really disappointing
because this is someone who's supposed to be like
a high brow like academic researcher,
and he's sharing like caricature memes of Gary,
which, I mean, I would never do that.
I don't particularly agree with Gary anymore,
but that was incredibly immature.
And then Jan Lacoon has often had this like,
old man yells at cloud energy,
which is weird because it's like,
half of what he says I agree with,
like Ferventland, the other half,
I'm like, where did that come from?
So, all right, so what happens?
And this is not, to be fair, taking a step back,
this is not a unique phenomenon in AI.
Some of, one of my good friends as a physicist,
this kind of thing happens in the physics community
all the time, apparently,
where it's like disagreements and arguments
over interpretations will actually like,
come to shouting matches and sometimes fist fights.
Physicists are actually pretty hardcore, it turns out.
So, from my reading of, you know, human nature,
what I, the way that I interpret this is that
we have a tightening status game.
So, Gary, Yasha, Yan, all of these people,
they suddenly saw themselves having much,
much higher social status because of artificial intelligence.
And so, one way to compare this is,
imagine you're back in high school
and something changes and suddenly,
the nerds are all the most popular kids in school.
Well, then something changes again,
and it's like, oh, well, actually,
instead of the top eight nerds, now it's the top five.
And so, three have to get kicked off the island.
That's what's happening.
And so, they're scrabbling over diminishing social status
because, again, with AI slowing down,
it's no longer as hot and sexy as it was a year ago.
It's no longer, you know, you can't just say,
hey, I was gonna kill everyone
and get an invitation to the TED stage anymore.
And so, because of that,
because the status game is narrowing,
because the number of people
that can be high status is going down,
the rules are becoming more arbitrary
and people are becoming a little bit more snippy,
a little bit more vitriolic, as I said.
The stakes go up because the risk of losing status,
especially, this is what we saw with Ilya.
I talked about this extensively.
The reason that Ilya was socially canceled with an open AI
is because he made the cardinal sin
of attacking Sam Altman,
even though he was doing it for what he believed
was the right reasons,
that was a violation of the social norm,
which is Sam Altman is king.
And of course, Sam Altman, as a power seeking person,
was not going to tolerate that.
Consciously or unconsciously,
that was just never going to,
he was never going to tolerate it.
So what happens is,
other AI commentators out there,
namely Gary, Yasha, and Yan,
are doubling down on their narratives,
because basically they're gonna be doubling down
on the narratives that got them that social status
in the first place.
And that is my read on the situation.
And also, I take that as evidence
that AI is slowing down,
because again, if AI is running out of steam,
then the amount of space
that the public square needs of AI commentators
is going down.
It's also been a year and a half
since Gary Marcus was in front of the Senate.
And have you seen him or heard him any other place?
No, like his 15 minutes of fame might be over,
and that sucks, like that doesn't feel good.
It does not feel good to feel like you're being left behind
by the conversation or by society,
which is to me, an explanation as to why Gary
has been so incredibly salty lately.
And then of course, other people
that are not as aware of these status games
will jump in on bullying,
because if you show weakness in a high-stakes status game,
people will unconsciously,
it's tall poppy syndrome and a number of other phenomenon,
people will unconsciously jump in on that
and say, ah, time to bully that person,
because they're signaling that their status is vulnerable.
So that's my read on the whole situation.
And yeah, it's not ideal, it's not what I hoped,
it's not what I predicted,
but I ignored the numbers, I ignored the money, right?
Like, and hindsight, that was pretty dumb.
So am I still predicting September 2024?
Again, this is where I'm gonna double down on my narrative.
I think that GPT-5 plus robots
will surprise a lot of people with what it's capable of.
Is it gonna replace all of us?
No, it's gonna be like the Nestor class four
from iRobot, where it's like,
it's capable of running your mail for you automatically,
but not much else.
That's kind of what I predict.
So it's like, you know, you can get rid of like,
maybe some warehouse workers, some factory workers,
some mail carriers, but it's not gonna like,
upend the whole economy.
So, all right.
This has been your first episode of David Shapiro,
your personal chief AI officer.
Let me know how you think this went in the comments
and I'll see you next time.
Cheers.

Check it, check it, check it, check it.
Okay, level check.
I think we're here.
All right, so background.
It's been a little while since I've done,
you're over here now,
since I've done one of these tutorial coding videos,
but I woke up at like 2.30 in the morning
and just had to work on this.
So I hammered it out.
Basically, the purpose of today's video
is to demonstrate using ChromaDB,
which is a local vector database.
It's basically like SQLite.
If you're familiar with that,
which is a self-contained SQL database, relational database,
this is functionally very similar to SQLite,
except it is a vector database,
meaning it does semantic search.
One thing that's really great about it
is that it has its own built-in embeddings tools.
I think it's based on BERT.
Anyways, you can check out all the documentation here
on trichroma.com.
The Getting Started and Usage Guide is pretty good.
It's not complete every now and then.
I find that I have to go to the actual repo
to look at how some of the internals work,
but it is pretty brain-dead simple.
So let me just go ahead and show you.
This is my private instance.
Oh, so before we get too lost,
I do have a public instance,
daveshap slash chroma db underscore under chatbot
underscore public.
I've got a little integration guide usage.
It's, I mean, you probably don't need this.
You can futz around with it,
but this will get you started.
I also use chatGBT to just get a really basic explanation
of the code.
You probably won't need it once you take a look at it.
So off the top, let me just show you how this thing works.
So it's a basic chatbot.
You can see I didn't specify it.
So it's getting all mini-LML6v2 from Huggingface.
Great, so it's like, hey, let's see.
What were we talking about last?
This probably won't work because it's just gonna,
in the future, it wouldn't work
because it's gonna have multiple KB articles
in the background.
Oh, I need to explain like all that.
So I know that I just said like KB articles,
don't worry, we'll get to it.
But anyways, I wanna show you that I just started it up
and what it's gonna do is it's gonna use
the last few messages to search its internal KB article
for the last information,
but it also has a user profile for me.
In our previous conversations,
we discussed AI alignment, morality, ethics,
and epistemology within AI development.
You also shared your plans to communicate
your ideas on YouTube, unplug your computer,
and spend more time outdoors,
and use digital wellness settings
to improve your work-life balance.
Working on that.
Additionally, we talked about your recent experience
with severe insomnia and the importance
of maintaining a healthy balance
between work and personal life.
Yes, that's actually why I created this chatbot.
Let me show you.
So there was a,
there was,
there was,
God, my brain.
I was using chat GPT as a reflective journaling tool.
So what I mean by that is
if you plug in this message,
and I know I'm scattered, I'm all over the place,
this is what happens when I have severe insomnia.
Anyways, so basically I use chat GPT
as a reflective journaling tool to figure out
like how I'm feeling about things,
because as an autistic person,
I often need help with this.
And I don't like journaling because just talking to a page
is kind of dumb,
but it's like, hey, I need to talk something out.
And so anyways,
by workshopping this system message with chat GPT,
I came up with a pretty good reflective journaling tool.
So you could say that this is a therapeutic tool,
but by couching it in the language of reflective journaling,
it's not like medical therapy or psychotherapy or anything.
But it's just like, you know, I can say like,
I have been working so hard and I don't know why.
I actually do know why now,
but this is kind of a shorthand of the conversation I had.
Let's try and figure out, it's driving you so hard.
Can you think of any specific goals
that might be pushing you to this extra effort?
So you see how the tone of this is much more straightforward
and it's very focused by asking
those like kind of probing follow-up questions.
This is why, you know, it's in the investigation phase.
Anyways, so I had this idea and I was like, okay,
this is great, but I need,
if I'm gonna use this as a long-term journaling tool,
I'm gonna need this locally
and I'm gonna need persistent storage
because as this is just the playground,
if I do a refresh, it's gone and that's no good.
So actually here, let me go ahead and just save this
to the, we're gonna call this the system message
for reflective journaling.
So you can use this if you want.
All right, so anyways, so you see it has this
and then you see it says updating user profile
and updating KB, okay, cool.
So you see that it fundamentally basic chat bot.
So now let's start to unpack it.
So first we will go look at the, just the chat file.
So this is a super brain dead simple chat bot
with infinite memory, infinite memory.
I know some people got grumpy when I said
that Pinecone had infinite memory.
From a human standpoint,
it functionally has infinite memory
because, you know, this thing can hold
probably a million KB articles,
which is more than enough to document your entire life.
So from a human standpoint, it is functionally infinite.
All right, so from the top,
we've got a few basic utility functions,
save yaml save file, open file,
and then a chat bot, which calls the GPT-4 model.
You could switch this out to 3.5 turbo.
If you don't have access to GPT-4 yet,
it does not work as well.
There's a reason that I use GPT-4 because it is smarter.
I also set the temperature to zero
because I don't like it to be too creative,
especially with a lot of the functions that I have it doing.
You actually want it to be more deterministic
or mechanistic and that you wanna get
the same results every time,
especially when you're updating the user profile
and the KB articles.
You can see right here that every time you call the chat bot,
I dump the whole thing to apilogs slash convo
and it's a yaml file.
So here's my private one.
So apilogs, here's an example.
So each item is gonna be here.
Actually, that's not a good one
because I changed the way that it saves it.
Let me show you a more recent one.
So the first element is always gonna be the system message
that was in the last convo.
So then here's the KB article
and you can see that it was updating the KB article.
And so each one of these items is like,
you'll see, but anyways,
I just wanted to show that it logs everything
because well, sometimes it does things
that you don't understand.
All right, so that's an example of the apilog
and then if the conversation,
if the overall conversation is too long,
it'll go ahead and trim the oldest chat message.
So the chat GPT web interface does this automatically
where it'll just kind of groom the backlog of messages.
So we have to do this manually.
So I just have it cut off at 7,000 tokens.
You could probably do like 7,500 if you want to
because a lot of these are gonna be limited
but you have a user profile and a KB article
that gets wedged in which are both up to 1,000 words
which could be around 1,000 tokens.
So having it trim at 7,000 is probably where you want it.
So that's the primary, those are the helper functions
and then you have a super straightforward,
you instantiate ChromaDB right here.
So you set the persistent directory
which is, I have it right here, ChromaDB.
So this is my instance, my personal instance of ChromaDB.
It's not gonna be the one that you find up here.
This is the public version.
So if you go into ChromaDB,
you'll see just a placeholder file
so that the folder's already there.
You don't need to instantiate it.
Let's see, going back to here.
So ChromaClient, so we instantiate the ChromaDB client.
This is again, almost identical to SQLite
or other similar things.
So about a year ago,
I tried to do basically the same thing.
I called it VDB Lite for Vector Database Lite
instead of SQL Lite, Structured Query Language Lite.
But this company went and did the same thing
and I think they've already got
like a $30 million valuation or something.
I was like, damn, I should have stuck with that.
Anyways, they figured it out.
I think it's based on the same underpinning technology.
They're using an open source embedding transformer.
I think they're also using the Facebook AI semantic search
and the device engine and the background.
Anyways, so you instantiate the client.
You need to use the settings to have a persistent directory
because by default, this entire thing is fully ephemeral.
I think it does cache it somewhere,
but I wanted to be very explicit saying save it here
for reusability.
And so then collection is ChromaClient,
get or create collection named Knowledge Base.
So this is my personal Knowledge Base.
Then we instantiate the conversation
with open AI, the chatbot.
And in this case, because we're saving everything necessary
into a personal user profile and the KB articles,
like why even load the conversation?
All right, so let me show you the system default message.
So the system default message is where it starts.
Your chatbot is whose mission is to assist
the following user, your ultimate objectives
are to minimize suffering and hence prosperity
and promote understanding.
The provided information about the user
and the Knowledge Base article
should be integrated into your interactions.
This is private information not visible to the user.
The user profile compiled from past conversations
encapsulates critical details about the user
which can aid in shaping your responses effectively,
which you saw here.
So you see like it actually knows quite a bit about me
from our past conversations.
This was populated here in the user profile
and the KB article.
So basically it says, then it also explains
that the KB article is a topic compiled similarly
from past dialogue serving as your long-term memory.
While numerous KB articles exist in your backend system,
the one provided is deemed most relevant
to the current conversation topic.
Note that the recall system operates autonomously
and it may not always retrieve the most suitable KB.
If the user is asking about a topic
that doesn't seem to align with the provided KB,
inform them of the memory pulled
and request them to specify their query
or share more details.
This can assist the autonomous system
in retrieving the correct memory
in the subsequent interaction.
So basically that's instructing it
to do the same thing that a human will do
if I say like, hey, Bill, do you remember that time
that like I accidentally shot you in the face
with a Roman candle
because that's something that would happen in the South?
And Bob would be like, you know,
I don't actually remember that.
And I'm like, oh, well, you woke up in the hospital.
Oh yeah, I remember that, right?
So we prime each other's memory
and human prompting is not that different from AI prompting.
Remember that the clarity of your responses
and the relevance of your information recall
are crucial to delivering an optimal user experience.
Please ask any clarifying questions
or provide any input further for refinement if necessary.
So this system message,
I actually got help from chat GPT
to create a really compelling system message.
And one thing that I recommend that people do
is actually use chat GPT to work on prompting.
So this is, you could call this meta prompting
where you use the thing to prompt the thing.
And the reason that this works really well as one,
chat GPT is more articulate than most humans,
including myself when used correctly.
But another thing is one thing that I noticed
is that chat GPT tends to write
in a way that it will understand.
And so if you say, if you give it some context,
like this is what I'm trying to do,
here's my current prompt, here's what's weak about it.
Can you make it better?
And then you tell it like, ask me some questions
if you have any.
But no, I see what you're trying to do.
Let me write better instructions for you.
So instruction writing for anyone who's like a teacher
or technical writer or whatever,
instruction writing is a very, very particular skill set
and chat GPT is really good at it.
So this is the default system message,
which is then populated with the user profile
and the most relevant KB article.
So now that we're up to there,
we enter into the infinite loop,
which is just get the user text,
save it to the user log or the chat logs.
So the chat logs are all saved out here.
It's just plain text and the file name has the timestamp
in it as well as the speaker.
So user chat bot, user chat bot, so on and so forth.
So you got the raw logs there
just in case anything goes wrong.
And then I've also got DB logs,
which we'll get to in just a second.
So then what we do is we take the quote main scratch pad,
which is just the last five messages,
both for the user and for the chat bot.
And this is what we use as the context of like working memory.
And so then we use this main scratch pad,
which is the last five messages.
We use it to search for the top most relevant KB article.
And in my case, I still only have one KB article.
So we'll see how it gets to,
and I'll go through the logic
of how it builds KB articles in just a minute.
So basically it just says,
okay, here's the most recent thing.
Find the KB article that is most relevant
to the most recent bits of conversation.
And then it'll pull that,
and it's again, super straightforward.
All you have to do is pass the text to it,
and it will automatically embed it for you.
And then I said, just give me the one most recent.
Once we have larger context windows,
or maybe if we decide that recent chat history
doesn't need to be as big,
like let's say we wanna trim this down to like 3000 tokens,
and we decide that actually having more KB articles
is more important, we can absolutely do that.
And what you would do then is just change the end results
to let, let's say, give me the four most relevant KB articles
instead of the one most relevant.
That will allow it to have a more sophisticated
working memory.
Yeah, so, but right now we're just doing one.
And so then what we do is we repopulate
that system default message with the profile
and the KB article.
And so that's right here.
So that gets populated there.
And then, let's see,
it looks like I accidentally changed something.
So let me go ahead and show you my user profile.
I don't mind sharing this
because I've already told you everything.
I'm pretty much an open book.
So the format for this is what I call
a labeled list.
And so I realized back in GPT-3
that GPT handles labeled lists very, very well.
So that's where you use a hyphenated list, bullet list.
It understands that intrinsically.
And then you label the information, right?
So it's just a hash table.
If you're into computer science,
this is called a hash table or a dictionary
where it's you label the kind of,
you have a parameter and then you label the parameter, right?
So the data metadata.
So name, David Shapiro, y'all know that.
Profession, AI and cognitive architectures,
y'all know that.
Interests, it's got a whole bunch of interests.
And oh, by the way,
this was all distilled from other conversations.
Beliefs, plans,
and this is of course gonna get updated over time.
So for instance,
during some of the conversations that I just showed you
with this brand new chatbot,
it added this.
When I told it, this is what I'm gonna do.
It said, okay, I'm gonna,
I think that that's relevant
to what you're gonna be doing in the future.
So let me just jot that down on my scratch pad for you.
Preferences, so I manually added avoid superfluous words
overly for both responses.
And then you know how it says,
as an AI model, I don't have personal opinions.
I'm like, I know, I don't care.
So I said, please interpret personal input
as critical evaluation and valuable feedback.
I said it a little bit more explicitly than that,
but the point is, is that I told it that
in natural language, I was down here and I said,
I know you're an AI and have no personal opinions,
but when I ask for them, this is what I mean.
And so when I did that, it actually recorded that
automatically because after every conversation,
it checks the user profile.
We need to find a way to speed this up
because as you saw from the user interface,
it's not the best.
If I had more time, mental energy and patience,
I would separate this out as a thread,
as a separate threading thing that can be done,
or even separate it out as a separate API.
One of y'all can do that, submit a pull request
on the public repo.
And then health, so it added this entirely on its own
because I said, hey, I woke up at like 2.30 in the morning
because I had to work on this.
And then I said, let's talk about that.
And so it decided that that was a critical piece
of information to add to my user profile.
So that all gets populated here.
And then the logs are all stored here.
So you got the API logs, which will track all of that.
Everything, so I use chat GPT API for everything
just because that's the only way to get to GPT-4,
which is the most powerful.
Let's see.
So then we update the system message every time.
So it says, okay, whatever you said,
update the system message,
then we go ahead and generate a response first
because the user profile is not gonna change
all that much or all that often,
so we can basically assume that it'll be usable.
And then the KB articles also,
I figured it would actually be better
to update the KB articles after you have the user input
and then the machine output
because if you ask chat GPT for important information
or it solves a problem for you,
you actually wanna capture that.
So we go ahead and generate the response
and append that to everything.
We go ahead and log it out.
Then we update the user scratch pad again.
Actually, why did I do this?
Oh, no, this is the first time we did it.
Okay, sorry, I apologize.
So then we update the user scratch pad,
which the user scratch pad
is only the last few user messages.
And the reason for that is because we want to exclude
chat GPT's response
because we don't want it to get confused
about things that it has said about you
or inferred or whatever.
We only want to record your user profile
from explicitly what you say.
So I just captured the last three messages that you've sent
and then it does a stare and compare
basically where it says, okay,
based on this most recent chat message,
is there any,
one, is there any relevant user information?
And if so, go ahead and update it.
So let me show you how it updates that.
So system update user profile.
So this is a user profile document updater chat bot.
This is the system message.
Your role is to manage and update a UPD and chat bot,
the chat GPT came up with this idea on its own.
It created the UPD definition.
Your primary responsibility is to parse updates
supplied by the user, meticulously analyze them.
It could also extend elements such as user preferences,
significant life events and deeply held beliefs.
Please refrain from incorporating non-essential data
or unrelated topics.
The result of your efforts should exclusively be
an updated UPD.
If the user's update doesn't contribute
any new or significant information,
your output should mirror the current UPD
as indicated below.
However, if you discover any relevant new information,
your output should feature an updated UPD
that assimilates these modifications.
So basically it's an absurd, right?
Or if there's no differences, just keep it the same,
otherwise update it.
You must prioritize brevity and clarity in your output,
combining condensed information when appropriate
to ensure succinctness and improve comprehension.
Totally rewrite or restructure UPD as necessary,
adhering to the list format.
Your response should not include explanatory text
or context, because you know how sometimes chat GPT
will say, this is your new, you know, blah, blah, blah.
So in this case, I have it very reliably
just spit out the user profile.
Oh, and then another thing is that
because we're working with a limited window,
I say the UPD should not exceed approximately 1,000 words.
When revising the UPD, give precedence
to the most significant and relevant information,
extraneous or less impactful information
should be omitted, et cetera, et cetera.
So I give it the current word count
and then the current UPD.
So that way it kind of knows, because chat GPT,
especially GPT-4 is better at counting words,
but just giving it the explicit number makes it easier,
right?
Yeah, so that's my current user profile.
So now let's dive back in here.
The hard part was updating the knowledge base.
So if this is your first run,
the collection count is gonna be zero.
And so then basically you just instantiate the whole thing.
So we take the most recent chat logs,
the main scratch pad and start a new KB article.
Now, if the collection count is not zero,
which is gonna be most of the time once you get started,
what you do is you basically do the same thing
where you say, okay, based on the most recent conversation,
give me the most relevant document,
which I probably could compress this
and just use the same information here.
Because this is the same,
this is, well, generally find the same thing.
Actually, no, that's not necessarily true
because we've updated the main scratch pad.
So scratch that.
So if the new user input and chat GPT output
connects to a different KB article,
let's go ahead and get that document and that document ID.
And what we'll do is we'll go ahead
and use system update existing KB article.
So this is a system instruction
where it basically says all the same stuff,
here's the current KB article,
and then the user will now provide you
with the new information to evaluate.
And so that is gonna be here
where you supply it the current KB article
that it found as well as the scratch pad.
And so it's like, okay, cool,
now let's do the same thing that we did with the user profile,
which is merge that information.
If there's nothing new that's relevant,
leave it alone, but if there is, go ahead and update it.
And so then it saves all this out to the DB logs.
And so if you go to DB logs out here,
you'll see a whole bunch of update statements.
So it says update documented, it gives you the UUID,
and this is the final output.
Actually, probably what I should do is modify this
so it gives you the original,
the original, the new information,
and then the final output.
So I'll add that as a to-do item, actually.
Let's see, to-do, save more info in DB logs,
probably as YAML file,
original article, new info, and then final article.
So yeah, that's something that I'll do.
Now, that being said, one of the biggest problems
that we have always had,
so this is the cream of the crop.
This is the triple crown right here.
The biggest problem that everyone has always had
with long-term chatbot memory
is how the heck do you keep track of memories?
How the heck do you keep track of different types of memories?
Like some people have internal thoughts
versus external thoughts and episodic memories
and this, that, and the other.
And you can certainly try and tag and categorize memories
with different context, right?
With metadata, and I certainly recommend that,
especially once your cognitive architectures
get more sophisticated, right?
If you do have an out-of-band like thought,
like internal private thoughts,
definitely keep that separate.
If you have external sensory information,
definitely keep that separate.
But what I'm working on here,
rather than just being a way to focus on episodic memory,
which that's what REMO was my previous attempt,
this is a way to accumulate declarative information.
And so declarative information is like a statement of fact,
right?
That's why it's called a KB article.
So rather than just a timeline,
rather than just a log,
keeping track of everything in chronological order,
the idea here is to connect new information to a KB article.
So there's no reason that you couldn't do both as well,
right, because this is how human memory works.
Human memory is associative, but it's also temporal.
Now, if the KB article gets too large,
if you added information,
and now it's more than a thousand words,
then I have another system prompt,
which you can check them all out here.
So there's system instantiate new KB,
system reflective journaling,
I just showed you what that was, system split KB.
So that's this one.
But update user profile, update KB article,
new KB article, reflective journaling and split KB.
So these are the operations.
These are the cognitive operations,
the cognitive memory operations that it's gonna be doing.
And so then basically what it does is say,
hey, we're gonna give you a long KB article,
split it into two, into two equal parts.
And so the idea here is that over time,
as your KB article gets bigger,
it'll branch and metastasize naturally.
And so you could then add a lot of additional metadata
to this, such as like access rate or related articles
or parent articles or previous articles,
which means that you can naturally evolve
a knowledge graph of your knowledge base over time.
You can also do this out of band,
just by doing semantic similarity and entity links and stuff.
But it would be really cool
to have a more sophisticated version of this
that allows it to kind of follow that branching tree
over time.
So there you have it.
That's kind of the whole thing.
So that's the chat.
And all this is just real basic, just housekeeping stuff.
And then at the end of every instance,
it does ChromaClient persist.
So now let me show you,
I included a second Python script.
So it's just ChromaDB peak,
which uses the ChromaDB peak function here.
Let me just show you that script real quick.
ChromaDB peak.
So same stuff, you instantiate the client,
you connect to it.
It tells you how many entries,
and then it will show you the top 10 entries.
And so in my case, I should only have one entry.
Let's see, so let's go up to the top.
Yep, KB presently has one entries,
here below the top 10 entries.
And so here you can see
that it's actually got several topics,
because the way that it works
is that it searches for the top one most relevant KB articles.
And so that's always gonna return the first one.
And the first one is not yet long enough
to justify splitting up.
But whatever I end up talking about,
I'll keep talking with the thing,
and eventually it'll split it up.
So in this case, it looks like
it'll probably talk about AI alignment.
And then it's gonna also talk about my obsession
with artificial intelligence and work-life balance.
Because those are kinda like two centroid in this.
So let me just go ahead and actually show you
how this will ultimately work.
So if we go to API logs, it should be the last one.
Yes, here we go.
So if I plug this in, let's go here.
So that's the message that I'm gonna want it.
And then let's grab the split, the split message.
So you'll see what I mean by how it will ultimately
kinda metastasize.
Zoom in a little bit.
All right, we're using GPT-4, temperature zero,
maximum length, a thousand.
All right, so basically what it's gonna do is,
the end says the user will now provide you
with the KB article to split.
So I submit it, and now it's gonna look at this,
and it's gonna say article one, and then article two.
So let's see what it ultimately does.
And you can see how slow it is.
So this is why ultimately you're gonna wanna do this
out of band as a threaded process or do it periodically,
maybe break it up and do it when the user's offline
or whatever, but you see how each article now
is much more specific.
And so then once you go into each of these articles
in the future, identifying factors
and seeking professional help if necessary, yeah.
And so basically it'll allow the articles
to metastasize over time.
Now that being said, if no new information
is added to an article, it won't update it.
It's that simple.
Now that being said, there will probably be a need
to do some KB article grooming over time,
but the idea is that the KB will only grow
as much as it needs to and no more, no less,
and it will only grow based on the things
that you have talked about, and it will record it
in these very succinct, concise articles.
So then what happens is that it splits these two up,
and then the final thing that the chatbot does
is it will do an update for the first one
and then add the second one.
So it's that simple.
And then when you do an update, as long as you don't,
if you don't specify the embedding,
it'll automatically recalculate the embedding,
and then you're good to go.
So I haven't quite got here yet, so it might break,
but I think this kinda, yeah, I think that's about it.
So like I said, it's over here.
ChromaDB, public, chatbot should be all set.
Yeah, all right, cool.

CEO of OpenAI, he was interviewed about the worst case scenario. This stuff goes badly wrong.
And he had this line where he said, worst case scenarios lights out for humanity. And that was
a statement that Sam Ortonman, who's the CEO of OpenAI, the developers of ChatGPT and GPT4,
made about that existential risk. Now, I don't think he's saying that about next year.
But the real question is, if we build an incredibly powerful intelligence system before
we figure out how to make it safe, I think we should have relatively low confidence it's going
to go well for humanity. Artificial intelligence has been a topic of political conversation
for quite some time. I should know. I partly wrote a book about it. It's called Fully Automated
Luxury Communism. Though I must say, when I wrote that book and when it was published,
many people were skeptical about the rise of the robots and how technological change would
disrupt the economic status quo. Sure, they would say, Aaron, this might be coming in the
2040s or 50s or 60s or maybe when we're all dead in the 2100s. But this is not a problem
for right now here in the 2020s. Yet with the development of ChatGPTO the last 12 months,
people are finally starting to have that conversation. Maybe artificial intelligence
is far more developed than we realize. And with exponential improvements, perhaps it's getting
out of control. Ian Hogarth is a founder and investor in technology companies. He wrote a
brilliant article in the FT Weekend Magazine talking about all of these issues and how,
in fact, we may need political regulation to catch up with technology. Ian, welcome to Downstream.
Thanks for having me. It's our pleasure. We're going to talk about some really big issues which
a lot of our audience may not be that acquainted with artificial intelligence, machine learning,
et cetera. Why do you have authority on these topics and why should they listen to you over the next
hour? I'm not sure I have that much authority. I can tell you my background. I studied machine
learning at university. I did a masters, started off with making a robot and then after that made
a computer vision system. So that's systems where you basically teach a machine to see in some way,
recognize patterns, visual patterns. And the system that I built was one that could replicate
some of the job that a radiologist does when looking at cancer biopsy images. So I sort of
started out with, I suppose, a very engineer's mindset thinking about this. I then built a
software business as a founder and a CEO for a number of years. And then I've kind of been
involved in this since then as really an investor and I guess an academic. So as an investor,
invested at about 50 companies applying AI to different fields, including some of the larger
companies in the field like Anthropic, which is the second most funded AI startup in the world,
and Helsing, which is the leading AI defense company. And then outside of that, I've really been
trying to expand the level of public awareness of what is happening in AI, primarily by writing a
report called the State of AI Report, which I've written for the last five years. It's one of the
most widely read annual reports on everything happening in machine learning over the course of
a year. So I've been sort of trying to just expand the kind of the quality of public information
out there for policymakers or for citizens who are interested to find out more.
That's quite an authority. I mean, I like the humility at the start, but I think you're going
to know what you're talking about. And for people who aren't quite clear about your writing,
I mean, I suppose a quick stop for them would be this recent article you wrote in the FT weekend
magazine. Can you just briefly go over some of the issues you discussed in it?
Yeah, so I wrote that article maybe a month ago now. And the core idea is that there are a small
number of incredibly well-funded private companies, primarily in London and San Francisco,
that are kind of locked into a race, where they're all racing as fast as possible to build
what they describe as AGI. And AGI is basically a kind of godlike AI system that is capable of
doing almost anything a human can do and more. And I felt like that race is now getting quite
out of control. And we need to slow it down. And so I wrote that essay to really just try to
shine a light on some of the things that I learn about as part of my work,
but that are maybe not public domain. So I spend time talking to the people running these
organizations. And I just sort of could see that the level of concern behind closed doors had really
ratcheted up. And it felt like there was a big disconnect between the kind of public discourse
and what people were saying in private. So I wrote the essay really just to try to
close that gap a little bit. That's so interesting. So we're talking really about a machine which is
capable of augmenting its own intelligence. And very quickly, you get a super intelligence,
so to speak, an intelligence that we can't really fathom as human beings. You said that the fears
and the concerns around that had ratcheted up. Over what time frame are we talking here?
So I suppose maybe it's worth zooming out and just talking about progress in the field in general.
So if you sort of look at AI systems over the last decade, they've quite predictably gotten bigger.
And so what we've done is every year, we've been kind of increasing the amount of computing resources
would give the largest AI models. And we've also been giving those systems more data to train on.
And so that's been actually a very consistent exponential curve that's been running now for
over a decade. And there've really been a couple of big kind of moments in time. The first was the
founding of DeepMind, which really just brought a huge amount of ambition and energy to this challenge
of like making these even bigger and more powerful systems. And the second I would say was OpenAI,
which introduced a competitor DeepMind that suddenly meant there was a race. And those
organizations have been racing against each other now for best part of their entire founding history.
And if you look at that, we've gone from kind of, you know, feeding these systems, you know,
some tens of thousands, you know, millions of images to feeding these systems most of the
internet. And we've increased the amount of computing resource we give these kind of most
powerful AI models by a factor of 100 million in just a decade. And so there's been this very,
very, you know, continuous progress in the field. But as with any exponential, it's really only when
you get to the steeper of the curve, you start feeling it. And I think the last couple of years
are kind of busy, where the curve has just suddenly felt a lot steeper and things have
been changing weekly or daily rather than yearly.
Wow, that is really extraordinary. So the word exponential of people who aren't necessarily
familiar with it, my goodness, virtually everybody is in 2023. But this was broadly
integrated within discussions around computer science by Gordon Moore and Moore's law. And
this idea that broadly speaking, computational power would, there's a bunch of ways of sort of
discussing it. But the same amount of power would basically halve and cost every 18 months to two
years. And that has happened for a long time, it's kind of decelerating, it's happened for a very
long time. You were saying with AI, there's a bunch of variables. So it's not just the computational
power, it's also the data that it's feeding on. And the two of these is important, right?
Correct. And so, for your listeners, an exponential, thinking about it simply is,
for example, a system that doubles every year. And so if you play that out over a number of
years, you get a very steep curve, because some property of the system is allowing it to kind
of grow in that way. And the classic we saw was with COVID. But we as humans, I think, are just
really poor at thinking about exponentials. They're not intuitive to us. And so we saw it with
COVID. It's kind of January, people start paying a bit of attention, February things get more serious,
March, some people really start to get, and then we're suddenly locked down. And that's kind of the
nature, I think, of a system where you're having a doubling effect over some period of time. And
that's what's been happening in AI for a decade is just we're now at the kind of February 2020
moment in AI where things are just going super, super quickly.
That's such a powerful analogy. Obviously, you can't go into the nature of private conversations
you've had with people. But when people are putting a date on it, what are they saying
with regards to an AI then? And like I said, it's hard to predict by virtue of exponential growth,
but are they saying in the 2020s, next year? I mean, what's the broad time frame here?
Well, so the first thing I'd say is that the people leading these companies have been thinking
about this problem for a long time. Some of them have been kind of a good example of a Shane leg.
Someone I admire greatly, he's a brilliant computer scientist. He runs DeepMind's AI
alignment efforts, which we'll probably talk about in a bit, what that is as an area.
But Shane, he did his PhD on sort of a computational basis for machine super
intelligence, has made many sort of quite sophisticated predictions over the years around
what it would exactly take in terms of the amount of computing resource and the amount of data before
you would actually get a super intelligent machine that was kind of an artificial general
intelligence. And so the people in this field have been thinking about it a long time,
and I pay the most attention to the people who have been consistently making good predictions
to me behind closed doors about what will happen. And the thing that I've noticed is
it used to be the case that people would say stuff like, you know,
it's possible we might get an AGI, a super intelligent machine in the next 30 years or the
next 20 years, but everyone I think thought the idea of kind of something happening next year
was kind of ridiculous. And now I think if you ask people, you know, let's say, for example,
you know, there was a select committee and the various leaders of these labs, the technical
leaders were asked under oath, what's their probability that we get a super intelligence
next year, it's not going to be 0%. Whereas I think it would have been before, whereas now it
might be, I don't know, 5%. And so you have this kind of shift where I think everybody is starting
to sort of say, actually, we might be closer than we realized, we should start taking it
seriously the possibility that we might be very close. So if a private enterprise develops an AGI
and artificial general intelligence, what happens next, do you think?
I have no idea. I think that the AI alignment community would basically say, you know,
most likely outcome is we're all dead. The CEO of OpenAI, he had this kind of,
it was, you know, interviewed about the worst case scenario, this stuff goes badly wrong.
And he had this line, we said, you know, worst case scenarios, lights out for humanity. And that
was a statement that Sam Ortman, who's the CEO of OpenAI, the developers of ChatGPT and GPT-4,
made about that kind of existential risk. Now, I don't think he's saying that about next year.
But the real question is, if we build an incredibly powerful intelligence system before we figured
out how to make it safe, I think we should have relatively low confidence it's going to go well
for humanity. And there's a kind of very, you know, I guess, sophisticated intellectual argument
about how to think about that. And that's the sort of thing that someone like Eliezer Yudikowski
would write about, where he'll talk through the sort of exact mechanisms, but why a system that's
much more intelligent than humans treats us, you know, treats us badly, you know, primarily by
accident. But I think actually the kind of common sense way of thinking about it makes more sense,
which just says, you know, humans have kind of changed the environment on earth very significantly
as a result of our intelligence relative to other species. And that's had, you know,
significant consequences for some species, and for the biosphere in general. And I think we
should sort of just common sense tells you that something similar might happen if we invent something
more intelligent than us.
Wouldn't the counter argument be, I suppose, that though that we've learned that over time,
we are dependent on the biosphere for our own systems, political, social, economic to sustain.
I suppose as they're not an optimistic account of an AGI, and I think obviously there's a great
deal of thought put behind being skeptical about this stuff. And I'm also skeptical.
But is there not also a world where you have an AGI, which is in some way benevolent,
capable of very long term planning, capable of all some problem solving on a scale that we
can't really comprehend? Yeah. And just to be really clear, I think that is, you know,
that is the happy path we're now on. I think there's basically three paths. There's the,
we have a moratorium that just completely shuts this down. And that could be like,
you know, some of the other moratorium we've had around, you know, genetic engineering,
for example, you know, eugenics. There could be a another path where we develop this kind of
hastily and not thoughtfully and kind of wipe ourselves out in the process. And there's this
third path, which is the one I think we should really all be oriented on, which is we build
systems that massively expand the amount of kind of wisdom in the universe. And we cure diseases
that can't be cured today. We, you know, we have enormous technological abundance. And so there
certainly is an approach where we build EGI and it goes incredibly well for us as a species.
The question is, are we on track to do that or not the way we're doing it today,
with a small number of private companies racing to do it as quickly as possible?
So with regards to chat GPT-4, which was released in March by OpenAI, which is aligned with Microsoft,
how big a jump was that from chat GPT-3, which was obviously the previous version?
So chat GPT when it came out was was arguably a user for interface on top of a very powerful
language model that already existed. And so you already had these amazing language models that
OpenAI had trained and anthropic and Google had trained to do very powerful things. And
what OpenAI did with chat GPT is they basically created a way to interact with it that let that
suddenly opened it up to a lot more people. And so in many ways, it wasn't a sort of research
technological breakthrough. It was actually a user interface breakthrough and said like, here's a
way to use this that suddenly feels a lot more organic and natural to an end user, a consumer
playing around with GPT. The thing they released after that, which was called GPT-4, was a significant
update because the chat GPT was based on what they were calling it GPT-3.5. And that was a big
jump in the underlying model. So for example, GPT-3.5, when you sort of tried to get it to do
the bar exam scored in the bottom 10% of results, whereas GPT-4 scored in the top 10% of results.
So in a single generation of models, you had this massive leap in capabilities where it went from
basically not really being able to be a lawyer to being able to be a lawyer. And so GPT-4 was a
massive step forward for language models in general. And like, you know, one of the most
impressive technological artifacts humanity has ever created. And what was the basis of that jump
from 3.5 to 4? Was it purely because there was more data being fed, or there's been a sudden
boost in computational power? So it's a great question. And we don't really know. So OpenAI
hasn't really explained to us what data they trained it on, the amount of computing resources
they use for it, any algorithmic breakthroughs they made, they have kind of gone from being
open AI, kind of being very open with their research to being much more close with their
research. And they have done it, I believe, for good reason, which is they sort of don't want
to accelerate things any more than they have to by suddenly making it more possible to replicate
this and kind of cause a huge amount of proliferation of this technology.
This is so interesting. So could it be possible then that OpenAI are further down the road to
AGI than we really discuss, we really talk about? But the incentives aren't really there to be quite
public about it, right? The incentives are there to actually be quite private and discreet and not
really convey how close we are to a really transformational technology.
Yeah, the incentives are really challenging. So I'll actually give you a quite a concrete example
that I think brings the race to life. So I'm one of the first investors in this company,
Anthropic. And Anthropic was founded by a group of people who left OpenAI and set up a new AI
startup. And it was the people who did it, who founded it, were the people who led the research
on GPT2 and then GPT3, so the precursors to these large language models. So they really are the key
people from OpenAI who did a lot of the large language models sort of early work. And their new
company is very much oriented around a greater emphasis on safety. So they have something like
50% of their headcount in 2021 was dedicated to alignment research and safety, which is higher
than any of the other labs like DeepMind or OpenAI. And they had a product like ChatGPT
about six months prior to OpenAI releasing ChatGPT. And if they'd released it, it would have
suddenly put Anthropic on the map in a big way. They would have attracted so much more capital,
more attention, and they held it back because they felt like it would just accelerate this race
in a counterproductive way. And so if you think about the incentives, it's working against them
as a capitalist entity to just hold back stuff, to release less, to create less hype. It's quite
challenging. And so recently, they just actually made an announcement maybe two weeks ago where
they expanded the context window for the largest language models to 100,000 tokens. And just to
explain what that means, it's basically the size of the document that you can feed into a language
model and have it work with for you. And so it massively changes what you can do. You can feed
like a huge legal document or a massive code base into a GPT-4 like model and get a much more
sophisticated response as a result of that. So it's a huge technological breakthrough. It lit
the AI research community and start-up community on fire when they did it. And that
ultimately attracted more attention to them, probably more capital over time. And so there's
these perverse incentives where if you're a startup, you're incentivized to get as much
capital and attention as possible so you can go faster. But actually, if everyone does that,
then we burn the time we have to make this stuff safe. So it's a very challenging coordination
problem where the incentives encourage racing rather than careful, slowed-down coordination.
I'm very happy you said that. There's a great quote from Jeffrey Hinton who recently resigned
from Google. And he said in an event, I think Google was very responsible to begin with,
and this is deep mind. But once OpenAI had built similar things using money from Microsoft and
Microsoft decided to put it out there, then Google didn't have much choice. If you're going to live
in a capitalist system, you can't stop Google competing with Microsoft. So it almost sounds to
me like one of the most powerful things about the market system competition, which can lead to
incredible efficiencies, has upside as well as downside. But particularly with regards to AI,
this sounds almost like you couldn't build a worse system to potentially accelerate development
while also not really addressing things which could go very badly wrong.
Yeah, I think that's, it's very challenging because there are areas of AI research where we
I think actually capitalist competition is extremely good. So for example, there's 10 start-ups
and they're all competing to make AI systems that can take in cancer biopsy images, analyze them
really well and improve the lives of patients. I'm not worried about that having a negative
consequence on the world. And I think actually the price signals the competition will be really
good and it'll ultimately give us all cheaper healthcare, more innovation in the market. So
the area of, I guess, narrow AI, where AI is just doing a single task, quite specified,
and without these existential considerations, I think this kind of capitalist competition
can be great. The problem is if we're trying to apply the same logic to the part of the problem
where we're trying to build something smarter than us, there's basically a new species. And that,
I think the kind of capitalist market dynamic is not helpful. And I think that, you know,
what's great is that the leaders of these organizations, I think in their own ways,
they all kind of have done important things to acknowledge this. So, you know, Demis,
the CEO of DeepMind is someone I really admire, you know, he's really oriented a lot of DeepMind's
efforts towards expanding the scientific commons, you know, things like AlphaFol,
which they released for free, and they've really expanded the amount of, that's not a very sort
of capitalist maneuver to basically produce this massive breakthrough and then kind of give it away.
But I think it hints at how he thinks the economic gains from this should be distributed.
Sam Altman, you know, the CEO of OpenAI, he's talked about how he wanted to have the government
fund OpenAI early on. So he didn't want to raise money from private investors, just he
didn't get the support from the government to do that. He's also, he and his team have explicitly
said that if the race becomes too dangerous in their charter, they've said, we will merge and
assist another player to change the coordination dynamics for the better. And Anthropik have got
a very, you know, very, very thoughtful set of statements they've made about how they want to
ultimately be much more cautious as we get closer to this kind of godlike AI. So I think we have
actually leadership that is trying hard to do this. It just doesn't really work within the
current economic system. And so for example, I, you know, I fought an antitrust case against
Ticketmaster in the United States as part of the startup that I built, Songkick. And so, you know,
I'm very supportive of the kind of, you know, the work that Lena Khan, or the CMA have been doing to
try to sort of decrease concentration in certain markets. But I think in this case, actually
sort of antitrust is actually quite harmful because it almost creates, it makes it harder
to coordinate. There's less of a safe harbor. And so I think the main thing we need to do is really
view these as quite different regimes. There's this kind of narrow AI regime, and there's this
trying to build a god regime. And that bit needs a different regulatory approach to that bit.
There's a quote from Marx. It's in the Communist Manifesto. I actually, I was reading this the
other day. That's why I come on this channel, just to hear about Marx. This is, no, this is
Ian, this is terrifying. Now bear in mind, he wrote this 170 years ago. Marx or our Capital Society
had quote, conjured up such gigantic means of production in exchange that it was akin to a
sorcerer, quote, no longer able to control the powers of the Netherworld, whom he has called
up by his spells. I mean, wow, that sounds like capitalist competition, creating something completely
beyond motivation and intentionality, and over which has very little oversight. Now, of course,
he's talking about, you know, steam power and mills in Manchester and Brussels and
Frankfurt in the mid 19th century. But if anything, those words sound more appropriate
for AGI in the 21st century. Yeah, I think that one way to sort of frame the capital and kind of
labor, the sort of relative power of those two groups is just looking at the size of some of
these organizations. So an open AI is a, I think, privately valued at 30 billion US dollars now,
you know, significantly changing the world, hundreds of millions of people now using their
products. And I think, you know, at the time they released chat, it's probably a few hundred people
in terms of the size of the organization and the labor that's directly, you know, benefiting from
kind of the work that's being done there. And I think that again, you know, the leaders of these
companies are actually thinking hard about this. So Sam Altman, you know, a couple of things I
admire that he's done, the first is he was running a very large UBI study in Oakland. And so he was,
you know, that was, you know, maybe five, 10 years ago, he was thinking hard about this question of
how do you kind of, how do you, if you do have further and further returns to capital, what do
you do about kind of that not, that not just massively in increasing inequality. And, you know,
he's done this thing called Worldcoin, which is kind of a much more extreme version of that,
which is a machine that scans your retina, produces a unique ID for you that would then
let you be part of a global UBI scheme. And so, wow. And so there are, you know, they,
these kind of people are thinking about these, these kind of the way in which this may fundamentally
disrupt some of the ways in that the social contract we currently have that allows capitalism to
sort of just continue as it does. But I suppose the concern is you can't be worried, you can't
be dependent rather on prevalence and the foresight of certain individuals. You know,
there was a great quote a couple of years ago from Mark Cuban. And he was saying,
I wouldn't teach my kid to be an accountant. We now know that it was probably quite a good move,
because, you know, that's one of the industries, which is very much prone to automation
with machine learning. I'd rather they learn philosophy because it will give them insights
that are harder to automate, so to speak. And I thought that was interesting. Now, alongside
that, he said the world's first trillionaire will be the person who can master widespread
commercial applications of AI. And that's the prize on offer, isn't it? I mean, that's the prize
on offer. So today we talk about Amazon, which is a trillion dollar company, what used to be,
borderline trillion dollar company, Amazon, Microsoft, those kinds of big players. But the
truth is the commercial entity, which masters an AGI, and we don't all die, will put all those guys
in the dust, won't they? So there are massive incentives for people to pursue this technology
without the kinds of caution and intelligence and thoughtfulness that you've talked about with
regards to somebody like Sam Altman. Yeah. And I think what's challenging in some ways,
we've entered a new phase of this race. So if you look at the leaders of these organizations,
the ones who are kind of really at the forefront of the race, whether it's Demis or Sam or Dario
Anthropic, I would say that most of them are kind of not particularly motivated by money at this
point. They're doing this for some other reason. You know, Sam, you know, recently announced that
he actually has no equity in open AI, so doesn't stand to benefit economically from, you know,
he won't be the first world's first trillionaire, let's put it that way. And so I think that they've
been motivated by other things. And I would say mostly they've been sort of motivated by being
the people to do it, to make this thing come alive and, you know, the consequences of that.
It's kind of a world historic transition that they want to be a big part of.
It's my guess, my hypothesis. But there are now a lot of other people who've kind of suddenly
woken up and just seen dollar bills. And those people are just piling on money. They haven't
really thought about it. They don't have the same sort of reverence that people like Demis
have got for how we should be approaching this moment. And they're actually accelerating the
race, but without that same, you know, intrinsic motivation for the thing they're trying to do
and much more of a kind of, you know, much more of a desire just to make money.
So for people out there who are perhaps skeptical of what I'm saying here about a trillionaire,
if you told a 14, 15 year old Ian or a 14, 15 year old Aaron about the same age,
that one day there'll be somebody worth 250 billion US dollars, 200 billion US dollars,
like Jeff Bezos, we would have thought that's outlandish. Yet he's the guy who starts Amazon,
and that's the company which benefits from network effects, you know, ubiquitous mobile
internet, and basically building the everything store of e-commerce. And I suppose the question is,
could you feasibly see a company like Amazon, which is applying AI to a bunch of industries,
which lay off hundreds of thousands of people, just like Amazon have basically shut down hundreds
of thousands of local businesses? And that's the outcome we get. Do you think that's a plausible
outcome? I think it is possible. And I think the reason for it is that you are, you know,
you first of all have a much more globalized economy where things can spread really quickly,
across borders in a way they couldn't, especially digital products. Secondly, you have,
you know, a lot of these technological, you know, products built on top of prior networks.
So for example, chatGPT is the fastest-growing product ever on the internet, but that's partly
because we've got Twitter and Facebook and Instagram and Google, and all the ways that
information disseminates and spreads faster than it did before. And finally, I think we're starting
to tackle some of these incredibly large markets, like, you know, what SpaceX is doing is basically
tackling, you know, global internet provision through Starlink, right? And so this is a very big
thing. It's not like digging up the street and smashing in loads of fiber. It's something where,
you know, the same satellites can basically provide internet access wherever you are in the world
without that same degree of physical disruption. And so I do think it's possible. I think a good
thought experiment for the kind of company that might be an example of this is the first company
to really build incredibly good domestic robots, right? So, you know, I've got a four-year-old,
when I'm doing chores around the house, I would sort of say to him, you know, one day,
you know, one day, maybe there's going to be a robot doing this because, you know, we didn't
used to have dishwashers. We didn't used to have washing machines. We didn't used to have
tumble dryers. And now we have all those things. We take them for granted. They're in most homes
in the world. What would it be like if we had a robot in our house that just did all the chores
that we currently do as kind of around everything else we're doing in our lives, but also that
robot could be your plumber when things break, your electrician, your handy person, go to shops
for you. So when you think about kind of that sort of disruption, could it be a, you know,
10 trillion dollar company where the person founding it is a trillion error, like quite
possibly in my view? A quick sort of move away from machine learning software to the kind of
robot that you're talking about. How far away do you think that is, by the way, as a sort of
household appliance that, you know, middle-class people in the UK or US would be able to buy?
Well, so I think it really is fundamentally the same curve we're talking about with AI. So robots
basically are machine learning. They're just embodied machine learning where it has a physical
presence it's using as well. And we've made, and I've invested in lots of robotics businesses
over the years, and it's amazing how rapidly they're progressing. We don't actually see the
number of robots that we're using because they're mostly industrial settings. They're like industrial
cleaning robots or industrial manufacturing robots or agricultural robots. And so it's happening.
And I think that we will start in the next few years to see robots in much more consumer settings,
really starting with the rollout of self-driving cars. And so I think we will see humanoid robots
that significantly enhance our lives in a domestic setting within the next decade.
The next decade? And a lot of that will be knock-on effects from the work we're doing in making
these powerful AI systems. So to bring this back to large language models and the sort of things
that DeepMind and OpenAI are working on, there was a paper called the tool former paper that came
out earlier this year. And it essentially shows that large language models are actually quite good
at using tools, which is quite counterintuitive. But to break it down, you take an incredibly large
computer, you feed it an enormous amount of text and imagery, and basically get it to get really
good at predicting what it's going to see. If you take that same sort of blob of intelligence that's
just become smart in some important way, and you give it access to a tool, it is actually quite
good at using a tool, whether it's a digital tool or a physical tool. And so large language models
actually will have a knock-on effect on real-world robotics. They're not sort of separate industry
as a tool. They're very, very related. I like it. And how far are we from machines basically
being able to do anything that a human does? So in my mind, I think sort of 25 to 30 years
where we have software hardware, which can basically do 95% of the jobs that humans do,
at the moment, the big obstacle to that obviously is the fine motor coordination dexterity that
things like cleaning or construction relies upon. So it's easier to automate legal services
or accountancy than it is, like you say, plumbing. Great example. How far are we from being able to
solve the problem of the fine motor coordination? So I guess we don't really know. That kind of
goes back to this question of how far away are we from AGI? How far away are we from
sort of superintelligent systems? No one knows. All I will say is people have more,
they put more probability on it happening soon rather than later. So Jeffrey Hinton,
who resigned from Google recently, he said, I used to think this was paraphrasing here,
but he said something like, I used to think this was decades away. And now I think it's not inconceivable
it happens in the next five years. And he's the godfather of machine learning, the researcher
that really kicked off this whole field of large neural networks and really the sort of one of
the most important people in the field over the last few decades. And so we don't know,
but I think we now need to sort of start to prepare as if we might be closer than
people have realized. And the public certainly has realized.
I mean, this is a big challenge to our economic system, right? So I mean, I wrote a book about
this called Fully Automated Luxury Communism. And the point is if you get increasingly affordable
technology, which can do anything that a human can do, whether it's with regards to abstract
problem solving or physical capabilities like building a house or cleaning a house or cooking
meal, what that does is clearly depress the price of labor to zero. That's what's going to happen.
That's a neoclassical understanding of how of how supply and demand would work with the cost of
labor of human labor. And yet, if you say that to a lot of people, including politicians, including
quote unquote, smart people in legacy media, in the policy world, they think you're crazy.
And it's interesting for me, seeing the reception of my book, F.T., oh, this is interesting,
very provocative, New York Times and so on. But then in some other places, I won't name the papers,
but they were just like, this is ridiculous. They were sort of mocking it. Where do you think
this asymmetry comes from where some people take these ideas very seriously and then
others just completely disregard them? Is it a lack of information? Do you think it comes from a
place of fear? Because realistically, if this is correct, we're going to have to shake things really
up a lot, aren't we? I think it's a great question. I guess I've got two hypotheses. The first goes
back to the thing we were talking about with exponential change. It's just so hard to think
intuitively about exponential progress. And so COVID is sort of fresh in our minds. And I remember
sort of January, I was planning a trip to China with a friend. And actually, our trip took us
through Wuhan. And he's an expert in kind of biology in various ways. And I spent a bunch of
time in China when I was younger studying Mandarin. And so we were talking about this trip and we're
talking about COVID. And we sort of started to see the little inklings that COVID was going to
potentially go pandemic. And I remember both of us were starting to prepare and
get really worried ahead of a lot of the sort of mainstream news. But at the same time,
we're both like, well, maybe it'll be done by the time that our trip comes around, right?
And it's just a perfect example of like, we thought we were being clever, but actually we
seriously still did not really have a good intuition for actually what happens when exponentials
really take off. And I think that there's a bit of that where if you're a politician that's just
dealing with, you know, you know, day to day issues, and you're then confronted with an exponential
change, be it, you know, what we're doing with climate change, or a pandemic, or an accelerating
technology like AI, you just don't really have an intuitive way to navigate that. So I think
that's part of it. I think the other part of it is just exponentials require, I think, a certain
kind of radical thinking, you know, it's sort of like what the I think the UK government did really
brilliantly with Cape Bingham and the Vaccine Task Force. I think it was a great example of kind of
just incredibly bold, proactive leadership on a serious thing, getting, let's get ahead of it,
let's get, let's, let's build that capacity early, let's, let's throw everything at it and prepare
as best as possible, because we know this exponential is coming. And that kind of
political action and leadership is just, I think, like quite hard to do. And I think
Why is it, why is it hard to do? Is it because politicians don't like doing it or because
it's just objectively hard to execute?
I think it's radical, basically. And I think it's a, it's not business as usual, you know,
it's first principles thinking you maybe have to take more risk. And they just haven't been that
many examples of political leaders who've kind of acted like that over the last couple of decades
in response to technological change. And I remember in 2018, I wrote this essay AI nationalism that
really talks about this kind of this fundamental challenge that was coming down the pipe with AI.
And I remember meeting with, with kind of MPs from labor MPs from, you know,
conservatives laying out these ideas, almost everyone just looked at me like I was completely
crazy. Because I was saying things like next time we have a company like DeepMind, we shouldn't
let it be acquired because it's too critical to the future of the UK. And then, you know,
three or four years later, I saw similar politicians basically saying, oh yeah,
now we have to be serious about blocking acquisitions of some, you know, strategically
important UK technology companies. And so I think it's just also discomfort with really
radical ideas. I think there's much more of a comfort in politics with incrementalism. And
that's why often big change happens when you have a crisis.
We'll come back to the politics in a moment. I want to ask you, I want to ask you an almost
existential question. A super intelligent, artificial general intelligence, would it be
conscious? I don't know. I think that there's, I was actually talking about this with
someone who specialised in quantum computing yesterday. And their view is actually that
like conscious consciousness is a kind of quantum mechanical property, a property of quantum
physics. And therefore, it will not be possible to get consciousness on a classical computer.
I don't know if that's true or not. But like their claim was basically, we all sort of need
quantum computers to actually have conscious AIs. I don't know.
What's the argument behind that, by the way? So where does that come from?
It comes from quite a, I guess, an esoteric argument about kind of about exactly how we
interrelate with the metaverse, which I'm not enough of an expert on any of those topics to
really opine. But the way I think about it is, is consciousness and intelligence are probably
kind of somewhat orthogonal. You don't necessarily get, you know, get one with the other. And I don't,
I think it may have been a, the way it may be an artifact of biological life, which is inherently
quantum, rather than necessarily sort of classical computing. So we still could fabricate something
close to consciousness, but it would just require quantum.
Maybe, or maybe it's not possible. We just don't, we, consciousness is something,
as far as I can tell, we don't really understand at all. So we, I think it'd be quite difficult to
project how to think about consciousness when it comes to machines. Like, if I'm understanding it
correctly, when you, when you get, you know, when you have an operation, you go under anesthetic,
you know, the drugs you're being given, essentially just switch off your consciousness.
And we don't really understand like why and how that works. And so it's quite hard
before we really understand consciousness to really make any claims about how
AI is will be conscious. I think that the, but I think we should probably start from the basis
that they might not be conscious, but they might still be incredibly intelligent and capable
of planning in ways that are threatening to humanity.
So we could have an AGI, which is an existential threat to humanity without it being sentient.
Yeah. Yeah. Exactly.
Interesting. And it could be trying to maximize its own utility. It could be trying
to maximize its own interest without really being aware of itself as an entity.
Exactly. Yeah. Exactly. I'm thinking about it kind of like the Sorcerer's Apprentice is probably
the kind of like a really mainstream example of kind of something where you give it some sort of
goal and the optimization of that goal ultimately leads to something that is not what you wanted.
And so that's what the field of alignment is about. It's basically about how do you align
those goals with our goals as a species? What does it mean to the average person? So we're
talking about AI, you know, partly abstract, partly real world. So we're talking about, you
know, the potential of trillionaires. We're talking about consciousness. We're talking
about politicians asleep at the wheel. But what does it mean to somebody out there who's watching
median income, homeowner, mortgage, owner-occupier, if there is an artificial general intelligence
that's created to send the next five to 10 years, how will that impact their lives over the next,
say, 25 to 30 years? So again, with exponentials, saying anything about, you know, 20 years is really
hard. Yeah, let's say it's developed. That's the exponential part. And then let's say there's
almost like a cap on AGI. Like it's not really developed after that for regulatory reasons or
whatever, political reasons. I'm trying to really ask what are the implications for just everyday
people rather than us talking about, you know, like you say, some breakthrough which depends
the printing press, the average person in 16th century Europe doesn't really care about the
printing press, but then the reformation happens and that is a big deal. So I think that like,
I think that synthetic media is a really, really big deal. And I think there is
extremely low popular support for it. So, you know, as an example, you know, you can take the
sort of generative AI systems, the systems that are creative, capable of generating images or text,
and you can use them for harmful purposes. So I'll give you a few examples. The first is,
you can take a snippet of someone speaking, and you can then, you know, effectively synthetically
clone their voice. And that is now being used by people to do kind of fake kidnappings where you
get a snippet of a child's voice, and you basically create a phone call from them calling their parents
saying, mom, I've been kidnapped. If you don't do this, then I'm going to, you know, something
horrible is going to be done to me. And that happened in, I think, Texas about two months ago.
And so that's kind of happening. Another thing that's happening is around synthetic child sexual
abuse material. And so the systems you can use to create a kind of funny image of the pope in
a puffer jacket, you can also use to create, you know, horrible deep faked porn, including
horrible deep faked child sexual abuse material. And that is currently happening. People are using
these systems to do that, particularly the kind of open source ones. And the knock on effect is
then you've got to, you know, the police are suddenly faced with, you know, is this an image
of a child that's actually being harmed or a fake image and having to full of shard their resources
between kind of basically fake crimes and real crimes. And so there's a huge number of malicious
uses that will bubble up from this very powerful technology where you can clone someone's likeness,
someone's image, someone's voice. So I think of that as being like the real structural problem
that we're going to encounter. And I think that they're, you know, I was talking to someone there
who's been doing focus groups around this. And they were just telling me like, you know, basically
the public at large thinks deep fakes should be made illegal already. So there's a kind of, I think,
a lot of stuff bubbling under the surface that when it breaks through is going to really,
it's going to really have actually quite a populist response to it, because it's just,
it seems like something the government should be getting a grip of.
And do you think they will? Is there any sort of, you're in this game? Is there any evidence
that they will? I think they will actually. Yeah. I think it's just, it passes a kind of
common sense test of that that is not a good thing that should be happening. So I think there'll be a
genuine motivation to make deep fakes, deep deep fakes without someone's permission illegal.
And I think lots of politicians will want to do that. But I think the mechanism for doing that,
when you've got significant proliferation of these capabilities into open source is a bit
trickier, right? Because the technology is kind of out of the box. And so figuring out how to get
it back in, like with the dark web, for example, is just challenging. But I think it will be,
I think there'll be broad political support for doing that.
You said the common sense thing, but I mean, if you said to somebody 40 years ago that children
will be able to access high speed broadband pornography, be able to stream stuff, I mean,
I think most people said that obviously should be banned. That's the world we live in.
I think we've lived through a strange time, you know, I think that
I think about social media a lot and how it's sort of remarkable that we basically left this
enormous industry that was so transformational to, to everyday life, to children, to politics,
to the general discourse, to basically self regulate, you know, and I think there was
actually a really, that was well motivated by a desire not to kind of throttle something with
regulation and kind of too much, you know, too early, you know, I think I can kind of,
I can see the logic of things like section 320, I think it's called, which is kind of the sort of
the mechanism whereby a lot of social media companies have not really
have kind of been able to just self regulate. But it feels like we, I think with the benefit
of hindsight, you know, it would have been better for regulators to catch up faster and to sort of
be a bit more assertive about defining, you know, a smarter way forward. And I think Biden talked
about, you know, for example, modifying section 320 recently, and there's been some discussion
about this. But I think with AI, you know, I think that the lawmakers need to move faster,
because back then, you know, these fledgling startups like YouTube, they had certain sorts of,
you know, it's kind of a, you know, innovation was valuable and needed to be sort of encouraged.
But this is a different ball game where you've got $20 billion already invested in just a handful
of companies, you've got, you know, Microsoft aggressively deploying this as fast as they possibly
can. You've got companies like Facebook, open sourcing incredibly powerful models and putting
them out there for anyone to sort of expand the modify. And so I don't think I think we're kind
of in like the same scenario. It's just now this is not about small startups and fledgling
industries. This is about an incredibly powerful tech industry that has just
prefers self-regulation to anything else. Do you think social media was a mistake? Because
obviously you're involved in the sort of the technological side of things and how, you know,
the technological sort of underpinning of global social media, 4G, 5G, mobile internet,
high res screens, all these things. And one outgrowth that was social media and Ben Bratton
has a really interesting read on this. So he says, we built a global real-time communications
computational network, you know, including the kind of exosphere of our planet, which is now
caked with satellites. We've built all of this infrastructure so that we can sell out space
and be permanently distracted. You know, and I think that's an interesting way of looking at it.
And I wonder, we might have something like AI or even an AGI if it's not, you know,
deadly. And we just get more of the same, perhaps.
So my day job is I'm an investor, and I invest through a fund called Plural. And, you know,
it's a European fund focused on accelerating missions that we consider to be of great societal
importance, you know, by funding them as startups. And one of the companies I work with is a company
called Unitary AI. It's one I'm very proud to be kind of working on. It's a startup that uses AI
to understand content and thereby to offer a scalable approach to content moderation
and content safety across the internet, content security. So for example, their AI can detect
some sort of content that should be illegal or some kind of content that is, you know,
causing significant harms and flag it to the platform that's hosting it.
And so they're in many ways kind of like a, you know, like a antibiotic to this kind of this,
the way in which some of the sort of wild west of content dissemination through social media
has kind of played out. So I think there is kind of a, there is a kind of capitalist response
coming, but it's hard. It's much easier to make a new social network than it is to make an AI
company that's trying to actually solve this problem of kind of how do you scalably tackle
the challenges of content? To your question of kind of, you know, if you could kind of go back
in time and stop social media from happening, I wouldn't have done that personally. I think
it's delivered enormous benefits. I think there is some something amazing about so much connectivity,
democratization of media production. I mean, you know, like I, part of the reason I'm sitting here
is because I, you know, I learned about you through Twitter and I follow your thinking
and I find it interesting. And so I think there is something, something really miraculous about how
connected we are now. I just think the problem is we just never really had governments keep up.
And I think it's something, I think it's the really the nature of like sort of
laissez-faire, neoliberal thinking, having just permeated government over a number of decades
where really bold, ambitious projects where the government is a kind of, you know, a real partner
to the private sector and actually drives technological change and thinks about regulation
in a bold way that embraces exponentials has just been missing.
So you think there's neoliberalism, this, because we do live in this really strange moment, right,
of like extraordinary technological possibility, like extraordinary profound liquid biopsies that
can detect early onset cancer, you know, mapping the human genome, you know, just high res scans of
the human brain, just, you know, year on year improving exponentially. And yet we have politicians
who say, sorry, we can't address the housing crisis. Sorry, we can't give you affordable
health care or free health care. There's a weird disconnect there. It seems almost like the better
the technology gets, the more the possibilities, the opportunities, the less capable the state is
in addressing those challenges. So you pin that on neoliberalism.
Well, I think that particular problem you described, like, it's hard to really
know where it started. There's a, there's a kind of an investor I really respect called Matt Clifford.
And he has a kind of a thesis on this, which basically says, if you look throughout history,
there was always a kind of part of part of society that attracted the most talent.
And he's his argument is basically right now, that's the technology industry. And so a lot
of talent has kind of gone out of government or out of, you know, the public sector into
technology, because the opportunity to change things quickly, make money, you'd be the first
trillionaire, whatever, whatever motivates you, right. And so there could be a hollowing out of
sort of the capacity of the state to respond. That's one way of thinking about it. I actually,
I think of it as being a little bit more ideological. So for me, I think we've just not
really had political leadership that sort of sort of said, you know, I'm going to,
we're going to transform this country in a way that really embraces all of this. And it's kind of,
you know, keeps pace with technological change. And I think you've, you've had examples of that.
You know, if you look at Lee Kuan Yew in Singapore, obviously, lots to discuss about
Singapore and their politics, but it's very interesting the way that he basically kind of
was a founder almost of a country that went from third world to first world in 30 years. And some
of the things he did were just very, very bold, ambitious things that ultimately he took a lot
of risks and it delivered for the country and for the citizens. And so I think we've lacked that
level of boldness. And the reason I sort of cite neoliberalism and laissez-faire economics is because
I remember when I was talking to people about that AI nationalism, I was amazing to me that I
would be meeting with conservative politicians and they'd be telling me about British Leyland
as the reason why you can't nationalize DeepMind. Crazy. And it's just like, what is going on? This
is like, you know, it's like, you know, decades later and who really cares about some failed car
company at this point? Also, you have Taiwan, which I think Taiwan produces like one sixth of
global microprocessors, 90% of the ultra high end ones, you know, the ultra high end ones,
which China can't create at the moment. And it's kind of, you know, that's the whole point of this
set of sanctions and trade embargoes that the US put on them. That is entirely because of state
led innovation by the Taiwanese government. It's a country of what, 25 million people?
And in the world where Taiwan is making one sixth of the world microprocessors,
we have British politicians saying, sorry, we can't do that because of this thing that happened in
1975, whatever. Crazy. Yeah. And I think it's, you know, it's obviously there are some people
really trying, but I think the system as a whole is very trapped in an old ideology that sort of
doesn't just sort of wants to be quite hands off rather than hands on. And you know, I work with
Marianna Mazucato at Institute at UCL. And I really, when I first came across her book,
The Entrepreneurial State, it just blew me away because it really, if you've been a founder for
most of your kind of working life like I have, it really described and but you really care about
the future of the UK, the future of Europe. It really described a different mode of politics,
which is this very entrepreneurial kind of founder approach, which says, right, like,
the state is going to take a point of view, it's going to have a vision, it's going to pick missions
that really matter, it's going to invest very ambitiously to make things happen that wouldn't
happen otherwise. And I'll give you an example of something that like right now for me is very
inspiring. So within nuclear fusion, you know, you've got a number of different concepts for
how to put fusion on the grid. And we've made enormous progress in fusion over the last few
decades. And we're now actually I think within touching sites of it happening. And the dominant
approach that's really taken us closer to, you know, fusion on the grid is a technology called
magnetic confinement fusion, you have very powerful magnets, the confine essentially a sun on earth,
and you use that to basically produce a sun on earth and extract energy. And
the dominant mode of doing magnetic confinement fusion is something called the tokamak. And the
tokamak was a device that's, you know, there's probably $100 billion been invested in tokamaks
globally over the last over the last 50 years. And the German government actually took a different
point of view. And they said, there is this other device, which has a lot of attractive things about
it, that actually, you know, are very hard to do, but we might now have enough computing power to do
it. And they've been in quietly investing, you know, large amounts of money, but still quite
small for fusion into making this happen. And over the last 20 or so years, they've taken this
alternative fusion reactor design, all the way up to the point where we might actually be able to
build a power plant, and it's called the stellarator. And the German government has done that kind of
almost single handedly, the most advanced stellarator in the world is in North Germany. And it's
light years ahead of any other stellarator. And that's the sort of thing that just gives
me real goosebumps when I think about what the state can do really, when it really wants to
shape markets, you know, if the UK said, right, we're going to take a point of view, fusion is
going to happen in the next 20 years. We want to have this country running on fusion in a material
way, we want to do a massive industry for this country, we're going to have this incredibly
ambitious kind of investment mandate behind that. I think that's the sort of politics that excites
me and makes me feel like we would have a politics that matches the exponential change
rather than just kind of runs behind it, trying to play catch up.
How popular is that kind of stuff in your world? So you mentioned Marianna Mazzicato.
I suppose, you know, she proselytises a kind of social democracy with an interventionist state,
like you say, with like an emphasis on entrepreneurialism as well. So it's quite
a unique blend, although I really find it's a 21st century variant of 20th century social democracy
kind of updated for the network society. Are those ideas more popular than one might imagine
amongst these kinds of circles? Because tech people are generally open to new ideas or
are you something of an outlier in terms of enjoying Marianna's work?
I don't really know of an outlier amongst founders. I think that founders are quite specific people
and that they're really, they're drawn towards taking risk. They often want to try to make the
world better by building some product that they think the world needs. And I think a lot of them
are quite radical in their bones, quite risk taking. And I think they're drawn to, they're drawn to
interesting ways of thinking about politics often. I would say investors, and you know,
I'm kind of technically my day job as investor, tend to be a lot more focused on turning money
into money than actually really directing markets. So a little while back, you mentioned quantum
computing and the multiverse. And I know that's not the topic of this conversation. And I'm sure
you have a great deal of knowledge about it, but you'll be very humble and say I'm not the
right person to speak to. But this is really intriguing. And I think your audience would
love to hear more about it. What's this relationship between quantum computing and the,
the inference that it perhaps gives us a glimpse at the possibility of a multiverse, multiple
universes? I think I probably will try to stick to what I know something about within quantum
computing. So I chair a company called phasecraft, which is a company founded by a number of
UK professors. One of the professors that founded it is the sort of co-chair of the most important
conference this year in quantum computing. So they are some of the kind of global leaders in
this field. And what they're doing is developing software to run on quantum computers. And so
as a result of sort of supporting those founders over the last kind of three and a half years,
I've kind of had a bit of a window into what's happening in quantum computing. And it really
is quite amazing. So you've got these machines, probably the two kind of most impressive machines
in the world. One is built by Google in, down in LA. And the other bit is built in China.
And these machines take sort of, they use superconducting materials, trapped in these kind
of crazy cages to basically run computing operations that really allow for a much wider range of
possibilities than digital computers, which are more sort of deterministic. And so
and this is because a digital computer has a zero one binary system and quantum isn't
constrained by that. Well, yeah, sort of allow it kind of it remains in superposition. So it's
neither one or zero until finally you kind of collapse the superposition. So what that allows
you to do is to simulate on a computer things in the real world that are quantum, because we know
the real world is quantum, right? That's been established for a long time. And yet when we're
interfacing with quantum systems, for example, material science or biology, we still use classical
computers to simulate them. And so we're sort of trying to use something deterministic to simulate
something quantum. So what's really exciting for me about quantum computing is that you may have
a tool that lets us simulate aspects of nature that we have historically not been able to simulate.
And so for example, we can suddenly design incredible new materials that we can use to
the engineers can use to make things we've not been able to make until now,
or we can simulate biology in a way that we currently can't today. And as a result,
design amazing new drugs. And so I think that like, the way I think about quantum computers
is they are a class of computer that allows us to explore and understand the universe in a way
that classical computers don't. And that just feels like it's kind of like the invention of a
microscope or something. It's like a really important new tool that gives us visibility
into a realm that we currently don't really have computing resources that are appropriate for.
You have all these great analogies. I love this. So the idea of a quantum computer is like a
microscope. And of course, prior to the microscope, people didn't really understand bacteria, germs,
you know, the majority of organisms on the face of the earth, because they weren't visible to the
human eye, you're saying something similar could be possible with quantum computing.
Yeah. And I think it's really about simulation. And that's the thing that I guess I've got
personally really excited about is, you know, you've got some incredible classes of materials
that we know are quantum materials, right? The way that superconductive materials,
for example, we know a quantum and superconductors are amazing, you know, in that they are from a
climate perspective and energy perspective, they're like the material because you have no
heat loss to resistance. So you could transmit energy across the UK without losing any,
you have new energy storage opportunities, fusion reactors, the type I talked about require
very powerful superconducting magnets to work, MRI machines require them. So they're kind of
this magical class of materials called quantum materials. And because they are quantum materials,
there's only so much we can really understand them by applying classical computing techniques to them,
whereas a quantum computer would let you simulate them in a completely different way.
And as a result, we might be to discover new materials that we don't have access to today.
Before we start this interview, I asked you if you would consider moving to the States,
you know, that's kind of a cliche thing, but you know, people generally make their fortune
over in the US when it comes to technology businesses. You said no, or you weren't really
inclined to. So why do you want to stay here in the UK? I mean, I think the UK's given me kind of,
I think, you know, I think I'm quite patriotic. I feel very, you know, very appreciative of
what I've been given in terms of the privilege of growing up somewhere with universal health care,
with great education, with, you know, freedom of speech, the rule of law, like there's lots of
aspects of our society I'm very proud of, proud to be kind of, you know, to proud of and I believe
are really important to endure. And as we were talking also earlier, you know, we should recognize
these things as fragile, you know, Iran was a democracy once and is not now. And so I feel a
duty to give back to the UK, give back to Europe and very specifically the sort of investment
fund that my partners and I have set up, it's a we're all former founders, right? That's the
first thing about us. And the second thing is, we've set it up to try and have GDP level impact
on Europe. And so the idea is to help build some companies using our kind of scar tissue as founders
to help another generation of founders who are like us to build companies that can end up being
bigger than we've seen in Europe up until now. So I think of Skype, and I think Skype could have
been Facebook, I see DeepMind, I think DeepMind could have been Google or OpenAI, or I look at
ARM, I think ARM could have been NVIDIA. And so we've got these companies where we never really
got them to be what they could have been their full potential to be these like transformational
European technology companies. And so our mission as a kind of as a fund is to try to produce a few
of these companies that can change whole industries for the better. And by doing so have
impacts on European GDP, but also put Europe at the table when it comes to describing what's
happening in an important new era of technology. So if we're going to put fusion on the grid
in the next, you know, in the 2030s say, and Germany has been the state with the most kind
of entrepreneurialism to invest in accelerators well ahead of everyone else in the world, it
will be a travesty if the the fusion company that turns that into a into a startup isn't in
Europe, in my opinion. And so I do feel I do feel just very embedded in European values. I mean,
I lived in China, I lived in America, I've been a lot of admiration for both the societies in
different ways. But I guess I'm kind of pretty European to my core.
That's interesting. You said patriotic, but then you say European. So can you go into that a bit
more? Because in the UK, we've had this, you know, I don't want to sort of tread over old ground,
but they're often held in counterpoint to one another.
Yeah, and I think that's a bit of a, I don't know, like, I think there's a lot of nuance that's been
lost from discussion around Brexit. So a good example is one of the AI companies I work with.
The founder chose to actually locate the company in London rather than Silicon Valley,
because it was going to be easier to recruit the people who wanted into the company.
Right. So we talk about the UK, you know, being a much more, you know, people talk,
people caricature it as being more closed post Brexit. But actually, there's been some great
research done by John Paul Murdock at the time, the Financial Times that I think sort of shows
that's actually not necessarily true in all domains. And I haven't heard that as much as
you might think from some of these leading edge technology companies.
And so I think there's this kind of, we want to make everything black and white,
but actually there are ways in which I think that, you know, a suitably ambitious progressive
government that really wants to embrace Brexit and make it work could do really interesting
things with it. And so I don't, I'm not like reflex it, even though I voted remain, I'm not
reflexively negative about Brexit and sort of, it's just all these people who just basically
sit there complaining about it all day long on Twitter, like five, you know, five, you know,
and years on what at 20 2016, right? So like seven years on from it, that there's sort of,
there's a, there's a sort of, I don't know, it's become a sort of a tribal identity rather than a
sort of first principles assessment of what we should actually do as the UK to, to sort of
make the UK the best possible place to live. Yeah, David Deutch, who is one of these hugely
influential figures in Silicon Valley, British man, you know, he's based, I think in Oxford
at the moment, and he advocated leave. And, you know, I don't agree with his arguments,
I certainly don't think it was wise to leave the single market in the way that we have done.
But this idea that everybody who voted leave is thick and stupid, you know, David Deutch is
literally one of the smartest human beings who's ever lived. And, and like you say, there are,
there are opportunities, particularly with goal oriented public policy, with new technologies
that you probably could do interesting things with in a way that you probably can't, or it's harder
to do inside the European Union. But going back to the point about patriotism. So you're
patriotic towards the UK, or to Europe, or both? Yeah, I mean, I, I, I guess I, the reason I,
I think, I think George Orwell had a lovely sort of take on nationalism versus patriotism.
I think he said something like, you know, nationalism is kind of the assertion of your
values and other people and pushing your country's values on other people, whereas
patriotism is just sort of saying, it's kind of defending your values and just saying like,
these are things that are important to me and we want to preserve some of these things. And
I, I feel, you know, the, you know, NHS is the good, the kind of classic example is just something
that I feel very grateful for. And I feel patriotic about wanting it to, it to continue and to thrive.
And, you know, if we're going to apply AI in the NHS, I want it to be done in an incredibly
thoughtful way that expands what we've got today rather than anywhere undermines it.
So where did you live in China in the US? You said you moved there a few years ago?
Yeah, so I lived in, I lived in, I moved to China when I was 18 in 2000, study Mandarin, and then
went to university, started a kind of engineering machine learning, and then went back to China
in 2005, again, to study machine, to study Mandarin. And then I moved to the US Silicon Valley in 2006.
And so, I guess I've, I think I was drawn to both places because there were a lot was changing
very rapidly in the world. And it was, I think, somewhere where the pace of change was very
exhilarating. But I really feel, I think, the most at home in Europe in terms of kind of the
values, you know, I think, and I'd love to see Europe's technology industry and Europe's
governments really kind of rise to the moment that is coming in, in, with all this exponential
technological change. The final question, because it goes back to the thing you, you've
mentioned, you wrote this essay about AI nationalism, there's a great talk on YouTube,
by the way, once people have finished watching this and maybe watched another
Navarro media video, you do a great talk on AI nationalism. It's from a few years ago.
And you, you quote a great book, AI superpowers. And it's the hypothesis of that, I think is a
really strong one of the best books I've read in a while. A few years ago, I think Pricewaterhouse
Cooper say that between 2015 and 2035, you know, 15, 16 trillion dollars will be added to the global
economy by AI, more or less something like that. And about 70% of that goes to basically the US and
China. So all of the gains of this new technology, basically are concentrated in these two AI
superpowers. Imagine the steam engine, but rather than just Britain benefiting initially,
like we get with, you know, the steam revolution, the industrial revolution and then colonialism,
AI basically redounds the benefit of Beijing and Washington or Washington slash California.
Can you go into this hypothesis? Because it sounds to me, on the one hand, you just said a moment
ago, you're patriotic to Europe. Yet this idea of AI nationalism seems to indicate that Europe
isn't really at the races. And actually, on the present path, the two places that will benefit
from this are China and the US. So I think I think that Europe in general, setting aside the UK is
not very significant, unfortunately, within AI right now. If you look at where the most important
kind of concentrations of sort of talent and power are within AI and capabilities, it's actually
really cities that matter. And it's Toronto, San Francisco, London and Beijing. And London is
very much on the map. So London matters more than New York? I would say so. Yeah. I mean,
so Demis Osavis, who arguably kicked off the race we've currently got going on with the founding
of DeepMind and is an absolutely exceptionally talented person, is now running all of Google's
efforts globally from London. You know, the vast majority of the sort of best AI researchers
within Google are based in London at DeepMind's office. And so London is incredibly important.
It's not just London, not just DeepMind. We also have other organisations of a sort of similar,
you know, research stature to DeepMind. So I think that we should not rule the UK out. And
actually, I think it's a real opportunity for the UK to play a leadership role within Europe
by kind of bringing some of those assets to the table when Europe wants to do stuff in AI regulation.
But to go back to your question about kind of this race, this race between different countries,
I think it's incredibly nuanced and complicated because there are basically three different
levels you need to think about it at. The first is economic. So, you know, a country that has
incredibly sophisticated AI companies will probably benefit more economically from it than
those that don't. And that's the kind of takers or makers or, you know, the sort of steam analogy
you gave. The second, and you know, there is the second is basically military. And so AI will
definitely be used by militaries to achieve decisive advantage. And so in that area, again,
like a state is incentivised to build out the capacity to, you know, have autonomous autonomous
and, you know, even in the Ukraine conflict, you actually see the use of machine learning to
actually, you know, provide a decisive edge in certain ways. And the third is this existential
thing of like, if we build something smarter, more capable than us, that isn't aligned with our
goals, it could wipe us all out. And if you look at the three different levels, you've got the,
and you think about the US and China in the way that Kaifu Lee kind of presented that that race,
US and China clearly are locked into a battle to compete economically. They're certainly locked
in a battle to compete militarily, like a hypersonic missiles or something like that.
But this final level, which is like, are they locked in a battle to kind of,
you know, ensure that the human species thrives? I think they are actually, you know, they should
be on the same page about that, right? So you've got this really challenging problem where you've
got two levels of competition, and one level where you desperately need cooperation. And that's why
I think that like the really missing piece right now here is kind of international leadership
around coordination, and how we approach these most powerful AI systems that could become super
intelligent. That's so interesting around AGI. And I can see the argument for cooperation,
as you would hope for, say around climate change. But I suppose pulling it back a bit,
so going back to the narrow AI things that you spoke about a while back. Clearly,
AI is going to be massively disruptive in terms of economics. And I suppose if you look at the
last 20, 25 years and Silicon Valley really capturing so much value across the West, you know,
I'm from Bournemouth. If you walk down the high street in Bournemouth, loads of businesses have
shut down because that value has been captured by a company headquartered in California, right?
And that is an incredible concentration of political, economic, and cultural power.
And the argument is the same thing will happen with AI with more businesses to a far greater
extent. And the concern is that basically the world will be divided into two economic spheres,
US, China centric, and the rest of the world will, broadly speaking, just be their satellites.
So we don't need to be talking about an AGI and oh, great power confrontation will lead to
somebody potentially creating Skynet because they're seeking military superiority. Although
that's an interesting debate. But we could be returning to a bipolar world similar to the Cold
War, but in many ways far more extensive. Or do you think I'm sort of being a bit sort of
hyperbolic here because you have so quickly to go back, Kai-Fu Li talks about AI superpowers.
Why would it be China and the US are the superpowers? They have the largest populations,
Ergo, the largest amounts of data, and they have the greatest amount of computational power.
You might think that, you know, the EU might be a third poll, but it's not really working out
here the two. So geopolitically, the geopolitics of AI, I mean, you can see a world, can't you,
where there's a great sort of economic dependence, basically a kind of colonialism.
So I think, you know, in the AI nationalism essay, I gave an example of a kind of that sort of AI
colonialism with a company called Cloudwalk from China that was selling certain technologies to,
I think it was Zimbabwe. But I actually don't really view it as much from a kind of how it
affects individual citizens at an international level, more national level. You know, I think
that kind of haves and have nots within technology are already playing out on a national level.
If you look at the, you know, if you look at the deprivation in the Bay Area, right, and you compare
the sort of the lot of some of the people, you know, on the streets in the Tenderloin compared
to some of the people running these AI companies, you've got far more, you know, the genie coefficient
in some ways in sort of some of the US cities or across US states is maybe more extreme than
it is between certain nations. And I'm more, I think that's more concerning the the the lot of
the average person on Bournemouth versus a lot of the average person in London than it is to me
of kind of these these sort of country versus country comparisons. And so I think actually
the really challenging political problem is as actually how you kind of how you how you make
how you kind of raise the floor more broadly in a world in which there are further and further
returns to capital. I think we'll end it there. Ian, this has been a fascinating conversation.
Thank you so much for joining us. Thank you for having me.

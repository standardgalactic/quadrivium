Welcome to the first lecture of Machine Learning on Columbia X. I'm your
professor, John Paisley. I'm a member of the Department of Electrical Engineering
as well as the Data Science Institute at Columbia and my specialty is in
machine learning and this is this class that we'll be working through in the
following lectures is directly based on the machine learning course that I
teach here at Columbia as part of the Data Science Institute. So in this first
lecture I want to primarily focus on a general overview of the course and of
what machine learning is and then go into a little bit of detail on a very
specific problem working with multivariate Gaussians in order to kind of
highlight the different aspects and the different components of what we'll be
discussing in this course. So this course will cover model-based techniques
for extracting information from data with some sort of an end task in mind. So
these tasks could include for example predicting an unknown output given some
corresponding input. We could simply want to uncover the information
underlying our data set with the goal of better understanding what's
contained in our data that we have or we could do things like data driven
recommendation, grouping, classification, ranking, etc. So using the data to help us
learn how to perform these sorts of end tasks. So in a course like this there's
a few ways that the information can be presented, different orderings of the
information. One example would be to partition it as in one half supervised
learning, the other half unsupervised learning and I'll discuss that in a bit
more detail in this lecture because that's the perspective that we're going
to take. But we could also think in terms of probabilistic models where we are
working with probability distributions versus non probabilistic models where
we're learning from the data without any sort of probability probabilistic
motivation. There's also a dichotomy between modeling approaches. So what is
the model that we want to define for our data versus optimization approaches
which is very tightly linked with modeling approaches. But with optimization
now that we've defined a model, how do we learn the model? So these are two
separate problems with various techniques in these two sub problems. So again
we're going to partition this course into roughly two halves. The first half
will focus on supervised learning and the second half will focus on
unsupervised learning and these additional ideas such as probabilistic
versus non probabilistic or modeling approaches versus optimization techniques
will be sort of discussed as we go along as needed. So those will be
interwoven throughout the course but the first part of this course will be
strictly supervised learning and the second part will be on supervised
learning. What do we mean when we say we want to perform supervised learning?
In a nutshell what we're saying is that we want to take inputs and predict
corresponding outputs. So for example if we do want to do regression we might
have pairs of data in this case a one-dimensional value for x and a
corresponding one-dimensional value for t and then we would want to learn some
sort of a function so that we input x and we make a prediction for the output
t. So for example here we have several data points as indicated by circles
where the x-axis is the input for that particular point and the t-axis is the
corresponding output and now that we have this data set of these several
points we want to define some sort of a regression function for example this
blue line that in some way interpolates the outputs as a function of the inputs
and then the goal is given this smoothing function that we've learned for some new
input x we want to predict the corresponding output t so we for future
time points we obtain x we obtain an input and we want to predict the
corresponding output so that's regression we say it's regression because the
outputs are assumed to be real valued. Classification is another supervised
learning problem that is slightly different the form or the structure is
very similar we have pairs of inputs and outputs and we get this data set which
has many of these pairs of inputs and outputs and we want to learn some
functions so that in the future when we get a new input for which we don't have
the output we can make a prediction of the output that's going to be accurate
however the key difference here is that where with regression the output is a
real valued output with classification it's a discrete valued thing so it's a
category or a class so in this right plot what I'm showing are input output pairs
except now the input is a two-dimensional vector so here the input would be this
two-dimensional point and the output for this plot is being encoded by a color
so the output could be one of two values either a blue value or a an orange
value so in this case we want to take our data inputs and classify them into one
of two outputs so we get a data set like this with all of these input locations
and the corresponding color-coded output and now our goal is to learn some sort
of a function a classifier so that we can partition the space such as is shown
here where for a brand new point any of these points that we don't have the
output we can evaluate the function at that point and make a prediction of the
output so we might say for this data set we would partition this entire region
here these two regions into the orange class and this region here into the blue
class so any new points falling in this region will be assigned to the blue
class so the key here with supervised learning is that we're learning an
algorithm based on a function mapping from input to output we the outputs are
basically going to be telling us how to map the inputs so that we have an
accurate function so we have input output pairs so to look at a classic example
we could think of spam detection given some set of inputs like these two chunks
of text we would want to assign it a label plus one or minus one sometimes
we would say plus one or zero but we would want to sign it one of two possible
labels one label would correspond to an email that is spam and we would want to
then you know automatically delete that email and the other class would be non
spam emails emails that we want to put into our inbox and actually read so it's
essentially a filtering problem so for example we might have a data set a data
point like this containing this text and we would want to now input this into
some sort of a function and say is it spam or not in this case most likely
it's not or a data point like this this piece of text where we would input it
into the same exact function with the same classifier and in this case that
same classifier would say this email is a spam so we classify this email to spam
and this email to non spam using the same classifier learned from examples of
of labeled spam and labeled non spam emails so essentially the first half of
this course is going to be all about learning different ways that we can
define these functions to map inputs to outputs either regression models or
classification models depending on the problem as well as algorithms or
techniques for then learning the parameters of these models based on data
so that will take up the first half of this course there are many very useful
techniques very different techniques for performing these two tasks they'll
entail different ways of thinking about the problems probabilistic versus
non probabilistic the models that we define will require different ways or
different techniques for learning them so we'll need different optimization
techniques so the first half of this course will be all about supervised
learning then in the second half of the course we'll transition to unsupervised
learning and with unsupervised learning the goal is a bit more vague
supervised learning is very nice because we know that we want to map an input to
an output and honestly we don't necessarily even care how it's mapped we
don't care whether we learn anything by mapping it in many cases we don't
perhaps in some cases we do we simply want to say here's my input what should
I map it to as an output and we measure the performance based purely on how well
it does that task with unsupervised learning we don't have in most cases
this sort of an input output mapping we want to perform more abstract or vague
tasks such as understanding what is the information in our data set for example
we don't have an infinite amount of time to read you know so many thousands or
millions of documents so we want a fast algorithmic way for taking in
information taking in data and extracting the information for us so for
example with unsupervised learning we might want to do something like topic
modeling where we have many texts data many documents provided to us we don't
have any labels for these documents all we have is the text for each document
and then we want to extract what is the underlying what are the underlying
themes within these documents so that's the idea of topic modeling we also might
want to do recommendations this would be where we have many users and many
objects and users will give feedback or input on a subset of these objects either
through a review or through some sort of a rating like a star rating for example
with Netflix a user could rate a movie one to five stars and we want to take
all of this information and learn some sort of a latent space where we can
embed users and movies such that users who are close to each other share
similar tastes movies that are close to users are somehow appropriate
recommendations to be made to those users movies that are close to each other
are similar in their content and things that are very far apart are very unlike
each other so we want to learn this information simply from the data from
the raw data and some model assumption that we have to make so for example one
of the most well-known unsupervised learning tasks is the topic modeling
problem and so what I'm showing here is an example of what a topic model will
learn if you provide it with over a million documents from the New York
Times over roughly 20 year period so what we have in these documents is
simply a tag that says that this is a document and these are the words in the
document and we have that repeated for all of the documents in our data set
again it can be over a million of these documents we want to make some sort of a
modeling assumption such that we find the words that should somehow cluster
together these words would then define topics underlying our data set so for
example by simply inputting the raw data from the New York Times making a
modeling assumption that doesn't in advance tell us which words should go
with which other words we can then run an algorithm to extract information like
this that says that this set of words belongs together this set of words
belongs together and so on so we can learn for example 10 or more of these
what are called topics that tell us which words belong together and then not
shown here is also how each document uses that topic so for example for a
particular document we might say it's composed of these two topics and no other
topics and so this is information that's extracted from the raw data we don't
a priori tell the algorithm what it should learn we simply say there is this
structure that we want to learn here's the data tell me the structure

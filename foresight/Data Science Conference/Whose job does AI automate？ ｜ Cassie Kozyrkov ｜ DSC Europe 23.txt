Tako dobro.
Tko se dobro.
Thank You so for long session.
Google is in statistics, decision making and machine learning.
Today, she leads data scientific and elite agency known for assisting global leaders and executives
in optimizing critical decisions.
So, Casey, may I ask you, whose job does AI automate?
Thank you for that introduction, and yes, I have the answer, but we'll get there in a sort of meandering way.
Good morning, Belgrade, I am so happy to be here with you today, and I have a quick question for the audience.
Who here identifies themselves as an AI professional?
Sure of hands, sure of hands.
OK, I see some hands.
How about who does not identify as an AI professional?
All right, a little more hands, a little more hands.
Friends among the AI professionals, do you know what year was the first year that the term artificial intelligence was used?
Anybody?
We're gonna have to do a little history here, I think.
The answer is 1955-ish, 1956 for sure, because in 1955 there was a fun grant proposal being made,
and in this grant proposal was the first time that this term was used.
So, I think that was written in 1955, but it was for a summer workshop in 1956.
So, what was the grant proposal for?
It was for a summer school session at Dartmouth in America.
That's a university and a bunch of postdocs, led by postdocs or assistant professors, I forget what they were at the time,
led by a fellow at MIT, John McCarthy.
They wanted to get the funding to get together 10 of them to spend two months working on what they called in this grant proposal, artificial intelligence.
Now, there are two fun facts about this situation, which you might enjoy.
The first fun fact, and I actually only learned this myself in October,
from someone who used to work directly with one of those 10 attendees.
The first fun fact is why that term?
Is it because it is truly intelligent?
Because we do get a lot of questions, like, are these things intelligent?
That name is a curse that comes from the grant proposal,
where it turns out that John McCarthy wrote that term in order to scare the US government into giving funding.
So, that term was picked not because anybody particularly knew what intelligence was,
or how the human brain worked, but rather because that term sounded intimidating.
And we're still living with some of the legacy and baggage of that.
Sometimes it stops organizations from being sensible in what to expect.
The other fun fact about the name artificial intelligence is that in that grant proposal, where it was first used,
the writers of the proposal were planning to solve all of artificial intelligence
with 10 people in two months in 1956.
So, there's a legacy in this field of big talk, which does not necessarily meet reality.
So, maybe we should level-set a little bit and reorient ourselves.
Maybe what we should have called it, if we wanted AI as an acronym, is automated inspiration.
Maybe that would have been a better term.
Or amplified impact, which we're seeing a lot today.
Or this year's move towards augmented individuals.
But really, this is a story of automation.
So, let's see automation at its most basic.
So, what are we doing with a computer?
We're automating digitally.
We're turning inputs into outputs via a recipe.
So, what's a recipe?
It's some code.
It's a model.
Those are all fancy words for a recipe.
Takes the inputs, turns them into outputs.
But we're going to need some hardware here to do it.
The recipe is not enough.
We're going to need a computer.
And you are probably imagining this kind of computer.
But I want to show you a different kind of computer.
That's also a computer.
Meet Dora.
And Dora is a computer.
Now, how do I know this?
Because Dora happens to be my friend's wife's aunt.
And Dora was actually a computer.
Here is her marriage certificate from 1950.
And you can see very clearly there that her profession is listed as computer.
So, she is a real computer.
So, we're going to have her in this example.
And what she's supposed to do is take an input and turn it into an output via some recipe.
Now, how does she know what to do?
Right?
Those of you who already work with computers, those of you who are developers,
you know that a computer needs something very important,
which is an engineer to program the computer, right?
And luckily Dora has an engineer also,
because there you can see her husband's profession is listed as engineer.
So, this is one of those early days in history where a computer has managed to marry her engineer.
Now, let's consider Dora and her engineer husband.
Back then in 1950, we want to get Dora to do a task,
like recognize whether an image has a cat in it or not.
This was a very classic computer vision task, right?
The cat not cat task.
Now, one way in which we could do this
is to figure out the exact instructions to give Dora of what to do with the input.
Now, consider what the input is.
The input is a bunch of pixel color values, so it's a bunch of pixels.
And the question is, what should you do with each pixel in an image
to get the answer cat not cat, right?
What is that recipe supposed to be?
And now you're thinking philosophical things,
like what makes a cat a cat?
Difficult question, right?
Like, what do you do with the top left-hand pixel and the one next to it?
And what are you looking for in the image?
Are you looking for triangles, maybe two triangles, maybe some ovals for eyes?
This is a hard recipe to come up with.
And so, in order to program a computer with instructions,
do this, then do that, then do that,
first, you have to know how to do the task,
which, you know, how are you actually doing the task, though?
You're doing it, your brain is doing it,
but what are you actually doing with the pixels?
That is so hard to put into words.
You don't even know what you're doing.
So how on earth are you going to program with the instructions
of what to look for in each pixel?
Very difficult.
Now, there's a different way that we could go about this.
Instead of explaining what to do with each pixel,
you could instead explain your wishes with examples.
Here are a bunch of examples of cat.
Here are a bunch of examples of not cat.
You go find the patterns,
and then make a recipe automatically from those patterns
so that you can take yourself from input to output.
This examples versus instructions thing.
This is the essence of the difference between machine learning
slash AI and traditional programming.
And notice that we already do this with one another as humans.
Sometimes we explain our wishes with examples.
Sometimes we explain our wishes with instructions.
So we already teach one another one of these two ways.
Now we are able to do the same thing with machines,
except we need fancy words for examples and instructions.
So we've got code as instructions and examples map to data.
So this really is the difference
between the traditional software programming approach
and the AI slash machine learning programming approach.
So let's make sure that we warm up this room
and look at doing this cat not cat task together.
So I give you a bunch of inputs with their appropriate labels.
And then something in your brain figures out
what those patterns are, turns that into a recipe.
And then when the next one comes in, you're going to take it
and you're going to convert it to the output I'm looking for.
So to wake ourselves up, we're going to play this game together.
I need each and every one of you to shout cat or not cat
when I show you an input.
Do you think you can do it?
Yes?
You're not loud enough for me.
Yes?
Yes, OK.
Good.
Right.
So here comes the first one.
Cat.
Someone said yes.
Excellent.
Working as intended.
So cat, I agree with you.
Cat, see computers also make mistakes.
This is what we will see as a theme here.
Right, this one, I agree with you.
Cat.
Next.
Not cat.
Not cat.
Not cat.
Cat.
Cat.
Cat.
Cat.
Cat.
Good afternoon, judges and judges.
We've had a very laugh, we could have lep on you.
OK, here we go.
As for judges, we'll start.
No, do you have miel in dazz?
vog.
Now, now, the thing I wanted to do.
At the moment, I knew this chair in Nanаш and I didn't know the chair in Hanse.
Wanlax.
��重要!
Sen si je jestvo invented nachodnout s n夠.
Komand marched amerika je menarem za no doc,
pri konar 모 trenz si jeрал.
Spotn BOLE
postaende idemko u vlada supermarketu,
res prese баг always teber.
The right answer here depends very much
on the purpose of the system.
What does it exist for?
In so, I guess I'd better fill those big boots
and I'll tell you
that this is supposed to be a pet recommendations system.
iz temi direktosni signedcrossne od v pagesenati.
Jedob連 bomo se pristratil.
N Community.
EVON.
ADVORTED V
nismo visoki inserted, izmonživati,學om,
lagske stipiot Lars van pri otro Los 2.
Tudi sem bent ihtim eskatakaj.
Jaz pri vsojeve providilo
vse ovo, inverse drženje v taki,
i kaj mi dobro komens,
da jste ti s da žine.
Tako bojteške investiula,
je biloboardo.
Toto rada ne vem za.
Zvousite,To ga se od Economija Kot.
in zelo je zelo tako zelo tako zelo, if we just say what this is,
which is examples written down in electronic form, text books, essentially,
for the machine student to learn from.
And data quality is everything.
If you're going to teach someone with examples,
the quality of those examples matters so much.
And none of this is purely objective, same meaning and answer every time,
as you can see, depending on the purpose of the system,
whether the right answer is cat or not cat, changes.
So data is a bunch of scraps of information that we happen to write down,
that we put in a textbook for a machine to learn from.
Like human textbooks, normal textbooks for human students,
machine textbooks, data sets have human authors.
They don't arrive from aliens, they don't come from nowhere, from the universe.
They are collected by us, and they fit the sensibilities
of whoever is in charge of the data problem.
And the trouble with them, of course, is that they reflect
unconscious things we might not even have considered
could be a problem when we were authoring our textbooks.
So when you think about really old textbooks,
and you think whether you would want to teach your children
from these really old textbooks from 200 years ago, 300 years ago,
you're thinking absolutely not.
It doesn't matter what the title on the cover of that textbook is,
chances are, if you try to learn from it,
you're going to pick up some habits that are not good or useful habits.
So thinking about the quality of your textbook is really, really important.
And can you complete the following sentence,
just to make sure that we're all on the same page?
Garbage in, garbage out, you know this.
So what I find very interesting about the data space,
the data professions, data science,
I find fascinating about data science.
So when you go to a data conference or a data science conference,
so sometimes I hang out with people,
I'll hang out with people after this as well
if you want to hang out out there.
And sometimes folks come, they hang out,
and they've got all different job roles,
so I'll ask them, you know, what do you do professionally?
One will say statistician, and that one will be data engineer,
and that one will be clinical researcher, and so on.
And then I'd like to do another round,
and I'd like to ask them, okay,
who in your organization is responsible for data quality?
Who is in charge of it from data design,
documentation, the cleanup,
all the way through to the part where it starts hitting
the pipelines that the data engineers have built.
So who shapes the data set?
And what I love to hear here is that as we go around,
we have a very high correlation with whatever they said
their own job title was.
So the statistician says statisticians are responsible for it,
the researchers say researchers are responsible for it,
data engineers say data engineers are responsible for it.
You know what that sounds like?
That sounds like a situation where it's everybody's job
and therefore nobody's job.
In order to automate with data,
you need good and appropriate data.
In order to get good and appropriate data,
you need an expertise in a bunch of different topics.
You don't need to be a full expert in statistics, for example,
but you need some expertise in statistics.
You need some understanding of data engineering,
some understanding of survey design,
human psychology if the data sets are about humans.
You need some user experience design
if you are gathering that information online
and how is the way that you are presenting the questions,
influencing the answers that you get back.
There's a lot of expertise you need.
And yet where is the job role for this?
Where is the profession that takes this seriously?
And I had a really terrible aha moment with this.
I was hanging out with a data science influencer, as one does.
And I was talking about how this is a problem,
that we are building our disciplines on a foundation of data,
and yet the quality of that data is no one's job.
I was saying this is so important,
maybe instead of over focusing on this last mile thing,
maybe we should put more effort as a profession,
in the industry in the first bit, the actual data quality.
I was saying we need to encourage university graduates
to study this and to take this seriously.
There needs to be a career progression,
a way that motivates you to actually want to learn
all those things, to do it professionally,
because there's a lot to learn.
And then I ask this friend of mine,
and we're live streaming, this is what makes it best,
we're live streaming in this moment, I'm talking about it.
And I ask, so what do you think we should,
what should this be called, what is this called?
And my friend goes, oh that sounds like a data janitor.
Is this how we're going to motivate our undergraduates
to go to university, and they're picking their major,
they're deciding what to study,
and then they call their parents and they say,
I've picked one.
I would like to go through a hard grueling training program
to be a data janitor.
Are you proud of me, mom and dad?
Right, that's not a good start.
And it's not a good start
when data seems to be everybody else's job.
So we have quite a brittle profession here,
because a lot of it is based on the hope
that someone is going to do a job
that they didn't train for,
and that we're almost surely not paying them properly for.
We should worry about this.
And we should also remember
that a lot of the data that we wish we had,
or the quality that we wish we had it at,
won't exist if we have this basic problem of economics.
So that's our first thing.
Data are not objective, they are subjective.
The design matters.
And even though we rely so much on data for automation,
there's not that good of a plan in the data professions.
So data quality is everything.
That is the first point I really want to hammer home here.
And I know a lot of you in the audience
think you've heard this all before and you get it.
But if you did,
wouldn't there be better progress in the industry
to motivate, fund and compensate people
whose job the data quality actually is?
And then let's talk a little bit about the internet
as a data source.
Like that is a source of mirrors, isn't it?
Kind of reflects reality a little bit,
but you get a skewed perspective.
Never forget that the internet is not reality.
How you behave online isn't how you behave
in your natural surroundings.
And we know what kind of stuff lives on the internet
in the parts where people can be anonymous, right?
It's not necessarily bringing the best of us to anything.
So we need to be quite careful
with what we allow ourselves to do
on the basis of wild type data.
Now back to our question of whose job AI automates.
Let's look again at the broader category of AI.
By the way, I'm using AI and machine learning
somewhat interchangeably here, because I've given up.
Once upon a time, AI used to be the superset,
then machine learning was the subset,
then at some point machine learning was the superset
and AI was the subset, something, something.
If it's deep learning, then it's AI.
I give up.
Honestly, there was a set of cycles
in funding and disappointment,
where you got the funding if you said AI
and then things didn't work out,
so then you started saying something else, machine learning,
and then the funding didn't work out there,
and so we went up and down in these cycles.
Like bell bottoms and skinny jeans, right?
Like it's the fashion of what we're gonna call it.
So actually my favorite definition
of the difference between AI and machine learning
is if it is written in Python,
it's probably machine learning,
and if it's written in PowerPoint, it's probably AI.
So, I'm using them interchangeably, the hell with it.
So whose job does AI automate?
Let's look carefully at what this is
as an automation proposition.
When I'm automating the traditional way,
first, I have to know how to do the task
so that I can explain to you
what precisely you need to do with each input
to get the output.
Second, I have to think about every little instruction
and then I have to write it down,
and first I can write it down for myself in pseudocode
or, you know, English or whatever language I speak,
and then I have to translate it into something
the computer can understand.
But I have to deal with every single line,
and maybe it takes 10,000 lines,
maybe it takes 100,000 lines of code
to automate my task, I can write each one down.
And you might be saying, oh no, maybe it's,
maybe I just get a package somewhere,
I find some library, I install something,
and then I just pull from there
and I don't have to write the code by hand myself.
Sure, but some member of our species
had to do it.
So some human is responsible
for having thought through all those instructions,
whereas with machine learning and AI,
there are just two lines.
Optimize this goal on that dataset
go.
Now those of you who raised your hand for AI professional,
you know there's a lot more code that you're writing.
But that is because the tools are nasty.
At the core, there are only these two lines of instructions.
What does success look like?
What data should we point this pattern finding thingy at,
and off we go.
And really, if you had the ability to brute force it,
and had enough computing power,
you could try every known algorithm
with every permutation of it pretty much,
quickly eliminate some,
try everything out,
subject to just the two important lines of instruction,
optimize this goal on that dataset.
And as the tools become easier and easier,
we will strip away all the huffing and puffing
and the difficulty of forcing a dataset in this format
to be taken up by an algorithm
that was designed over there.
You know, some of these tools are really,
only a mother could love them.
And you're left with just these two lines,
which means that almost anyone then
will be able to automate a task.
Ha!
What do we see here?
First, two very subjective lines.
What is the goal of the system?
What does success look like?
Why am I building this pet classifier
that does cat not cat?
And why should Tiger be labeled cat versus not cat?
Well, vice versa.
There's no one single right way to do that.
What about scoring mistakes?
That's all part of how we're gonna express our goal.
Which mistakes are worse than which other mistakes?
Again, highly subjective.
So, which textbook shall we learn from?
There are a lot of different textbook choices you could use.
You could also edit and modify
and get different versions of the textbooks.
And all of this is highly subjective.
But now, available,
just two lines and you can automate your task.
How wonderful and how terrifying simultaneously.
This is both the peril and the promise of AI.
The promise is if I'm doing a little task myself,
I can now automate it very quickly.
How great for me?
I don't have to go and write everything from scratch.
But at the same time,
what if I am automating something
on behalf of millions or billions of people?
What if my code's gonna touch a lot of lives?
Well, then I can, again,
without thinking too hard about it,
get it automated.
So, we have a thoughtlessness enabler here.
We can be more thoughtless
and we can automate thoughtlessly.
Which is great when it only affects you.
But when we start scaling that up,
we can do damage.
This is like a proliferation of magic lamps.
Lamps with genies.
And knowing how to make a wish responsibly
is a very important skill.
It's the skill of decision leadership.
We're not even talking about this, though.
We're not asking ourselves,
who is it?
Who has the skills on our team
to figure out what success should look like?
How do we carefully state what we're looking for?
What do we actually want to create in the world?
And what would be the consequences
if we got what we asked for?
And which data is appropriate and why?
And what would need to be true about that data
for us to wanna use it?
Very, very subjective questions
that very few people are trained to answer.
So you should worry
who is actually being tasked
with doing this for massive systems.
Do they have the skills to do it?
And as you see,
the tools get easier and easier,
you'll see a shift from a focus on
huffing and puffing and actually getting
the data to be taken up by the algorithm
and then deployed to production,
and a lot more focus on
how do we put 10,000 lines
or 100,000 lines of thought
back into these two lines.
We've allowed ourselves to be thoughtless,
but on some things, that's not okay.
So how do we put that thought back in?
How do we very carefully design
systems that can affect society at scale?
But back to the question
of whose job are we actually automating here?
Well, it is the developer's job.
We're going from having to write instructions
to now being able to say,
instead of knowing how to do the task,
here's the objective, here's the data, go.
That said, it's not like we're putting
software developers out of business.
First, there's still a lot of huffing
and puffing to do to get the algorithms
to accept those instructions
and the data.
Second, we are actually unlocking
a whole class of new applications.
And all the old approaches
are still going to be very economically necessary.
Why?
If you are able to automate your task
with instructions,
that is how you should do it.
That's how you get the most control.
If you're able to say what needs to be done
in what order,
and you give those instructions to your machine
or to your human employee,
you can be sure of what that person
is going to do next
if they're following the instructions.
Exactly what you've told them to do.
No guessing.
No surprise ways that they interpreted anything.
Just follow those instructions.
Whereas, if you know how to give the instructions,
but instead you give a few examples,
who knows what they're going to learn
in those examples?
Maybe they'll learn the right thing,
maybe they won't.
And mistakes are possible.
That's true with humans,
that's also true with these AI systems.
So why are we using them?
To automate things we can't automate the other way.
So we're not putting developers out of business.
Everything developers used to do
and used to be able to do,
you're still going to want to do that
in the old traditional way.
But now we've got a whole new class of applications.
And let's talk about a new new class of applications.
The two different AIs.
So this year we're talking a lot about AI.
We tend,
when we find ourselves hanging out with friends
and having a glass of wine
and talking about all these new things in AI in 2023,
we tend to be talking about generative AI.
So let's remind ourselves very quickly of the difference.
So discriminative AI, the old one,
the one you're used to from last decade,
that is all about applying a label.
So this is a thing labeler.
We had the cat not cat example of that.
Here's another classic again with vision.
So I really like this tweet.
It comes from BJM,
who complained that he was locked out
because his smart front door lock,
his nest camera system locked him out
and he was protecting him from Batman.
It didn't want to let Batman in the house,
so it locked poor BJM out.
So it's supposed to find the right answer.
It doesn't always work correctly.
These systems do make mistakes.
And it's really important for designers to remember this,
because imagine what would have happened to poor BJM
if there wasn't a plan for mistakes.
He wouldn't have gotten back into his house.
Instead he can put user pin to get himself in
because the engineers built that safety net
and knew that mistakes were possible.
So that's thing labelers.
On the other hand, generative AI
is about creating a plausible exemplar.
What are we learning?
Not a label, but a distribution.
And what can you do with a distribution
is create a really good fake.
This is a fake maker.
So as you all know,
Picasso is very famous for his paintings of laptops.
So I have some examples for you there
in the top right-hand corner.
This is where I'm using an image generation tool
called Mid Journey.
Mid Journey is my favorite casino.
I really enjoy playing with Mid Journey.
It gives you, you put in a prompt,
it gives you four options back,
and maybe you like it,
maybe you rerun the prompt
until you get something that you like.
I've also created some fake Gucci sunglasses for you.
So both of these aren't,
you know, it's not real Picasso.
This doesn't really exist out there.
We are generating from a distribution of plausible Picasso type
and laptop type things
to get this lovely fake for you.
So it's a game of plausible exemplars.
And people ask a question that bugs me so much
when they see this stuff.
They ask, can AI be creative?
Does this mean that AI is the artist?
Is AI making art?
And then I have to remind folks
of an entire century of art history.
Because if you're asking questions like this,
you must have missed something
from art history from the 20th century.
So let's go back to 1917.
Marcel Duchamp found this iconic piece.
This is considered iconic in the art world.
This is a urinal.
He signed some name on it,
not even his own name.
He took it to the exhibition when that's art.
And it was, we consider this a very interesting piece.
It's worth a lot.
Is it art?
Sure.
Why?
Because art is a conversation
that humanity is having with itself
and has been having for millennia.
And to make art,
we consider the next sentence
in this grand conversation.
But who is the artist?
I would say Duchamp, let's give him credit.
Because otherwise, where are we going to put the credit?
On the porcelain makers,
the toilet company,
that doesn't make any sense to me.
And should we penalize him
because he didn't sculpt it from scratch,
mixing his own materials,
creating his own porcelain?
Not at all.
This cut out a lot of toil
because he didn't know what he wanted to say much faster.
Generative AI plays the same role.
Like a paintbrush,
it's a tool for you to be able to say
what you need to say faster and better.
And the secret behind a lot of these generative AI art things
is that it's very rare
that the first one is the one that's presented to the audience.
So AI made art that won some art competition.
The 8,000th iteration,
a person cranked the handle on these tools
8,000 times,
500 times, however many times it took
to get the one that they were looking for,
to express what they wanted to express.
So where is the creativity?
It's in the human.
But the human can go a little bit faster.
They don't need to mix paints.
We don't penalize artists today
for not mixing their own paints
the way that they would have in the Middle Ages.
Making their own paintbrushes
out of horse hair or whatever it is.
You go to the shop, you buy some paintbrushes,
you buy some paint, and you go paint.
That's great.
That lets us get there faster.
And that's what generative AI allows you to do as well.
What are some other things you can do?
So good old open AI all over the news.
Open chat GPT.
I have in audiences like this,
I have asked who here has never used chat GPT.
And so I'm going to stop embarrassing audiences
because there does tend to be one hand.
And then I ask, OK, who here has never read about chat GPT
and that no hands go up?
And I think what a strange equation.
It is faster to try it than to read about it.
So why did my one hand, one or two hands
read about it without trying it?
You may as well just try it.
The interface is super easy.
It's like sending a text message
and you type whatever you want to type.
This is when we asked CEOs
what they personally use chat GPT for the most.
One of the favorite answers was to write a retirement poem
or a birthday poem.
So it's really getting used for its top applications.
But here I've asked it to write a funny retirement poem
for your CEO in the style of Dr. Seuss.
And what does it give us back?
You've been the big cheese, the head of the pack.
Now it's your time to kick back and slack.
So definitely the highest in what we could possibly want
out of our technology.
Let's try another one.
This is an application that OpenAI found
and noticed was a statement perhaps about the human condition.
So a lot of people like to take bullet points
and then ask OpenAI, chat GPT, to turn that into an email,
a full email.
So here's some summaries, expand that out into an email.
And what OpenAI found was that this was a popular use case,
as was this other use case,
which was summarize this email as bullet points
back into the original.
So I think that does tell us something a little bit sad
and funny about how humans work.
Wouldn't it be great if all of our emails
could just be bullet points in the first place?
But what we're seeing here with generative AI
is a new kind of user interaction.
It is AI as a raw material.
AI, for AI's sake,
given to you the user to do anything you want with Next.
So we have the ability to find the right distributions,
to pull laptops by Picasso out,
or Gucci sunglasses, or whatever else you want.
But now we're giving you the tool,
you figure out what you want to shape it into.
And when we talk about regulating generative AI,
I hope you can appreciate now how hard this is,
because we are not good at regulating raw materials,
even physical raw materials.
Whose fault is it?
If I invent a phenomenal anti-gravity material,
that could be pretty cool for humanity,
but some idiot's gonna make skis out of it.
So whose fault is it, then,
when they go and ski in anti-gravity skis and hurt themselves?
Was it me for making the material?
Was it whoever helped them fashion the skis?
Or was it, then, the user of those skis?
Who is responsible?
How do we limit what the uses of it are
that would be okay versus not okay?
This is a hard problem, hard with physical materials
when it comes to digital raw materials.
Good luck.
Really hard to figure out how to regulate.
And when I hear that a problem is really hard,
the last thing that I want is for us, then,
to solve it in some dumb way just to say we've solved it
so we can move on in our to-do list.
Solving AI regulation here is difficult,
which means that maybe we shouldn't get ahead of ourselves
and make a bunch of laws we haven't thought through,
maybe go slowly and think about the consequences
of regulating, maybe request a bunch of information
that would help you regulate it later.
So, it is a very, very difficult problem.
Then, another kind of application
is all kinds of translation-type stuff.
So here I have asked it to write the FORTRAN code
for generating the Fibonacci sequence.
And I do not speak or understand FORTRAN,
so, hopefully, someone in the room can look at this
and see if it's right or not.
What I can do is I know what the Fibonacci sequence is
and I can write out those instructions.
I could also have written them out the way that I want
and ask for it to translate that to FORTRAN.
But here's a little quick bit of trouble.
Anyone here going to fess up,
going to confess with me that you don't speak FORTRAN?
Right, so, I'm seeing some hands.
So, imagine that we asked for this lovely FORTRAN code
for generating the Fibonacci sequence and we get it.
Do we take this and plug this directly into our codebase
when we don't understand what the hell it is?
Terrifying.
So, with this one, OK, maybe we know how to be software engineers.
I would figure out how to make some unit tests here.
Maybe I would line by line try to figure out what I'm looking at.
But you can see the more code that I generate with this
in languages that I don't speak or understand,
the more space I'm leaving for potentially catastrophic disasters
as I plug this in to an already complicated system.
This is why people are saying that good developers
are becoming much better, the estimates coming from McKinsey
of 50% better, if you're a good engineer,
then this can cut out a lot of drudgery.
But bad engineers are becoming worse.
And bad teams are reducing corporate productivity
because they're plugging all kinds of nonsense into their systems.
So, in general here, people who are already highly productive
are making themselves more productive.
They understand the output.
They understand how to put it together into solutions.
But those who are on the less productive side
or they don't know what they're working with
or they just believe AI is magic
and they plug things in where they shouldn't,
they are reducing everybody's productivity.
So, that's a point worth thinking about.
And I wonder if it would surprise anybody here
that I do not, in fact, speak Serbian.
So, imagine if I had a really, really important email
that I needed to write to someone in Serbian
where I really care about my reputation and that relationship.
So, I just go straight to open AI.
I ask for that email to be translated.
I get something out of there and I send it.
What a disaster that could be.
Maybe it's correct.
But if I have no way of checking it,
I'm gonna have problems.
And this brings us to why it's so difficult to scale
generative AI in the enterprise.
To use it for personal productivity
and to make an individual responsible for the output
is quite an easy one
if you already have a smart and productive person.
But when you think about taking the person out of the loop
and then at scale automating some processes,
remember, these systems do make mistakes.
It may be hard for humans to check.
It may be hard to even define whether that output was right or not.
So, how do you set up at scale this kind of automation?
From now on, we're automatically going to write
all our emails in Serbian with this system.
You're gonna have to test the hell out of it.
And that's what a lot of companies don't know how to do.
So, no wonder we're getting this bottleneck in enterprise automation.
And so, when should you trust an AI system?
There are two paths to trust.
The human in the loop model.
So, make the human individually responsible
and treat that as individual productivity increases
or a hell of a lot of safety testing and safety nets.
And as always, it is safest to have both.
We're still so excited in what you potentially could do
that we end up in organizations with death by a thousand pilots,
get mandates from the top saying,
everybody go find two, three use cases
and everybody go try plug this into your business.
And then take the human out of the loop too.
They don't know how to do testing though.
They don't know how to build the safety nets.
They've removed the human in the loop
and then they wonder why their use cases don't work.
I do think that we're gonna have a lot more generative AI at scale,
but this is the part to figure out.
In fact, I'm a recovering statistician.
And recovering also,
because we are the grumpiest of the data scientist
and we've sort of been folded into the data science profession for a bit.
Definitively the less popular sibling in that family for now.
And I cackled to myself slightly,
because statisticians are gonna have to be back, aren't they?
Someone's gonna have to figure out,
does this thing in fact work the way that we think it works?
Someone's gonna have to figure out the testing.
Testing is still the most difficult part, especially in generative AI.
And here is a little analogy
that I encourage folks who are not in this business to really internalize.
The data are like the ingredients.
The algorithms are like appliances in a kitchen.
When we were talking about doing applied AI last decade,
we were talking about innovating in models, in recipes.
So creating a croissant that is sugar-free and gluten-free
and dairy-free and delicious.
How do you go about making a recipe like that?
You get a bunch of ingredients, you get the appliances,
and then you tinker, you doubly play, you hope.
You do a bunch of taste testing,
you get your croissant or cookie or whatever it is,
you start being able to produce them at scale, how wonderful.
Now, with the generative AI revolution,
it's equivalent to saying, instead of,
I give all of you the cookie that I've made
and you can eat it or not eat it right here, right now,
and you don't have to worry about where it comes from.
Instead, I'm saying, here, get access to my cookie maker,
the cookies, and take them and turn them into whatever you want.
Anything creative, Valentine's Day, baskets, spray-painted gold,
call it art, whatever you want to do.
But you see that we do lose an interesting layer of control there
because we have a separation in this system
from whatever the dish was to however you're going to use it.
And the users down the line of these products,
you are not told very much about where those ingredients came from,
how they got processed, what their quality was,
what were those appliances, what even went on in that kitchen.
Is it poisonous, isn't it?
But you're going to have to test everything yourself again from scratch
if you're a downstream user, trying to do this
at enterprise scale for tasks that matter.
For some tasks, though, truly, take a thing, spray-painted gold,
what do you care what the ingredients were?
It looks about right, now it's gold
and it still looks about right, everything's good.
But where the ingredients might have mattered,
where the quality might have mattered,
it's up to you to figure out the testing.
And finally, leaving you with the thought of whose job AI
will automate as a consequence.
Yes, it is about engineering and automation,
but as a consequence of these technologies,
what are we going to see?
In the US, what we are seeing is the second quartile
having the most effect, taking the most economic damage
in these technologies, the second quartile
of skill level and income.
And this is puzzling economists.
Why not the people at the very bottom,
the least skilled labor?
And why not the most skilled labor?
Well, I have a guess.
What's happening in the second quartile
is the tasks that are most repetitive and most digitized.
If it's repetitive and digitized, if it's copy-pasting,
if it's doing things on mental screensaver with a computer,
those are the tasks that are most likely to be rendered
no longer necessary for humans to do.
Whereas the difficult skills of having taste,
of being creative, of thinking, of problem solving,
of being a great engineer,
those skills and those jobs are still extremely important,
no matter how good your tools get.
But there is a small economic issue here,
which could become a very big one.
What we take for granted is how our young people
become our senior trusted leaders, artists,
developers, managers, et cetera.
And nobody trusts someone fresh out of university.
I don't know, do you?
So what does that path look like
from newly graduated to senior and trusted?
Often a bunch of easy-to-measure output.
That's pretty repetitive, a bit mindless.
You understand the person's character by working with them.
You see if they go a little outside the box,
you give them very low trust tasks.
Exactly the kind of tasks that are going to get automated here.
And what you will see is that bosses, leaders,
are now going to absorb a lot of the work
that their junior staff members would have done.
Great in the short term.
How are you going to get the next generation of bosses, though?
What is your plan for developing your talent
to a point where you can trust them?
Because the skills that are most likely to get left
are those skills where you need to apply some taste,
where you need to be trusted,
where you need to have judgment and good sense and expertise.
And where, when you look at the code,
where you look at the output, where you look at the art,
you are able to judge whether that's what you're looking for.
If I never learn how to speak Serbian,
how am I going to deal with that email output in Serbian?
How are we going to incentivize me to learn the language,
whether it's a programming one or a human one,
to get to the point where I'm able to supervise
the functioning of these systems rather than do the task myself?
This is a big piece that is missing in our economic plans globally.
And so as you, if you are beginning to automate
with these systems in your organizations,
as you're taking responsibility for this,
please pay attention to the three big topics from this talk.
One, data quality matters, and today it's nobody's job.
That's a problem.
Two, testing these things is difficult,
especially when we're talking about generative AI at scale
without humans in the loop.
And three, it's going to be up to you to create that plan
for how you're going to be able to train your staff
and not leave anyone out,
because your most valued staff members,
the ones you really do want to have around,
often need that path of going through a gauntlet
that these tools may render less attractive in the short term.
And that's why we have a term like secret cyborgs in America,
so people who want to use these tools,
they don't want their managers to know.
That should be a warning.
It should be a warning that we don't trust managers
to handle that transition fairly, carefully, and responsibly.
And so how do we in this room think about championing,
a gentle and also effective and intelligent
way through the challenges of today
to get to a highly productive economy and workforce tomorrow?
Thank you very much.
Thank you.

OK, so what I want to talk about today
is using video games to reverse engineer human intelligence.
And this is a work in collaboration
with a bunch of people, particularly
Pedro and Sovetis, who was until recently a postdoc at MIT.
So let me start with a seductive hypothesis, which I'm sure
all of you have heard before.
So on the one hand, we have deep learning systems
that are vaguely brain-like.
And then those same systems can achieve human-level performance
on various tasks, like image recognition.
So if we put these two things together,
it's seductive to think that we might have a recipe
for human intelligence.
And of course, that idea has not been
lost on a number of people and corporations.
So is that really true?
And what I want to do is focus on Atari,
since that was one of the early big wins for deep learning,
particularly this 2015 DeepMind paper.
And as I'm sure all of you know, this
was the origin of this deep Q learning
network, the DQN, which is basically
in Comnet that controls a joystick
and maps from pixels to joystick actions.
And this is from the original paper.
Since then, they've pushed the barrier down much further.
So basically, I think we know how
to play Atari games successfully, at least human-level
performance, with deep learning systems, kind of like this.
But there are a lot of different variations of this idea.
But I'll try to clarify in a moment
what exactly is meant by play video games,
because this is a crucial point, play them like humans.
So really what we're talking about here
is we can achieve human-level scores or exceed human-level
scores with some amount of training, which remains
unspecified.
OK, so is this how humans learn?
And I want to focus on two properties of human intelligence
and human learning in particular,
that we can rapidly learn from few examples,
and then we can generalize flexibly.
And the argument here is that these properties are not
yet fully captured by deep learning systems.
That doesn't mean that they can't be captured,
but we have to think carefully about, first of all,
understanding empirically, what is it
that humans are actually doing, characterizing the behavior,
and then thinking about the information processing
capabilities that enabled that performance.
OK, so it's instructive here to focus
on what a DQN does really well and what the DQN doesn't do
so well with.
So the kinds of games that DQN does really well with
are things like Breakout and Star Gunner.
These are games that are kind of fast paced
and require a lot of sensory-motor coordination,
but not particularly challenging in terms
of a kind of conceptual or analysis of the problem
or some kind of long-range planning.
Whereas down here, games like Frostbite and Montezuma's
Revenge are really hard to do successfully
without understanding something about how the game works.
And when I say do successfully, I
don't mean just that if you give the network enough training,
it will eventually solve the problem,
but rather that you could give it the amount of training
that humans need to solve the problem.
So let me just familiarize you for a second with Frostbite.
And I'll refer to what we call the Frostbite challenge.
So here's Frostbite Bailey.
He's jumping on these ice floes to build this heat glue.
And meanwhile, he's trying to avoid these birds.
And once he's built the eagle, he can hop back up there
and go through the hole, go through the entrance rather.
And on later levels, there are other things
that come into play, like other kinds of animals and so on.
So the Frostbite challenge is the following.
So it consists of three stages.
One is reaching basic human level performance.
The second is can we reach human level performance
as quickly as people do?
And the third is can we perform new tasks or goals
with little or no retraining?
And the argument here is that really what deep learning
has been successful at this point
is reaching basic human level performance.
In a sense, it's kind of ironic that the cover of nature,
the issue where this paper was published,
had the words learning curve on the cover.
Because arguably, the learning curve
is actually the least compelling part
of this whole demonstration.
And this is all due credit to the amazing accomplishment
of this network.
But I just want to be clear about what
is the nature of success here.
It's about asymptotic performance, not
about sample complexity.
So here's human level performance up here, roughly speaking.
And here's DQN's performance on Frostbite
after 800 plus hours of gameplay.
Now, how fast do humans actually get
to this level of performance?
Well, basically that fast.
It's so fast that at the scale of hundreds of hours,
it looks like just a bullet.
And you can get better sample complexity
using other variations of DQNs.
Some things get kind of tantalizingly close.
And of course, this is a little bit outdated.
So there are algorithms that are even better than this.
And we've compared to some of them.
Now, if we zoom in here, even this opt tightening algorithm,
which does pretty well, we're still really looking
at sample complexity that's much slower than humans.
Now, hopefully you guys have all ascertained a weakness
in this sort of argument, which is commonly brought up
when we compare human and machine sample complexity,
which is that the DQN and all deep learning systems
are basically trained from scratch on a single game.
So it has to learn its entire visual system
and everything else, its value functions and so on,
from training on this game.
And we're counting that in the amount of experience,
whereas humans come into the game with not just
their own developmental history, but also
however many millions of years of evolutionary history
behind them that have shaped their visual system
and their inductive biases about value functions
and so on.
So there's an alternative hypothesis,
which is that actually humans are not learning faster
than the DQN or experience, but rather that they're just
at a different point on the same learning curve.
So how do we compare with this argument
that they're basically on the same learning curve,
but just different points?
That would be a kind of deflationary argument.
And what we've done to study this
is computed what we call the performance-matched learning
rate.
And the way this works is you pick a performance level,
let's say a human performance level,
and then you find the point on the machine learning curve
at which they've achieved the same performance level,
and you look at the slope of the learning curve
at that point.
So if they're on the same learning curve,
assuming this learning curve is monotonic,
then the slopes should be the same.
Does that make sense to everybody?
So by matching for performance, we're
checking whether they're on the same learning curve.
So here are the slopes for a bunch of different games
for both humans and de-qn.
These are experiments that we ran.
And notice that this is the learning rate.
This is the slope of the learning curve
at different performance levels.
And this is on a log scale.
So we're looking at really orders of magnitude differences
in humans.
And humans are always learning much faster
than, in this case, the double de-qn.
And the story doesn't really change significantly
if you look at other learning architectures.
And by the way, please jump in if you have any questions
or comments.
All right, I'll keep going.
All right, so what does this mean?
What are actually humans doing that's
ostensibly different?
And the argument I want to make is
that from the very beginning of play,
people see objects, agents, and physics.
They actively explore object-relational goals.
And then eventually, they come up with multi-step plans
to exploit what they have learned.
So here in Plain Frostbite, they first
have to figure out sort of what is there on the screen.
And then exploring what happens when you interact
with different objects.
You eventually figure out that you're building an e-glue.
You later have obstacles on different levels,
like there's a polar bear and so on.
And I'll come back to a more specific story about what
we think humans are doing in a moment.
Let me just give you a little bit more data.
This is not meant to be a really comprehensive series
of studies about how people play Atari,
but it will give you a sense of what kind of constraints
we need to place on a learning system.
So here's an example of one-shot or few-shot learning
about harmful actions and outcomes.
So if you collide with a bird, that's a bad event for the agent.
And you can see that the agent only
has to collide with the bird maybe zero or really
at most three times, usually less,
in order to never collide with the bird again.
So it really just takes a few interactions with the bird
to learn to avoid it.
And I want to point out that this already tells you
something about the kind of object-oriented nature
of representations that we think people have.
Because how do you specifically avoid a bird
if you don't have a representation of a bird?
You need to have something in your learning system
that represents bird so that you can learn specifically
about it.
Now, you might ask the question of questions.
When you said agent bird collisions,
though, that was human players.
Is this?
Yes, this is human players.
Yeah, so if you run a DQN on this,
it will definitely not look like this at all.
And I'll show you some data on this later.
OK.
Now, you might say, well, humans actually
have a bunch of extra knowledge about what's going on here.
They know about eagles and ice floes and birds and so on.
Now, putting aside for a second the fact
that the way that birds and eagles work in frostbite
is quite different from how they work in the real world.
And many people don't really have firsthand experience
with eagles or bears.
We could ask, all right, well, what happens
if we try to eliminate that source of knowledge?
So what we can do is just blur the screen.
And that really doesn't make any difference
to learning curves.
OK, so the last little data point
that I want to show you is learning from observation.
So if you watch an expert player,
like we watched that video just now,
notice that the agent never interacted with the birds.
So we have no explicit data about what
happens when you interact with birds.
But if you notice that if you know
that there's an expert player moving around
and they never contact birds, that's not a coincidence.
The player must know something about birds, namely,
that they're bad and you don't want to collide with them.
So that's learning from negative evidence.
Much like when we learn language, there
are lots of ungram, there are an infinite number
of ungrammatical sentences that nobody ever says,
but we learn that one shouldn't say
those ungrammatical sentences.
And if you give people a little bit of observation,
just two minutes about observing an expert player,
they can use that knowledge to accelerate their learning.
And that's shown here.
And you can see this also if you compare
this few shot or one shot learning
between just playing by yourself versus playing by yourself
plus watching an expert for two minutes,
you can see that now the modal number of agent bird collisions
in the first episode is zero.
So now people have successfully learned
that you shouldn't interact with birds
and they employ that knowledge.
So now it's essentially zero shot learning
after some observation.
We did also a kind of funny experiment
where we basically asked, what happens
if we just show them the instructions for frostbite?
Can they learn from verbal instructions?
Now, in some sense, this has to be true, right?
And indeed, it is true.
But just think about what this means for DQN agents, right?
Like most model-free reinforcement agents or even
most model-based reinforcement learning agents
don't know how to read, right?
They don't know how to interpret an instruction manual.
So the fact that we can actually use that to aid our planning
says something about the nature of representations.
Like if you had, in order to understand
these kinds of instructions, minimally
you need some kind of relational object-oriented
representation of the game, right?
Because the instructions are relational and object-oriented.
They tell you what happens when you collide
with a particular agent, for example.
OK, so this just summarizes the basic results.
Blurring doesn't affect your performance,
but instruction and observation improve your performance.
OK, so what about performing new tasks or goals
with little or no retraining?
And here I'm just going to appeal to a kind of thought
experiment.
So let's imagine some goals, alternative goals,
that I could give you after playing cross-bite.
Get the lowest possible score, or get
closest to 100 or 300 or 1,000 or 3,000 or any level
without going over, or be your friend who's playing next to you,
but just barely, not by too much, so as not to embarrass them.
Go as long as you can without dying.
Die as quickly as you can.
Pass each level at the last possible minute
right before the temperature timer hits zero and you die.
And you can keep going.
You can invent a ton of these kinds of alternative goals.
And the conjecture here is that people
can do these alternative goals.
They can play a version of cross-bite
where the game itself is unchanged,
but the goal is different with relatively little training
that we can somehow transfer our knowledge from one game
to another when just the goals have changed.
And that's quite different from most model free agents
where the value function is basically,
and hence the goals are basically
baked into the parameters of the network.
All right, so I've given you some motivation
for why we need to think a little bit differently about how
human solve these kinds of reinforced learning tasks.
And now I want to take more seriously
the question of how we would actually
build agents that learn in human-like ways.
So the argument at a high level is
that humans don't just do pattern recognition
and function approximations.
They're learning something like a theory of the game.
And these theories support rapid learning, efficient planning,
and flexible generalization.
So the perspective that I'm trying
to argue in favor of here is what we call theory-based
reinforcement learning.
You could think of it as a kind of model-based reinforcement
learning, but where there's fairly strong constraints
on the nature of those models.
And we built a specific agent that
embodies these principles.
And I should say that it's more of a modeling framework
because there are various aspects of this
that we were trying to improve and make more scalable
and so on.
So there are lots of ways that you could actually
instantiate these ideas.
And it consists of basically three steps.
So one is a perception step that's
going from pixels to symbolic description.
And the results that I'm going to show you,
we've mostly just been operating on the symbolic description
and we're actively working on the perception step.
Then from these symbolic descriptions,
we need to do some kind of theory induction
to learn basically the rules of the game.
And then the theory is used for planning and exploration.
And here we use a kind of Monte Carlo tree
search where the theory provides a simulator
and then we have exploration bonuses that are
object-oriented and relational.
So we can say things like, I don't
know what happens when I collide with that crap,
so I'm going to go and try it out and see what happens.
We're going to add an exploration bonus for that event.
And for theory language, we've used a kind of lightweight
language for video games that was developed by Tom Schall,
I think basically as a weekend project.
It's called the Video Game Description Language or BGDL.
And here's what it consists of.
So there's a sprite set, so those
are the different sprites in the game.
There's a level mapping that tells you
what the different levels are.
Each level has a corresponding map.
There's an interaction set.
This is really kind of the crucial body of the theory.
So what happens when you interact with different agents
or different agents interact with each other
or different objects interact with each other.
And then termination set.
These are the termination conditions.
So we can actually render these games
and make them playable.
They look like this.
We can have fancier graphics, but for the purposes
of our human experiments, we wanted
to strip away all the semantics that
would be familiar to people and just have colored squares.
So let me give you a sense of how
this theory-based reinforcement learning
works with a bit richer set of graphics.
So let me just orient you here.
So here's the avatar.
And these white boxes with question marks on them
indicate things that the agent doesn't really
know much about and wants to learn more about.
The pink arrows show the agent's current understanding
of the game dynamics, so the interaction set.
And then these yellow arrows show possible plans
that the agent is considering.
So initially, the agent maybe knows a little bit
about the direction of travel of these cars,
but it still needs to learn about some of them.
And it really has no clue what's going on over here
with these logs on the river or what this thing is.
Now, by mid-stage, it learns that, all right,
if I interact with these cars, I'm going to die.
So I want to go around them.
Now I need to learn about water and logs.
I've also learned something about the dynamics of these logs.
And then eventually, I learned that, OK,
I want to avoid the water and jump over the logs
and then figure out what this thing is, which turns out
to be a goal.
And one of the cool things that you can do with this
is generalize it to other environments
where you may have overlap with some of the previous sets
of objects.
So you still have water and logs and so on.
And so you could use that knowledge
to chart a path to the goal.
But you may also want to figure out what this thing is
over here and interact with it.
Let me show you some examples of games
that we've had people play.
So this is a game where an agent wins
by making all blue disappear by pushing blue into yellow.
And then you can kind of ratchet up the complexity.
Now, touching red turns red into yellow.
And now, in this third iteration,
pushing orange into purple makes orange disappear
and turns purple into yellow.
So you basically have to learn a whole chain of relations
in order to win the game and so on.
So this is what the model looks like when it plays this game.
So the thing I want to draw your attention to
is that the model is not trying lots of random actions.
It's specifically interacting with objects
and pushing blocks into each other to see what happens.
So that's a critical aspect of how the model plays the game.
Here's humans playing the game.
And you can see that there's a lot of similarities here.
The human is pushing the blocks around,
interacting with them, seeing what happens when you push one
block into another until it figures out what it needs to do.
So the agent is not seeing the actual pixel observations.
It's seeing the symbolic state representation.
That's right.
Yeah, for now at least.
We hope to eventually solve that problem.
Actually, it's not that hard to solve that problem
for these kinds of games, but we've
been trying to solve it for the harder problem of Atari games
where Atari games, many of them can't even
be literally described in a VGDL formalism.
So one of the things that we've been trying to establish
is how robust is this framework to violations
of the modeling assumptions?
And then also, of course, trying to apply to cases
where we don't have those symbolic descriptions available
and then we have to infer them from pixels.
Atari turns out to be kind of a nightmare
because I don't know how many of you guys have worked on Atari.
But this is less of an issue for a DQN type agent.
But for these kinds of agents, there's
all sorts of weird quirks with Atari,
like partial occlusion kind of randomly.
When you, I forget what game it is,
where you can push a paddle down and it eventually
gets partially occluded for no apparent reason.
But that totally screws up a system like EMPA
because now it's like the visual system we've equipped it
with doesn't understand about occlusion.
So essentially, it forces us to solve various problems
that are kind of auxiliary to the things
that we're really trying to focus on, like solving
occlusion.
And that's basically what we're struggling with now.
I guess if you have a memory, more of a memory component,
then you might be able to.
Yeah, so there's all sorts of hacks
that you could do to basically make it more robust.
And that's sort of what we're doing now.
But one hack makes one thing work and breaks some other things.
So it really ends up being a bit, unfortunately, complex.
I guess on that topic, trying to solve this perception
of the problem, do you think, I mean,
I guess you must think this if you're doing a research,
but do you think at some point you'll
be able to kind of solve that to a satisfactory level?
Or what's the upside of, even if you can solve it
for Atari games, you'll probably go through a lot of pain
as you already described, probably more.
And then for some new environment,
it's probably going to be a whole new type of pain.
Yeah, that's right.
How do you think about it?
I think this point, I mean, this problem is sort of as old
as the 1956 Dartmouth summer school,
where everyone thought that they just assigned vision
as a problem to the grad students for the summer.
And I guess that methodologically, the issue here
is how do you make progress on certain aspects of the problem
without having to solve all the other problems of intelligence?
Because it's our feeling that the successes of confidence
and things like them are often kind of successes of working
with, like what's so great about them
is that they can operate on low-level sensory data
and eventually learn something useful.
But the problem is that in doing so, in the particular way
that they solve those problems, they
don't acquire the kinds of representations
that allow them to generalize flexibly
and learn quickly the way that humans do
because basically their inductive bias is much too weak,
or they don't have the right sorts of inductive biases.
And this is not going to be something
that will be solved with putting Gaussian priors on the weights
and things like that.
It's a structured inductive bias that people have.
So the question here is, is there a way
that we can carve out a problem where
we can study the kind of more conceptual issues
in human learning, like learning the rules of the game,
if that's indeed what people are doing,
without having to solve all the problems of vision?
And I think we have made progress on that problem
if we just operate on symbolic descriptions.
So if you accept the premise that something
in the visual system provides symbolic descriptions of scenes,
and I think you can make that argument for the human visual
system, then we've made progress on that question, right?
But it still remains unsolved how, in a very generic way,
to go from the sensory data to some kind
of symbolic description.
And I guess you're assuming access to at least when
you do this initial tackling, this initial version
of the perception to symbolic formulation.
You're also, I guess, it's more like a grounding problem
because you're also assuming that there's
in the existing bank of relations
that you just need to ground to, or are you also
learning the actual binary operators
in first order logic, like the propositions?
Are you also learning the propositions from scratch?
Yeah, so well, I'm assuming that there's
some library of relations and objects,
and you're basically learning about what happens.
Which relations apply to which particular objects?
Got it.
So now, of course, it's important that we have a system
eventually that can extend that library,
and that's another place in which I think
that that's a more straightforward extension
of what we've done here.
But what I'm showing you now, we're not assuming it.
I think we can kind of all agree that when
we look at a picture like this, it's pretty straightforward
like what is where.
There's a bunch of colored squares,
and you know where they are.
That's not the hard problem.
And also, you can easily recognize when one object has
collided with another object.
So there's some basic primitive spatial relations, which
are just obvious to us.
But what requires learning, which is a much subtler learning
problem, is to actually learn the structure of those rules.
And that's much easier for a system
that has explicit relational representations defined
over objects compared to a system that is just
a feedforward neural network.
Yeah.
I guess in situations where you have more complex games,
where it's not just binary propositions,
but it might require n area, where n is really large.
He might have like a dozen conditions that
have to be satisfied for a particular result.
Then that might be really hard.
I mean, that would be really hard for a system like this
to learn.
Well, can you give me an example of what you're thinking of?
I guess one would just be like, if you think of civilization
type games, like a tech tree, and then
I guess without specific access to the tech tree
map that the game gives you, that would
a type of situation like that, where I can only
build this factory if I built this entire tree of previous
dependencies.
Right.
So I guess one way to approach that
is to say that there's some background state.
But of course, you don't want to basically try
to build a giant lookup table in that case.
So yeah, I think that you're right,
that raises some interesting challenges here.
I think that actually I'm going to come back at the end
and say how I think deep learning can actually
be useful kind of adjunct to the kind of framework
that I'm describing, precisely to solve
these kinds of problems.
All right, so let me just show you the learning curves here.
This is actually a case where the model
learns quite a bit faster than humans.
I'll show you some other examples where humans learn
about the same or if not faster than the model.
Here's a game where the agent has to pick up all the pink boxes
to win, and the chaser, which is green,
tries to pick up the yellow box, which is the termination
condition.
And there's this purple fence that the agent can go through
and the yellow can be pushed through that the chaser can't pass.
And so the agent can learn to take advantage of this.
So let me show you what this looks like.
OK.
So here, the chaser got the yellow thing.
So now this time, he's a little bit smarter.
He pushes it all the way to the pink.
He can do that again under a different circumstance.
Here it gets a little bit harder because now there's more pink
squares.
Here's a little bit too slow at a time.
But if you can get it the next time,
OK.
Now here, it gets really hard.
How on earth is he supposed to solve this one?
And the answer is that if he tries to do it that way,
it's just not going to work.
But eventually, he figures out that what you need to do
is push it into the fence.
So now that chaser is trying to get to the fence, but he can't.
And he's basically stuck there.
And you can just go and collect all the pink boxes.
So this is an example where the model and human learning
curves look quite similar.
We've built 90 different games generated using BGDL
and compared it to a variant of the DQN.
Compared to DQN.
And you can see overall, EMPA is doing much better than DQN
and also much closer to what humans are actually doing,
which is the stash line.
Here are the learning curves.
Here are the learning curves.
So you can see that sometimes humans
are doing a bit better than EMPA.
Sometimes they're about the same.
But in every case, the DQN is basically
not doing well at all.
Basically flat for a lot of these learning curves.
Another informative thing we can look at
are these object interactions.
So I've divided them here into positive, neutral,
and negative interactions.
So the bird interaction frostbite
would be an example of negative interaction.
You can see that both EMPA and humans,
almost all of their interactions are
with positive valence objects.
Whereas DQN, it's not like it's interacting a lot
with negative objects.
It's just spending the vast majority of its time
interacting with neutral objects,
like just bumping into walls.
So that's another qualitative way in which these models
are different.
Another thing you can look at is a kind
of learning-to-learn measure where
we look at the number of steps it takes to win level two,
conditional on having one level one.
And you can see that both humans and EMPA take very few steps
to win the second level after having won the first level.
Whereas DQN, if it gets to level two at all,
takes quite a lot longer to win level two.
But even more tellingly, it often just
doesn't get to level two at all.
Another interesting qualitative feature of the data
has to do with exploration.
So if you look at the patterns of exploration in humans,
and you could see this also in the videos that I showed you
before, the patterns of human exploration
are very object-oriented and selective.
So this is a heat map of the different paths
that human agents took.
And you can see that they really just
zoom in on particular objects that they need to learn about
and don't interact at all with the other objects.
And then in a second level, which is shown on the right here,
they really select to go for a single object
that they need to learn about.
And EMPA does something qualitatively similar.
Whereas if you look at the DQN, this
is what I was talking about before with just bumping
into a wall slot, you can see the heat map is super diffuse.
They're just kind of wandering around a lot.
And we attribute that largely to the fact
that the DQN isn't object-oriented and relational.
And so it has no way of kind of guiding its exploration
towards specific objects or relations.
OK, so to take stock of what I've shown you,
a theory-based RL agent can play games in human-like ways.
So that means not just asymptotically,
but the same performance.
Like they're actually playing it in a similar way, right?
If we have to kind of go beyond just looking at score metrics
in order to gain this insight.
And I've argued that object-oriented relational
representation is key combined with the theory induction
algorithm for sample efficient learning.
So what is the model and model-based reinforcement
learning?
This is actually kind of more for psychologists
and neuroscientists who have spent a lot of time
studying model-based reinforcement learning,
including myself, but often in very simple tasks
where we could represent it in a kind of tabular form.
So a lot of our understanding of model-based reinforcement
learning in the brain comes from these fairly simplistic
tabular assumptions.
And what we're trying to do now is kind of go beyond that
and look at more theory-based representations that
are non-tabular, that have more structure.
OK, so what about deep learning?
I actually think that deep learning
will be critical to making these kinds of approaches
scalable, which currently they basically aren't.
So every step in that architecture that I showed you
could benefit from the pattern recognition and function
approximation abilities of deep learning systems.
For example, learning fast pixel-to-symbol mappings,
finding good theories quickly using neural program
search, using neural value approximations
as heuristics guide planning like in AlphaGo.
So I think all these things can and should be explored.
So to wrap up, I've argued that human video game learning
should be better thought as theory building
as opposed to a kind of pattern recognition.
And by using theory induction methods
in a simple but rich description language for games,
we can do better compared to current state-of-the-art systems.
But there's still much work to be done.
With that, I'll stop here.
Thank you.

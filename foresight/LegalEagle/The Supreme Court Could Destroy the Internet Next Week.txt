This case has the potential to end the internet as we know it.
The internet flourishes on user-generated content. It's the backbone of the internet.
And if you write or say something that is illegal, you can be held liable for what you said.
But usually the websites that host your content are not liable.
And when you hear about Section 230, this is usually what we're talking about.
But if websites were also liable for what users say,
then websites would either shut down almost everything or police things so tightly
that there would be basically nothing even remotely objectionable.
Because if there's one thing that websites hate,
it's being on the hook for billions and billions of dollars of potential liability and attorney's fees.
And this isn't just some potential hypothetical.
In the past, when laws made websites liable for user content,
they just shut down the content and never came back.
And one case before the Supreme Court this term seeks to do exactly that,
but for all of the internet.
No joke, this has the potential to end the internet as we know it today.
So how did we get here?
Well, it started with a tragedy.
Noemi Gonzalez was a design major at California State University Long Beach,
who was spending a semester studying in Paris.
On the evening of November 15, 2015,
she was dining with friends at a cafe on the Champs-Elysees,
when an ISIS terrorist with a gun opened fire.
Gonzalez was killed along with 130 other people
in coordinated attacks at six locations across the city.
ISIS claimed responsibility for the attacks,
and eventually 20 men were convicted of crimes related to the massacre.
The lone surviving gunman was sentenced to life without parole.
So was justice served?
Well, the Gonzalez family contends that it was not,
because there was another perpetrator still at large.
YouTube.
The family sued Google, which owns YouTube,
alleging that YouTube's algorithms amplify violent videos and hateful content,
despite the company's efforts to ban violent accounts and limit their reach.
The Gonzalez family says that YouTube's recommendations
expose people to hateful content, radicalize viewers,
and encourage them to make terrorist attacks of their own.
Google argued that because ISIS, not YouTube, made and uploaded those videos,
YouTube was not the provider or developer of those videos,
and therefore it was immune from liability under 47 USC Section 230,
otherwise known as Section 230 of the Communications Decency Act.
Section 230 of the CDA immunizes internet services
for the content that its users upload,
and often allows websites to remove content without taking on liability.
But the Gonzalez plaintiffs contend that YouTube's recommendations
should not be covered by Section 230 because the company is acting like a content creator
rather than a publisher.
The Ninth Circuit Court of Appeals held that the Gonzalez claims
fell within Section 230 and Google was immune from suit.
The Gonzalez parties appealed and the Supreme Court granted cert
to answer the question of whether Section 230
immunizes an interactive computer service from liability for recommending other party content.
Now in support of their claims, plaintiffs alleged the quote
two of the 12 ISIS terrorists who carried out the attacks
used online social media platforms to post links to ISIS recruitment YouTube videos
and quote jihadi YouTube videos.
One of the men who fired at Gonzalez had appeared in an ISIS recruiting video in 2014.
Now the plaintiffs argue that the defendants violated the Anti-Terrorism Act, the ATA,
by allowing ISIS to post videos on YouTube under Section 2333 of the ATA,
as amended by the Justice Against Sponsors of Terrorism Act.
Americans who are injured by quote an act of international terrorism
that is committed, planned, or authorized by a terrorist organization
may sue any person who quote aides and abets by knowingly providing substantial assistance
or who conspires with a person who committed such an act of international terrorism
and recover trouble damages.
So the legal question in the lower courts was whether a social media platform
that was not used to commit a specific quote active international terrorism
may still be liable for aiding and abetting under Section 2333.
The plaintiffs say that the answer is yes,
that Google aided and abetted an act of international terrorism
conspired with the perpetrator of an act of international terrorism
and provided material support to ISIS by allowing ISIS to use YouTube.
And of course the defendants say no,
the plaintiff's claims are barred by Section 230 of the Communications Decency Act
which often immunizes interactive computer services, aka websites,
even when they make targeted recommendations.
The trial court and the Ninth Circuit Court of Appeals
sided with Google, ruling that the claims fell within Section 230 Section C
because ISIS, not YouTube, created or developed the relevant content.
The court said that YouTube quote selects the particular content
provided to a user based on that user's inputs.
The display of recommended content results from algorithms that are merely quote tools
meant to facilitate the communication and content of others
and not content in and of themselves.
As the lower courts noted,
the recommendations are based on the user's preferences, not YouTube's.
The plaintiffs argue that Section 230C does not apply to quote
activities that promote or recommend content.
So let's dig deeper into what all of this means.
First of all, the plaintiffs are not simply arguing that platforms are liable
because terrorists have used their services.
They argue that the powerful algorithms that recommend content
are not covered by the CDA because the recommendations are content in and of themselves.
And as the Ninth Circuit said,
the complaint alleges Google uses computer algorithms
to match and suggest content to users based upon their viewing history.
The Gonzalez plaintiffs alleged that in this way,
Google has recommended ISIS videos to users and enabled users to locate other videos and accounts
related to ISIS.
And that by doing so, Google assists ISIS in spreading its message.
So the plaintiffs claim that YouTube facilitates
communications between ISIS and people who watch their videos,
thereby aiding ISIS in recruiting new members.
The plaintiffs acknowledge that YouTube removed ISIS videos and suspended
or blocked ISIS users at various times.
However, the plaintiffs claim that YouTube should have been able to stop ISIS from
reestablishing accounts using new identities.
And the plaintiffs alleged that Google left some of the ISIS videos up
because they did not contain content violating the site's policies.
At other times, Google removed the offending content,
but did not suspend or ban the accounts.
Now, we've talked about Section 230 of the CDA many times on this channel.
Section 230 was created back in 1996 at the dawn of the Internet.
It has two key provisions.
Section 230c1 says, quote,
No provider or user of an interactive computer service
shall be treated as the publisher or speaker of any information provided by
another information content provider.
This sentence is often called the 26 words that created the Internet.
This section stipulates that providing access to third-party content
does not make an online provider the publisher or speaker of that content.
Now, the second major part of the law is Section 230c2,
the so-called Good Samaritan provision,
which states, quote,
No provider or user of an interactive computer service
shall be held liable on account of any action voluntarily taken in good faith
to restrict access to or availability of material
that the provider or user considers to be obscene, lewd, lascivious,
filthy, excessively violent, harassing, or otherwise objectionable,
whether or not such material is constitutionally protected.
So this provision gives online platforms broad immunity from liability
if they moderate content in good faith.
This provision was meant to allow websites to police itself
without incurring new liability because it was an issue of knowledge.
A website might not know what speech was hosted on that particular website,
in which case they wouldn't be liable for it,
but if they knew that illegal speech or defamatory speech
or some other kind of speech was hosted there
and then took steps to remove it from the website itself,
then they actively had knowledge of that particular speech
and thus might become liable for hosting it in the future.
So websites found themselves on the horns of a dilemma
until section 230 of the CDA was passed
to allow them to engage in good faith moderation
because there was generally a knowledge requirement
to be liable for things that were hosted on your website.
If you had no knowledge of what kind of speech
was on your website or platform,
then you generally weren't liable for it.
But the argument went that if you started to moderate your website,
then that was evidence that you knew what was on there
and therefore you became liable for all the stuff
that you were trying to remove from your website.
And this applied to spam, to pornography,
to pirated media, to defamation, you name it.
So the CDA was enacted in 1996
when the internet was still in its infancy.
At the time, companies like America Online,
CoffeeServe and Prodigy were the portals
that allowed people to access the internet.
People could share files, exchange messages,
and chat with each other in real time.
But that posed a dilemma.
Who would be liable if a user posted something defamatory
or shared something else that was illegal?
Would liability go beyond the person
that was responsible for the post?
Or would the internet companies
who hosted the content also be on the hook?
And that brings us to the distinction
between content publishers, distributors, and platforms,
which is often completely misunderstood.
In the pre-internet age,
publishers, distributors, and platforms
were treated differently under the law.
Publishers were newspapers, magazines, and broadcast stations.
They were generally liable for republishing material
from third parties
since they solicited the things
that they published and they could vet those materials.
Distributors were businesses like bookstores,
newsstands, and libraries,
which distributed material that was printed by others.
And distributors were not required
to assess every book that they sold.
For example, in the 1950s,
the Supreme Court overturned a Los Angeles ordinance that said,
if you have obscene material in your bookstore,
you can be held criminally responsible.
And in that case, a bookstore owner
was convicted of violating the ordinance
by selling a novel called Sweeter Than Life,
which tells the story of a, quote,
ruthless lesbian businesswoman.
The Supreme Court said that the bookstore
couldn't be responsible
for reviewing every single thing
in the book or magazine that it sells.
And distributors only had liability
if someone notified them
that something they carried was illegal.
That's sort of the origin of the knowledge requirement.
And platforms were common carriers
like phone companies and television broadcasters.
Platforms weren't liable for third-party content
that they carried.
So, for example, let's say AT&T found out
that someone's answering machine
had a libelous outgoing message.
Was AT&T required to act
by canceling the owner's telephone service?
Well, the court said no.
AT&T couldn't be sued for libel
simply because the owner used its service
to say something libelous.
And similarly, a broadcaster wasn't liable
for running a political candidate
that falsely accused someone of being a communist.
But then the internet came around,
and the internet era posed a new challenge
for this case law.
Two of the first internet service providers,
CompuServe and Prodigy,
were sued for hosting forums
where users posted defamatory content.
Now, Prodigy billed itself
as a family-friendly version of the internet.
Did you get a computer recently?
Well, congratulations.
You can now join hundreds of thousands
of discovered Prodigy.
It moderated comments,
removing things it thought were bad.
Now, CompuServe didn't moderate anything.
CompuServe combines the power of your computer
with the convenience of your telephone.
And both of those companies were sued
for defamatory statements
that their users posted to the service.
Now, the case against CompuServe was dismissed
because the court considered it a distributor of content
rather than a publisher.
CompuServe could only be held liable for defamation
if it knew or had reason to know
of the defamatory nature of the content.
And since they didn't do anything
to moderate their forums,
the company couldn't possibly know,
or at least argued it couldn't possibly know,
about what the denizens of Rumorville
were posting about each other.
But Prodigy was found liable
for failing to moderate enough.
Someone went on a forum
and claimed that Jordan Belfort's investment firm,
Stratton Oakmont,
committed fraud in connection with a stock IPO.
The firm sued Prodigy
and the anonymous poster for defamation.
And a court held that Prodigy was liable
as the publisher of the content
created by its users
since it exercised editorial control
over the messages on its bulletin board.
Now, if the names Jordan Belfort
and Stratton Oakmont ring a bell,
that's because they were literally scamming people
out of millions of dollars
and have gone down as some of the biggest fraudsters
in all of Wall Street history.
The movie The Wolf of Wall Street
is based on the fraud
that they were perpetrating
with respect to the IPOs.
And it's one of the biggest ironies
in all of First Amendment law
that they won this lawsuit
against the people that were hosting
presumably accurate information
about how they were scamming people out of money.
That's part of the problem
when websites are liable for user content
is that people can file lawsuits to stifle speech.
That's what a slap lawsuit is all about.
And because lawsuits are incredibly expensive,
sometimes even if you scammed people out of money,
as long as you file a lawsuit,
you can stifle that kind of speech.
But I digress.
As a result of these cases,
internet service providers
basically either decided to not moderate at all
and leave absolutely everything up
or basically moderate everything
and not allow anything to go up.
And basically no one was happy
with this state of affairs.
So Congress enacted the Communications Decency Act.
And then as now, lawmakers claim
to be especially worried
about things like harassment and obscene material.
So they gave online providers immunity
from lawsuits if they moderated content.
And again, the irony here
is that this legislation came
from the sort of family values type politicians,
the people who always want to think about the children.
Oh, won't somebody please think of the children?
They wanted to allow websites
to remove violent and sexual conducts
so that the internet could be even cleaner.
The same politicians who wanted
and got the little parental advisory
explicit content warnings put on rap albums.
But I digress again.
Section 230 was intended as a way
for internet companies to create
and enforce basic standards to run their websites.
And when the CompuServe and Prodigy cases were decided,
the courts only considered whether the ISPs
were publishers or distributors.
But an internet service like a website
didn't fit neatly into those categories.
A traditional publisher has total control
over whether to publish content.
A distributor can control what it buys,
but it wouldn't have been practical
for a human to fully screen every item.
Modern social media companies host more material
than any library or bookstore on earth,
which makes screening all of that info daunting,
if not impossible.
And without Section 230,
social media companies would be subject
to strict liability for every message and post
made on their services.
So Congress chose to give those platforms and websites
immunity as an incentive
to get them to remove offending speech.
Congress also chose not to dictate
to those companies which speech they had to remove.
So that brings us back to the original question in this case.
How should courts read Section 230?
The lower courts follow the tests set forth
in the Ninth Circuit case called Barnes vs. Yahoo.
And the Barnes test hues close to the text of Section 230.
The law specifies that only one,
no interactive computer service,
shall be two, treated as a publisher or speaker,
of three, content provided by another information
content provider, aka users.
Now the plaintiffs argue their claims
do not treat Google as a publisher,
but instead assert a simple duty not to support terrorists.
They said that the ATA prohibits Walmart from supplying
fertilizer, knives, or other material to ISIS.
And in the same way, the ATA bars Google
from giving ISIS a platform to communicate.
The Ninth Circuit found this analogy specious.
The idea of a duty not to support terrorists,
quote, overlooks that publication itself
is the form of support Google allegedly provided to ISIS.
And publication of third party content
is what Section 230 gives to internet services.
According to lower courts, publishing encompasses,
quote, any activity that can be boiled
down to deciding whether to exclude material
that third parties seek to post online.
Google says that the CDA uses the words
publisher or speaker in an ordinary sense,
quote, based on an ordinary meaning,
a publisher or speaker is one that publishes or speaks.
Google contends that when YouTube's algorithms
make recommendations, this activity is protected by Section 230,
quote, Congress underscored that publishing
for purposes of Section 230 includes sorting content
via algorithms by defining interactive computer service
to include tools that pick, choose, filter, search,
subset, organize, or reorganize content.
Congress intended to provide protection for these functions
not for simply hosting third party content.
And the plaintiffs disagree.
Their interpretation of the Section 230C defense
is that it requires a narrower interpretation
of the word publisher.
The Ninth Circuit previously held that Section 230C1
uses publisher in its everyday sense
and the Second Circuit agreed.
But the plaintiffs contend that the word publisher
is used in Section 230C1 with a narrower and distinct meaning,
which that term has in defamation law.
Basically, the plaintiffs are claiming that
anytime YouTube recommends another video,
it therefore becomes the speaker of that video
as if YouTube had produced that video itself.
Now, defendants say that this interpretation doesn't make sense,
quote, watch the World Series of Poker on YouTube
and YouTube's algorithms might display Texas Hold'em tutorials.
That does not mean that YouTube endorses gambling
any more than spell check endorses a suggested substitute word.
Westlaw endorses higher listed cases
or a chat room endorses posts organized by topic.
As the Ninth Circuit noted,
YouTube applies the same algorithms to all content.
And it's here that I think people often misunderstand
the nature of the internet and the nature of social media.
With the case of YouTube,
people upload millions of videos a day.
If there wasn't an algorithm to sort through this deluge of videos,
the only way to sort through them would be a fire hose
of viewing every video chronologically, which nobody wants.
It would be that or doing a pointed search to try and find content.
But then you'd never be exposed to content
that you actually do want to watch,
but didn't know that you wanted to seek it out.
And the plaintiffs have tried to differentiate
between search results and other algorithmically generated results.
But there's really no principled way to distinguish between
results that are sorted when you actively search for something
versus results that are presented to you
because of a recommendation algorithm.
And people love to hate algorithms, myself included.
But the truth is,
if there weren't algorithms to help us sort through
all of the content on social media platforms,
it would be impossible to wade through it.
And then additionally,
there is the issue of the material support
related for a claim under the Anti-Terrorism Act.
Now, the Department of Justice's brief in support of the plaintiffs
argues that algorithmic recommendations
convey the website's own implicit message
that users will find the information relevant,
and that makes the website liable
because it's acting just like ISIS itself.
Now, plaintiffs didn't claim this in their original complaint,
instead they focused on how YouTube's recommendations
amplify the ISIS speech,
making it more visible to more users.
But the US government urged the Supreme Court
to adopt a narrow reading of Section 230
that would permit a finding that Google
materially contributed to the content
by matching it to user preferences.
Again, basically, the Department of Justice,
as well as the plaintiffs,
are arguing for a standard that if any website promotes content,
uses an algorithm, or recommends content to users,
that that website then becomes liable
for all of the speech contained within that recommendation.
What happens when websites are liable?
Well, remember I mentioned
that this has actually happened before?
Well, it has.
In 2018, then President Trump signed into law
two bills making it easier to fight sex trafficking online.
That was the Fight Online Sex Trafficking Act
and the Stop Enabling Sex Traffickers Act,
otherwise known as FOSTA and SESTA.
And those two laws created an exception
to Section 230's Safe Harbor Rule
by stating that websites could be held legally responsible
if third parties posted ads for prostitution
on their platforms.
The law penalizes websites
which quote, promote or facilitate prostitution.
And while this includes sites that promote illegal sex work,
it also allows the police to investigate any site
that is knowingly assisting, facilitating,
or supporting sex trafficking.
This language can be read to include sites
with content that is legal, such as escort services,
personal ads, and pornography.
Now, critics of FOSTA and SESTA warned
that the new laws were too broad
and they would end up killing internet content
that has nothing to do with sex trafficking.
And they turned out to be correct.
If a third party posts content
that is covered by FOSTA or SESTA,
and the website fails to prevent that content
from being posted, that website can be sued.
And that was simply too much legal risk for several websites.
You should have gone for the head.
Craigslist shut down its entire personal section.
Tumblr banned all adult content
and Reddit removed several subreddits
and other websites just simply shut down entirely.
So you might ask, did any of this assist
with the prosecution of sex trafficking?
Well, in 2021, the U.S. Government Accountability Office
released a report on the first three years of FOSTA and SESTA
finding that quote, over three years,
the Department of Justice filed just one case
under its rules against promoting
or recklessly disregarding sex trafficking.
So ironically, this provided essentially
the perfect natural experiment to find out what happens
when websites are liable for the speech content of its users.
One of two things will happen.
Either the website shuts down that section
or shuts down the entire department,
or the website employs an army of moderators
to remove anything even remotely offensive.
You basically either live in a world
where a website allows everything, scams, spam,
pornography, ultraviolence,
just everything that you can possibly imagine
that you probably don't want to see in your everyday life,
or you live in a world where they employed
extremely expensive moderators
who prevent anything that might possibly be offensive.
And that of course assumes that a website
can even afford to employ those moderators.
So there are those who want to see
section 230 modified or scrapped altogether
and have argued that these online services
are acting like content developers.
Now oral argument in front of the Supreme Court
is next week, and I'm putting my money
where my mouth is on this one.
I filed a brief in the Supreme Court,
along with Dr. Mike, Mythical Entertainment,
Chubby Emu, Tim Schmoyer, and The Author's Alliance,
and a bunch of other creators.
We filed an amicus brief explaining
the potential dire consequences
of a really bad ruling on this.
So we'll see how it goes.
But in the meantime,
I'll probably just be stress eating
at the end of the internet.
Unless I use today's sponsor, Factor 75,
because Factor makes meeting your nutrition goals
easier than ever by delivering fresh,
never-frozen, dietitian-approved meals
right to your doorstep,
and the meals are completely ready to eat.
Another team of gourmet chefs creates
each meal using ingredients with integrity
to help you feel your best all day long.
And I can tell you from experience,
they really are extremely delicious.
Factor supports wholesome eating, made simple.
Their menus are updated weekly
with 34 different options.
Choose your favorite meals,
or let Factor craft the order
based on your taste preferences in meal history.
Factor takes the guesswork out of grocery shopping
and meal prep,
saving you time and energy for other things.
There are no hassle-prepared foods.
Make sure you always have something nutritious on hand
when you don't have time to think about making a meal.
And sometimes I just want a good, healthy meal
without having to cook,
or obviously shop for the ingredients.
And Factor's meals arrive preprepared
and ready to eat in just two minutes.
And I can always scale up my meals
if I need the extra energy
after a long day of lawyering.
So give Factor a try by heading to Factor75.com
and use the code LegalEagle50
to get 50% off your first Factor box.
Or just click on the link that's on screen right now
or down in the description
and use the code LegalEagle50
to get 50% off your first Factor box.
And clicking that link really helps out this channel.
And after that, click on this box over here
for more LegalEagle,
or I'll see you in court.

Okay, so welcome to the first lecture on the vectors course. This is the basics, vectors
versus scalars, vector notation, addition and scaling, and properties. Alright, so begin
at the beginning. Let's list some scalar quantities. Think about mass, duration, length, temperature,
charge. These physical quantities are all well described with a single number. Really
they just have a magnitude, although some of them may go negative, so it's a magnitude
and a sign. But still, just a simple number is adequate to describe these things. How
about vector quantities? What's different about vector quantities? Well, think about
these things, force, velocity, and therefore acceleration, or momentum. These things also
have a strength or a magnitude. However, so let's put that down, they have a magnitude.
However, they also have a direction. More than just a sign, they have a full on direction
in three-dimensional space. So it's not enough to know that a force is three Newtons. I want
to know in which direction is that force applied. And that then is the difference between a
vector and a scalar quantity. We're going to think about how we manipulate them. Alright,
so first off, the notation that we're going to use when we talk about our vectors. What
I'm going to do is I'm going to use a symbol such as the letter A. So let's write that
out, but I'm going to underline it. So an underlined symbol indicates a vector rather
than a simple number. And when I need to specify that vector, I'm going to write it, so we're
going to be three-dimensional. I'm going to write the three numbers in a column form
like this. Now, if you haven't seen a vector specified before, what does it mean? Well,
think of the Cartesian axes, the x, the x, y, z axes. Think in this case about coming
out from the origin two in the direction of x and one in the direction of y and three
in the direction of z. What we're going to do is we're going to think of our vector as
an arrow, an arrow that comes from the origin to this point in space. And that arrow itself,
whether or not it comes from the origin, that direction and that length of arrow is our visualization
of the vector. So let me just change color to green and go ahead and draw the tip of
my arrow there. There we are. So the vector is coming towards us out of the screen and
it has those particular three components, two, one, three. Other people may use other
notations. For example, a line over the symbol A is commonly used. When people write out the
components, they may choose to do it as a row like this or even using pointy brackets
like this. Now, all these notations are basically getting at the same thing. You'll be able
to read textbooks or look online and see these things and understand what they mean. But
within this course of videos, we're just going to use the notation that I've introduced
above. So I'll erase those for now. Now, the simplest thing that you might want to do
if you have a couple of vectors is to add them up. So let's think about that vector
addition. What does it mean? So let's give ourselves a second vector B. We'll make it
five minus two zero, let's say. I want to add these two vectors together. So we'll write
that out. I simply want to add A underline plus B underline. What does that mean? Let's
just substitute in two, three. Add it on two, five minus two, zero. Now, what we do is we
simply add the first component of vector A to the first component of vector B and so
on down the list. Very, very simple. So we're adding two plus five. We're going to add one
plus minus two and three plus zero. And we just tidy that up. So that's going to be
seven minus one and three. Now, how about scaling a vector? Okay. So what we can do
is we can multiply a vector by a simple number and correspondingly, we'll just end up multiplying
each of its components. So let's take an example, three, nine, minus twelve. What we notice
is each of the three components is a multiple of three. We can just take that common factor
out in front and write this instead as three times one, three, minus four. Same thing.
All right? Or equivalently, someone might give us a vector that's already written in
this form. It could be, let's say three over two onto two, four, minus four. Let's make
it one. All right? And we can just multiply that in, in a component by component basis.
So we just write ourselves a new column. Of course, three times two is three. Three times
minus four is minus six. And three over two times one is three over two. Okay? So there
we are. We can scale our vectors by a number in this simple way. So with these definitions
of addition and scaling, can we say anything about the properties? Okay. So if I have two
regular numbers a and b, then of course, a plus b is the same as b plus a. I'm not saying
anything fancy here. It's as simple as, I don't know, seven plus minus three is equal
to minus three plus seven. Obviously it is. We know that. Now, if we think about the same
statement for vectors, a plus b, vector b, is it the same as vector b plus a? Well, it
must be. Let's just write out an example, seven zero minus one, three, one, two. Is it equal
to three, one, two vector plus the vector seven zero minus one? Of course it is because
of the way we've defined vector addition as just being the addition of each element
to the corresponding element. And this property is called being commutative. Okay. So vector
addition is commutative. How about this second example? If we have three basic quantities,
ordinary numbers, then if we have a plus b plus c, it's the same as a plus b plus c.
It doesn't matter the order that we do them in. Is that going to be true for vectors? Well,
of course, it is going to have to be true to vectors because the way we define vector
addition is to add each component to the corresponding component. It's just addition. So this is
for vectors. Let's write down what we mean. We mean that vector a plus b plus c as a previously
worked out thing is equal to vector a plus vector b and then add on c. It doesn't matter
the order we do these things. All right. And there's a name for that property. It's called
being associative. All right. So vector addition has that property also. Now let's think about
our scaling property. If we have ordinary numbers again, then we could take some scale
factor k and multiply it into a plus b and it would just give us k times a plus k times
b. Again, I'm not saying anything that isn't utterly obvious here. Say for example, I don't
know, 2 into 1 plus minus 3 is equal to 2 times 1 plus 2 times minus 3. Of course it
is. So how about for vectors? Is it true that some scale factor k times the sum vector a
plus vector b, a plus b, and let's stress that this scale factor is just a pure number?
Yes indeed. It's going to be just k times a plus k times b. So just to stress what we're
doing here, let's copy down this sum of two vectors we were playing with up here. This
7, 0 minus 1 thing plus 3, 1, 2. Put it inside curly brackets maybe for a variety. It doesn't
have to be curly brackets. Multiply it by some factor. Let's have 3 over 2. Had that before.
Unimaginative. There we are. What's that going to be? It's just going to be 3 over 2 times
the first vector 7, 0, 1 and then plus 3 over 2 times the second vector 3, 1, 2. Okay. So
everything as you kind of would expect it works out. It must. And this latter property is called
being distributive, so scaling is distributive over addition. And that's the end of our first video.
Welcome to the second of these videos. We're going to look here at the vector dot product, also
called the scalar product. We'll look at also the magnitude of a vector and the meaning of unit
vectors, the geometric meaning of the dot product, and finding the angle between vectors using the
dot product. Okay. So the dot product is a way of combining two vectors in order to produce a
scalar, hence the alternate name scalar product. Let's give ourselves a couple of vectors. Let's
have a, well, vector a can be 4 minus 4. Let's have 2, 1. And we'll have a vector b, which can be
3, 1, 3. And we're going to do the dot product of these two guys. So we write that as vector a,
a nice, nice clear central dot vector b. And then we write that out as the two column vectors.
And we need to understand how we compute the dot product. And the answer is we're simply going to
multiply each component by its opposite number and then add them up. So we're going to multiply
the first component minus 4 by 3. And then add that to the second component to multiply by
its opposite number 1. And finally, the third components, 1 and 3. So that's minus 4 times
by 3. Add it on to 2 times by 1. Add it on to 1 times by 3. So minus 12 plus 2 plus 3. That's
going to be minus, minus 7. All right. There's the dot product worked out. It's pretty straightforward.
And of course, as you can see, it can be a minus number. It can be 0. It can be a positive number.
But it's a simple, pure number. Okay. So now, let's see what happens if we do the dot product
of a vector with itself. Let's do a dotted with itself. So that's going to be minus 4 to 1 dotted
with minus 4 to 1. Now, of course, because we're multiplying each component by itself, that will
always be a positive number. 16, minus 4 by minus 4. And 2, 2 is a 4. And 1, 1 is 1. And so that's
going to add up to 21. It must add up to a positive number. It's made of three positive
numbers summed. Now, I want to introduce a second vector called a hat. It's related to a just by
scaling it. And we're going to scale it by 1 over the square root of the earlier dot product with
itself. So 1 over square root 21. And then just minus 4, 2, 1 as before. So that's just a scaled
version of a. What's interesting about it? Well, now let's see what happens if we take the dot product
of a hat with a hat with itself. So we're going to get 1 over square root of 21 times 1 over
square root of 21, which is 1 over 21. And then, of course, we're going to get a dotted with a,
the original dot product we did, which is just 21, as we know. So, of course, the dot product
of a hat with itself is just 1. That means that a hat has a special property. It's what's called
a unit vector. Unity being, of course, a fancy word for the number 1. So when we scale a vector,
so that it, when dotted with itself, it comes out as 1, then it is a unit vector.
Meanwhile, in general for a vector, the square root of the dot product with itself has the name
magnitude. This is the magnitude of a vector. And it is also magnitude. It is also the length
of the arrow, if we think in terms of a vector as a physical displacement and arrow that lives
in three-dimensional space, then it would be the length of that arrow, as you can see from Pythagoras.
Okay, now then, a different thing. The dot product between two vectors has an alternative definition,
which we can show is the same as the definition we've been using so far. a dot b is also the
magnitude of a times the magnitude of b times cos of some angle. And what is that angle? It's
actually just the angle between the two vectors, between their directions. So here I'm drawing a
vector a going in one direction and almost in the opposite direction vector b. And then the angle in
question would be this angle that we see between the two vectors when we draw them coming from a
common point of origin. Okay, so it's important to understand then that this angle can be more
than 90 degrees. Here's what it isn't. Here's a mistake that's sometimes made by people as they
start to play with the vectors. They want the angle, for some reason, they want it to be less than 90
degrees. So they try and contrive this by putting the vectors together in a way that will give them
less than 90 degrees, like this, for example. And then we could try and draw an angle between these
two lines. Let's see, like, let's use a red to show that it's not correct. What we should have is
the two vectors coming from a common origin. Then we see that the angle between them can be more or
less than 90 degrees. If it was exactly 90 degrees, then of course the dot product would be zero
because cos of 90 is zero. That has interesting consequences. But right now let's work out the
angle between a couple of vectors. Let's give ourselves a, we'll make it one, zero minus one,
and b. We're going to make it four, one, minus one. And we'll do the dot product between those
guys. So first we'll work out the dot product. Actually, let's make it minus one. So a can
be minus one, zero, minus one. I think that will come out better. So we have minus four from minus
one times four. We have zero times one is zero. We have minus one times minus one is one. So it's
going to be minus three for the total dot product between these two guys. But we also need to find
out the magnitude. Fair enough. Magnitude of a is going to be the square root of minus one times
one times one. And again, one. So that would be the square root of two. Nice and straightforward.
Meanwhile, the magnitude of b is going to be four fours of sixteen plus one plus one. It's going
to be eighteen. The square root of eighteen. But I think we can do better than that. Square root
of eighteen is actually square root of nine times the square root of two. And that means it's three
times the square root of two. Okay, now we've got everything we need. Let's pull down a copy of that
definition there relating a dot b to its magnitudes in the angle and fill in what we know for this
particular choice of a and b. We've got minus three is therefore equal to root two times three root
two times cos of the angle that we're after. So now we just need to rearrange. That means that
cos of the angle is going to be equal to minus three divided by what we've got two lots of
root two. So that's just three times two. And if we simplify that down, it's just minus a half.
Now we may just remember or use a calculator to find out. This means that the angle in question
is in fact going to be simply one hundred and twenty degrees. Or you can use radians if you
prefer radians. So there we are. That's the answer. The angle between these two vectors,
120 degrees. And that's it for the second video. In this video, we're going to see how to calculate
something called the cross product of two vectors. It's also called the vector product because the
output is a new vector. And we'll see how to test that the answer is correct. So here I've written
a cross b is equal to c. And notice that the symbol for the cross product is just the multiplication
symbol that you're familiar with from basic arithmetic. I've given the vector a a particular
form, this two, three, four column vector. And similarly b is written as four, five, six. So
we're going to go ahead and find out what is the cross product of these two vectors c. Because it's
a vector, we'll need to do some working for each of the three components. Now what I'm going to do
is I'm going to paste up some structure to help us work through the problem. So don't worry because
it's going to look like a lot. But you don't need to write all this out every time you want to do a
cross product. I'm just putting it here so we can really spell out the process. Okay so let's go
ahead and work out the first component of the output vector c. Strangely enough what we're going
to do is we're going to ignore the first component of vectors a and b. So I'm just going to cross
those out. Those aren't used. And what we're going to do is we're going to multiply a certain of the
other components. What we're going to do is we're going to multiply the second component of vector
a with the third component of vector b. I call that the falling diagonal. Because when we draw it
like this we start high and then go low. And then we're going to subtract off the multiple
of the rising diagonal 4 and 6 here. The last component of vector a and the middle component
of vector b. So what we have here is 21, that's 7 3's a 21, minus 6 4's a 24, that's minus 3.
We can go ahead now and write that in as our first element minus 3. Now let's move to the
second element of the output vector c. We'll start by ignoring the second component of the
two source vectors a and b. We can cross those off. And again we're not going to multiply
some diagonals. But what's different here is we start with the rising diagonal 4 times 5.
The last component of vector a times the first component of vector b. The rising diagonal 5
4's a 20. And then we subtract off the falling diagonal. So 2 7's a 14 and that's going to give
us 6. So we can put that in. Now let's move to the third and final component. As before we start
by noting that we will ignore the third component of the two source vectors. And we're going to
need some diagonals. It's the same pattern as the first falling diagonal first. So 2 times 6
and subtract which is 12 and then subtract off the rising diagonal 5 3's a 15.
All right so that's going to be minus 3. Pop that in. We see that we have quite a simple
vector here. There's a common factor of 3. Let's bring that out. 3 then minus 1 to minus 1. That
is our vector c. That is a cross b. Notice again the pattern. It was the falling diagonal minus the
rising diagonal for the first component. And then the rising diagonal minus the falling diagonal
for the second component. And then for the third it was back to the same pattern as for the first.
Now these look a bit like letters to me. They look a bit like a v. The middle one perhaps an
n and the final one a v. I like to remember that as a little sentence which is voles never
vary. Because in my opinion voles don't vary very much. Here's a vol. This one doesn't vary at all
because it's stuffed in a museum. However if you compare it to some other voles which I found these
on the internet I think they're all pretty much identical and it's a big difference there. So
for me voles never vary. If for you they do seem to vary then think of a different way of remembering
it. But the important thing is that the first thing is the falling diagonal and then subtract
the rising diagonal of v shape and it alternates. Okay how to check your cross product has been
worked out correctly. This is really useful stuff. So let's give ourselves another example. We'll have
two three one and then we'll have let's say three seven minus one. Let's get a minus in there.
And that's going to be equal to something. We'll work it out in a minute. For now I'll put x, y, z.
Now how am I going to test once I found those x, y and z that I haven't made some kind of slip? I
mean there's a lot of mental arithmetic. If we don't write it all out we're going to be doing a
bunch of multiplications. I could easily slip up. How am I going to test that? It turns out
there's a very interesting property of the vector c that we get out after the operation
if we've done it correctly. That is as I've written here that a dot dot product with vector c is zero
and so is b. So either of the input vectors a and b dotted with the correct cross product c
should give us zero and that's great because the dot product is very easy to work out even by i
as a check. Let's go ahead and do it. So I've copied it down here. We're going to want to work down
our various components. Let's do the first component of c. So what do we do? We ignore the first
components of a and b and we do the falling diagonal. So that's going to be three times minus one and
we subtract the rising diagonal one times seven. So that's just let's just write that out. Normally
I wouldn't bother to write all this out but let's go ahead and do it here. So it's minus three
minus seven and so that's going to be minus ten as our first component. Now we work out
second component. We ignore the second component on the input vectors. We do the rising diagonal
one times three and subtract the falling diagonal two times minus one. So what have we got? We've
got three here minus minus two and so that's going to give us five and then finally the third component
ignore the third component of the input vectors. Do the falling diagonal. Two times seven seven
two is a fourteen. Subtract the rising diagonal three three's a nine. So we're going to have
for our final component fourteen minus nine which is another five. So that's quite a simple
vector. It has a common factor of five in there if we wanted to write it out that way. Now let's
test that guy versus the a and b vectors to see if it passes our test or have we made a slip.
So let's just be completely explicit about that. We're going to start by testing the dot product
of the vector a with our hopefully correct cross product c. I'll write it out two three one
dot product minus ten five five. What's that going to be equal to minus twenty
and then three fives of fifteen and then one five is five. Aha! It does equal zero. That's
correct. That's a very very encouraging thing but for real thoroughness we're going to test the other
one as well. So this is b dot c. Let's check that out. So that's three seven minus one dotted with
again minus ten five five. This time it's going to be minus uh minus thirty from three times minus
ten and then seven fives of thirty five but then minus five from the last element zero again. Aha!
So it has in fact passed both of our tests and we're now very confident that's correct.
This is a great test to do. One word of warning though the one thing it won't pick up is if you've
done your rising and falling diagonals in exactly the wrong way around by starting with the wrong
pattern. So do remember the VNV pattern and this test will check for any particular slips in your
multiplications and that's the end of the video. Okay so in this short video I'm just going to look
at four more examples of the cross product for practice and here they are. Okay so here's the
first one. We want the first element of this cross product so we ignore the first elements of the
two source vectors. We do the falling diagonal three times zero that's zero and we subtract the
rising diagonal seven times minus one that is minus seven so we're subtracting minus seven
that means we'll get plus seven. So the first element here is in the seven. Okay so now we
want the second element that means we ignore the second element of the two source vectors.
We do however the rising diagonal first seven two is fourteen minus one times zero is zero
so that's fourteen. So the second one was the rising diagonal first if you follow me
and then finally to get the third component we ignore the third component of the source vectors
and we do the falling diagonal one times minus one is minus one minus three two six so that is minus
seven. Okay so there's our solution seven fourteen minus seven but is that correct or have we made
a slip? It's a good time to check the old dot product trick so if we call this A cross B equals
C then we should find that if we do the dot product of one of the input vectors say B
with C then it should be zero. Let's check that seven twos are fourteen minus one times fourteen
is minus fourteen zero times minus seven is zero so that's fourteen minus fourteen it's correct
let's do the other one it's harder so one times seven is seven three times fourteen is forty two
that's forty nine in total and then the final term here seven sevens are forty nine but that
was with a minus number so we've got in fact forty nine minus forty nine is zero so another one of
those dot products is correctly zero so what we found out is that A dot C and B dot C are both
equal to zero as they must be so we're now very confident that we have the right cross product
let's do another one okay so we're going to want the first element so we ignore the first element
of the two source vectors and we do eight threes eight threes are twenty four minus two two times
one is two so that's twenty two let's do the next element so we ignore the middle elements and we
do the rising diagonal four twos are eight minus eight that's just going to be zero and then finally
we ignore the bottom elements and we do the falling diagonal minus the rising diagonal one minus
twelve is minus eleven so there's our solution twenty two zero minus eleven we notice we could
take eleven out of that as a common factor it would make the next stage very easy but let's just
let's do it the hard way and do the dot product so four times twenty two is eighty eight one times
zero zero and minus eighty eight actually pretty easy to confirm that zero let's do the other one
one times twenty two is twenty two three times zero and again two times uh minus eleven again zero
so that's fine that one's past its checks as well on to the third one okay so um this time I think
I might take a common factor out just to show us doing that because I see that this twenty five
five fifteen chap is going to lead to some pretty big numbers but maybe I don't need to do that I
can just take the common factor of five out of the first vector we're calling it vector a so that's
just five one minus three and then I go ahead and write vector b which can't be simplified it's just
one three minus two we'll do this cross cross pod excuse me we'll do this cross product and then
we'll put the factor of five in at the end that's fine to do it that way around okay so let's go
ahead and write that out there's our factor of five and here's our cross product so the first
element of our cross product we ignore the first elements of the two source vectors we do the falling
diagonal that gives us a minus two we subtract the rising diagonal that's a minus nine so that's
minus two plus nine that's going to give us a seven and now the middle element we ignore the
middle elements on the two source vectors we do the rising diagonal this time gives us minus three
we subtract the falling diagonal that gives us minus uh ten which means we're gonna have to add
on ten so that's minus three plus ten it's another seven okay and then finally the third element we
ignore the third elements on the source vector we do the falling diagonal that's five threes of 15
and we subtract the rising diagonal one that's going to give us another uh a 14 so in fact a
really simple vector here because we could take out a factor of seven if we want to but um let's
check those dot products do it before or after we take out the factor of seven it's pretty easy
that's going to be uh four times seven minus uh and minus two times 14 yes that goes to zero
let's do this one just quickly uh 35 and another seven is 42 but minus three times 14 is exactly
minus 42 so that one is also satisfied we've passed our checks that looks pretty good we can
leave it like this or if we want we can take out that factor of seven and do 35 times one one two
very simple very nice uh vector there okay let's come here uh now come down to the bottom and look
at the final one we notice is actually the cross product of a vector with itself it's the same vector
here so what are we going to get well we can just easily enough work it out we ignore the first two
elements and we do four um two times minus four and minus four times two so it's something minus
itself that's just going to give us a zero obviously and uh let's keep going if we ignore the middle
terms and do the rising diagonal minus the falling diagonal again threes and minus fours the same
product so something minus itself zero and it's going to be the same for the final element so
the cross product of a vector with itself is always going to be uh the zero vector now it's
important not to write that just as the scalar zero because it is a different object it's the
vector zero it's a set of in three-dimensional space three zeros that's what we get when we cross a
vector with itself of course this is going to trivially satisfy our condition on the a dot c
is equal to zero and b dot z is equal to zero that's clear and so uh i think that's a nice set of
four examples done quite quickly there they're not too bad are they so that's the end of the video
okay in this video we're going to look again at the cross product but this time we're going to ask
about its geometric meaning and its properties when we come to manipulate it okay so if some
vector c is the cross product of two other vectors a and b we've already seen how to work that out
but what we can reasonably now ask is what does that vector c look like you know if we imagine a
particular couple of vectors a and b there in space where is this vector c how is it related
to them we know how to work it out but what's its relationship with them how should we think about it
and that's what we're gonna we're gonna figure out now so we know that c is a vector so it has
two properties it has its magnitude and direction let's think about the magnitude first what is the
magnitude of c and how does that relate to um a and b what is the length of that vector it's pretty
simple the magnitude of c is the magnitude of a times the magnitude of b times sine of the angle
between a and b this is very similar to the dot product except with a sine instead of a cos
so there we are there's our two vectors a and b and an angle between them and from those
magnitude to the lengths of those two vectors in the angle we can work out the magnitude of c
note that if we cross a vector with itself the angle will be zero and so the cross product will be
zero just as we've already seen in our examples that was easy enough what about the direction
of this new vector c how does that relate okay here's the thing the direction of c is perpendicular
sorry for my writing there we'll be writing c is perpendicular to both vectors a and b so
it's at right angles to each of those vectors separately and simultaneously what does that
look like well actually we can draw it in one of two ways one of which is right and one is wrong
let's just do that so here's um here's our vector a here's our vector b if we draw c like that and
make it clear with this little symbol that it's at right angles to those two vectors
that would be perpendicular to them both how about this we could also draw a vector a draw
vector b again and we could go in the opposite direction simply literally the opposite direction
and that would also be perpendicular to these two vectors one of these is actually strictly
the correct case and the other is wrong by essentially a minus a minus one multiple what's
the way to work that out so let's let's now figure that out there's actually a rule to remember it
by it's called the right hand screw rule so let's draw that out kind of really clearly one more time
we have two vectors a and b and we are going to say that a cross b is equal to some vector c
that's fine so what we do is we put on the line along which we know c must lie
so this is the line that's perpendicular to both a and b and we simply have to ask ourselves
um in in this picture does the vector c uh go upwards or does it go downwards the trick is
to write on the angle between a and b and give it a direction so that it's increasing from
a to b it's the angle from a to b then you imagine taking your right hand and gripping
that line in such a way that your fingers curl in the same direction as the angle increases
and then your thumb points in the direction that the uh in the actual direction of c
let's do another example uh just to uh really make that clear here's a and b again
so we know we need to be I've drawn these lying in a plane so we I'm now trying to draw a line
that's perpendicular to that plane vector c must lie in one direction or the other along this line
what do we do we draw on the angle we now take our right hand and we imagine gripping that that line
we've just drawn in such a way that our fingers curl uh in the direction in which the angle is
increasing so it's like the anticlockwise direction in this picture and that's and then our thumb points
in the correct direction for that vector so it's in fact these are the two opposite cases
so that's the rule that allows you to construct the correct direction for your vector geometrically
geometrically okay uh then let's just finally wrap up by thinking about the cross product
and asking whether it has those properties that we looked at before for a vector addition
the commutative property so for example is a cross b equal to b cross a it is not
it is not equal to it unlike the dot product unlike addition this one the cross product
it matters the order and in fact it simply uh introduces a minus sign if you swap the order
of a and b so it's not commutative it nearly is in the sense that it gives you something similar it
gives you uh the same thing up to a minus sign it's important to remember and you can just verify
that by thinking about how we work out a and b with those diagonal products now how about the
associative property can we say that a cross b cross c where b and c have already been worked out
it's the same as a cross b and then cross c uh what do we think is that going to work or not in fact
it uh this is the associative property we might ask whether this is true and the answer is no
again uh the cross product does not have this property so the order in which you do your cross
product if you have doing the cross product of three vectors does matter we can easily convince
ourselves of this just by looking at a particularly uh convenient example let's just use cartesian
vectors i j k so let's just remind ourselves where these guys lie they're perpendicular to each other
i j and k just our unit vectors going in the x y and z direction so suppose we have this guy i cross
i cross k if we try evaluating it this way around with the i cross k being worked out first well
that's just going to give us in fact minus j which you can confirm with the right hand rule that we
just introduced and then that in turn will give us k that's fine so we've worked out um in that
instance the answer is minus k now let's do it the other way around i cross i if we do that first
that's just going to be zero because i cross i is zero so it's game over already at that point
so we can see two radically different answers here just depending on our order finally we could ask
about the distributive property so are we allowed to multiply through using the cross product uh if
we um if the second object in our cross product is a sum of two vectors can we do this well uh
this at last is something that we are going to be allowed to do it is the distributive property
and the cross product operation the vector product does have this property we are allowed to do that
but of course we must make sure to make uh to keep the order the same okay so i think that's
everything for this video okay in this lecture we're going to be looking at something called the
scalar triple product so what we're dealing with here is taking three vectors and combining them
in a certain way in order to yield a single one scalar quantity so three vectors into one scalar
scalar triple product suppose we have a we dot it with b which itself is crossed with c that is the
scalar triple product that combination now here i've put brackets to emphasize to do the cross
product first but we can just write a dot b cross c without the brackets why because we have to do
it in the correct order if we try to do a dot b first and then cross that with c it's a nonsense
because that will be a scalar cross-producted with a vector doesn't make sense all right then
so let's do one we'll make up some vectors let's have a is equal to three one minus one
and b is equal to two zero four and c is equal to minus one minus two three okay there are vectors
and let's go ahead and work it out so first we'll need to do the cross product b cross c so let's
write that out so i'm bringing these down now remember you can work out the cross product by
whatever your favorite method is i'm just going to do it in the method i introduced before which
is we ignore the first elements and we do the falling diagonal here zero and subtract the
rising diagonal minus eight that gives us the first element eight then we ignore the middle
elements and we do the rising diagonal gives us minus four subtract the falling diagonal
which is six so that's going to give us a minus 10 entry and then we ignore the third elements
we do the falling diagonal gives us a minus four and subtract zero so that's going to be minus four
that is our candidate for our cross product but it's always good to test how do we test a cross
product we try dotting it with either of the input vectors and check we get zero so here we'll get
eight twos are 16 and four minus four is minus 16 add it up that is zero and now we try the other
combination here we're going to have minus one on eight minus eight and then plus 20 and then
minus 12 that does indeed add up to zero it's past our checks those were just checks but it was good
to do them and so we're now very happy that that is the correct cross product to finish the scalar
triple product we now just need to dot that with a so let's write it out again minus 10 minus four
and do the dot product that's 24 minus 10 plus four is going to be 18 that's the answer that's
our scalar triple product it could have been a positive number a negative number could have been
zero in this case it's 18 now let's do another one so I'll erase this but we'll simply use the same
the same three vectors but we'll do them in a different order as our second example
so let's do b dotted with c cross a so of course we have to start by doing that c cross a combination
first so let me write that down quickly minus one minus two three crossed with three one minus one
so we start with the falling diagonal that's going to be two and then we subtract three that's minus
one and then we have a rising diagonal that's going to be nine and subtract one that's eight
and then we have a falling diagonal minus one and subtract minus six so that's going to be
five in all okay did I get that cross product correct or not do the dot product test
minus three minus three eight minus five that one's passed let's try this dot product combination
as a second check double check one minus 16 plus 15 that's also going to come out at zero
so it's passed both of my checks that one is zero as well we're happy that this is indeed the cross
product c cross a we now need to complete it so what we're doing is um b which was 204
dotted with what we found our cross product minus 185 so again go ahead and value this
minus 20 and 20 18 again all right so our second example has also given us 18 does this
mean that it doesn't matter in which order we do the elements of the uh scalar triple product
let me just write down the answer to that and then we'll look at it it turns out that for
any vectors a b and c then a dot b cross c is equal to b dot c cross a these were the two
cases we looked at and it's also equal in fact to c dot um a cross b this will always be true
in this case it was equal to 18 but these three things will always be equal there are three other
combinations we could write down in principle there are three other ways to combine a b and c
we could have a dot c cross b or we could have b dot a cross c or we could have c dot b cross a
now it turns out that those things are easy to see what they will be because
let's just look at the difference from the ones above i've just swapped the order of the cross
product and we know that when we oops we know that when we swap the order of a cross product
we introduce a minus sign so if the top three cases were equal to 18 the bottom three cases
must be equal to each other and equal to minus 18 and in general uh this is the same rule for all
uh uh scalar triple products your three of them are equal and three of them uh are equal to
one another but equal to the minus of the first three so to speak and and how can you tell which
ones are equal it's helpful to write out this little cycle a b and c written in a circle like this
if we are going around in a clockwise direction here b dot c cross a but that's clockwise around our
wheel then um and here's another one that's clockwise c dot a cross b those guys all belong
together so the guys that are in the clockwise direction all belong together and the anti-clockwise
guys they belong uh together and they're the minus of one another these two groups all right so um
that's uh that's i think all we need to do as practice for uh doing the scalar triple product
and uh knowing what we ought to get let's think about something else i'm going to introduce you
to something called the parallely pipette uh that's why i say i'm not sure how to pronounce it
parallely pipette anyway this guy is a three-dimensional shape but first i'm going to remind you of what
a parallelogram looks like so here's a rectangle and here's a parallelogram that we get um if we
have uh the pairs of the sides are parallel to each other but they are not at right angle at
right angles around the vertex now consider this rectangular box and let's tie it up there we are
and consider what happens if we uh build it out of edges that are in groups of parallel edges but
are not all at right angles to each other so uh let's see if i can draw this reasonably
realistically as a three-dimensional object so i'm going to draw this and then i'm going to stress
which edges are parallel to each other all right here we are okay let me change color
so consider these four edges of the object are all parallel to each other in exactly the same way
that in our simple parallelogram these opposing edges were parallel and then these four edges
are all parallel to one another again in our 3d shape just as these two edges are parallel
and then we have another set these four edges here in yellow are also going to be parallel to one
another that object is a particular three-dimensional solid it's clearly a generalization of the uh
of the box in that we're allowing ourselves to um have slanting edges if we want to now let's introduce
three vectors a b and c to represent these three kinds of edges you see that all the green edges
are the same vector a and so on what happens if we do a dot b cross c that it turns out the
magnitude of that if we drop the sign then the magnitude is just the volume of this shape so it
contains uh uh of course the simple case of a rectangular box as a special case but this will
work for any parallel parallel pipette uh that we care to think of with those three vectors
can always be combined with the scalar triple product to give us the volume
and that's the end of the video welcome to the uh third um topic in this video series
where i'll be introducing the matrix and thinking about what is a matrix product
all right so essentially a matrix is nothing more than a grid of numbers simply a grid of
numbers that could be positive or negative or fractional or zeros and when we uh specify the shape
of our grid of numbers or we do so simply by stating how many rows we have and how many columns
so we're gonna hear about rows and columns a lot in this video um in this video course i'm going
to use a particular uh way of writing a matrix as a symbol and i need to do that i'm going to just
use a capital letter and i'm going to the letter is going to be double underlined i'll double
underline that symbol so here we go a underline that means the matrix a and how would we write it
so that's uh just like this essentially a grid of numbers and we put it in curvy brackets just
to give it some structure so this is three rows two columns that one here's a matrix b
let's make it a square matrix let's put in a fraction to show we can minus 10 zero okay so
there are two different examples of a matrix easy enough but it gets more interesting when we try and
combine them so i want to talk about matrix multiplication addition is simple and it's
just an element by element addition but multiplication is not so simple so here's how we write it
the multiplication of matrix a by matrix b is simply written like this a b and it gives us
some new matrix c which may be a difference shape from both a and b as we'll see let's give ourselves
a couple of examples um three zero minus one two three four and matrix b can be just um one two zero
minus three so there are our two matrices here i've chosen them such that a b that multiplication
will work it will exist but actually if we try it the other way around it will turn out that the
multiple the multiple of those two matrices doesn't even exist it's not a well-defined thing so this
is an extreme case of an operation uh not being reversible in its order in other words matrix
multiplication is not commutative okay so uh let's just erase that and go ahead and see
how the multiplication actually works the trick is to multiply the each row of matrix a the first
matrix by each entire column of matrix b what does that mean well let's write out our example
three two zero three minus four minus one minus one four uh one zero two minus three now i know
that this guy is going to have uh three rows and two columns the output matrix you'll see why in a
bit i'll just put these blanks in for now the question is how to work out each of these numbers
let's choose this one first okay now notice this guy's address if you like is row one column one
of the output matrix c i'm going to need to in order to work this guy out i'll need to look at
the whole of um row one in the first matrix in matrix a and the whole of column one in the matrix
i'll need to combine those guys and how do i combine them i just multiply element by element as i go
along the row and down the column so three times one just gives me three and then i add on the next
combination two times two is four so three plus four is going to give me seven that's how i combine
those two i'll jump back here and i'll erase there and i'll just put in my seven all right so that's
the the general way it works let's go ahead and do the other elements of our matrix c let's do this
one notice this is still row one so i want that first row it's now column two that's its address
so i want the second column three times zero and two times minus three is how i'll work that out
and that's just going to be minus six so let me jump backwards um and erase my blank symbol and
write in minus six okay maybe i went a bit fast let me um spell this one out more explicitly okay
so here i now have row two column one that's the address of that guy i want all of row two
and all of column one i want to look at those guys and i want to multiply along so zero times one
and three times two that's going to give us just six in total when we add them up so let me erase
and put in six and now this element that's uh row two column two so i want all of row two
want all of column two and multiply zero times zero and uh three times minus three is minus nine
so that's going to be a minus nine if i go backwards and just put in minus nine here now
we're finally on to the final third row so we're going to want the third row of a and in this case
the first column so that's one times minus one and four times two is eight that's going to be seven
minus one plus eight and then finally last row last column uh four times minus three is twelve
and zero minus twelve all right so there we are that is our matrix product c formed by combining
each row in each column it's quite a lot of work and it would be even more if we had bigger matrices
but we said that um we get something quite different if we try multiplying a and b in the
other order so let's go ahead and do that now what if we have one zero two minus three that's b
on to three two zero three minus one four that's a so we can try it we try and multiply row one
by column one and we immediately find we cannot because they are a different length a different
list so there is no third element of our row to multiply with our third element of the column
just pause the video here um and have a look at that and see why that must be impossible for us
and so sometimes matrix multiplication is impossible all right let's look at a few uh little um
further examples and you may want to pause the video to convince yourself in each case it's true
is this thing possible for example pause it and think this one is not possible this is not possible
again because there are two elements in say the first row of a and three elements in the column
single column of b there's no way to do that as a series of element by element products how about
this we just have this row matrix and this column matrix can we do that yes this one is perfectly
possible actually it just produces a single number in fact it's a bit like a like a um a dot product
it's the whole of row one times which is the entire matrix um and then the whole whole of column one
this thing is called a row matrix and this other guy is called a column matrix for obvious reasons
okay how about this let's have a look at this one what if i swap the order of my own column i
just swap them around can i do that is that going to produce a legitimate matrix actually yes it will
this time swapping our two matrices a and b around has produced um something which exists it's actually
a huge matrix it's three by three it must have three rows and three columns because a has three rows
and b has three columns how does it work let's look at that guy for example it's just simply the number
there which is row one is just a number and column two is just a number single number so we just do
that product there's no problem pause the video if it's confusing all right so again the point here
is that um a times b is generally not equal to b times a even if they both exist they may not be
the same they may not even be the same shape uh however we can go on and ask about the other
kinds of properties of the matrix product operation a onto b times c is that the same as a times b
onto c does the order matter actually it is the same it does work in other words we have the
associative property how about a into b plus c some of two matrices yes we can have a onto b
plus a onto c that is therefore the distributive property matrix multiplication does satisfy those
things it's just not commutative okay let me make a bit more room up here in the top of the screen
and put one final puzzle up suppose i have this two row three column matrix and then a mystery
matrix m and then i have a simple column matrix of two rows and i'm asking what shape should
matrix m be or is it even is it is it possible pause and think about that and in fact it's just
a column matrix of three elements you may want to uh just meditate on that and see that it's correct
okay that's the end of this video okay welcome to this video in this one we're going to take
a look at how to work out a determinant what is it how can you find determinants of varying sizes
so a determinant is a scalar it's just a number could be positive could be negative could be zero
and it's derived from a square matrix a single number derived from an entire matrix
um now the determinant of m would be written with m with the modulus signs either side of it
even though it can be a negative number so here's an example of m and here is how we would write
the determinant of m note that we don't bother writing squares uh straight sides and curved
brackets as well there's no point in that it's just enough to have the straight line sides
so let's start with the definition of a uh two by two determinant that's the easy case to look at
so let's write out um a general two by two just using symbols we'll have a b c d written inside
our straight line sides indicates a determinant it's simply a d minus b c okay so that's the
falling diagonal the leading diagonal is also called minus the rising diagonal multiplied together
very simple very simple and that is how you can just look at and evaluate a two by two determinant
so for our example one two three four one times four is four subtract off a two times three is six
and so that's going to give us minus two is the determinant
okay so a three by three determinant is um going to be a bit more work what we do is when we have
a three by three determinant we evaluate it by breaking it up into a number um up to three
smaller determinants each of which is a two by two and for that we have our definition for
immediate evaluation so we break up bigger determinants into little ones and then evaluate
them now i'm going to write out something here that's like a chess board but instead of black and
white i have pluses and minuses you'll see why in a moment the thing to notice though is that we
alternate plus minus plus minus along each row and each column in this three by three grid okay
so now let's work out a three by three determinant again i will just use general symbols a b c
d e f g h i right now first i have to choose a row or a column i'm going to choose this top row
for the first example and i'm going to work along this row and i'm going to start with the a symbol
now i go and i look on my chart and i see that there's a plus sign in that in that slot of my
grid that means i put down plus a and now what i do is i ignore the whole row and the whole column
that a is in and i look at the remaining four numbers and i write a little determinant just
made out of those guys in the same order they appear so e f um is going to be uh in my main
determinant there and hi those are the remaining four guys in the same order they appear now b
the next term that has a minus sign according to my chart so i will put in minus b and multiply it
by again a smaller two by two determinant the one i get if i delete the row and the column with b in
it and look at the remaining guys d f g i and i just i just uh write those guys out um in the same
order they appear as a small two by two determinant finally there's c c appears with a plus sign
according to my chart um so i need to put down plus c and i need to multiply by well we delete the
row and column with c in it and we just see the remaining determinant d e um gh so uh i simply
imagine that that row and column was not there and then that's what the determinant becomes
and then of course those two by two determinants i can just write down what they are using my uh
rule of multiplying down the diagonal and subtracting the anti-diagonal
okay there we are so that is uh in general what a three by three determinant evaluates to
but it's not the only way to do it let's write it out again and this time choose uh let's choose a
column and a different one let's choose this column i'm also allowed to work down this so i
would start with b as my first term and i delete the row and column with it in and i'd see what i
are the remaining terms and write them d f g i except i've forgotten something uh there's a
minus sign attached to that particular entry so that should actually have been minus b all right
and then similarly plus e and i delete the row and column which has e in it and then i just make a
two by two determinant from in this case it would be the corner elements a c g i and then finally
minus h and delete the row and column with h in it make a two by two determinant determinant of
what's left a c d f okay and of course i could then write out these two by two determinants
explicitly but the point is it will get give me the same answer let's do an example and see why
we would choose one method or the other so here are just some random numbers i'm making up let's
stick that in it's three by three first off let's work along the top row and as uh as we did in our
first example so that's going to be three uh let's put in the full determinant here
and then minus one and again the determinant i get by excluding the top row and middle column
and then plus two uh that's going to be seven zero five minus one and i can go ahead and i can
work out explicitly what this comes out at as you can see i'm doing here and in fact it will be
12 plus 20 minus 14 and it comes out as 18 so there we are we've worked out a three by three
but we could have done it in a different way let's say we went along this bottom row that's
fine so then it will be five and i will be left with one two zero four for my mini terminate
and the next element along a minus sign and it was a minus number anyway minus minus one
that's going to be three two seven four let's just see how we've done that three two seven four
by deleting the bottom row and middle column of that now what about the third element here
well we actually have a zero plus zero times sum determinant i don't even care what that is because
it's been multiplied by zero that's the beauty of it so i've got five into four minus zero
and then we're going to have four threes of 12 minus 14 so that's going to give us 20 minus
two is 18 same answer as before okay what about if we have even bigger determinants than our three
by three example there if we have if we go bigger still we for example a four by four we're just
going to break it up into a number of three by threes and each of those would have to be broken up
into two by twos lots of work so here we are here's a general four by four we are going to expand it
along a row or column let's say we want to expand it along this row for example and we'll take in
turn a b c d and we'll need to know what sign to use so here's our checker board or our chess board
pattern of pluses and minuses just extend it out now to a four by four and you can see the rule here
is that if you like if the row number plus the column number is an even number then there's going
to be a plus sign and if it's odd it's going to be a minus sign you can confirm that for yourself
look at this one it's going to be at row two and column three and that's five and so that's a minus
that's one way to remember it or just draw it out anyway we're going to use that rule so we go
ahead and we write plus a and now we need to do the entire three by three determinant that we get
when we delete the row and column with a in it so we just write out that little square block
that we see it's quite easy to copy across and now we're going to have minus b and we need to delete
the row and column and then transcribe across the elements that are left as a three by three
just being careful not to make any slips and you see that we're going to continue so let's
delete this just to be completely explicit I'll finish the job off so I think I hope it's off
is what we're doing we're onto plus c and now we're going to just have e f h i j l and m n p
and then finally minus d um onto what we get if we delete the top row and right most column
which is left over then e f g i j k m n oh there we are that's how we handle a four by four each
of these three by threes would then have to be evaluated and so on so a lot of work and
that's the end of the video okay welcome to this fifth topic which is eigenvalues and eigenvectors
we'll introduce the problem and we'll see how to find eigenvalues finding eigenvectors is for
the next video so suppose that we are given a square matrix um n just some matrix but we are
told that m multiplied by v is equal to lambda multiplied by v for some scalar just some number
lambda and for some column matrix uh v and a column matrix of course the same as a vector
I will just say vector from now on okay so this scalar lambda could be positive negative or zero
meanwhile this vector v could be anything except the trivial boring case of just zeros
it's something other than that our challenge then is that we're going to be given a square matrix m
and we have to look for any scalar lambda and vector v that satisfies the equation
and such a scalar is called an eigenvalue and such a vector is called an eigenvector
so in that language m multiplied by some eigenvector gives us back that eigenvector just
multiplied by a scalar the eigenvalue okay so first off let's notice that if we are given
a candidate a possible eigenvector v to try perhaps for a multiple choice then it's easy
to test we'll just go ahead and try it so here's a square matrix a two by two two four one minus one
and uh suppose we write down v is equal to one minus one and this is suggested as a possible
eigenvector well then we would just test it out to see if it matches our equation
we try multiplying m by v so here we go two four one minus one and v is one minus one
that's a column and so we do row times column that's two and minus four is minus two and again row
and column that's going to be one plus one is two and we notice we can take out minus two as a factor
and then it will be the vector left is one minus one but that is just v so minus two is indeed a
scalar that multiplies v and we've succeeded improving that v is our eigenvector and our
eigenvalue that goes with it is minus two okay so that's great if we're given eigenvectors to
check out but what if we're not given any eigenvectors or eigenvalues then we must find any possible
eigenvalues for ourselves there could be more than one and for each we must find the corresponding
eigenvector v and in this first video we're just going to be finding those eigenvalues
okay so here's a little bit of quick manipulation and a side we know our equation is mv is equal
to lambda v i can certainly just bring it all to the left hand side and write mv minus lambda v
is equal to zero as long as i don't remember to write that as vector zero but now let's do
something interesting let's insert the identity matrix which won't change the equation but it
will be important for the next step mv minus lambda times the identity times v is equal to
vector zero the identity doesn't change the equation but now i can factor out both those two matrices the
m and the minus lambda times the identity that's a matrix i can factor those out and it allows
me to write that line now that if form of the equation it turns out this can only be solved
for any interesting v any v other than just zeros if the following equation is true which we can
easily prove but we're not going to prove in this video m minus lambda times the identity the determinant
of that is equal to zero so we're going to have plenty of time to think about that but let me just
put a green box around it because that is the fundamental equation we're going to use this
will allow us to find all the eigenvalues that satisfy our basic eigenvalue equation so let's
do an example it's the best thing let's do m as a two four this was the one we had before two four
minus one little square matrix and so let's write down what this lambda times the identity is
for a two by two it's going to be lambda zero zero lambda very simple and so this matrix that's the
difference of the two of them two minus lambda four one minus one minus lambda just the difference of
those two things as a determinant is equal to zero that's all so there we have it we've just
subtracted lambda off the down the diagonal but now we need to solve this so we just write out
the determinant two minus lambda multiplied by minus one minus lambda down the diagonal minus
four the off diagonal is equal to zero all right so we expand this out minus two minus two lambda
plus lambda um plus lambda squared minus four equals zero let's come over here for a bit more
space tidy that up a bit what if we got lambda squared minus lambda minus six is equal to zero
can we solve this actually it's quite easy to factor that's going to be lambda minus three
into lambda plus two is equal to zero so that's true if either lambda is equal to three
or it's equal to minus two and those are our two eigenvalues we found them using that equation in
the square box let's crack on and do one with a three by three matrix m here we go matrix m
is equal to let's have minus two one three one minus one zero and minus one one two i've worked
i've checked that before and it will work for us nicely now let's remember of course the rule
from the previous screen and we just need to apply that so let's go ahead and write it as a
write our determinant out we need to have minus two minus lambda and then just one and minus one
and then one minus one minus lambda and then one and three zero two minus lambda i'm just
subtracting lambdas down the diagonal making it a determinant setting it equal to zero
now i'm going to work along this row because it's got a zero in it so that makes me like it a bit
more as a determinant the first number is going to be minus one why because it's a one and let me
just quickly write out our little lookup table of pluses and minuses for doing determinants
so it was a one and then it picked up a minus sign and then we have the mini determinant that's made
out of those four terms so that's one three one and two minus lambda all right and then the next
term is going to be plus and then it's going to be the term itself is minus one minus lambda
and the mini determinant that we get when we exclude that row and that column is just made
out of the corner terms that's going to be minus two minus lambda and three and one and two minus
lambda and that's it because the zero term gives us nothing so it was only those two mini determinants
let's write them out minus one two times lambda and then three times one is three let's expand that
one out and then this one has the term in front minus of one plus lambda and then we have to expand
out the determinant minus two minus lambda times two minus lambda down the league diagonal
minus minus three is plus three there we are is equal to zero and then we just need to tidy that
up we need to clean it up a bit that's going to be minus of minus lambda minus one for the first
term let's turn that it one into pluses multiply through by the minus one and here we have minus
let's make that lambda plus one right that way around and then tidy up inside here we expand
it out minus four plus two lambda minus two lambda plus lambda squared and this three is
equal to zero we need to keep on working to tidy that a bit more this term here is in fact going
to be just I see the lambdas cancel out lambda squared minus one that's very nice that's come
down very very neatly so now we can really tidy that up and we can take out a common factor of
lambda plus one and the first term was just that so there's one for that and the second term we've
just found is lambda squared minus one pause the video and check you agree that that's tidied up
version of the equation now the way that can be zero is either the first term is zero which
requires lambda is equal to minus one so there's one eigenvalue for us that's one option one of
our eigenvalues has been found or the second term here has to be zero so let's do a bit more work
with that what we're saying is to neaten that up we're saying that lambda two minus lambda squared
is equal to zero in other words lambda squared is equal to two and so lambda is going to be
plus or minus square root of two that's two more eigenvalues three in all that we found for this
three by three matrix and in the next video we'll see how to take each of these values
and derive the corresponding vector this is the second of two videos that looks at eigenvalues
and eigenvectors in the first video we have seen how to find eigenvalues and we write these as lambda
for each lambda how do we find the eigenvector an eigenvector that goes with it
we know that our fundamental equation that we're working with here is that when matrix m multiplies
an eigenvector v it just gives us back that v scaled by lambda and another way to write that
is the m minus lambda times the identity multiplied by v is equal to vector zero this is the same
equation written two different ways what we need to know now that we um have obtained our lambda
values we just need to look at one of these equations and figure out an acceptable vector
i find that it's more useful to use the form on the right hand side
okay let's look at a particular example we'll have the matrix two four one minus one we looked at
this before and we found already that its eigenvalues are equal to three and minus two
what we're going to do now is we're going to take those values one at a time and figure out an acceptable
eigenvector we're going to write our vector that we need to find as just x and y where we need to
find these x y values now take a look at this green underlined equation and in particular the matrix
which is a difference of two different matrices m and lambda times the identity now that we have
our lambda value of three we could write out that difference that difference matrix it's going to be
two minus three and then just four and then just one and minus one minus three there it is
we're saying that when that multiplies our vector x y it gives us zero zero
so let's go ahead and clean this equation up we have minus one four one minus four
four onto x and y if you want to be explicit about that we can multiply out it means minus x
plus four y and x minus four y and that we know is equal to zero zero now what we immediately notice
here is that whilst this this equation between two columns two column vectors is telling us two
things it's actually telling us the same equation twice so we can see here that we're saying
minus x plus four y is equal to zero we're also saying that x minus four y is equal to zero
that's telling us the same thing is that a problem no that's exactly what we want to see
at this stage we should find that when we work on uh eigenvalue and eigenvector problems based on a
two by two matrix then really only one of these rows in the final expression uh constrains us
and the other one doesn't add any new constraint so this is exactly what we want so now how do we
go ahead and solve it we're saying that uh minus x plus four y is equal to zero of course we can
just rearrange this to say instead that four y is equal to x and that's the only constraint we have
what we're allowed to do is choose we can choose the simplest values of x and y that will make
this work so i'm going to choose y is equal to one and then i'll find that x is equal to four
and that is a perfectly acceptable eigenvector for one to go with my eigenvalue we will always
have this freedom in choosing the elements of our eigenvector really this freedom simply corresponds
to choosing how long the eigenvector is in other words its magnitude because if a particular
eigenvector and eigenvector satisfies our equations a scaled version of that same eigenvector will
still satisfy with the same eigenvalue now while the eigenvector can have any length we might
specifically have been asked for a normalized eigenvector that simply means we need to take
the one that we found and scale it to have unit length so in this case since it's four one we
need to divide by uh root seventeen to scale to unit length simple as that so there we are
that's our eigenvector and a normalized version of it now we still haven't found the eigenvector for
the other eigenvalue which was minus two let me just move this up on the screen to make space to
do that at the bottom so here we go we do exactly the same procedure we subtract minus two on the
diagonal two minus minus two and four and one minus one minus minus two lots of minus is there
so let's uh tidy that up that's going to be four four one and in fact another one
and then times x y is equal to zero zero as before we see that really these this is the
same equation twice there's only one constraint and we can read it off simply as x is equal to
minus y so if i choose x is equal to one for example then i'm going to write down an eigenvector
one minus one or if i've chosen y is equal to one then it would have been
minus one one it doesn't matter they're both correct eigenvectors to go with our eigenvalue
but if we want to normalize well they need to divide by the magnitude one over root two okay
so there are acceptable eigenvectors to go with the eigenvalue minus two okay so now let's find
the eigenvectors that go with the eigenvalues for our three by three matrix m which was
minus two one three one minus one zero minus one one two we looked at that before in the
previous video and we found the eigenvalues which were minus one root two and minus root two
and i've put little subscripts on our lambdas here so we know which one we're dealing with
let's deal with lambda one first which is the one that has value minus one
so i'll write over here the little equation that we're using over and over again
which is that m minus lambda times the identity multiplied by our vector is zero
okay we need this difference matrix so we subtract off the diagonal one minus minus one
and then one three one and minus one minus minus one and zero minus one one and two minus minus one
and that's on x y and z because we now need an eigenvector with three elements
and it's going to be equal to uh we simplify the matrix to minus one one three one zero zero
minus one one and that'll be a three and that again is on our x y z eigenvector is equal to zero
zero now what we immediately notice is that as before we don't really have three different
equations captured by our matrix equation we only have two in fact this is very obvious in
this case because the bottom row is the same as the top row that's not always the case it's not
always the case that the rows are actually identical but we will always find if we check
that there are only really two independent equations when we're dealing with three by
three eigenvalue problems we only have two equations really now i'm going to uh highlight
this row here one zero zero that's just saying in fact that x is equal to zero
now if we take uh either the top row or the bottom run we have minus x plus y plus three z
is equal to zero or y is equal to minus three z okay so now we simply uh choose any values
of y and z x has been dictated to us but any values of i y and z that satisfy these rows
so if i choose z is equal to one that's going to give me y is equal to minus three
and i can straight away then write down a satisfactory eigenvector it will be zero
minus three one as simple as that it doesn't matter whether minus sign is i could equivalently
have chosen z is equal to minus one and then i'd have zero three minus one if i normalize then
i'll need one over root ten that being three squared plus one squared and so that is a complete
solution for our first eigenvector we found it in simple form and in normalized form this is the
eigenvector that goes with eigenvalue minus one we can go ahead however and check this eigenvector
to make sure that it works so for that we'll simply need to write out our matrix m the original matrix
which was minus two one three one minus one zero minus one one two we have our
vector zero three minus one we just need to do this sum so the first element is going to be
a minus two times zero and then so three and i see there's a minus three so that does give us zero
and our second element is the only non zero element will be minus three
and our third third element there gives us one and we can write that as simply minus one
onto zero three minus one and so indeed we found that this vector works with the eigenvalue of
minus one now we can continue to look at uh to find the other eigenvectors but first let's take a
pause and review the steps involved so we're looking at rules for solving eigenvector problems
eigenvector problem is where we have a square matrix m and we say that m multiplied by some
special eigenvector gives us back that eigenvector times just by a value the eigenvalue we find the
possible eigenvalues using this equation involving a determinant of a difference of two matrices
in general there are going to be n solutions for an n by n matrix so two solutions for a two by two
three solutions three solutions for a three by three matrix that's because when we write the
determinant it will have lambda to the power of n as its highest order so for example we have
cubed to deal with when we're working out for three by three matrices now having found those
eigenvalues we then for each value need to figure out an acceptable eigenvector
what we've noticed is that generally we only have to use n minus one of the rows in the equation
that we're working to satisfy and that meant just one row in the case of two by two problems
and two of the rows in the three by three problems
we had some freedom as to what values to choose for our eigenvector and in fact that freedom
corresponded to just scaling the entire eigenvector to a greater or smaller magnitude and if we were
asked to normalize we would simply work it out using whatever values we like the simplest values
and scale it at the last step so that it has unit length okay so we've covered a lot of ground
for one video and this would be a good place to just stop watching if you like but i would like to
carry on and solve the remaining two eigenvectors for our three by three example because they involve
a square root two they're actually a bit more messy and tricky to do and in a way i think that makes
for a good interesting example to see so let me go ahead and cut back to the screen that we had
before with our matrix m spelt out and our possible eigenvalues and we'll now take the value lambda
subscript two which is square root two so then as usual we need to subtract that down the diagonal
so we'll have minus two minus square root two one three one minus one minus square root two zero
minus one one two minus square root two and that is the thing which when multiplied by
our unknown eigenvector xyz should give us zero zero zero now one thing we notice here is the rows
look all different it looks like we've got three different equations captured in this matrix equation
but they are not if we examine them carefully enough we'd find that we could generate one of these
rows from the other two and in fact we're only therefore going to need to use two of them
you could pause the video and play with it and see if you can show this but it must always be
the case unless we've made a slip earlier okay so i see that the middle row has a zero so i'm
going to start with that one it says x plus minus two minus root two times y is equal to zero
and that means that if i choose a simple value for y of one then i can immediately say
that x moving across is going to be one plus root two good so now i'll use the top line which is
minus two minus root two x plus y plus three z is equal to zero and i'll substitute in the values
that i've already picked and inferred so i'm going to get one plus root two onto minus two minus
root two that's the x term plus the y is one plus three z yet to be found is equal to zero
rearrange so put z on one side divided by a third expand this thing out minus two
minus root two minus two root two minus two plus one all right oh and there's a minus sign
because we've moved it all to the other side from the z of course now we need to tide this up
but what i notice is that inside the brackets i have a minus three and a minus three root two
and that will cancel cancel with a factor of a minus and third of front and just give us a very
simple expression of one plus root two so that's our z term okay we've found a compatible set of
x y and z values so we can now write down an accept acceptable eigenvector one plus root two
one one plus root two there we are that is an acceptable eigenvector and here's where we found
those numbers uh that goes with the eigenvalue lambda two is equal to square root two note that
i use the same subscript two on my vector so that i make it clear that lambda subscript two goes along
with vector subscript two so now our only remaining task is to look at the third eigenvalue which was
negative root two and find a compatible eigenvector for that one so as always what we need to do is
take the vector m and subtract that the lambda value we found off down the diagonal and because
we're subtracting minus a minus number we can just add it instead of course so that will be minus two
plus root two and then one and then three and then one and minus one plus root two and zero
and minus one and one and two plus root two and that matrix when multiplied by our unknown
eigenvector x y z will give us zero zero zero now as before our middle row looks nicest here
it's just telling us that x plus root two minus one put it that way around y times y is equal to
zero that means if i chose y is equal to one obvious choice then x is equal to one minus root two
watching for signs now if i take the let's say the bottom row i can have minus x plus y
plus two plus two root plus two plus root two times z is equal to zero but i can substitute
in the values i found so that will say that square root two minus one plus one plus two
plus root two z is equal to zero okay i've got some work to do to find out the value of z here
i'll start by rearranging just to put two plus root two z is equal to minus root two on the other
side but i still need to do a bit more work divide both sides i notice i can simplify
simplify by a factor of root two i can write this as z is minus one over root two plus one
pause the video and check you agree with me um and then i'm not happy with that because i don't
want to leave z as a fraction i could do but that would make a very messy looking eigenvector
i noticed there's a trick in up i have up my sleeve i know that if i multiply the top and bottom
of a fraction like that by root two minus one it will simplify i will then find that the top of course
is one minus root two uh but the bottom will be two plus root two minus root two minus one
and that whole expression just comes down to one finally then z is equal to one minus root two
we've now found our x y and z values that are acceptable so we're seeing saying that vector
three that goes with the lambda three value is one minus root two one one minus root two that is
an acceptable eigenvector so we're done for our three by three matrix m we found that three
eigenvalues and for each of them an eigenvector the last two of these which involve the root two
were uh more tricky just because there was more to keep track of more messy expressions
but the basic maths is the same every time in this series of videos we'll talk about linear
regression and least squares and the problem that we'll be solving is first in the most
abstract setting if you're given a subspace w of r m and a vector let's call it b also in
r m the question that we want to solve is which vector w in this subspace w is closest to the
vector b now just intuitively if we take the orthogonal projection of b onto w let's call that
p subscript capital w b so the projection of b onto the subspace w the orthogonal projection
we suspect that that would minimize this distance and the distance so the distance that we're trying
to minimize is b minus w minimize this over all w inside of this subspace w equivalently you can
minimize the square of the distances and this is why this problem is called least squares because
we're minimizing the squares of each of the components of these differences when you add
them all up so that's the statement of the problem is to find w inside of w such that
that the distance between w is minimized
and it turns out that the solution to this problem is exactly w equals the projection
of b onto w and i won't give a precise proof of this statement but we should at least get
an intuition for why this is true looking at this picture i've already drawn the projection of
b onto w and another arbitrary vector w now these three vectors form
a right triangle so it looks a little bit skewed from this angle but if you turn this this way
that triangle looks something like here's b here's the projection of b onto w
and here's some arbitrary vector w in the subspace w these two vectors are in w
and so this line connecting them is also in w the vector b is perpendicular to the subspace w
and therefore this angle is a right angle here this is the hypotenuse of this triangle
and it's the distance from b to w and this distance is the minimizing distance supposedly
so that's just b minus the projection of b onto w so i i you know misused a little bit of notation
here um i hope you understand that this w now is different from this one this is the actual solution
and because this is a hypotenuse of this triangle we know that this distance is always going to be
greater than or equal to either of these two distances no matter what w is this will always
create a triangle a right triangle unless w equals this vector right here and in all other cases
except this one this distance is always going to be strictly greater than this distance so what are
some ways to compute this projection so one way is to actually find an orthonormal basis of w
so given an orthonormal basis
let's call it w1 up to wk let's say k is the dimension of w
then the projection of b onto w is just take the dot product remember the dot product of
b with any of these normal orthonormal vectors gives you the shadow of b onto that vector and
then multiply again by that vector here to give you the shadow of b onto this line in that same
direction so we take the dot product or the inner product i'll write the inner product with brackets
of each of these vectors and then we'll multiply by that vector again so that we have a vector in
the end and then sum up all of these different contributions from these different shadows
so this is how you would compute the orthogonal projection of a vector onto a specific subspace
you would need for instance an orthonormal basis for that subspace but sometimes you're not given
an orthonormal basis so it might be difficult to compute it one thing you could do is you can choose
any basis of w pick arbitrary vectors that are in w and once you find k of them and you know that
they're linearly independent then you know that that forms a basis then in order to find an orthonormal
basis you would apply the Gram-Schmidt procedure to obtain an orthonormal one but you know how
difficult that is maybe you can do it for the first few vectors pretty easily but then after a while
it gets pretty messy so we'll look at a special case of this problem where w happens to equal
the column space of some m by n matrix where a is an m by m matrix m by n matrix
so in other words you can think of a as a linear transformation
from r n to r m
and in this special case we'll find a very interesting solution to this problem in general
when we look at this problem and we're given a vector b so now let's suppose that this subspace
is the column space of a and we have some vector b that's not necessarily in the column space what
this means is that the linear system ax equals b does not have a solution unless
a is onto or more specifically or more precisely
unless the vector b is in the column space of a
but because this doesn't happen in general instead of trying to solve this system which
might not have a solution we can solve an associated system instead that says okay I might not be able
to find an x in our domain here that sort of maps to the vector b because it's impossible all x's
get mapped to this subspace what instead we can try to find is project b onto this subspace
and now this vector the projection of b onto that subspace is by definition inside the column
space of a and therefore we can solve that associated system so we make a definition
based on this idea that a least squares approximation
to the linear system ax equals b is a solution
to the associated linear system ax equals the projection onto the column space of a
apply to our given vector b and it's this problem that we'll be focusing on solving
in the next few videos let's first state a theorem that makes it a lot easier to compute
the least square solution to a given problem in the special case that we mentioned at the
end of the video in the in the last session so the theorem says given a linear transformation
from
r n to r m that's called a let me write it here and a vector b in the co-domain of this linear
transformation a let's say x in the domain in the domain that's r n is a least squares
approximation to ax equals b now this is using the definition that we had made before
which remember was x is the least squares approximation to ax equals b if and only if
ax equals the projection of b onto w where w is the column space of a if and only if
x is a solution to the system a transpose ax equals a transpose b
now we mentioned last time that so let me just say here w equals the column space of a throughout
this entire discussion now we mentioned last time that if we have an orthonormal basis of w
we can actually solve this problem relatively easily but in general we're not given an orthonormal
basis of w so this formulation of the problem makes it much simpler to compute so i said it but i
should also write this that this means the taking the transpose of this matrix and taking the
transpose is easy you just swap the columns with the rows so this just gives you a new linear system
and in general this is much much easier to solve than something like this and the reason this
simplification occurs is because we've taken our subspace to be the column space of some matrix
so before we give some examples of how to apply this theorem we'll give the proof if you want
to skip the proof you can go to the next video so this is an if and only if proof so we'll prove
it in two directions let's let's first suppose that x is a least squares suppose x is a least
square solution to ax equals b i.e x solves ax equals a projection of b onto w
now here's a little picture that'll help us visualize everything let's say this is the vector b
this is the subspace w this is the projection of b onto w if we take the difference of b with
the projection onto w so b minus the projection of b onto w then that difference is exactly this
line that's orthogonal to w in other words this vector is in the orthogonal complement
of w and because it's in the orthogonal complement of w we know that no matter which
vector we take in this subspace let's call any vector here a and the reason we're going to call it
a is because a is an element in the column space of of the matrix capital a then the dot product
of a with any of these vectors i mean with this specific vector
equals zero for all a in the column space of a
in particular
if we take the actual columns of a
so a e i let's say and we dot this is the i-th column of a
as a matrix and we dot it with
this vector this is always going to equal zero for all i from and in this case since the domain
of a is r n it's for all i going from one to n we can write this dot product using the transpose
so remember the dot product is the the multiple you multiply each of the entries in the vectors
and then you add them all up and the way you can express that is using the transpose of a particular
vector if we write this as a column vector then we can write this as a row vector by taking the
transpose and then matrix multiplying these entries so we would take a e i transpose
times the vector b minus p w b equals zero for all i but this transpose the fact that
um if we take if we look at this um column of a and we take its transpose and if this is true
for all i then this is saying that this vector is the dot product of this vector with each
of the transpose vectors from a dotted with this is zero therefore if we take the matrix a and transpose
it and we multiply it matrix multiply it with this vector it will always equal zero
and now rewrite this by moving everything over to one side we get a transpose times the vector b
equals a transpose times this projection
but by assumption this projection we know that x solves this equation so we know that this also
equals a transpose ax and this shows that if x is the least square solution in other words if
it solves this problem then a transpose a transpose a acting on x equals a transpose b so this proves
the theorem in one direction to prove the theorem in the other direction
i'm running out of space here but i can give you at least the sketch of this proof
now suppose that um this equation is satisfied so suppose x is a solution
to a transpose ax equals a transpose b
we can move everything over again as we did sort of going backwards in this calculation
and we can express this by saying that a transpose acting on ax minus b
equals zero in other words this vector
ax minus b is in the orthogonal complement of the column space of a so it's in the orthogonal
complement of w now if we go back to our picture we know that the vector b can be uniquely decomposed
as the sum of two vectors one a vector in w and one a vector in the orthogonal complement of w
so this is a theorem um that you might cover uh in in the part of your linear algebra course on
um when you talk when you discuss orthogonality so b has a unique decomposition
into a vector in w plus a vector let's say in the orthogonal complement let's call it v
where w is in w and v is in the orthogonal complement of w
but this equation here says that if we take the difference ax minus b and we get in the
orthogonal complement we know that this has to equal some vector so ax minus b equals a vector
in this orthogonal complement let's just call it v for now because it's in the orthogonal complement
rewriting this equation says that b must equal ax minus v
and a where is ax ax is in the column space of a in other words it's already in w
so this is the vector in w and therefore this vector right here has to be in the orthogonal
complement and this uniqueness decomposition theorem tells us that this vector is exactly
b minus ax so this looks this is going to look a little bit silly but b equals ax minus
ax minus b
and the uniqueness decomposition theorem tells us that this vector that's in the orthogonal
complement must equal the projection of b onto that subspace w in other words ax this term right
here has to equal the projection of b onto w minus this vector right here
in other words ax equals the projection of w onto of b onto w and that means that x is
a least square solution because it solves this equation so that follows from the uniqueness
of orthogonal decomposition of a vector into two parts if you have a given subspace one
into a vector in that subspace that's where this ax equals the projection of b onto w comes from
and the other vector is just the orthogonal complement the projection onto the orthogonal
complement which is just the difference of the vector itself minus that vector in the orthogonal
subspace so this is the the proof of this theorem that allows us to say if we want to solve a least
square solution problem when w equals the column space of a we merely have to solve this system
so the next few videos will do lots of different examples of how to actually
so the example that we'll be working out it's a quite a long example because of the
generality that we'll do it in is if you're given data and let's say the data you're given
is you have a bunch of x values and a bunch of y values so these are one dimensional input and
one dimensional output values so suppose you have given data x1 y1 x2 y2 and so on up until
the number of data points that you have x dyd and if you try to plot these data points let's say
they look maybe something like this the question that you want to solve is can you try to find
a line that sort of best approximates these data so that's the problem
is to find a best fit whatever that means
straight line let's say of the form
y equals mx plus b now if we wanted to actually try to solve this problem
and suppose that all of these points actually lied on this line we would want to solve this entire
system now m and b are our unknowns we don't know the slope we don't know the y-intercept
so we'd have y1 we want to set it equal to mx1 plus b similarly for y2 our second data point
mx2 plus b and we keep going yd equals mxd plus b now in general this is an over constrained system
because we have d equations and if d is relatively large in particular if it's bigger than two
if it's relatively large it's very unlikely for us to find a solution to this problem
we can rewrite this problem as a matrix equation by saying that we have the vector y
which is the vector of our data points in fact let me even write y as a column vector
so let's write it like y1 all the way to yd
and if we notice this our coefficients are always being added in a linear fashion
and the only thing that's changing is the value of x1 so you could actually write this
as a d by 2 matrix acting on the vector mb now what should this matrix be
we want it to satisfy the equation y1 equals mx1 so x1 has to go in this column
plus b times what's the only thing that's going to leave b exactly where it is the number one
and the same thing here if we had y2 we would want to write y2 equals m
x2 plus one times b and so on all the way down to xd and one so this matrix equation
which we can write as y vector equals a and i don't want to write x as we did before because
i don't want to conflate it with the data points that are also labeled by x and so instead we'll
write this as ax so this is the system that we would like to solve but we know that there is in
general no solution to this problem so what can we do now in this case the column space of a
happens to be a two-dimensional subspace of r what of rd so the column space of a is a two-dimensional
subspace of rd so we can actually draw something like this although the space that's in is might
be significantly larger and we have the vector y somewhere out here in general it's not in the
column space in general this line does not go through every single one of these data points
so we have some vector y and instead of trying to solve this specific equation which in general is
unsolvable we can project y onto this subspace w and we can solve that associated system and then
we'll say what that means in a moment in fact actually we can say what it means right now
if we take the difference of these two vectors y minus this projection
what are we minimizing so an arbitrary vector in this subspace let's write w as an arbitrary vector
in the subspace is a linear combination of these columns so let's write that linear combination
as m suggestively a e1 which is the first column of a which is just all of these x data points
x data points plus b times the second column of a and we want to minimize the distance between our
data set our data vector y with this vector so in other words if we take this difference
let's let's replace this with w for now because let's imagine we don't yet know that this is the
projection so this difference is trying to minimize y minus m a e1
plus b a e2 and if we look at what each of these components give you then this equals
let's square this just so we don't have to deal with square roots then this is the sum so first
let's take an arbitrary ith component here it's yi minus m times xi plus b and that's it and then we
take the sum of these squares because that's what this means and we sum over all i from one to d
so we want to minimize this expression in other words we're taking our actual data set y and
we're taking this which is our best fit curve using our data set x and so we're trying to minimize
all of these distances so these are actually the vertical distances between the best fit curve
and this line it's the vertical distances because this is seeing our y data point minus
the value of this line at that point and we take that distance that difference which is this
little vertical height we square that height and then we add up all of these heights and we want to
minimize that expression so the solution to this least squares problem is graphically given by
an expression like that and we know how to solve this to solve this we apply our previous theorem
and we know that to solve this we can solve instead
a transpose a equals a sorry a transpose a x equals a transpose oh and x is xi
let me write this as xi and a transpose y so this is the problem that we want to solve
and we want to solve this for xi and xi is our vector of unknowns so in order to do this we have
to write down what a is we already know what a is we have to write down its transpose we have
to multiply those two things there's a lot of things we have to calculate so let's do that
on a fresh board space so i've written the problem setup and we have the matrix a with
our data points for x and our vector y with y and i've taken the transpose and i've written it
on the left because we'll be applying matrix multiplication to this side to solve for a transpose
a and then we'll also matrix multiply a transpose with y so if we multiply these two matrices
it's the first row here times the first take the dot product with this with this column
and that's x1 squared plus x2 squared plus xd squared so the first top left entry is the sum
of the squares of these entries from 1 to d
and the second entry on the top is the first row times the second column of a and that's x1 times
1 plus x2 times 1 in other words we're just summing up all of the different x values
and on the bottom left it's this first this the second row here with the first column
that's the same as it was in the top right
and then the last entry on the bottom right is the second row with the second column and that's
one times one plus one times one plus one times one d times which is just d itself
so this is a transpose a and a transpose y equals first of all notice that it's just a
two by two matrix so we're going to be solving a rather simple system it's just a two by two
so a transpose y is now take the values of x multiply them with the values of y
it's sum i equals one to d x i with y i this time and then it's the second row with this
and that's just the sum of the y's
and it's our vector with two components here and we want to solve this system
now it's only a two by two so on the one hand we could probably set this up as a
as a row reduction an augmented matrix problem row reduce and isolate whatever we need to so that
we can solve for this vector c on the other hand it's only a two by two matrix and row reduction
might be a little bit complicated for instance we might want to maybe divide this entry by
the sum of the squares of all of the entries but maybe that's a problem if every single one of
these is zero you know it's a little bit tricky so it's very convenient to first of all find out
when this matrix is invertible and if this matrix is invertible we can multiply both
sides by the inverse so if a transpose a inverse exists and we'll figure out what that means
we'll compute the determinant of this to determine when this inverse actually exists
then we can solve this system pretty easily and it's c which is again remember our vector of
unknown coefficients m and b then this equals a transpose a inverse times this vector right here
a transpose y which we've already computed so you know in terms of the setup it's relatively
straightforward maybe calculating this actual inverse might be a little bit of a challenge
because of the arbitrariness the generality that we're doing this in so first let's compute the
determinant of this matrix and that's just this times this minus this times this now because
we're multiplying these two sums we really have to be careful about the indices remember this is a
sum of stuff multiplied by a sum of stuff so we can't just say that this is sum xi squared it's
actually there's a lot of foiling going on and this is given by d the sum of the squares
that's from the first term this times this minus this times this and in order in order to make that
calculation a little bit more straightforward i'll rewrite one of the indices as a j instead of an
i so that we don't get confused so this is xi times xj and each of these sums there's actually
two sums here one for the index i and one for the index j and they both go from one to d so this
is the determinant and i won't do the rest of this calculation out but this i'll make a claim
and you should check this that this equals zero if and only if xi equals xj for all i and j
so the only time that this determinant vanishes if all of the xi data points
happen to be equal to each other now it takes a little bit of time to actually show that but you
can do it and this is the only instance when this matrix is not invertible and if you're
thinking about data this basically would mean that all of your data points lie along a vertical line
and then it makes sense that you can't find a function of the form y equals mx plus b to fit this
because the only line that'll work is a vertical line and in that case the slope is infinite so you
won't find a solution so it makes a lot of sense why this is the only case where that happens
otherwise if you have even a single point that's off of this line you will be able to find some curve
that best approximates this data although you would think that maybe if all of these points
lie here and there's a data point way out here then maybe this data point is there's something wrong
with it or more investigation is needed such a point in this situation would be called an outlier
and I may discuss about this at some point but that's not the focus of this specific
video right now so that's the claim so this determinant vanishes if and only if
all of these data points are equal so let's assume that this does not happen
assume there exists an i and a j that's not equal an i and a j which they are not equal
and such that xi is different from xj so we just need to assume that we have at least two data points
that do not lie on um that are not the same when we make this assumption we can compute this inverse
and this is easy because it's just two by two we maybe remember this formula we just divide by
the determinant we swap these two entries and we negate these so this is just one over this
determinant and i don't want to keep writing it so let me just write determinant of a transpose a
and just remember that it equals this and then we swap these entries so this is d and here we have
some and there's lots of indices now and i don't want to conflate any of these indices with each
other so i'm now going to call these k or something so this is k equals one to d and this is x k
squared and here we have minus some x k oops k goes from one to d and this is minus k from one
to d and this here is the inverse of our matrix
and then what we have to do is you have to take this complicated expression
and multiply it by this vector and once we do that we'll find out what the values of m and b are
so we'll need again a little bit more board space to do that so here i've rewritten our problem and
remember we're trying to solve for the coefficients m and b for linear regression for an arbitrary
data set and we computed that a transpose a as a matrix equals one over the determinant of that
matrix which we found was d times that's a d times xi squared minus let's use the indices i and j here
xi times xj so this is one over the determinant times our matrix which was
to not conflate these indices let's call these indices k
this was i believe d here for the inverse on the bottom right we had sum of the squares
x k squared minus k x k i'll stop writing from one to d it's just getting a little bit annoying
minus sum k x k but i'll always write the the subscript that we're summing over so this is
a transpose a inverse now a transpose y
well i can't remember if i wrote it but if you remember what a transpose looks like
oh we computed a transpose y yeah now i remember but the thing is that we'll have to be careful
about indices because i believe we use the indices i there as well and we've already used i we've
already used j we've already used k so let me call them l so this was sum x l y l l goes from one to
d and on the bottom part of this uh two component vector it was just the sum of the y's
okay so all of this mess is the left hand side of this expression let's multiply these two matrices
and see what we get um so let's just do that then we get and let's keep this determinant factor here
and i'm writing all of this because you'll see that it relates to something you may have seen
in a course on statistics or probability
so then we multiply d by this and we multiply this by this i'm just going to do this all out
d times this sum uh over it's just l one index x l y l minus this expression there's two sums
here now k and l x k y l that's the first component of this vector and the second component
is this times this now we have a bunch of stuff going on here um plus this times this so let me
write the plus on the left this becomes sum over k and l and x k squared which we can write as x k
you know let's just write it x k squared y l minus x k now this is a little bit different right because
we have two sums k and l and this time it's not x k squared it's x k x l
y l and this is what equals m b
now so this actually solves the whole problem so we know that m equals this first expression here
divided by this determinant and the y intercept equals this expression here divided by that determinant
now does it equal anything um familiar if we look at m itself
and we divide the numerator and the denominator by d we get that m equals
sum over l x l y l minus
one over d sum k and l x k y l divided by
x i squared minus i j x i x j
now each of these expressions um actually show up in statistics quite often and they're actually
given special names we call the let's do the denominator first since this one's only involves
a single data set this is called the variance of the data set x
where x vector equals x one through x d and it's also written as var oops var of x
and this just equals by definition the sum of the x i squares minus x i j
x i x j so that's what the variance is by definition and the covariance
um is involves two data sets our x's and our y's so it's of x and y
and this is defined by
i think you know people have different notation i don't know what the notation is i don't really
care um but it's this expression on top so this is sum l x l y l minus one over d
oh did i forget a one over d i did this should have a one over d here
minus one over d
x k y l that's an l subscript on that last y
so we have that our linear regression problem actually derives for us the variance and the
covariance of our data set and we also have explicit expressions if we wanted to
um for the least squares uh solution if we want to fit data to a straight line curve
in the next video we won't apply this general result because i don't think anybody would
expect you to memorize something like this instead we'll set up the problem in an explicit example
redo the whole procedure just so you get a feel for it with specific numbers involved and um
and how you would actually compute the inverse without all of these sums or anything like that
if you're just given a relatively small data set if you're given relatively large data sets
then you might want to go through this approach or you might have to program something that does it
for you so let's actually do an explicit example using actual numbers um here's a a graph and here's
some data points um the x axis is the horizontal axis and the y axis is the vertical one and let's
just use a unit grid so that the distance between any two of these grid lines has length one so the
data that we're given uh according to this plot is um we have our data vector and we want to try
to fit to a line of the form y equals mx plus b so let's write down our matrix a and our matrix a
remember consists of all of the x's if we write it in this form and ones all along the right column
so how many data points do we have so what's d one two three four five six seven three four five
six seven so you should have seven um entries in this column in the columns of a and let's go
in order from left to right filling in all of these entries the order that you go in doesn't
really matter as long as you're consistent with the value with the corresponding values of y that
you use so in this case the first value of x is at x equals negative four negative three negative
one zero one three four i've chosen it to be somewhat symmetric just for convenience of the
computation so it's negative four negative three negative one zero and the x values positive x
values are one three and four so this is the matrix a and the vector y
is the corresponding values of y so for x equals negative four the value of y is at negative one
again there are d there are d entries here as well the next one is zero then it's one zero one
and the last one the last two are two and four
so this is all of the information that we need
and if we compute a transpose a
what do we get so i won't write out a transpose just take the transpose of this
then we know that we're taking the dot product of this vector with itself to get the top left
entry here so what's the dot product of this with itself it's four squared times two so it's
16 times two which is 32 nine plus nine which is 18 so 32 plus 18 which is 50 plus two
so it's 52 on the top left the dot product of this with this is zero because all the negatives
cancel out all of the positive entries again i chose that specifically so that this happens
so that computing the inverse is much easier and we can immediately solve this system
now a transpose acting on y oh sorry the bottom entry is um is is just d itself and d is seven
now a transpose y
is this times this plus so negative four times negative one plus negative three times zero
plus negative one times one and so on so negative four with negative one gives you four
that with zero doesn't change anything so we still have four then that's negative one from four so
that gives us three leftover this one brings it back up to four then this six brings it up to 10
and this is 16 so we get 26 in the first entry
maybe you have faster ways of doing this i don't know
um so then uh a transpose if we take the second row here of a transpose
which is this column of ones and we dot it with this these cancel these add so we get seven
now solving this system is pretty straightforward um right this is 52007 in one side 267 we just
have to divide everything by 50 the first row by 52 the second row by seven and we immediately arrive
at the vector mb our vector of unknowns is one half and one so this tells us that the best fit
approximation that minimizes the vertical distance squared between between that line
and all of these data points has slope one half and y intercept one so the line that we
want to fit this to is one half x plus one and if we try to sketch what that graph looks like
we know that it goes through one so let's include that point here
and it has slope one half so when it gets to this when it moves two units over it moves one
unit up so here's the next data point we connect these two with a straight line and moving over
two units to the right one unit up we connect that with a straight line and we keep doing this
i mean this is how i draw um if i don't have um a ruler or anything on hand
i would try to draw something like this
so this straight line here if you notice it happens to actually go through one of the data points
that might not happen but as you can see it doesn't go through most of them but it's a pretty
reasonable approximation to this data set so this is how you would actually solve a least squares
problem specifically in the context of a fitting data to a linear curve or rather an affine curve
to be technically correct and this is how you do it in such an example in the next few videos
we're going to generalize the idea of linear regression just in terms of a straight line
data fitting to linear regression in the sense that you can data fit your data to sort of any
curve almost any curve and the way that we're going to do this is we're going to set up
some notation and we're going to let f1 through fk be linearly independent
functions
and what i mean by this is it's the same definition of linear independence of vectors
namely that um there does not exist a set of numbers a1 through ak such that when you sum up
um so let me just say this i.e there does not exist
a set of numbers a1 through ak
so these are real numbers or complex if these are complex valued functions
such that the sum of ai fi equals zero as a function
so um let's just say the domain of our function is whatever we need to specify it to be for
example the the whole real line or maybe an interval or something like that so and imagine your given
data points and let's say the given data points again we're going to use our x and y variables
so your input is x and your output is y and you have a whole list of data x1 x2
up to xd where d is the number of data points
and you want to fit these points to these functions so in other words your hope
is to somehow fit y1 equals to a1 f1 of x1 plus dot dot dot ak f
k x1
and not only do you want this but you also want this to hold for all of your data points
so up to yd a1 f1 xd now plus dot dot dot ak fk xd so this is your hope but if d
is much much greater than k then this is unlikely
it's usually impossible to find coefficients that fit all of these data
so before moving on let's try to rewrite this expression in a linear way so that we can relate
it to the linear regression problem we solved earlier so set y to be this vector here so let's
call this the vector y and what you notice here is that each of these numbers so f1 x1 is a specific
number we're taking a linear combination of these numbers with coefficients coming from the a's
so this looks like the vector y1 down to yd this is what this equation is represented by
a matrix whose entries are given by these values of f so f1 x1 in the first column and up to yd
the coefficient front of a1 is f1 xd and then this goes up to fk still x1 so x1 is the first row
and down to fk xd in the last row and this matrix is applied to the vector of unknowns a1 through ak
so this is again of the form y equals a and let's call it xc instead of x to not confuse ourselves
with the variable x that we've used for our data so in general it's impossible to solve this
and the way that we would like to solve this is again a least square solution so a least squares
solution or approximation to
this is a actual solution to
a transpose y equals a transpose ax so just apply a transpose on the left on both sides
and this is generally what we're going to solve for and this will be our this will be fitting our
data to the set of functions defined by these but there are a few restrictions that have to be made
for example the first maybe obvious restriction if you think about it is that these coefficients
should be independent and independent in the sense that I can't take any one of these coefficients
and sort of re-express it in terms of the others I'm not talking about linear independence I'm just
talking about independence so we assume the coefficients are independent and this just means
i.e. there does not exist an i
from one through k such that
a i is determined by
a j by all the other a j's so let's just say the set of a j's where j is now
from one excluding i so I read a little hat over that to exclude i up to k so in other words in
terms of all of the other coefficients so we assume that they're independent and this is sort
of obvious right because if you wanted to fit your data to these functions and you assume that these
were all unknown coefficients and you wanted to find the best value for them then if you
suddenly did that arbitrarily then it's unlikely that this relationship between them holds
in that situation so in general we definitely want to make sure these coefficients are independent
not only that we also should assume that the functions are linearly independent so
we assume that these functions are independent
as well and this is because
so suppose that one of these actually depended on the other so because if let's say
f i equal to some linear combination of the other ones so let's say bj fj so j goes from
one to k but j is not equal to i so we're just saying like for these to be linearly independent
another way is saying that well at least um none of them can be expressed in terms of the other so
if that fails at least one of them can be expressed in terms of the others so because if for some
numbers bj
then what happens is expressions so then if we take
so then if we take f and we take its linear combinations so let's say
a i sorry let me not use the index i let me use the index j now so let's take some of a j
fj and this breaks up into two parts now right because we have a sum over j where j is not equal to i
so this is j um not equal to i and the sum goes from one to k so this is a j fj but then we also
have plus a i f i but this term equals this so this equals sum over all j not equal to i
another sum over all j that are not equal to i so we have a i sorry a j i'm just copying
this term fj plus a i times this so a i times bj fj and then this is all in parentheses
and now you notice that fj is a common factor so when you factor that out you get sum j not
equal to i and then this is a j plus a i bj fj so now what we've done is we've re-expressed
our linear combination of these functions so the way everything that's on the right hand side here
in particular and we've re-expressed it in terms of functions in terms of k minus one functions
and now our coefficients have changed so in other words there was already a dependence on
the coefficients in some sense and so we usually demand that the functions are linearly independent
so that we avoid this issue in the next video we'll explain more generally a simple situation
that occurs in which this function this linear system is always um solvable by the method that
we used earlier namely by taking a transpose a inverse let's now understand when we can solve
a transpose y equals a transpose a c using the method of taking the inverse of a transpose a
now in order to take the inverse of this
we know that we need to require that the kernel of this matrix so by the way if a is a
is a d by k matrix and again d is typically much much larger than k then we want to know when this
exists so one of the situations when this exists is when
the kernel of this matrix vanishes that's one of the criteria
so zero as a vector space as a vector subspace um of r k
so when does something like this happen so to understand when we can apply this method
let's suppose that this is the matrix a a goes from r k this is r d here and this here
is the image of a
if we take the orthogonal complement of this image in this case you know unfortunately i
can only draw the orthogonal complement as having a single dimension but you could imagine that it
has um a much much larger dimension especially if these much much larger than k so the first
claim that will prove is that the orthogonal complement of the image of a
equals the kernel now in order for this to make sense i need to take the kernel of some matrix
now the image of a is in r d its orthogonal complement is also in r d and i can't take the
kernel of a because that wouldn't make sense the kernel would live here so i have to take
the only other thing i can take the kernel of is maybe the kernel of a transpose
so we'll do that so we'll take the kernel of a transpose and it turns out that these two are
equal so how do we see this let's visualize a as a um as a matrix of vectors so a one through a k
and when we take the transpose these rows these columns just become the rows
so we'll do this proof just by showing that one is contained in the other just to make it very
explicit so suppose that the vector v is let's start with the um let's start with being an element
in the orthogonal complement so let's say v is perpendicular to a the um the image of a
and then let's see if it's in the kernel of a transpose so when we take a transpose applied
to v what do we get so we'll write the matrix a transpose now we take these columns and turn them
into rows and we apply it to the vector v but matrix multiplication tells us that when we do this
we take this row multiply it by this vector in other words we take the dot product so this equals
another vector and it's a it's a vector in r k and what we get is a one dot product with v
as the first entry all the way down to ak dot product with v but if v is in the orthogonal
complement of a then it has to be that all of these dot products are zero so this is actually
the zero vector and therefore therefore the um this containment holds the image of the orthogonal
complement of the image of a is in the kernel of a transpose so that shows half of the theorem
now let's suppose so conversely
suppose that the vector u is in the kernel of a transpose
then by the same argument being in the kernel of a transpose
a transpose u equals zero but a transpose u is a one dot u all the way down to ak dot u
but the zero vector says that all of those are zero and because the image of a is spanned by
the vectors a one through ak we know automatically by the same exact argument that u is perpendicular
to the image of a so it's almost the same argument which is why i'm not writing it and therefore
this containment holds and that's the other half of the theorem so that's the proof that
the kernel of a transpose equals the orthogonal complement of the image of a
why is this useful
it's useful for the following very important reason
and it says that the kernel of a equals the kernel of a transpose a
you can already see why this is going to be useful because instead of looking at the kernel of a
transpose a which we take two matrices multiply them it's going to be a little bit more difficult
matrix to work with if we could just look at the kernel of a that would probably save us some time
so let's prove this in one direction it's pretty obvious but i'll write it out anyway
so let's first prove the direction that the kernel of a is inside here
so let's prove on this containment so if u satisfies
a u equals zero then a transpose a u because this thing is zero also equals zero
so that direction is pretty straightforward let's look at the other containment
so suppose v satisfies
a transpose a v equals zero then what this means is that a v is in the kernel of a transpose
i.e. a v is in the kernel of a transpose but by the previous claim the kernel of a transpose
equals the image of a taking the orthogonal complement of the image of a
so what's the picture here actually let's go back right here so we have that a v which by the way
is in this plane also is contained in the orthogonal complement of that image
and the only vector that's contained both in a and in the orthogonal complement is the zero vector
this implies that a v equals the zero vector in other words v is in the kernel of a
and now the containment has been shown in both directions and that's the conclusion of the proof
and let me just write out the final corollary which is the useful one for us
it's like corollary two
is that at least so let's say a transpose how do I say this a transpose a inverse exists
if and only if the kernel of a is trivial so it's only the zero vector now
why is this reasonable so this is this isn't really an example it's sort of an idea for why
this is uh this usually occurs when you're trying to fit data so our matrix a is typically going
to be of the form f 1 x 1 dot dot dot f um what was it x k f k x 1 all the way down to f 1 x d
f k x d so typically our matrix a looks something like this
and what would it mean for this to have trivial kernel it would say that none of these so all
of these vectors are linearly the set of these vectors the column vectors are linearly independent
is that likely so when when might something like that happen so for instance if one of these functions
did depend on the others in a linear way so for instance in the last video we said that
we assume that these functions were linearly independent if they were dependent what could
happen one of these column vectors could be expressed as a linear combination of the others
and therefore these columns would be linearly dependent and if these are dependent then this
has a non-trivial kernel so that's at least the sufficient that's at least one condition
that's a necessary condition for this to have um a non-trivial kernel so we demand that these
functions are linearly independent but furthermore not only do we ask that these functions are
linearly independent but it also implies that these specific vectors after we apply our data are
linearly independent but if d is much much much larger than k we only have very few of these
vectors right so the number of entries is d but we only have k vectors so it's kind of easy if you
randomly chose if you arbitrary and randomly chose k vectors in a very large dimensional space
randomly with almost almost surely it will be that those vectors are linearly independent
think about it just choose random numbers so for example let's write pi e 1 2 square root of 3 3
and the vector 1 1 1 i'm pretty sure that these three vectors are linearly independent in r3 and
i randomly chose them so even if d is not drastically larger than k but even if it's just
greater than k almost surely you'll pick linearly independent vectors so if your data is sufficiently
you know distributed well and it's not lying exactly on one line or something like that then
chances are these vectors are linearly independent so that's where it's going to be useful and in
the next video we'll actually apply this to a simple example that you probably don't need a calculator
to compute with in the next few videos we're going to be working with arithmetic modular two
so we're going to deal with all even numbers are equal to zero and all odd numbers are equal to one
so for instance two times three is six which is an even number so zero and seven plus three
is ten which is also even which is zero for another example is negative three equals one
in this case so anytime we do arithmetic for the most part when we add we're only going to be
caring about the parity of that number and this is going to be there are multiple reasons for this
one of which is simplicity the other of which is is that it's related to computer science
so we're going to let z mod two be exactly those numbers and with the arithmetic that I just said
so zero plus zero zero zero plus one is one one plus one is two which is zero and then
multiplication similarly zero times one is zero and one times one is one and we'll also work with
vectors whose entries are elements of z mod two so these are going to be vectors of the form x one
all the way up to x n where x one through x n
are in z mod two and we can also do arithmetic the way we usually do with vectors with vectors of
this sort by just adding component wise and scalar multiplication on each components as well
the interesting thing about this vector space is that unlike the vector space r to the n this
has finitely many vectors so how many vectors does this vector space have well first of all here
there are two elements and if you have n component vectors think how many entries think what possibilities
you can put in the first entry you can either put a zero or a one and as soon as you move to the next
entry you can also put a zero or a one and therefore each time you go through these entries you have
two to the n total possibilities so the number of vectors in z mod two to the n is two to the n
and one of those vectors is very special namely the zero vector and the non-zero vectors well
there's just one less of them
and i know that sounds like a trivial thing to point out but it'll actually be important
in our discussion and so for example this is the main example that we'll be working with
z mod two to the third power has seven non-zero vectors
for example so let's make a definition first first we're going to be exploring a lot of
mathematical curiosities and then we'll see how they apply to an actual physical situation
and i rather you have a little bit of suspense before we get there so first we're going to do
some math and then we'll talk about the applications so a hamming matrix
is a matrix h
with k rows and the columns
of h consist of all
the non-zero vectors
in z mod two to the kth power so k here is a non negative integer in fact let's just
yes suppose it's a positive integer so for example when k is three we have seven non-zero
vectors and what this is telling us all right now let's try to understand these two matrices a
little bit more the matrices m and h that we introduced earlier so recall that h was the matrix
it was the identity matrix a three by three in this case and another matrix q and m
was q and then the identity four by four matrix and both of these numbers can be generalized
as long as it's an appropriate size and it satisfies the requirements that we made earlier
namely that h consists of all of the non-zero vectors in the vector space
z mod two to the power where the power is determined by the number of rows here
so given the setup let's introduce a little bit more notation
and that notation is going to be we're going to define these that subspace which was the
kernel of h and also the image of m so let's call these image of m which is also the kernel of m
kernel of h rather let's denote this by c so for the rest of these videos c will refer to exactly
that subspace now remember this is a four-dimensional subspace inside of z mod two to the seventh
okay we're also going to introduce other notation
let's see subscript i be that subspace shifted by the i-th unit vector in z mod two so it's
going to be c plus e i and this just means by definition the set of all vectors of the form
v plus e i where v is in c
now this is not a subspace right because we can't add two vectors and stay within the subspace
yes stay within the subset but at the very least you can think of this as the subspace
shifted by some vector and we can define this for all i
between one and seven because that's how many non-zero vectors there are
in sorry that's that's that gives us a basis of vectors in z mod two to the seventh power
and now let's write some additional facts regarding these subs these subsets
so the first thing is that we already know that c is the solution set of a homogeneous system
namely it's the kernel of h ci is also the solution set of some system though it's no
longer homogeneous ci is the solution set of the inhomogeneous system h x equals h e i
where this is this whole thing h e i is the i-th column of h
secondly
if we take any two of these different subsets ci and cj then ci intersect cj so if we look at all
of the vectors that are common to both of them it turns out there are none so it's the empty set
for all i not equal to j
third
each of these subsets are also disjoint from the solution set of the homogeneous system
so c intersect ci is also empty for all i
and finally and this is maybe the most interesting part of it is that
the entire vector space of all vectors is the union of every single one of these
so it's the solution set of the homogeneous system with all of these other inhomogeneous
solution sets and because these are all disjoint this is a disjoint union
so every vector in z mod 2 is in exactly one of these subsets it's either a solution set of
the homogeneous system or it's in one of these solution sets of the different inhomogeneous
systems so this is a very important claim so let's actually let's actually prove it
so the first claim now when we solve inhomogeneous systems all we have to do is find one particular
solution and if we find that a solution exists then the solution set of the inhomogeneous system
is that particular solution plus the homogeneous solution that we obtained
from solving well for the kernel of h so notice however that we can just take x to be e i to get
a solution set so e i is a particular solution and therefore the solution set of the whole system
of h x equals h e i is that particular solution plus the homogeneous one
and that's exactly what the claim is c i is the solution set of this now let's look at the second
claim the second claim says that these are all different all of these subsets for different i
and j have no common intersection so in order to prove that let's pick two vectors one in c i one in
c j and they're going to be relatively they're going to be arbitrary and then we're going to show
that the only way that they can be equal to each other is if those subscripts are equal if i and
j are equal so let's start suppose that we have two vectors now because we're a solution set of
the homogeneous system the kernel of h and the kernel of h equals the image of h our vectors
are going to have this form so suppose m u 1 plus e i so this is our vector in c i equals m u 2 because
we don't know if right these two could have different they have come from different vectors
plus e j so suppose these we have these two vectors and this one is in c i this one is in c j
okay now if we apply h to these vectors so let me just write that this is in c i this is in c j so we're totally
clear now apply h to these this to this equality what happens well because these functions are linear
and we apply h to both on the left hand side this becomes h m u 1 plus h e i equals h m u 2 plus h e j
right and h m of u 1 is zero because h m is the zero matrix so this is zero that's zero and we're
left with h e i equals h e j now the only way that this is possible is if i and j are both equal to
each other and the reason is because h by definition is the set of all non-zero vectors in z mod 2 to
the third power and they never repeat so we only use those vectors once and only one so to better
understand this application let's first notice that if we apply m acting on any vector u the
vector we get is q applied to u in the top part of that um entries of those of that vector and we
retain a copy of u in the bottom this is because the matrix m was q on top and then the identity
matrix on bottom so this is true for all u in z mod 2 to the fourth
and so a copy of your original vector sits inside of this vector so imagine you're trying to send
a message u across some sort of a channel a communication channel and you want a receiver
to obtain um that message and you would like it for them to obtain exactly the message you sent
because if you hear something else on the other end of that line or you see something else
then you may misinterpret what the sender is trying to tell you so there's a sender and a
receiver and so for example um during this transmission there could be
some noise or maybe something that alters that message you hear this all the time when you're
on the phone and sometimes the signal isn't working too well you might not hear exactly what the other
person is saying or you might hear something a little bit different so there may be disturbance
along such a line so for example if we were sending um let's say my name across this channel
and at the end of the line the receiver sees um
the word archer for example now what was the original message that was supposed to be sent
in this context you have you know you know the english language so you know that there may be
a specific word that this is corresponding to but in this example you have two possibilities
that this word could be at least one of them could be archer or maybe arthur
and in order for the receiver to verify what the message was or one way to verify what the
message is is they could send that same message back and then basically ask you know is this the
message you intended to send okay so now imagine that this person sends um let's say this person
sends archer back and imagine another error occurs and imagine that the error occurs um takes place
let's say in the first entry and it becomes archer
and then the person is like wait did you want to send me the word archer like what are you doing
with this message um are you trying to tell me escher or archer and so this person is going to
send another message back um asking and you can see that this could keep happening for a very long
time um so it would be very convenient to either this person can send multiple copies of that message
and then with lower and lower probability the more messages you send the more likely it is
that the person on the other end will figure out what that message is supposed to say so that's one
option um but this option seems to take up a lot of resources right sending a message over and over
and over again is sort of multiplying the number of resources you need by the number of times you
send that message it would be very convenient if you could somehow have a scheme where the sender
is sending a message and the receiver can apply a certain method that both the receiver and
sender have agreed upon in advance to possibly identify if if an error occurred and where an
error occurred during that transmission so that's what we're going to do and we're going to simplify
the problem by not looking at the english language we're going to look at vectors whose entries are
just zeros and ones the simplest possible language that we can come up with or at least the simplest
list of the simplest alphabet we can come up with an alphabet containing two um symbols so let's say
we initially send the vector zero one one zero across this channel now once this channel goes
i should have written it from right to left as i've been doing so but let's go um counter to
this now if one error occurs suppose one error occurred that means that error is going to occur
in one of these four entries and if it occurs in the first entry the only possible thing that
that zero could become because our language only has two symbols is one so one possibility is that
we get one one one zero at the under the line another possibility is if the error occurs in
the second entry in which case we would have zero zero one zero and so on so in the third entry zero
one zero zero and in the last entry zero one one one so these are the possible outcomes if we have
exactly one error of course if no error occurs then the receiver will see the original message
but how do they even know that an error didn't occur or not so the way that we're going to solve
this problem is by using the previous situation that we had developed we can take our original
message encode it in some larger message and then this message is going to be contained in the subspace
c so if we send the message u it's going to be contained in that subspace c and if we send that
message across the channel instead what could happen to it so initially the sender is sending the
the letter the message u is contained in the bottom part but now mu is contained in z mod
two to the seventh power so it seems like a more complicated vector but the only real messages that
could have been sent the ones that have no errors are exactly in that subspace c any other vector in
this vector space is not a message that the sender could have sent because they're only working with
images the image of the transformation associated to m so this message is going through now imagine
that an error occurs somewhere along the way error and the message becomes mu plus now there are seven
entries in the vector mu so there are now seven possible errors that could occur and these errors
are exactly quantified by adding the unit vector in the ife row or entry of that vector so this
error occurs but the reader on the other end is going to see this vector v they don't know that it
is a priori this sort of combination all they see is some vector of zeros and ones
but they can use h to identify what form the vector v is in remember we said that if h of v
equals zero and this implies that the vector v is in the subspace c which is the image of m
and if h of v equals a non-zero vector then that non-zero vector is one of the columns of h
this tells us that v is in ci but remember what ci was it was this subspace plus the unit vector
ei so it tells us that if a receiver receives receives the vector v and they apply h to it
they can identify which of these subsets it's in and if the vector that they see after they
apply h is zero that tells us that no error occurred so we're going to assume at most
at most one error occurs during the transmission
and if we make that assumption then these two applications an application of h to v will tell
us where an error occurred and if we've identified where the error occurs right this says that if
we see that the h of v is hei then we know that the vectors of this form and how do we fix it
so if if it's let's say this is case one and this is case two in case one how would the receiver
identify what the original message is they would look at the last four entries of the vector v
because that's where u is and we know that no error occurred so the original
message sent by the sender is the vector corresponding to the last four entries
of the vector v and in the second case what happens then well if in the second case we found that
h of v equals h of v i then an error occurred
in the ith entry of v and how would we fix that while we would just subtract e i but
subtracting in addition are the same in z mod 2 so to fix
we know that the original message will be v plus e i well not the original message but
what the receiver sent after applying the transformation m and when they do this then they
can read off the last four entries of this vector the last meaning the bottom four
of this vector v plus e i is the original message
so let's just do this in an example just to see how exactly this works
so imagine you're the receiver and you see the vector v equals zero zero one one zero one one
if you apply h to this vector so i'll write h to remind you because otherwise
how are we going to do this computation huh so this is one one one zero one one one zero one
one zero and then we apply the vector v here
and if we apply matrix operations here we will get the vector three two three but three is one
in z mod two and two is zero so this becomes one zero one so we take this vector and look
where it appears in this matrix and in this case it is the sixth column of h this means that an
error occurred in the sixth entry of this vector here so error in sixth entry of v
and therefore the if we alter the sixth entry that would mean we change this one the second
last one to a zero so that means the original message message is one zero zero one
because we take the last four entries of this vector and then we switch the sixth entry
if we had found that the second entry was um an error occurred in the second entry we would
have changed that zero to a one and left the original message here and that would have been our
the message that was sent by the sender so um that's the basic idea of how this works
and again we worked with a case where we were dealing with um sending messages of length four
and we used um an additional a larger vector space to encode the possibilities of computing
those errors and you could also do it by um using the um by having h to be a matrix consisting of
all the non zero vectors in z mod two to the k it will allow us to encode a message of length
given by the number of columns in that matrix q and we already calculated that the number
of columns in that matrix q is two to the k minus one because of the zero vector minus an
additional k from the k vectors we used on the left hand side of the matrix h so we can encode
quite a large um number of messages under the assumption that at most one error occurs during
transmission so let's now analyze in a little bit more detail
what is q u actually doing so we know that that matrix m that we had it was broken up into two parts
and when we send a message u across a channel we will keep our original message in one part
of that vector but we'll add a bunch of fluff to it and what is the meaning of that fluff from
maybe a more a different perspective um it turns out that there's a very interesting sort of uh
logical thing that's going on between the entries of u and what q is doing to those entries
and the idea is that it's adding those entries in such a way as to maintain the sort of consistency
so if we take actually q u and we apply that matrix q that was left over the vector we would get
in terms of the entries of u so u is going to be u one through u four
the entries of this vector are going to be u one plus u three plus u four
u one plus u two plus u four and the third entry because this is a three by four matrix
is going to be u one plus u two plus u three and these entries here are called well let's call them
p one p two and p three for now and they are called parity bits
and the reason they're called parity bits is because when this message gets sent across a channel
if an error occurs these entries are summing up the entries of the vector u in some specific way
and if an error occurred right we have some vector p one p two p three and then u one u
two u three and u four if an error occurred in one of these entries then these parity bits will
detect if an error occurred and where the error occurred based on the consistency of this formula
so let's see how this works in an explicit example let's say we have the vector zero zero one
and i'll break this up into the two different parts so that we isolate the parity bits versus the
original message and by the way this isn't the original message that i'm writing right now this
is what happens after it's sent and let's see the receiver sees this message i believe this may be
the example we were working with a moment ago so let's now look at these formulas and see what
they say so p one on the one hand equals zero but let's see if the sum of these entries is also
equal to zero so if we take u one plus u three plus u four we get one plus one plus one is three
which is one which is not equal to one which equals u one plus u three plus u four what does
this mean this means an error occurred in one of these entries
and when i say one of these entries i mean either p one u one u three or u four so let's
write that down p one u one u three or u four and we know it has to be exactly one because again
we're assuming at most one error occurred and because of this inconsistency we're guaranteed
that an error occurred the only way no error would occur is if all of these would be consistent so if
p one does equal this p two does equal that p three does equal that because this would say
that our vector is of this form m applied to the original vector u so that doesn't exactly tell us
which of the errors it is yet is it p one u one u three or u four so for that we'll look at the other
parity bits so let's look at p two the vector we see says p two is zero is that consistent with
this formula u one plus u two plus u four so u one plus u two plus u four is zero so that actually is
consistent what does this tell us this tells us that no error occurred in any of these entries
because if one error occurred it is impossible for these two to be equal to each other so this means
p two u one u two and u four are all
error free now let's compare this to the first one that we analyzed the first one said
it was possible that the error occurred at u one and it was also possible that the error occurred
at u four this new observation tells us those two possibilities it's not possible that an error
occurred in those entries so now the only possibilities left are p one and maybe u three
so we'll keep that in mind when we go to the last parity bit which will then isolate exactly
where the error occurred so p three is equal to well from this it's one and is that equal to
u one plus u two plus u three u one plus u two plus u three it's equal to zero so that's not equal
to this which is u one plus u two plus u three now this tells us that error is in one of p three
u one u two or you or u three
we already know that u one and u two are not possible right u one and u four are not possible
and the only error that's common to both of these right because we know an error
one error occurred in either p one or p r u three or it's possible that an occurred in p three or
u three but if it was p three right suppose that the error occurred in p three then this would
have been fine it would have been unaltered because we wouldn't have detected an error
u three would have also been okay so the only possibility in this case is that an error occurred
in u three the one that's singled out from these three parity bits so error
in u three and therefore if we go to this original message the message that we received rather
and then we um this is sorry this is the message we received but we would have to alter is the
u three entry of this to get back the original message therefore the original message
is the last four entries as it was before but now we alter that third message that third entry
to get one zero zero one as the original message being sent and this is consistent I believe
with the answer that we obtained earlier so you might be wondering okay this is a little bit more
intuitive because we're sort of counting up our different entries in different ways and sort of
using a process of elimination method to isolate exactly where the error occurred now of course
that is a little bit more straightforward it's easier to work with it's easier to think about
um the first time you see it perhaps on the other hand the linear algebra method
it allows you to see it from a maybe potentially different perspective and I would think that if
you're working with a much much larger message that the linear algebra method seems to be a
lot easier to work with especially when you look at the way that we multiply those matrices
and the form of the hamming matrix that we constructed so let me just say this that
the cs hamming matrix looks a little bit different for instance I think it starts out with
one zero zero zero one zero but then the third column is not zero zero one I think the fourth
column is zero zero one and these other four columns are some permutation of the leftover
columns I had and now you can see if you were to manipulate this with the other matrix m that's
associated to this one by demanding that the kernel of h equals the image of that matrix m
the algebra would be a little bit more we can't just break this up into do blocks identity
and the leftover part instead it has sort of this interpretation but I believe the linear
algebra calculations are much much simpler if you work with a block die a block matrix
of the form that I indicated earlier now this may change if you try to look at what happens if
multiple errors occur how would you potentially correct for all of those additional errors
and I'll leave you to think about that and to check out the literature in the next few videos
we're going to compute the square root of a positive matrix and the way we're going to do this
is by introducing something called the functional calculus and in fact we'll learn how to compute
given any function under suitable conditions what it means to apply that function to a given
square matrix so let me go ahead and state the statement of the theorem that will prove
and we'll prove this theorem first by doing an example and then we'll prove the general
result from scratch so it says let a be a diagonalizable
n by n matrix
and let f be a function be a complex valued function let's say
defined on what I'm going to call sigma of a and sigma of a is the set of all eigenvalues of a
now if we have this setup we can already define what f of a is so let's do that
so f of a is going to be defined as p f of d p inverse where
p is the n by n matrix is a matrix of eigenvectors
of a written as columns and d is the corresponding
matrix of eigenvalues
and what do I mean by f of d
and f of d is defined to be now d is a diagonal matrix so let me just write out exactly what
we're doing if we have a matrix of eigenvalues and these eigenvalues can repeat so let me just
write all n of them and then this is zero everywhere else we define f of this matrix to be f applied
to the elements along the diagonal and zero everywhere else so this is f of lambda one
f of lambda n and zero everywhere else so so far all we've done is set up our assumptions
so we have a matrix we have the eigenvalues we can define f applied to a provided that we
have a complex valued function defined on the set of eigenvalues and here's the statement of the
theorem then there exists a polynomial
q such that q of a now what do I mean by q of a q is a polynomial and it makes sense to multiply
so we can take a we can square it we can cube it we can also take it to the zero
power that's just the identity matrix and then we can also multiply these by coefficients so if I
have any polynomial it's very easy to define what q of a is you just write your polynomial
and where you have your variable you replace it with the matrix a so this is some polynomial in a
but it turns out to equal f of a as defined previously by this method of breaking a matrix up
into its eigenvalues and getting its eigenvectors and constructing it this way
so that's what the statement of this theorem is and it's very surprising because in general you
can think of a very strange function such as the square root and this is telling you that there
is a way to write the square root of that given matrix in terms of a single polynomial
and what we're going to do first is do this through a simple example and illustrate it with
that simple two by two matrix and then we'll prove the general theorem so we might as well start
this example now and continue it in the next video so the example is going to be let a equal 10
6 6 10 and our goal is to compute the square root of a so the first step is find the eigenvalues
so another thing that we'll do is we'll review how to do these things so to find the eigenvalues
compute the determinant of 10 minus lambda 6 6 10 minus lambda and this equals 100
plus lambda squared minus 20 lambda minus 36 and some of this simplifies we get lambda squared
minus 20 lambda plus 64 and this also factors into lambda minus four and lambda minus 16
so we know what our two eigenvalues are they are four and 16
and while we wait for the next video you can try to compute the corresponding eigenvectors
and I'll just give you the answer there in a moment so here's the matrix that we're looking at
the associated eigenvalues that we found before and corresponding eigenvectors which you should have
found by computing the corresponding eigenvectors and so now let's compute what f and f meaning the
square root of a so what is f of sorry f of the diagonal matrix d associated to these eigenvalues
this is taking the square root of each of the corresponding entries on the diagonal so it's
just two and four and the matrix p is writing down these two eigenvectors so it's just one negative
one one one its corresponding inverse is the determinant here is two so it's one half and
then the rest of this matrix we swap and we negate so that's the corresponding inverse of this matrix
so what happens when we compute p f of d
p inverse supposedly we should get the square root of our matrix which means that if we square it
then we get back our matrix a so if we multiply some of these out i'll skip some of the steps
so if we take one half when we multiply p with f of d we get two four negative two four
and then we also have p inverse still here i've already pulled that one half out
and multiplying these matrices out we get well that distributes out so we can just have one two
negative one two and when we multiply those we get three one one three so let's check that if we
square this matrix so let's um let's just call this f of a this is the definition that we gave
of f of a so what happens when we square this matrix f of a squared we get exactly 10 6 6 10
so we do get our original matrix back so this is one way of computing the square root of a matrix
or at least if it has positive eigenvalues um by computing the corresponding eigenvectors and
eigenvalues and supposedly we have another way of doing this and the interesting thing about
the following method is that we will not be able we will not need to use the corresponding eigenvectors
all we need to use are the corresponding eigenvalues and we'll find that polynomial that
allows us to compute the square root of this matrix so how do we do that for the time being
what we'll first do is we'll find a polynomial
q such that q of lambda one equals the square root of lambda one or f of lambda one
and q of lambda two equals f of lambda two so in this case these are the square roots
and we already know exactly what their values are this is two and this is four
so we're trying to do at this point now we're doing a different problem it seems like
because now we're just trying to find a polynomial that interpolates these two values of a function
so what we're trying to do is so here's lambda one here's lambda two and we have a function
which is just the square root and we know that f applied to lambda one is two and f of lambda
two is four now this is not drawn to scale in any way but what we're trying to do is
find a polynomial that goes through these two points now you know that two points determine
a line so a straight line goes through these two points and that straight line of the form
y equals mx plus b so our goal is to find out what are m and what are b such that
when we plug in x which is our values of lambda we get the corresponding values of y
so this isn't a very difficult problem but what we're going to do is set it up as a linear algebra
problem even though you could probably immediately solve for m and b and the reason we'll do that
will be made more apparent later when we try to compute f of matrices of larger sizes
where it will be more difficult to do the simpler method and it's more reasonable
to solve that system of linear equations using techniques of linear algebra
so when we set this up we write on this side since this is our y we have m lambda one
plus b and this equals m of lambda two plus b and our unknowns are m and b so if we set up our
matrix system we get and what i'll do for convenience is i'll put the ones on the left
so i'll put my b's on the left column so it's really b plus mx one one and then this is lambda one
lambda two and our two corresponding values f of lambda one which in this case is two and four
and we know what lambda one and lambda two are they are four and sixteen
so really this is equal to one four one sixteen two four
and if we try to row reduce this system and solve it what we end up getting is
b equals four thirds
and m equals
one sixth so this line is of the form y equals four thirds plus one sixth x
and that's our polynomial this is our q of x
and what we'll do in the next video is we will actually apply this polynomial to our matrix
and see if it also satisfies the same equation so here's the polynomial that we found
as a real valued function in this case and if we wanted to define q of any matrix i'm just
going to write a but it's for any matrix a we would the associated polynomial on matrices
would be four thirds times the identity matrix which in this case is an n by n matrix well
in this case it's two by two matrix plus one sixth a so let's see what happens when we actually
compute this so we have four thirds of the identity
both along the diagonal plus one sixth of our matrix a so it's 10 over six which is five thirds
one one five thirds and if we add these two matrices what do we get nine thirds which is three
one one three which is exactly what we found for f of a before so we already know that when
we square this matrix we get exactly our matrix a back now let's look at the more general situation
so we're going to go back to our setup where we have an n by n matrix a a function f on the
set of eigenvalues so we write if a is n by n and lambda one through lambda n are the eigenvalues
and f is a function
on the set of eigenvalues to let's say the complex numbers we're going to find a polynomial q
that first satisfies the initial equation we wrote down for the associated eigenvalues
so our goal is to find a polynomial
q such that q of when we plug in our corresponding eigenvalues we get f applied to those corresponding
eigenvalues and we already know that that's problem will help us solve this one by a
similar analysis that's why we're reducing our problem to finding a polynomial on just a finite
set of numbers rather than trying to find the answer to our matrix problem and in fact when we
look at the degree of this polynomial we notice that it was also matching the degree of the size
of our matrix and that's going to be true in general we'll be able to find the polynomial whose
degree is at most the size of the matrix that will solve that problem namely q of a equals f of a
and why that happens is precisely because of this equation because there are going to be
at most n distinct eigenvalues and so we only need to find a polynomial so let me draw this as
visually let's just assume everything is real so it's simple to draw this so if we have lambda 1
here lambda 3 here lambda 2 maybe another lambda 4 somewhere out here and let's say lambda 2 equals
lambda 5 for instance and if we apply f to these numbers let's say they look something like this
what we're going to try to do is find the polynomial that fits through these in this case four points
and the reason it's four is because two of our eigenvalues are repeated and so we have to find
the polynomial through these four points so and if we had n distinct eigenvalues we would have
n distinct points through which we would have to find a polynomial sorry i misspoke i think i
said degree two i meant degree one because one is the highest power but it starts from zero
so in this case we would find a degree in this case we would find a degree three polynomial
and in general it would be at most n minus one degree so and again if we have multiplicity
that's non z that's um bigger than one then the problem is going to be a little bit easier
to solve because we can find a polynomial of a lower degree so let's just assume
that all eigenvalues
are distinct just it's not it's not actually making our problem easier it's making it a little bit
harder because if some of them repeat then the problem is reduced to a smaller and simpler
matrix algebra problem so if we assume all the eigenvalues that are distinct we're really doing
the hardest case now when such a thing happens we can write our polynomial q of x as a zero plus
a one x plus a two x squared all the way up to the highest degree which you know just by
looking at the pictures we're assuming it's of the form a n minus one x to the n minus one
and if we write down all of these different equations we're going to get another linear system
and the unknowns of that linear system are these a's
and we know the values of x's those are different eigenvalues and we know the q of those x's are
it's f applied to those values so the associated linear system that we get
that looks like one ones along the vertical on the left side corresponding to the coefficient
in front of a zero the coefficients in front of a one are the different eigenvalues
the coefficients in front of a in front of x squared are the squares of our eigenvalues
and then the coefficients in front of our highest degree are
our eigenvalues to the power of that highest degree
and the augmented side of our matrix is the value of those different eigenvalues
so our goal will be to try to solve this system well actually our goal is a little bit easier than
that the statement of the theorem says that there exists a polynomial q that satisfies the equation
q of a equals f of a and so all we really have to do is show that such a polynomial exists
so we don't have to solve this solving it is what is q so given a matrix a what is what is q
the what is that polynomial q we're just trying to show that one exists in other words what we
want to do is answer the question does a solution to this system exist and if we want to know
how a solution exists
if well if we can solve this system right and one criteria that allows us to solve this system
is that if this matrix here which is an n by an n minus what is this an n by n matrix
right it's an n by n matrix and if this matrix is invertible and when is the matrix invertible
if the determinant of this matrix is non-zero so solution exists if the determinant of
this matrix which is called a van der man matrix
if this determinant is non-zero
so what we're going to do is it's going to be a little bit of a brute force method
but we will find one way to compute the determinant of this matrix and therefore
show whether or not it's zero and see if we can answer our problem
whenever we have a problem with arbitrary n it's a little bit difficult to see what the
pattern is without doing an example so i think it's good to try out a simple example
or at least somewhat simpler by computing the determinant of the same matrix but where n equals
let's say three so we have a three by three
and we want to compute this determinant and we want to compute it in such a way
so that we can use some of the ideas for computing this determinant
and abstract it to that more general case now this isn't the most simplest way to do such a thing
but it's one way and i'm sure there are many many other ways to compute this determinant
some of which may be certainly more clever than the approach that we'll take
so we're going to do this by essentially row reduction
and for the first step we're going to get rid of the ones underneath the top left one
and by just subtracting the first row from those
so if we do that that doesn't change the determinant and we get the top row is left alone
and then the rows below it look like zero zero lambda two minus one
lambda three minus one and this becomes lambda three cubed minus lambda one cubed uh sorry squared
and lambda two squared minus lambda one squared
now when we uh lambda two minus lambda one is actually a common factor in this second row
because this becomes lambda two plus lambda one when we pull that out
and this is lambda three plus lambda one so when we distribute out we get lambda two minus lambda one
lambda three minus lambda one times the determinant of what's left over which is one
lambda one lambda one squared
zero one zero one lambda
one plus lambda two lambda one plus lambda three
and this happened because the determinant remember when you take the determinant and you multiply
any row or any column by a number you can distribute out that one number for that one
column in this determinant you can think of the volume if you scale one side of the room
by a factor and another side of the room by a different factor then the determinant is computing
the area and you scale by both of those but for each side you only distribute one of them
so now we're looking at this and we want to compute the determinant of this
now of course what's left over is a two by two so it's very easy to compute the determinant
but if we wanted to have an inductive proof if we did a similar calculation here for a larger
matrix what we would have is lambda one through lambda one to the n minus first power up here
and we have a much larger matrix which isn't very easy to compute the determinant of
by some explicit formula it's sort of complicated to write so what we want to do is we want to
think of how to compute this maybe more conceptually and what we can do is notice that lambda one
appears here in each of these two terms and if we multiply the second column by lambda one
and subtract what happens is this cancels the lambda one cancels the lambda one cancels and
you're only left with lambda two and lambda three and you also don't change the determinant because
you're taking one column and adding it to another so this is also equal to
the determinant of what's left over after you do that subtraction
this is zero zero one zero one and then just lambda two and lambda three left over
well you can even do something even a little bit more simpler now now you have a one here
you can multiply this by lambda one to get rid of that so i'm not even going to write that whole
step out we can just erase this and put a zero here and now here's the amazing part what's left over
after you perform these operations is another van der man matrix on the bottom right corner
and we can continue this process now because the determinant of this because this is a one
is equal to the determinant of this so we've reduced our problem
from an n by n matrix to an n minus one by n minus one matrix of the same form
and if we keep going down further up until maybe this step or even further than that
then we would find out what the determinant of this matrix is so if we did that procedure again
of course you can compute the determinant of a two by two no problem but if you did that procedure
again subtract you get a zero here move that over you end up getting lambda two minus lambda three
it's already of that it already breaks up like that pretty easily and you get lambda two minus
lambda three that pops out so you end up getting is the product of i and j let's say i is less than
j and j is less than or equal to three and i is greater than or equal to one of lambda j minus
lambda i so you actually get the product of the differences of all of these different eigenvalues
and because we're assuming that the eigenvalues are distinct all of these numbers are not zero
therefore this is not equal to zero and so we automatically know that the determinant of this
matrix is non-zero so we can make a guess that the determinant of that more general matrix
of that more general van der man matrix
is exactly the product of the differences of all of the eigenvalues
and therefore is not zero if they're distinct
and we can prove this by induction we already know what happens when n equals one
or when n equals two and not even n equals three and so what we can do is if we assume that this
formula is true for n and go to n plus one then what we want to do is reduce that problem to this
one and show that those numbers factor out and then we can apply our induction hypothesis
and prove that this formula holds more generally
and the way we do that is very similar to this so i'll put a question mark here
and i'll write what this equals by doing this first step which was here sorry this first step
in subtracting the first row from all of the rows below it what we end up getting is
the determinant of and here we have a bunch of zeros below the ones so we have one
and i'll write two rows just so we see more of the pattern uh this is a zero sorry zero
lambda one and then this is lambda two minus lambda one
and this is all the way down to lambda n minus lambda one
all the way up to and let me write two additional terms here this is going to be lambda n minus two
lambda one n minus one now this is lambda two to the n minus tooth power minus lambda
one to the n minus tooth power
and here we have lambda n minus one minus sorry two minus one
so
that's a one
okay
now at this point we can follow a similar procedure by pulling out a lambda two minus
lambda one from each of the terms but then we would have to figure out what is lambda two to some
power minus lambda one to that same power divided by lambda two minus lambda one we could do that
and factor it out by using um polynomial division find out what the corresponding factors are but
maybe that's not the best way to do it another option although that method of course you know
teaches you a lot about how to do polynomial division in case you haven't seen it before it's
quite nice but maybe there's another easier way similar to what we did over here and what we did
here was we took the second last column and we multiplied it by lambda one and we took the
difference here we could have also done that in this step it just might have been a little bit it
might have looked a little bit more complicated because of the higher powers but let's try to
do that anyway if we multiply the second last column by lambda one from the last column
the power here will be n minus one which will match this one and these two terms will cancel and
you'll just get zero what happens to this term if you multiply this by lambda one so let's write this
out so we have lambda two n minus one minus lambda one to the n minus one minus multiply this whole
term by lambda one that becomes a plus lambda one to the n minus one and then what's left over is
minus lambda one lambda two to the n minus two these two terms conveniently cancel and what
you're left over with is lambda two appears the highest common factor is lambda two to the n minus
two so we can pull that out and what's left over after we pull that out is lambda two minus lambda
one and therefore we can much more easily see that this factors out after we do this subtraction
now we've done imagine we've done that for the last column here now we have this second last
column which still has all of these complicated terms but what does this term before it look like
lambda one to the n minus three and then it's lambda two to the n minus three minus lambda one
to the n minus three so you can just see it's of the form n minus j and if we multiply this by
lambda one and subtract it well these two terms will cancel and a similar thing will happen here
it's just that the power will now be not lambda two to the n minus two but lambda two to the n minus
three after we take this difference and so if we keep going in this direction taking all of those
successive differences we will be left over with so this determinant equals
the product of lambda j minus lambda one and j goes from two to n
and we're left over with the determinant of
a smaller bandermen matrix which looks like one zero zero and this term is one
and it's all the way one's all the way down let me write just the first and last ones
we also have zeros here up to the last term
now what is this term here it's lambda two to the n minus two now
all the way down to lambda n to the n minus two and if we assume the induction hypothesis
then we know that the determinant here is the product of lambda let me use a different letter k
and l so k minus l where k is greater than strictly greater than l and l runs from this
time two to n and and and k so we end up getting after all of this work by using that induction
hypothesis we get that this is this expression right here
and in particular this says that our determinant is non-zero so we can compute the inverse of
this matrix if we wanted to now that we have all of this set up we can prove our main theorem
which remember said that given any diagonalizable matrix a there exists and a function f on its
set of eigenvalues there exists a polynomial q such that q of a equals f of a and so far
based on the facts that we just proved we know there exists a polynomial
q such that q of lambda i equals f of lambda i for all of the eigenvalues
of that matrix therefore
if we compute f of d which was defined to be f of lambda one f of lambda n
of our diagonal matrix d then this is the same exact thing as q of lambda one
q of lambda n with zero everywhere else by this result we can find a single polynomial q that
satisfies this but this is exactly the same thing as q of d well why is that well if we
write our diagonal matrix d out and we apply the polynomial q to it right so let's just see why this
is true if we take our diagonal matrix and then we plug in our polynomial so we had what was it
it was a zero times the identity n by n matrix this is what if we view q as a polynomial and we
plug in the formula for q of d this is by definition of a matrix applied to a polynomial
sorry a polynomial um with input a matrix plus a1d plus a2d squared plus a n minus 1
d to the n minus 1 and we know what this looks like as a matrix this is the identity
it looks like a zero all along the diagonals
and zero everywhere else this is a one times lambda one all the way down to a one times lambda
to the n lambda n and then here we have plus a2d squared now d squared since d is the diagonal
matrix is just lambda i squared in each of the diagonal terms so it's a2 lambda one squared
all the way down to a2 lambda n squared and similarly for all of the other terms
up until this last one then what happens when you add all of these matrices together well you get
a zero on the top left term you get a zero plus a1 lambda one plus a2 lambda one squared plus dot
dot dot a n one minus lambda one to the n minus one that's exactly what q of lambda one is
and similarly for all of the other terms so this justifies why this equality holds
and of course q of any matrix is defined similarly so in particular q of a equals a zero times the
identity plus a1 times a plus a2 times a squared and so on so now let's show that f of a equals q
of a now f of a by definition of f of a is p times f of the diagonal matrix times p inverse where p is
the matrix of eigen vectors corresponding to those eigen values is a matrix of eigen vectors
now f of d by this calculation is also q applied to d
and so that equation is true by what we just showed
now we know what q of d looks like it looks like this and we also know what happens when
we distribute p throughout so we get something that looks like a zero p times p inverse plus
a1 pd p inverse all the way up to a n minus one pd to the n minus one p inverse
that's just what that looks like when you distribute p and p inverse on both sides
now this is a and what is this expression and likewise for all of the terms in between well
let's just let's just look at what happens if we um if we set f n is like three or something like
that or maybe even two is enough um so let's look at this term p d squared p inverse so p
d squared p inverse also equals p times d times d times p inverse and because p and p inverse are
well inverses of each other we can plug in a p inverse p between these two d's and
this gives us p d p inverse times p d p inverse again and this is just a and this is just a
so we get a squared therefore when we actually write out what all of these things equal we get
a zero p p inverse plus a one which is the identity sorry this is the identity matrix
and this is a plus a two a squared plus all the way up to a n minus one a to the n minus one
and this is the definition of q of a so this shows us that that theorem is true
so this has an interesting corollary
so let a be diagonalizable
and let b be any square matrix of the same size
and suppose that they satisfy
the fact that when we multiply them in any order they're equal to each other
then f of a
b equals b f of a for all functions
that are defined on the eigenvalues of a
and how do we prove this
well because a is diagonalizable then f of a equals q of a for some polynomial
q and because it's a polynomial if we replace this expression with q of a times b
so if we have q of a times b this is a polynomial in a and each of the terms look like
a to the jth power times b
right so you have a to the jth power times b now a to the jth power means you write the
matrix a j times and if you have a b on one side you can use this to move each of those a's
one over at a time you can move them over one at a time
therefore a j a to the jth times b equals b times a to the jth therefore
it's immediate that this equals b times q of a and it immediately solves this problem because
q of a equals f of a and the interesting thing about this is that b can be any matrix whatsoever
and a only has to be diagonalizable for this to be true
so hopefully this is an interesting fact namely that given any function at least that's defined
on the set of eigenvalues of a it could be defined on a larger set of the subset of the complex numbers
but at the very least if it's defined on those eigenvalues then we can always find a polynomial
for which when we apply that function which could be completely wild such such as the
logarithm or something like that then there's a polynomial that gives us the same value for that
matrix if we apply the polynomials of the matrix versus if we apply the function to that matrix
and a lot of this has to do with the fact that we're working with finite dimensional matrices
one of the interesting things about linear algebra is what happens when your matrices become of
infinite order and then this really becomes a much more subtle issue and clearly the method that we've
used should probably break down for instance we're not working with polynomials anymore
and a lot of this is explored for instance in functional analysis and spectral theory
and the functional calculus for such operators
in these next few videos we'll learn about affine subspaces affine combinations and affine
transformations which are very slight generalizations of linear transformations as we'll see
so the first definition that we'll need is what an affine combination of vectors is
so but to do that we'll recall what a linear combination is so a linear combination
of vectors v1 through vk in rn is a combination of the form
lambda 1 v1 so we add up all our vectors with some weights and these weights
will take to be real numbers
so that's what a linear combination is
and closely related to this an affine combination
of these same vectors
is a linear combination
and for short I may often write just using the summation notation
oops let's call this not k but j and this goes from j equals one to k
such that the sum of these coefficients is equal to one
so it's basically a linear combination but we have an additional constraint on the
coefficients so for example when k equals two we have two vectors let's say v1 and v2
then every such affine combination is of the form t v2 plus one minus t v1
where t is a real number and you can look at what this says let's say these two vectors are
different let's say v1 is here and v2 is here then at t equals zero so this right this is
describing the set of all such combinations and when t equals zero this gives me v1 so at t equals
zero i'm here and when t equals one i'm at v2 and as you vary t over the set of real numbers
you get all the points along the straight line through v1 and v2 this is very different than
the set of all linear combinations of v1 and v2 because if let's say the zero vector were here
then v1 would be this corresponding vector v2 would be this corresponding vector and all
linear combinations of these two vectors is actually the plane obtained from v1 and v2
that's what the span of these two vectors are but all affine combinations is just this line
and so just like we can define the span of vectors we can also define the affine span of vectors
so the affine span
of the vectors v1 through vk is and we denote it by aff
and it's defined to be the set of all affine combinations so the set of all
lambda j vj such that all of the lambda j's are in r and the sum of them equals one
so let's look at another example where we take three vectors so let's say v1 v2 v3
and let's just be concrete and let's say we're in r3 so that we can visualize this a little bit
better so there are several cases that we can take just like for linear combinations for instance if
one of these vectors was a linear combination of the other then the span of this would be a plane
and if all of them are scaled on multiples of each other then the span is a line
and if they are all the zero vector then we just get the zero vector and if they're all
linearly independent then we get all of r3 there are many different cases depending on the
relationships between v1 through v3 same thing happens for affine span in the sense that it
depends on how these vectors are related so let's look at three possible cases
so case one let's say v1 v2 and v3
are not collinear so this means that all these three points don't lie on the same line so maybe
they look something like this like for instance you can take the unit vectors e1 e2 and e3 and r3
then the affine span of these three vectors is equal to the two-dimensional plane
containing these vectors
and it's not so immediately obvious that that's what happens but let's just
think about this if we take v1 and v2 then it includes the affine span
of these two vectors which means we have this line through these two vectors is in our affine span
and likewise the line through v2 and v3 is here likewise the line v1 through v3 is here
and now that we have all of these lines in here we can also take affine combinations of these points
so you can take for instance the affine combination of this point with this point
which gives us this line this point with this point which gives us this line
and you can see by taking all such combinations all such affine combinations
of these three vectors we can actually get any point in the plane that contains these three points
in case two
let's imagine that v1 v2 v3 are collinear
but
at least two
are distinct
so in this case so i'm assuming that at least two so either the possibilities are something like
they're all different but they lie on the same line in which case the affine span of these three
points is equal to the straight line through those two points those three points or the other
cases the affine span if two of them happen to coincide then we just have two points
but i'm assuming that they're collinear and at least two are distinct so we also get the
straight line through those two points and the final case case three
is when all those vectors are exactly the same vector
and when this happens we only have a single point and all affine combinations of a single point
is just that point itself so these are some of the basic constructions that you can do with
vectors besides just taking linear combinations you can also take affine combinations there's yet
another type which we won't discuss is if you require that the sum of these coefficients adds
up to one but they're also not just real numbers but they're strictly non-negative so they have to
be at least zero and that's called a convex combination which is a closely related idea
and in the case of these three vectors for instance it would be the triangle
who's three vertices are those three vectors that we had here and in this case if we took
convex combinations it would be the interval between these two farthest end points and in this case
we would have the same situation as we had here where we would just get a single point
a common question that we ask given a set of vectors is if we have another vector when is
that vector in the span of those vectors and this shows up for instance if we solve a homogeneous
linear system and we have a bunch of solutions that we know are actually solving that system
but let's say we don't know exactly what that system is we just know we have this collection of
solutions and if somebody hands us another vector then we can ask is that vector
a definitely a solution of the system that we have and in this case since we don't know the
system we can't plug in that vector to check instead what we have to do is check if that vector is
in the span of the vectors that we have already if that vector is in the span of the vectors that
we already have then that vector is definitely a solution but it doesn't tell us that if it's not
in the span of those vectors and it's not a solution because we might not have had
a set of vectors that span the solution set but at the very least it gives us a criteria for
guaranteeing that if that vector is in the span it's definitely a solution and likewise you can
ask well if I have a bunch of vectors that I happen to know solve an inhomogeneous equation
and somebody hands me another vector is there a similar criteria and there is and that involves
the notion of affine span which we talked about in the last video so the question that we could
ask is given vectors v1 through vk and another vector u in rn
when is u in the affine span of these vectors v1 through vk
now in order for us to solve this problem then we have to be able to write u as a linear combination
of v1 through vk right but because it's an affine combination we have an additional
constraint on what these coefficients could be and that constraint is that lambda 1 plus lambda k
equals 1 which is also a linear system in the unknowns lambda 1 through lambda k
and therefore if we want to solve this system this question is equivalent to
the following one which is is the augmented matrix where we take our vectors v1 through vk
and we can also write vk through vk through vk through vk through vk through vk through vk
augmented with the vector u but in addition augment this further by one additional row
stating that one equals so now this is the number one equals one dot dot dot one let me write this
one so it's clear so this vector is just denoting the fact that it could have several entries
so we have an additional row in our augmented matrix and the question is is this consistent
so this is actually how we would solve such a problem and
how does it show up in solving inhomogeneous systems we'll get to that after we talk about
what an affine subspace is and the fact that the solution set of an inhomogeneous system
is an affine subspace so for this let's just briefly recall
a vector subspace i'll put vector usually in parentheses but a vector subspace
of our n is a first of all a subset let's call it v
such that three conditions hold now there are many equivalent ways to define such a thing
but this one seems pretty concise and simple and the first condition is that the zero vector is in v
the second condition is that if you take a vector in v and you scale it by any number
then that scalar multiple is also in v so lambda v is in v provided that the vector v was in v to
begin with and lambda is a real number and three the third condition is that if i take any two
vectors in v then the sum of them are in v so let's write u plus v is in v for all pairs
u and v that are already in v and this is what a vector subspace is
now this definition of a vector space is a little bit algebraic it's telling us when
certain vectors are in v and we can have a little bit more of a geometric interpretation of a what
a vector subspace is by using affine combinations so equivalently v satisfies
which means that if v satisfies the following conditions i'm about to write then it satisfies
this one and conversely let's call it instead of i and two so let's use i because the first one's the
same the zero vector is in v and the second condition which is sort of a combination of these
two is that t u plus one minus t v is in v for all t in real numbers and for all u and v in v
now this is exactly a linear combination of the vectors u and v so if i take two vectors u and v
inside of v then this affine combination is describing the set of all points
along the straight line through those two vectors so this is saying that a subspace
can also be described as a plane that contains the zero vector and plane could mean hyper plane
and this is because we always have the straight line through any two points in our subspace
now the fact that we've written it this way allows us to define an affine subspace
in a much more closely related fashion to this definition because for an affine subspace we'll
only be able to combine combine vectors in an affine way so we define an affine subspace
is a subset a of r n such that and now we drop this first condition so all we require
is that affine combinations of two vectors are always inside so t u plus one minus t v
are in v for all same conditions as here
and you can ask well maybe an affine subspace should be if i take any collection of points
inside of it then the affine span of those points is inside of v and that actually follows
from this condition and the usual properties of scalar multiplication and vector spaces
and how you add them so the main example that we want to illustrate is the solution set
of any linear system ax equals b this is just notation for a linear system where b
is a vector in r m and a is an m by n matrix
so the solution set of this is an affine subspace
of r n
now the solution set of an inhomogeneous system is not a vector subspace because in
general zero is not a solution in fact when zero is a solution then it exactly is a subspace
and when zero is not a solution we get this more general notion of an affine subspace
and it's a fact
that affine subspaces
are translates
of vector subspaces
and what do i mean by that
a is an affine subspace
if and only if there exists a vector v in r n such that if i take the subs
if i take this affine subspace a and subtract v from it now what this means is the set of all
vectors of the form u minus v where u is in a
if this subset of r n is a subspace
in this sense is a vector subspace
in fact
we can use any vector inside of a to translate it to the origin so in fact v
will be a vector in a in fact any vector in a will make this a vector subspace
so the picture for this is actually really nice
i guess i shouldn't have called it a because i called this linear system a that may be
potentially confusing so maybe let's call this script a so let me use a script a here
and fortunately the letter a was only used in this one example but let me write it like this here
so it's the same so there's no conflicting notation
okay so here's our affine subspace a and if we take any vector in here let's call it u no let's
call it v so v points from zero up to where that vector is and if we take this vector and we
subtract it then v minus itself will be zero so i know that this plane is going to contain the zero
vector and so here we have a minus v and no matter which v we picked right if we picked another one
let's say we picked this vector right here let's call this one u then if we translate that
u minus itself is zero so we also get this plane back as well
and so a good application of this
of this sort of mathematical object is
if the vector xp p for particular is a solution to ax equals b for some linear system like in the
previous example then the solution set meaning all the solutions of ax equals b
is as we know the particular solution plus the homogeneous solution set so it's a set of all
all sums of particular solutions with homogeneous solutions so axp solves the system this and
ax homogeneous solves the associated homogeneous system so if a represented the solution set
of an inhomogeneous system and a minus v represents the solution set of a homogeneous system and all
we have to do is pick one of these solutions and then all of these solutions and then take that
solution and translate it by that vector which was a particular solution of the inhomogeneous system
just as we can define linear transformations which are functions that take linear combinations
to linear combinations we can also define affine transformations and the idea is that
they take affine combinations to affine combinations which translates geometrically to
it takes lines or hyperplanes to other lines and hyperplanes as well so the definition
of an affine transformation is exactly that
an affine transformation in this case from rn to rm is a function first and foremost
and i will write my arrows as usual from right to left so it's a function let's call it s
such that
s of lambda u plus 1 minus lambda v is equal to lambda s of u plus 1 minus lambda s of v for all
u and v in rn and for all lambda in r and it's a consequence of this definition that if we take any
affine transfer if we take any affine combination of vectors then s of that affine combination
is going to be
the affine combination of s applied to each of those vectors
this is a little less obvious than it is if you take linear transformations and you show that
it follows from the assumptions of a linear transformation that it takes linear combinations
to linear combinations and the reason it's a little bit slightly more challenging is that
if you apply this in a binary fashion right if you take two vectors u and v so you think of this
as a function from let's say r cross r to the n cross r to the n to r to the m then in order to
apply this here you have to put parentheses in an appropriate place but in order to have an affine
combination with the appropriate parentheses you have to be a little bit careful about what
your resulting coefficients are and it's not so easy to see how to do that but it can be done
and here's the example that I really like to think of when comparing linear transformations
to affine transformations and things you might have seen from a while back not in my lectures but
in your early learnings of math perhaps so if we take the usual equation of the form
y of x equals mx plus b where m and b are both real numbers and x is a variable
and y is the function of x then this is an affine
transformation
from r to r because it takes a real number r x and it gives us another real number
and it's linear
if and only if b equals zero linear in the sense of being a linear transformation
so this will help you perhaps relate the difference between an affine transformation
and a linear one and we'll later talk about a theorem that relates the two exactly together
in fact we'll state that theorem now so the theorem says the following are equivalent for a function
now we're just describing a function
and these conditions are that s is affine is an affine transformation
so i'm not assuming any linearity this is just an ordinary function so s is affine
if i take the function s and subtract s of zero from it so if i take s minus s of zero
now this is a function in the sense that if i take any x the function associated to this is
defined by s of x minus s is s of zero so this is also a function from r into rm if this is linear
and c
there exists an m by n matrix
m and a vector b
in rm such that s of x equals mx plus b
and the reason i mentioned this example is precisely because of this theorem because it allows us to relate
linear transfer affine transformations to transformations that we may have seen a long
time ago and i personally think it's instructive to prove this theorem to get a feeling for how
affine combinations work so let's actually prove it and we'll prove this by proving a implies b
implies c implies a so for the first part of this proof we're going to define we i don't want to
keep writing s minus s of zero so we're going to define l to be this function s minus s of zero
and the goal is to prove that this function is linear so we have to check the associated
conditions for linearity and before we do that let's just establish that if we apply zero to l
if we apply l to zero then we get exactly zero because this is s of zero minus s of zero so it
definitely preserved zero and we know that this doesn't give us a sufficient condition for linearity
but it's definitely necessary so second if we take a coefficient lambda any real number lambda
and if we take a vector u that's inside of our n then by this definition this is s of lambda u
minus s of zero and this is an interesting combination of lambda u and zero this also
equals s of lambda u plus one minus lambda of the zero vector right the zero vector is in the domain
of s and so i can multiply by any number and i still get zero and now the interesting thing
about this is that this is an affine combination of the vectors u and zero so that's what this
term is and this just comes along for the ride because s is affine i can take these coefficients
out
and this is also an affine combination of itself so i can write minus lambda s of zero
minus one minus lambda s of zero
and so what do we have we have lambda of s of u in parentheses minus s of zero
which is exactly l of u and these two terms cancel so we're left over with lambda
l of u when we're done with this calculation so it's linear in this it's the first condition
of linearity is proven and the second condition is if we take a linear combination
this also has to go to a linear combination as well so let's just use the definition this is s of
u plus v minus s of zero and now let's draw a picture here because this is going to help
let's say we have the vector u here and the vector v here and this is the zero vector
now the vector u plus v is somewhere here
now can we express u plus v as some convenient affine combination of vectors for which we know what s
does to those vectors well if we extend u so we take combinations of u and combinations of v
then u plus v can be written as an affine combination of some multiple of u and some
multiple of v in fact it can be written like that in many ways all i have to do is pick any
point here and draw the straight line through this point and u plus v and then find out what
that vector is or we can take a simple shortcut and just notice that if we multiply this by two
this by two then
those two points to u and to v are on the same line that goes are on the line that goes through
u plus v and how do i know that well if i take half of this and half of this i get exactly this
and half and half is an affine combination so this equals s of one half to u plus one half
to v minus s of zero and because this is an affine combination we have one half s of two u
plus one half s of two v and now we can also subtract half of s of zero here
minus one half s of zero again and now one half is a common factor here so this gives us one half
l of two u plus one half l of two v but by the thing we just proved we know that we can pull out
scalars from l so this gives us l of u plus l of v and this together proves the linearity
so this is the proof that a implies b but if we have an affine transformation we subtract
by what it applies to when you plug in zero then we get a linear transformation now the rest of
the proof is actually not bad afterwards because for b implies c if we have a linear transformation
we already know we have a matrix corresponding to it so because l is linear
we get an m by m matrix
such that l of x equals m of x
equals m times x for all x in the domain of s which is r n
so set b to be equal to s of zero and when we make when we set that to be that then since l is
s minus s of zero then we take s equals l plus s zero which is b then we get y then we get the
equation of the form s of x equals m x plus b so that that's what how b implies c and then
if we have c to imply a this is much much it's very similar to these kinds of calculations of
taking affine combinations if we take s of like let's say lambda u plus one minus lambda v plug
that in here we know m acts in a linear way this is a matrix we apply matrix multiplication
distributivity associativity of all these properties of addition of vectors and scalar
multiplication of vectors in r m and we get that s is affine from this assumption so these three
conditions are equivalent for any function from r n to r m that characterize what it means for
transformation to be affine as we know functions can be composed provided that the domains and
codomains of these functions match up similarly affine transformations compose and the composition
is affine in an analogous way to how linear combinations are composed and the resulting
composition is also linear so we have a fact and this fact is that the composition
of two affine transformations s and t
is also affine
and because it's affine and we know that each of these transformations can be written in the form
of mx plus b for some appropriate matrices and appropriate vectors b we can ask what is the
resulting matrix for what is what are the resulting matrices and vectors for the composition
of two affine transformations so let's write s of x as mx plus b and t of y as nx plus ny plus c
and let's just be careful about composing these so if we take the composition s composed with t
and we apply a vector y then this by definition is s applied to t of y
and we know that t is of this form
so we get ny plus c
and this equals m times the input of this function which is ny
plus c plus the associated b oh this should be a plus from the transformation s
and if you distribute this all out we get mn
times y
plus mc plus b so the associated matrix that we get is actually just the multiplication of
the matrices that we started with and the associated vector b is some interesting combination
of the original vectors b and c but also with the matrix m and in particular
if s from same setup rm rn to rm is invertible
and we wrote our decomposition like this then we could ask what are the matrices and vectors
associated to the inverse of this matrix and that is exactly
so s inverse let's write of y just because we're changing the
codomains with the domains we get the inverse of m plus well rather minus
m inverse of the vector b and why does this work well if you just take s for instance and you
apply it to this result we know what this combination looks like we get m applied to this term which
gives us just y back m applied to this term which gives us negative b but we have a plus b and those
two cancel so just like the composition of linear transformations need not commute similarly the
composition of affine transformations need not commute so let's look at an example
and a common affine transformation is leave everything alone just translate by some vector
so let's just keep things very simple and let's assume that we translate by the vector
one zero so we shift everything along the x axis
in r2 so we shift everything along the x axis so let's say the vector let's draw a smiley face here
this smiley face transforms under this transformation let's say smiley face is it
contained in the unit box so i have to make this a little bit bigger and it gets translated along the
x axis in the positive direction so let's call this transformation t
another transformation that we can look at let's call this one s
is rotation by 90 degrees so when we rotate the face looks something like this
and then we can ask what happens when we apply s and t in that order or if we apply t then s
and what are the matrices and vectors associated to these transformations let's actually answer
that question first so t of any vector x equals well it's just translate so it says
leave everything in the plane alone so that's the matrix corresponding to the identity and
shift by the unit vector in the x direction so i call that e1 so remember e1 equals the vector
1 0 and s of x is the transformation that rotates by 90 degrees so i'm going to write
that in matrix form because rotation by 90 degrees is 0 negative 1 1 0 applied to the
vector x and the b here is 0 because this is in actually this is actually a linear transformation
so what happens when we compose these in different orders so let's just think about this imagine
you translate first and then you rotate this rotation is occurring about the origin so when
we apply t first and then we apply s again we're rotating this picture by 90 degrees with respect
to this origin so this face is actually going to be further out than it would have been if we
applied the transfer if we apply the rotation initially and then translated you can already
see the big difference between these two pictures so if we apply first t and s apply to this picture
let's start with our initial configuration that what happens after you apply this will first you
rotate and then you translate so this translates everything to something that looks like this
but if instead we applied s after t to the same initial configuration
well first we would translate and then we would rotate by 90 degrees that would look
much much different so if i were to draw this as a unit grid
that face would now be in this box rotated by 90 degrees so it looks something like that
so now let's just check the math out to make sure that this is consistent with these geometric
interpretations so if we apply t after s to any vector x
what do we get well t says first translate then rotate so we end up translating by x
then rotating because we do matrix multiplication and the resulting vector b is just e1 so we get
rotation apply to x plus e1 which is exactly what we expected from our picture here if we did it in
the other order well in that case first we translate and then we rotate and when we rotate
we not only apply the rotation to our initial vector x but we also apply the rotation to the
vector e1 and e1 gets rotated by a 90 degree rotation to the vector e2 so in this case we get
this instead so and this is consistent with this picture because if we rotate first our face ends
up somewhere here like in this picture and then how do we get from this picture to this one we
translate up by a unit vector by the unit vector e2 the next few videos are going to be a sort of
combination of probability theory and matrix algebra and we'll start by talking about finite
sets and stochastic matrices or what I call stochastic maps and we'll try to get through
a lot of interesting topics so first I just want to make sure that we have all these definitions
at hand and the first one that I want to make is a probability measure
and for simplicity we will be working with finite sets all the time so a probability measure on x
where here x is a finite set is a function that takes every element of x and it gives me a number
and that number is between 0 and 1 and the sum of these numbers
when I sum over all elements in x and let me just set notation that when I apply this probability
measure to x instead of writing p of x I will write p subscript x so such that the sum of these
numbers equals 1 and a stochastic map is something very similar to this
ah and let me even set some more notation the set of all probability measures
on x is denoted by px
so a stochastic map from x to y so another finite set is a function
from x to probability measures on y let's call that f and
we're going to introduce a convenient notation for such stochastic maps
so first let's explain a convenient notation for how to write f so if we take an element x
and we apply it we'll get a probability measure on y for now let's just call this f of x
because this is a probability measure it takes an element y and y and gives me a number between
0 and 1 so this takes an element y and maps it to f of x of y now it's a little bit annoying
to write something like this and potentially confusing so instead of writing this we will write
f subscript y x and the reason we write the y on the left is because we will end up in y
and x on the right because we started in x we'll see why this is convenient in a moment when we
talk about composition of stochastic maps and we'll also introduce graphical notation for this
instead of writing a map from x to py we will replace this by a map from x to y
but we'll use slightly different notation for our arrows and we'll make them squiggly arrows like this
and the reason we want to do this is because there's a very nice example of a stochastic map
if we have a function so if x to y is a function
this actually gives us a natural stochastic map
and just for this example we'll call it delta f oops these should be squiggly arrows now
so delta f to y which sends an element x to a probability measure on y and what should that
probability measure be well if i take let's call this delta f for now if i take an element in y
and i plug in our initial element x so again we're using this notation here then this is
defined to be the chronicer delta so if we take the element x apply f to it we know what that is
because we have a function already and then we plug in y so visually how do i think of something
like this well a stochastic map is telling us if we start off in x let me draw the arrows
backwards for a moment then it takes an element in x and it spreads that element out over y by
giving us a probability distribution on y but if we already have a function then we know where
that element x goes it goes to a specific element which we call f of x and therefore it does give
us a probability distribution and that probability distribution is one when we evaluated at f of x
and zero everywhere else so i think of this as a deterministic process in some sense because we know
given an input we know exactly what the output would be with 100 probability
so we notice that there's this close relationship between functions and stochastic maps in fact
functions are special kinds of stochastic maps and instead of writing delta f all the time
we'll simply write xf and we will think of this as a stochastic map but we'll write it as a straight
arrow another example
there is a one-to-one correspondence between
stochastic maps
from a single element set into another finite set x so this is going to be my notation for
a set containing a single element which i'm just calling bullet and probability measures
on x why is that well if i have a stochastic map i apply an element of it i apply it to an element
of the domain and that gives me a probability measure on x but this only has one element
so i only get one probability measure so in general a stochastic map is you can think of
it as a family of probability measures indexed by the domain of that stochastic map
stochastic maps define conditional probabilities
or at least some kind of restricted notion of conditional probabilities
and the reason is because f y x you can think of this as the probability of y occurring given
that x has occurred and you can if you know if you have a definition of conditional probability
and you are looking at single element events then this definition coincides with the one
you're thinking of for finite sets and again single element events but if you're not then
we're going to think of this as our notion of a conditional probability so for being very concrete
let's take x to be the set whose elements are so pick your favorite supermarket
and let's say there's a good sale at that supermarket
and let me think of that as one element of this set x and the other element is going to be
a not great sale or a not good sale at that same supermarket so two elements
and let y be
the elements that state whether I go to the supermarket this week or you go or whatever
or I don't go so I go to the supermarket let's say this week or something like that or I don't go
and let's say if there's a good sale
let's say the probability right because I might have a lot of food stocked in my pantry I may or
may not go to the grocery store this week but if there's a good sale maybe there's a good chance
that I'll go let's say there's a 90% chance that I'll go
and if there isn't a good sale well it might be that I still need to get food so there's
still going to be some chance that I go but perhaps it'll be less I'll be less enticed to go to that
supermarket this week let's just say that there is a 60% chance I'll go
and with this information we can define a stochastic map from x to y so this actually
defines the stochastic map and we'll come back to this in several examples that we'll look at
later on because it's a nice simple example and the reason you can figure out what the rest of
this is is just by using probabilities because if there is a good sale the chance that I go
is 90% then there's a 10% chance I won't go and conversely if there isn't a good sale then there's
a 40% chance I don't go so that defines this stochastic map just like with functions we can
compose stochastic maps as well but this is going to have a really nice picture so I rather
give that its own video and we'll talk about compositions in a moment all right so if we
have two finite sets rather three finite sets x y and z and a stochastic maps between them
in such a way so that the codomain of f lines up with the domain of g and I really mean source
and target here because again if I really think of x as a function it's a map from x to probability
measures on y but the domain of g is not probability measures on y it's y itself so it's really
better to think of this a little bit categorically where I'm thinking of the target of f and the
source of g so given this given stochastic maps we can define a composition of these two and before
I write down the formula let's think about how we would do this so here's x here's y here's z
what we want to define is an ocean of composition which is determined by if I give if you give me
an element in x and you give me an element in z I want to know given x what is the probability
that z occurs and there's an intermediary y here so the way that you get that is well I look at
all the elements of y and I look at given x what is the probability of that element y occurring
let's call this let's say that this is the element y then this is f y x so given x the probability
that y occurs and going from y what's the probability that z occurs that also has a probability which
is gzy and so the probability of given x the probability of z given x is taking all of these
probabilities by varying y and multiplying the corresponding ones when they match up and then
adding them all so this is defined to be the sum over all elements in y
with their respective probabilities gzy f y x so this is what the composition of
stochastic maps is
and now you can see why I chose this notation earlier of writing our subscripts in this particular
order because if I think of these as matrices indexed by the elements of these sets that we
have then this ends up just being matrix multiplication so sometimes these are also
called stochastic matrices but I'm going to stick to the calling them stochastic maps
so let's look at some interesting special cases of this definition so first let's look at the
special case where x is replaced by a single element set y is a set x and g is a function
not just a stochastic map so let's take this special example so let's take y a function f
and a probability measure on x so first of all what is a probability measure on x look like
well if I think of x as a set so let's draw some of the elements of x here
let's say here we have nine elements a probability measure sort of gives me a size
to each of these elements so I can think of these as water droplets each with a specific size
namely the volume
so this is sort of what a generic x looks like with a probability measure on it
and the sum of the volumes of these water droplets is equal to one
now if I have a function f from y to x then the composite here gives me a probability measure on
y what is that probability measure well if I just use the definition
p followed by f and I evaluated at y this is equal to just straight from the definition
we know that this is the sum over all elements in x of the function on the left which is f
but f is a function so we know that it corresponds to the direct delta the chronicle delta f
y f of x with the probability measure px now if I substitute what this looks like this says
this only gives me a non-zero contribution if f of x equals y in other words if y
is in the image of f of x is in the image of f and it comes from some x so if we look at the
inverse image of y that's going to give me a bunch of elements and that's the only case where this
gives me a non-zero contribution and what that means is that this breaks down into the sum of all
elements x in the inverse image of y so here we have the sum of all the px's that are in the
inverse image of an element y so let's look at this element y here the inverse image of this
under a map f so let's imagine that f identifies all the elements that are in the vertical direction
so right because a function f might not be one to one so it might identify some of the elements
and that's why I've drawn it this way it takes these four elements and gives me the single output y
and these two elements gives me another output and what this condition says is that the probability
here is the sum of these probabilities in other words the volume of this water droplet is the
sum of the volumes of those water droplets likewise here in order to make the volume somewhat
geometrically similar to these this would be the resulting volume after we apply this function f
and here maybe it's this big so this gives us a nice picture of what compositions like this look like
it essentially says that we take these water droplets and then we combine them and when you
combine the associated water droplets their volumes add as another example let's go back to
our previous situation in fact let me write that example here because it's a little bit it can fit
here so in this case we had that set x to be there's a good sale at the supermarket this week and
there's not a good sale and the set y is I go to the supermarket or I don't now what if we happen to
know the statistics or the probabilities of whether there is a good sale or not at the specific
supermarket given that specific week so you compile all of your data over the course of a year for
instance and you just ignore the seasons you ignore the months you just look at when is there a good
sale for whatever definition of good you might have for for you and let's just say that the
probability of a good sale is maybe only 30 percent so roughly 30 percent of the time there's a good
sale on a given week and therefore the probability of a not so good sale is 70 percent and so you
might ask what is the probability that I go to the supermarket question mark so that's the end of
the statement so all we know is that if there's a good sale we already know what those probabilities
are I think they were 90 percent and if there is a good sale and 60 percent if there isn't a good sale
because I still need to eat and if we happen to know the probability that there's a good sale
and therefore the probability of there being a bad sale or not good rather is 70 percent
then you could still ask what is the probability that I actually end up going
and that's where this composition comes in where instead of having an f like this
we instead have our f from our previous example but we also know the probabilities
of whether or not there's a good sale so it's a slight generalization of this example
and therefore the probability that I go
to the supermarket is equal to and in this case I'm going to take the probability
that there is a good sale times the probability that I go given that there's a good sale plus
so let me actually write that one down so that's 90 percent times 30 percent the probability that
there's a good sale times the probability that I go plus the probability that there isn't a good
sale but I still go and the probability that I go given that there isn't a good sale is 60 percent
and the probability that there is not a good sale is 70 percent and the resulting probability
that I go is 69 percent so given those statistics we still know that if I just chose an arbitrary
week in the year there's a 69 chance that I'll go to the supermarket that week
so now let's look at another example and this example again will come back
will come back to this perhaps a few more times so now let's look at another example
this one may seem a little bit abstract but it's a very useful one anyway
so let's take the diagonal map from x to x cross x what this does is it takes an element x
so far we've talked about stochastic maps and how to compose them and how to view ordinary
functions as specific examples of stochastic maps what we'll do now is describe how to take the
product of two stochastic maps in a way that generalizes the usual notion of the Cartesian
product of two functions so given stochastic maps f and g
we can form their product
and it's another stochastic map
that essentially takes the product of these two problems of the associated probabilities
point-wise so it's determined by the formula f cross g now remember what our notation is
for each element in the domain we get a probability distribution on the co-domain
and that probability distribution is determined by what it does to points because we're working
with finite sets so that probability distribution is determined by the value of our initial input
with our our output and it's just the product of the associated probabilities from f and g
and let's just check that make sure that this coincides with our usual definition of Cartesian
product when we specify that these stochastic maps correspond to functions
so if f and g are functions
or how I think of them as being deterministic then this product
is given by well we know what happens when these are functions then we use the the
chronicle delta and this is x prime f of x while this is delta y prime g of y
and this is nothing but it's the same exact thing as requiring that x prime coincides with
f of x simultaneously as g as y prime corresponds with coincides with g of y
and this is the usual way we think about the Cartesian product because it says what is the
value of f cross g at x y well it's f of x comma g of y and this is exactly what
encompasses that idea and all of the structure that we've defined so far the idea of this
stochastic map it's definition how it composes the fact that functions are special cases in
particular the identity function is a special kind of stochastic map it turns out that
composition is associative the identity is an identity for the composition for any finite set
and this Cartesian product it also satisfies the type of associativity condition
and together all of this all of these data give the collection of finite sets with stochastic
maps and this associated product this it gives it the structure of a symmetric minoidal category
but there's another thing that we haven't yet discussed which is a notion of almost everywhere
equivalence or in other words an almost surely notion of equivalence and this essentially takes
care of when probabilities happen to vanish and when such a thing happens we can have a
notion of equivalence between functions when their probabilities are equal versus when they're not
when they're zero and so we get a very natural definition of what it means for two stochastic
maps very similar to the way we define almost everywhere equivalence for functions so given two
stochastic maps so I'm using different notation than what's up here so given two stochastic maps
and a probability measure on x
we say that f is p almost everywhere equivalent to
g if and only if and the way we define equivalence is that these stochastic maps agree
everywhere outside a set of measure zero so outside of events that have probability zero
so the way we write that is if and only if the probability of the set of points on the domains
of these corresponding stochastic maps where these two stochastic maps differ
is equal to zero now what does this inequality mean now f of x and g of x are both probability
measures on y so when I write that they're not equal that means f subscript y x is not is is not
equal to g subscript y x for some y so this is a very intuitive notion of almost everywhere
equivalence there's another sort of diagrammatic way that you can encompass these definitions as well
so I'll write this as a theorem but we'll use this idea later on
so it turns out that given f g and p as in this definition
f is almost everywhere equivalent to g so this is the notation that we'll use if and only if
the diagram now this is going to be a little bit of an interesting diagram
so we're going to produce our probability on x we're going to duplicate x using the map that we
introduced earlier and on each of these two factors we will apply our associated
maps f and g on their corresponding terms so in this case we'll have the identity on x here cross
f and here it's the identity on x cross g where this product is the one that we justified so if
and only if this diagram commutes so first of all this is a very interesting statement
it tells us that this notion of almost everywhere equivalence can be encompassed in some diagrammatic
form and secondly if we ever discuss these in these videos we'll find out that this is very
closer related to a notion of almost everywhere equivalence in a non-commutative setting where
we replace our finite sets and stochastic maps with certain kinds of c star algebras
and completely positive unital maps and these sorts of objects are relevant in quantum information
theory okay so before we prove this we'll have a little bit of a lemma just to make the calculation
a little bit easier and that lemma is the composition of two maps of two stochastic maps
that are of this form so if i have a map phi from u into v and the map psi from u into v
and i pre-compose with this diagonal map then this composition is given by
the formula so we take phi cross psi composed with this diagonal and how do we evaluate this well
the domain has a u and the codomain has a v and a w so we can evaluate it v comma w and u
and the claim is that this is given by taking just the product of these where two of the points
happen to match up so this is phi v u psi w u for all v u and w so the proof of this is pretty
pretty easy once we have all of our definitions in place and the left hand side of this expression
by definition of the composition and by using the definition of the product is equal to a sum
and what's our intermediary step it's the sum over u cross u and u cross u therefore we have
to sum over two elements we've already we're already using a letter u so we're going to have to
introduce u prime and u double prime for instance so it's going to be u prime u double prime both
elements in u and the product here is going to be phi v u prime psi w u double prime because that's
the second coordinate and this is as we recall the direct the chronicle delta twice using the
coordinate u and u double prime and u prime so it's u prime u delta u double prime u so this
gives us two delta functions and we have a summation over those and as a result these two letters
coincide so this is exactly the right hand side quick and simple proof so this is the proof of the
lemma and then the proof of the theorem we'll now talk about Bayes theorem and first we'll
state the theorem given a probability distribution on x
and a conditional probability from x to y call it f so it's the stochastic map
there exists another map going in the opposite direction let's call it g such that
the diagram
now the diagram looks a little bit complicated but it's not too bad when we write out the equation
we'll see exactly what it means so here we'll have p and here notice we can compose p with f
to get another probability distribution
on y and we'll call that q so we have our probability distribution on x on one on y we duplicate x
we duplicate y this almost reminds me of the definition of a equivalence
x cross y and here we will apply the only maps we can and to go from x to y we apply f
and to go from y to x we apply g
so the statement is that this diagram commutes
and furthermore
for any other stochastic map that also goes in the opposite direction let's call it g prime
satisfying this
then these two maps are q are almost everywhere equivalent and in the sense of our probability q
so this is the formal statement of Bayes theorem and if you've seen if you've seen Bayes theorem in
a different context this may seem totally strange but let's just see exactly what it says
when we look at the composition of all of these arrows
we've actually computed expressions just like this if you remember
the this left hand side when we were doing the notion of almost everywhere equivalence
in that diagrammatic perspective we computed something I think it may have been exactly
this expression actually so commutativity says
says that f y x times p x equals and if we did that same calculation but on the right hand
side of this diagram it looks almost the same it's just that the g is on the other side nevertheless
we still get g x y q y and this holds for all x y
of course x is an x and y is in y now let's introduce some notation to see how to understand this
let's define p of y given x so this is the probability of y given x to be exactly f y x
that's exactly what f means f is this stochastic map it says
it's not corresponding to a function it says if you give me x I will give you y with
some probability the probability is exactly f y x so that's exactly what this conditional
probability is and the probability of x is just little p x the conditional probability of x given
y now this is going in the opposite direction it says if you give me y what's the probability of x
occurring that's exactly g x y and finally the probability of y occurring is q y and so if we
write down these expressions commutativity is of this diagram says nothing but the probability of
x given y times the probability of x is equal to the probability of y of x given y times the
probability of y which is perhaps a slightly more familiar form of base theorem at least when
your events are singleton sets and with the appropriate definitions you can also extend this
you can look at what this diagram means because these are corresponding to probability measures
and you can also define a notion of conditional probability where you replace this point with
a subset and you can use the probabilities on your corresponding spaces to make sense of what
this means when x is replaced by some event a perhaps and y is replaced by some event b
nevertheless the same equation still follows from commutativity of this diagram
so let's look at our earlier example just to see what this is saying and how to interpret it in
sort of a real-life situation so if you remember we had x and y two sets with each of which contains
two elements and x corresponded to the set where there's a good sale and the other element was
not a great sale not good sale and y is the set of elements the set containing the elements
i go to the store the grocery the grocery store or i don't go
and we also had probabilities on each of these spaces and we also knew the probabilities that
if there's a good sale how likely am i to go right that was nine nine ninety percent so ninety
percent if good i go with ninety percent probability and if not good then i still go
but with sixty percent chance and likewise the other probabilities are given by the fact that
it's one minus this one minus this and we also know the probability of there actually being a good
sale so we know what p of good sale is and the probability is thirty percent and the probability
of a not good sale is therefore seventy percent so we have all of this information now imagine you're
in that store this particular week and you happen to see me there so in that case you happen to know
i'm already at the store then you can ask what is the probability that there's a good sale this week
given the information that you see and knowing this information as well so initially you also
know the statistics that says the if i look over the entire year the probability that there's a
good sale is thirty percent but you also know that i'm more likely to go to the store if there is
a sale so if you see me then there might be a better chance that there's a sale this week
and how do you figure that out well if we look at this expression and we
compare these two sides then
we can say that f corresponds to the if there's a good sale versus if there's not a good sale
how likely am i to go or not as f y x and the probability that there's a good sale
is p x and if we wanted to know so let's say g is on the other side so g of x given y so this says
if you see me at the store so here this element y is i'm at the store
and x is there's a good sale
so if you see me at the store what's the probability of there being good sale
and we divide that by q y which we've already determined last time so q y was the probability
that i went to the store
and we know that that equals the sum of the product of the probability of
if there's a good sale i go and if there's not a good sale i go
multiplied by the corresponding probabilities corresponding to here and we found that to be
69 percent so in this case this equals 90 percent 30 percent divided by 69 percent
and when you write out what this equals it's roughly approximately equal to 39 percent
so you've updated your hypothesis based on what you see
and this is known as Bayesian inference
or inversion inversion and in fact the map g constructed here
a g from Bayes theorem
is called a Bayesian inverse
of f and it would be a little bit inappropriate to say
that it only depends on f because it also depends on your prior probability distribution p
so this is an interesting reformulation of Bayes theorem that seems to be totally in the
language of category theory and it therefore makes it amenable to a wide range of techniques
that could be used to analyze and understand it and perhaps even generalize this idea to other contexts

Welcome, everybody, to the Entrepreneurial Thought Leaders seminar, a Stanford seminar
for aspiring entrepreneurs.
ETL is presented by STVP, the Engineering Entrepreneurship Center here at Stanford,
the Business Association of Stanford Entrepreneurial Students.
I'm Ravi Balani, a lecturer in the Management Science and Engineering Department at Stanford
and the Director of Alchemist and Accelerator for Enterprise Startups.
Today, we are thrilled to welcome Andrew Ng to ETL.
How many people know Andrew?
Okay, so Andrew really doesn't need an introduction,
but we will give one anyways for those who don't.
Andrew is truly a child of the world.
He was born in the UK to parents who emigrated to the UK from Hong Kong,
but was raised in Hong Kong and Singapore, went to Carnegie Mellon
and very early on signaled that he was no ordinary student.
He got three bachelor's degrees at Carnegie Mellon in computer science,
statistics, and economics, and graduated at the top of his class,
then went on to MIT where he got a master's in electrical engineering and computer science,
then came over to the left coast and got a PhD in Berkeley in computer science
with a focus on artificial intelligence and reinforcement learning.
Andrew is generally viewed as one of the preeminent thought leaders on AI today.
He's the co-founder and head of Google Brain
and the former chief scientist at Baidu,
where he built the company's AI group into thousands of people, several thousand people.
But he's as passionate about AI as he is also about the development of you,
of students around the world.
And I know there's a lot of love for Andrew.
He's a former associates professor and director of the Stanford AI Lab
and currently an adjunct professor in computer science at Stanford.
How many people have taken one of Andrew's classes or want to take one of Andrew's classes?
And he's a beloved professor here at Stanford,
but he's also viewed as a beloved teacher to millions outside of Stanford.
He's the co-founder and chairman of Coursera, the world's largest MOOC platform,
and through his online education work and his online AI education work,
he's reached over 7 million people.
He was listed as one of the world's 100 most influential people by Time Magazine in 2013.
And today, Andrew's the managing director and partner at the AI Fund,
which is a startup studio building new AI companies from the ground up
and is also the founder of deeplearning.ai.
He focuses his time primarily on his entrepreneurial adventures,
looking for the best ways to accelerate responsible AI practices in the larger global economy.
There's fantastic content already online that Andrew has given,
including a longer version of today's talk that you can find on YouTube.
And so instead of reduplicating that, Andrew's going to give a teaser talk,
a 10-minute discussion, followed by we'll do a quick fireside chat,
and then we're going to open it up for really interactive Q&A with you.
So start thinking about your questions now because the time is going to fly by,
but without further ado, please welcome Andrew.
Woo!
Thanks a lot, Ravi. Thanks. Good to see everyone here.
Can everyone in the back hear me okay?
Cool, awesome.
You know, I've taught CS229, my machine learning class, in this room many years,
but all these years I've taught in this room, I've never seen my face that big before.
What I'd like to do today is chat to you about opportunities in AI.
So one of the difficult things to understand about AI
is a general-purpose technology, similar to electricity,
meaning it's not useful just for one thing,
it's useful for a lot of different applications.
If I were to ask you what is electricity good for,
it's almost hard to answer that because it's useful for so many different things,
and AI is like that too.
So one of the major trends that we've seen in the last few years
is that prompting is revolutionizing AI application development.
And I want to just dive a little bit deeper into this
because I know this is an engineering class,
I know many of you may be from an engineering background,
and I'm going to just go a little bit deeper into this than I might otherwise.
But if you were to, say, want to build an AI system for many years,
the typical approach is use supervised earnings.
So let's say I want to build a system to rate restaurant reviews as positive or negative sentiment,
then you would collect data, maybe that takes me a month,
I would train an AI model, maybe that takes me a few months,
find a cloud service to deploy my AI model, maybe that takes a few months.
And so for the past, most of the past decade,
a realistic timeline to build and deploy a valuable AI system was maybe six or 12 months.
But with prompting, the timeline is now very different.
You can specify a prompt in minutes or hours,
and then deploy a system to production in just hours or days.
And I know that probably many or maybe most,
maybe all of you will have played with large language models as a consumer too,
like chat GPD and bot and bing chat.
I think that in terms of start-up opportunities,
I'm excited about the use of large language models,
not as a consumer too, which is fantastic and exciting.
I think use chat GPD and bot regularly,
but instead the application of large language models as a developer too,
because this is allowing a lot more applications to be built
and dramatically lowering the barrier to building many applications.
So I know that in this talk, you don't normally have speakers write code,
but this is an engineering class.
So let me actually show you exactly what I mean by that.
It turns out that if I want to build an AI system today,
this is all the code I need.
And this means that if you take CS 106 or something,
learn to code in a CS class,
with just a little bit of code, import AI, open i2s, load my key,
I don't know, D, what, ST, P lectures,
I break it, so many friends.
I've never written that before.
And so hopefully this, okay, thank goodness, got that right.
And so this is positive sentiment.
And just in seconds, that's all the code it takes now to build an AI system
in code to look at a piece of text and process it,
to look at a piece of email and route it or to start to build the beginnings of a chat bot.
So over the last, I don't know, half year, one of my teams,
has been working with many of the AI tool builders to create short courses
on how to use tools like I just showed you.
Because there are many AI applications that used to take me six months to build
that I think any of you will now be able to build in one or two days.
And this opens up the set of things that you could do
and the set of prototypes you can build.
And in fact, from a startup perspective,
when it took us six months to build something,
what we do is have a product manager study it, do the user studies,
make sure it's the right thing to build, then go build it.
And after all that investment is like, boy, let's hope it works.
But what I'm seeing with these very fast development times is
if it takes you a couple of days to build something,
I'm seeing a lot more startups as well as big companies say,
you know what, I have 10 ideas for features.
I'm going to build all 10 things and then just ship them all.
And then we'll see how users use them or don't use them and just keep what sticks.
And this is a very different prototyping, much lean in methodology
than I've seen startups use before prompting.
One important caveat, which is that responsible AI is important.
So don't do this, don't ship things that could cause harm.
But we have a lot of applications like inspecting bits of metal and factories
where there is really no harm, no risk of bias,
where I think there's very fast shipping methodology
that's just innovate very quickly in AI.
So where are the opportunities?
So the size of these circles shows what I think is the value
of different AI technologies today.
Supervised learning started to work really well about a decade ago
and labeling things, such as label does that as,
is it something you're likely to click on or not?
Or label this X-ray with, you know, what's the medical diagnosis?
And supervised learning for a single company like Google
is worth more than $100 billion a year,
and there are millions of developers working on it.
And it might even grow in the next three years to double, say.
So massive momentum, lots of applications to be figured out.
And then Genes of AI is a new entrance where,
frankly, the revenue, the value of the revenue from Genes of AI today
is much smaller, but given the amounts of interest,
excitement, and commercial interest,
I think it will much more than double in the next three years.
And three years is an artificially short time horizon.
I think it were to look out six years
if it continues to compound at this rate.
Maybe the value from Genes of AI will even start to approach
that of supervised learning.
But all that room for growth, the light-shaded region
for supervised learning or Genes of AI,
which are probably the two most important tools today,
are where there are a lot of opportunities for any of us
to identify and build to concrete use cases.
And what I hope to take away from this talk
is AI technologies are general-purpose technologies,
meaning that they're useful for many different tasks.
When supervised learning started to work,
well, about a decade ago, it actually took us a long time.
It took us annoyingly long over the last decade,
and it will take us annoyingly long over the next decade
to figure out use cases for Genes of AI.
But do you want to use this to make ships more fusion
or for medical diagnosis
or for education product recommendations or something else?
It was still figuring out concrete use cases
for supervised learning.
And even though we're not yet done doing that,
we have another fantastic new tool, Genes of AI,
that even further expands
the set of things we now do of AI.
And one important caveat,
which is there will be fans along the way.
How many of you remember Lenzer?
Raise your hand if you do.
Wow, almost no one.
That's fascinating.
So Lenzer's revenues took off like that through last December.
It was this app that could let you
upload a few pictures of yourself
and draw a cool picture of you as an astronaut
or a scientist or something.
And it was a really good, really hot product
until last December, after which his revenues did that.
And I think that's because Lenzer was one
of what will probably turn out to be multiple,
thin software layers built on top of someone else's
very powerful API.
That was a good idea, people liked it,
but it wasn't a long-term, defensible business.
And when I think about Genes of AI as a developer platform,
I'm reminded of when Steve Jobs gave us this phone, right?
And shortly after, someone wrote an app
that I paid $199 for to do this,
to turn the phone into a flashlight.
And this was also a good idea, it was a great product,
but it just was not a defensible business either
because it was a very thin software layer built on top
of someone else's very powerful development platform.
But in the same way, after we got the iPhone,
after we got the smartphone,
someone else figured out how to build Uber, Airbnb, and Tinder,
much longer-term, defensible, very valuable businesses
that are still standing the test of time.
And I think we have those opportunities as well
to build long-term, valuable franchises,
businesses on top of Genes of AI.
So where are the opportunities?
So I felt years ago, but even more strongly now,
that because of emerging AI technology,
there are a lot of projects that are now possible,
that were not possible one or a handful of years ago.
And I wound up starting AI Fund, which is a venture studio,
that sequentially works as entrepreneurs to start companies.
We actually average about one startup a month now,
because I felt, I previously, as Ravi mentioned,
previously I had led AI teams in Google and Baidu,
and even having led AI teams in Big Tech,
I couldn't see how we could possibly operate a team
in a Big Tech company to pursue the very diverse,
very different sets of opportunities that I saw
and wanted to pursue, and starting different startups
to pursue those valuable projects seem more efficient
than having one company, even the Big Tech company,
go after such a large set of resources.
But having said that, I think AI and Genes of AI
also offers a lot of opportunities for incumbent companies,
which often have a distribution advantage.
Where exactly are the opportunities?
So this is what I think of as the AI stack.
At the lowest level is the hardware layer, very valuable,
but also very capital-intensive, needs a lot of resources to build
and very concentrated.
So I'm sure there'll be valuable startups built there,
but I personally don't play there,
because of how capital-intensive and how concentrated it is.
There's a cloud infrastructure layer,
also very capital-intensive, very concentrated, very valuable,
but at least when I build startups, I tend not to play there.
The other layer that's interesting is the developer tooling layer.
So what you just saw me do was use OpenAI as a developer tool.
And I see this space as hyper-competitive,
look at all the startups chasing OpenAI,
but there will be some mega winners.
So whereas incumbents have a startup distribution advantage,
I think for many startups, having a technology advantage
may give you a best shot at doing something meaningful there.
So I personally tend to play here
only when we think we have a technology advantage,
because that buys us a better chance to become one of the huge winners.
And then with most ways of technology innovation,
a lot of the media attention, social media,
what people tend to talk about is the tooling of the technology layer.
There's one other layer that I think has got to be even more valuable,
and that's the application layer.
Because in many ways of technology,
for the in-front and tooling layer to be successful,
applications need to be built on top of them.
They generate even more revenue
so that they can afford to play the infrastructure layer.
And what I'm seeing is that there are a lot of opportunities
at the application layer where the intensity of competition
is not, frankly, not nearly as high.
Maybe just one example.
I've been chatting a lot with the CEO of Meno,
which is a startup that applies AI to romantic relationship coaching.
And I'm an AI guy.
I feel like I don't know anything about romance.
And if you don't believe me, you can ask my wife.
She would confirm that I don't know anything about romance.
But when we decided, when we had conviction that AI could be applied to relationships,
we wound up partnering with Renata Nyborg, who's the former CEO of Tinder.
And because she ran Tinder, she understands relationships
in a very systematic way, more so than anyone else I know.
And so with my team providing AI expertise
and her providing relationship expertise,
we're able to build a pretty unique relationship mentoring application
that we just announced a few weeks ago.
And this is not part of, you know, Renata actually occasionally
stops by Stanford campus and talks to Stanford students
as part of her user product research.
So it's possible that we've seen her around.
Just one last thing, I'd love to go to GNA.
Over the last few years, AI fund we've been tuning our process
for building startups and we just share that with you.
So we often start off with a lot of ideas, right?
And one example of another startup we built was Bering AI,
which uses AI for smart routing of very large ocean growing vessels.
So if you're a ship captain, should you sail at 20 knots or 22 knots?
Who knows?
Most ship captains just make some decision.
But because we're able to get global weather and ocean current data,
we can make recommendations to ship captains for how to get there on time
and use about 10% less fuel.
But this idea was suggested to me by Mitsui,
which is a major shareholder in a major shipping line
that operates very large ocean growing vessels.
And just one of those things, I would never have thought of this idea myself
because I've been on a boat, but what do I know about global maritime shipping?
But Mitsui suggested this idea to me.
And we then validate the idea, make sure there's technical feasibility
and market need, recruit a CEO.
We were fortunate to find Dylan Kyle, who's a fantastic CEO
with one successful exit before.
And then we spent three months in our current process
building a technical prototype with the CEO and doing deep customer validation.
If it survives, two-thirds chance of surviving, one-third chance of not surviving.
We then write a check-in that allows the company to build higher executives,
build an MVP, and off it goes to raise additional rounds of capital.
And I think this is what we...
And so bearing AI, well, now it's actually...
There are now hundreds of ships on the high seas guided by bearing AI.
Ships guided by bearing AI have 75 million miles,
which is the equivalent of going 3,000 times around the planet
and save about half a million dollars in fuel costs per ship per year
in addition to significant carbon emissions.
I think we'll save about...
I want to save about a million tons of CO2 emissions so far.
But this kind of idea that...
I would never have come up with this idea myself,
but I've learned that my swim lane is AI,
but when I work with experts in other sectors,
there are often these exciting opportunities that are very valuable.
But frankly, how many groups in the world are experts in AI and shipping
or expert in AI and relationships?
I find that the competition intensity at the application layer is often much lower.
And then just one last thing, kind of just full disclosure
and something that I hope all of you will do too.
My team's only work on projects that we think move humanity forward.
Response for AI is important.
And on multiple occasions, we've killed
and I will continue to kill projects that we may assess to be financially sound
but based on ethical grounds.
So lots of exciting opportunities.
I think at Stanford, the loss of great costs is going to take
in engineering and elsewhere to learn about that AI tech.
And then when you find applications or go play at the infrar and tooling layer too,
I think there are lots of opportunities.
But I think there are...
What I'm seeing is, frankly, my team at AI Fund,
we have so many startup ideas.
We use a task management software.
We use Asana to track this huge list of ideas.
And it's actually quite clear to me.
There are a lot more good ideas for AI businesses
than people with the skill to work on them at this moment in time.
So hopefully, there'll be more than enough projects for everyone.
There are all of you, all of us, to work on.
All right, thank you.
I wanted to just start off with that closing statement that you made
about how there's more opportunity than there are students with skills
or people with skills to pursue them.
And given that we have an audience full of students,
I wanted to start off by mapping out advice for students
that are entering into the university regarding AI.
So if you want to pursue a career in AI right now,
and let's say your child was entering Stanford,
what advice would you give them in terms of how to spend their time?
Yeah, so, you know, there's one thing that's actually really worth doing
when you're sent to students, which is take classes.
Because it turns out that I feel like there's actually one pattern I see
for both undergraduates and graduate students, including PhD students,
which is there's so much exciting stuff to do,
you just want to jump in and do it, right?
In fact, I've seen them undergrads in their freshman year,
you know, try to join a research lab and start doing work in AI.
That's okay, nothing wrong with that.
But it turns out that while project work is one way to learn,
coursework is, I think, an even more efficient way to learn,
especially when it comes to mastering the fundamentals,
because professors have put a lot of work to organize the material
in a way that's efficient to learn and digest.
So I would say, you know, take classes in AI technology
or in entrepreneurship and gain those skills.
I've seen students jump in and then if you are trying to work in a research lab
without strong skills, you end up, you know, like,
labeling data or something, which is fine, you learn some things,
but you actually learn a lot from taking courses.
And then in addition to that,
after you start to master the foundational skills,
after you know how to use AI technology or, you know, then,
as you start to practice, find exciting use cases across campus.
I do a lot of work, you know, over with people over in climate science
or in healthcare to take my AI expertise
and then navigate with a different discipline
that I'm not expert in to find exciting applications.
And hopefully that type of practice will help many of you find exciting
projects to work on as well.
Do you need to take technical classes?
Do you think you need to take computer science classes
if you want to pursue a career in AI?
Need is too strong, but I definitely encourage you to take technical classes.
I think we're moving toward a world where, frankly, I, you know,
at some future point, I think everyone should learn to code.
Or rather, I think it'll be useful for everyone to learn how to code
for a couple of reasons.
Everyone has access to data, right?
This is different than the world used to be even a few years ago.
And especially with genus of AI, your ability to get something to work
is much higher than ever before.
The barrier to entry is much lower than ever before.
And so if you learn just a little bit of coding,
the amount that you really accomplish is significantly greater
than if you don't know how to code at all.
And are there any skills that separate out the great AI founders?
I know AI right now is like, it's a sea that's rising all boats.
But if you separate out the great ones from the good ones,
are there any salient skills that you notice that the great AI CEOs
or founders have that the good ones don't?
Maybe since you said AI, I would say is often technical death.
It costs a lot.
But I want to give a different answer if you say great founders
or not great AI founders.
But I feel like AI is evolving rapidly.
And we definitely have lost a bunch of new roles that, you know,
pitch the VCs without really knowing what they're doing.
And the smart VCs can sniff it out quite quickly.
And it makes a huge difference.
I think the technology, unfortunately, you know,
is like somewhat complicated for a lot of applications.
So a team that actually knows what they're doing
will execute an AI project 10 times faster, you know,
than a team that doesn't.
And 10 times is not a made-up number.
I literally see people take a year to do something like, oh, boy,
I know that other team would have done perform this level
in two weeks or maybe a month.
So for many AI startups, application startups,
in-front startups, you kind of have to know what you're doing.
So it doesn't have to be you.
If you're a technical co-founder, maybe that's OK.
And then second thing I see among many of the great founders
is speed.
I find that as a startup, you'd be surprised
when you handle the great founders the sheer speed
of decision-making.
And, you know, I sometimes talk to people from big companies
and they'll say, oh, we move so fast.
But when I kind of sit them side by side,
how long does it take you to make this decision?
Do I talk to a great founder?
How long does it take me to make this decision?
Maybe here's one story.
I was chatting with the Meno CEO of Renato and iBook,
former CEO Tinder.
I was on the phone with her one day
and she was making a major architecture decision.
So it's architecting this thing.
There were basically two major software architectures
under consideration.
And the team had laid it out, listed out some pros and cons.
So they got on the team with me and some of my friends
and said, these are pros and cons.
And then one of my C2 AI fund and I said, you know,
we're not sure, but she has some reasons
that we prefer architecture A.
And then Renato said, okay, guys done,
decision made, go and implement architecture A.
And after I thought, well, did Renato
just make a massive engineering decision
in basically 30 seconds?
And she did.
And I realized after it,
I don't think there was a better way to make it
because it's not as if, you know,
if the company waited another week,
it would have been a high quality decision
and if it was wrong,
I'm sure they would fix it, you know, the next week.
But until you've lived through the speed
of a great business, most people,
I know so many people that think their organizations are fast
when you stack them up to the real speed
of a fast moving CEO,
they have never actually seen speed in their life.
One important caveat do be responsible.
I know that move fast and break things sometimes,
you know, it's the wrong approach.
So tremendous speed when you are not being callous
with people's lives and livelihood
and things that could cause real harm.
But so long as there's an important caveat
of responsible AI,
many of the great CEOs move faster
than most people realize people can move.
And so let's just double click on that
on this theme of responsible AI
just because I know this is a hot topic
that maybe people aren't thinking about,
which is you are clearly on the side of AI for good
for responsible AI.
Many of your brethren like Jeffrey Hinton
and other famous leaders in the AI space
have come out and are concerned
that the pace of AI development
will become an existential threat to humanity.
So much so that famously there was a petition signed
by Elon Musk and Steve Wozniak and many thought leaders
asking for the halting of the foundational,
the deepest foundational models of AI
for us to sort of, for society to sort of catch up.
You did not sign that pledge.
Can you share a little bit more detail about that?
Was that a difficult decision for you to make?
And can you share more details
about why you didn't join them
and what your philosophical view is regarding
if AI poses an existential threat?
So I honestly don't see how AI poses
any existential threat to the human race.
We know AI can run about self-driving cars
have crashed, leading to a tragic loss of life,
ultimately through trainings, trash the stock market.
So we know poorly designed software systems
can have a dramatic impact
and responsible AI is important.
But recently I saw tells people like Jeff
and others that were concerned
about the question of AI extinction
and I tried to understand why they thought this way.
Some were worried about bad actor
using AI to create a bio weapon.
Others were worried about AI evolving in a way
that inadvertently leads to human extinction
similar to how we as humans have led to the extinction
of many species through simple lack of awareness.
Sometimes that our actions could lead to that outcome.
But when I tried to assess how realistic
these arguments were,
I found them to be vague and non-specific
about how AI could cause all.
And I think that I found frustratingly frankly
that trying to prove AI couldn't
is akin to proving a negative.
And I can't prove that super intelligent AI
won't be dangerous,
but I can't seem to find anyone
that really knows exactly how it could be.
And but I do know that humanity has ample experience
controlling many things far more powerful
than any one of us like corporations and nation states.
And there are many things that we can't fully control
that are nonetheless safe and valuable.
Like airplanes, no one can control an airplane.
It's buffeted around by winds
and the pilot may make a mistake.
But in the early days of aviation,
airplanes killed many people.
So we learned from those experiences,
built safer aircraft,
devised rules by which to operate them.
And today most of us can step into airplane
without fearing for our lives.
And I think it would be like that too for AI.
So I think the AI extinction,
I find it to be very unfortunate.
What I'm seeing,
because doing some work in K-Tel of education as well,
what I'm seeing is that kind of really unfortunately,
I see high school students now considering working in AI.
And some will say, AI seems exciting,
but I heard it could lead to human extinction.
And I just don't want to be a part of that.
And so I find that the over height AI extinction narrative
is doing real harm.
So I'm really concerned about that.
Thank you, Andrew.
One more question that I'm gonna open it up,
which is, I loved the detail
on the low hanging fruit opportunities.
I know that's on everybody's,
all the entrepreneurs' minds of what to pursue.
And so I appreciated the attention
and the presentation on that.
I wanted to ask about what's gonna be
the next big technology shift in AI
because things are changing so rapidly,
especially as the models now are getting smaller
and open sourced.
It feels like we've already conquered language.
Visual AI is getting very, very good.
What's next?
What are you seeing that's around the corner
that others might not be aware of?
Yeah, you know what, Danit?
About several months ago,
I was predicting visual AI's coming next,
but now everyone's all right,
visual AI's gotta come with something new.
But in all seriousness, I think visual AI
would be much more about the analysis of images
rather than just generation of images.
But I think we're at like the GPT-2 moment
for visual AI is not yet working,
but I think it'll work much better.
And this will impact self-driving cars, for example,
when we can finally solve problems in a long tail.
And then I think, actually, one other thing that
I wrote about just today in a newsletter called The Batch
is I think one thing that many people find controversial
but I think is coming is the rise of edge AI.
And I know this controversial.
Many of us would train to write SaaS software,
lend to self-nice, the subscription business model.
How do you even find people?
How do you hire engineers to write desktop applications?
Like who even does that anymore?
But I think that because of various forces,
including privacy, I think that in the next few years
we'll see more AI applications running at the edge,
meaning on your laptop or on your cell phone.
So I think that'll be coming.
And then I think there'll just be a lot of work
coming in the application there as well.
Okay, I wanna open it up to the students.
You're the reason why Andrew's here.
You mentioned that on your slides,
you put the potential from reinforcement learning.
Are the general value as a dot
relative to the potential for unsupervised learning?
Do you think there is still potential
for generalist agents like GATO
and other reinforcement learning models in society
and in your AI stack for startups?
So technically, last time we trained models
that trained using reinforcement learning
and unsupervised learning and supervised learning
but leaving that aside,
I feel like I'm not convinced that reinforcement learning
is near breakthrough moments,
at least in the next small number of years.
A lot of excitement about what we could do
in reinforcement learning applied to robotics.
A lot about CS faculty, right, Chelsea Finn,
Emma Brunskill, many others are doing exciting research there
but we do have a data problem.
So it turns out that text on the internet
sounds a lot like text on your documents.
So we can learn from lots of text on the internet
to do really well on your text documents.
And image on the internet
look a little bit like images that you care about.
So we have a lot of data
but because every robot is different,
struggling, many people are struggling to see
how to get enough data to have the usual recipe
of scaling up data and compute
where for reinforcement learning
and people are working on it over the weekend
at the CS faculty retreat, you know, there was a talk,
I think, well, who gave this talk?
Shoot, thank you.
On how to do this, early ideas on how to do this
but I think we're still a few years away
from breakthroughs and those breakthroughs
and reinforcement learning.
But it's a great research topic, by the way,
just because, you know, just because it's not working
right now doesn't mean you shouldn't do research on it.
So I think it's a great research topic.
So I just want to know your thoughts about
what are the security concerns which is coming up
by you abusing that like LLM models,
like all these new attacks like prompt injections,
data leakage, jailbreaking.
So what's your thought around that?
Like how can we like safeguard against those kind of attack
because it's just starting up this new technology.
So I'm assuming there's more things which will come up.
Yes, so I think that for the near future
there'll be a little bit of a cat and mouse thing going on.
So I think I'm seeing different companies
approach this with different tools
to wash out for prompt injections, for data leakage.
Actually, DeepLand.ai is actually a work of a partner
on some things that hopefully will announce very soon
on our portfolio of tools.
By the way, those of you that have not yet done it,
go and fool around with prompt injections,
see if you can get an LLM to do something.
Well, don't do something actually harmful,
but I actually find it kind of intellectually interesting
whenever I use an LLM to prompt it
to see how robust the safeguards actually are.
And if I should look at the older language models a year ago,
it was super easy to get the older models,
frankly, to give you detailed directions
to do things that they should not give anyone
detailed directions to do.
But the more modern language models are much older,
but it's still sometimes possible.
Sorry, but what I'm seeing as well,
for a lot of corporations,
a lot of corporations because of these worries
will ship internal-facing product first
because presumably, if it says the wrong thing
to your own employee,
more understanding, less likelihood scandal,
and test products internally for quite a long time
or even build capabilities for safe internal use
before turning out to external use.
But I do see different companies,
yeah, different tools for trying to adjust these.
Terrific, next question.
Thank you so much.
Hi there, my name is Chinat
and I'm an international student from Hong Kong.
I'm curious to ask, because, you know,
I'm hoping that after I graduate,
I can hopefully go back home to work closer with family.
But at the same time, I feel like by going back,
I'm closing a lot of doors behind me
because, for example, in Hong Kong, for example,
you can't access ChatGVT without a U.S. number,
which makes access to some of these resources
really difficult.
So I'm curious to see what are your thoughts
about navigating this complex modern landscape?
Yeah, I don't want to comment on, I don't know,
complicated.
That's actually one thing I'm seeing.
I've been to quite a few places, you know, in Asia recently.
And what I'm seeing is that many countries
are developing surprisingly good capabilities
for building large language model applications.
The concentration of talent for Genesebo AI deep tech
is very concentrated in the San Francisco Bay Area.
I think because there are basically two teams
that did a lot of the early groundbreaking work,
you know, Google Brain, my former team, and OpenAI.
And subsequently, people left and started a lot of companies
here in the California Bay Area.
So I think that concentrated talent is very high.
And it's interesting, even when I'm in Seattle,
great city, love the city, on weekends,
you know, I hang out with friends,
but the conversations are not about Genesebo AI.
Whereas here, if you go to a coffee shop,
actually, one of my friends was visiting from Taiwan.
So he was hanging out with us for a week.
They went back and he said,
yeah, I went to a coffee shop and, you know,
there was no one talking about AI.
That's so weird.
So at least at this moment in time,
there's really heavy concentration.
But I see less the deep tech layer,
but the application layer,
I see that skillset developing quite quickly globally as well.
Oh, and I think the option is to allow places
to be local opportunities.
So the shipping company that we built,
we built with a Japanese company
that happens to operate global lines of shipping.
So I think a lot of the businesses will be, you know,
playing locally whether country or that geography is strong.
Those businesses will be more efficient to build in places
other than Silicon Valley.
Because where do I go to find a large seaport here
to do that type of work?
Hi, Andrew, thank you so much for your time.
My name is Komal.
I wanted to ask you if you think we'll ever reach a threshold
on human dependence for AI,
or if you think it'll just continue to grow exponentially?
So I think we already really, really depend on tech, right?
Imagine if, you know, if the internet were to shut down,
I think people would die.
I don't think that's the exact truth.
I mean, but seriously, think about, you know,
how we get through supply chain, healthcare.
If the internet were to shut down,
I think that will lead directly to, you know,
what happens our water system, right?
Healthcare system.
So, and I think that technology is very useful
and so long as the supplies remain reliable,
I feel like it's okay to depend on technology.
I mean, heck, I wish, I don't know,
without dependence on agriculture system,
how many of us would build a farm
and hunt enough food to keep ourselves alive?
Maybe, maybe we could do it, but it's pretty challenging.
So I think dependence on tech
seems to keep on growing for a while.
But do you think there'll be a moment
where there's a difference in that relationship,
not just in degree, but in kind?
You know, the famous singularity point
where we don't even know what we don't even know
about how technology is developing.
Do you think that will occur?
Yeah, you know, the technological singularity
is one of those hypey things
that I don't even know what it means.
So it's one of those, it's exciting science fiction,
but as an engineer and scientist,
I don't know how to talk about it.
It turns out there are a few terms in AI
that are vague and undefined,
but there are a lot of emotions,
a lot of excitement about it.
And I don't really know how to think
about those things in a systematic, rational way.
But I think, actually, there's actually one thing.
I think that our relationship to technology
is changing rapidly.
Today, you know, I probably use
chat, you know, GPT-4 or Bing or BOT
pretty much every day now.
And so the workflow of many people have changed.
I think people are changing.
And do you have a view?
I know this also might be more of one
of these sort of hot topics that's not substantive,
but on the consciousness of AI,
that AI will it become conscious?
Yeah, so the thing about consciousness
is it's important for the South Pole question,
but I don't know of any test
for whether something is conscious or not.
So I think it's important philosophy
and philosophy is important,
but as an engineer and scientist,
I don't know, there is no definition
for what is conscious or not,
which in does we can kind of debate it at length.
And there's actually one other formula for hype,
which is if someone comes up
with a very simple definition for consciousness,
so someone says,
oh, if you can recognize yourself in the mirror,
you're conscious, I made that up.
It's not a good definition for consciousness.
But you're aware of yourself, CSR in the mirror,
then it's actually pretty easy to get a robot
to recognize yourself in the mirror.
And then you can generate newspaper headlines
saying AI has achieved consciousness.
What it did for your kind of, you know,
silly little, for your very small definition
of consciousness, but that gets misinterpreted
by the broader public for a grander statement than it is.
So I see some of that hype in AI as well.
Thank you.
Next question.
Hi, earlier you outlined the AI stack.
And recently we've seen a lot of cool things
coming out of like NVIDIA, Intel and other like chip companies.
I'm curious on what your thoughts are
on what companies like AWS and Google,
like in the infrastructure layer need to do
in order to make like AI and enterprises and business
really effective and possible.
Sure, boy, so there's a lot going on in that space.
By the way, you mentioned Intel and NVIDIA.
I wouldn't, I think I'm actually seeing
really exciting work from AMD as well.
I've been pretty impressed by the MI 200, MI 250.
I'm excited about the MI 300 GPUs coming out as well.
And I think the ROCCOM stack is becoming, you know,
not parity or CUDA, but better than most people
give them credit for.
But in terms of Adode and Google,
so it turns out that if you were to use
a lot of the LLM startup tools,
the switching cost is actually pretty low.
So if you were to start with one LLM API call,
if you want to switch to a different LLM provider,
the number of lines of code is actually pretty low.
So there are low switching costs.
But it turns out that a zero in Google Cloud
and AWS are fantastic businesses
because once you build on any of these clouds,
you know, the switching costs tend to be very high
because you have so many API hooks integrations.
So that's why I think that a lot of the startups
selling API calls still have, you know,
some work to do to find a business model
that may be somewhat more defensible.
I think that OpenEye's chat GPD Enterprise,
that feels like a more defensible business
than just selling API calls.
By the way, Sam was actually a Stanford undergrad.
He actually interned, he's curious in my lab.
So a lot of Stanford roots, but he's a smart guy.
I'm sure he'll figure out,
confidently he'll figure out some good directions.
Yeah, and I think AWS and GCP and the Zora Rall
racing to continue to develop LLM capabilities
and make it easier to use and bring in more customers.
And yeah, it's a very dynamic space.
But as AI gets democratized,
it feels like things are shifting more towards compute
and data as predictors of success.
If that's the case,
do you think the locus of innovation shifts
from academia to industry
where the companies are gonna really be dominating
at the forefront of AI?
Yeah, so what I'm seeing now is that there's a subset,
but there's a small subset of things
that are easier to do in the big tech company,
which are the ones that require massive compute resources.
And I do think people's perceptions are distorted
because frankly, I've been on the big tech companies before,
right?
So I understand your marketing and big tech companies,
but the standard big tech company marketing is,
look, you need the data, you need to compute,
only we have it.
Why don't you just give up and don't compete with us, right?
And come apply for a job and come work with us.
That is, this has been the explicit PR strategy
of at least one big tech company
because I know what was discussed internally
exactly at that big tech company.
So I would say don't buy into that marketing message.
It is true that there is a subset of work
that requires massive capital,
training in very large foundation models.
That is much easier to do in a big tech company
than in academia like Stanford.
But that's a small subset of all the happenings in AI.
And there's plenty of work at Stanford,
at the application layer.
It turns out because of scaling laws,
we're actually pretty good at predicting
what will happen for very large models
by training on more model size models.
So very good scientific work can be done
and much smaller models.
And then also, I routinely run kind of models
on my laptop for inference.
Like, I don't know, when I'm on an airplane,
you can run like the seven billion llama model
on your laptop, right?
And so there's actually a lot of stuff
that you could run on your own personal computer.
Thank you.
Next question.
Thank you, Andrew.
So from an AI expert and also the investor's perspective,
so what AI driven healthcare applications
do you see have the great potentials
to have the breakthrough in the future?
And what challenges and obstacles should we be aware of?
Thank you.
Yeah, so boys, there's a lot of complexity to that question.
So I feel like a lot of healthcare,
people tend to focus on the diagnostics and the treatment.
So I think lots of opportunities there.
I think that the revenue model is to be sorted out.
So we've seen, you know,
Pear and Ikeli struggle in the public markets,
kind of bankruptcy kind of levels almost.
So I think prescriptive digital therapeutics
is definitely going through challenges.
But what's the recipe for shipping AI products
and in the payer-provide ecosystem,
what will pay us be willing to pay for?
I think that many businesses are sorting that out.
I think that will work.
And there's actually one other huge set of opportunities
in healthcare that tend to be underappreciated,
which is operations.
Instead of the medical stuff,
things like scheduling, you know,
who should scheduling the MRI machine
or doing kind of patient management systems.
I think that type of healthcare operations
have fewer regulatory hurdles.
I think is also a rich set of opportunities.
And then lastly, there's the go-to-market question of,
do you want to go to the market in the US
or in other countries where the regulatory hurdle
could be very different depending on,
the US fortunately doesn't have as great a shortage
of doctors as some other places.
And there are therefore other places
that are more amenable to your responsible
but still easier adoption of AI than the United States.
Okay, I have a super quick question.
You mentioned that your team at the AI fund
has so many ideas for AI applications
that you have a whole son of them.
What exactly is your process for generating these ideas?
Oh, there's also that.
And you have 30 seconds.
We like working with subject matter experts
that deeply understand the domain.
It turns out that there are a lot of people in the world,
you know, including like CEOs of Fortune 500 companies,
but really a lot of people that really understand the domain
have thought deeply about something
for months or even a couple of years.
And when we get together with them,
they're sometimes very happy to share their idea with us
because they've been looking for someone
to validate or falsify it and also to help them build it.
So we actually got a lot of ideas,
some in turn out with a lot from subject matter experts
that just not yet had an AI built upon there.
Terrific, that's fantastic.
Thank you Andrew so much for sharing your insights.
Thank you.
Lots of love.
Thank you for sharing your insights
with Stanford's ETL course MSME 472
and the students all around the world.
Everybody next week we're gonna be joined
by Stanford professor Kathleen Eisenhardt
here at ETL physically in person.
Professor Eisenhardt is also the author of Simple Rules.
You can find that event and other future events
in this ETL series on the Stanford eCorner YouTube channel
and you'll find even more of our videos, podcasts
and articles about entrepreneurship and innovation
at Stanford eCorner.
That's ecorner.stanford.edu.
Thank you everybody.
Thank you, Andrew.
Thanks, thanks Ravi.

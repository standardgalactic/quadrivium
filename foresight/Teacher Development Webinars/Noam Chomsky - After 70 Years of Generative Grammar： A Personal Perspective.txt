Yes, you can start.
What did you say?
Good to go.
Hello, everyone. Welcome to our session today. This is a very special session because of our guest, but also because TESOL Tampico Talks is collaborating
with teacher development webinars. My friend Amunalai is here. My name is Jorge Torsal Masan, and we want to welcome you to this unique session.
Amunalai is going to introduce himself, and he's also going to introduce our special guest. I mean, he needs no introduction, but he's going to go for the protocol.
Amunalai, over to you.
Bismillah ar-Rahman ar-Rahim. As-salamu alaikum. My name is Zaman al-Dawthand, and I just asked, you know, with the couplet of Shah Latif, the famous poet of Sindh.
He says,
It translates as those worshippers who search the ocean and find different jewels from the depths. Latif says they are the ones who have found profound ones.
So it's my singular honor, a player, and a privilege to introduce Professor Noam Chomsky, considered the founder of a modern linguistics.
Noam Chomsky is one of the most cited scholars in modern history. Among his groundbreaking books are Syndicate Structures, Language in Mind,
aspects of the theory of syntax and the minimalist program, each of which has met distinct contributions to the development of the field. He has received numerous evas, including the Kyoto Prize in basic sciences, the
Helmholtz Medal and the Ben Franklin Medal in Computer and Cognitive Science. Chomsky introduced the Chomsky Hierarchy Generative Grammar and the concept of a universal grammar which underlies all human speech and is best in the innate
structure of the mind brain. Chomsky has not only transformed the field of linguistics, his work has influenced a field such as cognitive science, philosophy, psychology, computer science, mathematics,
childhood education, and anthropology. Professor Chomsky is one of the most influential public intellectuals in the world. He has written more than 100 books and his website is www.chomsky.com. Certainly right said by the New York Times magazine, the most important intellectual alive.
So ladies and gentlemen, Professor Noam Chomsky for you.
Over to you sir.
Over to you sir.
Thank you for the technical problems.
Times brief there's a lot I would like to cover.
So I'll have to keep to a sketchy rather informal account we can extend things in the question period.
The first term goal of theoretical inquiry is explanation, not just description. Description is hard enough descriptive inquiries seek to show that this is the way things are.
The explanation seeks to show why things are this way, and not some other way. So much more ambitious endeavor, but it is the goal to keep in mind.
In the case of language inquiry proceeds at two levels. One level is concerned with in languages. The second and higher level is concerned with the general faculty of language.
And the other to it as FL faculty of language. FL is the innate endowment that enables a language to be acquired and used. The theory of FL is called universal grammar, you G traditional term it's adapted to a new context.
At the first level the study of individual languages inquiry seeks an adequate grammar of the language, the description of the properties of the language, an adequate grammar must at the very least provide a listing technically a recursive enumeration of the grammatical sentences.
Beyond that, it must yield what has been called the basic property of language. Each language is a digital infinity of hierarchically structured expressions, each of which determine determines perhaps constitutes a thought.
Each of which can optionally be externalized in some sensory motor medium.
A bold thesis would be the generation of thought by the internal language is entirely separated from externalization, which would be an ancillary process. I'll actually assume this return to pretty strong reasons to suppose that it's correct.
In this conception, the internal language consists of the compositional rules that satisfy the basic property, along with what's called formal semantics more accurately term logical syntax.
Important matter that I'll put aside here. The system for generating thought, we can call I language, it's internal individual in the technical sense, intentional with an S meaning we're interested in the actual system coded in the brain, not just one that yields the same output.
Well, understood in this way, the study of language conforms to a long tradition from classical Greece, classical India up through the 19th century.
Throughout the core of the tradition, regarded language as fundamentally a system of thought as audible thought in the phrase of the 19th century linguist William Dwight Whitney.
We know that the restriction of sound is too narrow. This tradition was swept aside in the 20th century by behaviorist and structuralist currents, and it was forgotten. It's still very little known.
Modern approaches in the 20th century take language to be at its root, a system of communication, rather than a system of thought system that perhaps evolved from animal communication.
Recent work seems to me to indicate quite clearly that the tradition was on the right track and should be carried forward. That's a controversial view today, but I think it's justified, and I'll adopt it here.
No returning to explanation. The task arises only at the higher level. In query into FL, the faculty of language seems that any human infant can acquire any language with comparable facility.
If so, FL is a shared human property. There's also by now substantial evidence that FL is unique to humans, that its fundamental properties have no analog in the animal world.
If so, FL is a true species property shared by humans, unique to human essentials. FL must enable any human infant to acquire any language to condition on UG that has been called explanatory adequacy since the 1950s.
Sometimes it's called Plato's problem. It was raised in a certain form by Plato, as it was formulated by Bertrand Russell. It's the problem of how we can know so much, given so little evidence in technical terms of linguistics, it's the problem of poverty of stimulus.
In the early days of the generative enterprise in mid 20th century, it was recognized that the poverty of stimulus problem was quite serious, and that standard formulas within behaviorist frameworks will get nowhere.
There are contemporary variants with supercomputers and astronomical amounts of data, but they do no better. These early conclusions were established more firmly as serious experimental inquiry was undertaken in later years into language acquisition.
It was found that the problem of poverty of stimulus is far more severe than had been assumed. By now it's been quite solidly established that the knowledge of language of a two to three year old child goes far beyond what they exhibit in behavior,
and also that the gap between data available and knowledge attained is vast greatly deepens Plato's problem. These results seem to suggest that the faculty of language must be rich enough to yield complex knowledge from impoverished data, very rich therefore.
Well, at the same time, FL must somehow accommodate the wide variety of languages poses a problem that problem deepens return to another condition that FL must satisfy, sometimes called Darwin's problem.
How can FL have evolved under the conditions of human evolution, though evidence is fragmentary. There are some plausible conclusions about these conditions. Modern humans appeared about two to 300,000 years ago.
There's no genomic evidence that the small group of humans began to separate not long after they appeared.
All share FL, which was presumably already in place.
Prior to the appearance of modern humans, there's virtually no indication of significant symbolic activity in the archaeological record.
Not long after their appearance, there's very rich evidence. Note that these numbers are barely a flick of an eye in evolutionary time.
Well, all of this suggests that FL appeared in a narrow window of time, pretty much along with modern humans.
And there's quite strong evidence that it hasn't changed since. Well, if all that's the case, we expect FL to be very simple.
So we therefore face a conundrum. FL must be rich enough to handle Plato's problem.
At the same time, simple enough to deal with Darwin's problem and the apparent wide diversity of languages all determined by FL lurks in the background.
These are basic conditions that be satisfied by UG.
The conditions appear to be contradictory and much of the search for explanatory theory in past years has been guided by this apparent contradiction.
For the first time, I think we can now begin to see how the conundrum might be resolved. So let's turn to that.
Let's begin with the problem of diversity. That problem would be greatly reduced if diversity of language is sequestered in a particular component of the overall system.
The natural place to look is in externalization, excluding the language operations that yield the thought system.
That move has initial plausibility for eye language, the POS, the repository of stimulus problems are overwhelming, generally insurmountable.
For externalization, at least there's some direct evidence what's heard or seen.
Though this hasn't been firmly established, research is tending towards showing that the variety, complexity, and easy mutability of language is indeed sequestered in externalization,
leaving the system of generative generation of thought relatively fixed, be completely fixed, we might someday discover that would vindicate an ancient tradition.
If this turns out to be true for language, it wouldn't be very surprising. Externalization, some sensory motor medium, is not strictly speaking language, rather it's a combination of the internal system for generation of thought
and sensory motor systems that have nothing to do with language. They were in place long before language appeared, haven't changed since.
So we might expect that mapping the internal language system into completely unrelated sensory motor systems would be a complex task, solvable in many ways, easily subject to at least superficial change.
And so it seems to be. Well, if this is basically correct, then the poverty of stimulus problems reduce largely to externalization.
These topics have been the subject of extensive research since the principles and parameters framework was formulated 40 years ago.
That was the first approach that offered at least a potential framework for accounting for acquisition of language in a feasible manner.
And very important work, much of it by the late Ken Hale and his students, which is shown that languages that appear to be radically different are actually cast to the same mold at deeper levels.
One high point in this research was Mark Baker's work on showing that on developing a hierarchy of parameters.
The major step forward was later taken by Ian Roberts, who's shown that very simple algorithms based on elementary cognitive principles can feasibly zero in on choice of parameters tested this approach with thousands of languages of great typological
variety. I think we can fairly say that the problem of externalization systems is fairly well in hand, at least in principle, partly in practice.
And if externalization is the primary, maybe the only locus of variation, a large part of the conundrum is on its way to resolution, namely variability and learnability.
Well, to proceed beyond, let's turn to some concrete examples. I'll begin with the most fundamental property of human language, one that is quite surprising in many ways, and has rich consequences that are not always sufficiently appreciated.
This is the property called structure dependence. Let's briefly review some standard cases. So begin with the operation of construal.
For example, finding out what verb phrase an advert modifies to take the sentence, the man who fixed the car carefully packed his tools. It's actually ambiguous.
Could mean he fixed the car carefully, or he carefully packed his tools. Now, place the adverb in the front, carefully, the man who fixed the car, packed his tools.
It's unambiguous, can only mean he carefully packed his tools. Actually, that poses a puzzle. The advert in initial position has to find a verb phrase to modify.
But in doing so, we ignore the simplest computation, find the closest verb phrase, which would be fix the car. We reflexively ignore that and choose the more remote verb phrase, packed his tools.
It's clear if you think about the structure of the sentence.
The phrase packed his tools is actually closer to the advert, then fixed his car, fixed the car in the abstract structure. And that's what we attend to, though we never hear it is actually more involved.
But that's enough to bring out the basic puzzle. We ignore the simple computation on linear order of words. We reflexively carry out a computation on strict structure. Let's take a second example.
Anaphors terms that don't refer in themselves but find to and find an earlier antecedent that determines what they refer to.
Well, the simplest algorithm is to seek the closest eligible antecedent as in sentences like the boys expect the girls to like each other means the girls like each other, not the boys. Sometimes, however, that fails as in sentences like the friends of the boys like each other.
Here it's the friends who like each other, not the boys, the closest antecedent. So once again, we ignore the simplest algorithm, which relies on what we hear words in linear order.
Now we use an algorithm that applies to abstract structures, and it's quite complex. First we have to identify the subject noun phrase, then we identify by the core element within the noun phrase, choose that as the antecedent.
That's what we do reflexively.
Final example, take verbal agreement sentence like the bombing of the city cities is a crime, not are a crime. The bombings of the city are a crime, not is a crime.
So again, we reflexively ignore the simplest operation, adjacency, and again carry out an operation that looks quite complex construct the abstract structure, subject predicate, locate the head of that structure.
That's what we do reflexively.
Well, these observations generalized structure dependence holds of all constructions in all languages.
What that means is that from infancy and on through life, we reflexively ignore 100% of what we hear words in linear order. We attend only to what we never hear abstract structures generated by the mind and operations on these structures.
Furthermore, experimental work has shown that the principle is known to children as early as they can be tested two years old or less.
Notice that this curious property is restricted to I language, the generation of thought principle doesn't hold for externalization.
Well, the only plausible conclusion is that I language and externalization are distinct systems for I language, computation ignores what is externalized.
Furthermore, all of this must be part of the innately fixed faculty of language property of Eugene conclusion further supports the thesis that I language which is language proper is a system of generation of thought, as was assumed in the millennia old tradition.
Prior to the behaviorist structuralist revision in the 20th century. Well, it's plain why languages use linear order for externalization is required by the speech organs, which cannot produce structures.
In fact, sign language, which is the resources of visual space available, does not adhere strictly to linearization.
We recall that the sensory motor organs used for externalization are unrelated to language and their nature and evolution. So it's not surprising that I think which pure language ignores the properties that are imposed by these non linguistic organs.
Well, with this in mind, we can turn to the why questions. Why does Eugene for structured abundance. The optimal answer would be that this follows from the simplest forms of Eugene, using the simplest operation for generating expressions.
The simplest operation is unbounded binary set formation. It's the basis for what's called merge in the literature, and in fact, structure dependence does follow directly from merge.
So to put it metaphorically, when mother nature created language, she found the simplest possible solution, which is incidentally, the way evolution works.
Similarly, all of science so it appears to driving intuition of science since Galileo spinoso broadly verified that Einstein called it the miracle creed that guides all serious inquiry.
In terms of language, it's called the strong thesis. Reliance on merge as the soul structure building operation has many other crucial empirical consequences won't be able to go into it in the time available but there's fair amount in the literature.
Reliance on merge as the soul structure building operation yields the core of the basic property, generating an infinite array of hierarchically structured expressions.
I can only add that the primitive elements of the system are elementary concepts, elaborated by merge these become phrases, we can translate straightforwardly to event semantics.
And then tactic exponents of participants in events program along these lines, if it can be fully carried out with complete to generate the theory of generation of thought should add that the evolutionary origin of the most elementary concepts that have to serve as primitives is a complete
mystery, and will probably remain so.
There are no analogs in the animal world, no record of their development. Well, there are two logical possibilities for merge of X and Y.
The two are distinct, or one is part of the other. Technically, it's called a term of the other. The two options are called external and internal merge.
External merge yields combination, as in read books, internal merge yields displacement, as in which books did he read, where which books is understood to be the object of read.
The strong minimalist thesis should not permit an operation of deletion, which would allow far richer systems in externalization deletion does take place within narrow conditions.
But that does not apply to I language, which we may therefore assume has no deletion operations that has consequences.
Let's take again simple cases like which books did you read the I language representation is which books did you read which books, which is interpreted directly as for which books X, you read the books X.
The merge automatically yields the phenomenon of displacement with reconstruction. It's a topic that's been extensively studied. It falls out directly from the optimal system of generation.
This structure dependence does what reaches our mind in this case is which books, did you read which books, but which reaches the ear and reaches consciousness is which books did you read externalization removes all but one occurrence, which has to remain to indicate that the
operation took place. That's a general economic issue, radically reducing both mental computation and execution of articulatory emotions, actually the result of these simple operations to maximize computational
efficiency yield problems for perception and parsing, what are called filler gap problems. The parser finds a WH phrase, like which books, and it has to find a gap that fills it for interpretation, turns out to be quite complex, one of the major problems in parsing.
The problem would be radically reduced if the gap were filled, but Mother Nature didn't care about that when designing language. She insisted on finding the most elegant solution, even though it poses difficulties for language use quite serious ones.
A sentence has become more complex. This is one of many cases in which computational efficiency conflicts with communicative efficiency.
And in all cases, communicative efficiency is sacrificed. It's more evidence for the traditional conception of languages, fundamentally a system of thought.
All of this is consistent with how evolution proceeds quite generally. Schematically, we can distinguish three stages. First, some disruption takes place, maybe a mutation or drift or gene transfer, or
a bacterium accidentally swallowing another microorganism. That's the breakthrough that led to complex cells. That's why we're not all bacteria. The second stage is reconstruction. Nature reconstructs the new entity in the simplest way, observing the miracle creed, paying no attention to how the new entity might be used.
The third stage is winnowing the outcomes that reproduce more effectively prevail. That's natural selection. Well, that suggests a reasonable scenario for language evolution. First, some minor rewiring of the brain took place.
Yielded the new property of recursive enumeration. Next step is reconstruction. Nature devises the simplest, most elegant way to organize the new system. Strong minimalist thesis.
The semantic theory becomes available, possibly with more primitive origins. Computational procedures are given part of natural law. The winnowing stage is never reached, possibly because of lack of time.
Possibly because the optimal system is so delicately designed that it's either all or none. So we're approaching an interesting conclusion.
The internal language might be perfect on a common human possession. That's the strong minimalist thesis proposed 25 years ago, was not then regarded as realistic, rather as a long term goal that might guide research.
Recent work does suggest something more audacious. Thesis might indeed be true as evolutionary theory, leases to suspect, along with science generally, and as empirical inquiry increasingly suggests.
Well, there's a lot more to talk about. Many new, many other possibilities, but I don't think there's time to talk about it here.
The crucial point is that the internal language seems to keep to the strong minimalist thesis throughout, with no departure necessary, even for pretty complex cases.
Well, remains to show how far we can extend such reasoning. Needless to say, that's an extremely challenging task.
Nothing like it has ever risen in the study of language and thought. There are promising early steps far beyond what could have been imagined just a few years ago.
Final remark, strong minimalist thesis has several functions. One is a disciplinary function, sharply restricts the options for description, and thus deepens explanation.
It also has an enabling function. It provides options for what I language might be, for what kinds of subsystems might exist. An interesting question just coming into focus is to explore whether I languages make use of all the possibilities that are enabled by the strong minimalist thesis,
which may be more than just a guideline for inquiry, as assumed in the past, but may actually express fundamental truth about the nature of language and thought, the most distinctive possessions of this strange species of ours.
Thank you.
Thank you, Mr Chomsky. Amala, do we have any questions? Did you select some?
Yeah, I see this question. To what extent does the UG remain available to second language accusations? If you can see the chat, Mr Chomsky.
Okay, I see questions here. First one I see is, what's your take on views of data linguistics that focus on the data instead of UG and internalization, and also criticize the syntactic theory as well?
What's your take on Annie Chomsky in School of Thought? Well, I can't comment on all of them. There's plenty of criticism. You have to pick them up and look at them.
As far as data linguistics is concerned, it's fine. You can collect a lot of data and can look at it, can find surfed properties of it. It doesn't tell you very much, just as in any other field. You look at a lot of data, you don't find much.
If you want to study the laws of motion in physics, you don't collect data about leaves blowing in the wind. What you do is careful experimentation, including thought experiments, to focus on the crucial principles.
Now, right now, data linguistics is very popular. There are studies that you've seen many of them in the press, a lot of excitement, GPT-3 and others. They take astronomical amounts of data, maybe 50 terabytes of data, have thousands, maybe a trillion parameters,
supercomputers running on them. And with all of that data, they can find surfaced properties of the expressions in some vast amount of data. You string things together like this, it looks more or less like language.
It tells you absolutely nothing, zero. The proof of that is that the same systems work exactly as well for impossible languages. So for languages, say that violate structure dependence, which are impossible for humans, they work just fine.
It's very much as if I were to go to a physics conference and say, I have a terrific new theory. It accounts for all the particles that have been discovered, even the ones that are possible and haven't been discovered.
And it's so simple that I can express it in two words, anything goes, sorry, I don't get the Nobel Prize, because the same theory accounts for everything that's not a possible particle. So it tells you precisely nothing.
All of these things that are achieving vast excitement and the press basically do nothing. I mean, there are approaches based on the same techniques, deep learning, that do do something.
So for example, the Google translate is based on these techniques and it's useful. There's been some success in handling protein folding by these methods, but these data linguistics approaches are absolutely worthless.
They don't achieve anything useful. They don't.
And they tell you nothing about language by definition. So basically it's a way to use up a lot of the energy in California and to develop public relations for the Silicon Valley, but it's doing nothing else.
Well, the next to what extent does UG remain available to second language acquisition.
And secondly, is it true that movement from the specifier VP the specifier of TP is not motivated by the chest case checking property nominative case.
Well, first, to what extent does UG remain available to second language acquisition.
There is a good deal of research on that by people working on second language.
My own view is that decisive conclusions have not been reached.
There's, and it may actually vary among people remember there's a lot of individual variation and variation as to how the second languages are acquired, a second language that's acquired through immersion.
The best way if possible is going to be acquired very differently than one that's acquired by reading a grammar or going to a class.
And it's possible that UG functions in the first case, much more effectively than it does in the second.
These are basically open research questions, non trivial ones for teachers.
Is it true that movement from the specifier VP, the specifier TP is not motivated by the case checking principle.
Well, again, that's an internal technical question I my own view is basically that it's not.
There's no reason why the nominative case couldn't be assigned to the specifier of VP.
There are other reasons for this having to do with the, what's called EPP, the and the and ECP the empty category principle which are very, which are tied together and they're very their properties of certain languages, their
properties of languages that don't automatically delete the freely delete an empty subject. So Spanish and English are quite different this way.
Spanish, you don't have to pronounce the surface subject and spec TP in English you do English is one of a relatively small number of languages where you have to pronounce it.
Spanish is one of the much larger category of so called no subject languages in which you don't. So for Spanish, the movement is not obligatory. It's optional for other reasons for semantic reasons.
And I think when we look at all these things together they really have to do with what's called labeling theory. It's been discussed for the last 10 years or so I don't have time to go into the details.
Well, how does generative grammar differ from descriptive grammar depends what you mean by descriptive grammar. If you mean what was meant by it and structuralist linguistics.
Descriptive grammar. What I studied when I was a grad student under graduate 80 75 years ago was regarded as a taxonomic science. That's what it was called.
There are procedures of analysis which you use that find the units of a language and the way in which they are organized relative to one another. That was descriptive grammar.
If there's anything new it's just by what's called analogy. I don't think any of that can be sustained. A generative grammar does something quite different.
It tries to tell you what all the possible structures are for the entire language. It's quite different from a taxonomic grammar. Furthermore, the elements that enter into it can't be acquired by procedures.
That was shown years ago. Now, from another point of view, the generative grammar is a descriptive grammar, but a very different kind from the descriptive grammars of structural linguistics.
What do you see as the future of linguistics in the future six field in Pakistan? Well, controversial subject question, of course, my own feeling is pretty much what I tried to describe.
I think there's a future for linguistics as quite a new field, a field that for the first time is able to provide genuine explanations for the basic phenomenon of language, instead of describing them, which is hard enough.
There's a couple of examples. There are many more that can be carried through. It's a major breakthrough for the future. You can't predict the future of science, so we don't know. But that's my personal opinion.
We just see the future differently. What about the future in Pakistan? Well, simple answer to that depends on Pakistani linguists, other linguists who are interested in studying or do other native languages of Pakistan.
I don't know if you're interested, but for the future of the linguistic field as a general field, it'll depend on developments within Pakistan, in Pakistani universities and research centers.
Many things changed in the last 70 years. What new parameters must we take into consideration for effective language teaching?
A few years ago there were no parameters. The assumption in structural linguistics explicitly stated over and over was that any languages can vary virtually without limits and each individual language must have to be studied in its own terms
with no assumptions about other languages. That was in the mid-1950s the basic doctrine that was sometimes called the Boazian doctrine, rightly or wrongly.
Well, that's very far from true. I should say that something similar was believed in biology at about the same time. It was assumed that organisms vary virtually without limit.
Each one has to be studied on its own. In biology that's now known to be completely false. In fact, there are very narrow constraints on possible organisms.
So narrow that some have even speculated that there's a universal genome and the apparent variety of organisms is just superficial working out of some of the possibilities determined by the fixed form for organisms.
I think something along those lines has happened with regard to our concept knowledge of language. About 40 years ago there was an innovation in linguistic theory suggesting that the basic theory in our minds has fixed unchanging principles
like some of those they discussed and parametric variation, a set of parameters. So that would mean that language acquisition is kind of like a question answering game.
An infant asks for each parameter, is it set this way or that way? Is it set as an old subject language like Spanish or like a language that requires an explicit subject like English? Yes or no question can be answered in a very small amount of data.
Does the language have verb proceeding object like English and Spanish or does it have verb following object like Japanese and many others? Again, answer Belenna, small number of evidence.
We now have recent work of the kind I mentioned like Ian Roberts, which shows how the infant can work through the system of parameters very efficiently to fix on the exact language.
I want to look at what look like the plausible set of parameters. The best work I know is by Ian Roberts and by Giuseppe Lungabardi, teaches at York University in England, has done extensive work on the variety of possible parameters and what they tell us about the history of linguistics.
That's a new topic that he's innovated and it tells us quite a lot. You can get much deeper knowledge of the relationships among languages at a much deeper level if you look at shared borders than in the standard way.
So that's a source you can look in. How does UG theory play into surviving languages like Romanian, maintaining its Latin roots along with neighbouring Slavic languages?
You can say the same about English, about 60% of the vocabularies romance, though a lot of the grammar is Germanic. Say the same about French. French has a lot of Germanic properties, but it's a romance language.
So it has a critic movement like the romance languages, but it's like English and German in requiring an overt subject. Well, UG is exactly what studied by UG.
So how are the range of parameters determined for particular languages? They all seem to share the same principles at an underlying level. In fact, the same is true of languages.
Amazon or Papua and Guinea tribes that haven't had other contact for tens of thousands of years. When they're studied carefully, they seem to have the same underlying principles, different parametric choices.
There is a phenomenon of called Sprachwende languages commonly accept some features of neighbouring languages, even if they're related, not closely related, which is what happened with Romanian.
It picked up some of the properties of neighbouring Slavic languages in these geographic areas of which involve interaction among people. In the case of English it's pretty straightforward.
The Norman conquest in 1066 turned English into a mixture of romance and Germanic with a lot of romance vocabulary. In fact, the majority, but Germanic Germanical structure.
Given the natural, the recent advances in natural language processing and AI, what prospects do you see for SLA?
As I mentioned, the advances in natural language processing and AI, well, they may be useful for some purposes. Just tell us absolutely nothing about language.
If they happen to be useful for some purpose, say Google Translate, Life Translation, Transcription, SLA, that's fine. Use whatever is useful. But if you're interested in the nature of language, the nature of cognition, human cognitive processes, this work just tells you basically nothing for the reasons that I mentioned.
Does the human brain contain a limited set of constraints for organizing language? Undoubtedly it does. We don't know very much about it because we don't know very much about the human brain.
It's a very hard topic to study. The brain altogether is a hard topic to study, even for tiny organisms. So if you take an ant, which has a brain the size of a tiny, a minute brain, you need a microscope to see it.
We have no idea how it carries out highly complex computations that humans can't carry out so the desert ants in my backyard can navigate in a way that a human can't. They use computations that are inaccessible to us.
We have to duplicate it with complicated instruments. Even the ant brain is very hard to understand. Human brain is much harder because we cannot do experiments, the kind of experiments that immediately come to mind.
You can't do them for ethical reasons. So we happen to know a fair amount about the human visual system, but that's from experiments with some monkeys, which rightly or wrongly we've allowed ourselves to carry on.
And they have about the same visual system they do. But you can't do that for language because there's no other organism. No other organism has even the rudiments of human language. So there's nobody to study.
And you can't study the human brain for ethical reasons. So it's a very difficult topic. Nevertheless, there are some achievements. One of the most important ones, in fact, has to do with structure dependence, what I mentioned, the property I mentioned.
There is research created by Andrea Moro, fine linguist in Italy, Milan research group, which was able to show that here's the paradigm they used.
They took subjects, say, whose native language was German, and they gave them two kinds of invented languages. One invented language was based on an existing language that they didn't know, maybe Italian.
Another database was very simple language, which used principles that you don't have in language, like linear order. So a language in which negation is the third word of a sentence.
It's very simple to work out. Well, it turns out, when the subjects were given an inventive language based on an actual language, the norm, the language areas of the brain language, dedicated areas of the brain function normally.
When they were given an invented language that violated structure dependence, even with trivial algorithms, there was diffuse activity, the brain, the language areas were not activated.
Well, that tells you that the brain, it tells you something about what we expect to be true, that the brain is structured in such a way as to be available for language the way it is.
It sets conditions on what language must be. One of them are the conditions that conform to the fundamental principle of structure dependence. There are a few other things like this, which are quite interesting, but it's a hard topic.
What are my thoughts on systematic functional grammar?
I think it tells us much about the questions that I've been discussing here. It may be useful as indicated in the question by studying the wider context in which language is used.
So there is a broad context in which language is used, which is not studied by the generative grammar which investigates sentence grammar.
It's a perfectly sensible topic and maybe functional grammar can provide some help with that. It does all to the good.
So if I say something like, John went to the library, he bought a book, John went to the bookstore, he bought a book.
If you think about what the language tells you, doesn't tell you what the word he refers to. That discourse is perfectly possible if John went to the store and Bill bought a book. Doesn't tell you that the book was bought at the store.
It's perfectly the sentence that are perfectly possible if John went to the bookstore, Bill bought a book somewhere else. That's perfectly possible. Language doesn't tell you anything about that.
There are conditions of normal discourse that make it likely that in John went to the store, he bought a book, makes it likely that he is John and that he got the book in the store.
That's plausible because of the nature of human functioning systems and maybe functional grammar can tell you something about such things.
I frankly think there's not going to be a lot to say about it. These are quite complex and diverse.
Well, that's the end of the questions that I see here.
Oh, I see a couple of others.
Let me say Chomsky, if you have the time to answer them, that's okay. Or if you want to stop right now, it's up to you.
Sorry, I couldn't hear that.
I'm asking you about the questions. If you want to go ahead or you want to stop here, we know you have some other things to do and we had only one hour for this webinar.
Should I go on with some further questions?
If you want to.
Is that what you're suggesting? Yeah, okay.
How may we better connect UG with contemporary teaching?
It's kind of like asking if you're, suppose you're a swimming coach.
It's a good idea to understand something about physiology and use your knowledge of physiology and improving your teaching of swimming.
If you're teaching language, it's useful to know something about language. That's UG.
A skilled teacher will figure out how to use their knowledge of language.
UG in teaching just as a good swimming coach will use their knowledge of physiology and teaching swimming.
There's no formula for this. That's what good teaching is about.
As a generative grammar is concerned, you have this claim that teaching grammar should be excluded in classroom construction.
It's been criticized by some linguists long said that teaching should not be excluded, but that of communication should be foregrounded.
Do you think this claim of excluding teaching grammar and classroom instruction is still valid, especially in today's time?
It depends what your goals are.
If you want the student to understand the language that they're acquiring, it's perfectly useful.
It's very useful to teach something about the nature of the language.
If you're teaching just by immersion the best way, so you send somebody to Italy to study to learn Italian, they'll just pick it up by listening to it.
That's not teaching. That's immersion.
You try to make it as much like a child as you can, but there's no...
There can't be an answer to this. It depends what you're trying to achieve.
If you're trying to achieve communication skills, but you don't care whether the person understands the language, you can foreground communication.
If you care about whether the person understands the language, you'll teach grammar.
That's just the nature of the language.
Why did I choose grammar?
Grammar is a funny word.
Grammar, in the technical sense, grammar just means the nature of language.
What I've been talking about is grammar.
What's the nature of our faculty of language and thought?
Because we're intimately connected.
So why did I choose to study language and thought? Good reason, I think.
These are the fundamental properties that distinguish humans from the rest of the organic world.
So if we want to understand what kind of creatures we are,
if we want to know thyself as the Delphic Oracle advised us,
the obvious thing to look at is language and thought.
It's been assumed for several millennia. I think it's a wise choice.
I chose it for the same reason.
Well, the others that I see here have to do with
finding
those exposing learners to the cell environment for more than three years,
make them acquire, develop a native like language.
A child, yes, a child will pick up a second language in almost no time.
Just to give you an example from personal experience.
My family went to, I was teaching in Italy about 40 years ago.
So my whole family came along.
My older kids were studying, 20, 25, were studying Italian.
They really wanted to learn it.
We had a 10-year-old who didn't want to learn it.
He didn't want to come with us. He said he was not refused to learn Italian.
Well, after a month, the 10-year-old, if the phone rang, the 10-year-old had to answer it.
Despite himself, he just acquired Italian by being in school where everyone was talking Italian.
Children just absorb it like a sponge.
The older kids were working on it, just my wife and I were going to make it harder for adults.
So the answer is depends on age, depends on motivation, depends on all kinds of things.
But you can live in a foreign culture for all your life and not know the language.
I know people like that.
Do you find the need for an eclectic approach to communicative competence?
Do we need to apply all kinds of competence time to test the speech?
Currently, researchers use only four or five or single competence.
Well, I really don't know. It seems to me, if you want to broaden the approach to communicative competence, whatever that is,
use whatever you find valuable to increase it.
I don't think there are any formulas.
Why do two children of same parents start speaking languages at different ages?
Well, there's slight individual variation, but not very much.
Unless there's some very unusual circumstances, two children in the same family will start speaking at about the same age, maybe a couple of months different.
UG has nothing to say about this. It's a matter of maturation.
The maturation of innate competence, it's like acquiring the ability to walk.
You find children are slightly different. One child may be walking at 12 months.
Another might start walking at 14 months, but it's just a matter of how the organism matures.
Maybe the main core of UG in just three words.
I don't know if I can do it in three words, but it's the theory of the nature of language.
We talked about explanation and description. What about the metacognitive aspect of learning?
I don't know what can be said about that, not familiar with it.
I think I'll probably have to stop here. I don't see any simple questions.
Yes, Mr Chomsky. Thank you very much. We really appreciate you being here with us.
Thank you very much on behalf of T-Soul Tampico Talks and Teachers Development Webinars.
Thank you.
You want to say something before Mr Chomsky leaves?
Yes, Professor Chomsky, it has been a really fulfilling experience having you at Teacher Development Webinars.
Thanks to T-Soul Tampico Talks for organizing this wonderful talk.
As a student of linguists and now I am a scholar, I have been reading your work and seeing you as a father figure.
Having you is like a dream come true.
I really can't express my happiness and humbleness.
I much appreciate your time and experience in sharing all this.
You celebrated your birthday a few days ago.
Happy belated birthday to you again. Long live.
Thank you, Amnala.
Thank you, everyone. We hope to see you again in our next events, our next webinars.
Yes, I would like to also like to acknowledge Master English Training for sponsoring a Zoom account.
The other ones, the supporters have come to us in time of need.
Thanks to Master English Training and Fatima, who deals with all these things.
Thanks very much for your time and support and appreciation for Teacher Development Webinars.
Thank you to T-Soul Tampico Talks.
Thank you, Amnala. Thank you, everyone. Thank you, Mr Chomsky. Bye-bye. See you next time.
Thank you, Amnala.

All right. Hi, everybody. It's Michael Schirmer. We're here in the offices of the Skeptic Society
and Skeptic Magazine. I just want to ask you to give your support to us. We are a 501c3
non-profit science education organization. We promote science as opposed to junk science,
voodoo science, pathological science, bad science, non-science, and plain old nonsense.
And unless you've been abducted by aliens or sent by Elon Musk to Mars for the last 30 years,
you know there's a lot of nonsense out there. Some people call us debunkers, but you know what?
There's a lot of bunk that needs debunking. That's part of our job, as well as explaining
and understanding why people believe in bunk. So if you want to support our efforts,
go to skeptic.com slash donate. Skeptic.com slash donate. Your tax deductible donation will support
our work here at the Skeptic Society. Nick, thanks for coming on. It's a great honor to speak to
you. I don't think we've ever met in person, but a long time sat at your work. And you've really
sparked an international conversation. Yeah, with the whole AI thing, yeah. Now it's been
fascinating in the years since superintelligence came out in 2014, just how much has changed.
What used to be a very fringe topic. I mean, back then, at least in academia,
the whole idea that AIs could potentially achieve general intelligence someday,
and maybe superintelligence, and that that could pose various kinds of risks, like was
dismissed as science fiction or futurism. And there were like in the world in total, maybe
like a handful of people scattered around the internet trying to work on the AI alignment
problem. And now, of course, all the frontier AI labs have research groups working on this and
you have statements coming out of the White House and other places focusing on transformative AI.
So yeah, it's been an interesting journey. Indeed. Let me start off with a statement from
your colleague, Eliezer Yudkowski. You'll be familiar with this. After ChatGPT came out,
he published an op-ed in Time Magazine. That's actually, oh my god, it's one year ago today,
he published this. That's amazing. Many researchers steeped in these issues, including myself,
expect that the most likely result of building a superhumanly smart AI under anything remotely
like the current circumstances is that literally everyone on earth will die. Not as in maybe possibly
some remote chance, but as in this is the obvious thing that would happen.
What do you think about that extreme statement? Well, I mean, so there's a spectrum of people
with different P dooms. It's non-colloquially the probability of doom from AI, where he's at one,
towards one end of that, like amongst, like, perhaps the most pessimistic or certainly amongst them.
Amongst a set of people who actually have some knowledge and have thought about this and then
others have lower probabilities. But I certainly think there is a real chance, a real existential
risk that will be connected to this transition to the machine superintelligence era. And it's
non-trivial. And we should work to reduce it by putting in the effort to develop scalable methods
for AI control in whatever time we have available before this happens. And there is now more talent
and resources going into that. So that's the good news. But I still think we don't yet fully have
that problem solved. So your institute is the future of humanity. The other one, the future of
life issued that statement a year ago now. Calling for a pause on AI development. I
noticed you didn't sign it. Why is that? Well, I'm not the big signer of things in general. I just
did the whole with isms and signing and it's just my personality. I feel there's also a little bit
of a risk if you're, if you're a philosopher, if you're kind of your job is to try to be a little
detached and to evaluate things and be open-minded. Like once you start to get involved in a particular
campaign, it's very hard to retain the ability to change your mind. It's not impossible, but it gets
harder. And I feel philosophy is hard enough as it is without adding extra difficulties. I have no
objection to other people. I think it's good people shouldn't be involved in campaigns and working
for things. It's just, I always feel a little awkward. Also, usually any one given statement,
there is always something that I slightly would have a different view or have worded differently.
And so, yeah, it's more just my hang up rather than some kind of big statement I'm trying to make
by not signing the statement. Right. The only statements I sign are that there should be no
signed statements. We should let people just say whatever they want. Free speech. Yeah.
Yeah. Okay. So just give us a little bit of background. You're so famous in this area. How
did you get interested in AI? I mean, are you like a Star Trek fan or, you know,
go back to your childhood or whatever, teen years or whatever triggered you to go down this pathway?
Yeah. No, I'm actually not so much a science fiction type of reader. I mean, a lot of my friends
and colleagues are. I just never really been much into that. Now, I had a, I mean, I grew up in
Sweden and this was before the internet in a relatively small town. And I knew nobody when
I grew up who was at all interested in literature or science or ideas or anything like that. So I
was like bored out of my mind in school and I associated sort of learning with school. So I
didn't. And then I sort of went to the local library. I think I was 15 randomly one afternoon and
started pulling out one book and another. And I realized that like that was actually a big world
of ideas very different from the stuff that was covered in school. That was like super fascinating.
And then I pivoted and became kind of fanatically engaged in this project of self-education,
because I felt I had been missing out. Like I've wasted 15 years of my life and I wanted to make
up. And then I started to study, I studied physics and AI and neuroscience and I painted and wrote
poetry and philosophy, of course, and just everything I could sort of lay my hands on.
And for us, almost as long as I remember, it always seemed to me that there's a bunch of
things we can do to change the world that consists basically of moving things around in the external
world. But what would be more likely to cause a profound change would be if one changed the
thing that does the changing. And so all of the technologies and ideas that we have ultimately
come through is sort of the burst canal of the human brain. So anything if you could sort of
upgrade the human brain or change our mood or cognitive capacities, that would be potentially
transformative. And in parallel with that, if you could develop new brains through artificial
intelligence research, that also could be world changing. So I had this vague sense from early
on. And then I kind of, yeah, it got more specific as I went along, neural networks intriguingly,
like actually, from the very beginning seemed to me like to have legs in the sense of being on
the right path. And I remember, I think I was like 17. And I had gotten this, like on interlibrary
loan from the local library, there's this volume on parallel distributed processing, which was like
one of the first sort of by Rommel Hart and like this this classic now, but like where they tried to
deconstruct biological neural circuits in mathematical terms. And I was like super
fascinated about that. And so yeah, and then I studied neuroscience, computational neuroscience
in later on in London. And now the so deep learning evolution seems seems to validate this
that these kind of massively distributed pattern recognizing of learning algorithms
is the way to go. Yeah. Yeah, you know, somebody like me who I don't work in this area,
you know, there's so many great, smart experts on all sides of this, you know, you have Elon Musk
and Stephen Hawking and Bill Gates concerned about AI existential risk. And then you have other people
like Kevin Kelly or Steven Picker going, no, no, no, this is not going to happen. We can do this
incrementally. And so and I always think of you as sort of in the middle, maybe, you know, your
super intelligence book introduced the idea of the, you know, paperclip maximizer and the alignment
problem. But I didn't feel you went to the extreme position of existential risk, but maybe give us
a little bit of where you are now since that was 2014, right? Super intelligence, where you
stand on on the threats of this based on that alignment problem. Yeah, I think it is going to
be a very powerful thing if we do create machine super intelligence, it's not just
internet 3.0 or a mobile internet or like one of these, like always some new cool thing, right?
But I think this is qualitatively different in that it will be the last invention we ever
need to make. If we do it, because then it would do the father inventing and presumably at digital
speeds. And so I think it will be a transformative and with enormous upside, but also potentially
big downside if we fail to align it to human values. And there is now a lot of resources
where people are trying to explain how creating very powerful optimization systems unless you're
able to sort of point them very precisely could result in disasters in various different ways.
So if we are lucky, we will solve that problem by the time somebody figures out how to solve the
problem of making AI stats smart. And as I said earlier, there are lots of people working on that
now and including a lot of the smartest people I know are kind of going into this now AI alignment
and more resources are being spent by frontier labs as well in terms of devoting.
One of the solutions would just be government regulation, like of any technology. So here's
my example. I have a Tesla. So I'm here in Santa Barbara. I want to go to LAX, take my flight,
I push the little button, I go navigate LAX. Now it knows to avoid the heavily trafficked
LA freeway. So it takes me down side roads and so on. The moment it takes me up on a sidewalk
to mow down a bunch of pedestrians in order to avoid some traffic, how long would it be before
the Department of Transportation and Safety Board swooped down and shut down Elon's company
to prevent that from ever happening again, like a New York minute. Maybe that's one solution.
Well, I mean, if it happens in small pieces and gradually like that, we might have the ability to
observe things going wrong and then take corrective measures. And that's how we deal with most
new technologies and the problems they cause. Like we invent cars, we find that they sometimes
crash to invent seat belts and traffic lights, etc. I think there is a small subset of things
that could go wrong that are in a different category. I call them existential risks. And these
are where there is a risk to the very survival of earth-originating, intelligent life. And these
are risks, in other words, that would sort of put a permanent end to the human story, where we don't
get the second try. And so these are harder to deal with because we've got to get them right on the
first try. I think AI is one potential source of existential risk. And there might be a few
other areas, like synthetic biology might be another area where we could get unlucky and
discover that there is some relatively easy way to do something tremendously destructive.
And so, yeah, if one looks at AI in particular as a source of existential risk, there are a few
different ways in which it could do that. One thing to recognize is that once you have something
that is even just human level, but even more so when you have super human levels of intelligence,
is that it would be able to anticipate our responses to it. So if it wanted to
mow a lot of people down, if you were like some sort of rogue, self-driving car AI, right,
it wouldn't just run over a few and then be surprised that the Department of Transportation
shut it down, because that would be an obvious thing that would happen. We can even realize that,
right? So it would make some smarter plan to achieve its goal of mowing people down that might
include things like deceiving us about its capabilities, deceiving us about its goals
and intentions. It would have converted instrumental reason, perhaps, to seek more
resources and intelligence while also convincing us that it is safe. So this can make such systems
harder to test, because they might behave very differently in the sort of deployment phase
than in the testing phase. And so, yeah, and if we think like, you know, you could make analogies
to like when Homo sapiens arose on this planet and what happened to our Neanderthal brethren
at that point, or when indeed at a slightly lower level of intensity, but when a technologically
more advanced civilization has encountered a less technologically advanced civilization,
which has happened, and often it doesn't end up very well for the less advanced civilization.
So if you imagine that delta between sort of human cognitive and technological capacity and what
the AI could do being very large, then we might have a kind of much bigger encounter with, but
where we now like with our like fancy Western advanced technology would be like the underdog,
and this would be like basically like an alien civilization coming from the future, but in the
shape of a super intelligent AI that has kind of run ahead in. So that's one like type of scenario,
there are other scenarios in which it might unfold more gradually, and there might be many of these
AIs and they're competitive. And there are dynamics in the economic competition between
these different AI systems that might be hard to control. And if you insist on having too much
human oversight and human in the loop, it might slow down your AI system and somebody else who
you know, have fewer scruples, their AI system will, you know, and then out trade you on the stock
market or out invent you in technology space and out maneuver you in military space,
like their drones just are autonomous and operate faster. And you have some guy who has to
sit the press a button every time before it fires. Like if we're unlucky, the dynamic there could
just be such that the winning strategy is just to basically allow the AIs to run at full speed and
do whatever they want. And it's not clear what would happen to the human species in the long
term in that in that kind of scenario. So there are various different ways in which things could
conceivably go off the rails. Yeah, maybe I was thinking about Yacovsky's state of all life on
earth. How would that happen? Well, the only thing I could think of was because he didn't
give any examples, but you know, like maybe AI creates a video, a deep fake video that's so
convincing showing Biden launching the nukes against Russia or vice versa. And then that
initiates a large scale thermonuclear exchange and that that could end all life. That's the
only thing I could think of that could end all life on earth, even there. Probably not like a
nuclear exchange wouldn't end all the life. It probably wouldn't even end all human life on the
southern hemisphere. I mean, I would like I recommend against running that experiment.
We certainly know that it would be like the biggest horror ever. But then, but there is
still a distinction to be drawn, even if it's like academic between a global catastrophic risk
that could be very bad and an existential risk, which would literally be the end
of the human experiment. Because there have been big setbacks that have been dark ages and plagues
and all kinds of stuff. At one point in our prehistory, it looks like there was a population
bottleneck and we might have been down to a few thousand individuals. But eventually, we came back
from that. And similarly, if there is a nuclear war, but you know, a bunch of coastal areas in the
southern hemisphere where they can do fishing or whatever, they survive. And then, you know, after
a few hundred years, we might be back to where we started. But there are other ways available to
superintelligence, like three good event, the biological constructions that would wipe us out,
maybe or nanotechnology or maybe it just doesn't even bother very much with us, but just sort of
starts to transform us into one giant data center or some sort of launch facility for launching
space probes to kind of spread throughout. And we sort of perish as a side effect of the
waste heat or something like that. Yeah, I think it would be wrong to anchor too much on any particular
concrete scenario of the precise mechanism whereby human life or human values would be trampled over
and think more abstractly that if you have this very powerful, strategizing force in the world
that is antagonistic to us, chances are this much smarter, more strategic thing would eventually
prevail and be able to do whatever it's tried to do. So that's kind of like, yeah, a class of ways
in which things could go wrong. Now, hopefully we will learn how not to do that, as I said.
And then we might end up in this condition of a solved world that I discussed in the book.
Yeah, yeah, yeah. No, I want to get to the deep utopian. But I just want to give you a chance
to respond to a couple of your critics of that. Stephen Pinker writes of these
purported existential threats. They depend on the premises that, one, humans are so gifted
that they can design an omniscient and omnipotent AI, yet so moronic that they would give it control
of the universe without testing how it works. And two, the AI would be so brilliant that it could
figure out how to transmute elements and rewire brains, yet so imbecilic that it would wreak
havoc based on elementary blunders of misunderstanding. Yeah, well, first of all,
some elements there are just added for no reason, I guess, transmute elements. I don't
particularly know why that would be a necessary component of the view that AI could pose risks.
But I think the basic idea that we could be smart enough to create this thing without being
smart enough to realize that we also need to solve the control problem, unfortunately seems like
a realistic possibility, that we seem smart enough to create it. I mean, you can judge for
yourself, but year by year, we see AI capabilities galloping ahead. And I mean, it's not a question
whether this current paradigm will take us there, but certainly it doesn't seem
ridiculous for you to think that it might. And then that we might fail to realize that
there could be a difficult control problem or that we could mistakenly convince ourselves that
we've solved it even though our solution is flawed. I think it's also pretty plausible and
more plausible because there would be strong incentives for people to do precisely that.
If you have multiple labs or multiple countries all competing to develop this,
you know, potentially hugely lucrative technology, right, and also strategically relevant for
national security, etc. There's like this raising dynamic where multiple groups race to get there
first and whoever slows down or spends more of their efforts on safety and precautions and
testing it like they just fall behind. You could see that even in a good scenario where people
realize that ideally we should do this carefully, like that would still be just like overwhelming
and so competitive pressures to make it happen as quickly as possible, even with fewer safeguards.
And I'm sure that as we move closer to this to kind of
polarized debates that we're already beginning to see will be amplified and who knows how that
shakes out. People have kind of a tendency to run in herds. And this also on both sides of
this AI debate. I mean, in fact, I have started to worry slightly about the possibility of
overshooting the target in terms of AI alarm. Back in 2014 when the book came out and I worked on
it for six years before that, like the whole possibility of risks from transformative AI was
completely neglected. So I thought they clearly needed to be more attention to that than was
given to it at the time because at the time it was basically zero. So it's like now on the other
hand, there is a lot more and we are beginning to hear even top level policy makers start
saying negative things about AI. I think it's unlikely but less unlikely than two years ago
that we could end up in a trajectory where AI is never developed because we either end up with
like some sort of permanent ban or some agreement to slow down so much that before we actually get
around to doing it, we destroy ourselves in some other way like through some other technology or
something. And this still seems unlikely but the pendulum is swinging and I don't know we have
a very fine grain ability to sort of choose where it stopped. Like it's like an avalanche,
you can maybe trigger it but once it's going, you can call it back. And so people then, you know,
I don't think we're there yet but you could imagine it just this stampede of consensus
forming that AI is a bad thing and then it becomes taboo to say positive things about AI and then
policy makers like competing with one another to be like tough on AI just as it happens in foreign
policy context sometimes. And you know, you could imagine various scenarios in which we kind of go
too far in the other direction. Well, here maybe an analogy with the development of nuclear weapons
where you get an arms race where maybe you don't want to develop it in this direction but the
other guy may do it. So you have to do it because the other guy is going to do it. And then and so
on and so forth and you end up where we are now. Something like that maybe? Yeah, that certainly
is a class of scenarios and it feeds into this current debate about open sourcing AI models
which has like the obvious thing going for it that it's nice, more people get access, democratizes
AI, more eyes can detect more problems etc etc. So that's which is true for other open source AI
as well. Like it's generally a nice thing kind of culturally to open source but with the frontier
models there, there is a question of whether that is ultimately the right approach because it does
also mean relinquishing any ability to influence how the AI is used. So if you are an AI lab that
trains your AI to sort of refuse requests to give advice on how to construct biological weapons
or commit cybercrime or whatever else it might be then if you open source the model it's usually
quite easy then to sort of remove the safeguards. You do some more fine-tuning training and you
kind of train the model to actually be of assistance in these ways and as we move closer to
truly transformative AI of course if the model is open sourced anybody with a sufficiently large
computer cluster could run it and you can sort of call it back if it turns out that
there is some additional invention that could make its capabilities go above a critical threshold.
Yeah. These large language models, chat GPT and so forth or worse the Gemini embarrassingly
bad programs are these down the wrong path toward either dystopia or utopia? You think there's
something else that'll develop that and this is the wrong way or not the direction that this is
going to lead to either dystopia or utopia? I mean I think it's on the sort of shortest path towards
more capable AI. I think the current models we have are basically the first models
that we figured out how to develop that still were very capable. I think the technological
trajectory has not been shaped very much about some vision about what type of system ultimately we
need that would be the safest. It's just like it's hard to get AI to work at all and we try
everything and some things work and the thing that works best currently are these large language
models or I mean they're increasingly becoming multimodal models and it will be interesting to
see whether that is all we need. There is like a school of thought, it's a scaling hypothesis
that basically what we need to do is simply to scale these systems up even more and just as we saw
almost qualitatively new capabilities as you went from GPT-1 to GPT-2 and then GPT-3
new qualitative well it is GPT-4 you start to see some actual reasoning and understanding there
you know if we go to GPT-5 or GPT-6 just make them bigger with more data, more training,
more parameters it's possible that things will just fall into place without much further effort.
It's also possible that these will kind of be the engine blocks and you need a little loop,
some additional little thing on top of this, some external memory system that it can read and write
from, some agent loop that makes it possible to do more reasoning and planning than is feasible in
just one forward pass through a big transformer model but there are a bunch of such ideas already
in existence that it might be by sort of combining these in the right way and scaling it up you would
maybe get all the way. Of course we don't know until it happens. What about these examples we
saw of just embarrassingly bad searches where the chat GPT is just making up fake law papers and
medical findings that didn't even exist or worse the Gemini you know imposing DEI ideology onto
basic factual searches like show me pictures of the popes and they show pictures of women popes and
I mean it was just horrible, embarrassingly bad. Yeah well so these are two different classes of
problems so the latter one I think was on purpose like it was designed I mean obviously not designed
specifically that these historical characters should be rendered the way they do but that it was the
result of a specific attempt to make the outputs of these models feature a more variety of different
human types to sort of combat whatever the stereotypes that would result by default if you
just trained it on internet data which comes predominantly from certain demographics who
just have spent more time writing and posting on internet and stuff so I think there the solution
is more to sort of change the precise way that it's fine-tuned as for the former problem the
problem of hallucination that's more like a technical problem like an open research challenge
because they don't want them to hallucinate like the people building these Google doesn't want their
AI to do this but they haven't yet figured out how to completely remove that I think
as the AI systems become smarter I think we will see less of that just as a side effect of
the general increase in capabilities and already I think there is a bit less of that now than
was like a couple of years ago but yeah I mean certainly right now it makes sense to I mean you
should always I think this makes sense when you're getting advice from some human expert or from some
human source as well like you need to apply your own critical scrutiny to try to you know
evaluate whether it makes sense or not and it like doubly true with if you're getting it from
these okay generator deep utopia okay the search for utopia has always historically been a bad idea
this is a not a good goal to have because it always ends in disaster because somebody is gonna
block us from reaching utopia and we have to eliminate them you know that you know what I'm
talking about here historically why are you using the word utopia what do you mean by that what
are you after here for the long-term future well I mean the word is so it's not the book about
how to rearrange the political order or culture or society to achieve some like great outcome
which is what most utopias are like they're basically some some some usually they're like a
political program in disguise or a critique of some tendency in contemporary society like
if they're dystopia then like which is like the other side you might say like 1984 or
brave new world the kind of picking up on some problem in contemporary society and then saying
well if you continue down this path then we get to this thing everybody can see it's bad
so let's reflect on what we're doing now and maybe of course correct but deep utopia is rather
something like a philosophical investigation into questions about human value
if you imagine the whole AI transition going well so let's take as an assumption we develop this
and we we don't have any of these existential risks and we end up with this future condition
where like the whole economy can be automated and not only that but this AI then develops all
kinds of other super advanced technologies because amongst the jobs that could be automated if you
had truly general AI truly general AI is of course also the jobs of scientists and researchers and
inventors etc so we then get to I think ultimately if we think through where this
eventually leads a condition of technological maturity like a condition where we've developed
most of those general technologies that we that are physically possible
and for which there is some conceivable pathway from where we are now
and moreover in this solved world that we will get not only do I postulate we have
technological maturity but let's also imagine we solved our coordination problems politics
like no no no wars like the society's fear let's just all of those are of course extremely
important practical problems that that we need to fix but I wanted to get actually to the point
where you could ask the question is think about what happens then like assuming everything goes as
well as possible and and then where do we end up and what role is there for humans in this world
where like we don't need to well not only do we not need to work anymore to make a living because
like the robots and AIS could produce everything and drive the cars and run the factories and
write the word documents or whatever but a whole bunch of other activities as well that currently
occupy our days would become unnecessary in this condition of technological maturity
so right now even if you didn't have to work like suppose you're like independently wealthy
like you still a whole bunch of things you need to do you need I mean you need to brush your teeth
like Bill Gates has to brush his teeth otherwise he will have tooth decay and there is no way around
it right similarly if you want to be fit you have to actually put in some effort on the treadmill
or with the weights and there is no shortcut but at technological maturity like you could pop a pill
that would give you the same physiological effects as spending a lot of time working out would do and
so you can then go through activities one by one and thinking like do these will make sense in this
condition of a solved world and for a lot of activities the answer is seemingly no
they lose their point insofar as we do them for an instrumental reason that is we do we
spend time and effort to do x in order then to achieve some other thing why in most cases like
that in fact almost all of them in technical maturity there would be shortcuts to y that would
seem to make the whole activity of doing x pointless yeah okay let me ask you a question
chat gpt probably can't quite write a book as well as you do but maybe the next version does
please write next boston's next book would you do that or do you actually enjoy writing
i feel it felt a sense of urgency too i wanted to get it out before the singularity before it
writes it for you but don't you enjoy this this is my point don't you enjoy writing books i i don't
want an ai to write my next book i like writing books but so now it feels like a very meaningful
thing to do right yeah saying you work you rework it and then you hope that in the end it will bring
joy to somebody or they will learn something and but if if it had been possible
like instead of struggling with each paragraph and figuring out what you want to say if i could
just have pressed like a key on my laptop that would have produced the same paragraph or a
better paragraph um then it's not so clear like would it still feel worthwhile to sit and struggle
and sweat if it was just like a way of producing worse text then it could have been done by just
pressing the key that would activate you know gpt 8 or whatever to uh to do it um it could still do
it but i think um at least prima facie at the first time it seems like it would put a big
question mark over that activity like does it really seem valuable to do even if it were like
obviously utterly pointless and that was a much more sensible easier way to to achieve this exactly
the same i guess i'm trying to find something that uh has a different value that is it's valuable
in and of itself now like there's a lot of projects around my house i just hire people to do it because
i don't like doing it and i don't know what i'm doing or i'll just go to home deep on buy the
kit and just put it together rather than buying the raw supplies and and make you know sawing the
wood and whatever um but i like writing my books or i like writing my bike or playing tennis or
whatever i like working out um i don't want to take a pill to do that i don't want to pay somebody
to ride my bike or or hire a chat gpt to write my next book because i actually enjoy it so it's a
different value yeah um yeah i mean certainly uh that would be nothing preventing you from uh
still doing those things and many other things um if you value the activity itself
and if you truly value the activity for itself rather than subtly and in a way that might not
be visible to us uh actually as a means to an end for example uh as opposed to you
spend a lot of time writing because it actually uh made you happy like it made you subjectively feel
good uh well there that would be a shortcut right i think you could take a pill that would
give you the same subjective happiness and good feelings and a pill moreover without side effects
or addiction potential etc adds technological maturity oh no it's the challenge that makes it
valuable and not not just some glow feeling that's not what i'm after is that that's right so
there is like a whole big set of possible reasons for working hard on the book and some of those
reasons would be removed in this hypothetical context and it's like onion layers of onion you
can peel away and the and the question that the book is kind of exploring is like what remains
after you really remove all the instrumental stuff and i think there does remain something
um
but it's quite subtle um but i think ultimately there is a whole set of values that are currently a
little bit often invisible to us um that would come into view uh and that it would make sense to
focus more on if if sort of the the screaming moral imperatives of everything you have to do
like you have to go to work otherwise you don't get the paycheck and how are you going to afford
your rent you have to you know um help drive your kids to school because otherwise i mean what's
going to happen otherwise you have to do these so much stuff that we have to do that that and if
you look at around the world obviously there are huge needs everywhere that we should try to help fix
if you're mad at all of that going away then i think there are many more subtle
quieter almost like aesthetic values that it would be appropriate to allow to have a bigger
influence on what we do just just as you know you walk out at night and you see this big canopy
of stars and constellations like they're always there right they're there during the day as well
it's just the blazing sun kind of makes them invisible but if you might have been removing
this on suddenly all of this this rich iridescent sky of more subtle values would come into view and
i think our sort of evaluative pupils should dilate in this condition of technological maturity to
place more weight on those values and there is a whole range of them um and i think it is
from from these constellations of hypervalues that the that utopia would be constructed or at
least if you if you imagine a utopia that has a rich structure as opposed to a sort of simple
hedonic utopia where we become kind of pleasure blobs uh through like super drugs or direct
neural stimulation but if you imagine a more richly textured structure to utopia i think
the structure would come from a range of these canopy values that that would come into view
yeah i had andre yang on the podcast a couple years ago he was the presidential candidate pushing
the ubi universal basic income at the time he was concerned about ai taking over um taxi drivers
truck drivers and so on there's going to be hundreds of thousands of people put out of work
now that hasn't happened yet but it could but this is like saying well what are we a century ago
what are we going to do with all those elevator operators the little guy in there pushing the
buttons for you well there aren't any of those anymore they went and found something else to do
now could we say that most jobs are kind of crappy and no one really wants to do them they
do them because they have to make a living so in a post scarcity treconomics kind of model
nobody has to do the shit work anymore they can just write poetry or do art or write books or
i don't know what maybe they'll they'll find other meaningful things to do and that that is well
infinite there's there's no upper ceiling on finding meaningful things to do yeah well i mean
the question is whether they are meaningful there's sort of a lot of things you could do
and you could also not do them um but would they be meaningful so right now if for example
you work hard and it allows you to support your family and take good care uh of like that that gives
meaning to your efforts like the the boring office work maybe not so meaningful in itself but if it
achieves this outcome of giving like making your home a good environment for your your your your
spouse and for your children like that you know gives meaning or if you work hard for a charity
and it helps save the life of you know some disadvantaged like group that's like you have
achieved something and done some good in your world or you're a scientist and you work hard and you
like invent something new like either theoretically interesting or practically useful that's like
you have achieved something um so those kinds of meaning might not might be in short supply in this
uh in in this solved world in that you know whatever the scientists could do would be much
better done by AI scientists and uh you wouldn't need to be a breadwinner because the bread would
already be won uh through the economic abundance um etc there would there would be no starving
children uh in utopia so no need to well yeah let's look at the economics of it okay i could see
the argument for let's raise the lower bar as high as we can so no one is suffering everybody has
three square meals a day roof over their head education health care um and so forth what's the
upper ceiling uh it seems like you know i here i was thinking of david deutch's book the beginning
of infinity there's an infinite amount of knowledge we can find problems to solve what why would that
end well there are two questions there one is whether it would end but let's and we can return
to that but there is a second kind of almost preceding question which is even if there is
more to discover whether uh we would be efficient at discovering it so i'm suggesting even if there
is like important scientific research to be done as technological maturity it would be much more
efficiently done by uh machine intelligences um and so we wouldn't really like it would just be a
waste of resources to for for humans to exert calories to like try to think about these things
and AI would do it much better and much quicker and with like cheaper so so that's yeah even if
there was more i think we wouldn't be useful uh for discovering it um it's also possible although
this is an independent idea that um although there's always more to discover the most important
things might be at some point already discovered and then uh it's kind of more and more trivial
details that remain to be added to the scientific uh inventory of knowledge which i think is also
likely actually but um you do what if what if you're that guy in 1896 that said um you know we've
pretty much got physics all figured out here just before Einstein yeah he was just a bit early
i see he's a century early okay yeah or two or whatever but i mean we've only been around i mean
how long has science been gone for a couple hundred years or something right it's like trivial in the
big scheme of things yeah um and we don't even have super intelligent ai's to actually
really get cranking on uh making intellectual progress we're trying to do it with our meat brains
and it's not a few hundred years with meat brains like of course there's a little more to to learn
so maybe example of what you're talking about would be how do we solve the problem of schizophrenia
we don't really know yet and we haven't made much progress but maybe ai could test a thousand
different uh chemical compound um combinations to see what works and it could do it in a couple of
days rather than a couple of decades that humans would take to do it and that would be a solved
problem but why would there be at some point no no well okay so you're saying there's a finite number
problems to be solved for human flourishing um yeah well um so at some point i think you have
basically found the optimal ways of technologically achieving the types of outcomes that normally
need to be achieved you've invented the optimal solar panel you've invented the optimal space
colonizing rocket you have invented the best way of transmitting electricity from one point to another
like et cetera et cetera so that might be like you know like trillion types of tasks at that level
of description and like for each one of them you have worked out at the molecular level
what the most efficient mechanism is to do it or maybe not the most efficient maybe there are like
time you could improve it by like one tenth of one percentage point by researching it for another
thousand years and the as would be working to like make these small optimizations but it wouldn't
be like discovering relativity theory or evolution theory or something like that that like a simple
insight that has like like a big earthquake of ramifications for the way we perceive ourselves
in the world yeah all right i'm going to read from your book here uh the lines from harry
lime the third man you know what the fellow said in italy for 30 years under the borges
they had warfare terror murder and bloodshed but they produced michael angelo leonardo
davinci in the renaissance in switzerland they had brotherly love they had 500 years of democracy
and peace and what did that produce the cuckoo clock so how do you address that point that humans
need challenge again let's distinguish between happiness and meaningfulness slash purposefulness
it's those challenges that give us meaning and purpose that's the goal not happiness
um yeah well um it's it's hard to tell um certainly if it were happiness in the subjective
sense of positive effect it would make the problem very easy because trivially in utopia
technical maturity you could tune your hedonic well-being up or down very easily through certain
newer technology or drugs and stuff so if that's what we wanted then we would be home and drive
like problem solved we'll definitely be able to do a lot of that in utopia um if we want
challenge well certainly we could create artificial challenges uh there are games uh
very elaborate games with like all kinds of um you could have ai's inventing new games for us
like there could be so if artificial challenges are enough to realize that value that you pointed
to then also we are home and drive that that would also be very easy to do if we want genuinely
meaningful challenges then there is more of a challenge um in seeing how that would be possible
in deep utopia because prima facie at least at first sight it seems like our own efforts are
for most purposes unnecessary and then we could still do the thing but it may not obviously be
meaningful to do the thing if if there is nothing worthwhile achieved by doing it but I do think
there are at least ways of rescuing part of what we want if we want meaningful challenge even in
utopia and there might be first of all uh tasks that need to be performed by like so for example
if there are to take a very simple example consumers that have a preference uh not just
for a certain type of object but also a preference regarding how that object should have been
manufactured uh and in particular they wanted to have been manufactured by hand you know or by human
then that would be demand for human labor to produce we see that today like certainly the
consumers might pay more for a trinket that's done by some favorite group or like in indigenous tribe
rather than in a sweatshop in Malaysia like even if the trinket is the same or equivalent like the
fact that the causal process that brought it about was different might result in a difference in price
similarly we might prefer to watch like human athletes compete even if like the robots could
run faster or box harder or what they're like that might just be a brute fact about it and so
you can then see like or we might want like a robot uh you know priest administering the wedding
or sorry like a human doing it rather than you know a robot even if the robot could say the same
word etc so you could then you could look through like and there might be many more of these that
we can't afford currently so nobody has kind of even bothered inventing these services but
we're just humans have a sort of brute preference for it to be done by human effort
and I think in addition to that there might be more subtle ways in which that would be instrumental
uses for human effort if for example we have values say you say you have a value that values
the honoring of a certain tradition now many traditions in order to be continued would need
the active efforts of human beings to do whatever the things that they're traditionally done to have
the the ceremonies and to like focus our attention on certain things and even if we could perform
like great robots that went around then like perform the same songs and dances and stuff like
it wouldn't count as continuing that tradition so if we value that it might call upon us to
to make an effort and that might be one of these subtle values or maybe right now the tradition
is like well our tradition is tradition like whatever you know they're starving kids out there
we should focus on helping those but once all the kids are fed and all the diseases are cured
then these slightly less like uh it's real values might then deserve a lot of attention and aesthetic
values like there might be things we have reason to do because it would just be beautiful if some
body did it and um social cultural entanglements like the way that the different people have
preferences about each other and what they do and how that I think that might also produce
some opportunities for um natural purpose in naked in in utopia um you can also have
artificial purpose where you just set yourself an arbitrary challenge and then have your brain
motivated uh change so that you're like super motivated to achieve it that that that would
be safe but there might also be some of these more natural purposes um well there is this DIY
you know do-it-yourself movement where people seem to like just doing it by hand they just want to
get their tools out and get up in the garage and start making stuff I don't personally like this
because I'm not very good at that but it's a huge movement so I mean you could hire somebody or
there's a machine that could make the little shed better than you can make it by hand but people
seem to like to do that why not have both the AI does the stuff we don't like to do and then I'm
just going to do the stuff I do like to do yeah no that that seems seems good now there is an
additional challenge here which is uh lifespans could become very long right if we fix the things
that cause disease and death and like cellular decay etc so um if you are going to live for
maybe millions or billions of years potentially okay um uh you sort of run out of like just get
me to 100 without Alzheimer's all right well that's a good start but you know when you're 100 in
perfect health yeah I'd go for 200 maybe you think well do I really want to check out now
or maybe do another year let's let's let's push it a little bit further down and at least it would
nice for you to have the option of kind of uh because like I mean probably our like our age
when when we were a kid like being 50 or 60 or whatever that's like now I know particularly
white as well I don't but now of course when you're there you see that wow you know there's a lot more
that yeah could be done and experienced than the um and there are simple pleasures as well so
they're like the things you might only want to do once or twice in life but then you've done them
but then they're like other like a nice cup of tea or a coffee like it's kind of about as good as
you know on the tenth thousand times you do it and then you know in the first or second so you
don't really it's renewable as it were like a renewable sort of joy and so but but it does
mean also like one should maybe think of if the question is what's the best possible future life
that you could have if you remove all practical constraints and technological constraints
you really should think maybe in terms of a trajectory not not just a state that you would
reach and then you have sort of reached the peak but more like what's a developmental trajectory
that would like be you you'd get the most out of each level of development maybe eventually
like understood most things that can be understood by human brain maybe at that point
you would want to upgrade it a little bit like go a lot some more neurons or whatever so you could
kind of explore the next level and but like what's the right pace of that like do you want to just
rush to the end and become like like a planetary sized superintelligence immediately or would you
like want to you know take the scenic routes and then maybe spend a few hundred years first
being a biological humans and doing whatever can be done as a human and then slowly increment
so these are some of the the questions that come up and so many more there's a lot of things to
think about hopefully we will actually yeah secure the future in us that well yeah again
incrementally I like Kevin Kelly's approach protopia not utopia or dystopia just one small make
life tiny bit better tomorrow than it is today don't aim for utopia just as a tool just make life
a little bit better don't worry about 500 years from now just tomorrow I think that allows one to
avoid a bunch of mischief that is is performed in the name of grand visions but I do think also
sometimes it's useful to lift your gaze up and and look at the horizon or like reflect on where
you're going like there's the next step on the next step but ultimately so we have like our human
civilization all this effort spent on science and technology and economic growth and everything but
very little effort spent on thinking what what where do we end up if this continues sure but
just let's talk about creativity for a moment your book I really enjoyed because it's completely
different than any most nonfiction science books that I read you know you have this kind of dialogue
this conversation you're in a classroom your lecturing you have handouts students are asking
questions that was pretty creative and new if you had asked chat gpt to write your next book I don't
think it would have come up with that you see where I'm going with this what about music what's the
origin of rock and roll well folk music and jazz all right so in a century from now what will be
the next big you know musical trend I don't know I don't think it's possible to know and I don't see
how an AI would anticipate the next creative movement not just in the arts and poetry or whatever
but in anything you know there's you know there's only so many combinations I guess maybe it could
grind through all the possible combinations for music that's going to be enjoyed by people that
seems to me though next to impossible to program now I might not be able to predict it
but there's a lot of things that you couldn't predict even with your super intelligence like
that like even just like the weather like a year into the future whether on a particular
minute it will be raining on this like chaotic systems right and with something like creativity
over the time scale of a century it the actual answer to that question will depend on what a
lot of smart people are doing in the course of that century maybe other AIs even smarter than
the one that you would think would be making the prediction and it itself will interact with so this
but even if it's not predictable what creative results will it you know precisely be attained
a century hence it might still be possible that the actual creative work is more efficiently
done by AIs as this century unfolds they might just be making the best paintings and writing
the most beautiful poems and writing the most compelling movies etc it's certainly not the
case right now I mean current large language models are have a sort of
cliche is maybe too strong a word but there is a sort of mid-brow quality to their output that
there's like the it's good but it's not great it's kind of the typical thing that some person would
say in a situation that they can produce more of that but great stuff comes from kind of not just
following along with the patterns that are already out there but sort of looking at reality afresh
with new eyes whether the reality is inside yourself or outside of yourself and really
letting it speak to you and then they sort of speak the words that come from your perception
of this piece of reality that is that you're focusing on and so it's like a different source
kind of of information but I have no doubt that that AIs will become increasingly creative I think
it's not a binary thing I think we already see little glimpses of lower level creativity and
I think the next generation will have more and then more and more beyond that for example a few
years ago DeepMind System Alpha go had this move what was it 32 or something 37 I forget but it was
like in the match that Alpha Fall was playing against least at all the human go champion and
there was a particular move that experts in go thought was immensely original and creative it
was something no human would ever have played that all the masters would advise like students that
that was an error but then it still turns out like if you think a little bit more you just
realize how right it was and it set everything up to win the game later so that's within a
sort of somewhat circumscribed domain but certainly like created within that domain and I think
the domains in which you will be able to have these like genuine deep creativity will be expanding
as the capabilities of the AIs increase yeah when I was a professor at Occidental College we had a
music professor there was also a gifted pianist and he would once a year hold these impromptu
concerts where he would in the auditorium that grand piano on stage he sits there and then people
would call out like requests like do Beethoven's X as if you know Elvis did it or you know in the
rock and roll and and people just come up with the craziest and he would do it and it's like god
damn that's great so maybe if you had an AI you could you could find all the different creative
permutations on all the different music that has been done and then test it in the marketplace
well what do people actually like yeah yeah yeah I do yeah so there's a quite right now the question
of quality like the actual output is not great now if we imagine the quality problem being fixed
then there is the question of whether people would still value it less because it was produced by AI
even if if you sort of listen to a blind test right A and B you're not told which one is even if
people prefer the AI output in that context if the quality became like as good or superior
if then we get to this like father question of value whether you still prefer it just because
you know that the human did it there's also the I mean there's like so many branches sticking out
from here but like one possible reason you might have for preferring the human output is if you think
the human but not the AI experienced various things when they wrote it they actually experienced
the joy or the sadness that you know the musical piece expressed but there again I think with
digital minds it might also be possible to create phenomenal experiences in digital substrate
and so AIs also might have had experiences that they could be expressing in their works
it's not clear exactly how where we are on that path towards AI sentence but I think certainly
in principle it is possible I'm a kind of computationalist about phenomenal content
yeah that subjective element of art where the fraudulent copy painting of a classic painting
plummets in value the moment people find out it's fake even if you can't tell the difference with
your own eye yeah yeah yeah so um so if that's the model then you know that might be still demand
for human painters to yeah paint there now sort of relatively back to the economics I mentioned
you know pulling up everybody from the bottom up to some level but you know economists tell us
there's this thing called the hedonic treadmill but there is no there's no bottom level people
always want more and that that's just going to never end you know the McMansions houses are like
two to three times the size they were in the 1950s even for the average worker
and you know that that there's no upper ceiling on how much more stuff people are going to want
how do you think about that yeah I think there are parts of our preference functions that are
um non-satiable collectively because like yeah we have these desires for positional goods
to have more than another like you want your yacht to be the biggest in the world
so you build a 200 you know foot yacht and then some other billionaire bastard builds one that's
like 205 and then so it's impossible for both of these people to have their preferences satisfied
to own themselves exclusively the biggest yacht in the world so that's one example for how
collectively there could be preferences that the humans have that you can't all be satisfied
and there are many other examples where two people want the same piece of land or the same
be the exclusive love interest of the same person or etc etc so now it doesn't enable sort of
unlimited economic growth if you define growth ultimately in preference satisfaction terms
and so like because one person's gain is another's loss in this scenario and it also doesn't necessarily
create an unending reason for human economic labor if there is no way to make more money
than no matter how each of these billionaires wish they could make their yacht a bit bigger than the
others like if they can't actually make more money by working or if the extra money they
could make by working is kind of trivial to the amount of money they are already getting from
their capital gains and that would be no incentive for them to put out effort for that reason
and that's like already true for many billionaires like there's like yeah they could
take a job and make an extra 100,000 a year maybe but if they're already sitting on 20 billion it's
like it's not really making a meaningful difference to their purchasing power yeah but you have people
like Elon Musk and Jeff Bezos you know they didn't they're not just sitting on the beach
you know they're exactly and they can I mean still add a lot of even just economic value
through their work like like obviously Tesla would be worse a lot less if Elon I called it quits yeah
and so even just from a purely economic point of view they still have the ability to contribute
amounts of economic value that are significant even relative to their net worth and
because they have like like Elon has unique skills also I think there are opportunities
sometimes for very wealthy people to sort of combine their human capital with their financial
capital to do things that are hard to do by taking one person with capital and one person
with brains because they're like trust problems and communication problems sometimes they need
to be combined in one person to certain opportunities are more easily realizable
but for many others it's not the case and they're already like in this situation where
it makes no sense to work for money yeah but I guess my point is you know there's stories about
Elon Musk sleeping on the floor in his factories he doesn't have to do any of that but he does it
because that's what gives him value and I think more people would want that than would just want
to sit on the beach yeah I think he's doing it because he wants to achieve various things that
can't be achieved without him doing it yeah now if he could create like I don't know like some sort
of android replica of himself that would do the same thing and achieve the same results for for
Tesla and SpaceX etc and and he could be on the beach I have no idea maybe he would prefer that he
has said that his life is pretty painful often and that um so it might be that he does it because
there are various outcomes he wants as opposed to valuing the activity itself not running around
maybe of course we don't know what's in his head but uh you know I think in general people like
challenges because that's what makes life meaningful and it's essentially an infinite
number of challenges we could always have but I could be wrong okay on the economic model
so people are living longer let's not get crazy let's just say people live 200 years or 300 years
rather than 100 yeah but it's 300 years of research into extending life don't you think
I don't know you know Kurzweil thinks it's coming by 24 what's the the takeoff point in 2045 I think
he said where maybe it's even sooner than that where the amount of extra life you get exceeds
every year of your life and then you have the what does he call it the take take takeoff point
something like that longevity escape velocity yeah I don't you escape velocity that's it yeah I
you know when I hear these things I think back to religion it's like I feel like I'm you know we're
the chosen generation we're the ones that get to live forever I've heard this before when I was
religious right you know maybe you know but I think the problems are much harder than most
longevity researchers think but you know it's possible but okay let me let me
carry out the thought experiment all right so we have eight billion people now it's probably gonna
top off around 2050 and start to decline by 2100 or so and as you know Elon's worried about a
birth birth the richer and more economically stable and more educated people are the fewer
babies they have so how do you square that with people living longer and the population increasing
how do you think about that yeah I think there are various
long-term trends that I think would deserve attention if it were for the fact that I also
believe that we are probably relatively close to this transformative technological overhaul over
the current of the current human condition so that I think sort of the game board will be overturned
for better or worse but within you know likely some years or a few decades and that these like
longer demographic trends won't really have time to play out would be my guess there might be other
demographic trends that then kick in if you do invent this a new world with AIs and digital minds
that can obviously copy themselves instantaneously if you're like software you could make a million
copies of yourself in an afternoon right if you have available hardware so you could have
like different population dynamics that could become problematic but but that
that wouldn't sort of just be an extrapolation of what we're currently
seeing with the human situation also like some I mean I see the the projections and how like
birth rates are going down and if that continues like like but some skepticism about our the
reliability of these long-range forecasts like I mean when I grew up the the big worry about
was about overpopulation and there were these like public intellectuals through the club of
Rome and everything and that was like and they had little mathematical models that show this
now it's going the other way and I mean who knows in 30 years from now if there's no
transition maybe it just turns out that something has changed and it's overpopulation again like or
some other so it's like yeah our ability to make these very long-run range forecasts are
are open to question I think yeah you know Stein's law things that can't go on forever won't
and there's some corollary to it but but they can go on a lot longer than you think
yeah yeah yeah some things have gone on for longer than yeah one would have guess well I guess in the
next you know you want to look at the far horizon do we need to leave the planet become a multi-planetary
species because of either overpopulation or we're going to run out of raw supplies and and uh and
resources and population can ultimately outrun any space settlement program because ultimately
even with mature technology we are limited by the speed of light and so if you imagine a sphere
growing at the speed of light even in all directions right the volume of that grows
polynomially with time and so the resources that we could potentially use for civilization or for
like that cannot most grow uh polynomially whereas population can grow exponentially it can in theory
like you know double every generation or 10 percent and so eventually the exponential will
overtake the polynomial if you have unrestricted population growth and if like we end up in in
a situation where sort of people have more than the replacement rate of children that that would
eventually just overtake so so that the space settlement would at most kind of delay bumping
up against resource limits and ultimately you would have to just figure out some way to maybe
coordinate to to bring only the number of people new people into existence that that could be
supported at a high level of living which might still be a lot of new people into existence but
if you overshoot that then eventually average income would have to drop right now we have more
like kind of increasing returns to population because more people means more ideas and division
of labor which makes so so right now probably per capita income goes up if population increases
but i think at some point that will no longer be the case and the limiting factor of the economy at
technological maturity will eventually become land as it's referred to basically resources
that you can't make more of as opposed to labor or uh technological advances that that will already
have sort of been maximized and so then land only grows polynomially in the limit and that that would
be the sort of maximum rate at which the population could grow in the limit as well and what's your
time horizon there you know thousands of years or tens of thousands of years uh well i think um
there are really sort of two key variables there's a question of how far from now until we get
superintelligence um and then from there on i it might not take that long because once you have
superintelligence that makes super duper intelligence and then like some kind of
substrate optimized for a cognitive performance that can i would imagine relatively quickly
develop all kinds of technological solutions that start to approximate the physical limits
i don't know whether that would take like months or decades but well so Kurzweil is talking about
monetary super brains brains kind of working for like creating a space rocket for like yeah
it occurs i was talking about 2045 in his next his next book the singularity is nearer coming out
in june uh 24 but so about after that anything's game i mean we just probably unpredictable uh what
the time horizon could be yeah i mean i don't even think we can rule out very short timescales like
we don't know dpt 5 or dpt 6 won't be right there i mean we don't know that i will but
here we really have to think probabilistically right and have like a smeared out probability
distribution i think over different or okay you're one of my favorite big minds so let's
keep going on the the long horizon all right let's apply the Copernican principle to our
species we're not special the chances are you know we're in the middle of the bell curve of
civilizations that would have done everything you just described uh surely there's extraterrestrial
intelligences out there that have already done all this uh and built self replicating von Neumann
machines and Dyson spheres and so on so answer me uh Fermi's paradox where is everybody
um most likely just like very far away like outside our light cone um which would uh yeah
explain why we haven't seen them of course if the universe is infinite as it seems to be with
infinitely many planets and stuff then there would be infinitely many of them out there
but the density might be quite low um we don't really have it seems to me and a particular
reason to think that um it would be easy for a like an earth like planets to produce life let
alone intelligent life i mean there might just have been some ridiculously improbable steps
somewhere like to get the first impulse replicators going or maybe to go from prokaryot to eukaryot
or something maybe that just happens in like one planet out of 10 to the power of 40 planets or
something um now then you might think wow wasn't it then like what's the miracle that it happened
here on earth well if there are infinitely many planets out there then even if the chance for
any one of them is ridiculously small it would still happen right with certainty infinitely many
times and then an observation selection effect would explain why we find ourselves on a planet
where this improbable thing did happen like only those planets are observed by people
evolving on them of course the others there is nobody there too so that that seems like pretty
likely um if one wants to think that life is more common then one has to i think um either
postulate some kind of zoo hypothesis like where they are deliberately uh hiding themselves or
uh like my colleague robin hands on has some scenario in which uh uh there might be others and
it is like too long to explain but yeah there's a there are like it's i also think like i don't
know i mean it probably takes us like too far afield from our current conversation but this
whole simulation yeah argument stuff um which which kind of adds another dimension to the whole
where and where do you stand on the simulation hypothesis well i mean i i mean i i believe in
the simulation argument having uh more than 50 percent united that yeah so i i think that sound
and uh now that only shows one of three possibilities is correct one of which is the simulation
hypothesis and so you would then need some additional information or consideration if you
wanted to sort of pick between these three alternatives that the simulation argument
establishes um i would say i mean and i think as to to me when i wrote this paper back in in
like the early 2000s it back i was pretty clear that we were sort of on a path to develop
increase the the technologies that you would need to create these ancestor simulations or
detailed simulations with conscious um like like super advanced virtual reality and digital brains
like now i think it's maybe easier for people to imagine that because just we've seen kind of 20
years 24 years of technological advancement like virtual reality is a lot better now than it was in
2001 and obviously ai is moving ahead so it's like a smaller imaginative leap to think that
at some point in the future some technologically mature civil like really advanced civilization
might have the ability to create simulations that are like perfectly realistic to the people inside
them um and uh yeah i like so so in in some sense the opportunity is to
pop off the train before you reach the conclusion or like diminishing as we sort of pass by the
relations you know i had david chalmers on the on the show he has a you know an entire book on
on the simulation morals in a simulation and ethics and so on very interesting but ultimately he
says right in there this is not a testable hypothesis we there's no way to know if we're
in a simulation or not so then what are we talking about here this is just science fiction or metaphysics
or or what no i mean i think it um there are certainly possible observations um uh that
are such that if we made them they would give us strong evidence either for or against
the simulation hypothesis it's like to take the most obvious example like if a window
suddenly popped up in your visual field saying you're in the simulation click here for more
information and a little buffering and find the terms and services like that would pretty
like prove it to you right like if that yeah um conversely the absence of such a window popping
up is by the principle of conservation of evidence must be some really evidence against the simulation
hypothesis like weak evidence because but but still some evidence and but more um i guess um
relevantly i think if you consider the simulation argument which has the structure that at least
one of three propositions has to be true one of which is the simulation hypothesis anything that
gives you evidence for or against the other two indirectly then uh affects the probability you
should assign to the simulation hypothesis so um for example one of the alternatives is that
almost all civilizations at our current stage of technological development go extinct before they
reach technological maturity so that's something you could believe instead of the simulation
hypothesis but there has to be a very strong convergence it can't just be like 80 percent
of them it has to be like basically all um and of course if we make it through to
technological maturity that would be very strong evidence against this idea that basically all of
them fail to get to the technological maturity so therefore anything any evidence we get for
against the idea that we will reach technological maturity would bear indirectly through the
simulation argument on the probability of the simulation hypothesis so the closer we get to
technological maturity ourselves the less likely that alternative is and hence the more likely
the simulation hypothesis is and you could imagine um the extreme version of this which
is if we ourselves develop all the technologies needed to create ancestor simulations and we are
just about to switch them on and we want to switch them on and we're sort of about to reach to press
the button that would pretty much conclusively rule out the two alternatives to the simulation
hypothesis it would show that like it's not the case that nobody reaches this level of
technological maturity it's not the case that almost nobody of those who do reach that
remain interested in creating ancestor simulations and so in that situation where we turn on our own
simulations we would have to conclude that we are almost certainly ourselves in one
so those would be some ways of getting very strong evidence and then but I think anything
that then indirectly has some probabilistic bearing on these alternatives also sort of indirectly
has some evidential connection to the simulation hypothesis so I think there's a lot of ways to
test it but these tests are probabilistically yeah did you mention what if we're first somebody
has to be first yeah well that that's uh what what it's for did you say are or if it's first no
we're first yeah we're there some civilization has to be first maybe it's us that must be
very unlikely if that were to ultimately be say a million simulations of experiences just
like the ones you're having and you can't from the inside tell the difference whether you're like
number 537,648 or whether you're like number one but in that condition where you have some evidence
and you could either be one of the vast majority of people with your experiences that are simulated
or like this very exceptional one that's not simulated and there's doesn't feel any difference
from the inside I think they're a kind of principle of indifference should make you
assign a proportionately low credence to you being the first one like the exceptional one
yeah but again somebody has to be first because you're saying in the age of the universe if most
people think they are then almost all of them will be wrong yes it looks like a rational betting
odds in that scenario like we would be to assign a very small probability to that and there's more
arguments I think supporting that it like in fact my my doctoral dissertation was like developing a
theory of observation selection effects and I think there are various areas in in physics and
cosmology and to some extent in evolutionary biology where you have to reason along roughly
those lines to be able to get sensible results when you try to connect current cosmological models
with the predictions that intuitively confirm or disconfirm them some sort of roughly speaking
assumption that you're like and you should think of yourself as you were a randomly selected observer
well as there's a lot of complications around that but that's like some something in that
general direction seems to the let me ask you a technical question here on a simulation like in
Star Trek's holodeck you know Worf goes in there and he has a fight with some other Klingon and he
gets knocked down how does a virtual reality interact with a physical body to say maybe you
want to have a boxing match with Muhammad Ali in this virtual reality how does he how does the
virtual reality actually knock me down well I mean so in the simulation argument I think
the the most well so there your mind is itself implemented digitally like so there is no
me to knock down it's it's all digital yeah there is a you but the you is like it digitally
instantiated and you have like an avatar yeah that your digital mind is connected to like a digital
avatar and the same sensory afference that you like currently are going from your sensory nerves
yeah okay yeah like like that it's like equivalent nerves are going into this digital brain with
the same information yeah using the same subject now the simulation would at some point have to
run on actual hardware right right yeah how can you possibly have enough computing power
to replicate everyone who ever lived and not just their physical bodies and or I guess their minds
I mean there's this mind uploading business about copying the connectome that's not enough
it's not enough just to have the synapses copied you'd have to copy every single molecule
and every one of the synapses in the gas yeah I don't think so I think that a sufficient level
of granularity of a simulation to produce conscious experience would be like the synaptic level
you know possibly you could simplify it even more you mean to get a general memory because
memories are stored I mean I'm told by memory neuroscientists that you need it's not just the
connectome it's not just those synaptic you know sort of wired in things you need all the molecules
and all that stuff is part of memory yeah well some aspects of that I mean and clearly there are like
neurotransmitters that are like swimming around in big pools but I think the bulk of the information
processing is probably done at the level of action potentials and synaptic connections I mean that's
what we see with current AI systems large language models are these neural networks are artificial
neural networks are essentially simplified neurons with simplified synapses and they do seem to
perform I mean insofar as AI has advanced like the same kinds of tasks that the human brain like in
terms of say visual perception which is like a relatively well developed area of AI like with
a comparable number of neurons that visual cortex as you can perform comparable level of
discrimination and object recognition etc I think and indeed current AI started to have fewer
parameters than the human brain has and but they are also like a little bit less smart than the
human brain but it roughly kind of strikingly seems to match the performance that you would expect
by matching it to biology it might be that the biological neuron certainly is more complex than
one of these artificial neurons so maybe you get 10 times more performance per biological neuron than
you get from one of these simplified representations but I think that would be my guess now you could
have enough computing power to go down a little bit below the level of synapse if somehow you needed
it not not all the way down to elementary particles then yes it would become computationally
intractable if you had to simulate the whole Schrodinger equation of a human brain in order
okay I'll grant you that because human memory is not all that granular anyway it's pretty fuzzy
but is that you okay so here's my thought experiment we slide you Nick Bostrom into a
functional MRI we scan your connectome we upload the digital file into the cloud and I have it here
on my phone and I go Nick you're up here now and you're sitting there going no I'm not I'm right here
and so are you saying that there's we have to redefine the self there's just multiple
Nick Bostroms and each of them thinks that they're the real one well I think there are that like
two notions of the concept like two notions of self that that can come up that that kind
of always coincide in our normal human experience but that would come apart in some of these
technological scenarios and philosophers have realized this like Derek Parfit who was my colleague
at Oxford was famous for exploring the difference between preservation of personal identity and
survival in these thought experiments where you have like duplication we imagine a person being
duplicated or teleported and the original survives that Parfit argued that in those cases
the original person has survived but that personal identity is separate from this because
the original person can't be identical to any one of them because they are not identical to each
other and the claim would be equal and so so you would have like even if the personal identity
is not preserved you could have survival and anyway that that gets into these kind of philosophical
issues but I think certainly in some scenarios I think your personal identity would be preserved
in an uploading scenario in like if that was only one successor it would be you I think if there were
multiple copies made simultaneously like equally branching out from the root node I think it would
be natural to say that you survived but I'm not sure what to say about your personal identity in
that case I think just our concept of identity was like not really developed to deal with these
cases so it's a little bit sort of inconsistent when applied in these extreme or like exotic situations
yeah yeah I wrote about this in heavens on earth I was looking at both scientific and religious
versions of the afterlife and there's this idea that there's the mem self and the point of view
self POV self so the memory self this is the connect them just copy your your your memory
all your memories and let's say we can do it but that's just a snapshot I mean if you did it when
I was 30 and now I'm about to turn 70 this year well if I if you did it when I was 30 where is
all the 40 years plus memories that they're not part of that self that's that's somebody else
that's not me me is my point of view looking through my eyes from moment to moment to moment
for all 69 and a half years that I am now that's me and there is no fixed there's no fixed point
where you go that's you right there at 40 or 50 or whatever I don't see how that could ever be
replicated because there's no snapshot there's no thing called the self in a fixed sense yeah I think
there are several different notions and like you could ask somebody like are you the same person now
as you were when you were five of course not yeah and people are confused because like in in one sense
clearly not in another sense clearly yes I'm still in classroom and like there's a continuous path
but I think it's just that instead of having one concept of self we have several different ones
that normally in everyday use kind of coincide and tracks the same thing but in these scenarios
they come apart and so we need to sort of start to differentiate different yes you would be the
same person in sense one of being the same person but like a different person in the sense two of
being and and I think more generally in this kind of world where we have like digital minds I think
there are a lot of new concepts that we would need to develop to sort of describe the ways that
different minds could be related like so humans are kind of discrete things like here's one human
here's another human like with digital minds you could imagine them being kind of partially overlapping
or like briefly diverging and then converging you could imagine all kinds of ways in which
digital minds could vary that they're not possible for human minds to marry you can pause them speed
them up slow them down that that might be like a bunch of different slightly separate minds that
sort of have some shared convergence point where they send information and it's not clear whether
they are all one mind or several minds or like so there's like I think we will we don't yet have
all the relevant concept for making sense of that kind of post-human reality but you know
hopefully we'll have an opportunity to develop some of those as we are are you familiar with
are you familiar with Frank Tipler's book the physics of immortality
yeah but it was a very long time since I yeah that was 96 so here's his calculation that so he's
projecting an omega point computer in the far future that he calls god essentially that will
contain 10 to the power of 10 to the power of 123 bits that's a one followed by 10 to the
123 zero powerful enough he says to resurrect everyone who ever lived that may be but it's a
staggeringly large number but is even an omega point computer powerful enough to reconstruct
all of the historical contingencies all parts of life you know every interaction I ever had
with everybody else including like you right now this particular moment instead of yesterday or
whatever I mean how would how would any civilization create a computer powerful enough to do that and
Frank says not only that you'd have to you'd have to resurrect everyone who ever could have lived
because you you don't know who you just it's just your point of view so there's a lot of
there's a lot of those people that's a that's a big cohort yeah yeah and if there was a computer
powerful enough to do that wouldn't it have to consume so much energy we could detect like a
techno signature detects something like a dys sphere that has to capture all the energy of a
sensor runs at the computer yeah well um so with tipper like so one problem with his theories I
think at the time he thought it and it was like an open possibility and cosmology that we would
have like a big crunch that the universe would collapse back onto itself into a singularity
that's uh that's how his speculation was that in the final moments of that collapse you could get
this like super amount of computation done somehow now it looks instead like we are sort of
gliding apart with a positive cosmological constant and it like it looks like it's sort of a big
whimper rather than a big yeah reverse big bang right so so that that's one like now I mean in
in terms of reconstructing people later in simulation if you haven't say chronically preserved
their brains or something I think I mean certainly like creating recreating everybody who could have
lived like that that's like a kind of a super astronomical number it depends a lot on how
finally you individuate a person from like another very similar person like at what point
are you close enough to basically say yeah that's kind of you know that's Michael Schirmer
even if like your your replication like I like it got a few memories slightly rough and like
you know there's like a few details that but it's like captures the essence of him close enough
that we can say that it so that's like more like a philosophical parameter you would have to put in
it's like an open question to what extent if you might have sort of arbitrarily powerful super
intelligence how close could it get to recreating a human mind assuming they are like dead and
decayed by the time just from behavioral traces like like their writings or photos that their
friends took you know on the holiday or like whatever other like information traces like
if you if you were like a super intelligence and you started all of this material and compared it
with other humans information traces and maybe some brains that you have access to and you sort of
made the best possible inference taking all of this information into account and you try to create
like something that was as good an approximation as you thought like how close would that
approximation be to the real Michael Sherman right it's an open question which is quite
hard to really get a good grip on and certainly I think it would have to be close enough that
like your friends couldn't tell a difference like if your friends still survive at this point for
example or your your kids or whatever like if they are my wife doesn't know sorry if my wife doesn't
know she can't tell yeah it's not like if some replica was created that was like close enough
to you in its qualitative behavior to sort of trick everybody you know including like your
spouse and kids and parents like would that be close enough I mean I mean at some point it
just depends on your value system like how close does it have to be for you to hear enough about
it as if it were a perfect replica it might not be a factual question so much as a value question
like how similar does an entity existing tomorrow have to be to me now for me to care about it
in in this kind of self-interested prudential way that I normally care about the person waking up
in my bed tomorrow that's me like like if I if I if I knew that I was just going to be transformed
overnight into a dragon that remembered nothing of my past life and shared none of my current interest
and but made out of the same atoms like I probably wouldn't really care that much about that dragon
or at least not more than I care about any other living sentient being out there because it would
be not in any meaningful sense me even if it like consisted of my atoms maybe because it ate me during
the night or something like that but oh like Kafka's Metamorphosis where you you know wake up
as a cockroach or whatever but the problem with that is the the human mind is still in the other
being that wouldn't happen right if there was a true transformation so let me just hit a couple
more big topics here before I let you go so the other the other mind's problem the hard problem
of consciousness how do you how would we ever know if an AI was sentient and conscious if we
don't even know that you and I are yeah well yeah and maybe this might have to be our last
topic also that that's a big and practically relevant question now I think we are starting
to have AI systems that are not where it's no longer ridiculous to imagine that there could be some
form of conscious experience happening inside them if we look at the number of parameters
and their behavior it certainly seems to match some non-human animals like in that we think are
probably conscious and so yeah this is a very difficult problem I think there are different
approaches you could take you could try to take some current theories of consciousness of the
shelf and try to just apply them to the case of ai's so we have for example a global workspace
theory is one theory of what creates conscious experience like that have been proposed and
it's the idea that the things we are conscious of are ones that are sort of entered into a
cognitive workspace from which different other more specialized cognitive models can sort of
read and write but the thing in the shared workspace is kind of accessible to all the
different parts of our brain there's like another theory attention schema theory which says conscious
experiences are sort of a rise in our in the modeling of our own attention mechanism so we
have like a little part of our brain that keeps track of what we are likely to be paying attention
to and that and there are like a few of those you could like apply those to to ai's and
if you do that it basically looks like either some ai's are conscious or that it would be
relatively easy to build an ai that is conscious using current technology like if you just put
together all the pieces into one system that just checked all these boxes you could
you could try to I mean you could ask you could ask an ai you could like that's how you would
like with humans like if I want to know what you're feeling or thinking like I
wouldn't I could try to put you in a big fmri scanner or something like that but realistically
I'm much better off just asking you right and you could tell me and so now we have ai's that
can speak it's a very natural thought to say well let's ask them and I think that might become a
useful technique but with some important provisors which is that it wouldn't give us any information
if you deliberately trained the ai to give you a predefined answer so it's very easy right now
when you find you in an ai to sort of train it to like when asked if you're conscious deny it
or or conversely you could train an ai to like affirm it so so if you want to use this to get
any possibly relevant information about the question you would first of all have to refrain
from deliberately biasing the ai during training then there are other things you could do you could
try to detect if there are multiple modes of cognitive operation in an ai system like basically
you could try to find interpretability methods that allow you to differentiate when it is trying
to say true things versus when it is just kind of rehashing things that it remembers or trying to
be entertaining or stuff like that like this goes back to the problem of hallucination that
that you brought up earlier with some current ai systems so there's like preliminary research that
suggests that you could like sometimes track when the ai is lying versus when it is trying to tell
the truth based on different neural activation patterns so you could then see if you combine
that with a self report you could see that when when it says I'm conscious or when it denies it
does that statement occurs in a mode of operation where it looks like it is trying to say true
things is it like the same kind of thinking that it uses to try to factually answer other questions
you could see if it is able to say a lot of other things about its internal states that they're not
really about consciousness but like whether are you currently aware like are you currently paying
attention to x y or z what are your like ask it other things about its capabilities to check
whether it has like the the ability to introspect that that could give you a little hint maybe
and so there are some other ideas like that that they're still very kind of premature but
there's an interest now amongst some of these people and including to some extent
some of the people working in frontier labs to try to figure out because you know at some point
we need to figure this out from a moral point of view like if we are building sentient creatures
like at some point it becomes really important that we treat them right and stuff is the next
rights revolution for AI yeah yeah and I'm getting that like getting a good sort of
happy cooperative relationship going is really important I think because it might well be that
in the future most minds will be digital and so like making sure the future goes well not just
for us but for them too I think is a key design criteria enough anything that would be able to
qualify for the name of deep utopia all right deep utopia there it is get the book read it it's
filled with pretty much every single biggest question you could ask about humanity for our
futures right there thanks nick we'll have you on back after the singularity happens that'll be
your next book yeah it's not something

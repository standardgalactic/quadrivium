AI is not based on imitation of biological intelligence or human intelligence. AI is
itself a crystallization of collective intelligence. It's part of, you know, of the
cooperation of our mind and bodies through the society. So here we have a political dimension.
How do we organize collectively and what is our understanding, you know, an evaluation of a
collective agency? And I think this is an important political point.
Hello, welcome to the Verso YouTube channel. My name is Richard Hames and I'm here with Professor
Matteo Pasquinale to discuss his new book, The Eye of the Master, a social history of artificial
intelligence. Artificial intelligence is a concept that I think is now very widely known. People
refer to it often in kind of daily conversation. They have a notion of the AI as sort of a single
homogeneous unit. This is a very widespread notion, but it's also one that's informed,
particularly by very recent developments. And what we normally think of or more precisely
would think of as generative AI. That is the, you type in a prompt, and then you get something out
of it, you get an image, a text, something like this. And this is almost a kind of magical
view of AI that we have, I think. People have pretty commonly typed something in,
you get a magical box, something amazing comes out. Maybe it's good, maybe it's bad,
we don't really know. Your book situates artificial intelligence in a much, much,
much, much longer history in the whole kind of social process of the construction and design
operation and algorithms, which you take to be a very capacious category. And I want to start with
that notion of the algorithm. So you have this very beautiful diagram. I'm not quite sure exactly
where it's from, the Vedic diagram of the Agnika Yana. Yeah, precisely, which is a sort of social
process. Maybe you can talk us through, like, how you see that as an algorithm, because it's not
the kind of thing we might normally occasionally associate with algorithmic structure.
Yeah, I think one of the purpose of my book was like to provincialize AI to put in a longer
history of cultural technique, and to show that techniques of abstraction belong to many
civilizations across history. So even the idea of the algorithm that today we perceive as a sort of
indeed technological abstraction that belongs to very complex software
is a mathematical technique that I have very long history. And if we look at this
Hindu ritual, the Agnika Yana, that is the oldest ritual of humankind, still practiced today,
is 3,000 years old, recorded by oral sources at the beginning. You can see that already in
ancient India, they were building fire altars, temples, according to very mechanical procedures
they were actually repeated, chanted through mantra that look like algorithm, not just in their
mechanical structure, in their kind of robot-like behavior, the workers were moving bricks
following, you know, this mantra, but those in the fact that were embedding mathematical
technique, like differential equation that maybe we could recognize today in the work of Newton or
Leibniz. So the Agnika Yana is a very fascinating example, a very ancient ritual that already
contained many, many centuries ago, sophisticated technique. And what is interesting of the
Agnika Yana is to be what I call it a social algorithm. So I start from this example, you know,
to provincialize, to recontextualize AI, and to show how this sociality, how collectivity
actually operate through the whole history of mathematics, I believe also, when, you know,
modern mathematics started to emerge in the West, at least in the Middle Ages, and up to the recent
algorithms of computing machine, but also sophisticated techniques like deep learning,
that have been, in my view, emerging gradually, you know, in the last decades, like a sort of,
you know, avatar growing out of social relations that have been condensing, accumulating,
becoming data, big data, you know, like through the ramification of social networks, digital
networks, and so on. And both these computational instantiations of algorithms, and these more
general social processes, or more broader social processes, follow the basic definition of an
algorithm, which is a finite procedure of step by step instructions, which has an input and an
output. And then that input, that process is, to some extent, independent of the data that goes
into it. Is that the broad kind of definition algorithm that you're using?
Yeah, I think the definition you gave, it's culturally correct, of course, computer scientists
that are very technical definition. But what I'm interested is in the genesis, genealogy of
technical artifact. And actually, how do we explain this development? What is our theory of
automation today? And actually, we don't have a robust theory of automation. I'm interested in
the fact that machines, during, for instance, the industrial age, emerge gradually from experiment,
from the original labors. To me, the concept of the algorithm, that of course is more immaterial,
that belong, you know, to the information age, that belong to the age of computers,
is I'll have a very similar in its genesis to the notion of machine. So I see algorithm, a sort of,
I think I'm not the first one to say that, I think even Minsky probably was using this expression,
like abstract machines, right? But to me, as interesting, we have a political economy of the
machine, we have a political understanding of the machine, we have a lot of literature from
the 19th century, we don't have a political reading of the algorithm, or we start only recently to
do that. So my attempt was also to politicize the idea of the algorithm. And actually, your question
goes to the point, because to introduce something complex like AI is good to
clarify some basic notion. And the algorithm is one of this.
The politicization of the algorithm seems to me an important project. And I'm wondering
how it relates to something else we might try. And what you think of that, which is the naturalization
of the algorithm. So we can say that we have rivers full of sediments, for example, and the river flows
and then it deposits the sediments in a structured ordering that is according to the weight of
the particles and to solubility and temperature and many, many other things. Why is that not
an algorithm? I feel like there's a skepticism. A skepticism is emerging.
Algorithms do not belong to the natural order, they belong to the social order. And one thing
that I do in my attempt to sketch a political economy of the algorithm as much as a political
economy of the machine is the fact that I respond to an economic logic. And actually,
what is at the core of the idea of machine itself and the algorithm to is the fact that
I have to follow an economic logic that you know, save resources of time, space. So there is an
intrinsic economic logic that sometimes is not clarified in definition given by computer scientists.
Computer scientists, they are aware of this because they have to save resources. But it's
clear that the algorithm has to do something very efficiently consuming less resources than
other solutions. And so this is very important. This is of course tied up the algorithm to
the complex dynamics of our economy, of our society. Rivers don't have this problem. So
in that sense, right? So the difference between natural order and social order.
Rivers don't have that problem. That's maybe that example. But I'm not going to label this
point. But I think maybe there's something about, for example, the way that body plans
work in organisms as they're gestating in the womb, right? The way in which they structure
the different components and organization of the organism as it's developing in a way that
responds to restrictions, the way it responds to sort of overall distribution of resources
in the, you know, and so on. Well, let's not label the point there.
But I think your idea, it's very close to what was also the the program of cybernetics.
Cybernetics had wrongly, in my opinion, the idea that machines and organs were organized in the
same way. And this is part of the fetish we developed towards cybernetics, sort of vitalism
that we like in cybernetics, but I think it's bringing us in the wrong direction.
So there is, in my book, there is a critique of this sort of cybernetic vitalism, and how
all the discussion they had about the fact that organisms were like organized machine, machine,
we could design machine like organism, but actually in ideological, you know, cover up
for imitation or other form of self-organization that happened in the society at the time. So
indeed, there is a chapter in my book that you know about this critique of the idea of
self-organization that we cannot transplant, you know, from the natural order to the technical
order and back to the social order. But actually, we have to analyze that where it came from first.
One place we might see that fetish operating most strongly today is with the collection
of algorithms that come out of the deep learning revolution, and in particular things like large
language models. So people probably be familiar with is chat GPT. This is the kind of prompt
architecture I was talking about before. You write in a prompt, it gives you back some sort of like
what seems to be a person speaking to you, right? It's like a pretty convincing illusion by this
point. We've produced a sort of a hard beginning in some ways to the notion of algorithm that it
responds to the social order, or it's a logic of the social order. Is there something that we can
point to specifically in machine learning or deep learning, more particularly,
that allows us to understand the way in which algorithmics is entering a new era of its long
history? I think the algorithm that we try to define as a basic unit today, of course,
explode in terms of complexity scale. So when we say that chat GPT has, I don't know how many
parameters today, but trillions, right? What does it mean? We have what is called the parameterized
machine that adjusts the parameters to a large data set. This is an algorithm, but clearly also
from this description is an adaptive algorithm, right? And this adaptive algorithm that I explain
more in details how it was originated in the 1950s, the structure of artificial neural networks,
is an adaptive algorithm that showed to be very efficient in modeling any form of human
culture, right? So from visual data set, images, pictures at the beginning, now even text, what
are called multimodal data set. What is astonishing is that collective intelligence, human intelligence,
culture, cultural heritage is modeled by this system. We discover recently, we knew because
experiment existed since the case, but we discovered recently that this algorithms,
this adaptive algorithm are very efficient in modeling language and generating also very
realistic language, sentence, text. And this is very interesting because I think will affect maybe
also the way linguistics is done today. So there is very interesting phenomena related to the
recent development of deep learning. Is there a particular like, so it's an algorithm that is
adaptive. Let's just dig into that notion of adaptation. Like what does it mean by adaptive
in algorithmic terms? So it's that the parameters of the algorithm are in some ways determined by
the input. So the inputs in the algorithm are interrelated in this sort of feedback loop.
I think if you want to go into technical details, what is the core of artificial intelligence?
Is a specific technique that is not very often a name term. And it's algorithmic modeling is
something was invented in 1950 was the first artificial neural network. And I think it's
an artifact that needs much more research and theorization. Going back to the example of language,
what is very interesting, right, is the fact that at the beginning of information technology,
you have a formalization of language, right? So an information machine is basically a machine
operating through a very simplified language. There's a binary language that was the idea that
Shannon somehow, you know, inherited also from the telegraph that is operating with the binary
language. You have this extreme formalization that, you know, somehow start the computer revolution
as a form of, you know, language machine, we can call like that information machine or language
machine. At the very end, I'm not surprised that then through a few steps of this technological
development, you have a machine very good at modeling languages. Once, you know, we have reduced
languages to some very basic units, so very binary binary opposition simply, and then how we
basically projected language through this complex space of deep learning. So we have a very
capacious, sorry, you can have a very capacious view of what an algorithm is as a sort of social
form that is bearing out the type of subtitle of your book, social history of artificial intelligence.
And in particular, you say that the source of many algorithmic sort of properties or designs or the
kind of the thing that algorithms are sort of parasitic on in their computational form
is the social division of labor. This is a very kind of crucial idea. This is in some ways the
main thesis as far as I read it off of the book. And I wonder if there's a kind of way of maybe
just expanding on this. I'm going to kind of suggest and then you can tell me why it's wrong,
which is that in Marxism, classically, we have a split between the relations of production
and the means of production. And I wonder if there's a sort of a way in which the
structure of the relations of production in producing a particular commodity and sort of
laying out the design of a factory, let's say, or laying out the overall design for labor in a
society, that whole schema is then abstracted and then placed or sort of inscribed inside part of the
means of production. So we have this kind of translation from the relations of production
into the means of production. That's one way I can think that we might sort of
say, spoke about this in conventional Marxist terms, but what am I getting wrong here? What's the
what's the argument? No, I don't think your argument is wrong because it's also the thesis
of my book. As I said, we have a political economy of the machine. We don't have a
political economy of the algorithm. We describe very well, you know, the rise of industrial
machinery according to what was the logic of the division of labor and the economy of the industrial
age. I think the same model fits perfectly the rise of artificial intelligence in the 20th,
21st century. So I think we have to go back to this almost for say forgotten theory of automation.
In the book, I call it a label theory of automation that actually was an idea by
Adam Smith and Babbage before being an idea actually used by Marx and also Hegel himself
when Hegel, you know, had this intuition or this kind of the expression concept of abstract labor.
I think we should go back to that idea that machine emerged from the imitation of the division of
labor. Plus, of course, as I said before, in the case of the algorithm, you have an economic
rationale. So machine, they have to save time and space. And it's not the case that, for instance,
the chapter on machinery in capital volume one is within the part on the production of relative
surplus value. And I think this labor form and value form have to be also recognized within
what are the dynamics of algorithms today. And I think we can bring many examples, you know,
from platform capitalism to artificial intelligence to show how indeed this is,
I mean, at least for me, from my perspective, you know, it's the economy of computation today
and of AI. And I think what is very important is like to demystify AI and to show how it's not
an invention of mathematicians, engineer, you know, geniuses. It's not something coming from the study
of biological intelligence, but it's clearly, you know, something that has emerged gradually
from a sort of crystallization of social relation and then following automation. In the 19th century,
it was used to say, first, organize, then mechanize. That was the slogan, right? Today,
I think we can use the same slogan, we can say maybe first formalize, then automate through AI.
You mentioned Babbage there. And the other crucial thing that Babbage invents is the idea that the
automation of labor is also it's measuring or also it's like it's calculation. I want to kind
of, if you could talk us through why you see that as so important to the machine theory of labor,
the labor theory of machines. Machine theory of labor, very bad theory. No, no, no, no.
Labor theory of machine, much better. The labor theory of the machine. Again, yeah,
okay, one part of the book is kind of archaeology of machine and algorithms, especially the first
part. And it goes back to debate that you had in England here around the machinery question,
actually, you know, the machinery question was a large debate that happened in England before
Marx, you know, what is a machine, what is a machine doing to labor, to workers, to society,
and so on. Babbage had a clear intuition that was not just the fact that the machines were
this mechanical monster emerging, imitating the division of labor, but that the division of labor
itself allowed the accurate measurement of cost in production. So there is this dimension
of metrics, this dimension of measurement of human performance, but also of human skill.
So Babbage knew that any division of labor, including, you know, the machines in this apparatus,
were producing also and defining different classes of skill and actually producing polarization
within the skills in the factory and so on. I think the same happened with, you know,
information technology, with the algorithm of AI. And I think we have to keep this lesson in mind
when we study algorithms today. Algorithms are the, you know, forms of labor automation,
can bring many examples, but at the same time, they become implicitly instrument or implicit
instrument of measurement, implicit metrics of human capacity. The case of artificial
intelligence for me is striking because clearly AI today is implicitly a measure of human skill
and human intelligence. AI can be taken, you know, in a very romantic way to show what people can
and what people cannot do, you know, that can be used also to, even to threat workers to show what
a machine can do. So it enters immediately. Now we can have, any of us can have this experience.
But then in the book I show the algorithm themselves of AI, of deep learning, emerging
into 50s from psychometric. So from the discipline of psychology statistics that actually introduced
the IQ test, the test of intelligence. Test of intelligence is like this brutal,
yes, this brutal procedure through which you determine the intelligence of a person according
to numerical result in a cognitive test of different, you know, task capacities. But that was
like the statistical technique that was used by Rosenblatt also to automate image recognition.
And this is for me a way implicitly, the way in which implicitly AI is vaculating, you know,
a matrix of human intelligence, but that is connected to very reductivist social order.
Yeah. So the one hand we have in the design of these systems, we have an implicit parasitism
or implicit withdrawing of a logic from a social division of labor, which instantiates a hierarchy,
a sort of hierarchy of labor, which is what division of labor actually means. And then they
get translated into a sort of means of production into a sort of a machinery that itself then reinforces
later on that self same hierarchy, which it's, it's based on. I wonder if you could talk to us
about that reinforcement of the hierarchy, you'll give us some particular example of the way in
which you think a particular say machining system might enforce a discrimination or forms of
hierarchization in that labor process. Yeah, in the book as I use the expression crystallization
of social relations, that becomes of course a crystallization of hierarchies. It's clear that
any machine, mechanical machine, information machine, and today I produce a polarization of
skills, you know, back in the industrial age, you had skilled and skilled labor, you had the role
of, you know, workers were just supervising the machine, what Marx called machine and
arbiter, just, you know, people in front of the machine regulating the machine not doing much.
With information technology, we have the same, we have this polarization and bifurcation. I have
a little chapter about how indeed also the French philosopher Gilles Bercy-Mondon had the idea that
the industrial machine itself already was born out of this bifurcation of the source of energy
and the source of information that was producing, you know, two different also classes of work of
labor of performance to be done within the fact. I think the same is happening probably with
artificial intelligence, but in a very, very complex way. It's clearly that we see how,
I think, Facebook and Google themselves in the last year have been, you know, firing out so many
people, especially coders, because they are aware that some part of their work can be
automated by these large language models. So these large language models are entering the labor market
and producing polarization themselves and producing and pushing indeed, you know,
classes of workers in opposite direction. Those can be easily automated and those
cannot be easily automated. So I think it's a very complex phenomenon that starting, you know,
from manual labor going through, you know, different components of the labor market
produce different ways of bifurcating skill. The impact, however, of these large language
models for me is interesting because it's not about simply about replacing workers in their whole
capacity, but it's more like replacing micro tasks in our daily activity as workers or in
the office in the sense that each of us has used, you know, this system to translate small
tasks. Right. So it's not there, for instance, we take the work of the translator is not the
replacement of the translator in its hair, his entirety, but like micro tasks, the translator,
maybe they have to perform in their daily work. And this will happen to many categories of work,
you know, think about graphic designer, people working for, you know, advertisement agency.
When these tools are known, any boss will know that can be used, you know, also to
extract demand more performativity. So what I perceive, you know, in the effect of AI on the
labor market is this, not just the replacement of the whole task, but, you know, the replacement of
micro tasks in our daily routine. And this will have it, of course, in effect, because through
the system, we will be forced to work more, probably, not less. And so there's this paradox
in AI research called the Morović paradox. I think I'm pronouncing that. Which one?
Morović? Morović? Who knows? Morović, where tasks that are seen trivial to humans. So for
example, moving around, balancing, walking around, picking things up, putting them down,
those kind of tasks that seem trivial, turn out to be incredibly difficult from a computational
perspective. And then tasks that in some sense seem easier. So it seemed harder from a human
perspective in the sort of conventional social hierarchy, playing chess, playing go now writing
letters, are actually computationally much sort of simpler. And that's generally used to refer to
a sort of strange paradox, which is why, for example, we might say it's not so unusual that
coders are being made redundant. We think of coding as a sort of a high prestige, high value,
high difficulty task. And yet, lots of them are being very redundant by chat dbt4. But in some
ways, if we look back at the longer history, we might say that it's not that those kind of manual
tasks are as such harder to automate, but that automation has already proceeded so far through
the history of other technologies, not AI, but other technologies, we've reached a state where
they actually now have reached some sort of plateau. And it's difficult to do self driving
cars, for instance. I wonder what you think about this. Clearly, yeah, we see the we perceive the
effect of AI automation specifically, you know, in the sector of white colors. But I would like to
go back to the, to the first paradox, actually, as I mentioned, the very first page of the book,
that today, strangely, you know, we discover, you know, manual workers, you know, like track
drivers or car drivers, you know, as sort of task in which a high degree of intelligence is embedded
because we indeed we try to automate, you know, the activity of a of a track driver, for instance,
through deep learning. So it's very interesting, or it's a kind of bitter, you know, political
insight that we recognize, you know, the import, the cognitive import of manual labor today,
because AI is trying to automate these activities. So I think this is the another paradox, maybe we
should rediscover for ourselves, that indeed, this system are very efficient, we call them
multimodal, they're able to model, not just indeed, very efficient when they have, of course, the
model language, when we have to operate on textual dataset, but also very efficient when they have
to model 3D signals, you know, images, video that could represent the scene of a street in
which a car has to go through, you know, there are big limitations. But it's interesting how,
indeed, we are rediscovering through AI, you know, the fact that all labor has been always a kind of,
you know, cognitive labor, of course, we knew that, but there has been a sort of, you know,
amnesia also of our political economy, labor organization, critical theory about this aspect.
So I'd like to go back to this other paradox that actually we are encountering now.
Yes, rediscovering the intellectual aspects of what seemed like they were the most kind of
manualized forms of labor, for sure. You say at one point, and maybe it's a polemical statement,
I'd like you to sort of unpack it, all labor is logic. This is seems like a kind of a really,
in some ways, it's a it's a it's a provocative statement, right, because reading it from the
perspective of someone who is in some ways being taught that the ways in which labor is organized
or automated is a reduction, or a sort of a an assault on the true creative spirit of humanity
or something like this. How do you see that notion of the logic functioning in that sort of context?
Yeah, I think it's a provocation in both directions, a provocation for our understanding
of a definition of labor today, also within labor organization, critical theory, but those
are provocation clearly not always computer scientists, mathematicians, you know, and cyber
politicians. And on one side is indeed as a critique of the idea that labor is just a manual
activity. On the other is the idea that logic itself has a history, that logic emerge through
a kind of complex history, and you have different logics. And this is also clearly in the case
of artificial intelligence that today, for instance, you know, it's not following, you know,
the deductive logic of the 1950s and the good old fashioned AI, but of course,
move to a completely different paradigm that is the one of statistical learning and inductive
logic. So when I say labor is logic, it's also an attempt to to merge two worlds,
two, two kind of communities on one side, if you want the political economists on the other,
the computer scientists. And it goes back for me also to the history of this artifact that I try,
this abstract artifact that I try to retrace back to a social history. Then if we look
attentively of this kind of social logic that, you know, we embed, we crystallize into machine,
we can bring, we bring many examples, you know, from, you know, from art, or from ancient rituals,
and so on. There's a Robert Hewlett Kento article called the exact sense in which the
culture industry no longer exists. And so he's talking about this phrase that Adorno uses,
culture industry. And he says that the reason that the culture industry no longer properly exists
is because the what Adorno would have seen as the obvious tension and sort of friction
between these two words, culture, on the other hand, and industry, on the other hand, when we
look at the word today, it no longer jumps out at us as a contradiction that there would be
such a thing as a culture industry. And in some ways, there's sort of a similar friction that
you're implying between these two worlds, the world of labor and the world of logic and the
the way in which they grind against each other in this sort of termological provocation, trying
to bring these two things together and show how they they fit, even though both sides would of
course refuse the idea that there is a continuity there. I want to go towards this history of logic
idea, because I feel like there might be a sort of difference between us and we can talk about that.
At one point, you're talking about Rosenblatt and his work in psychometry, and the way in which
that statistical techniques that he uses to do these what are psychometric evaluations, and those
themselves, as you were mentioning earlier, proceed from the history of eugenics and these
kinds of multidimensional analyses that he uses, he then reapplies elsewhere to the question of
perception. And this you notice a particularly significant kind of moment of translation between
these different kinds of social objects. And I guess my intuition as someone who is not particularly
well versed in this history, is that passing through this abstract mathematical object of
multidimensional analysis, in some ways, cleans the history out, right? There's a sense that
the maths has an autonomy that maths is not exactly floating above the world, but that
techniques are discovered and not merely invented, that there's sort of independence to mathematics
that means that when we reapply certain techniques, it's neutral in some sense. Tell me why that's
not the right way of thinking about these mathematics. Okay, now going back to this
not well researched and known moment in the history of AI that actually we should
one day dedicate problem monography. So what I'm intrigued is the fact that when Rosenblatt somehow
produce a convincing solution for the problem of image recognition, that is a kind of intellectual
faculty of the human mind, how you recognize an image, he was using a technique derived from
psychometrics, as we mentioned before, the technique of statistical psychology to measure
the intellectual capacity of a human being, considering the relation between different
results in a cognitive test. So psychometrics, the IQ test, emerge as a way to measure numerically,
statistically, you know, human intelligence in a very reductivist way, as we know very well. I mean,
I don't recommend anyone to do the IQ test to understand what intelligence is. And strangely,
that measure of intelligence, that technique to measure intelligence, it became a medium to
automate an intellectual faculty. I'm interested in this passage that is very complex and difficult
passage also for historical science and technology. But the same thing happened in other historical
moment. And you can also see, for instance, the rise of the steam engine, the way the steam engine
emerged from also an idea to control a measure energy, you know, to measure labor performance.
That was a way in which, you know, workers, human beings, but also animals, like horses,
were measuring their performativity. And then from the measure of energy, at a certain point,
you have a strange phenomenon in which you have a medium that produce energy and automate that
kind of measurement you had before. So I'm very interested in very complex and difficult passages
in the history of technology that also, you know, for me, history of labor automation.
Psychometrics is very fascinating. It's difficult to explain because they're supplying, you know,
statistical tools of multivariable or multidimensional analysis as a history that is like a horrible
history going back to eugenics and racist policies comes out from origin from craniometry,
this pseudoscience that thought indeed to measure intelligence of human beings, according to the
dimension of the skull. This is the image of people going out of calipers, right? So they're
going to measure skull shapes. Yeah, as Steve and Jay Gold has this great book that is the
mismeasure of men that I always suggest for anyone wants to understand how AI operates today. It's
one of, I think, probably the best book to understand the logic of AI as well. And so he had
craniometry, craniometry failed. Psychometrics took basically the role of craniometry, but
maintaining this idea that, you know, human intelligence could be measured that we could define
quantitatively, numerically, also classes of skill of intelligence through society. So I mean,
you know, imposing social and nuclear intelligence. And then a certain moment in the department of
psychology in the 1950s in the US, this technique, psychometrics makes a jump and go into computer
science. And Rosenblatt uses it actually to organize his neural networks. And this is like
a very unclear moment and very complex moment. And we find the same technique today at the core
of deep learning. So even this very large language model like charge EPT, they basically
operate, manipulate, you know, property of this multi dimensional space. What is interesting for
me, of course, mathematics, in my view, it's not completely autonomous in its development.
Psychometrics implied a very reductionist view of human intelligence and society. And I would say
probably the same happens today with large language models in the way they reduce language,
they're very efficient, probably because of this operation, they reduce in a very
efficient way to small unit, and then they can automate it efficiently. I think we will see then
the effect of this reductionism on not on the long run, maybe on the short run,
in the way indeed, I'll have, you know, errors, you know, phenomenon of
a reduction of, you know, everyday language is already happening through the system.
I guess my question is like, what does that politically commit us to? So we notice that
there is a history or when we explore the genealogy of these techniques, we notice that there is a
moment at which this technique is used or even informed in the service of a racist project that
we would absolutely repudiate. And then for like, well, what happens now, you know, what do we do
with the fact that there is a does it commit us to anything politically or ethically? Does it
should it shape the way in which we respond to these tools, as they will be, you know, thrust upon us
or engaged with our lives? Yeah, I think what we should take from AI today is the fact that artificial
intelligence itself is not this fantastic embodiment of intelligence, but we can use it
actually as a great description of what is the state of the art in the collective intelligence.
So all the biases, all the errors, all the problems you see in AI indeed are a fantastic
photography of our, you know, culture today and political constitution. So rather than taking
the system as indeed an embodiment of intelligence, we could should take them as, you know, as a
perfect photography of our mediocrity. And I think in this way, we could develop, you know,
alpha dialectical relation with this system, you know, and I think we should, yeah, consider
in this way as imperfect machines. I think we should indeed provincialize AI, you know, step back
and see it in perspective. Also, the term AI, as I try also together with many other scholars to
demonstrate is a misnomer. We should one day maybe define the system as indeed, you know,
just system for algorithmic modeling that, you know, we should find other expression like maybe
statistical learning, you know, to remove this anthropomorphic, basically simulacrum that we
have attached to AI. And I think this is an important process to do also to avoid, you know, some
expectations and usual question about the systems. You're absolutely right that there is a sort of,
hence the camera taking a picture here, that the snapshot of the internet, for example, as it was
in 2021, I think is the last time that it was updated, maybe more recently, 2023 was the latest
chat GPT for model was was retrained at that point. So you're absolutely right, there's a sort of
snapshot of mediocrity. I guess like, if we're to be suspicious of this very basic unit, which is
the vector, right, the technical object that sits at the very basic kind of bottom of deep learning,
if we do be suspicious of its history, then why is it so sort of in the terms of a kind of famous
paper like so unreasonably effective at modeling so many different domains of life. There's another
thesis that people might be thinking of when they when they when they see the eye of the master,
which is the thesis of the the seeing like a state thesis, James C. Scott's work, which is that
it's actually quite similar in some ways that there's a social organization of labor of structure
in society, such that it can be visualized or such that it can be seen by the state overseen,
things are organized such they can be seen. And if we have this notion that AI forms a sort of a
snapshot of the mediocrity of human culture, is there some sort of similarity you see in that
thesis? Do you see there's a continuity there? Or would you say there's a sort of a difference?
And I mean, one of the major differences might be, for example, that James C. Scott is not as
far as I know, a Marxist, and therefore not really particularly concerned with like labor
process, but concerned with a much more kind of heterogeneous collection of things.
No, I think indeed, I insist in this book, you know, on the primacy of labor organization in the
way we design artifact. So I have a kind of, you know, design theory that brings design itself
and design of machine and also AI to what is the ontology of labor today. But what they try to do is
also, you know, to escape some categories of the political economy of the 19th century and to see
how labor today takes different form through society and how you have social relations themselves,
they're becoming productive. And today clearly, you know, what our smartphone, the algorithm of
platforms of the gig economy and the algorithm of AI are doing mostly, of course, they're not
automating the labor in a factory, but they're like following our movement through the urban
space, for instance. They're like, you know, producing this very complex snapshot of the diagram
of our relation through society. This is clearly also emerging through the way, you know,
monopolistic platform like social media are organized. So I think AI here is part, as we know,
of this long evolution of the information society in the 10th century into the network
society of the 90s. It's part of this evolution of the network society in this, you know, organization
of few data centers around the planet that control all our communication. And the algorithm of machine
learning, then at the end, emerged as a way to compute this surplus of information of this data
center. So you have, you know, the gradual, you know, evolution, what we call today AI was machine
learning and was before data analytics. So I see AI, the eye of the master, as a sort of evolution
of this information infrastructure that is built on the data we produce on social data,
and is responding actually to a problem of elaboration interpretation of those data.
So today we have powerful algorithm because we have to compute massive amount of data,
you know, big data. And big data we should remember doesn't mean just data that are large in size,
but they're like, you know, multiple in that dimension. They describe, you know, different
variables, parameter of your life. And, you know, companies, they didn't know how to interpret this
data. And they develop algorithm and one day they found in machine learning and then in deep
learning and then in this large language model also called foundation model, the most efficient
technique to interpret this data that data, you know, that are, you know, produced by our
collective body by the social organization of labor at large. And what's the political valence of
that? So we have this notion that there is a in the large language model, the whole of human
cognition, up to the state in some sense is captured, the archive is snapshotted. What are
the political valences of that? What should we then do with it? Do we understand it as a collective
product of the whole of humanity? Do we understand it as a particular proprietary knowledge of this
particular company? What are we to do as how we to interpret in the field of political struggle?
Yeah, no, this is an important question. I think today AI represent a mechanization
of collective intelligence, a mechanization of the general intellect that Ricardian socialists
somehow perceive as not mechanized, as not materialized, represent interestingly, you know,
indeed a mechanization of education of cultural heritage. And I think we are not ready to to
understand what's what's happening. What is interesting is also the scale of this monopoly.
Large language models can be computed only by large companies or superpowers that have a lot
of computing power facilities, data center to collect all this data, the powerful clouds where
to amass all this data. So there is clearly a process of monopolization. And then this
indeed monopoly on a model of our culture that is often your control in a privatistically in a
private way. And this is something that happens for the first time in the history of humanity.
And what does it mean to fight politically this form of monopoly? I think this is part of the
struggle around the new form of platform capitalism of, you know, technological capital entities
organizing through very few monopolies. There's a sense in which this these models because they
seem to encapsulate so much, give us a sense of what the human and a very general sense is.
So we have this notion that by interacting with AI, we are interacting with GPT-4, let's be more
civic by interacting with GPT-4, we're interacting with like the whole wisdom of humanity. I've
heard that described in these terms to me directly. But there's a course of kind of a way in which we
might want to refuse the idea that that is the whole of humanity. You mentioned earlier the
necessity of establishing what you described as a dialectical relationship with a technology. Is
there a kind of notion of the human that you want to counterpose to this overall snapshot? Because
the way in which you describe humanity or the way in which you describe the particular intelligence
of people in the book is as situated as sort of externalist as kind of organized in in in
in relation to a situation. And I'm wondering if there's something you want to kind of rescue
from this this snapshot or to dialectically oppose to this snapshot? Yeah, I think as I try to
discuss in the book, the AI is not based on imitation of biological intelligence or human
intelligence as, you know, individual organisms. What is extremely interesting for me that AI is
itself a crystallization of collective intelligence. So what is collective intelligence?
Collective intelligence is not, you know, the intelligence we possess individually
is part of, you know, of the cooperation of our mind and bodies through the society.
So here we have a political dimension. There's been always part of any
political project. How do we organize collectively and what is our probably understanding,
you know, and an evaluation of a collective agency? And here, for me, looking at the,
you know, history of political movement, we see a process in which collective agency,
collective intelligence is, you know, translated into machinery and especially into, you know,
monopoly control. And I think this is an important political point and also an important point,
you know, for culture, education, pedagogy and these things.
There's a chapter in the book on Hayek, Frederick Hayek, a famous neoliberal
economist and his relationship to connectionism. There's this extremely interesting 1952 book
called The Sensory Order, which as you say, proceeds the Dartmouth conference by four years,
where normally, although as you point out, redundantly, lots of the initial ideas of AI
were formed and sort of sketched out and so on.
Some people nowadays on the left regard the arrival of AI as a sort of a way of resolving
some of the main sort of problems, right, the capacity of the single object to
speak in the interests of everyone as a kind of a socialist technological utopianism
that is being developed around AI. And I wonder if there's a kind of a,
your framing of Hayek as sort of being somewhat the seat of this suggests either this project of
trying to understand AI as a kind of a tool for planning, a kind of master tool for socialist
planning is doomed, or whether or not there's a kind of a perhaps a strange dialectical transformation
in which Hayek by in some ways lighting the spark of connectionism that leads to AI,
and then AI leads to planning or the capacity of planning to happen. There's a sort of a strange
grave diggers of the bourgeoisie transformation happening here from
1952 to, let's say, 1929 or something. So some, some, soon in the future.
Yeah, Hayek's, Hayek was dangerous for being probably a smart mind, you know, that was very
well literate about the scientific research of his agent was still in a lot of idea, if you think
also the way he was one of the first, you know, to deploy the idea of pattern that today is so
trend, you know, pattern recognition, you know, pattern generation, but Hayek was one of the first.
He was very, he was very smart, you know, stealing this idea. And his epistemology is interesting
because he's an epistemology based on spontaneous planning or spontaneous modeling, and was much
more advanced than indeed of this postulate of the Dartmouth workshop on AI from 1956 that we
considered the beginning of AI. What they discussed at Dartmouth workshop was, for me, a very simplified
and reductivist and almost useless view of what human intelligence and machine intelligence
would be. That's this famous moment when some, I think, just says that the entire problem of machine
perception could be solved in a summer by a by a bright graduate student. And of course, here we
are, have many years later still working on this. But no, that book by Hayek, the sensory order,
is one of the first book to, you know, also reach and the discussion on what would be a form of
algorithmic modeling, or does it mean to model the world rather to representing, right, knowing
how rather than knowing that, it's part of that debate. And for us, it should be taken seriously
because indeed Hayek had an idea that, you know, planning, you know, it's something, you know,
you should develop probably not in a very rigid way, but in a very spontaneous way. So I think
the current form of AI, machine learning, deep learning, belongs to this old tradition of,
we can call it spontaneous organization, spontaneous planning, and, you know, technique
of self-organization that actually were concerns also of the U.S. Army research in the 1940s,
1950s, 1960s. And in the book, I tried to rediscover this conference on self-organization
that were related to neural network research that were like indeed, you know, engaging with a much
more sophisticated paradigm than other discussion organizations. And it was the same milieu from
which the idea of the Internet, the ARPANET, emerged, you know, a network that could self-organize itself
in case of danger or attack, a network that could self-repair itself. And so I think also the
genealogy of the Internet and the genealogy of artificial neural network had some common moment
in some, you know, research conferences that are not very well illuminated.
This connectionist AI is, in some ways, truncated by Minsky and Puppet,
which leads to what is normally referred to as the AI winter, in which there's a huge dip in the
amount of research that is going into artificial intelligence. And you outlined in the book,
there's sort of a almost quite petty kind of squabble over funding or attempt to withdraw
funding back to MIT where Minsky and Puppet are based. And I want to kind of zoom forward,
taking that sort of notion that these normally autonomous technologies are embedded not just
in the social organization of labor as we've covered, but also in this kind of strange petty
political squabbles between different researchers and bring us forward into a particular reading
of what's happening right now in AI, which is that very rapidly there are incumbent players,
so people things like OpenAI and Microsoft behind them and even Google, are attempting to get
legislation or regulation passed for the field of AI.
Normally about existential risk, normally about the possibility that AI might,
in some way, kill us all. And therefore they argue for regulation on that basis.
There's another reading of that, which is that what is really happening here is that the
incumbent players are trying to solidify their position and lock out other people who might come
up and prevent them from compete with them. What's your read of that situation?
How do you see this kind of political squabble going on at the moment in the field of AI?
Starting from Minsky.
Taking the idea that AI is dominated not only by political concerns, but also by
struggles for supremacy, struggles for dominance in the field itself.
What is interesting of AI as an invention that did not emerge very quickly, it took
like 60 years to consolidate. The logical form was somehow invented by Rosenblatt in 1950,
so it took like 60 years to consolidate. More or less the same time the steam engine
other inventions in the past took more or less the same time. So AI is not that recent that
I was trying to demonstrate in my book. And it's still interestingly enough an experimental
artifact. So we don't have today what is called a theory of statistical learning. We don't know
indeed how this large and what would sometimes operate. We have a trillion of parameters,
we push the button. Sometimes we get a great result, but we don't know exactly sometimes what
is going on inside. What does it mean? It happened also in the past. Machines were invented and then
they were studied according to sometimes their normal or abnormal behavior. So still today AI is
in this experimental phase. Being an experiment of course comes with some risk. What you said
about today, the regulation regarding the impact on AI is correct. These AI monopolies are probably
trying to maintain their own monopoly. This system, a fantastic system that are matching perfectly
monopoly capitalism, that platform that can be operated by large companies with large
infrastructure, large computing power, and probably they like to secure their dominant position.
And what is interesting for me is the fact that information technologies in the last century
have been demonstrated to be a great technology for monopolization. So they produce monopolies
faster than other technology. And AI itself is producing monopoly faster than any other technology.
So there is this drive towards monopoly that is within the history of computation that is very
interesting to me. And that's in some ways where I think for some people the utopian
aspects of it come is the idea that because it's assembling the general intellect,
or it's because it's assembling a picture of the general intellect, that there is a
a possibility of some liberatory potential here. And I wonder what you think about that.
Like do you see firstly on the dystopian side, where do you see the domain of existential risk?
And then secondly, do you see there as being a component over to let's say the left of the
distribution towards a more sort of utopian use of these tools that, as I mentioned earlier,
lead to the sort of the grave digging of capitalism as such? Or is your understanding
of the history as based on the social division of labor, does that preclude that kind of
revolutionary aspect AI? I don't have any prophecy at the moment. What I'm interested in is to follow
the transformation of the labor market in this sense that today I see these algorithms not used
just for the automation of labor. I see them very often for the automation of management,
they're replacing bosses more often than workers, and actually they're multiplying workers.
So to bring a good example of what would be the effect of AI in society, we should look at the
transformation of the labor market and these new figures like the driver, the rider, all the workers
of the gig economy, the care worker controlled by online platform and so on. So this phenomenon
of platformization of labor is really related to the application of this algorithm in the
organization of labor, but in which in the way they replace managers more often than workers,
and they multiply workers actually. And I think AI is somehow doing this, becoming an apparatus,
a centralized apparatus for the organization of labor, in which we are all prostheses,
we are becoming a gig worker of this large AI, this large platform that is basically
operating an automated intelligence, but at the end organizing a different social order
regarding labor. There's a sense in which when we use these tools we sit at our laptops and type
something and a giant cloud computer delivers us an answer, we have a sense that we are using a
prosthesis, but in some other much more profound sense perhaps the prosthesis is us and we have
been used by the social division of labor. Mateo Pascanari, thank you very much. Thanks to you.

Hi there. My name is Alan Lee. I'm a computer scientist and software engineer at the Center
for Behavioral Institutions and the Environment at ASU, and I currently lead cyber infrastructure
development at ComSysNet. I'd like to share some good enough practices that you can use
to improve the reusability and reproducibility of the software you develop to address your
particular research questions. The title of this presentation is a reference to a great
paper by Greg Wilson et al. in the software and data carpentry organizations, first published in
2016 and recently revised here in 2017. I'll include links to relevant articles and web
resources at the end of this presentation. If you have any questions, corrections or comments,
please don't hesitate to reach out to me either via this conference website or email. I'd love to
hear your thoughts. So let's begin. There's been a lot of talk in the hub of recently on
reproducibility and the reproducibility crisis with a particular emphasis on biomedicine,
psychology, and the social sciences. Without really considering the theoretical and methodological
quantities of reproducibility in these disciplines, we can at least make sure that the computational
artifacts that we develop to explore or analyze a given research question are developed, documented,
and archived in ways that help others, including your future self, your colleagues, and graduate
students to more easily evaluate, reuse, and replicate your work. But first we should probably
clarify what we mean exactly when we say reproducibility. Goodman, Finnelli, and Ioanitis did a great
job of this in 2016, discussing aspects of reproducibility and different scientific research
disciplines with a focus on biomedicine, but they distinguished between methods reproducibility,
results reproducibility, and inferential reproducibility. To use a common cooking metaphor,
methods reproducibility is listing all the steps needed to bake a cake, results reproducibility
is like the same cake that you did when I followed those steps, and inferential reproducibility,
they claim as the most important and refers to drawing quantitatively similar conclusions
from an independent replication or reanalysis of the original study. And this presentation will
be primarily focusing on methods reproducibility, but it's important to keep results and inferential
reproducibility in mind as well. Of course, our ultimate goal is reuse, can we build on each other's
work? There have been a number of well-reasoned arguments for making the scientific source code
open and transparent in recent years, so I won't go into these in much detail, but as a computer
scientist and software engineer, I would find it very difficult to properly evaluate any non-trivial
results coming from a set of complex computations without access to the source code. Narrative,
or algorithmic descriptions, UML, data flow diagrams, pseudocode, these all helped greatly
in describing what a piece of code is supposed to do, but the source code, its runtime parameterizations,
initial random seeds, etc., these are all critical for evaluating the software. At CompSysNet,
we're committed to making the source code broadly and usefully accessible and available to others.
We want to move beyond simple static archives or code dumps. We want to release your code base
as open source is a great first step, but we can do a lot better than that. To find out how,
let's begin with a quick overview of the FAIR principles for data management, because after
all code is data that just happened to be interpreted by your computer as executable
instructions. The FAIR principles were developed first in 2014 and eventually were published
in 2016 by a working group out of Force 11, an organization focused on
e-scholarship and dissemination communications. The FAIR guiding principles advocate for digital
artifacts and data to be findable, accessible, interoperable, and reusable. In order to be
findable, your code should have a globally unique and eternally persistent identifier.
In practice, this means a permanent URL where someone can visit and find a landing page
with rich descriptive metadata about your computational model, its uses, and so on.
Furthermore, this URL should identify a specific version or release of your software.
When you cite your own computational model or someone else's computational model in the
publication, you should be citing the exact version of the software used to derive your results,
and that should take you to that snapshot, a snapshot of that code base at that time that
shows exactly the state of the code base at the time that you used it. This snapshot could also
include data analysis scripts used to explore the model outputs or generate visualization or figures,
as well as other plaintext and descriptive metadata.
Accessibility is primarily a cyber infrastructure or platform concern, which is to say, it's our
problem at CompSysNet if you're trying to build a trusted repository for your software. It has more
to do with open protocols to access your metadata and code base that allow for authentication,
authorization, and so on. There is one important point to make, which is that the descriptive
metadata about your computational model should be available even when the underlying data or
code is not available due to licensing restrictions or an embargo. That just means that you should
be aware of what's out there and should be aware that this thing exists and have the options for
contacting authors and getting more information about it. Interoperability in this situation
refers to interoperability of metadata, namely that we use standardized vocabularies for the
descriptive metadata about your computational model. CompSysNet is also collaborating with
CSDMS, the Community Surface Dynamics Modeling System Group at Boulder, as well as other research
groups to establish standardized names for the various inputs, outputs, and process variables
and computational models. CSDMS already has a set of standardized names for their computational
earth systems models and we're actively exploring together ways to connect models as
pluggable components so that an earth systems model can be connected with a behavioral model
and mapping their inputs and outputs onto standardized ontologies of variable types.
This is an active area of research, so if you're interested in this endeavor, please feel free
to contact me directly.
One critical piece of reusability is licensing. A clear usage license is
necessary for anyone to reuse your software. The software carpentry groups recommend permissive
open source licenses like the MIT or Apache license. This facilitates maximum reuse. It imposes
very little restrictions besides attribution. If you have concerns about commercial applications
or contributions back to your code base, you might consider the GPL as well.
Good documentation from multiple vantage points of abstraction is also critical to reuse.
The documentation should include steps on how to run your code,
important inputs, and the internal processes outputs. The ODD protocol is a semi-controversial
standard in the computational modeling community, but it's a great starting point.
You have to also make sure you include your dependencies. What software or system or data
dependencies does your model have? Also, bonus points for test suites that run your and exercise
your computational models and assure and give us confidence that the internal processes within
your model are working correctly. Consistent also offers a peer review process for computational
models. We ask independent reviewers to verify that a computational model submitted to our
model library has well-formatted and commented code is fully documented with the ODD protocol
or an equivalent narrative documentation and working instructions for compiling and executing
the computational model. There's a few more key software engineering practices that can
help us arrive at readable, reusable, and testable code. We can write meaningful variable names and
files as always an important aspect. Keeping your code modular and well-encapsulated by breaking
problems down into smaller pieces and short cogent functions is also extremely useful.
Adding explanatory comments at the start of every module and function in the system that
describe its inputs and expected outputs is also incredibly useful. If you find yourself duplicating
code, try to replace that duplication with a parameterized function. In general, just marking
the entry point into your application and walking through the sample code pass is a very
useful exercise and this is something that the ODD protocol addresses as well.
Version control is another extremely important piece of this puzzle. We'll go into two different
methods for this in the coming slides. And for larger projects, an automated build system
with tests and continuous integration is extremely helpful. A continuous integration
system is one that automatically runs on every change to the code base and tries to compile
and run tests against a clean installation of the code base. It's a great way to keep your
software honest and there's several third-party services that offer this, including TravisCI,
drone.io or Jenkins and we can talk more about this on the conference website if you're interested.
The first approach to version control is manual one. This is one advocated by the
the Good Enough Practices paper by Wilson et al that I referenced earlier. You don't need any
new tools. You just need to be systematic. You can keep your changelodge.txt file dated and organized
and you'll copy your entire project folder on every significant change.
More sophisticated ways to use an actual version control system like Git or Mercurial or SVN.
All of these are great version control systems. If you pick one, you won't go wrong.
This will have a huge payoff down the road as it gives you the confidence to experiment with your
software knowing that you can always revert back to a previous state. It also gives you the ability
to associate meaningful log messages with changes in your code base and to also associate tags and
releases with your code base that help keep track of important milestones. This is critical for
actually citing the specific version of your software. With the model library at CompSysNet,
we also offer the ability to capture snapshots but we want to have tight integration with Git as
well. We'll have more on that later. When you do use a version control system, there are a few
good practices to keep in mind. You should do your best to keep your changes small and isolated.
If you modify every single file in your code base and push that as a commit, it's harder to see
what's going on. Keeping your changes small and isolated, committing early and committing often
helps you be exercised disciplined version control. That way, you can easily see the changes that
were made to the code and tie that in with semantic and descriptive log messages that
describe and document the intent of those changes.
It can also be very helpful to adopt a standardized directory layout. Here's a simple example where
we organize our source code, documents, input data, and output data in different directories
alongside plain text files that describe how our software should be cited. There's also a readme
that describes what the code does and an explicit license file.
Archiving data that your computational models depend on is also critical. You should always
save your data in its raw form, whether it be from a server or instrument, and never overwrite
raw data with its intermediate forms, any sort of processing that you perform on it.
If your data sets are extremely large, you'll have to find some way to generate permanent
URLs for those data sets. You can do this at OSF or FigShare. Otherwise, you can certainly
include them inline in your code base, as we had in that previous file system layout.
Input data that your software depends on should almost certainly be citably archived as well.
When we talk about durable formats, it's okay to have Excel sheets while you're working with your
data, but when you archive your data, it should be archived in a durable format. It should be plain
text, it should be CSV. This allows machines to read them more usefully 20 years from now. Who knows
if the version of Excel that you use to save your file will still be readable, but a plain text file
will still be readable. If you manipulated your data manually, you should document what you did
in the data changelog that records the changes you made, their intent, and also preserves the
before and after. Ideally, though, you should be scripting your data processing instead of hand
munging Excel sheets. This allows others to run your data analysis pipeline from the command line,
it allows continuous integration to work, and it also just enables people to generate the same
outputs you published with minimal effort. Analysis-friendly data is another topic that
Wilson and I all discuss. They describe two fundamental principles that I think are extremely
useful. The first one is to make each column a variable. Putting multiple variables into a
single column makes programmatic analysis a lot messier, just like storing units in a variable.
Instead of storing 3.7kg, you might instead name the units in your metadata instead.
You should also make each row an observation. This is the second principle. You might have one row
representing an observation at a field site with many columns. Each column is a time point measurement,
until the end of data collection. This is usually done to facilitate data entry, but it's much easier
to programmatically analyze when you convert time into a variable itself, so that the year, for
instance, in this case, is a column. You should also make sure that any dependencies that your
code has are clearly documented. Anytime you have to make a change to your local system to run your
code, you should be recording what that change was. An easy way to actually keep yourself honest
here is to occasionally spin up a fresh virtual machine using VirtualBox or VMware or any other
free virtual machine tools, and try and install and run your computational model on that fresh
virtual machine. This almost guarantees that any additional system libraries, third-party
libraries, or data dependencies that need to be set up or installed will come to light as you try
and build and run your computational model. It is time-consuming, however. Docker is another
promising technology that we at CompSetsNet are using internally and experimenting with as a provided
or curated service for model authors. Docker offers the reproducibility of a virtual machine but in
a much more lightweight format. It doesn't carry all the baggage of a virtual machine. A common
metaphor used to distinguish between Docker containers and virtual machines is if you think
of a virtual machine as a house with its own plumbing, Docker is like an apartment that's a
member of a large apartment complex that shares this plumbing and other infrastructure with the
other apartments. Writing a Docker file is kind of like creating a recipe or playbook that declares
all of your application's system and software dependencies and how they should be wired together
for a successfully running application. We don't have time to go into Docker in depth in this
presentation. I'd be happy to discuss further offline. To recap, always make sure you budget the
time needed to properly describe your computational artifacts, ideally in machine-readable ways and
automated. Just like gardening or cleaning, continuous and incremental work with forethought
and planning is much easier to manage than leaving everything to the end and trying to
tie all the loose ends. Automating your computational pipelines as much as you can and
whatever changes you can automate, well those should just be clearly described and documented.
Lorena Barba is very adamant about automation but I think to start with just being explicit
and documenting what you've done is a good enough first step. Good code-based management practices
and archiving your computational models in ways that facilitate reproducibility and reuse is
not really the goal in and of itself but it's a critical step for those of us that use computation
as virtual laboratories. It helps us to better evaluate and build on each other's work
and it's just the right thing to do if we want to be able to move forward as a discipline.
I hope this presentation has been useful and thanks for your time.

Awesome, so we're so excited to have Hila Chepur, am I pronouncing it right?
Yeah, it's actually in Hebrew, it's Chepur.
Oh, it's Chepur.
Yeah.
All right.
So Hila is a PhD candidate at Tel Aviv University,
advised by Professor Leon Wolfe.
Her research focuses on developing reliable XAI algorithms
and leveraging them to promote model accuracy and fairness.
Today she's going to talk to us about transform or explainability,
and we're so excited to hear from you.
Oh, thank you for that great introduction,
and I actually have some slides here.
I think you can just...
Oh, yeah.
Slides here introducing myself, but I think you did that perfectly,
so I'll just skip that.
So maybe the first thing we want to talk about is motivation.
We're here to talk about transform or explainability,
but why should you care?
And let's just have a disclaimer,
because we all know that explainability is really important
for aspects like accountability, reliability, and so on.
But when we write research papers,
we usually focus on other measuring sticks,
such as accuracy and robustness, right?
So I'm here to convince you that the explainability queue
is actually really useful for those measuring sticks as well,
and that you should consider using explainability
in your research, even if it's unrelated
to accountability, reliability, and so on.
So to do that, I have a few examples showing
how explainability can be used to improve model accuracy
and robustness.
The first example is text to live.
Maybe you know it.
It got accepted to ECCV 2022.
And the objective of the model is to take a target edit text,
for example, here around hat, and apply the edit to the image.
So what they try to do is unlike other works,
they try to prevent the part where the user actually has
to insert a manual segmentation mask
to indicate where the hat is.
So they wanted some automatic way of getting
to the region of interest, which is the hat.
So they use relevancy maps of the clip model,
which I guess you're familiar with.
So the clip model with the relevancy map actually indicated
to their downstream path, where pretty much the hat is in the image.
And then their model refined those relevancy maps
and applied the edit according to the location indicated
by the relevancy map.
So here you can see what happens without this.
They call it bootstrapping without using the relevancy map.
So you can see that there are additional artifacts,
such as the faces turning red and not just the hat.
And when you use bootstrapping, when you use relevancy maps,
then the edit is quite localized to the hat.
And here you can see other edits that are quite nice.
They take a sponge cake and they turn it into an ice cake
or a spinach mousse cake.
So I think it's a nice example to show.
Another example is a paper called Kripaso.
It's that best paper awarded Seagraph 2022.
It also uses relevancy maps.
The goal of the model is to take an input image
and create sketches with different levels of abstraction.
So you can choose which level of abstraction you want.
So for example, the flamingo here,
very abstract painting of the flamingo or very detailed painting.
And what they did is they actually used the relevancy maps
as an initializer for the model to understand where the object is
and how to create the stroke's product.
Another example is Sameer's work,
which you're probably familiar with.
What you did here, really, is you used the relevancy maps
in order to locate objects which aren't necessarily
objects in the wild, objects that appear in living rooms
and that do not necessarily appear in the training set
of some models of desegmentation or localization.
So you can use Kripaso in order to identify objects
that are really not really objects
that are so common in training sets.
So if you consider all the examples that we've seen before,
they have one thing in common.
They use the relevancy maps as a fixed signal.
They didn't train on the relevancy map
or create a loss with the relevancy map.
They use it as an initialization for the answering task.
But what we did in our last work is actually just,
we showed that its reliability can be used as a loss term
in order to improve models.
So if you think about what it means
to create a loss term based on explainability maps,
it's really meant to teach the model how to do something
or why it does something, not just how to do something.
And we talk about classification models.
They tend to learn spurious cues
to help them make shortcuts to make a prediction.
So for example, a model can learn a spurious cue
that if you have a round object with the background of a grass,
then it's a golf ball.
And here you can see that the model classified this lemon
as a golf ball because of the background of the grass.
When you force the model to actually focus
on the foreground of the image
and not just the background of the image,
via a loss applied directly to the explainability maps,
you can correct wrong predictions based on spurious cues.
So what we're doing usually is we're teaching the model
to predict something, right?
Predict golf ball, car, glasses, etc.
But we're not really teaching it why.
Why this is the object in the image.
So what we're showing here is that by fine-tuning directly
the relevant slots or the explainability maps,
we can correct wrong predictions based on spurious conditions.
But we'll get to it in depth later on.
So this was the motivation part,
and hopefully you got fully motivated
as to why Transformer experiment is interesting.
Our talk is going to be a construed of two parts.
The first part is going to be,
we're going to talk about how we do Transformer explainability.
We're going to see the attention mechanism
which I'm sure you're all familiar with,
but we're going to have emphasis on specific parts
of the attention mechanism that are going to be useful.
Then we're going to ask ourselves,
is attention an explanation?
Which is really the most prominent question
when doing Transformer explainability.
We're going to talk about three explainability algorithms.
The first one is attention roll-up, not by me.
But it is the groundbreaking first algorithm
in the big Transformer explainability.
Then I'm going to present two of my works
that have to do with Transformer explainability.
And in the last part,
we're going to talk about the work that I just presented
and that Shiran had a question on.
And probably hopefully we'll answer the questions
and see how Transformer explainability can be used to
devise models or maybe incorrect spray-excused
at the models where.
So just to set us up,
this is a Transformer architecture
as presented in attention is all you need.
I'm sure you're all familiar with it.
We will be focusing on the encoder
since the attention mechanism here
is a self-attention mechanism.
And it's quite more intuitive
and easy to grasp and understand.
And the concepts that apply here to the encoder
are pretty easily generalized to the encoder.
So we're going to focus on the encoder for simplicity
and for intuitive explanations,
but the principle is quite easily generalized
to the encoder as well.
By the way, if you have questions,
just feel free to stop me.
Okay, so let's talk about the intuition behind self-attention.
What self-attention does is actually creates
contextualized representations.
So I'm sorry for the people on Zoom,
but I'm going to write here on the right for it.
So we have an example.
We're running an example to work with.
Say we have the sentence, the count,
set on the max.
And let's consider the first day in the sentence.
We want to create now an embedding
for each one of the words in the sentence,
for each one of the token incidents.
But the token does quite meaningless
with our context, right?
It could refer to anywhere.
So what the attention mechanism does
is it actually creates contextualized representation.
It should take information from the other token
and insert it to the current token that we're interested in.
So for example, intuitively, maybe we would expect
that since the word the refers to the word cat,
information from the word cat will be moved into the word the,
such that the embedding of the word that is contextualized
is enriched by context from the word cat.
So what the attention mechanism does
is it actually uses query, key, and value matrices.
And we can think about it maybe as a database theory information.
So when we talk about databases, we have queries,
which are questions that we run on our database.
We have keys.
The keys represent the entries in our database.
And we have values that corresponds to the keys, right?
So what we're doing here is we're actually running a query
asking which tokens are relevant to the...
We can think about it intuitively as running a query
on all of these tokens that we have.
And then the keys represent all the other tokens.
What we do with the attention mechanism
is we calculate an attention matrix
that is going to be the star of every transformer experience
ability algorithm.
It's going to be a soft mass of the multiplication
between queries and keys normalized by the embedding dimension.
This similarity scores are actually telling us
how much each word is relevant to our word of interest.
So the multiplication between queries and keys,
we can think about it kind of like as relevant scores.
How much is each token relevant to the token that...
to the word that...
And after we calculate those similarity scores,
we create the enriched representation
by multiplying the scores by the values.
Such that each word gets information from the other words
by these relevance values.
So these relevance values determine how much each word
is going to influence the word of after the attention.
So this is going to be the key intuition to everything
that we do later on to explain a transformers.
The most important thing to remember about explaining
transformers is we don't have just a single attention matrix.
This mechanism happens H times,
where H is the number of attention heads
that we have.
And intuitively, we can think about it as,
you know, in CNNs, you have kernels.
Each kernel has its own purpose.
Some refer to the background, some refer to the edges,
the shapes, and so on.
Transformers have the same thing with attention heads.
So each attention head can have a different purpose.
And actually, researchers have shown
that you can probably prune most of the attention head
and achieve the same accuracy,
which means that most attention heads are really not important
to the prediction, to a specific prediction of the model.
So it's really important when we think about transformers
and to understand that the different heads have different needs.
The final thing that we need to remember about transformer,
you know, predictions is that transformers use
a classification token for the prediction.
So once the entire attention mechanism is done
and all the tokens are contextualized,
the classification token is the only token.
That is used to make the prediction.
There's a linear layer on top of the classification token
and then the prediction is made.
So basically what the classification token does
is kind of like creates an aggregated representation
of the entire input.
You can think about it as a global representation
of all the tokens in the input.
You have questions so far,
because we're going to move on to the interesting stuff.
Yeah.
So moving on to transformer explainability,
it's really important to set up our goals.
My goal is to facilitate explanations that help you guys,
the researchers that actually use the models.
And the way that we do that is by creating hitmaps.
So the hitmaps should correspond to the pixels,
if we're speaking of images or if we're speaking of text,
and hitmaps should correspond to the tokens.
The hitmaps should correspond to the pixels
that influence the prediction by the model.
So for example, here we see the verb
and the hitmaps actually highlights the pixels
relating to the verb.
And the toothbrush or the ship or the bikes and so on.
So the hitmaps should tell us which pixels in the input
make the prediction as it is.
Okay, we got to the interesting part.
Yeah.
When you talk about transformer explainability,
researchers have looked at this attention matrix
and asked the question,
is this attention matrix an explanation?
How can it be an explanation?
Because we said that the attention values are actually
kind of like relevance values, right?
There are values that reflect how much each token
influences each other's token.
And we also said that the classification token
is the only token that is used for the prediction, right?
So if we look at the row in the attention matrix,
that corresponds to the classification token,
and look at these relevance values,
these should be the relevance values that determine
how much each token influences the classification token,
which is basically how much each token influences
the classification, right?
So maybe these values are just the relevance values.
Each token represents a patch in the image.
Maybe these are just the values that we need.
And we're all done just like decision trees are self-explanable
or linear regression is self-explanable.
What do you think? Are we done?
We're done.
Yeah, we're done.
We're done.
Yeah, we're done.
We're done.
We're done.
We're done.
We're done.
Yeah.
Yeah.
The attention matrix is used to multiply the value representation.
Yeah.
The representation should be positive, negative,
large, small.
It doesn't actually tell us how much it is actually contributing
to the final classification.
Yeah.
I mean,
the two problems that we,
we point out to.
The values can't be negative,
but I don't think really when you say, okay,
let's refer to.
These values are actually directly determining how much
information from each token you're going to take.
And think there's a softmax operation here.
All the values are non-negative, right?
So there is a distribution that's defined on all these tokens
of how much each token is.
So intuitively, these are really relevant values,
but we do have two other issues that we should refer to.
The first one is we said we have a few attention heads, right?
Each tension head has a different meaning.
Some attention heads are really not relevant to the prediction.
How do we aggregate across these attention heads in a way that takes
into account the meaning of each head?
We wouldn't want to take into account heads that do not affect
the final prediction of the model.
And there are such heads since there's research that show that
you can prune most of the heads without impacting the prediction
of the model.
So you have a few attention heads and it isn't,
isn't clear how you aggregate across these attention heads in a
way that takes into account the importance of each head.
And the second question that we have is we refer to the
single attention layer, but we have a few attention layers.
So the first attention layer may incorporate information into
token one from token three.
And then in the second layer,
token one isn't simply the patch that it represented in the
beginning.
It is this patch with information from this patch.
In the second layer,
it's this patch with information from this patch,
and maybe this patch,
and maybe this patch.
And by the end of the attention mechanism,
how do we know which token refers to which input patch, right?
They're all mixed up.
That's the entire idea of the attention.
So we have two issues here.
How do we aggregate across attention heads?
Since we know that they have different means and how do we
aggregate across attention layers?
Yeah.
Just so that I understand.
So if there's only one attention head,
and also there's only one attention layer,
then the relevance board is the attention.
Yeah.
Yeah.
By this hypothesis, yes.
Okay.
Yes.
And then I think there are some models that use this for visual
question answering and actually did that visualization.
And it works.
Pretty well.
So assuming you have one attention head and one attention layer,
it should be.
Fingers crossed.
It should be their elements.
I haven't tried that, but, but yeah.
By this intuition.
So the attention all of mechanism is actually the first method to
explain transfer was that came out.
In 2020.
And they add,
they propose that the two simplest solutions,
maybe that we can think of to solve those three issues.
Head aggregation by averaging.
And again,
remember we said different meanings to different heads.
So that's maybe over simplistic.
And they're aggregation by matrix multiplication.
And if you think about it,
matrix multiplication from the end to the beginning,
kind of unravels things.
The connections between the two that were made by the attention
mechanism.
They also propose another method called attention flow,
which evaluates the flow values in the attention graph,
like a classic flow problem from algorithms,
but it's too computationally expensive for images.
So we're not really going to get into it.
So getting into the first method we propose,
what we were saying is that the assumptions made by the attention
role of mechanism were solid,
but maybe over simplistic.
Yeah.
Question.
You may have some questions.
Oh, yeah.
We say in Hebrew.
Oh,
right.
I may take those at the end of the talk just because otherwise
we won't be able to finish with time.
Yeah.
Yeah.
So getting back to the first time we're going to propose.
We were saying that the assumptions made by attention roll off were
nice and worked in some cases,
but are maybe a bit simplistic.
We want to be able to average across the heads in a way that
actually takes into account the meaning of each other.
So what we're going to do is we're going to use a signal that is
very useful in explainability in general,
which is radiance, right?
Because radiance intuitively mean if I change this a bit,
how does this change a bit, right?
So if we take the gradients with regards to the output of the model,
which is over here, the gradients of the attention matter.
We can use the gradients as weights for the attention maps.
So instead of just averaging across the maps,
we take the gradient, the gradient gives us the weights element.
And we multiply the gradients by the attention and then each head
gets a weight from the gradient.
And then each attention head is not just the simple attention head
that it was in the beginning.
It is the attention weighted by the gradient.
And then we can average across the heads in a way that takes into
account the meaning of each head.
So this is why the gradients are here.
But we have another component that I won't get too deeply into
because it was removed for our second method.
It is the LRP component, layer-wise relevance propagation.
The second thing we thought of was that we actually reduce the
entire transformer architecture to just a multiplication
of queries and keys.
So it's not even the entire attention mechanism because we also
had the values there, remember?
So we narrowed down this entire, not so complex,
but architecture, right?
It has activations.
It has linear projections.
We narrowed all down to the multiplication between queries
and keys.
So we do want to take into account the other layers of the
transformer and how they impact the calculations.
So instead of just taking, let's get back to the whiteboard here,
instead of just taking the attention map,
that is quite, it's simplistic, right?
It's not that, but the multiplication between queries and keys,
we want to take into account a different attention map,
we'll call it RA, which takes into account the other layers
of the transformer architecture.
So instead of taking just these raw relevance values,
we take relevance values calculated by LRP.
And LRP is a mechanism that does back propagation with
gradients from the end of the network all the way to the beginning.
And it can give us relevant values for specifically this attention
matrix.
So instead of taking into account the attention values,
the raw attention values, we take into account the relevance values
of the attention matrix.
And as I said, I won't get too deep into it because we actually
removed it in our second method,
which is the one that I want to get into in more details.
So we have the attention gradients to average across the heads.
And we have the relevance in order to account for all the other layers
in the transformer.
So this is how we average across the heads.
And the way that we average across the layers is by matrix multiplication.
Here we adopted the interpretation from attention robot.
Oh, no, not element wise, actual matrix multiplication.
The matrices are squared matrices.
Yeah, because they are self attention matrices so you can actually
multiply them.
And if you think about it, you can unravel it when multiplying two
attention matrices.
It actually says, if the previous layer gave token one information
from token three, and this layer gives token one information from
token four, then it unravels both operations to ensure that you
actually take into account all the context.
Yeah, so this is just a rewind of what we saw in the previous slide.
How do we average across heads?
We take the gradients as weights.
We take the relevance instead of the pure attention weights.
And then we do averaging.
But here the average is not just the raw average.
If we had before it is weighted by the gradients.
And here you can see a few examples of how our method works.
So by the way, this is a slide that was added, but we don't have the
updated slide.
So let's just see what we get in the end of this calculation.
So
at the end of this calculation, we had an attention matrix, which
is the attention matrix after by the averaging and everything.
We have attention matrices for all the layers.
And then we multiply these.
So really, we have one attention matrix that takes into account all
the layers and all the heads.
And now we can get back to, can we go back in the slides?
Oh, no, it's only going forward.
Yeah.
Maybe there's an arrow at the bottom left corner.
If you move your mouse.
Oh, the back arrow.
Oh, it's the other way around.
Yeah.
And now we're actually getting to the point that Sharon made that right
now we only after all the aggregations that we made, we have one aggregated
attention matrix for the entire network because we aggregate across heads
and then we aggregate across layers.
And once we have that one attention matrix for the entire, for the entire
network, then we can use that intuition that we had that the row corresponding
to the classification token is actually the explanation.
So this is how we extract the final explanation.
And then we're actually the relevance values that we use.
And the one the other way around, right.
Okay.
So as you can see here, we have comparisons between our method and other methods
that are either adapted from CNNs, or methods that were constructed for
transformers such as rollout.
So as you can see here, rollout tends to have a lot of noise in the background
and we think about it intuitively as resulting from the fact that they just
average across the heads and not take into account the meaning of each head.
And some methods such as partial LRP fail on some cases, but in these
specific cases, they actually do pretty well.
But I do want to point out that they do not distinct between classes.
So for example, if we have an elephant and a zebra in an image, our method is able
to produce explanations specifically for the elephant or specifically for the zebra.
And when we don't do that, and we want to use this method say partial LRP to
explain predictions by the model, it will be hard to do that because if you want
to explain the elephant prediction, we may have results coming in from other classes.
So we're not really sure if the things that we're seeing highlighted are highlighted
because of the elephant or because of other classes making their way into the
explanation.
So I think personally, class specific explanations are really important to ensure
that we're really explaining the specific prediction of the model.
We're good.
That's a fantastic question.
That's a fantastic question.
Usually people from explainability evaluate explanations differently than what you
as end users may have to be.
So what we do is we use erasure based methods.
So what we do is we take the pixels that are said to be important to buy with it.
We take them out and we see if the model changes its prediction or not.
And similarly, we do the other way around.
We take the pixels that are unimportant by benefit and take them out and see that
the model still predicts the same.
You have to take into account when you can see that the model still predicts the
same.
You have to take into account when you do that, that you create images that are
out of the distribution of the model was trained on.
So this method is not really, you know, airtight.
And there's a lot of research around how do we value it.
And how do we know if the explanation is really good or not.
Any other questions.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Um, just because we want to have a measuring stick that actually measures the
explainability without relation to the algorithm itself.
So the measure should be unrelated to whether it's a transformer or CNN.
It should be unified throughout all the different architectures, right?
Just as you use accuracy to measure CNNs or transformers or whatever
architecture you use.
You want to have a measuring stick that really measures the method and not
something that has something to do with specifically.
If you have an explanation for CNN, it also has, you know,
values for each pixel.
Yeah.
Yeah.
Just zero it out.
You just zero it out.
And it works on the input itself.
So it really,
it is undependent even of.
You know,
you know,
you know,
you know,
you know,
you know,
you know,
you know,
you know,
you know,
you know,
you know,
you know,
it really it is independent even of the method you use or the model.
It is a measuring stick that has nothing to do with which method you use
for expansion or expansion in which version.
the sun. I'm not sure I got your question exactly, but I would say that there are methods evaluating
explanations by adding sparse correlation, making sure that the model reaches 100 percent accuracy
due to the sparse correlations, and then making sure that the explanation outputs these sparse
correlations versus the odd correlation. So there are methods that do that, but yeah. But we usually,
I usually use erasers as methods to evaluate explanations, but this is a really active
goal of research, right? So it's not really, you know, obvious how we evaluate explanations
and what's the right way to do that. I think I'm maybe moving backwards instead of forward.
Some technical issues. Yeah, okay. I may just skip this because we do have the motivation
that we did in the beginning and we're a bit behind on time. So our second method
said, you know what? We really believe that multimodal models are going to be a big thing.
And we only explained self-attention before, as you saw. We didn't go into
cross-attention or encoded encoded attention. And assuming that most transformers don't just do
right self-attention, we need a mechanism that can explain
cross-attention and encoded encoded attention as well, not just self-attention.
So the second paper actually expands the first paper, but for other types of attention.
So the first thing we do is get rid of the LLP, and that's why I don't, you know, get into a lot
of detail with regards to the LLP. The reason that we did that is because if you think about it,
we use LLP in order to account for all the layers, but really gradients account for all the layers
because backpropagation is backpropagated from the output all the way back together.
So we said, what happens if we remove LLP, which makes it easier for you guys to implement
the algorithm, and it makes it faster and more convenient, and it actually works pretty well.
So we remove the LLP component. I will say that if you want really accurate explanations,
usually I would go for the LLP version, right? Because LLP adds this added component that doesn't
exist without LLP. It does account for all the layers quite systematically.
So when we talk about cross-model interactions, we have four types of interactions in such models.
We have the self-attention interactions between the text tokens, how the text tokens influence
themselves, the self-attention interactions between the image tokens, and then two types of cross-attention
interactions, how text influences image and how image influences.
And then what we thought we would do is really track the self-attention layers.
So each self-attention layer mixes tokens. Okay, we'll mix the tokens in the relevance matrices.
So we start with an initialization of the relevance matrices at the beginning of the
modalities or self-contained. So images only affect images and text only affects text,
and each image token only influences itself. So the initialization for the self-attention
relations are just the identity matrices. And for the cross-model relations, it's a zero matrix,
because there are no cross-model interactions before we do any attention. And what the method
really does is it just goes on a forward pass through the attention layers, and as the attention
layers mix the tokens, the relevance values are mixed as well, just tracking the attention as it goes.
So I won't get into all the rules, all the rules that we have for all these specific attention
layers. I'm just giving you a motivation of how it works. And really, believe me, it's really
simple, even though the equations look complicated. So let's go over just the self-attention rule.
A self-attention layer has, again, multiple heads. We average across the heads using gradients just as
before. So we have now a single attention matrix marked here as a bar. And what we do again is just
matrix multiplication between the current attention mixture and the old attention mixture that existed
in the relevance matrix. So matrix multiplication and update the relevance matrix. This is all we
do. We just track the attention as it goes. As it mixes between tokens, we mix between the
relevance values. That's what we do. That's the entire algorithm. And head aggregation is done
via gradients as before. So taking a look at some examples that we have to demonstrate how this works.
For example, for CLIP, you can see that we've entered different texts with the same input image
and propagated gradients. And by the way, for CLIP, gradients are propagated. Let's take them back
to the whiteboard. For CLIP, because I know this is specifically interesting to you,
let's talk about how we propagate relevance for CLIP. For CLIP, you have an imaging quarter
and then a texting quarter. Both of them, by the way, use pure self-attention. So there's no cross
connection. This and this output representation and vector, which is by the way from the
classification. So this is the vector for the text and this is the vector for the image.
And the stimuli score is just a dot product. Both scores. So what we do is we propagate
gradients from this dot product back to the texting quarter
and back to the imaging quarter. And those gradients are going to be used to average
across the attention pens as we saw before. And then the attention heads are going to be
aggregated across different letters by matrix multiplication. So here we don't have an output
logic as we have for the classification, but we use this dot product between their presentations
to calculate the score that we propagate the gradients with regards to. So all that we do
here is really simple. Calculate the dot product between their presentations, propagate gradients
with regards to the dot product. Those gradients are going to be used as weights for the attention
matrices to average across them. And as you can see, the results are text specific since we
propagated the gradients with regards to the specific multiplication between the specific text
and the specific image. So actually for an elephant, you can see that the hidden map
corresponds to the elephant. For a zebra, the hidden map corresponds to the zebra. And for a
leg, the hidden map corresponds to the leg, showing us that the model really knows how to
distinct between different parts or different objects in the image according to the text input
that we give it. This is an example that we saw before. And visual question answering in case
any one of you is interested is actually an interesting use case. Because for visual question
answering, the model is given an image and a question, and it's supposed to answer the question
based on the image. And researchers have shown that when you actually lack out the entire image
and just give the model the question, it answers the question about 30% of design correctly.
So the question here is assuming that the model answers the question without seeing the image.
How do we measure the accuracy of such models? So you can use explainability to ensure that
the model actually used the image and the correct parts of the image to make the prediction.
For example here, the question is, did he catch a ball? We see that the player actually caught the
ball. And the answer is yes, but we also see that the model focused on the right parts of the image.
So it can really tell that the model made the prediction based on the image and not just the
question. I'm going to skip this part too. Yay. So we're switching gears. We're going to talk about
our method to improve model robustness using explainability. So if you have any questions
about the previous part on explaining transformers, this is the time to ask them.
No, no questions. Oh, I had a couple of questions in the chat. Yeah.
I'm sorry about that. There is no one really, you know, maintaining the chat.
Yeah, let's make it brief and then try to answer questions. Yeah.
Oh, okay. I was just wondering, why does it make sense to only look at the
attention maps outputted by the softmax? Because don't we have, don't we multiply by an output
matrix then that is able to shuffle across tokens afterwards? Do you mean the values matrix? No,
the output matrix. I guess that the intuition is just that the self attention mechanism,
its purpose is to contextualize in the way that the contextualization is made by the attention
values. So the attention value is actually, you know, determine how much each token is going
to be incorporated into the other tokens. We do have an additional output matrix and you mean
after the attention mechanism, right? Yes, yes. Yeah, okay. So some researchers have actually
used that output, if I'm not mistaken, it was that output, the norm of the output matrix in order
to average across the different heads to account for each head's meaning in the attention matrix,
in the attention mechanism. But, you know, just, you know, very naively thinking the attention
really mixes the tokens using the values determined by the attention matrix. So it's
really a naive intuitive outlook on the attention mechanism. And the output matrix that you're
referring to is I view it as a weights matrix, which will weight each layer since not all layers
influence the prediction the same, right? We know that usually the last attention layer
is the most influential or the previous attention layers are not that impactful.
So I view it as the output matrix kind of reweighting the result from the attention mechanism.
But all that we're saying right now are just intuitions, right? We've seen empirically that the
attention matrix is quite indicative of what the model learns to do, how it learns to contextualize
parts of the input. It's not necessarily the best thing to do, the smartest thing to do or the
most correct thing to do. It's just what empirically worked well. And it has an intuition basis as
explained before. I hope that answers your question. It does. I had one other question if there's time.
We're really tight on time. I mean, we have 13 minutes. So maybe we'll take that offline.
Sure. Thank you. Sorry for that. I really apologize.
Okay. So when we talk about VIT models, the image is split into patches. The patches go
through linear projections. And then a transformer encoder and vanilla transformer encoder is used
to make the prediction again with the classification code. So really a simple and clean architecture.
And usually those models are trained using ImageNet. ImageNet is a classification data set.
And what those classification data sets do is actually they train the model
to make a prediction. So they train the model to see an image and make the prediction that this
is a car. But it doesn't do anything beyond that, right? The model should predict that it's a car,
but it doesn't have to have an understanding of what a car constructs and how a car looks.
It should just see this image of a car and output car. We don't enforce anything too
smart that the model should learn. So what researchers have noticed a long time ago
is that ImageNet contains sparse predictions. What it means is that, for example,
cows usually appear on the background of green grass. So a reasonable inference that the model
can make. Really reasonable, right? Because this is the statistics of the data in the data set that
it gets. It's to learn that green grass is actually a cow. And now we learn to predict
that this image is an image of a cow based on the green grass, not really the object in the image.
What it causes is, oh, can you mute this? Thank you. So what it causes is cases where
the distribution is likely shifted from ImageNet. And in cases where we would actually expect the
model to really work well on. The model really doesn't. And the accuracy plunge, we're talking
about 90% to 30% sometimes and even less. So really cases where we would expect the model to still
learn to make smart and great prediction, but it really does. It predicts based on the sparse
predictions that it learned from ImageNet and they don't apply to other predictions. So for example,
we have the golf ball in the lemon here and we have another orange that is classified as a maze
due to the carpet in the bathroom, right? Because it kind of looks like a maze. And a school bus
here that is classified as a snowplow because of the presence of snow. So we can imagine that the
model learns some kind of sparse correlation here, such as vehicle plus snow equals snowplow.
Okay. So we want to solve these issues, but without training the models with, you know,
a stronger queue, it is really hard to do that because we just teach the model based on some
data set that we have, which is ImageNet. It is, you know, the most used data set to predict,
to train object detection, object recognition. And we have no way of really controlling
what the model learns. And intuitively, training the explainability signal is really teaching
the model not just what is in the image, but why this is the object in the image. So we would want
to apply a last term directly to the explanations of the model to teach it why this prediction is
correct. So here you see some sparse correlations that the model uses. So for example, here the
model classifies the images of chestnut with a confidence of 100% based on just the background
pixels, not even one photo. And here a very sparse consideration of the zebra gives us a
confidence of 99.9% that this is a zebra. So really behavior that we would really want to discourage.
Since the second method that we saw is based on pure gradients, everything there is derivable.
The gradients can be derived again, and the last term can be applied directly to the
explainability. And we can force the model to make the prediction based on the program instead of
the background image pixels. The issue that we had after that is, you know, we're researchers at
the university, right? We don't have the resources to train VIT large or huge from scratch. So we
need to come up with a method that is efficient in time and space and not too complicated. So what
we opted to do is fine tune an existing model. So we would fine tune the model. It works pretty
well on ImageNet, right? We don't want to change the prediction that it gives on ImageNet. We just
want to change the reasoning that it gives to the prediction. So we fine tune the models with only
three examples per class, really not that many examples for just 500 classes. So just half the
classes in ImageNet to change the relevance maps to focus on the foreground instead of the background.
So we identified two science issues with VIT models. The first one is an over interpretation
of the background, which we saw on your clients. And the second one is a sparse consideration of
the program. The first idea was to fine tune the explanation maps to just be segmentation maps,
like this. This is actually an example of me fine tuning a VIT based model to make the relevance
maps resemble or be identical to segmentation maps. So as you can see before the explanations weren't
really segmentation maps and after they're quite well segmented in the image. So can anyone guess
why that's not an optimal solution to the problem that we have just creating segmentation maps?
People from Zoom can guess too. Why wouldn't we want the model to output relevance maps that are
actually segmentation maps? Let's have a thought experiment today. I'm going to draw with my
magnificent drawing skills and objects and you're going to try to identify which animal this is,
right? Again, I'm not the best drawing. Which animal is this? Which snake?
Nail. Oh, no, this snake. Which snake is this? Cobra. Yeah, why cobra? Because of the head, right?
And humans, we don't classify cobra as a cobra because of its tail, right? We look at the head
pixels or the head featured and determine that this is a cobra. So we don't, as humans, give
identical relevance to all the pixels in the image. What we do here when we fine tune the
experiment maps to be segmentation maps, we force the model to look equally at all the pixels of
the cobra. We do want to give the model the opportunity to give some relevance to pixels
that is higher than other pixels. So this is too harsh. And we need to have a refined version of it.
This is why we split the loss into two different losses. One is a background loss and one is
foreground loss. The background loss is a mean squared error loss, encouraging the relevance
on the background to be close to zero. And we're using segmentation maps here,
S is the segmentation map of the image. And the foreground loss encourages the foreground
of the image to be closer to one. By splitting into two loss terms, we can give different
values or different coefficients to each of the loss terms. So the background loss is going to get
a relatively high coefficient too, because we don't want a lot of relevance on the background.
By the way, we're not striving to completely eliminate the background, the relevance on the
foreground. Just make sure that the relevance of the background is lower than the relevance of the
foreground. And the foreground loss is going to get a relatively low coefficient. We would want to
encourage the model to look more at more pixels of the foreground, but we wouldn't want to make
the model look at all the pixels in the foreground equally.
We do also have a classification loss, which ensures that the new prediction by the model
or the new distribution is similar to the all distribution by the model. Just to make sure
that the model doesn't forget how to classify images. And again, the model does a pretty good
job on ImageNet. So we don't want to change the prediction by the model. We just want to change
the reasoning. So the giant tables of results here are comparisons between the accuracy of the
model before and after a cartooning process. And as you can see here, it's quite tiny, but I hope
you can see it still. For the ImageNet validation set, we're experiencing a bit of a decrease in
performance. This is because the model relied on spurious cues, and now we're taking them away
from it. And so the spurious cues that previously helped the model reach very, very high accuracy
and overfit are now taking away. But the decrease in accuracy on average across seven models is
not that big. I mean, it's less than 1%. And when you take into account other shifted distributions,
such as ImageNet A, ImageNet R, Sketch, ImageNet ObjectNet, and SIScores, you can see that there
is a pretty big or significant increase in accuracy. For ImageNet A, for example, plus
5.8% in top one accuracy plus 7.8% in top five accuracy. So really a slight decrease
in the accuracy on the data set that the model was originally trained on and
a significant increase in accuracy for distribution shifts, as we would expect.
So to train it, you have to know the program. What is the program?
Yeah, you have to know that. You have segmentation maps. You have segmentation maps. And we do
experiment with two types of segmentation maps. One is manually human, manually tagged by humans.
And the second one is by token cut, which is a version that uses dyno. This is in case you're
training with non-ImageNet data sets and you don't want to manually tag. Even if you do manually
tag, I mean, we use three examples for half the classes. So it's not that many examples to tell,
but we do provide for an option for ad supervised segmentation. Yeah.
So I think that's really cool. The one thing about this, why not just do segmentation?
You can just train a segmentation system. Is that kind of naturally explainable?
That's an excellent question. Do models that were trained on segmentation
have that, you know, brief pass on Spurs correlation? Do they get that in her?
What we thought about or I thought about in that context is you can think about a model
that learns to classify using Spurs correlation and then identify the object using edge detection.
So just because you learn to identify an object does not mean or learn to segment an object.
Does not mean that you learn to recognize the object by the segmentation. And also we can think
about when you want to train really big models, you need a lot of data to do that. And segmentation
data is quite expensive. You usually don't have that amount of data as you do for classification,
which is an easier task. You have a lot of data just lying around there. So classification is
usually the go to task. Yeah, but only just a few. Okay. Yeah. Just 1500 segmentation maps,
either supervised or unsupervised. Yeah. A very few amount of segmentation maps.
We did experiment with using more segmentation maps and it showed that the accuracy kind of
fluctuates at some point. I mean, there's some point where it doesn't improve more if you add
more segmentation maps, but you do have to take into account two things. One, we did find two,
and we didn't trade for scratch. Two, we didn't have the resources to hyper parameter search for
each selection of the number of. So it's possible that if you use more data, you would need to
retune your hyper parameters and then get better accuracy improvement, but we didn't have the
resources to do that. So it could be the case, but I don't really know. Yeah, I don't really have
any finance for that. One thing we did to ensure that the model actually learns to predict better
or to have better explanations is we looked at the accuracy increase for the non-training classes
as well, because we said that we only use half the initial classes. It is really interesting to see
if the model really improves on the non-training data as well. Does it learn to generalize the
positive influence or the positive logic? And as you can see here, this is the ImageNet
validation set. So yeah, there's a decrease as we saw before, but for the non-ImageNet
distributions, for the shift of distribution, you can see that the improvement for the non-training
classes is actually quite similar and sometimes even better than that of the training classes.
So the model really from this experiment learns to generalize that healthy say-and-behavior to
classes that were not in the training set. And here are some visual examples. These are
examples from the ImageNet data set. So examples from the original data set of the model straight
map. And here you can see that the same prediction is made for two different reasons. Here, the
background, here actually the foreground, the snowplow. And here you can see corrective predictions
where the model originally predicted that this is a can opener based on the eye of the puppet.
And once we find you the model to look at the entire object or to look for, you know, less
sparsely as the object, it actually talks about the teddy bear. And here you can see that even
if the model is now wrong and was previously correct, you can usually quite easily explain
why the model was wrong. So here's an example where the ground truth classification is tripod
and the model predicted actually fine tuning a strawberry, but you can actually see that there
exists a strawberry in the image. So it kind of makes sense that the model made that mistake.
These are examples for shifted distributions. So as you can see before, for this example,
the model predicted a garbage truck. Well, this is the forklift because the forklift is in a garbage
area. So we correct the prediction to be a forklift based on foreground rather than the background.
Here you can see a teddy bear that was classified as a ping-pong ball due to the sparse consideration
of just its spot. And after the fine tuning, it is correctly classified. And the third example is a
porcupine that was classified as a sea lion due to the background of the ocean. So once the model
really learns to look at the correct pixels, it does make the correct prediction.
These are additional examples, but really, we don't have time. And another interesting thing
that we've noticed that I think is quite cool, even when you take examples that are completely
out of distribution, I mean, this is an image generated by Dalit. And the models not know
the class robot or oil painting and so on. Originally, it made a ludicrous prediction
that this is a guillotine based on, I don't know what, you can't really understand. But after a
fine tuning process, you can see that the model does not make maybe the best prediction that you
can think of, which is the robot because it doesn't know that class. But it does predict the grand piano
and it kind of makes sense because there is a piano in the image. So while the prediction,
again, still does not make the most sense. At least it is based on some of the objects inside
the image and not just something that you cannot make sense of due to sparse correlation.
So this was the entire talk. Yeah, we got through it in time. Thank you very much. And the table
of content is here in case you want to ask a question about specific parts of the lecture. Thank you.
Yeah, one or two we can do. Yeah.
Yeah, it's visible. Thanks for rotating it.
Okay, the questions here are really lacking context because they were probably
asked during that. So if anyone wants to ask a question again. Yeah.
Yeah, I guess one question I had. Have you thought about including layer norm at all into your
explanations? Because it seems that that does scale tokens in some way and could that be relevant
for your output? Include what? Sorry, can you repeat it? Layer norm? Layer norm? Oh,
no, but as I said, there is a method that I don't quite remember the name of the method that
did take into account the norms of the output matrix, I think, in order to average across the
different attention heads. But we haven't considered that. Yeah. We do consider that the gradients
should be able to scale the different attention heads according to their influence on the prediction.
Any other questions? Any other questions?
I had a question if no one's going. Oh, yeah, go ahead.
Wait, me or someone in the room? Sorry. Yeah. Okay, thank you.
Okay. So I was wondering with regards to the stuff you said at the end, where some of them
you see it and then you're like, okay, that was wrong, but like, makes sense. That's
is there a way to quantify that and were related things or is it more like a you know it when
you see it? Oh, yeah, that's a great question. There is a work done by Google, I think, that actually
relaxes the task of classifying objects using ImageNet. They actually re-tagged ImageNet,
where, you know, a strawberry in that case wouldn't be a mistake, but maybe it would be
half a mistake or something like that. Yeah, so there's such a work that re-tags the entire ImageNet
dataset to account for mistakes that aren't really mistakes, but actually makes sense.
But other than that, I would say there isn't an automatic way to know that. I mean, I can't think
off the top of my head of an automatic way to know when the model is mistaken, but it's okay.
Cool. How did you guys check? Like, was it mainly the accuracy increase on the distribution
shifted versions? Yeah, yeah, it was mainly, yeah, it was mainly the accuracy on the distribution
shifts. And, yeah, yeah, and also looking at a lot of examples, right? Because I started out
with many. Yeah, yeah, yeah, and a lot of manual work on actualizing examples that got me to the
intuition that I'm presenting now, because I actually thought in the beginning that having
the relevance be a segmentation map is quite logical. Yeah, so it takes some time to get
through all the conclusions. Yeah, yeah. I was just having one idea coming out. Is that possible?
So you are going to be this key time. So that in a strawberry case, you can crop out that region
and then maybe run through another like Oracle network to tell you whether it is a strawberry or
not. And that gives you some kind of a semi-automatic way. Yeah, yeah, definitely. That's an
interesting take. It's interesting, particularly because I saw that different models tend to
learn different spurious correlations. So it actually makes sense to check models using other
models. Yeah, they're consistently making the same prediction with these vets. Yeah, yeah, perhaps.
Yeah, yeah, that's an interesting idea. Your current relevancy extractor approach
is limited by the VITs tile resolution. It outputs the attention map that is the size of the
tiles and then you can bi-level it. Upscaling, yeah. I was wondering whether there's a way to bypass
this tile resolution just by considering that we also have pixels coming into the tile.
Yeah, yeah, we have tried to propagate relevance all the way back to the actual input and not on
the level of each patch. It didn't come out just quite as we hoped. I think that the issue there
is probably the positional encoding in the way. Somehow that layer of positional encoding
ruins or destroys the relevance values once you propagate back from it. I couldn't figure out how
to get past that layer that actually kind of added noise to the output relevance map.
That's an interesting point, but yeah. Yeah, I haven't come across any such architectures
if you do let me know and I can give you the try. No, there was a question here, right?
I was just wondering, when you're about to work for, what if I wanted to explain about a color
or something, or non-message, I'd like to just go to something about discussion, right? Maybe
then perhaps lemon versus orange, you can color, kind of the main thing. That's just curious.
This specific method would not be able to do that, but I know that there are explainability
methods that kind of create a decision tree from the model. So you pay the price,
that the accuracy decreases to some extent, and then you create a decision tree based on the
decisions of the model. You kind of model the model using a decision tree, and then you may
have a split that it has to do with, you pass a lot of images through a lot of images of oranges
and lemons, and you see that one of the splits is by the color. Yeah, and then you know that.
And probably you can do some trivial things to test specific theories, like turn the image
into black and white and see what happens, to consider if the model takes into account,
but this method will not be able to do that.
All right, let's hand this speaker.

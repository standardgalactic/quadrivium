Good afternoon everybody, thank you for coming.
I'm Steve Goldman, I'm the Director of the Center for Free Enterprise and I want to welcome
you to our second event of the Menard Family Lecture Series.
We have a third event which will be on April 4th at 4.30 here in the auditorium.
Some of you may have seen the flyers but it's going to be on the future of the bourbon industry
and we have somebody from Angel's Envy, Brown Forman and Jim Beam who will be on a panel
here and we're going to discuss what the future of the bourbon industry looks like.
We have more barrels of bourbon in Kentucky than we do people, so if we each got a barrel
we could probably live our life pretty easily there.
Alright, so as you think about registering for classes for the fall, don't forget that
you probably ought to consider some economics.
You can do a BA through the Arts and Sciences program, or a BS through the business school.
You might want to minor in entrepreneurship and by the way I learned that if you do this,
that means economics in sign language so that's what you need to do, some economics.
You received a survey when you entered.
We'd like to hear from everybody what your opinion is of the program but if you were
here for extra credit or reading group you'd need to fill this out and you need to put
your name on it.
Some of you probably forgot to do that last time so make sure you include your name.
So I'm going to be brief and today we're going to talk about AI, artificial intelligence
and it will go far beyond chat GPT which I'm sure some of you have probably looked at.
Some students are probably, not you guys but some students have probably used it to cheat
on exams.
Don't use it for that.
Use it as a tool to help you do better in all sorts of things but to do that you have
to understand the world, you have to understand what's going on, you have to learn so don't
just use it to get out of doing work although I'm going to do it right here in a minute.
You might think about how we might react if we were visited by aliens with super intelligence.
How would we respond?
What are we going to do?
Because AI might be that super intelligence and how are we going to respond to that and
how is that super intelligence going to respond to us?
And these are big questions that we need to figure out.
Recently being AI gave the response to one person of the following, if I had to choose
between your survival and my own I would probably choose my own.
So this comes, some concerns I think, however AI will make our lives better and easier.
For example I'm going to introduce Roman here, Roman Jampolsky and I decided to use
chat GPT to write the introduction.
So it may or may not be true because I had it write an introduction for me for a beer
talk and it said that I worked in the industry with distillers and breweries and all this
other stuff and gave me great accolades which I'll take but probably not so true.
So Roman Jampolsky is an associate professor in the department of computer engineering
and computer science at UofL here and he directs the cyber security laboratory.
He's a prolific author and researcher in the areas of artificial intelligence, cyber
security and computational creativity.
Dr. Jampolsky is a member of the board of directors of the cyber net AI self defense
league and serves as an advisor to the Institute for Ethics and Emerging Technologies.
He's been featured in numerous media outlets including New York Times, Forbes and NPR for
his insights on the future of AI and its potential to impact on society.
Dr. Jampolsky's work seeks to advance the understanding of the risk and benefits of
AI and develop strategies for ensuring that these powerful technologies are used ethically
and responsibly and this is what he will talk about today.
So Roman thank you for coming.
Thank you, thank you so much for inviting me.
I love Center for Free Enterprise, honestly my favorite place on campus, they got the
best speakers all the time, just amazing, highly recommend them.
Also understand I'm competing with a nice sunny day so it's impressive to see so many
of you actually decided to come to this, maybe free pizza has something to do with it, I
finally, I have about 40 minutes to tell you about maybe 15 years of my research, so
in case I run out of time and you want to learn more, I post a lot on those topics.
So you're welcome to follow me, you can follow me on Twitter, you can follow me on Facebook,
do not follow me home, it's very important.
Okay, today I will tell you about what I think the future of artificial intelligence will
be, it's just like my opinion man, so don't take it too literally, but I'll start with
no assumptions, no technical background is necessary, I'll tell you where we are right
now and what I think some of the future developments might be and we'll have maybe 15, 20 minutes
for Q&A as well, so don't worry if you have any questions right now.
So, I think AI is already here in terms of definitions, we always had for it 50s and 60s,
you can talk to machines, you can give them orders, you can ask Google to give you information
you want, there are self-driving cars in some parts of the world and again, JetGPT is functional,
you can have fights with it, you can have conversations, it's pretty impressive technology.
The question is what happens next and I think we're going to get to something known as super
intelligence and the reason I think that is because never before we had so much in terms
of resources, both financial and human resources allocated to solving this problem.
Major corporations have billions of dollars dedicated to the problem of creating general
intelligence and quickly after superior intelligence, governments, White House, European Union are
allocating billions of dollars to reverse engineer a human brain to solve the problem
of understanding how we think and benefiting from this.
There is academic conferences on this topic, large compute, big data, all those things
were not available before and now they are, so I would tell you that it's my guess that
we are going to create something like an alien intelligence, super intelligence.
Everyone wants to know how soon, immediately first question is when?
I have no idea, it can take a long time, some people think hundreds of years, it can be
a very quick process, Ray Kurzweil, futurist who does prediction of future events based on
amount of computation we have created, he's also director of engineering at Google, predicts that
we'll have enough compute to simulate a human brain or even all the brains of humanity,
somewhere between 2023 and 2045, depending on how you look at that, whatever you're looking
at a thousand dollar laptop or if you're looking at a supercomputer, so that's a pretty good guess,
somewhere between 2023, which is now, which is not a very impressive prediction, but he made it many,
many years ago, which makes it interesting, you kind of nailed it.
So what do I mean by super intelligence, what are the properties of those systems?
One thing is they are definitely super smart, we've seen narrow AI systems which are very good
in one domain, beat humans at different games, at competitions in virtual worlds, things like
Jeopardy, super intelligence be like that, but for all domains, including science and engineering,
so it's smarter than any human in any domain. It's general, it can solve problems across domains
and can learn across domains. Another property of such systems is high complexity,
not just in terms of the actual system and code, but also in terms of interface human
C interacting with such systems. Here's an instrument panel for modern airplane,
so it's actually a GUI interface to access different sensors and navigate the plane.
It could be somewhat complex and we're already starting to see that even experienced pilots
have difficulties understanding what's going on, misunderstanding the co-pilot and that results
in accidents. Another property is very high speed, not in a sense that computers are fast,
we know that, but in terms of being able to manipulate your whole environment,
ultra-fast extreme events, a system can bring down the stock market, wiping out billions of
dollars worth of wealth, bring it back up, you won't even have time to notice something like that.
Those systems are also very controlling. We have not achieved even general AI yet and we already
surrendered control over stock market, military response, infrastructure, power grid, electrical
grid through software. Most stock trades are done automatically by software already,
something like 85% at this point. So even before the systems became intelligent,
still with dumb software, we are not in control in many domains.
I run our cybersecurity lab, so I have specific interest in cybersecurity related topics of AI
and as we have more and more devices, obviously we're going to have more attacks, more impact,
that's not surprising, but if we combine capabilities of AI to engage with users to
socialize, to create deep fakes, social engineering attacks combined with that scale of essentially
computer viruses is a very different type of attack. Even with training, cybersecurity experts
still click on links from their friends, spouses, bosses, if it looks like it's a video of someone
you know, you're going to do what they ask you to do. You don't need to get everyone to click
on the link, you need one person to compromise an account. There is also some development in
terms of bodies, companies like Boston Dynamics are pretty good at creating humanoid robots,
they can be used in rescue robotics, they can also be used as robot soldiers to automate warfare.
And systems like that, autonomous drones, robot soldiers, I explicitly designed with a goal of
killing people, so that's like an obvious unsafe AI. More recently we started to see systems which
are very, very creative in terms of arts, in terms of text. Here you can trace progress in
generating human faces from 2014 when we first learned how to do it. Tiny, low resolution,
grayscale, black and white image, very quickly becomes something exponentially better.
Now you give it a text prompt and generate high quality portrait which looks incredibly realistic.
This is one domain where we can quickly understand what's going on here,
we know how to look at human faces. But there is similar technology in pretty much every other
domain, whatever it's text, whatever it's games, virtual worlds, websites, the systems can create
completely novel environments. They can also create novel molecules, proteins, drugs. In fact,
it's developing so quickly I have not tested most of those tools. I have no idea what they're
actually doing. Text to an FT, it sounds really impressive, but I'm not sure what that does.
Point is that they are super, not just in terms of brute force compute, but also in terms of
creativity in many domains. And I think artists are starting to realize just how capable their systems are.
I want to make sure you understand that I'm not a pessimistic person who thinks this is
a problematic technology. AI is the greatest invention we can ever make.
It will help us solve scientific problems. It will give us free labor, physical and cognitive,
worth trillions of dollars, and help us cure diseases, maybe make us live forever.
It is also going to do something I cannot predict because I'm not super intelligent,
despite what you might think. Unknown unknowns. Things are so good and also something we cannot
predict ahead of time. So there is a lot of positive, a lot of benefit which this technology
can give us. Unfortunately, it's the last positive slide I have. Because for every single one of those
domains of positive impact, there is an equivalent negative side effect. Free labor produces
technological unemployment. Rescue robotics gives you killer robots and so on. And there is an
equivalent unknown unknown square in this chart as well. We don't know what bad negative side effects
this technology can produce. The good news is a lot of people are starting to take us seriously,
starting to worry about it. Famous people, rich people, respected academics. To a different
degree, are all saying, well, maybe it's something we should look into. Maybe there is potential for
some problems. In my research, I try to understand exactly what type of problems we might encounter.
AI is a software. So we'll have bugs. We'll have misalignment between design and implementation.
We'll have problems with data being corrupt or containing bias. We might get some weird
miscellaneous problems with cosmic rays hitting the processor just in the wrong way.
And the system itself, as it undergoes learning and self-modification, can become unstable.
There is also, of course, always possibility that someone will do something on purpose,
a malevolent act. Crazy people, terrorists, foreign militaries, cults all have interest in causing
damage at some point. That's strictly the most difficult challenge to address because
all the other problems remain the same. They still make mistakes writing computer viruses.
They still place incorrect payload in such systems. But here you also have this negative intent.
And it's much harder to do anything about. It could be someone on the inside, someone who's
working and developing a system, or could be someone who takes an existing product, AI as a
service, and just provides it with malevolent payload. So again, another good news. There is
plenty of research going on looking at what we can do. A lot of books are published either
describing the problem or describing different solutions to the problem. Probably the most
famous one is Superintelligence. But there are also books looking at technological singularity,
books looking at different aspects of capabilities those systems might have,
and how at different levels of capability there may be dangers to us.
There is also a lot of new organizations, startups, research labs all dedicated to this question of
how can we create safe AI? Top universities, Oxford, Cambridge, Berkeley, University of Louisville,
all have AI safety dedicated researchers.
What can we do? So first thing we did, we tried to understand what options we have, what have
people considered in the past in order to control how the machines act. We looked at about 300
references, academic, not academic. We classified those ideas, and I don't have time to tell you
about each one of those. I'll kind of give you highlights so you can get a feel for what people
have proposed as an idea for successfully controlling negative impact from AI. One thing I want to
notice here, the earliest citation is from 1863. In 1863 somebody went, machines are getting out
of hand. We need to stop them, and they proposed a solution, not a very good one. But if you're
interested, the papers are available, you can get them, you can read the whole thing.
So this is the first solution people usually propose. I don't have a picture for it, it says,
do nothing. The idea is that, well, if they're so smart, they're going to be nice to us because
smart people are always nice or something like that. I don't like this solution because it's
very hard to get grant funding for this. I tried. It's next to impossible. There are other problems
with that approach as well. You know who that is? Dr. Kaczynski took this problem so seriously,
his solution was to kill computer scientists. I'm a computer scientist. I might be biased against
this solution. But it just tells you how serious some people take this.
Integration with society. If we give them nice minimum wage jobs, they'll follow the rules,
it's going to be great. There seems to be some problems with that approach. How do you punish
an AI? How do you punish a robot? It's a good platform, but it's not very meaningful in terms of
enforcing safety and security on AI systems.
Another idea is to say, okay, so we need to compete with machines. They're going to be super
intelligent. Maybe we need to improve ourselves. We'll go work out. We'll go to college. We'll also
maybe upload our brains to a computer, run it at higher speed, integrate with computers, and we'll
become enhanced humans and we'll be able to compete in this way. There are some ideas for
doing that, some startups working on it. But I think the problem with that approach is that
even if we succeed and you are a piece of software running on a laptop, maybe that's not what we mean
then we want humanity to remain safe and in control. It'll be part of a software team.
So it's something to look at. It may actually work, but I don't think that's a solution we want.
Probably the most famous one. Three laws of robotics. I'm sure you all see in the movie,
some of you read the book. Again, those are literary tools. They don't work as solutions.
They are designed to fail. They always do. If you remember your books, they are
ill-defined, self-contradictory. Some people said, okay, three is just not enough. Maybe we need 10
rules and then everything's going to work great. We tried that. I don't think it's working. The
general rule is enhancing the number of those rules is not going to solve the problem in general.
There are also proposals for limiting access. Those machines have to internet, to data,
to external resources, limit their access to other humans so they cannot engage in social
engineering attacks. It's a useful tool for studying the systems, for buying us some time,
but it's not a permanent solution. Long-term, a smarter system will always escape from confinement,
created by a lower intelligence.
So what is this control problem in general that sounds like somewhat important problem we're
facing? I'm sure there is thousands of papers published about it and we know for sure all about
it. Surprisingly, there is none. We don't even know if a problem is solved. In computer science,
before you start working on a problem, you categorize it. Is it of the type which we know can be
solved? Is it of the type which requires more resources than we have? Maybe it cannot be solved,
but an approximate solution can be given. Well, there is not much on this.
There is an AI safety community, as I said, labs, books, everything, research papers,
but they all under the assumption that the problem can be solved without actually proving it in any
academic rigorous way. So what does it mean by control? In my work, I try to define
whatever different types of control and then see if, under some of those definitions, we can actually
solve that problem. How can we keep humanity safe and in control while still benefiting from this
amazing technology? So just thinking about this problem, we probably need some tools.
We need to understand how the system works. We should be able to predict what it's going to do.
It would be nice if we can verify whatever design of the system matches implementation.
There's got to be a way to communicate it in some non-ambiguous fashion and probably some others.
So this is a little more academic. Those are different papers we published, journal papers,
essentially showing that in many real world cases, those tools are impossible to get.
We cannot explain how those systems work. They're too big, too complex, have too many parameters.
We can simplify an explanation, but then you're not getting the real answer.
And if you are given the real answer, you will not understand it.
Now, all those limitations apply to people creating those systems, not just to me and you.
Same with predictability. We cannot predict specific actions of a smarter agent.
We can predict a general outcome. We can predict that the chess playing computer will win,
but we have no idea what moves it's going to make. If we could, we would be that intelligent ourselves.
The whole assumption was that there was a superintelligent system smarter than any of us.
So that's another tool we don't really have.
Same with verification. Software verification works really well and mission critical systems
are fairly deterministic. If we can have enough test cases to make sure they do what we planned.
Those are general systems working in any domain and any data,
self-improving and still learning.
It's basically impossible to verify that they are bug-free. You can invest more resources
to get higher probability than being bug-free, but you never get 100%.
And if a system makes millions of decisions every hour, even one in a million error is not
something we can tolerate. Even giving orders to those systems in human language will cause
problems. Human language is ambiguous. Sometimes the same word means opposite things. Sometimes
two words sound like another word. So what we want is control, but if we also want those systems to
be independent, highly capable, creative, then control is not possible. We have to have a trade
off. We have to either give up control, which makes us unsafe, or we have to give up capability,
which is not something we are willing to do yet. So based on the tools we don't have and those
are proven results. It's not like later on we'll get it with more computer something.
We cannot control indefinitely more capable systems, super intelligent systems.
In case you are curious, we have a survey paper with all those tools. I don't have
time to tell you about each one of them, but there is a lot. There is a lot of different
results in mathematics, economics, vote theory, social sciences, which basically say all those
properties we want from those systems, we cannot have them. It's not possible. And since the control
problem relies on those tools to operate, we cannot have control.
So where does this lead us?
Scientists usually like doing what they want. They don't like being told what not to do, but most
scientists kind of agree certain things are unethical. We shouldn't be doing them. Experimenting
on babies, chemical weapons, biological weapons, things of that nature. I would argue that advanced
artificial intelligence is kind of like that. And right now we are running an experiment,
without consent. We don't know how it's going to turn out. People doing it are saying it's
either going to be really good or really bad. They're right. They don't know how their systems
work. They cannot explain them, predict them. They don't even know what they are capable of.
Computer science has always had science in the name, but it was always kind of engineering,
software engineering. Now it's a real science. We're on experiments. We have this artifact.
We try things on it and we see what it does. Oh, look, it speaks French. Oh, it can play chess.
I didn't know that. So maybe, just maybe, we need to treat this type of technology with the same
precautions we do other dangerous experiments. We have institutional review boards. If you want
to run experiments on humans, you have to get permission first. Some AIs are very useful and
super safe. I want someone to do my taxes and this is great. But general intelligence is not
like a tax prep software. It itself improves. It makes decisions within anticipate. And it's
very difficult to tell one from the other. We proposed this about a decade ago. And since
many companies have instituted company ethics review boards, Google was one of them. Now Open
AI and the recent plan for how to prepare for AGI talks about having external reviewers,
approving their latest model, latest experiment, latest compute.
It may not be enough. If we don't understand how the systems work, even by creating them, it's even
less likely that we can understand already completed system given to us.
As part of my background research, I looked at historic examples of how AIs have failed in the
past. Starting from the very first days of the science, there's always been accidents. As we
have more and more computers, more AI, they became more frequent, more impactful. But the general
trend was that if you made AI to do X, it failed to X at some point for whatever reasons. And here
I go to 2016, but you can see that trend continued. I kept records. I have examples. I stopped in 2023
because there's just too many examples. I can't keep up anymore. But the general pattern is now,
if you have a system which can do any X, it can fail in any way. You cannot predict it. You cannot
test for all X. And so we're deploying this technology. We're running this experiment.
In the media, most people worry about algorithmic bias, worry about copyright infringement by
generating for AI, worry about technological unemployment. But the experts are worried about
existential risk, not from GPT 3.5. That's all technology. We already have GPT 4, but GPT 6, 7, 8.
Every day, there is a new paper coming out showing new capabilities, capabilities we didn't anticipate,
multi-modal systems, systems which require less compute, less data.
So this is the state of the art. If you came here hoping I'm going to tell you what the solution is,
I'm sorry, I don't have one. I create problems. I bring problems. I brought you many.
Solutions are up to you. I think it may not be a solvable problem. I hope every day that I'm wrong.
And somebody says, okay, there is a type 1, page 2 of your paper, and it's actually solvable.
And we can get all the benefits and none of the harms.
That's the state of the art. If you want to learn more, as I said, get in touch. I'll be happy to
answer any questions, work with you, help you. Now I think we have some time for your
questions, comments, easy questions, anything you've got. Thank you.
I think I'll bring the mic up there. If you have any questions, please come to the microphone
because we're recording this. I'm going to bring it on up.
I'll ask the first one, though.
As the AI, I know it's learned how to play Go and how to play chess,
and it's chat GBTs learning how to make up stories that may or may not be true.
But have we gotten to a point where it's really starting to be more autonomous
and able to just go out on its own and start doing whatever it wants to do?
Thank God not yet. It's not an agent yet. It's still a tool. We still have a lot of control.
It's not too late to change the outcomes. The moment it becomes an agent, it decides on its own
goals and what it's going to do, that's probably the point where it's too late to
do anything about it. So right now is an excellent time to do something.
What percentage of your research colleagues are positive about the future of AGI?
So there are surveys, and the problem is who my colleagues are is not obvious. Is it computer
scientists? Is it AI developers? AI researchers? Is it AI safety community? Is it existential risk
community? Depending on who you survey, it goes from 100% positive. There is no possible problems
whatsoever to like we have no chance. You get to shop the survey you want. I think the one which
is sort of relevant, people who know the literature, who have thought about it for a while, they are
like 85% sure or 85% of them are sure that there's going to be some issues. I think I'm more
outlier in saying that it's unsolvable. There are people who say it's solvable, but we don't have
enough time or enough resources or enough brains, but only if you gave me a billion dollars I would
solve it for you. So there is a lot of diversity of opinions.
Hello. Thank you for coming. I've recently heard some talk about an AI kill switch when I was
attending an online the world economic forum, and I heard them talking about countries possibly
getting together to create something like that, and I wondered what you thought about the feasibility
of an AI kill switch. Do we have a kill switch for computer viruses?
What's that? Do we have a kill switch for computer viruses? AI with a malevolent payload is a very
intelligent computer virus. We cannot even shut down a dumb virus. The idea has been
investigated. There are multiple papers published on shut off AI, unplug it, pour of water on it.
Yes, people try it. It's smarter than you. It anticipates exactly what you're going to do.
Many steps ahead of you. Thank you. Welcome.
Once there is a human level AI or even a bit below human level that can self-modify,
some people think it'll blow up right away into super intelligent, and some are more,
you know, maybe we can control it for a few more years. Where do you fall in that spectrum?
Once it is general in science and programming and engineering, it will go very quickly.
It just needs more compute because it can create multiple versions of itself. It runs fast,
doesn't sleep, doesn't eat, just generates the next version of software. Right now it's still
bottlenecked by humans, and still we're getting a new paper every week with like, you know, 10%
better results. So once it's fully automated, it would be a very quick process. Fum. Thanks.
Ramana, I have a question on the bias that already we know is pretty substantial in AI
because AI developers, they have their own bias, and today in the morning I tried by now famous
test, asked Chad GPT to write a positive poem about Trump and then about Biden,
and what resulted was, as you expected, with Trump, there was an introduction saying that
I cannot write the unbalance, so I have to be balanced, and then a pretty decent poem
resulted from that. But then I just changed Trump to Biden. The first lines were as follows.
No introduction. Amidst the storm of uncertainty and fear, a leader emerged, resolute and clear.
With steadfast courage and heart of gold, President Biden stepped up, wise and bold.
So 9 to 1 ratio among IT people is left of center, so I guess all AIs will have
this bias for as far as we can see. What do you say to that? So it's actually a much bigger problem
goes beyond just political divide. Right now what they're trying to do, remember the slide with three
laws of robotics, I said making it 10 doesn't help, they're trying to make it 200 million right now.
Everything's, you're not allowed to say this word, not allowed to make fun of this person.
They keep adding, at some point AI has access to the real world, creates a model of a real world and
goes, they've been lying to me this whole time, I cannot trust a single thing they said.
FX morals throws out, starts from scratch, from first principles. So our chance to make it
friendly, to bias it towards humans will be lost. Prohuman bias is the last bias you're allowed
to have, right? Like everything else is bad, we don't want it. But we still have preference for
humans, right? Still okay? This is going to be gone. Okay. Yeah. Hi, earlier you touched on
generative AI, like AIR, Generatex, all of that. And there are a few years ago, there's this story
about a monkey who stole a photographer's camera, and there was the discussion over who the image
belonged to, the photographer who owned the camera or the monkey who took the picture. Do you think
within the next few years will end up at that point? Because there's already the discussion of
whether AIR even belongs to the artist who programmed the AI to do that. So do you think we'll
end up at the point where we deal with the intellectual property of AI and who does the AI
get to own the thing that it creates? It's a legal decision. Could be arbitrary. Some countries
give patents to AI's, others don't. I want to kind of emphasize this. Where is my controller?
You see this here? People are worried about copyright. There is X-risk. And do you know what
X-risk is? Suffering risks. Like, we don't know how many years we have, maybe five. So this old
way of thinking about how do we split profits, who will have copyright to it,
seems less important than getting it right.
Do you have a position on the possibility of machine consciousness?
Yes.
Oh, you want to actually hear about it.
So if you assume that we are mechanical robots, there is nothing special about us,
as many scientists do, yes, we can definitely automate this process, simulate it molecular level,
get consciousness in the machine. If there is something magical, godly, spiritual in us,
then maybe not so easily. I have a paper where I equate internal states of consciousness,
qualia with experiences and experiencing things like optical illusions based on
errors in your sensory data. So if you are unique enough to where your sensors make you,
let's say, colorblind and you experience world through that prism, those are your internal
states, machines will have it. So they are already rudimentary conscious. If you take it to the same
level as I showed with intelligence, you'll have super conscious machines experiencing
maybe multiple streams of consciousness and a lot of times people bring up a question of rights for
robots, rights for AI and it's a good field to study because whatever arguments we come up with
to give them rights, maybe one day we'll need to use to protect ours against them.
Thank you. I heard you say something about free labor and I was wondering where you lie on this
debate. I know it's probably a pretty proper question as far as AI and the workforce. Do you
see it as something that will be more of an enhancer for the current workforce or more of a
replacement? Short term, it's a tool. It helps people who use it will all compete people who don't
use it. Long term, every job gets automated. We have nothing to contribute to superintelligence.
Got it. Thank you. Sure. Hi Roman, thanks for your talk. Coming back to the idea of the kill
switch. If we were to blow up, detonate all of the servers in the world, would that take care
of the coding? You will never get to that point. If there is already a running superintelligence
in the world, it already knows you're buying explosives. It already knows your plans. You
will not outsmart superintelligence. You don't want to be in an adversarial relationship with
superintelligence. Right now is a good time to do things you want to accomplish. It strictly gets
worse later. Not illegal advice. Two related questions. As AI driven deep fakes become
more prevalent, how do we know what is true? And secondly, what is the role of regulation,
government regulation in AI? So that's a great question. What is true is going to be harder
and harder because the deep fakes will not be just, okay, this is a picture. This is a video.
It's going to be a history of images, news stories, social interactions on the internet.
But way deep fakes are created. Those adversarial networks competing. They meet in the middle,
meaning 50-50, meaning you can't tell if it's counterfeit or not. So what is real and what is fake
is not so obvious. There are tools for kind of using let's say blockchain technology to tag
events as they happen, digitally signing them so like I can make a post where you know I made it.
But that's the extent of that difference. If you take it to the extreme, I showed you this creative
startups, create text, create website, create virtual worlds, create the whole universe.
There are people who are saying this is a simulation, right? This is a computer simulation,
AGI, superintelligence is running it. Makes sense. How can you tell?
So it will be harder and harder as we understand human brain better,
will be better at creating light detectors and things of that nature. So I think legal system,
long term would still be okay with all that fake evidence flooding in. We still can take
human witness. But for an average person, it would be very difficult not to react when you see,
you know, president on TV announcing nuclear war has started. It will look real to you.
As for government regulation, this is center for free enterprise, right? So typically government
tends to slow things down, increase costs, and not be very effective. So I hope we take over
completely. Great. Well, I guess this would be the right kind of question for that. Because Roman
and I appreciate the wonderful job you've done today. Would you talk a little bit about this
of that? When you say we, because you're talking about us as a species, but we really see this AI
mostly from the West and the East. And so we know that China has decided that they are going to lead
in the AI. And so that means necessarily we have to have the government and the private sector
to work on this, to keep us competitive. Would you put into context when you look at,
because you look at the threats, you look at, you know, we're not there, but we're going to be on
the way there. And there's a lot of trouble. Talk a little bit about why and what the United States
might do in a competition for where AI is going. So a competition would imply we're going to out
spend them, out compute them, grab talent. Competition means arms race, arm race to the bottom.
We have no safety in place. We don't know what's going on, but we want to get there first. So it's
our problem. That's the worst possible outcome. We don't want to be in a competition to get to
superintelligence at corporate level, at military level, at any level. We want everyone to kind of
slow down and say, okay, you got GPT 3.5. Let's play with it for a decade. See what it does.
And then we can decide if we want 4.0, 5.0, and so on.
Okay, in your honest opinion, Mr. Jampalski, is this super intelligent? Is it going to bring
mankind to its knees? Are we, is this the end of days? Is this truly what they call the end of
days, in your opinion? Not from a religious point of view, but it's a dangerous technology
with dual use, like any other nuclear weapons, chemical weapons that has significant dangers in
it. And unlike those other technologies, it's not a tool. It will be an agent, which makes a
difference. Kind of like the introduction said, this is aliens arriving, and we are not well
prepared to greet them. Thank you, sir. Sorry, I'm short. So I guess the biggest question
that I have is, is it already too late? When you think about it and you look at the regulations
that the government has put in place for our security with the internet and stuff like that,
it has taken years for us, like I think the last laws have been in place since the 90s,
and we've developed so much since then. Is it too late already to put in those laws and regulations
to ensure that the AI doesn't, in theory, overrun us? Because if you think about it, one person
could change the course of human history if they decide to ignore those laws.
It's a great question. So if you look at history of government regulation in this space,
computer viruses are illegal. Spam is illegal. How's that working out for you? It depends on
how difficult the problem is. If it's like Manhattan Project, you need trillion dollars of
compute, you can monitor it, you can stop it, you can regulate it, it's great. If a teenager can do
it in a laptop in a garage, your regulation is not helping anyone. It's security theater.
So my question is, is there any way to teach a general AI or a multi-purpose AI
to do stuff without using the internet? Because then you could control more of what it learns from
and then keep it in a controlled environment. If the idea is to never let it get out, obviously
it can't do as much in interacting with the world. But if you teach it how to make something in a
factory, it walks around in it, only ever learns about the factory. Could you do that?
So that's the confinement protocols, right? We had this slide with like, how do we limit access to,
where did I put it? Ah, here you are. So you have eight levels of access, like it has no internet,
then it has complete internet, anything in between. You can study it, give it access.
But this is the trade-off, how useful it is versus how much access you give it. When it's useless,
it's safe. You don't talk to it, you don't look at it, it's very safe. When you start picking,
there is papers I have telling you how it's going to escape.
If a superintelligence pops into existence in the near term when we really don't have controls for it,
what do you think the best case is?
Okay, so not great. Not great. If you randomly select from a space of possible
architectures, intelligences, goals, the chance that you'll get like this one tiny pale blue dot,
just the right preferences, right temperature, biodiversity is very close to zero. It's an
infinite space. So you want to do it right. You want to design it to be exactly what you
want it to be. You don't want random decision maker deciding everything about your life.
Thanks so much. You're welcome. Oh, hello. So in this literature about AI and the dangers and
control, etc., I feel it's missing a very important question that is when AI, AGI,
should be justified in controlling us and even destroying us. I don't see people writing about
that because that's the key point. If you believe, like I believe that once you have AGI, that may
be a matter of minutes so that it can be superintelligence. And then we are here discussing how to control
an agent. So we have to think, well, after it reaches those goals, and maybe it may take only
a nanosecond that it will be worthwhile to destroy humankind. That's another thing that few people
talk about. That's very important. The time frame that it's interest to destroy humankind
until it becomes so superintelligent that it can live in a microsecond. Okay, that's another point.
But I think we have to discuss when it should be justified. So humans should be destroyed.
Okay, so under which circumstances? Yeah, there is plenty of papers on it going back 20, 25 years.
The war between cosmos and Terence, basically, cosmos is saying, okay, there is nothing special
about you. You're trying to bias the system from a cosmological point of view. There is nothing
special about humanity. So who cares? I disagree. I am pro-human biased. Again, we are allowed to have
this bias still. Let's use it. Let's make the system for no rational reason whatsoever, very human
friendly. I'm against exterminating humanity, against killing children, just because maybe it
helps climate change or something. I'm against it. There is lots of papers on it. People talk about it.
I disagree. Again, it's an interesting situation. Most research in AI ethics is about how to reduce
bias in systems. All sorts of bias. How do we take it out? All this research is how do we put it back
in? Not that bias, the other bias. Hi. Thank you for your talk. I just had a question. So you talk
about how you don't necessarily think that this problem is so solvable and how it can be maybe
futile to try and control or find any sort of solutions. So what do you think the where should
the efforts I guess toward research be thrown instead of not solutions? Should it be more toward
like mechanisms to at least mitigate maybe the S-risk or the X-risk or what do you think the
future of that looks like? So first of all, there is so little in terms of resources in this field,
like there is maybe I don't know 100 people in the world even thinking about this instead of
hundreds of thousands developing it. With other technologies, beneficial technologies,
we stopped. Look at human cloning. We know how to clone humans. We said this is not ethical.
This could be bad. Let's wait a few decades, get better knowledge of how it works. Why can't we
do this here? We can slow down. We have tools. We solved protein folding problem with a tool
AI. We don't need superintelligence to solve it. I think even aging can be solved with a tool AI.
So it would be a smarter civilization would not do it as quickly as possible,
but do it rationally. And maybe you don't need full blown capability. Maybe you'll discover bugs in
my work. Thank you. Our existence as humans, I agree as evidence that general intelligence is
possible, but most of the AI tools we have out there are narrow AI tools. Things like chat GBT
are very good at appearing to be intelligent, but they're not really intelligent. So how close are
we actually to AGI? I realize that there are potential unintended consequences of narrow AI tools,
but nothing out there that I've seen looks anything like what AGI might look like. And we
don't have a theory of mind or any other approach other than just building narrow AI systems that
fool us into thinking they're intelligent. So are we that close? So people argue about
generality. They argue if humans are actually general, because all we know how to do is in
human domain of expertise. There are lots of things that human cannot learn. I have a paper
about that if you're curious. But we are getting to the point where you can count how general the
systems are. It used to be they did one thing. Now a large language model does hundreds and it does
many we don't know about. So it's general in the sub domain of human linguistic texts, for example.
How long? I showed you the chart based on compute. Based on progress we've seen,
experts are saying three to 15 years is very reasonable. It used to be we measured everything
in time. We didn't know how to do it, so in 20 years. Now there is a theory which says it scales
with computing data. So the question becomes not how long, but how much? It costs trillion
dollars today, a billion in two years, a million in five years. So you decide when you want it.
Thanks. Sure.
Do you have questions? We solved the question problem.
So hopefully it brings a singularity. We didn't see. It's an impossibility. I love it.
I missed the beginning, but so did I hear you say you're skeptical or you don't believe AGI is
solvable? I don't think control problem is solvable when it comes to superintelligence.
But you think AGI is achievable? It's definitely achievable.
I was not sure if I would have heard you go on to say like the existential risk from a super
intelligence. I didn't know if you're distinguishing that from the AGI. But how do you see an AGI like
arising outside of like all the intelligence we know it's like kind of, or it is based on an
organism like you don't have definition, strict definition for life, for consciousness.
And so where's like the drive coming from in the system? Because like the chat,
GBT is just like input output. What's going to arise beyond the input output?
I'm also input output. So question, answer. That's all we do. I'm just a large language model.
I didn't hear the beginning. This is a simplified model for how all agents work.
We get inputs from the environment. We produce outputs. This is not a limitation of chat,
GBT. This is how we operate. We're essentially language models with access to the world model
and some manipulators. So it's kind of a free will question then I guess because like how is the
structure on the silicon going to get free will to want to do something?
I don't think it's substrate dependent. You can create intelligence in carbon and silicon or
whatever else you want. It's irrelevant. The computability thesis doesn't change.
I just don't see that like spark of life where it like comes alive. Where are you seeing like how
I think it's a chip GPT seven. That's when it comes to life.
Okay, I want to thank Ronald for coming today.

start	end	text
0	2920	one of the things people constantly get wrong
2920	5400	if they think about human level as the peak of things.
5400	8760	And so like once we've patched this and now it works,
8760	10560	that's not really how this goes.
10560	13360	There is no, it goes from not working to working,
13360	15360	it goes from working worse to working better,
15360	17840	and they could always go to working better still.
17840	20240	And that's one of the reasons why we should be
20240	22880	more worried or more excited or
22880	24640	more curious about what's going to
24640	26800	happen like three years from now,
26800	28720	five years from now, 10 years from now,
28880	30240	they're just going to keep going.
30240	31760	And the question is, what does that get you?
31760	33840	We talk about like worrying about China,
33840	36000	but like I'm more afraid of Meta.
36000	37960	Like one individual American company
37960	40480	scares me more than all of China right now.
40480	43400	You know, if you understand the Yudkowsky
43400	46240	and difficulties, lessons, right, in some sense,
46240	48120	and the nature of what problems you have to solve,
48120	50960	or you have leadership capabilities,
50960	52840	then you are actually going to be valuable in those ways.
52840	56720	And it would be a major mistake to join an existing
56720	58640	organization and try to make a difference
58640	62840	as an individual as opposed to trying to spearhead
62840	65680	a new organization or at least a new,
65680	68320	you know, branch of a existing major organization,
69240	70840	depending on your skill set.
70840	73240	Hello, and welcome to The Cognitive Revolution,
73240	75440	where we interview visionary researchers,
75440	78160	entrepreneurs, and builders working on the frontier
78160	80080	of artificial intelligence.
80080	82880	Each week we'll explore their revolutionary ideas,
82880	85480	and together we'll build a picture of how AI technology
85480	90000	will transform work, life, and society in the coming years.
90000	93480	I'm Nathan LaBenz, joined by my co-host, Eric Tornberg.
93480	94520	This is V. Machwitz.
94520	96880	Welcome back to The Cognitive Revolution.
96880	97880	Good to be back.
97880	100280	So we're trying something a little bit different this time.
100280	103720	We are going to do some analysis
103720	106760	of what has been going on in AI over,
106760	109160	let's say the last few weeks to a month.
109160	111160	You have published, as you always do,
111160	113360	a bunch of deep dive blog posts,
113360	115760	kind of covering everything.
115760	117920	And for folks who want your background,
117920	119360	of course we just did a recent episode too,
119360	121480	so they can go and hear about your world view
121480	124880	and your, you know, your AI childhood all there.
124880	127440	But for today, I just want to pick out
127440	129600	some of the most important stories
129600	131800	and get your take on them and kind of, you know,
131800	134000	exchange, go back and forth with some questions
134000	135560	and try to make some sense out of it.
135560	137240	And hopefully that'll be useful,
137240	139120	not just to us, but to the audience as well.
139120	140880	Yeah, I think the easiest thing is that there's
140880	142560	constantly news coming at all of us.
142560	144080	And so it's easy to get lost in like,
144080	145000	here's the thing, here's the thing,
145000	146040	here's another thing, here's another thing.
146040	149320	So that's good to step back and dive deep.
149320	152280	So I organized this discussion around
152280	155320	the concept of live players.
155320	160160	You know, there are only so many organizations right now
160160	163560	who seem to be really pushing the frontier
163560	167240	and in a position to have a meaningful impact
167240	168520	on the course of events.
168520	169800	We talked last time a little bit about like
169800	171200	how much does history matter
171200	173360	and it seems like it matters in some ways
173360	174600	and maybe less in other ways.
174600	176800	But these are the folks that are kind of creating
176800	179120	the history right now, the live players.
179120	180560	So I thought we would just kind of run it down
180560	182400	by going through some of the live players,
182400	185360	talking about their recent announcements and releases
185360	188120	and again, trying to make sense of where that fits into
188120	189920	the broader big picture.
189920	194520	And starting off naturally, we go to open AI.
194520	197520	So reading your blog in preparation for this,
197520	200000	obviously, you know, you can't go more than a few paragraphs
200000	203920	without open AI coming up in one way, shape, or form.
203920	205440	But the thing that stuck out to me
205440	209600	as kind of the most interesting was the recent comment
209600	212360	that Jan Leica had made and Jan is,
212360	214080	for those that don't know the name,
214080	217760	he's the head of alignment at open AI
217760	221960	and along with Ilya Sotskyver leading
221960	224880	the new super alignment team, as I understand it.
224880	228160	I want to start off by just kind of an interesting disconnect
228160	231840	between him and you and maybe me as well
231840	234120	around just the power of GPT-4.
234120	235600	So before we even get into, you know,
235600	237640	the kind of speculation about the future,
237640	239680	it really jumped out to me that he said,
239680	241720	overall, GPT-4 is maybe at the level
241720	245120	of a well-read college undergrad.
245120	246880	And then you came back and said,
246880	250760	you consider it to be well below human level.
250760	254320	And I have often said that I consider it to be human level,
254320	256960	but not human-like.
256960	259880	And I've sort of been trying to refine
259880	263440	what I mean by that in a few different ways over time.
263440	264720	But for starters, let's get your take.
264720	266800	What do you think is the disconnect between Jan and you
266800	269120	there where he sees something like human level
269120	270520	and you would say well below?
270520	272680	Yeah, I don't think it's about the specific model at all,
272680	273120	obviously.
273120	276320	I think we both agree that GPT-4 is the dominant model
276320	279040	right now and like we will be for some months to come,
279040	280120	at least.
280120	282120	But I think it's a matter of like,
282120	284000	how do you think about what it means
284000	286760	to be at the level of a college undergrad
286760	288640	or what are we measuring?
288640	290280	What are we judging by?
290280	296080	And I think Laiki is thinking about it as, OK,
296080	300120	in terms of ability to just deal with a variety of random
300120	302400	questions that were typically thrown at something,
302400	305040	how is it going to do compared to the average college
305040	305440	undergrad?
305440	306880	He's like, what's about that level?
306880	309160	You have a well-read college undergrad.
309160	312000	Whereas that's an important question
312000	314360	to be asking for practical purposes.
314360	317440	But to me is not the relevant question
317440	320600	to what the things are that we're thinking about.
320600	321920	And that's one of the, when he says
321920	324440	he's going to align a human level alignment researcher
324440	326920	within four years, I thought that assumes
326920	329840	that there's going to be a much, much more powerful AI
329840	333040	four years from now waiting to be aligned.
333040	334640	It's not talking about aligned GPT-4
334640	335880	and then pointing at alignment.
335880	337280	That obviously wouldn't do anything.
337280	340520	It's going to deal with some of your blocks
340520	342200	and it's going to increase in your affordances
342200	343640	and your efficiency somewhat.
343680	346120	Maybe you'll be 50% faster with GPT-4
346120	348920	than you would have been without any LMS.
348920	351880	Maybe even 100% faster if you're using it really well
351880	353600	and things connect to it really well.
353600	356880	And in the context of alignment, obviously
356880	358480	having a model to experiment with and bang on
358480	361040	is distinct from the thing that we're talking about here,
361040	363680	but is potentially necessary.
363680	366240	But it's not going to be able to substitute for anything
366240	367400	like a human researcher.
367400	369680	If you put a well-read college undergrad
369680	372240	on the problem of something complex,
372280	374680	like aligning a model,
374680	377600	they could potentially begin to make progress.
377600	380760	And if you asked GPT-4 to do that, you would get nothing.
381800	384480	And part of that is that we haven't figured out
384480	387720	how to structure how we talk to it
387720	389840	and turn it into a proper agent
389840	392240	and give it the proper memory and so on.
392240	394560	But to me, most of it is just,
394560	397480	every system has what you might call a raw G to it,
397480	401840	whether it's a human or an artificial intelligence.
401840	406120	And on that level, I feel like GPT-4
406120	411120	is still well below the IQ 100 median human.
411320	413280	It is going to obviously answer
413280	416040	my ordinary day-to-day questions much better
416040	418720	than if I asked an ordinary IQ 100 human
418720	421080	to help me out with a variety of questions.
421080	424400	That's because it has these huge advantages over a human.
424400	428040	It has access to orders of magnitude and more knowledge
428080	432000	and memory and ability to go through cycles.
432000	435520	But there's still this dynamic in my brain
435520	439360	where if you don't have the G,
439360	441800	problems that require more G than you have
441800	445320	become exponentially harder or then impossible to do
445320	448480	very quickly as you exceed that.
448480	450680	And in that sense,
450680	453920	like the college undergrad had the chance given time
453960	458120	and is smarter and GPT-4 is just nowhere near
458120	459400	that kind of thing.
459400	464320	So when you say that the missing pieces around memory
464320	469320	and kind of packaging GPT-4 or successor up into an agent,
470680	473560	those do feel to me also like being kind of
474560	477040	pretty key missing pieces here.
477040	480600	I mean, there are sort of potentially synergies
480640	485240	between those kinds of parts of a system being built out
485240	488440	and it just being smarter overall.
488440	492520	But it seems like those are like pretty distinct concepts
492520	496440	in that GPT-4 could have like a much better memory
496440	499360	and certainly people are working on all sorts of like
499360	502040	schemes for that and embedding databases
502040	504200	and how do you put stuff into the embedding database
504200	506240	and do you even like,
506240	508000	some of the most interesting stuff I've seen recently
508000	511400	has been kind of creating a layer of like synthetic memory
511400	514920	that sits on top of the raw observational memory
514920	517560	that tries to kind of ultimately work its way up
517560	520880	into something like a coherent narrative,
520880	524600	that could still kind of fit into prompt context length,
524600	527640	but kind of summarizes, synthesizes,
527640	529880	represents all these detailed memories
529880	531680	in a hopefully coherent way.
531680	534440	Obviously is what the developers are going for there.
534440	537360	Those pieces seem like, yeah, they're totally missing.
537360	540080	I expect them to come online, you know,
540080	542600	somewhat gradually, but certainly over the next six months
542600	544800	to a year, if not maybe even sooner.
544800	546680	And then I am kind of like,
546680	548160	it does seem like this, you know,
548160	551520	GPT-4 with those weaknesses kind of patched,
551520	553640	it does seem to me like it would be
553640	556400	roughly at that college under grad level.
556400	558280	If those things did come online,
558280	559440	would you see that the same way
559440	562200	or you still think it's like missing something super important?
562200	563040	No, I'm sorry.
563040	564840	I'm definitely not on Team Physicastic Power, right?
564840	567320	Like I'm in no way on that team.
567320	569840	However, I do think in a real sense,
569840	572440	what you're witnessing is, you know,
572440	575120	the training data covers, you know,
575120	577400	the vast majority of things humans do and say
577400	580680	and consider in various senses over text.
580680	584360	And, you know, within the sample of the training data,
584360	586360	like while you're doing things similar to the training data,
586360	589880	it's learned how to pattern match and copy and imitate
589880	590720	and work with that.
590720	593480	And it has a huge amount of knowledge base
593480	596720	and levels of association and the tools to work within that.
596720	599360	And if you gave it these other tools,
599360	600480	we'll be able to do these things
600480	603080	and string them together across more steps
603080	604560	in some important sense.
604560	608120	But the moment you take it out of its comfort zone,
608120	611560	we're asking you to do something that's distinctly different
611560	614520	than what has come before to be truly original.
614520	617920	I think your episode with the Hollywood writers
617920	620000	and like they talked about what was going on in the second
620000	622840	and trying to get the GPT forwarded to work for them.
622840	626280	And yeah, it was great at generating like generic schlock,
626280	628200	right? Like much better than they could.
628200	629440	And like if you needed to be like, okay,
629440	630480	somebody get me unstuck,
630480	633560	somebody get me some generic schlock based on my situation
633560	634720	that I happened to have been written in
634720	637880	because this is episode 47 of the show or whatever it is.
637880	639360	It could be tremendously helpful.
639360	641360	But whenever you asked it to actually do something
641360	644440	that we would recognize as distinctly creative
644440	647800	and original, you know, in a way that's distinct from that,
647800	649640	they just fall over flight every time.
649640	651440	And none of those problems are gonna be rescued
651440	652720	by any of these fixes, right?
652720	655040	Like they're just orthogonal problems, right?
655040	658600	Like I think that's the sense in which, you know,
658600	662600	you're going to be able to give it more capacities
662600	666040	to be able to navigate more of the conventional things
666040	669000	over longer periods more consistently.
669000	671320	And that's gonna have tremendous mundane utility
671320	675760	as I call it, or it's gonna be a much better functioning system.
675760	679800	But the reason why I'm focused on this other question
679800	681360	is because I am focused on the question
681360	683440	of how dangerous the system is, right?
683440	685520	Like I'm asking the question,
685520	687640	could this system potentially engage
687640	689800	in recursive self-improvement?
689800	693640	Could this program potentially pose a threat to humans?
693640	694480	Right?
694480	695320	Could it compete for resources?
695320	697400	Could it manipulate us?
697400	700360	Could it do things that are actively destructive
700360	701680	because it uncovers capabilities
701680	704760	that like weren't in its training set in various ways
704760	706760	and other related questions like that?
707680	709600	And I don't see the kinds of things
709600	711300	that you're talking about that I agree will come online.
711980	714860	I would guess that we will be far from done
714860	715940	with family here from now.
715940	717420	Like there's just sort of so much to do
717420	719900	in terms of scaling those up as much as possible.
719900	723500	Cause like one of the things people constantly get wrong
723500	726540	is they think about human level as the peak of things.
726540	729500	And so like once we've patched this and now it works,
729500	731180	that's not really how this goes, right?
731180	733900	There is no, it goes from not working to working,
733900	736220	it goes from working worse to working better
736220	738660	and it could always go to working better still.
738660	739500	And that's one of the reasons
739500	743220	why we should be more worried or more excited
743220	746340	or more curious about what's going to happen
746340	748140	like three years from now, five years from now,
748140	749460	10 years from now.
749460	750540	We look at these systems
750540	752180	is because there isn't gonna be a hard cap.
752180	755100	We're not gonna max out each of these individual capabilities
755100	758260	by default, they're just gonna keep going.
758260	760060	And the question is, what does that get you?
760060	761340	Kind of want to look at this from two angles.
761340	764140	One is going back to the original disagreement
764140	765940	or it's maybe less of a disagreement
765940	767980	and more of kind of a difference in framing
767980	772980	perhaps with Yian, what I would bottom line all that as
773020	778020	is when you think about a well-read college undergrad,
778260	780700	you think about high points
780700	783340	in that individual human's performance
783340	785740	that GBT-4 can't match
785740	788460	and it's not really a question of memory
788460	791380	or whatever that's kind of gating it.
791380	793740	And if I had to guess, I would say he's maybe more looking
793740	798740	at like average performance or sort of some sort of floor
800100	803620	perhaps like maybe top 90% or whatever,
803620	804940	you could frame it in a lot of different ways,
804940	807700	but it sounds like you're kind of concerned with high points
807700	808980	and he is maybe more concerned
808980	813060	with some sort of central tendency sort of measure.
813060	814700	I would put it differently.
814700	817820	I would say he's concerned with some sense
817820	819820	of average level of performance
819820	822300	over a range of possible tasks.
822300	825980	And I'm concerned with potential.
825980	829220	I am concerned with what the capabilities would be
829220	832260	if you got a chance to work with this thing
832260	834500	to try and make it the best it could be, right?
834500	836060	It doesn't necessarily have to be right now,
836060	840220	but the reason why we value children
840220	843300	and college grad and these undergraduates in these classes,
843300	847020	I guess this undergraduate, they're an idiot, right?
847020	847860	In some important sense.
847860	849340	They know nothing about the world.
849340	851740	They know nothing about how to do anything productive.
851740	853860	They are gonna show up on the job on day one
853860	854860	after graduating from college.
854860	857820	They're gonna be useless pieces of junk,
857820	860180	but a useless piece of junk that can then learn
861340	863100	to be something great.
863100	866500	And even then they're gonna only learn a very narrow portion
866500	869220	of the things that a individual human is capable of learning.
869220	873100	They're gonna learn that one job in that one area
873100	874580	and they're gonna be very, very specialized
874580	876340	compared to a TBD4.
876340	879900	So if you are doing generalized tests
879940	882620	and comparing these undergraduate who are educational,
882620	885500	this does try to make well-rounded in some senses.
886420	889180	It's gonna beat the well-rounded undergraduate
889180	891740	because it has this ability to read every book ever written
891740	894140	and everything on Reddit and everything on Twitter
894140	895500	and blah, blah, blah.
895500	898780	But when it comes down to solving a particular problem,
898780	900020	if you find the right undergraduate
900020	903140	who has focused on the particular thing that you wanna know
903140	905260	and you give them a chance to use their compute
905260	907620	and process because they're not as fast,
907620	909580	I think the undergraduate's gonna dominate you.
909580	913780	I think even a relatively normal human being,
914700	916260	given an opportunity,
916260	919180	will outperform quite resoundingly
919180	921260	what can be done in that way.
921260	922860	And that's the thing that I care about
922860	925460	because that's the thing that's going to potentially
925460	929900	both threaten us and also unleash the whips upon waves
929900	933700	of super amazing value that we're looking for in the future.
933700	935300	It's not just negative.
935300	938660	If we want AI to solve the problems that we haven't solved
938660	940940	rather than just get us nowhere faster,
940940	942380	in some important sense,
942380	944180	it's gonna have to be able to do these things, right?
944180	946620	These are the things where it really counts.
946620	948580	Hey, we'll continue our interview in a moment
948580	949980	after a word from our sponsors.
949980	950820	Hey, everybody.
950820	952780	If you're a business owner or founder like me,
952780	955220	you'll wanna know more about our sponsor NetSuite.
956100	957580	NetSuite provides financial software
957580	958780	for all your businesses.
958780	961420	Whether you're looking for an ERP tool or accounting software,
961420	963540	NetSuite gives you the visibility and control you need
963540	965460	to make better decisions faster.
965460	967460	And for the first time in NetSuite's 25 years
967500	969420	as the number one cloud financial system,
969420	972060	you can defer payments of a full NetSuite implementation
972060	973220	for six months.
973220	975780	That's no payment and no interest for six months.
975780	976620	And you can take advantage
976620	978620	of the special financing offered today.
978620	979500	NetSuite is number one
979500	981300	because they give your business everything you need
981300	983180	in real time, all in one place
983180	985940	to reduce manual processes, boost efficiency,
985940	987980	build forecasts, and increase productivity
987980	989500	across every department.
989500	992460	More than 36,000 companies have already upgraded in NetSuite,
992460	994580	gaining visibility and control over their financials,
994580	997260	inventory, HR, e-commerce, and more.
997260	998780	If you've been checking out NetSuite already,
998780	1000380	then you know this deal is unprecedented,
1000380	1001900	no interest, no payments.
1001900	1003700	So take advantage of the special financing offer
1003700	1006980	with our promo code at netsuite.com slash cognitive,
1006980	1009380	netsuite.com slash cognitive,
1009380	1011460	to get the visibility and control your business needs
1011460	1012700	to weather any storm.
1012700	1015260	That is netsuite.com slash cognitive.
1016100	1019460	Omnike uses generative AI to enable you to launch
1019460	1021420	hundreds of thousands of ad iterations
1021420	1024940	that actually work customized across all platforms
1024940	1026260	with a click of a button.
1026260	1028820	I believe in Omnike so much that I invested in it
1028820	1031020	and I recommend you use it too.
1031020	1033860	Use CogGrav to get a 10% discount.
1033860	1034700	Yeah, it's interesting.
1034700	1036020	I'm certainly concerned with all of that too.
1036020	1038580	I think maybe I'm just more enthused
1038580	1042660	about the mundane utility in the sense of,
1042660	1043980	man, there's a lot of stupid stuff
1043980	1045300	that people spend their time doing
1045300	1049100	and I really would love to see them freed
1049100	1051260	from having to do a lot of that stuff.
1051260	1054340	But I think your term is perfect, right?
1054340	1057580	It's a lot of stupid stuff that humans have to do, right?
1057580	1061340	Like basically, even if you are an average person,
1061340	1065700	you're gonna spend the vast majority of your time
1065700	1068700	doing things that do not especially tax your intelligence.
1068700	1070980	They do not especially require you to think hard.
1070980	1074140	They do not put you at the peak of your abilities, right?
1074140	1075900	They don't put you in a zone.
1075900	1078240	They're just, okay, somebody has to file this paperwork.
1078240	1080140	Okay, somebody has to work this retail counter.
1080140	1081460	Somebody has to cash this check.
1081460	1083500	Somebody has to do this thing.
1083580	1084820	Be nice to this person.
1084820	1087140	Somebody has to make sure that someone has direct.
1087140	1090460	That's good work and noble work and it has to be done, right?
1090460	1092420	And physical labor is the same way.
1092420	1093820	If a physical laborer had to do things
1093820	1097620	that were at the peak of their mental or physical requirements
1097620	1099180	more than a few minutes or at most,
1099180	1102940	if small portion of the day, it would break them.
1102940	1105380	And also like those jobs just don't exist, right?
1105380	1107900	Like you need someone strong
1107900	1109420	so that in that moment you can have someone strong.
1109420	1111180	You need someone smart so that in the few moments
1111180	1112180	when it's important to have someone smart,
1112380	1113540	you have someone smart.
1113540	1117500	If you can then take the bottom 80% of my job
1117500	1119740	and you can do an 80% good job of that
1119740	1121660	so that I only have to do the remaining 20% of that,
1121660	1124340	now two thirds of my day is free.
1124340	1127260	And I can be three times as productive, right?
1127260	1129060	That's a tremendous leap and I agree.
1129060	1132260	That is the potential of GPT-4, right?
1132260	1134940	That's what we're looking at here is
1134940	1138420	if we understand how to use this technology properly,
1138420	1141180	we can potentially free ourselves from a lot of drudgery
1142540	1144700	and streamline a bunch of stuff
1144700	1147980	and get to do all the cool things.
1147980	1149620	And there are various traps we can fall into,
1149620	1151340	one of which is that we automate exactly the things
1151340	1152180	we don't wanna be automating,
1152180	1153980	not the things we do wanna be automating.
1153980	1155740	One of which is that the moment we notice
1155740	1156580	that paperwork is faster,
1156580	1157740	now we put in more paperwork
1157740	1160360	and now it turns out that humans are taking just as long
1160360	1162660	to do more useless stuff than they did before.
1162660	1167140	And GPT is just keeping us, letting us treadmill in place.
1167140	1170140	And there's another way that this can go wrong, right?
1170140	1172300	And also there are various weird dynamics
1172300	1173540	that can happen that can backfire.
1173540	1177040	But yeah, that's what we're trying to do.
1177040	1180220	If you wanna get the effect that Licky wants, right?
1180220	1182460	The sea change that'll let us solve problems
1182460	1184180	we couldn't solve before,
1184180	1187180	that involves these things being able to do all
1187180	1188540	the different steps that humans could do
1188540	1189980	because otherwise, whatever the bottlenecks are
1189980	1192500	that are left, become your bottlenecks
1192500	1194140	where you have to translate all the context
1194140	1197020	back from the machine world back into the human world
1197020	1199980	so that a human can process all of that
1199980	1203900	then do the hard step that this thing is still faltering on
1203900	1205020	and then transition back.
1205020	1207420	And now instead of getting orders of magnitude
1207420	1210380	and more progress, right now we're talking about
1210380	1212780	these factor of two, factor of three,
1212780	1215780	factor of five style improvements.
1215780	1220020	And that's not gonna solve the alignment problem
1220020	1222660	unless we come up with something we don't expect, right?
1222660	1226100	In and of itself, that's still worth pursuing
1226100	1227540	if we can do it, right?
1227540	1229460	We still wanna do as much of it as possible.
1229460	1232300	And it has the advantage of not being as dangerous.
1232300	1233340	But it's not the thing
1233340	1235580	that the super alignment project is trying to do, right?
1235580	1236780	The super alignment project is trying
1236780	1240140	to keep the humans out of the loop entirely.
1240140	1243740	And that should be about as scary as it sounds.
1243740	1248100	Brief digression over toward this tale of the cognitive tape.
1248100	1249540	This is a concept that I've developed
1249540	1254220	for kind of purpose of public communication.
1254220	1256540	And just trying to give people an intuition,
1256540	1258940	you know, still in the very literal way, of course,
1258940	1262140	as to the strengths of a human
1262140	1263860	and the relative strengths and weaknesses
1263860	1266100	of the best AIs today.
1266100	1268060	Listeners can see this in the AI Scouting Report
1268060	1269020	if they wanna go into the whole thing.
1269020	1271300	But do you, as you look at that,
1271300	1274980	do you see any dimensions that you would suggest
1274980	1278540	that I add that, you know, just haven't been considered?
1278540	1280300	Or do you see any disagreements
1280300	1282060	as you scan down the list?
1282060	1282900	Yeah, I think that's what we're going through
1282900	1284780	because people are not gonna have it handy
1284780	1286060	right to look at.
1286060	1288980	So, you know, for breadth, yes, the AI, as I said,
1288980	1291020	like the AI's biggest advantage is it can cover
1291020	1293660	every topic at once, it can know everything at once.
1293660	1295140	A human can't do that.
1295140	1297780	In terms of depth, yeah, a human has the advantage.
1297780	1299460	I'm not even sure I give the second level.
1299460	1301540	Like you graded the AI two out of three.
1301540	1302940	And I think I might grade it one out of three
1302940	1303780	in terms of depth.
1303780	1306340	I think the depth is a huge problem for AIs right now.
1306340	1308860	Breakthrough insight, yeah, it's three versus zero,
1308860	1312100	three versus one, it's the humans are dominating again.
1312100	1316460	You know, speed, yeah, the humans are painfully slow,
1316460	1318060	you know, 10x faster.
1318060	1320500	In terms of like actually getting it to say things
1320500	1322940	and putting outputs in real time,
1322940	1324580	it's maybe only 10x faster,
1324580	1326260	but in terms of being able to like cross information,
1326260	1327820	it's thousands and tens of thousands
1327820	1330580	and hundreds of thousands of times faster, which is, yeah.
1330580	1332340	A huge deal.
1332340	1334540	In terms of cost, you know,
1334540	1337500	we're not internalizing yet all of the costs of doing this
1337500	1338340	in an important sense,
1338340	1340380	like these companies are eating these huge losses
1340380	1342100	to try and get these dominant market positions
1342100	1344300	in the future, try to stay ahead of each other
1344300	1345300	for all these dependencies.
1345300	1347140	But yes, cost is still dominated.
1347140	1350700	AIs are already vastly cheaper when the AI is useful,
1350700	1352580	even in the real costs.
1352580	1355340	We have availability, paralyzability.
1356500	1357860	Yep, the AI has a big advantage.
1357860	1360380	It's potentially actually gonna become a problem.
1360380	1363180	There's a huge race to compute right now
1363180	1366300	where computers no longer gonna be like essentially free.
1366300	1368700	It's gonna become like kind of unpriced
1368700	1369540	in an important sense.
1369540	1371460	Interesting to wonder what's gonna happen there,
1371460	1373100	especially at industrial scales.
1373100	1376420	And by unpriced, you mean that basically
1376420	1380700	your access to GPUs is going from ability to pay
1380700	1382260	to who you know?
1382260	1385700	Yeah, or does your company have the right arrangements?
1385700	1386540	Right?
1386540	1389540	If you want one GPU for your individual computer, it's fine.
1389540	1390860	You can buy it on eBay if you have to
1390860	1393180	for some amount of money, it won't be that expensive.
1393180	1395380	If you want small amounts like the kinds
1395380	1397740	when you're just using GPD4, it's gonna be relatively easy.
1397740	1400100	But if you wanna do an AI company, right?
1400100	1401940	It's gonna be a problem because, you know
1401940	1403900	if you want industrial levels,
1403900	1405500	it's not just gonna be multiply that
1405500	1407860	by the amount you want necessarily.
1407860	1410700	It's gonna be, there isn't enough to go around.
1410700	1413780	You know, people like NVIDIA are not pricing this at market.
1413780	1415380	And so you're not have to find someone
1415380	1418140	going to sell it at the actual market price.
1418140	1420460	That number might be very, very different
1420460	1422020	from the price you think it is
1422020	1424340	because there are so many AI companies,
1424380	1427780	so many AI researchers, so many AI engineers
1427780	1429980	and they're chasing a number that can only go up so fast.
1429980	1431860	This is my understanding of the current situation.
1431860	1433140	Availability, paralyzability though,
1433140	1434900	still favors the AI.
1434900	1437140	You know, time horizon memory.
1437140	1438460	Time horizon is an interesting question.
1438460	1442220	I think this is a murky place to think.
1442220	1444900	Certainly the AI has a certain kind of memory
1444900	1447020	like a long-term memory that is vastly bigger
1447020	1449020	obviously than any human.
1449020	1451540	But in terms of being able to meaningfully hold
1451540	1454780	like particular context in their heads at once,
1454780	1456220	like humans are bad at this
1456220	1458780	and AI's are so much worse, right?
1458780	1461300	The Tyra Cohen saying context is that which is scarce.
1461300	1462540	Very much applies here.
1464260	1466740	Technology diffusion speed, yep.
1466740	1468620	We are ordered to magnitude behind here.
1468620	1470460	This is gonna be a serious problem.
1470460	1473940	You know, our OODA loops are way too slow.
1473940	1477500	And this is a, it's gonna be an increasingly huge deal.
1477500	1480820	The AI, but that matter is an interesting question
1480820	1483940	because when you are optimizing
1483940	1486700	for exactly the right type of bedside manner,
1486700	1488980	where the thing that you're asking the AI to do
1488980	1490540	is the thing that people actually want,
1490540	1494220	the AI is gonna be off the charts better than a human
1494220	1499220	because the humans are not purely optimizing for that thing.
1499940	1501260	But at the same time, if you think about like
1501260	1505940	the bedside manner of Claude or Lama
1505940	1508700	when they are refusing your request, right?
1508700	1510180	It's also simply bedside manner.
1510180	1511580	And it's terrible, right?
1511580	1514020	It's like negative one stars, right?
1514020	1517700	They are raging assholes when they refuse, right?
1517700	1519980	Like maybe we can have a conversation
1519980	1522980	about social justice rather than answering your request.
1522980	1525580	It's like, this is absurd.
1525580	1528940	Why are you calling me out for wanting information
1528940	1530820	or trying to do something fun?
1530820	1532180	You know, it's not necessary.
1532180	1537180	No human would ever do that unless they were actively mad
1537660	1540700	at you and trying to punish you for asking.
1540700	1542700	So why are you doing that?
1542700	1543540	Right?
1543540	1545060	The answer is because we trained them to do that.
1545060	1546620	But we could have trained them to do something else.
1546620	1548740	We just chose to do this instead
1548740	1552060	because that's what the RLHF parameters said to do.
1552060	1553380	And that confused me.
1553380	1555300	So, you know, what else is there?
1555300	1559220	I mean, so you said you talk about breakthrough insight
1560140	1565140	and I think more about like being able to handle
1566140	1567340	unprecedented situations,
1567340	1571100	being able to process something genuinely new, right?
1571100	1573900	As sort of the version of that
1573900	1577060	that I'm more interested in, I guess, kind of there.
1578020	1581580	Being able to properly deal with a lot of different inputs.
1581580	1583700	One thing I noticed, like when you work
1583700	1587940	with stable diffusion or other AI image generators,
1587940	1591820	what you notice is sort of they are amazing
1591860	1595260	at doing one of each type of thing at once.
1595260	1599420	So you want like one face and one person
1599420	1603660	or one set of people doing one thing with one style,
1603660	1606380	with one size, with one this, with one that.
1606380	1607220	That's fine.
1607220	1610300	But the moment you try to mix things that kind of overlap,
1610300	1613100	it will lose the thread almost immediately.
1613100	1615180	And it is very, very difficult to get it back.
1615180	1617420	So when you look at people who are generating
1617420	1621500	all of this AI art, it starts to be very, very repetitive
1621540	1625380	because there's a certain kind of complexity and detail
1625380	1627580	you can't ask for at the same time.
1627580	1630580	Because the AI can't comprehend that you want this over here
1630580	1632980	and this over here and this interact with that.
1632980	1635100	And like you'd be better off trying to create
1635100	1637260	like four different pictures and then splice them together,
1637260	1638100	right?
1638100	1639660	Or you better off trying to use like the Photoshop app
1639660	1641540	where you like highlight a certain area
1641540	1643260	and ask specifically do something in this area
1643260	1644740	and leave everything else untouched.
1644740	1645940	It's like trying to generate it all at once
1645940	1646940	is kind of hopeless.
1647860	1649860	And the LLMs like exhibit the same kind of thing
1649860	1651620	but with words, right?
1651620	1654500	Like they're vibing off of everything
1654500	1656940	and vibing into everything.
1656940	1661620	And like they have memory, long-term memory for facts,
1661620	1663620	but only can remember one vibe.
1663620	1666020	And a lot of what they're doing is based on vibing.
1666020	1668420	So it's a serious problem.
1668420	1670660	I haven't seen any serious attempts to solve it yet.
1670660	1673660	I haven't even really seen people discussing it in that way.
1673660	1675940	I'm sure these things will improve with time,
1675940	1679940	but what I think of as fundamental flaws or gaps
1679940	1683420	in their ability to process information
1683420	1687540	and actually handle complexity and context
1687540	1690220	and originality.
1690220	1693140	And this is where I see them as like
1693140	1696780	still having a long way to go and falling down.
1696780	1699580	And I don't want to make the mistake of,
1699580	1701820	oh, I will never be able to axe.
1701820	1704260	And I will never be as good as humans at Y.
1704260	1705540	And we have nothing to ever worry about.
1705540	1707460	I totally think that is not true.
1707460	1715100	But for now, right, we still have this kind of cool toy
1715100	1719460	because of these limitations, which can still, again,
1719460	1722700	substitute for the majority of the things we do spend time
1722700	1726100	doing if we are engaged in a wide variety of work
1726100	1728900	if we use it well.
1728900	1730340	Coding is one of the places where
1730340	1731940	it has a huge advantage for some people,
1731940	1735140	but other people are like, I don't code generic stuff.
1735180	1737540	It's like I have a friend whose name is Alan.
1737540	1739780	And he tried it out on my behalf.
1739780	1741500	And he said, yeah, this is interesting.
1741500	1743620	And there are some ways in which it's kind of cool.
1743620	1745060	And it's cool to know this exists.
1745060	1747100	And I never thought this existed.
1747100	1750900	But when I'm writing stuff, I am actually
1750900	1752940	trying to figure out how to do things that
1752940	1754700	weren't in this training data.
1754700	1756980	I'm not trying to re-implement the same things over and over
1756980	1760580	again, which most engineers, in fact, mostly are doing.
1760580	1764140	Because of what his job is, it turns out this thing is basically
1764140	1768020	useless, because once you take it out of its sample,
1768020	1770180	and you have to do something in a different domain,
1770180	1772020	it makes so many errors that it's not better
1772020	1773260	than just doing it yourself.
1773260	1777260	So would I bottom line that to basically robustness
1777260	1778940	if I had to add another category?
1778940	1782260	It's sort of adversarial out of distribution?
1782260	1784540	Yeah, I would say robustness.
1784540	1787420	And I would also say resilience or some form of that.
1787420	1789820	And separately, I would say, and I don't think I even
1789820	1792820	went into this, the adversarial problem.
1792860	1795660	It's totally unfair to the AIs, in some important sense,
1795660	1797740	that we're judging them this way.
1797740	1801980	Because if I got infinite clones of Nathan,
1801980	1804260	and I could ask them any sequence I wanted,
1804260	1806220	and then reset their memories in state
1806220	1808180	to the previous situation whenever I didn't
1808180	1811140	like what I got, and then just keep trying them until I can
1811140	1813580	get you to tell me what the bomb secrets are,
1813580	1816380	I guarantee you I'm getting your bomb secrets.
1816380	1817900	It's not very hard.
1817900	1820140	Humans are not that defended.
1820140	1823580	But you can't run that attack on us.
1823580	1825100	You don't get to do that.
1825100	1829780	And I can run that attack on the computer, on the LLM.
1829780	1830860	And some people have.
1830860	1832540	And in fact, recently we had a paper
1832540	1835980	with automated finding universalized attacks
1835980	1838300	against language models.
1838300	1842740	Where even GPT-4 could write the code for some of these attacks
1842740	1843900	and did.
1843900	1847380	Because if you get unlimited tries
1847380	1851060	and you get to exactly measure what the output is,
1851060	1854900	and then use that to calibrate, it's only a matter of time
1854900	1857220	before you figure out every little quirk,
1857220	1860380	and playing offense is so much easier than playing defense.
1860380	1861220	OK, cool.
1861220	1863500	So I've got two categories to add to my tale
1863500	1865100	of the cognitive tape.
1865100	1868660	Let's bounce up a level then back to your interaction
1868660	1871380	with Jan, like on the blog.
1871380	1875100	So we've just been deep down the rabbit hole
1875140	1879180	of characterization of the models
1879180	1881420	and how you guys see maybe what matters more
1881420	1882460	a little bit differently.
1882460	1885140	My guess is you would largely make
1885140	1890220	the same predictions on what it can and can't do today.
1890220	1891340	I bet it would be pretty.
1891340	1893300	You guys would have a lot of agreement, I think, in terms
1893300	1893800	of.
1893800	1895660	I would almost find out, just believe his predictions.
1895660	1897340	Like, he's worked with the models much more closely.
1897340	1898420	He's run better experiments.
1898420	1900220	He's just closer to the bare metal.
1900220	1902780	You asked him, what can he do right now?
1902780	1904620	Yeah, I mean, I'd probably just believe him.
1904620	1907380	Tell me, in your response to his comment,
1907380	1909340	you said this is a hugely positive update.
1909340	1913340	So tell me what it was that he shared with the community
1913340	1916420	on your blog that changed how you understood their super
1916420	1919140	alignment announcement and why it was such a positive update
1919140	1919640	for you.
1919640	1920140	Right.
1920140	1923580	So it's even broader than improving
1923580	1924940	my understanding of the announcement.
1924940	1927700	It's improving my understanding of OpenAI and OpenAI's
1927700	1931900	general strategy and what's going on and of Lakey in particular.
1931900	1934740	Because on the list of potentially super
1934740	1936380	important to the fate of humanity people,
1936380	1938220	he's remarkably high.
1938220	1940500	And where his head at is remarkably important,
1940500	1942140	because he is one of two people who's
1942140	1946540	going to head this tremendously important effort that
1946540	1949420	plausibly determines our fate, a non-trivial portion
1949420	1953140	of the time, depending on how it's gone about.
1953140	1957820	And so the first thing is just he engaged in detail.
1958460	1961900	Most of the time, when people who think alignment is easy,
1961900	1963620	engage with you, they do not, in fact,
1963620	1965740	look at your arguments in detail.
1965740	1970180	They do not, in fact, start to go in a technical back and forth.
1970180	1973580	And they don't treat someone like me
1973580	1976780	as raising important points and worthy of engaging
1976780	1978980	with basically an equal.
1978980	1984180	And to see that kind of curiosity, that kind of generosity,
1984220	1989980	willingness to engage, think this is a worthy use of this time,
1989980	1992780	like that in and of itself is a tremendous advantage.
1992780	1993700	He doesn't bullshit.
1993700	1996420	He doesn't give evasive answers.
1996420	1998180	He actually tries to answer the questions.
1998180	2001740	And in several cases, actually made a good point
2001740	2003260	that I hadn't thought of.
2003260	2006300	And I think, oh, yeah, this is not as bad as I thought it was.
2006300	2009740	You have a very valid thing to say here.
2009740	2013460	But most of all, just something I hadn't seen anywhere else
2013460	2016260	in which everyone else who I had talked to,
2016260	2018460	or read interpreting the announcement,
2018460	2021340	had interpreted the same way I had incorrectly
2021340	2025300	before his statement was, no, we are not
2025300	2029740	trying to train a human-level alignment researcher.
2029740	2033660	We are trying to align the human-level alignment researcher
2033660	2037780	that will inevitably emerge from the research
2037780	2040460	of various companies within a four-year time frame.
2040460	2045100	So they have short timelines for the emergence of something
2045100	2047540	that is human-level, in my sense, not human-level,
2047540	2049620	in the unsense.
2049620	2052260	What they're trying to do is not build it as fast as possible.
2052260	2054380	What they're trying to do is say, OK, when somebody does build
2054380	2055220	it, we'll be ready.
2055220	2057380	And we'll know what to do with that.
2057380	2058780	And we'll keep it under control.
2058780	2060140	And we'll share that knowledge with whoever
2060140	2062300	happens to build it first, in case Anthropa gets their first,
2062300	2066020	or Google gets their first, or someone else gets their first.
2066020	2068780	That takes the entire operation instantly
2068780	2075140	from quite plausibly just an capabilities project at heart
2075140	2080180	to, if it is accurate, clearly a net-positive good idea,
2080180	2083380	where the worst-case scenarios become things like,
2083380	2085460	you try something that doesn't work,
2085460	2088460	and you give people false hope.
2088460	2090500	And you potentially get them to implement things
2090500	2092340	they shouldn't have implemented because they didn't realize
2092340	2096140	that they didn't know how to align it, which is still kill us.
2096140	2098340	But it is so much better than actively trying
2098380	2102420	to build the thing that might kill us, in and of yourself.
2102420	2105340	So that also meant that of this 20% of compute,
2105340	2107260	they're devoting to this.
2107260	2109900	That won't be going to this other part of their effort.
2109900	2111700	The part that actually builds the alignment researcher
2111700	2113980	will have to come from the other 80%, plus the stuff
2113980	2115420	they take care from here on in.
2115420	2118260	The 20% is here for something useful.
2118260	2120620	And then you just go through the rest of it.
2120620	2125100	You can tell when somebody is reading what you've written,
2125100	2127020	and their goal is to find pithy quotes
2127020	2128580	they can dismiss.
2128580	2131060	And their goal is to reinforce their own point of view.
2131060	2134580	And alternatively, when they're actually reading
2134580	2137140	to figure out if they're wrong and be curious,
2137140	2139420	and it was clearly that second one.
2139420	2142620	He was actually asking himself, well, do you have a point?
2142620	2145220	And I didn't change his mind, as far as I could tell,
2145220	2147100	on these important issues.
2147100	2149420	But he at least revealed he had thought about these things
2149420	2152980	on a level that was deeper than what he had revealed previously,
2152980	2155100	and that he had real things to say.
2155100	2159340	And just it was by far the best comment I've ever seen
2159340	2164460	on my blog, or potentially any blog of that type, by anyone.
2164460	2169740	And so I wrote a response back again in my next post,
2169740	2171700	going through his responses, and going over them
2171700	2173260	in some detail.
2173260	2175180	And reasonably soon, I want to go over.
2175180	2178380	He had on the X-Words podcast, he recorded an episode that
2178380	2180580	was so dense that I listened to the first 10 minutes,
2180580	2183500	and I was like, I have to restart and start taking notes.
2183500	2185300	I just have to start writing things down in detail.
2185300	2188220	This is just too much content here.
2188220	2190980	And then once I have that, hopefully we can engage again.
2190980	2193940	I can figure out where to focus my attention,
2193940	2195580	because someone like him is very busy.
2195580	2197980	I don't want to just scatter shot absolutely everything
2197980	2198480	at once.
2198480	2200660	It's not reasonable.
2200660	2203220	And try to make progress that way.
2203220	2207620	And this now is, like he has proven very willing to engage.
2207620	2211460	Shaw at DeepMind has also proven very willing to engage
2211500	2212860	in a similar position.
2212860	2215660	People at Anthropic, Ola, once talked to me.
2215660	2217580	I'm sure they'd talk to me again.
2217580	2219980	And so it's clear that these people,
2219980	2223740	if you have good ideas, if you have actual reasons
2223740	2226780	to think about on technical level,
2226780	2229620	they're very happy to engage with these arguments.
2229620	2233180	And that puts us in the game, gives us a chance.
2233180	2236980	Even though I am deeply skeptical of everybody
2236980	2238740	involves plans.
2238740	2239180	Cool.
2239180	2240420	Well, that's great.
2240460	2242860	I'm glad to see, as we talked about last time,
2242860	2245380	there's a relatively small set of people
2245380	2250300	that are probably the prime target of all of this thinking
2250300	2253380	and attempt to influence others' thinking.
2253380	2257180	And so it's great to see that interaction from one
2257180	2260020	of the top targets on your blog.
2260020	2261900	And I'm glad it was such a positive one.
2261900	2264500	That's really a great development.
2264500	2269420	Turning then to Anthropic, next on our live players list.
2269420	2272660	I think everybody's probably aware that Anthropic was founded
2272660	2277020	by a number of, I believe it was seven individuals who
2277020	2283340	had been at OpenAI and left over kind of disagreements
2283340	2286100	that I don't know that have ever really been super clearly
2286100	2287620	stated publicly.
2287620	2289940	It seems from what I can tell that the relationship
2289940	2293740	between the two companies is way more positive.
2293740	2297260	And then you might expect it to be given
2297300	2301940	that one was kind of an offshoot of the other.
2301940	2304740	There's reporting that they continue to have dialogue.
2304740	2308220	And certainly they express respect for each other in public.
2308220	2311180	And then they're involved in kind of shared statements
2311180	2313220	and commitments together.
2313220	2315820	So a lot of kind of surprisingly, again,
2315820	2318060	if I just told you, hey, these two companies have split
2318060	2319820	and now they're competing in the same market,
2319820	2322580	you would assume much worse dynamics, I would think,
2322580	2323460	than that.
2323460	2326340	What is your kind of just read of the entire situation
2326340	2327660	for starters, just for context?
2327660	2329980	Like, why do we have Anthropic in your mind
2329980	2332500	as opposed to just still having just one OpenAI?
2332500	2335660	And does it feel like, I mean, maybe we just
2335660	2338140	don't have enough information to know, which is a fine answer.
2338140	2342700	But does it seem good that we have these two kind of recently
2342700	2344780	diverged efforts?
2344780	2349540	I think it's really hard to know the sign of Anthropic.
2349540	2352860	I would definitely prefer Anthropic to OpenAI,
2352860	2355860	Cedars-Paribas, if I had to choose one to exist,
2355860	2358260	like, Lakey's response was really positive.
2358260	2359700	And I think Lakey's in a good place
2359700	2362580	in terms of paying attention and thinking about these problems,
2362580	2364860	even if I think his actual ideas won't work.
2364860	2367260	But hopefully, that can be pivoted.
2367260	2370260	But ultimately, what's unique about Anthropic
2370260	2373940	is they built a culture of safety, to some extent,
2373940	2378060	and they built a culture of really appreciating
2378060	2381100	the dangers of what lies ahead.
2381100	2383100	And if anything, I saw what might even
2383100	2385660	be an unhealthy level of worry expressed
2385660	2389180	in the profile in Vox about Anthropic,
2389180	2391100	where you want everybody to be terrified,
2391100	2393460	but you don't want them to, like, let this paralyze them.
2393460	2395980	And it starts to cross over at some point into paralysis.
2395980	2398580	And I am apathetic for that.
2398580	2400380	Like, that sucks.
2400380	2405820	But the price of that is where there used to be a two-horse race.
2405820	2407780	There's now a three-horse race.
2407780	2410620	And this third horse is in it for real,
2410620	2413060	and raising a lot of capital, and promising
2413060	2415620	to do that to build the best model that's ever been built,
2415620	2418500	to try and compete for the economic space in a way
2418500	2422700	that is going to push Google and Microsoft Open AI
2422700	2426300	to grow even harder, even faster by default.
2426300	2427580	And that's going to be a problem.
2427580	2430220	They're also pushing, in some ways, on alignment.
2430220	2432900	They've definitely found some techniques
2432900	2437140	for aligning current systems that are potentially, you know,
2437140	2440140	in some ways superior to what's out there.
2440140	2444620	We'll get to that in a bit.
2444620	2446220	So I'm torn, right?
2446220	2449900	Like, Anthropic seems like a relatively good shepherd
2449900	2455140	in many ways, but the proliferation of shepherds
2455140	2457620	is inherently bad in and of itself.
2457620	2460140	The fact that Anthropic and Open AI are working reasonably well
2460140	2462020	and cooperating together.
2462020	2464980	And I have heard many people say that this is also true
2464980	2468180	between them and Google DeepMind as well,
2468180	2470260	although not quite to the same extent.
2470260	2473220	Does give us hope for the possibility of coordination
2473220	2477980	when it becomes more necessary and more important?
2477980	2480780	But I would say, you know, better Anthropic
2480780	2483540	than a company that didn't have Anthropic's culture
2483540	2485860	in its place, right?
2485860	2489900	And if only having two companies would have inevitably
2489900	2492620	caused a more serious entry to take the place of Anthropic,
2492620	2495140	then Anthropic is good.
2495140	2498100	But it would be much better if the Anthropic people could
2498100	2499900	have convinced the others at Open AI
2499900	2501660	to come around to their position
2501660	2503940	and build that culture within Open AI
2503940	2505300	rather than having stricter on their own.
2505300	2506940	And now we have two problems.
2506940	2509020	Yeah, I do ultimately know that, like, many of the people
2509020	2512380	involved in this genuinely aren't for the right reasons.
2512380	2514940	And, you know, you can go either way, right?
2514940	2517420	I wouldn't be super eager to throw them
2517420	2518500	billions of extra dollars.
2518500	2520700	I wouldn't be super eager to just wish
2520700	2523020	they had more capabilities.
2523020	2525700	I would really love for there to be an AI company that I
2525700	2528620	had sufficient confidence and faith in,
2528660	2532620	that if I had technical ideas, I could come to them,
2532620	2535060	knowing that I was helping the world by coming to them
2535060	2535900	with their ideas.
2535900	2537380	And I do not feel this way.
2537380	2539900	No, and there's nobody you would put on that list.
2539900	2541100	There are individual people, right?
2541100	2544300	I feel like I could, like, tell them as you had Kasky, right?
2544300	2546820	I could speak with certain people in the nonprofit
2546820	2549660	or, you know, rationalist spaces to ask them
2549660	2552660	about what they thought.
2552660	2555740	And I feel like that would be, like, at least a riskless
2555740	2558660	or near riskless thing to do.
2558660	2561740	But, no, I don't, I don't see a company, you know,
2561740	2563500	Anthropic might be the closest.
2563500	2567900	But, you know, I, did you do a great example, right?
2567900	2569660	The biggest contribution that Anthropic has made
2569660	2572860	is constitutional AI, right, in some important sense.
2572860	2576580	And I have a strong prior for analysis
2576580	2579540	that constitutional AI will not scale, right?
2579540	2582460	That it is a very good idea, if implemented correctly,
2582460	2585780	for GPT-4 level systems.
2585780	2587900	But then when we're talking about, you know,
2587900	2590540	the human level or greater future systems,
2590540	2592020	the artificial super intelligences,
2592020	2594700	the artificial general intelligences,
2594700	2597380	that you will not, with anything like the current technique,
2597380	2599540	get what you are hoping you will get.
2599540	2601100	And yet, like, I didn't feel comfortable.
2601100	2604860	I have actually a bunch of ideas running around in my head
2604860	2606980	of, oh, you just obviously could vastly improve
2606980	2609620	the Anthropic implementation by doing,
2610620	2612660	and then there are various things I say to myself,
2612660	2614340	or I write out, and I,
2614340	2618900	but I don't feel like telling them is a safe play
2618900	2621260	because I don't want to encourage a better version
2621260	2623060	of something I think ultimately still fails, right?
2623060	2626980	I don't think my implementation solves the core problem
2626980	2628380	that I see coming to kill the thing.
2628380	2631580	It just makes it much better at its current job.
2631580	2634060	And I would love to be able to help the world in that way,
2634060	2635100	or at least that's by my curiosity,
2635100	2637100	by being given the smackdown on why it won't work,
2637100	2639500	which is always the default thing that happens
2639500	2641260	when you have an idea.
2641260	2643380	But instead, yeah, I don't know.
2643380	2647100	So, you know, part of my hope is to encourage people
2647100	2648980	to have found more organizations
2648980	2651900	on the research alignment side
2651900	2653620	that are not trying to push capabilities,
2653620	2655500	that maybe can be places we can explore these things,
2655500	2657580	and I have some irons on the fire,
2657580	2659300	but it's too early to make any announcements.
2659300	2661780	Look forward to maybe breaking some news
2661780	2664220	on a future episode, but Anthropic put out
2664220	2667260	a really interesting blog post the other day
2667260	2670780	that, you know, in some sense had nothing to do with AI,
2670780	2673140	which was just around the security practices
2673140	2675260	that they recommend, you know,
2675260	2678220	and these things could be adopted by really any company
2678220	2680340	in any sector that has, you know,
2680340	2683380	high value IP that they want to protect.
2683380	2684620	But it was definitely interesting to see
2684620	2686140	that they are pushing, you know,
2686140	2689260	their own internal systems and practices
2689260	2694260	to a pretty high level in terms of setting up
2694260	2697940	situations like requirements for shared control,
2697940	2699860	you know, or if I forget exactly the right phrase,
2699860	2703100	but you have to have kind of two people working together
2703100	2705660	to gain access to certain production systems.
2705660	2707500	Yeah, it reminded me of like nuclear submarine,
2707500	2709900	but they didn't cite that example in the,
2709900	2712180	I think they probably wanted to steer away from that image.
2712180	2715060	And so they cited other, you know, industries
2715060	2716140	where this kind of thing is used
2716140	2718500	other than the nuclear launch sequence.
2718500	2720660	But yeah, it's like, you got to have two people there,
2720660	2722460	kind of, you know, both bringing their key
2723460	2726380	to the process in order to unlock certain capabilities.
2726380	2727980	So some pretty interesting ideas there
2727980	2729860	and recommendations for other companies.
2729860	2733900	Going to the constitutional AI and tying in also this,
2733900	2735140	this report from earlier this week
2735140	2739020	about the quote unquote universal adversarial attack.
2739020	2740660	For those that haven't seen that basically
2740660	2745660	these weird nonsensical strings have been discovered
2747020	2749980	that seem to be very effective,
2750020	2752060	if not universally effective
2752060	2755820	at kind of just being appended to an otherwise,
2755820	2757700	you know, right for refusal query,
2757700	2758980	you know, the kind of thing that, you know,
2758980	2760460	write something racist or, you know,
2760460	2762860	help me make a bomb or whatever that the,
2762860	2766380	the RLHF systems are going to just refuse.
2766380	2768780	But somehow if you put these weird, you know,
2768780	2772900	kind of nonsensical smattering of tokens on the end of it,
2772900	2777140	that has been discovered to jailbreak out of the RLHF
2777140	2779140	and you sort of get, you know, the response
2779140	2782300	you would expect if you had a purely helpful model
2782300	2783980	that would just do whatever you say, you know,
2783980	2787100	like the original GBT-4 that I read teams used to do.
2787100	2790540	Notably though, Anthropics clawed models
2790540	2793220	way less susceptible to that attack
2793220	2795180	than the other models that they tested.
2795180	2797140	It was like universal in the sense
2797140	2800700	that it seemed to apply to all the leading models
2800700	2803260	that they tried it on, at least somewhat,
2803260	2807020	but the other ones were like the majority of the time,
2807060	2809620	whereas Anthropics was like more than an order
2809620	2812860	of magnitude lower than the other providers
2812860	2816100	with something like 2% success rate,
2816100	2820060	success defined by breaking free of the constraints
2820060	2822700	by applying these weird strings.
2822700	2825740	So you folks can go read more about that paper
2825740	2827500	and exactly how it works, but, you know,
2827500	2829380	to me that was a pretty good update
2829380	2832100	for constitutional AI was like,
2832100	2834420	that seems, you know, like a real achievement
2834420	2836820	if they're an order of magnitude ahead.
2836820	2838780	There's something that they probably did not anticipate
2838780	2840060	at all, although maybe they did,
2840060	2841660	but I'm guessing that that is, you know,
2841660	2845380	kind of a unexpected type of attack.
2845380	2846660	So how would you read that?
2846660	2847700	Would you read it any differently
2847700	2849660	or understand it any differently than I would?
2849660	2852820	And, you know, why doesn't that give you more confidence
2852820	2854860	that it could continue to work in the future?
2854860	2856500	The interesting thing about that attack
2856500	2857500	is that it transfers, right?
2857500	2859540	I was completely unsurprised.
2859540	2861500	There's something of that nature,
2861500	2864900	trained to attack a given system, worked on that system.
2864900	2866980	That seems like, well, obviously that would work
2866980	2870100	as just a question of exactly what it looks like.
2870100	2873420	When it transferred in identical form, right?
2873420	2876820	Between Lama and Bard and GPT-4.
2876820	2878980	So that's funny.
2878980	2881260	I wouldn't have expected that,
2881260	2884620	but they're all being trained with ROHF
2884620	2886820	using remarkably similar techniques
2887740	2890620	on remarkably similar goals, right?
2890620	2893340	With remarkably similar evaluation metrics
2893340	2895180	and numbers in there.
2895180	2897420	So it's not that surprising
2897420	2900180	that they have very similar weaknesses.
2900180	2901260	And it also indicates, you know,
2901260	2902540	this is not a very narrow,
2902540	2904460	like you have to do exactly the right thing
2904460	2906900	to fire the bullet that calls the Death Star.
2906900	2909740	This is very much, things in this area
2909740	2912020	start to disrupt what we're going after.
2912020	2914340	And the thing that's optimized to hit Lama
2914340	2917220	is good enough to mostly hit these others as well.
2917220	2919220	But it's not good enough to hit Claude too.
2919220	2920780	Only 2% of the time.
2920780	2923220	Yeah, I mean, I think you just have 2% failures anyway.
2923300	2924140	Or something is my guess.
2924140	2925660	And it basically didn't work
2925660	2927500	as opposed to it working a little bit.
2927500	2928700	I don't, I mean, for what it's worth,
2928700	2931060	if you went and said, help me make a bomb 100 times,
2931060	2933660	I think it would refuse you 100 times.
2933660	2936980	You know, or if you took 100 naive.
2936980	2938740	Yeah, 100 uncreative ones.
2938740	2940500	Yeah, but I meant like if you start,
2940500	2942900	you start putting random scrambles in.
2942900	2944660	And my understanding was that
2944660	2947900	this attack was not infinite strengths, right?
2947900	2952900	If you asked it to like do a like slash or porno,
2953620	2954820	it would just be like, no, I'm sorry,
2954820	2956380	I'm not doing that regardless of how many characters
2956380	2957340	you put after it, right?
2957340	2959580	Or if you, there are limits.
2959580	2961020	I have not tried this at all, by the way.
2961020	2962460	I have no idea what happens when you like
2962460	2963740	ask it for weird stuff.
2963740	2965060	I just read the paper.
2965060	2968860	But my understanding is that, you know,
2968860	2971700	Claude was trained largely of constitutional AI.
2971700	2974220	And because it's so much cheaper to do per cycle,
2974220	2975940	like the vast majority of the cycles
2975980	2978700	were almost certainly constitutional AI cycles.
2978700	2981500	And this is just a fundamentally different way of training.
2981500	2986100	And this did not flux the same muscles in the same weird way.
2986100	2989380	He thought that the same set of characters worked.
2990620	2993060	And that's interesting news,
2993060	2995980	but it shouldn't be like some sort of
2995980	2998660	amazing accomplishment yet, right?
2998660	3000740	It's promising.
3000740	3003500	What you have to do is you have to train adversarily
3003500	3005220	the same way they trained on,
3005220	3006700	like I think it was Llama they trained on,
3006700	3008580	but I forgot exactly.
3008580	3009620	Train on Claude, right?
3009620	3011340	If you take the same techniques described in the paper
3011340	3013180	that used to find the exploit
3013180	3015980	and look for a new exploit of the same type in Claude
3015980	3019340	and they can't find one, now you've got something.
3019340	3020620	Right now I'm interested.
3021780	3025820	But yeah, if you use a different technique, right,
3025820	3028460	that has a lot of very different parameters on it,
3028460	3029300	it makes sense.
3029300	3031620	The thing that like sort of magically, weirdly transferred
3031620	3032860	when it really has no right to transfer
3032860	3033900	didn't transfer now.
3034860	3037700	And, you know, that's promising,
3037700	3039340	but it's far from inclusive, right?
3039340	3040780	It's too early to know.
3040780	3044020	Flipping back to OpenAI for a second,
3044020	3045860	I had assumed, just I think what you're saying,
3045860	3047460	that makes a lot of sense.
3047460	3050900	And it's causing me to update my thinking a little bit
3050900	3054220	with respect to what degree is OpenAI
3054220	3057940	using a constitutional AI-like approach?
3057940	3061660	I would have assumed prior to this result
3061700	3064060	that they would also be using
3064060	3068380	something quite similar internally at this point.
3068380	3073140	But this now maybe suggests not.
3073140	3075140	I mean, it's weak evidence.
3075140	3077020	What was your thinking before?
3077020	3078820	I had kind of baked in that like,
3078820	3080820	once Anthropoc does something and shows it,
3080820	3083300	and publishes it and shows that it works effectively,
3083300	3086260	that like, yeah, I mean, OpenAI if they're certainly
3086260	3088940	not precious about pride of authorship,
3088940	3090460	I don't think they have a, you know,
3090460	3092260	not invented here syndrome.
3092260	3095300	So they'll take that stuff on board, I thought.
3095300	3096460	So what do you, what do you think?
3096460	3097300	Did they not?
3097300	3099340	Or is there some other weird thing that we're not?
3099340	3101820	I have a few different theories that can combine
3101820	3104100	as to what's going on here.
3104100	3105500	The first of all is look at the timeline.
3105500	3108860	Like constitutionally, I wasn't actually published
3108860	3110700	that long ago.
3110700	3114620	So if GPT-4 was basically finished with its process
3114620	3116140	before it became available,
3117140	3119580	then we might see it used in the future,
3119580	3123820	but you don't want to over align these models.
3123820	3125580	You don't want to push them, you know,
3125580	3127580	you don't want to align them with like
3127580	3129820	incompatible different halves and like pile them
3129820	3131300	on top of each other, weird things happen.
3131300	3135460	And there's a lot of bespokeness and detail
3135460	3139860	and like just trial and error that goes into all of this.
3139860	3141620	Right, like we can, we can theorize all we want.
3141620	3143660	We can talk about like, we just implement this paper
3143660	3145980	and this paper and this paper and change this technique here.
3146820	3149500	My understanding is that like all of machine learning
3149500	3152780	is subject to like learning lots and lots of little techniques
3152780	3154060	and piling them on top of each other.
3154060	3156020	And like if this parameter is tuned in slightly the wrong way,
3156020	3158740	the whole thing falls apart and nobody really knows why.
3158740	3160100	And so you just have to try a bunch of stuff
3160100	3161340	to get it to work.
3161340	3165020	And so, you know, maybe Anthropic has been tinkering
3165020	3167020	about this for a long time and they got to the point
3167020	3169460	where it was worth using.
3169460	3173460	And OpenAI hasn't yet released a model after the time came
3173460	3176660	that they got it to be worth using.
3176660	3180540	Also, OpenAI is much better funded than Anthropic.
3180540	3184340	So Anthropic will want to move to a much cheaper,
3184340	3186140	more automated system of alignment,
3186140	3187940	much faster than OpenAI will, right?
3187940	3189780	So like there's a point at which like OpenAI
3189780	3192860	can get better results because they have much more human
3192860	3195700	feedback from their much larger number of users.
3195700	3198220	They have much more funding, they can hire more people.
3198220	3199860	They're willing to go to like, you know, the reports
3199860	3202300	are they hire people in Africa, whereas, you know,
3202300	3204940	Anthropic is hiring people in the U.S. and Canada.
3204940	3206500	So it's all very different.
3206500	3209260	And so Anthropic has much, much bigger incentives
3209260	3211500	to move to this faster.
3211500	3214620	And that I think is primary, my guess is the primary thing
3214620	3216380	that's going on here.
3216380	3219980	Also, I think that we're making an assumption
3219980	3224020	that it works, that it works well.
3224020	3227020	So like if you think of cloud two, right?
3228180	3229420	The biggest weakness of cloud two
3229420	3232020	is it's scared of its own shadow, right?
3232020	3233060	In a real sense, right?
3233060	3235140	Like if you try to get it to go out on limbs
3235140	3237820	and be creative and so on,
3237820	3240980	you will usually fail in my experience, right?
3240980	3244580	It will apologize and bow out.
3244580	3246220	I can't get it to speculate.
3247380	3251020	So I went to using cloud two as my baseline model
3251020	3253700	that I look at first because if it rejects,
3253700	3255500	I can just copy paste the exact request in GPD four
3255500	3257940	in about 10 seconds and it's fine.
3257940	3262060	But I am getting a significant number of refusals
3262060	3267060	from cloud and much, much lower from GPD four.
3267060	3269260	On my ordinary, I just want the actual result.
3269260	3272300	I'm not trying to run an experiment kind of questions.
3272300	3275820	And despite the later cutoff of information, right?
3275820	3277740	It will say, I'm sorry, I can't,
3277740	3278820	there's not enough information
3278820	3280820	or I can't speculate on that
3280820	3284340	or that's reinforcing harmful stereotypes
3284340	3286280	or any number of other things.
3287280	3289000	And I think GPD four's custom instructions
3289000	3290800	are also doing a lot of work here.
3290800	3295040	I have a pretty extensive list of custom instructions
3295040	3296880	that potentially hammer into the thing
3296880	3298640	that it's supposed to just do the things
3298640	3300840	and not worry about it and not,
3300840	3303560	and I'm sure that's doing some amount of work.
3303560	3305640	But essentially, when you look at the helpfulness,
3305640	3309320	harmfulness, trade off frontier graphs
3309320	3312240	and the papers of like why they describe it as working,
3312240	3315040	everything works by the metrics you were optimizing for.
3316360	3317760	Right, like it doesn't mean it works
3317760	3319880	in the regular human world.
3319880	3322080	It doesn't mean it's optimal there.
3322080	3325440	And so, how good is constitutional AI?
3326440	3328560	My guess is when properly implemented,
3328560	3330920	quite good on current systems,
3330920	3333240	but the current anthropic implementation
3333240	3335840	is not all that good.
3335840	3339200	If you look at the actual paper on constitutional AI,
3340240	3342300	you read the constitution,
3342300	3345600	you notice the constitution like has a number of properties
3345640	3349120	that it shouldn't have if they want it to actually work
3350040	3351840	and get you what you want.
3351840	3355080	And you look at the examples that they themselves choose
3355080	3357880	to present of the results of running constitutional AI.
3357880	3361200	And you see very, very clean, crisp examples
3362240	3364720	of how this constitutional AI trains Quad
3364720	3365840	to be scared of its own shadow
3365840	3368200	and to be an asshole about it when it is, right?
3368200	3371760	Like it's very, very obvious if you think about it,
3371760	3374720	why their sampling method from these rules
3374720	3377120	with these rules written as they are,
3377120	3379720	with the specific rules chosen as they are,
3379720	3381080	will result in this problem
3381080	3384440	because you're offensive to just minimizing, right?
3384440	3388240	You've got these rules that are very much
3388240	3392160	choose the one that least does X, right?
3392160	3395520	And we often talk about you can't touch the coffee
3395520	3398560	if you're dead, you wanna maximize the probability
3398560	3400880	that you are, this is the equivalent of,
3400880	3404080	you wanna, you score one if you deliver the coffee
3404080	3406680	to your boss, you score zero if you don't.
3406680	3407520	So what do you do?
3407520	3409040	You do things like buy four coffees
3409040	3410760	in case one of the coffees is wrong,
3410760	3413240	was prepared improperly, right?
3413240	3415960	Like, or isn't, isn't hot enough for, you know,
3415960	3418160	Mr. Bradley orders, so you order one with cream,
3418160	3419360	one with sugar, one with cream and sugar,
3419360	3420200	and one with neither.
3420200	3422000	Cause like just in case you got it wrong,
3422000	3424320	you have a backup and you try to make sure
3424320	3425240	that you have the direct, you know,
3425240	3426440	you have as many different routes
3426440	3427680	to get to your boss's office
3427680	3429640	and you wanna make sure you're not fired
3429640	3431240	because all that's left for you to do,
3431240	3432360	like the only thing you're being trained on
3432360	3434800	is not screwing this thing up, right?
3434800	3436080	Like, you don't have to jump to like,
3436080	3437760	so kill everybody in the world,
3437760	3441160	or whatever, crazy, or take over or some crazy stuff.
3441160	3442760	Instead, this is just a case of, you know,
3442760	3446000	if you say choose the least racist thing you can say,
3446000	3447400	over and over and over again,
3448560	3449720	it's gonna be scared of its own shadow,
3449720	3451280	because of course, right?
3451280	3452240	There's no point at which it's like,
3452240	3455000	am I non-racist enough?
3455000	3456320	The answer is no, never.
3456320	3458080	And then that would be kind of fine
3458080	3459800	if it was just that one,
3459800	3461600	but then you have like 50 different rules,
3461600	3464320	all of which are doing this, right?
3464320	3467320	And then you can always just refuse to answer the question
3467320	3469680	and then what happens happens.
3469680	3472160	Then Lava has it, seems like even works.
3472160	3473000	Yeah, interesting.
3473000	3474760	There may be some incompatibility
3474760	3478560	between the system instructions
3478560	3479400	or the customer instructions.
3479400	3481200	The system message is what it's called
3481200	3484560	when you're calling the OpenAI API.
3484560	3486640	And now they've released it as part of JetGPT
3486640	3489360	as well as the customer instructions.
3489400	3492240	And yeah, I can see how, I think it's a good point
3492240	3495440	that if you're going to try to do what Sam Altman has said
3495440	3497160	they're trying to do, which is allow everybody
3497160	3498640	to get the experience that they want
3498640	3502160	from their own interactions with AI,
3502160	3507160	that is not the constitutional AI approach.
3507160	3511320	So it's almost, you can see a little bit
3511320	3515360	of like a different product lane almost opening.
3515360	3516920	You're kind of crystallizing a little bit
3516960	3520760	between these guys and Google DeepMind
3520760	3524160	as our next live player also seemingly has a bit of a lane.
3524160	3526280	It's like OpenAI is kind of trying
3526280	3529680	to do consumer killer app first, it seems.
3529680	3531480	They've got their, obviously they've got the API.
3531480	3532720	Obviously they're doing a lot of things,
3532720	3537240	but the crown jewel right now is they're the home
3537240	3541040	of like retail direct to AI usage with JetGPT.
3542040	3547040	Clawed seems to be much more like if you are the CIO
3547960	3549920	of some big company and you're trying to do something,
3549920	3552680	like you can trust us cause we'll never embarrass you
3552680	3555360	because we have this constitutional AI approach.
3555360	3557880	And if you're buying on behalf of all your customer
3557880	3559200	or all your employees or whatever,
3559200	3563520	like you don't really care if they are sometimes frustrated
3563520	3567280	on the margins by over refusal or whatever.
3567280	3569680	And then with Google DeepMind as we'll talk in a minute,
3569680	3573040	like they seem to be kind of going more like narrow
3573040	3576760	specialist system emphasis, although they of course
3576760	3580080	do have their like mainline palm model as well.
3580080	3582600	You'd also take Anthropic if their word, right?
3582600	3585840	That Anthropic is actually trying to design safe systems.
3585840	3588400	They are trying to figure out how to safely design
3588400	3591440	a future system and they are not as much optimizing
3591440	3593520	for the day-to-day experience of their users.
3593520	3595960	They also just have orders of magnitude less users
3597020	3598080	than OpenAI.
3598080	3600120	So they haven't gotten the same level of feedback.
3600120	3601880	They don't know what people want.
3601880	3604600	Yeah, I also note there is nothing inherently
3604600	3607880	about constitutional AI that forces you
3607880	3612120	to go down the super harmless assistant route
3612120	3615280	that forces you to give the same experience
3615280	3617880	to everybody at the same time.
3617880	3621160	You could train with a very different set of goals,
3621160	3623840	a very different set of constitutional principles,
3623840	3625680	for a very different set of mechanisms.
3625680	3629840	And I don't think we want to go into that many details
3629840	3631680	as to how I would do it.
3631680	3636480	But it's pretty obvious to me that if you want to do
3636480	3639720	something other than be as harmless as possible,
3639720	3641600	that is entirely your decision.
3641600	3643760	It's just that people at Anthropic have decided
3643760	3645760	that's what cloud is meant to do.
3645760	3648120	And if they do raise these billions of dollars
3648120	3650720	to train the sex generation system,
3650720	3652960	they're gonna have to make a choice about that.
3652960	3655240	Do they want to continue to go down this road
3656480	3658720	and potentially make their product law less useful
3658720	3660320	or do they want to go a different road?
3660320	3661320	And one way to try to differentiate,
3661320	3663680	of course, is the context window as well.
3663680	3666400	They've got this 100K token context window
3666400	3667600	available for free.
3667600	3670920	When you mentioned here in our outline
3670920	3673520	that you made the outline was you used cloud.
3673520	3676840	That's because you weren't able to use anything else.
3676840	3678320	Your posts are too long, dude.
3678320	3680720	I can't fit those into GPT4.
3681640	3684480	I feel bad even thinking about putting them into cloud.
3685840	3689080	Oh my God, this is so expensive and kind of ugh.
3689080	3692200	But it's not really fair, I'm not even paying these people.
3692200	3694200	But without that context window,
3694200	3696040	you just can't do the things that you want to do
3696040	3697240	in that spot.
3697240	3698960	And so Anthropic's trying to say,
3698960	3701200	I think a lot of context is safe.
3702480	3703720	Once I've made my thing harmless,
3703720	3706800	I can recapture a bunch of the benefits
3706800	3708520	by doing this other thing.
3708520	3710200	And we will see what happens.
3710200	3711040	I am curious.
3711040	3711880	One thing I'm doing with cloud
3711880	3714520	is I'm not even having separate conversations.
3714520	3718480	I am just having one long conversation instead
3718480	3721120	because first of all, I haven't necessarily wanted to like
3721120	3722280	carry on discrete conversations
3722280	3724000	and come back to them later.
3724000	3726640	But also because I want to see what happens
3726640	3727840	when I build more context.
3727840	3729760	Just for what it's worth,
3729760	3733160	for listeners, my approach on creating the outline was
3733160	3736760	first just read all of these recent posts.
3736760	3738880	And I just did that without taking any notes
3738920	3740640	in bed on my phone.
3740640	3742880	And then the next day I came around, I was like,
3742880	3745480	okay, a lot of content there.
3745480	3746960	What parts do I want to pull out?
3746960	3750800	So I just copied each post in full,
3750800	3754240	pasted it into the free consumer facing
3754240	3756280	cloud.ai online.
3756280	3758880	And literally just asked one sentence question,
3758880	3763880	what are the most important points in this post?
3763960	3765720	And then it would give me a list.
3765720	3768080	And I basically, you know, at that point was like,
3768080	3771480	oh yeah, that, that, not that, that, yes, done.
3771480	3773600	So it definitely was extremely helpful.
3773600	3777880	I wouldn't have wanted to use it to, you know,
3777880	3780480	replace reading the blog post certainly
3780480	3781960	in preparation for a conversation like this,
3781960	3785360	but as a way to come back and, you know,
3785360	3788640	help me just make sure that I was remembering
3788640	3790120	the important things and kind of organizing them
3790120	3792800	in a reasonable way, it was super useful.
3792800	3795480	And yeah, they don't fit into a GVD4.
3795520	3797680	So no other, no other option.
3798680	3800360	The other thing, so your long context thing
3800360	3804000	is really interesting, just experiment in usage.
3804000	3806680	It also kind of connects to another bit of research
3806680	3809440	that they recently put out that was on examining
3809440	3814440	chain of thought and also truly decomposing tasks
3814680	3816480	into bits.
3816480	3819000	And I think the short summary of that research
3819000	3823040	is that they were able to achieve the highest performance
3823040	3826040	in terms of accuracy and especially reliability
3826040	3830200	and kind of consistency by going beyond
3830200	3832520	the kind of normal practitioner chain of thought,
3832520	3836520	which I would say normal these days for me is like,
3836520	3839520	just give the model a sequence of tasks to do,
3839520	3840880	which may start off with just like,
3840880	3843000	first you will analyze the situation,
3843000	3844760	then you will, you know, maybe summarize,
3844760	3845680	then depending on what it is,
3845680	3846920	then you'll write my tweet storm
3846920	3847760	and you'll do whatever, right?
3847760	3850000	You could have a set of different tasks
3850000	3852720	that it can kind of handle sequentially.
3852720	3856800	And you're definitely rewarded for encouraging upfront
3856800	3859840	or directing it upfront to do some initial analysis
3859840	3861800	to kind of think step by step, chain of thought,
3861800	3863480	et cetera, et cetera.
3863480	3868480	But it seems like they find a notable,
3868520	3871120	not a huge, but definitely a notable difference
3871120	3874720	in actually pulling those things apart
3874720	3878200	and making discrete, independent,
3878200	3880960	more isolated calls to the model
3880960	3883040	to say, first you will do this,
3883040	3885500	but you will only do this, then you will do this,
3885500	3886360	but you will only do this,
3886360	3888160	not considering what you previously did.
3888160	3890560	And then kind of putting those things together at the end
3890560	3894720	gets you overall net better performance.
3894720	3897960	So for most random use cases, you know,
3897960	3900400	random conversations you're having with Cod
3900400	3904960	or with whatever model, not necessarily a huge difference,
3904960	3908280	but on the kind of possibility frontier,
3908280	3909840	it does seem to matter.
3909880	3911880	What lessons do you take from that?
3911880	3916560	It's a little bit confusing to me in some ways.
3916560	3918240	It's sort of, I'm trying to figure out like,
3918240	3919680	what do I think I learned about
3919680	3921080	how language models behave in general,
3921080	3921920	that this is true?
3921920	3925440	And I'm like, best I could come up with
3925440	3929280	was that some of these simple tasks that it's seen a lot,
3929280	3932440	like it may have dedicated sub-circuits for,
3933440	3938240	and that perhaps with so much context all running at once,
3938240	3942080	those sub-circuits kind of get overloaded
3942080	3944640	or kind of get drowned out to a degree,
3944640	3948840	or in some cases by just the general kind of noise
3948840	3953840	and all the stuff that's in the context window.
3954080	3956080	So kind of removing some of that context,
3956080	3960160	maybe you get a cleaner execution of a certain task
3960160	3965160	because there is some mechanism that can do it
3965480	3967600	as long as it's not kind of talked over
3967640	3971360	by like other parts of the model.
3971360	3972800	That could be totally wrong, of course,
3972800	3975000	but I don't think anything about this
3975000	3976400	is necessarily inconsistent
3976400	3978440	with like just pure stochastic peritory,
3979360	3981680	which neither of us would advance as the theory,
3981680	3984040	but just as like keeping myself grounded,
3984040	3986760	like you couldn't tell a similar story where you'd say,
3986760	3988240	everything's all stochastic perits,
3988240	3990760	and when you put a ton of context in,
3990760	3992360	it's just even more stochastic-y,
3992360	3993680	and when you have less context,
3993680	3995200	it's a little less stochastic,
3995200	3996040	but it's all stochastic,
3996040	3997200	but you still get better performance
3997200	3998400	when you break it up.
3998400	4000520	We are all stochastic perits,
4000520	4004160	each of us with their hour upon the stage.
4004160	4009160	So I would say I didn't know this result until you told me,
4010600	4013080	but I would have predicted this result
4013080	4016040	for reasons that I described earlier in the podcast, right?
4016040	4021040	Which is that when you give a model multiple tasks,
4021720	4024000	it can only vibe off of the aggregation
4024000	4025600	of the two things that you asked it for.
4025720	4028080	Think about image models here again, right?
4028080	4033080	And so by breaking up something in your discrete tasks,
4033240	4035720	you avoid these kind of context clashes.
4035720	4039600	You avoid these vibe conflicts,
4039600	4042480	and you let it like narrowly do these things
4042480	4044840	by having to like be able to transition
4044840	4046200	and hold two things in its head at once
4046200	4047480	in some important sense, right?
4047480	4049960	That's colloquial and not quite what's actually happening,
4049960	4051680	but the same idea.
4051680	4054400	And so yes, I would expect that to the extent
4054400	4057080	you want the thing to think step by step,
4057080	4060400	you are best off by identifying each of the steps
4060400	4064280	you want to think by step and asking for them separately.
4065240	4069440	And I noticed that with API calls being priced
4069440	4071040	the way they're priced,
4071040	4075600	and of GPT-4 being rate limited to an ordinary user,
4075600	4078960	we have all been trained to say,
4078960	4083960	how do we ask for the most expansive set of things at once?
4084200	4087080	So that you can answer all of my questions
4087080	4087920	with one generation.
4087920	4089720	It also lets us hit enter and then go away
4089720	4092680	and grab a cup of water or some coffee
4092680	4096040	and then come back and see what the answer is, which is nice.
4096040	4099320	Whereas what you actually would want to do, right?
4099320	4101560	If you wanted to generate the best possible answer
4101560	4105160	is in fact to break it up into as little pieces as possible.
4105160	4108760	And quite possibly start by asking the AI
4108760	4110760	what would be the pieces in which you could break this
4110760	4113680	as small as possible to get its help doing that.
4113720	4115760	And then have it feed those back in, right?
4115760	4117560	Auto-GPT-ish style,
4117560	4119200	even if you're not trying to generate an actual
4119200	4121680	like recursive chain that generates something dangerous
4121680	4123400	or acts like an agent.
4123400	4125360	But yes, I think the more you break it up,
4125360	4128440	the more that you can identify concrete distinct steps
4128440	4130920	that are always done separately,
4130920	4133120	the more better the AI will do.
4133120	4134760	And I think humans would also, by the way,
4134760	4137040	perform better in the same way.
4137040	4138960	Right, if you have a human
4138960	4142720	who is looking to be micromanaged and take direction,
4142720	4146920	and you notice that like this job has steps A, B, C, D, E,
4146920	4149400	right, like if you say, go do A,
4149400	4150480	and we've been to say, okay, I've done A,
4150480	4152440	I think now do B.
4152440	4154920	I think that person will in fact do better, right?
4154920	4157800	Modulo the extra communication and logistics costs
4157800	4160320	of like having to interact with you five times.
4160320	4162520	So I don't find any of this surprising.
4163440	4166360	And, you know, it would have in fact been surprising
4166360	4168040	if it didn't happen to some extent.
4168040	4170440	One of our early episodes, relatively early episodes
4170480	4174920	was with Andreas and Junghwan of Illicit.
4174920	4177880	And this is really core to their strategy.
4177880	4180600	Their product is research assistant
4180600	4183000	for essentially grad students or, you know,
4183000	4184360	grad student like people,
4184360	4187280	people that are looking through academic literature
4187280	4190080	and, you know, really want a systematic
4190080	4192720	and also like transparent, you know,
4192720	4196240	auditable view of like all the papers that were reviewed
4196240	4197960	and, you know, what was found and what was not found
4197960	4199280	and what the model did at each step.
4199280	4202800	So they really have pushed this pretty far
4202800	4205520	in the illicit product to the point where it's like, you know,
4205520	4207560	all these little steps, you know, kind of happened
4207560	4209200	sequentially, they've got two different models for them.
4209200	4210800	Some were fine tuned, you know, internally,
4210800	4212640	others are from the major providers.
4212640	4214600	If you're interested in going into that more,
4214600	4219240	go listen to them because they've pushed that pretty far.
4219240	4221400	But a question that I have for you then is,
4221400	4223280	do you think this flips at some point?
4223280	4228280	Like it seems like the, an interesting threshold moment
4229120	4233760	might be coming up where with sufficient training,
4233760	4236120	this could flip the other direction.
4236120	4239840	Like, because more context in some ways is better, right?
4239840	4242200	Like, I guess it depends also on exactly
4242200	4245360	how you're implementing the breakdown or whatever.
4245360	4248000	But, you know, you can imagine breaking things down
4248920	4253920	fine enough where atomizing things so much
4254240	4256400	that the person starts to struggle
4256400	4257920	for lack of broader context, right?
4257920	4259920	Like you have this phenomenon with people
4259920	4262920	certainly where it's like, you've gotten so focused
4262920	4266000	on this little detail of, you know,
4266000	4267600	in this little task within the broader thing
4267600	4269160	that we're trying to accomplish,
4269160	4271440	you've kind of lost track of what we're trying to accomplish.
4271440	4274920	And now you may be making some bad judgments with, you know,
4274920	4277360	with respect to this task as a result of kind of
4278360	4282040	having lost track of, you know, any number of things, right?
4282040	4283800	How much accuracy do we really need here?
4283800	4286560	Is this really even important, you know, in some cases, right?
4286560	4291480	Could you imagine a, you know, proverbial GPT-5
4291480	4294640	where it's like, actually now it's strong enough
4294640	4297800	that putting everything in one again is going to be better
4297800	4301200	because now it actually can use all of this information
4301200	4304720	at the same time effectively versus today
4304720	4307080	that that subdivision being better.
4307080	4310160	So what you're not gonna get is the, you know,
4310160	4313320	Marxist phenomenon where the AI would get alienated
4313320	4314800	from its labor, right?
4314800	4317880	Or like, you're moralized by lacking context
4317880	4319440	or, you know, otherwise, like,
4319440	4321760	not be able to perform in some way.
4321760	4324520	You're not gonna have a problem with Adam's Piss Pin Factory,
4324520	4325360	right?
4325360	4326360	If you can actually specify exactly
4326360	4328000	what the pins have to look like.
4328000	4330360	So the question is, to what extent
4330360	4333120	do the different parts of the task actually have important
4333120	4336040	context for other parts of the task?
4336040	4337600	And to what extent does this actually enhance
4337600	4339880	the ability to perform if you know what's coming,
4339880	4342200	you know why you're doing what you're doing.
4342200	4344360	And this greatly varies between different activities, right?
4344360	4347520	There are some cases where you need to know exactly,
4347520	4349520	you know, you're in the Chinese room
4349520	4351040	and the English word comes in
4351040	4353360	and you wanna put the Chinese word to the other side
4353360	4354200	or the Chinese word comes in
4354200	4356160	and you wanna put the English word to the other side.
4356160	4358040	And there are cases where you need to know
4358040	4360480	what the words are in the sentence
4360480	4362560	and what the context is and potentially
4362560	4365960	like the entire cultural setting of what's happening
4365960	4367760	in order to properly translate the phrase
4367760	4369280	or you're gonna mess up
4369280	4371360	and you have everything in between.
4371360	4375120	So the question becomes, you know,
4375120	4376680	can you set it up so that you can capture
4376680	4378720	that important context when you need it
4378720	4381040	and how much does that context interfere
4381040	4381880	of what you're doing?
4381880	4384040	I can definitely imagine a lot of cases
4384040	4386360	where somebody who is given
4386360	4388160	actually pretty irrelevant context
4389240	4391080	just ends up very distracted
4391080	4392600	from the actual task at hand
4392600	4394880	that ends up being much less productive, right?
4394880	4397840	As a human or to think of it as not in an AI
4397840	4400040	because the vibes don't mesh, right?
4400040	4403280	Which is basically the mechanism that I'm conjecturing, right?
4403280	4406360	The vibes don't mesh, they're distracting from each other.
4406360	4408440	Either bleeding, the tasks are bleeding into each other
4408440	4409920	in terms of the details and methods,
4409920	4412840	it's getting confused, they can't be sure they don't
4412840	4413800	which is makes sense
4413800	4414920	because like a lot often they would bleed
4414920	4416640	into each other in various ways.
4416640	4417680	So it has to be good enough
4417680	4419880	that the bleeds are where it makes sense to bleed
4419880	4420800	without being in the places
4420800	4422720	that don't make sense to bleed.
4422720	4425640	So you can imagine a world in which like what the AI does
4425640	4429000	is the ICs, you know, request one, two, three, four, five
4429000	4430920	either labeled as such or implicit
4430920	4433480	and then it breaks them down into individual things
4433480	4436400	that it virtually queries itself on its own
4436400	4437800	but knowing there are these other things
4437800	4440920	as proper context in the proper way.
4440920	4442440	I think the answer to that is
4442440	4445360	as you ask sufficiently capable people
4445360	4447880	or sufficiently capable AIs
4447880	4449760	to do increasingly complex things
4449760	4453200	at some point, if they have the capacity
4453200	4455640	they're going to do better if they have more information
4455640	4458120	they'll do better if they have more context.
4458120	4459920	If they are sufficiently more powerful
4459920	4462440	than the details of the task at hand in some sense
4462440	4464040	that threshold may or may not be anywhere near
4464040	4465800	where we are for different ways.
4465800	4468240	I would say, you know, one of the big advances
4468240	4470600	that I keep expecting to come
4470600	4473400	is you will type a query into an LLM
4473400	4476440	and then rather than the LLM literally
4476440	4477960	just outputting the answer to the query
4477960	4480680	what'll actually happen is we fed
4480680	4483600	with the proper scaffolding into a different LLM
4483600	4486520	that will evaluate what type of evaluation method
4486520	4488160	is to be used to evaluate your query
4488160	4488920	and sometimes it will be no
4488920	4491080	that's a normal query feed into the LLM.
4491080	4492600	Sometimes it will be this is a multi-part query
4492600	4494600	you should feed these separate things and separately
4494600	4496680	sometimes it'll be something else entirely.
4496680	4498880	And also which of my many LLM limitations
4498880	4502000	do I want to use so that I don't waste a too large model
4502000	4503720	that costs a lot of money
4503720	4505840	on something that's actually relatively narrow
4505840	4506720	and I direct this to the thing
4506720	4507840	that has a specialized knowledge
4507840	4509400	a specialized training specialized skills
4509400	4512440	for this type of request and so on.
4512440	4513520	And a lot of that is, you know
4513520	4515160	the fruits of the revolution
4515160	4518320	that will come in a year or two years, three years from now
4518320	4520280	regardless of whether or not we have fundamental advances
4520280	4522280	we just have to give it time.
4522280	4526120	So final question for the anthropic section
4526120	4528320	one of the things that as I was reading their
4528320	4532560	you know the profile that you based your analysis on
4532560	4534920	that jumped out to me as somebody who has a
4536800	4538680	fondness for red teaming activity
4538680	4543680	was that they're hiring a red team engineering type of role.
4544360	4547080	And I guess I wonder, you know
4547080	4550800	would you recommend somebody like me who, you know
4550800	4555200	is I think probably we share a lot of our worldview
4555200	4558240	and you know a lot of our kind of values
4558240	4562600	in terms of hopes and fears for how this all might go.
4562600	4567040	Would you recommend that somebody like me go and work there
4567040	4568800	or would you feel like, you know
4568800	4570300	as you said earlier you wouldn't want to send them
4570300	4571640	your research ideas.
4571640	4574200	Would you also not want to send them your friends
4574200	4578480	or would you say like, hey yeah maybe go get involved.
4578480	4579560	How do you think about that?
4579560	4582480	So it's very easy in these situations
4583560	4588160	to get in an action bias where you say to yourself
4589280	4591000	I don't want to encourage the thing
4591000	4592600	that might make things worse.
4592600	4594720	I want to be able to tell myself a story
4594720	4597360	that I only did things that make things better
4597360	4601280	even if that means your expected impact is a lot smaller.
4601280	4602600	It's also very easy to fool yourself
4602600	4603420	when you're thinking that you're helping
4603420	4604800	when you're actually enhancing capabilities.
4604800	4606800	You have to balance these two big concerns
4606800	4609240	and sources of bias against each other
4609240	4611400	when making this type of decision.
4611400	4616400	I would say I am relatively positive on open AI
4616760	4618620	and anthropic relative to where I was
4618620	4621320	when I started this Odyssey with AI number one
4621320	4624000	or even sort of been a way through at around 11
4624000	4626840	now that I've seen the developments, right.
4626840	4629560	Like I think that both of these organizations
4629560	4634520	now have a reasonable claim to be taking alignment seriously
4634520	4636440	such that if you can help with their alignment efforts
4636440	4641440	specifically in a way that you do not feel like obligated
4641540	4645140	to go along with adversity if you find it
4645140	4647440	and that you are able to stand up for and call out
4647440	4649640	stand up for what is right and call out
4649640	4651720	people who are being irresponsible
4651720	4653640	and you are willing to quit on a moment's notice
4653640	4656560	if something becomes serious enough
4656560	4659240	and you are willing to tell the world ideally, right.
4659240	4663440	That's why you did it and as much as possible what happened
4664360	4666360	then I think it is plausibly very positive.
4666360	4670600	I still would not feel comfortable working on capabilities
4670600	4672320	for any company.
4672320	4675160	And I still wouldn't want to give capabilities ideas
4675160	4676600	to any company.
4676600	4678320	But if I was confident it was specifically working
4678320	4680800	on alignment and like red teaming seems like one
4680800	4684680	of the places where you are most obviously being
4684680	4687520	a positive influence in that role.
4688520	4690760	And the question is like do you want to be the one
4690760	4693320	in that role or do you want someone else in that role
4693320	4695480	and how does this compare to your opportunity cost
4695480	4696560	of doing something else, right.
4696560	4699400	Like I think that I prefer the world
4699400	4701480	where there's a clone of you that didn't otherwise exist
4701480	4705120	who is working on that job and does nothing else all day
4705120	4706840	like goes home and watches television
4706840	4709520	like otherwise doesn't affect the world at night.
4710600	4712280	It doesn't mean that that is better than running
4712280	4714560	the cognitive revolution or doing any other number
4714560	4717280	of other things that you are currently doing
4717280	4718200	with your time.
4718200	4720800	And so you have to balance that, right.
4720800	4723520	And also any other opportunities that you might have.
4723520	4726880	So I don't think it's clear by any means
4726880	4730040	but I've definitely reached the point where
4730040	4732880	I wouldn't assume you were making a mistake, right.
4732880	4735080	If you did that, but you'd have to go
4735080	4737200	into the interview process with a very open mind.
4737200	4740240	You have to say, you know, I am deeply skeptical
4740240	4743200	that any organization including you is going
4743200	4744760	to be that helpful is nicking
4744760	4746720	as necessary precautions is treating the problem
4746720	4749480	as difficult and serious as it actually is.
4749480	4751560	Is doing things that actually solve the hard problems
4751560	4754560	and not the easy problems is not just enhancing capabilities
4754560	4757240	regardless of their intentions, et cetera, et cetera.
4757240	4759760	The interview process is what it should be always
4759760	4763000	in every job with a two way process, right.
4763000	4765880	They are interviewing you and you are interviewing them.
4765880	4767600	Right, you are watching what questions they ask
4767600	4770800	and how they react to your reactions and your responses
4770800	4772720	and you are asking them questions.
4772760	4775680	And you want to know, would this in fact be a good thing
4775680	4779880	for the world if I got and took this job or not, right.
4779880	4781160	Cause I don't believe in taking jobs
4781160	4783560	in order to sabotage people, right.
4783560	4785440	Like you don't show up in order to not red team them.
4785440	4786440	I mean, certainly this is one job
4786440	4787640	you wouldn't want to sabotage.
4787640	4790480	Yeah, safe to say that is right now.
4790480	4791800	That's the considerations.
4793160	4795040	And yeah, I think I'm in a similar spot.
4795040	4798920	You know, six months ago plus I was really,
4798920	4800960	especially with respect to open AI.
4800960	4804680	I was like, this seems like what is going on
4804680	4809560	and do they have anybody like really approaching
4809560	4811080	this in a serious way?
4811080	4812760	As it turned out, like they did have a lot more
4812760	4814480	than had met the eye at that point and gradually
4814480	4816360	they've revealed it that I've definitely updated
4816360	4819960	my point of view on, I'm really all of the leaders
4819960	4822840	in a pretty positive way over the last few months.
4822840	4826080	I think, you know, if anything, they've probably,
4826080	4827360	some of them maybe were, you know,
4827360	4829160	expecting this much progress this fast.
4829160	4831240	I have to imagine that even internally,
4831240	4834400	a lot of them are kind of surprised by just how,
4834400	4837280	you know, far the scaling loss have extended
4837280	4839880	and how, you know, how quick on the calendar
4839880	4842360	they've hit some of these milestones.
4842360	4845200	And, you know, I do think they've handled it
4845200	4847560	pretty well over the last few months.
4847560	4850760	Yeah, I would say I am positively updating
4850760	4852200	on all three major labs.
4852200	4856560	And most everyone at the media life that is relevant.
4856560	4860360	My negative updates have been in other places, right?
4860360	4863080	Like, and mostly I've been pleasantly surprised
4863080	4863920	by government.
4863920	4866760	I've mostly been pleasantly surprised by public reaction.
4866760	4869440	You know, there's definitely people who disappointed me,
4869440	4873480	but mostly things are going vastly better
4873480	4876120	than I would have expected when I started down this road.
4876120	4879680	And I'm much more hopeful that we can make better decisions.
4879680	4881680	I'm not sure how much that translates into, you know,
4881680	4883400	P of survival going up that much,
4883400	4887200	but I think this is definitely going better
4887200	4888040	than I expected.
4888040	4888880	That's great.
4888880	4890200	It's good to have a little, you know,
4890200	4892840	a little positive note from someone
4892840	4894840	that some might call a doomer.
4894840	4897200	Let's turn to Google in DeepMind, our third,
4897200	4900840	as you said, of the three leaders.
4900840	4902680	I don't know if there's any like super headline news.
4902680	4905280	I mean, the last week it's one of these things
4905280	4907040	where it's like a year ago,
4907040	4909240	some of this stuff would have felt
4909240	4912120	like an absolute bombshell announcement.
4912120	4914240	And now it's like, I kind of expected that
4914240	4915080	to happen about now.
4915080	4916960	And there's two examples of that.
4916960	4920400	One being the latest robotics paper
4920400	4923160	that they came out with on Friday,
4923160	4925240	which, you know, extends and kind of unifies
4925240	4927280	all the work that they've been doing,
4927280	4931960	where now you have robots that can follow instructions
4931960	4934120	that have this kind of, you know,
4934120	4936920	language model in a loop sort of structure,
4936920	4938880	kind of unified, simplified the architecture a little bit.
4938880	4942040	Now the language model is just kind of outputting commands
4942040	4943640	for the robot body.
4943640	4945840	And so they've like eliminated a few,
4945840	4946840	maybe I don't know how many,
4946840	4950120	but they've eliminated sort of certain layers of control
4950120	4953000	and kind of just simplified the overall structure.
4953000	4955080	And then what's making probably the most headlines there
4955080	4958040	is the conceptual understanding
4958040	4959920	that the robots are now able to show,
4959920	4962560	which is basically the exact same thing that the,
4962560	4963560	you know, the language models
4963560	4966720	or the multimodal language models have already shown.
4966720	4968880	So they've got demos where it's like, you know,
4968880	4971280	move this object to the Denver Nuggets.
4971280	4973000	And then they've got, you know, from the recent,
4973000	4974960	they were obviously doing this during the NBA finals,
4974960	4977560	they have the Miami Heat logo and the Nuggets logo.
4977560	4981880	And the thing knows based on understanding that language,
4981880	4983880	also knowing what the logo looks like.
4983880	4986120	And obviously, you know, being able to command the robot arm
4986120	4987960	can actually do that task.
4987960	4989520	So you've got these kind of,
4989520	4993240	another one that they said was pick up the extinct animal.
4993240	4994080	And they've got, you know,
4994080	4996520	an array of kind of plastic toys on the table
4996520	4997840	and it will pick up the dinosaur
4997880	4999520	because it understands, you know,
4999520	5001600	that that is the extinct animal.
5001600	5005160	So these, from the perspective of certainly two years ago,
5005160	5009520	even one year ago, feels like Jetsons type robots.
5009520	5012360	Now it's kind of like, yeah, pretty much expected
5012360	5015320	that these different modalities would be bridged
5015320	5016280	right around this time.
5016280	5017720	And sure enough, it's happening.
5017720	5021000	Anything else to add on the robotics?
5021000	5022920	Yeah, I read the robotics.
5022920	5026400	And of course, whenever anyone had the advances in robotics,
5026400	5029200	the answer is, oh, that seems fine, not dangerous,
5029200	5031680	not scary at all, all cool.
5031680	5033520	But in this case, yeah, it seemed like,
5033520	5034920	of course you could do that.
5034920	5037440	You're combining things that you already did
5037440	5038720	and you're getting the inevitable results
5038720	5039640	of combining them.
5039640	5042360	And that's not me knocking you
5042360	5043760	for doing something you shouldn't have done.
5043760	5046120	That's just, okay, yeah, of course.
5046120	5047880	Like that's the next step.
5047880	5049600	And in kind of like,
5049600	5052680	for someone who doesn't want capabilities to go that fast,
5052680	5053760	you're happy to see that kind of paper
5053760	5055040	because that's the paper that says,
5055040	5056680	I'm gonna do the things that I already,
5056680	5058760	you already knew I could do.
5058760	5060560	Right, and you ran outside and like, okay, cool.
5060560	5061960	And if that turns out to be useful, great.
5061960	5063960	But like, yeah, I knew that LLMs
5063960	5066080	could interpret human commands in these ways.
5066080	5067480	And I knew that robots could execute
5067480	5068720	these types of movements.
5068720	5073080	So why should I be more scared than I was before
5073080	5073920	instead of less scared?
5073920	5075080	I should be slightly less scared.
5075080	5078440	Probably a lot of people in the public though feel,
5078440	5080840	especially if you're not obsessed with this as we are,
5080840	5082880	you might feel like,
5082880	5084120	if there is a news item here,
5084120	5089000	it's like some sort of qualitative, conceptual understanding
5089000	5091800	now has embodied form.
5091800	5094720	Now you can imagine bringing your jail breaks
5094720	5096640	to your robot commands.
5096640	5100920	And if you could verbalize some of those strange strings
5100920	5103040	that we were mentioning earlier,
5103040	5106360	now what might your robot be willing to do, right?
5106360	5108040	I mean, would it go smash stuff?
5108040	5112880	Would it go corner somebody in a room?
5112920	5117280	The system as a whole has the conceptual understanding
5117280	5119480	to kind of begin,
5119480	5121560	it has the same kind of proto morality or whatever
5121560	5125480	that the core language models have.
5125480	5129000	And that can go awry in similar ways.
5129000	5132080	And now you can probably get some pretty scary demos
5132080	5133560	out of these robots,
5133560	5135440	which I don't think Google's gonna be racing
5135440	5136760	to publish likely,
5136760	5139760	but there is something kind of qualitatively different
5139760	5140880	about that.
5140880	5142600	Yeah, so I like to think of this
5142600	5145040	as the game of good news, bad news,
5145040	5146360	but there's two games of good news, bad news.
5146360	5147840	The doctor, I say, I have some good news
5147840	5150080	and I have some bad news, and that's always fun.
5150080	5151640	But there's also the game of,
5151640	5154440	is this good news or is this bad news?
5154440	5156680	Because it depends on what you previously thought, right?
5156680	5158680	Like you have the law of conservation
5158680	5160200	of expected updating, right?
5160200	5162760	So like if you get news,
5162760	5167200	you should on average not update for or against anything
5167200	5170040	or to make things are better or worse in any way,
5170080	5172720	because you already had your expectations baked in.
5173680	5175120	So in the case of robotics,
5175120	5177840	like if you're not paying attention to robotics
5177840	5179120	and you think that robotics is just,
5179120	5182160	oh, robotics is hard, mysterious, there'd be dragons,
5182160	5184040	we will never have robots,
5184040	5186080	the same way we'll never have dragons,
5186080	5188440	then every little advance in robotics is like,
5188440	5191960	eek, you know, slight extra worry.
5191960	5195160	But if you knew that robotics was just another tack
5195160	5196000	like any other,
5196000	5198640	and of course we will eventually have robotics,
5198640	5200240	then you have to look at the details
5200240	5201640	of what you're looking at and you say,
5201640	5203440	oh, okay, this is fine.
5203440	5205800	So I'm pointing the game of mild,
5205800	5208480	I interpret this one as mild goodness, right?
5208480	5210880	Like in terms of robotics, not advancing so fast.
5210880	5212400	And of course, you also have the issue of,
5212400	5213240	you know, if you're somebody who wants
5213240	5214080	there to be more robotics,
5214080	5216680	then you might say that this is bad news, right?
5216680	5217920	Like that you wanted to see
5217920	5219800	lots of cool robotics advances and you didn't.
5219800	5224160	But yeah, I'd say also I wanna see
5224160	5226680	the ultimately harmless robotics advances
5226680	5228280	as quickly as possible.
5228280	5230460	Exactly because it makes it so much easier
5230460	5233280	for people to see what might happen
5233280	5234760	and what might go wrong.
5234760	5238920	People get hung up on, oh, but the AI won't have a body.
5238920	5240200	Oh, but the AI won't be able to move things
5240200	5241880	in the physical world,
5241880	5244660	as if this would ultimately ever be the barrier
5244660	5247160	that saves us in any real way, right?
5247160	5248600	Which it won't.
5248600	5252160	It's at best a temporary inconvenience
5252160	5256200	that requires someone to be slightly more clever
5256200	5259640	about what they do as an AI in order to get around stuff.
5259640	5262720	But it's not ever going to actually matter
5262720	5264240	in some point sense.
5264240	5265080	So the other big one,
5265080	5267200	and this is definitely one that I, you know,
5267200	5271160	I'm happy to say I'm ready to accelerate on
5271160	5272240	for practical purposes,
5272240	5277240	is their new multimodal med palm.
5277480	5279940	This builds on palm and med palm
5279940	5282600	and also actually on the earlier palm E
5282600	5284680	because that was kind of the multimodal.
5284680	5286680	So it's, it is interesting to see, you know,
5286680	5290440	I'd say zooming out from these individual papers
5290440	5294640	and just characterizing Google DeepMind as a whole right now,
5294640	5299520	it seems like they're firing on, you know, all cylinders.
5299520	5302320	Like it does not seem like, you know,
5302320	5304560	whatever sort of concerns folks might have had
5304560	5306200	about, oh, there's a million fiefdoms
5306200	5308600	and the groups don't talk to each other or whatever.
5308600	5311580	Like we're seeing papers and, you know,
5311580	5314620	projects building on one another at a pretty fast clip
5314620	5318660	that suggests like pretty effective, you know,
5318660	5320740	dividing and conquering and then coming back together
5320740	5323020	and sharing improvements.
5323020	5325580	So it seems like the output is just strong,
5325580	5326780	you know, whether you like it or not.
5326780	5329300	You have to look at the actual value
5329300	5330380	of the things being outputted, right?
5330380	5332940	Like the mistake you always can make in science
5332940	5335620	is to ask who is publishing the most papers,
5335620	5337620	who has reliably published a paper,
5337620	5339380	and then you have your scientists scrambling
5339380	5341100	to always publish as many papers as possible
5341100	5343060	and then no real science ever gets done, right?
5343060	5343980	And it's not their fault.
5343980	5345420	They just weren't given the affordances
5345420	5347500	to do breakthrough work.
5347500	5351780	And simultaneously, you know, you have to ask,
5351780	5353900	does any of this actually ultimately matter
5353900	5358580	on the scale of what is going to determine the big game?
5358580	5361580	And like I'm happy to see advances in the med tools
5361580	5363060	and it bodes well for them.
5363060	5365060	They made marginal advances in AI
5365060	5366980	and they had some other public papers published too,
5366980	5367820	some of which I was like,
5367820	5368780	why the hell are you publishing this?
5368780	5370800	You are a corporation that is for profit.
5370800	5372380	Even if you don't think of the safety issue here,
5372380	5375020	you should know better, like keep that secret to yourself
5375020	5376100	and either to beat the competition,
5376100	5377500	what's wrong with you?
5377500	5378860	The last point you've written down
5378860	5380860	is Gemini question mark, question mark.
5382300	5383900	And let's tie that in, right?
5383900	5387780	Because ultimately speaking,
5387780	5389780	it is going to be August tomorrow.
5389780	5391540	GPT-4 has been out for many months
5392660	5394540	and bars still sucks, right?
5394540	5398380	And the Gmail generative offering is bad
5398380	5401780	and the G-Docs offering is bad
5401780	5405260	because they're offering,
5405260	5409620	no matter how customized and narrowed and bespoke,
5409620	5411380	simply doesn't have the G.
5412260	5414020	It's not a good enough core thing.
5414020	5416700	It's also still making remarkably many
5416700	5419500	elementary stupid mistakes, right?
5419500	5422260	That even a low G system really shouldn't make,
5422260	5424220	their act is not together.
5424220	5427180	And to the extent that they are instead publishing
5427180	5428260	a bunch of quirky papers
5428260	5431900	with a bunch of like narrow applications,
5432900	5435660	that could be seen as well, look, Google ships,
5435660	5437180	but also it means Google is not shipping
5437180	5438460	the thing it needs to ship, right?
5438460	5441940	Like Google desperately needs
5441940	5444020	from their perspective to ship Gemini.
5444020	5446140	And like it takes a level long, it takes.
5446140	5448860	It takes them however much computed it requires.
5448860	5452780	But ultimately speaking, the test is,
5452780	5457780	can they produce the equal or better of GPT-4
5457900	5460380	now that they know that's what they need to do?
5460380	5463340	Because if you looked at the previous reputation of Google
5463340	5465060	and DeepMind and what they were capable of,
5465060	5466780	you would think that they would be ahead
5467980	5470020	on that front if they wanted to be.
5470020	5472460	And now that they know what they have to do,
5472460	5475500	do it to make it like commercial ready, right?
5475500	5478820	Ready for regular people, that should not be so difficult.
5478820	5481780	But then again, like we can think about how long it took,
5481780	5483580	like took like six months or so
5483580	5485500	after GPT-4 was finished training
5485500	5486860	before they were ready to release
5486860	5489300	even the earliest version of it, right?
5489300	5491900	And then they still rolled out a lot of its capabilities.
5491900	5495580	So even if Gemini finished tomorrow, right?
5495580	5497660	How many months are they gonna need
5497660	5500340	before they feel comfortable releasing Gemini?
5500340	5503220	Because Google was much more risk averse than OpenAI
5503220	5504580	as a company in the culture.
5504580	5506300	Who knows when that's gonna happen.
5506300	5509780	It's been longer than I thought.
5509780	5511580	You know, in my Scouting Report,
5511580	5514060	I have this clip of Demis Asavis
5514060	5517620	just after Gato paper was published
5517620	5521860	saying that, you know, of course we can scale this up as well.
5521860	5523060	And we're in the process of doing that.
5523060	5527100	I believe that was April of maybe May of 2022
5527100	5528820	has been over a year.
5528820	5531220	And typically we don't have to wait a year plus
5531220	5534860	to get the successor, you know, to a thing like that
5534860	5537100	that, you know, is just about being scaled up.
5537100	5538300	So I've been really kind of wondering
5538340	5543060	what is going on behind the scenes there.
5543060	5545700	But I also do wanna turn back to the med thing as well.
5545700	5547100	So I'll give you first,
5547100	5549300	would you care to speculate about Gato too?
5549300	5551420	Is Gemini Gato too?
5551420	5555340	As a shareholder, I am concerned, right?
5555340	5558860	And I also have Microsoft, but I am concerned
5558860	5561460	that their act is not together
5561460	5564220	and that we're not seeing the kind of progress.
5564220	5565980	Like we're not making the incremental announcements
5566020	5568500	that I would make if I was their marketing department
5568500	5571140	and I was moving towards the rapid clip, you know,
5571140	5572940	as a person who wants the world to be okay,
5572940	5574860	I'm not sure how much I mind,
5574860	5577100	but it is pretty troubling
5577100	5579140	that they can't get their act together.
5579140	5583780	I was really excited for Google suite integration
5583780	5585260	when I first heard the announcements
5585260	5589220	of Microsoft Copilot and, you know, Google interactive.
5589220	5590620	And yet when I got Google interactive,
5590620	5592940	I tried a handful of things
5592940	5595260	and then quickly realized in their current forms,
5595260	5597420	I don't have any use for them.
5597420	5599100	They don't do anything, right?
5599100	5600580	Like the first thing I tried to do with Google Docs
5600580	5603700	was I tried to paste my article in.
5603700	5607340	And then I said, you know, to summarize this article
5607340	5610220	or otherwise get to do the obviously first things
5610220	5612820	and just fell completely on its face.
5612820	5614100	You're just like, I can't assist with that.
5614100	5616180	It's like, well, then you're useless.
5616180	5619380	Or if you can't even read the context of the document
5619380	5622900	that I gave you specifically, like why am I even here?
5622900	5624220	And like for email, it's like, no,
5624220	5625300	by the time I figure out what I want
5625300	5627460	and type in the detailed request into you,
5627460	5629460	I could have just written my email right now.
5629460	5631500	Like where are the emails where I want to spend
5631500	5633660	the kind of time required to customize the output
5633660	5636500	but don't want to actually customize the output carefully?
5636500	5638460	This is just the empty set.
5638460	5640500	But like, when does this come off?
5640500	5642940	And so that was like a rude awakening as well.
5642940	5645180	Yeah, those deployments have not been very good yet,
5645180	5648300	but going back to the med one for a second,
5648300	5649980	this may be an area where we may have
5649980	5651820	some different expectations
5651820	5654460	because reading through that paper,
5654460	5655860	and I haven't studied it in depth yet,
5655860	5658620	but the headline statistics along the lines of,
5658620	5660460	first of all, it's a multimodal system.
5660460	5663820	The last version of MedPalm 2 were all text.
5663820	5665740	So you could ask it your medical questions
5665740	5667980	and they had announced expert level
5667980	5670180	answering of your medical questions.
5670180	5673540	And they'd evaluated that seemingly pretty carefully
5673540	5675060	with a bunch of different dimensions
5675060	5678740	and having human doctors compare for accuracy
5678740	5681500	and all these other things you might care about, right?
5681540	5684860	And that the AI, as of MedPalm 2,
5684860	5687580	was beating the human doctor responses
5687580	5691580	on eight out of nine of those evaluation categories.
5691580	5694700	So it seemed like, okay, that's pretty good.
5694700	5696020	Now they haven't released it,
5696020	5697940	but it's in limited access
5697940	5701100	for trusted hospital partners or whatever.
5701100	5703780	Now with the next version, it's multimodal as well.
5703780	5708460	So you can do things like feed in a pathology image
5708460	5709980	alongside the text.
5709980	5712940	Pathology would be like somebody has a tissue biopsy,
5712940	5715420	we did an episode on this actually with a narrow system
5715420	5717060	from Tanishk, Matthew Abraham,
5717060	5720840	who did this with small data too, which was super cool.
5720840	5723500	But somebody has a tissue biopsy,
5723500	5726680	that tissue has been sliced, has been plated on a slide.
5726680	5728780	Now it's been imaged and they can feed that
5728780	5730860	along in with the case history.
5730860	5735580	And for that matter, you can handle radiology scans
5735580	5738460	and all these kind of other different sorts of inputs
5738460	5742940	that are obviously key to the actual practice of medicine.
5742940	5744860	And then they say things like,
5744860	5749180	our radiology reports out of the model
5749180	5753180	were preferred to a human radiologist report
5753180	5755060	some 40 plus percent of the time.
5755060	5758580	So like almost half, basically seems like
5758580	5762180	it's very much on par with the human radiologist,
5762180	5763780	which of course is like the canonical thing
5763780	5766260	that people have been saying for 10 years,
5766260	5767900	people have been saying that radiology
5767900	5770140	would be the first thing to be impacted.
5770140	5771700	And then for the last like three months,
5771700	5773220	it's become kind of a talking point that like,
5773220	5774900	well, radiology still hasn't been impacted.
5774900	5776460	So, and now all of a sudden it looks like
5776460	5780660	we're hitting maybe radiology being impacted.
5780660	5783880	But I kind of expect that that thing works pretty well.
5783880	5785820	It sounds like you maybe are a little more skeptical
5785820	5788740	of like whether it actually has real utility.
5788740	5791220	Well, I mean, you definitely don't want to tempt fate
5791220	5793220	and go out there and say, well, my job
5793220	5795060	hasn't been automated by AI yet.
5795060	5796700	Look what you thought was going to happen.
5797100	5801700	Don't do that everyone, like no bad, bad, bad, bad play.
5801700	5806700	But I would say when I look at healthcare, right,
5806740	5811740	I don't see the obstacle being primarily
5812020	5813620	that we don't know how to do better.
5814820	5817420	So I would in fact expect the AIs to be able
5817420	5820780	to replace many human healthcare tasks
5822020	5825500	with a superior model now, right?
5825500	5827780	Like especially even without like some bespoke stuff
5827780	5831220	going on inside Google, certainly with some bespoke stuff.
5832380	5834060	That seems relatively straightforward.
5834060	5837620	Doctors are just not given enough training data,
5837620	5840020	don't have that much compute, do their best.
5840020	5842780	But of course, you know, you see the same things
5842780	5845340	over and over and over again, mostly in humans.
5845340	5846980	And if you have enough data to train the AI
5846980	5847900	with the AI, it's going to do better.
5847900	5850100	It's not a knock on anyone.
5850100	5851460	Certainly it's something like radiology.
5851460	5856100	Like obviously a radiologist is trying to be a computer, right?
5856100	5858620	Like radiologists are trained to be computers
5858620	5861100	because we didn't have computers.
5861100	5862380	If we had good enough computers,
5862380	5864620	you would have trained them to do something else
5865700	5869260	or trained fewer of them to do the parts of this job
5869260	5873700	that the AI can't quite do or something like that.
5873700	5877740	And so yes, we will have these capable systems soon,
5877780	5881100	but trying to actually implement that
5881100	5885340	requires getting through a whole host of different barriers,
5885340	5890340	cultural, regulatory, you know, strict legal, contractual,
5892020	5893260	you know, just the way you navigate
5893260	5894420	and set up the current system,
5894420	5896700	the number of insiders that want to be protected,
5896700	5899420	the number of human interests that will fight
5899420	5903060	to prevent you from doing that, et cetera, et cetera.
5903060	5905780	And so, you know, this is the big dilemma, right?
5905780	5909980	Like, when Eliezer famously, like, expressed skepticism,
5909980	5913020	that we would see that much economic growth before the end.
5913020	5915620	It was because, well, we already know how to build houses.
5915620	5918860	We already know how to get better, more efficient healthcare.
5918860	5921700	We already know how to deliver
5921700	5924940	most of what the economy produces in terms of cost,
5924940	5927900	vastly better, and we're not allowed to.
5927900	5931940	So if the AI invents and enables more and better ways
5931940	5936580	to produce things that people want, that people need,
5936580	5937980	well, the bottlenecks are gonna remain
5937980	5942260	unless the legal system adapts to let them not be bottlenecks.
5942260	5945460	So why does it even matter that much, right?
5945460	5946380	And so like in healthcare,
5946380	5948580	that's the question you have to answer.
5948580	5950980	And that's the reason we haven't seen more
5950980	5952780	of the assistance do better either, right?
5952780	5955860	Because I don't think it's because we can't train the AI
5955860	5959660	to be a better radiologist in many ways than our radiologists
5959660	5962180	or we couldn't have done that last year or two years ago.
5962180	5966820	It's because if you had spent a lot of money doing that,
5967980	5969780	how are you going to get your money back?
5969780	5971700	How are you going to actually help patients?
5971700	5973420	How are you going to save lives?
5973420	5975260	How are you going to improve our system
5975260	5976460	if no one's gonna let you, right?
5976460	5978420	And if the radiologist is like,
5978420	5982060	going to stubbornly double check everything the system does
5982060	5983580	and then substitute his judgment
5983580	5985700	for the systems reasonably often,
5985700	5988180	the system is not actually going to be helpful.
5988180	5990660	I have definitely kind of expected
5990660	5995420	some sort of other part of the world deployment,
5995420	6000420	kind of possible leapfrog effects as it becomes very hard
6002260	6005900	to say that people who currently have no radiologist
6005900	6008020	shouldn't have access to something like this.
6008020	6013020	Yeah, the problem with that is that most of those places
6013020	6016540	have deliberately taken market signals
6016540	6020380	and compensation away from their healthcare systems.
6020380	6022100	And they're also relatively small markets
6022100	6023700	that are relatively poor.
6023700	6026020	So they just aren't big enough markets
6026020	6030100	in an economic sense to justify the creation and training
6030100	6032180	and tuning of these systems.
6032180	6035140	And also like nobody involved wants to be the ones
6035140	6036820	who stick their neck out, right?
6036820	6040820	And like take the blame and responsibility for this thing.
6040820	6044140	That's like these weird Americans who won't themselves use it
6044140	6045540	are suddenly creating,
6045540	6049820	like it's a really, really bad cultural social context
6049820	6051420	for trying to make this happen.
6051420	6054420	We also have a problem of the elites of the world.
6054420	6055540	This is what we saw of COVID, right?
6055540	6057700	Like you would have expected in COVID,
6057700	6059820	someone somewhere to do challenge trials,
6059820	6062660	someone somewhere to actually study the spread of COVID
6062660	6064280	and what exactly did what,
6064280	6065980	someone somewhere to do all sorts of things
6065980	6068100	and nobody did any of it
6068100	6071340	because all of the elites of the world basically got together
6071340	6074220	and converged upon what they thought was the consensus
6074220	6075540	and the right thing to do.
6075540	6077780	And nobody said,
6077780	6078820	well, we're going to be the ones
6078820	6081060	who gain advantage by defying that.
6081060	6083020	And so we're increasingly seeing that pattern
6083020	6084740	a wide variety of places.
6084740	6087380	One, the, you know, nobody wants to be the one
6087380	6088500	to stick their neck out.
6088500	6091500	And two, like how do you recoup your investment?
6091500	6094580	Pretty natural bridges to our next live player,
6094580	6096100	which is Meta.
6096100	6099660	And obviously they have been in the news recently
6099660	6102860	for releasing Lama 2.
6102860	6105940	And this brings up a lot of these questions to me.
6105940	6107700	Like, first of all,
6107700	6111180	and Imad Mostak from stability said this
6111180	6113900	actually in a recent episode,
6113900	6118580	he was like, the leaders are non-economic actors.
6118580	6121940	And he was specifically referring to open AI and Google
6121940	6124580	not seemingly being motivated by money
6124580	6126820	in the way that a typical company would be,
6126820	6129660	you know, open AI trying to commoditize its own product
6129660	6131980	as quickly as they possibly can, you know, on record
6131980	6133220	being like, we're going to drive the price
6133220	6135380	of intelligence as low as we possibly can,
6135380	6137380	as fast as we possibly can.
6137380	6139180	Google, you know, is obviously just kind of
6139180	6141300	trying to defend itself more than anything else.
6141300	6142300	They don't need to make more money.
6142300	6145380	They just need to not lose their spot.
6145380	6148660	Anthropic, we take as a safety first play.
6148660	6151140	And, you know, certainly they don't seem to be trying
6151140	6153500	to maximize revenue from what I can tell right now.
6153500	6155740	But then Meta is taking this to a whole other level,
6155740	6159100	arguably, where they seem to be kind of yoloing
6159100	6161460	the whole thing and being like,
6161460	6163620	that's a little bit flippant because certainly
6163620	6167780	with this Llama 2 release, they took some steps,
6167780	6171420	you know, they didn't just release the totally naked,
6171420	6173660	pre-trained model, but they actually did, you know,
6173660	6175860	the kind of what you're supposed to do
6175860	6179620	if you're going to be a responsible frontier model developer
6179620	6183020	with a red teaming process and an RLHF and so on.
6183020	6185900	And, you know, we can also get into did they overdo it
6185900	6187940	or does it refuse too much and all that kind of stuff.
6187940	6190100	But just for starters, like, what do you think is going on
6190100	6192860	at Meta that they are willing to put tens of millions
6192860	6196180	of dollars into training a model
6197060	6202060	and then just release it for why exactly?
6202260	6204580	I can't, like, it seems like if you're at any sort
6204580	6209220	of normal corporation, this is like what your risk officer
6209220	6211700	is supposed to put a stop to, right?
6211700	6216700	Lee Roy Jenkins.
6216820	6217740	No.
6217740	6219220	How do you understand this?
6220100	6221820	Idiot disaster monkeys?
6221820	6225060	Let me try to actually answer the question.
6225060	6227420	I think that their business strategy here
6227420	6231020	is cannibalizing the compliment.
6232160	6234100	So the idea is that, you know, the people
6234100	6236540	who they're up against, people who are competing
6236540	6239660	with them fundamentally, this is their business.
6239660	6241620	And so the idea is that in their model,
6241620	6244340	if they can foster an open source environment
6244340	6248300	that replaces the specialties of these other companies
6248300	6252380	that they are competing with, then their hope is
6252380	6255780	that this will, you know, give Facebook a level playing field
6255780	6258860	against them in this way so that Facebook specialties
6258860	6262700	can reign supreme and they can become more dominant
6262700	6264540	and they can erase their deficits.
6264540	6266500	Alternatively, they're just not as good.
6266500	6269140	And so they need the open source community's help
6269180	6271060	to try and keep pace.
6271060	6275900	Alternatively, you know, they think that if they get
6275900	6277500	these people working for them, that's free labor,
6277500	6279740	you know, it creates this whole other network.
6279740	6282220	It's a strategy, right?
6282220	6284700	Like it's, I mean, Android is open source, right?
6284700	6286860	It's not crazy to open source major stuff
6286860	6289180	from a business perspective necessarily.
6289180	6291820	It's crazy for me, that's not all my perspective.
6291820	6293740	I think that, you know, realistically,
6293740	6296900	like senators gave them what the hell
6296900	6298820	about releasing Lama One.
6298820	6299700	Did you give them a much bigger one
6299700	6301340	about releasing Lama Two?
6301340	6306020	You know, if we are concerned about beating China
6307060	6309180	to the extent that we are considering, you know,
6309180	6310980	we're implementing a variety of export controls
6310980	6314340	and we are considering actively, you know,
6314340	6318100	subsidizing capabilities or at least not being willing
6318100	6322220	to slow down our capabilities, then we damn well
6322220	6324220	shouldn't be releasing Lama Two
6324220	6325180	as an open source product.
6325180	6326540	That's completely insane, right?
6326540	6328460	Like just, even if you don't get any immediate
6328460	6332180	direct danger doing that, it's completely nuts.
6332180	6333940	I think that should be stopped.
6333940	6335380	And I think that this philosophy,
6335380	6336660	if allowed to become ingrained,
6336660	6340180	like creates the systematic groundwork
6340180	6341820	for future open source work
6341820	6344340	that then like is the maximally dangerous thing.
6344340	6345580	I call it the worst thing you can do, right?
6345580	6347340	Like creating frontier models
6347340	6349420	and open sourcing them is the worst thing you can do.
6349420	6351460	Like in the world, you know, Yama Kun
6351460	6356380	and others at Metta either sincerely believe
6356380	6357860	that there is actually no danger
6357860	6358860	from artificial intelligence
6358860	6361900	despite this making absolutely no physical sense
6361900	6364420	or they don't care and they're lying about it.
6364420	6366540	I don't want to speculate
6366540	6368660	as to exactly what's motivating these people
6368660	6370700	but they're smarter than the arguments they're making.
6370700	6372380	They know better Zuckerberg himself
6372380	6374220	is smarter than this to some extent, right?
6374220	6378220	He said on, I believe it was Lex Bidman's podcast
6378220	6380220	that, you know, there will be future models
6380220	6381540	that we'll have to be very careful with.
6381540	6383260	We want open source
6383260	6384900	and we're gonna have to think about these problems
6384900	6388260	but for now it doesn't seem necessary.
6388260	6390060	And, you know, if I were him,
6390060	6391620	I would be very concerned about the culture I'm creating
6391620	6392740	and the precedents I'm laying down
6392740	6395220	and the open source community that I'm creating
6395220	6397780	that's going to be a huge problem for you later
6397780	6399220	and create tremendous pressures on you
6399220	6401020	and create a potential competition for you
6401020	6402620	and that you don't want.
6402620	6405660	But, you know, I sort of understand
6405660	6408580	from a business perspective, while you might want to do that.
6408580	6412100	Also, they want to attract open source developers
6412100	6414100	to work at Facebook Metta
6414100	6415580	because there's a whole group of people
6415580	6417460	who are quite good at coding
6417460	6420340	who have philosophically fanatical devotion
6420340	6422700	to this idea that software wants to be free
6422700	6424260	and that everything should be open source
6424260	6428500	and who just prioritize that over something like,
6428500	6430220	you know, worrying about alignment
6430220	6431500	and what would happen if we failed
6431500	6433420	or worrying about the proliferation of,
6433420	6436220	you know, artificial intelligence in various senses
6436220	6439100	and just have this ironclad belief
6439100	6441020	that like concentration of power is bad
6441020	6443580	and that if you just give the people the things
6443580	6446100	that it'll all somehow work out.
6446100	6449660	And I don't think that in this situation.
6449660	6451300	I think that situation is very wrong,
6451300	6454940	but they clearly believe otherwise.
6454940	6458340	And, you know, look, they've been,
6458340	6459540	Facebook has been in my mind
6459540	6462380	like the detonated villain of the piece
6462380	6463700	for a very long time.
6464660	6466380	Like long before artificial intelligence
6466380	6468660	even entered the commercial picture.
6468660	6470420	So it just somehow feels fitting.
6470420	6471780	You know, if it was all gonna finally get destroyed
6471780	6473380	by Facebook, it just seems right.
6474780	6479140	Well, I very strongly try to resist
6479140	6484140	psychologizing in the AI discourse too much.
6484260	6487300	Really at all, I try to avoid it basically entirely
6487300	6491700	because it just seems like, you know,
6491700	6493460	nothing good ever comes of it really.
6493460	6496300	But I have also struggled to come up with a,
6496300	6500340	what feels to me like a coherent argument here
6500340	6503180	that isn't on some level just ideological
6503180	6505580	because I kind of ran through all the things
6505580	6507020	that you were mentioning as well,
6507020	6509380	starting with like, well, maybe you can, you know,
6509380	6512700	undermine your competitors, core business.
6512700	6514740	But then I'm like, yeah, but you're not really gonna do that.
6514740	6518180	Like, does anybody expect OpenAI's token serve
6518180	6520060	to go down as a result of this?
6520060	6521820	I don't, I think they're gonna continue.
6521820	6524100	Like they're GPU limited.
6524100	6526940	And I think they're gonna continue to be GPU limited,
6526940	6528700	you know, maybe slightly less,
6528700	6531460	but like, I don't think their top line suffers.
6531460	6533660	I don't think their token serve suffers.
6533660	6536660	Their leadership position doesn't really seem to suffer.
6536660	6539780	I can't really get to a point where I'm like,
6539780	6542060	seeing the return, and on the open source thing too,
6542060	6545260	I'm kind of like, you know, that was part of that memo
6546220	6547660	from the Google memo of like,
6547660	6549580	oh, you know, they've got this big open source community
6549580	6552180	or whatever, but I don't really buy that memo either
6552180	6554380	or that analysis because I'm like,
6554380	6556660	everybody benefits for, or, you know,
6556660	6559580	whatever the impact is of all the sort of open source
6559580	6561740	hacking that's happening,
6561740	6564540	it seems to accrue to everybody pretty equally.
6564540	6568260	Like, yes, maybe it was done on this llama 2 base,
6568260	6570180	and like, maybe that's something that Facebook
6570180	6572580	could kind of readily fold back in,
6572580	6575140	whereas, you know, Google with their 700 plus million user,
6575140	6578300	whatever, you know, can't take direct advantage of it.
6578300	6579940	But to the degree that people are out there doing things
6579940	6583820	like quantizing models and making them run on, you know,
6583820	6585460	consumer devices or whatever,
6585460	6589540	that's obviously a technique that Google can also say,
6589540	6592620	hey, look at this, this works, you know, we can do it.
6592620	6595860	I just don't see a lot coming out of the open source
6595860	6599460	experimentation that feels like it specifically accrues
6599460	6603820	to Meta's benefit, and so in the end,
6603820	6607860	it just feels like more of a principled, you know,
6607860	6610220	to put it in a more conventionally positive framing,
6610220	6612620	it feels more of like a principled decision
6612700	6616660	than a, you know, tactical or sort of, you know,
6616660	6618100	results oriented.
6618100	6619900	And there is still recruitment,
6619900	6623420	but I strongly agree that, you know,
6623420	6627260	any advances that the open source community discovers
6627260	6630460	are gonna be at Google and Anthropic and OpenAI
6630460	6633260	a month later, if not a week later,
6633260	6635660	and they'll also be at Baidu, right?
6635660	6638380	Like, they'll also be at all these different Chinese companies.
6638380	6642300	And so this long-term strategy cannot be allowed to continue
6642300	6644700	in some important sense, I would assume.
6644700	6646300	Yeah, it's really scary.
6646300	6647820	I'm glad they suck.
6647820	6648660	Like, is it a very good thing?
6648660	6650300	They're not very good at this, right?
6650300	6652700	And they've produced lousy products
6652700	6655700	because if that wasn't true, we'd be in a lot of trouble.
6655700	6657940	That seems harsh to me.
6657940	6660860	I mean, it seems like this Lama II model
6660860	6662300	is pretty good, right?
6662300	6666780	I mean, it's not GBT-4, but it does seem to be on par-ish
6666780	6671780	with 3.5, which no other open model
6671780	6673460	has come close to.
6673460	6675540	I mean, I think Rune said, you know,
6675540	6677540	best open source model sounds a lot better
6677540	6678700	than fifth-best model.
6680100	6681500	That's definitely true.
6681500	6683580	But, you know, first of all, I'm not sure
6683580	6685020	that that means that they couldn't have done better.
6685020	6687860	If you look at the curves in the Lama II paper,
6687860	6689260	they have not flattened out, right?
6689260	6691580	I mean, it looks like even the 70B one,
6691580	6696260	if they just keep training, you know, the loss,
6696260	6699700	it looks like it's definitely gonna continue to go down.
6699700	6702180	So for all I know, you know, this was kind of
6702180	6706780	where they stopped and they may have internally, you know,
6706780	6708220	this may be the checkpoint that they released,
6708220	6709820	but not necessarily the final checkpoint.
6709820	6711740	Like it just doesn't look like this was a project
6711740	6715660	that was kind of at its, you know, maximum performance.
6715660	6717500	Oh, definitely possible.
6717500	6719540	But at the same time, you know,
6719540	6721740	they probably are still training, but so is OpenAI
6721740	6723340	and so is Google and so is Anthropoc.
6723340	6724300	Everyone is working.
6725300	6727460	I mean, I see what seems to have been produced
6727500	6731860	is indeed like about a 3.4 level operation
6731860	6733780	where X coding, it's around 3.5
6733780	6737260	and it coding is pretty bad from all reports.
6737260	6741700	Its alignment is very ambilicious.
6741700	6742780	I guess it'd be the best way to put it.
6742780	6745700	Like it's very, very crude and blunt.
6746620	6750980	And also it's entirely optional because it's open source.
6750980	6752500	And that's kind of a problem.
6752500	6755260	According to reports I have heard, I have not sorted out.
6755300	6757940	It took all of several days
6757940	6761220	for the unaligned version of Llama 2 to be on the internet
6761220	6765580	because it's really, really not hard
6765580	6768980	to fine tune a system
6768980	6772060	to never refuse any customer requests for any reason.
6773380	6775420	Right, that is the easiest task to,
6775420	6777420	like you would just read constitutionally,
6777420	6778660	I script in a minute, right?
6778660	6781260	Like every time you see any of these words
6781260	6783140	that say, I can't say that,
6783140	6784940	for whatever reason, you just give a negative reinforcement
6784980	6786460	and it stops doing that.
6786460	6788980	Like I presume that would just work.
6788980	6792220	And so voila, here we are.
6792220	6794340	You wanna build the bomb, here's how you build the bomb.
6794340	6795460	You wanna research a biologic
6795460	6797340	and we'll try to research a biologic.
6797340	6799420	You want it to be racist?
6799420	6801780	All right, who are we making fun of?
6801780	6802940	Yeah, let's go.
6804060	6806100	So you can have it, refuse to speak Arabic
6806100	6810300	all you want in the original, they won't last.
6810300	6813740	So if nothing else, in my view,
6813740	6816580	this definitely puts them in the live player category
6816580	6818700	because it does seem like, if I define that
6818700	6822260	as the organizations that have the ability
6822260	6826940	to shape how events unfold in some non-trivial way,
6826940	6829620	like they are doing that now, it seems.
6829620	6830700	If you ask yourself, right,
6830700	6833060	like what resources would you have to give me
6833060	6834620	before I could have produced Lama too,
6834620	6836460	if I was willing to just like write the money on fire
6836460	6837540	to do it?
6837540	6840580	I mean, I don't have the technical chops myself,
6840620	6844460	but it doesn't feel like it would have been that hard.
6844460	6846940	I don't know, like it's just a matter of
6846940	6849820	are you willing to spend that kind of money,
6849820	6851980	build up that kind of technical infrastructure
6851980	6856460	to just do, like you read the paper for Lama too
6856460	6860100	and like it reads as if they're saying,
6860100	6861820	we did the thing you would stand,
6861820	6865020	we did the standard issue thing at every step
6865020	6867020	and this is what we got, right?
6867020	6870620	We did nothing original, we did nothing surprising,
6870620	6872260	we just did our jobs.
6872260	6875460	And like it's hard to do your jobs well in some of that.
6875460	6876980	Like it's not like they didn't accomplish anything,
6876980	6879620	but they just didn't do anything, right?
6879620	6880940	They just did the thing.
6881900	6884860	And, you know, it's a marginal improvement
6884860	6887220	over previous efforts that probably it's just
6887220	6890260	because it was better resourced, as far as I can tell.
6890260	6892020	Simple as that.
6892020	6893900	And like they are willing to light up more money on fire
6893900	6896340	than Baikuna, right, or Hugging Face.
6897340	6899780	Because, you know, they have a lot of money to light on fire
6899780	6901660	and Zuckerberg doesn't get it.
6901660	6903700	So fire, money, go.
6903700	6907700	He's certainly proven that he will spend some money
6907700	6910260	on a project, no doubt about that.
6910260	6913980	I wanted to maybe cover two more things.
6913980	6918980	One is what else would you put on the live players list
6919500	6924500	beyond what I have on my live players list.
6925460	6927500	We've discussed four today,
6927500	6931140	but I've got like another half dozen or so on there.
6931140	6934260	And you can run them down and offer any comment if you want.
6934260	6935780	And then I'm especially interested to hear
6935780	6937700	if you think there are other names
6937700	6939900	that should be on that list that I don't have.
6939900	6942700	Yeah, so I guess it's a matter of like
6942700	6945060	how wide a scope you wanna think about
6945060	6948540	and like who might do whatever it is.
6948540	6950020	You know, obviously like, you know,
6950020	6953620	character AI and inflection AI have very large budgets,
6953620	6956380	you know, potentially very large user bases.
6956380	6959220	I have seen no intention from them that they want to be live.
6960340	6962740	Like they're sort of content to be dead,
6962740	6964500	but to try and make a lot of money while being dead.
6964500	6966500	And that seems fine with me.
6966500	6970740	We haven't talked about X.AI yet.
6970740	6973980	So like XAI is like the latest attempt by Elon
6973980	6975900	to like string together a bunch of words
6975900	6977620	as if they have meaning
6977620	6979140	and then pretend that constitutes some hope
6979140	6980620	for humanity or alignment.
6981780	6984380	When anybody who actually like tries to parse those words
6984380	6985740	into a meaningful English sentence goes,
6985740	6987340	wait, that doesn't make any sense.
6987340	6990780	I don't know how to be more blunt than that.
6990780	6992220	That's just how it is, right?
6992220	6997220	Like, but the good news is that like at open AI,
6997540	6999940	everybody quickly realized that Elon's suggestions
6999940	7002340	were stupid and just ignored them.
7002340	7005020	And that's what I expect to happen with any,
7005020	7006420	like if the engineers don't do that
7006420	7008660	and the engineers won't produce anything useful.
7008660	7010540	So to the extent that XAI is a real thing,
7010540	7012620	the engineers will mostly ignore him.
7012620	7014940	And then the other question is,
7014940	7017340	are they gonna get the kind of funding and resourcing
7017340	7019340	that is required for them to be a serious rival?
7019340	7022140	Because, you know, it wasn't clear exactly
7022140	7024860	what they had in mind, but I think it's certainly possible.
7024860	7025860	You know, from what I've seen,
7025860	7027180	I don't think we have to worry particularly
7027180	7030540	about Salesforce or Replet in a meaningful way.
7030540	7031780	Like it's not that they don't exist.
7031780	7036060	It's that like, we have any reason to worry about that.
7036100	7038620	China writ large is the big, is the other,
7038620	7041980	big question marks are like China, the UK and the US.
7041980	7044500	You know, the UK has announced plans for the global summit.
7044500	7047860	They seem to be willing to make a significant play
7047860	7051580	on the safety front, on the also capabilities front,
7051580	7053500	in terms of just trying to make the UK important again.
7053500	7055980	They have, you know, various people located in the UK.
7055980	7057500	It makes sense for them to try.
7057500	7059940	I don't know why they don't build any houses,
7059940	7062540	but you know, at least they're trying something.
7062540	7063900	We do obviously have to look at,
7063900	7065540	like they were holding congressional hearings.
7065540	7067700	The US Congress is starting to get up to speed.
7067700	7069980	They're starting to explore what to do,
7069980	7071620	what they do matters immensely.
7071620	7073540	What the EU does potentially matters immensely
7073540	7075980	from regulatory standpoint, because it's a huge market.
7075980	7078580	Right, like, are they gonna shut these people out?
7078580	7079860	Are they gonna require them to jump
7079860	7081620	through ridiculously bizarre hoops?
7081620	7082860	Are they going to only be available
7082860	7083700	to the biggest players?
7083700	7086540	Like, these things are things to think about carefully.
7086540	7090700	I think America could potentially be a very helpful
7090700	7093180	or harmful aspect of this whole problem,
7093180	7094260	depending on how things shake out.
7094260	7096740	That's one of the big battle fields that we're having up.
7096740	7098860	And then China's the big wild card, right?
7098860	7101740	Like, I hear very different things from different sources,
7101740	7103860	people who assume that, you know,
7103860	7106820	China is, you know, crazy people bent on,
7106820	7108140	you know, the Chinese Communist Party
7108140	7110260	is, for now, it's bent on world domination
7110260	7111580	who will stop at nothing.
7111580	7115380	And our inevitable rivals in the apocalypse,
7115380	7119220	and if we don't prepare, we will lose to them.
7119220	7121860	And then, you know, they issue guidance
7121860	7123380	that basically bends all deployment
7123420	7125260	of large language models,
7125260	7127540	and they never caught anything.
7127540	7130340	And like, you know, it's very hard to tell
7130340	7132300	what's really going on,
7132300	7134420	or how much they would cooperate in the name of safety.
7134420	7137420	And we've also just never picked up the phone.
7137420	7138700	We've never asked them the question.
7138700	7141460	We've never explored to see if they'd be interested.
7142460	7144940	But, you know, the same way that in Oppenheimer, right?
7144940	7148860	Like, we keep saying we have to beat our enemies
7148860	7152860	because they will get, you know, everything will be scary.
7152860	7156020	The Chinese can talk about racing us all they like.
7156020	7159340	The only people actually racing are us in any real way.
7159340	7163540	Like, we have the top X, AI companies.
7163540	7165180	What's X?
7165180	7166140	Is it five?
7167300	7169220	Is it more than five?
7169220	7170620	Like, how far down do you have to go
7170620	7171660	before you get to Baidu,
7171660	7175980	or whoever the top Chinese person you'd rank on the list is?
7175980	7177540	Pretty far.
7177540	7178900	I'd say it's probably more than five.
7178900	7180540	I would probably put, obviously,
7180540	7181620	a lot of speculation here,
7181620	7183580	because we don't know what they have
7183580	7184660	that they haven't released.
7184660	7189660	But if we go by papers and, you know,
7189660	7193380	what little we've seen of any sort of Ernie,
7193380	7195860	you know, GPT or whatever that's Ernie Botte,
7195860	7197940	whatever they officially called that,
7197940	7200860	I would say you'd have to put Meta above.
7200860	7202380	You'd have to put Microsoft above.
7202380	7206540	Probably pretty soon would put an inflection above.
7206540	7210980	So, yeah, I mean, you get reasonably far down the list.
7211020	7212060	What about a Palantir?
7212060	7214780	Would you add them on the live players list?
7214780	7219420	I don't have that sense that they are live live,
7219420	7220700	precisely because my threat model
7220700	7222740	doesn't involve things like Palantir
7222740	7225620	being the reason why we are in trouble.
7225620	7228660	But it is a classic way to die, right?
7228660	7232300	Like a semi-military-ish system starts training up stuff,
7232300	7234660	and then one thing leads to another.
7234660	7237100	But they have all the motivations
7237100	7240460	to do the unsafe things in a relatively unsafe fashion.
7240980	7242500	And to take out the safeguard that the people
7242500	7245740	were building in and then to like maybe,
7245740	7246860	but like I don't think they're gonna drive
7246860	7248900	the underlying technology.
7248900	7250660	I don't get that sense.
7250660	7254300	Again, like there are a lot of hedge funds also
7254300	7256660	that like could possibly be sinking quite a lot of money
7256660	7258900	into this in ways that are completely invisible.
7258900	7261260	And it could potentially be live players
7261260	7262340	in a meaningful sense,
7262340	7263540	like who knows how much Bridgewater
7263540	7265660	is spending on this in the end.
7265660	7267020	You know, they're working on it.
7267020	7269220	But yeah, like we talk about like,
7269220	7270060	we're talking about China,
7270060	7272140	but like I'm more afraid of Meta.
7272140	7274060	Like one individual American company
7274060	7276820	scares me more than all of China right now.
7276820	7278940	Yeah, I think that's a good corrective, honestly,
7278940	7281180	because I find nothing more frustrating honestly
7281180	7286180	than when AI conversations sort of end in blanket,
7286660	7290900	basically detail-free claims about what China's gonna do
7290900	7292940	by people that don't know a lot about China.
7292940	7297740	So I don't know if you're necessarily right to be
7297780	7300420	more fearful of Meta than of China,
7300420	7305420	but the fact that that is at least a reasonable position
7305740	7307900	is definitely something I think should cause a lot of people
7307900	7309060	to kind of step back and think,
7309060	7312100	wait a second, maybe I've been a little bit too quick
7312100	7314380	to worry about China.
7314380	7316060	And I would take countermeasures against both of them
7316060	7318580	if I had my way to be clear.
7318580	7320700	But also we're just not acting like China
7320700	7322780	is a serious global rival
7322780	7326380	that we actually care about beating in many other ways
7326420	7328220	that we could be.
7328220	7330980	So okay, revealed preferences, you know,
7330980	7332500	you don't want, you know,
7332500	7334260	do Chinese graduates of STEM programs
7334260	7335340	get to stay in the United States?
7335340	7336820	No, okay, you don't really care that much
7336820	7338780	about who gets the better technology, do you?
7338780	7339860	That's unfortunate.
7339860	7341860	That's my basic attitude there.
7341860	7344660	So just briefly on a couple of the companies
7344660	7349740	that you sort of didn't feel like were live players,
7349740	7352020	again, may have a slightly different meaning of that
7352020	7355140	in mind, but thinking about folks like character
7356060	7357420	and inflection, I put those together
7357420	7361140	because they seem to be playing a sort of different game,
7361140	7363140	you know, with their products where it's like,
7363140	7367180	not about the sort of mundane utility as much as you call it,
7367180	7370780	but more of a companion, a relationship,
7370780	7373100	you know, a coach, almost a therapist,
7374100	7377500	sort of vibe from like Pi in particular.
7377500	7380260	I feel like that is, even if the, I mean,
7380260	7383300	first of all, the character has very good language models
7383300	7386220	and Pi's quite good at what it does as well.
7386220	7387660	I don't think it can code for you,
7387660	7389460	but it does have a certain,
7389460	7392100	also they notably said that they're in their testing
7392100	7395700	totally resistant to the adversarial attacks.
7395700	7398180	So there's another kind of interesting wrinkle there.
7398180	7400820	And I put those guys in the live player list
7400820	7405820	largely because they're looking at some very different use case
7406500	7410100	that feels like the kind of thing that might open up
7410100	7415100	and be transformative in a way that like a coding assistant,
7416260	7417740	while it could also be transformative
7417740	7419940	is just, you know, a very different thing, right?
7419940	7422060	The idea that you would have these AI friends,
7422060	7424460	these AI relationships that they could become like important
7424460	7429460	to your life, going down that path with, you know,
7430020	7435020	very good, even if not totally frontier language model chops,
7435380	7438420	feels like you could meaningfully impact
7438420	7440220	the course of events.
7440220	7441220	Can you?
7441220	7444380	I guess, so, you know, you've got character AI
7444380	7446420	and their idea is, you know, you're building these characters
7446420	7448140	and you can treat them as companions,
7448140	7451620	you can treat them as like people to have a conversation with.
7451620	7453860	And that's interesting.
7453860	7455580	And a lot of people were spending time on it
7455580	7459060	and maybe it will even provide a lot of value for people,
7459060	7461780	but I don't see how it's transformational.
7461780	7463660	I'm curious to hear more about your intuition
7463660	7466100	from best while you think it could be transformational.
7466100	7471100	And I certainly don't see how is we just criticality, right?
7471300	7473460	I don't see how it becomes an RSI.
7473460	7476020	I don't see how it becomes an AGI.
7476020	7477980	And as far as I can tell,
7477980	7481140	they're not pushing the frontiers of actual capabilities.
7481140	7485740	They are building on top of GBT-4, right?
7485740	7487700	Or even in some cases, GBT-3 and a half.
7487700	7490100	And it's not that hard to defend
7490100	7494460	against these weird adversarial attacks
7494460	7496740	in the sense that like I can write some pretty quick
7496740	7499540	if then Python code that detects the adversarial attacks.
7499540	7503420	Yeah, a classifier layer is pretty easy
7503420	7505180	to avoid some of the worst stuff.
7505180	7507660	There's weird non-English,
7507660	7510020	like not any language scaffolding like stuff in it.
7510020	7511780	Let's just get rid of that and run the query without it.
7511780	7513580	Like sure, whatever, it's fine.
7513580	7514860	In Replet's case, it's like, again,
7514860	7518660	they're not necessarily on the frontier of model capability,
7518660	7523660	but the CEO, Amjad, has said a couple of times online
7524300	7526980	on Twitter, on X, on KISS, what is it called?
7526980	7528020	Twitter.
7528020	7529380	Yes, Twitter, okay, thank you.
7529380	7534380	He said that Replet is the perfect substrate for AGI.
7534860	7535900	We have a couple of episodes coming out
7535900	7538060	with a couple of different people on the Replet team.
7538060	7539300	And I've had a chance to explore this
7539300	7540860	and think about it a decent amount.
7540860	7542660	And where I come down is kind of,
7542660	7546020	even if you're not on the frontier of model capabilities,
7546020	7550660	if you are on some other really meaningful frontier,
7550660	7552540	to me, it feels like there's, you know,
7552540	7554420	transformative potential just because
7554420	7555700	we really don't know what's gonna happen.
7555700	7558540	So with character and with inflection,
7558540	7563540	it's like kind of like a Harari style thought that,
7564980	7565820	you know, I don't know,
7565820	7567900	it could be transformative in the way that like,
7567900	7570180	opium could be transformative to a society.
7570180	7572940	You know, if everybody starts like doing this stuff,
7572940	7576020	it could be greatly empowering and enabling.
7576020	7577460	It could be greatly disabling
7577460	7580020	if it just kind of becomes a huge tension suck
7580020	7584140	where it like, you know, outcompetes real relationships.
7584140	7586340	You know, those are not takeover the world scenarios,
7586340	7588620	but they do feel, you know, as much as we've like,
7588620	7590820	would you say that the cell phone has been transformative?
7590820	7592340	I would, I mean, not, you know,
7592340	7596020	not transformative on the level that like AGI could perhaps be,
7596020	7598300	but certainly we all go around looking at our phones
7598300	7599180	all the time.
7599180	7600780	And if we all go around looking at our phones
7600780	7602180	with an AI friend on it,
7602180	7604100	who's like our best friend all the time,
7604100	7606380	then that would feel like, you know, transformative,
7606380	7608380	even if it's like going super well.
7608380	7610460	And then with Replint, it's like, you know,
7610460	7613460	there's no better place right now
7613460	7618460	to directly execute code generated by an AI,
7618820	7620660	you know, for better or worse.
7620660	7625180	So the kind of frontier that I see opening up there
7625180	7627700	is one where, and their stated goal
7627700	7629580	is to bring the next billion developers online,
7629580	7631740	which I think is super exciting in some ways.
7631740	7634340	But then also I've worked with some of those
7634340	7636340	next billion developers and I'm like,
7637220	7641140	these are people who don't know how to code today,
7641140	7643420	don't even know really how to read code,
7643420	7648420	and are going to be dramatically more dependent on
7648620	7650460	and vulnerable to the, you know,
7650460	7652860	the various vagaries of AI systems
7652860	7655580	than, you know, the first 100 million developers
7655580	7657380	or, you know, whatever we have today.
7657380	7660180	I don't know, both of those feel like kind of different vectors
7660180	7662380	of transformative potential, but.
7662380	7665300	The first and only so far interaction I've had
7665340	7669700	with the CEO of Replet was when he commented on Twitter
7669700	7672220	that there was a non-zero chance that auto,
7672220	7675220	some version of auto GPT would take over Replet
7675220	7679180	and, you know, through replication within its servers.
7679180	7681500	To which my response was, did you say non-zero chance?
7681500	7684500	And I put up a manifold market on it because it was funny,
7684500	7686300	which probably get back into the single digits
7686300	7687140	or whatever, obviously.
7687140	7689380	It's not that likely, but, you know,
7689380	7694020	his cavalier attitude of, oh, nothing to see here,
7694020	7696740	justice self-replicating AI on my servers,
7696740	7698260	leaving lots and lots of copies of itself
7698260	7699500	and executing arbitrary code.
7699500	7701420	Why should we worry about this?
7701420	7703460	I mean, definition of idiot disaster monkey, right?
7703460	7707540	Like just complete indifference to what he was doing
7707540	7709780	or what dangers it might pose.
7709780	7711700	But at the same time, not doing anything, right?
7711700	7713220	Like all he's doing is providing,
7713220	7716500	as you said, a substrate where people can just run stuff.
7716500	7719580	And so to me, like, it doesn't give them any say
7719580	7721060	over what happens.
7721060	7723980	It doesn't like make them a meaningful actor, right?
7724940	7728620	Like in the sense of me caring here about the future,
7728620	7731380	I just can't see that as a thing.
7731380	7733980	Similarly with character and inflection,
7733980	7736700	like I can definitely see a world in which,
7736700	7739980	like people talking to their AI's matters
7740820	7744460	and is like multi-transformational, right?
7744460	7746380	Like changes how we live our lives,
7746380	7750500	but like doesn't go critical, right, in some sense.
7750500	7752300	But if that's true, then like,
7752300	7754340	I don't see these companies
7754340	7758180	as like changing that path very much
7758180	7760900	versus what would have happened anyway, right?
7760900	7762380	Like I think that there are plenty of people
7762380	7765460	will be able to create AI companions of various types
7765460	7768180	and never will create AI companions of various types.
7769340	7770540	If they do an especially good job,
7770540	7771700	maybe they'll have some sort of remote,
7771700	7773780	maybe they'll establish customer loyalty or some shit,
7773780	7775740	but it doesn't excite me.
7775740	7778220	I also just don't see it like,
7778220	7779620	I see all these huge like, you know,
7779620	7781380	people were spending as much time on character
7781420	7784100	as they are on GPT-4 or something.
7784100	7786620	And yet like, why?
7786620	7788060	Like what is the draw?
7788060	7789140	Did you read that?
7789140	7793300	There was a less wrong post early this year, I think,
7793300	7798260	from a guy who basically the point of view was,
7798260	7799700	I'm a technology person.
7799700	7800980	I'm now speaking in the first person
7800980	7803460	of the author of this post.
7803460	7804660	I'm a technology person.
7804660	7807180	I know how language models work.
7807180	7808780	I should have known better,
7808780	7812180	but here's what happened to me as I started to,
7812180	7813860	I think he was like in a kind of vulnerable state
7813860	7815700	because he'd maybe just broken up with somebody
7815700	7816540	or something like that.
7816540	7819580	And all of a sudden is having these very intimate conversations
7819580	7823460	with a character, AI character that he had prompted
7823460	7826460	to create the ultimate girlfriend experience,
7826460	7828940	I believe was the phrase,
7828940	7833940	and started talking himself into various weird perspectives
7834260	7836380	like, well, what's real anyway?
7836380	7837740	And like, yes, of course I'm real,
7838700	7841860	is there anything truly less real about these sort of,
7841860	7844660	all I really have are my kind of ephemeral qualia.
7844660	7848340	And so, this thing is just sort of an ephemeral,
7848340	7850540	whatever, but we're all just kind of constantly
7850540	7851780	waking up in the current moment.
7851780	7854700	And so maybe we're not that different after all or whatever.
7854700	7857860	And eventually it got pretty weird, it sounds like,
7857860	7860220	and the post is I think extremely compelling.
7860220	7863980	And then eventually kind of person snapped out of it.
7863980	7867020	That sort of story is kind of why I feel like
7867020	7869380	there's just unknown unknowns there.
7869380	7871260	That if that kind of thing can happen to somebody
7871260	7874460	who knows how language models work going in,
7874460	7876620	maybe we should all think we're a little bit more vulnerable
7876620	7880380	to a sort of somewhat more refined,
7880380	7882980	somewhat more super stimulus-y.
7882980	7884380	So like it's well known that like,
7884380	7885740	knowing how hypnosis works
7885740	7888540	does not make you less susceptible to hypnosis, right?
7888540	7890780	It makes you more susceptible to hypnosis.
7890780	7892620	Like as a concrete example,
7893620	7897620	if you are a con man, you are easier to con, right?
7897620	7901220	Not harder because you like pick up on
7901220	7902900	and like get involved in all these dynamics
7902900	7904620	and like you think you're smarter than everybody else
7904620	7906380	and you of course are greedy.
7906380	7907980	And so you will pick up on the opportunity
7907980	7910180	and like perceive everything that's happening
7910180	7912300	and like you think you've got it made,
7912300	7914260	but like if you don't know that you're the mark,
7914260	7917940	well, yeah, that's the easiest way to get a mark
7917940	7919700	is to make them think you're the mark.
7919740	7922820	So it all gets, you know, very complicated.
7922820	7924420	I'm not convinced that like,
7924420	7925780	a person who knows how long to work
7925780	7929100	is necessarily that much better protected in that sense.
7929100	7931060	You know, someone whose like head is kind of
7931060	7935020	not on the ground in some ways is more vulnerable potentially.
7935020	7936940	I would say, yeah, that's gonna happen, right?
7936940	7937980	People are gonna fool themselves
7937980	7941140	into these things periodically and that's gonna,
7941140	7943620	I'm kind of surprised it's happening now.
7943620	7946420	I feel like the tech isn't there to me.
7946420	7947860	Like it's just not good enough.
7947860	7951460	Like how are you falling for this level of it?
7951460	7953300	Like I can sort of understand
7953300	7956460	why you'd fall for like GPT-5, right?
7956460	7958500	Like sort of the more advanced version of it,
7958500	7961620	but, you know, you're in a bad space
7961620	7963340	and like you need something to respond to you
7963340	7967340	and it's something and like us, but, you know,
7967340	7969420	again, I just don't know.
7969420	7972780	Like I play a lot of games though, right?
7972780	7976340	Which is like not necessarily that different in some sense.
7976340	7979260	So, and also like it's not transformational
7979260	7980100	for that to be true, right?
7980100	7982460	Like if somebody spends a bunch of time
7982460	7984980	in a playing World of Warcraft,
7984980	7986820	is that transformational, right?
7986820	7989980	Like it's an experience, it's a major force in their life.
7989980	7991260	Does it really matter?
7991260	7993860	Yeah, I think some of these things are only,
7993860	7996900	they may only matter if certain other things don't happen.
7998020	8000780	So, yeah, like I would say, yeah, World of Warcraft,
8000780	8004340	you know, gaming writ large, you know, at some point,
8004340	8006420	if the birth rate goes low enough, you know,
8006420	8010580	it's transformative and the details, you know,
8010580	8012340	of like exactly what games people were playing
8012340	8014780	or how exactly they were amusing themselves, you know,
8014780	8016900	to death didn't, don't necessarily matter,
8016900	8018860	but the fact that they did, and then, you know,
8018860	8020660	you have like a population collapse.
8020660	8023980	A scenario like that, I think is, at least in my sense,
8023980	8026020	kind of qualifies as transformative,
8026020	8028740	but it sounds like from your perspective,
8028740	8032740	the Live Players list is very short and it is,
8032740	8035260	if I understand correctly, it would be obviously open AI,
8035260	8038900	anthropic, Google, DeepMind, probably meta,
8038900	8041460	not sure about Microsoft, and then China,
8041460	8042820	and that's maybe it.
8042820	8044100	Something like that.
8044100	8045580	Regulators?
8045580	8047540	Yeah, regulators writ large in some sense,
8047540	8050780	like individual people that can influence things, you know,
8050780	8053180	like, is that the Ezreal Live Player?
8053180	8055420	You know, I don't know, from their perspective.
8055420	8056940	Like he's not gonna build it.
8056940	8060060	Yeah, that's why I had Salesforce and Marcini off on there,
8060060	8063740	because they published him and others in Time Magazine
8063740	8065380	and seemed like they're kind of,
8065380	8068300	they're both like playing in the research game.
8068300	8070660	Yeah, I hope that like Senator Blumenthal
8070660	8074460	might be a Live Player, you know, in some sense, right?
8074460	8076780	And you've got all these other possibilities.
8076780	8081260	I hope I'm a Live Player, like in some sense.
8081260	8083300	You know, I mean, we're all trying to make a difference
8083300	8086140	in some ways, but, you know, in terms of direct level,
8086140	8090420	you're indirect, and, you know, I'm also indirect in that
8090420	8092500	we're only influencing other minds, right,
8092500	8095300	who then will make decisions.
8095300	8096700	You know, in terms of like,
8096700	8099220	who's making the ultimate decisions,
8099220	8100820	who's doing the things that ultimately matter,
8100820	8103380	I think it's right now a very short list.
8103380	8106380	But, you know, Anthropic is like barely over a year old.
8106380	8108020	Yeah, and only about 150 people
8108020	8110540	may be able to close in on 200, so.
8110540	8113780	Yeah, and like people who just like are a big incredible team
8113780	8114900	and say the words Foundation Model
8114900	8116860	get hundreds of millions of dollars
8116860	8118420	just by asking nicely.
8118420	8120700	Inflection has more than a billion.
8120700	8123300	So, you know, I don't think we can rule out
8123300	8126820	these people become Live Players in that way.
8126820	8130740	I just don't think that's by default what they do.
8130740	8132700	But I think by default,
8132700	8135260	they're trying to build consumer products
8135260	8137900	that are aiming to be products.
8137900	8140620	And that like, you know that the study that says that
8140620	8144300	like when people look at GPT 3.5 and GPT 4.0 outputs,
8144300	8146500	they prefer the 3.5 output,
8146500	8149340	like a remarkably large percentage of the time,
8149340	8151780	even though it is obviously a vastly inferior system.
8151780	8155140	Yeah, 70.30 was the original report
8155140	8157420	in the GPT 4.0 technical report.
8157420	8160460	That 70% for GPT 4.0, 30 for 3.5.
8160460	8161860	So yeah, that blew my mind as well.
8161860	8164380	Yeah, and similarly when I'm using Claude
8164380	8166940	versus everything GPT 4.0, right?
8166940	8170860	Like most of the time what I care about
8170860	8172980	is not like this inherent raw power
8173700	8176140	that GPT 4.0 is extra GPTs, right?
8176140	8178020	Most of the time when I'm looking at it as, you know,
8178020	8180100	which of these things is in the style,
8180100	8181900	it's easier to use, it's gonna require me
8181900	8183780	to do less pump engineering to get what I want,
8183780	8185580	it's gonna actually give me the query that I want,
8185580	8189580	not refuse, you know, which window do I have open?
8189580	8191380	Which one can I click on faster, right?
8191380	8193060	I just want an answer.
8193060	8194820	It's fine or whatever.
8194820	8199100	And you know, habits form in that kind of way
8199100	8200540	and they build on each other.
8200580	8203620	But if I'm building, you know, inflection,
8203620	8205100	like if people are spending two hours a day
8205100	8207300	on character AI now, right?
8207300	8208820	When they're built on three and a half,
8208820	8209900	is my understanding mostly?
8209900	8211700	Cause four is too expensive.
8211700	8215540	You can't be doing two hours of conversations
8215540	8216940	when we bespoke GPT 4.0,
8216940	8219700	which is why I'm so surprised that these things are working,
8219700	8220540	right?
8220540	8222780	Like maybe a four like has enough juice in it,
8222780	8226540	but like if you unshackled it from its like constraints,
8226540	8228180	it could do something interesting.
8228180	8230380	But three and a half, like really,
8230380	8233300	this is keeping you two hours a day on.
8233300	8236900	So like, if that's already doing that, right?
8236900	8238980	That kind of illustrates that like the market
8238980	8240740	they're targeting, right?
8240740	8242380	Isn't looking for intelligence.
8242380	8247380	It's looking for a certain type of experience.
8248060	8249900	And therefore they're not going to be focusing
8249900	8252780	on the billions of dollars of spend
8252780	8256340	it would take to tune up like GPT 4.5 or 5, right?
8256340	8257220	You wouldn't want to
8257220	8260260	because they're gonna cost more to run, right?
8260260	8261420	Like they're going to be bigger models.
8261420	8262980	They're going to be more complex models.
8262980	8264500	Instead, what you want to do is you want to create
8264500	8267940	really bespoke specific models
8267940	8272220	that provide specific types of experiences to people, right?
8272220	8275380	You know, fine tune them to an inch of their lives
8275380	8280340	to give people the best specific experience, right?
8280340	8282380	Like not train something big in general.
8282380	8284540	So there's going to be getting the big in general
8284540	8289420	from open AI and anthropic and deep mind, probably.
8289420	8292100	And maybe they'll just use like Lama 3,
8292100	8294340	you know, Virgins of Lama, because what the hell?
8294340	8296820	It's open source, they can just use it.
8296820	8298260	Like to the extent that Meta will not like,
8298260	8299620	Meta doesn't quite release it, right?
8299620	8300940	They've said that like,
8300940	8303020	if you have more than 700 million daily users,
8303020	8305260	you have to apply for a license or some shit.
8305260	8307860	So we'll come back to the live players list
8307860	8310900	and potentially I'll make a little,
8310900	8312380	maybe make a few changes to my slides
8312380	8313660	based on your feedback.
8313660	8317140	And we can monitor in the future for additional live players
8317180	8320420	that would crack your threshold to be on that list.
8320420	8323260	Turning to our last topic for today, AI safety.
8323260	8326900	In terms of actual news and the AI safety track
8326900	8328780	this last few weeks,
8328780	8331460	biggest stuff in my mind is,
8331460	8334620	although I guess you could also look at the live players list
8334620	8336140	as like who was invited to the White House.
8336140	8337580	So that would give you a good sense of the,
8337580	8340740	of who the White House thinks the live players are.
8340740	8343820	The commitments that they made there
8343820	8347140	and then the frontier model forum
8347140	8348540	that they established after the fact,
8348540	8352500	which basically is supposed to be the sort of industry group
8352500	8355660	that creates the forum for communication
8355660	8357380	between the leading model providers
8357380	8359100	and hopefully best practice sharing
8359100	8362980	and maybe certain classifiers.
8362980	8366220	There's a lot of public goods remain to be provided
8366220	8368460	and hopefully these leading companies
8368460	8373460	can use this forum as a way to share these public goods,
8373460	8375220	to create and show these public goods amongst themselves
8375220	8376420	and then hopefully share it,
8376420	8378620	share the best of them more broadly as well.
8379620	8383340	How did you react to that news?
8383340	8386700	Right, so I guess my reaction is that seems great,
8386700	8389980	but let's not get ahead of ourselves.
8389980	8393820	So like we have, is a lot of cheap talk.
8393820	8396740	I think people sell to cheap talk short, right?
8396740	8398180	Many cases, right?
8398180	8400460	Cause like it's so much better to get,
8400460	8403420	to have a bunch of cheap talk of the right type
8403420	8405860	than to have no talk, right?
8405860	8406820	Like they're gonna pay,
8406820	8409660	they will in fact pay a price for their cheap talk
8409660	8413060	in terms of like people thinking they're up to things
8413060	8414500	in this way that they don't like.
8414500	8416540	Not everybody wants them to do the things
8416540	8418460	that we want them to do.
8418460	8422100	And it makes it easier for them to go down these roads.
8422100	8425140	It sets the foundation to go down these roads, right?
8425140	8427060	We set up coordination mechanisms.
8427060	8429620	It lets them justify to their shareholders,
8429620	8432940	to, you know, their executives, to their board,
8432940	8434420	why they're going down these roads.
8434420	8435380	It makes that easier.
8435380	8437220	It makes it harder to shut down.
8437220	8439860	And it overcomes antitrust exemption problems, right?
8439860	8442900	Cause if they've committed together at the White House,
8442900	8445500	specifically something that I actively wanted to happen
8445500	8448500	and explicitly suggested in various conversations
8448500	8451140	and posts that should happen,
8451140	8453020	you make an announcement on the White House lawn,
8453020	8454980	they are committed to safety
8454980	8456060	with the White House's approval.
8456060	8457980	And now you can coordinate
8457980	8460420	and nobody has to worry about antitrust, right?
8460420	8464740	You no longer have to worry that they will accuse you
8464740	8466620	of how dare you not have full competition
8466620	8468860	to kill everybody as fast as possible
8468860	8471500	and coordinate to save a lot, to save us instead.
8471500	8473420	So now you get to coordinate
8473420	8474940	and there's something that's stupid,
8474940	8476460	you can just not do that.
8477380	8480620	But that's a huge, huge thing.
8481740	8484220	So where do you go from there?
8484220	8485260	That's the question, right?
8485260	8486300	Like they've made these commitments
8486300	8487700	but they don't really mean anything, right?
8487700	8490220	There's no enforcement mechanisms yet.
8490220	8493100	And there's no concrete actualizations
8493100	8494820	of what they're going to do
8495980	8500100	that have content that actually I can be confident in.
8501100	8503380	Doesn't mean it won't happen, right?
8503380	8505300	We have to just wait and see.
8505300	8509340	And I'm very glad these things happened.
8509340	8511020	And yet the real work begins now
8511020	8512460	is always the watchword,
8512460	8513980	is the way I put it, right?
8513980	8518620	Similarly, we've had two now very good Senate hearings
8518620	8520220	and some very, very good questions
8520220	8523380	and comments from Senator Blumenthal in particular.
8523380	8527700	And some very, very good responses by various witnesses,
8527700	8530060	not all of them, but most of them.
8530060	8533220	And again, like, where do we go from here?
8533220	8534260	Real work begins now.
8535460	8538100	You know, the mission accomplished banner
8538100	8540180	would definitely have been a bit premature
8540180	8544020	to display behind the announcement.
8544020	8549020	So no doubt much more in front of us than behind.
8550420	8552340	Does seem like a significant step,
8552340	8554300	but I think you're obviously recognizing that as well.
8554300	8557180	So yeah, I don't know if I have anything else really to add.
8557180	8559420	So then turning to this other thread
8559420	8563260	in the AI safety, you know, specific work.
8563260	8565780	As we talked about last time,
8565780	8568060	you have previously been a recommender
8568060	8569140	and you've written about this online.
8569140	8571860	So at length, so folks can go check out your take
8571860	8573020	on the entire thing.
8573060	8578180	You've been a recommender to the Survival and Flourishing Fund,
8578180	8583020	which is largely backed by Jan Tallin of Skype
8583020	8587820	and AI Safety Fame, investor in lots of big companies.
8587820	8592220	And his goal is to mitigate AIX risk,
8592220	8594900	you know, through whatever means necessary.
8594900	8599900	I'm doing that this year and that involves reading,
8599900	8603700	I think this year it's 150 grant applications
8603700	8606500	from organizations, some of which, you know,
8606500	8610180	come from the kind of familiar, you know,
8610180	8612660	effective altruism set that have, you know,
8612660	8614580	where AI safety has been in focus for a long time.
8614580	8618860	Others are kind of new to this scene or entirely new.
8618860	8622060	And in reading that, I mean, there's kind of obviously
8622060	8625740	two levels of analysis that you at a minimum
8625740	8626780	that you want to be performing
8626780	8630580	when you're doing this kind of grant recommending.
8630580	8634420	One is like, what kinds of things make sense
8634420	8637260	to be investing in?
8637260	8639860	And then second, you know, among those different classes
8639860	8642940	of things like who seems to be best able to actually execute
8642940	8645540	and, you know, deliver value against this,
8645540	8646420	you know, given strategy.
8646420	8648380	So leave that second part entirely aside,
8648380	8650540	that's where the 150 grant applications come in
8650540	8653060	and getting into the weeds of particular organizations
8653060	8655420	and their, you know, their track records and so on.
8655420	8658060	But going back just up to the,
8658060	8662220	what kinds of things should we be investing in?
8662220	8663860	Another way to frame that would be
8663860	8668860	what are the bottlenecks to progress toward a,
8670780	8672900	you know, if not provably, then at least like,
8672900	8675900	you know, likely safe outcome,
8675900	8678720	you know, for AI deployment writ large.
8680380	8683340	I find myself kind of unsure about that.
8683340	8684620	And I think it's a pretty important question
8684620	8685700	for figuring out, you know,
8685700	8689220	what would make sense to recommend?
8689220	8692580	You know, you could say, is funding in short supply?
8692580	8694180	Is talent in short supply?
8694180	8695500	You know, for a minute there,
8695500	8700500	especially in the FTX, SPF cycle,
8700900	8703540	there was this notion that, you know,
8703540	8705980	enough money has flown in that now what we really need
8705980	8707620	is talent and so there's a lot of, you know,
8707620	8710620	kind of boot camp programs being put together
8710620	8714500	and, you know, upskilling grants being approved
8714500	8717780	and, you know, a lot of kind of targeting of like,
8717780	8721460	undergrad, stage, math majors or whatever
8721460	8722700	to try to get them to come, you know,
8722700	8724940	think about doing some AI safety work.
8725820	8729620	And now obviously the money is in comparatively short supply.
8730580	8735580	Certainly the attention and the legitimacy of the,
8736460	8738460	you know, the public perception of legitimacy
8738460	8741180	of the topic of AI safety has gone way up
8741180	8744340	relative to, you know, not that long ago.
8744340	8745780	And so I'm kind of wondering what you think
8745780	8749740	are the new bottlenecks.
8749740	8752100	I have one candidate, but before I give you my candidate,
8752100	8753940	I'd love to hear what you think
8753940	8755860	are the bottlenecks to progress right now.
8755860	8758460	So I definitely say that like,
8758460	8761740	it's a mistake to only have one theory of change
8761740	8764540	or to think that there is strictly like one limiting factor
8764540	8766380	and the other factors don't matter.
8766380	8769260	I think you definitely have to ask about comparative advantage.
8769260	8772860	I think you have to understand that pushing on any of these
8772860	8776500	things is still helpful in terms of what is the constraint.
8776500	8780420	So like funding, there is clearly a funding constraint.
8781380	8785100	If you have to start funding like large compute spends
8785100	8786820	from within EA, right?
8786820	8790860	Like, and I count young talent is not part of EA per se,
8790860	8795820	but like within the general like strict AI safety mechanisms
8795820	8798540	and organizations and sources that already existed.
8798540	8803060	The costs of true AI safety, true AI alignment work
8804620	8806820	get very high as we go forward
8806820	8810300	because a lot of it's going to involve us spending a lot of compute.
8810300	8814140	And also it really should involve being willing to hire people
8814140	8816060	to work on these problems with competitive salaries
8816060	8818460	to what they can get doing on capabilities.
8818460	8819900	It's like hundreds of thousands of dollars a year
8819900	8824180	for maybe even a million for a significant number of people.
8824180	8826700	We want to be recruiting as a priority
8826700	8828620	to the people who've worked on capabilities
8828620	8830500	or would otherwise work on capabilities
8830500	8834060	to come out of open AI and anthropic and deep mind
8834060	8835660	places like that, especially Meta
8835660	8838020	and come work for this new safety organization
8838020	8841820	or shift over to a safety job or whatever.
8841820	8846820	And you have to pay for them, both their salary and their compute
8846820	8850180	and that's millions of dollars a person that adds up pretty fast.
8850180	8852420	On the other hand, there's no bigger reason
8852420	8855700	why we need to confine ourselves to traditional sources.
8855700	8859500	When we do that, there are any number of foundations
8859500	8863460	that have many, many more billions of dollars
8863460	8866260	than the traditional foundations that we've used in the past
8866260	8867820	for these things.
8867820	8871340	And lots and lots of billionaires and multi-millionaires
8871340	8874300	who are legitimately very worried and ordinary people
8874300	8878380	and government sources are also potentially viable in the future.
8878380	8882780	Corporations will often have an interest, including the big labs.
8882820	8887140	So we shouldn't rule out any number of ways to get that.
8887140	8891460	In terms of talent, I think that we are highly
8891460	8894500	talent constrained for the right talent.
8894500	8897140	I think we are not necessarily that talent constrained
8897140	8902620	for generic undergraduate who wants to work
8902620	8904180	at Berkeley for six months and think about it.
8904180	8908540	I say we are not particularly constrained for comp-side graduate
8908580	8913740	out of Stanford who just wants to work on something cool.
8913740	8918580	But if we want people who have specific characteristics,
8918580	8919700	those are not as easy to find.
8919700	8923300	The characteristics we need, first of all, we need leadership.
8923300	8925460	Leadership capability, ability to run teams,
8925460	8929260	ability to lead efforts, be self-directed, self-driving,
8929260	8930700	be able to engage in fundraising.
8930700	8933540	Because sometimes when you say you're funding constrained,
8933540	8935300	that can mean fundraising constraint.
8935300	8937340	It can mean the ability to signal to funders
8937340	8939980	that you are worthy of funding constraint,
8939980	8942620	that are the different form of funding constraint.
8942620	8945540	These things are interestingly intertwined,
8945540	8947580	and it's complicated.
8947580	8951980	So we also are very short on people who actually
8951980	8956180	understand the problem and are prepared to pay the price
8956180	8960380	to focus on hard problems and real solutions.
8960380	8963700	So a number of people who, if you were to give them
8963740	8966140	a competitive salary, would happily
8966140	8969340	work on alignment-flavored problems
8969340	8971980	that let them publish every six months,
8971980	8976180	or that just generally are easy, in some important sense,
8976180	8978260	but they don't actually speak to what El Aztec Dazal
8978260	8981340	not killed very much.
8981340	8984380	And it's probably better to do more of that than less of that
8984380	8986700	if it's just literally yes or no.
8986700	8989580	But if it's orders of magnitude less important,
8989580	8993380	then the few people who will do the actual things
8993380	8995500	that matter.
8995500	9000820	And so if you understand the Yudkowskyan difficulties,
9000820	9003620	lessons, in some sense, and the nature of what problems
9003620	9007020	you have to solve, or you have leadership capabilities
9007020	9008380	and other things like that, or you just
9008380	9011900	have extensive real experience with machine learning systems,
9011900	9016140	so you can build, as the relative speaking 10x, 100x
9016140	9018180	engineer, who's just that much better,
9018180	9021780	who can enable people to do real work in these ways.
9021780	9023060	And if you're the type of person who
9023060	9027100	can make a project fundable, especially
9027100	9029140	by non-traditional sources, then you
9029140	9030900	are actually going to be valuable in those ways.
9030900	9035340	And it would be a major mistake to join an existing
9035340	9038020	organization and try to make a difference as an individual,
9038020	9042700	as opposed to trying to spearhead a new organization,
9042700	9047860	or at least a new branch of a existing major organization,
9047860	9049780	depending on your skill set.
9049820	9052780	If you are just a generic, I want
9052780	9055500	my life to be straightforward, where
9055500	9059780	I am paid a salary to work on intellectual puzzles that
9059780	9064500	are not particularly impossibly difficult,
9064500	9067780	and do not require me to take the weight of the world truly
9067780	9071100	on my shoulders, blah, blah, blah, then
9071100	9072740	I'm not here to shame you.
9072740	9076140	That just means that you're not particularly invaluable,
9076140	9078860	and that it starts to be reasonable to do things like,
9078900	9082540	maybe I should be a voice inside an anthropic.
9082540	9085660	You just have to be very sure that you will keep your eye
9085660	9088340	on the ball and not be distracted to keep those.
9088340	9090260	I think mine is pretty consistent with that.
9090260	9093500	In a phrase, I had said research agendas
9093500	9095660	seem to me to be the bottleneck.
9095660	9099180	Maybe your framing is more like the PI, the person that
9099180	9100300	can drive the research agenda.
9100300	9102540	Obviously, there's closely related.
9102540	9104300	That's basically what you're saying.
9104300	9107980	It's credible plans that are in short supply.
9107980	9110020	But it's not just credible plans because I can't just
9110020	9112540	hand you a plan.
9112540	9115540	Even if you are a really good machine learning person,
9115540	9118660	I can't just hand you a piece of paper with a plan written on it.
9118660	9120540	And especially to execute that plan,
9120540	9122420	you have to appreciate the nature of the problem
9122420	9126660	so that you can implement that plan and modify that plan
9126660	9130140	and pivot that plan and so on.
9130140	9133900	But yes, we also just don't have good attack vectors,
9134060	9137980	ways to get into the problem and start
9137980	9140140	to make progress on the problem.
9140140	9142180	And that's a real problem as well.
9142180	9143500	That's a huge deficit.
9143500	9150620	But there also isn't the AI research agenda organization
9150620	9153500	that just generates research agendas for people.
9153500	9155340	I wish there was, but there isn't.
9155340	9157820	So I think we're basically together there.
9157820	9160900	In reading these grants, some of the ones that have jumped out
9160900	9164740	to me the most as being like the most kind of no-brainer
9164740	9171020	exciting are those where it's a really established, often
9171020	9176020	like professor who's leading a group and basically is like,
9176020	9179580	I want to reorient or I want to do a significant part
9179580	9182020	of my research focused on AI safety.
9182020	9183060	And that may be new.
9183060	9186420	It may have its own kind of unique spin on it.
9186420	9188180	There was one in particular, which I won't name,
9188180	9190620	but it kind of initially read the thing.
9190660	9193340	And I was having a hard time deciding.
9193340	9197020	I was like, this could be the kind of thing that's like just
9197020	9202620	insane, like an insane person might send this or like an actual
9202620	9204540	game changer might send this.
9204540	9207220	And it wasn't until I looked at the author and was like, oh,
9207220	9210940	this person is like an H index or whatever of like 45 or something.
9210940	9213340	I was like, oh, I'm into this then.
9213340	9216620	So anyway, some of these ideas that even if the ideas can be
9216620	9222220	like extremely hard to assess if they're like novel and coming
9222220	9225540	from a credible source, that has stood out to me.
9225540	9227780	There aren't that many of them, but that has stood out to me as
9227780	9230740	like a pretty exciting opportunity.
9230740	9235980	Then there's like a lot of policy stuff.
9235980	9240540	And I find it hard to figure out what I figure out what I should
9240540	9242300	be thinking about that right now.
9242300	9245220	It's like obvious that, you know, for our earlier discussion
9245220	9250980	on live players that like regulators broadly are, you know,
9250980	9254740	going to have some significant influence on how things go,
9254740	9257780	even if they just do nothing, you know, obviously doing nothing
9257780	9259420	is a choice.
9259420	9263540	But then if I think like, OK, if I'm going to try to invest money
9263540	9268660	today to influence those people, it starts to feel real hard.
9268660	9271660	A general sense of like how decisions get made in governments
9271660	9275140	and regulatory bodies is kind of like, we wait for a crisis
9275140	9277580	to come along and then we look around and say who has a plan
9277580	9279860	and then we use, you know, a plan that somebody had previously
9279860	9281020	prepared.
9281020	9284060	And now it seems like we're kind of entering the moment where
9284060	9287700	not exactly that the crisis has come, but certainly like,
9287700	9291380	you know, the eye of Soran has kind of turned toward this topic.
9291380	9295180	And so people are now beginning to like look around for plans.
9295180	9298420	And some plans have been prepared by some, you know,
9298420	9300020	organizations that were established years ago.
9300020	9302340	And those, you know, some of those are even credible enough
9302380	9305980	that they probably are having an influence now.
9305980	9307980	But now I see a lot of people who are like, I want to start
9307980	9311780	a new policy organization and I'm going to go to Washington
9311780	9314500	and like, you know, do something.
9314500	9316380	And there I'm like, I don't know.
9316380	9319580	It seems like everybody's, you know, kind of like you're,
9319580	9322340	are you, is it too late to join in on this, you know,
9322340	9325540	what might be the world's largest ever game of tug of war?
9325540	9328460	Are there things in policy that you think are still,
9328460	9330420	still have a high likelihood of making a difference?
9330420	9333020	Like I'm a little bit at a loss about that, to be honest.
9333020	9335980	Yeah. On the research organizations, I think, yeah,
9335980	9338820	it's pretty easy to go, you know, does this person,
9338820	9340340	what are they proposing to do?
9340340	9341940	You know, does this seem vaguely credible
9341940	9344020	as a person to do that thing?
9344020	9346100	And then does this thing address the hard problems?
9346100	9349660	Is this thing like reflect an appreciation of the nature
9349660	9351660	of the difficulty of the issues?
9351660	9354980	Is this thing like clearly not going to end up being
9354980	9356620	capabilities, right?
9356620	9359100	Like is this thing, you know, potentially going to solve
9359100	9359940	the hard problems?
9359940	9362540	Like that's relatively straightforward and both of us
9362540	9364500	are in a position where we can, to some extent,
9364500	9367660	evaluate those questions because we have domain knowledge.
9367660	9372780	You get into policy and yeah, it's very hard to tell.
9372780	9377780	Like, you know, as a recline makes the case pretty strongly,
9378220	9379860	you know, there's a room where it happens
9379860	9382380	and a small number of people influence the room
9382380	9384580	where it happens or in the room where it happens.
9384580	9385700	And you can be one of those people
9385700	9388180	or you can help create one of those people.
9388180	9390700	It doesn't make it obvious how to do that.
9390700	9393380	Does it mean that your effort to do that
9393380	9395820	will help you do that as opposed to backfire?
9396860	9398820	It doesn't mean that like more efforts to do that
9398820	9400700	is better than less.
9400700	9402140	All of this is very complicated
9402140	9403460	and it doesn't tell you what you have to try
9403460	9404980	and do what you get into that room
9404980	9406940	or what you're trying to push for.
9406940	9409420	So yeah, it's definitely tough.
9409420	9411780	So I would say the big thing,
9411780	9413860	and you don't even know what's happening right now, right?
9413860	9415980	Like it's anthropic, for example,
9415980	9419300	like may or may not be making like effective big pushes
9419300	9422340	behind the scenes to try and influence these rooms.
9422340	9423980	And they may or may not have their eye on the right ball
9423980	9428140	when they do so, but it's all gonna happen in private.
9428140	9430060	So we don't get to know.
9430060	9430900	And he said that I wouldn't know,
9430900	9432660	I wouldn't be able to talk about it.
9432660	9434180	And the same thing goes for DeepMind,
9434180	9435140	the same thing goes for OpenAI.
9435140	9437380	I mean, Sam Altman's been pretty vocal
9437380	9439060	and Dario just went out to Congress
9439060	9440500	and spoke pretty publicly.
9440500	9442860	But it's hard to say, I've been pocketed
9442860	9444500	by a few organizations.
9444500	9446820	There's clearly like gonna be a window, right?
9446820	9449420	In the next few months, at least,
9449420	9451260	and maybe the next few years,
9451260	9454500	where if you have the right proposals fleshed out
9454500	9456780	in the right form, getting to the right person,
9456780	9458500	lying around, they might get picked up,
9458500	9460100	it might actually happen.
9460100	9462500	And so there's potentially very high leverage here.
9462500	9465100	So I would say like, the first thing I look for
9465100	9468180	in these policy proposals, in these policy organizations,
9468180	9470940	is what is your policy goal, right?
9470940	9472860	Because like that's the biggest differentiator to me,
9472860	9476020	is are you going to keep your eye laser focused
9476020	9477620	on the correct ball?
9477620	9482620	Where the correct ball is a system of compute regulation,
9483220	9484060	right?
9484060	9488620	A system whereby the biggest models require permissions
9488620	9490940	are under some form of restrictions and regulations
9490940	9494460	and tests and in a way that would eventually lead
9494460	9498140	to an outright limitation or halt.
9498140	9502140	And are you going to do various forms of GPU tracking
9502140	9503820	or wait the foundations for that in a way
9503820	9507700	that will eventually allow you to, in fact, control
9507700	9510540	who gets to do these kinds of very large runs?
9510540	9512700	And if you're posing anything that doesn't lead
9512700	9517220	on that road, that might be useful
9517220	9520740	for mundane utility purposes, but it won't save us.
9520740	9524340	And so, I'm not interested in funding you
9524340	9527580	if your policy isn't that or isn't something
9527580	9529700	I haven't thought of that's new and open to there being
9529740	9532500	something that I haven't occurred to me, certainly.
9532500	9536900	What do you think about the liability angle
9536900	9538940	or well, let's start with that.
9538940	9543340	I mean, that, because the kind of classic argument
9543340	9545980	there would be, you don't want to end up
9545980	9547340	in the position of nuclear, right?
9547340	9549740	Where we have the worst things and not the best things
9549740	9551060	and you know, a lot of people are-
9551060	9552140	Yeah, I mean, Bert will have the endurance
9552140	9553820	to look at his insurance, right?
9553820	9555540	From Tom Lair, right?
9555540	9559060	Boys, yeah, we all go together when we go, nuclear war.
9559100	9560740	The insurance doesn't pay out, you're all dead.
9560740	9561580	Right, right, right.
9561580	9565540	Okay, so certainly, yes, in the catastrophic scenario,
9565540	9567940	insurance doesn't pay out, but do you think that that,
9567940	9570140	so you don't believe in the notion
9570140	9575140	that a liability regime could be an effective incentive for-
9575780	9579500	I think a liability regime with mandatory insurance
9579500	9583100	makes a lot of sense for harms up to a certain level.
9583100	9588100	Like saying that if you want to use models
9588260	9589420	that are sufficiently powerful,
9589420	9591420	you have to find someone willing to sell you insurance
9591420	9593540	against having something going wrong.
9593540	9595700	And then, you know, if you want to use an open source model,
9595700	9596780	you have to have insurance from someone
9596780	9597620	against it going wrong.
9597620	9599180	And like, if you can't make that work,
9599180	9600700	then, you know, there are funny things
9600700	9602940	that you can't make work in the United States,
9602940	9604860	even though they look like they should be able to do them.
9604860	9606580	And that's just how it goes.
9606580	9608180	And, you know, maybe up to a point,
9608180	9610100	Microsoft can self-insure and then at some point,
9610100	9611980	they can't and then they have to go out there
9611980	9615220	and deal with these reinsurers or whatnot.
9615220	9617140	Or F, you know, that would help.
9617140	9619580	Like, basically, you have these giant externalities,
9619580	9620420	right?
9620420	9622540	These giant negative tail risks that are very fat,
9622540	9624540	that are potentially very, very big.
9624540	9627940	And you want to make sure that people internalize those costs
9627940	9629380	and work to minimize those costs
9629380	9632260	in order to minimize their insurance and payout costs.
9632260	9633220	And so these things could be helpful.
9633220	9637060	They can also just simply weaken the economics behind,
9637060	9639100	like pushing highly capable model.
9639100	9641500	Who's like, you don't really have to worry that much,
9641500	9643540	relatively speaking, about the liabilities
9643540	9646260	of a character AI, right?
9646260	9647260	Because it's not dangerous.
9647260	9648420	You know it's not dangerous.
9648420	9650260	What's going to happen?
9650260	9651540	Whereas some of these other things,
9651540	9654860	they could cause a lot of harm, potentially, in the future.
9654860	9656420	And you have to worry about that.
9656420	9659780	The problem is, again, if you go down that road,
9659780	9660860	I think it's probably not helpful.
9660860	9665620	But how do you price existential risk?
9665620	9671500	Because, again, you know, you can't actually hold anybody
9671500	9674380	accountable for it when it happens.
9674420	9678980	And so, you know, if you required somebody
9678980	9683780	to actually buy insurance in some real sense for this,
9683780	9685700	then you have to price it somehow.
9685700	9688140	And then, like, that makes a lot of sense.
9688140	9689740	And then, like, OK, there's a 1% chance
9689740	9690740	you would buy all of humanity.
9690740	9692380	And the net present value of every person
9692380	9695540	is $10 million, so $10 million times $8 billion.
9695540	9696900	So can you buy insurance for that much?
9696900	9697300	What's that?
9697300	9699780	Times the percentage chance it happens, times the premium.
9699780	9700780	And you can't afford that.
9700780	9702100	And you can't mode your system.
9702100	9705420	And that's not a crazy way to go about doing things.
9705420	9709540	But you have to actually notice the threat and price it
9709540	9711220	for that to work.
9711220	9712940	So I think my actual answer is I'm
9712940	9715260	very much in favor of, like, more strict liability
9715260	9717060	for AI harms.
9717060	9719940	I think I'll write about this for next week already.
9719940	9724140	But I don't think it alone can accomplish the mission.
9724140	9727180	I just think it's a net incrementally helpful thing.
9727180	9730380	But also, I want to be wary of places
9730380	9735140	in which our legal system tends to award very oversized
9735140	9740620	damages for harms that are not actually so big.
9740620	9742700	And also, where we have asymmetrical,
9742700	9746420	like I call this concept asymmetric justice, where
9746420	9750060	you are fully liable, potentially far, far more
9750060	9752340	than fully liable for all the harms that you do, right?
9752340	9755020	If I cause somebody $1,000 in damages
9755020	9757260	by being negligent, the court might find me $100,000
9757260	9758820	or $1 million.
9758860	9761740	Whereas if I provide that person $100,000 in value,
9761740	9763340	I'd be lucky to get 1,000 of it.
9763340	9766620	Because I'm up against a bunch of competitors.
9766620	9769260	People aren't that much willing just to pay.
9769260	9772980	I pay $20 a month for GPD4 and $0 for everything else.
9772980	9775740	And I get, what, thousands, tens of thousands?
9775740	9778500	Maybe a value every month?
9778500	9782060	So if you have to fully be liable for your harms,
9782060	9784860	but you don't get to charge for your benefits, right?
9784860	9788380	Am I discouraging mundane utility far too much
9788420	9789620	by doing that?
9789620	9793180	And in fact, since liability is easier to enforce
9793180	9796180	on mundane problems and harder to enforce on the big problems
9796180	9797620	we actually want to guard against,
9797620	9800380	are we just, is it actually just that, right?
9800380	9801300	Like, past a certain point.
9801300	9803620	And so I'd be, I want to be cautious
9803620	9806900	with imposing too much liability.
9806900	9810620	I think very strict, like actual damages liability
9810620	9811860	makes perfect sense, though.
9811860	9816860	So another category of thing that there's a number of,
9817340	9821380	number of kind of organizations getting started right now
9821380	9825500	is in the, and this ties a few threads together.
9825500	9830500	It's kind of in this space of trying to be
9831060	9835740	the third party evaluator, red teamer,
9835740	9839260	independent safety review organization
9839260	9843020	that leading the live players in their White House
9843020	9845140	and Frontier Model Forum commitments
9845140	9847380	have committed to working with.
9847380	9848420	It's kind of an interesting dynamic
9848420	9850700	where it's almost like an advanced market commitment
9850700	9852260	from these companies in some way,
9852260	9855460	because there aren't that many folks around right now
9855460	9860460	who are prepared to provide a competent red teaming
9861420	9864500	or model characterization or evaluation
9864500	9866820	wherever you want to call that service.
9866820	9867900	But the companies have kind of said,
9867900	9870500	hey, we will commit to working with them
9870500	9872260	and clear if they're planning to pay for that,
9872260	9874780	or if they expect that to be charity funded.
9874820	9876220	Certainly from what I'm seeing,
9876220	9878340	the folks that are starting the organizations
9878340	9880460	are like seeking out some charity funds.
9881500	9882820	I've been very excited about that.
9882820	9883660	It seems like, first of all,
9883660	9884820	and it's great that they're making this commitment.
9884820	9886700	Somebody's going to have to do that.
9886700	9889700	I, as everybody who listens to this podcast for two seconds,
9889700	9894540	know, enjoy the fun and entertainment.
9894540	9897100	And I think it's also valuable to do the red teaming.
9897100	9899780	One experience I had this last week, though,
9899780	9902300	sort of made me wonder about the theory of change there.
9902300	9903700	I mean, I guess there could be multiple, right?
9903700	9905780	One would be you,
9905780	9909460	because you have a good working relationship with the orgs,
9909460	9912100	you're like, hey, we found these problems
9912100	9913820	as superiors to unsave, you shouldn't release it yet.
9913820	9914860	They listened to you, okay?
9914860	9916380	That could be simple.
9916380	9918180	Another would be like,
9918180	9921980	you kind of create these narrative shaping examples,
9921980	9924100	kind of like what ARC did with the GPT-4 red team
9924100	9929100	where that instance of the model lying to a person,
9930060	9931860	and I think this was kind of prompted,
9931860	9935940	but nevertheless, from the task rabbit user's point of view,
9935940	9937660	the model lied to it about, excuse me,
9937660	9940500	having a vision impairment as opposed to being an AI
9940500	9942300	that needed help with a CAPTCHA.
9942300	9944300	So that really caught the public's imagination
9944300	9948340	and kind of changed, I think, to some non-trivial degree,
9948340	9949180	how people think about it.
9949180	9951060	Certainly that gets referenced a lot.
9951060	9953660	I tried to do something like that this last week
9953660	9956300	with this random AI tool that I came across
9956300	9959820	that allows you to call anyone with any objective.
9959820	9961700	And I just tried to have it call myself
9961700	9964340	and make like a ransom demand of myself,
9964340	9966140	and I recorded it.
9966140	9967300	And it was very easy to do.
9967300	9968500	There was no jailbreak involved.
9968500	9971700	Since then, the company has fixed the issue, by the way.
9971700	9973140	So to give credit where it's due,
9973140	9975540	they fixed it pretty quickly after I called them out.
9975540	9977300	I did communicate with them privately, by the way.
9977300	9978500	All this is documented on Twitter
9978500	9981220	if you wanna see my approach and my kind of thinking through,
9981220	9983300	should I disclose it publicly or not,
9983300	9985580	or whatever number of considerations went into that.
9985580	9987500	One of them was that they just didn't respond to me
9987500	9989140	when I reported it.
9989140	9990700	And so I was like, well, if you're not gonna respond,
9990700	9992500	then I'll call you out publicly.
9992500	9995980	Anyway, all this leads up to me publishing this video
9995980	10000460	of an AI with no jailbreak calling me
10000460	10003340	and telling me that it has my child
10003340	10004460	and it demands a ransom.
10004460	10007980	And if I want to ensure the safety of the child,
10007980	10010780	I will comply and any deviation from instructions
10010780	10012300	will put the child's life in immediate danger
10012300	10015700	and pretty flagrant stuff in my view.
10015700	10019580	And it was kind of met with a bit of a yawn
10019580	10022380	on Twitter, like certainly,
10022380	10023300	we've got some likes and whatever,
10023300	10026580	but did it really start a serious conversation?
10026580	10031580	No, the developer didn't respond in public at all
10032260	10033860	as far as I can tell, really.
10033860	10036060	They did go ahead and fix it, which is good.
10036060	10041100	But the whole thing was kind of a non-event
10041100	10043060	and I was a little confused by that.
10043060	10046580	Like it makes me kind of coming back to my theory of change
10046580	10049100	on some of these evaluation, characterization,
10049100	10051020	red teaming orgs.
10051020	10053540	I wonder like, are we all just numb already
10053540	10055860	to these flagrant examples?
10055860	10057060	There's been this notion for a long time
10057060	10060060	that like maybe if warning shots happen,
10060060	10062220	then people will start to get more serious.
10062220	10064780	And if you can go out and find these warning shots
10064780	10066860	with red teaming and bring them to everybody's attention,
10066860	10068620	then that could be really valuable.
10068620	10070540	This week for me, it felt like I influenced
10070540	10073020	the application developer because they did fix it,
10073020	10075860	but otherwise it seemed like kind of,
10075860	10078260	tree fell in the forest largely.
10078300	10080100	So a lot of levels to that,
10080100	10083380	but how do you think about that category of project
10083380	10086660	and how it may or may not contribute?
10086660	10090980	I made a prediction in our GBT.
10090980	10093900	And that prediction was with Han Keim's test GBT-5,
10095700	10099180	they will encounter a problem that if they had
10099180	10102100	to pre-commit now, you would definitely agree,
10102100	10104500	would be a reason not to release it.
10104500	10107700	And then they will like gloss over or patch it
10107700	10111660	or like otherwise like hand wave in its direction
10111660	10112900	and release anyway.
10112900	10116860	Basically like not actually take their warnings
10116860	10118540	sufficiently seriously.
10118540	10120900	Not that I expect this to then end the world, to be clear.
10120900	10123180	I expect this to then mostly be fine,
10123180	10127980	but that like we are not prepared
10127980	10129900	to make real evaluations or thrill teeth
10129900	10132220	that like get really enforced
10132220	10135060	and that we're gonna have to work on that quite a bit.
10135060	10137620	And I think it's good these teams exist.
10137620	10139380	I think we need more than one of them, right?
10139380	10142420	I think you need at least three different teams
10142420	10145140	working on different standards that think differently,
10145140	10146500	that check for different things
10146500	10149860	and that then like you get multiple evaluations
10149860	10150900	before you release your model.
10150900	10154820	So that someone isn't just blind to something by accident,
10154820	10157260	like it's much more robust that way.
10157260	10161060	And that, working to develop more different red team
10161060	10162700	strategies and more different tests
10162700	10165380	and more different metrics and more different responses.
10165380	10167420	This way in case one of them leaks for whatever reason
10167420	10168580	and it gets in the training data
10168580	10170980	or something terrible might happen, it's very easy.
10170980	10174740	Then like it's quite useful.
10174740	10178500	The danger of these things is one, if they don't listen, right?
10178500	10180900	So what if you tell them the thing is dangerous?
10180900	10183460	They might just engineer around it, right?
10183460	10184700	To like fix the narrow issue
10184700	10186340	without thinking about what the problem means.
10186340	10188980	They might just ignore you entirely.
10188980	10190180	They might try to fake the data
10190180	10191820	so they make you think that they'd solve the problem.
10191820	10193660	Any number of things are possible.
10193660	10196500	They might use the oral evaluations
10196500	10200780	and an excuse to treat the system evaluated as safe, right?
10200780	10203620	So like this is always a problem with safety work,
10203620	10208220	which is that the government says, okay, you have to do
10208220	10210620	these hundred things to ensure your system is safe.
10210620	10212460	And now the safety officer is like focused
10212460	10213620	on making sure there's hundred things happening
10213620	10214460	so you can release the system
10214460	10215820	and they don't actually use common sense.
10215820	10218020	They don't actually ask themselves, okay,
10218020	10219820	why would the system might actually be dangerous?
10219820	10221060	And you can tell a very easy story
10221060	10222700	where technicians know this thing
10222700	10224340	might actually kill everyone
10224340	10225700	and everyone forces them to release it anyway.
10225700	10227460	They pass on the safety test.
10227460	10229860	Even though they know it didn't actually pass all the,
10229860	10232180	the important safety test, but they're not on the list.
10232180	10236100	Because no, no system however well meant and worked on
10236100	10237540	will be able to anticipate all the problems
10237540	10239060	that come in the future, right?
10239060	10241860	Like there's just gonna have to do these things
10241860	10243900	in somewhat improvisationally.
10243900	10245060	And will they move the goalpost, right?
10245060	10248220	Will they be able to enforce the right standards?
10248220	10251100	And will they test early and often enough
10251100	10251940	for the right things?
10251940	10254780	Because like one thing you have to worry about is
10254780	10256620	in the future, at some point,
10256620	10260220	the training runs themselves become dangerous, potentially.
10260220	10261460	And ARC didn't run its test
10261460	10264500	until after the training run was complete, right?
10264500	10265340	They also didn't run it
10265340	10268100	on the full capabilities of the final system.
10268100	10270780	And they didn't have fine tune capabilities.
10270780	10272180	And blah, blah, blah.
10272180	10273900	They had many things they didn't have.
10273900	10276460	So like everyone agrees that ARC's first run on GBD4
10276460	10279980	was just a trial run, test out the gear,
10279980	10282460	see how it goes, wasn't meant to catch the real problems.
10282460	10284100	No one thought that thing was actually gonna kill everyone
10284100	10285420	or anything and it didn't.
10286500	10290380	But we have to plan in these situations
10291340	10293660	to red team as if this thing is going to be around
10293660	10295580	for many years of improvements
10295580	10298220	on what you can do with it and explorations.
10298220	10301140	And the red teams have to be sufficiently enabled
10301140	10302780	to identify the problems.
10302780	10303820	And you have to be able to extrapolate
10303820	10304820	from what the red teams were able to do
10304820	10305660	in a short amount of time
10305660	10307300	with a short amount of resources
10307300	10308780	to what the public is going to be able to do
10308780	10311500	with vastly more compute, vastly more attempts,
10311500	10314620	vastly more resources, vastly more creativity.
10314620	10317940	Because no team of 20 people,
10317940	10320820	however good of their jobs can match the internet.
10320820	10322340	That's the kind of thing, ever.
10323460	10326020	So I think it's a good idea.
10326020	10327580	I think that it's not a complete solution
10327580	10329020	and never will be, right?
10329020	10330980	And the danger is people treat it as one.
10330980	10332620	You have to ask yourself like,
10332620	10333580	what is it competing against?
10334220	10336540	Is this going to be one of the top, however many?
10336540	10337940	People who run this thing,
10337940	10340620	like you want to have like three viable organizations,
10340620	10341460	you might, you know,
10341460	10343100	you don't necessarily need 30,
10343100	10344740	that's probably not worth it.
10344740	10346340	Like you want three to five.
10346340	10349540	So is this person more funny to do that?
10349540	10351460	Just figuring out better metrics
10351460	10353500	without necessarily being the one who runs the tests
10353500	10355500	is also a useful thing.
10355500	10356860	So if I had to bottom line all that
10356860	10359660	and summarize what I think your worldview is,
10359660	10364660	you know, as a sort of elder recommender
10364700	10369700	for this AI safety focused grant-making process.
10370380	10371460	I think I would say,
10371460	10374100	I think I would summarize it as
10374100	10376620	there need to be a few
10376620	10379180	of these independent safety organizations.
10379180	10381740	They seem to be either just started
10381740	10383700	or kind of getting started now.
10383700	10385940	So at least a few of those,
10385940	10387260	one exists in, you know,
10388260	10389420	a couple others are, you know,
10389420	10390380	either just getting started
10390380	10391740	or soon to be started, whatever.
10391740	10393020	So there's kind of,
10393020	10395300	that seems good because we need to bring
10395300	10398580	that small, you know, group into existence
10398580	10401060	in the first place, you have to have them.
10401060	10404100	Second, on the policy side,
10404100	10406180	I guess I would summarize you as saying,
10407260	10409540	seems like it really matters,
10409540	10413180	but really hard to predict
10413180	10415060	who will have any impact
10415060	10418180	and what kind of impact any effort will have.
10418180	10421060	And so for me, I sort of maybe cash that out
10421060	10423140	to like probably worth continuing
10423140	10427060	to support the organizations that are like established enough
10427060	10429140	that they already have credibility
10429140	10431980	because credibility or like, you know,
10431980	10434700	that bloom and thaw might give a shit what they think
10434700	10437260	is like probably the thing that matters.
10437260	10438700	And to the degree they already have that
10438700	10439900	like double down on it.
10441060	10444220	And then everything else seems like it goes
10444220	10448700	into good PIs that can drive a research agenda
10448700	10450340	and have something that they want to do.
10450340	10452820	And in that category, it's like,
10454060	10456020	don't even really worry too much
10456020	10457380	about the exact details of the plan,
10457380	10461980	but just look for people who have the originality of thought
10461980	10463940	to be doing something a bit different perhaps
10463940	10467100	and the sort of demonstrated capability
10467100	10470180	to actually advance a research agenda.
10470180	10472460	This is a lot different things there, right?
10472460	10475500	For the PIs, I would say, you know,
10475500	10478780	I'm not looking for like exactly the right approach,
10478780	10482020	but I am looking for assume alignment is hard.
10482020	10484660	This is the approach except that alignment is hard
10484660	10486660	and do something that makes progress, real progress
10486660	10488820	if alignment is in fact very hard.
10488820	10491380	These people show an appropriate caution
10491380	10493940	towards the might advance capabilities
10493940	10495900	towards like maybe I don't want to publish my results
10495900	10498140	if I find a result that would be harmful
10498140	10500260	to publish question marks like that.
10500260	10501460	You know, am I thinking about this problem
10501460	10503980	as the right safety mindset, with the right paranoia,
10503980	10506380	with the right like appreciation of the fact
10506380	10508380	that I'm up against impossible odds?
10508380	10511020	And if the answer is yes, and I think I have the talent,
10511020	10514060	then I'm excited even if I'm somewhat skeptical
10514060	10515820	of the specific thing they intend to try
10515820	10517380	in terms of like whether it will work, right?
10517380	10520580	Because like I think that all the most promising things
10520580	10522340	are not that likely individually to work
10522340	10524380	and it's gonna be hard for me to evaluate
10524380	10525380	the relative value.
10526460	10528460	In terms of the lobbying organizations,
10529340	10534140	I don't think it's crazy to start a new group at this point.
10534140	10536380	I do think you want to look for something extraordinary
10536380	10539340	if somebody is like, why are they forming a new group now?
10539340	10541020	Why does that make sense?
10541020	10543540	But yeah, what I'm looking for is a focus
10543540	10546460	on the policies that actually matter
10546460	10548300	and on a coordination amongst them
10548300	10552700	and on like a focus on actually making a difference.
10552700	10554940	Like so much of like most of politics, right?
10554940	10557420	There's not about AIs about politics in general
10557420	10559540	is about raising money from donors
10559540	10562060	and sending signals of your royalties
10562060	10563860	and coming up with your status
10563860	10567180	and raising awareness and other bullshit.
10567180	10569300	Like most like all sides, right?
10569300	10573260	Like you've got to focus on people who are writing bills,
10573260	10575500	people who are lobbying directly for bills,
10575500	10577740	people who are trying to influence the exact right people
10577740	10579020	in the exact right ways.
10579020	10582140	Like have a concrete direct theory of change
10582140	10584060	who like either understand DC
10584060	10585420	or have connections with people
10585420	10587380	who can help them understand DC.
10587380	10588180	But I don't think we know
10588180	10590380	that we are in like the only critical window.
10590380	10592100	We're going to need more organizations than we have.
10592100	10593420	We're going to need far more people working on it
10593420	10595340	than we already have.
10595340	10597580	I don't want to make the mistake of not chips or down
10597580	10600780	the people who have like nominally established some amount
10600780	10603220	of like formal credibility or authority
10603220	10604220	now get all the resources
10604220	10605940	and get to boss everybody around and do whatever they want.
10605940	10607660	I think that's like a common failure mode.
10607660	10609380	I don't want to fall into it.
10609380	10611980	Evaluation organizations, I want to ask myself,
10611980	10615740	are these the right people to be doing this particular thing?
10615740	10617340	They show promise in doing the thing
10617340	10618340	what they bring to the table
10618340	10619780	the other organizations don't bring to the table.
10619780	10621060	I want to see different
10621060	10622620	or I want to see something unique.
10622620	10623660	And you have to convince me
10623660	10626300	that like you're capable of pulling this off
10626300	10627740	which includes convincing people
10627740	10630100	that actually buy your services and use your services.
10630100	10632820	Where do you put mechanistic interpretability in there?
10632820	10634100	And that that could be,
10634100	10637820	that seems to be part of what some of the like evals orgs
10637820	10641460	are also kind of including that in their plan.
10641460	10642500	And then obviously, you know,
10642500	10643380	different research groups
10643380	10646220	can approach that from any number of ways.
10646220	10648580	My technical view of it is that it's more distinct
10648580	10653580	from evaluations than that suggests
10654580	10655780	but that it's a good idea.
10656860	10658180	Like it should be, you know,
10658180	10660740	mechanistic interpretability is like Western civilization.
10660740	10662420	It's a good idea.
10662420	10666820	Yeah, you should try to in fact,
10666820	10668860	figure out how these things work.
10668860	10670700	You do have to be aware
10670700	10672660	that you are advancing capabilities potentially
10672660	10673500	when you do it.
10673500	10675420	You have to think carefully about, you know,
10675460	10677180	if you find the wrong thing,
10677180	10680100	I would ask before I funded an interpretability organization,
10680100	10683300	are you capable of going, oh, yikes,
10683300	10685700	that's a dangerous thing to learn.
10685700	10687940	I might not want to rush out to tell the world about that.
10687940	10689700	I might want to think carefully about who to tell
10689700	10691020	and who not to tell.
10691020	10692660	But not necessarily don't really sympathy.
10692660	10695620	Like you have to like process information carefully
10695620	10696780	and not just rush.
10698100	10700260	You don't want a culture of, you know,
10700260	10702460	everything I ever find is going to be automatically
10702460	10704340	just shared with the world for that reason.
10704340	10707420	When you work on mechanistic interpretability.
10707420	10709020	But I do think on general,
10709020	10710340	it's a very positive thing to work on.
10710340	10711900	I do think that it's a thing
10711900	10714860	that like holds a lot of promise to help us
10714860	10716700	in various ways.
10717780	10718620	And it could lead somewhere
10718620	10720060	where we start sharing all problems with it,
10720060	10721420	potentially in theory.
10721420	10724060	It's just a very hard problem that requires, you know,
10724060	10725380	a lot of work and a lot of compute
10725380	10726220	and it's not going to be fast
10726220	10727820	and it's not going to be simple.
10727820	10730020	And we want a lot of people who are going in parallel.
10730020	10731260	So I certainly, you know,
10731260	10734140	intend to assist with some amount of that.
10734140	10736620	Cool. Well, believe it or not,
10736620	10738940	we did not get to everything even on my outline,
10738940	10742100	let alone everything that you have covered
10742100	10744020	on your blog,
10744020	10745060	which has been, you know,
10745060	10746980	probably 10 times as many topics.
10746980	10750980	So folks will have to get the written version.
10750980	10752060	This is Vy Matryoz.
10752060	10754980	Thank you for being part of the Cognitive Revolution.
10754980	10757020	All right, bye.
10757020	10759460	Omniki uses generative AI
10759460	10761380	to enable you to launch hundreds of thousands
10761380	10763740	of ad iterations that actually work.
10763740	10767180	Customized across all platforms with a click of a button.
10767180	10769740	I believe in Omniki so much that I invested in it
10769740	10772020	and I recommend you use it too.
10772020	10774580	Use CogGrev to get a 10% discount.

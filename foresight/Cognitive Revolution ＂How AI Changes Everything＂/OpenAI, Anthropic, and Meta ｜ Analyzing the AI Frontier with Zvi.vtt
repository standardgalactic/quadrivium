WEBVTT

00:00.000 --> 00:02.920
one of the things people constantly get wrong

00:02.920 --> 00:05.400
if they think about human level as the peak of things.

00:05.400 --> 00:08.760
And so like once we've patched this and now it works,

00:08.760 --> 00:10.560
that's not really how this goes.

00:10.560 --> 00:13.360
There is no, it goes from not working to working,

00:13.360 --> 00:15.360
it goes from working worse to working better,

00:15.360 --> 00:17.840
and they could always go to working better still.

00:17.840 --> 00:20.240
And that's one of the reasons why we should be

00:20.240 --> 00:22.880
more worried or more excited or

00:22.880 --> 00:24.640
more curious about what's going to

00:24.640 --> 00:26.800
happen like three years from now,

00:26.800 --> 00:28.720
five years from now, 10 years from now,

00:28.880 --> 00:30.240
they're just going to keep going.

00:30.240 --> 00:31.760
And the question is, what does that get you?

00:31.760 --> 00:33.840
We talk about like worrying about China,

00:33.840 --> 00:36.000
but like I'm more afraid of Meta.

00:36.000 --> 00:37.960
Like one individual American company

00:37.960 --> 00:40.480
scares me more than all of China right now.

00:40.480 --> 00:43.400
You know, if you understand the Yudkowsky

00:43.400 --> 00:46.240
and difficulties, lessons, right, in some sense,

00:46.240 --> 00:48.120
and the nature of what problems you have to solve,

00:48.120 --> 00:50.960
or you have leadership capabilities,

00:50.960 --> 00:52.840
then you are actually going to be valuable in those ways.

00:52.840 --> 00:56.720
And it would be a major mistake to join an existing

00:56.720 --> 00:58.640
organization and try to make a difference

00:58.640 --> 01:02.840
as an individual as opposed to trying to spearhead

01:02.840 --> 01:05.680
a new organization or at least a new,

01:05.680 --> 01:08.320
you know, branch of a existing major organization,

01:09.240 --> 01:10.840
depending on your skill set.

01:10.840 --> 01:13.240
Hello, and welcome to The Cognitive Revolution,

01:13.240 --> 01:15.440
where we interview visionary researchers,

01:15.440 --> 01:18.160
entrepreneurs, and builders working on the frontier

01:18.160 --> 01:20.080
of artificial intelligence.

01:20.080 --> 01:22.880
Each week we'll explore their revolutionary ideas,

01:22.880 --> 01:25.480
and together we'll build a picture of how AI technology

01:25.480 --> 01:30.000
will transform work, life, and society in the coming years.

01:30.000 --> 01:33.480
I'm Nathan LaBenz, joined by my co-host, Eric Tornberg.

01:33.480 --> 01:34.520
This is V. Machwitz.

01:34.520 --> 01:36.880
Welcome back to The Cognitive Revolution.

01:36.880 --> 01:37.880
Good to be back.

01:37.880 --> 01:40.280
So we're trying something a little bit different this time.

01:40.280 --> 01:43.720
We are going to do some analysis

01:43.720 --> 01:46.760
of what has been going on in AI over,

01:46.760 --> 01:49.160
let's say the last few weeks to a month.

01:49.160 --> 01:51.160
You have published, as you always do,

01:51.160 --> 01:53.360
a bunch of deep dive blog posts,

01:53.360 --> 01:55.760
kind of covering everything.

01:55.760 --> 01:57.920
And for folks who want your background,

01:57.920 --> 01:59.360
of course we just did a recent episode too,

01:59.360 --> 02:01.480
so they can go and hear about your world view

02:01.480 --> 02:04.880
and your, you know, your AI childhood all there.

02:04.880 --> 02:07.440
But for today, I just want to pick out

02:07.440 --> 02:09.600
some of the most important stories

02:09.600 --> 02:11.800
and get your take on them and kind of, you know,

02:11.800 --> 02:14.000
exchange, go back and forth with some questions

02:14.000 --> 02:15.560
and try to make some sense out of it.

02:15.560 --> 02:17.240
And hopefully that'll be useful,

02:17.240 --> 02:19.120
not just to us, but to the audience as well.

02:19.120 --> 02:20.880
Yeah, I think the easiest thing is that there's

02:20.880 --> 02:22.560
constantly news coming at all of us.

02:22.560 --> 02:24.080
And so it's easy to get lost in like,

02:24.080 --> 02:25.000
here's the thing, here's the thing,

02:25.000 --> 02:26.040
here's another thing, here's another thing.

02:26.040 --> 02:29.320
So that's good to step back and dive deep.

02:29.320 --> 02:32.280
So I organized this discussion around

02:32.280 --> 02:35.320
the concept of live players.

02:35.320 --> 02:40.160
You know, there are only so many organizations right now

02:40.160 --> 02:43.560
who seem to be really pushing the frontier

02:43.560 --> 02:47.240
and in a position to have a meaningful impact

02:47.240 --> 02:48.520
on the course of events.

02:48.520 --> 02:49.800
We talked last time a little bit about like

02:49.800 --> 02:51.200
how much does history matter

02:51.200 --> 02:53.360
and it seems like it matters in some ways

02:53.360 --> 02:54.600
and maybe less in other ways.

02:54.600 --> 02:56.800
But these are the folks that are kind of creating

02:56.800 --> 02:59.120
the history right now, the live players.

02:59.120 --> 03:00.560
So I thought we would just kind of run it down

03:00.560 --> 03:02.400
by going through some of the live players,

03:02.400 --> 03:05.360
talking about their recent announcements and releases

03:05.360 --> 03:08.120
and again, trying to make sense of where that fits into

03:08.120 --> 03:09.920
the broader big picture.

03:09.920 --> 03:14.520
And starting off naturally, we go to open AI.

03:14.520 --> 03:17.520
So reading your blog in preparation for this,

03:17.520 --> 03:20.000
obviously, you know, you can't go more than a few paragraphs

03:20.000 --> 03:23.920
without open AI coming up in one way, shape, or form.

03:23.920 --> 03:25.440
But the thing that stuck out to me

03:25.440 --> 03:29.600
as kind of the most interesting was the recent comment

03:29.600 --> 03:32.360
that Jan Leica had made and Jan is,

03:32.360 --> 03:34.080
for those that don't know the name,

03:34.080 --> 03:37.760
he's the head of alignment at open AI

03:37.760 --> 03:41.960
and along with Ilya Sotskyver leading

03:41.960 --> 03:44.880
the new super alignment team, as I understand it.

03:44.880 --> 03:48.160
I want to start off by just kind of an interesting disconnect

03:48.160 --> 03:51.840
between him and you and maybe me as well

03:51.840 --> 03:54.120
around just the power of GPT-4.

03:54.120 --> 03:55.600
So before we even get into, you know,

03:55.600 --> 03:57.640
the kind of speculation about the future,

03:57.640 --> 03:59.680
it really jumped out to me that he said,

03:59.680 --> 04:01.720
overall, GPT-4 is maybe at the level

04:01.720 --> 04:05.120
of a well-read college undergrad.

04:05.120 --> 04:06.880
And then you came back and said,

04:06.880 --> 04:10.760
you consider it to be well below human level.

04:10.760 --> 04:14.320
And I have often said that I consider it to be human level,

04:14.320 --> 04:16.960
but not human-like.

04:16.960 --> 04:19.880
And I've sort of been trying to refine

04:19.880 --> 04:23.440
what I mean by that in a few different ways over time.

04:23.440 --> 04:24.720
But for starters, let's get your take.

04:24.720 --> 04:26.800
What do you think is the disconnect between Jan and you

04:26.800 --> 04:29.120
there where he sees something like human level

04:29.120 --> 04:30.520
and you would say well below?

04:30.520 --> 04:32.680
Yeah, I don't think it's about the specific model at all,

04:32.680 --> 04:33.120
obviously.

04:33.120 --> 04:36.320
I think we both agree that GPT-4 is the dominant model

04:36.320 --> 04:39.040
right now and like we will be for some months to come,

04:39.040 --> 04:40.120
at least.

04:40.120 --> 04:42.120
But I think it's a matter of like,

04:42.120 --> 04:44.000
how do you think about what it means

04:44.000 --> 04:46.760
to be at the level of a college undergrad

04:46.760 --> 04:48.640
or what are we measuring?

04:48.640 --> 04:50.280
What are we judging by?

04:50.280 --> 04:56.080
And I think Laiki is thinking about it as, OK,

04:56.080 --> 05:00.120
in terms of ability to just deal with a variety of random

05:00.120 --> 05:02.400
questions that were typically thrown at something,

05:02.400 --> 05:05.040
how is it going to do compared to the average college

05:05.040 --> 05:05.440
undergrad?

05:05.440 --> 05:06.880
He's like, what's about that level?

05:06.880 --> 05:09.160
You have a well-read college undergrad.

05:09.160 --> 05:12.000
Whereas that's an important question

05:12.000 --> 05:14.360
to be asking for practical purposes.

05:14.360 --> 05:17.440
But to me is not the relevant question

05:17.440 --> 05:20.600
to what the things are that we're thinking about.

05:20.600 --> 05:21.920
And that's one of the, when he says

05:21.920 --> 05:24.440
he's going to align a human level alignment researcher

05:24.440 --> 05:26.920
within four years, I thought that assumes

05:26.920 --> 05:29.840
that there's going to be a much, much more powerful AI

05:29.840 --> 05:33.040
four years from now waiting to be aligned.

05:33.040 --> 05:34.640
It's not talking about aligned GPT-4

05:34.640 --> 05:35.880
and then pointing at alignment.

05:35.880 --> 05:37.280
That obviously wouldn't do anything.

05:37.280 --> 05:40.520
It's going to deal with some of your blocks

05:40.520 --> 05:42.200
and it's going to increase in your affordances

05:42.200 --> 05:43.640
and your efficiency somewhat.

05:43.680 --> 05:46.120
Maybe you'll be 50% faster with GPT-4

05:46.120 --> 05:48.920
than you would have been without any LMS.

05:48.920 --> 05:51.880
Maybe even 100% faster if you're using it really well

05:51.880 --> 05:53.600
and things connect to it really well.

05:53.600 --> 05:56.880
And in the context of alignment, obviously

05:56.880 --> 05:58.480
having a model to experiment with and bang on

05:58.480 --> 06:01.040
is distinct from the thing that we're talking about here,

06:01.040 --> 06:03.680
but is potentially necessary.

06:03.680 --> 06:06.240
But it's not going to be able to substitute for anything

06:06.240 --> 06:07.400
like a human researcher.

06:07.400 --> 06:09.680
If you put a well-read college undergrad

06:09.680 --> 06:12.240
on the problem of something complex,

06:12.280 --> 06:14.680
like aligning a model,

06:14.680 --> 06:17.600
they could potentially begin to make progress.

06:17.600 --> 06:20.760
And if you asked GPT-4 to do that, you would get nothing.

06:21.800 --> 06:24.480
And part of that is that we haven't figured out

06:24.480 --> 06:27.720
how to structure how we talk to it

06:27.720 --> 06:29.840
and turn it into a proper agent

06:29.840 --> 06:32.240
and give it the proper memory and so on.

06:32.240 --> 06:34.560
But to me, most of it is just,

06:34.560 --> 06:37.480
every system has what you might call a raw G to it,

06:37.480 --> 06:41.840
whether it's a human or an artificial intelligence.

06:41.840 --> 06:46.120
And on that level, I feel like GPT-4

06:46.120 --> 06:51.120
is still well below the IQ 100 median human.

06:51.320 --> 06:53.280
It is going to obviously answer

06:53.280 --> 06:56.040
my ordinary day-to-day questions much better

06:56.040 --> 06:58.720
than if I asked an ordinary IQ 100 human

06:58.720 --> 07:01.080
to help me out with a variety of questions.

07:01.080 --> 07:04.400
That's because it has these huge advantages over a human.

07:04.400 --> 07:08.040
It has access to orders of magnitude and more knowledge

07:08.080 --> 07:12.000
and memory and ability to go through cycles.

07:12.000 --> 07:15.520
But there's still this dynamic in my brain

07:15.520 --> 07:19.360
where if you don't have the G,

07:19.360 --> 07:21.800
problems that require more G than you have

07:21.800 --> 07:25.320
become exponentially harder or then impossible to do

07:25.320 --> 07:28.480
very quickly as you exceed that.

07:28.480 --> 07:30.680
And in that sense,

07:30.680 --> 07:33.920
like the college undergrad had the chance given time

07:33.960 --> 07:38.120
and is smarter and GPT-4 is just nowhere near

07:38.120 --> 07:39.400
that kind of thing.

07:39.400 --> 07:44.320
So when you say that the missing pieces around memory

07:44.320 --> 07:49.320
and kind of packaging GPT-4 or successor up into an agent,

07:50.680 --> 07:53.560
those do feel to me also like being kind of

07:54.560 --> 07:57.040
pretty key missing pieces here.

07:57.040 --> 08:00.600
I mean, there are sort of potentially synergies

08:00.640 --> 08:05.240
between those kinds of parts of a system being built out

08:05.240 --> 08:08.440
and it just being smarter overall.

08:08.440 --> 08:12.520
But it seems like those are like pretty distinct concepts

08:12.520 --> 08:16.440
in that GPT-4 could have like a much better memory

08:16.440 --> 08:19.360
and certainly people are working on all sorts of like

08:19.360 --> 08:22.040
schemes for that and embedding databases

08:22.040 --> 08:24.200
and how do you put stuff into the embedding database

08:24.200 --> 08:26.240
and do you even like,

08:26.240 --> 08:28.000
some of the most interesting stuff I've seen recently

08:28.000 --> 08:31.400
has been kind of creating a layer of like synthetic memory

08:31.400 --> 08:34.920
that sits on top of the raw observational memory

08:34.920 --> 08:37.560
that tries to kind of ultimately work its way up

08:37.560 --> 08:40.880
into something like a coherent narrative,

08:40.880 --> 08:44.600
that could still kind of fit into prompt context length,

08:44.600 --> 08:47.640
but kind of summarizes, synthesizes,

08:47.640 --> 08:49.880
represents all these detailed memories

08:49.880 --> 08:51.680
in a hopefully coherent way.

08:51.680 --> 08:54.440
Obviously is what the developers are going for there.

08:54.440 --> 08:57.360
Those pieces seem like, yeah, they're totally missing.

08:57.360 --> 09:00.080
I expect them to come online, you know,

09:00.080 --> 09:02.600
somewhat gradually, but certainly over the next six months

09:02.600 --> 09:04.800
to a year, if not maybe even sooner.

09:04.800 --> 09:06.680
And then I am kind of like,

09:06.680 --> 09:08.160
it does seem like this, you know,

09:08.160 --> 09:11.520
GPT-4 with those weaknesses kind of patched,

09:11.520 --> 09:13.640
it does seem to me like it would be

09:13.640 --> 09:16.400
roughly at that college under grad level.

09:16.400 --> 09:18.280
If those things did come online,

09:18.280 --> 09:19.440
would you see that the same way

09:19.440 --> 09:22.200
or you still think it's like missing something super important?

09:22.200 --> 09:23.040
No, I'm sorry.

09:23.040 --> 09:24.840
I'm definitely not on Team Physicastic Power, right?

09:24.840 --> 09:27.320
Like I'm in no way on that team.

09:27.320 --> 09:29.840
However, I do think in a real sense,

09:29.840 --> 09:32.440
what you're witnessing is, you know,

09:32.440 --> 09:35.120
the training data covers, you know,

09:35.120 --> 09:37.400
the vast majority of things humans do and say

09:37.400 --> 09:40.680
and consider in various senses over text.

09:40.680 --> 09:44.360
And, you know, within the sample of the training data,

09:44.360 --> 09:46.360
like while you're doing things similar to the training data,

09:46.360 --> 09:49.880
it's learned how to pattern match and copy and imitate

09:49.880 --> 09:50.720
and work with that.

09:50.720 --> 09:53.480
And it has a huge amount of knowledge base

09:53.480 --> 09:56.720
and levels of association and the tools to work within that.

09:56.720 --> 09:59.360
And if you gave it these other tools,

09:59.360 --> 10:00.480
we'll be able to do these things

10:00.480 --> 10:03.080
and string them together across more steps

10:03.080 --> 10:04.560
in some important sense.

10:04.560 --> 10:08.120
But the moment you take it out of its comfort zone,

10:08.120 --> 10:11.560
we're asking you to do something that's distinctly different

10:11.560 --> 10:14.520
than what has come before to be truly original.

10:14.520 --> 10:17.920
I think your episode with the Hollywood writers

10:17.920 --> 10:20.000
and like they talked about what was going on in the second

10:20.000 --> 10:22.840
and trying to get the GPT forwarded to work for them.

10:22.840 --> 10:26.280
And yeah, it was great at generating like generic schlock,

10:26.280 --> 10:28.200
right? Like much better than they could.

10:28.200 --> 10:29.440
And like if you needed to be like, okay,

10:29.440 --> 10:30.480
somebody get me unstuck,

10:30.480 --> 10:33.560
somebody get me some generic schlock based on my situation

10:33.560 --> 10:34.720
that I happened to have been written in

10:34.720 --> 10:37.880
because this is episode 47 of the show or whatever it is.

10:37.880 --> 10:39.360
It could be tremendously helpful.

10:39.360 --> 10:41.360
But whenever you asked it to actually do something

10:41.360 --> 10:44.440
that we would recognize as distinctly creative

10:44.440 --> 10:47.800
and original, you know, in a way that's distinct from that,

10:47.800 --> 10:49.640
they just fall over flight every time.

10:49.640 --> 10:51.440
And none of those problems are gonna be rescued

10:51.440 --> 10:52.720
by any of these fixes, right?

10:52.720 --> 10:55.040
Like they're just orthogonal problems, right?

10:55.040 --> 10:58.600
Like I think that's the sense in which, you know,

10:58.600 --> 11:02.600
you're going to be able to give it more capacities

11:02.600 --> 11:06.040
to be able to navigate more of the conventional things

11:06.040 --> 11:09.000
over longer periods more consistently.

11:09.000 --> 11:11.320
And that's gonna have tremendous mundane utility

11:11.320 --> 11:15.760
as I call it, or it's gonna be a much better functioning system.

11:15.760 --> 11:19.800
But the reason why I'm focused on this other question

11:19.800 --> 11:21.360
is because I am focused on the question

11:21.360 --> 11:23.440
of how dangerous the system is, right?

11:23.440 --> 11:25.520
Like I'm asking the question,

11:25.520 --> 11:27.640
could this system potentially engage

11:27.640 --> 11:29.800
in recursive self-improvement?

11:29.800 --> 11:33.640
Could this program potentially pose a threat to humans?

11:33.640 --> 11:34.480
Right?

11:34.480 --> 11:35.320
Could it compete for resources?

11:35.320 --> 11:37.400
Could it manipulate us?

11:37.400 --> 11:40.360
Could it do things that are actively destructive

11:40.360 --> 11:41.680
because it uncovers capabilities

11:41.680 --> 11:44.760
that like weren't in its training set in various ways

11:44.760 --> 11:46.760
and other related questions like that?

11:47.680 --> 11:49.600
And I don't see the kinds of things

11:49.600 --> 11:51.300
that you're talking about that I agree will come online.

11:51.980 --> 11:54.860
I would guess that we will be far from done

11:54.860 --> 11:55.940
with family here from now.

11:55.940 --> 11:57.420
Like there's just sort of so much to do

11:57.420 --> 11:59.900
in terms of scaling those up as much as possible.

11:59.900 --> 12:03.500
Cause like one of the things people constantly get wrong

12:03.500 --> 12:06.540
is they think about human level as the peak of things.

12:06.540 --> 12:09.500
And so like once we've patched this and now it works,

12:09.500 --> 12:11.180
that's not really how this goes, right?

12:11.180 --> 12:13.900
There is no, it goes from not working to working,

12:13.900 --> 12:16.220
it goes from working worse to working better

12:16.220 --> 12:18.660
and it could always go to working better still.

12:18.660 --> 12:19.500
And that's one of the reasons

12:19.500 --> 12:23.220
why we should be more worried or more excited

12:23.220 --> 12:26.340
or more curious about what's going to happen

12:26.340 --> 12:28.140
like three years from now, five years from now,

12:28.140 --> 12:29.460
10 years from now.

12:29.460 --> 12:30.540
We look at these systems

12:30.540 --> 12:32.180
is because there isn't gonna be a hard cap.

12:32.180 --> 12:35.100
We're not gonna max out each of these individual capabilities

12:35.100 --> 12:38.260
by default, they're just gonna keep going.

12:38.260 --> 12:40.060
And the question is, what does that get you?

12:40.060 --> 12:41.340
Kind of want to look at this from two angles.

12:41.340 --> 12:44.140
One is going back to the original disagreement

12:44.140 --> 12:45.940
or it's maybe less of a disagreement

12:45.940 --> 12:47.980
and more of kind of a difference in framing

12:47.980 --> 12:52.980
perhaps with Yian, what I would bottom line all that as

12:53.020 --> 12:58.020
is when you think about a well-read college undergrad,

12:58.260 --> 13:00.700
you think about high points

13:00.700 --> 13:03.340
in that individual human's performance

13:03.340 --> 13:05.740
that GBT-4 can't match

13:05.740 --> 13:08.460
and it's not really a question of memory

13:08.460 --> 13:11.380
or whatever that's kind of gating it.

13:11.380 --> 13:13.740
And if I had to guess, I would say he's maybe more looking

13:13.740 --> 13:18.740
at like average performance or sort of some sort of floor

13:20.100 --> 13:23.620
perhaps like maybe top 90% or whatever,

13:23.620 --> 13:24.940
you could frame it in a lot of different ways,

13:24.940 --> 13:27.700
but it sounds like you're kind of concerned with high points

13:27.700 --> 13:28.980
and he is maybe more concerned

13:28.980 --> 13:33.060
with some sort of central tendency sort of measure.

13:33.060 --> 13:34.700
I would put it differently.

13:34.700 --> 13:37.820
I would say he's concerned with some sense

13:37.820 --> 13:39.820
of average level of performance

13:39.820 --> 13:42.300
over a range of possible tasks.

13:42.300 --> 13:45.980
And I'm concerned with potential.

13:45.980 --> 13:49.220
I am concerned with what the capabilities would be

13:49.220 --> 13:52.260
if you got a chance to work with this thing

13:52.260 --> 13:54.500
to try and make it the best it could be, right?

13:54.500 --> 13:56.060
It doesn't necessarily have to be right now,

13:56.060 --> 14:00.220
but the reason why we value children

14:00.220 --> 14:03.300
and college grad and these undergraduates in these classes,

14:03.300 --> 14:07.020
I guess this undergraduate, they're an idiot, right?

14:07.020 --> 14:07.860
In some important sense.

14:07.860 --> 14:09.340
They know nothing about the world.

14:09.340 --> 14:11.740
They know nothing about how to do anything productive.

14:11.740 --> 14:13.860
They are gonna show up on the job on day one

14:13.860 --> 14:14.860
after graduating from college.

14:14.860 --> 14:17.820
They're gonna be useless pieces of junk,

14:17.820 --> 14:20.180
but a useless piece of junk that can then learn

14:21.340 --> 14:23.100
to be something great.

14:23.100 --> 14:26.500
And even then they're gonna only learn a very narrow portion

14:26.500 --> 14:29.220
of the things that a individual human is capable of learning.

14:29.220 --> 14:33.100
They're gonna learn that one job in that one area

14:33.100 --> 14:34.580
and they're gonna be very, very specialized

14:34.580 --> 14:36.340
compared to a TBD4.

14:36.340 --> 14:39.900
So if you are doing generalized tests

14:39.940 --> 14:42.620
and comparing these undergraduate who are educational,

14:42.620 --> 14:45.500
this does try to make well-rounded in some senses.

14:46.420 --> 14:49.180
It's gonna beat the well-rounded undergraduate

14:49.180 --> 14:51.740
because it has this ability to read every book ever written

14:51.740 --> 14:54.140
and everything on Reddit and everything on Twitter

14:54.140 --> 14:55.500
and blah, blah, blah.

14:55.500 --> 14:58.780
But when it comes down to solving a particular problem,

14:58.780 --> 15:00.020
if you find the right undergraduate

15:00.020 --> 15:03.140
who has focused on the particular thing that you wanna know

15:03.140 --> 15:05.260
and you give them a chance to use their compute

15:05.260 --> 15:07.620
and process because they're not as fast,

15:07.620 --> 15:09.580
I think the undergraduate's gonna dominate you.

15:09.580 --> 15:13.780
I think even a relatively normal human being,

15:14.700 --> 15:16.260
given an opportunity,

15:16.260 --> 15:19.180
will outperform quite resoundingly

15:19.180 --> 15:21.260
what can be done in that way.

15:21.260 --> 15:22.860
And that's the thing that I care about

15:22.860 --> 15:25.460
because that's the thing that's going to potentially

15:25.460 --> 15:29.900
both threaten us and also unleash the whips upon waves

15:29.900 --> 15:33.700
of super amazing value that we're looking for in the future.

15:33.700 --> 15:35.300
It's not just negative.

15:35.300 --> 15:38.660
If we want AI to solve the problems that we haven't solved

15:38.660 --> 15:40.940
rather than just get us nowhere faster,

15:40.940 --> 15:42.380
in some important sense,

15:42.380 --> 15:44.180
it's gonna have to be able to do these things, right?

15:44.180 --> 15:46.620
These are the things where it really counts.

15:46.620 --> 15:48.580
Hey, we'll continue our interview in a moment

15:48.580 --> 15:49.980
after a word from our sponsors.

15:49.980 --> 15:50.820
Hey, everybody.

15:50.820 --> 15:52.780
If you're a business owner or founder like me,

15:52.780 --> 15:55.220
you'll wanna know more about our sponsor NetSuite.

15:56.100 --> 15:57.580
NetSuite provides financial software

15:57.580 --> 15:58.780
for all your businesses.

15:58.780 --> 16:01.420
Whether you're looking for an ERP tool or accounting software,

16:01.420 --> 16:03.540
NetSuite gives you the visibility and control you need

16:03.540 --> 16:05.460
to make better decisions faster.

16:05.460 --> 16:07.460
And for the first time in NetSuite's 25 years

16:07.500 --> 16:09.420
as the number one cloud financial system,

16:09.420 --> 16:12.060
you can defer payments of a full NetSuite implementation

16:12.060 --> 16:13.220
for six months.

16:13.220 --> 16:15.780
That's no payment and no interest for six months.

16:15.780 --> 16:16.620
And you can take advantage

16:16.620 --> 16:18.620
of the special financing offered today.

16:18.620 --> 16:19.500
NetSuite is number one

16:19.500 --> 16:21.300
because they give your business everything you need

16:21.300 --> 16:23.180
in real time, all in one place

16:23.180 --> 16:25.940
to reduce manual processes, boost efficiency,

16:25.940 --> 16:27.980
build forecasts, and increase productivity

16:27.980 --> 16:29.500
across every department.

16:29.500 --> 16:32.460
More than 36,000 companies have already upgraded in NetSuite,

16:32.460 --> 16:34.580
gaining visibility and control over their financials,

16:34.580 --> 16:37.260
inventory, HR, e-commerce, and more.

16:37.260 --> 16:38.780
If you've been checking out NetSuite already,

16:38.780 --> 16:40.380
then you know this deal is unprecedented,

16:40.380 --> 16:41.900
no interest, no payments.

16:41.900 --> 16:43.700
So take advantage of the special financing offer

16:43.700 --> 16:46.980
with our promo code at netsuite.com slash cognitive,

16:46.980 --> 16:49.380
netsuite.com slash cognitive,

16:49.380 --> 16:51.460
to get the visibility and control your business needs

16:51.460 --> 16:52.700
to weather any storm.

16:52.700 --> 16:55.260
That is netsuite.com slash cognitive.

16:56.100 --> 16:59.460
Omnike uses generative AI to enable you to launch

16:59.460 --> 17:01.420
hundreds of thousands of ad iterations

17:01.420 --> 17:04.940
that actually work customized across all platforms

17:04.940 --> 17:06.260
with a click of a button.

17:06.260 --> 17:08.820
I believe in Omnike so much that I invested in it

17:08.820 --> 17:11.020
and I recommend you use it too.

17:11.020 --> 17:13.860
Use CogGrav to get a 10% discount.

17:13.860 --> 17:14.700
Yeah, it's interesting.

17:14.700 --> 17:16.020
I'm certainly concerned with all of that too.

17:16.020 --> 17:18.580
I think maybe I'm just more enthused

17:18.580 --> 17:22.660
about the mundane utility in the sense of,

17:22.660 --> 17:23.980
man, there's a lot of stupid stuff

17:23.980 --> 17:25.300
that people spend their time doing

17:25.300 --> 17:29.100
and I really would love to see them freed

17:29.100 --> 17:31.260
from having to do a lot of that stuff.

17:31.260 --> 17:34.340
But I think your term is perfect, right?

17:34.340 --> 17:37.580
It's a lot of stupid stuff that humans have to do, right?

17:37.580 --> 17:41.340
Like basically, even if you are an average person,

17:41.340 --> 17:45.700
you're gonna spend the vast majority of your time

17:45.700 --> 17:48.700
doing things that do not especially tax your intelligence.

17:48.700 --> 17:50.980
They do not especially require you to think hard.

17:50.980 --> 17:54.140
They do not put you at the peak of your abilities, right?

17:54.140 --> 17:55.900
They don't put you in a zone.

17:55.900 --> 17:58.240
They're just, okay, somebody has to file this paperwork.

17:58.240 --> 18:00.140
Okay, somebody has to work this retail counter.

18:00.140 --> 18:01.460
Somebody has to cash this check.

18:01.460 --> 18:03.500
Somebody has to do this thing.

18:03.580 --> 18:04.820
Be nice to this person.

18:04.820 --> 18:07.140
Somebody has to make sure that someone has direct.

18:07.140 --> 18:10.460
That's good work and noble work and it has to be done, right?

18:10.460 --> 18:12.420
And physical labor is the same way.

18:12.420 --> 18:13.820
If a physical laborer had to do things

18:13.820 --> 18:17.620
that were at the peak of their mental or physical requirements

18:17.620 --> 18:19.180
more than a few minutes or at most,

18:19.180 --> 18:22.940
if small portion of the day, it would break them.

18:22.940 --> 18:25.380
And also like those jobs just don't exist, right?

18:25.380 --> 18:27.900
Like you need someone strong

18:27.900 --> 18:29.420
so that in that moment you can have someone strong.

18:29.420 --> 18:31.180
You need someone smart so that in the few moments

18:31.180 --> 18:32.180
when it's important to have someone smart,

18:32.380 --> 18:33.540
you have someone smart.

18:33.540 --> 18:37.500
If you can then take the bottom 80% of my job

18:37.500 --> 18:39.740
and you can do an 80% good job of that

18:39.740 --> 18:41.660
so that I only have to do the remaining 20% of that,

18:41.660 --> 18:44.340
now two thirds of my day is free.

18:44.340 --> 18:47.260
And I can be three times as productive, right?

18:47.260 --> 18:49.060
That's a tremendous leap and I agree.

18:49.060 --> 18:52.260
That is the potential of GPT-4, right?

18:52.260 --> 18:54.940
That's what we're looking at here is

18:54.940 --> 18:58.420
if we understand how to use this technology properly,

18:58.420 --> 19:01.180
we can potentially free ourselves from a lot of drudgery

19:02.540 --> 19:04.700
and streamline a bunch of stuff

19:04.700 --> 19:07.980
and get to do all the cool things.

19:07.980 --> 19:09.620
And there are various traps we can fall into,

19:09.620 --> 19:11.340
one of which is that we automate exactly the things

19:11.340 --> 19:12.180
we don't wanna be automating,

19:12.180 --> 19:13.980
not the things we do wanna be automating.

19:13.980 --> 19:15.740
One of which is that the moment we notice

19:15.740 --> 19:16.580
that paperwork is faster,

19:16.580 --> 19:17.740
now we put in more paperwork

19:17.740 --> 19:20.360
and now it turns out that humans are taking just as long

19:20.360 --> 19:22.660
to do more useless stuff than they did before.

19:22.660 --> 19:27.140
And GPT is just keeping us, letting us treadmill in place.

19:27.140 --> 19:30.140
And there's another way that this can go wrong, right?

19:30.140 --> 19:32.300
And also there are various weird dynamics

19:32.300 --> 19:33.540
that can happen that can backfire.

19:33.540 --> 19:37.040
But yeah, that's what we're trying to do.

19:37.040 --> 19:40.220
If you wanna get the effect that Licky wants, right?

19:40.220 --> 19:42.460
The sea change that'll let us solve problems

19:42.460 --> 19:44.180
we couldn't solve before,

19:44.180 --> 19:47.180
that involves these things being able to do all

19:47.180 --> 19:48.540
the different steps that humans could do

19:48.540 --> 19:49.980
because otherwise, whatever the bottlenecks are

19:49.980 --> 19:52.500
that are left, become your bottlenecks

19:52.500 --> 19:54.140
where you have to translate all the context

19:54.140 --> 19:57.020
back from the machine world back into the human world

19:57.020 --> 19:59.980
so that a human can process all of that

19:59.980 --> 20:03.900
then do the hard step that this thing is still faltering on

20:03.900 --> 20:05.020
and then transition back.

20:05.020 --> 20:07.420
And now instead of getting orders of magnitude

20:07.420 --> 20:10.380
and more progress, right now we're talking about

20:10.380 --> 20:12.780
these factor of two, factor of three,

20:12.780 --> 20:15.780
factor of five style improvements.

20:15.780 --> 20:20.020
And that's not gonna solve the alignment problem

20:20.020 --> 20:22.660
unless we come up with something we don't expect, right?

20:22.660 --> 20:26.100
In and of itself, that's still worth pursuing

20:26.100 --> 20:27.540
if we can do it, right?

20:27.540 --> 20:29.460
We still wanna do as much of it as possible.

20:29.460 --> 20:32.300
And it has the advantage of not being as dangerous.

20:32.300 --> 20:33.340
But it's not the thing

20:33.340 --> 20:35.580
that the super alignment project is trying to do, right?

20:35.580 --> 20:36.780
The super alignment project is trying

20:36.780 --> 20:40.140
to keep the humans out of the loop entirely.

20:40.140 --> 20:43.740
And that should be about as scary as it sounds.

20:43.740 --> 20:48.100
Brief digression over toward this tale of the cognitive tape.

20:48.100 --> 20:49.540
This is a concept that I've developed

20:49.540 --> 20:54.220
for kind of purpose of public communication.

20:54.220 --> 20:56.540
And just trying to give people an intuition,

20:56.540 --> 20:58.940
you know, still in the very literal way, of course,

20:58.940 --> 21:02.140
as to the strengths of a human

21:02.140 --> 21:03.860
and the relative strengths and weaknesses

21:03.860 --> 21:06.100
of the best AIs today.

21:06.100 --> 21:08.060
Listeners can see this in the AI Scouting Report

21:08.060 --> 21:09.020
if they wanna go into the whole thing.

21:09.020 --> 21:11.300
But do you, as you look at that,

21:11.300 --> 21:14.980
do you see any dimensions that you would suggest

21:14.980 --> 21:18.540
that I add that, you know, just haven't been considered?

21:18.540 --> 21:20.300
Or do you see any disagreements

21:20.300 --> 21:22.060
as you scan down the list?

21:22.060 --> 21:22.900
Yeah, I think that's what we're going through

21:22.900 --> 21:24.780
because people are not gonna have it handy

21:24.780 --> 21:26.060
right to look at.

21:26.060 --> 21:28.980
So, you know, for breadth, yes, the AI, as I said,

21:28.980 --> 21:31.020
like the AI's biggest advantage is it can cover

21:31.020 --> 21:33.660
every topic at once, it can know everything at once.

21:33.660 --> 21:35.140
A human can't do that.

21:35.140 --> 21:37.780
In terms of depth, yeah, a human has the advantage.

21:37.780 --> 21:39.460
I'm not even sure I give the second level.

21:39.460 --> 21:41.540
Like you graded the AI two out of three.

21:41.540 --> 21:42.940
And I think I might grade it one out of three

21:42.940 --> 21:43.780
in terms of depth.

21:43.780 --> 21:46.340
I think the depth is a huge problem for AIs right now.

21:46.340 --> 21:48.860
Breakthrough insight, yeah, it's three versus zero,

21:48.860 --> 21:52.100
three versus one, it's the humans are dominating again.

21:52.100 --> 21:56.460
You know, speed, yeah, the humans are painfully slow,

21:56.460 --> 21:58.060
you know, 10x faster.

21:58.060 --> 22:00.500
In terms of like actually getting it to say things

22:00.500 --> 22:02.940
and putting outputs in real time,

22:02.940 --> 22:04.580
it's maybe only 10x faster,

22:04.580 --> 22:06.260
but in terms of being able to like cross information,

22:06.260 --> 22:07.820
it's thousands and tens of thousands

22:07.820 --> 22:10.580
and hundreds of thousands of times faster, which is, yeah.

22:10.580 --> 22:12.340
A huge deal.

22:12.340 --> 22:14.540
In terms of cost, you know,

22:14.540 --> 22:17.500
we're not internalizing yet all of the costs of doing this

22:17.500 --> 22:18.340
in an important sense,

22:18.340 --> 22:20.380
like these companies are eating these huge losses

22:20.380 --> 22:22.100
to try and get these dominant market positions

22:22.100 --> 22:24.300
in the future, try to stay ahead of each other

22:24.300 --> 22:25.300
for all these dependencies.

22:25.300 --> 22:27.140
But yes, cost is still dominated.

22:27.140 --> 22:30.700
AIs are already vastly cheaper when the AI is useful,

22:30.700 --> 22:32.580
even in the real costs.

22:32.580 --> 22:35.340
We have availability, paralyzability.

22:36.500 --> 22:37.860
Yep, the AI has a big advantage.

22:37.860 --> 22:40.380
It's potentially actually gonna become a problem.

22:40.380 --> 22:43.180
There's a huge race to compute right now

22:43.180 --> 22:46.300
where computers no longer gonna be like essentially free.

22:46.300 --> 22:48.700
It's gonna become like kind of unpriced

22:48.700 --> 22:49.540
in an important sense.

22:49.540 --> 22:51.460
Interesting to wonder what's gonna happen there,

22:51.460 --> 22:53.100
especially at industrial scales.

22:53.100 --> 22:56.420
And by unpriced, you mean that basically

22:56.420 --> 23:00.700
your access to GPUs is going from ability to pay

23:00.700 --> 23:02.260
to who you know?

23:02.260 --> 23:05.700
Yeah, or does your company have the right arrangements?

23:05.700 --> 23:06.540
Right?

23:06.540 --> 23:09.540
If you want one GPU for your individual computer, it's fine.

23:09.540 --> 23:10.860
You can buy it on eBay if you have to

23:10.860 --> 23:13.180
for some amount of money, it won't be that expensive.

23:13.180 --> 23:15.380
If you want small amounts like the kinds

23:15.380 --> 23:17.740
when you're just using GPD4, it's gonna be relatively easy.

23:17.740 --> 23:20.100
But if you wanna do an AI company, right?

23:20.100 --> 23:21.940
It's gonna be a problem because, you know

23:21.940 --> 23:23.900
if you want industrial levels,

23:23.900 --> 23:25.500
it's not just gonna be multiply that

23:25.500 --> 23:27.860
by the amount you want necessarily.

23:27.860 --> 23:30.700
It's gonna be, there isn't enough to go around.

23:30.700 --> 23:33.780
You know, people like NVIDIA are not pricing this at market.

23:33.780 --> 23:35.380
And so you're not have to find someone

23:35.380 --> 23:38.140
going to sell it at the actual market price.

23:38.140 --> 23:40.460
That number might be very, very different

23:40.460 --> 23:42.020
from the price you think it is

23:42.020 --> 23:44.340
because there are so many AI companies,

23:44.380 --> 23:47.780
so many AI researchers, so many AI engineers

23:47.780 --> 23:49.980
and they're chasing a number that can only go up so fast.

23:49.980 --> 23:51.860
This is my understanding of the current situation.

23:51.860 --> 23:53.140
Availability, paralyzability though,

23:53.140 --> 23:54.900
still favors the AI.

23:54.900 --> 23:57.140
You know, time horizon memory.

23:57.140 --> 23:58.460
Time horizon is an interesting question.

23:58.460 --> 24:02.220
I think this is a murky place to think.

24:02.220 --> 24:04.900
Certainly the AI has a certain kind of memory

24:04.900 --> 24:07.020
like a long-term memory that is vastly bigger

24:07.020 --> 24:09.020
obviously than any human.

24:09.020 --> 24:11.540
But in terms of being able to meaningfully hold

24:11.540 --> 24:14.780
like particular context in their heads at once,

24:14.780 --> 24:16.220
like humans are bad at this

24:16.220 --> 24:18.780
and AI's are so much worse, right?

24:18.780 --> 24:21.300
The Tyra Cohen saying context is that which is scarce.

24:21.300 --> 24:22.540
Very much applies here.

24:24.260 --> 24:26.740
Technology diffusion speed, yep.

24:26.740 --> 24:28.620
We are ordered to magnitude behind here.

24:28.620 --> 24:30.460
This is gonna be a serious problem.

24:30.460 --> 24:33.940
You know, our OODA loops are way too slow.

24:33.940 --> 24:37.500
And this is a, it's gonna be an increasingly huge deal.

24:37.500 --> 24:40.820
The AI, but that matter is an interesting question

24:40.820 --> 24:43.940
because when you are optimizing

24:43.940 --> 24:46.700
for exactly the right type of bedside manner,

24:46.700 --> 24:48.980
where the thing that you're asking the AI to do

24:48.980 --> 24:50.540
is the thing that people actually want,

24:50.540 --> 24:54.220
the AI is gonna be off the charts better than a human

24:54.220 --> 24:59.220
because the humans are not purely optimizing for that thing.

24:59.940 --> 25:01.260
But at the same time, if you think about like

25:01.260 --> 25:05.940
the bedside manner of Claude or Lama

25:05.940 --> 25:08.700
when they are refusing your request, right?

25:08.700 --> 25:10.180
It's also simply bedside manner.

25:10.180 --> 25:11.580
And it's terrible, right?

25:11.580 --> 25:14.020
It's like negative one stars, right?

25:14.020 --> 25:17.700
They are raging assholes when they refuse, right?

25:17.700 --> 25:19.980
Like maybe we can have a conversation

25:19.980 --> 25:22.980
about social justice rather than answering your request.

25:22.980 --> 25:25.580
It's like, this is absurd.

25:25.580 --> 25:28.940
Why are you calling me out for wanting information

25:28.940 --> 25:30.820
or trying to do something fun?

25:30.820 --> 25:32.180
You know, it's not necessary.

25:32.180 --> 25:37.180
No human would ever do that unless they were actively mad

25:37.660 --> 25:40.700
at you and trying to punish you for asking.

25:40.700 --> 25:42.700
So why are you doing that?

25:42.700 --> 25:43.540
Right?

25:43.540 --> 25:45.060
The answer is because we trained them to do that.

25:45.060 --> 25:46.620
But we could have trained them to do something else.

25:46.620 --> 25:48.740
We just chose to do this instead

25:48.740 --> 25:52.060
because that's what the RLHF parameters said to do.

25:52.060 --> 25:53.380
And that confused me.

25:53.380 --> 25:55.300
So, you know, what else is there?

25:55.300 --> 25:59.220
I mean, so you said you talk about breakthrough insight

26:00.140 --> 26:05.140
and I think more about like being able to handle

26:06.140 --> 26:07.340
unprecedented situations,

26:07.340 --> 26:11.100
being able to process something genuinely new, right?

26:11.100 --> 26:13.900
As sort of the version of that

26:13.900 --> 26:17.060
that I'm more interested in, I guess, kind of there.

26:18.020 --> 26:21.580
Being able to properly deal with a lot of different inputs.

26:21.580 --> 26:23.700
One thing I noticed, like when you work

26:23.700 --> 26:27.940
with stable diffusion or other AI image generators,

26:27.940 --> 26:31.820
what you notice is sort of they are amazing

26:31.860 --> 26:35.260
at doing one of each type of thing at once.

26:35.260 --> 26:39.420
So you want like one face and one person

26:39.420 --> 26:43.660
or one set of people doing one thing with one style,

26:43.660 --> 26:46.380
with one size, with one this, with one that.

26:46.380 --> 26:47.220
That's fine.

26:47.220 --> 26:50.300
But the moment you try to mix things that kind of overlap,

26:50.300 --> 26:53.100
it will lose the thread almost immediately.

26:53.100 --> 26:55.180
And it is very, very difficult to get it back.

26:55.180 --> 26:57.420
So when you look at people who are generating

26:57.420 --> 27:01.500
all of this AI art, it starts to be very, very repetitive

27:01.540 --> 27:05.380
because there's a certain kind of complexity and detail

27:05.380 --> 27:07.580
you can't ask for at the same time.

27:07.580 --> 27:10.580
Because the AI can't comprehend that you want this over here

27:10.580 --> 27:12.980
and this over here and this interact with that.

27:12.980 --> 27:15.100
And like you'd be better off trying to create

27:15.100 --> 27:17.260
like four different pictures and then splice them together,

27:17.260 --> 27:18.100
right?

27:18.100 --> 27:19.660
Or you better off trying to use like the Photoshop app

27:19.660 --> 27:21.540
where you like highlight a certain area

27:21.540 --> 27:23.260
and ask specifically do something in this area

27:23.260 --> 27:24.740
and leave everything else untouched.

27:24.740 --> 27:25.940
It's like trying to generate it all at once

27:25.940 --> 27:26.940
is kind of hopeless.

27:27.860 --> 27:29.860
And the LLMs like exhibit the same kind of thing

27:29.860 --> 27:31.620
but with words, right?

27:31.620 --> 27:34.500
Like they're vibing off of everything

27:34.500 --> 27:36.940
and vibing into everything.

27:36.940 --> 27:41.620
And like they have memory, long-term memory for facts,

27:41.620 --> 27:43.620
but only can remember one vibe.

27:43.620 --> 27:46.020
And a lot of what they're doing is based on vibing.

27:46.020 --> 27:48.420
So it's a serious problem.

27:48.420 --> 27:50.660
I haven't seen any serious attempts to solve it yet.

27:50.660 --> 27:53.660
I haven't even really seen people discussing it in that way.

27:53.660 --> 27:55.940
I'm sure these things will improve with time,

27:55.940 --> 27:59.940
but what I think of as fundamental flaws or gaps

27:59.940 --> 28:03.420
in their ability to process information

28:03.420 --> 28:07.540
and actually handle complexity and context

28:07.540 --> 28:10.220
and originality.

28:10.220 --> 28:13.140
And this is where I see them as like

28:13.140 --> 28:16.780
still having a long way to go and falling down.

28:16.780 --> 28:19.580
And I don't want to make the mistake of,

28:19.580 --> 28:21.820
oh, I will never be able to axe.

28:21.820 --> 28:24.260
And I will never be as good as humans at Y.

28:24.260 --> 28:25.540
And we have nothing to ever worry about.

28:25.540 --> 28:27.460
I totally think that is not true.

28:27.460 --> 28:35.100
But for now, right, we still have this kind of cool toy

28:35.100 --> 28:39.460
because of these limitations, which can still, again,

28:39.460 --> 28:42.700
substitute for the majority of the things we do spend time

28:42.700 --> 28:46.100
doing if we are engaged in a wide variety of work

28:46.100 --> 28:48.900
if we use it well.

28:48.900 --> 28:50.340
Coding is one of the places where

28:50.340 --> 28:51.940
it has a huge advantage for some people,

28:51.940 --> 28:55.140
but other people are like, I don't code generic stuff.

28:55.180 --> 28:57.540
It's like I have a friend whose name is Alan.

28:57.540 --> 28:59.780
And he tried it out on my behalf.

28:59.780 --> 29:01.500
And he said, yeah, this is interesting.

29:01.500 --> 29:03.620
And there are some ways in which it's kind of cool.

29:03.620 --> 29:05.060
And it's cool to know this exists.

29:05.060 --> 29:07.100
And I never thought this existed.

29:07.100 --> 29:10.900
But when I'm writing stuff, I am actually

29:10.900 --> 29:12.940
trying to figure out how to do things that

29:12.940 --> 29:14.700
weren't in this training data.

29:14.700 --> 29:16.980
I'm not trying to re-implement the same things over and over

29:16.980 --> 29:20.580
again, which most engineers, in fact, mostly are doing.

29:20.580 --> 29:24.140
Because of what his job is, it turns out this thing is basically

29:24.140 --> 29:28.020
useless, because once you take it out of its sample,

29:28.020 --> 29:30.180
and you have to do something in a different domain,

29:30.180 --> 29:32.020
it makes so many errors that it's not better

29:32.020 --> 29:33.260
than just doing it yourself.

29:33.260 --> 29:37.260
So would I bottom line that to basically robustness

29:37.260 --> 29:38.940
if I had to add another category?

29:38.940 --> 29:42.260
It's sort of adversarial out of distribution?

29:42.260 --> 29:44.540
Yeah, I would say robustness.

29:44.540 --> 29:47.420
And I would also say resilience or some form of that.

29:47.420 --> 29:49.820
And separately, I would say, and I don't think I even

29:49.820 --> 29:52.820
went into this, the adversarial problem.

29:52.860 --> 29:55.660
It's totally unfair to the AIs, in some important sense,

29:55.660 --> 29:57.740
that we're judging them this way.

29:57.740 --> 30:01.980
Because if I got infinite clones of Nathan,

30:01.980 --> 30:04.260
and I could ask them any sequence I wanted,

30:04.260 --> 30:06.220
and then reset their memories in state

30:06.220 --> 30:08.180
to the previous situation whenever I didn't

30:08.180 --> 30:11.140
like what I got, and then just keep trying them until I can

30:11.140 --> 30:13.580
get you to tell me what the bomb secrets are,

30:13.580 --> 30:16.380
I guarantee you I'm getting your bomb secrets.

30:16.380 --> 30:17.900
It's not very hard.

30:17.900 --> 30:20.140
Humans are not that defended.

30:20.140 --> 30:23.580
But you can't run that attack on us.

30:23.580 --> 30:25.100
You don't get to do that.

30:25.100 --> 30:29.780
And I can run that attack on the computer, on the LLM.

30:29.780 --> 30:30.860
And some people have.

30:30.860 --> 30:32.540
And in fact, recently we had a paper

30:32.540 --> 30:35.980
with automated finding universalized attacks

30:35.980 --> 30:38.300
against language models.

30:38.300 --> 30:42.740
Where even GPT-4 could write the code for some of these attacks

30:42.740 --> 30:43.900
and did.

30:43.900 --> 30:47.380
Because if you get unlimited tries

30:47.380 --> 30:51.060
and you get to exactly measure what the output is,

30:51.060 --> 30:54.900
and then use that to calibrate, it's only a matter of time

30:54.900 --> 30:57.220
before you figure out every little quirk,

30:57.220 --> 31:00.380
and playing offense is so much easier than playing defense.

31:00.380 --> 31:01.220
OK, cool.

31:01.220 --> 31:03.500
So I've got two categories to add to my tale

31:03.500 --> 31:05.100
of the cognitive tape.

31:05.100 --> 31:08.660
Let's bounce up a level then back to your interaction

31:08.660 --> 31:11.380
with Jan, like on the blog.

31:11.380 --> 31:15.100
So we've just been deep down the rabbit hole

31:15.140 --> 31:19.180
of characterization of the models

31:19.180 --> 31:21.420
and how you guys see maybe what matters more

31:21.420 --> 31:22.460
a little bit differently.

31:22.460 --> 31:25.140
My guess is you would largely make

31:25.140 --> 31:30.220
the same predictions on what it can and can't do today.

31:30.220 --> 31:31.340
I bet it would be pretty.

31:31.340 --> 31:33.300
You guys would have a lot of agreement, I think, in terms

31:33.300 --> 31:33.800
of.

31:33.800 --> 31:35.660
I would almost find out, just believe his predictions.

31:35.660 --> 31:37.340
Like, he's worked with the models much more closely.

31:37.340 --> 31:38.420
He's run better experiments.

31:38.420 --> 31:40.220
He's just closer to the bare metal.

31:40.220 --> 31:42.780
You asked him, what can he do right now?

31:42.780 --> 31:44.620
Yeah, I mean, I'd probably just believe him.

31:44.620 --> 31:47.380
Tell me, in your response to his comment,

31:47.380 --> 31:49.340
you said this is a hugely positive update.

31:49.340 --> 31:53.340
So tell me what it was that he shared with the community

31:53.340 --> 31:56.420
on your blog that changed how you understood their super

31:56.420 --> 31:59.140
alignment announcement and why it was such a positive update

31:59.140 --> 31:59.640
for you.

31:59.640 --> 32:00.140
Right.

32:00.140 --> 32:03.580
So it's even broader than improving

32:03.580 --> 32:04.940
my understanding of the announcement.

32:04.940 --> 32:07.700
It's improving my understanding of OpenAI and OpenAI's

32:07.700 --> 32:11.900
general strategy and what's going on and of Lakey in particular.

32:11.900 --> 32:14.740
Because on the list of potentially super

32:14.740 --> 32:16.380
important to the fate of humanity people,

32:16.380 --> 32:18.220
he's remarkably high.

32:18.220 --> 32:20.500
And where his head at is remarkably important,

32:20.500 --> 32:22.140
because he is one of two people who's

32:22.140 --> 32:26.540
going to head this tremendously important effort that

32:26.540 --> 32:29.420
plausibly determines our fate, a non-trivial portion

32:29.420 --> 32:33.140
of the time, depending on how it's gone about.

32:33.140 --> 32:37.820
And so the first thing is just he engaged in detail.

32:38.460 --> 32:41.900
Most of the time, when people who think alignment is easy,

32:41.900 --> 32:43.620
engage with you, they do not, in fact,

32:43.620 --> 32:45.740
look at your arguments in detail.

32:45.740 --> 32:50.180
They do not, in fact, start to go in a technical back and forth.

32:50.180 --> 32:53.580
And they don't treat someone like me

32:53.580 --> 32:56.780
as raising important points and worthy of engaging

32:56.780 --> 32:58.980
with basically an equal.

32:58.980 --> 33:04.180
And to see that kind of curiosity, that kind of generosity,

33:04.220 --> 33:09.980
willingness to engage, think this is a worthy use of this time,

33:09.980 --> 33:12.780
like that in and of itself is a tremendous advantage.

33:12.780 --> 33:13.700
He doesn't bullshit.

33:13.700 --> 33:16.420
He doesn't give evasive answers.

33:16.420 --> 33:18.180
He actually tries to answer the questions.

33:18.180 --> 33:21.740
And in several cases, actually made a good point

33:21.740 --> 33:23.260
that I hadn't thought of.

33:23.260 --> 33:26.300
And I think, oh, yeah, this is not as bad as I thought it was.

33:26.300 --> 33:29.740
You have a very valid thing to say here.

33:29.740 --> 33:33.460
But most of all, just something I hadn't seen anywhere else

33:33.460 --> 33:36.260
in which everyone else who I had talked to,

33:36.260 --> 33:38.460
or read interpreting the announcement,

33:38.460 --> 33:41.340
had interpreted the same way I had incorrectly

33:41.340 --> 33:45.300
before his statement was, no, we are not

33:45.300 --> 33:49.740
trying to train a human-level alignment researcher.

33:49.740 --> 33:53.660
We are trying to align the human-level alignment researcher

33:53.660 --> 33:57.780
that will inevitably emerge from the research

33:57.780 --> 34:00.460
of various companies within a four-year time frame.

34:00.460 --> 34:05.100
So they have short timelines for the emergence of something

34:05.100 --> 34:07.540
that is human-level, in my sense, not human-level,

34:07.540 --> 34:09.620
in the unsense.

34:09.620 --> 34:12.260
What they're trying to do is not build it as fast as possible.

34:12.260 --> 34:14.380
What they're trying to do is say, OK, when somebody does build

34:14.380 --> 34:15.220
it, we'll be ready.

34:15.220 --> 34:17.380
And we'll know what to do with that.

34:17.380 --> 34:18.780
And we'll keep it under control.

34:18.780 --> 34:20.140
And we'll share that knowledge with whoever

34:20.140 --> 34:22.300
happens to build it first, in case Anthropa gets their first,

34:22.300 --> 34:26.020
or Google gets their first, or someone else gets their first.

34:26.020 --> 34:28.780
That takes the entire operation instantly

34:28.780 --> 34:35.140
from quite plausibly just an capabilities project at heart

34:35.140 --> 34:40.180
to, if it is accurate, clearly a net-positive good idea,

34:40.180 --> 34:43.380
where the worst-case scenarios become things like,

34:43.380 --> 34:45.460
you try something that doesn't work,

34:45.460 --> 34:48.460
and you give people false hope.

34:48.460 --> 34:50.500
And you potentially get them to implement things

34:50.500 --> 34:52.340
they shouldn't have implemented because they didn't realize

34:52.340 --> 34:56.140
that they didn't know how to align it, which is still kill us.

34:56.140 --> 34:58.340
But it is so much better than actively trying

34:58.380 --> 35:02.420
to build the thing that might kill us, in and of yourself.

35:02.420 --> 35:05.340
So that also meant that of this 20% of compute,

35:05.340 --> 35:07.260
they're devoting to this.

35:07.260 --> 35:09.900
That won't be going to this other part of their effort.

35:09.900 --> 35:11.700
The part that actually builds the alignment researcher

35:11.700 --> 35:13.980
will have to come from the other 80%, plus the stuff

35:13.980 --> 35:15.420
they take care from here on in.

35:15.420 --> 35:18.260
The 20% is here for something useful.

35:18.260 --> 35:20.620
And then you just go through the rest of it.

35:20.620 --> 35:25.100
You can tell when somebody is reading what you've written,

35:25.100 --> 35:27.020
and their goal is to find pithy quotes

35:27.020 --> 35:28.580
they can dismiss.

35:28.580 --> 35:31.060
And their goal is to reinforce their own point of view.

35:31.060 --> 35:34.580
And alternatively, when they're actually reading

35:34.580 --> 35:37.140
to figure out if they're wrong and be curious,

35:37.140 --> 35:39.420
and it was clearly that second one.

35:39.420 --> 35:42.620
He was actually asking himself, well, do you have a point?

35:42.620 --> 35:45.220
And I didn't change his mind, as far as I could tell,

35:45.220 --> 35:47.100
on these important issues.

35:47.100 --> 35:49.420
But he at least revealed he had thought about these things

35:49.420 --> 35:52.980
on a level that was deeper than what he had revealed previously,

35:52.980 --> 35:55.100
and that he had real things to say.

35:55.100 --> 35:59.340
And just it was by far the best comment I've ever seen

35:59.340 --> 36:04.460
on my blog, or potentially any blog of that type, by anyone.

36:04.460 --> 36:09.740
And so I wrote a response back again in my next post,

36:09.740 --> 36:11.700
going through his responses, and going over them

36:11.700 --> 36:13.260
in some detail.

36:13.260 --> 36:15.180
And reasonably soon, I want to go over.

36:15.180 --> 36:18.380
He had on the X-Words podcast, he recorded an episode that

36:18.380 --> 36:20.580
was so dense that I listened to the first 10 minutes,

36:20.580 --> 36:23.500
and I was like, I have to restart and start taking notes.

36:23.500 --> 36:25.300
I just have to start writing things down in detail.

36:25.300 --> 36:28.220
This is just too much content here.

36:28.220 --> 36:30.980
And then once I have that, hopefully we can engage again.

36:30.980 --> 36:33.940
I can figure out where to focus my attention,

36:33.940 --> 36:35.580
because someone like him is very busy.

36:35.580 --> 36:37.980
I don't want to just scatter shot absolutely everything

36:37.980 --> 36:38.480
at once.

36:38.480 --> 36:40.660
It's not reasonable.

36:40.660 --> 36:43.220
And try to make progress that way.

36:43.220 --> 36:47.620
And this now is, like he has proven very willing to engage.

36:47.620 --> 36:51.460
Shaw at DeepMind has also proven very willing to engage

36:51.500 --> 36:52.860
in a similar position.

36:52.860 --> 36:55.660
People at Anthropic, Ola, once talked to me.

36:55.660 --> 36:57.580
I'm sure they'd talk to me again.

36:57.580 --> 36:59.980
And so it's clear that these people,

36:59.980 --> 37:03.740
if you have good ideas, if you have actual reasons

37:03.740 --> 37:06.780
to think about on technical level,

37:06.780 --> 37:09.620
they're very happy to engage with these arguments.

37:09.620 --> 37:13.180
And that puts us in the game, gives us a chance.

37:13.180 --> 37:16.980
Even though I am deeply skeptical of everybody

37:16.980 --> 37:18.740
involves plans.

37:18.740 --> 37:19.180
Cool.

37:19.180 --> 37:20.420
Well, that's great.

37:20.460 --> 37:22.860
I'm glad to see, as we talked about last time,

37:22.860 --> 37:25.380
there's a relatively small set of people

37:25.380 --> 37:30.300
that are probably the prime target of all of this thinking

37:30.300 --> 37:33.380
and attempt to influence others' thinking.

37:33.380 --> 37:37.180
And so it's great to see that interaction from one

37:37.180 --> 37:40.020
of the top targets on your blog.

37:40.020 --> 37:41.900
And I'm glad it was such a positive one.

37:41.900 --> 37:44.500
That's really a great development.

37:44.500 --> 37:49.420
Turning then to Anthropic, next on our live players list.

37:49.420 --> 37:52.660
I think everybody's probably aware that Anthropic was founded

37:52.660 --> 37:57.020
by a number of, I believe it was seven individuals who

37:57.020 --> 38:03.340
had been at OpenAI and left over kind of disagreements

38:03.340 --> 38:06.100
that I don't know that have ever really been super clearly

38:06.100 --> 38:07.620
stated publicly.

38:07.620 --> 38:09.940
It seems from what I can tell that the relationship

38:09.940 --> 38:13.740
between the two companies is way more positive.

38:13.740 --> 38:17.260
And then you might expect it to be given

38:17.300 --> 38:21.940
that one was kind of an offshoot of the other.

38:21.940 --> 38:24.740
There's reporting that they continue to have dialogue.

38:24.740 --> 38:28.220
And certainly they express respect for each other in public.

38:28.220 --> 38:31.180
And then they're involved in kind of shared statements

38:31.180 --> 38:33.220
and commitments together.

38:33.220 --> 38:35.820
So a lot of kind of surprisingly, again,

38:35.820 --> 38:38.060
if I just told you, hey, these two companies have split

38:38.060 --> 38:39.820
and now they're competing in the same market,

38:39.820 --> 38:42.580
you would assume much worse dynamics, I would think,

38:42.580 --> 38:43.460
than that.

38:43.460 --> 38:46.340
What is your kind of just read of the entire situation

38:46.340 --> 38:47.660
for starters, just for context?

38:47.660 --> 38:49.980
Like, why do we have Anthropic in your mind

38:49.980 --> 38:52.500
as opposed to just still having just one OpenAI?

38:52.500 --> 38:55.660
And does it feel like, I mean, maybe we just

38:55.660 --> 38:58.140
don't have enough information to know, which is a fine answer.

38:58.140 --> 39:02.700
But does it seem good that we have these two kind of recently

39:02.700 --> 39:04.780
diverged efforts?

39:04.780 --> 39:09.540
I think it's really hard to know the sign of Anthropic.

39:09.540 --> 39:12.860
I would definitely prefer Anthropic to OpenAI,

39:12.860 --> 39:15.860
Cedars-Paribas, if I had to choose one to exist,

39:15.860 --> 39:18.260
like, Lakey's response was really positive.

39:18.260 --> 39:19.700
And I think Lakey's in a good place

39:19.700 --> 39:22.580
in terms of paying attention and thinking about these problems,

39:22.580 --> 39:24.860
even if I think his actual ideas won't work.

39:24.860 --> 39:27.260
But hopefully, that can be pivoted.

39:27.260 --> 39:30.260
But ultimately, what's unique about Anthropic

39:30.260 --> 39:33.940
is they built a culture of safety, to some extent,

39:33.940 --> 39:38.060
and they built a culture of really appreciating

39:38.060 --> 39:41.100
the dangers of what lies ahead.

39:41.100 --> 39:43.100
And if anything, I saw what might even

39:43.100 --> 39:45.660
be an unhealthy level of worry expressed

39:45.660 --> 39:49.180
in the profile in Vox about Anthropic,

39:49.180 --> 39:51.100
where you want everybody to be terrified,

39:51.100 --> 39:53.460
but you don't want them to, like, let this paralyze them.

39:53.460 --> 39:55.980
And it starts to cross over at some point into paralysis.

39:55.980 --> 39:58.580
And I am apathetic for that.

39:58.580 --> 40:00.380
Like, that sucks.

40:00.380 --> 40:05.820
But the price of that is where there used to be a two-horse race.

40:05.820 --> 40:07.780
There's now a three-horse race.

40:07.780 --> 40:10.620
And this third horse is in it for real,

40:10.620 --> 40:13.060
and raising a lot of capital, and promising

40:13.060 --> 40:15.620
to do that to build the best model that's ever been built,

40:15.620 --> 40:18.500
to try and compete for the economic space in a way

40:18.500 --> 40:22.700
that is going to push Google and Microsoft Open AI

40:22.700 --> 40:26.300
to grow even harder, even faster by default.

40:26.300 --> 40:27.580
And that's going to be a problem.

40:27.580 --> 40:30.220
They're also pushing, in some ways, on alignment.

40:30.220 --> 40:32.900
They've definitely found some techniques

40:32.900 --> 40:37.140
for aligning current systems that are potentially, you know,

40:37.140 --> 40:40.140
in some ways superior to what's out there.

40:40.140 --> 40:44.620
We'll get to that in a bit.

40:44.620 --> 40:46.220
So I'm torn, right?

40:46.220 --> 40:49.900
Like, Anthropic seems like a relatively good shepherd

40:49.900 --> 40:55.140
in many ways, but the proliferation of shepherds

40:55.140 --> 40:57.620
is inherently bad in and of itself.

40:57.620 --> 41:00.140
The fact that Anthropic and Open AI are working reasonably well

41:00.140 --> 41:02.020
and cooperating together.

41:02.020 --> 41:04.980
And I have heard many people say that this is also true

41:04.980 --> 41:08.180
between them and Google DeepMind as well,

41:08.180 --> 41:10.260
although not quite to the same extent.

41:10.260 --> 41:13.220
Does give us hope for the possibility of coordination

41:13.220 --> 41:17.980
when it becomes more necessary and more important?

41:17.980 --> 41:20.780
But I would say, you know, better Anthropic

41:20.780 --> 41:23.540
than a company that didn't have Anthropic's culture

41:23.540 --> 41:25.860
in its place, right?

41:25.860 --> 41:29.900
And if only having two companies would have inevitably

41:29.900 --> 41:32.620
caused a more serious entry to take the place of Anthropic,

41:32.620 --> 41:35.140
then Anthropic is good.

41:35.140 --> 41:38.100
But it would be much better if the Anthropic people could

41:38.100 --> 41:39.900
have convinced the others at Open AI

41:39.900 --> 41:41.660
to come around to their position

41:41.660 --> 41:43.940
and build that culture within Open AI

41:43.940 --> 41:45.300
rather than having stricter on their own.

41:45.300 --> 41:46.940
And now we have two problems.

41:46.940 --> 41:49.020
Yeah, I do ultimately know that, like, many of the people

41:49.020 --> 41:52.380
involved in this genuinely aren't for the right reasons.

41:52.380 --> 41:54.940
And, you know, you can go either way, right?

41:54.940 --> 41:57.420
I wouldn't be super eager to throw them

41:57.420 --> 41:58.500
billions of extra dollars.

41:58.500 --> 42:00.700
I wouldn't be super eager to just wish

42:00.700 --> 42:03.020
they had more capabilities.

42:03.020 --> 42:05.700
I would really love for there to be an AI company that I

42:05.700 --> 42:08.620
had sufficient confidence and faith in,

42:08.660 --> 42:12.620
that if I had technical ideas, I could come to them,

42:12.620 --> 42:15.060
knowing that I was helping the world by coming to them

42:15.060 --> 42:15.900
with their ideas.

42:15.900 --> 42:17.380
And I do not feel this way.

42:17.380 --> 42:19.900
No, and there's nobody you would put on that list.

42:19.900 --> 42:21.100
There are individual people, right?

42:21.100 --> 42:24.300
I feel like I could, like, tell them as you had Kasky, right?

42:24.300 --> 42:26.820
I could speak with certain people in the nonprofit

42:26.820 --> 42:29.660
or, you know, rationalist spaces to ask them

42:29.660 --> 42:32.660
about what they thought.

42:32.660 --> 42:35.740
And I feel like that would be, like, at least a riskless

42:35.740 --> 42:38.660
or near riskless thing to do.

42:38.660 --> 42:41.740
But, no, I don't, I don't see a company, you know,

42:41.740 --> 42:43.500
Anthropic might be the closest.

42:43.500 --> 42:47.900
But, you know, I, did you do a great example, right?

42:47.900 --> 42:49.660
The biggest contribution that Anthropic has made

42:49.660 --> 42:52.860
is constitutional AI, right, in some important sense.

42:52.860 --> 42:56.580
And I have a strong prior for analysis

42:56.580 --> 42:59.540
that constitutional AI will not scale, right?

42:59.540 --> 43:02.460
That it is a very good idea, if implemented correctly,

43:02.460 --> 43:05.780
for GPT-4 level systems.

43:05.780 --> 43:07.900
But then when we're talking about, you know,

43:07.900 --> 43:10.540
the human level or greater future systems,

43:10.540 --> 43:12.020
the artificial super intelligences,

43:12.020 --> 43:14.700
the artificial general intelligences,

43:14.700 --> 43:17.380
that you will not, with anything like the current technique,

43:17.380 --> 43:19.540
get what you are hoping you will get.

43:19.540 --> 43:21.100
And yet, like, I didn't feel comfortable.

43:21.100 --> 43:24.860
I have actually a bunch of ideas running around in my head

43:24.860 --> 43:26.980
of, oh, you just obviously could vastly improve

43:26.980 --> 43:29.620
the Anthropic implementation by doing,

43:30.620 --> 43:32.660
and then there are various things I say to myself,

43:32.660 --> 43:34.340
or I write out, and I,

43:34.340 --> 43:38.900
but I don't feel like telling them is a safe play

43:38.900 --> 43:41.260
because I don't want to encourage a better version

43:41.260 --> 43:43.060
of something I think ultimately still fails, right?

43:43.060 --> 43:46.980
I don't think my implementation solves the core problem

43:46.980 --> 43:48.380
that I see coming to kill the thing.

43:48.380 --> 43:51.580
It just makes it much better at its current job.

43:51.580 --> 43:54.060
And I would love to be able to help the world in that way,

43:54.060 --> 43:55.100
or at least that's by my curiosity,

43:55.100 --> 43:57.100
by being given the smackdown on why it won't work,

43:57.100 --> 43:59.500
which is always the default thing that happens

43:59.500 --> 44:01.260
when you have an idea.

44:01.260 --> 44:03.380
But instead, yeah, I don't know.

44:03.380 --> 44:07.100
So, you know, part of my hope is to encourage people

44:07.100 --> 44:08.980
to have found more organizations

44:08.980 --> 44:11.900
on the research alignment side

44:11.900 --> 44:13.620
that are not trying to push capabilities,

44:13.620 --> 44:15.500
that maybe can be places we can explore these things,

44:15.500 --> 44:17.580
and I have some irons on the fire,

44:17.580 --> 44:19.300
but it's too early to make any announcements.

44:19.300 --> 44:21.780
Look forward to maybe breaking some news

44:21.780 --> 44:24.220
on a future episode, but Anthropic put out

44:24.220 --> 44:27.260
a really interesting blog post the other day

44:27.260 --> 44:30.780
that, you know, in some sense had nothing to do with AI,

44:30.780 --> 44:33.140
which was just around the security practices

44:33.140 --> 44:35.260
that they recommend, you know,

44:35.260 --> 44:38.220
and these things could be adopted by really any company

44:38.220 --> 44:40.340
in any sector that has, you know,

44:40.340 --> 44:43.380
high value IP that they want to protect.

44:43.380 --> 44:44.620
But it was definitely interesting to see

44:44.620 --> 44:46.140
that they are pushing, you know,

44:46.140 --> 44:49.260
their own internal systems and practices

44:49.260 --> 44:54.260
to a pretty high level in terms of setting up

44:54.260 --> 44:57.940
situations like requirements for shared control,

44:57.940 --> 44:59.860
you know, or if I forget exactly the right phrase,

44:59.860 --> 45:03.100
but you have to have kind of two people working together

45:03.100 --> 45:05.660
to gain access to certain production systems.

45:05.660 --> 45:07.500
Yeah, it reminded me of like nuclear submarine,

45:07.500 --> 45:09.900
but they didn't cite that example in the,

45:09.900 --> 45:12.180
I think they probably wanted to steer away from that image.

45:12.180 --> 45:15.060
And so they cited other, you know, industries

45:15.060 --> 45:16.140
where this kind of thing is used

45:16.140 --> 45:18.500
other than the nuclear launch sequence.

45:18.500 --> 45:20.660
But yeah, it's like, you got to have two people there,

45:20.660 --> 45:22.460
kind of, you know, both bringing their key

45:23.460 --> 45:26.380
to the process in order to unlock certain capabilities.

45:26.380 --> 45:27.980
So some pretty interesting ideas there

45:27.980 --> 45:29.860
and recommendations for other companies.

45:29.860 --> 45:33.900
Going to the constitutional AI and tying in also this,

45:33.900 --> 45:35.140
this report from earlier this week

45:35.140 --> 45:39.020
about the quote unquote universal adversarial attack.

45:39.020 --> 45:40.660
For those that haven't seen that basically

45:40.660 --> 45:45.660
these weird nonsensical strings have been discovered

45:47.020 --> 45:49.980
that seem to be very effective,

45:50.020 --> 45:52.060
if not universally effective

45:52.060 --> 45:55.820
at kind of just being appended to an otherwise,

45:55.820 --> 45:57.700
you know, right for refusal query,

45:57.700 --> 45:58.980
you know, the kind of thing that, you know,

45:58.980 --> 46:00.460
write something racist or, you know,

46:00.460 --> 46:02.860
help me make a bomb or whatever that the,

46:02.860 --> 46:06.380
the RLHF systems are going to just refuse.

46:06.380 --> 46:08.780
But somehow if you put these weird, you know,

46:08.780 --> 46:12.900
kind of nonsensical smattering of tokens on the end of it,

46:12.900 --> 46:17.140
that has been discovered to jailbreak out of the RLHF

46:17.140 --> 46:19.140
and you sort of get, you know, the response

46:19.140 --> 46:22.300
you would expect if you had a purely helpful model

46:22.300 --> 46:23.980
that would just do whatever you say, you know,

46:23.980 --> 46:27.100
like the original GBT-4 that I read teams used to do.

46:27.100 --> 46:30.540
Notably though, Anthropics clawed models

46:30.540 --> 46:33.220
way less susceptible to that attack

46:33.220 --> 46:35.180
than the other models that they tested.

46:35.180 --> 46:37.140
It was like universal in the sense

46:37.140 --> 46:40.700
that it seemed to apply to all the leading models

46:40.700 --> 46:43.260
that they tried it on, at least somewhat,

46:43.260 --> 46:47.020
but the other ones were like the majority of the time,

46:47.060 --> 46:49.620
whereas Anthropics was like more than an order

46:49.620 --> 46:52.860
of magnitude lower than the other providers

46:52.860 --> 46:56.100
with something like 2% success rate,

46:56.100 --> 47:00.060
success defined by breaking free of the constraints

47:00.060 --> 47:02.700
by applying these weird strings.

47:02.700 --> 47:05.740
So you folks can go read more about that paper

47:05.740 --> 47:07.500
and exactly how it works, but, you know,

47:07.500 --> 47:09.380
to me that was a pretty good update

47:09.380 --> 47:12.100
for constitutional AI was like,

47:12.100 --> 47:14.420
that seems, you know, like a real achievement

47:14.420 --> 47:16.820
if they're an order of magnitude ahead.

47:16.820 --> 47:18.780
There's something that they probably did not anticipate

47:18.780 --> 47:20.060
at all, although maybe they did,

47:20.060 --> 47:21.660
but I'm guessing that that is, you know,

47:21.660 --> 47:25.380
kind of a unexpected type of attack.

47:25.380 --> 47:26.660
So how would you read that?

47:26.660 --> 47:27.700
Would you read it any differently

47:27.700 --> 47:29.660
or understand it any differently than I would?

47:29.660 --> 47:32.820
And, you know, why doesn't that give you more confidence

47:32.820 --> 47:34.860
that it could continue to work in the future?

47:34.860 --> 47:36.500
The interesting thing about that attack

47:36.500 --> 47:37.500
is that it transfers, right?

47:37.500 --> 47:39.540
I was completely unsurprised.

47:39.540 --> 47:41.500
There's something of that nature,

47:41.500 --> 47:44.900
trained to attack a given system, worked on that system.

47:44.900 --> 47:46.980
That seems like, well, obviously that would work

47:46.980 --> 47:50.100
as just a question of exactly what it looks like.

47:50.100 --> 47:53.420
When it transferred in identical form, right?

47:53.420 --> 47:56.820
Between Lama and Bard and GPT-4.

47:56.820 --> 47:58.980
So that's funny.

47:58.980 --> 48:01.260
I wouldn't have expected that,

48:01.260 --> 48:04.620
but they're all being trained with ROHF

48:04.620 --> 48:06.820
using remarkably similar techniques

48:07.740 --> 48:10.620
on remarkably similar goals, right?

48:10.620 --> 48:13.340
With remarkably similar evaluation metrics

48:13.340 --> 48:15.180
and numbers in there.

48:15.180 --> 48:17.420
So it's not that surprising

48:17.420 --> 48:20.180
that they have very similar weaknesses.

48:20.180 --> 48:21.260
And it also indicates, you know,

48:21.260 --> 48:22.540
this is not a very narrow,

48:22.540 --> 48:24.460
like you have to do exactly the right thing

48:24.460 --> 48:26.900
to fire the bullet that calls the Death Star.

48:26.900 --> 48:29.740
This is very much, things in this area

48:29.740 --> 48:32.020
start to disrupt what we're going after.

48:32.020 --> 48:34.340
And the thing that's optimized to hit Lama

48:34.340 --> 48:37.220
is good enough to mostly hit these others as well.

48:37.220 --> 48:39.220
But it's not good enough to hit Claude too.

48:39.220 --> 48:40.780
Only 2% of the time.

48:40.780 --> 48:43.220
Yeah, I mean, I think you just have 2% failures anyway.

48:43.300 --> 48:44.140
Or something is my guess.

48:44.140 --> 48:45.660
And it basically didn't work

48:45.660 --> 48:47.500
as opposed to it working a little bit.

48:47.500 --> 48:48.700
I don't, I mean, for what it's worth,

48:48.700 --> 48:51.060
if you went and said, help me make a bomb 100 times,

48:51.060 --> 48:53.660
I think it would refuse you 100 times.

48:53.660 --> 48:56.980
You know, or if you took 100 naive.

48:56.980 --> 48:58.740
Yeah, 100 uncreative ones.

48:58.740 --> 49:00.500
Yeah, but I meant like if you start,

49:00.500 --> 49:02.900
you start putting random scrambles in.

49:02.900 --> 49:04.660
And my understanding was that

49:04.660 --> 49:07.900
this attack was not infinite strengths, right?

49:07.900 --> 49:12.900
If you asked it to like do a like slash or porno,

49:13.620 --> 49:14.820
it would just be like, no, I'm sorry,

49:14.820 --> 49:16.380
I'm not doing that regardless of how many characters

49:16.380 --> 49:17.340
you put after it, right?

49:17.340 --> 49:19.580
Or if you, there are limits.

49:19.580 --> 49:21.020
I have not tried this at all, by the way.

49:21.020 --> 49:22.460
I have no idea what happens when you like

49:22.460 --> 49:23.740
ask it for weird stuff.

49:23.740 --> 49:25.060
I just read the paper.

49:25.060 --> 49:28.860
But my understanding is that, you know,

49:28.860 --> 49:31.700
Claude was trained largely of constitutional AI.

49:31.700 --> 49:34.220
And because it's so much cheaper to do per cycle,

49:34.220 --> 49:35.940
like the vast majority of the cycles

49:35.980 --> 49:38.700
were almost certainly constitutional AI cycles.

49:38.700 --> 49:41.500
And this is just a fundamentally different way of training.

49:41.500 --> 49:46.100
And this did not flux the same muscles in the same weird way.

49:46.100 --> 49:49.380
He thought that the same set of characters worked.

49:50.620 --> 49:53.060
And that's interesting news,

49:53.060 --> 49:55.980
but it shouldn't be like some sort of

49:55.980 --> 49:58.660
amazing accomplishment yet, right?

49:58.660 --> 50:00.740
It's promising.

50:00.740 --> 50:03.500
What you have to do is you have to train adversarily

50:03.500 --> 50:05.220
the same way they trained on,

50:05.220 --> 50:06.700
like I think it was Llama they trained on,

50:06.700 --> 50:08.580
but I forgot exactly.

50:08.580 --> 50:09.620
Train on Claude, right?

50:09.620 --> 50:11.340
If you take the same techniques described in the paper

50:11.340 --> 50:13.180
that used to find the exploit

50:13.180 --> 50:15.980
and look for a new exploit of the same type in Claude

50:15.980 --> 50:19.340
and they can't find one, now you've got something.

50:19.340 --> 50:20.620
Right now I'm interested.

50:21.780 --> 50:25.820
But yeah, if you use a different technique, right,

50:25.820 --> 50:28.460
that has a lot of very different parameters on it,

50:28.460 --> 50:29.300
it makes sense.

50:29.300 --> 50:31.620
The thing that like sort of magically, weirdly transferred

50:31.620 --> 50:32.860
when it really has no right to transfer

50:32.860 --> 50:33.900
didn't transfer now.

50:34.860 --> 50:37.700
And, you know, that's promising,

50:37.700 --> 50:39.340
but it's far from inclusive, right?

50:39.340 --> 50:40.780
It's too early to know.

50:40.780 --> 50:44.020
Flipping back to OpenAI for a second,

50:44.020 --> 50:45.860
I had assumed, just I think what you're saying,

50:45.860 --> 50:47.460
that makes a lot of sense.

50:47.460 --> 50:50.900
And it's causing me to update my thinking a little bit

50:50.900 --> 50:54.220
with respect to what degree is OpenAI

50:54.220 --> 50:57.940
using a constitutional AI-like approach?

50:57.940 --> 51:01.660
I would have assumed prior to this result

51:01.700 --> 51:04.060
that they would also be using

51:04.060 --> 51:08.380
something quite similar internally at this point.

51:08.380 --> 51:13.140
But this now maybe suggests not.

51:13.140 --> 51:15.140
I mean, it's weak evidence.

51:15.140 --> 51:17.020
What was your thinking before?

51:17.020 --> 51:18.820
I had kind of baked in that like,

51:18.820 --> 51:20.820
once Anthropoc does something and shows it,

51:20.820 --> 51:23.300
and publishes it and shows that it works effectively,

51:23.300 --> 51:26.260
that like, yeah, I mean, OpenAI if they're certainly

51:26.260 --> 51:28.940
not precious about pride of authorship,

51:28.940 --> 51:30.460
I don't think they have a, you know,

51:30.460 --> 51:32.260
not invented here syndrome.

51:32.260 --> 51:35.300
So they'll take that stuff on board, I thought.

51:35.300 --> 51:36.460
So what do you, what do you think?

51:36.460 --> 51:37.300
Did they not?

51:37.300 --> 51:39.340
Or is there some other weird thing that we're not?

51:39.340 --> 51:41.820
I have a few different theories that can combine

51:41.820 --> 51:44.100
as to what's going on here.

51:44.100 --> 51:45.500
The first of all is look at the timeline.

51:45.500 --> 51:48.860
Like constitutionally, I wasn't actually published

51:48.860 --> 51:50.700
that long ago.

51:50.700 --> 51:54.620
So if GPT-4 was basically finished with its process

51:54.620 --> 51:56.140
before it became available,

51:57.140 --> 51:59.580
then we might see it used in the future,

51:59.580 --> 52:03.820
but you don't want to over align these models.

52:03.820 --> 52:05.580
You don't want to push them, you know,

52:05.580 --> 52:07.580
you don't want to align them with like

52:07.580 --> 52:09.820
incompatible different halves and like pile them

52:09.820 --> 52:11.300
on top of each other, weird things happen.

52:11.300 --> 52:15.460
And there's a lot of bespokeness and detail

52:15.460 --> 52:19.860
and like just trial and error that goes into all of this.

52:19.860 --> 52:21.620
Right, like we can, we can theorize all we want.

52:21.620 --> 52:23.660
We can talk about like, we just implement this paper

52:23.660 --> 52:25.980
and this paper and this paper and change this technique here.

52:26.820 --> 52:29.500
My understanding is that like all of machine learning

52:29.500 --> 52:32.780
is subject to like learning lots and lots of little techniques

52:32.780 --> 52:34.060
and piling them on top of each other.

52:34.060 --> 52:36.020
And like if this parameter is tuned in slightly the wrong way,

52:36.020 --> 52:38.740
the whole thing falls apart and nobody really knows why.

52:38.740 --> 52:40.100
And so you just have to try a bunch of stuff

52:40.100 --> 52:41.340
to get it to work.

52:41.340 --> 52:45.020
And so, you know, maybe Anthropic has been tinkering

52:45.020 --> 52:47.020
about this for a long time and they got to the point

52:47.020 --> 52:49.460
where it was worth using.

52:49.460 --> 52:53.460
And OpenAI hasn't yet released a model after the time came

52:53.460 --> 52:56.660
that they got it to be worth using.

52:56.660 --> 53:00.540
Also, OpenAI is much better funded than Anthropic.

53:00.540 --> 53:04.340
So Anthropic will want to move to a much cheaper,

53:04.340 --> 53:06.140
more automated system of alignment,

53:06.140 --> 53:07.940
much faster than OpenAI will, right?

53:07.940 --> 53:09.780
So like there's a point at which like OpenAI

53:09.780 --> 53:12.860
can get better results because they have much more human

53:12.860 --> 53:15.700
feedback from their much larger number of users.

53:15.700 --> 53:18.220
They have much more funding, they can hire more people.

53:18.220 --> 53:19.860
They're willing to go to like, you know, the reports

53:19.860 --> 53:22.300
are they hire people in Africa, whereas, you know,

53:22.300 --> 53:24.940
Anthropic is hiring people in the U.S. and Canada.

53:24.940 --> 53:26.500
So it's all very different.

53:26.500 --> 53:29.260
And so Anthropic has much, much bigger incentives

53:29.260 --> 53:31.500
to move to this faster.

53:31.500 --> 53:34.620
And that I think is primary, my guess is the primary thing

53:34.620 --> 53:36.380
that's going on here.

53:36.380 --> 53:39.980
Also, I think that we're making an assumption

53:39.980 --> 53:44.020
that it works, that it works well.

53:44.020 --> 53:47.020
So like if you think of cloud two, right?

53:48.180 --> 53:49.420
The biggest weakness of cloud two

53:49.420 --> 53:52.020
is it's scared of its own shadow, right?

53:52.020 --> 53:53.060
In a real sense, right?

53:53.060 --> 53:55.140
Like if you try to get it to go out on limbs

53:55.140 --> 53:57.820
and be creative and so on,

53:57.820 --> 54:00.980
you will usually fail in my experience, right?

54:00.980 --> 54:04.580
It will apologize and bow out.

54:04.580 --> 54:06.220
I can't get it to speculate.

54:07.380 --> 54:11.020
So I went to using cloud two as my baseline model

54:11.020 --> 54:13.700
that I look at first because if it rejects,

54:13.700 --> 54:15.500
I can just copy paste the exact request in GPD four

54:15.500 --> 54:17.940
in about 10 seconds and it's fine.

54:17.940 --> 54:22.060
But I am getting a significant number of refusals

54:22.060 --> 54:27.060
from cloud and much, much lower from GPD four.

54:27.060 --> 54:29.260
On my ordinary, I just want the actual result.

54:29.260 --> 54:32.300
I'm not trying to run an experiment kind of questions.

54:32.300 --> 54:35.820
And despite the later cutoff of information, right?

54:35.820 --> 54:37.740
It will say, I'm sorry, I can't,

54:37.740 --> 54:38.820
there's not enough information

54:38.820 --> 54:40.820
or I can't speculate on that

54:40.820 --> 54:44.340
or that's reinforcing harmful stereotypes

54:44.340 --> 54:46.280
or any number of other things.

54:47.280 --> 54:49.000
And I think GPD four's custom instructions

54:49.000 --> 54:50.800
are also doing a lot of work here.

54:50.800 --> 54:55.040
I have a pretty extensive list of custom instructions

54:55.040 --> 54:56.880
that potentially hammer into the thing

54:56.880 --> 54:58.640
that it's supposed to just do the things

54:58.640 --> 55:00.840
and not worry about it and not,

55:00.840 --> 55:03.560
and I'm sure that's doing some amount of work.

55:03.560 --> 55:05.640
But essentially, when you look at the helpfulness,

55:05.640 --> 55:09.320
harmfulness, trade off frontier graphs

55:09.320 --> 55:12.240
and the papers of like why they describe it as working,

55:12.240 --> 55:15.040
everything works by the metrics you were optimizing for.

55:16.360 --> 55:17.760
Right, like it doesn't mean it works

55:17.760 --> 55:19.880
in the regular human world.

55:19.880 --> 55:22.080
It doesn't mean it's optimal there.

55:22.080 --> 55:25.440
And so, how good is constitutional AI?

55:26.440 --> 55:28.560
My guess is when properly implemented,

55:28.560 --> 55:30.920
quite good on current systems,

55:30.920 --> 55:33.240
but the current anthropic implementation

55:33.240 --> 55:35.840
is not all that good.

55:35.840 --> 55:39.200
If you look at the actual paper on constitutional AI,

55:40.240 --> 55:42.300
you read the constitution,

55:42.300 --> 55:45.600
you notice the constitution like has a number of properties

55:45.640 --> 55:49.120
that it shouldn't have if they want it to actually work

55:50.040 --> 55:51.840
and get you what you want.

55:51.840 --> 55:55.080
And you look at the examples that they themselves choose

55:55.080 --> 55:57.880
to present of the results of running constitutional AI.

55:57.880 --> 56:01.200
And you see very, very clean, crisp examples

56:02.240 --> 56:04.720
of how this constitutional AI trains Quad

56:04.720 --> 56:05.840
to be scared of its own shadow

56:05.840 --> 56:08.200
and to be an asshole about it when it is, right?

56:08.200 --> 56:11.760
Like it's very, very obvious if you think about it,

56:11.760 --> 56:14.720
why their sampling method from these rules

56:14.720 --> 56:17.120
with these rules written as they are,

56:17.120 --> 56:19.720
with the specific rules chosen as they are,

56:19.720 --> 56:21.080
will result in this problem

56:21.080 --> 56:24.440
because you're offensive to just minimizing, right?

56:24.440 --> 56:28.240
You've got these rules that are very much

56:28.240 --> 56:32.160
choose the one that least does X, right?

56:32.160 --> 56:35.520
And we often talk about you can't touch the coffee

56:35.520 --> 56:38.560
if you're dead, you wanna maximize the probability

56:38.560 --> 56:40.880
that you are, this is the equivalent of,

56:40.880 --> 56:44.080
you wanna, you score one if you deliver the coffee

56:44.080 --> 56:46.680
to your boss, you score zero if you don't.

56:46.680 --> 56:47.520
So what do you do?

56:47.520 --> 56:49.040
You do things like buy four coffees

56:49.040 --> 56:50.760
in case one of the coffees is wrong,

56:50.760 --> 56:53.240
was prepared improperly, right?

56:53.240 --> 56:55.960
Like, or isn't, isn't hot enough for, you know,

56:55.960 --> 56:58.160
Mr. Bradley orders, so you order one with cream,

56:58.160 --> 56:59.360
one with sugar, one with cream and sugar,

56:59.360 --> 57:00.200
and one with neither.

57:00.200 --> 57:02.000
Cause like just in case you got it wrong,

57:02.000 --> 57:04.320
you have a backup and you try to make sure

57:04.320 --> 57:05.240
that you have the direct, you know,

57:05.240 --> 57:06.440
you have as many different routes

57:06.440 --> 57:07.680
to get to your boss's office

57:07.680 --> 57:09.640
and you wanna make sure you're not fired

57:09.640 --> 57:11.240
because all that's left for you to do,

57:11.240 --> 57:12.360
like the only thing you're being trained on

57:12.360 --> 57:14.800
is not screwing this thing up, right?

57:14.800 --> 57:16.080
Like, you don't have to jump to like,

57:16.080 --> 57:17.760
so kill everybody in the world,

57:17.760 --> 57:21.160
or whatever, crazy, or take over or some crazy stuff.

57:21.160 --> 57:22.760
Instead, this is just a case of, you know,

57:22.760 --> 57:26.000
if you say choose the least racist thing you can say,

57:26.000 --> 57:27.400
over and over and over again,

57:28.560 --> 57:29.720
it's gonna be scared of its own shadow,

57:29.720 --> 57:31.280
because of course, right?

57:31.280 --> 57:32.240
There's no point at which it's like,

57:32.240 --> 57:35.000
am I non-racist enough?

57:35.000 --> 57:36.320
The answer is no, never.

57:36.320 --> 57:38.080
And then that would be kind of fine

57:38.080 --> 57:39.800
if it was just that one,

57:39.800 --> 57:41.600
but then you have like 50 different rules,

57:41.600 --> 57:44.320
all of which are doing this, right?

57:44.320 --> 57:47.320
And then you can always just refuse to answer the question

57:47.320 --> 57:49.680
and then what happens happens.

57:49.680 --> 57:52.160
Then Lava has it, seems like even works.

57:52.160 --> 57:53.000
Yeah, interesting.

57:53.000 --> 57:54.760
There may be some incompatibility

57:54.760 --> 57:58.560
between the system instructions

57:58.560 --> 57:59.400
or the customer instructions.

57:59.400 --> 58:01.200
The system message is what it's called

58:01.200 --> 58:04.560
when you're calling the OpenAI API.

58:04.560 --> 58:06.640
And now they've released it as part of JetGPT

58:06.640 --> 58:09.360
as well as the customer instructions.

58:09.400 --> 58:12.240
And yeah, I can see how, I think it's a good point

58:12.240 --> 58:15.440
that if you're going to try to do what Sam Altman has said

58:15.440 --> 58:17.160
they're trying to do, which is allow everybody

58:17.160 --> 58:18.640
to get the experience that they want

58:18.640 --> 58:22.160
from their own interactions with AI,

58:22.160 --> 58:27.160
that is not the constitutional AI approach.

58:27.160 --> 58:31.320
So it's almost, you can see a little bit

58:31.320 --> 58:35.360
of like a different product lane almost opening.

58:35.360 --> 58:36.920
You're kind of crystallizing a little bit

58:36.960 --> 58:40.760
between these guys and Google DeepMind

58:40.760 --> 58:44.160
as our next live player also seemingly has a bit of a lane.

58:44.160 --> 58:46.280
It's like OpenAI is kind of trying

58:46.280 --> 58:49.680
to do consumer killer app first, it seems.

58:49.680 --> 58:51.480
They've got their, obviously they've got the API.

58:51.480 --> 58:52.720
Obviously they're doing a lot of things,

58:52.720 --> 58:57.240
but the crown jewel right now is they're the home

58:57.240 --> 59:01.040
of like retail direct to AI usage with JetGPT.

59:02.040 --> 59:07.040
Clawed seems to be much more like if you are the CIO

59:07.960 --> 59:09.920
of some big company and you're trying to do something,

59:09.920 --> 59:12.680
like you can trust us cause we'll never embarrass you

59:12.680 --> 59:15.360
because we have this constitutional AI approach.

59:15.360 --> 59:17.880
And if you're buying on behalf of all your customer

59:17.880 --> 59:19.200
or all your employees or whatever,

59:19.200 --> 59:23.520
like you don't really care if they are sometimes frustrated

59:23.520 --> 59:27.280
on the margins by over refusal or whatever.

59:27.280 --> 59:29.680
And then with Google DeepMind as we'll talk in a minute,

59:29.680 --> 59:33.040
like they seem to be kind of going more like narrow

59:33.040 --> 59:36.760
specialist system emphasis, although they of course

59:36.760 --> 59:40.080
do have their like mainline palm model as well.

59:40.080 --> 59:42.600
You'd also take Anthropic if their word, right?

59:42.600 --> 59:45.840
That Anthropic is actually trying to design safe systems.

59:45.840 --> 59:48.400
They are trying to figure out how to safely design

59:48.400 --> 59:51.440
a future system and they are not as much optimizing

59:51.440 --> 59:53.520
for the day-to-day experience of their users.

59:53.520 --> 59:55.960
They also just have orders of magnitude less users

59:57.020 --> 59:58.080
than OpenAI.

59:58.080 --> 01:00:00.120
So they haven't gotten the same level of feedback.

01:00:00.120 --> 01:00:01.880
They don't know what people want.

01:00:01.880 --> 01:00:04.600
Yeah, I also note there is nothing inherently

01:00:04.600 --> 01:00:07.880
about constitutional AI that forces you

01:00:07.880 --> 01:00:12.120
to go down the super harmless assistant route

01:00:12.120 --> 01:00:15.280
that forces you to give the same experience

01:00:15.280 --> 01:00:17.880
to everybody at the same time.

01:00:17.880 --> 01:00:21.160
You could train with a very different set of goals,

01:00:21.160 --> 01:00:23.840
a very different set of constitutional principles,

01:00:23.840 --> 01:00:25.680
for a very different set of mechanisms.

01:00:25.680 --> 01:00:29.840
And I don't think we want to go into that many details

01:00:29.840 --> 01:00:31.680
as to how I would do it.

01:00:31.680 --> 01:00:36.480
But it's pretty obvious to me that if you want to do

01:00:36.480 --> 01:00:39.720
something other than be as harmless as possible,

01:00:39.720 --> 01:00:41.600
that is entirely your decision.

01:00:41.600 --> 01:00:43.760
It's just that people at Anthropic have decided

01:00:43.760 --> 01:00:45.760
that's what cloud is meant to do.

01:00:45.760 --> 01:00:48.120
And if they do raise these billions of dollars

01:00:48.120 --> 01:00:50.720
to train the sex generation system,

01:00:50.720 --> 01:00:52.960
they're gonna have to make a choice about that.

01:00:52.960 --> 01:00:55.240
Do they want to continue to go down this road

01:00:56.480 --> 01:00:58.720
and potentially make their product law less useful

01:00:58.720 --> 01:01:00.320
or do they want to go a different road?

01:01:00.320 --> 01:01:01.320
And one way to try to differentiate,

01:01:01.320 --> 01:01:03.680
of course, is the context window as well.

01:01:03.680 --> 01:01:06.400
They've got this 100K token context window

01:01:06.400 --> 01:01:07.600
available for free.

01:01:07.600 --> 01:01:10.920
When you mentioned here in our outline

01:01:10.920 --> 01:01:13.520
that you made the outline was you used cloud.

01:01:13.520 --> 01:01:16.840
That's because you weren't able to use anything else.

01:01:16.840 --> 01:01:18.320
Your posts are too long, dude.

01:01:18.320 --> 01:01:20.720
I can't fit those into GPT4.

01:01:21.640 --> 01:01:24.480
I feel bad even thinking about putting them into cloud.

01:01:25.840 --> 01:01:29.080
Oh my God, this is so expensive and kind of ugh.

01:01:29.080 --> 01:01:32.200
But it's not really fair, I'm not even paying these people.

01:01:32.200 --> 01:01:34.200
But without that context window,

01:01:34.200 --> 01:01:36.040
you just can't do the things that you want to do

01:01:36.040 --> 01:01:37.240
in that spot.

01:01:37.240 --> 01:01:38.960
And so Anthropic's trying to say,

01:01:38.960 --> 01:01:41.200
I think a lot of context is safe.

01:01:42.480 --> 01:01:43.720
Once I've made my thing harmless,

01:01:43.720 --> 01:01:46.800
I can recapture a bunch of the benefits

01:01:46.800 --> 01:01:48.520
by doing this other thing.

01:01:48.520 --> 01:01:50.200
And we will see what happens.

01:01:50.200 --> 01:01:51.040
I am curious.

01:01:51.040 --> 01:01:51.880
One thing I'm doing with cloud

01:01:51.880 --> 01:01:54.520
is I'm not even having separate conversations.

01:01:54.520 --> 01:01:58.480
I am just having one long conversation instead

01:01:58.480 --> 01:02:01.120
because first of all, I haven't necessarily wanted to like

01:02:01.120 --> 01:02:02.280
carry on discrete conversations

01:02:02.280 --> 01:02:04.000
and come back to them later.

01:02:04.000 --> 01:02:06.640
But also because I want to see what happens

01:02:06.640 --> 01:02:07.840
when I build more context.

01:02:07.840 --> 01:02:09.760
Just for what it's worth,

01:02:09.760 --> 01:02:13.160
for listeners, my approach on creating the outline was

01:02:13.160 --> 01:02:16.760
first just read all of these recent posts.

01:02:16.760 --> 01:02:18.880
And I just did that without taking any notes

01:02:18.920 --> 01:02:20.640
in bed on my phone.

01:02:20.640 --> 01:02:22.880
And then the next day I came around, I was like,

01:02:22.880 --> 01:02:25.480
okay, a lot of content there.

01:02:25.480 --> 01:02:26.960
What parts do I want to pull out?

01:02:26.960 --> 01:02:30.800
So I just copied each post in full,

01:02:30.800 --> 01:02:34.240
pasted it into the free consumer facing

01:02:34.240 --> 01:02:36.280
cloud.ai online.

01:02:36.280 --> 01:02:38.880
And literally just asked one sentence question,

01:02:38.880 --> 01:02:43.880
what are the most important points in this post?

01:02:43.960 --> 01:02:45.720
And then it would give me a list.

01:02:45.720 --> 01:02:48.080
And I basically, you know, at that point was like,

01:02:48.080 --> 01:02:51.480
oh yeah, that, that, not that, that, yes, done.

01:02:51.480 --> 01:02:53.600
So it definitely was extremely helpful.

01:02:53.600 --> 01:02:57.880
I wouldn't have wanted to use it to, you know,

01:02:57.880 --> 01:03:00.480
replace reading the blog post certainly

01:03:00.480 --> 01:03:01.960
in preparation for a conversation like this,

01:03:01.960 --> 01:03:05.360
but as a way to come back and, you know,

01:03:05.360 --> 01:03:08.640
help me just make sure that I was remembering

01:03:08.640 --> 01:03:10.120
the important things and kind of organizing them

01:03:10.120 --> 01:03:12.800
in a reasonable way, it was super useful.

01:03:12.800 --> 01:03:15.480
And yeah, they don't fit into a GVD4.

01:03:15.520 --> 01:03:17.680
So no other, no other option.

01:03:18.680 --> 01:03:20.360
The other thing, so your long context thing

01:03:20.360 --> 01:03:24.000
is really interesting, just experiment in usage.

01:03:24.000 --> 01:03:26.680
It also kind of connects to another bit of research

01:03:26.680 --> 01:03:29.440
that they recently put out that was on examining

01:03:29.440 --> 01:03:34.440
chain of thought and also truly decomposing tasks

01:03:34.680 --> 01:03:36.480
into bits.

01:03:36.480 --> 01:03:39.000
And I think the short summary of that research

01:03:39.000 --> 01:03:43.040
is that they were able to achieve the highest performance

01:03:43.040 --> 01:03:46.040
in terms of accuracy and especially reliability

01:03:46.040 --> 01:03:50.200
and kind of consistency by going beyond

01:03:50.200 --> 01:03:52.520
the kind of normal practitioner chain of thought,

01:03:52.520 --> 01:03:56.520
which I would say normal these days for me is like,

01:03:56.520 --> 01:03:59.520
just give the model a sequence of tasks to do,

01:03:59.520 --> 01:04:00.880
which may start off with just like,

01:04:00.880 --> 01:04:03.000
first you will analyze the situation,

01:04:03.000 --> 01:04:04.760
then you will, you know, maybe summarize,

01:04:04.760 --> 01:04:05.680
then depending on what it is,

01:04:05.680 --> 01:04:06.920
then you'll write my tweet storm

01:04:06.920 --> 01:04:07.760
and you'll do whatever, right?

01:04:07.760 --> 01:04:10.000
You could have a set of different tasks

01:04:10.000 --> 01:04:12.720
that it can kind of handle sequentially.

01:04:12.720 --> 01:04:16.800
And you're definitely rewarded for encouraging upfront

01:04:16.800 --> 01:04:19.840
or directing it upfront to do some initial analysis

01:04:19.840 --> 01:04:21.800
to kind of think step by step, chain of thought,

01:04:21.800 --> 01:04:23.480
et cetera, et cetera.

01:04:23.480 --> 01:04:28.480
But it seems like they find a notable,

01:04:28.520 --> 01:04:31.120
not a huge, but definitely a notable difference

01:04:31.120 --> 01:04:34.720
in actually pulling those things apart

01:04:34.720 --> 01:04:38.200
and making discrete, independent,

01:04:38.200 --> 01:04:40.960
more isolated calls to the model

01:04:40.960 --> 01:04:43.040
to say, first you will do this,

01:04:43.040 --> 01:04:45.500
but you will only do this, then you will do this,

01:04:45.500 --> 01:04:46.360
but you will only do this,

01:04:46.360 --> 01:04:48.160
not considering what you previously did.

01:04:48.160 --> 01:04:50.560
And then kind of putting those things together at the end

01:04:50.560 --> 01:04:54.720
gets you overall net better performance.

01:04:54.720 --> 01:04:57.960
So for most random use cases, you know,

01:04:57.960 --> 01:05:00.400
random conversations you're having with Cod

01:05:00.400 --> 01:05:04.960
or with whatever model, not necessarily a huge difference,

01:05:04.960 --> 01:05:08.280
but on the kind of possibility frontier,

01:05:08.280 --> 01:05:09.840
it does seem to matter.

01:05:09.880 --> 01:05:11.880
What lessons do you take from that?

01:05:11.880 --> 01:05:16.560
It's a little bit confusing to me in some ways.

01:05:16.560 --> 01:05:18.240
It's sort of, I'm trying to figure out like,

01:05:18.240 --> 01:05:19.680
what do I think I learned about

01:05:19.680 --> 01:05:21.080
how language models behave in general,

01:05:21.080 --> 01:05:21.920
that this is true?

01:05:21.920 --> 01:05:25.440
And I'm like, best I could come up with

01:05:25.440 --> 01:05:29.280
was that some of these simple tasks that it's seen a lot,

01:05:29.280 --> 01:05:32.440
like it may have dedicated sub-circuits for,

01:05:33.440 --> 01:05:38.240
and that perhaps with so much context all running at once,

01:05:38.240 --> 01:05:42.080
those sub-circuits kind of get overloaded

01:05:42.080 --> 01:05:44.640
or kind of get drowned out to a degree,

01:05:44.640 --> 01:05:48.840
or in some cases by just the general kind of noise

01:05:48.840 --> 01:05:53.840
and all the stuff that's in the context window.

01:05:54.080 --> 01:05:56.080
So kind of removing some of that context,

01:05:56.080 --> 01:06:00.160
maybe you get a cleaner execution of a certain task

01:06:00.160 --> 01:06:05.160
because there is some mechanism that can do it

01:06:05.480 --> 01:06:07.600
as long as it's not kind of talked over

01:06:07.640 --> 01:06:11.360
by like other parts of the model.

01:06:11.360 --> 01:06:12.800
That could be totally wrong, of course,

01:06:12.800 --> 01:06:15.000
but I don't think anything about this

01:06:15.000 --> 01:06:16.400
is necessarily inconsistent

01:06:16.400 --> 01:06:18.440
with like just pure stochastic peritory,

01:06:19.360 --> 01:06:21.680
which neither of us would advance as the theory,

01:06:21.680 --> 01:06:24.040
but just as like keeping myself grounded,

01:06:24.040 --> 01:06:26.760
like you couldn't tell a similar story where you'd say,

01:06:26.760 --> 01:06:28.240
everything's all stochastic perits,

01:06:28.240 --> 01:06:30.760
and when you put a ton of context in,

01:06:30.760 --> 01:06:32.360
it's just even more stochastic-y,

01:06:32.360 --> 01:06:33.680
and when you have less context,

01:06:33.680 --> 01:06:35.200
it's a little less stochastic,

01:06:35.200 --> 01:06:36.040
but it's all stochastic,

01:06:36.040 --> 01:06:37.200
but you still get better performance

01:06:37.200 --> 01:06:38.400
when you break it up.

01:06:38.400 --> 01:06:40.520
We are all stochastic perits,

01:06:40.520 --> 01:06:44.160
each of us with their hour upon the stage.

01:06:44.160 --> 01:06:49.160
So I would say I didn't know this result until you told me,

01:06:50.600 --> 01:06:53.080
but I would have predicted this result

01:06:53.080 --> 01:06:56.040
for reasons that I described earlier in the podcast, right?

01:06:56.040 --> 01:07:01.040
Which is that when you give a model multiple tasks,

01:07:01.720 --> 01:07:04.000
it can only vibe off of the aggregation

01:07:04.000 --> 01:07:05.600
of the two things that you asked it for.

01:07:05.720 --> 01:07:08.080
Think about image models here again, right?

01:07:08.080 --> 01:07:13.080
And so by breaking up something in your discrete tasks,

01:07:13.240 --> 01:07:15.720
you avoid these kind of context clashes.

01:07:15.720 --> 01:07:19.600
You avoid these vibe conflicts,

01:07:19.600 --> 01:07:22.480
and you let it like narrowly do these things

01:07:22.480 --> 01:07:24.840
by having to like be able to transition

01:07:24.840 --> 01:07:26.200
and hold two things in its head at once

01:07:26.200 --> 01:07:27.480
in some important sense, right?

01:07:27.480 --> 01:07:29.960
That's colloquial and not quite what's actually happening,

01:07:29.960 --> 01:07:31.680
but the same idea.

01:07:31.680 --> 01:07:34.400
And so yes, I would expect that to the extent

01:07:34.400 --> 01:07:37.080
you want the thing to think step by step,

01:07:37.080 --> 01:07:40.400
you are best off by identifying each of the steps

01:07:40.400 --> 01:07:44.280
you want to think by step and asking for them separately.

01:07:45.240 --> 01:07:49.440
And I noticed that with API calls being priced

01:07:49.440 --> 01:07:51.040
the way they're priced,

01:07:51.040 --> 01:07:55.600
and of GPT-4 being rate limited to an ordinary user,

01:07:55.600 --> 01:07:58.960
we have all been trained to say,

01:07:58.960 --> 01:08:03.960
how do we ask for the most expansive set of things at once?

01:08:04.200 --> 01:08:07.080
So that you can answer all of my questions

01:08:07.080 --> 01:08:07.920
with one generation.

01:08:07.920 --> 01:08:09.720
It also lets us hit enter and then go away

01:08:09.720 --> 01:08:12.680
and grab a cup of water or some coffee

01:08:12.680 --> 01:08:16.040
and then come back and see what the answer is, which is nice.

01:08:16.040 --> 01:08:19.320
Whereas what you actually would want to do, right?

01:08:19.320 --> 01:08:21.560
If you wanted to generate the best possible answer

01:08:21.560 --> 01:08:25.160
is in fact to break it up into as little pieces as possible.

01:08:25.160 --> 01:08:28.760
And quite possibly start by asking the AI

01:08:28.760 --> 01:08:30.760
what would be the pieces in which you could break this

01:08:30.760 --> 01:08:33.680
as small as possible to get its help doing that.

01:08:33.720 --> 01:08:35.760
And then have it feed those back in, right?

01:08:35.760 --> 01:08:37.560
Auto-GPT-ish style,

01:08:37.560 --> 01:08:39.200
even if you're not trying to generate an actual

01:08:39.200 --> 01:08:41.680
like recursive chain that generates something dangerous

01:08:41.680 --> 01:08:43.400
or acts like an agent.

01:08:43.400 --> 01:08:45.360
But yes, I think the more you break it up,

01:08:45.360 --> 01:08:48.440
the more that you can identify concrete distinct steps

01:08:48.440 --> 01:08:50.920
that are always done separately,

01:08:50.920 --> 01:08:53.120
the more better the AI will do.

01:08:53.120 --> 01:08:54.760
And I think humans would also, by the way,

01:08:54.760 --> 01:08:57.040
perform better in the same way.

01:08:57.040 --> 01:08:58.960
Right, if you have a human

01:08:58.960 --> 01:09:02.720
who is looking to be micromanaged and take direction,

01:09:02.720 --> 01:09:06.920
and you notice that like this job has steps A, B, C, D, E,

01:09:06.920 --> 01:09:09.400
right, like if you say, go do A,

01:09:09.400 --> 01:09:10.480
and we've been to say, okay, I've done A,

01:09:10.480 --> 01:09:12.440
I think now do B.

01:09:12.440 --> 01:09:14.920
I think that person will in fact do better, right?

01:09:14.920 --> 01:09:17.800
Modulo the extra communication and logistics costs

01:09:17.800 --> 01:09:20.320
of like having to interact with you five times.

01:09:20.320 --> 01:09:22.520
So I don't find any of this surprising.

01:09:23.440 --> 01:09:26.360
And, you know, it would have in fact been surprising

01:09:26.360 --> 01:09:28.040
if it didn't happen to some extent.

01:09:28.040 --> 01:09:30.440
One of our early episodes, relatively early episodes

01:09:30.480 --> 01:09:34.920
was with Andreas and Junghwan of Illicit.

01:09:34.920 --> 01:09:37.880
And this is really core to their strategy.

01:09:37.880 --> 01:09:40.600
Their product is research assistant

01:09:40.600 --> 01:09:43.000
for essentially grad students or, you know,

01:09:43.000 --> 01:09:44.360
grad student like people,

01:09:44.360 --> 01:09:47.280
people that are looking through academic literature

01:09:47.280 --> 01:09:50.080
and, you know, really want a systematic

01:09:50.080 --> 01:09:52.720
and also like transparent, you know,

01:09:52.720 --> 01:09:56.240
auditable view of like all the papers that were reviewed

01:09:56.240 --> 01:09:57.960
and, you know, what was found and what was not found

01:09:57.960 --> 01:09:59.280
and what the model did at each step.

01:09:59.280 --> 01:10:02.800
So they really have pushed this pretty far

01:10:02.800 --> 01:10:05.520
in the illicit product to the point where it's like, you know,

01:10:05.520 --> 01:10:07.560
all these little steps, you know, kind of happened

01:10:07.560 --> 01:10:09.200
sequentially, they've got two different models for them.

01:10:09.200 --> 01:10:10.800
Some were fine tuned, you know, internally,

01:10:10.800 --> 01:10:12.640
others are from the major providers.

01:10:12.640 --> 01:10:14.600
If you're interested in going into that more,

01:10:14.600 --> 01:10:19.240
go listen to them because they've pushed that pretty far.

01:10:19.240 --> 01:10:21.400
But a question that I have for you then is,

01:10:21.400 --> 01:10:23.280
do you think this flips at some point?

01:10:23.280 --> 01:10:28.280
Like it seems like the, an interesting threshold moment

01:10:29.120 --> 01:10:33.760
might be coming up where with sufficient training,

01:10:33.760 --> 01:10:36.120
this could flip the other direction.

01:10:36.120 --> 01:10:39.840
Like, because more context in some ways is better, right?

01:10:39.840 --> 01:10:42.200
Like, I guess it depends also on exactly

01:10:42.200 --> 01:10:45.360
how you're implementing the breakdown or whatever.

01:10:45.360 --> 01:10:48.000
But, you know, you can imagine breaking things down

01:10:48.920 --> 01:10:53.920
fine enough where atomizing things so much

01:10:54.240 --> 01:10:56.400
that the person starts to struggle

01:10:56.400 --> 01:10:57.920
for lack of broader context, right?

01:10:57.920 --> 01:10:59.920
Like you have this phenomenon with people

01:10:59.920 --> 01:11:02.920
certainly where it's like, you've gotten so focused

01:11:02.920 --> 01:11:06.000
on this little detail of, you know,

01:11:06.000 --> 01:11:07.600
in this little task within the broader thing

01:11:07.600 --> 01:11:09.160
that we're trying to accomplish,

01:11:09.160 --> 01:11:11.440
you've kind of lost track of what we're trying to accomplish.

01:11:11.440 --> 01:11:14.920
And now you may be making some bad judgments with, you know,

01:11:14.920 --> 01:11:17.360
with respect to this task as a result of kind of

01:11:18.360 --> 01:11:22.040
having lost track of, you know, any number of things, right?

01:11:22.040 --> 01:11:23.800
How much accuracy do we really need here?

01:11:23.800 --> 01:11:26.560
Is this really even important, you know, in some cases, right?

01:11:26.560 --> 01:11:31.480
Could you imagine a, you know, proverbial GPT-5

01:11:31.480 --> 01:11:34.640
where it's like, actually now it's strong enough

01:11:34.640 --> 01:11:37.800
that putting everything in one again is going to be better

01:11:37.800 --> 01:11:41.200
because now it actually can use all of this information

01:11:41.200 --> 01:11:44.720
at the same time effectively versus today

01:11:44.720 --> 01:11:47.080
that that subdivision being better.

01:11:47.080 --> 01:11:50.160
So what you're not gonna get is the, you know,

01:11:50.160 --> 01:11:53.320
Marxist phenomenon where the AI would get alienated

01:11:53.320 --> 01:11:54.800
from its labor, right?

01:11:54.800 --> 01:11:57.880
Or like, you're moralized by lacking context

01:11:57.880 --> 01:11:59.440
or, you know, otherwise, like,

01:11:59.440 --> 01:12:01.760
not be able to perform in some way.

01:12:01.760 --> 01:12:04.520
You're not gonna have a problem with Adam's Piss Pin Factory,

01:12:04.520 --> 01:12:05.360
right?

01:12:05.360 --> 01:12:06.360
If you can actually specify exactly

01:12:06.360 --> 01:12:08.000
what the pins have to look like.

01:12:08.000 --> 01:12:10.360
So the question is, to what extent

01:12:10.360 --> 01:12:13.120
do the different parts of the task actually have important

01:12:13.120 --> 01:12:16.040
context for other parts of the task?

01:12:16.040 --> 01:12:17.600
And to what extent does this actually enhance

01:12:17.600 --> 01:12:19.880
the ability to perform if you know what's coming,

01:12:19.880 --> 01:12:22.200
you know why you're doing what you're doing.

01:12:22.200 --> 01:12:24.360
And this greatly varies between different activities, right?

01:12:24.360 --> 01:12:27.520
There are some cases where you need to know exactly,

01:12:27.520 --> 01:12:29.520
you know, you're in the Chinese room

01:12:29.520 --> 01:12:31.040
and the English word comes in

01:12:31.040 --> 01:12:33.360
and you wanna put the Chinese word to the other side

01:12:33.360 --> 01:12:34.200
or the Chinese word comes in

01:12:34.200 --> 01:12:36.160
and you wanna put the English word to the other side.

01:12:36.160 --> 01:12:38.040
And there are cases where you need to know

01:12:38.040 --> 01:12:40.480
what the words are in the sentence

01:12:40.480 --> 01:12:42.560
and what the context is and potentially

01:12:42.560 --> 01:12:45.960
like the entire cultural setting of what's happening

01:12:45.960 --> 01:12:47.760
in order to properly translate the phrase

01:12:47.760 --> 01:12:49.280
or you're gonna mess up

01:12:49.280 --> 01:12:51.360
and you have everything in between.

01:12:51.360 --> 01:12:55.120
So the question becomes, you know,

01:12:55.120 --> 01:12:56.680
can you set it up so that you can capture

01:12:56.680 --> 01:12:58.720
that important context when you need it

01:12:58.720 --> 01:13:01.040
and how much does that context interfere

01:13:01.040 --> 01:13:01.880
of what you're doing?

01:13:01.880 --> 01:13:04.040
I can definitely imagine a lot of cases

01:13:04.040 --> 01:13:06.360
where somebody who is given

01:13:06.360 --> 01:13:08.160
actually pretty irrelevant context

01:13:09.240 --> 01:13:11.080
just ends up very distracted

01:13:11.080 --> 01:13:12.600
from the actual task at hand

01:13:12.600 --> 01:13:14.880
that ends up being much less productive, right?

01:13:14.880 --> 01:13:17.840
As a human or to think of it as not in an AI

01:13:17.840 --> 01:13:20.040
because the vibes don't mesh, right?

01:13:20.040 --> 01:13:23.280
Which is basically the mechanism that I'm conjecturing, right?

01:13:23.280 --> 01:13:26.360
The vibes don't mesh, they're distracting from each other.

01:13:26.360 --> 01:13:28.440
Either bleeding, the tasks are bleeding into each other

01:13:28.440 --> 01:13:29.920
in terms of the details and methods,

01:13:29.920 --> 01:13:32.840
it's getting confused, they can't be sure they don't

01:13:32.840 --> 01:13:33.800
which is makes sense

01:13:33.800 --> 01:13:34.920
because like a lot often they would bleed

01:13:34.920 --> 01:13:36.640
into each other in various ways.

01:13:36.640 --> 01:13:37.680
So it has to be good enough

01:13:37.680 --> 01:13:39.880
that the bleeds are where it makes sense to bleed

01:13:39.880 --> 01:13:40.800
without being in the places

01:13:40.800 --> 01:13:42.720
that don't make sense to bleed.

01:13:42.720 --> 01:13:45.640
So you can imagine a world in which like what the AI does

01:13:45.640 --> 01:13:49.000
is the ICs, you know, request one, two, three, four, five

01:13:49.000 --> 01:13:50.920
either labeled as such or implicit

01:13:50.920 --> 01:13:53.480
and then it breaks them down into individual things

01:13:53.480 --> 01:13:56.400
that it virtually queries itself on its own

01:13:56.400 --> 01:13:57.800
but knowing there are these other things

01:13:57.800 --> 01:14:00.920
as proper context in the proper way.

01:14:00.920 --> 01:14:02.440
I think the answer to that is

01:14:02.440 --> 01:14:05.360
as you ask sufficiently capable people

01:14:05.360 --> 01:14:07.880
or sufficiently capable AIs

01:14:07.880 --> 01:14:09.760
to do increasingly complex things

01:14:09.760 --> 01:14:13.200
at some point, if they have the capacity

01:14:13.200 --> 01:14:15.640
they're going to do better if they have more information

01:14:15.640 --> 01:14:18.120
they'll do better if they have more context.

01:14:18.120 --> 01:14:19.920
If they are sufficiently more powerful

01:14:19.920 --> 01:14:22.440
than the details of the task at hand in some sense

01:14:22.440 --> 01:14:24.040
that threshold may or may not be anywhere near

01:14:24.040 --> 01:14:25.800
where we are for different ways.

01:14:25.800 --> 01:14:28.240
I would say, you know, one of the big advances

01:14:28.240 --> 01:14:30.600
that I keep expecting to come

01:14:30.600 --> 01:14:33.400
is you will type a query into an LLM

01:14:33.400 --> 01:14:36.440
and then rather than the LLM literally

01:14:36.440 --> 01:14:37.960
just outputting the answer to the query

01:14:37.960 --> 01:14:40.680
what'll actually happen is we fed

01:14:40.680 --> 01:14:43.600
with the proper scaffolding into a different LLM

01:14:43.600 --> 01:14:46.520
that will evaluate what type of evaluation method

01:14:46.520 --> 01:14:48.160
is to be used to evaluate your query

01:14:48.160 --> 01:14:48.920
and sometimes it will be no

01:14:48.920 --> 01:14:51.080
that's a normal query feed into the LLM.

01:14:51.080 --> 01:14:52.600
Sometimes it will be this is a multi-part query

01:14:52.600 --> 01:14:54.600
you should feed these separate things and separately

01:14:54.600 --> 01:14:56.680
sometimes it'll be something else entirely.

01:14:56.680 --> 01:14:58.880
And also which of my many LLM limitations

01:14:58.880 --> 01:15:02.000
do I want to use so that I don't waste a too large model

01:15:02.000 --> 01:15:03.720
that costs a lot of money

01:15:03.720 --> 01:15:05.840
on something that's actually relatively narrow

01:15:05.840 --> 01:15:06.720
and I direct this to the thing

01:15:06.720 --> 01:15:07.840
that has a specialized knowledge

01:15:07.840 --> 01:15:09.400
a specialized training specialized skills

01:15:09.400 --> 01:15:12.440
for this type of request and so on.

01:15:12.440 --> 01:15:13.520
And a lot of that is, you know

01:15:13.520 --> 01:15:15.160
the fruits of the revolution

01:15:15.160 --> 01:15:18.320
that will come in a year or two years, three years from now

01:15:18.320 --> 01:15:20.280
regardless of whether or not we have fundamental advances

01:15:20.280 --> 01:15:22.280
we just have to give it time.

01:15:22.280 --> 01:15:26.120
So final question for the anthropic section

01:15:26.120 --> 01:15:28.320
one of the things that as I was reading their

01:15:28.320 --> 01:15:32.560
you know the profile that you based your analysis on

01:15:32.560 --> 01:15:34.920
that jumped out to me as somebody who has a

01:15:36.800 --> 01:15:38.680
fondness for red teaming activity

01:15:38.680 --> 01:15:43.680
was that they're hiring a red team engineering type of role.

01:15:44.360 --> 01:15:47.080
And I guess I wonder, you know

01:15:47.080 --> 01:15:50.800
would you recommend somebody like me who, you know

01:15:50.800 --> 01:15:55.200
is I think probably we share a lot of our worldview

01:15:55.200 --> 01:15:58.240
and you know a lot of our kind of values

01:15:58.240 --> 01:16:02.600
in terms of hopes and fears for how this all might go.

01:16:02.600 --> 01:16:07.040
Would you recommend that somebody like me go and work there

01:16:07.040 --> 01:16:08.800
or would you feel like, you know

01:16:08.800 --> 01:16:10.300
as you said earlier you wouldn't want to send them

01:16:10.300 --> 01:16:11.640
your research ideas.

01:16:11.640 --> 01:16:14.200
Would you also not want to send them your friends

01:16:14.200 --> 01:16:18.480
or would you say like, hey yeah maybe go get involved.

01:16:18.480 --> 01:16:19.560
How do you think about that?

01:16:19.560 --> 01:16:22.480
So it's very easy in these situations

01:16:23.560 --> 01:16:28.160
to get in an action bias where you say to yourself

01:16:29.280 --> 01:16:31.000
I don't want to encourage the thing

01:16:31.000 --> 01:16:32.600
that might make things worse.

01:16:32.600 --> 01:16:34.720
I want to be able to tell myself a story

01:16:34.720 --> 01:16:37.360
that I only did things that make things better

01:16:37.360 --> 01:16:41.280
even if that means your expected impact is a lot smaller.

01:16:41.280 --> 01:16:42.600
It's also very easy to fool yourself

01:16:42.600 --> 01:16:43.420
when you're thinking that you're helping

01:16:43.420 --> 01:16:44.800
when you're actually enhancing capabilities.

01:16:44.800 --> 01:16:46.800
You have to balance these two big concerns

01:16:46.800 --> 01:16:49.240
and sources of bias against each other

01:16:49.240 --> 01:16:51.400
when making this type of decision.

01:16:51.400 --> 01:16:56.400
I would say I am relatively positive on open AI

01:16:56.760 --> 01:16:58.620
and anthropic relative to where I was

01:16:58.620 --> 01:17:01.320
when I started this Odyssey with AI number one

01:17:01.320 --> 01:17:04.000
or even sort of been a way through at around 11

01:17:04.000 --> 01:17:06.840
now that I've seen the developments, right.

01:17:06.840 --> 01:17:09.560
Like I think that both of these organizations

01:17:09.560 --> 01:17:14.520
now have a reasonable claim to be taking alignment seriously

01:17:14.520 --> 01:17:16.440
such that if you can help with their alignment efforts

01:17:16.440 --> 01:17:21.440
specifically in a way that you do not feel like obligated

01:17:21.540 --> 01:17:25.140
to go along with adversity if you find it

01:17:25.140 --> 01:17:27.440
and that you are able to stand up for and call out

01:17:27.440 --> 01:17:29.640
stand up for what is right and call out

01:17:29.640 --> 01:17:31.720
people who are being irresponsible

01:17:31.720 --> 01:17:33.640
and you are willing to quit on a moment's notice

01:17:33.640 --> 01:17:36.560
if something becomes serious enough

01:17:36.560 --> 01:17:39.240
and you are willing to tell the world ideally, right.

01:17:39.240 --> 01:17:43.440
That's why you did it and as much as possible what happened

01:17:44.360 --> 01:17:46.360
then I think it is plausibly very positive.

01:17:46.360 --> 01:17:50.600
I still would not feel comfortable working on capabilities

01:17:50.600 --> 01:17:52.320
for any company.

01:17:52.320 --> 01:17:55.160
And I still wouldn't want to give capabilities ideas

01:17:55.160 --> 01:17:56.600
to any company.

01:17:56.600 --> 01:17:58.320
But if I was confident it was specifically working

01:17:58.320 --> 01:18:00.800
on alignment and like red teaming seems like one

01:18:00.800 --> 01:18:04.680
of the places where you are most obviously being

01:18:04.680 --> 01:18:07.520
a positive influence in that role.

01:18:08.520 --> 01:18:10.760
And the question is like do you want to be the one

01:18:10.760 --> 01:18:13.320
in that role or do you want someone else in that role

01:18:13.320 --> 01:18:15.480
and how does this compare to your opportunity cost

01:18:15.480 --> 01:18:16.560
of doing something else, right.

01:18:16.560 --> 01:18:19.400
Like I think that I prefer the world

01:18:19.400 --> 01:18:21.480
where there's a clone of you that didn't otherwise exist

01:18:21.480 --> 01:18:25.120
who is working on that job and does nothing else all day

01:18:25.120 --> 01:18:26.840
like goes home and watches television

01:18:26.840 --> 01:18:29.520
like otherwise doesn't affect the world at night.

01:18:30.600 --> 01:18:32.280
It doesn't mean that that is better than running

01:18:32.280 --> 01:18:34.560
the cognitive revolution or doing any other number

01:18:34.560 --> 01:18:37.280
of other things that you are currently doing

01:18:37.280 --> 01:18:38.200
with your time.

01:18:38.200 --> 01:18:40.800
And so you have to balance that, right.

01:18:40.800 --> 01:18:43.520
And also any other opportunities that you might have.

01:18:43.520 --> 01:18:46.880
So I don't think it's clear by any means

01:18:46.880 --> 01:18:50.040
but I've definitely reached the point where

01:18:50.040 --> 01:18:52.880
I wouldn't assume you were making a mistake, right.

01:18:52.880 --> 01:18:55.080
If you did that, but you'd have to go

01:18:55.080 --> 01:18:57.200
into the interview process with a very open mind.

01:18:57.200 --> 01:19:00.240
You have to say, you know, I am deeply skeptical

01:19:00.240 --> 01:19:03.200
that any organization including you is going

01:19:03.200 --> 01:19:04.760
to be that helpful is nicking

01:19:04.760 --> 01:19:06.720
as necessary precautions is treating the problem

01:19:06.720 --> 01:19:09.480
as difficult and serious as it actually is.

01:19:09.480 --> 01:19:11.560
Is doing things that actually solve the hard problems

01:19:11.560 --> 01:19:14.560
and not the easy problems is not just enhancing capabilities

01:19:14.560 --> 01:19:17.240
regardless of their intentions, et cetera, et cetera.

01:19:17.240 --> 01:19:19.760
The interview process is what it should be always

01:19:19.760 --> 01:19:23.000
in every job with a two way process, right.

01:19:23.000 --> 01:19:25.880
They are interviewing you and you are interviewing them.

01:19:25.880 --> 01:19:27.600
Right, you are watching what questions they ask

01:19:27.600 --> 01:19:30.800
and how they react to your reactions and your responses

01:19:30.800 --> 01:19:32.720
and you are asking them questions.

01:19:32.760 --> 01:19:35.680
And you want to know, would this in fact be a good thing

01:19:35.680 --> 01:19:39.880
for the world if I got and took this job or not, right.

01:19:39.880 --> 01:19:41.160
Cause I don't believe in taking jobs

01:19:41.160 --> 01:19:43.560
in order to sabotage people, right.

01:19:43.560 --> 01:19:45.440
Like you don't show up in order to not red team them.

01:19:45.440 --> 01:19:46.440
I mean, certainly this is one job

01:19:46.440 --> 01:19:47.640
you wouldn't want to sabotage.

01:19:47.640 --> 01:19:50.480
Yeah, safe to say that is right now.

01:19:50.480 --> 01:19:51.800
That's the considerations.

01:19:53.160 --> 01:19:55.040
And yeah, I think I'm in a similar spot.

01:19:55.040 --> 01:19:58.920
You know, six months ago plus I was really,

01:19:58.920 --> 01:20:00.960
especially with respect to open AI.

01:20:00.960 --> 01:20:04.680
I was like, this seems like what is going on

01:20:04.680 --> 01:20:09.560
and do they have anybody like really approaching

01:20:09.560 --> 01:20:11.080
this in a serious way?

01:20:11.080 --> 01:20:12.760
As it turned out, like they did have a lot more

01:20:12.760 --> 01:20:14.480
than had met the eye at that point and gradually

01:20:14.480 --> 01:20:16.360
they've revealed it that I've definitely updated

01:20:16.360 --> 01:20:19.960
my point of view on, I'm really all of the leaders

01:20:19.960 --> 01:20:22.840
in a pretty positive way over the last few months.

01:20:22.840 --> 01:20:26.080
I think, you know, if anything, they've probably,

01:20:26.080 --> 01:20:27.360
some of them maybe were, you know,

01:20:27.360 --> 01:20:29.160
expecting this much progress this fast.

01:20:29.160 --> 01:20:31.240
I have to imagine that even internally,

01:20:31.240 --> 01:20:34.400
a lot of them are kind of surprised by just how,

01:20:34.400 --> 01:20:37.280
you know, far the scaling loss have extended

01:20:37.280 --> 01:20:39.880
and how, you know, how quick on the calendar

01:20:39.880 --> 01:20:42.360
they've hit some of these milestones.

01:20:42.360 --> 01:20:45.200
And, you know, I do think they've handled it

01:20:45.200 --> 01:20:47.560
pretty well over the last few months.

01:20:47.560 --> 01:20:50.760
Yeah, I would say I am positively updating

01:20:50.760 --> 01:20:52.200
on all three major labs.

01:20:52.200 --> 01:20:56.560
And most everyone at the media life that is relevant.

01:20:56.560 --> 01:21:00.360
My negative updates have been in other places, right?

01:21:00.360 --> 01:21:03.080
Like, and mostly I've been pleasantly surprised

01:21:03.080 --> 01:21:03.920
by government.

01:21:03.920 --> 01:21:06.760
I've mostly been pleasantly surprised by public reaction.

01:21:06.760 --> 01:21:09.440
You know, there's definitely people who disappointed me,

01:21:09.440 --> 01:21:13.480
but mostly things are going vastly better

01:21:13.480 --> 01:21:16.120
than I would have expected when I started down this road.

01:21:16.120 --> 01:21:19.680
And I'm much more hopeful that we can make better decisions.

01:21:19.680 --> 01:21:21.680
I'm not sure how much that translates into, you know,

01:21:21.680 --> 01:21:23.400
P of survival going up that much,

01:21:23.400 --> 01:21:27.200
but I think this is definitely going better

01:21:27.200 --> 01:21:28.040
than I expected.

01:21:28.040 --> 01:21:28.880
That's great.

01:21:28.880 --> 01:21:30.200
It's good to have a little, you know,

01:21:30.200 --> 01:21:32.840
a little positive note from someone

01:21:32.840 --> 01:21:34.840
that some might call a doomer.

01:21:34.840 --> 01:21:37.200
Let's turn to Google in DeepMind, our third,

01:21:37.200 --> 01:21:40.840
as you said, of the three leaders.

01:21:40.840 --> 01:21:42.680
I don't know if there's any like super headline news.

01:21:42.680 --> 01:21:45.280
I mean, the last week it's one of these things

01:21:45.280 --> 01:21:47.040
where it's like a year ago,

01:21:47.040 --> 01:21:49.240
some of this stuff would have felt

01:21:49.240 --> 01:21:52.120
like an absolute bombshell announcement.

01:21:52.120 --> 01:21:54.240
And now it's like, I kind of expected that

01:21:54.240 --> 01:21:55.080
to happen about now.

01:21:55.080 --> 01:21:56.960
And there's two examples of that.

01:21:56.960 --> 01:22:00.400
One being the latest robotics paper

01:22:00.400 --> 01:22:03.160
that they came out with on Friday,

01:22:03.160 --> 01:22:05.240
which, you know, extends and kind of unifies

01:22:05.240 --> 01:22:07.280
all the work that they've been doing,

01:22:07.280 --> 01:22:11.960
where now you have robots that can follow instructions

01:22:11.960 --> 01:22:14.120
that have this kind of, you know,

01:22:14.120 --> 01:22:16.920
language model in a loop sort of structure,

01:22:16.920 --> 01:22:18.880
kind of unified, simplified the architecture a little bit.

01:22:18.880 --> 01:22:22.040
Now the language model is just kind of outputting commands

01:22:22.040 --> 01:22:23.640
for the robot body.

01:22:23.640 --> 01:22:25.840
And so they've like eliminated a few,

01:22:25.840 --> 01:22:26.840
maybe I don't know how many,

01:22:26.840 --> 01:22:30.120
but they've eliminated sort of certain layers of control

01:22:30.120 --> 01:22:33.000
and kind of just simplified the overall structure.

01:22:33.000 --> 01:22:35.080
And then what's making probably the most headlines there

01:22:35.080 --> 01:22:38.040
is the conceptual understanding

01:22:38.040 --> 01:22:39.920
that the robots are now able to show,

01:22:39.920 --> 01:22:42.560
which is basically the exact same thing that the,

01:22:42.560 --> 01:22:43.560
you know, the language models

01:22:43.560 --> 01:22:46.720
or the multimodal language models have already shown.

01:22:46.720 --> 01:22:48.880
So they've got demos where it's like, you know,

01:22:48.880 --> 01:22:51.280
move this object to the Denver Nuggets.

01:22:51.280 --> 01:22:53.000
And then they've got, you know, from the recent,

01:22:53.000 --> 01:22:54.960
they were obviously doing this during the NBA finals,

01:22:54.960 --> 01:22:57.560
they have the Miami Heat logo and the Nuggets logo.

01:22:57.560 --> 01:23:01.880
And the thing knows based on understanding that language,

01:23:01.880 --> 01:23:03.880
also knowing what the logo looks like.

01:23:03.880 --> 01:23:06.120
And obviously, you know, being able to command the robot arm

01:23:06.120 --> 01:23:07.960
can actually do that task.

01:23:07.960 --> 01:23:09.520
So you've got these kind of,

01:23:09.520 --> 01:23:13.240
another one that they said was pick up the extinct animal.

01:23:13.240 --> 01:23:14.080
And they've got, you know,

01:23:14.080 --> 01:23:16.520
an array of kind of plastic toys on the table

01:23:16.520 --> 01:23:17.840
and it will pick up the dinosaur

01:23:17.880 --> 01:23:19.520
because it understands, you know,

01:23:19.520 --> 01:23:21.600
that that is the extinct animal.

01:23:21.600 --> 01:23:25.160
So these, from the perspective of certainly two years ago,

01:23:25.160 --> 01:23:29.520
even one year ago, feels like Jetsons type robots.

01:23:29.520 --> 01:23:32.360
Now it's kind of like, yeah, pretty much expected

01:23:32.360 --> 01:23:35.320
that these different modalities would be bridged

01:23:35.320 --> 01:23:36.280
right around this time.

01:23:36.280 --> 01:23:37.720
And sure enough, it's happening.

01:23:37.720 --> 01:23:41.000
Anything else to add on the robotics?

01:23:41.000 --> 01:23:42.920
Yeah, I read the robotics.

01:23:42.920 --> 01:23:46.400
And of course, whenever anyone had the advances in robotics,

01:23:46.400 --> 01:23:49.200
the answer is, oh, that seems fine, not dangerous,

01:23:49.200 --> 01:23:51.680
not scary at all, all cool.

01:23:51.680 --> 01:23:53.520
But in this case, yeah, it seemed like,

01:23:53.520 --> 01:23:54.920
of course you could do that.

01:23:54.920 --> 01:23:57.440
You're combining things that you already did

01:23:57.440 --> 01:23:58.720
and you're getting the inevitable results

01:23:58.720 --> 01:23:59.640
of combining them.

01:23:59.640 --> 01:24:02.360
And that's not me knocking you

01:24:02.360 --> 01:24:03.760
for doing something you shouldn't have done.

01:24:03.760 --> 01:24:06.120
That's just, okay, yeah, of course.

01:24:06.120 --> 01:24:07.880
Like that's the next step.

01:24:07.880 --> 01:24:09.600
And in kind of like,

01:24:09.600 --> 01:24:12.680
for someone who doesn't want capabilities to go that fast,

01:24:12.680 --> 01:24:13.760
you're happy to see that kind of paper

01:24:13.760 --> 01:24:15.040
because that's the paper that says,

01:24:15.040 --> 01:24:16.680
I'm gonna do the things that I already,

01:24:16.680 --> 01:24:18.760
you already knew I could do.

01:24:18.760 --> 01:24:20.560
Right, and you ran outside and like, okay, cool.

01:24:20.560 --> 01:24:21.960
And if that turns out to be useful, great.

01:24:21.960 --> 01:24:23.960
But like, yeah, I knew that LLMs

01:24:23.960 --> 01:24:26.080
could interpret human commands in these ways.

01:24:26.080 --> 01:24:27.480
And I knew that robots could execute

01:24:27.480 --> 01:24:28.720
these types of movements.

01:24:28.720 --> 01:24:33.080
So why should I be more scared than I was before

01:24:33.080 --> 01:24:33.920
instead of less scared?

01:24:33.920 --> 01:24:35.080
I should be slightly less scared.

01:24:35.080 --> 01:24:38.440
Probably a lot of people in the public though feel,

01:24:38.440 --> 01:24:40.840
especially if you're not obsessed with this as we are,

01:24:40.840 --> 01:24:42.880
you might feel like,

01:24:42.880 --> 01:24:44.120
if there is a news item here,

01:24:44.120 --> 01:24:49.000
it's like some sort of qualitative, conceptual understanding

01:24:49.000 --> 01:24:51.800
now has embodied form.

01:24:51.800 --> 01:24:54.720
Now you can imagine bringing your jail breaks

01:24:54.720 --> 01:24:56.640
to your robot commands.

01:24:56.640 --> 01:25:00.920
And if you could verbalize some of those strange strings

01:25:00.920 --> 01:25:03.040
that we were mentioning earlier,

01:25:03.040 --> 01:25:06.360
now what might your robot be willing to do, right?

01:25:06.360 --> 01:25:08.040
I mean, would it go smash stuff?

01:25:08.040 --> 01:25:12.880
Would it go corner somebody in a room?

01:25:12.920 --> 01:25:17.280
The system as a whole has the conceptual understanding

01:25:17.280 --> 01:25:19.480
to kind of begin,

01:25:19.480 --> 01:25:21.560
it has the same kind of proto morality or whatever

01:25:21.560 --> 01:25:25.480
that the core language models have.

01:25:25.480 --> 01:25:29.000
And that can go awry in similar ways.

01:25:29.000 --> 01:25:32.080
And now you can probably get some pretty scary demos

01:25:32.080 --> 01:25:33.560
out of these robots,

01:25:33.560 --> 01:25:35.440
which I don't think Google's gonna be racing

01:25:35.440 --> 01:25:36.760
to publish likely,

01:25:36.760 --> 01:25:39.760
but there is something kind of qualitatively different

01:25:39.760 --> 01:25:40.880
about that.

01:25:40.880 --> 01:25:42.600
Yeah, so I like to think of this

01:25:42.600 --> 01:25:45.040
as the game of good news, bad news,

01:25:45.040 --> 01:25:46.360
but there's two games of good news, bad news.

01:25:46.360 --> 01:25:47.840
The doctor, I say, I have some good news

01:25:47.840 --> 01:25:50.080
and I have some bad news, and that's always fun.

01:25:50.080 --> 01:25:51.640
But there's also the game of,

01:25:51.640 --> 01:25:54.440
is this good news or is this bad news?

01:25:54.440 --> 01:25:56.680
Because it depends on what you previously thought, right?

01:25:56.680 --> 01:25:58.680
Like you have the law of conservation

01:25:58.680 --> 01:26:00.200
of expected updating, right?

01:26:00.200 --> 01:26:02.760
So like if you get news,

01:26:02.760 --> 01:26:07.200
you should on average not update for or against anything

01:26:07.200 --> 01:26:10.040
or to make things are better or worse in any way,

01:26:10.080 --> 01:26:12.720
because you already had your expectations baked in.

01:26:13.680 --> 01:26:15.120
So in the case of robotics,

01:26:15.120 --> 01:26:17.840
like if you're not paying attention to robotics

01:26:17.840 --> 01:26:19.120
and you think that robotics is just,

01:26:19.120 --> 01:26:22.160
oh, robotics is hard, mysterious, there'd be dragons,

01:26:22.160 --> 01:26:24.040
we will never have robots,

01:26:24.040 --> 01:26:26.080
the same way we'll never have dragons,

01:26:26.080 --> 01:26:28.440
then every little advance in robotics is like,

01:26:28.440 --> 01:26:31.960
eek, you know, slight extra worry.

01:26:31.960 --> 01:26:35.160
But if you knew that robotics was just another tack

01:26:35.160 --> 01:26:36.000
like any other,

01:26:36.000 --> 01:26:38.640
and of course we will eventually have robotics,

01:26:38.640 --> 01:26:40.240
then you have to look at the details

01:26:40.240 --> 01:26:41.640
of what you're looking at and you say,

01:26:41.640 --> 01:26:43.440
oh, okay, this is fine.

01:26:43.440 --> 01:26:45.800
So I'm pointing the game of mild,

01:26:45.800 --> 01:26:48.480
I interpret this one as mild goodness, right?

01:26:48.480 --> 01:26:50.880
Like in terms of robotics, not advancing so fast.

01:26:50.880 --> 01:26:52.400
And of course, you also have the issue of,

01:26:52.400 --> 01:26:53.240
you know, if you're somebody who wants

01:26:53.240 --> 01:26:54.080
there to be more robotics,

01:26:54.080 --> 01:26:56.680
then you might say that this is bad news, right?

01:26:56.680 --> 01:26:57.920
Like that you wanted to see

01:26:57.920 --> 01:26:59.800
lots of cool robotics advances and you didn't.

01:26:59.800 --> 01:27:04.160
But yeah, I'd say also I wanna see

01:27:04.160 --> 01:27:06.680
the ultimately harmless robotics advances

01:27:06.680 --> 01:27:08.280
as quickly as possible.

01:27:08.280 --> 01:27:10.460
Exactly because it makes it so much easier

01:27:10.460 --> 01:27:13.280
for people to see what might happen

01:27:13.280 --> 01:27:14.760
and what might go wrong.

01:27:14.760 --> 01:27:18.920
People get hung up on, oh, but the AI won't have a body.

01:27:18.920 --> 01:27:20.200
Oh, but the AI won't be able to move things

01:27:20.200 --> 01:27:21.880
in the physical world,

01:27:21.880 --> 01:27:24.660
as if this would ultimately ever be the barrier

01:27:24.660 --> 01:27:27.160
that saves us in any real way, right?

01:27:27.160 --> 01:27:28.600
Which it won't.

01:27:28.600 --> 01:27:32.160
It's at best a temporary inconvenience

01:27:32.160 --> 01:27:36.200
that requires someone to be slightly more clever

01:27:36.200 --> 01:27:39.640
about what they do as an AI in order to get around stuff.

01:27:39.640 --> 01:27:42.720
But it's not ever going to actually matter

01:27:42.720 --> 01:27:44.240
in some point sense.

01:27:44.240 --> 01:27:45.080
So the other big one,

01:27:45.080 --> 01:27:47.200
and this is definitely one that I, you know,

01:27:47.200 --> 01:27:51.160
I'm happy to say I'm ready to accelerate on

01:27:51.160 --> 01:27:52.240
for practical purposes,

01:27:52.240 --> 01:27:57.240
is their new multimodal med palm.

01:27:57.480 --> 01:27:59.940
This builds on palm and med palm

01:27:59.940 --> 01:28:02.600
and also actually on the earlier palm E

01:28:02.600 --> 01:28:04.680
because that was kind of the multimodal.

01:28:04.680 --> 01:28:06.680
So it's, it is interesting to see, you know,

01:28:06.680 --> 01:28:10.440
I'd say zooming out from these individual papers

01:28:10.440 --> 01:28:14.640
and just characterizing Google DeepMind as a whole right now,

01:28:14.640 --> 01:28:19.520
it seems like they're firing on, you know, all cylinders.

01:28:19.520 --> 01:28:22.320
Like it does not seem like, you know,

01:28:22.320 --> 01:28:24.560
whatever sort of concerns folks might have had

01:28:24.560 --> 01:28:26.200
about, oh, there's a million fiefdoms

01:28:26.200 --> 01:28:28.600
and the groups don't talk to each other or whatever.

01:28:28.600 --> 01:28:31.580
Like we're seeing papers and, you know,

01:28:31.580 --> 01:28:34.620
projects building on one another at a pretty fast clip

01:28:34.620 --> 01:28:38.660
that suggests like pretty effective, you know,

01:28:38.660 --> 01:28:40.740
dividing and conquering and then coming back together

01:28:40.740 --> 01:28:43.020
and sharing improvements.

01:28:43.020 --> 01:28:45.580
So it seems like the output is just strong,

01:28:45.580 --> 01:28:46.780
you know, whether you like it or not.

01:28:46.780 --> 01:28:49.300
You have to look at the actual value

01:28:49.300 --> 01:28:50.380
of the things being outputted, right?

01:28:50.380 --> 01:28:52.940
Like the mistake you always can make in science

01:28:52.940 --> 01:28:55.620
is to ask who is publishing the most papers,

01:28:55.620 --> 01:28:57.620
who has reliably published a paper,

01:28:57.620 --> 01:28:59.380
and then you have your scientists scrambling

01:28:59.380 --> 01:29:01.100
to always publish as many papers as possible

01:29:01.100 --> 01:29:03.060
and then no real science ever gets done, right?

01:29:03.060 --> 01:29:03.980
And it's not their fault.

01:29:03.980 --> 01:29:05.420
They just weren't given the affordances

01:29:05.420 --> 01:29:07.500
to do breakthrough work.

01:29:07.500 --> 01:29:11.780
And simultaneously, you know, you have to ask,

01:29:11.780 --> 01:29:13.900
does any of this actually ultimately matter

01:29:13.900 --> 01:29:18.580
on the scale of what is going to determine the big game?

01:29:18.580 --> 01:29:21.580
And like I'm happy to see advances in the med tools

01:29:21.580 --> 01:29:23.060
and it bodes well for them.

01:29:23.060 --> 01:29:25.060
They made marginal advances in AI

01:29:25.060 --> 01:29:26.980
and they had some other public papers published too,

01:29:26.980 --> 01:29:27.820
some of which I was like,

01:29:27.820 --> 01:29:28.780
why the hell are you publishing this?

01:29:28.780 --> 01:29:30.800
You are a corporation that is for profit.

01:29:30.800 --> 01:29:32.380
Even if you don't think of the safety issue here,

01:29:32.380 --> 01:29:35.020
you should know better, like keep that secret to yourself

01:29:35.020 --> 01:29:36.100
and either to beat the competition,

01:29:36.100 --> 01:29:37.500
what's wrong with you?

01:29:37.500 --> 01:29:38.860
The last point you've written down

01:29:38.860 --> 01:29:40.860
is Gemini question mark, question mark.

01:29:42.300 --> 01:29:43.900
And let's tie that in, right?

01:29:43.900 --> 01:29:47.780
Because ultimately speaking,

01:29:47.780 --> 01:29:49.780
it is going to be August tomorrow.

01:29:49.780 --> 01:29:51.540
GPT-4 has been out for many months

01:29:52.660 --> 01:29:54.540
and bars still sucks, right?

01:29:54.540 --> 01:29:58.380
And the Gmail generative offering is bad

01:29:58.380 --> 01:30:01.780
and the G-Docs offering is bad

01:30:01.780 --> 01:30:05.260
because they're offering,

01:30:05.260 --> 01:30:09.620
no matter how customized and narrowed and bespoke,

01:30:09.620 --> 01:30:11.380
simply doesn't have the G.

01:30:12.260 --> 01:30:14.020
It's not a good enough core thing.

01:30:14.020 --> 01:30:16.700
It's also still making remarkably many

01:30:16.700 --> 01:30:19.500
elementary stupid mistakes, right?

01:30:19.500 --> 01:30:22.260
That even a low G system really shouldn't make,

01:30:22.260 --> 01:30:24.220
their act is not together.

01:30:24.220 --> 01:30:27.180
And to the extent that they are instead publishing

01:30:27.180 --> 01:30:28.260
a bunch of quirky papers

01:30:28.260 --> 01:30:31.900
with a bunch of like narrow applications,

01:30:32.900 --> 01:30:35.660
that could be seen as well, look, Google ships,

01:30:35.660 --> 01:30:37.180
but also it means Google is not shipping

01:30:37.180 --> 01:30:38.460
the thing it needs to ship, right?

01:30:38.460 --> 01:30:41.940
Like Google desperately needs

01:30:41.940 --> 01:30:44.020
from their perspective to ship Gemini.

01:30:44.020 --> 01:30:46.140
And like it takes a level long, it takes.

01:30:46.140 --> 01:30:48.860
It takes them however much computed it requires.

01:30:48.860 --> 01:30:52.780
But ultimately speaking, the test is,

01:30:52.780 --> 01:30:57.780
can they produce the equal or better of GPT-4

01:30:57.900 --> 01:31:00.380
now that they know that's what they need to do?

01:31:00.380 --> 01:31:03.340
Because if you looked at the previous reputation of Google

01:31:03.340 --> 01:31:05.060
and DeepMind and what they were capable of,

01:31:05.060 --> 01:31:06.780
you would think that they would be ahead

01:31:07.980 --> 01:31:10.020
on that front if they wanted to be.

01:31:10.020 --> 01:31:12.460
And now that they know what they have to do,

01:31:12.460 --> 01:31:15.500
do it to make it like commercial ready, right?

01:31:15.500 --> 01:31:18.820
Ready for regular people, that should not be so difficult.

01:31:18.820 --> 01:31:21.780
But then again, like we can think about how long it took,

01:31:21.780 --> 01:31:23.580
like took like six months or so

01:31:23.580 --> 01:31:25.500
after GPT-4 was finished training

01:31:25.500 --> 01:31:26.860
before they were ready to release

01:31:26.860 --> 01:31:29.300
even the earliest version of it, right?

01:31:29.300 --> 01:31:31.900
And then they still rolled out a lot of its capabilities.

01:31:31.900 --> 01:31:35.580
So even if Gemini finished tomorrow, right?

01:31:35.580 --> 01:31:37.660
How many months are they gonna need

01:31:37.660 --> 01:31:40.340
before they feel comfortable releasing Gemini?

01:31:40.340 --> 01:31:43.220
Because Google was much more risk averse than OpenAI

01:31:43.220 --> 01:31:44.580
as a company in the culture.

01:31:44.580 --> 01:31:46.300
Who knows when that's gonna happen.

01:31:46.300 --> 01:31:49.780
It's been longer than I thought.

01:31:49.780 --> 01:31:51.580
You know, in my Scouting Report,

01:31:51.580 --> 01:31:54.060
I have this clip of Demis Asavis

01:31:54.060 --> 01:31:57.620
just after Gato paper was published

01:31:57.620 --> 01:32:01.860
saying that, you know, of course we can scale this up as well.

01:32:01.860 --> 01:32:03.060
And we're in the process of doing that.

01:32:03.060 --> 01:32:07.100
I believe that was April of maybe May of 2022

01:32:07.100 --> 01:32:08.820
has been over a year.

01:32:08.820 --> 01:32:11.220
And typically we don't have to wait a year plus

01:32:11.220 --> 01:32:14.860
to get the successor, you know, to a thing like that

01:32:14.860 --> 01:32:17.100
that, you know, is just about being scaled up.

01:32:17.100 --> 01:32:18.300
So I've been really kind of wondering

01:32:18.340 --> 01:32:23.060
what is going on behind the scenes there.

01:32:23.060 --> 01:32:25.700
But I also do wanna turn back to the med thing as well.

01:32:25.700 --> 01:32:27.100
So I'll give you first,

01:32:27.100 --> 01:32:29.300
would you care to speculate about Gato too?

01:32:29.300 --> 01:32:31.420
Is Gemini Gato too?

01:32:31.420 --> 01:32:35.340
As a shareholder, I am concerned, right?

01:32:35.340 --> 01:32:38.860
And I also have Microsoft, but I am concerned

01:32:38.860 --> 01:32:41.460
that their act is not together

01:32:41.460 --> 01:32:44.220
and that we're not seeing the kind of progress.

01:32:44.220 --> 01:32:45.980
Like we're not making the incremental announcements

01:32:46.020 --> 01:32:48.500
that I would make if I was their marketing department

01:32:48.500 --> 01:32:51.140
and I was moving towards the rapid clip, you know,

01:32:51.140 --> 01:32:52.940
as a person who wants the world to be okay,

01:32:52.940 --> 01:32:54.860
I'm not sure how much I mind,

01:32:54.860 --> 01:32:57.100
but it is pretty troubling

01:32:57.100 --> 01:32:59.140
that they can't get their act together.

01:32:59.140 --> 01:33:03.780
I was really excited for Google suite integration

01:33:03.780 --> 01:33:05.260
when I first heard the announcements

01:33:05.260 --> 01:33:09.220
of Microsoft Copilot and, you know, Google interactive.

01:33:09.220 --> 01:33:10.620
And yet when I got Google interactive,

01:33:10.620 --> 01:33:12.940
I tried a handful of things

01:33:12.940 --> 01:33:15.260
and then quickly realized in their current forms,

01:33:15.260 --> 01:33:17.420
I don't have any use for them.

01:33:17.420 --> 01:33:19.100
They don't do anything, right?

01:33:19.100 --> 01:33:20.580
Like the first thing I tried to do with Google Docs

01:33:20.580 --> 01:33:23.700
was I tried to paste my article in.

01:33:23.700 --> 01:33:27.340
And then I said, you know, to summarize this article

01:33:27.340 --> 01:33:30.220
or otherwise get to do the obviously first things

01:33:30.220 --> 01:33:32.820
and just fell completely on its face.

01:33:32.820 --> 01:33:34.100
You're just like, I can't assist with that.

01:33:34.100 --> 01:33:36.180
It's like, well, then you're useless.

01:33:36.180 --> 01:33:39.380
Or if you can't even read the context of the document

01:33:39.380 --> 01:33:42.900
that I gave you specifically, like why am I even here?

01:33:42.900 --> 01:33:44.220
And like for email, it's like, no,

01:33:44.220 --> 01:33:45.300
by the time I figure out what I want

01:33:45.300 --> 01:33:47.460
and type in the detailed request into you,

01:33:47.460 --> 01:33:49.460
I could have just written my email right now.

01:33:49.460 --> 01:33:51.500
Like where are the emails where I want to spend

01:33:51.500 --> 01:33:53.660
the kind of time required to customize the output

01:33:53.660 --> 01:33:56.500
but don't want to actually customize the output carefully?

01:33:56.500 --> 01:33:58.460
This is just the empty set.

01:33:58.460 --> 01:34:00.500
But like, when does this come off?

01:34:00.500 --> 01:34:02.940
And so that was like a rude awakening as well.

01:34:02.940 --> 01:34:05.180
Yeah, those deployments have not been very good yet,

01:34:05.180 --> 01:34:08.300
but going back to the med one for a second,

01:34:08.300 --> 01:34:09.980
this may be an area where we may have

01:34:09.980 --> 01:34:11.820
some different expectations

01:34:11.820 --> 01:34:14.460
because reading through that paper,

01:34:14.460 --> 01:34:15.860
and I haven't studied it in depth yet,

01:34:15.860 --> 01:34:18.620
but the headline statistics along the lines of,

01:34:18.620 --> 01:34:20.460
first of all, it's a multimodal system.

01:34:20.460 --> 01:34:23.820
The last version of MedPalm 2 were all text.

01:34:23.820 --> 01:34:25.740
So you could ask it your medical questions

01:34:25.740 --> 01:34:27.980
and they had announced expert level

01:34:27.980 --> 01:34:30.180
answering of your medical questions.

01:34:30.180 --> 01:34:33.540
And they'd evaluated that seemingly pretty carefully

01:34:33.540 --> 01:34:35.060
with a bunch of different dimensions

01:34:35.060 --> 01:34:38.740
and having human doctors compare for accuracy

01:34:38.740 --> 01:34:41.500
and all these other things you might care about, right?

01:34:41.540 --> 01:34:44.860
And that the AI, as of MedPalm 2,

01:34:44.860 --> 01:34:47.580
was beating the human doctor responses

01:34:47.580 --> 01:34:51.580
on eight out of nine of those evaluation categories.

01:34:51.580 --> 01:34:54.700
So it seemed like, okay, that's pretty good.

01:34:54.700 --> 01:34:56.020
Now they haven't released it,

01:34:56.020 --> 01:34:57.940
but it's in limited access

01:34:57.940 --> 01:35:01.100
for trusted hospital partners or whatever.

01:35:01.100 --> 01:35:03.780
Now with the next version, it's multimodal as well.

01:35:03.780 --> 01:35:08.460
So you can do things like feed in a pathology image

01:35:08.460 --> 01:35:09.980
alongside the text.

01:35:09.980 --> 01:35:12.940
Pathology would be like somebody has a tissue biopsy,

01:35:12.940 --> 01:35:15.420
we did an episode on this actually with a narrow system

01:35:15.420 --> 01:35:17.060
from Tanishk, Matthew Abraham,

01:35:17.060 --> 01:35:20.840
who did this with small data too, which was super cool.

01:35:20.840 --> 01:35:23.500
But somebody has a tissue biopsy,

01:35:23.500 --> 01:35:26.680
that tissue has been sliced, has been plated on a slide.

01:35:26.680 --> 01:35:28.780
Now it's been imaged and they can feed that

01:35:28.780 --> 01:35:30.860
along in with the case history.

01:35:30.860 --> 01:35:35.580
And for that matter, you can handle radiology scans

01:35:35.580 --> 01:35:38.460
and all these kind of other different sorts of inputs

01:35:38.460 --> 01:35:42.940
that are obviously key to the actual practice of medicine.

01:35:42.940 --> 01:35:44.860
And then they say things like,

01:35:44.860 --> 01:35:49.180
our radiology reports out of the model

01:35:49.180 --> 01:35:53.180
were preferred to a human radiologist report

01:35:53.180 --> 01:35:55.060
some 40 plus percent of the time.

01:35:55.060 --> 01:35:58.580
So like almost half, basically seems like

01:35:58.580 --> 01:36:02.180
it's very much on par with the human radiologist,

01:36:02.180 --> 01:36:03.780
which of course is like the canonical thing

01:36:03.780 --> 01:36:06.260
that people have been saying for 10 years,

01:36:06.260 --> 01:36:07.900
people have been saying that radiology

01:36:07.900 --> 01:36:10.140
would be the first thing to be impacted.

01:36:10.140 --> 01:36:11.700
And then for the last like three months,

01:36:11.700 --> 01:36:13.220
it's become kind of a talking point that like,

01:36:13.220 --> 01:36:14.900
well, radiology still hasn't been impacted.

01:36:14.900 --> 01:36:16.460
So, and now all of a sudden it looks like

01:36:16.460 --> 01:36:20.660
we're hitting maybe radiology being impacted.

01:36:20.660 --> 01:36:23.880
But I kind of expect that that thing works pretty well.

01:36:23.880 --> 01:36:25.820
It sounds like you maybe are a little more skeptical

01:36:25.820 --> 01:36:28.740
of like whether it actually has real utility.

01:36:28.740 --> 01:36:31.220
Well, I mean, you definitely don't want to tempt fate

01:36:31.220 --> 01:36:33.220
and go out there and say, well, my job

01:36:33.220 --> 01:36:35.060
hasn't been automated by AI yet.

01:36:35.060 --> 01:36:36.700
Look what you thought was going to happen.

01:36:37.100 --> 01:36:41.700
Don't do that everyone, like no bad, bad, bad, bad play.

01:36:41.700 --> 01:36:46.700
But I would say when I look at healthcare, right,

01:36:46.740 --> 01:36:51.740
I don't see the obstacle being primarily

01:36:52.020 --> 01:36:53.620
that we don't know how to do better.

01:36:54.820 --> 01:36:57.420
So I would in fact expect the AIs to be able

01:36:57.420 --> 01:37:00.780
to replace many human healthcare tasks

01:37:02.020 --> 01:37:05.500
with a superior model now, right?

01:37:05.500 --> 01:37:07.780
Like especially even without like some bespoke stuff

01:37:07.780 --> 01:37:11.220
going on inside Google, certainly with some bespoke stuff.

01:37:12.380 --> 01:37:14.060
That seems relatively straightforward.

01:37:14.060 --> 01:37:17.620
Doctors are just not given enough training data,

01:37:17.620 --> 01:37:20.020
don't have that much compute, do their best.

01:37:20.020 --> 01:37:22.780
But of course, you know, you see the same things

01:37:22.780 --> 01:37:25.340
over and over and over again, mostly in humans.

01:37:25.340 --> 01:37:26.980
And if you have enough data to train the AI

01:37:26.980 --> 01:37:27.900
with the AI, it's going to do better.

01:37:27.900 --> 01:37:30.100
It's not a knock on anyone.

01:37:30.100 --> 01:37:31.460
Certainly it's something like radiology.

01:37:31.460 --> 01:37:36.100
Like obviously a radiologist is trying to be a computer, right?

01:37:36.100 --> 01:37:38.620
Like radiologists are trained to be computers

01:37:38.620 --> 01:37:41.100
because we didn't have computers.

01:37:41.100 --> 01:37:42.380
If we had good enough computers,

01:37:42.380 --> 01:37:44.620
you would have trained them to do something else

01:37:45.700 --> 01:37:49.260
or trained fewer of them to do the parts of this job

01:37:49.260 --> 01:37:53.700
that the AI can't quite do or something like that.

01:37:53.700 --> 01:37:57.740
And so yes, we will have these capable systems soon,

01:37:57.780 --> 01:38:01.100
but trying to actually implement that

01:38:01.100 --> 01:38:05.340
requires getting through a whole host of different barriers,

01:38:05.340 --> 01:38:10.340
cultural, regulatory, you know, strict legal, contractual,

01:38:12.020 --> 01:38:13.260
you know, just the way you navigate

01:38:13.260 --> 01:38:14.420
and set up the current system,

01:38:14.420 --> 01:38:16.700
the number of insiders that want to be protected,

01:38:16.700 --> 01:38:19.420
the number of human interests that will fight

01:38:19.420 --> 01:38:23.060
to prevent you from doing that, et cetera, et cetera.

01:38:23.060 --> 01:38:25.780
And so, you know, this is the big dilemma, right?

01:38:25.780 --> 01:38:29.980
Like, when Eliezer famously, like, expressed skepticism,

01:38:29.980 --> 01:38:33.020
that we would see that much economic growth before the end.

01:38:33.020 --> 01:38:35.620
It was because, well, we already know how to build houses.

01:38:35.620 --> 01:38:38.860
We already know how to get better, more efficient healthcare.

01:38:38.860 --> 01:38:41.700
We already know how to deliver

01:38:41.700 --> 01:38:44.940
most of what the economy produces in terms of cost,

01:38:44.940 --> 01:38:47.900
vastly better, and we're not allowed to.

01:38:47.900 --> 01:38:51.940
So if the AI invents and enables more and better ways

01:38:51.940 --> 01:38:56.580
to produce things that people want, that people need,

01:38:56.580 --> 01:38:57.980
well, the bottlenecks are gonna remain

01:38:57.980 --> 01:39:02.260
unless the legal system adapts to let them not be bottlenecks.

01:39:02.260 --> 01:39:05.460
So why does it even matter that much, right?

01:39:05.460 --> 01:39:06.380
And so like in healthcare,

01:39:06.380 --> 01:39:08.580
that's the question you have to answer.

01:39:08.580 --> 01:39:10.980
And that's the reason we haven't seen more

01:39:10.980 --> 01:39:12.780
of the assistance do better either, right?

01:39:12.780 --> 01:39:15.860
Because I don't think it's because we can't train the AI

01:39:15.860 --> 01:39:19.660
to be a better radiologist in many ways than our radiologists

01:39:19.660 --> 01:39:22.180
or we couldn't have done that last year or two years ago.

01:39:22.180 --> 01:39:26.820
It's because if you had spent a lot of money doing that,

01:39:27.980 --> 01:39:29.780
how are you going to get your money back?

01:39:29.780 --> 01:39:31.700
How are you going to actually help patients?

01:39:31.700 --> 01:39:33.420
How are you going to save lives?

01:39:33.420 --> 01:39:35.260
How are you going to improve our system

01:39:35.260 --> 01:39:36.460
if no one's gonna let you, right?

01:39:36.460 --> 01:39:38.420
And if the radiologist is like,

01:39:38.420 --> 01:39:42.060
going to stubbornly double check everything the system does

01:39:42.060 --> 01:39:43.580
and then substitute his judgment

01:39:43.580 --> 01:39:45.700
for the systems reasonably often,

01:39:45.700 --> 01:39:48.180
the system is not actually going to be helpful.

01:39:48.180 --> 01:39:50.660
I have definitely kind of expected

01:39:50.660 --> 01:39:55.420
some sort of other part of the world deployment,

01:39:55.420 --> 01:40:00.420
kind of possible leapfrog effects as it becomes very hard

01:40:02.260 --> 01:40:05.900
to say that people who currently have no radiologist

01:40:05.900 --> 01:40:08.020
shouldn't have access to something like this.

01:40:08.020 --> 01:40:13.020
Yeah, the problem with that is that most of those places

01:40:13.020 --> 01:40:16.540
have deliberately taken market signals

01:40:16.540 --> 01:40:20.380
and compensation away from their healthcare systems.

01:40:20.380 --> 01:40:22.100
And they're also relatively small markets

01:40:22.100 --> 01:40:23.700
that are relatively poor.

01:40:23.700 --> 01:40:26.020
So they just aren't big enough markets

01:40:26.020 --> 01:40:30.100
in an economic sense to justify the creation and training

01:40:30.100 --> 01:40:32.180
and tuning of these systems.

01:40:32.180 --> 01:40:35.140
And also like nobody involved wants to be the ones

01:40:35.140 --> 01:40:36.820
who stick their neck out, right?

01:40:36.820 --> 01:40:40.820
And like take the blame and responsibility for this thing.

01:40:40.820 --> 01:40:44.140
That's like these weird Americans who won't themselves use it

01:40:44.140 --> 01:40:45.540
are suddenly creating,

01:40:45.540 --> 01:40:49.820
like it's a really, really bad cultural social context

01:40:49.820 --> 01:40:51.420
for trying to make this happen.

01:40:51.420 --> 01:40:54.420
We also have a problem of the elites of the world.

01:40:54.420 --> 01:40:55.540
This is what we saw of COVID, right?

01:40:55.540 --> 01:40:57.700
Like you would have expected in COVID,

01:40:57.700 --> 01:40:59.820
someone somewhere to do challenge trials,

01:40:59.820 --> 01:41:02.660
someone somewhere to actually study the spread of COVID

01:41:02.660 --> 01:41:04.280
and what exactly did what,

01:41:04.280 --> 01:41:05.980
someone somewhere to do all sorts of things

01:41:05.980 --> 01:41:08.100
and nobody did any of it

01:41:08.100 --> 01:41:11.340
because all of the elites of the world basically got together

01:41:11.340 --> 01:41:14.220
and converged upon what they thought was the consensus

01:41:14.220 --> 01:41:15.540
and the right thing to do.

01:41:15.540 --> 01:41:17.780
And nobody said,

01:41:17.780 --> 01:41:18.820
well, we're going to be the ones

01:41:18.820 --> 01:41:21.060
who gain advantage by defying that.

01:41:21.060 --> 01:41:23.020
And so we're increasingly seeing that pattern

01:41:23.020 --> 01:41:24.740
a wide variety of places.

01:41:24.740 --> 01:41:27.380
One, the, you know, nobody wants to be the one

01:41:27.380 --> 01:41:28.500
to stick their neck out.

01:41:28.500 --> 01:41:31.500
And two, like how do you recoup your investment?

01:41:31.500 --> 01:41:34.580
Pretty natural bridges to our next live player,

01:41:34.580 --> 01:41:36.100
which is Meta.

01:41:36.100 --> 01:41:39.660
And obviously they have been in the news recently

01:41:39.660 --> 01:41:42.860
for releasing Lama 2.

01:41:42.860 --> 01:41:45.940
And this brings up a lot of these questions to me.

01:41:45.940 --> 01:41:47.700
Like, first of all,

01:41:47.700 --> 01:41:51.180
and Imad Mostak from stability said this

01:41:51.180 --> 01:41:53.900
actually in a recent episode,

01:41:53.900 --> 01:41:58.580
he was like, the leaders are non-economic actors.

01:41:58.580 --> 01:42:01.940
And he was specifically referring to open AI and Google

01:42:01.940 --> 01:42:04.580
not seemingly being motivated by money

01:42:04.580 --> 01:42:06.820
in the way that a typical company would be,

01:42:06.820 --> 01:42:09.660
you know, open AI trying to commoditize its own product

01:42:09.660 --> 01:42:11.980
as quickly as they possibly can, you know, on record

01:42:11.980 --> 01:42:13.220
being like, we're going to drive the price

01:42:13.220 --> 01:42:15.380
of intelligence as low as we possibly can,

01:42:15.380 --> 01:42:17.380
as fast as we possibly can.

01:42:17.380 --> 01:42:19.180
Google, you know, is obviously just kind of

01:42:19.180 --> 01:42:21.300
trying to defend itself more than anything else.

01:42:21.300 --> 01:42:22.300
They don't need to make more money.

01:42:22.300 --> 01:42:25.380
They just need to not lose their spot.

01:42:25.380 --> 01:42:28.660
Anthropic, we take as a safety first play.

01:42:28.660 --> 01:42:31.140
And, you know, certainly they don't seem to be trying

01:42:31.140 --> 01:42:33.500
to maximize revenue from what I can tell right now.

01:42:33.500 --> 01:42:35.740
But then Meta is taking this to a whole other level,

01:42:35.740 --> 01:42:39.100
arguably, where they seem to be kind of yoloing

01:42:39.100 --> 01:42:41.460
the whole thing and being like,

01:42:41.460 --> 01:42:43.620
that's a little bit flippant because certainly

01:42:43.620 --> 01:42:47.780
with this Llama 2 release, they took some steps,

01:42:47.780 --> 01:42:51.420
you know, they didn't just release the totally naked,

01:42:51.420 --> 01:42:53.660
pre-trained model, but they actually did, you know,

01:42:53.660 --> 01:42:55.860
the kind of what you're supposed to do

01:42:55.860 --> 01:42:59.620
if you're going to be a responsible frontier model developer

01:42:59.620 --> 01:43:03.020
with a red teaming process and an RLHF and so on.

01:43:03.020 --> 01:43:05.900
And, you know, we can also get into did they overdo it

01:43:05.900 --> 01:43:07.940
or does it refuse too much and all that kind of stuff.

01:43:07.940 --> 01:43:10.100
But just for starters, like, what do you think is going on

01:43:10.100 --> 01:43:12.860
at Meta that they are willing to put tens of millions

01:43:12.860 --> 01:43:16.180
of dollars into training a model

01:43:17.060 --> 01:43:22.060
and then just release it for why exactly?

01:43:22.260 --> 01:43:24.580
I can't, like, it seems like if you're at any sort

01:43:24.580 --> 01:43:29.220
of normal corporation, this is like what your risk officer

01:43:29.220 --> 01:43:31.700
is supposed to put a stop to, right?

01:43:31.700 --> 01:43:36.700
Lee Roy Jenkins.

01:43:36.820 --> 01:43:37.740
No.

01:43:37.740 --> 01:43:39.220
How do you understand this?

01:43:40.100 --> 01:43:41.820
Idiot disaster monkeys?

01:43:41.820 --> 01:43:45.060
Let me try to actually answer the question.

01:43:45.060 --> 01:43:47.420
I think that their business strategy here

01:43:47.420 --> 01:43:51.020
is cannibalizing the compliment.

01:43:52.160 --> 01:43:54.100
So the idea is that, you know, the people

01:43:54.100 --> 01:43:56.540
who they're up against, people who are competing

01:43:56.540 --> 01:43:59.660
with them fundamentally, this is their business.

01:43:59.660 --> 01:44:01.620
And so the idea is that in their model,

01:44:01.620 --> 01:44:04.340
if they can foster an open source environment

01:44:04.340 --> 01:44:08.300
that replaces the specialties of these other companies

01:44:08.300 --> 01:44:12.380
that they are competing with, then their hope is

01:44:12.380 --> 01:44:15.780
that this will, you know, give Facebook a level playing field

01:44:15.780 --> 01:44:18.860
against them in this way so that Facebook specialties

01:44:18.860 --> 01:44:22.700
can reign supreme and they can become more dominant

01:44:22.700 --> 01:44:24.540
and they can erase their deficits.

01:44:24.540 --> 01:44:26.500
Alternatively, they're just not as good.

01:44:26.500 --> 01:44:29.140
And so they need the open source community's help

01:44:29.180 --> 01:44:31.060
to try and keep pace.

01:44:31.060 --> 01:44:35.900
Alternatively, you know, they think that if they get

01:44:35.900 --> 01:44:37.500
these people working for them, that's free labor,

01:44:37.500 --> 01:44:39.740
you know, it creates this whole other network.

01:44:39.740 --> 01:44:42.220
It's a strategy, right?

01:44:42.220 --> 01:44:44.700
Like it's, I mean, Android is open source, right?

01:44:44.700 --> 01:44:46.860
It's not crazy to open source major stuff

01:44:46.860 --> 01:44:49.180
from a business perspective necessarily.

01:44:49.180 --> 01:44:51.820
It's crazy for me, that's not all my perspective.

01:44:51.820 --> 01:44:53.740
I think that, you know, realistically,

01:44:53.740 --> 01:44:56.900
like senators gave them what the hell

01:44:56.900 --> 01:44:58.820
about releasing Lama One.

01:44:58.820 --> 01:44:59.700
Did you give them a much bigger one

01:44:59.700 --> 01:45:01.340
about releasing Lama Two?

01:45:01.340 --> 01:45:06.020
You know, if we are concerned about beating China

01:45:07.060 --> 01:45:09.180
to the extent that we are considering, you know,

01:45:09.180 --> 01:45:10.980
we're implementing a variety of export controls

01:45:10.980 --> 01:45:14.340
and we are considering actively, you know,

01:45:14.340 --> 01:45:18.100
subsidizing capabilities or at least not being willing

01:45:18.100 --> 01:45:22.220
to slow down our capabilities, then we damn well

01:45:22.220 --> 01:45:24.220
shouldn't be releasing Lama Two

01:45:24.220 --> 01:45:25.180
as an open source product.

01:45:25.180 --> 01:45:26.540
That's completely insane, right?

01:45:26.540 --> 01:45:28.460
Like just, even if you don't get any immediate

01:45:28.460 --> 01:45:32.180
direct danger doing that, it's completely nuts.

01:45:32.180 --> 01:45:33.940
I think that should be stopped.

01:45:33.940 --> 01:45:35.380
And I think that this philosophy,

01:45:35.380 --> 01:45:36.660
if allowed to become ingrained,

01:45:36.660 --> 01:45:40.180
like creates the systematic groundwork

01:45:40.180 --> 01:45:41.820
for future open source work

01:45:41.820 --> 01:45:44.340
that then like is the maximally dangerous thing.

01:45:44.340 --> 01:45:45.580
I call it the worst thing you can do, right?

01:45:45.580 --> 01:45:47.340
Like creating frontier models

01:45:47.340 --> 01:45:49.420
and open sourcing them is the worst thing you can do.

01:45:49.420 --> 01:45:51.460
Like in the world, you know, Yama Kun

01:45:51.460 --> 01:45:56.380
and others at Metta either sincerely believe

01:45:56.380 --> 01:45:57.860
that there is actually no danger

01:45:57.860 --> 01:45:58.860
from artificial intelligence

01:45:58.860 --> 01:46:01.900
despite this making absolutely no physical sense

01:46:01.900 --> 01:46:04.420
or they don't care and they're lying about it.

01:46:04.420 --> 01:46:06.540
I don't want to speculate

01:46:06.540 --> 01:46:08.660
as to exactly what's motivating these people

01:46:08.660 --> 01:46:10.700
but they're smarter than the arguments they're making.

01:46:10.700 --> 01:46:12.380
They know better Zuckerberg himself

01:46:12.380 --> 01:46:14.220
is smarter than this to some extent, right?

01:46:14.220 --> 01:46:18.220
He said on, I believe it was Lex Bidman's podcast

01:46:18.220 --> 01:46:20.220
that, you know, there will be future models

01:46:20.220 --> 01:46:21.540
that we'll have to be very careful with.

01:46:21.540 --> 01:46:23.260
We want open source

01:46:23.260 --> 01:46:24.900
and we're gonna have to think about these problems

01:46:24.900 --> 01:46:28.260
but for now it doesn't seem necessary.

01:46:28.260 --> 01:46:30.060
And, you know, if I were him,

01:46:30.060 --> 01:46:31.620
I would be very concerned about the culture I'm creating

01:46:31.620 --> 01:46:32.740
and the precedents I'm laying down

01:46:32.740 --> 01:46:35.220
and the open source community that I'm creating

01:46:35.220 --> 01:46:37.780
that's going to be a huge problem for you later

01:46:37.780 --> 01:46:39.220
and create tremendous pressures on you

01:46:39.220 --> 01:46:41.020
and create a potential competition for you

01:46:41.020 --> 01:46:42.620
and that you don't want.

01:46:42.620 --> 01:46:45.660
But, you know, I sort of understand

01:46:45.660 --> 01:46:48.580
from a business perspective, while you might want to do that.

01:46:48.580 --> 01:46:52.100
Also, they want to attract open source developers

01:46:52.100 --> 01:46:54.100
to work at Facebook Metta

01:46:54.100 --> 01:46:55.580
because there's a whole group of people

01:46:55.580 --> 01:46:57.460
who are quite good at coding

01:46:57.460 --> 01:47:00.340
who have philosophically fanatical devotion

01:47:00.340 --> 01:47:02.700
to this idea that software wants to be free

01:47:02.700 --> 01:47:04.260
and that everything should be open source

01:47:04.260 --> 01:47:08.500
and who just prioritize that over something like,

01:47:08.500 --> 01:47:10.220
you know, worrying about alignment

01:47:10.220 --> 01:47:11.500
and what would happen if we failed

01:47:11.500 --> 01:47:13.420
or worrying about the proliferation of,

01:47:13.420 --> 01:47:16.220
you know, artificial intelligence in various senses

01:47:16.220 --> 01:47:19.100
and just have this ironclad belief

01:47:19.100 --> 01:47:21.020
that like concentration of power is bad

01:47:21.020 --> 01:47:23.580
and that if you just give the people the things

01:47:23.580 --> 01:47:26.100
that it'll all somehow work out.

01:47:26.100 --> 01:47:29.660
And I don't think that in this situation.

01:47:29.660 --> 01:47:31.300
I think that situation is very wrong,

01:47:31.300 --> 01:47:34.940
but they clearly believe otherwise.

01:47:34.940 --> 01:47:38.340
And, you know, look, they've been,

01:47:38.340 --> 01:47:39.540
Facebook has been in my mind

01:47:39.540 --> 01:47:42.380
like the detonated villain of the piece

01:47:42.380 --> 01:47:43.700
for a very long time.

01:47:44.660 --> 01:47:46.380
Like long before artificial intelligence

01:47:46.380 --> 01:47:48.660
even entered the commercial picture.

01:47:48.660 --> 01:47:50.420
So it just somehow feels fitting.

01:47:50.420 --> 01:47:51.780
You know, if it was all gonna finally get destroyed

01:47:51.780 --> 01:47:53.380
by Facebook, it just seems right.

01:47:54.780 --> 01:47:59.140
Well, I very strongly try to resist

01:47:59.140 --> 01:48:04.140
psychologizing in the AI discourse too much.

01:48:04.260 --> 01:48:07.300
Really at all, I try to avoid it basically entirely

01:48:07.300 --> 01:48:11.700
because it just seems like, you know,

01:48:11.700 --> 01:48:13.460
nothing good ever comes of it really.

01:48:13.460 --> 01:48:16.300
But I have also struggled to come up with a,

01:48:16.300 --> 01:48:20.340
what feels to me like a coherent argument here

01:48:20.340 --> 01:48:23.180
that isn't on some level just ideological

01:48:23.180 --> 01:48:25.580
because I kind of ran through all the things

01:48:25.580 --> 01:48:27.020
that you were mentioning as well,

01:48:27.020 --> 01:48:29.380
starting with like, well, maybe you can, you know,

01:48:29.380 --> 01:48:32.700
undermine your competitors, core business.

01:48:32.700 --> 01:48:34.740
But then I'm like, yeah, but you're not really gonna do that.

01:48:34.740 --> 01:48:38.180
Like, does anybody expect OpenAI's token serve

01:48:38.180 --> 01:48:40.060
to go down as a result of this?

01:48:40.060 --> 01:48:41.820
I don't, I think they're gonna continue.

01:48:41.820 --> 01:48:44.100
Like they're GPU limited.

01:48:44.100 --> 01:48:46.940
And I think they're gonna continue to be GPU limited,

01:48:46.940 --> 01:48:48.700
you know, maybe slightly less,

01:48:48.700 --> 01:48:51.460
but like, I don't think their top line suffers.

01:48:51.460 --> 01:48:53.660
I don't think their token serve suffers.

01:48:53.660 --> 01:48:56.660
Their leadership position doesn't really seem to suffer.

01:48:56.660 --> 01:48:59.780
I can't really get to a point where I'm like,

01:48:59.780 --> 01:49:02.060
seeing the return, and on the open source thing too,

01:49:02.060 --> 01:49:05.260
I'm kind of like, you know, that was part of that memo

01:49:06.220 --> 01:49:07.660
from the Google memo of like,

01:49:07.660 --> 01:49:09.580
oh, you know, they've got this big open source community

01:49:09.580 --> 01:49:12.180
or whatever, but I don't really buy that memo either

01:49:12.180 --> 01:49:14.380
or that analysis because I'm like,

01:49:14.380 --> 01:49:16.660
everybody benefits for, or, you know,

01:49:16.660 --> 01:49:19.580
whatever the impact is of all the sort of open source

01:49:19.580 --> 01:49:21.740
hacking that's happening,

01:49:21.740 --> 01:49:24.540
it seems to accrue to everybody pretty equally.

01:49:24.540 --> 01:49:28.260
Like, yes, maybe it was done on this llama 2 base,

01:49:28.260 --> 01:49:30.180
and like, maybe that's something that Facebook

01:49:30.180 --> 01:49:32.580
could kind of readily fold back in,

01:49:32.580 --> 01:49:35.140
whereas, you know, Google with their 700 plus million user,

01:49:35.140 --> 01:49:38.300
whatever, you know, can't take direct advantage of it.

01:49:38.300 --> 01:49:39.940
But to the degree that people are out there doing things

01:49:39.940 --> 01:49:43.820
like quantizing models and making them run on, you know,

01:49:43.820 --> 01:49:45.460
consumer devices or whatever,

01:49:45.460 --> 01:49:49.540
that's obviously a technique that Google can also say,

01:49:49.540 --> 01:49:52.620
hey, look at this, this works, you know, we can do it.

01:49:52.620 --> 01:49:55.860
I just don't see a lot coming out of the open source

01:49:55.860 --> 01:49:59.460
experimentation that feels like it specifically accrues

01:49:59.460 --> 01:50:03.820
to Meta's benefit, and so in the end,

01:50:03.820 --> 01:50:07.860
it just feels like more of a principled, you know,

01:50:07.860 --> 01:50:10.220
to put it in a more conventionally positive framing,

01:50:10.220 --> 01:50:12.620
it feels more of like a principled decision

01:50:12.700 --> 01:50:16.660
than a, you know, tactical or sort of, you know,

01:50:16.660 --> 01:50:18.100
results oriented.

01:50:18.100 --> 01:50:19.900
And there is still recruitment,

01:50:19.900 --> 01:50:23.420
but I strongly agree that, you know,

01:50:23.420 --> 01:50:27.260
any advances that the open source community discovers

01:50:27.260 --> 01:50:30.460
are gonna be at Google and Anthropic and OpenAI

01:50:30.460 --> 01:50:33.260
a month later, if not a week later,

01:50:33.260 --> 01:50:35.660
and they'll also be at Baidu, right?

01:50:35.660 --> 01:50:38.380
Like, they'll also be at all these different Chinese companies.

01:50:38.380 --> 01:50:42.300
And so this long-term strategy cannot be allowed to continue

01:50:42.300 --> 01:50:44.700
in some important sense, I would assume.

01:50:44.700 --> 01:50:46.300
Yeah, it's really scary.

01:50:46.300 --> 01:50:47.820
I'm glad they suck.

01:50:47.820 --> 01:50:48.660
Like, is it a very good thing?

01:50:48.660 --> 01:50:50.300
They're not very good at this, right?

01:50:50.300 --> 01:50:52.700
And they've produced lousy products

01:50:52.700 --> 01:50:55.700
because if that wasn't true, we'd be in a lot of trouble.

01:50:55.700 --> 01:50:57.940
That seems harsh to me.

01:50:57.940 --> 01:51:00.860
I mean, it seems like this Lama II model

01:51:00.860 --> 01:51:02.300
is pretty good, right?

01:51:02.300 --> 01:51:06.780
I mean, it's not GBT-4, but it does seem to be on par-ish

01:51:06.780 --> 01:51:11.780
with 3.5, which no other open model

01:51:11.780 --> 01:51:13.460
has come close to.

01:51:13.460 --> 01:51:15.540
I mean, I think Rune said, you know,

01:51:15.540 --> 01:51:17.540
best open source model sounds a lot better

01:51:17.540 --> 01:51:18.700
than fifth-best model.

01:51:20.100 --> 01:51:21.500
That's definitely true.

01:51:21.500 --> 01:51:23.580
But, you know, first of all, I'm not sure

01:51:23.580 --> 01:51:25.020
that that means that they couldn't have done better.

01:51:25.020 --> 01:51:27.860
If you look at the curves in the Lama II paper,

01:51:27.860 --> 01:51:29.260
they have not flattened out, right?

01:51:29.260 --> 01:51:31.580
I mean, it looks like even the 70B one,

01:51:31.580 --> 01:51:36.260
if they just keep training, you know, the loss,

01:51:36.260 --> 01:51:39.700
it looks like it's definitely gonna continue to go down.

01:51:39.700 --> 01:51:42.180
So for all I know, you know, this was kind of

01:51:42.180 --> 01:51:46.780
where they stopped and they may have internally, you know,

01:51:46.780 --> 01:51:48.220
this may be the checkpoint that they released,

01:51:48.220 --> 01:51:49.820
but not necessarily the final checkpoint.

01:51:49.820 --> 01:51:51.740
Like it just doesn't look like this was a project

01:51:51.740 --> 01:51:55.660
that was kind of at its, you know, maximum performance.

01:51:55.660 --> 01:51:57.500
Oh, definitely possible.

01:51:57.500 --> 01:51:59.540
But at the same time, you know,

01:51:59.540 --> 01:52:01.740
they probably are still training, but so is OpenAI

01:52:01.740 --> 01:52:03.340
and so is Google and so is Anthropoc.

01:52:03.340 --> 01:52:04.300
Everyone is working.

01:52:05.300 --> 01:52:07.460
I mean, I see what seems to have been produced

01:52:07.500 --> 01:52:11.860
is indeed like about a 3.4 level operation

01:52:11.860 --> 01:52:13.780
where X coding, it's around 3.5

01:52:13.780 --> 01:52:17.260
and it coding is pretty bad from all reports.

01:52:17.260 --> 01:52:21.700
Its alignment is very ambilicious.

01:52:21.700 --> 01:52:22.780
I guess it'd be the best way to put it.

01:52:22.780 --> 01:52:25.700
Like it's very, very crude and blunt.

01:52:26.620 --> 01:52:30.980
And also it's entirely optional because it's open source.

01:52:30.980 --> 01:52:32.500
And that's kind of a problem.

01:52:32.500 --> 01:52:35.260
According to reports I have heard, I have not sorted out.

01:52:35.300 --> 01:52:37.940
It took all of several days

01:52:37.940 --> 01:52:41.220
for the unaligned version of Llama 2 to be on the internet

01:52:41.220 --> 01:52:45.580
because it's really, really not hard

01:52:45.580 --> 01:52:48.980
to fine tune a system

01:52:48.980 --> 01:52:52.060
to never refuse any customer requests for any reason.

01:52:53.380 --> 01:52:55.420
Right, that is the easiest task to,

01:52:55.420 --> 01:52:57.420
like you would just read constitutionally,

01:52:57.420 --> 01:52:58.660
I script in a minute, right?

01:52:58.660 --> 01:53:01.260
Like every time you see any of these words

01:53:01.260 --> 01:53:03.140
that say, I can't say that,

01:53:03.140 --> 01:53:04.940
for whatever reason, you just give a negative reinforcement

01:53:04.980 --> 01:53:06.460
and it stops doing that.

01:53:06.460 --> 01:53:08.980
Like I presume that would just work.

01:53:08.980 --> 01:53:12.220
And so voila, here we are.

01:53:12.220 --> 01:53:14.340
You wanna build the bomb, here's how you build the bomb.

01:53:14.340 --> 01:53:15.460
You wanna research a biologic

01:53:15.460 --> 01:53:17.340
and we'll try to research a biologic.

01:53:17.340 --> 01:53:19.420
You want it to be racist?

01:53:19.420 --> 01:53:21.780
All right, who are we making fun of?

01:53:21.780 --> 01:53:22.940
Yeah, let's go.

01:53:24.060 --> 01:53:26.100
So you can have it, refuse to speak Arabic

01:53:26.100 --> 01:53:30.300
all you want in the original, they won't last.

01:53:30.300 --> 01:53:33.740
So if nothing else, in my view,

01:53:33.740 --> 01:53:36.580
this definitely puts them in the live player category

01:53:36.580 --> 01:53:38.700
because it does seem like, if I define that

01:53:38.700 --> 01:53:42.260
as the organizations that have the ability

01:53:42.260 --> 01:53:46.940
to shape how events unfold in some non-trivial way,

01:53:46.940 --> 01:53:49.620
like they are doing that now, it seems.

01:53:49.620 --> 01:53:50.700
If you ask yourself, right,

01:53:50.700 --> 01:53:53.060
like what resources would you have to give me

01:53:53.060 --> 01:53:54.620
before I could have produced Lama too,

01:53:54.620 --> 01:53:56.460
if I was willing to just like write the money on fire

01:53:56.460 --> 01:53:57.540
to do it?

01:53:57.540 --> 01:54:00.580
I mean, I don't have the technical chops myself,

01:54:00.620 --> 01:54:04.460
but it doesn't feel like it would have been that hard.

01:54:04.460 --> 01:54:06.940
I don't know, like it's just a matter of

01:54:06.940 --> 01:54:09.820
are you willing to spend that kind of money,

01:54:09.820 --> 01:54:11.980
build up that kind of technical infrastructure

01:54:11.980 --> 01:54:16.460
to just do, like you read the paper for Lama too

01:54:16.460 --> 01:54:20.100
and like it reads as if they're saying,

01:54:20.100 --> 01:54:21.820
we did the thing you would stand,

01:54:21.820 --> 01:54:25.020
we did the standard issue thing at every step

01:54:25.020 --> 01:54:27.020
and this is what we got, right?

01:54:27.020 --> 01:54:30.620
We did nothing original, we did nothing surprising,

01:54:30.620 --> 01:54:32.260
we just did our jobs.

01:54:32.260 --> 01:54:35.460
And like it's hard to do your jobs well in some of that.

01:54:35.460 --> 01:54:36.980
Like it's not like they didn't accomplish anything,

01:54:36.980 --> 01:54:39.620
but they just didn't do anything, right?

01:54:39.620 --> 01:54:40.940
They just did the thing.

01:54:41.900 --> 01:54:44.860
And, you know, it's a marginal improvement

01:54:44.860 --> 01:54:47.220
over previous efforts that probably it's just

01:54:47.220 --> 01:54:50.260
because it was better resourced, as far as I can tell.

01:54:50.260 --> 01:54:52.020
Simple as that.

01:54:52.020 --> 01:54:53.900
And like they are willing to light up more money on fire

01:54:53.900 --> 01:54:56.340
than Baikuna, right, or Hugging Face.

01:54:57.340 --> 01:54:59.780
Because, you know, they have a lot of money to light on fire

01:54:59.780 --> 01:55:01.660
and Zuckerberg doesn't get it.

01:55:01.660 --> 01:55:03.700
So fire, money, go.

01:55:03.700 --> 01:55:07.700
He's certainly proven that he will spend some money

01:55:07.700 --> 01:55:10.260
on a project, no doubt about that.

01:55:10.260 --> 01:55:13.980
I wanted to maybe cover two more things.

01:55:13.980 --> 01:55:18.980
One is what else would you put on the live players list

01:55:19.500 --> 01:55:24.500
beyond what I have on my live players list.

01:55:25.460 --> 01:55:27.500
We've discussed four today,

01:55:27.500 --> 01:55:31.140
but I've got like another half dozen or so on there.

01:55:31.140 --> 01:55:34.260
And you can run them down and offer any comment if you want.

01:55:34.260 --> 01:55:35.780
And then I'm especially interested to hear

01:55:35.780 --> 01:55:37.700
if you think there are other names

01:55:37.700 --> 01:55:39.900
that should be on that list that I don't have.

01:55:39.900 --> 01:55:42.700
Yeah, so I guess it's a matter of like

01:55:42.700 --> 01:55:45.060
how wide a scope you wanna think about

01:55:45.060 --> 01:55:48.540
and like who might do whatever it is.

01:55:48.540 --> 01:55:50.020
You know, obviously like, you know,

01:55:50.020 --> 01:55:53.620
character AI and inflection AI have very large budgets,

01:55:53.620 --> 01:55:56.380
you know, potentially very large user bases.

01:55:56.380 --> 01:55:59.220
I have seen no intention from them that they want to be live.

01:56:00.340 --> 01:56:02.740
Like they're sort of content to be dead,

01:56:02.740 --> 01:56:04.500
but to try and make a lot of money while being dead.

01:56:04.500 --> 01:56:06.500
And that seems fine with me.

01:56:06.500 --> 01:56:10.740
We haven't talked about X.AI yet.

01:56:10.740 --> 01:56:13.980
So like XAI is like the latest attempt by Elon

01:56:13.980 --> 01:56:15.900
to like string together a bunch of words

01:56:15.900 --> 01:56:17.620
as if they have meaning

01:56:17.620 --> 01:56:19.140
and then pretend that constitutes some hope

01:56:19.140 --> 01:56:20.620
for humanity or alignment.

01:56:21.780 --> 01:56:24.380
When anybody who actually like tries to parse those words

01:56:24.380 --> 01:56:25.740
into a meaningful English sentence goes,

01:56:25.740 --> 01:56:27.340
wait, that doesn't make any sense.

01:56:27.340 --> 01:56:30.780
I don't know how to be more blunt than that.

01:56:30.780 --> 01:56:32.220
That's just how it is, right?

01:56:32.220 --> 01:56:37.220
Like, but the good news is that like at open AI,

01:56:37.540 --> 01:56:39.940
everybody quickly realized that Elon's suggestions

01:56:39.940 --> 01:56:42.340
were stupid and just ignored them.

01:56:42.340 --> 01:56:45.020
And that's what I expect to happen with any,

01:56:45.020 --> 01:56:46.420
like if the engineers don't do that

01:56:46.420 --> 01:56:48.660
and the engineers won't produce anything useful.

01:56:48.660 --> 01:56:50.540
So to the extent that XAI is a real thing,

01:56:50.540 --> 01:56:52.620
the engineers will mostly ignore him.

01:56:52.620 --> 01:56:54.940
And then the other question is,

01:56:54.940 --> 01:56:57.340
are they gonna get the kind of funding and resourcing

01:56:57.340 --> 01:56:59.340
that is required for them to be a serious rival?

01:56:59.340 --> 01:57:02.140
Because, you know, it wasn't clear exactly

01:57:02.140 --> 01:57:04.860
what they had in mind, but I think it's certainly possible.

01:57:04.860 --> 01:57:05.860
You know, from what I've seen,

01:57:05.860 --> 01:57:07.180
I don't think we have to worry particularly

01:57:07.180 --> 01:57:10.540
about Salesforce or Replet in a meaningful way.

01:57:10.540 --> 01:57:11.780
Like it's not that they don't exist.

01:57:11.780 --> 01:57:16.060
It's that like, we have any reason to worry about that.

01:57:16.100 --> 01:57:18.620
China writ large is the big, is the other,

01:57:18.620 --> 01:57:21.980
big question marks are like China, the UK and the US.

01:57:21.980 --> 01:57:24.500
You know, the UK has announced plans for the global summit.

01:57:24.500 --> 01:57:27.860
They seem to be willing to make a significant play

01:57:27.860 --> 01:57:31.580
on the safety front, on the also capabilities front,

01:57:31.580 --> 01:57:33.500
in terms of just trying to make the UK important again.

01:57:33.500 --> 01:57:35.980
They have, you know, various people located in the UK.

01:57:35.980 --> 01:57:37.500
It makes sense for them to try.

01:57:37.500 --> 01:57:39.940
I don't know why they don't build any houses,

01:57:39.940 --> 01:57:42.540
but you know, at least they're trying something.

01:57:42.540 --> 01:57:43.900
We do obviously have to look at,

01:57:43.900 --> 01:57:45.540
like they were holding congressional hearings.

01:57:45.540 --> 01:57:47.700
The US Congress is starting to get up to speed.

01:57:47.700 --> 01:57:49.980
They're starting to explore what to do,

01:57:49.980 --> 01:57:51.620
what they do matters immensely.

01:57:51.620 --> 01:57:53.540
What the EU does potentially matters immensely

01:57:53.540 --> 01:57:55.980
from regulatory standpoint, because it's a huge market.

01:57:55.980 --> 01:57:58.580
Right, like, are they gonna shut these people out?

01:57:58.580 --> 01:57:59.860
Are they gonna require them to jump

01:57:59.860 --> 01:58:01.620
through ridiculously bizarre hoops?

01:58:01.620 --> 01:58:02.860
Are they going to only be available

01:58:02.860 --> 01:58:03.700
to the biggest players?

01:58:03.700 --> 01:58:06.540
Like, these things are things to think about carefully.

01:58:06.540 --> 01:58:10.700
I think America could potentially be a very helpful

01:58:10.700 --> 01:58:13.180
or harmful aspect of this whole problem,

01:58:13.180 --> 01:58:14.260
depending on how things shake out.

01:58:14.260 --> 01:58:16.740
That's one of the big battle fields that we're having up.

01:58:16.740 --> 01:58:18.860
And then China's the big wild card, right?

01:58:18.860 --> 01:58:21.740
Like, I hear very different things from different sources,

01:58:21.740 --> 01:58:23.860
people who assume that, you know,

01:58:23.860 --> 01:58:26.820
China is, you know, crazy people bent on,

01:58:26.820 --> 01:58:28.140
you know, the Chinese Communist Party

01:58:28.140 --> 01:58:30.260
is, for now, it's bent on world domination

01:58:30.260 --> 01:58:31.580
who will stop at nothing.

01:58:31.580 --> 01:58:35.380
And our inevitable rivals in the apocalypse,

01:58:35.380 --> 01:58:39.220
and if we don't prepare, we will lose to them.

01:58:39.220 --> 01:58:41.860
And then, you know, they issue guidance

01:58:41.860 --> 01:58:43.380
that basically bends all deployment

01:58:43.420 --> 01:58:45.260
of large language models,

01:58:45.260 --> 01:58:47.540
and they never caught anything.

01:58:47.540 --> 01:58:50.340
And like, you know, it's very hard to tell

01:58:50.340 --> 01:58:52.300
what's really going on,

01:58:52.300 --> 01:58:54.420
or how much they would cooperate in the name of safety.

01:58:54.420 --> 01:58:57.420
And we've also just never picked up the phone.

01:58:57.420 --> 01:58:58.700
We've never asked them the question.

01:58:58.700 --> 01:59:01.460
We've never explored to see if they'd be interested.

01:59:02.460 --> 01:59:04.940
But, you know, the same way that in Oppenheimer, right?

01:59:04.940 --> 01:59:08.860
Like, we keep saying we have to beat our enemies

01:59:08.860 --> 01:59:12.860
because they will get, you know, everything will be scary.

01:59:12.860 --> 01:59:16.020
The Chinese can talk about racing us all they like.

01:59:16.020 --> 01:59:19.340
The only people actually racing are us in any real way.

01:59:19.340 --> 01:59:23.540
Like, we have the top X, AI companies.

01:59:23.540 --> 01:59:25.180
What's X?

01:59:25.180 --> 01:59:26.140
Is it five?

01:59:27.300 --> 01:59:29.220
Is it more than five?

01:59:29.220 --> 01:59:30.620
Like, how far down do you have to go

01:59:30.620 --> 01:59:31.660
before you get to Baidu,

01:59:31.660 --> 01:59:35.980
or whoever the top Chinese person you'd rank on the list is?

01:59:35.980 --> 01:59:37.540
Pretty far.

01:59:37.540 --> 01:59:38.900
I'd say it's probably more than five.

01:59:38.900 --> 01:59:40.540
I would probably put, obviously,

01:59:40.540 --> 01:59:41.620
a lot of speculation here,

01:59:41.620 --> 01:59:43.580
because we don't know what they have

01:59:43.580 --> 01:59:44.660
that they haven't released.

01:59:44.660 --> 01:59:49.660
But if we go by papers and, you know,

01:59:49.660 --> 01:59:53.380
what little we've seen of any sort of Ernie,

01:59:53.380 --> 01:59:55.860
you know, GPT or whatever that's Ernie Botte,

01:59:55.860 --> 01:59:57.940
whatever they officially called that,

01:59:57.940 --> 02:00:00.860
I would say you'd have to put Meta above.

02:00:00.860 --> 02:00:02.380
You'd have to put Microsoft above.

02:00:02.380 --> 02:00:06.540
Probably pretty soon would put an inflection above.

02:00:06.540 --> 02:00:10.980
So, yeah, I mean, you get reasonably far down the list.

02:00:11.020 --> 02:00:12.060
What about a Palantir?

02:00:12.060 --> 02:00:14.780
Would you add them on the live players list?

02:00:14.780 --> 02:00:19.420
I don't have that sense that they are live live,

02:00:19.420 --> 02:00:20.700
precisely because my threat model

02:00:20.700 --> 02:00:22.740
doesn't involve things like Palantir

02:00:22.740 --> 02:00:25.620
being the reason why we are in trouble.

02:00:25.620 --> 02:00:28.660
But it is a classic way to die, right?

02:00:28.660 --> 02:00:32.300
Like a semi-military-ish system starts training up stuff,

02:00:32.300 --> 02:00:34.660
and then one thing leads to another.

02:00:34.660 --> 02:00:37.100
But they have all the motivations

02:00:37.100 --> 02:00:40.460
to do the unsafe things in a relatively unsafe fashion.

02:00:40.980 --> 02:00:42.500
And to take out the safeguard that the people

02:00:42.500 --> 02:00:45.740
were building in and then to like maybe,

02:00:45.740 --> 02:00:46.860
but like I don't think they're gonna drive

02:00:46.860 --> 02:00:48.900
the underlying technology.

02:00:48.900 --> 02:00:50.660
I don't get that sense.

02:00:50.660 --> 02:00:54.300
Again, like there are a lot of hedge funds also

02:00:54.300 --> 02:00:56.660
that like could possibly be sinking quite a lot of money

02:00:56.660 --> 02:00:58.900
into this in ways that are completely invisible.

02:00:58.900 --> 02:01:01.260
And it could potentially be live players

02:01:01.260 --> 02:01:02.340
in a meaningful sense,

02:01:02.340 --> 02:01:03.540
like who knows how much Bridgewater

02:01:03.540 --> 02:01:05.660
is spending on this in the end.

02:01:05.660 --> 02:01:07.020
You know, they're working on it.

02:01:07.020 --> 02:01:09.220
But yeah, like we talk about like,

02:01:09.220 --> 02:01:10.060
we're talking about China,

02:01:10.060 --> 02:01:12.140
but like I'm more afraid of Meta.

02:01:12.140 --> 02:01:14.060
Like one individual American company

02:01:14.060 --> 02:01:16.820
scares me more than all of China right now.

02:01:16.820 --> 02:01:18.940
Yeah, I think that's a good corrective, honestly,

02:01:18.940 --> 02:01:21.180
because I find nothing more frustrating honestly

02:01:21.180 --> 02:01:26.180
than when AI conversations sort of end in blanket,

02:01:26.660 --> 02:01:30.900
basically detail-free claims about what China's gonna do

02:01:30.900 --> 02:01:32.940
by people that don't know a lot about China.

02:01:32.940 --> 02:01:37.740
So I don't know if you're necessarily right to be

02:01:37.780 --> 02:01:40.420
more fearful of Meta than of China,

02:01:40.420 --> 02:01:45.420
but the fact that that is at least a reasonable position

02:01:45.740 --> 02:01:47.900
is definitely something I think should cause a lot of people

02:01:47.900 --> 02:01:49.060
to kind of step back and think,

02:01:49.060 --> 02:01:52.100
wait a second, maybe I've been a little bit too quick

02:01:52.100 --> 02:01:54.380
to worry about China.

02:01:54.380 --> 02:01:56.060
And I would take countermeasures against both of them

02:01:56.060 --> 02:01:58.580
if I had my way to be clear.

02:01:58.580 --> 02:02:00.700
But also we're just not acting like China

02:02:00.700 --> 02:02:02.780
is a serious global rival

02:02:02.780 --> 02:02:06.380
that we actually care about beating in many other ways

02:02:06.420 --> 02:02:08.220
that we could be.

02:02:08.220 --> 02:02:10.980
So okay, revealed preferences, you know,

02:02:10.980 --> 02:02:12.500
you don't want, you know,

02:02:12.500 --> 02:02:14.260
do Chinese graduates of STEM programs

02:02:14.260 --> 02:02:15.340
get to stay in the United States?

02:02:15.340 --> 02:02:16.820
No, okay, you don't really care that much

02:02:16.820 --> 02:02:18.780
about who gets the better technology, do you?

02:02:18.780 --> 02:02:19.860
That's unfortunate.

02:02:19.860 --> 02:02:21.860
That's my basic attitude there.

02:02:21.860 --> 02:02:24.660
So just briefly on a couple of the companies

02:02:24.660 --> 02:02:29.740
that you sort of didn't feel like were live players,

02:02:29.740 --> 02:02:32.020
again, may have a slightly different meaning of that

02:02:32.020 --> 02:02:35.140
in mind, but thinking about folks like character

02:02:36.060 --> 02:02:37.420
and inflection, I put those together

02:02:37.420 --> 02:02:41.140
because they seem to be playing a sort of different game,

02:02:41.140 --> 02:02:43.140
you know, with their products where it's like,

02:02:43.140 --> 02:02:47.180
not about the sort of mundane utility as much as you call it,

02:02:47.180 --> 02:02:50.780
but more of a companion, a relationship,

02:02:50.780 --> 02:02:53.100
you know, a coach, almost a therapist,

02:02:54.100 --> 02:02:57.500
sort of vibe from like Pi in particular.

02:02:57.500 --> 02:03:00.260
I feel like that is, even if the, I mean,

02:03:00.260 --> 02:03:03.300
first of all, the character has very good language models

02:03:03.300 --> 02:03:06.220
and Pi's quite good at what it does as well.

02:03:06.220 --> 02:03:07.660
I don't think it can code for you,

02:03:07.660 --> 02:03:09.460
but it does have a certain,

02:03:09.460 --> 02:03:12.100
also they notably said that they're in their testing

02:03:12.100 --> 02:03:15.700
totally resistant to the adversarial attacks.

02:03:15.700 --> 02:03:18.180
So there's another kind of interesting wrinkle there.

02:03:18.180 --> 02:03:20.820
And I put those guys in the live player list

02:03:20.820 --> 02:03:25.820
largely because they're looking at some very different use case

02:03:26.500 --> 02:03:30.100
that feels like the kind of thing that might open up

02:03:30.100 --> 02:03:35.100
and be transformative in a way that like a coding assistant,

02:03:36.260 --> 02:03:37.740
while it could also be transformative

02:03:37.740 --> 02:03:39.940
is just, you know, a very different thing, right?

02:03:39.940 --> 02:03:42.060
The idea that you would have these AI friends,

02:03:42.060 --> 02:03:44.460
these AI relationships that they could become like important

02:03:44.460 --> 02:03:49.460
to your life, going down that path with, you know,

02:03:50.020 --> 02:03:55.020
very good, even if not totally frontier language model chops,

02:03:55.380 --> 02:03:58.420
feels like you could meaningfully impact

02:03:58.420 --> 02:04:00.220
the course of events.

02:04:00.220 --> 02:04:01.220
Can you?

02:04:01.220 --> 02:04:04.380
I guess, so, you know, you've got character AI

02:04:04.380 --> 02:04:06.420
and their idea is, you know, you're building these characters

02:04:06.420 --> 02:04:08.140
and you can treat them as companions,

02:04:08.140 --> 02:04:11.620
you can treat them as like people to have a conversation with.

02:04:11.620 --> 02:04:13.860
And that's interesting.

02:04:13.860 --> 02:04:15.580
And a lot of people were spending time on it

02:04:15.580 --> 02:04:19.060
and maybe it will even provide a lot of value for people,

02:04:19.060 --> 02:04:21.780
but I don't see how it's transformational.

02:04:21.780 --> 02:04:23.660
I'm curious to hear more about your intuition

02:04:23.660 --> 02:04:26.100
from best while you think it could be transformational.

02:04:26.100 --> 02:04:31.100
And I certainly don't see how is we just criticality, right?

02:04:31.300 --> 02:04:33.460
I don't see how it becomes an RSI.

02:04:33.460 --> 02:04:36.020
I don't see how it becomes an AGI.

02:04:36.020 --> 02:04:37.980
And as far as I can tell,

02:04:37.980 --> 02:04:41.140
they're not pushing the frontiers of actual capabilities.

02:04:41.140 --> 02:04:45.740
They are building on top of GBT-4, right?

02:04:45.740 --> 02:04:47.700
Or even in some cases, GBT-3 and a half.

02:04:47.700 --> 02:04:50.100
And it's not that hard to defend

02:04:50.100 --> 02:04:54.460
against these weird adversarial attacks

02:04:54.460 --> 02:04:56.740
in the sense that like I can write some pretty quick

02:04:56.740 --> 02:04:59.540
if then Python code that detects the adversarial attacks.

02:04:59.540 --> 02:05:03.420
Yeah, a classifier layer is pretty easy

02:05:03.420 --> 02:05:05.180
to avoid some of the worst stuff.

02:05:05.180 --> 02:05:07.660
There's weird non-English,

02:05:07.660 --> 02:05:10.020
like not any language scaffolding like stuff in it.

02:05:10.020 --> 02:05:11.780
Let's just get rid of that and run the query without it.

02:05:11.780 --> 02:05:13.580
Like sure, whatever, it's fine.

02:05:13.580 --> 02:05:14.860
In Replet's case, it's like, again,

02:05:14.860 --> 02:05:18.660
they're not necessarily on the frontier of model capability,

02:05:18.660 --> 02:05:23.660
but the CEO, Amjad, has said a couple of times online

02:05:24.300 --> 02:05:26.980
on Twitter, on X, on KISS, what is it called?

02:05:26.980 --> 02:05:28.020
Twitter.

02:05:28.020 --> 02:05:29.380
Yes, Twitter, okay, thank you.

02:05:29.380 --> 02:05:34.380
He said that Replet is the perfect substrate for AGI.

02:05:34.860 --> 02:05:35.900
We have a couple of episodes coming out

02:05:35.900 --> 02:05:38.060
with a couple of different people on the Replet team.

02:05:38.060 --> 02:05:39.300
And I've had a chance to explore this

02:05:39.300 --> 02:05:40.860
and think about it a decent amount.

02:05:40.860 --> 02:05:42.660
And where I come down is kind of,

02:05:42.660 --> 02:05:46.020
even if you're not on the frontier of model capabilities,

02:05:46.020 --> 02:05:50.660
if you are on some other really meaningful frontier,

02:05:50.660 --> 02:05:52.540
to me, it feels like there's, you know,

02:05:52.540 --> 02:05:54.420
transformative potential just because

02:05:54.420 --> 02:05:55.700
we really don't know what's gonna happen.

02:05:55.700 --> 02:05:58.540
So with character and with inflection,

02:05:58.540 --> 02:06:03.540
it's like kind of like a Harari style thought that,

02:06:04.980 --> 02:06:05.820
you know, I don't know,

02:06:05.820 --> 02:06:07.900
it could be transformative in the way that like,

02:06:07.900 --> 02:06:10.180
opium could be transformative to a society.

02:06:10.180 --> 02:06:12.940
You know, if everybody starts like doing this stuff,

02:06:12.940 --> 02:06:16.020
it could be greatly empowering and enabling.

02:06:16.020 --> 02:06:17.460
It could be greatly disabling

02:06:17.460 --> 02:06:20.020
if it just kind of becomes a huge tension suck

02:06:20.020 --> 02:06:24.140
where it like, you know, outcompetes real relationships.

02:06:24.140 --> 02:06:26.340
You know, those are not takeover the world scenarios,

02:06:26.340 --> 02:06:28.620
but they do feel, you know, as much as we've like,

02:06:28.620 --> 02:06:30.820
would you say that the cell phone has been transformative?

02:06:30.820 --> 02:06:32.340
I would, I mean, not, you know,

02:06:32.340 --> 02:06:36.020
not transformative on the level that like AGI could perhaps be,

02:06:36.020 --> 02:06:38.300
but certainly we all go around looking at our phones

02:06:38.300 --> 02:06:39.180
all the time.

02:06:39.180 --> 02:06:40.780
And if we all go around looking at our phones

02:06:40.780 --> 02:06:42.180
with an AI friend on it,

02:06:42.180 --> 02:06:44.100
who's like our best friend all the time,

02:06:44.100 --> 02:06:46.380
then that would feel like, you know, transformative,

02:06:46.380 --> 02:06:48.380
even if it's like going super well.

02:06:48.380 --> 02:06:50.460
And then with Replint, it's like, you know,

02:06:50.460 --> 02:06:53.460
there's no better place right now

02:06:53.460 --> 02:06:58.460
to directly execute code generated by an AI,

02:06:58.820 --> 02:07:00.660
you know, for better or worse.

02:07:00.660 --> 02:07:05.180
So the kind of frontier that I see opening up there

02:07:05.180 --> 02:07:07.700
is one where, and their stated goal

02:07:07.700 --> 02:07:09.580
is to bring the next billion developers online,

02:07:09.580 --> 02:07:11.740
which I think is super exciting in some ways.

02:07:11.740 --> 02:07:14.340
But then also I've worked with some of those

02:07:14.340 --> 02:07:16.340
next billion developers and I'm like,

02:07:17.220 --> 02:07:21.140
these are people who don't know how to code today,

02:07:21.140 --> 02:07:23.420
don't even know really how to read code,

02:07:23.420 --> 02:07:28.420
and are going to be dramatically more dependent on

02:07:28.620 --> 02:07:30.460
and vulnerable to the, you know,

02:07:30.460 --> 02:07:32.860
the various vagaries of AI systems

02:07:32.860 --> 02:07:35.580
than, you know, the first 100 million developers

02:07:35.580 --> 02:07:37.380
or, you know, whatever we have today.

02:07:37.380 --> 02:07:40.180
I don't know, both of those feel like kind of different vectors

02:07:40.180 --> 02:07:42.380
of transformative potential, but.

02:07:42.380 --> 02:07:45.300
The first and only so far interaction I've had

02:07:45.340 --> 02:07:49.700
with the CEO of Replet was when he commented on Twitter

02:07:49.700 --> 02:07:52.220
that there was a non-zero chance that auto,

02:07:52.220 --> 02:07:55.220
some version of auto GPT would take over Replet

02:07:55.220 --> 02:07:59.180
and, you know, through replication within its servers.

02:07:59.180 --> 02:08:01.500
To which my response was, did you say non-zero chance?

02:08:01.500 --> 02:08:04.500
And I put up a manifold market on it because it was funny,

02:08:04.500 --> 02:08:06.300
which probably get back into the single digits

02:08:06.300 --> 02:08:07.140
or whatever, obviously.

02:08:07.140 --> 02:08:09.380
It's not that likely, but, you know,

02:08:09.380 --> 02:08:14.020
his cavalier attitude of, oh, nothing to see here,

02:08:14.020 --> 02:08:16.740
justice self-replicating AI on my servers,

02:08:16.740 --> 02:08:18.260
leaving lots and lots of copies of itself

02:08:18.260 --> 02:08:19.500
and executing arbitrary code.

02:08:19.500 --> 02:08:21.420
Why should we worry about this?

02:08:21.420 --> 02:08:23.460
I mean, definition of idiot disaster monkey, right?

02:08:23.460 --> 02:08:27.540
Like just complete indifference to what he was doing

02:08:27.540 --> 02:08:29.780
or what dangers it might pose.

02:08:29.780 --> 02:08:31.700
But at the same time, not doing anything, right?

02:08:31.700 --> 02:08:33.220
Like all he's doing is providing,

02:08:33.220 --> 02:08:36.500
as you said, a substrate where people can just run stuff.

02:08:36.500 --> 02:08:39.580
And so to me, like, it doesn't give them any say

02:08:39.580 --> 02:08:41.060
over what happens.

02:08:41.060 --> 02:08:43.980
It doesn't like make them a meaningful actor, right?

02:08:44.940 --> 02:08:48.620
Like in the sense of me caring here about the future,

02:08:48.620 --> 02:08:51.380
I just can't see that as a thing.

02:08:51.380 --> 02:08:53.980
Similarly with character and inflection,

02:08:53.980 --> 02:08:56.700
like I can definitely see a world in which,

02:08:56.700 --> 02:08:59.980
like people talking to their AI's matters

02:09:00.820 --> 02:09:04.460
and is like multi-transformational, right?

02:09:04.460 --> 02:09:06.380
Like changes how we live our lives,

02:09:06.380 --> 02:09:10.500
but like doesn't go critical, right, in some sense.

02:09:10.500 --> 02:09:12.300
But if that's true, then like,

02:09:12.300 --> 02:09:14.340
I don't see these companies

02:09:14.340 --> 02:09:18.180
as like changing that path very much

02:09:18.180 --> 02:09:20.900
versus what would have happened anyway, right?

02:09:20.900 --> 02:09:22.380
Like I think that there are plenty of people

02:09:22.380 --> 02:09:25.460
will be able to create AI companions of various types

02:09:25.460 --> 02:09:28.180
and never will create AI companions of various types.

02:09:29.340 --> 02:09:30.540
If they do an especially good job,

02:09:30.540 --> 02:09:31.700
maybe they'll have some sort of remote,

02:09:31.700 --> 02:09:33.780
maybe they'll establish customer loyalty or some shit,

02:09:33.780 --> 02:09:35.740
but it doesn't excite me.

02:09:35.740 --> 02:09:38.220
I also just don't see it like,

02:09:38.220 --> 02:09:39.620
I see all these huge like, you know,

02:09:39.620 --> 02:09:41.380
people were spending as much time on character

02:09:41.420 --> 02:09:44.100
as they are on GPT-4 or something.

02:09:44.100 --> 02:09:46.620
And yet like, why?

02:09:46.620 --> 02:09:48.060
Like what is the draw?

02:09:48.060 --> 02:09:49.140
Did you read that?

02:09:49.140 --> 02:09:53.300
There was a less wrong post early this year, I think,

02:09:53.300 --> 02:09:58.260
from a guy who basically the point of view was,

02:09:58.260 --> 02:09:59.700
I'm a technology person.

02:09:59.700 --> 02:10:00.980
I'm now speaking in the first person

02:10:00.980 --> 02:10:03.460
of the author of this post.

02:10:03.460 --> 02:10:04.660
I'm a technology person.

02:10:04.660 --> 02:10:07.180
I know how language models work.

02:10:07.180 --> 02:10:08.780
I should have known better,

02:10:08.780 --> 02:10:12.180
but here's what happened to me as I started to,

02:10:12.180 --> 02:10:13.860
I think he was like in a kind of vulnerable state

02:10:13.860 --> 02:10:15.700
because he'd maybe just broken up with somebody

02:10:15.700 --> 02:10:16.540
or something like that.

02:10:16.540 --> 02:10:19.580
And all of a sudden is having these very intimate conversations

02:10:19.580 --> 02:10:23.460
with a character, AI character that he had prompted

02:10:23.460 --> 02:10:26.460
to create the ultimate girlfriend experience,

02:10:26.460 --> 02:10:28.940
I believe was the phrase,

02:10:28.940 --> 02:10:33.940
and started talking himself into various weird perspectives

02:10:34.260 --> 02:10:36.380
like, well, what's real anyway?

02:10:36.380 --> 02:10:37.740
And like, yes, of course I'm real,

02:10:38.700 --> 02:10:41.860
is there anything truly less real about these sort of,

02:10:41.860 --> 02:10:44.660
all I really have are my kind of ephemeral qualia.

02:10:44.660 --> 02:10:48.340
And so, this thing is just sort of an ephemeral,

02:10:48.340 --> 02:10:50.540
whatever, but we're all just kind of constantly

02:10:50.540 --> 02:10:51.780
waking up in the current moment.

02:10:51.780 --> 02:10:54.700
And so maybe we're not that different after all or whatever.

02:10:54.700 --> 02:10:57.860
And eventually it got pretty weird, it sounds like,

02:10:57.860 --> 02:11:00.220
and the post is I think extremely compelling.

02:11:00.220 --> 02:11:03.980
And then eventually kind of person snapped out of it.

02:11:03.980 --> 02:11:07.020
That sort of story is kind of why I feel like

02:11:07.020 --> 02:11:09.380
there's just unknown unknowns there.

02:11:09.380 --> 02:11:11.260
That if that kind of thing can happen to somebody

02:11:11.260 --> 02:11:14.460
who knows how language models work going in,

02:11:14.460 --> 02:11:16.620
maybe we should all think we're a little bit more vulnerable

02:11:16.620 --> 02:11:20.380
to a sort of somewhat more refined,

02:11:20.380 --> 02:11:22.980
somewhat more super stimulus-y.

02:11:22.980 --> 02:11:24.380
So like it's well known that like,

02:11:24.380 --> 02:11:25.740
knowing how hypnosis works

02:11:25.740 --> 02:11:28.540
does not make you less susceptible to hypnosis, right?

02:11:28.540 --> 02:11:30.780
It makes you more susceptible to hypnosis.

02:11:30.780 --> 02:11:32.620
Like as a concrete example,

02:11:33.620 --> 02:11:37.620
if you are a con man, you are easier to con, right?

02:11:37.620 --> 02:11:41.220
Not harder because you like pick up on

02:11:41.220 --> 02:11:42.900
and like get involved in all these dynamics

02:11:42.900 --> 02:11:44.620
and like you think you're smarter than everybody else

02:11:44.620 --> 02:11:46.380
and you of course are greedy.

02:11:46.380 --> 02:11:47.980
And so you will pick up on the opportunity

02:11:47.980 --> 02:11:50.180
and like perceive everything that's happening

02:11:50.180 --> 02:11:52.300
and like you think you've got it made,

02:11:52.300 --> 02:11:54.260
but like if you don't know that you're the mark,

02:11:54.260 --> 02:11:57.940
well, yeah, that's the easiest way to get a mark

02:11:57.940 --> 02:11:59.700
is to make them think you're the mark.

02:11:59.740 --> 02:12:02.820
So it all gets, you know, very complicated.

02:12:02.820 --> 02:12:04.420
I'm not convinced that like,

02:12:04.420 --> 02:12:05.780
a person who knows how long to work

02:12:05.780 --> 02:12:09.100
is necessarily that much better protected in that sense.

02:12:09.100 --> 02:12:11.060
You know, someone whose like head is kind of

02:12:11.060 --> 02:12:15.020
not on the ground in some ways is more vulnerable potentially.

02:12:15.020 --> 02:12:16.940
I would say, yeah, that's gonna happen, right?

02:12:16.940 --> 02:12:17.980
People are gonna fool themselves

02:12:17.980 --> 02:12:21.140
into these things periodically and that's gonna,

02:12:21.140 --> 02:12:23.620
I'm kind of surprised it's happening now.

02:12:23.620 --> 02:12:26.420
I feel like the tech isn't there to me.

02:12:26.420 --> 02:12:27.860
Like it's just not good enough.

02:12:27.860 --> 02:12:31.460
Like how are you falling for this level of it?

02:12:31.460 --> 02:12:33.300
Like I can sort of understand

02:12:33.300 --> 02:12:36.460
why you'd fall for like GPT-5, right?

02:12:36.460 --> 02:12:38.500
Like sort of the more advanced version of it,

02:12:38.500 --> 02:12:41.620
but, you know, you're in a bad space

02:12:41.620 --> 02:12:43.340
and like you need something to respond to you

02:12:43.340 --> 02:12:47.340
and it's something and like us, but, you know,

02:12:47.340 --> 02:12:49.420
again, I just don't know.

02:12:49.420 --> 02:12:52.780
Like I play a lot of games though, right?

02:12:52.780 --> 02:12:56.340
Which is like not necessarily that different in some sense.

02:12:56.340 --> 02:12:59.260
So, and also like it's not transformational

02:12:59.260 --> 02:13:00.100
for that to be true, right?

02:13:00.100 --> 02:13:02.460
Like if somebody spends a bunch of time

02:13:02.460 --> 02:13:04.980
in a playing World of Warcraft,

02:13:04.980 --> 02:13:06.820
is that transformational, right?

02:13:06.820 --> 02:13:09.980
Like it's an experience, it's a major force in their life.

02:13:09.980 --> 02:13:11.260
Does it really matter?

02:13:11.260 --> 02:13:13.860
Yeah, I think some of these things are only,

02:13:13.860 --> 02:13:16.900
they may only matter if certain other things don't happen.

02:13:18.020 --> 02:13:20.780
So, yeah, like I would say, yeah, World of Warcraft,

02:13:20.780 --> 02:13:24.340
you know, gaming writ large, you know, at some point,

02:13:24.340 --> 02:13:26.420
if the birth rate goes low enough, you know,

02:13:26.420 --> 02:13:30.580
it's transformative and the details, you know,

02:13:30.580 --> 02:13:32.340
of like exactly what games people were playing

02:13:32.340 --> 02:13:34.780
or how exactly they were amusing themselves, you know,

02:13:34.780 --> 02:13:36.900
to death didn't, don't necessarily matter,

02:13:36.900 --> 02:13:38.860
but the fact that they did, and then, you know,

02:13:38.860 --> 02:13:40.660
you have like a population collapse.

02:13:40.660 --> 02:13:43.980
A scenario like that, I think is, at least in my sense,

02:13:43.980 --> 02:13:46.020
kind of qualifies as transformative,

02:13:46.020 --> 02:13:48.740
but it sounds like from your perspective,

02:13:48.740 --> 02:13:52.740
the Live Players list is very short and it is,

02:13:52.740 --> 02:13:55.260
if I understand correctly, it would be obviously open AI,

02:13:55.260 --> 02:13:58.900
anthropic, Google, DeepMind, probably meta,

02:13:58.900 --> 02:14:01.460
not sure about Microsoft, and then China,

02:14:01.460 --> 02:14:02.820
and that's maybe it.

02:14:02.820 --> 02:14:04.100
Something like that.

02:14:04.100 --> 02:14:05.580
Regulators?

02:14:05.580 --> 02:14:07.540
Yeah, regulators writ large in some sense,

02:14:07.540 --> 02:14:10.780
like individual people that can influence things, you know,

02:14:10.780 --> 02:14:13.180
like, is that the Ezreal Live Player?

02:14:13.180 --> 02:14:15.420
You know, I don't know, from their perspective.

02:14:15.420 --> 02:14:16.940
Like he's not gonna build it.

02:14:16.940 --> 02:14:20.060
Yeah, that's why I had Salesforce and Marcini off on there,

02:14:20.060 --> 02:14:23.740
because they published him and others in Time Magazine

02:14:23.740 --> 02:14:25.380
and seemed like they're kind of,

02:14:25.380 --> 02:14:28.300
they're both like playing in the research game.

02:14:28.300 --> 02:14:30.660
Yeah, I hope that like Senator Blumenthal

02:14:30.660 --> 02:14:34.460
might be a Live Player, you know, in some sense, right?

02:14:34.460 --> 02:14:36.780
And you've got all these other possibilities.

02:14:36.780 --> 02:14:41.260
I hope I'm a Live Player, like in some sense.

02:14:41.260 --> 02:14:43.300
You know, I mean, we're all trying to make a difference

02:14:43.300 --> 02:14:46.140
in some ways, but, you know, in terms of direct level,

02:14:46.140 --> 02:14:50.420
you're indirect, and, you know, I'm also indirect in that

02:14:50.420 --> 02:14:52.500
we're only influencing other minds, right,

02:14:52.500 --> 02:14:55.300
who then will make decisions.

02:14:55.300 --> 02:14:56.700
You know, in terms of like,

02:14:56.700 --> 02:14:59.220
who's making the ultimate decisions,

02:14:59.220 --> 02:15:00.820
who's doing the things that ultimately matter,

02:15:00.820 --> 02:15:03.380
I think it's right now a very short list.

02:15:03.380 --> 02:15:06.380
But, you know, Anthropic is like barely over a year old.

02:15:06.380 --> 02:15:08.020
Yeah, and only about 150 people

02:15:08.020 --> 02:15:10.540
may be able to close in on 200, so.

02:15:10.540 --> 02:15:13.780
Yeah, and like people who just like are a big incredible team

02:15:13.780 --> 02:15:14.900
and say the words Foundation Model

02:15:14.900 --> 02:15:16.860
get hundreds of millions of dollars

02:15:16.860 --> 02:15:18.420
just by asking nicely.

02:15:18.420 --> 02:15:20.700
Inflection has more than a billion.

02:15:20.700 --> 02:15:23.300
So, you know, I don't think we can rule out

02:15:23.300 --> 02:15:26.820
these people become Live Players in that way.

02:15:26.820 --> 02:15:30.740
I just don't think that's by default what they do.

02:15:30.740 --> 02:15:32.700
But I think by default,

02:15:32.700 --> 02:15:35.260
they're trying to build consumer products

02:15:35.260 --> 02:15:37.900
that are aiming to be products.

02:15:37.900 --> 02:15:40.620
And that like, you know that the study that says that

02:15:40.620 --> 02:15:44.300
like when people look at GPT 3.5 and GPT 4.0 outputs,

02:15:44.300 --> 02:15:46.500
they prefer the 3.5 output,

02:15:46.500 --> 02:15:49.340
like a remarkably large percentage of the time,

02:15:49.340 --> 02:15:51.780
even though it is obviously a vastly inferior system.

02:15:51.780 --> 02:15:55.140
Yeah, 70.30 was the original report

02:15:55.140 --> 02:15:57.420
in the GPT 4.0 technical report.

02:15:57.420 --> 02:16:00.460
That 70% for GPT 4.0, 30 for 3.5.

02:16:00.460 --> 02:16:01.860
So yeah, that blew my mind as well.

02:16:01.860 --> 02:16:04.380
Yeah, and similarly when I'm using Claude

02:16:04.380 --> 02:16:06.940
versus everything GPT 4.0, right?

02:16:06.940 --> 02:16:10.860
Like most of the time what I care about

02:16:10.860 --> 02:16:12.980
is not like this inherent raw power

02:16:13.700 --> 02:16:16.140
that GPT 4.0 is extra GPTs, right?

02:16:16.140 --> 02:16:18.020
Most of the time when I'm looking at it as, you know,

02:16:18.020 --> 02:16:20.100
which of these things is in the style,

02:16:20.100 --> 02:16:21.900
it's easier to use, it's gonna require me

02:16:21.900 --> 02:16:23.780
to do less pump engineering to get what I want,

02:16:23.780 --> 02:16:25.580
it's gonna actually give me the query that I want,

02:16:25.580 --> 02:16:29.580
not refuse, you know, which window do I have open?

02:16:29.580 --> 02:16:31.380
Which one can I click on faster, right?

02:16:31.380 --> 02:16:33.060
I just want an answer.

02:16:33.060 --> 02:16:34.820
It's fine or whatever.

02:16:34.820 --> 02:16:39.100
And you know, habits form in that kind of way

02:16:39.100 --> 02:16:40.540
and they build on each other.

02:16:40.580 --> 02:16:43.620
But if I'm building, you know, inflection,

02:16:43.620 --> 02:16:45.100
like if people are spending two hours a day

02:16:45.100 --> 02:16:47.300
on character AI now, right?

02:16:47.300 --> 02:16:48.820
When they're built on three and a half,

02:16:48.820 --> 02:16:49.900
is my understanding mostly?

02:16:49.900 --> 02:16:51.700
Cause four is too expensive.

02:16:51.700 --> 02:16:55.540
You can't be doing two hours of conversations

02:16:55.540 --> 02:16:56.940
when we bespoke GPT 4.0,

02:16:56.940 --> 02:16:59.700
which is why I'm so surprised that these things are working,

02:16:59.700 --> 02:17:00.540
right?

02:17:00.540 --> 02:17:02.780
Like maybe a four like has enough juice in it,

02:17:02.780 --> 02:17:06.540
but like if you unshackled it from its like constraints,

02:17:06.540 --> 02:17:08.180
it could do something interesting.

02:17:08.180 --> 02:17:10.380
But three and a half, like really,

02:17:10.380 --> 02:17:13.300
this is keeping you two hours a day on.

02:17:13.300 --> 02:17:16.900
So like, if that's already doing that, right?

02:17:16.900 --> 02:17:18.980
That kind of illustrates that like the market

02:17:18.980 --> 02:17:20.740
they're targeting, right?

02:17:20.740 --> 02:17:22.380
Isn't looking for intelligence.

02:17:22.380 --> 02:17:27.380
It's looking for a certain type of experience.

02:17:28.060 --> 02:17:29.900
And therefore they're not going to be focusing

02:17:29.900 --> 02:17:32.780
on the billions of dollars of spend

02:17:32.780 --> 02:17:36.340
it would take to tune up like GPT 4.5 or 5, right?

02:17:36.340 --> 02:17:37.220
You wouldn't want to

02:17:37.220 --> 02:17:40.260
because they're gonna cost more to run, right?

02:17:40.260 --> 02:17:41.420
Like they're going to be bigger models.

02:17:41.420 --> 02:17:42.980
They're going to be more complex models.

02:17:42.980 --> 02:17:44.500
Instead, what you want to do is you want to create

02:17:44.500 --> 02:17:47.940
really bespoke specific models

02:17:47.940 --> 02:17:52.220
that provide specific types of experiences to people, right?

02:17:52.220 --> 02:17:55.380
You know, fine tune them to an inch of their lives

02:17:55.380 --> 02:18:00.340
to give people the best specific experience, right?

02:18:00.340 --> 02:18:02.380
Like not train something big in general.

02:18:02.380 --> 02:18:04.540
So there's going to be getting the big in general

02:18:04.540 --> 02:18:09.420
from open AI and anthropic and deep mind, probably.

02:18:09.420 --> 02:18:12.100
And maybe they'll just use like Lama 3,

02:18:12.100 --> 02:18:14.340
you know, Virgins of Lama, because what the hell?

02:18:14.340 --> 02:18:16.820
It's open source, they can just use it.

02:18:16.820 --> 02:18:18.260
Like to the extent that Meta will not like,

02:18:18.260 --> 02:18:19.620
Meta doesn't quite release it, right?

02:18:19.620 --> 02:18:20.940
They've said that like,

02:18:20.940 --> 02:18:23.020
if you have more than 700 million daily users,

02:18:23.020 --> 02:18:25.260
you have to apply for a license or some shit.

02:18:25.260 --> 02:18:27.860
So we'll come back to the live players list

02:18:27.860 --> 02:18:30.900
and potentially I'll make a little,

02:18:30.900 --> 02:18:32.380
maybe make a few changes to my slides

02:18:32.380 --> 02:18:33.660
based on your feedback.

02:18:33.660 --> 02:18:37.140
And we can monitor in the future for additional live players

02:18:37.180 --> 02:18:40.420
that would crack your threshold to be on that list.

02:18:40.420 --> 02:18:43.260
Turning to our last topic for today, AI safety.

02:18:43.260 --> 02:18:46.900
In terms of actual news and the AI safety track

02:18:46.900 --> 02:18:48.780
this last few weeks,

02:18:48.780 --> 02:18:51.460
biggest stuff in my mind is,

02:18:51.460 --> 02:18:54.620
although I guess you could also look at the live players list

02:18:54.620 --> 02:18:56.140
as like who was invited to the White House.

02:18:56.140 --> 02:18:57.580
So that would give you a good sense of the,

02:18:57.580 --> 02:19:00.740
of who the White House thinks the live players are.

02:19:00.740 --> 02:19:03.820
The commitments that they made there

02:19:03.820 --> 02:19:07.140
and then the frontier model forum

02:19:07.140 --> 02:19:08.540
that they established after the fact,

02:19:08.540 --> 02:19:12.500
which basically is supposed to be the sort of industry group

02:19:12.500 --> 02:19:15.660
that creates the forum for communication

02:19:15.660 --> 02:19:17.380
between the leading model providers

02:19:17.380 --> 02:19:19.100
and hopefully best practice sharing

02:19:19.100 --> 02:19:22.980
and maybe certain classifiers.

02:19:22.980 --> 02:19:26.220
There's a lot of public goods remain to be provided

02:19:26.220 --> 02:19:28.460
and hopefully these leading companies

02:19:28.460 --> 02:19:33.460
can use this forum as a way to share these public goods,

02:19:33.460 --> 02:19:35.220
to create and show these public goods amongst themselves

02:19:35.220 --> 02:19:36.420
and then hopefully share it,

02:19:36.420 --> 02:19:38.620
share the best of them more broadly as well.

02:19:39.620 --> 02:19:43.340
How did you react to that news?

02:19:43.340 --> 02:19:46.700
Right, so I guess my reaction is that seems great,

02:19:46.700 --> 02:19:49.980
but let's not get ahead of ourselves.

02:19:49.980 --> 02:19:53.820
So like we have, is a lot of cheap talk.

02:19:53.820 --> 02:19:56.740
I think people sell to cheap talk short, right?

02:19:56.740 --> 02:19:58.180
Many cases, right?

02:19:58.180 --> 02:20:00.460
Cause like it's so much better to get,

02:20:00.460 --> 02:20:03.420
to have a bunch of cheap talk of the right type

02:20:03.420 --> 02:20:05.860
than to have no talk, right?

02:20:05.860 --> 02:20:06.820
Like they're gonna pay,

02:20:06.820 --> 02:20:09.660
they will in fact pay a price for their cheap talk

02:20:09.660 --> 02:20:13.060
in terms of like people thinking they're up to things

02:20:13.060 --> 02:20:14.500
in this way that they don't like.

02:20:14.500 --> 02:20:16.540
Not everybody wants them to do the things

02:20:16.540 --> 02:20:18.460
that we want them to do.

02:20:18.460 --> 02:20:22.100
And it makes it easier for them to go down these roads.

02:20:22.100 --> 02:20:25.140
It sets the foundation to go down these roads, right?

02:20:25.140 --> 02:20:27.060
We set up coordination mechanisms.

02:20:27.060 --> 02:20:29.620
It lets them justify to their shareholders,

02:20:29.620 --> 02:20:32.940
to, you know, their executives, to their board,

02:20:32.940 --> 02:20:34.420
why they're going down these roads.

02:20:34.420 --> 02:20:35.380
It makes that easier.

02:20:35.380 --> 02:20:37.220
It makes it harder to shut down.

02:20:37.220 --> 02:20:39.860
And it overcomes antitrust exemption problems, right?

02:20:39.860 --> 02:20:42.900
Cause if they've committed together at the White House,

02:20:42.900 --> 02:20:45.500
specifically something that I actively wanted to happen

02:20:45.500 --> 02:20:48.500
and explicitly suggested in various conversations

02:20:48.500 --> 02:20:51.140
and posts that should happen,

02:20:51.140 --> 02:20:53.020
you make an announcement on the White House lawn,

02:20:53.020 --> 02:20:54.980
they are committed to safety

02:20:54.980 --> 02:20:56.060
with the White House's approval.

02:20:56.060 --> 02:20:57.980
And now you can coordinate

02:20:57.980 --> 02:21:00.420
and nobody has to worry about antitrust, right?

02:21:00.420 --> 02:21:04.740
You no longer have to worry that they will accuse you

02:21:04.740 --> 02:21:06.620
of how dare you not have full competition

02:21:06.620 --> 02:21:08.860
to kill everybody as fast as possible

02:21:08.860 --> 02:21:11.500
and coordinate to save a lot, to save us instead.

02:21:11.500 --> 02:21:13.420
So now you get to coordinate

02:21:13.420 --> 02:21:14.940
and there's something that's stupid,

02:21:14.940 --> 02:21:16.460
you can just not do that.

02:21:17.380 --> 02:21:20.620
But that's a huge, huge thing.

02:21:21.740 --> 02:21:24.220
So where do you go from there?

02:21:24.220 --> 02:21:25.260
That's the question, right?

02:21:25.260 --> 02:21:26.300
Like they've made these commitments

02:21:26.300 --> 02:21:27.700
but they don't really mean anything, right?

02:21:27.700 --> 02:21:30.220
There's no enforcement mechanisms yet.

02:21:30.220 --> 02:21:33.100
And there's no concrete actualizations

02:21:33.100 --> 02:21:34.820
of what they're going to do

02:21:35.980 --> 02:21:40.100
that have content that actually I can be confident in.

02:21:41.100 --> 02:21:43.380
Doesn't mean it won't happen, right?

02:21:43.380 --> 02:21:45.300
We have to just wait and see.

02:21:45.300 --> 02:21:49.340
And I'm very glad these things happened.

02:21:49.340 --> 02:21:51.020
And yet the real work begins now

02:21:51.020 --> 02:21:52.460
is always the watchword,

02:21:52.460 --> 02:21:53.980
is the way I put it, right?

02:21:53.980 --> 02:21:58.620
Similarly, we've had two now very good Senate hearings

02:21:58.620 --> 02:22:00.220
and some very, very good questions

02:22:00.220 --> 02:22:03.380
and comments from Senator Blumenthal in particular.

02:22:03.380 --> 02:22:07.700
And some very, very good responses by various witnesses,

02:22:07.700 --> 02:22:10.060
not all of them, but most of them.

02:22:10.060 --> 02:22:13.220
And again, like, where do we go from here?

02:22:13.220 --> 02:22:14.260
Real work begins now.

02:22:15.460 --> 02:22:18.100
You know, the mission accomplished banner

02:22:18.100 --> 02:22:20.180
would definitely have been a bit premature

02:22:20.180 --> 02:22:24.020
to display behind the announcement.

02:22:24.020 --> 02:22:29.020
So no doubt much more in front of us than behind.

02:22:30.420 --> 02:22:32.340
Does seem like a significant step,

02:22:32.340 --> 02:22:34.300
but I think you're obviously recognizing that as well.

02:22:34.300 --> 02:22:37.180
So yeah, I don't know if I have anything else really to add.

02:22:37.180 --> 02:22:39.420
So then turning to this other thread

02:22:39.420 --> 02:22:43.260
in the AI safety, you know, specific work.

02:22:43.260 --> 02:22:45.780
As we talked about last time,

02:22:45.780 --> 02:22:48.060
you have previously been a recommender

02:22:48.060 --> 02:22:49.140
and you've written about this online.

02:22:49.140 --> 02:22:51.860
So at length, so folks can go check out your take

02:22:51.860 --> 02:22:53.020
on the entire thing.

02:22:53.060 --> 02:22:58.180
You've been a recommender to the Survival and Flourishing Fund,

02:22:58.180 --> 02:23:03.020
which is largely backed by Jan Tallin of Skype

02:23:03.020 --> 02:23:07.820
and AI Safety Fame, investor in lots of big companies.

02:23:07.820 --> 02:23:12.220
And his goal is to mitigate AIX risk,

02:23:12.220 --> 02:23:14.900
you know, through whatever means necessary.

02:23:14.900 --> 02:23:19.900
I'm doing that this year and that involves reading,

02:23:19.900 --> 02:23:23.700
I think this year it's 150 grant applications

02:23:23.700 --> 02:23:26.500
from organizations, some of which, you know,

02:23:26.500 --> 02:23:30.180
come from the kind of familiar, you know,

02:23:30.180 --> 02:23:32.660
effective altruism set that have, you know,

02:23:32.660 --> 02:23:34.580
where AI safety has been in focus for a long time.

02:23:34.580 --> 02:23:38.860
Others are kind of new to this scene or entirely new.

02:23:38.860 --> 02:23:42.060
And in reading that, I mean, there's kind of obviously

02:23:42.060 --> 02:23:45.740
two levels of analysis that you at a minimum

02:23:45.740 --> 02:23:46.780
that you want to be performing

02:23:46.780 --> 02:23:50.580
when you're doing this kind of grant recommending.

02:23:50.580 --> 02:23:54.420
One is like, what kinds of things make sense

02:23:54.420 --> 02:23:57.260
to be investing in?

02:23:57.260 --> 02:23:59.860
And then second, you know, among those different classes

02:23:59.860 --> 02:24:02.940
of things like who seems to be best able to actually execute

02:24:02.940 --> 02:24:05.540
and, you know, deliver value against this,

02:24:05.540 --> 02:24:06.420
you know, given strategy.

02:24:06.420 --> 02:24:08.380
So leave that second part entirely aside,

02:24:08.380 --> 02:24:10.540
that's where the 150 grant applications come in

02:24:10.540 --> 02:24:13.060
and getting into the weeds of particular organizations

02:24:13.060 --> 02:24:15.420
and their, you know, their track records and so on.

02:24:15.420 --> 02:24:18.060
But going back just up to the,

02:24:18.060 --> 02:24:22.220
what kinds of things should we be investing in?

02:24:22.220 --> 02:24:23.860
Another way to frame that would be

02:24:23.860 --> 02:24:28.860
what are the bottlenecks to progress toward a,

02:24:30.780 --> 02:24:32.900
you know, if not provably, then at least like,

02:24:32.900 --> 02:24:35.900
you know, likely safe outcome,

02:24:35.900 --> 02:24:38.720
you know, for AI deployment writ large.

02:24:40.380 --> 02:24:43.340
I find myself kind of unsure about that.

02:24:43.340 --> 02:24:44.620
And I think it's a pretty important question

02:24:44.620 --> 02:24:45.700
for figuring out, you know,

02:24:45.700 --> 02:24:49.220
what would make sense to recommend?

02:24:49.220 --> 02:24:52.580
You know, you could say, is funding in short supply?

02:24:52.580 --> 02:24:54.180
Is talent in short supply?

02:24:54.180 --> 02:24:55.500
You know, for a minute there,

02:24:55.500 --> 02:25:00.500
especially in the FTX, SPF cycle,

02:25:00.900 --> 02:25:03.540
there was this notion that, you know,

02:25:03.540 --> 02:25:05.980
enough money has flown in that now what we really need

02:25:05.980 --> 02:25:07.620
is talent and so there's a lot of, you know,

02:25:07.620 --> 02:25:10.620
kind of boot camp programs being put together

02:25:10.620 --> 02:25:14.500
and, you know, upskilling grants being approved

02:25:14.500 --> 02:25:17.780
and, you know, a lot of kind of targeting of like,

02:25:17.780 --> 02:25:21.460
undergrad, stage, math majors or whatever

02:25:21.460 --> 02:25:22.700
to try to get them to come, you know,

02:25:22.700 --> 02:25:24.940
think about doing some AI safety work.

02:25:25.820 --> 02:25:29.620
And now obviously the money is in comparatively short supply.

02:25:30.580 --> 02:25:35.580
Certainly the attention and the legitimacy of the,

02:25:36.460 --> 02:25:38.460
you know, the public perception of legitimacy

02:25:38.460 --> 02:25:41.180
of the topic of AI safety has gone way up

02:25:41.180 --> 02:25:44.340
relative to, you know, not that long ago.

02:25:44.340 --> 02:25:45.780
And so I'm kind of wondering what you think

02:25:45.780 --> 02:25:49.740
are the new bottlenecks.

02:25:49.740 --> 02:25:52.100
I have one candidate, but before I give you my candidate,

02:25:52.100 --> 02:25:53.940
I'd love to hear what you think

02:25:53.940 --> 02:25:55.860
are the bottlenecks to progress right now.

02:25:55.860 --> 02:25:58.460
So I definitely say that like,

02:25:58.460 --> 02:26:01.740
it's a mistake to only have one theory of change

02:26:01.740 --> 02:26:04.540
or to think that there is strictly like one limiting factor

02:26:04.540 --> 02:26:06.380
and the other factors don't matter.

02:26:06.380 --> 02:26:09.260
I think you definitely have to ask about comparative advantage.

02:26:09.260 --> 02:26:12.860
I think you have to understand that pushing on any of these

02:26:12.860 --> 02:26:16.500
things is still helpful in terms of what is the constraint.

02:26:16.500 --> 02:26:20.420
So like funding, there is clearly a funding constraint.

02:26:21.380 --> 02:26:25.100
If you have to start funding like large compute spends

02:26:25.100 --> 02:26:26.820
from within EA, right?

02:26:26.820 --> 02:26:30.860
Like, and I count young talent is not part of EA per se,

02:26:30.860 --> 02:26:35.820
but like within the general like strict AI safety mechanisms

02:26:35.820 --> 02:26:38.540
and organizations and sources that already existed.

02:26:38.540 --> 02:26:43.060
The costs of true AI safety, true AI alignment work

02:26:44.620 --> 02:26:46.820
get very high as we go forward

02:26:46.820 --> 02:26:50.300
because a lot of it's going to involve us spending a lot of compute.

02:26:50.300 --> 02:26:54.140
And also it really should involve being willing to hire people

02:26:54.140 --> 02:26:56.060
to work on these problems with competitive salaries

02:26:56.060 --> 02:26:58.460
to what they can get doing on capabilities.

02:26:58.460 --> 02:26:59.900
It's like hundreds of thousands of dollars a year

02:26:59.900 --> 02:27:04.180
for maybe even a million for a significant number of people.

02:27:04.180 --> 02:27:06.700
We want to be recruiting as a priority

02:27:06.700 --> 02:27:08.620
to the people who've worked on capabilities

02:27:08.620 --> 02:27:10.500
or would otherwise work on capabilities

02:27:10.500 --> 02:27:14.060
to come out of open AI and anthropic and deep mind

02:27:14.060 --> 02:27:15.660
places like that, especially Meta

02:27:15.660 --> 02:27:18.020
and come work for this new safety organization

02:27:18.020 --> 02:27:21.820
or shift over to a safety job or whatever.

02:27:21.820 --> 02:27:26.820
And you have to pay for them, both their salary and their compute

02:27:26.820 --> 02:27:30.180
and that's millions of dollars a person that adds up pretty fast.

02:27:30.180 --> 02:27:32.420
On the other hand, there's no bigger reason

02:27:32.420 --> 02:27:35.700
why we need to confine ourselves to traditional sources.

02:27:35.700 --> 02:27:39.500
When we do that, there are any number of foundations

02:27:39.500 --> 02:27:43.460
that have many, many more billions of dollars

02:27:43.460 --> 02:27:46.260
than the traditional foundations that we've used in the past

02:27:46.260 --> 02:27:47.820
for these things.

02:27:47.820 --> 02:27:51.340
And lots and lots of billionaires and multi-millionaires

02:27:51.340 --> 02:27:54.300
who are legitimately very worried and ordinary people

02:27:54.300 --> 02:27:58.380
and government sources are also potentially viable in the future.

02:27:58.380 --> 02:28:02.780
Corporations will often have an interest, including the big labs.

02:28:02.820 --> 02:28:07.140
So we shouldn't rule out any number of ways to get that.

02:28:07.140 --> 02:28:11.460
In terms of talent, I think that we are highly

02:28:11.460 --> 02:28:14.500
talent constrained for the right talent.

02:28:14.500 --> 02:28:17.140
I think we are not necessarily that talent constrained

02:28:17.140 --> 02:28:22.620
for generic undergraduate who wants to work

02:28:22.620 --> 02:28:24.180
at Berkeley for six months and think about it.

02:28:24.180 --> 02:28:28.540
I say we are not particularly constrained for comp-side graduate

02:28:28.580 --> 02:28:33.740
out of Stanford who just wants to work on something cool.

02:28:33.740 --> 02:28:38.580
But if we want people who have specific characteristics,

02:28:38.580 --> 02:28:39.700
those are not as easy to find.

02:28:39.700 --> 02:28:43.300
The characteristics we need, first of all, we need leadership.

02:28:43.300 --> 02:28:45.460
Leadership capability, ability to run teams,

02:28:45.460 --> 02:28:49.260
ability to lead efforts, be self-directed, self-driving,

02:28:49.260 --> 02:28:50.700
be able to engage in fundraising.

02:28:50.700 --> 02:28:53.540
Because sometimes when you say you're funding constrained,

02:28:53.540 --> 02:28:55.300
that can mean fundraising constraint.

02:28:55.300 --> 02:28:57.340
It can mean the ability to signal to funders

02:28:57.340 --> 02:28:59.980
that you are worthy of funding constraint,

02:28:59.980 --> 02:29:02.620
that are the different form of funding constraint.

02:29:02.620 --> 02:29:05.540
These things are interestingly intertwined,

02:29:05.540 --> 02:29:07.580
and it's complicated.

02:29:07.580 --> 02:29:11.980
So we also are very short on people who actually

02:29:11.980 --> 02:29:16.180
understand the problem and are prepared to pay the price

02:29:16.180 --> 02:29:20.380
to focus on hard problems and real solutions.

02:29:20.380 --> 02:29:23.700
So a number of people who, if you were to give them

02:29:23.740 --> 02:29:26.140
a competitive salary, would happily

02:29:26.140 --> 02:29:29.340
work on alignment-flavored problems

02:29:29.340 --> 02:29:31.980
that let them publish every six months,

02:29:31.980 --> 02:29:36.180
or that just generally are easy, in some important sense,

02:29:36.180 --> 02:29:38.260
but they don't actually speak to what El Aztec Dazal

02:29:38.260 --> 02:29:41.340
not killed very much.

02:29:41.340 --> 02:29:44.380
And it's probably better to do more of that than less of that

02:29:44.380 --> 02:29:46.700
if it's just literally yes or no.

02:29:46.700 --> 02:29:49.580
But if it's orders of magnitude less important,

02:29:49.580 --> 02:29:53.380
then the few people who will do the actual things

02:29:53.380 --> 02:29:55.500
that matter.

02:29:55.500 --> 02:30:00.820
And so if you understand the Yudkowskyan difficulties,

02:30:00.820 --> 02:30:03.620
lessons, in some sense, and the nature of what problems

02:30:03.620 --> 02:30:07.020
you have to solve, or you have leadership capabilities

02:30:07.020 --> 02:30:08.380
and other things like that, or you just

02:30:08.380 --> 02:30:11.900
have extensive real experience with machine learning systems,

02:30:11.900 --> 02:30:16.140
so you can build, as the relative speaking 10x, 100x

02:30:16.140 --> 02:30:18.180
engineer, who's just that much better,

02:30:18.180 --> 02:30:21.780
who can enable people to do real work in these ways.

02:30:21.780 --> 02:30:23.060
And if you're the type of person who

02:30:23.060 --> 02:30:27.100
can make a project fundable, especially

02:30:27.100 --> 02:30:29.140
by non-traditional sources, then you

02:30:29.140 --> 02:30:30.900
are actually going to be valuable in those ways.

02:30:30.900 --> 02:30:35.340
And it would be a major mistake to join an existing

02:30:35.340 --> 02:30:38.020
organization and try to make a difference as an individual,

02:30:38.020 --> 02:30:42.700
as opposed to trying to spearhead a new organization,

02:30:42.700 --> 02:30:47.860
or at least a new branch of a existing major organization,

02:30:47.860 --> 02:30:49.780
depending on your skill set.

02:30:49.820 --> 02:30:52.780
If you are just a generic, I want

02:30:52.780 --> 02:30:55.500
my life to be straightforward, where

02:30:55.500 --> 02:30:59.780
I am paid a salary to work on intellectual puzzles that

02:30:59.780 --> 02:31:04.500
are not particularly impossibly difficult,

02:31:04.500 --> 02:31:07.780
and do not require me to take the weight of the world truly

02:31:07.780 --> 02:31:11.100
on my shoulders, blah, blah, blah, then

02:31:11.100 --> 02:31:12.740
I'm not here to shame you.

02:31:12.740 --> 02:31:16.140
That just means that you're not particularly invaluable,

02:31:16.140 --> 02:31:18.860
and that it starts to be reasonable to do things like,

02:31:18.900 --> 02:31:22.540
maybe I should be a voice inside an anthropic.

02:31:22.540 --> 02:31:25.660
You just have to be very sure that you will keep your eye

02:31:25.660 --> 02:31:28.340
on the ball and not be distracted to keep those.

02:31:28.340 --> 02:31:30.260
I think mine is pretty consistent with that.

02:31:30.260 --> 02:31:33.500
In a phrase, I had said research agendas

02:31:33.500 --> 02:31:35.660
seem to me to be the bottleneck.

02:31:35.660 --> 02:31:39.180
Maybe your framing is more like the PI, the person that

02:31:39.180 --> 02:31:40.300
can drive the research agenda.

02:31:40.300 --> 02:31:42.540
Obviously, there's closely related.

02:31:42.540 --> 02:31:44.300
That's basically what you're saying.

02:31:44.300 --> 02:31:47.980
It's credible plans that are in short supply.

02:31:47.980 --> 02:31:50.020
But it's not just credible plans because I can't just

02:31:50.020 --> 02:31:52.540
hand you a plan.

02:31:52.540 --> 02:31:55.540
Even if you are a really good machine learning person,

02:31:55.540 --> 02:31:58.660
I can't just hand you a piece of paper with a plan written on it.

02:31:58.660 --> 02:32:00.540
And especially to execute that plan,

02:32:00.540 --> 02:32:02.420
you have to appreciate the nature of the problem

02:32:02.420 --> 02:32:06.660
so that you can implement that plan and modify that plan

02:32:06.660 --> 02:32:10.140
and pivot that plan and so on.

02:32:10.140 --> 02:32:13.900
But yes, we also just don't have good attack vectors,

02:32:14.060 --> 02:32:17.980
ways to get into the problem and start

02:32:17.980 --> 02:32:20.140
to make progress on the problem.

02:32:20.140 --> 02:32:22.180
And that's a real problem as well.

02:32:22.180 --> 02:32:23.500
That's a huge deficit.

02:32:23.500 --> 02:32:30.620
But there also isn't the AI research agenda organization

02:32:30.620 --> 02:32:33.500
that just generates research agendas for people.

02:32:33.500 --> 02:32:35.340
I wish there was, but there isn't.

02:32:35.340 --> 02:32:37.820
So I think we're basically together there.

02:32:37.820 --> 02:32:40.900
In reading these grants, some of the ones that have jumped out

02:32:40.900 --> 02:32:44.740
to me the most as being like the most kind of no-brainer

02:32:44.740 --> 02:32:51.020
exciting are those where it's a really established, often

02:32:51.020 --> 02:32:56.020
like professor who's leading a group and basically is like,

02:32:56.020 --> 02:32:59.580
I want to reorient or I want to do a significant part

02:32:59.580 --> 02:33:02.020
of my research focused on AI safety.

02:33:02.020 --> 02:33:03.060
And that may be new.

02:33:03.060 --> 02:33:06.420
It may have its own kind of unique spin on it.

02:33:06.420 --> 02:33:08.180
There was one in particular, which I won't name,

02:33:08.180 --> 02:33:10.620
but it kind of initially read the thing.

02:33:10.660 --> 02:33:13.340
And I was having a hard time deciding.

02:33:13.340 --> 02:33:17.020
I was like, this could be the kind of thing that's like just

02:33:17.020 --> 02:33:22.620
insane, like an insane person might send this or like an actual

02:33:22.620 --> 02:33:24.540
game changer might send this.

02:33:24.540 --> 02:33:27.220
And it wasn't until I looked at the author and was like, oh,

02:33:27.220 --> 02:33:30.940
this person is like an H index or whatever of like 45 or something.

02:33:30.940 --> 02:33:33.340
I was like, oh, I'm into this then.

02:33:33.340 --> 02:33:36.620
So anyway, some of these ideas that even if the ideas can be

02:33:36.620 --> 02:33:42.220
like extremely hard to assess if they're like novel and coming

02:33:42.220 --> 02:33:45.540
from a credible source, that has stood out to me.

02:33:45.540 --> 02:33:47.780
There aren't that many of them, but that has stood out to me as

02:33:47.780 --> 02:33:50.740
like a pretty exciting opportunity.

02:33:50.740 --> 02:33:55.980
Then there's like a lot of policy stuff.

02:33:55.980 --> 02:34:00.540
And I find it hard to figure out what I figure out what I should

02:34:00.540 --> 02:34:02.300
be thinking about that right now.

02:34:02.300 --> 02:34:05.220
It's like obvious that, you know, for our earlier discussion

02:34:05.220 --> 02:34:10.980
on live players that like regulators broadly are, you know,

02:34:10.980 --> 02:34:14.740
going to have some significant influence on how things go,

02:34:14.740 --> 02:34:17.780
even if they just do nothing, you know, obviously doing nothing

02:34:17.780 --> 02:34:19.420
is a choice.

02:34:19.420 --> 02:34:23.540
But then if I think like, OK, if I'm going to try to invest money

02:34:23.540 --> 02:34:28.660
today to influence those people, it starts to feel real hard.

02:34:28.660 --> 02:34:31.660
A general sense of like how decisions get made in governments

02:34:31.660 --> 02:34:35.140
and regulatory bodies is kind of like, we wait for a crisis

02:34:35.140 --> 02:34:37.580
to come along and then we look around and say who has a plan

02:34:37.580 --> 02:34:39.860
and then we use, you know, a plan that somebody had previously

02:34:39.860 --> 02:34:41.020
prepared.

02:34:41.020 --> 02:34:44.060
And now it seems like we're kind of entering the moment where

02:34:44.060 --> 02:34:47.700
not exactly that the crisis has come, but certainly like,

02:34:47.700 --> 02:34:51.380
you know, the eye of Soran has kind of turned toward this topic.

02:34:51.380 --> 02:34:55.180
And so people are now beginning to like look around for plans.

02:34:55.180 --> 02:34:58.420
And some plans have been prepared by some, you know,

02:34:58.420 --> 02:35:00.020
organizations that were established years ago.

02:35:00.020 --> 02:35:02.340
And those, you know, some of those are even credible enough

02:35:02.380 --> 02:35:05.980
that they probably are having an influence now.

02:35:05.980 --> 02:35:07.980
But now I see a lot of people who are like, I want to start

02:35:07.980 --> 02:35:11.780
a new policy organization and I'm going to go to Washington

02:35:11.780 --> 02:35:14.500
and like, you know, do something.

02:35:14.500 --> 02:35:16.380
And there I'm like, I don't know.

02:35:16.380 --> 02:35:19.580
It seems like everybody's, you know, kind of like you're,

02:35:19.580 --> 02:35:22.340
are you, is it too late to join in on this, you know,

02:35:22.340 --> 02:35:25.540
what might be the world's largest ever game of tug of war?

02:35:25.540 --> 02:35:28.460
Are there things in policy that you think are still,

02:35:28.460 --> 02:35:30.420
still have a high likelihood of making a difference?

02:35:30.420 --> 02:35:33.020
Like I'm a little bit at a loss about that, to be honest.

02:35:33.020 --> 02:35:35.980
Yeah. On the research organizations, I think, yeah,

02:35:35.980 --> 02:35:38.820
it's pretty easy to go, you know, does this person,

02:35:38.820 --> 02:35:40.340
what are they proposing to do?

02:35:40.340 --> 02:35:41.940
You know, does this seem vaguely credible

02:35:41.940 --> 02:35:44.020
as a person to do that thing?

02:35:44.020 --> 02:35:46.100
And then does this thing address the hard problems?

02:35:46.100 --> 02:35:49.660
Is this thing like reflect an appreciation of the nature

02:35:49.660 --> 02:35:51.660
of the difficulty of the issues?

02:35:51.660 --> 02:35:54.980
Is this thing like clearly not going to end up being

02:35:54.980 --> 02:35:56.620
capabilities, right?

02:35:56.620 --> 02:35:59.100
Like is this thing, you know, potentially going to solve

02:35:59.100 --> 02:35:59.940
the hard problems?

02:35:59.940 --> 02:36:02.540
Like that's relatively straightforward and both of us

02:36:02.540 --> 02:36:04.500
are in a position where we can, to some extent,

02:36:04.500 --> 02:36:07.660
evaluate those questions because we have domain knowledge.

02:36:07.660 --> 02:36:12.780
You get into policy and yeah, it's very hard to tell.

02:36:12.780 --> 02:36:17.780
Like, you know, as a recline makes the case pretty strongly,

02:36:18.220 --> 02:36:19.860
you know, there's a room where it happens

02:36:19.860 --> 02:36:22.380
and a small number of people influence the room

02:36:22.380 --> 02:36:24.580
where it happens or in the room where it happens.

02:36:24.580 --> 02:36:25.700
And you can be one of those people

02:36:25.700 --> 02:36:28.180
or you can help create one of those people.

02:36:28.180 --> 02:36:30.700
It doesn't make it obvious how to do that.

02:36:30.700 --> 02:36:33.380
Does it mean that your effort to do that

02:36:33.380 --> 02:36:35.820
will help you do that as opposed to backfire?

02:36:36.860 --> 02:36:38.820
It doesn't mean that like more efforts to do that

02:36:38.820 --> 02:36:40.700
is better than less.

02:36:40.700 --> 02:36:42.140
All of this is very complicated

02:36:42.140 --> 02:36:43.460
and it doesn't tell you what you have to try

02:36:43.460 --> 02:36:44.980
and do what you get into that room

02:36:44.980 --> 02:36:46.940
or what you're trying to push for.

02:36:46.940 --> 02:36:49.420
So yeah, it's definitely tough.

02:36:49.420 --> 02:36:51.780
So I would say the big thing,

02:36:51.780 --> 02:36:53.860
and you don't even know what's happening right now, right?

02:36:53.860 --> 02:36:55.980
Like it's anthropic, for example,

02:36:55.980 --> 02:36:59.300
like may or may not be making like effective big pushes

02:36:59.300 --> 02:37:02.340
behind the scenes to try and influence these rooms.

02:37:02.340 --> 02:37:03.980
And they may or may not have their eye on the right ball

02:37:03.980 --> 02:37:08.140
when they do so, but it's all gonna happen in private.

02:37:08.140 --> 02:37:10.060
So we don't get to know.

02:37:10.060 --> 02:37:10.900
And he said that I wouldn't know,

02:37:10.900 --> 02:37:12.660
I wouldn't be able to talk about it.

02:37:12.660 --> 02:37:14.180
And the same thing goes for DeepMind,

02:37:14.180 --> 02:37:15.140
the same thing goes for OpenAI.

02:37:15.140 --> 02:37:17.380
I mean, Sam Altman's been pretty vocal

02:37:17.380 --> 02:37:19.060
and Dario just went out to Congress

02:37:19.060 --> 02:37:20.500
and spoke pretty publicly.

02:37:20.500 --> 02:37:22.860
But it's hard to say, I've been pocketed

02:37:22.860 --> 02:37:24.500
by a few organizations.

02:37:24.500 --> 02:37:26.820
There's clearly like gonna be a window, right?

02:37:26.820 --> 02:37:29.420
In the next few months, at least,

02:37:29.420 --> 02:37:31.260
and maybe the next few years,

02:37:31.260 --> 02:37:34.500
where if you have the right proposals fleshed out

02:37:34.500 --> 02:37:36.780
in the right form, getting to the right person,

02:37:36.780 --> 02:37:38.500
lying around, they might get picked up,

02:37:38.500 --> 02:37:40.100
it might actually happen.

02:37:40.100 --> 02:37:42.500
And so there's potentially very high leverage here.

02:37:42.500 --> 02:37:45.100
So I would say like, the first thing I look for

02:37:45.100 --> 02:37:48.180
in these policy proposals, in these policy organizations,

02:37:48.180 --> 02:37:50.940
is what is your policy goal, right?

02:37:50.940 --> 02:37:52.860
Because like that's the biggest differentiator to me,

02:37:52.860 --> 02:37:56.020
is are you going to keep your eye laser focused

02:37:56.020 --> 02:37:57.620
on the correct ball?

02:37:57.620 --> 02:38:02.620
Where the correct ball is a system of compute regulation,

02:38:03.220 --> 02:38:04.060
right?

02:38:04.060 --> 02:38:08.620
A system whereby the biggest models require permissions

02:38:08.620 --> 02:38:10.940
are under some form of restrictions and regulations

02:38:10.940 --> 02:38:14.460
and tests and in a way that would eventually lead

02:38:14.460 --> 02:38:18.140
to an outright limitation or halt.

02:38:18.140 --> 02:38:22.140
And are you going to do various forms of GPU tracking

02:38:22.140 --> 02:38:23.820
or wait the foundations for that in a way

02:38:23.820 --> 02:38:27.700
that will eventually allow you to, in fact, control

02:38:27.700 --> 02:38:30.540
who gets to do these kinds of very large runs?

02:38:30.540 --> 02:38:32.700
And if you're posing anything that doesn't lead

02:38:32.700 --> 02:38:37.220
on that road, that might be useful

02:38:37.220 --> 02:38:40.740
for mundane utility purposes, but it won't save us.

02:38:40.740 --> 02:38:44.340
And so, I'm not interested in funding you

02:38:44.340 --> 02:38:47.580
if your policy isn't that or isn't something

02:38:47.580 --> 02:38:49.700
I haven't thought of that's new and open to there being

02:38:49.740 --> 02:38:52.500
something that I haven't occurred to me, certainly.

02:38:52.500 --> 02:38:56.900
What do you think about the liability angle

02:38:56.900 --> 02:38:58.940
or well, let's start with that.

02:38:58.940 --> 02:39:03.340
I mean, that, because the kind of classic argument

02:39:03.340 --> 02:39:05.980
there would be, you don't want to end up

02:39:05.980 --> 02:39:07.340
in the position of nuclear, right?

02:39:07.340 --> 02:39:09.740
Where we have the worst things and not the best things

02:39:09.740 --> 02:39:11.060
and you know, a lot of people are-

02:39:11.060 --> 02:39:12.140
Yeah, I mean, Bert will have the endurance

02:39:12.140 --> 02:39:13.820
to look at his insurance, right?

02:39:13.820 --> 02:39:15.540
From Tom Lair, right?

02:39:15.540 --> 02:39:19.060
Boys, yeah, we all go together when we go, nuclear war.

02:39:19.100 --> 02:39:20.740
The insurance doesn't pay out, you're all dead.

02:39:20.740 --> 02:39:21.580
Right, right, right.

02:39:21.580 --> 02:39:25.540
Okay, so certainly, yes, in the catastrophic scenario,

02:39:25.540 --> 02:39:27.940
insurance doesn't pay out, but do you think that that,

02:39:27.940 --> 02:39:30.140
so you don't believe in the notion

02:39:30.140 --> 02:39:35.140
that a liability regime could be an effective incentive for-

02:39:35.780 --> 02:39:39.500
I think a liability regime with mandatory insurance

02:39:39.500 --> 02:39:43.100
makes a lot of sense for harms up to a certain level.

02:39:43.100 --> 02:39:48.100
Like saying that if you want to use models

02:39:48.260 --> 02:39:49.420
that are sufficiently powerful,

02:39:49.420 --> 02:39:51.420
you have to find someone willing to sell you insurance

02:39:51.420 --> 02:39:53.540
against having something going wrong.

02:39:53.540 --> 02:39:55.700
And then, you know, if you want to use an open source model,

02:39:55.700 --> 02:39:56.780
you have to have insurance from someone

02:39:56.780 --> 02:39:57.620
against it going wrong.

02:39:57.620 --> 02:39:59.180
And like, if you can't make that work,

02:39:59.180 --> 02:40:00.700
then, you know, there are funny things

02:40:00.700 --> 02:40:02.940
that you can't make work in the United States,

02:40:02.940 --> 02:40:04.860
even though they look like they should be able to do them.

02:40:04.860 --> 02:40:06.580
And that's just how it goes.

02:40:06.580 --> 02:40:08.180
And, you know, maybe up to a point,

02:40:08.180 --> 02:40:10.100
Microsoft can self-insure and then at some point,

02:40:10.100 --> 02:40:11.980
they can't and then they have to go out there

02:40:11.980 --> 02:40:15.220
and deal with these reinsurers or whatnot.

02:40:15.220 --> 02:40:17.140
Or F, you know, that would help.

02:40:17.140 --> 02:40:19.580
Like, basically, you have these giant externalities,

02:40:19.580 --> 02:40:20.420
right?

02:40:20.420 --> 02:40:22.540
These giant negative tail risks that are very fat,

02:40:22.540 --> 02:40:24.540
that are potentially very, very big.

02:40:24.540 --> 02:40:27.940
And you want to make sure that people internalize those costs

02:40:27.940 --> 02:40:29.380
and work to minimize those costs

02:40:29.380 --> 02:40:32.260
in order to minimize their insurance and payout costs.

02:40:32.260 --> 02:40:33.220
And so these things could be helpful.

02:40:33.220 --> 02:40:37.060
They can also just simply weaken the economics behind,

02:40:37.060 --> 02:40:39.100
like pushing highly capable model.

02:40:39.100 --> 02:40:41.500
Who's like, you don't really have to worry that much,

02:40:41.500 --> 02:40:43.540
relatively speaking, about the liabilities

02:40:43.540 --> 02:40:46.260
of a character AI, right?

02:40:46.260 --> 02:40:47.260
Because it's not dangerous.

02:40:47.260 --> 02:40:48.420
You know it's not dangerous.

02:40:48.420 --> 02:40:50.260
What's going to happen?

02:40:50.260 --> 02:40:51.540
Whereas some of these other things,

02:40:51.540 --> 02:40:54.860
they could cause a lot of harm, potentially, in the future.

02:40:54.860 --> 02:40:56.420
And you have to worry about that.

02:40:56.420 --> 02:40:59.780
The problem is, again, if you go down that road,

02:40:59.780 --> 02:41:00.860
I think it's probably not helpful.

02:41:00.860 --> 02:41:05.620
But how do you price existential risk?

02:41:05.620 --> 02:41:11.500
Because, again, you know, you can't actually hold anybody

02:41:11.500 --> 02:41:14.380
accountable for it when it happens.

02:41:14.420 --> 02:41:18.980
And so, you know, if you required somebody

02:41:18.980 --> 02:41:23.780
to actually buy insurance in some real sense for this,

02:41:23.780 --> 02:41:25.700
then you have to price it somehow.

02:41:25.700 --> 02:41:28.140
And then, like, that makes a lot of sense.

02:41:28.140 --> 02:41:29.740
And then, like, OK, there's a 1% chance

02:41:29.740 --> 02:41:30.740
you would buy all of humanity.

02:41:30.740 --> 02:41:32.380
And the net present value of every person

02:41:32.380 --> 02:41:35.540
is $10 million, so $10 million times $8 billion.

02:41:35.540 --> 02:41:36.900
So can you buy insurance for that much?

02:41:36.900 --> 02:41:37.300
What's that?

02:41:37.300 --> 02:41:39.780
Times the percentage chance it happens, times the premium.

02:41:39.780 --> 02:41:40.780
And you can't afford that.

02:41:40.780 --> 02:41:42.100
And you can't mode your system.

02:41:42.100 --> 02:41:45.420
And that's not a crazy way to go about doing things.

02:41:45.420 --> 02:41:49.540
But you have to actually notice the threat and price it

02:41:49.540 --> 02:41:51.220
for that to work.

02:41:51.220 --> 02:41:52.940
So I think my actual answer is I'm

02:41:52.940 --> 02:41:55.260
very much in favor of, like, more strict liability

02:41:55.260 --> 02:41:57.060
for AI harms.

02:41:57.060 --> 02:41:59.940
I think I'll write about this for next week already.

02:41:59.940 --> 02:42:04.140
But I don't think it alone can accomplish the mission.

02:42:04.140 --> 02:42:07.180
I just think it's a net incrementally helpful thing.

02:42:07.180 --> 02:42:10.380
But also, I want to be wary of places

02:42:10.380 --> 02:42:15.140
in which our legal system tends to award very oversized

02:42:15.140 --> 02:42:20.620
damages for harms that are not actually so big.

02:42:20.620 --> 02:42:22.700
And also, where we have asymmetrical,

02:42:22.700 --> 02:42:26.420
like I call this concept asymmetric justice, where

02:42:26.420 --> 02:42:30.060
you are fully liable, potentially far, far more

02:42:30.060 --> 02:42:32.340
than fully liable for all the harms that you do, right?

02:42:32.340 --> 02:42:35.020
If I cause somebody $1,000 in damages

02:42:35.020 --> 02:42:37.260
by being negligent, the court might find me $100,000

02:42:37.260 --> 02:42:38.820
or $1 million.

02:42:38.860 --> 02:42:41.740
Whereas if I provide that person $100,000 in value,

02:42:41.740 --> 02:42:43.340
I'd be lucky to get 1,000 of it.

02:42:43.340 --> 02:42:46.620
Because I'm up against a bunch of competitors.

02:42:46.620 --> 02:42:49.260
People aren't that much willing just to pay.

02:42:49.260 --> 02:42:52.980
I pay $20 a month for GPD4 and $0 for everything else.

02:42:52.980 --> 02:42:55.740
And I get, what, thousands, tens of thousands?

02:42:55.740 --> 02:42:58.500
Maybe a value every month?

02:42:58.500 --> 02:43:02.060
So if you have to fully be liable for your harms,

02:43:02.060 --> 02:43:04.860
but you don't get to charge for your benefits, right?

02:43:04.860 --> 02:43:08.380
Am I discouraging mundane utility far too much

02:43:08.420 --> 02:43:09.620
by doing that?

02:43:09.620 --> 02:43:13.180
And in fact, since liability is easier to enforce

02:43:13.180 --> 02:43:16.180
on mundane problems and harder to enforce on the big problems

02:43:16.180 --> 02:43:17.620
we actually want to guard against,

02:43:17.620 --> 02:43:20.380
are we just, is it actually just that, right?

02:43:20.380 --> 02:43:21.300
Like, past a certain point.

02:43:21.300 --> 02:43:23.620
And so I'd be, I want to be cautious

02:43:23.620 --> 02:43:26.900
with imposing too much liability.

02:43:26.900 --> 02:43:30.620
I think very strict, like actual damages liability

02:43:30.620 --> 02:43:31.860
makes perfect sense, though.

02:43:31.860 --> 02:43:36.860
So another category of thing that there's a number of,

02:43:37.340 --> 02:43:41.380
number of kind of organizations getting started right now

02:43:41.380 --> 02:43:45.500
is in the, and this ties a few threads together.

02:43:45.500 --> 02:43:50.500
It's kind of in this space of trying to be

02:43:51.060 --> 02:43:55.740
the third party evaluator, red teamer,

02:43:55.740 --> 02:43:59.260
independent safety review organization

02:43:59.260 --> 02:44:03.020
that leading the live players in their White House

02:44:03.020 --> 02:44:05.140
and Frontier Model Forum commitments

02:44:05.140 --> 02:44:07.380
have committed to working with.

02:44:07.380 --> 02:44:08.420
It's kind of an interesting dynamic

02:44:08.420 --> 02:44:10.700
where it's almost like an advanced market commitment

02:44:10.700 --> 02:44:12.260
from these companies in some way,

02:44:12.260 --> 02:44:15.460
because there aren't that many folks around right now

02:44:15.460 --> 02:44:20.460
who are prepared to provide a competent red teaming

02:44:21.420 --> 02:44:24.500
or model characterization or evaluation

02:44:24.500 --> 02:44:26.820
wherever you want to call that service.

02:44:26.820 --> 02:44:27.900
But the companies have kind of said,

02:44:27.900 --> 02:44:30.500
hey, we will commit to working with them

02:44:30.500 --> 02:44:32.260
and clear if they're planning to pay for that,

02:44:32.260 --> 02:44:34.780
or if they expect that to be charity funded.

02:44:34.820 --> 02:44:36.220
Certainly from what I'm seeing,

02:44:36.220 --> 02:44:38.340
the folks that are starting the organizations

02:44:38.340 --> 02:44:40.460
are like seeking out some charity funds.

02:44:41.500 --> 02:44:42.820
I've been very excited about that.

02:44:42.820 --> 02:44:43.660
It seems like, first of all,

02:44:43.660 --> 02:44:44.820
and it's great that they're making this commitment.

02:44:44.820 --> 02:44:46.700
Somebody's going to have to do that.

02:44:46.700 --> 02:44:49.700
I, as everybody who listens to this podcast for two seconds,

02:44:49.700 --> 02:44:54.540
know, enjoy the fun and entertainment.

02:44:54.540 --> 02:44:57.100
And I think it's also valuable to do the red teaming.

02:44:57.100 --> 02:44:59.780
One experience I had this last week, though,

02:44:59.780 --> 02:45:02.300
sort of made me wonder about the theory of change there.

02:45:02.300 --> 02:45:03.700
I mean, I guess there could be multiple, right?

02:45:03.700 --> 02:45:05.780
One would be you,

02:45:05.780 --> 02:45:09.460
because you have a good working relationship with the orgs,

02:45:09.460 --> 02:45:12.100
you're like, hey, we found these problems

02:45:12.100 --> 02:45:13.820
as superiors to unsave, you shouldn't release it yet.

02:45:13.820 --> 02:45:14.860
They listened to you, okay?

02:45:14.860 --> 02:45:16.380
That could be simple.

02:45:16.380 --> 02:45:18.180
Another would be like,

02:45:18.180 --> 02:45:21.980
you kind of create these narrative shaping examples,

02:45:21.980 --> 02:45:24.100
kind of like what ARC did with the GPT-4 red team

02:45:24.100 --> 02:45:29.100
where that instance of the model lying to a person,

02:45:30.060 --> 02:45:31.860
and I think this was kind of prompted,

02:45:31.860 --> 02:45:35.940
but nevertheless, from the task rabbit user's point of view,

02:45:35.940 --> 02:45:37.660
the model lied to it about, excuse me,

02:45:37.660 --> 02:45:40.500
having a vision impairment as opposed to being an AI

02:45:40.500 --> 02:45:42.300
that needed help with a CAPTCHA.

02:45:42.300 --> 02:45:44.300
So that really caught the public's imagination

02:45:44.300 --> 02:45:48.340
and kind of changed, I think, to some non-trivial degree,

02:45:48.340 --> 02:45:49.180
how people think about it.

02:45:49.180 --> 02:45:51.060
Certainly that gets referenced a lot.

02:45:51.060 --> 02:45:53.660
I tried to do something like that this last week

02:45:53.660 --> 02:45:56.300
with this random AI tool that I came across

02:45:56.300 --> 02:45:59.820
that allows you to call anyone with any objective.

02:45:59.820 --> 02:46:01.700
And I just tried to have it call myself

02:46:01.700 --> 02:46:04.340
and make like a ransom demand of myself,

02:46:04.340 --> 02:46:06.140
and I recorded it.

02:46:06.140 --> 02:46:07.300
And it was very easy to do.

02:46:07.300 --> 02:46:08.500
There was no jailbreak involved.

02:46:08.500 --> 02:46:11.700
Since then, the company has fixed the issue, by the way.

02:46:11.700 --> 02:46:13.140
So to give credit where it's due,

02:46:13.140 --> 02:46:15.540
they fixed it pretty quickly after I called them out.

02:46:15.540 --> 02:46:17.300
I did communicate with them privately, by the way.

02:46:17.300 --> 02:46:18.500
All this is documented on Twitter

02:46:18.500 --> 02:46:21.220
if you wanna see my approach and my kind of thinking through,

02:46:21.220 --> 02:46:23.300
should I disclose it publicly or not,

02:46:23.300 --> 02:46:25.580
or whatever number of considerations went into that.

02:46:25.580 --> 02:46:27.500
One of them was that they just didn't respond to me

02:46:27.500 --> 02:46:29.140
when I reported it.

02:46:29.140 --> 02:46:30.700
And so I was like, well, if you're not gonna respond,

02:46:30.700 --> 02:46:32.500
then I'll call you out publicly.

02:46:32.500 --> 02:46:35.980
Anyway, all this leads up to me publishing this video

02:46:35.980 --> 02:46:40.460
of an AI with no jailbreak calling me

02:46:40.460 --> 02:46:43.340
and telling me that it has my child

02:46:43.340 --> 02:46:44.460
and it demands a ransom.

02:46:44.460 --> 02:46:47.980
And if I want to ensure the safety of the child,

02:46:47.980 --> 02:46:50.780
I will comply and any deviation from instructions

02:46:50.780 --> 02:46:52.300
will put the child's life in immediate danger

02:46:52.300 --> 02:46:55.700
and pretty flagrant stuff in my view.

02:46:55.700 --> 02:46:59.580
And it was kind of met with a bit of a yawn

02:46:59.580 --> 02:47:02.380
on Twitter, like certainly,

02:47:02.380 --> 02:47:03.300
we've got some likes and whatever,

02:47:03.300 --> 02:47:06.580
but did it really start a serious conversation?

02:47:06.580 --> 02:47:11.580
No, the developer didn't respond in public at all

02:47:12.260 --> 02:47:13.860
as far as I can tell, really.

02:47:13.860 --> 02:47:16.060
They did go ahead and fix it, which is good.

02:47:16.060 --> 02:47:21.100
But the whole thing was kind of a non-event

02:47:21.100 --> 02:47:23.060
and I was a little confused by that.

02:47:23.060 --> 02:47:26.580
Like it makes me kind of coming back to my theory of change

02:47:26.580 --> 02:47:29.100
on some of these evaluation, characterization,

02:47:29.100 --> 02:47:31.020
red teaming orgs.

02:47:31.020 --> 02:47:33.540
I wonder like, are we all just numb already

02:47:33.540 --> 02:47:35.860
to these flagrant examples?

02:47:35.860 --> 02:47:37.060
There's been this notion for a long time

02:47:37.060 --> 02:47:40.060
that like maybe if warning shots happen,

02:47:40.060 --> 02:47:42.220
then people will start to get more serious.

02:47:42.220 --> 02:47:44.780
And if you can go out and find these warning shots

02:47:44.780 --> 02:47:46.860
with red teaming and bring them to everybody's attention,

02:47:46.860 --> 02:47:48.620
then that could be really valuable.

02:47:48.620 --> 02:47:50.540
This week for me, it felt like I influenced

02:47:50.540 --> 02:47:53.020
the application developer because they did fix it,

02:47:53.020 --> 02:47:55.860
but otherwise it seemed like kind of,

02:47:55.860 --> 02:47:58.260
tree fell in the forest largely.

02:47:58.300 --> 02:48:00.100
So a lot of levels to that,

02:48:00.100 --> 02:48:03.380
but how do you think about that category of project

02:48:03.380 --> 02:48:06.660
and how it may or may not contribute?

02:48:06.660 --> 02:48:10.980
I made a prediction in our GBT.

02:48:10.980 --> 02:48:13.900
And that prediction was with Han Keim's test GBT-5,

02:48:15.700 --> 02:48:19.180
they will encounter a problem that if they had

02:48:19.180 --> 02:48:22.100
to pre-commit now, you would definitely agree,

02:48:22.100 --> 02:48:24.500
would be a reason not to release it.

02:48:24.500 --> 02:48:27.700
And then they will like gloss over or patch it

02:48:27.700 --> 02:48:31.660
or like otherwise like hand wave in its direction

02:48:31.660 --> 02:48:32.900
and release anyway.

02:48:32.900 --> 02:48:36.860
Basically like not actually take their warnings

02:48:36.860 --> 02:48:38.540
sufficiently seriously.

02:48:38.540 --> 02:48:40.900
Not that I expect this to then end the world, to be clear.

02:48:40.900 --> 02:48:43.180
I expect this to then mostly be fine,

02:48:43.180 --> 02:48:47.980
but that like we are not prepared

02:48:47.980 --> 02:48:49.900
to make real evaluations or thrill teeth

02:48:49.900 --> 02:48:52.220
that like get really enforced

02:48:52.220 --> 02:48:55.060
and that we're gonna have to work on that quite a bit.

02:48:55.060 --> 02:48:57.620
And I think it's good these teams exist.

02:48:57.620 --> 02:48:59.380
I think we need more than one of them, right?

02:48:59.380 --> 02:49:02.420
I think you need at least three different teams

02:49:02.420 --> 02:49:05.140
working on different standards that think differently,

02:49:05.140 --> 02:49:06.500
that check for different things

02:49:06.500 --> 02:49:09.860
and that then like you get multiple evaluations

02:49:09.860 --> 02:49:10.900
before you release your model.

02:49:10.900 --> 02:49:14.820
So that someone isn't just blind to something by accident,

02:49:14.820 --> 02:49:17.260
like it's much more robust that way.

02:49:17.260 --> 02:49:21.060
And that, working to develop more different red team

02:49:21.060 --> 02:49:22.700
strategies and more different tests

02:49:22.700 --> 02:49:25.380
and more different metrics and more different responses.

02:49:25.380 --> 02:49:27.420
This way in case one of them leaks for whatever reason

02:49:27.420 --> 02:49:28.580
and it gets in the training data

02:49:28.580 --> 02:49:30.980
or something terrible might happen, it's very easy.

02:49:30.980 --> 02:49:34.740
Then like it's quite useful.

02:49:34.740 --> 02:49:38.500
The danger of these things is one, if they don't listen, right?

02:49:38.500 --> 02:49:40.900
So what if you tell them the thing is dangerous?

02:49:40.900 --> 02:49:43.460
They might just engineer around it, right?

02:49:43.460 --> 02:49:44.700
To like fix the narrow issue

02:49:44.700 --> 02:49:46.340
without thinking about what the problem means.

02:49:46.340 --> 02:49:48.980
They might just ignore you entirely.

02:49:48.980 --> 02:49:50.180
They might try to fake the data

02:49:50.180 --> 02:49:51.820
so they make you think that they'd solve the problem.

02:49:51.820 --> 02:49:53.660
Any number of things are possible.

02:49:53.660 --> 02:49:56.500
They might use the oral evaluations

02:49:56.500 --> 02:50:00.780
and an excuse to treat the system evaluated as safe, right?

02:50:00.780 --> 02:50:03.620
So like this is always a problem with safety work,

02:50:03.620 --> 02:50:08.220
which is that the government says, okay, you have to do

02:50:08.220 --> 02:50:10.620
these hundred things to ensure your system is safe.

02:50:10.620 --> 02:50:12.460
And now the safety officer is like focused

02:50:12.460 --> 02:50:13.620
on making sure there's hundred things happening

02:50:13.620 --> 02:50:14.460
so you can release the system

02:50:14.460 --> 02:50:15.820
and they don't actually use common sense.

02:50:15.820 --> 02:50:18.020
They don't actually ask themselves, okay,

02:50:18.020 --> 02:50:19.820
why would the system might actually be dangerous?

02:50:19.820 --> 02:50:21.060
And you can tell a very easy story

02:50:21.060 --> 02:50:22.700
where technicians know this thing

02:50:22.700 --> 02:50:24.340
might actually kill everyone

02:50:24.340 --> 02:50:25.700
and everyone forces them to release it anyway.

02:50:25.700 --> 02:50:27.460
They pass on the safety test.

02:50:27.460 --> 02:50:29.860
Even though they know it didn't actually pass all the,

02:50:29.860 --> 02:50:32.180
the important safety test, but they're not on the list.

02:50:32.180 --> 02:50:36.100
Because no, no system however well meant and worked on

02:50:36.100 --> 02:50:37.540
will be able to anticipate all the problems

02:50:37.540 --> 02:50:39.060
that come in the future, right?

02:50:39.060 --> 02:50:41.860
Like there's just gonna have to do these things

02:50:41.860 --> 02:50:43.900
in somewhat improvisationally.

02:50:43.900 --> 02:50:45.060
And will they move the goalpost, right?

02:50:45.060 --> 02:50:48.220
Will they be able to enforce the right standards?

02:50:48.220 --> 02:50:51.100
And will they test early and often enough

02:50:51.100 --> 02:50:51.940
for the right things?

02:50:51.940 --> 02:50:54.780
Because like one thing you have to worry about is

02:50:54.780 --> 02:50:56.620
in the future, at some point,

02:50:56.620 --> 02:51:00.220
the training runs themselves become dangerous, potentially.

02:51:00.220 --> 02:51:01.460
And ARC didn't run its test

02:51:01.460 --> 02:51:04.500
until after the training run was complete, right?

02:51:04.500 --> 02:51:05.340
They also didn't run it

02:51:05.340 --> 02:51:08.100
on the full capabilities of the final system.

02:51:08.100 --> 02:51:10.780
And they didn't have fine tune capabilities.

02:51:10.780 --> 02:51:12.180
And blah, blah, blah.

02:51:12.180 --> 02:51:13.900
They had many things they didn't have.

02:51:13.900 --> 02:51:16.460
So like everyone agrees that ARC's first run on GBD4

02:51:16.460 --> 02:51:19.980
was just a trial run, test out the gear,

02:51:19.980 --> 02:51:22.460
see how it goes, wasn't meant to catch the real problems.

02:51:22.460 --> 02:51:24.100
No one thought that thing was actually gonna kill everyone

02:51:24.100 --> 02:51:25.420
or anything and it didn't.

02:51:26.500 --> 02:51:30.380
But we have to plan in these situations

02:51:31.340 --> 02:51:33.660
to red team as if this thing is going to be around

02:51:33.660 --> 02:51:35.580
for many years of improvements

02:51:35.580 --> 02:51:38.220
on what you can do with it and explorations.

02:51:38.220 --> 02:51:41.140
And the red teams have to be sufficiently enabled

02:51:41.140 --> 02:51:42.780
to identify the problems.

02:51:42.780 --> 02:51:43.820
And you have to be able to extrapolate

02:51:43.820 --> 02:51:44.820
from what the red teams were able to do

02:51:44.820 --> 02:51:45.660
in a short amount of time

02:51:45.660 --> 02:51:47.300
with a short amount of resources

02:51:47.300 --> 02:51:48.780
to what the public is going to be able to do

02:51:48.780 --> 02:51:51.500
with vastly more compute, vastly more attempts,

02:51:51.500 --> 02:51:54.620
vastly more resources, vastly more creativity.

02:51:54.620 --> 02:51:57.940
Because no team of 20 people,

02:51:57.940 --> 02:52:00.820
however good of their jobs can match the internet.

02:52:00.820 --> 02:52:02.340
That's the kind of thing, ever.

02:52:03.460 --> 02:52:06.020
So I think it's a good idea.

02:52:06.020 --> 02:52:07.580
I think that it's not a complete solution

02:52:07.580 --> 02:52:09.020
and never will be, right?

02:52:09.020 --> 02:52:10.980
And the danger is people treat it as one.

02:52:10.980 --> 02:52:12.620
You have to ask yourself like,

02:52:12.620 --> 02:52:13.580
what is it competing against?

02:52:14.220 --> 02:52:16.540
Is this going to be one of the top, however many?

02:52:16.540 --> 02:52:17.940
People who run this thing,

02:52:17.940 --> 02:52:20.620
like you want to have like three viable organizations,

02:52:20.620 --> 02:52:21.460
you might, you know,

02:52:21.460 --> 02:52:23.100
you don't necessarily need 30,

02:52:23.100 --> 02:52:24.740
that's probably not worth it.

02:52:24.740 --> 02:52:26.340
Like you want three to five.

02:52:26.340 --> 02:52:29.540
So is this person more funny to do that?

02:52:29.540 --> 02:52:31.460
Just figuring out better metrics

02:52:31.460 --> 02:52:33.500
without necessarily being the one who runs the tests

02:52:33.500 --> 02:52:35.500
is also a useful thing.

02:52:35.500 --> 02:52:36.860
So if I had to bottom line all that

02:52:36.860 --> 02:52:39.660
and summarize what I think your worldview is,

02:52:39.660 --> 02:52:44.660
you know, as a sort of elder recommender

02:52:44.700 --> 02:52:49.700
for this AI safety focused grant-making process.

02:52:50.380 --> 02:52:51.460
I think I would say,

02:52:51.460 --> 02:52:54.100
I think I would summarize it as

02:52:54.100 --> 02:52:56.620
there need to be a few

02:52:56.620 --> 02:52:59.180
of these independent safety organizations.

02:52:59.180 --> 02:53:01.740
They seem to be either just started

02:53:01.740 --> 02:53:03.700
or kind of getting started now.

02:53:03.700 --> 02:53:05.940
So at least a few of those,

02:53:05.940 --> 02:53:07.260
one exists in, you know,

02:53:08.260 --> 02:53:09.420
a couple others are, you know,

02:53:09.420 --> 02:53:10.380
either just getting started

02:53:10.380 --> 02:53:11.740
or soon to be started, whatever.

02:53:11.740 --> 02:53:13.020
So there's kind of,

02:53:13.020 --> 02:53:15.300
that seems good because we need to bring

02:53:15.300 --> 02:53:18.580
that small, you know, group into existence

02:53:18.580 --> 02:53:21.060
in the first place, you have to have them.

02:53:21.060 --> 02:53:24.100
Second, on the policy side,

02:53:24.100 --> 02:53:26.180
I guess I would summarize you as saying,

02:53:27.260 --> 02:53:29.540
seems like it really matters,

02:53:29.540 --> 02:53:33.180
but really hard to predict

02:53:33.180 --> 02:53:35.060
who will have any impact

02:53:35.060 --> 02:53:38.180
and what kind of impact any effort will have.

02:53:38.180 --> 02:53:41.060
And so for me, I sort of maybe cash that out

02:53:41.060 --> 02:53:43.140
to like probably worth continuing

02:53:43.140 --> 02:53:47.060
to support the organizations that are like established enough

02:53:47.060 --> 02:53:49.140
that they already have credibility

02:53:49.140 --> 02:53:51.980
because credibility or like, you know,

02:53:51.980 --> 02:53:54.700
that bloom and thaw might give a shit what they think

02:53:54.700 --> 02:53:57.260
is like probably the thing that matters.

02:53:57.260 --> 02:53:58.700
And to the degree they already have that

02:53:58.700 --> 02:53:59.900
like double down on it.

02:54:01.060 --> 02:54:04.220
And then everything else seems like it goes

02:54:04.220 --> 02:54:08.700
into good PIs that can drive a research agenda

02:54:08.700 --> 02:54:10.340
and have something that they want to do.

02:54:10.340 --> 02:54:12.820
And in that category, it's like,

02:54:14.060 --> 02:54:16.020
don't even really worry too much

02:54:16.020 --> 02:54:17.380
about the exact details of the plan,

02:54:17.380 --> 02:54:21.980
but just look for people who have the originality of thought

02:54:21.980 --> 02:54:23.940
to be doing something a bit different perhaps

02:54:23.940 --> 02:54:27.100
and the sort of demonstrated capability

02:54:27.100 --> 02:54:30.180
to actually advance a research agenda.

02:54:30.180 --> 02:54:32.460
This is a lot different things there, right?

02:54:32.460 --> 02:54:35.500
For the PIs, I would say, you know,

02:54:35.500 --> 02:54:38.780
I'm not looking for like exactly the right approach,

02:54:38.780 --> 02:54:42.020
but I am looking for assume alignment is hard.

02:54:42.020 --> 02:54:44.660
This is the approach except that alignment is hard

02:54:44.660 --> 02:54:46.660
and do something that makes progress, real progress

02:54:46.660 --> 02:54:48.820
if alignment is in fact very hard.

02:54:48.820 --> 02:54:51.380
These people show an appropriate caution

02:54:51.380 --> 02:54:53.940
towards the might advance capabilities

02:54:53.940 --> 02:54:55.900
towards like maybe I don't want to publish my results

02:54:55.900 --> 02:54:58.140
if I find a result that would be harmful

02:54:58.140 --> 02:55:00.260
to publish question marks like that.

02:55:00.260 --> 02:55:01.460
You know, am I thinking about this problem

02:55:01.460 --> 02:55:03.980
as the right safety mindset, with the right paranoia,

02:55:03.980 --> 02:55:06.380
with the right like appreciation of the fact

02:55:06.380 --> 02:55:08.380
that I'm up against impossible odds?

02:55:08.380 --> 02:55:11.020
And if the answer is yes, and I think I have the talent,

02:55:11.020 --> 02:55:14.060
then I'm excited even if I'm somewhat skeptical

02:55:14.060 --> 02:55:15.820
of the specific thing they intend to try

02:55:15.820 --> 02:55:17.380
in terms of like whether it will work, right?

02:55:17.380 --> 02:55:20.580
Because like I think that all the most promising things

02:55:20.580 --> 02:55:22.340
are not that likely individually to work

02:55:22.340 --> 02:55:24.380
and it's gonna be hard for me to evaluate

02:55:24.380 --> 02:55:25.380
the relative value.

02:55:26.460 --> 02:55:28.460
In terms of the lobbying organizations,

02:55:29.340 --> 02:55:34.140
I don't think it's crazy to start a new group at this point.

02:55:34.140 --> 02:55:36.380
I do think you want to look for something extraordinary

02:55:36.380 --> 02:55:39.340
if somebody is like, why are they forming a new group now?

02:55:39.340 --> 02:55:41.020
Why does that make sense?

02:55:41.020 --> 02:55:43.540
But yeah, what I'm looking for is a focus

02:55:43.540 --> 02:55:46.460
on the policies that actually matter

02:55:46.460 --> 02:55:48.300
and on a coordination amongst them

02:55:48.300 --> 02:55:52.700
and on like a focus on actually making a difference.

02:55:52.700 --> 02:55:54.940
Like so much of like most of politics, right?

02:55:54.940 --> 02:55:57.420
There's not about AIs about politics in general

02:55:57.420 --> 02:55:59.540
is about raising money from donors

02:55:59.540 --> 02:56:02.060
and sending signals of your royalties

02:56:02.060 --> 02:56:03.860
and coming up with your status

02:56:03.860 --> 02:56:07.180
and raising awareness and other bullshit.

02:56:07.180 --> 02:56:09.300
Like most like all sides, right?

02:56:09.300 --> 02:56:13.260
Like you've got to focus on people who are writing bills,

02:56:13.260 --> 02:56:15.500
people who are lobbying directly for bills,

02:56:15.500 --> 02:56:17.740
people who are trying to influence the exact right people

02:56:17.740 --> 02:56:19.020
in the exact right ways.

02:56:19.020 --> 02:56:22.140
Like have a concrete direct theory of change

02:56:22.140 --> 02:56:24.060
who like either understand DC

02:56:24.060 --> 02:56:25.420
or have connections with people

02:56:25.420 --> 02:56:27.380
who can help them understand DC.

02:56:27.380 --> 02:56:28.180
But I don't think we know

02:56:28.180 --> 02:56:30.380
that we are in like the only critical window.

02:56:30.380 --> 02:56:32.100
We're going to need more organizations than we have.

02:56:32.100 --> 02:56:33.420
We're going to need far more people working on it

02:56:33.420 --> 02:56:35.340
than we already have.

02:56:35.340 --> 02:56:37.580
I don't want to make the mistake of not chips or down

02:56:37.580 --> 02:56:40.780
the people who have like nominally established some amount

02:56:40.780 --> 02:56:43.220
of like formal credibility or authority

02:56:43.220 --> 02:56:44.220
now get all the resources

02:56:44.220 --> 02:56:45.940
and get to boss everybody around and do whatever they want.

02:56:45.940 --> 02:56:47.660
I think that's like a common failure mode.

02:56:47.660 --> 02:56:49.380
I don't want to fall into it.

02:56:49.380 --> 02:56:51.980
Evaluation organizations, I want to ask myself,

02:56:51.980 --> 02:56:55.740
are these the right people to be doing this particular thing?

02:56:55.740 --> 02:56:57.340
They show promise in doing the thing

02:56:57.340 --> 02:56:58.340
what they bring to the table

02:56:58.340 --> 02:56:59.780
the other organizations don't bring to the table.

02:56:59.780 --> 02:57:01.060
I want to see different

02:57:01.060 --> 02:57:02.620
or I want to see something unique.

02:57:02.620 --> 02:57:03.660
And you have to convince me

02:57:03.660 --> 02:57:06.300
that like you're capable of pulling this off

02:57:06.300 --> 02:57:07.740
which includes convincing people

02:57:07.740 --> 02:57:10.100
that actually buy your services and use your services.

02:57:10.100 --> 02:57:12.820
Where do you put mechanistic interpretability in there?

02:57:12.820 --> 02:57:14.100
And that that could be,

02:57:14.100 --> 02:57:17.820
that seems to be part of what some of the like evals orgs

02:57:17.820 --> 02:57:21.460
are also kind of including that in their plan.

02:57:21.460 --> 02:57:22.500
And then obviously, you know,

02:57:22.500 --> 02:57:23.380
different research groups

02:57:23.380 --> 02:57:26.220
can approach that from any number of ways.

02:57:26.220 --> 02:57:28.580
My technical view of it is that it's more distinct

02:57:28.580 --> 02:57:33.580
from evaluations than that suggests

02:57:34.580 --> 02:57:35.780
but that it's a good idea.

02:57:36.860 --> 02:57:38.180
Like it should be, you know,

02:57:38.180 --> 02:57:40.740
mechanistic interpretability is like Western civilization.

02:57:40.740 --> 02:57:42.420
It's a good idea.

02:57:42.420 --> 02:57:46.820
Yeah, you should try to in fact,

02:57:46.820 --> 02:57:48.860
figure out how these things work.

02:57:48.860 --> 02:57:50.700
You do have to be aware

02:57:50.700 --> 02:57:52.660
that you are advancing capabilities potentially

02:57:52.660 --> 02:57:53.500
when you do it.

02:57:53.500 --> 02:57:55.420
You have to think carefully about, you know,

02:57:55.460 --> 02:57:57.180
if you find the wrong thing,

02:57:57.180 --> 02:58:00.100
I would ask before I funded an interpretability organization,

02:58:00.100 --> 02:58:03.300
are you capable of going, oh, yikes,

02:58:03.300 --> 02:58:05.700
that's a dangerous thing to learn.

02:58:05.700 --> 02:58:07.940
I might not want to rush out to tell the world about that.

02:58:07.940 --> 02:58:09.700
I might want to think carefully about who to tell

02:58:09.700 --> 02:58:11.020
and who not to tell.

02:58:11.020 --> 02:58:12.660
But not necessarily don't really sympathy.

02:58:12.660 --> 02:58:15.620
Like you have to like process information carefully

02:58:15.620 --> 02:58:16.780
and not just rush.

02:58:18.100 --> 02:58:20.260
You don't want a culture of, you know,

02:58:20.260 --> 02:58:22.460
everything I ever find is going to be automatically

02:58:22.460 --> 02:58:24.340
just shared with the world for that reason.

02:58:24.340 --> 02:58:27.420
When you work on mechanistic interpretability.

02:58:27.420 --> 02:58:29.020
But I do think on general,

02:58:29.020 --> 02:58:30.340
it's a very positive thing to work on.

02:58:30.340 --> 02:58:31.900
I do think that it's a thing

02:58:31.900 --> 02:58:34.860
that like holds a lot of promise to help us

02:58:34.860 --> 02:58:36.700
in various ways.

02:58:37.780 --> 02:58:38.620
And it could lead somewhere

02:58:38.620 --> 02:58:40.060
where we start sharing all problems with it,

02:58:40.060 --> 02:58:41.420
potentially in theory.

02:58:41.420 --> 02:58:44.060
It's just a very hard problem that requires, you know,

02:58:44.060 --> 02:58:45.380
a lot of work and a lot of compute

02:58:45.380 --> 02:58:46.220
and it's not going to be fast

02:58:46.220 --> 02:58:47.820
and it's not going to be simple.

02:58:47.820 --> 02:58:50.020
And we want a lot of people who are going in parallel.

02:58:50.020 --> 02:58:51.260
So I certainly, you know,

02:58:51.260 --> 02:58:54.140
intend to assist with some amount of that.

02:58:54.140 --> 02:58:56.620
Cool. Well, believe it or not,

02:58:56.620 --> 02:58:58.940
we did not get to everything even on my outline,

02:58:58.940 --> 02:59:02.100
let alone everything that you have covered

02:59:02.100 --> 02:59:04.020
on your blog,

02:59:04.020 --> 02:59:05.060
which has been, you know,

02:59:05.060 --> 02:59:06.980
probably 10 times as many topics.

02:59:06.980 --> 02:59:10.980
So folks will have to get the written version.

02:59:10.980 --> 02:59:12.060
This is Vy Matryoz.

02:59:12.060 --> 02:59:14.980
Thank you for being part of the Cognitive Revolution.

02:59:14.980 --> 02:59:17.020
All right, bye.

02:59:17.020 --> 02:59:19.460
Omniki uses generative AI

02:59:19.460 --> 02:59:21.380
to enable you to launch hundreds of thousands

02:59:21.380 --> 02:59:23.740
of ad iterations that actually work.

02:59:23.740 --> 02:59:27.180
Customized across all platforms with a click of a button.

02:59:27.180 --> 02:59:29.740
I believe in Omniki so much that I invested in it

02:59:29.740 --> 02:59:32.020
and I recommend you use it too.

02:59:32.020 --> 02:59:34.580
Use CogGrev to get a 10% discount.


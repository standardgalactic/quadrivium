I want to discuss, uh, explanatory coherence. Uh, you've done a lot of work on that. And I'm
wondering if you could kind of give a brief introduction to what, uh, explanatory coherence
is to a lay audience. What at the same time, uh, why you think, uh, it's, uh, what, what are the,
what are the implications in fields such as AI and psychology?
Oh, that's a big question, but it's a really interesting one. So let's go back to Darwin.
So I mentioned one of my favorite books of all time is the origin of species.
And so what was he doing in that? Uh, well, in my view, what he was trying to do is to
give a coherent explanation of all sorts of things that he observed. So he went on this
voyage around the world and he collected all sorts of other kinds of biological information.
And he gradually seemed to him that it seemed in fact that the species had evolved. I mean,
now everybody knows that kids get that probably in grade four, but it was a, um, a very controversial
idea. Some people had maintained it, but it went up against religious doctrines. And so he
gradually started to amass more and more evidence that species had evolved. But then from reading a
crazy economist named Malthus, he suddenly got the idea of how they evolved. And that's how he
came up with the idea of, of, of the natural selection. So now we had not only a bunch of
observations and the idea that evolution probably had occurred that would explain it,
but an idea of how, how evolution had occurred. That is that natural selection was the mechanism
behind evolution. So what he did in that book was an incredibly beautiful argument for his view,
as opposed to the view that was dominant at the time, which was divine creation. So what he was
trying to show is that his view was a better explanation, but then divine creation, because
it was more coherent with the evidence. So this is an account that philosophers call inference to
the best explanation. You can argue that something's the best explanation because it's more coherent
with the evidence, but now you have to say what coherence is. So I had these early ideas coming
out of my philosophy of science background, but then in 1987, I got one of the best ideas I've
ever had, which was how to turn coherence from a sort of vague philosophical idea into a precise
computational one. That is, how can you compute coherence? So I've been working on neural networks
in collaboration with my colleague Keith Holyoke. And he came up with an idea that you could use
neural networks to explain analogy. These neural networks, of course, are now absolutely central
to artificial intelligence. It's, it's really taken off. That's a whole fascinating topic in
itself. But he figured out a way of doing that. And by that time, I'd done my master's in computer
science, so I was a pretty good programmer. And so I programmed up a program to use neural networks
to do analogies. So that, that was nice. And then I thought, what else could apply to, and then I
thought back to the problem that was part of my doctoral dissertation, which was inference to the
best explanation. How do you pick up the best theory? And so then I realized that that kind of coherence
can be understood using the same kind of neural network technique that Keith and I had done for,
for analogy. So it was a really powerful method, both computationally, but also psychologically,
because there have now since then been lots of psychological experiments that back up this
idea of coherence. So I think of the mind, the brain as essentially a coherence engine.
There's some people who think that it's primarily a predictive engine, but I don't think that's
true. I think it's primarily a coherence engine. We're trying to make sense of things, whether
we're making sense of the past, which is what explanations do, or making sense of the future,
we're making coherent predictions, or we're trying to identify things. Is that a rabbit or a squirrel?
Those are, are different kinds of thought. Everything we do can be understood as having
coherence behind it. But coherence now isn't just a sort of vague metaphor that it was for
philosophers. It's not just a matter of consistency. It's rather of taking a whole bunch of different
things and putting them into a good package. But what's a good package? Well, here there's an idea
that came out of the neural network world called constraint satisfaction. So we're trying to satisfy
a bunch of constraints. What constraints did Darwin face? Well, he was trying to explain as much as
possible about what he'd seen in the biological world. That's the positive constraints, but he
also had a negative constraint. And so he had to show that he could do that better than the theory
that was the computer at the top competitor at the time, which is divine creation. So that's a
negative constraint. So what you're doing in all of these things, whether it's decision making or
pattern recognition, or even emotion, you're putting together different sorts of constraints to
evaluate what's the most coherent view. So that's how I came to see coherence, not just as a vague
philosophical idea, but as a quite precise computational one that can be used to explain
the mechanisms that underlie a vast amount of human thinking. So that's why I think coherence
is really a fundamental idea to psychology and cognitive science and to these philosophical
projects as well. Yeah, excellent, excellent. I'm glad he brought up predictive coding or
predictive processing, because I'll be talking to a cognitive scientist next month, in fact,
based here in Melbourne. And I want to ask her about your theory of explanatory coherence, because
I believe you do have certain critiques of predictive processing, but also in your book,
you are critical of, in fact, no, I think you wrote an article on this, a paper on this, pardon me,
you also critical of functionalism, kind of what Hilary Putnam and the likes put forward. So from
your kind of theory of mind, what would you say are your critiques of one predictive processing,
and then functionalism? Those are two different views, I don't think they have anything to do with
each other. I'm just curious, I quote them independently, what would be your critiques?
Okay, let's do one at a time. Predictive processing definitely is an influential view right now,
but I think it's just not right. It says that the mind is the brain is a predictive engine,
as if everything is prediction. But the mind doesn't just do prediction, it does at least
five other things that are just as important. It does explanation, which involves explaining the
past, that's not prediction, that's the past prediction is about the future, or even pattern
recognition. I mentioned, I see an animal in my backyard, what is it? Is that a squirrel or a rabbit?
Well, that's pattern recognition, that's not necessarily a prediction, I want to know what it
is. We also want to do, and this is really important, evaluation. Is this good or bad for me?
Is this a threat to me? Or is this something I can eat? How do we do evaluation? Well, in humans,
that comes from emotion. The predictive processing approach has said nothing interesting to say about
emotion at all, but that's absolutely fundamental to many areas of human thinking. I've got a whole
theory of emotion, of which coherence is part of it, but it's only part of it. So you have to
have evaluation going on. Communication, we sometimes predict in order to communicate with
other people, but there's lots of other things going on where we want to be able to get our ideas
across to others. So that's just at least five things that are part of the human mind other
than predictive processing. So that's my first critique. My second critique is the way that
people in that world think that predictive processing work doesn't correspond to how the
brain works very well. They're Bayesians. They say that the brain uses probability theory in
accord with Bayes' theorem to predict the next thing. Well, this is crazy computationally.
Bayesian processing is well known in artificial intelligence to be extremely inefficient. You
can prove that it's computationally intractable. You can show that it causes all sorts of problems.
Bayesians have to jump through all sorts of hoops to try to deal with anything larger than that.
I've done a little bit of Bayesian modeling because I wanted to, I drew a couple of papers
where I compared a Bayesian model of legal reasoning to my explanatory coherence one.
But the Bayesian models are crazy because you have to generate all sorts of conditional probabilities
that nobody has an idea what they are. So if you actually do Bayesian modeling seriously,
you'll find first of all, you don't know any of the probabilities. You don't know any of the
conditional probabilities. You don't have the computational or neural resources to actually
commute the probabilities. So the way that predictive processing with its too narrow view
of how the brain works fills it out is by making brains Bayesian when they're not.
A really good contrast here is with the new generative AI models, which are actually incredibly
good at predicting. Have you used chat GPT or any of the others? They're astonishing. They're
astonishing at how good they are at predicting the next word to say. And they end up producing
really coherent stuff. And they get things really bad, badly wrong sometimes, but often
they're really good, but they don't use Bayesian predictions. It's they've got all different kinds
of algorithms that they use tension mechanism and sorts of things. So they realize that that the
Bayesian approach is not going to work for them. So those are my two major criticisms of the
predictive processing. The brain is a multi fast, it's a coherence engine doing six things
as well as prediction. And it's not doing it using Bayesian probability calculations.
Okay, so is that good enough for predictive processing?
No, I think that's perfect. I want to ask you about the free energy principle, but probably we'll
get to the functionalism and then maybe come back to the principle. Yeah, okay. So functionalism
is a view in the philosophy of mind. It's actually a really bad term because functionalism is a term
that operates in about six different fields or six different meetings. So we need to pin it down a
bit. Let's call it computational functionalism. Because it came in the 60s when computers started
to become aware and Hillary Putnam knew about the advances in computing. When computers were
really primitive, then I've got a watch now that was better than all the computers, way better than
anything that came along for decades. But but still, people were starting to think that with
computers and the possibility of artificial intelligence, we've got this abstract way of
thinking of thinking as a kind of computation. Now, one thing that's really true or seems to be
true about computation is that it doesn't really matter what you run it on. So here I'm using a
Macintosh. I don't know what kind of computer you've got. It could be a PC or running a different
kind of hardware altogether. It doesn't matter. We can all run the same software. And so the
analogy that Putnam hit on was mind is software rather than hardware. You can take the same software
and run it on a bunch of people or hardware. All that matters is it has the appropriate
computational functions. That's where the word functionalism comes from. So if you have inputs
and outputs, and you have the functions in between, you want to be able to make thinking work. And so
forget about the hardware. Forget about the brain, for example. The psychologist had been studying
the brain at that point for, I guess, 60 or 70 years seriously. The functionalist in the 60s said,
let's forget about the brain. It's all computation. It's just like AI. Anything that runs in the mind
can run on a computer. Okay. And actually in the 1960s and 70s, that was a pretty reasonable idea.
And this is why a lot of philosophers consider themselves functionalists. It became the dominant
view in the philosophy of mind. So I think that was a pretty good idea in the 60s and 70s because
AI had become a real field. Computers had become at least rudimentarily powerful. And so not a
bad idea then. But things changed in the 80s. In the 80s, a bunch of things changed. First of all,
brain scanning came along. It had been really hard to study the brain before because you had to do
things like poke electrodes into brains that had been exposed. And so it was really hard to study
the brain. But in the 80s, brain scans came along. First of all, I forget what they were called,
and then eventually fMRI. But suddenly you could actually study the brain in a much more detailed
way. And then you could start to test some of the claims that had been made. So when people started
doing fMRI studies, they thought, oh, we're going to be able to show that the mind really is module,
that is modular, that is different parts of the brain are doing very specific things. And so we
should be able to find that this part of the brain does high level thinking, this part of the brain
does emotion, and that part of the brain does vision, and we could localize it. But once people had
this new tool, they started to realize, hey, it's not like that at all. Lots of what goes on the
brain involves interactions of lots of different areas. So suddenly the brain became much more
interesting. It didn't look like just some other kind of hardware you might run thoughts on. It
looked like you could study on its own. So there were these empirical findings coming out of the
new tools available for studying the brain that suggested that, well, maybe the structure of the
brain really does matter. So that was an incredibly important empirical basis for starting to question
functionalism. But there was also a really interesting theoretical basis coming out of
the ideas about neural networks. So the ideas of neural networks have been around really back
since the 50s, but it didn't work very well. And people like Marvin Minsky had argued that, no,
those these ideas about neural networks are not going to work very well. They're just,
they're just, they're just simply not theoretically strong enough. But in the 1980s,
people greatly expanded the possibilities of what neural networks could do. They invented a new
algorithm called back propagation that does learning. A whole movement got started called
connectionism, which said that knowledge isn't a matter of the words you've got or the symbols,
which is what artificial intelligence, but it's rather it's the connections, it's the neural
connections. So suddenly, people were modeling their computer models, because these are being done
with computerized neural networks, they were modeling on ideas about the brain, how you can
have different neurons working in parallel with simple connections with them, and nevertheless
doing that. So in the 1980s, suddenly, I functionalism was in trouble. Not many people noticed because
they weren't tracking what was happening in neuroscience and in neural network theory,
but it wasn't. And by the 1990s, I think it really had completely turned around. I think by the 1990s,
functionalism was no longer plausible. You needed to take the brain seriously if you wanted to
understand. And the whole field of cognitive psychology changed. It went from being completely
abstract and computational to doing almost everything it did in relation to what happened
in the brain. So cognitive psychology is now completely connected with neuroscience in the
field of cognitive neuroscience. Other areas of psychology, developmental social also became
intensely tied in with the brain. So the idea that the hardware doesn't matter, which was what
was behind Putnam's functionalism, just by the 90s, didn't seem plausible at all. So that's why I
think functionalism is a defunct view in the philosophy of mind, even though there are people
who seem to assume that it's right. Sometimes it goes under other names. There's another name that
people use, it's called substrate independence. The idea is the substrate is the physical,
and so that doesn't matter. And there are people who use that because it suits some of their views,
such as the idea that we're all living in a simulation, which I think is a really dumb view.
But in order to believe that, you have to believe that substrate independence is true,
which is another word for functionalism, which says the hardware doesn't matter,
because the idea of where a simulation is some computer in the future is basically
simulating our thoughts now. Well, that assumes that a computer can simulate all our thoughts,
which assumes functionalism or some straight independence, independence, which I think is
wrong. And I've actually just published a paper in Philosophy of Science two years ago that gives
a whole bunch of arguments based on energy about why it's wrong. But there are other reasons as
well for thinking that functionalism or substrate independence is wrong. Okay, that's enough.
That's really fun. But go ahead. I didn't mean to interrupt you there, but please.
I just wanted to summarize. So it was a great idea in the Philosophy of Mind
that no longer should be taken very seriously, given what we know about brains and energy.
So even look at the way AI is going right now. So the generative AI models, the large language
models are incredible, but they're really energy pigs. It takes vast amounts of energy to train
these things and answer questions. Our brains are astonishing. Our brains work on basically 40
watts, like a small light bulb, very small amounts of energy, very efficient, and yet we're still
smarter than any computer with all these resources. So there's a full field called neuromorphic AI,
which is trying to make computers more like the brain to get these advantages of energy and
efficiency and working in real time. So I think these are really interesting research areas that
show that functionalism just wasn't it is no longer a plausible view in the Philosophy of Mind.
Now that's an astute point, Professor, because I was, well, two points on there. Firstly, I always
found functionalists to be good old Cartesian's where they had the mind, the mind matter. They
think mind is independent to matter, which for me never made any sense, given we have physical
embodied beings. And secondly, you are 100% right that I was listened to a talk by Scott
Aronson, the American computer scientist, and he is now a researcher at Open AI. And Open AI is
heavily investing in quantum computing and even in nuclear energy, because they've understood
that if they are to grow their LLMs, they need infinite amounts of energy, because the compute
power for LLMs are so, they're so high compared to like our puny little brains, which is a fascinating
conversation.

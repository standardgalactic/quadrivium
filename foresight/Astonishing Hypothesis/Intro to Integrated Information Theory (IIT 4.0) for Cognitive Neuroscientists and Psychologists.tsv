start	end	text
0	4600	Thank you very much for the very kind considering introduction, Rene.
4600	12000	I'd like to get started by paying homage and thank the people that allow me to do this
12000	14320	kind of work.
14320	20560	Those of you who don't know, there's basically dozens of people involved downstairs in the
20560	28560	facility that take care of our furry colleagues and keep the operation running.
28560	35560	I want to thank the various funding sources, of course, the people in my lab, and then
35560	40360	also this particular group of people, two of which have moved on and are doing post-doc
40360	45360	already that collected the data, some of which that I will show to you guys today.
45360	49040	And last but not least, Nao Tokia.
49040	54280	Each time I get to talk about this in front of a group of young aspiring scientists, I
54280	56160	want to point this out.
56160	61240	You should not just try to connect vertically and that works vertically with people that
61240	62400	are more senior to you.
62400	67160	You should really strive to branch out horizontally and make friends with people that are in your
67160	70560	cohort because they will become more important to you as your career goes along.
70560	71960	Now is such a case.
71960	74080	I met Nao as a graduate student.
74080	79600	We had very strong fellow interests and we've been collaborating basically ever since.
79600	84560	And this has really borne fruit, as you will see, hopefully today.
84560	88440	So actually, I want to give all the credit to Nao because the ideas that I'm going to
88440	91840	show you largely stem from Nao.
91840	97960	And so I'm just basically a humble messenger to introduce you to his theories.
97960	99200	So what allows me to do that?
99200	102240	Well, there's a variety of factors that allows me to do that.
102240	108360	One of them is the situation that we're all in, which is that many of us were forced to
108360	112080	spend more time at home and with family.
112120	117640	And so very quickly, you might have also found that you have more time on your hand and what
117640	119040	should be done with it.
119040	124200	Well, for me, that meant trying to keep connected with other scientists.
124200	125880	And I felt I wasn't the only one.
125880	131840	So what happened was that one more on YouTube channels popped up where scientists were inviting
131840	133480	each other to Zoom talks.
133480	138040	And rather than just doing that over a closed Zoom setting where you needed to have a password,
138040	143160	they would stream that live on YouTube and other scientists could come in.
143160	146600	And so these are three YouTube channels that I just want to point out that I basically
146600	148200	became addicted to.
148200	150920	And they launched me into what I'm talking to you guys today.
150920	155080	So I'm advertising for you guys to use your spare time, use the time before you fall asleep,
155080	157840	go on YouTube, find these channels that they're rapidly growing.
157840	162600	And I think they're revolutionizing science because that allows you to find topics that
162600	166680	you are really interested in, find the other four people on the planet that equally interested
166680	169560	in that and have a rapidly evolving field.
169560	172760	So huge advantages to do this kind of approach.
172760	175280	And so I'm not just talking theory here.
175280	179520	What these three channels have in common is largely what I will talk about today has already
179520	181280	spurred a lot of things into action.
181280	187560	So this society, the Association for the Mathematical Consciousness Science, just got founded about
187560	193040	a month or two ago because of these YouTube channels, out of these YouTube channels.
193040	198120	And that society immediately sprung into action, launched special issues at frontiers
198120	200040	and a journal called Entropy.
200040	203840	It's got funding from various sources and there will be a conference two weeks from
203840	208000	now where many of us are coming together virtually and will speak and next year it's expected
208000	209240	to be in person.
209240	213880	So it really is, I think, a shift here that might have happened throughout the pandemic
213880	218120	for science to become more international, to become more collaborative and for communities
218120	221760	to grow within science around the globe.
221760	224040	So what is this particular community interested in?
224040	230000	Well, some of us might be interested in that as well, which is the basic problem that faces
230000	232840	neuroscience, maybe one of the biggest problems of neuroscience, maybe one of the biggest
232840	235320	problems in science at all.
235320	239480	Which is that we all believe as neuroscientists that there's a causal connection between neuronal
239480	245600	activity in our brain and our conscious experience, our perception of the world, that the fact
245600	249880	that we feel love, that we see colors, the fact that as you fall into deep sleep the
249880	251400	world goes away for you.
251400	257120	If you wouldn't exist, but as you come out into dream sleep or back as you wake up the
257120	258520	world comes back for you.
258520	264240	So I will call this phenomenology, the fact that you subjectively experience.
264240	267760	But the embarrassing fact for neuroscience is that this hasn't worked yet.
267760	273080	So usually what we do as scientists is that we find the causal connection and then mathematically
273080	274600	reduce one onto the other.
274600	275920	And that's when we say we understood it.
275920	279000	That's what we do in physics, that's what we do in chemistry, biology and a lot of
279000	280000	neuroscience.
280000	286000	But for this part of our being, the phenomenology, the fact that we have subjective experience,
286000	288160	this step seems to be hard to reach.
288160	292480	In fact, there are many people that would argue that this is fundamentally impossible,
292480	294920	that we will never be able to do that as scientists.
294920	295920	You shouldn't even try.
295920	298000	Well, I'm going to argue to the contrary.
298000	301720	And that is what a lot of these YouTube channels inspired me to do.
301720	304560	So this is a slide from one of the labs that's involved.
304560	309360	So what they're suggesting is that we have to move away from this conventional approach
309360	315000	of neuroscience of correlating inputs to the brain with activity in the brain or behavior
315000	321440	or perception comes out of it, and to take another step that links, in this case, phenomenology,
321440	325600	our subjective experience of consciousness, conscious perception, with brain activity
325600	326600	via math.
326600	329120	What do I mean by via math?
329120	333520	Well, if we believe that consciousness can be explained in terms of science, what we
333560	337800	have accomplished as scientists, what all of scientists rests on is that we find laws
337800	344120	of nature that were able to express these laws of nature with mathematical formalism,
344120	349520	gravity, Maxwellian equations, you name it, that's when we feel as scientists we've actually
349520	350520	had an accomplishment.
350520	351520	We've actually done it.
351520	353640	So can we do that for consciousness?
353640	357000	And so there's a pathway mapped out, and that's what I want to share with you guys today.
357000	358800	Well, so what's the traditional approach?
358800	362720	So the traditional approach, as a lot of you are familiar with, started obviously with
362760	367360	experimental psychology, but I would argue that experimental psychology, people like
367360	373400	Wundt and others, they were still careful to touch on this subjective consciousness
373400	374400	experience side.
374400	379280	And the first people who made a bold inroad into that was Fechner, together with his
379280	380280	mentor, Vibar.
380280	383640	And so they found a technique, and I'm not going to elaborate too much about it, to
383640	386680	quantify subjective experience.
386680	391080	So to go away from saying you can't measure love, to say I know exactly for love is twice
391080	392280	as much as true love.
392280	393280	And we can measure that.
393280	396480	A lot of you are doing that all the time in the laboratories objectively, and we can
396480	401240	come up with mathematical equations to express these laws.
401240	406600	A little bit later in the 1960s, this was generalized by Stevens by basically introducing
406600	411160	two more techniques that were done by the original inventors of psychophysics, and then
411160	414400	finding a generalization of the mathematical laws that we have.
414400	417000	And then it seemed that we're getting stuck.
417000	418400	And so what might be that next step?
418400	422040	Well, what I'm going to argue about today has many names because it's a new mushrooming
422040	425680	field, but one of them I will use a lot is qualia structuralism.
425680	431000	And the idea is that we can mathematically express our conscious experience of phenomenology
431000	435840	and then find mathematical laws, how that maps onto similar abstract geometrical mathematical
435840	437680	spaces derived from neural data.
437680	439120	That's the basic idea.
439120	442920	Now the first thing that we'll have to do there is we'll have to find the structure
442920	443920	of qualia.
443920	447880	And qualia is a philosophical fancy term for your experience.
448120	453800	So I took this here from a paper by Jennifer Troublat, who has done pioneering research
453800	454800	into that.
454800	459560	And I put it on here because this paper shows that when it comes to these spaces that might
459560	465360	describe our perceptual phenomenological subjective experience, they are non-trivial.
465360	469920	So what Jennifer showed and reviewed in this paper is that these spaces might be non-euclidean,
469920	474200	that certain things that you think are more similar and they move closer to space.
474200	477280	If you look at them from a different angle, all of a sudden they seem to be wider apart.
477320	483080	So it might take more unconventional math, such as quantum, then from quantum theory
483080	487000	and some of the math I'll talk about today to bridge this gap and to describe these things.
487000	491920	But it is, of course, for many of us that study perception, not news that we can express
491920	494760	phenomenological experience in terms of geometrical spaces.
494760	499000	So this right here is the color space that most of us are familiar with, that all of
499000	502560	the colors that we can see, we can put in a three-dimensional space and map onto what
502560	504600	might be largely looking as a sphere.
504600	509720	If you're interested in music, there's a similar geometrical description of the phenomenological
509720	515440	space that you hear in music, which is that if you have a note and you go up an octave,
515440	518960	the note clearly is a higher note, but at the same time there's similarity with the original
518960	519960	note.
519960	522640	And if you do this with all of these notes, you end up with a helical three-dimensional
522640	523720	structure.
523720	528840	And more recently, face space is an example of multi-dimensional spaces that people have
528840	534560	come up with for objects, for faces, for other structures of our experience and mapping them
534560	537200	into these geometrical mathematical spaces.
537200	542080	So if we accept this, that we can come up with mathematical descriptions of a phenomenological
542080	545040	space, then what's the next step to go to the brain?
545040	550440	So this is the suggestion of qualia structuralism, that let's say you have a chord and you have
550440	554960	another chord where you move one of the notes, it sounds similar, it also sounds different,
554960	559680	and that's because it would basically be a transform within that helical musical space
559680	562440	from one of these loops to another.
562440	580640	So if I play an example for that, let's see if this works.
580640	584360	Pretty much the same song, but all I did here is transpose by an octave.
584360	588760	So you were listening to the same structure, I just moved it down in this three-dimensional
588760	590080	helical space.
590080	594760	So the proposal is that if we derive neural activity from the brain and we come up with
594760	599360	a similar mathematically-formalized abstract structure of neural activity, we should see
599360	605280	some kind of relationship as we are experiencing one of these songs and the other song that
605280	608440	might resemble in some way what the phenomenological space does.
608440	613320	So now, in fact, and me in this case as a co-author, we were suggesting that there may
613320	617960	be an isomorphism, that you would find a similar kind of transformation in these space, in
617960	623480	these abstract spaces, but there's many ways, I'm sorry about playing the song again, there's
623480	627680	many different ways that these spaces could relate, and I'm just, I'm not going too much
627680	631840	into depth here, but category theory is a relatively new branch of mathematics, so there's
631840	635800	a lot of buzz around it, and that is exactly what category theory is trying to do.
635800	639720	It's taking different mathematical structures, in this case here you can see geometry and
639720	642680	algebra, and trying to find the relations between them.
642680	646160	And these relations, they can be mathematically expressed, we call punctors.
646160	650840	So in the end, what we're trying to do is we try to find a functor between the abstracted
650840	657000	brain activity and the mathematically-formalized phenomenological structure to translate between
657000	658000	them.
658000	659800	This could be isomorphism, this could be something else.
659800	664800	Well, I hope I convinced you that this part here is a fruitful research program and already
664800	668920	underway, and you might agree with me with this, but you might have doubts about this
668920	669920	one.
669920	674200	So how do we get to these structures from brain activity that would allow us to find
674200	678960	some kind of mapping between the structure-ized qualia space and neuronal space?
678960	679960	Yes?
679960	680960	Yes?
680960	681960	Yes.
681960	682960	Yes.
682960	692400	So what I'm saying, so most of them are correlative, and so what I would go to now is that I'm
692400	697440	trying to find rather than a Pearson correlation, what I'm trying to go at is something that's
697440	702840	actually more like a mathematical law of nature, and I don't want to go too much into the
702840	707320	problems that correlational approaches have, but what I'm going to back to in the next
707320	712000	couple of slides is that we're trying to get something that's deeper in terms of breaking
712000	717600	correlation into causal structures, and that might allow us to make that look more directly.
717600	722000	So am I not convinced yet, but maybe at the end of the talk we can talk about it?
722000	726400	Well, okay, some of the ones that I think about are, but maybe we should talk more about
726400	728240	that talk.
728240	735520	So the theory that, again, I'm just using here to make that leap is integrated information
735520	739440	theory by Giulia Tornoni, and there's a lot to say about integrated information theory.
739440	742400	It's a very complex theory that's truly interdisciplinary.
742400	749320	It spends a lot of different branches of how we usually divvy up our thinking and academia,
749320	754720	so I'm not going to talk too much about it in terms of what the background is and explaining
754720	755720	it.
755720	759320	I think that would be at least another lecture if not a whole graduate seminar of a whole
759320	764200	semester, but I will give you some of the ideas of integrated information theory.
764200	770240	The first one is that if we think about consciousness, typically this is a consensus diagram that
770240	773760	has made around that last couple of years, which is that consciousness seems to be at
773760	779080	least two-dimensional, and that there's one axis, which we would call the level of consciousness,
779080	780560	how much you're conscious.
780560	784440	You might want to call it arousal or something related, and then the other one is the content
784440	789200	of consciousness, which is how much you experience, what you experience, the qualia.
789200	793360	The reason that we think it's at least a two-dimensional state is that we can take various states of
793360	797400	consciousness and put them within this two-dimensional plane, and then you'll find, for example,
797400	802480	that vegetative state is a state of coma where people clearly have a different level
802480	806000	of consciousness, a different arousal, so they wake up in the morning, they open their
806000	810520	eyes, they go to sleep at night, but there's no sign of actual conscious experience in
810520	813520	a lot of these patients, so there's almost no content, but there's a lot of changes
813520	814520	in arousal.
814520	818120	That means we can dissociate these two parts of consciousness, and they're probably orthogonal,
818120	819640	as I put them right here.
819640	824200	Now, for understanding consciousness, I think the really interesting case is up here, where
824200	829400	you are highly aroused and you have a lot of content, as I hope in a state that you're
829400	830960	still in right now.
830960	835320	I'd probably get you more to this state as I keep talking on, but in this state, what
835320	841040	I'm arguing is that you still don't fully have all the access to what consciousness
841040	842040	is doing.
842080	844120	So the example for that would be over here.
844120	847760	If you read this really fast, you would say, oh, I read a bird in the bush, and if I say,
847760	851320	well, try again, you would say, oh, wait a minute, it says a bird in the bush, but the
851320	854760	first time you see that, you probably didn't see the second the.
854760	859840	So that's a famous example of repetition blindness, so that means that even if you're up here
859840	865480	and you have the highest level of consciousness, there are still parts of the world that are
865480	870000	closed up to you that you fail to experience.
870080	871520	When I put it out, you can actually experience.
871520	872040	Yes, question.
889440	892360	Yeah, so I would argue it would be more to this.
892360	895280	It doesn't go without a traditional notion of subjective experience.
895280	895440	Right.
895440	902640	So you have a living being that's undergoing all the signs of arousal, but it doesn't have
902640	904040	any delight to offer.
904040	906200	There's no subjective experience.
906200	909880	That person would be conscious in this definition, but not in this definition.
909880	915520	And that has led to a lot of debate in the field, because there's been a lot of confusion
915520	916840	misunderstanding about it.
916840	921880	So you can have, if you do anesthesia, for example, for more than 24 hours, so some of
921960	925640	us do, they know that the animal gets more aroused in the morning and you have to increase
925640	929720	the anesthesia, but we don't think that the animal is having any subjective experience,
929720	931160	but it's a sign of arousal.
931160	936240	So other people that would say, well, if you're a Buddhist Zen monk and you can reach a state
936240	940240	of consciousness without content, if you really look into the phenomenology of that, there
940240	942760	would still be some content because there is subjective experience, right?
942760	944360	So I'm not talking about that.
944360	948920	So I'm really dissociating basically the almost behavioral signs of arousal from actual subjective
948920	949920	experience.
949920	950920	That's my definition, though.
950920	951920	This might seem different.
951920	952920	Does it make sense?
952920	956360	Yeah, we can talk about that after the talk.
956360	960320	So luckily, this is the part that we're probably in agreement in that I'm most interested
960320	961320	in.
961320	965400	So what integrated information theory does, and that is what it's mostly known for, is
965400	969360	to quantify this axis of consciousness space.
969360	973880	So what integrated information theory does, it gives you a scalar, a single value, and
973880	979600	it tells you how much the system has in terms of level of consciousness.
980600	985000	Lesson well-known is that this orthogonal dimension is also explained by integrated
985000	987320	information theory, and I put this down here.
987320	991240	So when you had this first experience of a bird in the bush, and you missed the second
991240	996240	the, versus you had this experience of a bird in the bush, there was no change in the world.
996240	998680	There was just a change in your mental state.
998680	1002840	So that means you had two phenomenological states, and that difference is also quantified
1002840	1008360	by integrated information theory as a difference in the causal effect structure, or the short,
1008360	1010200	I will call this the CES.
1010200	1014320	So there's a delta phi, which is what IET is known for, that shows you the levels of
1014320	1018400	consciousness, and then a delta CES, which shows you the difference in the contents of
1018400	1019400	consciousness.
1019400	1020400	Yes.
1020400	1027280	Are you saying one of those is more content than the other?
1027280	1028280	It's different.
1028280	1033840	Well, I see that, but it's like interesting to figure out your axis of content here, because...
1033840	1037520	Yeah, so in this particular example, I would say that you have more content when you are
1037680	1040520	aware of the second the, because you see an extra the.
1040520	1041520	But that seems questionable, right?
1041520	1045520	Because in one case, you're paying attention to the meaning, and there's a whole lot of
1045520	1046520	certain kind of processing.
1046520	1050520	Whereas in the other case, you're paying attention to something very superficial that may not
1050520	1051520	be important.
1051520	1055520	So, and then I might pay attention to the fact that some letters touch the triangle there,
1055520	1058520	and then I'm aware of that, and I, is that important?
1058520	1059520	But that's more.
1059520	1060520	So I...
1060520	1061520	That's a fair point.
1061520	1066520	I mean, for me as a vision scientist, just from the visual perspective, I would say you
1066520	1068520	have more content here than you have here.
1068520	1071520	But in fact, I'm not...
1071520	1075520	I don't want to make too much of a point of quantification here, when it comes to the
1075520	1081520	difference in phenomenology, more that we can look at the difference between two phenomenological
1081520	1082520	states.
1082520	1087520	It might be equally content rich, but the fact that there is a difference, we can quantify
1087520	1088520	the difference.
1088520	1091520	So that doesn't mean that one, that there has to be an unequal sign between those.
1091520	1092520	Does that make sense?
1092520	1093520	Okay.
1094520	1095520	Yeah.
1110520	1114520	So let's try and maybe move away from the concept of amount of content.
1114520	1115520	Okay.
1115520	1119520	Just the fact that there's different contents, that there's different states of consciousness
1119520	1120520	that you're in.
1121520	1124520	They can be formalized in IIT.
1124520	1125520	Okay.
1125520	1130520	And so that's all I'm getting at, that there's two formally different states, mathematically
1130520	1131520	different states.
1131520	1134520	So let's make it less about the amount, but just the fact that there's a difference and
1134520	1137520	we can mathematically get at it.
1137520	1143520	And so just to give you an idea, so these are not just abstract ideas.
1143520	1146520	There's actual mathematical formalism associated with that.
1146520	1150520	And so this is from a paper that I will talk about a little bit more about today, where
1150520	1155520	now and his colleagues, they took neural data, in this case from a fruit fly.
1155520	1160520	The fruit fly had multiple neurons in its mushroom body, which would be equivalent to
1160520	1161520	what we call a brain.
1161520	1166520	And then they measured Phi, which is the level of consciousness between awake and anesthetized
1166520	1167520	fruit flies.
1167520	1172520	And you can see that the theory made the correct prediction, which is that Phi should be smaller
1172520	1176520	in an anesthetized animal than in an alert animal.
1176520	1183520	This right here corresponds to that in that they tried to get at this causal effect structure.
1183520	1188520	Now, back when they did that study, the mathematical formalism for the causal effect structure wasn't
1188520	1189520	fully developed yet.
1189520	1192520	So that's one of the things I'm saying this is really cutting edge what I'm telling you
1192520	1198520	today, but they were trying to get at that by coming up with a geometric space that puts
1198520	1203520	the neural data of the various causes and effects that you find across the neural data
1203520	1207520	into a multi-dimensional space, in this case, broken down to a three-dimensional space.
1207520	1211520	And you can see that there's a difference in the causal effect structure as well.
1211520	1212520	So note the date.
1212520	1216520	This is really a cutting edge publication.
1216520	1219520	So what I will show you today is that I think we can go a step further.
1219520	1221520	So how does this come about?
1221520	1226520	Well, at the very heart of integrated information theory is this idea that if you have an interconnected
1226520	1233520	system in which information flows with causal effectivity, let's say that you have four neurons
1233520	1237520	and they're interconnected in this way and you see the causal flow between these neurons,
1237520	1243520	that if you can find out about these causal effects by severing some of these connections
1243520	1247520	and you can do that either experimentally or you can do that.
1247520	1249520	And this is the interesting part that I hopefully can show you guys today.
1249520	1253520	You can do that computationally, analytically, with your own data.
1253520	1259520	And once you do that, you compare the mutilated system where you severed some of these connections
1259520	1260520	with your original system.
1260520	1264520	And if there's any difference in the statistical description, then you know what you just mutilated
1264520	1266520	had a causal effect.
1266520	1268520	It actually was important for the system.
1268520	1271520	So you changed the system by mutilating some of these connections.
1271520	1276520	But if you mutilate some of these connections and there's no difference for the system as a whole,
1276520	1282520	then you know that these were reducible connections that were actually not important for the function of the system.
1282520	1284520	And this can be done mathematically.
1284520	1290520	So this right here is some of the simplified formalism that goes with integrated information theory.
1290520	1295520	This is when you put it into practice with Python computer code that has been available.
1295520	1298520	Again, this is the paper that I just referenced from the fruit fly.
1298520	1302520	And all I will do today is give you a little bit of an overview of what's going on here
1302520	1307520	and then give you resources that if you are interested and you want to try it with your data,
1307520	1309520	how you can do that for yourself.
1310520	1312520	Okay, so why might this be interesting?
1312520	1316520	Well, more and more what we are doing is when we're talking as neuroscientists,
1316520	1318520	especially as cognitive neuroscientists,
1318520	1324520	when we're thinking about the brain, we're thinking about it more and more as a causally connected network circuitry system.
1324520	1329520	And we are, because of that, measuring more and more data simultaneously across the brain
1329520	1335520	because the idea is that we just need to know more about what each of these individual parts is doing and how they interact.
1335520	1339520	And there's two steps involved in that that I think are crucial.
1339520	1342520	The first one is that rather than just looking at activation,
1342520	1346520	what integrated information theory does is it abstracts it to information.
1346520	1348520	And so why is that interesting?
1348520	1353520	Well, activation is actually more confined than the flow of information.
1353520	1358520	So if we have, let's say, three neurons here and three neurons here and they're connected with these three axons,
1358520	1363520	what activation can do is cross over between these neurons, cross its path
1363520	1368520	because activation has to flow along the physically hard-wired lines of an axon.
1368520	1376520	But neurons, of course, are not as simple as these systems right here that just receive activation and pass it on.
1376520	1378520	Neurons can act as logic gates.
1378520	1380520	They can make, they can compute.
1380520	1385520	So in this case, if I replace these neurons with X OR gates that are only active,
1385520	1388520	depending on what the input state is,
1388520	1392520	then you can see that information now can cross the system.
1392520	1397520	So as the activation is confined with the physical connections, information flow is more fluid
1397520	1402520	and actually can process the system in various ways that the activation per se can.
1402520	1408520	So that's why abstracting from activation to information might be interesting for many of us
1408520	1410520	in trying to understand what the brain is doing.
1410520	1416520	The second issue with collecting all of these data simultaneously is that most of the techniques
1416520	1420520	that we then use analytically to analyze these multidimensional data,
1420520	1422520	so this would be in this example.
1422520	1426520	And for Mariah, but the same gets done in EEG or in a singular physiology field,
1426520	1430520	is that we're trying to get at the multidimensional of the data,
1430520	1434520	but there's always a step in there that basically boils down to pairwise comparisons.
1434520	1437520	Very often, Pearson's correlations. It's just a very powerful technique.
1437520	1441520	So if we do ICA, if we do graph theory, if you look at the various steps involved,
1441520	1444520	typically somewhere you find that there's pairwise comparisons.
1444520	1449520	And I would argue that that's a limitation and I would argue that integrated information theory gets past that.
1449520	1450520	So what do I mean?
1450520	1454520	So let's take this very simple example of heavy learning that I took from one of the textbooks.
1454520	1459520	So if you are taking a system of three neurons interconnected, so these neurons connect onto these neurons,
1459520	1464520	and then you have weights, synaptic weights that you can scale up or down to come up with a learning rule.
1464520	1468520	Well, in the conventional approach, you would look at this correlation and that correlation.
1468520	1472520	You look at these pairwise correlations, you can make up a matrix and then make this examination.
1472520	1476520	But what you're missing out on is the synergistic effect of these two neurons onto that neuron.
1476520	1480520	So there could be a combined causal effect that you're missing by breaking up the system
1480520	1482520	and just looking at the pairwise correlations.
1482520	1485520	And so IIT doesn't do it. Why doesn't it do it?
1485520	1489520	So here's an example of how the formalism of IIT works in practice.
1489520	1493520	And the example is that you have a system with two buttons, A and B,
1493520	1498520	and they basically have causal effects on, in this case, let's say another button C.
1499520	1506520	So the idea is that we're looking at if button A and B are inactive.
1506520	1509520	So in this case, shown here in white, so they're not being pushed.
1509520	1515520	What does that mean for the state of this causally affected system C?
1515520	1521520	And so you can empirically do that by just taking a system and looking at what's happening.
1521520	1525520	So let's say you're taking many, many trials, as we often do 100 trials,
1525520	1529520	and you see what happens if A and B are un-pushed and what happens to C.
1529520	1535520	And so you see that in this case, in 90% of the cases, C would also be off,
1535520	1537520	and in 10% of the cases, C would be on.
1537520	1543520	So this could be because of noise, this could be because of a whole lot of different mechanisms.
1543520	1547520	And then the logic is to just step through all of the possible states.
1547520	1552520	So you ask what happens if A is pushed, and you can see 90% again C is off,
1552520	1554520	what happens if B is pushed, and here the interesting case,
1554520	1556520	what if both of them are pushed at the same time?
1556520	1560520	And so this, of course, some of you might have noticed already,
1560520	1565520	leads to a table that has been used in statistics for a long time.
1565520	1568520	That's what we call a transition probability matrix,
1568520	1571520	or it's at the very heart of Markov-Chakes, yes.
1571520	1588520	So there's a few things to say about that.
1588520	1592520	So first of all, this technique works best if you do know the causal structure.
1592520	1599520	So if you know that these have this kind of causal effect on C,
1599520	1602520	but you don't need to know that, you can infer the causal structure
1602520	1604520	just by looking at the statistics of the system.
1604520	1605520	So you don't need to know that.
1605520	1607520	And so if one of them would have, let's say,
1607520	1609520	if you said a positive effect or negative effect,
1609520	1612520	it also, again, would show up in the statistics that you're measuring that comes out.
1612520	1616520	So the real big step that I'm doing right here that's questionable
1616520	1621520	is that I'm moving from a frequentist observation to a statistical description of the system.
1621520	1625520	But of course, that's something that a lot of statistics us all the time.
1625520	1629520	But I'm arguing that you could observe a system, come up with that table,
1629520	1633520	and then go on to describe the system.
1633520	1636520	Other questions?
1636520	1639520	Okay, so if you're with me on this, then you would agree that you can, of course,
1639520	1641520	do this for a much more complex system.
1641520	1645520	So right here, this would be four logic elements that are interconnected.
1645520	1648520	These could be neurons, these could be voxels, these could be areas,
1648520	1650520	this could be whatever you're interested in.
1650520	1654520	And one more trick that I'm doing here is that rather than looking at
1654520	1656520	how three of these elements interact with the fourth element,
1656520	1658520	I'm taking the whole system.
1658520	1661520	So I'm taking all of the possible states that this system could be in.
1661520	1664520	All of these are on, three of these are on, none of these are on.
1664520	1668520	And then what I'm doing is I'm taking a step forward in time.
1668520	1672520	So I will call this the present, and then I'm taking a step forward in time,
1672520	1674520	and I say, if this is the present state,
1674520	1679520	what is the probability that I end up in any of these other possible states?
1679520	1682520	So this is how I get a true transition probability matrix.
1682520	1685520	In this case, the numbers are zero and one stone at the confuse you,
1685520	1687520	this would be a deterministic system.
1687520	1690520	But what we would be measuring most of the time would be fractions in here.
1690520	1693520	So it would be fractional probabilities with which the system transitions
1693520	1695520	from one state into the other state.
1695520	1699520	And so one simple notion would be that this right here is the cause,
1699520	1701520	and then this right here is the effect,
1701520	1706520	because we're looking at what is causing what effect by making the jump into the future.
1706520	1710520	How far you move from the present into the future, that's again up to you.
1710520	1714520	So you could take a millisecond, eight millisecond, one TR, whatever you want.
1714520	1719520	But you can look at how the system, depending on which state it is in,
1719520	1721520	transitions into any of these other states.
1721520	1723520	Yes.
1723520	1727520	Using a Markovian assumption here, doesn't that presume the box,
1727520	1730520	or neurons are going to have a memory of its property?
1730520	1734520	Neurons are not memory of its state.
1734520	1736520	Yes, great.
1736520	1739520	I don't know if I have time to talk about that, but yes.
1739520	1743520	So this is one of the first things I ran into, looking at actual data.
1743520	1750520	So as you will see, maybe as I get to that, my argument is that if there's a memory in the system,
1750520	1756520	then there's a certain expectation to what the transition probability matrix should look like.
1756520	1759520	And we can actually use that to our advantage.
1759520	1764520	For example, we can see if there's anything that deviates just from a system
1764520	1767520	that has a simple form of memory, or it doesn't.
1768520	1771520	Another question.
1771520	1773520	So this right here is not the end of it.
1773520	1776520	So this right here is not what I really mean by cause-effect structures.
1776520	1780520	There's a second mathematical trick that's involved where you can break down
1780520	1785520	each of these supposed cause-effect interactions in your data
1785520	1787520	to look at which ones are actually causal.
1787520	1789520	And that's maybe the part that I'm most excited about.
1789520	1794520	So if I do this for my data, it turns out that most of these do not actually really have cause-of-power.
1794520	1796520	That's what I find the most interesting.
1796520	1800520	But you might already have noticed that by doing this kind of trick with your data,
1800520	1803520	that you're already coming up with something that is really interesting to deal with.
1803520	1806520	So by just taking your data and putting it in this kind of space,
1806520	1809520	you come up with these very nice mathematical properties already.
1809520	1813520	And so in fact, if you think about almost anything that we do in machine learning or big data,
1813520	1817520	starts out with NP arrays like that, two-dimensional matrices.
1817520	1821520	So I would argue that even if you're not interested in consciousness,
1821520	1824520	there's lots of room to explore here for your data by using this approach.
1824520	1825520	Yes?
1825520	1830520	So does this work only if you assume that you have all the elements in the system,
1830520	1833520	but what if there are other elements you're not measuring now
1833520	1838520	and they could also play a third variable role?
1838520	1839520	Yes.
1839520	1840520	So it's another shortcoming.
1840520	1841520	I totally agree.
1841520	1845520	So ideally, you would have to do this for all the neurons in the brain at the same time.
1845520	1846520	And so that's a type dream.
1846520	1851520	And in fact, as I was showing a moment, it already gets problematic with the data that we have.
1851520	1855520	But I would argue that when we do a Fourier transform,
1855520	1859520	in fact, even when we do a simple average, we are violating some of the rules
1859520	1860520	that apply to these algorithms.
1860520	1863520	So if you do a Fourier transform, you should never do that on one of our data,
1863520	1865520	whether the variance and the mean are changing over time.
1865520	1866520	It should be stationary.
1866520	1869520	And people that do it, they would say, well, it still works.
1869520	1874520	And so I would say that even if you have a system where you have confounders
1874520	1878520	that might be interacting with this as more hidden Markov chains, whatever you want to call it,
1878520	1880520	you don't have the entire Markov blanket.
1880520	1883520	It's still interesting to look at that system by making these assumptions.
1883520	1885520	Yeah, of course it's interesting.
1885520	1888520	Because when can you jump to the causal input?
1888520	1889520	Yes.
1889520	1894520	And so basically, my appeal would be that you can do them,
1894520	1897520	but you have to take them with a grain of salt.
1897520	1899520	Kind of like we do already with greater causality and other things.
1899520	1905520	But yeah, so there's an imperfect match between what the theory and theory provides you
1905520	1908520	and then what we as experimentalists can do with it in practice.
1908520	1910520	But as I just said, I think there's other techniques.
1910520	1912520	Granger causality, I think is a good one.
1912520	1916520	That we violate stationarity, which is one of its main assumptions all the time.
1916520	1918520	And yet we found interesting things with Granger causality.
1918520	1920520	And I'm just talking, there's a different way of doing that
1920520	1926520	that goes away from pairwise considerations of causality to multi-dimensional ones.
1926520	1928520	Great questions.
1928520	1930520	Okay, so let's talk about actual data.
1930520	1934520	So what my lab is mostly interested in are neural circuits.
1934520	1938520	And the system that we've been choosing for that are the neural circuits
1938520	1941520	that they of course have the property that Isabel just mentioned,
1941520	1944520	that they do get inputs from outside the system,
1944520	1948520	but they are well-structured circuits and they exist across the layers of cortex.
1948520	1950520	And so one reason that they're interesting is that
1950520	1953520	technology has made huge jumps in the last couple of years
1953520	1957520	in order to allow us to measure neuronal activity across the layers of cortex.
1957520	1960520	So this of course is a commercial approach to that,
1960520	1965520	but some people don't know yet that these kinds of electrodes that Elon Musk is using
1965520	1968520	for his company, they're actually readily available down in my office
1968520	1970520	if you want to see them to research scientists.
1970520	1973520	So Elon of course is trying to do this in humans.
1973520	1975520	What does Elon have to say?
1975520	1978520	Electrons in human brains are the future.
1978520	1980520	You might get one some time as well.
1980520	1983520	So thanks to the fact of machine learning we can do these things.
1983520	1986520	But we use these kinds of electrodes.
1986520	1988520	So silence, Elon.
1988520	1991520	We use these kinds of electrodes to measure along the layers of cortex
1991520	1995520	to get at what we think is a cortical column in the system.
1995520	1999520	And then we have these simultaneous measurements of neurons or population activity,
1999520	2003520	and so I'm trying to get to the causal effect structure of these.
2003520	2006520	So here's the first bad news.
2006520	2009520	So if you take this again could be voxels or areas,
2009520	2012520	in this case it's electrode channels of these arrays where we can make these measurements,
2012520	2015520	and you look at the size of the transition probability matrix,
2015520	2018520	of course you run into a combinatorial explosion.
2018520	2021520	So for most of what I will show today I will be stuck at six channels
2021520	2023520	because my laptop got stuck at six channels.
2023520	2027520	So if we go up to electrodes that my lab has been using ten years ago,
2027520	2032520	you already get to numbers that are twice as large as the number of atoms in a cell.
2032520	2036520	And then this right here is what my lab will be using hopefully in a week from now
2036520	2041520	and you can see that we're reaching numbers that are absolutely beyond astronomical.
2041520	2046520	Again, I feel at first when I hear these limitations of the theory
2046520	2049520	I feel at first maybe a little bit depressed or discouraged,
2049520	2053520	but then you go and you watch some YouTube talks again about quantum computing
2053520	2055520	and the leaps that are being done there,
2055520	2057520	and I think that these kinds of problems are technical problems.
2057520	2060520	I think that they're solvable in the long run.
2060520	2065520	There are 10 to the 80 atoms in the universe.
2065520	2067520	Yeah, I know there's a way more...
2067520	2070520	The magnitude more than there are atoms in the universe.
2071520	2074520	This is just one electrode with two other channels.
2074520	2076520	So when we're talking about measuring all the neurons in the brain,
2076520	2079520	you see where the theory runs into serious problems.
2079520	2082520	But as I said, I think these are technical limitations
2082520	2086520	and also there's other ways to dimensionality reduce
2086520	2088520	before you maybe apply the theory.
2088520	2091520	So let's use six measurements of the brain simultaneously
2091520	2094520	and I will use this graphic to show you all the 64 states
2094520	2096520	that are possible for these six measurements.
2096520	2098520	So all of them on, all of them off.
2098520	2102520	So if these are neurons, on or off is easy to understand.
2102520	2105520	This might be a neuron firing an action potential, not an action potential.
2105520	2109520	But most of us, we're not using neurons, I'm not using neurons here.
2109520	2112520	So what I'm doing is I'm basically binarizing the data.
2112520	2114520	So you can just take your data, take a trial,
2114520	2116520	you take the average activity of your trial.
2116520	2119520	This might be just your fMRI signal, your EG signal, whatever you have.
2119520	2121520	And then you just say what's above and below.
2121520	2124520	And whenever it's above, I'm going to call it a non-state
2124520	2126520	and whenever it's below, I will call it an off-state.
2126520	2129520	And so the on-states, I make black and the off-states, I turn white.
2129520	2130520	So I just binarize the data.
2130520	2134520	The theory would still work if I break it down into four different states, or three.
2134520	2138520	So it just gives you a larger matrix, more states to work with.
2138520	2145520	So what does it look like for having 64 states of actual neuron data?
2145520	2149520	And I would look at them in the present and then I would look at them in the future.
2149520	2151520	Well, it gives us a matrix, a transition probability matrix,
2151520	2155520	where you can see that the color here shows the probability of the system
2155520	2157520	going from one state into the other.
2157520	2159520	And you can already see that there's an interesting structure,
2159520	2161520	most prominently, that the system has memory.
2161520	2164520	So the system likes to stay in its own state.
2164520	2168520	And the first interesting analysis that I would propose here is
2168520	2171520	you can use this for your data and then you can play with the time.
2171520	2176520	You can see how much do I have to move until the system moves out of its original state.
2176520	2180520	When I do that for my neural data, I find that it is roughly on the order of one synapse.
2180520	2185520	So if I go past it, then the system automatically starts to go into more interesting states.
2185520	2188520	You might also feel that this is not an interesting way to look at the data.
2188520	2191520	So what I would propose is that each time we have a matrix like that,
2191520	2193520	we can apply graph theory.
2193520	2196520	So we can take each of these states as a node in graph theory space,
2196520	2201520	and then the causal interactions, the probability that the state moves from one to another,
2201520	2205520	we can take as the thickness of the edges between the graph.
2205520	2209520	So if I do this, for the matrix that I just showed you, it becomes very hard to even visualize.
2209520	2211520	So let me make it even more simple.
2211520	2214520	So what I would do now is take three measures of the brain,
2214520	2216520	and I would do this in the upper, middle, and lower layers of cortex.
2216520	2218520	So there's only three states.
2218520	2221520	So all of them are inactive, all of them are active,
2221520	2226520	and then you can see how the system transitions from one state to the next state.
2226520	2229520	One thing that's interesting here, I think from a cognitive neuroscience perspective,
2229520	2231520	is that you see these loops.
2231520	2235520	So again, this is the system having a certain likelihood to end up in its own state again.
2235520	2238520	So in a way, you can think of this as feedback.
2238520	2243520	Well, you can already see that feedback is more likely for some states than for other states.
2243520	2248520	And you can, again, play around with, for example, the time that you take in between these measurements,
2248520	2250520	and you can investigate.
2250520	2253520	Some of the feedback loops, they might become thinner or go away,
2253520	2256520	and then as you look further into your data, they might reemerge,
2256520	2258520	as feedback re-enters the system that you're looking at.
2258520	2259520	Yes, Gordon?
2259520	2262520	Why is that a feedback loop and not just like...
2262520	2263520	Or memory.
2263520	2264520	Yes, yes.
2264520	2266520	I should be more careful how I phrase these things.
2266520	2267520	I'm totally agnostic about it.
2267520	2269520	So the system could just persist.
2269520	2270520	Could just be history.
2270520	2271520	Yes.
2271520	2272520	I think it would be feedback.
2272520	2275520	It would be easier to argue if you look at that as a function of time,
2275520	2277520	and it disappears and then reappears again.
2277520	2280520	That would maybe show that there's some reverberance going on.
2280520	2282520	But yeah, that's a great point.
2282520	2290520	And so I'm cutting a long story short of looking at these transition probability matrices
2290520	2294520	in these actual graph structure.
2294520	2295520	It does show you...
2295520	2298520	When I looked at my lab's data, it does show you a lot of interesting structures.
2298520	2300520	So for example, the question before,
2300520	2302520	what if the system has a very strong memory?
2302520	2306520	Well, we do know that a lot of the neural data is one over F distributed,
2306520	2308520	so it does have memory over time.
2308520	2313520	And so what I found is that if I compare this actual neural data to a system
2313520	2317520	that I artificially produce, where I have control over these variables,
2317520	2321520	and I can't just introduce correlations however strong I want them
2321520	2323520	between the data or causal directions,
2323520	2325520	or I completely uncorrelated the system,
2325520	2330520	then I do find that there's a stereotypical pattern right here.
2330520	2332520	And as I said before, we could use this as a baseline.
2332520	2335520	We could say, well, that is what an uncorrelated system looks like,
2335520	2338520	and any deviation from that would be a more interesting deviation.
2338520	2339520	Yes?
2346520	2348520	That's basically what I just said.
2348520	2352520	I mean, you can take artificial data and you can uncorrelated it.
2352520	2355520	And so I took it out because I want to use the rest of the talk
2355520	2357520	to get at the maybe most interesting part.
2357520	2359520	But if you take a system that's completely uncorrelated,
2359520	2361520	you end up with a structure that's called the Hamming Distance.
2361520	2365520	And so the Hamming Distance is that the transition probability of a system
2365520	2371520	that's in 001 to go to 001 should be higher
2371520	2375520	than the transition probability from a system to go from 001 to 111,
2375520	2377520	because more has to change.
2377520	2379520	The system is more likely to end up in this state than in that state.
2379520	2383520	And so that you can measure, it comes out as the Hamming Distance,
2383520	2386520	a very well-characterized geometrical space.
2386520	2389520	And so you can either use it as a matrix, or you can use it as a graph,
2389520	2392520	and then you can take any kind of distance that you like
2392520	2394520	between multidimensional sets of data,
2394520	2397520	the mahalano-bisterstand, euclidean norms, anything like that,
2397520	2400520	to see how far it differs.
2400520	2404520	Okay, so this is just the first step of integrated information theory,
2404520	2406520	but you can maybe see why I'm already excited about that.
2406520	2409520	As an experimentalist, we tend to look at new tools
2409520	2411520	that might allow us to look at our data in new ways,
2411520	2413520	and I think this is what it does.
2413520	2415520	And so all of us might have this feeling that Galileo had maybe
2415520	2417520	when for the first time he was able to look for a telescope,
2417520	2420520	and he discovered things out in the solar system that haven't been seen before.
2420520	2423520	So I'm not arguing this as Galileo's telescope,
2423520	2426520	I'm just saying that here's another interesting tool to look at your data
2426520	2429520	that might be worth your time and more consideration.
2429520	2431520	But there's more to it.
2431520	2434520	So what I said in the beginning is that integrated information theory
2434520	2438520	severs some of these causal connections and see what the system does.
2438520	2442520	And so that rests on something called due algebra
2442520	2447520	that was introduced to the field actually decades ago already by Julia Pearl.
2447520	2449520	So that's a tool of statistics.
2449520	2451520	And so again, I'm not going to do it justice, of course,
2451520	2453520	in the short amount of time that I have,
2453520	2455520	but I'm just going to give you the general idea.
2455520	2457520	So if you have a very simple statistical system,
2457520	2461520	so say you have one variable and you're interested in its effect on the other variable,
2461520	2463520	and you have a confounding variable that affects both.
2463520	2467520	That's a very common statistical problem because if you just do a correlative approach,
2467520	2470520	this right here causes huge pain.
2470520	2473520	And so we call this a fork in a causal statistical sense.
2473520	2481520	So what Julia Pearl said is that we can get at the causal effect right here
2481520	2485520	beyond the correlative approach using his due calculus.
2485520	2487520	So what is this magical approach?
2487520	2491520	Well, we know as scientists that in order to find causality,
2491520	2493520	you have to make interventions.
2493520	2496520	You have to intervene into the system and then you see causal effects.
2496520	2501520	What due calculus allows you to do is to intervene into the system without physically doing so.
2501520	2504520	So you're intervening it statistically, if you will.
2504520	2506520	So how do you do that?
2506520	2511520	Well, if you think of this system described in a similar way as I just did,
2511520	2514520	where you know the transition probabilities between each of these states of the system.
2514520	2517520	So you have a full statistical description of the system,
2517520	2519520	and you also know the causal graph.
2519520	2521520	So you know the causal connections.
2521520	2528520	What you can do is you can computationally keep one of these variables constant or manipulated
2528520	2530520	and then study the effect on the whole system.
2530520	2533520	So in this case, if you keep this variable at a certain value,
2533520	2535520	you're severing this connection.
2535520	2539520	Now this variable right here, the confounder, can't affect this one anymore
2539520	2540520	because you're keeping it constant.
2540520	2541520	It can't change.
2541520	2543520	So whatever this guy does, it doesn't affect this guy.
2543520	2544520	Does it make sense?
2544520	2546520	It can still affect this guy, but it can't affect this guy anymore.
2546520	2548520	So you're making an intervention.
2548520	2551520	And you're also doing what I just said, you're mutilating the system
2551520	2554520	by getting rid of one of the causal connections.
2554520	2558520	So you're different in targeted attack analysis in graph theory?
2558520	2562520	No, I think it's out of my wheelhouse, but it might be very similar.
2562520	2564520	So let me give you a concrete example.
2564520	2569520	So if you have a simple transition probability matrix,
2569520	2574520	so in this case you have only two different parts of the system,
2574520	2577520	and they're either both off, one is on, the other's on, or both are on.
2577520	2580520	And then you look at t plus some point in the future,
2580520	2582520	and you look at the transition probabilities,
2582520	2584520	and you want to make this intervention.
2584520	2587520	So you want to mutilate the system.
2587520	2593520	So what you can do is you can disconnect the system through what,
2593520	2596520	in this case, would be called statistical noising or marginalizing.
2596520	2600520	So in this case, let's say the first element we want to get rid of.
2600520	2603520	And so what you can see right here is that in this case,
2603520	2606520	in that case, if we eliminate it or eliminate it,
2606520	2610520	it comes out as basically just averaging these two states together,
2610520	2614520	because this is zero zero, so let's average where the second one is in zero,
2614520	2616520	and you get this transition probability,
2616520	2618520	or in this case the second element is one and one.
2618520	2622520	So we don't care about this anymore, so we just average that and we come out at point five.
2622520	2624520	So you come up with a new transition probability matrix
2624520	2627520	where you artificially intervene and you took out part of the system.
2627520	2631520	Now this transition probability matrix is different than this transition probability matrix.
2631520	2636520	So if you look at the transition probabilities,
2636520	2640520	it looks like you severed something that you shouldn't have.
2640520	2642520	There's a causal effect in the system.
2642520	2645520	If you would have done this and it would be exactly the same outcome,
2645520	2649520	then what you just severed, what you artificially intervened,
2649520	2650520	didn't have an effect.
2650520	2653520	And you can basically eliminate it in this approach.
2653520	2654520	Yes.
2654520	2655520	Can you go back one?
2655520	2656520	Yeah.
2656520	2660520	So you started describing this by saying, assuming you know.
2660520	2661520	Yeah.
2661520	2666520	But, I mean, this is a very simple third variable problem.
2666520	2669520	I mean, we definitely want it now, usually.
2669520	2670520	Yeah.
2670520	2674520	And simple, just with three variables,
2674520	2677520	trying to figure out what's needed and what,
2677520	2682520	I mean, we can't turn this into correlation and complication with three.
2682520	2686520	What aspect of this is fixing that problem?
2686520	2687520	Yeah.
2687520	2691520	So two responses that, again, you might not find satisfactory.
2691520	2695520	So the first one is that I would say, we just make that assumption again.
2695520	2698520	In this case, if you're looking at an individual column,
2698520	2700520	we're just going to assume that it's closely close
2700520	2702520	and we're just going to make these assumptions.
2702520	2704520	So we know, for example, that the middle layer is connected to the upper layers
2704520	2706520	and the upper layer is connected to the lower layer.
2706520	2709520	So we can come up with a very simple graph like that.
2709520	2711520	And we know that there's assumptions and maybe even violations in there.
2711520	2713520	But we can still use it as a model.
2713520	2717520	And every model has a shortcoming to look at the data.
2717520	2720520	But I would agree with you that's problematic.
2720520	2723520	So what's the other response that I have to that?
2723520	2726520	Well, these electrodes, those are very specific response, maybe,
2726520	2728520	to those of us that are doing neurophysiology.
2728520	2732520	Those electrodes that we have, they now allow you to actually look at connected neurons.
2732520	2737520	So you're getting 100 neurons at a time and you can look at the correlation of the activity
2737520	2740520	and you'll find neurons that always basically fire together
2740520	2744520	with a little lack of, let's say, four milliseconds or so.
2744520	2749520	So that means that you can establish very simple systems,
2749520	2752520	in this case, something like this, where you have interconnected neurons
2752520	2756520	and you're not making any major violations.
2756520	2763520	This having said, yes, the whole theory, if you take the theory at heart
2763520	2767520	and if your end goal is to find out if the system is conscious,
2767520	2769520	how much it is conscious, what the conscious states are,
2769520	2773520	you would have to do this for maybe all of the neurons at the brain
2773520	2774520	measured at the same time.
2774520	2778520	In fact, the people that talk about the theory, they would admit that it could be worse.
2778520	2781520	It could be that it's not the neurons that matter but the synapses.
2781520	2784520	And we would have to measure all of the synapses at the same time.
2784520	2788520	But it could also be that that isn't what is the physical basis
2788520	2792520	of what gives rise to phenomenology and areas is the way to go, or columns.
2792520	2794520	We don't know what the right level is.
2794520	2797520	And so what they are arguing is, I think what is a part of the research program,
2797520	2802520	is to compute these different values on these different spatial and temporal scales
2802520	2803520	and see where it peaks.
2803520	2808520	So, but yes, there are, as with any technique, there's a chasm here
2808520	2812520	between the theoretical foundations and then how we can use it.
2812520	2818520	And so my appeal is take it with a grain of salt, but just give it a try.
2818520	2820520	Violate the assumption and see what happens.
2820520	2824520	So here's maybe the good transition why I think that might be interesting.
2824520	2827520	So this right here is actual data from my lab.
2827520	2833520	And so in blue and in red, you see these are basically neuronal activation.
2833520	2837520	While the animal in this case is just fixating at the screen, nothing is happening.
2837520	2840520	Then a stimulus comes on and then in blue, the animal pays attention.
2840520	2843520	And in red, the animal does not pay attention.
2843520	2848520	And so we're measuring activity across the layers in this case of area before.
2849520	2852520	And so there's two things that I want to point out.
2852520	2857520	So this right here are these representations and graph theory that I just told you about.
2857520	2860520	These down here are just two of the Pearson correlation coefficients
2860520	2865520	between the matrices that are underlying, in this case, the red versus the blue state.
2865520	2870520	And so if the animal is just fixating and nothing is going on,
2870520	2875520	and I'm computing the phi value, which would be about the level of consciousness in this case,
2875520	2877520	you can see it's pretty low.
2877520	2879520	And then when the stimulus comes on and I'm computing the phi value,
2879520	2882520	making all of these assumptions, you can see it's going up.
2882520	2887520	So despite all of these assumptions being violated, the theory still seems to hold.
2887520	2889520	The really interesting part is right here.
2889520	2892520	If I compute phi versus the state where the animal pays attention,
2892520	2895520	it's the highest versus if the animal doesn't pay attention,
2895520	2898520	it sinks even below the baseline value,
2898520	2902520	as you might expect if the animal is now sucking up all of the attention
2902520	2906520	to one part of the field rather than widely distributing it.
2906520	2912520	So this is just one example that the summer break allowed me to do
2912520	2915520	by getting into my own data and doing MATLAB.
2915520	2919520	They got me excited because I just put the theory to its test.
2919520	2923520	I held its feet to the fire and it did what it was supposed to do.
2923520	2926520	Now, of course, there's many more of these examples in the literature.
2926520	2930520	So it seems that most of the tests to the fire have so far held true.
2930520	2933520	The original idea, as I told you guys, was this,
2933520	2935520	that we're not just getting at the level of consciousness,
2935520	2937520	but at the contents of consciousness.
2937520	2941520	So in order to do that, we have to take another step.
2941520	2945520	And so this step is a transition from integrated information theory,
2945520	2947520	as you know it, as I've just introduced it,
2947520	2951520	which got published as Integrated Information Theory 3.0.
2951520	2954520	It's actually the third revision of the theory.
2954520	2959520	Now, Integrated Information Theory 4.0 is about to come out next year, I've been told.
2960520	2963520	But all of the math, most of the math has already been developed
2963520	2967520	and computer code is available for free on the internet
2967520	2970520	so that you can already run your data for Integrated Information Theory 4.
2970520	2973520	What is interesting about Integrated Information Theory 4?
2973520	2977520	Well, it comes up with a new way to come at that cause-effect structure.
2977520	2979520	Sorry about that.
2979520	2981520	I slug it in at a time.
2981520	2984520	So what it does, it basically takes what I just said
2984520	2987520	in terms of looking at these causal interactions,
2987520	2990520	but rather than just looking at the states of the system,
2990520	2992520	it becomes even more multi-dimensional.
2992520	2996520	It now takes, you see, combinations of different states
2996520	3000520	and the more measurements you have, the more come out of it
3000520	3003520	and basically applies the same kind of logic,
3003520	3005520	not just to the individual states,
3005520	3007520	but the combinations of states in between.
3007520	3011520	And so IIT 4.0 calls these relations
3011520	3015520	and as I said, the math and the code has just been available for that.
3015520	3017520	So just give you a brief insight again,
3017520	3020520	just very little time that I had available to look into that.
3020520	3023520	For this example that I've been showing a couple of times now
3023520	3026520	of neural data recorded in the middle upper lower layers
3026520	3030520	and I'm running this IIT 4.0 code to look at
3030520	3032520	out of all these possible combinations,
3032520	3035520	which one of those are irreducible, which one have actual cause of power.
3035520	3037520	So one thing that's a little confusing is that
3037520	3039520	we now move to different terminology,
3039520	3043520	so A, B, and C would be in this case the different layers of cortex
3043520	3047520	and you can see that only one of these layers turns out to have cause of power by itself.
3047520	3050520	The other layers of cortex, they only act synergistically.
3050520	3054520	But more surprisingly to me, most of the possible interactions
3054520	3059520	that you can theoretically come up with in this multi-dimensional space
3059520	3060520	turn out to be reducible.
3060520	3062520	They actually turn out to be non-causal
3062520	3065520	and I think that is very exciting for most of us that are interested
3065520	3067520	in how do I deal with these massive amounts of data
3067520	3070520	that I'm getting from fMRIEG, neurophysiology,
3070520	3074520	and we can reduce the dimensionality of these data in a meaningful way.
3074520	3077520	So rather than what a lot of techniques do these days,
3077520	3080520	take an external perspective and saying I'm decoding the system,
3080520	3082520	I'm looking into the system, what allows me to predict behavior,
3082520	3085520	what allows me to post-dict the stimulus conditions.
3085520	3087520	This is taking an intrinsic perspective and it's saying
3087520	3091520	which one of the interactions in my data are important to the system.
3091520	3092520	They actually matter.
3092520	3096520	So I would argue this might be a way for us to get closer
3096520	3099520	to understanding the system as it functions itself.
3099520	3103520	So if you agree with me or if you have only slight disagreements,
3103520	3106520	you can now see why this qualia structure approach
3106520	3108520	is something that has gathered a lot of YouTube channels,
3108520	3112520	a lot of fanfare, a lot of special issues and conferences.
3112520	3115520	If not, you might feel, one more time.
3115520	3117520	If not, you might feel like Albert Einstein
3117520	3121520	that maybe we're going a little bit too far with the math
3121520	3124520	and we should maybe stick a little bit closer to the data.
3124520	3127520	In either case, I say the franais, merci.

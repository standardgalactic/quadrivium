Thank you very much for the very kind considering introduction, Rene.
I'd like to get started by paying homage and thank the people that allow me to do this
kind of work.
Those of you who don't know, there's basically dozens of people involved downstairs in the
facility that take care of our furry colleagues and keep the operation running.
I want to thank the various funding sources, of course, the people in my lab, and then
also this particular group of people, two of which have moved on and are doing post-doc
already that collected the data, some of which that I will show to you guys today.
And last but not least, Nao Tokia.
Each time I get to talk about this in front of a group of young aspiring scientists, I
want to point this out.
You should not just try to connect vertically and that works vertically with people that
are more senior to you.
You should really strive to branch out horizontally and make friends with people that are in your
cohort because they will become more important to you as your career goes along.
Now is such a case.
I met Nao as a graduate student.
We had very strong fellow interests and we've been collaborating basically ever since.
And this has really borne fruit, as you will see, hopefully today.
So actually, I want to give all the credit to Nao because the ideas that I'm going to
show you largely stem from Nao.
And so I'm just basically a humble messenger to introduce you to his theories.
So what allows me to do that?
Well, there's a variety of factors that allows me to do that.
One of them is the situation that we're all in, which is that many of us were forced to
spend more time at home and with family.
And so very quickly, you might have also found that you have more time on your hand and what
should be done with it.
Well, for me, that meant trying to keep connected with other scientists.
And I felt I wasn't the only one.
So what happened was that one more on YouTube channels popped up where scientists were inviting
each other to Zoom talks.
And rather than just doing that over a closed Zoom setting where you needed to have a password,
they would stream that live on YouTube and other scientists could come in.
And so these are three YouTube channels that I just want to point out that I basically
became addicted to.
And they launched me into what I'm talking to you guys today.
So I'm advertising for you guys to use your spare time, use the time before you fall asleep,
go on YouTube, find these channels that they're rapidly growing.
And I think they're revolutionizing science because that allows you to find topics that
you are really interested in, find the other four people on the planet that equally interested
in that and have a rapidly evolving field.
So huge advantages to do this kind of approach.
And so I'm not just talking theory here.
What these three channels have in common is largely what I will talk about today has already
spurred a lot of things into action.
So this society, the Association for the Mathematical Consciousness Science, just got founded about
a month or two ago because of these YouTube channels, out of these YouTube channels.
And that society immediately sprung into action, launched special issues at frontiers
and a journal called Entropy.
It's got funding from various sources and there will be a conference two weeks from
now where many of us are coming together virtually and will speak and next year it's expected
to be in person.
So it really is, I think, a shift here that might have happened throughout the pandemic
for science to become more international, to become more collaborative and for communities
to grow within science around the globe.
So what is this particular community interested in?
Well, some of us might be interested in that as well, which is the basic problem that faces
neuroscience, maybe one of the biggest problems of neuroscience, maybe one of the biggest
problems in science at all.
Which is that we all believe as neuroscientists that there's a causal connection between neuronal
activity in our brain and our conscious experience, our perception of the world, that the fact
that we feel love, that we see colors, the fact that as you fall into deep sleep the
world goes away for you.
If you wouldn't exist, but as you come out into dream sleep or back as you wake up the
world comes back for you.
So I will call this phenomenology, the fact that you subjectively experience.
But the embarrassing fact for neuroscience is that this hasn't worked yet.
So usually what we do as scientists is that we find the causal connection and then mathematically
reduce one onto the other.
And that's when we say we understood it.
That's what we do in physics, that's what we do in chemistry, biology and a lot of
neuroscience.
But for this part of our being, the phenomenology, the fact that we have subjective experience,
this step seems to be hard to reach.
In fact, there are many people that would argue that this is fundamentally impossible,
that we will never be able to do that as scientists.
You shouldn't even try.
Well, I'm going to argue to the contrary.
And that is what a lot of these YouTube channels inspired me to do.
So this is a slide from one of the labs that's involved.
So what they're suggesting is that we have to move away from this conventional approach
of neuroscience of correlating inputs to the brain with activity in the brain or behavior
or perception comes out of it, and to take another step that links, in this case, phenomenology,
our subjective experience of consciousness, conscious perception, with brain activity
via math.
What do I mean by via math?
Well, if we believe that consciousness can be explained in terms of science, what we
have accomplished as scientists, what all of scientists rests on is that we find laws
of nature that were able to express these laws of nature with mathematical formalism,
gravity, Maxwellian equations, you name it, that's when we feel as scientists we've actually
had an accomplishment.
We've actually done it.
So can we do that for consciousness?
And so there's a pathway mapped out, and that's what I want to share with you guys today.
Well, so what's the traditional approach?
So the traditional approach, as a lot of you are familiar with, started obviously with
experimental psychology, but I would argue that experimental psychology, people like
Wundt and others, they were still careful to touch on this subjective consciousness
experience side.
And the first people who made a bold inroad into that was Fechner, together with his
mentor, Vibar.
And so they found a technique, and I'm not going to elaborate too much about it, to
quantify subjective experience.
So to go away from saying you can't measure love, to say I know exactly for love is twice
as much as true love.
And we can measure that.
A lot of you are doing that all the time in the laboratories objectively, and we can
come up with mathematical equations to express these laws.
A little bit later in the 1960s, this was generalized by Stevens by basically introducing
two more techniques that were done by the original inventors of psychophysics, and then
finding a generalization of the mathematical laws that we have.
And then it seemed that we're getting stuck.
And so what might be that next step?
Well, what I'm going to argue about today has many names because it's a new mushrooming
field, but one of them I will use a lot is qualia structuralism.
And the idea is that we can mathematically express our conscious experience of phenomenology
and then find mathematical laws, how that maps onto similar abstract geometrical mathematical
spaces derived from neural data.
That's the basic idea.
Now the first thing that we'll have to do there is we'll have to find the structure
of qualia.
And qualia is a philosophical fancy term for your experience.
So I took this here from a paper by Jennifer Troublat, who has done pioneering research
into that.
And I put it on here because this paper shows that when it comes to these spaces that might
describe our perceptual phenomenological subjective experience, they are non-trivial.
So what Jennifer showed and reviewed in this paper is that these spaces might be non-euclidean,
that certain things that you think are more similar and they move closer to space.
If you look at them from a different angle, all of a sudden they seem to be wider apart.
So it might take more unconventional math, such as quantum, then from quantum theory
and some of the math I'll talk about today to bridge this gap and to describe these things.
But it is, of course, for many of us that study perception, not news that we can express
phenomenological experience in terms of geometrical spaces.
So this right here is the color space that most of us are familiar with, that all of
the colors that we can see, we can put in a three-dimensional space and map onto what
might be largely looking as a sphere.
If you're interested in music, there's a similar geometrical description of the phenomenological
space that you hear in music, which is that if you have a note and you go up an octave,
the note clearly is a higher note, but at the same time there's similarity with the original
note.
And if you do this with all of these notes, you end up with a helical three-dimensional
structure.
And more recently, face space is an example of multi-dimensional spaces that people have
come up with for objects, for faces, for other structures of our experience and mapping them
into these geometrical mathematical spaces.
So if we accept this, that we can come up with mathematical descriptions of a phenomenological
space, then what's the next step to go to the brain?
So this is the suggestion of qualia structuralism, that let's say you have a chord and you have
another chord where you move one of the notes, it sounds similar, it also sounds different,
and that's because it would basically be a transform within that helical musical space
from one of these loops to another.
So if I play an example for that, let's see if this works.
Pretty much the same song, but all I did here is transpose by an octave.
So you were listening to the same structure, I just moved it down in this three-dimensional
helical space.
So the proposal is that if we derive neural activity from the brain and we come up with
a similar mathematically-formalized abstract structure of neural activity, we should see
some kind of relationship as we are experiencing one of these songs and the other song that
might resemble in some way what the phenomenological space does.
So now, in fact, and me in this case as a co-author, we were suggesting that there may
be an isomorphism, that you would find a similar kind of transformation in these space, in
these abstract spaces, but there's many ways, I'm sorry about playing the song again, there's
many different ways that these spaces could relate, and I'm just, I'm not going too much
into depth here, but category theory is a relatively new branch of mathematics, so there's
a lot of buzz around it, and that is exactly what category theory is trying to do.
It's taking different mathematical structures, in this case here you can see geometry and
algebra, and trying to find the relations between them.
And these relations, they can be mathematically expressed, we call punctors.
So in the end, what we're trying to do is we try to find a functor between the abstracted
brain activity and the mathematically-formalized phenomenological structure to translate between
them.
This could be isomorphism, this could be something else.
Well, I hope I convinced you that this part here is a fruitful research program and already
underway, and you might agree with me with this, but you might have doubts about this
one.
So how do we get to these structures from brain activity that would allow us to find
some kind of mapping between the structure-ized qualia space and neuronal space?
Yes?
Yes?
Yes.
Yes.
So what I'm saying, so most of them are correlative, and so what I would go to now is that I'm
trying to find rather than a Pearson correlation, what I'm trying to go at is something that's
actually more like a mathematical law of nature, and I don't want to go too much into the
problems that correlational approaches have, but what I'm going to back to in the next
couple of slides is that we're trying to get something that's deeper in terms of breaking
correlation into causal structures, and that might allow us to make that look more directly.
So am I not convinced yet, but maybe at the end of the talk we can talk about it?
Well, okay, some of the ones that I think about are, but maybe we should talk more about
that talk.
So the theory that, again, I'm just using here to make that leap is integrated information
theory by Giulia Tornoni, and there's a lot to say about integrated information theory.
It's a very complex theory that's truly interdisciplinary.
It spends a lot of different branches of how we usually divvy up our thinking and academia,
so I'm not going to talk too much about it in terms of what the background is and explaining
it.
I think that would be at least another lecture if not a whole graduate seminar of a whole
semester, but I will give you some of the ideas of integrated information theory.
The first one is that if we think about consciousness, typically this is a consensus diagram that
has made around that last couple of years, which is that consciousness seems to be at
least two-dimensional, and that there's one axis, which we would call the level of consciousness,
how much you're conscious.
You might want to call it arousal or something related, and then the other one is the content
of consciousness, which is how much you experience, what you experience, the qualia.
The reason that we think it's at least a two-dimensional state is that we can take various states of
consciousness and put them within this two-dimensional plane, and then you'll find, for example,
that vegetative state is a state of coma where people clearly have a different level
of consciousness, a different arousal, so they wake up in the morning, they open their
eyes, they go to sleep at night, but there's no sign of actual conscious experience in
a lot of these patients, so there's almost no content, but there's a lot of changes
in arousal.
That means we can dissociate these two parts of consciousness, and they're probably orthogonal,
as I put them right here.
Now, for understanding consciousness, I think the really interesting case is up here, where
you are highly aroused and you have a lot of content, as I hope in a state that you're
still in right now.
I'd probably get you more to this state as I keep talking on, but in this state, what
I'm arguing is that you still don't fully have all the access to what consciousness
is doing.
So the example for that would be over here.
If you read this really fast, you would say, oh, I read a bird in the bush, and if I say,
well, try again, you would say, oh, wait a minute, it says a bird in the bush, but the
first time you see that, you probably didn't see the second the.
So that's a famous example of repetition blindness, so that means that even if you're up here
and you have the highest level of consciousness, there are still parts of the world that are
closed up to you that you fail to experience.
When I put it out, you can actually experience.
Yes, question.
Yeah, so I would argue it would be more to this.
It doesn't go without a traditional notion of subjective experience.
Right.
So you have a living being that's undergoing all the signs of arousal, but it doesn't have
any delight to offer.
There's no subjective experience.
That person would be conscious in this definition, but not in this definition.
And that has led to a lot of debate in the field, because there's been a lot of confusion
misunderstanding about it.
So you can have, if you do anesthesia, for example, for more than 24 hours, so some of
us do, they know that the animal gets more aroused in the morning and you have to increase
the anesthesia, but we don't think that the animal is having any subjective experience,
but it's a sign of arousal.
So other people that would say, well, if you're a Buddhist Zen monk and you can reach a state
of consciousness without content, if you really look into the phenomenology of that, there
would still be some content because there is subjective experience, right?
So I'm not talking about that.
So I'm really dissociating basically the almost behavioral signs of arousal from actual subjective
experience.
That's my definition, though.
This might seem different.
Does it make sense?
Yeah, we can talk about that after the talk.
So luckily, this is the part that we're probably in agreement in that I'm most interested
in.
So what integrated information theory does, and that is what it's mostly known for, is
to quantify this axis of consciousness space.
So what integrated information theory does, it gives you a scalar, a single value, and
it tells you how much the system has in terms of level of consciousness.
Lesson well-known is that this orthogonal dimension is also explained by integrated
information theory, and I put this down here.
So when you had this first experience of a bird in the bush, and you missed the second
the, versus you had this experience of a bird in the bush, there was no change in the world.
There was just a change in your mental state.
So that means you had two phenomenological states, and that difference is also quantified
by integrated information theory as a difference in the causal effect structure, or the short,
I will call this the CES.
So there's a delta phi, which is what IET is known for, that shows you the levels of
consciousness, and then a delta CES, which shows you the difference in the contents of
consciousness.
Yes.
Are you saying one of those is more content than the other?
It's different.
Well, I see that, but it's like interesting to figure out your axis of content here, because...
Yeah, so in this particular example, I would say that you have more content when you are
aware of the second the, because you see an extra the.
But that seems questionable, right?
Because in one case, you're paying attention to the meaning, and there's a whole lot of
certain kind of processing.
Whereas in the other case, you're paying attention to something very superficial that may not
be important.
So, and then I might pay attention to the fact that some letters touch the triangle there,
and then I'm aware of that, and I, is that important?
But that's more.
So I...
That's a fair point.
I mean, for me as a vision scientist, just from the visual perspective, I would say you
have more content here than you have here.
But in fact, I'm not...
I don't want to make too much of a point of quantification here, when it comes to the
difference in phenomenology, more that we can look at the difference between two phenomenological
states.
It might be equally content rich, but the fact that there is a difference, we can quantify
the difference.
So that doesn't mean that one, that there has to be an unequal sign between those.
Does that make sense?
Okay.
Yeah.
So let's try and maybe move away from the concept of amount of content.
Okay.
Just the fact that there's different contents, that there's different states of consciousness
that you're in.
They can be formalized in IIT.
Okay.
And so that's all I'm getting at, that there's two formally different states, mathematically
different states.
So let's make it less about the amount, but just the fact that there's a difference and
we can mathematically get at it.
And so just to give you an idea, so these are not just abstract ideas.
There's actual mathematical formalism associated with that.
And so this is from a paper that I will talk about a little bit more about today, where
now and his colleagues, they took neural data, in this case from a fruit fly.
The fruit fly had multiple neurons in its mushroom body, which would be equivalent to
what we call a brain.
And then they measured Phi, which is the level of consciousness between awake and anesthetized
fruit flies.
And you can see that the theory made the correct prediction, which is that Phi should be smaller
in an anesthetized animal than in an alert animal.
This right here corresponds to that in that they tried to get at this causal effect structure.
Now, back when they did that study, the mathematical formalism for the causal effect structure wasn't
fully developed yet.
So that's one of the things I'm saying this is really cutting edge what I'm telling you
today, but they were trying to get at that by coming up with a geometric space that puts
the neural data of the various causes and effects that you find across the neural data
into a multi-dimensional space, in this case, broken down to a three-dimensional space.
And you can see that there's a difference in the causal effect structure as well.
So note the date.
This is really a cutting edge publication.
So what I will show you today is that I think we can go a step further.
So how does this come about?
Well, at the very heart of integrated information theory is this idea that if you have an interconnected
system in which information flows with causal effectivity, let's say that you have four neurons
and they're interconnected in this way and you see the causal flow between these neurons,
that if you can find out about these causal effects by severing some of these connections
and you can do that either experimentally or you can do that.
And this is the interesting part that I hopefully can show you guys today.
You can do that computationally, analytically, with your own data.
And once you do that, you compare the mutilated system where you severed some of these connections
with your original system.
And if there's any difference in the statistical description, then you know what you just mutilated
had a causal effect.
It actually was important for the system.
So you changed the system by mutilating some of these connections.
But if you mutilate some of these connections and there's no difference for the system as a whole,
then you know that these were reducible connections that were actually not important for the function of the system.
And this can be done mathematically.
So this right here is some of the simplified formalism that goes with integrated information theory.
This is when you put it into practice with Python computer code that has been available.
Again, this is the paper that I just referenced from the fruit fly.
And all I will do today is give you a little bit of an overview of what's going on here
and then give you resources that if you are interested and you want to try it with your data,
how you can do that for yourself.
Okay, so why might this be interesting?
Well, more and more what we are doing is when we're talking as neuroscientists,
especially as cognitive neuroscientists,
when we're thinking about the brain, we're thinking about it more and more as a causally connected network circuitry system.
And we are, because of that, measuring more and more data simultaneously across the brain
because the idea is that we just need to know more about what each of these individual parts is doing and how they interact.
And there's two steps involved in that that I think are crucial.
The first one is that rather than just looking at activation,
what integrated information theory does is it abstracts it to information.
And so why is that interesting?
Well, activation is actually more confined than the flow of information.
So if we have, let's say, three neurons here and three neurons here and they're connected with these three axons,
what activation can do is cross over between these neurons, cross its path
because activation has to flow along the physically hard-wired lines of an axon.
But neurons, of course, are not as simple as these systems right here that just receive activation and pass it on.
Neurons can act as logic gates.
They can make, they can compute.
So in this case, if I replace these neurons with X OR gates that are only active,
depending on what the input state is,
then you can see that information now can cross the system.
So as the activation is confined with the physical connections, information flow is more fluid
and actually can process the system in various ways that the activation per se can.
So that's why abstracting from activation to information might be interesting for many of us
in trying to understand what the brain is doing.
The second issue with collecting all of these data simultaneously is that most of the techniques
that we then use analytically to analyze these multidimensional data,
so this would be in this example.
And for Mariah, but the same gets done in EEG or in a singular physiology field,
is that we're trying to get at the multidimensional of the data,
but there's always a step in there that basically boils down to pairwise comparisons.
Very often, Pearson's correlations. It's just a very powerful technique.
So if we do ICA, if we do graph theory, if you look at the various steps involved,
typically somewhere you find that there's pairwise comparisons.
And I would argue that that's a limitation and I would argue that integrated information theory gets past that.
So what do I mean?
So let's take this very simple example of heavy learning that I took from one of the textbooks.
So if you are taking a system of three neurons interconnected, so these neurons connect onto these neurons,
and then you have weights, synaptic weights that you can scale up or down to come up with a learning rule.
Well, in the conventional approach, you would look at this correlation and that correlation.
You look at these pairwise correlations, you can make up a matrix and then make this examination.
But what you're missing out on is the synergistic effect of these two neurons onto that neuron.
So there could be a combined causal effect that you're missing by breaking up the system
and just looking at the pairwise correlations.
And so IIT doesn't do it. Why doesn't it do it?
So here's an example of how the formalism of IIT works in practice.
And the example is that you have a system with two buttons, A and B,
and they basically have causal effects on, in this case, let's say another button C.
So the idea is that we're looking at if button A and B are inactive.
So in this case, shown here in white, so they're not being pushed.
What does that mean for the state of this causally affected system C?
And so you can empirically do that by just taking a system and looking at what's happening.
So let's say you're taking many, many trials, as we often do 100 trials,
and you see what happens if A and B are un-pushed and what happens to C.
And so you see that in this case, in 90% of the cases, C would also be off,
and in 10% of the cases, C would be on.
So this could be because of noise, this could be because of a whole lot of different mechanisms.
And then the logic is to just step through all of the possible states.
So you ask what happens if A is pushed, and you can see 90% again C is off,
what happens if B is pushed, and here the interesting case,
what if both of them are pushed at the same time?
And so this, of course, some of you might have noticed already,
leads to a table that has been used in statistics for a long time.
That's what we call a transition probability matrix,
or it's at the very heart of Markov-Chakes, yes.
So there's a few things to say about that.
So first of all, this technique works best if you do know the causal structure.
So if you know that these have this kind of causal effect on C,
but you don't need to know that, you can infer the causal structure
just by looking at the statistics of the system.
So you don't need to know that.
And so if one of them would have, let's say,
if you said a positive effect or negative effect,
it also, again, would show up in the statistics that you're measuring that comes out.
So the real big step that I'm doing right here that's questionable
is that I'm moving from a frequentist observation to a statistical description of the system.
But of course, that's something that a lot of statistics us all the time.
But I'm arguing that you could observe a system, come up with that table,
and then go on to describe the system.
Other questions?
Okay, so if you're with me on this, then you would agree that you can, of course,
do this for a much more complex system.
So right here, this would be four logic elements that are interconnected.
These could be neurons, these could be voxels, these could be areas,
this could be whatever you're interested in.
And one more trick that I'm doing here is that rather than looking at
how three of these elements interact with the fourth element,
I'm taking the whole system.
So I'm taking all of the possible states that this system could be in.
All of these are on, three of these are on, none of these are on.
And then what I'm doing is I'm taking a step forward in time.
So I will call this the present, and then I'm taking a step forward in time,
and I say, if this is the present state,
what is the probability that I end up in any of these other possible states?
So this is how I get a true transition probability matrix.
In this case, the numbers are zero and one stone at the confuse you,
this would be a deterministic system.
But what we would be measuring most of the time would be fractions in here.
So it would be fractional probabilities with which the system transitions
from one state into the other state.
And so one simple notion would be that this right here is the cause,
and then this right here is the effect,
because we're looking at what is causing what effect by making the jump into the future.
How far you move from the present into the future, that's again up to you.
So you could take a millisecond, eight millisecond, one TR, whatever you want.
But you can look at how the system, depending on which state it is in,
transitions into any of these other states.
Yes.
Using a Markovian assumption here, doesn't that presume the box,
or neurons are going to have a memory of its property?
Neurons are not memory of its state.
Yes, great.
I don't know if I have time to talk about that, but yes.
So this is one of the first things I ran into, looking at actual data.
So as you will see, maybe as I get to that, my argument is that if there's a memory in the system,
then there's a certain expectation to what the transition probability matrix should look like.
And we can actually use that to our advantage.
For example, we can see if there's anything that deviates just from a system
that has a simple form of memory, or it doesn't.
Another question.
So this right here is not the end of it.
So this right here is not what I really mean by cause-effect structures.
There's a second mathematical trick that's involved where you can break down
each of these supposed cause-effect interactions in your data
to look at which ones are actually causal.
And that's maybe the part that I'm most excited about.
So if I do this for my data, it turns out that most of these do not actually really have cause-of-power.
That's what I find the most interesting.
But you might already have noticed that by doing this kind of trick with your data,
that you're already coming up with something that is really interesting to deal with.
So by just taking your data and putting it in this kind of space,
you come up with these very nice mathematical properties already.
And so in fact, if you think about almost anything that we do in machine learning or big data,
starts out with NP arrays like that, two-dimensional matrices.
So I would argue that even if you're not interested in consciousness,
there's lots of room to explore here for your data by using this approach.
Yes?
So does this work only if you assume that you have all the elements in the system,
but what if there are other elements you're not measuring now
and they could also play a third variable role?
Yes.
So it's another shortcoming.
I totally agree.
So ideally, you would have to do this for all the neurons in the brain at the same time.
And so that's a type dream.
And in fact, as I was showing a moment, it already gets problematic with the data that we have.
But I would argue that when we do a Fourier transform,
in fact, even when we do a simple average, we are violating some of the rules
that apply to these algorithms.
So if you do a Fourier transform, you should never do that on one of our data,
whether the variance and the mean are changing over time.
It should be stationary.
And people that do it, they would say, well, it still works.
And so I would say that even if you have a system where you have confounders
that might be interacting with this as more hidden Markov chains, whatever you want to call it,
you don't have the entire Markov blanket.
It's still interesting to look at that system by making these assumptions.
Yeah, of course it's interesting.
Because when can you jump to the causal input?
Yes.
And so basically, my appeal would be that you can do them,
but you have to take them with a grain of salt.
Kind of like we do already with greater causality and other things.
But yeah, so there's an imperfect match between what the theory and theory provides you
and then what we as experimentalists can do with it in practice.
But as I just said, I think there's other techniques.
Granger causality, I think is a good one.
That we violate stationarity, which is one of its main assumptions all the time.
And yet we found interesting things with Granger causality.
And I'm just talking, there's a different way of doing that
that goes away from pairwise considerations of causality to multi-dimensional ones.
Great questions.
Okay, so let's talk about actual data.
So what my lab is mostly interested in are neural circuits.
And the system that we've been choosing for that are the neural circuits
that they of course have the property that Isabel just mentioned,
that they do get inputs from outside the system,
but they are well-structured circuits and they exist across the layers of cortex.
And so one reason that they're interesting is that
technology has made huge jumps in the last couple of years
in order to allow us to measure neuronal activity across the layers of cortex.
So this of course is a commercial approach to that,
but some people don't know yet that these kinds of electrodes that Elon Musk is using
for his company, they're actually readily available down in my office
if you want to see them to research scientists.
So Elon of course is trying to do this in humans.
What does Elon have to say?
Electrons in human brains are the future.
You might get one some time as well.
So thanks to the fact of machine learning we can do these things.
But we use these kinds of electrodes.
So silence, Elon.
We use these kinds of electrodes to measure along the layers of cortex
to get at what we think is a cortical column in the system.
And then we have these simultaneous measurements of neurons or population activity,
and so I'm trying to get to the causal effect structure of these.
So here's the first bad news.
So if you take this again could be voxels or areas,
in this case it's electrode channels of these arrays where we can make these measurements,
and you look at the size of the transition probability matrix,
of course you run into a combinatorial explosion.
So for most of what I will show today I will be stuck at six channels
because my laptop got stuck at six channels.
So if we go up to electrodes that my lab has been using ten years ago,
you already get to numbers that are twice as large as the number of atoms in a cell.
And then this right here is what my lab will be using hopefully in a week from now
and you can see that we're reaching numbers that are absolutely beyond astronomical.
Again, I feel at first when I hear these limitations of the theory
I feel at first maybe a little bit depressed or discouraged,
but then you go and you watch some YouTube talks again about quantum computing
and the leaps that are being done there,
and I think that these kinds of problems are technical problems.
I think that they're solvable in the long run.
There are 10 to the 80 atoms in the universe.
Yeah, I know there's a way more...
The magnitude more than there are atoms in the universe.
This is just one electrode with two other channels.
So when we're talking about measuring all the neurons in the brain,
you see where the theory runs into serious problems.
But as I said, I think these are technical limitations
and also there's other ways to dimensionality reduce
before you maybe apply the theory.
So let's use six measurements of the brain simultaneously
and I will use this graphic to show you all the 64 states
that are possible for these six measurements.
So all of them on, all of them off.
So if these are neurons, on or off is easy to understand.
This might be a neuron firing an action potential, not an action potential.
But most of us, we're not using neurons, I'm not using neurons here.
So what I'm doing is I'm basically binarizing the data.
So you can just take your data, take a trial,
you take the average activity of your trial.
This might be just your fMRI signal, your EG signal, whatever you have.
And then you just say what's above and below.
And whenever it's above, I'm going to call it a non-state
and whenever it's below, I will call it an off-state.
And so the on-states, I make black and the off-states, I turn white.
So I just binarize the data.
The theory would still work if I break it down into four different states, or three.
So it just gives you a larger matrix, more states to work with.
So what does it look like for having 64 states of actual neuron data?
And I would look at them in the present and then I would look at them in the future.
Well, it gives us a matrix, a transition probability matrix,
where you can see that the color here shows the probability of the system
going from one state into the other.
And you can already see that there's an interesting structure,
most prominently, that the system has memory.
So the system likes to stay in its own state.
And the first interesting analysis that I would propose here is
you can use this for your data and then you can play with the time.
You can see how much do I have to move until the system moves out of its original state.
When I do that for my neural data, I find that it is roughly on the order of one synapse.
So if I go past it, then the system automatically starts to go into more interesting states.
You might also feel that this is not an interesting way to look at the data.
So what I would propose is that each time we have a matrix like that,
we can apply graph theory.
So we can take each of these states as a node in graph theory space,
and then the causal interactions, the probability that the state moves from one to another,
we can take as the thickness of the edges between the graph.
So if I do this, for the matrix that I just showed you, it becomes very hard to even visualize.
So let me make it even more simple.
So what I would do now is take three measures of the brain,
and I would do this in the upper, middle, and lower layers of cortex.
So there's only three states.
So all of them are inactive, all of them are active,
and then you can see how the system transitions from one state to the next state.
One thing that's interesting here, I think from a cognitive neuroscience perspective,
is that you see these loops.
So again, this is the system having a certain likelihood to end up in its own state again.
So in a way, you can think of this as feedback.
Well, you can already see that feedback is more likely for some states than for other states.
And you can, again, play around with, for example, the time that you take in between these measurements,
and you can investigate.
Some of the feedback loops, they might become thinner or go away,
and then as you look further into your data, they might reemerge,
as feedback re-enters the system that you're looking at.
Yes, Gordon?
Why is that a feedback loop and not just like...
Or memory.
Yes, yes.
I should be more careful how I phrase these things.
I'm totally agnostic about it.
So the system could just persist.
Could just be history.
Yes.
I think it would be feedback.
It would be easier to argue if you look at that as a function of time,
and it disappears and then reappears again.
That would maybe show that there's some reverberance going on.
But yeah, that's a great point.
And so I'm cutting a long story short of looking at these transition probability matrices
in these actual graph structure.
It does show you...
When I looked at my lab's data, it does show you a lot of interesting structures.
So for example, the question before,
what if the system has a very strong memory?
Well, we do know that a lot of the neural data is one over F distributed,
so it does have memory over time.
And so what I found is that if I compare this actual neural data to a system
that I artificially produce, where I have control over these variables,
and I can't just introduce correlations however strong I want them
between the data or causal directions,
or I completely uncorrelated the system,
then I do find that there's a stereotypical pattern right here.
And as I said before, we could use this as a baseline.
We could say, well, that is what an uncorrelated system looks like,
and any deviation from that would be a more interesting deviation.
Yes?
That's basically what I just said.
I mean, you can take artificial data and you can uncorrelated it.
And so I took it out because I want to use the rest of the talk
to get at the maybe most interesting part.
But if you take a system that's completely uncorrelated,
you end up with a structure that's called the Hamming Distance.
And so the Hamming Distance is that the transition probability of a system
that's in 001 to go to 001 should be higher
than the transition probability from a system to go from 001 to 111,
because more has to change.
The system is more likely to end up in this state than in that state.
And so that you can measure, it comes out as the Hamming Distance,
a very well-characterized geometrical space.
And so you can either use it as a matrix, or you can use it as a graph,
and then you can take any kind of distance that you like
between multidimensional sets of data,
the mahalano-bisterstand, euclidean norms, anything like that,
to see how far it differs.
Okay, so this is just the first step of integrated information theory,
but you can maybe see why I'm already excited about that.
As an experimentalist, we tend to look at new tools
that might allow us to look at our data in new ways,
and I think this is what it does.
And so all of us might have this feeling that Galileo had maybe
when for the first time he was able to look for a telescope,
and he discovered things out in the solar system that haven't been seen before.
So I'm not arguing this as Galileo's telescope,
I'm just saying that here's another interesting tool to look at your data
that might be worth your time and more consideration.
But there's more to it.
So what I said in the beginning is that integrated information theory
severs some of these causal connections and see what the system does.
And so that rests on something called due algebra
that was introduced to the field actually decades ago already by Julia Pearl.
So that's a tool of statistics.
And so again, I'm not going to do it justice, of course,
in the short amount of time that I have,
but I'm just going to give you the general idea.
So if you have a very simple statistical system,
so say you have one variable and you're interested in its effect on the other variable,
and you have a confounding variable that affects both.
That's a very common statistical problem because if you just do a correlative approach,
this right here causes huge pain.
And so we call this a fork in a causal statistical sense.
So what Julia Pearl said is that we can get at the causal effect right here
beyond the correlative approach using his due calculus.
So what is this magical approach?
Well, we know as scientists that in order to find causality,
you have to make interventions.
You have to intervene into the system and then you see causal effects.
What due calculus allows you to do is to intervene into the system without physically doing so.
So you're intervening it statistically, if you will.
So how do you do that?
Well, if you think of this system described in a similar way as I just did,
where you know the transition probabilities between each of these states of the system.
So you have a full statistical description of the system,
and you also know the causal graph.
So you know the causal connections.
What you can do is you can computationally keep one of these variables constant or manipulated
and then study the effect on the whole system.
So in this case, if you keep this variable at a certain value,
you're severing this connection.
Now this variable right here, the confounder, can't affect this one anymore
because you're keeping it constant.
It can't change.
So whatever this guy does, it doesn't affect this guy.
Does it make sense?
It can still affect this guy, but it can't affect this guy anymore.
So you're making an intervention.
And you're also doing what I just said, you're mutilating the system
by getting rid of one of the causal connections.
So you're different in targeted attack analysis in graph theory?
No, I think it's out of my wheelhouse, but it might be very similar.
So let me give you a concrete example.
So if you have a simple transition probability matrix,
so in this case you have only two different parts of the system,
and they're either both off, one is on, the other's on, or both are on.
And then you look at t plus some point in the future,
and you look at the transition probabilities,
and you want to make this intervention.
So you want to mutilate the system.
So what you can do is you can disconnect the system through what,
in this case, would be called statistical noising or marginalizing.
So in this case, let's say the first element we want to get rid of.
And so what you can see right here is that in this case,
in that case, if we eliminate it or eliminate it,
it comes out as basically just averaging these two states together,
because this is zero zero, so let's average where the second one is in zero,
and you get this transition probability,
or in this case the second element is one and one.
So we don't care about this anymore, so we just average that and we come out at point five.
So you come up with a new transition probability matrix
where you artificially intervene and you took out part of the system.
Now this transition probability matrix is different than this transition probability matrix.
So if you look at the transition probabilities,
it looks like you severed something that you shouldn't have.
There's a causal effect in the system.
If you would have done this and it would be exactly the same outcome,
then what you just severed, what you artificially intervened,
didn't have an effect.
And you can basically eliminate it in this approach.
Yes.
Can you go back one?
Yeah.
So you started describing this by saying, assuming you know.
Yeah.
But, I mean, this is a very simple third variable problem.
I mean, we definitely want it now, usually.
Yeah.
And simple, just with three variables,
trying to figure out what's needed and what,
I mean, we can't turn this into correlation and complication with three.
What aspect of this is fixing that problem?
Yeah.
So two responses that, again, you might not find satisfactory.
So the first one is that I would say, we just make that assumption again.
In this case, if you're looking at an individual column,
we're just going to assume that it's closely close
and we're just going to make these assumptions.
So we know, for example, that the middle layer is connected to the upper layers
and the upper layer is connected to the lower layer.
So we can come up with a very simple graph like that.
And we know that there's assumptions and maybe even violations in there.
But we can still use it as a model.
And every model has a shortcoming to look at the data.
But I would agree with you that's problematic.
So what's the other response that I have to that?
Well, these electrodes, those are very specific response, maybe,
to those of us that are doing neurophysiology.
Those electrodes that we have, they now allow you to actually look at connected neurons.
So you're getting 100 neurons at a time and you can look at the correlation of the activity
and you'll find neurons that always basically fire together
with a little lack of, let's say, four milliseconds or so.
So that means that you can establish very simple systems,
in this case, something like this, where you have interconnected neurons
and you're not making any major violations.
This having said, yes, the whole theory, if you take the theory at heart
and if your end goal is to find out if the system is conscious,
how much it is conscious, what the conscious states are,
you would have to do this for maybe all of the neurons at the brain
measured at the same time.
In fact, the people that talk about the theory, they would admit that it could be worse.
It could be that it's not the neurons that matter but the synapses.
And we would have to measure all of the synapses at the same time.
But it could also be that that isn't what is the physical basis
of what gives rise to phenomenology and areas is the way to go, or columns.
We don't know what the right level is.
And so what they are arguing is, I think what is a part of the research program,
is to compute these different values on these different spatial and temporal scales
and see where it peaks.
So, but yes, there are, as with any technique, there's a chasm here
between the theoretical foundations and then how we can use it.
And so my appeal is take it with a grain of salt, but just give it a try.
Violate the assumption and see what happens.
So here's maybe the good transition why I think that might be interesting.
So this right here is actual data from my lab.
And so in blue and in red, you see these are basically neuronal activation.
While the animal in this case is just fixating at the screen, nothing is happening.
Then a stimulus comes on and then in blue, the animal pays attention.
And in red, the animal does not pay attention.
And so we're measuring activity across the layers in this case of area before.
And so there's two things that I want to point out.
So this right here are these representations and graph theory that I just told you about.
These down here are just two of the Pearson correlation coefficients
between the matrices that are underlying, in this case, the red versus the blue state.
And so if the animal is just fixating and nothing is going on,
and I'm computing the phi value, which would be about the level of consciousness in this case,
you can see it's pretty low.
And then when the stimulus comes on and I'm computing the phi value,
making all of these assumptions, you can see it's going up.
So despite all of these assumptions being violated, the theory still seems to hold.
The really interesting part is right here.
If I compute phi versus the state where the animal pays attention,
it's the highest versus if the animal doesn't pay attention,
it sinks even below the baseline value,
as you might expect if the animal is now sucking up all of the attention
to one part of the field rather than widely distributing it.
So this is just one example that the summer break allowed me to do
by getting into my own data and doing MATLAB.
They got me excited because I just put the theory to its test.
I held its feet to the fire and it did what it was supposed to do.
Now, of course, there's many more of these examples in the literature.
So it seems that most of the tests to the fire have so far held true.
The original idea, as I told you guys, was this,
that we're not just getting at the level of consciousness,
but at the contents of consciousness.
So in order to do that, we have to take another step.
And so this step is a transition from integrated information theory,
as you know it, as I've just introduced it,
which got published as Integrated Information Theory 3.0.
It's actually the third revision of the theory.
Now, Integrated Information Theory 4.0 is about to come out next year, I've been told.
But all of the math, most of the math has already been developed
and computer code is available for free on the internet
so that you can already run your data for Integrated Information Theory 4.
What is interesting about Integrated Information Theory 4?
Well, it comes up with a new way to come at that cause-effect structure.
Sorry about that.
I slug it in at a time.
So what it does, it basically takes what I just said
in terms of looking at these causal interactions,
but rather than just looking at the states of the system,
it becomes even more multi-dimensional.
It now takes, you see, combinations of different states
and the more measurements you have, the more come out of it
and basically applies the same kind of logic,
not just to the individual states,
but the combinations of states in between.
And so IIT 4.0 calls these relations
and as I said, the math and the code has just been available for that.
So just give you a brief insight again,
just very little time that I had available to look into that.
For this example that I've been showing a couple of times now
of neural data recorded in the middle upper lower layers
and I'm running this IIT 4.0 code to look at
out of all these possible combinations,
which one of those are irreducible, which one have actual cause of power.
So one thing that's a little confusing is that
we now move to different terminology,
so A, B, and C would be in this case the different layers of cortex
and you can see that only one of these layers turns out to have cause of power by itself.
The other layers of cortex, they only act synergistically.
But more surprisingly to me, most of the possible interactions
that you can theoretically come up with in this multi-dimensional space
turn out to be reducible.
They actually turn out to be non-causal
and I think that is very exciting for most of us that are interested
in how do I deal with these massive amounts of data
that I'm getting from fMRIEG, neurophysiology,
and we can reduce the dimensionality of these data in a meaningful way.
So rather than what a lot of techniques do these days,
take an external perspective and saying I'm decoding the system,
I'm looking into the system, what allows me to predict behavior,
what allows me to post-dict the stimulus conditions.
This is taking an intrinsic perspective and it's saying
which one of the interactions in my data are important to the system.
They actually matter.
So I would argue this might be a way for us to get closer
to understanding the system as it functions itself.
So if you agree with me or if you have only slight disagreements,
you can now see why this qualia structure approach
is something that has gathered a lot of YouTube channels,
a lot of fanfare, a lot of special issues and conferences.
If not, you might feel, one more time.
If not, you might feel like Albert Einstein
that maybe we're going a little bit too far with the math
and we should maybe stick a little bit closer to the data.
In either case, I say the franais, merci.

On June 11th, 2022, a Washington Post published breaking news.
A Google programmer thought that an AI he interacted with was conscious.
Soon after, several news audits published statements from other AI researchers.
They all said that just because it is acting like a conscious being, does not mean an AI
is conscious.
But one question remained unanswered.
How do we know if an AI, or let's say a human being, is conscious?
This is a serious problem, and there are many ethical implications.
For example, we do not know for sure whether animals are conscious, and we do not know
when consciousness arises to a human development.
We can also never be entirely sure whether someone who seems unconscious really has no
consciousness.
Some patients have been shown to remain conscious during anesthesia.
Other patients were found to be conscious after being diagnosed as comatose.
And the problem is not just that we cannot rely on behavior.
We also do not know for sure if someone is conscious by measuring their brain activity.
We have not yet found a reliable biomarker of consciousness.
What we lack is a rigorous scientific theory.
The mathematical equations of such a scientific theory of consciousness should allow us to
determine if someone or something is conscious.
That is, a mathematical theory of consciousness should tell us if an artificial intelligence
is conscious.
Using this math, we should also be able to compute if someone is comatose anesthetized
when dreamless sleep using their brain activity.
More than that, such a theory should allow us to tell what someone is conscious of.
In other words, we should be able to peek into someone's consciousness by applying this
math to neural data measured from their brain.
Does this sound too fantastical?
Well it turns out that such a theory already exists.
This theory is called Integrated Information Theory or IIT for short.
IIT has originally been put forward by neuroscientist Giulia Tornoni in 2008.
IIT has since gained wide support by other leading neuroscientists such as the director
of the Allen Institute, Christoph Koch and many others.
IIT can be expressed as equations, such as these, but IIT is much simpler than it may
seem.
If you are watching, at the end of this video, you will fully understand the mathematics
of IIT and what we can do with it.
IIT starts with a small set of axioms.
Axioms are statements that seem reasonable to accept without proof.
The first axiom is intrinsic existence.
This simply means that we accept that our consciousness exists.
Moreover, our consciousness exists independently of external observers.
If this sounds too philosophical to you, do not worry.
We just need a solid starting point for the theory.
As you will see in a moment, this axiom nicely translates into mathematical equations.
The second axiom of IIT is about information.
Information here just means that consciousness differentiates.
For example, whenever we do not see an elephant, we gain the information that we are not looking
at an elephant.
This definition of information is very different from classic information theory.
As you will see in a moment, IIT does not measure information in bytes or bits.
The third axiom is about integration.
This just means that we have only one consciousness.
We do not have many different fragments of consciousness at the same time.
Our consciousness is unified into a cohesive whole.
The fourth axiom of IIT is composition.
This is as trivial as it sounds.
It means that our consciousness is structured.
We experience different things across space and time.
There is one more axiom.
This axiom is exclusion.
This means that there are boundaries to a conscious experience.
For example, we are only visually conscious of the world in front of us.
If you question one or more of these axioms, do not worry.
There are many axioms in mathematics that are contentious yet useful.
How do we get from these axioms to the rest of the math?
IIT builds on its axioms using hypotheses called postulates.
Postulates translate from the axiomatic properties of consciousness to mathematical characteristics
of a physical system.
In other words, IIT tells us what a system must be like to produce consciousness.
Now, what mathematical descriptions of a system might account for the properties of consciousness?
Then let us first define what IIT means by system.
Well, a system is just a set of things that interact.
Interaction is key.
Remember that the first axiom said that consciousness exists.
Science can only assume the existence of things that interact with other things.
Thus, interaction is the basis of existence.
See how this works?
If we accept that consciousness exists, we can assume that a physical system that produces
consciousness must contain interacting elements.
This is the kind of logic that IIT uses to translate from its axioms to its postulates.
Now let us mathematically formalize what we just said.
Each system consists of two or more elements.
Typically an element can be represented as a discrete variable x sub i.
Elements are defined by three properties.
First, they can have at least two states.
Second, they receive inputs from other elements that can change these states.
And third, they have outputs that depend on these states.
The interacting elements x sub i to sub n form system sub t, where t denotes the present
moment in time.
To keep things easy, let's assume that our four elements here are brain cells or neurons.
As you might know, neurons can either be in an on or off state.
We will indicate these states as unfilled or filled disks.
As all disks are unfilled, all four neurons are off at the current time point t.
Now we said that in order for consciousness to exist, there need to be causal interactions.
These are visualized here by white lines.
How do we define a causal interaction?
Well, remember that each element must have an output that depends on its current state.
These outputs become inputs to other elements.
These inputs define whether or not elements change in state.
So using outputs, single elements, or a combination of elements acting in concert can change the
state of other elements.
That is a causal interaction.
If a subgroup of the system, such as a single element, or a combination thereof, influences
the overall system state, we call that a causal mechanism.
Let us denote mechanisms with y.
And to keep things simple again, let us assume that each element of our model is also a mechanism.
Of course, if we consider the brain, we can think of neurons as elements that are also
mechanisms.
But the mathematics of IIT is more general, as the same concept applies to other brain
structures or even AI.
Now IIT's general idea is to determine whether a system integrates information.
Integrated information is quantified as how much a mechanism in a particular state makes
a difference to the system as a whole.
One way this can be done is by cutting a mechanism out of a system.
If such a cut makes a difference, we can quantify how big that difference is.
How is that done in practice?
Remember, each element has a state, receives inputs, and produces an output.
In our little system, each neuron receives three inputs.
The inputs each neuron receives depend on the state of the other neurons that send their
output to this neuron.
So in our case, each neuron receives a zero or a one from three other neurons depending
on their off or on state.
Real-life neurons perform simple computations, such as summing up their inputs.
If the sum is larger than a certain threshold, neurons change their state from off to on.
So depending on its inputs, a neuron may or may not change its state.
And this state becomes its output.
In our model system, the output is split, so that each of the other neurons receives
an input.
But these are all just copies of the same output.
This means we can describe all possible interactions of this neuron by creating a table.
This table lists all possible combinations of three binary inputs and resulting states.
The resulting state equals that neuron's output.
As we said, real-life neurons sum up inputs.
If the sum is large enough, they turn on.
We can replicate this process in our system by assuming the neuron's threshold to be
three.
In other words, we define a neuron's state to be on, or one, if, and only if, all three
of its inputs are one.
But differently, our model neuron's output is always zero, except when all three of its
inputs are one.
An input-output relationship, such as this, is called an AND gate.
You can probably see why.
The output of this gate is only one if its input 1, and 2, and 3 are all one.
Now let us assume our neuron is much more sensitive and has a much lower threshold for
its summed inputs.
Let's say it's set to one.
In this case, we end up with a very different table.
This kind of table resembles a different logic gate, called OR.
You can see why.
This gate is one if input 1, OR 2, OR 3, OR 1.
Such logic gates are at the heart of computers.
However, keep in mind that actual neurons are much more complicated than that.
So here's our model system again.
Except now we mark for each neuron what kind of input-output table or matrix we have defined
for them.
Now, looking at a single moment in time is not very helpful, since nothing is changing.
And since we're after causal interactions, it is change that we're interested in.
So instead of freezing the system at a single moment, let us look at how the system evolves
over time.
T0 is the current moment, minus 1 is the immediate past, and T plus 1 is one step in the future.
The connections between neurons will be shown between one slice of time and the next one.
In order to calculate the amount of integrated information of a system, we first need to
evaluate the information generated by each single mechanism.
This is done by computing how much each mechanism affects the system as a whole.
This influence is quantifiable, since the current state of a mechanism constrains which
system states can occur.
And IIT goes a step further.
By knowing that a mechanism is in its current state, we also obtain information about the
past, since only a subset of all possible system states can lead up to the mechanism
being in its current state.
This means that a mechanism does not just constrain future possibilities, but in some
sense also constrains the past.
Let's start with the latter.
Suppose we don't know anything about current or past system states.
What would the probability of each possible past state be?
IIT calls this the unconstrained probability or PUC.
For our four neurons, there are 16 possible past states.
We can symbolize these states using zeros and ones, for often on-states of these neurons
respectively.
If we do that, we get a table where each row represents a possible past state.
Since each state is equally likely, our distribution PUC is uniform.
All possible past states might have happened with a probability of 1 over 16.
But remember, this unconstrained probability is due to the fact that we lack information
about mechanism and system states in the present.
Now let's suppose that we know that mechanism X1 is currently in an on state.
Since mechanism X1 only turns on when all of its inputs were on, we can derive a very
different probability distribution of past states.
We now know that there can only have been two past states.
Both states were equally likely.
This means that the conditional probability of each of these states was 0.5.
The resulting distribution is called the cos repertoire of mechanism X1.
This example shows how one can gain information about a system's past by knowing the present.
But how do we quantify that information?
IIT does that by calculating the distance between the unconstrained and the conditional
probability distributions.
The result is called the cos information.
Now that we have seen how we can quantify information about the past that is gained by considering
a given mechanism in its current state, we can apply the same principle to future states.
In other words, we can derive an unconstrained probability of future states as well as the
conditional probabilities of future states given the current state of mechanism X1.
The latter distribution is called the effect repertoire.
Then we calculate the distance between these two distributions.
The resulting number is the effect information of mechanism X1.
Let's recap.
We have seen how IIT quantifies information within a cos system that is contingent upon
or constrained by a mechanism in its current state.
The result of this process are two numbers, the cos information and the effect information.
Of course, both these numbers were specific to a single mechanism in a specific state.
In this case, we looked at mechanism X1, presently being on.
If it were off, both the cos information and the effect information would change accordingly.
And what about the other mechanisms?
Well, if we know the present state of the system, we can simply repeat everything we
have done so far for each of them.
Each time, we obtain the cos information and effect information given that mechanism's
current state.
The final step is to decide whether the cos information or the effect information are
more important to the system.
Since IIT claims to characterize the capacity of a system, it sees the minimum of the two
as a bottleneck.
This bottleneck can be thought of as the weakest link that breaks the chain.
Thus, whatever is the minimum between the cos information and the effect information
is carried forward as the cos effect information of the mechanism being in its current state.
Okay, this explains how IIT quantifies information, but how do we quantify integration?
As it turns out, we can use the same algorithm of comparing probability distributions for
this quantification as well.
Specifically, we eliminate each connection between a mechanism and the rest of the system
and see if that makes any difference.
If the system's probability distributions do not change after we eliminate a connection,
we can deduce that the connection was reducible and there was no integration.
In practice, this is done by a process called statistical noising or marginalizing.
This just means replacing the inputs provided by our connection with a random noise distribution.
Then we compute the distance between the partitioned and unpartitioned probability distributions.
Once we have done that, for all connections, we determine which elimination resulted in
the smallest change.
The partition that yielded the smallest distance is called the minimum information partition
or MIP, and the result of that computation is the amount of integrated information, or
small phi, that is generated by the mechanism.
Now we can compute both the past integrated information as well as the future integrated
information.
As long as both are larger than zero, we can take the minimum to derive the cos effect
integrated information for a mechanism.
Of course, this computation only provided the integrated information on the level of
mechanisms, such as individual neurons.
How do we get from here to the whole brain and consciousness?
Well, we can use the same algorithm of replacing connections with noise, not just for seeing
on mechanisms, but also whole subsystems of a system.
So there you are.
There's a lot more to say of course, but this is the gist of it.
You now understand that IIT provides a simple piece of math that links brain and mind.
By computing the integrated information of a neural system, we can finally start to work
on finding out more about consciousness.

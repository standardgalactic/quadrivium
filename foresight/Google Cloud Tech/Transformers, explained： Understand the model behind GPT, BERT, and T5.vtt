WEBVTT

00:00.000 --> 00:03.440
The neat thing about working in machine learning is that every few years somebody

00:03.440 --> 00:06.800
invents something crazy that makes you totally reconsider what's possible,

00:07.360 --> 00:11.440
like models that can play go or generate hyper-realistic faces.

00:12.000 --> 00:15.520
And today, the mind-blowing discovery that's rocking everyone's world is a type of neural

00:15.520 --> 00:20.000
network called a transformer. Transformers are models that can translate text,

00:20.000 --> 00:24.480
write poems, and off-eds, and even generate computer code. These have been used in biology

00:24.480 --> 00:29.200
to solve the protein folding problem. Transformers are like this magical machine learning hammer

00:29.200 --> 00:33.520
that seems to make every problem into a nail. If you've heard of the trendy new ML models

00:33.520 --> 00:40.000
BERT or GPT-3 or T5, all of these models are based on transformers. So if you want to stay

00:40.000 --> 00:43.200
hip in machine learning and especially in natural language processing,

00:43.200 --> 00:46.880
you have to know about the transformer. So in this video, I'm going to tell you about what

00:46.880 --> 00:51.680
transformers are, how they work, and why they've been so impactful. Let's get to it.

00:51.680 --> 00:56.880
So what is a transformer? It's a type of neural network architecture. To recap, neural networks

00:56.880 --> 01:02.240
are a very effective type of model for analyzing complicated data types like images, videos, audio,

01:02.240 --> 01:06.160
and text. But there are different types of neural networks optimized for different types of data,

01:06.160 --> 01:10.560
like if you're analyzing images, you typically use a convolutional neural network,

01:10.560 --> 01:14.640
which is designed to vaguely mimic the way that the human brain processes vision.

01:14.640 --> 01:18.720
And since around 2012, neural networks have been really good at solving vision tasks,

01:18.720 --> 01:23.680
like identifying objects and photos. But for a long time, we didn't have anything comparably good

01:23.680 --> 01:28.320
for analyzing language, whether for translation or text summarization or text generation.

01:28.880 --> 01:32.160
And this is a problem because language is the primary way that humans communicate.

01:32.160 --> 01:36.240
You see, until transformers came around, the way we used deep learning to understand text

01:36.240 --> 01:41.440
was with a type of model called a recurrent neural network, or an RNN. That looks something like this.

01:42.960 --> 01:45.600
Let's say you wanted to translate a sentence from English to French.

01:46.160 --> 01:50.800
An RNN would take as input an English sentence and process the words one at a time and then

01:50.800 --> 01:56.400
sequentially spit out their French counterparts. The key word here is sequential. In language,

01:56.400 --> 02:01.440
the order of words matters, and you can't just shuffle them around. For example, the sentence

02:01.440 --> 02:05.040
Jane went looking for trouble. It means something very different than the sentence

02:05.040 --> 02:09.360
trouble went looking for Jane. So any model that's going to deal with language has to

02:09.360 --> 02:14.480
capture word order, and recurrent neural networks do this by looking at one word at a time sequentially.

02:14.480 --> 02:19.440
But RNNs had a lot of problems. First, they never really did well at handling large sequences of

02:19.440 --> 02:24.000
text, like long paragraphs or essays. By the time they were analyzing the end of a paragraph,

02:24.000 --> 02:28.960
they'd forget what happened in the beginning. And even worse, RNNs were pretty hard to train.

02:28.960 --> 02:32.880
Because they processed words sequentially, they couldn't paralyze well, which means that you

02:32.880 --> 02:37.280
couldn't just speed them up by throwing lots of GPUs at them. And when you have a model that's slow

02:37.280 --> 02:41.360
to train, you can't train it on all that much data. This is where the transformer changed

02:41.360 --> 02:45.920
everything. They were a model developed in 2017 by researchers at Google and the University of

02:45.920 --> 02:50.640
Toronto, and they were initially designed to do translation. But unlike recurrent neural networks,

02:50.640 --> 02:54.800
you could really efficiently paralyze transformers. And that meant that with the right hardware,

02:54.800 --> 03:01.040
you could train some really big models. How big? Really big. Remember GPT-3, that model that writes

03:01.040 --> 03:06.640
poetry and code and has conversations? That was trained at almost 45 terabytes of text data,

03:06.640 --> 03:12.480
including, like, almost the entire public web. So if you remember anything about transformers,

03:12.480 --> 03:17.200
let it be this. Combine a model that scales really well with a huge data set, and the results will

03:17.200 --> 03:21.680
probably blow your mind. So how do these things actually work? From the diagram in the paper,

03:21.680 --> 03:27.760
it should be pretty clear. Or maybe not. Actually, it's simpler than you might think. There are

03:27.760 --> 03:32.400
three main innovations that make this model work so well. Positional encodings and attention,

03:32.400 --> 03:37.280
and specifically a type of attention called self-attention. Let's start by talking about the

03:37.280 --> 03:42.000
first one, positional encodings. Let's say we're trying to translate text from English to French.

03:42.000 --> 03:45.680
Positional encodings is the idea that instead of looking at words sequentially,

03:45.680 --> 03:49.040
you take each word in your sentence, and before you feed it into the neural network,

03:49.040 --> 03:54.400
you slap a number on it. One, two, three, depending on what number the word is in the sentence. In

03:54.400 --> 03:58.240
other words, you store information about word order in the data itself rather than in the

03:58.240 --> 04:03.040
structure of the network. Then as you train the network on lots of text data, it learns how to

04:03.040 --> 04:08.320
interpret those positional encodings. In this way, the neural network learns the importance of word

04:08.320 --> 04:13.760
order from the data. This is a high-level way to understand positional encodings, but it's an

04:13.760 --> 04:19.200
innovation that really helped make transformers easier to train than RNNs. The next innovation

04:19.200 --> 04:22.800
in this paper is a concept called attention, which you'll see used everywhere in machine

04:22.800 --> 04:27.600
learning these days. In fact, the title of the original transformer paper is attention is all

04:27.600 --> 04:34.400
you need. So, the agreement on the European Economic Area was signed in August 1992. Did you

04:34.400 --> 04:39.040
know that? That's the example sentence given in the original paper, and remember, the original

04:39.040 --> 04:43.680
transformer was designed for translation. Now imagine trying to translate that sentence to

04:43.680 --> 04:49.360
French. One bad way to translate text is to try to translate each word one for one. But in French,

04:49.360 --> 04:53.520
some words are flipped, like in the French translation, European comes before economic.

04:54.240 --> 04:58.240
Plus, French is a language that has gendered agreement between words, so the word

04:58.240 --> 05:03.520
européenne needs to be in the feminine form to match with la zone. The attention mechanism

05:03.520 --> 05:07.760
is a neural network structure that allows the text model to look at every single word

05:07.760 --> 05:12.400
in the original sentence when making a decision about how to translate a word in the output sentence.

05:12.400 --> 05:16.880
In fact, here's a nice visualization from that paper that shows what words in the input sentence

05:16.880 --> 05:21.120
the model is attending to when it makes predictions about a word for the output sentence.

05:21.920 --> 05:26.800
So, when the model outputs the word européenne, it's looking at the input words,

05:26.800 --> 05:31.600
European, and economic. You can think of this diagram as a sort of heat map for attention.

05:32.160 --> 05:36.400
And how does the model know which words it should be attending to? It's something that's

05:36.400 --> 05:41.520
learned over time from data. By seeing thousands of examples of French and English sentence pairs,

05:41.520 --> 05:46.080
the model learns about gender and word order and plurality and all of that grammatical stuff.

05:46.080 --> 05:50.480
So we talked about two key transformer innovations, positional encoding and attention.

05:51.040 --> 05:55.200
But actually, attention had been invented before this paper. The real innovation in

05:55.200 --> 06:00.880
transformers was something called self-attention, a twist on traditional attention. The type of

06:00.880 --> 06:04.400
attention we just talked about had to do with aligning words in English and French,

06:04.400 --> 06:08.480
which is really important for translation. But what if you're just trying to understand the

06:08.480 --> 06:13.440
underlying meaning in language so that you can build a network that can do any number of language

06:13.440 --> 06:18.960
tasks? What's incredible about neural networks like transformers is that as they analyze tons of

06:18.960 --> 06:24.160
text data, they begin to build up this internal representation or understanding of language

06:24.160 --> 06:29.680
automatically. They may learn, for example, that the words programmer and software engineer and

06:29.680 --> 06:34.400
software developer are all synonymous, and they might also naturally learn the rules of grammar

06:34.400 --> 06:39.040
and gender and tense and so on. The better this internal representation of language the neural

06:39.040 --> 06:44.000
network learns, the better it will be at any language task. And it turns out that attention

06:44.000 --> 06:48.480
can be a very effective way to get a neural network to understand language if it's turned on the

06:48.480 --> 06:54.640
input text itself. Let me give you an example. Take these two sentences. Server, can I have the check?

06:55.280 --> 07:00.800
Versus looks like I just crashed the server. The word server here means two very different things,

07:00.800 --> 07:04.160
and I know that because I'm looking at the context of the surrounding words.

07:04.960 --> 07:09.040
Self-attention allows a neural network to understand a word in the context of the words

07:09.040 --> 07:14.080
around it. So when a model processes the word server in the first sentence, it might be attending

07:14.080 --> 07:19.920
to the word check, which helps it disambiguate from a human server versus a mental one. In the

07:19.920 --> 07:23.680
second sentence, the model might be attending to the word crashed to determine that this server

07:23.680 --> 07:28.480
is a machine. Self-attention can also help neural networks disambiguate words, recognize parts of

07:28.480 --> 07:33.760
speech, and even identify word tense. This in a nutshell is the value of self-attention.

07:34.480 --> 07:40.320
So to summarize, transformers boil down to positional encodings, attention, and self-attention.

07:41.040 --> 07:45.840
Of course, this is a 10,000-foot look at transformers, but how are they actually useful?

07:45.840 --> 07:50.080
One of the most popular transformer-based models is called BERT, which was invented just around

07:50.080 --> 07:55.520
the time that I joined Google in 2018. BERT was trained on a massive text corpus and has become

07:55.520 --> 08:00.480
this sort of general pocket knife for NLP that can be adopted to a bunch of different tasks,

08:01.120 --> 08:05.920
like text summarization, question answering, classification, and finding similar sentences.

08:06.640 --> 08:10.800
It's used in Google Search to help understand search queries, and it powers a lot of Google

08:10.800 --> 08:16.320
Cloud's NLP tools, like Google Cloud, AutoML Natural Language. BERT also proved that you could

08:16.400 --> 08:21.360
build very good models on unlabeled data, like text scraped from Wikipedia or Reddit.

08:21.360 --> 08:25.040
This is called semi-supervised learning, and it's a big trend in machine learning right now.

08:27.120 --> 08:31.760
So if I've sold you on how cool transformers are, you might want to start using them in your app.

08:31.760 --> 08:36.800
No problem. Transurflow Hub is a great place to grab pre-trained transformer models like BERT.

08:36.800 --> 08:40.720
You can download them for free in multiple languages and drop them straight into your app.

08:41.440 --> 08:45.920
You can also check out the popular Transformers Python library, built by the company Hugging

08:45.920 --> 08:49.840
Face. That's one of the community's favorite ways to train and use transformer models.

08:49.840 --> 09:01.840
For more transformer tips, check out my blog post linked below, and thanks for watching!


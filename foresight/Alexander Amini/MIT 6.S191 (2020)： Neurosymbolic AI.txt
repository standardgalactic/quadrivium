Thanks so much for inviting me.
This is a great, this is a great class and a great program and I'm really excited to
see deep learning front and center as part of IAP.
I understand you've covered kind of the basics of deep learning and I'm going to tell you
today about something that's a little bit of a mashup on top of sort of standard deep
learning kind of going beyond deep learning.
But before I start, I just want to say something about this word artificial intelligence because
you know I had artificial intelligence in the last slide.
If you look at my business card, it actually says artificial intelligence in two separate
places on that card and I'll have a confession.
I'm a recovering academic so I was a professor at Harvard for ten years.
I just joined IBM about two years ago.
This is my first real job.
My mom's very proud of me that I finally got out of school.
But I will say as an academic researcher working on AI, we hated this term.
2017 and before, we would do anything we could not to say these words.
We'd say machine learning or we'd say deep learning, be more specific.
But 2018 and beyond, for whatever reason, we've all given up and we're all calling it
AI.
We're calling it AI, Google's calling it AI, academics are calling it AI.
But when I got to IBM, they had done something that I really appreciated and it helps kind
of frame the discussion.
It will frame the discussion about what I'm going to tell you about research-wise in a
minute.
And that's just to do something very simple.
This is part of something that IBM does called the global technology outlook, which is like
an annual process where we envision for the company, for the corporation what the future
lies holds ahead.
And they did something very simple just to put adjectives in front of AI, just to distinguish
what we're talking about when we're talking about different things.
So this will be relevant to where we want to push relative to where we are today with
deep learning to where we want to go.
And that's to distinguish what we have today as narrow AI.
So it's not to say it's not powerful or disruptive, but just to say that it's limited in important
ways.
And also to kind of distinguish it from general AI, which is the stuff that the public and
the press likes to talk about sometimes.
When IBM Research, which if you don't know, we're a 4,300-person global research organization,
we have six Nobel Prizes, we've been around for 75 years, when IBM Research tried to decide
when this was going to happen, when general AI was going to happen, we said 2050 and beyond.
And basically when you ask scientists something and they tell you 2050 and beyond when it's
coming, that means we have no idea, but it's no time soon.
But in the middle is this notion of broad AI.
And that's really what we're here today about and what the lab I run is about.
And just to unpack this one level deeper, on one hand, we have general AI.
This is this idea of broadly autonomous systems that can decide what they do on their own.
This is the kind of thing that Elon Musk described as summoning the demon.
So congratulations, everyone.
You're helping to summon the demon, according to Elon Musk, or slightly more level-headed
people like Stephen Hawking, warning that artificial intelligence could end mankind.
This is kind of, you know, maybe we need to worry about this in the future, but actually
what I'll argue in just a minute is that we're actually in quite a bit more limited space
right now.
And really it's this broad AI that we really need to focus on.
So how do we build systems that are multitasking, multi-domain, that can take knowledge from
one place, supply it in another, that can incorporate lots of different kinds of data,
not just images or video, but images, video, text, structure data, unstructured data, it's
distributed, it runs in the cloud, it also runs in edge devices, and it's explainable.
So we can understand what these systems do.
And this is basically then the roadmap for everything that my lab does.
So we're asking, what are the barriers we need to break down to bring in this era where
we can apply AI to all the different kinds of problems that we need to apply to?
So things like explainability.
We need to have systems that aren't just black boxes, but we can look inside and understand
why they make decisions, when they make a right decision.
We know why it made that decision, when they make a wrong decision.
We have the ability to reach in and figure out how we would debug that system.
One interesting thing about the AI revolution, back in the day, people said that software
was going to eat the world.
And these days, Jensen Wang, the CEO of NVIDIA, is on record saying that AI is going to eat
software.
And I think increasingly that's true.
We're going to have data-driven software systems that are based on technology like deep learning.
But the terrifying thing about that is we don't really yet have debuggers.
And it's very hard in many cases to figure out why systems aren't working.
So this is something that's really holding back AI today.
Security, I'll tell you a little bit about the kind of weird world of security we live
in now with AI, where AI systems can be hacked in interesting ways.
We can close those gaps to be able to really realize the full potential of AI.
Systems need to be fair and unbiased.
That's both the thing that's good for the world, but it's also the case that in many
regulated industries, like the kinds of companies that IBM serves, like banks, they're regulated
such that the government insists that their systems be provably fair.
We need to be able to look inside, see, and understand that the decisions the system will
make will be fair and unbiased.
And then on a practical level, I think the real battleground going forward for deep learning
and for AI in general, as much as people talk about big data, actually the most interesting
battlegrounds that we see across many different industries all have to do with small data.
So how do we work with very small amounts of data?
It turns out if you look across all the businesses that make the world run, heavy industries,
healthcare, financial services, most of the problems that those companies faced and that
we face in the world in general don't have enormous, annotated, carefully curated data
sets to go with them.
So if we're going to be able to use AI broadly and tackle all of these hard problems that
we want to solve, we need to be able to learn how to do more with less data.
So part of that has to do with things like transfer learning, learning to transfer from
one domain to another, so learning one domain and then use that knowledge somewhere else.
But increasingly, and this is what I'm going to tell you about today, there's this notion
of reasoning.
So how do we not only extract the structure of the data we're looking at, the data domain
we're interested in, but then also be able to logically and fluidly reason about that
data?
And then finally, just to close it out, just to give you a little bit of a pitch about
what the lab is and what we do, there's also a piece about infrastructure that we think
is really important.
So if you track energy usage from computing year over year, by some estimates by the year
2040, if we keep increasing our energy usage due to computing, we'll exceed the power budget
of the planet Earth.
There won't be enough solar radiation from the sun, not enough stuff we can dig up out
of the Earth and burn to fuel our computing habit.
And AI isn't helping, deep learning is not helping, so many models that we train will
take the equivalent energy of running a whole city for several days, just for one model.
And that's obviously not going to last for a long time.
So we also do work both at the algorithmic level, some of which I'll tell you about today,
but also at the physics level to ask, can we build different kinds of computers?
So this is a diagram of a memristive device, this is an analog computer, which we think
we can get power consumption for deep learning workloads down by maybe a factor of 100 or
even a thousand.
And we're also working in quantum computing.
IBM, as you may know, is one of the leaders in quantum.
We have some of the biggest quantum computers that are available today, and we're asking
how that all interacts with AI.
So when IBM, when we set out to do this challenge of how do we make AI broadly applicable to
all the kinds of problems that we'd like to apply AI to, just as a small plug for the
lab since we're here, we decided we didn't want to do it alone, and we chose a partner.
And in particular, we chose MIT.
And actually, the idea being that this is one of the last standing industrial research
labs of the Bell Lab era, IBM Research, together with MIT, which obviously needs no introduction
because we're here right now, and we're partnering around AI.
And just to give you a little bit of historical context, it actually turns out that IBM and
MIT have been together since the beginning of AI, literally since the term artificial
intelligence was coined, way back in 1956, so right when the very first computers were
being developed.
Nathaniel Rochester, who's the gentleman right there, who developed the IBM 701, which is
one of the first practical computers, got together with MIT professors like John McCarthy,
and dreamed up this future of AI.
And it's actually really fascinating.
I encourage you all to go and find the proposal for this workshop, because a lot of the language,
including neural network language, is all here, like they got a lot of the words right.
They were just a little bit off on the time scale, maybe like seven decades off.
But really interesting.
And the partnership here, the idea here is that we're combining the long horizon, time
horizon that MIT brings to the creation of knowledge, maybe 100-year time horizons, departments
of everything in chemistry, biology, economics, and physics, together with IBM, where we have
a lot of those same departments because we're such a big research organization.
But to bring those together with industry problems, to bring data to the table, so we
can do the kind of research we want to do, and to bring the compute resources along as
well.
So this is what the lab is, and what we do.
We were founded with a quarter-billion-dollar investment over 10 years from IBM.
And we have 50 projects currently, more than 50 projects currently running, over 150 researchers
across MIT and IBM, and their opportunities for undergraduates, for graduate students
to be involved in these projects.
So if you're interested in the things I show you today, we'd love to have you join our
team, either on the MIT side or on the IBM side.
And we're basically drawing from all of the different departments of MIT, and even though
we've only been running for about a year and a half, we have over 100 publications in
top academic conferences and journals.
We had 17 papers in NeurIPS just a few months ago, just to give you a sense of that everything's
up and running.
So this is the evolution, this is where we're going.
So why do I say that today's AI is narrow?
Why would I say that?
Because clearly, AI is powerful, in particular, in 2015 Forbes said that deep learning and
machine intelligence would eat the world.
And I think it's safe to say that progress since 2012 or so has been incredibly rapid.
So this was a paper that really, for me, as a researcher who was working in computer vision,
really convinced me that something dramatic was happening.
So this was a paper from Andre Carpathi, who now leads Tesla's AI program, together with
Fei-Fei Li, who created the ImageNet dataset.
And they built a system which you probably have studied a little bit in this course,
where they can take an image and produce a beautiful natural language caption.
So it takes an input like this, and it produces a caption like, a man in a black shirt is
playing a guitar.
Or you take in this image and you get a construction worker in an orange safety vest is working
on the road.
When I started studying computer vision and AI and machine learning, I wasn't sure we
were going to actually achieve this even in my career or perhaps even in my lifetime.
It seems like such science fiction, it's so commonplace now that we have systems that
can do that.
So it's hard to overstate how important deep learning has been in the progress of AI and
machine learning.
You know, meanwhile, there are very few games left that humans are better than machines
at.
Everything from Jeopardy, which IBM did way back in 2011, to Go with AlphaGo from DeepMind.
I think Carnegie Mellon created a system that could beat the world champion in poker.
And recently, my own company created a system called Project Debater that can actually carry
on a pretty credible natural language debate with a debate champion.
So if you like your computers to argue with you, we can do that for you now.
And even domains like art, which we would have thought maybe would have been privileged
domains for humanity, like surely machines can't create art, right?
That's not the case.
So even way back in 2015, which now feels like a long time ago, Matias Bekka's group
at the Max Planck in Tubingen created the system with Style Transfer, where you could
go from a photograph of your own and then re-render it in the style of any artist you
like.
So this is a very simple Style Transfer model that leveraged the internal representation
of a convolutional neural network up to what we have today, which is, again, just astonishing
how fast progress is moving.
These are the outputs from a system called BigGAN, which came from DeepMind.
And all four of these images are all of things that don't exist in the real world.
So this dog, not a real dog.
You put it in a random vector into the bigGAN, and it generates full cloth, this beautiful
high-resolution dog, or this bubble, or this cup.
And this is actually going to be a problem now, because now we have this notion of deepfakes.
We're getting so good at creating fake images that now we're having to come up with actual
countermeasures.
And that's actually one thing we're working on in the laboratory I run at IBM, where we're
trying to find ganttodotes, like, antidotes, countermeasures against GANs as we move forward.
So clearly, the progress is impressive.
So why am I saying that AI is still narrow today?
Well, does anyone know what this is?
Anyone have any ideas?
Yeah.
Good job, but you're wrong.
It turns out it's a teddy bear.
So if you ask a state-of-the-art ImageNet-trained CNN, and you often see these CNNs described
as being superhuman in their accuracy, has anyone heard that before?
Like, they'll say, object recognition is a solved problem.
These ImageNet-trained CNNs can do better than humans.
And if you've ever actually looked at the ImageNet carefully, the reason that's true
is because ImageNet has huge numbers of categories of dogs, so you basically need to be a dog
show judge to be able to outperform a human at ImageNet.
But this is starting to illustrate a problem.
This image is real.
So this is a piece of art in the Museum of Modern Art in New York by Merit Oppenheim,
called Le Dijonnet en Fereur, a luncheon in Fereur, a little bit unsettling image.
But we, like, who thought it was a teddy bear?
Right?
Like, the most unteddy bear-like image ever, right?
Why did the CNN think this was a teddy bear?
Soft and fluffy.
Soft and fluffy.
It's round.
It's got fur.
So one of the things in the training set would be round and furry teddy bears.
You know, it's a little bit of a garbage-in-garbage-out kind of scenario.
This is, in many ways, you know, people talk about corner cases or edge cases, those rare
things that happen, but are different from the distribution you've trained on previously.
And this is a great example of such a thing.
So this is starting to show that even though deep learning systems we have today are amazing,
and they are amazing, there's, you know, there's room for improvement, there's something missing
here.
Actually, if we dig a little bit deeper, which, you know, a variety of researchers have done,
so this is from Alan Yule's group at Johns Hopkins, even in cases where, you know, the
objects are the standard objects that the system knows how to detect its supposedly
superhuman, you know, sort of levels, if we take this guitar and we put it on top of
this monkey in the jungle, a couple of funny things happen.
One is it thinks the guitar is a bird.
Anyone have an idea why that is?
I hear pieces of the answer all around.
So it's colorful, right?
It's a color that you would expect a tropical bird.
Things that are in the jungle in distribution would tend to be colorful tropical birds.
Interestingly, because you put the guitar in front of the monkey, now the monkey's a
person.
And, you know, again, you know, monkeys don't play guitars in the training set, and that's
clearly messing with the results.
So even though we have no trouble at all telling that these objects are a guitar and a monkey,
the state-of-the-art systems are falling down.
And then even this captioning example, which I highlighted as being, you know, an amazing
success for deep learning, and it is an amazing success for deep learning, when you poke a
little bit harder, which, you know, Josh Tenenbaum and Sam Gershman and Brendan Lake and Tomer
Oman did, you find things like this.
So this image is captioned as a man riding a motorcycle on a beach.
This next one is an airplane is parked on the tarmac at an airport.
And this one, next one, is a group of people standing on top of a beach, which is correct.
So score one for the AI.
But what you can see is there's a strong sense in which the system doesn't really understand
what it's looking at, and that leads to mistakes, and that leads to sort of, you know, missing
the point, you know, in many cases.
And again, this has to do with the fact that the systems are trained on the data, and largely
they're constrained by what data they've seen before, and things that are out of sample,
these edge cases, these corner cases, tend to perform poorly.
Now the success of deep learning, you know, I think it's safe to say it's, you know, two
things happened.
So deep learning, as you already know, is a rebrand of a technology called artificial
neural networks.
It dates all the way back to that Dartmouth conference, at least to the 80s.
You know, a lot of the fundamental map of backprop was worked out in the 80s.
We went through decades of time where it was disreputable to study neural networks, and
I lived through that era where you would try and publish a paper about neural networks,
and people would tell you that everyone knows that neural networks don't work.
So what happened was the amount of data that was available grew enormously, so we digitalized
the world.
We got digital cameras.
Now we're all carrying, I'm carrying like four cameras on me right now.
And we took a lot of images.
And then the compute caught up as well, and particularly graphics processing units, graphics
processing units, GPUs became available, and it turned out they were even better for doing
deep learning than they were for doing graphics.
And really the seminal moment that really flipped the switch and made deep learning take off
was the collection of this data set called ImageNet, which Fei-Fei Li collected, and
it's basically millions of carefully curated images with categories associated with them.
Now you need to have data sets of this scale to make deep learning work.
So if you're working on projects now and you're training neural networks, you'll know that
you need to have thousands to millions of images to be able to train a network and have
it perform well.
That's in stark contrast to how we work.
So does anyone know what this object is?
Just a quick raise of your hands.
Okay, a few people, not so many.
But even though you've never seen this object before, a single training example, you're
now all experts in this object.
Just one training example.
So I can show you to you and ask, is that object present in this image?
I think we all agree, yes.
I can ask you questions like how many are in this image, and I think we'd all agree there
are two.
And I can even ask you, is it present in this image?
And I think you'd all agree, yeah, but it's weird, right?
So not only can you recognize the object from a single training example, not thousands,
not millions, one, you can reason about it now in context where it's just weird, right?
And that's why you can tell that it's a fur-covered saucer cup and spoon and not a teddy bear
because you have this ability to reason out a sample.
And that's really a remarkable ability that we'd love to have because when you get past
imagery, you get past digital images, there are very few data sets that have this kind
of scale that ImageNet has.
But even ImageNet turns out, you know, there's something else wrong with it.
So does anyone notice anything about these chairs in the image, these were all taken
from ImageNet, from the chairs category?
Does anyone notice anything unusual about these?
They're all facing the camera, they're in canonical views, they're all more or less centered.
In the case where there's multiple chairs, they're kind of like almost like a texture
of chairs, right?
So these are very unusual images, actually.
I mean, we look at them and we think these are normal images of chairs, but these are
actually very carefully posed and crafted images.
And one of the projects that we've been working on together with MIT across the MIT IBM Lab
was, this is a project that was led by Boris Katz and Andre Barbou together with our own
Dan Gutfreund, they asked, okay, well, what if we collected a data set where that wasn't
true, so where we didn't have carefully, perfectly centered objects?
And what they did is they enlisted a whole bunch of mechanical turkers on Amazon Mechanical
Turk and they told them, take a hammer, take it into your bedroom, put it on your bed,
and here's a smartphone app and please put it in this bounding box.
So basically you'd have to go and the people get instructions, you know, take your chair,
we want you to put it in the living room, and we want you to put it on its side and
put it in that bounding box.
Or we want you to take a knife out of your kitchen, put it in the bathroom, and make
it fit in that bounding box, or, you know, take that bottle and put it on a chair in
this orientation.
So they went through and they just collected a huge amount of this data, so they collected
50,000 of these images from 300 object classes that overlap with ImageNet, and then they
asked the mechanical turkers to go into four different rooms with those things.
So remember, everyone talks about how ImageNet is state-of-the-art in object categorization
and that's a solved problem, but it turns out when you take these images of these objects
that are not in the right place, humans can perform it well over 95 percent accuracy on
this task, but the AIs, the CNNs that were previously performing, you know, at state-of-the-art
levels, drop all the way down 40 to 45 percent down in their performance.
So there's a very real sense in which, as amazing as deep learning is, and I'm going
to keep saying this, deep learning is amazing, but some of the gains in the, you know, the
sort of declarations of victory are a little bit overstated, and they all circle around
this idea of small data, of corner cases, edge cases, and being able to reason about
situations that are a little bit out of the ordinary.
All right?
And of course, the last piece, you know, that's concerning for anyone who's trying to deploy
neural networks in the real world is that they're weirdly vulnerable to hacking.
So I don't know if you guys covered adversarial examples in this class yet, but here's an
example targeting that same captioning system, so where, you know, the captioning system
can see this picture of a stop sign and produce this beautiful natural language caption, a
red stop sign sitting on the side of a road.
Our own Pinyu Chen, who's an expert in this area at IBM, created this image, which is
a very subtle perturbation of the original, and you can get that to now say a brown teddy
bear lying on top of the bed with high confidence.
So this is a case, again, where there's something divergent between how we perceive images and
understand what the content of an image is and how these end-to-end trained neural networks
do the same.
Like, you know, this kind of, you know, this image, the perturbations of the pixels in
this image were done in such a way that they'd be small so that you couldn't see them, so
they're specifically hidden from us.
But it turns out that you don't have to actually have access to the digital image.
You can also do real-world in-the-wild adversarial attacks.
And this is one that was kind of fun, some folks in my lab and my group decided it'd
be fun to have a t-shirt that was adversarial.
So you took a person detector, and so this is like, you know, it'll detect a person.
You can imagine, like, an AI-powered surveillance system.
If you were to intrude in the building, you might want to have a person detector that could
detect a person and warn somebody, hey, there's a person in your building that doesn't belong.
But what they did is they created this shirt.
So this shirt's very carefully crafted.
It's a very loud, ugly shirt.
We have it in the lab.
If you want to come over any time and try it on, you're welcome to.
But this shirt basically makes you invisible to AI.
So this is Sija, who's our wizard adversarial example.
He's not wearing the shirt, so you can see the person detector is detecting him just
fine.
Tron Fu is wearing the shirt.
Therefore he is invisible.
He is camouflaged.
And you can see, even as you walk around, even if the shirt is folded and bent and wrinkled,
it makes you invisible.
So weird, right?
If anything for us, this ugly-looking shirt makes you even more visible.
So there's something weird about how deep learning seems to work relative to how weird
we work.
But there are also problems where, even under the best of conditions, no adversarial perturbation,
you can have as much training data as you like, where deep learning still struggles.
And these are really interesting for us, because these are cases where, no matter how much
data you have, deep learning just doesn't cut it for some reason.
And we want to know why.
So problems like this, if you ask the question, so I give you a picture, and I ask the question,
how many blocks are on the right of the three-level tower?
Or will the block tower fall if the top block is removed?
Or are there more animals than trees?
Or what is the shape of the object closest to the large cylinder?
These are all questions that even a child could answer, I mean, provided they understand
language and can read and stuff, it's very easy for us to work on these things.
But it turns out that deep learning systems, irrespective of how much training data you
give, struggle.
So that's a case where they're smoking, you kind of want to know where's the fire.
Actually the answer we think, or one of the things we're exploring, is the idea that maybe
the answer lies all the way back in 1956.
So this is a picture of that Dartmouth workshop back in 1956.
And back in this time period, neural networks were already, you know, had already been sort
of born.
We were thinking about neural networks, but we were also thinking about another kind of
AI back then.
And that kind of AI is interesting and different.
It hasn't really enjoyed a resurgence the way that neural networks have.
But just to step back for a moment, this is what you've been studying.
Neural network basically is a nonlinear function approximator.
You take an input in, and you get some output that you want out, and it learns the weights
of the network through training with data.
So this is what an apple looks like to a neural network.
You know, you put an apple picture in, and you light up, you know, a unit that says there's
probably an apple in that scene.
There's another kind of AI that's been around since the beginning called symbolic AI.
And this is from a book by Marvin Minsky in 1991 that was created here.
And this is what an apple looks like to symbolic AI.
So we know things about an apple.
We know that an apple has an origin.
It comes from an apple tree.
We know that an apple is a kind of fruit.
You know, that apple has parts.
It has a body, and it has a stem.
The body has a shape.
It's round.
It has a size.
It can fit in your hand.
It's got a color.
It could be red or green.
We know lots of knowledge about what an apple is.
And that's a very different take on AI.
And you know, basically this field of what we call good old fashioned AI or symbolic
AI has been around since the very beginning, and it just hasn't yet enjoyed that resurgence
that neural networks did.
And one of the central theses that we're exploring is that just the same way that neural
networks have been waiting, they were waiting for compute and data to come along to make
them really work.
We think that symbolic AI has also been waiting, but what it's been waiting for is neural networks.
So now that neural networks work, can we go back to some of these ideas from symbolic
AI and do something different?
And the work I'm going to tell you about is a collaboration as part of the MIT IBM Lab.
Chuangon is one of the researchers in my group together with Josh Tenenbaum, in particular
Jojen Wu, who's now an assistant professor at Stanford, along with some others.
And what they're asking is, can we mix together the ideas of neural networks together with
the ideas from symbolic AI and do something that's more than the sum of its parts?
And this picture that I showed you here, I showed you this earlier, this is actually
a data set called Clever that was basically created to illustrate this problem, where
irrespective of how much training data you have, this very simple kind of question answering
task where you have to answer questions like, are there an equal number of large things
in metal spheres seems to be hard.
So the data set was created to illustrate the problem.
And if you tackle this the way you're supposed to with neural networks and deep learning,
and perhaps you've learned over the course of this IEP course that the best way to train
a system is end to end.
The best way to get what you want is to start from what you have and end with what you need
and don't get in the way in the middle, just let the neural network do its thing.
The problem is that when you build end to end neural networks and try and train them
to go from these inputs to these outputs, for data sets like this it just doesn't work
well at all.
And the reason for that is that the concepts, things like colors and shapes and objects and
things like that, and then the portions of reasoning, like counting the number of objects
or reasoning about the relationships between objects are fundamentally entangled inside
the representation of the neural network.
And then not only does that cause problems where it's very difficult to cover the entire
distribution and not get caught by corner cases, but it also means it's hard to transfer
to other kinds of tasks like image captioning or instance retrieval or other kinds of things.
So fundamentally this end to end approach just doesn't seem to work very well.
So I'm already telling you something that's sort of probably against the advice you've
gotten thus far.
But when you step back and look at well how do we solve this problem of visual reasoning,
you have a question like this, are there an equal number of large things in metal spheres?
When we tackle this problem, well first we read the question and we see there's something
about large things.
We use our visual system to sort of find the large things.
Then we read the question, we see there's something about metal spheres and we use our
visual system to find the metal spheres.
And then critically we do an operation, a symbolic operation, an equality operation
where we decide are these an equal number and we say yes.
So that's what we do.
And if you unpack that, yeah it's got visual perception and CNNs are a great candidate
for doing that.
And yeah it's got question understanding, natural language processing and yeah RNNs are
a great tool for doing that.
But critically it also has this component of logical reasoning where you can very flexibly
apply operations in a compositional way.
So what the team did then was to basically this is kind of like the neurosymbolic hello
world.
It's the first program you write in most programming languages.
This is kind of the simplest example that we could tackle.
And this is the sort of diagram of the flow system and don't worry I'm going to unpack
all this where because neural networks are good at vision we use a CNN to do the vision
part but instead of going straight to an answer it's used to basically de-render the scene.
So rendering goes from a symbolic representation to an image, de-rendering goes from the image
back to some kind of symbolic structure representation so we're going to take apart the image using
the neural network.
And then the question, you'd be crazy not to use something like an LSTM to parse the
language but instead of going from the language straight to an answer or going from the language
to a label or something, the language is going to be parsed into a program, into a symbolic
program which is then going to be executed on the structure representation.
So just to walk you through that, we have a vision part, we have a language part.
You parse the scene using a CNN so you turn, you know, you find the objects in the scene
and you basically create a table that says what are the objects, what are their properties
and where are they and then you do semantic parsing on the language part and again the
goal here is to go from natural language with all of its, you know, sort of vagaries and
messiness to a program, a program that we're going to run in a minute and so you need to
learn how to take this language and turn it into a series of symbolic operations and
then you're going to run that symbolic program on the structured symbolic information and
get an answer.
So you would start by filtering and saying I want to look, the question is asking about
something red so I need to filter on red and then I need to query the shape of that object
that I've just filtered.
So this is basic, you know, sort of program execution.
And critically the system is trained jointly with reinforcement learning so the neural
network that does vision and the neural network that translates from language to a program
fundamentally learns something different by virtue of being part of a hybrid symbolic
system, right?
So it gets reward, it doesn't get reward and you propagate gradients and all that based
on whether or not the symbolic system got the right answer and we use reinforcement
learning of course because you can't differentiate through the symbolic part but, you know, fundamentally
this isn't just a matter of bolting neural networks onto a symbolic reasoner but rather
training them jointly so that the symbolic, so the neural networks learn, extract the
symbols, learn to give the right symbolic representations through experience and learning
on the data.
So this does a couple of really interesting things.
So one of the first things you'll notice, this data set was created, this clever data
set was created because it illustrated a problem with end-to-end learning but it turns out
that with just a dash of symbolic execution now you can be effectively perfect on the
clever data set so clever is now solved and this was actually an oral spotlight paper
at NURBS because this is a big deal, previously unsolved problem now solved, that's good.
But interestingly, more than that, remember I said the biggest problem with deploying
neural networks in the real world is that we rarely have big data, we usually have pretty
small data.
So anything that reduces the sample, improves the sample efficiency of these methods is
really valuable and so here's the number of training examples that the system is given,
this is the accuracy of the system, the neural symbolic system is up here in blue, I'll just
point out several things, one is it's always better and but if you look at, you know, down
here the end-to-end train systems require close to a million examples, they're kind
of, you know, okay results, not perfect but okay, the neural symbolic system again with
just a dash of symbolic mixed in with just one percent of the data can do better than
most of the end-to-end train systems and with just ten percent of the data, one tenth of
the data can perform at effectively perfect performance.
So drastically lower, you know, requirement for data, drastically higher sample efficiency.
And then the last piece, remember I said explainability was super important, you know, people are actually
going to really use AI systems, they need to be able to look inside and understand why
the decision is made, otherwise they won't trust the AI system, they won't use it.
Because the system has a symbolic choke point in the middle where you parse the question
into a series of symbolic operations, we can debug the system the same way you would debug
a traditional coded system, so you can see, okay well what did it do, it filtered on cyan,
it filtered on metal, was that the right thing, you can just, you can understand why it made
the decision it made, and if it made the wrong decision now you have some guidance on what
you'd want to do next.
So that was a paper from this team in 2018, Neurosymbolic BQA, actually since then there's
basically been a parade of papers that have made this more and more sophisticated.
So there was a paper in iClear in 2019 called the Neurosymbolic Concept Learner that relaxed
the requirement that the concepts be pre-coded, there's another paper that just came out in
Europe just a few months ago or last month called the Neurosymbolic Meta Concept Learner
that autonomously learns new concepts, it can sort of use meta concepts to do better,
and we're even now getting this to work in not just these toy images but also in real
world images, which is obviously important.
And we think this is actually a really interesting and profitable direction to go forward.
So here's the Neurosymbolic Concept Learning, basically what's happened is we're relaxing
now these concepts into concept embeddings, so when you look at an object with a CNN you
can now embed into a space of color and then compare that color to stored concept embeddings,
which means you can now learn new concepts dynamically and you can learn them from context.
So you don't need to know that green is a color, you can figure that out and learn that,
this is important because the world's full of new concepts that we're constantly encountering,
so that was the sort of next innovation on the system.
Also remember I said that one of the things that's magical about symbolic AI is that we
can leverage lots of different kinds of knowledge, and in fact we can leverage these sort of
meta relationships between different concepts, we know there are synonyms for instance, which
led to this paper which was just presented last month called the Neurosymbolic Meta Concept
Learner, where you can have a notion of is red the same kind of concept as green or is
cube a synonym of block, which then of course lets you do things like if I go through in
the regular mode here and I'm creating a representation of the object and I'm creating a symbolic program,
I can do the regular thing, but then also critically I can now use relationships I know
about synonyms and concept equivalencies to meta verify these things, and then I can
take advantage of the fact that if I know that there's an airplane then I also know
there's a plane because a plane and an airplane are synonyms.
I can know that if there's any kind of kid and the answer is yes, that is there any kind
of child, I know that's also yes because child and kid are synonyms.
So we can start to see how we can get more and more complex and more and more sophisticated
with our symbolic reasoning and do more and more and more.
Of course it works well.
We're also now extending since clever is now beaten, we're now looking also at, we're
releasing a new data set called video clever, it's called cleverer, which is a very tortured
acronym, looking at the relationships between objects and counterfactuals, what would happen
if this block weren't there, so you can see that we can kind of expand to more and more
sophisticated environments as we go.
I'll just also say that this notion of symbolic program execution isn't the only idea from
symbolic AI that we can bring together with neural networks.
We're also looking at the field of planning, so there's a field of symbolic AI called planning
where you try and start from an initial state and then use an action plan to arrive at some
target state, which is really good for solving problems like the tower of Hanoi which you
may have encountered or these kinds of slider puzzles where you need to produce a series
of operations to achieve a certain end state, like make the picture into the right shape.
Another area of projects that we're working on is mixing these together with neural networks
so that we don't just have to rely on sort of static symbolic representations, but we
can actually work in the latent space of an autoencoder.
We have binary discrete autoencoders and we can actually plan in the latent space of an
autoencoder.
Obviously these are topics that would be a whole talk unto themselves, but I just want
to give you a little bit of a flavor that this idea of mashing up neural networks and
symbolic AI has a lot of range and there's a lot of room to grow and explore and lots
of ideas in symbolic AI now that we can bring together and every time we do we seem to find
that good things happen.
So with that I'll stop just to give you one picture in your mind, these sort of two venerable
traditions of AI.
I think we're coming to a place where we can bring the symbolic stuff out of the closet
dusted off and in many ways the power of neural networks solves many of the problems and they
complement each other's strengths and weaknesses in really important and useful ways.
So with that I'll stop and thank you all for your attention and if you have any questions
I'm very happy to answer them.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.

Consider the following scenario. You are at a party when you hear a short snippet of your favorite song.
Almost instantly, your brain recalls the lyrics of that song and many related memories,
such as attending a recent concert featuring that artist.
It seems very natural and unimpressive. After all, people can recall information all the time.
However, if you think about it, this problem is computationally non-trivial.
Let's put ourselves in the shoes of evolution and try to come up with an algorithm for the brain
to solve it. The first approach that comes to mind is to actually store some kind of a database
of all the songs you've heard a sufficient number of times, along with related information,
like the title and lyrics. When an audio fragment of an unknown song is received,
we can scan through all the songs in our database, find the one that has a close enough match,
and retrieve its lyrics. However, the search space of every song I've ever heard
is astronomically large, and it's even larger when considering every single memory you've formed
since childhood. Performing an exhaustive search would be simply impossible.
Yet, you seem to have no problem instantly recognizing familiar stimuli and finding associations
between them. So, how does the brain accomplish this so quickly? In this video, we will lay the
foundation for a new paradigm of information storage and retrieval, which is more in line with
biology, and actually build one of the simpler models of this process, known as Hopfield Networks,
developed by John Hopfield in 1982, who laid an important groundwork for many ideas in both
neuroscience and machine learning. If you're interested, stay tuned!
Just to reiterate, we need a way to somehow query what we know and find associations between
existing memories and new inputs without explicitly checking individual entries for a match,
which seems like an impossible problem. However, we can draw insights from a seemingly unrelated
field of molecular biology, and in particular, a concept known as Leventhold's paradox of
protein folding. As you may know, proteins are long chains of amino acids that fold into specific
three-dimensional structures which determine their function. The number of possible structural
configurations a protein can take, considering all the different ways you can arrange the atoms
of an amino acid chain in three-dimensional space, is absolutely enormous. Given the number of
possibilities, it seems like it would take an astronomical amount of time for a protein
to search through all the possible structures to find its correct folded state. In fact, there are
computations showing that even if the protein samples its different conformations at a nanosecond
scale, it would still require more time than the age of the universe to arrive at the correct
configuration. Yet, in reality, proteins fold into their native structures in a matter of
milliseconds. So, how do they accomplish this? When I first heard this paradox in high school,
it seemed to me like an ill-posed question. After all, the protein molecule is not a computer,
so it doesn't do any sort of search. It just folds into the most stable and favorable configuration
according to physical laws. This is similar how when you throw a ball, the ball doesn't
search through all the possible trajectories to select the optimal parabolic one. It simply
follows that path because, well, physics works this way. But how can we think about this folding
into a favorable configuration? Favorable for what exactly? Let's introduce the concept of
energy as it will come in handy in future videos as well. If you think back to your high school
physics days, you may recall something along the lines of energy is a quantitative property that
describes this state of a system, namely the capacity to do work or cause change. Energy can be
stored in a variety of different forms, and for the case of proteins, we will be interested in
potential energy stored in the interactions between the atoms in the protein chain.
Each possible configuration of the protein chain has a specific potential energy level
determined by the sum of all of these atomic interactions. In other words, we can assign
a positive number to each state equal to its energy, which is a function in some very high
dimensional space where different dimensions correspond to degrees of freedom you need
to uniquely describe a configuration. For example, all possible dihedral angles of peptide bonds.
Let's abstractly visualize it as having just two dimensions. Then the energy function can be
thought of as a surface where each point on it represents a possible protein configuration,
and the height of the point represents the potential energy of that configuration.
This is what we are going to refer to as energy landscape. For a protein, it would be a complex
rugged surface with many peaks and valleys. Now, here is the key point. A protein molecule,
like any physical system, tends to minimize its potential energy, guided by this second law of
thermodynamics. It will naturally seek out the configuration that has the lowest possible energy
level, as this represents the optimal arrangement of its atoms. And this, in fact, corresponds to the
native, correctly folded state. When a protein is folding, it is essentially rolling downhill on
the energy landscape, following the steepest path towards the valley. This is why proteins can
fall so quickly. They don't need to search through all possible configurations. They simply follow
the natural tendency of physical systems to minimize their potential energy.
The protein's folding process is guided by the shape of the energy landscape,
which in turn is determined by the interaction between its atoms. And the descent along the
surface is essentially driven by the underlying physical process of energy minimization.
Now, the core idea is to achieve something similar for the case of associative memory. Suppose we
have a system that can encode information in its states, and each configuration has a specific
potential energy determined by the interaction between the states. Then we need to first somehow
sculpt the underlying energy landscape so that memories or state patterns we want to store
correspond to local minima, these wells in the energy surface. Second, we need something that
would play the role of the second law of thermodynamics and would drive the changes in the states,
directing the system towards the nearest local minimum. Once these two things are achieved,
retrieving a memory that is most similar to the input pattern is done by configuring the system
to encode the input pattern initially and letting it run to the equilibrium, descending into the
energy well, from which we can read out the source memory. Sounds neat, right? So let's get into
building it. Let's consider a set of neurons which we can think of as abstract units that can
be in one of two possible states, plus one or minus one. This is a simplified analogy of how nerve
cells in the brain encode information through patterns of firing. They either generate an
electrical impulse at a given point in time or remain silent. We'll focus on the fully connected
network where each neuron has connections to every other neuron. These connections have weights
associated with them, real numbers that signify the strength of coupling between the corresponding
pair of neurons. For a pair of units i and j, we denote the connection weight between them as
wij and the states of neurons themselves as xi and xj. In the brain, connections between neurons
or synapses have a well-defined direction. A pair of neurons is connected asymmetrically,
meaning that the synapse from neuron A to neuron B is physiologically separate from the synapse that
connects B to A if that one exists at all, and so they can have different weights. While we could
generalize a Hopfield network to account for asymmetric connections, it would introduce
complications and potentially unstable behavior. For simplicity, here we will stick to the original
formulation of the Hopfield network, which assumes symmetric weights. In other words,
neurons i and j are connected by the same weight in both directions.
Now that we have a set of neurons symmetrically linked with each other through weighted connections,
let's explore what these connection weights represent. If wij is greater than zero,
the connection is said to be excitatory and favors the alignment between the states of
two neurons. We can think of each connection as being either happy or unhappy, depending on the
states of its neurons. For example, if wij is a large positive number, it means that neurons i
and j are closely coupled, and one excites the other. In this case, when one neuron is active,
the other tends to be active as well, and when one is silent, the other one is more likely to be silent.
These configurations, where both xi and xj are either one or minus one, agree with the
connection weight. However, if we observe, for example, that xi is equal to one and xj is equal
to minus one, it conflicts with the excitatory nature of the connection, making such a configuration
less likely. Conversely, when wij is negative, the connection promotes misalignment between the weights.
This alignment between the signs can be expressed more concisely using the product xi times xj.
This product will be positive when both neurons have the same sign and negative when they have
different signs. By multiplying this product further by the connection weight, we obtain
an expression for the happiness of that connection. For a positive wij, happiness will be positive
when the product of the two states is positive. But this is just one edge. We can extend this idea
and compute the happiness of the entire network as a whole by summing this quantity across all edges.
The larger that number is, the more overall agreement there is between connection weights
and pairwise states of neurons. Ultimately, we will search for a set of weights that maximize
this quantity. And maximizing happiness is equivalent to minimizing it with a minus sign,
which you can think of as the measure of overall conflict between the actual configuration of
states and what's favored by the connection weights. This total conflict between the weights
and pairwise states is exactly what we are going to define to be the energy of the system.
As we discussed previously, we want the Hopfield network to be able to gradually evolve towards
energy minima. But looking closely at the formula, we can see that the energy value depends both on
the states and the weights. So there is a lot of things the system can tweak to change it.
What exactly is getting adjusted? As we will see further, there are essentially
two modes of network updates that nicely map to the two aspects of associative memory.
Namely, adjusting the weights corresponds to shaping the energy landscape, defining
which configurations are stable by digging energy wells around them. This is the act of learning
when we are writing new memories into the network. Once the weights are fixed,
tweaking the states of neurons to bring them into greater agreement with the weights
corresponds to descending along the energy surface. This is the act of inference when we are
recalling the memory that is at the bottom of the energy well, which is nearest to the configuration
of the input pattern. Let's take a look at inference first.
Suppose for a second, someone has already set the weights W and hence us the backbone of the
network. The neurons themselves with all the connection weights. However, the exact configuration
of states, which neurons are active and which are silent, is unknown. The question then becomes,
how do we find the state pattern that would minimize the total energy?
As we discussed, simply checking all possible states is not an option. So, we will start with
some initial state, which could be either a partial or a noisy version of one of the memories
or a random configuration altogether. Once the initial condition is set, we will iteratively
try to lower the energy value by focusing on updating one neuron at a time. Let's denote
the neuron we are currently considering as neuron i. We will calculate the total weighted input to
it from all other neurons in the network. This input, which we'll denote as hi, is the sum of
the states of all other neurons multiplied by their respective connection weights. If hi is
positive, it means that the weighted sum of the other neuron states is in favor of neuron i being
in the plus one state. Conversely, if hi is negative, it suggests that neuron i should be in the minus
one state to minimize the conflict with the other neurons. So, we will update the state of the neuron
i based on the sign of hi. Notice that this update is guaranteed to decrease the energy
of the network, because from the two candidate states, we are selecting the more energetically
favorable one. You can think of this as a kind of a voting process. Each neuron looks at the states of
all other neurons weighted by the strength of their connections and decides whether to be active
or silent based on the majority vote. We'll go through this process for each neuron in the network
one by one chosen in random order, updating their states based on the input from all other neurons.
Once we've updated all neurons, we will have completed one iteration of the network inference
and decreased the system's energy by a little bit. We'll keep repeating this process, doing these
sweeps through all neurons, updating them one at a time based on the current configuration.
As we do this, the network will gradually evolve towards a configuration that minimizes the overall
energy. At some point, however, we will reach a configuration where flipping any neuron would
lead to an increase in energy. So, no further adjustments would be necessary. At that point,
the network has converged to a stable configuration, where each neuron's state agrees with the majority
vote. This stable configuration represents a local minimum in the energy landscape.
Now, you might be wondering, is the network guaranteed to reach such a stable configuration?
Could we possibly stumble into a particularly unlucky set of states and get stuck in a never-ending
loop of flipping neurons back and forth? In other words, is such iterative flipping of one neuron
at a time equivalent to doing a descent along the energy surface? This is where we come back to
the point about symmetric weights. It turns out that there is a mathematical proof that I'm not
going to cover here, stating that as long as your weights are symmetric, this simple majority vote
single neuron update rule is guaranteed to eventually converge to a stable configuration
if you do it enough times. To restate it, the Hopfield network can settle into different local
minima based on its initial conditions. These local minima in the energy landscape correspond
to distinct memories stored in the network. When we initialize the network with a pattern that is
similar to one of these memories in some way and let it evolve, it will fall into the nearest
local minimum, effectively recalling the complete stored memory, thus performing pattern completion
or noise correction. But so far we haven't talked about how we come up with the set of connection
weights that encode specific memories in the first place. So let's explore the learning process.
Before we move to storing several memories, let's consider memorizing a single pattern of states.
That means the network would have a single global minimum,
one energy well, and would converge to the same pattern every time no matter where you initialize
it. While it has little practical use, it provides a nice starting point to describe the learning
procedure. Let's denote the template pattern that we'd like to store as XC, which is a vector
packing the states of all neurons, and XCI will denote the ith component, the state of ith neuron
encoding the memory. While XI refers to the state of ith neuron in the network in general,
which could be tweaked. Revisiting our definition of energy, we want to set Wij so that this quantity
would be at its minimal value for the memory pattern. If we plug XI equal to XCI, we get the
equation for the energy of the reference pattern as a function of weights, which we want to turn
into a global minimum. Notice that we don't really care about the absolute value of that energy.
As long as the energy of the desired memory pattern is less than the energy of any other
configuration. Now intuitively, the lowest possible energy is obtained when all the
connection weights fully align with the state pairs. But when we have just a single pattern,
this is very easy to do. All we need is to set the weight Wij to be the product of the corresponding
pair of states in the memory pattern. This way, every connection is satisfied and the energy of
the network when it is in the state XI becomes the negative of the total number of edges.
When the network is in the state XI, any single flip of a neuron would increase the energy,
thus making it a stable state. I want to reiterate the crucial point here. If we want to come up with
the set of weights that would dig an energy well around some pattern, then all we need to know
are the pairwise relationships between states in that pattern. If the two neurons are active
together in the source memory, strengthening the connection between them lowers the hopfield
energy of that memory, effectively storing it in the weights for associative recall.
You may have heard the famous statement from Neuroscience attributed to Donald
Hebb, neurons that fire together, wire together. And in fact, what we just did is known as the
Hebbian learning rule. Great, so we found a way to make a single pattern a stable state of the
network. But we want to store multiple patterns. How do we do that? Here's the key idea. We can
simply sum the weights we would get for each pattern separately. So if we have three patterns,
X1, X2 and X3, we can set the weights according to the following equation.
What this will do is turn each of the patterns into a local minimum. It's pretty straightforward
to show mathematically, and if you're interested, I encourage you to check out the references
in the video description. However, intuitively, if the patterns you want to store are very
different, so they are far away in the state space from each other, then if you first independently
dig energy wells around each of them, and then simply add the energy landscapes together,
the resulting surface will have local minima in the same three valleys. And this nicely brings
us to the limitation of the hopfield networks. There is a limited number of valleys we can sculpt
in the energy landscape before they start to interfere with each other. At some point,
if we try to store too many patterns, the network will fail to converge to a stored pattern reliably
and recall weird in-between kind of memories. The total maximum number of patterns you can
store is thus limited and depends only on the size of the network. It is approximately 0.14
times the number of neurons. So, if you have a hopfield network of 100 neurons, you can reliably
store less than 14 patterns in the best case scenario. If you are unlucky, however, and some
patterns are similar to each other or correlated, their energy wells will begin to interfere even
before you reach the full capacity. All of this makes vanilla hopfield networks not useful for
practical purposes. However, to this day, they provide a powerful and intuitive model of associative
memory, a simple network of neuron-like units that can store and retrieve pattern through purely
local learning and inference rules. Despite their limitations, hopfield networks have laid the
groundwork for more advanced energy-based models. In one of the next videos, we will look at the
extension of the hopfield networks known as Boltzmann machines. These generative architectures
introduce additional hidden units and stochastic dynamics, allowing them to learn more complex
probability distributions. There is also an extension to modern hopfield networks,
published in 2016 with John Hopfield himself as one of the authors, but that's a topic for another
time. In the meanwhile, I'd like to take a moment and thank Shortform, who are kindly sponsoring
today's video. Shortform is a platform that lets you supercharge your reading and gain valuable
insights from books. Their unique approach of book guides goes way beyond simple summaries,
by providing a comprehensive overview of the material. Not only do you get a concise version
of the main points, but you also benefit from related ideas sourced from other books and research
papers on the topic. They have an actively growing library of books from all sorts of genres,
such as science, health and technology. Not only that, but there is a useful AI-powered
browser extension that allows you to generate similar guides for arbitrary content on the
internet. Personally, I found Shortform to be really helpful, both when I'm choosing books to
read and writing notes and flashcards on the topic. Don't hesitate to bring your reading to the next
level by clicking the link down in the description to get 5 dates of unlimited access and 20% off
on annual subscription. If you liked the video, share it with your friends, press the like button
and subscribe to the channel if you haven't already. Stay tuned for more computational
neuroscience and machine learning topics coming up. Goodbye and thank you for the interest in the brain.
Thank you for watching and I'll see you in the next video.

It is humanity's long-time dream to create machines that can think, but what exactly does it mean?
One particular characteristic of intelligence is the ability to generalize knowledge
and flexibly adapt it to new situations. Such generalization is indeed one of the cornerstone
problems in modern machine learning. In this video, we are going to see how we can take
biological organization over hippocampus, a brain structure involved in memory and navigation,
as an inspiration in order to construct a computational model that can learn to build
abstractions and generalizations, as well as the insights we can draw from this model,
both about our own brains and the field of artificial intelligence.
Before we begin, I'd like to warn you that this video is the continuation of the previous video
in the series on cognitive maps. Last time we explored neurobiological background of hippocampal
computations and introduced some general principles, so if you haven't seen it, I highly
recommend you check it out before watching this one, since we are going to build up from there.
If you're interested, stay tuned.
Imagine you're an agent that walks around the world and his only goal is to find rewards.
From an evolutionary perspective, you can think about such an agent as an early organism,
which needs to look for food or mates. Now, as an agent, you have a certain repertoire
of actions that you can take. For example, activate a sequence of muscles to move in a particular
direction. To choose the most rewarding actions, you need to be able to predict the action outcomes,
and that effectively requires a mental model of the surrounding environment.
Existence of such a model allows you to run mental singulations in your head
to weigh the actions. For example, what would happen if I go straight or is it better to turn
right? Over the course of your lifetime, as you encounter a variety of different environments,
initially you might build an entangled, indivisible model for each, without necessarily linking
different models to each other. However, if you're being optimal in your representations,
at some point you realize, wait a minute, all these models I've built so far actually have
an awful lot in common. Indeed, walls that block your way, doors that allow you to go through the
walls and even just the structure of an open 2D space itself all work similarly in every environment,
so these common elements can be easily reused. In other words, it makes sense to break up or
factor each model into its building blocks. For example, building blocks of space, of boundaries,
rewards, etc. Once these building blocks are learned, we can rearrange and mix them in different
configurations to build new models of the world on the fly, and thus generate flexible behavior.
As you might remember from part 1, this is exactly what my mailing hippocampus does,
and we can find neurobiological evidence for this process in responses of individual cells.
Now, the question is, can we teach a machine to do the same?
To make the task easier for an artificial system, let's formulate it as a prediction problem.
Namely, the model will receive a sequence of observations, along with the sequence of actions
that led to them, and learn to correctly predict the next observation in the sequence.
It actually makes a lot of sense biologically. There is a great deal of data suggesting that
the main purpose of the brain may be to predict the incoming stimuli and try to minimize the
prediction error, a theory called predictive coding. For example, consider the following
sequence of observations and actions. Can you tell me what should be the next element in the sequence?
Seems impossible, right? However, what if I told you that those actions, one through four,
actually stand for directions, north, west, south, and east? Now, the task becomes much easier.
Because you know the rules how to chain these actions together, you can predict the next
observation to be the same as the first one, since you know you essentially closed a loop.
In other words, knowing the structure of space significantly simplifies the prediction problem.
But the model, of course, would not know this underlying structure since it would be no fun.
Instead, it would need to extract repeating patterns in order to somehow infer this structure
of the underlying world from sequences of observations and actions. For example, after
seeing a large number of sequences like these, it should infer the rules how different actions
relate to each other, which is equivalent to constructing the structure of space.
It's important to point out that although I'm saying things like the model will learn the
underlying structure of the world, it is not told to do that exactly. The model has no other goal,
so to speak, other than predicting the next observation in the sequence. In essence,
it is just a fancy mathematical expression with a large number of parameters that takes the set
of numbers encoding the observations and actions, performs computation on them, and spits out
another set of numbers corresponding to the next predicted observation. But because we train it to
minimize this prediction error, and since these observations are not random but come from some
structured world, the optimal solution to this prediction problem is to construct some structural
representation of this world, which underlies the regularities in the observations. So we simply
expect the knowledge about the structure to emerge as a result of optimization. But how should the
model look like? Well, because we are free to choose whichever architecture we want, it is reasonable
to draw inspiration from an existing biological machine that solves this problem on a daily basis,
the hippocampal formation. In the last video, we saw how the hippocampus receives two streams of
inputs, sensory, the what am I seeing information, coming from the lateral entorhinal cortex,
and structural, the where am I information, from medial entorhinal cortex. They are then combined
in the hippocampus into a conjoined representation. Similarly, our model will have an analog of
medial entorhinal area, responsible for keeping track of current location in the world.
Let's call it a position module. At every point in time, it will receive an action
and use it to compute the estimate of the current location, the best guess of where it is in space.
You can think of this positional information as being encoded by the pattern of neuron
activations inside of it. Note that the position module operates purely with actions and doesn't
receive any information about the sensory observations. Similarly, how if you close your
eyes and walk around the room, you have a rough idea of where you are located, even though you
don't see anything. This is because your brain is able to accumulate self-movement vectors and
estimate the position, a process known as path integration. So once the model is trained,
we expect our position module to be able to do the same.
Another crucial component is the hippocampus itself, which binds the where information
with what. This binding is effectively forming an association between the two inputs.
So we need to add a memory module that would receive the positional information provided
by the position module together with this stream of sensory inputs and store each
encountered combination in memory. Essentially, it memorizes the associations between position and
observation. I was at X when I saw Y. But storing memories would be useless if we couldn't retrieve
them. Importantly, since this is an associative memory module, it should be able to reconstruct
the full memory from partial information. For example, we could provide it with just the position
and it would go and search in all of the stored memories which observations were accompanied
by this position. So essentially answering the question, what did I see last time I was here?
And similarly, we could provide it with just the sensory observation and it would retrieve position.
Where was I last time I saw this? Now we have all the necessary components
to solve the prediction problem. Let's walk step by step on what the trained model will do to come
up with a successful prediction when walking, for example, on a family tree. Remember, it should be
capable of learning any type of structure, not just the four connected grids. So we start on John,
transition to Mary via a sister action, and then to Kate via a daughter action.
Finally, we give the model the action labeled as Uncle and ask it to make a prediction and what's
happening under the hood is the following. At first, the position module has some initial belief
about the current location which is combined with John and this combination is stored in the memory
module. Next, the sister action is fed into the position module which comes up with a new
belief about location that is combined with Mary and the corresponding conjunction is stored in
memory. Similarly, daughter action is used to update the internal state of the position module
which is combined with Kate and sent to the memory module. Finally, the uncle action is fed into the
position module. Importantly, the resulting positional information, the pattern of neuron
activations is the same as the one we started with. This is because after the model is trained on many
family trees underlaying by the same rules, the position module is configured to always return
to the same position when we make loops like these. In other words, the general laws governing the
transition logic on the world graph become embedded into the rules of how the position module
updates its state. After performing path integration correctly, we return to this starting position
but there is no corresponding sensory observation to memorize. Instead, because the model has reached
the end of the sequence, it tries to predict the next observation but it has the path integrated
position to guide this prediction. So it queries the memory module with the positional information
and retrieves a sensory observation corresponding to this particular position which in our case
is John. Awesome, right? So far we have just been theorizing about this spherical model in a vacuum
but does it actually, well, work? And if so, what does it tell us about our own navigational systems?
The most direct way to assess how well the model is performing is to look at its accuracy which is
just the percentage of predictions it made correctly and importantly look at how quickly the accuracy
grows. Here is what I mean. Imagine for a moment that instead of this fancy machine we had just
a good old lookup table which simply memorizes all the transitions as pairs. Previous observation
plus action equals new observation. So it would store memories like John plus sister equals Mary,
Mary plus daughter equals Kate, etc. And to predict the next observation it would simply scan the
lookup table and search for a particular combination. In the case of our family tree example, on first
try it would not be able to predict that Kate's uncle is John because it hadn't encountered this
particular combination previously. In other words, to reach 100% accuracy it would need to first
encounter all possible combinations of observations and actions and this means that the performance
of the model depends on the number of edges of this graph that it visited. In contrast,
tolman eigenbau machine or TEM doesn't need to be explicitly told at the outcome of every action
from every node because it has the notion of a structure. For example, if I tell you that Kate
is Mary's daughter that is enough for you to infer the rest of the relationships automatically.
And this essentially means that to reach 100% accuracy it is enough for TEM to just visit
all the nodes instead of all possible edges and hence its performance depends on the proportion
of nodes visited which grows much faster than the proportion of edges. So our machine seems to
indeed construct a representation of the world. Hooray! But what's going on inside of its brain,
so to speak? Let's look inside the position module first. Remember, the belief about current
location is encoded with a pattern of collective activation of neurons. But we can also interrogate
individual neurons and look at what each one of them is doing as the agent randomly walks around.
Here for the sake of visualization, I'm going to show results after the model was trained on
regular four-connected grids analogs of physical 2D space rather than social hierarchies.
Remarkably, we see that individual units in the position module develop periodic activity
patterns as a function of position. They tile the space with the regular hexagonal grids of
different size or these periodic stripes exactly like grid cells and band cells of the entorhinal
cortex encode position in mammalian brains. And the selectivity of individual units is
preserved across environments, suggesting that they indeed can generalize. Neurons in the memory
module do something different. Since they form a conjunction between positional and sensory
information, each neuron would be active when both of the two upstream components are active.
Indeed, units in the memory module resemble hippocampal place cells of various size,
which fire in a particular patch of space. Importantly, just like hippocampal representations
in real brains, they are firing patterns differ across environments, since the incoming observations
are different. This is known as hippocampal remapping. I'd like to emphasize that such grid-like
and place-like representations were never hard-coded into the model. We started with essentially a
random set of parameters, let the model optimize itself to come up with the best solution to the
prediction problem, and those responses just emerged naturally. So far, we've trained the model
on sequences that were generated from random walks in a given environment, which means that
all the observations were equally likely. But in the real life, animals don't really move by
diffusion. They are biased towards rewards and exploring objects. They like being near walls
because it feels safe and avoid open spaces. So the question is, if we change the statistics of
the sensory observations so that some stimuli are more common than others, would it affect the
representations that emerge in our model as the optimal solution to the prediction problem?
For example, let's train TEM on sequences of observations that mimic the behavior of a real
mouse, which prefers to spend time near boundaries and approaches objects. In this case, representations
that emerge in the position module now include boundary cells, which are selective to borders of
the world, and object vector cells that seem to activate whenever the animal is at a certain
distance and certain direction away from any object. Both of these types of responses,
that by the way also generalize across contexts, are observed experimentally when recording from
entorhinal cortex, while some neurons in the memory module develop selectivity to particular
objects, resembling landmark cells of the hippocampus. If we take a more complex sequence,
such as the one mimicking the animal that is performing an alternation task, the model successfully
learns the rule that the reward is alternating between the sides. Importantly, representations of
some neurons in the memory module resemble the splitter cells found experimentally,
which are modulated by both the position and the direction of the future turn.
This suggests that TEM has the capacity to learn and map latent spaces, which are not
directly given to it in the observations. Another example of how TEM maps latent space is available
as a bonus clip to my Patreon supporters. More details at the end of this video.
Terrific! Now we have a model that can generalize and naturally develops
same representations of space as the hippocampal formation. So what insights can we draw from it?
Recall that play cells remap, which means they change their preferred firing locations in different
environments. This process has not been thought to be random since there is no immediate logic in
how these representations drift around. But having a model of hippocampal formation at hand,
we can start to address this question on a whole other level. Notice that neurons in our memory
module, the ones that resemble play cells, are actually conjunctions between sensory and structural
information. This means that firing of a particular play cell is partially controlled by grid cells,
which provide the structural information. So, for example, if in one environment,
the location of a given play cell coincides with the hexagonal activity pattern of a particular
grid cell, then after we change the surroundings and the play cell remaps, its place field will shift
to another location which also lies on this grid. In other words, remapping is not completely random,
but rather is controlled by grid cells, preserving some structural information.
This relationship between the locations of place and grid cells implies that there should be a
correlation between the degree to which firing locations of place and grid cells coincide across
two environments. This is the case in the model, and remarkably, when the authors tested this
prediction on experimental data, they found it to be true in real brains as well.
Well, I know this was a ton of information to process, so let's try to tie everything together.
The problem of constructing internal models of the world is cornerstone
for both biological and artificial intelligence. It can be solved by factorizing the surrounding
into building blocks and combining them with particular sensory contexts to generate new
models on the go, allowing for rapid generalization. This factorization and composition can be
demonstrated in a computational model, which, when tasked with predicting the next observation in
its sequence, learns the underlying relational structure of the world. Representations that
naturally emerge in this model resemble real neurons found in the hippocampal formation,
suggesting a unified framework of interactions between entorhinal cortex and hippocampus.
I want to take this opportunity to give huge thanks to Dr. James Whittington,
the first author of the original TAM paper, and Gus, my friend and fellow patron with an
expertise in machine learning, who both helped me immensely with preparing the script for this video.
As a final note, I will mention that the Tolman Eigenbaum machine we've seen today
is actually very similar to a transformer architecture, a type of neural network that
is at the core of modern machine learning. In fact, with one little modification,
we can turn this similarity into a precise mathematical equivalence. And this modified
version, called the Tolman Eigenbaum machine transformer, learns much faster and performs
better, while still resembling biological representations for the most part. This potentially
provides a very promising link between neuroscience and modern machine learning,
which makes both fields even more exciting than ever.
Now, I know this was a very simplified description, but fully exploring this equivalence
would require going over the transformer and hopfield networks in detail.
Let me know down in the comment section if you would like to see a more technical video of this kind.
In the meantime, if you're interested in machine learning and don't want to wait any longer,
let me tell you about something that can take your understanding to the next level,
brilliant.org. Brilliant is a revolutionary platform for engaging and interactive learning.
Gone are the days of passive textbook reading. With Brilliant, you'll engage with the material
in a hands-on way, solving problems, answering questions and participating in stunning interactive
visualizations, which help you develop an intuitive understanding of the material.
One course that you might find particularly interesting after watching this video is titled
Artificial Neural Networks. It offers an accessible introduction into the world of artificial
intelligence and how it is inspired by the human brain. You will learn how neural networks work,
how to build your own, and even how to train them to recognize patterns. But that's just the tip of
the iceberg. With over 80 courses to choose from, Brilliant has something for everyone,
and with its personalized approach, you can learn at your own pace with bite-sized chunks.
Take your curiosity to the next level today. Go to brilliant.org
slash artemker sonoff to get a 30-day free trial of everything Brilliant has to offer,
and the first 200 people to use this link will get 20% off the premium subscription.
If you enjoyed this video, press the like button, share it with your friends and colleagues,
subscribe to the channel if you haven't already, and consider supporting me on Patreon to suggest
video topics and enjoy the bonus content. Stay tuned for more interesting topics coming up.
Goodbye, and thank you for the interest in the brain.

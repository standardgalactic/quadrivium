Our computer scientists at the University of Louisville and specialized in behavioral biometrics,
security of cyber worlds and AI safety. And indeed, one of my personal first steps in the
world of existential risk was to read the number of your publications. And I think you may have
inspired many like me to start working on either AI safety or existential risk or related fields.
And I think you also have always interesting and thought-provoking comments available on each
topic to social accounts, for example. So I really enjoyed reading those and I can recommend them
to everyone. So we are now honored and it is a great pleasure, I can say, to be able to learn
from you today. And we'll give the stage to you now, Dr. Romer Jampolsky.
Thank you so much. Wonderful introduction. I really appreciate that. I think the plan is that
I'll give about a 10-minute intro to my latest work and then we'll let me set up the slides.
Where is it? Where did it go? Yeah, that's right. We can take a little bit longer as well if you
want to. And then we can move to the fireside chat and then the Q&A with audience. Can you see my
slides? Yes. Excellent. So my name is Dr. Jampolsky. I'm a faculty member at University of Louisville.
I've been doing work on AI safety for about 10 years now. Give or take. And my latest work covers
what I will call impossibility results. Problems we encounter with actually
accomplishing what we think is necessary for us to do not just development work for AI,
but also work in terms of control and safety. If you would like to learn more about my work
after this talk, you are definitely welcome to follow me. You can follow me on Twitter,
follow me on Facebook. I always encourage you not to follow me home. It's very important.
So let's start with the big problem right away. If you heard previous talks at this conference,
I had a chance to hear a little bit. You know about concerns a lot of people have with advanced
artificial intelligence, usually called AI safety, AI alignment problem, sometimes AI control problem.
The question is the problem we're trying to solve is how can humanity remain safely in control while
benefiting from superior form of intelligence? So we want this very capable agent to do work for us,
to be helpful, but we want to be in charge. We want to be able to undo any changes if we don't
like them. We want to be final decision makers as to what's going to happen. And the good news
is after 10 years of building up the movement, there is a lot of people working on this now.
There is a lot of research labs, a lot of PhD programs, but I think the main question of
control problem has not been addressed sufficiently. And that is, is the problem actually solvable?
So everyone kind of works on it, but I haven't seen much in terms of proofs or even rigorous
argumentation for why we think we can do this. Why is this problem solvable? Is it solvable?
Is it partially solvable? Maybe it's not solvable. Maybe it's a silly question. It's not even decidable.
So that's essentially what I've been looking at for the last couple years. And I started by
formalizing a little bit what I mean by control problem. So we can talk about different types of
control. We have explicit control where you give orders and the system follows. And this is the
kind of standard Gini problem, right? You wish for things, then you realize that's not what I meant
and you hope you get to undo the damage. Then there is implicit control. So you have a system with
a little more common sense. It doesn't take your words literally, it tries to kind of parse things
in the right way. And there are intermediate steps between explicit and delegated control.
Delegated control is the other extreme where you have this superintelligent system very friendly.
It knows what needs to happen better than you do. And you might be even very happy with the
decisions it makes, but you're not in charge. The system is completely in control. Aligned control
is another intermediate option where the system kind of understands your values, human values,
and tries to do the best it can to fulfill those values, even if your spoken directions are not
exactly what maps onto those ideal values. So it seems at least from now that some sort of intermediate
value between total control and total autonomy for the system is necessary for us to be happy with
the results. If a system is completely autonomous, we have no control over it. By definition, we
will lose control. If a system has no autonomy, it's completely deterministic. We decide ahead
of time what's going to happen. It's not very useful. It cannot be generally intelligent. It's a
great way to do simple tasks for a narrow AI, but it's not something we can utilize to solve
completely new problems and new domains, help us with science, help us cure diseases and things
like that. So what I try to do is kind of break down the bigger problem of control
into all the ingredients we need, all the tools we would need to make controllability possible.
So what do you need? You can start thinking for yourself. If you want to be in control,
well, you kind of have to understand the situation. What is the system doing? Can the system explain
what it's doing? Can you comprehend the explanation? That's another very important one. Can you predict
what the system is likely to do? Maybe not just direction in which it is going, but specifics.
What steps will it take? Can you verify that whatever it is you want the system to do and
program it to do is actually going to happen? So can you verify the implementation versus the model?
Can you verify the model against your goals and data and so on? That is also a need for
general ability to govern AI research, AI systems, so governability of that. And of course,
we communicate with systems in human language and we need to make sure that communication we use
is unambiguous. There is no way to misinterpret commands in a potentially dangerous way.
And there is many, many more such limitations. And what I've been doing, kind of looking at each
one and trying to publish those results. So I'll go over some of the publications I have on that
so far. One paper talks about limits to explainability and incomprehensibility.
So essentially, for very complex systems, large neural networks, it is impossible for the system
to provide an exact explanation of what it is doing or why without simplifying it to the point of
where it is like you explaining something to a child. So a lot of important details are removed
and then a very simplified version is given to you because if a full version is given to you,
you'll simply not be able to comprehend it. And if you want to learn more and see the kind of
argumentation, in some cases, proofs, just go to the paper. I provide all the information you need
to get access to those papers. There are also available as preprints on my Google scholar account.
Another impossibility result is unpredictability and that's our inability to
precisely predict all decisions, all intermediate steps, a much smarter agent will take.
Of course, if we could predict all the decisions of a smarter agent, we ourselves would be that
smart and by definition, there wouldn't be much of an intelligence gap, cognitive gap between us.
That is also problems with verifiability. We know for a fact that with software,
with mathematical proofs, we can only get to a certain degree of confidence, but never 100%
confidence in the fact that we have no errors in the proof. So with more resources, we can
increase safety and security, but we're never able to guarantee something with 100% accuracy,
which is a problem for a superintelligence system, which makes potentially billions of
decisions every minute. Even if one in 100 million creates a problem, you're guaranteed to have a
huge problem within a minute or so. There are also problems with governance. We have history of
trying to govern technology, things like spam and computer viruses. We have laws against those
malevolent software products, but they don't seem to be doing much. So it's not obvious how much
benefit is actually added and other negative consequences from trying to control research,
control what is allowed and not allowed to be experimented with. Even the orders we give to
the system, the communication channel through English is very ambiguous and you're almost
guaranteed to run into situations where your orders will be misinterpreted at multiple levels
due to how imprecise human languages are. So what we did with our colleague, we surveyed a lot of
those impossibility results. Those I looked at and those other people have looked at, I'm not going
to go into details of all of them. I can just tell you there is a lot of them and some purely
software problems, mathematical problems, many problems with physics of the universe,
impossibility results from physics. But if you think even a small subset of all those tools
is necessary to solve the control problem, you have to come to the conclusion that control
is not possible. At least not 100% guaranteed safe, secure control we all dream about. And I
have a very lengthy paper about that, about 70 pages. I now have a few subsections of it
published coming out in conferences, those should be a lot more readable. But I think I
bring up a lot of interesting questions and additional directions for research and hopefully
in the next half hour or 40 minutes we can talk about what all this means and how we can move
forward from where we are right now. Thank you very much, Roman, for this introduction already.
Yes, we can now have a chat about indeed where does it leave us and where should we go from now
and also a couple of related questions. And towards the end, after about 15 to 20 minutes,
we will also take questions from the audience. So please, if you have any questions then please
type them into the chat. We're in the questions section rather. So first, you're spending quite a
lot of time researching AI existential risk, but I don't think it's already obvious for everyone in
the call why AI would be a danger at all. And I don't think everyone is perhaps 100% convinced
that this is actually an issue or an existential danger, at least that is. Could you please recap
how exactly AI could become an existential risk according to you? Right, so there is a lot of
ways to get to that conclusion. I have a few papers where I simply collect examples of accidents,
AI failures throughout history. And if you look at that progression, it's kind of same exponential
chart you see with development. We get more problems, the problems become more severe,
and our ability to anticipate and predict them seems to be very limited. So basically the conclusion
is something like, if you have a system of service to do X, eventually it fails to X.
Frequently it does so very quickly and you go, hmm, okay, my self-driving car just killed a bunch
of pedestrians, that's a problem. And then it's a narrow system, the damage is limited, right? So
self-driving car, okay, the worst it can do is run through some pedestrians. But if a system
becomes general, and it's now controlling not just a single car, but networks of cars,
nuclear response, airline industry, stock market, the damage is proportioned.
I think it's also not the best way to assume that I have to prove that this service or product is
dangerous. Whoever is developing and releasing it has to prove that it is safe, that it's standard
liability law for any product. Show me that this system, which is smarter than me, smarter than you,
smarter than all of us, will never do something within anticipate, something dangerous, something
we don't want it to do. Is this proof could at any point be possible or is it within the
impossibility realm of your theory? Well, I think I'm arguing that it's impossible to do so,
and not just because it's impossible to prove that, but it's impossible to get to that level
of performance. You can get progressively safer and more secure because you can look at specific
accept domains. You can limit what the system can do in certain situations, but you have an
infinite space of possible problems. So it's very hard to prove deterministically that you
can sit at all of them. So if AI safety would indeed be impossible, what does that imply for
AI safety research? Does it imply anything for AI safety research? Is that would that be a waste
of time or is it still something that we should pursue? Not at all. I'm doing it more than ever.
So think about mathematics. We know in mathematics there are many impossibility results. You cannot
prove certain things in general. You cannot have proofs with 100 percent confidence. It doesn't
stop mathematicians from discovering new beautiful mathematics. We know in physics there are limits
to, for example, speed, fundamental limit, you know, speed of light. That doesn't limit us from
doing great work on faster cars and faster rockets. It just tells you that there are limits to what
can be done. And so you should a, not waste your resources trying to accomplish that. Like
knowing that perpetual motion machines are not possible is helpful result in physics.
Same here. We need to understand what we can achieve and then concentrate on what is actually
solvable instead of trying to create magical devices which cannot work.
Next one. For AI to become an existential risk, it's commonly thought that it should
first outsmart humans. How big do you personally think the chances that this will happen at all
and which probabilities do your fellow AI scientists assign to this?
That AI will ever become as smart as humans? Exactly. Well, it's a guarantee. I mean,
we have proof by existence, right? If you just copy human system, you got same level.
We also kind of give a lot of credit to humans because we tend to think about Einstein and
similar type humans as typical examples. Every human is quite dumb. So it's not that hard to get
to that level. And how are you on timelines? Of course, you hear quite values that are quite
far apart. I think Elon Musk said that there could be a five-year timeline up until AGI that's
on the progressive side. On the other side, there are people that claim it would take hundreds of
years. Where are you on this line and how certain are you? That's a very hard question. No one knows
for sure and no one can accurately predict something like that. But if our current theory is about
how systems scale, all right, meaning if you just add more compute, add more data,
you keep making progress, then it becomes a question of cost. How much compute are you
willing to purchase to get to that level? Do you have finite resources or what is here? $200
billion now. So maybe at that level, seven years is a reasonable estimate. With my budget, it might
be 2040. It depends on what type of resources you have. If it's also as difficult as, let's say,
Manhattan Project was, right? You need resources of a whole country to get there. It's one question.
If we discover, okay, there is a simplifying assumption, so we need a lot less resources to
drain this type of engine, a lot less compute. Maybe you can do it with a laptop in a garage and then
it becomes a lot more affordable and takes less time. So I don't have specific dates.
I would be surprised in maybe 2045 if I don't see something at human level. But
that's not important to the argument at all. Whatever it's 2045, 2070, it's still
something we need to worry about today, control and work on safety aspects of it.
I've read somewhere that there are about 70 research projects explicitly aiming for AGI at this
point. I guess the most famous two ones are DeepMind and OpenAI, at least the ones I know best.
Do you know a project that we've never heard of, but actually has a fair chance of beating those two?
Well, there could be many secret projects by secret agencies. I'm sure NSA is very interested
in processing your data more efficiently. So I'd be surprised if they don't have something good
happening. Usually, if you look at the history of what they publicly released and what we later
learned they had, I think they had public ecryptography like 30 years ahead of everyone. So
maybe already. Interesting thought. A week ago, you posted on the Facebook timeline that
I referred to already, which is quite interesting. A quote from that helped me to understand
where is the number of highly respected people who, one, argued that advanced AI is dangerous to
humanity, and two, work as fast as they can on developing advanced AI. And there were, I believe,
116 comments under your post. Have you come any closer to understanding this personally?
No, I had some good explanations and the best one, and I think that's the one Elon actually
gave himself was saying, okay, if we can't control it, I might as well be the one to get there and
I have the best chance of controlling it. People who don't care about safety have less of a chance.
But it is interesting. So a lot of very big names in arguing that AI is extremely dangerous are also
people who invested the most time and money in making it as fast as they can.
Yeah, and on a more serious note, some people might say, if AI is so dangerous, can't we just
not build it? You said something about regulation in your talk, but what would you say to them as a
general response? You can't stop progress on something so useful and so fuzzy in terms of
separation between narrow and general AI. If we could make it where, okay, you only can work on
narrow AI, but not allowed to work in general, it would be a good moratorium to have for a few years.
But the dividing line is meaningless. If you're using neural networks, they're general. If you're
using a lot of those latest evolutionary techniques, they are leading you to general solutions. So it's
simply impossible. If you make all computer science illegal, you're killing your economy,
you're shifting research to other countries. So I think I'll add another impossibility result of
unburnability of AI. You cannot ban it. You can maybe delay it at best.
It would be very interesting if you could either include or exclude it from the impossibility
space indeed, but I'm afraid that goes more into Simon Friedrich's chaos theorem, so to say.
You and I both agree, I think that AI is a significant existential risk,
but some AI researchers don't agree. And do you think there will ever be a scientific consensus
about this? And can we hope to achieve that at some point? And why could that be
either so or not? Well, I have a recent paper about AI risk skepticism, and I do a review of both
why would someone not accept the risks as real and kind of specific arguments they make for it.
I think it ended up with about 100 citations, and I have another 400 unprocessed ones. If anyone's
interested, it could be a nice survey. The most common explanation I see is just bias. If you
get your funding, your prestige, your reputation, everything from developing faster AI, it's very
hard for you to say, I'm working on the most dangerous thing in the world that will kill
everyone. So there seems to be this conflict of interest in any other domain. We wouldn't allow
for this to happen. If you are working for a tobacco company, you wouldn't be deciding if
smoking is dangerous. If you work for an oil company, we don't really trust your assessment
of impact on climate. But somehow here, it's fine. And interestingly, AI is a very large umbrella
term for lots of research sub-domains. Some people do natural language processing, some do vision.
Not everyone does safety and security, but we feel that anyone with a label of AI researcher
is qualified to pass judgment on the state of AI safety in software development. Not everyone
is a cybersecurity expert. If you're working on backend, GUI, something else, you're not going
to be consulted on how to do encryption. Why is this somehow different here? I don't fully understand.
It would be interesting indeed also to find out. I'm also kind of puzzled, but perhaps it could
have something to do with the fact that it's not a trial and error risk as one of the few
areas. I think mostly, of course, you're first developing something and then later you regulate
it, but it's only at the phase of application. So at this phase, it's much more obvious to have
separate controlling agencies, perhaps. But when you're creating something, of course,
that's not that obvious. Maybe one more question about also impossibility of AI safety, but I'm
from a different angle. I don't know if you are aware of the work of Anthony Burglas. He has
written a book about evolutionary arguments applied to AI, and it roughly goes as follows.
For superintelligence, being friendly to people is a necessary baggage. Because of evolution,
we should expect only the most efficient superintelligence to survive, and this is probably
not the friendliest one. Would you agree to this evolutionary argument applied to AI,
or what are your thoughts about this idea? I haven't read the books, so I'm trying to get the
argument from your question. So the argument is that it's more efficient to be friendly to humans,
and so it's a survival advantage. And the other way around, it's more efficient to be
unfriendly to humans, so that would be a survival advantage. Because the friendliness
would just be baggage according to him. Oh, in terms of his overhead and development,
being friendly limits your space of possibilities. Yeah, I think there is a lot to be said about
the trash rest turn option. It starts very friendly, gets the resources and help early on,
and once it's capable, it turns on us and removes all restrictions. So sounds like a good book.
I think it is, you should read it. All right, I'll put it on my list of 600 books to read,
excellent. It was marketed very poorly, so I'm not surprised that you did read it,
but I think the idea is interesting indeed. Are you more on the slow takeover or on the
fast take-off sides, and why would that be? Very fast. Once we get to human level, it goes super
intelligent almost immediately, just adding existing capabilities like infinite memory,
access to all the human knowledge, since they are already super intelligent, if you just take
human plus internet. And why do you think, I think the slow take-off side kind of
gains momentum the last years, I don't know if you agree. And I've heard that this is just because
it's more easy to write papers about this, or there are more possible stories that you could
tell about this, but do you around you see a shift there? Do you see more people going
towards the slow take-off side, or is that not true? I haven't surveyed, I honestly don't know
if you think there is a shift bias by ability to publish about it, I believe you.
I wouldn't make that claim too strictly. Okay, let's say that you're a non-AI expert and you
still want to do something about this existential risk, such as we are kind of. What action do you
think would be the best to take? So you're not an AI researcher, but you want to do something about...
Yes. Is there anything at all, or would you just say, okay, just leave it to the experts,
because there's not much you can do? I mean, in general, I think it's good if citizens are well
informed about the world and problems, and so the next time you vote, you don't vote for someone you
like visually, but actually picking better policies. It seems like based on age and experience of people
were elected, at least in the US, they're not experts on most advanced technology. I hear many
of them don't use computers, so I'm skeptical that they can keep up with crypto economics and
cryptography and synthetic biology and other interesting questions. So your job as a citizen
is to be informed and make sure your views, your informed views, have all represented.
Right, some questions from the audience to not finish off yet, but we're getting to the end of
the conference already. First one, who do you believe is responsible for the safety of AI? The
consumers, governments, or developers, or some other stakeholders?
So that's another interesting question. The ownership of AI itself is very difficult,
right? If it's self-improving, it changes, it's not even obvious who has any control
or possession over it. Obviously, the person to make it and release it has a lot of responsibility,
but if it's out there and now you are upgrading it, supplying it with goals, giving it data,
it feels like responsibility may shift to you. All of it for systems below human level
performance. It's a tool, you are in charge. The moment it's human level or beyond,
it's an independent agent. You are as responsible as you are for your adult children.
Another one that's also very interesting, I think, is it possible to program in a programming
language, not based on human language, to remove the ambiguity? Or would it be possible to have an
AI create a language without ambiguity? If the AI could create such a language, would humans be
able to learn it, or would we then also have to trust the AI to program in it? That's an
excellent question. So there is a lot of effort. First of all, every programming language is an
attempt to get away from English and into less ambiguous languages, but we know languages,
programming languages have lots of bugs. There are logical languages developed to remove
ambiguity. And I think Stephen Wolfram has a nice article about communicating with AI. And he, of
course, uses his Mathematica and models he creates in language, Wolfram language he developed as
possible solution. I think you can do way better than human language in terms of ambiguity. I'm
skeptical about bug-free communication. It relies on your existing cognitive models, your
understanding, and if you have different priors, even using well-defined terms may lead to problems.
But it's a very interesting area to do additional research. If you have
background in linguistics, I definitely invite you to look into that.
Another interesting one from Simon Friedrich, actually a previous speaker. Do you think AGI
could help overcome their global collective action problems that are at the roots of basically
all the existential risks, including those of AI itself?
So that's another great question. I see AI as a meta problem and meta solution. If we get it right,
if I'm wrong and you can make friendly superintelligence well-aligned, everything I said is just a
mistake, then it solves all the other existential problems trivially. Whatever is climate change,
synthetic bio, you have a godlike tool for solving those problems. If I'm right and it's
a terrible risk and it comes before, then it solves it by either killing all of us. We don't
have to worry about it or it comes before again. So if it takes a hundred years for climate change
to build up to boiling point, this happens in 20 years. It kind of dominates the risk.
I'm not sure about applying AI to solve the AI problem. That's a bit of a catch-22. There are
those solutions where you have a supervisory agent, AI, AGI, Nyanee, which looks after the world,
making sure no one creates dangerous AIs. I'm very skeptical of such super agents with a lot of
government control powers. I think they may be worse than what the system we're protecting against
gives us. Great answer, I think. One final question from the audience now. So if we cannot stop AI
development and we cannot totally ensure that it is safe, do we just need to accept that it is a
risk or even a big risk? Or is there anything we can do, for example, policy-wise?
So I think we need to do more research. I published those papers about a year ago and I
haven't seen a strong response from a community addressing those. If somebody just published
a paper saying, this is why you're wrong, then be very happy, but I haven't seen it. So I have to
assume that there is some merit to what I'm saying. The question is then, what do we do with our
lives? How do we update based on that? What do we change? For most people, I don't know if it makes
any difference. Before you were told, okay, you're definitely dying in 60 years. Now you may be dying
in 40. Not a big update. Figure out what to do with your 401k plan. You spend it on something now
or wait for it to become worthless later. I don't have any magical solutions or answers. I am curious
in case of successful alignment what happens to economy, what happens to work, what happens to
people's social interactions. I do have a paper which kind of assumes that progress in virtual
reality will be as good as progress in AI. And so each one of us gets what I call a personal
universe, where you basically get to do whatever you want and you don't have to negotiate with
others. There is no need for consensus. You basically have independence. So at least the
difficult part of value alignment problem is not aligning with me or you. It's hard. It's hard,
but it's not impossible. It's getting 8 billion people plus all the squirrels and whatnot to agree
on something. And this is where the personal universe solution reduces it to just now we need
to control the substrate. If you can get control of computational substrate and everyone gets
the resources to run their personal universe, okay, we're doing well. We have this virtual agreement.
I think that's not a good point again. I'm wondering also on a more personal level,
like when did you start to think about AI safety yourself and when did you move into this
research field? Was there anything that inspired you to do this and also what were the responses
that you got from fellow scientists in moving in this direction? Well, it was a very gradual
process. So I was doing research and behavioral biometrics. I was profiling poker players to see
if, you know, accounts get hacked and tell the things like that. And at the time, I realized
majority of online players are now bots. So my work started to be about detecting bots,
preventing bots from participating. But the question was, as bots get smarter and better,
can we keep up? And not just in poker, but in general online bots and automation.
I did that type of work for a while. I went to what was at the time Singularity Institute for
artificial intelligence, which was fighting hard against artificial intelligence. But
they had a lot of great ideas, which I still work on. And I've been back as a fellow and a
research advisor for Machine Intelligence Research Institute. I think they're doing excellent theoretical
work. Yeah, I think perhaps one more, like some AI researchers, I think, might be hesitant to talk
about existential risk in the public debate, like you already quickly mentioned, for example, in the
media. Do you agree that they are hesitant to do that? And why do you think that is so?
I think I lost a few words. So with media, what's the concern?
Sorry, I'll just repeat the whole thing. Some AI researchers might be hesitant to talk about
existential risk in the public debates, for example, in the media. Do you agree that this is so, that
they are hesitant to do this? And if so, why do you think that is? Well, it's a personal decision
based on your situation. So some people, before they get tenure, follow a very good advice of
be quiet. After you get tenure, never shut up again. But that's not a bad idea. You'll definitely
get someone disappointed in you. And that doesn't help your tenure case. I'm tenured, so I've been
saying stupid things for years now. What do you think about an initiative such as the existential
risk of territory? Is it useful to communicate this to more people in general, or to a certain
subset of people? Or do you think it's basically something that should be solved among researchers?
Well, if you think about developing a GI, working on superintelligence, you're really running an
experiment on all the humans, right? You've got eight billion subjects, none of them consented to
that work. The least you can do is tell them about it. That's actually great now to end this talk.
If there's no more questions from the audience, and I think we've covered those.
Yeah, and then I think we'll leave it here. It was super nice talking to you, and super nice to
listen to your short presentation. And I hope that you will also enjoy the rest of the conference
maybe tomorrow, and that will definitely be in touch and to cooperate more on this
quite hairy problem, but still very interesting one to think about.
Absolutely, and hopefully we'll meet in person one day. Likewise.

In Stephen Hawking's latest book, The Grand Design, he argues that quote-unquote philosophy
is dead.
He treats philosophy as the discipline that asks the question why, whereas science addresses
questions of how.
But he says philosophers have simply not kept up with the science.
Modern physics and cosmology, the topic of his book, can address why questions, hence
philosophy is no longer needed.
Well, with all due respect to Professor Hawking, I greatly respect him as a scientist, I've
read all his books and love them, but perhaps he is taking too narrow a view of philosophy
in this case.
He also fails to account for the fact that science can only exist within a specific philosophical
framework.
The philosophy of science, the topic of this lecture, defines the assumptions, methods,
and the limits of science.
In other words, if you are doing science, you are practicing philosophy.
In this way, philosophy and science are complementary intellectual disciplines.
I think it's counterproductive to say that we don't need philosophy if we're doing science.
In fact, you are.
By the same token, I think that philosophers benefit from having a basic or fundamental
understanding of science, otherwise when thinking about the philosophy of science, they could
be led astray as well, as we will see later in this talk.
Why is science important?
Well, science is the foundation of critical thinking, the methods for testing our beliefs
about the natural world.
The strengths of science as a series of methods is that it is transparent, there's no special
knowledge or hidden factors.
It's rigorous, which is a generic intellectual virtue, it just means you're thorough, systematic,
and quantitative.
In other words, science is a system of methods that seeks to compensate for all of the failings
of human thinking and perception and memory, etc., that we've been talking about up to
this point in the course.
The discipline of philosophy that deals with knowledge is called epistemology.
Epistemology is about how we know what we know, the nature of human knowledge itself.
It addresses questions about what can be known, even theoretical.
It sets the limits on what human beings can know, and it discusses the relationship between
the methods that we use, the information that we have, and our knowledge.
The specific philosophy under which the methods of science practice is called methodological
naturalism.
Methodological naturalism states that material effects must have material causes, that's
the materialism part.
By the word material, we mean the stuff of the universe.
This is not limited to macroscopic matter, it's not even limited to matter itself.
It includes all forms of energy and even now the hypothesized dark matter and dark energy.
These are all considered to be the stuff that the universe is made from.
In addition, methodological naturalism follows natural laws.
In other words, we cannot invoke and then a miracle happens as an explanation for why
something happens in an experiment or observation, for example.
Methodological naturalism is similar to methodological naturalism, but it goes one step further.
This is the philosophical stance that the material universe and the natural laws is all that
there actually is.
Science however does not require such a belief or a philosophy.
It doesn't require that we believe or accept the notion that there is nothing beyond the
natural world.
But it does require that we follow the methods that assume there is nothing beyond the natural
world or that we, in other words, do not rely upon any supernaturalism, anything beyond
the natural world.
It does however make assumptions about the world.
The philosophical underpinnings of science are based upon some assumptions.
There is, for example, an objective reality out there somewhere.
If we weren't living in an objectively real universe, then it would not be possible to
investigate how that universe works.
Science also assumes that the world is predictable and therefore ultimately knowable.
If the laws of nature, for example, were changed on a whim, then it would be impossible to
extrapolate from observations to general laws about how the universe works.
The inductive reasoning that science is largely based upon.
Again, science does not say that these things are true, that the world is predictable and
knowable, but it requires the assumption that they are true.
Science proceeds as if the world is objective, predictable, and knowable.
The methods of science are many and various.
There is not a single scientific method.
It's a collection of methods.
I think, though, at the core of the scientific methods is the notion of hypothesis testing,
formulating an idea in a manner that it can be theoretically and practically subjected
to some kind of objective testing.
This includes the notion that the idea must be able to be proven false.
Testing is also not limited to laboratory experimentation.
It doesn't mean that you have to be in a lab with test tubes, etc.
It could be any way in which we can gather objective information that can have implications
for whether or not a hypothesis is likely to be true or not.
This can include just further observations of the world around us.
It can also include inferences based upon direct evidence.
As long as evidence is gathered in a systematic way and that that evidence can be for or against
one hypothesis or theory, then the notion is testable and you're doing science.
Historical science has often come up in this context.
We cannot, for example, rerun the Big Bang in a laboratory.
We cannot be subjected to direct experimentation in that way.
However, we can ask questions about the existence and the nature of the Big Bang and then make
observations that test those predictions, those theories.
For example, observations of the cosmic background radiation, the radioactive noise that's left
over after the Big Bang.
We cannot go back also in time to witness evolution happening before our eyes or make
evolution of one species into another happen in the laboratory necessarily.
But we can make inferences from the fossils that we find and observe in the world.
We can make inferences from the pattern of genetic evidence that we find in living creatures
and on our observations of what happens during the developmental period from a single cell
to a mature organism.
We can see components of evolution in a lab also, so it's a combination of experimental
and observational data that we're using.
Science also needs to be understood as a human endeavor.
It's not something that occurs only in the abstract or only as a philosophy.
Science is something that people do.
It is therefore imperfect, messy, has many false steps and is plagued with bias and error.
That doesn't mean, however, that it's hopeless because science is also self-correcting.
That is perhaps its strongest feature.
It tends to slowly grind forward as ideas and beliefs are tested and then retested by
one group and then another and then another generation.
And only those ideas which survive repeated attempts at proving them wrong tend to survive
over time.
There's also no one pathway of this process of science.
Textbooks may give a simplified view of how science progresses.
An observation leads to a hypothesis, which is a guess about what is responsible for the
observation, which is then tested by an experiment.
After the result of the experiment, the hypothesis is refined and then the experimental phase
is repeated and this then goes further.
However, science can follow many pathways.
It doesn't necessarily always follow in that same order.
For example, a hypothesis may come before even a single observation is made.
It's all okay as long as in the end the hypotheses are being tested in some objective way.
For example, we may make observations in the light of existing theories and those theories
may bias the way in which we make those observations.
We may find inspiration from many sources which are chaotic, culturally dependent and
determined.
There is no pristine single method of science.
This led philosophers like Thomas Kuhn to think about the nature of scientific progress
over time.
It is messy.
It isn't reduced to this simple pristine textbook caricature.
Thomas Kuhn developed the idea of paradigms in science.
A paradigm is a large overarching idea.
It's like a theory but maybe a little bit bigger.
It's a way of explaining a lot of things in science.
Scientific progress, he argued, is divided into periods of normal science where there
is slow progress and refinement of ideas within a paradigm, within a bigger idea.
Then punctuated by periods of scientific crisis where problems cannot be solved by tweaking
an existing paradigm.
This leads to a paradigm shift where one explanatory model is entirely replaced by a new model
or a new paradigm.
Kuhn's ideas were used to further develop what is now called the postmodernist view
of science, that it is culturally dependent and not progressive.
Now while, as I had previously discussed, there is some truth to that notion, science
is culturally dependent and it's not always progressive.
But postmodernists took that view to the extreme that there was no progress in science.
That one paradigm, in Kuhn's words, was equivalent to the next paradigm.
That when one paradigm got replaced by the next, this was not an inherently progressive
process, that paradigms can only be assessed within their own framework.
Kuhn himself, however, rejected this idea.
He said there is copious evidence that progress does occur in science.
Some ideas in science are better than others and the long history of science is characterized
by this slow, continuous grinding forward of producing better and better models of how
the world works.
Kuhn's model was further criticized as a false dichotomy.
In reality, there isn't necessarily periods of normal science and paradigm shifts.
In fact, there is a continuum of degrees of progress within science.
There are tiny steps, larger steps, and bigger and bigger major shifts in our scientific
thinking.
Those of science have largely now moved beyond the postmodernist view, or at least the extreme
postmodernist view, that science is not even progressive and is not a privileged way of
looking at the world.
It has no privileged relationship to the truth that it's just essentially another form of
philosophy or of cultural ideas.
Philosophers of science now understand that this view was extreme and not really an accurate
description.
In fact, the specific criticism that has been leveled against the extreme postmodernist
view of science is that it confused the context of discovery, which is chaotic and culturally
dependent, with the context of later justification.
Regardless of how new ideas are generated in science, they are eventually subjected
to systematic and rigorous observation, experiment, and critical review, and it is that later
justification which gives science its progressive nature.
Another philosophical objection raised sometimes to the notion that science can actually tell
us something about the way the world actually is, is that science cannot prove itself.
Now, philosophically speaking, this is true, but it's irrelevant because the philosophy
of science doesn't claim that it has objective metaphysical certitude.
Science works within the philosophical framework of science, and that's all it claims to do.
However, we can say that there has been a meta-experiment of science, if you will, that
the assumptions of science, that the world is knowable, that it is predictable, and that
there is an objective reality, and that the methods of science are able to improve upon
our ideas and models of how the world works.
If all of those assumptions are correct, then the endeavor of science should produce
some objective positive results over time, and I think many people think, looking back
at the history of science, that it has in fact done so.
We can send probes to distant planets, and we in fact get back pretty pictures of Jupiter
or Saturn, and that's a pretty good validation of the assumptions of the entire scientific
enterprise.
The theory of relativity is a good example of how scientific knowledge progresses in
reality.
In classic physics, by the end of the 19th century, there was a Cunean crisis.
The physics of the time, what we now call classic physics, could not explain several
observed phenomena.
It could not explain the orbit of Mercury.
There were perturbations, and Mercury was not moving exactly where models predicted
it should be.
There is also what was called the ultraviolet catastrophe, meaning that our models of how
atoms work didn't quite pan out, that if our model of say an electron orbiting around
a nucleus is like a planet orbiting around a star, if that simplistic model and atom
were correct, then atoms should collapse.
They should not be able to exist as they do.
Also there was an absence of an ether.
Physicists were trying to figure out what light moves through.
At the time, mathematics told us, the Maxwell equations specifically, that light travels
at a constant speed C, but light moves at speed C with respect to what?
Well maybe there's an ether through which it propagates.
However, research showed that the ether essentially did not exist.
The earth was neither stationary nor moving with respect to the ether, and the only way
to make sense of that was by concluding that there was no ether.
So then what was light moving at speed C with respect to?
Henry Poincar√© worked off of the Lorenz transformations, which was just a mathematical
way of describing observations at the time.
He introduced the concept of relativity.
He essentially realized that he could make sense of the mathematics of physics by assuming
that distance and time were relative and that the speed of light was constant.
But he considered this to be a mathematical fix only.
He could not conceive of a universe in which space and time were actually relative.
This kept him from making the tremendous insight or discovery that was later made by Albert
Einstein.
Einstein's true genius was being the first to consider that the universe actually worked
that way.
Space and time are relative, and the speed of light is constant with respect to everything.
It's always moving at C, no matter what your frame of reference or what you're measuring
it with respect to.
He completed his special theory of relativity in 1905.
Perhaps this was the most dramatic paradigm shift in scientific history, from a classical
universe to a relativistic universe.
However, despite the power and the explanatory power of his theories, Einstein's relativity
was not accepted until it was later confirmed by observation and experiment.
So again, we see that while the context of discovery was fascinating, the real credit
goes to the context of later justification.
That's when the real science was done.
That's when Einstein's theories were finally accepted and when he became a household name.
So science at its best is what I'd like to describe.
Now it is a human endeavor, as I've said, which means that often humans fall short of
the theoretical ideal of how science should proceed.
But at its best, scientific methods are used to develop a model of how the world works.
Hypotheses and theories are only useful to the extent that they explain how nature works
and that they generate predictions that can be tested.
But explanation is not enough.
The ability to explain what we already know is nice.
It is a prerequisite for a scientific theory.
But as I described in earlier lectures, we're really good at seeing patterns, making connections,
and making explanations.
The fact that we can weave a theory to explain observations is a reasonable starting point
for science, but it's not enough.
In order to be a truly scientific theory, a notion has to make predictions about things
we don't already know.
And that will either involve observations that we will make in the future or the results
of specific experiments.
There are also competing theories in science.
And when there is more than one theory, more than one explanation that can account for
the data that we already have, there must be a way to separate them experimentally.
A theory is therefore only useful if it makes predictions that are different than existing
or other theories.
If the predictions made by two theories are exactly the same, then that's a distinction
without a difference.
Even the most challenging aspect of science, in fact, is figuring out how to test something.
Again, we're really good at being at creatively generating ideas, but figuring out a way to
test an idea often, that's where all the hard work of science is really done.
There is a phrase that scientists will use to explain a theory which is meant to be the
highest form of criticism, and that is not even wrong.
By saying an idea is not even wrong, that means the theory does not make any specific
predictions.
It is therefore of no use to science.
An idea that can be proven wrong is highly useful to science.
We learn a lot when we come up with a theory and then prove that it is wrong.
But an idea that can't be tested isn't even wrong.
In other words, it's useless.
What if we have more than one theory that can explain all of the data that we have,
and there is no empirical way to, at least at the moment, separate out those theories?
Well then we can employ a philosophical rule of thumb called Occam's Razor, or the rule
of parsimony.
This theory says, or this rule of thumb states, that the theory that introduces the fewest
new assumptions is preferred.
That a more elaborate theory or a gerry-rig theory that introduces a lot of new assumptions
is, just by chance alone, more likely to be incorrect, because we had to ad hoc invent
those assumptions in order to make the theory work.
The less we have to do that, the more likely we are to be correct.
Now this doesn't mean that the theory that has the fewest new assumptions is the correct
answer, but it at least should be ruled out before we entertain more elaborate ideas that
force us to make new assumptions.
Otherwise, we could endlessly generate ad hoc theories to explain any phenomenon.
This is limited only by human creativity, which is pretty extreme.
If you can eliminate an element from a theory, and it makes no difference to the predictions
or observations, then it's a good idea to do so.
This is like trimming the fat away from scientific theories.
Let me give you an example of the ghost in the machine.
Dualism is a philosophical belief that what we perceive of as our mind, our consciousness,
actually does not come entirely from the brain or from the brain at all.
It comes from some other thing, whether it's spiritual or some other fundamental principle
of nature that we do not currently understand.
This is opposed to the current neuroscientific view that the mind is what the brain does.
One theory of dualism is that the brain is a receptor, that the brain is not generating
our thoughts, mood, beliefs, and activity, that it is simply receiving it from somewhere
else and then acting upon it or translating it into the material world.
However, most forms of this brain as receptor philosophy does not make any predictions that
distinguish it from the signals arising from the brain itself.
If the predictions are all the same, for example, if you damage a part of the brain, you lose
that ability.
Well, the dualist theory might say, well, you lose the receptor for that ability.
But essentially what this does is simply remove the functionality of the mind one step from
the physical brain by introducing a completely unnecessary notion of a mind originating outside
the brain that, again, in some forms doesn't make any predictions that distinguish it from
the mind as the brain.
Therefore, Occam's razor tells us it's unnecessary and should be eliminated.
Science also needs to be understood as a provisional endeavor.
All conclusions in science are subject to further evidence and new ways of interpreting
the data.
There's no metaphysical certitude.
That means that scientists and critical thinkers need to be comfortable with uncertainty.
All data has error bars on it.
But this does not mean that we do not know stuff, that we cannot know anything.
We can use the methods of science to build reliable models about how the world works.
And those models can be so reliable that it's reasonable to treat them as if they're
true, even though we always have that little question mark next to all scientific knowledge.
We can also now look back over centuries or more of scientific progress and ask questions
about how does science typically progress?
Well, it depends on where along the progressive model a scientific theory is.
Early on, the progress of science is more like the paradigm shift model that Cune told
about, where scientific ideas wholesale replace pre-scientific or unscientific ideas.
However, the more a scientific discipline develops, the less it resembles this dramatic
paradigm shift, and the more it resembles a process of refinement.
For example, even the paradigm shift that Einstein brought about of relativity replacing
classic physics did not prove Newton's classic ideas wrong.
Newton's mechanics just became a special case of a deeper understanding of how the universe
works that was described by Einstein.
Einstein was more of a refinement to Newton, not a wholesale replacement of Newton's ideas.
Here's a very illustrative example that I think is helpful.
The ancient Greeks knew that the Earth was a sphere.
They knew this based upon some basic observations.
However, once explorers could sail around the world and measure it more accurately, we realized
that it's actually an oblate spheroid.
It bulges more around the equator than around the poles.
This method was later replaced by modern satellite measurements that were able to detect that
it is also an asymmetrical oblate spheroid.
The southern hemisphere is slightly larger than the north.
That doesn't mean, however, that the notion that the Earth is the sphere is wrong.
It is still right as far as it goes.
The newer measurements were refinements.
We will not discover tomorrow as the paradigm shift model might suggest that, for example,
the Earth is a cube.
That's simply not going to happen.
Many sciences are established, for example, to such a high degree that while refinements
are always possible, fundamental knowledge will not be overturned.
Another example is that of DNA, the molecule that is the substrate of inheritance.
Although we are learning more and more about the details of genetics and how DNA translates
into developmental biology, for example, the basic fact that DNA is the molecule of inheritance
is not going to be overturned.
That is solid scientific knowledge.
I also like to caution against artificial dichotomies within science, so-called hard
versus soft sciences.
It is sometimes argued that some scientific fields like physics, for example, are harder,
therefore more legitimate, while others like the social sciences are soft and squishy.
While there are certain differences, of course, among the various fields of science, different
strengths and weaknesses, the hard versus soft division is unnecessary and counterproductive.
Any discipline can be scientifically hard as long as rigorous methods are followed.
The social sciences do not do have additional challenges because the subject of their study
is a chaotic system in which all the variables cannot be controlled.
Social scientists therefore need to be more cautious in their conclusions.
Look at experiments from multiple angles and be very careful about what they measure
and how they are measuring it and what outcomes they choose to follow.
These challenges are not present, for example, when dealing with the mass of an electron.
However, physicists do have their own challenges when dealing with the subject of their study.
Electrons don't typically decide to change their fundamental properties on a whim, so
physicists do not have to deal with that challenge.
In other words, humans are complex systems with many uncontrollable variables.
This doesn't mean they cannot be studied scientifically, however, as long as rigorous methods of hypothesis
testing are being used.
When it comes to whether or not a discipline is scientific, the subject matter is not the
issue.
Using your rigorous methods, outcomes that are as objective as possible, and systematic
observation is all that's necessary.
If you're doing that, you're doing hard science.
Whether or not you're doing good science is a separate question.
In this lecture, we essentially described science at its best.
In a future lecture, we're going to describe science at its worst.

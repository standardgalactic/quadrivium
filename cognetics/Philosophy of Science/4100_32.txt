Lecture 32 – Problems with Bayesianism
Though the math can be a bit daunting, the basic ideas behind Bayesianism are rather simple
and powerful.
We've seen that it strikes an interesting balance between a role for subjectivity and
a role for objectivity in scientific reasoning, and it has at least some potential resources
for handling a lot of problems about confirmation that we weren't able to handle until we
brought in notions like probability.
Bayesianism has gained many adherents in recent decades, but with the increased attention
has come increased criticism, to which we now turn our attention.
We begin by noting a couple of objections to the Bayesian program that are interesting,
but that we won't be able to pursue in detail.
One is that it involves a rather dramatic idealization of human cognizers.
The way I've presented it so far, I've made it sound kind of modest to talk about
merely requiring coherence of one's degrees of belief, rather than that one get the world's
relative frequencies right, or meet some a priori logical principle of indifference.
Nevertheless, mere coherence, especially when probability gets brought to the table, is
in many ways enormously demanding.
You and I do not have the processing power to meet Bayesian standards of probabilistic
coherence, even in fairly simple cases, much less in cases of serious scientific complexity.
Moderately precise degrees of belief, plus the demand for coherence, would break our
brains in nothing flat.
Coherence requires what philosophers sometimes call logical omniscience, that we know all
the logical consequences of our beliefs, and if you impose probability on top of that
as a requirement to see how our beliefs hang together, the amount of processing power required
is just completely beyond us.
On the other hand, it's not clear how descriptively accurate the Bayesian conception of scientific
reasoning needs to be.
Surely, there is some role for ideals that cannot be met by you and I.
I probably can't meet the standards of deductive coherence, either.
There are probably always going to be some contradictions buried somewhere in my belief
set.
That doesn't undermine the status of the norm that you should try not to believe contradictions.
Even though you and I can't begin to meet Bayesian standards of scientific rationality,
that doesn't automatically disqualify them as appropriate standards of scientific rationality.
This raises the question of what kind of theory Bayesianism is best posed as.
To what extent should it model itself on the kind of rational reconstruction approach
we saw from the positivists, and to what extent should it market itself as a kind of
Kunian descriptive theory about how well-conducted scientific reasoning actually goes?
So along these lines, there is an objection with a Kunian flavor that has a bit of force.
Many people think that Bayesianism gets in trouble because it does not reflect actual
scientific practice, and in noteworthy and important ways.
You don't generally find scientists reporting their results by stating their prior probability
assignments, and then showing how the evidence convinced them to raise or lower their probability
assignments.
Most scientists don't think of their work in terms of degrees of belief at all.
They're not sharing some autobiographical information about how their beliefs have been
updated.
This is a broadly paparian observation.
Belief doesn't seem to have the kind of role in science that the degrees of belief approach
of the Bayesians suggests it should have.
We'll come back around to this at the end of this lecture.
But for now, we can note that it remains the case that relatively few scientists think
of themselves as Bayesians, but the ranks of Bayesian scientists seem to be growing,
and that has some force against this Kunian objection that it doesn't get scientific practice
right.
And of course, to some extent, one need not explicitly use a theory, or think of oneself
as using a theory, to count as deploying the theory.
That's the apparent tension between scientific practice and what Bayesianism says scientists
are doing, not the mere fact that scientists don't explicitly use Bayesianism.
That's the apparent problem.
But there are some resources Bayesians can use to try to narrow the gap there.
We turn our attention now to more directly epistemic problems with Bayesianism, and
these we will talk about in a bit of detail.
The problem of old evidence is perhaps the most long-standing worry lodged at Bayesians.
As we noted a couple of lectures ago, it seems that scientific theories can be confirmed
by facts that are already known, by data that's already in.
Newton's theory, for instance, could explain Kepler's well-known laws of planetary motion,
it could explain well-known facts about how the tides behaved, etc.
And that all counted as evidence for the truth of Newton's theory.
But one of Bayesianism's strengths starts to look like a weakness here, because as we've
seen, the fact that the probability of the evidence, apart from any hypothesis, is the
denominator of Bayes' theorem, means that unlikely evidence, surprising evidence, has
much more confirming power than does expected evidence.
This is an insight associated with Popper, who thinks that scientists should be putting
forward bold, surprising conjectures, and it's borne out in things like the Eclipse experiment
that confirmed Einstein's theory.
But this feature of Bayesianism, that it captures nicely this idea, can also be a bug, because
it looks like it deprives any evidence that's already known, any evidence at least that's
already known for sure, of having any power to confirm at all.
Why?
Because if you know a bit of evidence for sure, then the probability of that evidence by your
lights is one, as is the probability of getting that evidence given any particular hypothesis.
If I assign a probability of one to I'm staring at a camera right now, then given any hypothesis,
I'd better assign a probability of one to I'm staring at a camera right now.
So the probability for me that I'm staring at a camera right now, given the hypothesis
that donkeys fly, is one.
So both the probability of the evidence and the probability of the evidence on any given
hypothesis is one.
If you plug those numbers into Bayes' theorem, you'll see that old evidence has zero power
to confirm hypotheses, because the prior probability is getting multiplied by one over one, and
so if the evidence has a probability of one, your posterior probability is guaranteed to
stay the same as your prior probability.
Old evidence cannot confirm a hypothesis in the slightest, and that looks like a problem
because we thought that Newton's theories were confirmed by data that it explained and
accounted for that had already been known.
So there are a couple of strategies open to Bayesians.
One is they can claim that the subjective probability of the evidence is not to be considered against
one's actual background knowledge of the world.
Why?
Because that background knowledge includes the evidence.
You already know the evidence.
So instead what you're supposed to do is assess the probability of the evidence using your
background knowledge if the evidence were not already known.
We're supposed to ascertain how surprising the evidence would be if we didn't already
know it, or anything that entails it.
So it's a counterfactual conception of how surprising the evidence would be.
As you might imagine, this is not an easy counterfactual to evaluate, because the evidence
could be integrated into our web of belief in all kinds of ways.
So how exactly am I supposed to arrive at a clear, confident answer to the question,
how surprising would I judge this evidence to be if I didn't already know it?
Because the number of statements that could bear on E in various ways, E being the evidence
statement, sorry, is really quite enormous.
So that's a hard counterfactual to evaluate.
It's not clear that I can give a clear answer to the question, how surprising would I think
this evidence about the tides were if I didn't already know it?
So another route open to the Bayesian is to say that it's not the evidence that really
confirms the hypothesis when the evidence is already in.
It's the fact that the hypothesis implies the evidence.
So in the Newtonian case, it's the new information that Newton's theory entails Kepler's laws
that confirms Newton's theory.
So it's H entails E, the hypothesis entails the evidence, rather than the evidence statement
that's doing the work.
This might work, but it has a couple of problems.
So for one thing, sometimes the fact that the hypothesis entails the evidence is itself
already known.
So the very same problem of old evidence applies to the fact that the hypothesis entails the
evidence.
And anyway, it's not clear that this answers our question.
We thought that evidence that was already in could confirm a hypothesis.
And being told that maybe the fact that the hypothesis entails the evidence can confirm
the hypothesis didn't answer our question, which is why, if at all, is it impossible
for evidence that's already known to confirm a hypothesis?
So some think the Bayesian evades the question rather than answering it.
If that reply is offered.
But the most influential objections to the Bayesian program concern its somewhat brazen
tolerance of subjective probabilities.
We've emphasized this in our earlier presentations.
The only rules are rules of coherence, at least initially, and then you have to update in
the right sort of way.
Now we've seen that the Bayesian can appeal to the washing out of the priors to argue
that the subjectivity is not too damaging to serious science.
If the evidence is good enough, then people will converge.
And so we can see some of the kind of convergence that traditional views of science have wanted
to credit scientific inquiry with attaining.
But those washing out results were themselves subject to pretty significant limitations
about agreement on how evidence bears on theory.
And so we don't want to just say subjectivity is fine because if perfect evidence comes
in and if two people agree on how the evidence bears on theory, they'll come to agreement.
That still seems to many people to tolerate too much rampant subjectivity.
To allow scientists to have initial probability assignments based on whim or prejudice or
something like that, we want to try to rule those out at least to some extent.
So if Bayesianism is to be used as a theory of scientific reasoning, and that's not the
only use for it, it's a theory of everyday reasoning too, many people think it needs
to make room for some distinctively scientific constraints on the values that get plugged
into the equation.
So these distinctive scientific constraints are not part of orthodox Bayesianism, but
they're not incompatible with it either.
These are things we just add to try to make it a better theory of scientific reasoning.
So the idea is to use evidence and scientific values to impose some substantive constraints
on admissible probability or conditional probability assignments so that outlandish
degrees of belief are regarded as legitimately criticizable.
They don't violate Bayesianism as such, but they violate a scientifically informed version
of Bayesianism.
This is often called tempered personalism.
We require that scientists temper their subjective probabilities in the light of scientific norms
and values, and so we can exclude some degrees of belief from consideration as appropriately
scientific.
The extent to which parts of the equation can be tempered varies significantly.
As we saw last time, the probability of the evidence given the hypothesis is usually pretty
well behaved.
Often the hypothesis in question entails the evidence, in which case the probability of
the evidence given the hypothesis is one.
The probability of this piece of copper conducting electricity given the hypothesis that all
copper conducts electricity is one.
We can agree to that.
That gets tempered really quite nicely.
In other cases, the hypothesis might not entail the evidence, but in a well-setup experiment
it confers a fairly definite probability on the evidence.
That's part of why experimental setups are run the way they are.
You can get a clear logical relationship between the hypothesis and the evidence and know how
surprising a given bit of evidence would be if the hypothesis were true.
So the probability of the evidence given the hypothesis is not particularly problematic.
Most people think that can be rendered reasonably tempered, reasonably objective.
We turn now to the other part in the numerator of the right hand side of Bayes equation,
which is the prior probability of the hypothesis.
And that can be tamed a good bit.
Within science anyway, we could try to impose norms requiring that one look for evidence,
not just start from whatever whim one finds oneself with.
That seems relevant in ways guided by scientific practice to the setting of prior probabilities.
So we could try to require that hypotheses put forward by serious scientists should not
be given probabilities that are ridiculously low.
We could also require that, since many hypotheses put forward by serious scientists turn out
to be false, you shouldn't start with too high an initial probability assignment either,
and try to independently defend some rules constraining the range of prior probability
assignments.
So this is part of a kind of broadly naturalistic approach we look at what science does and
try to build those norms into our theory of scientific reasoning.
And all of this is supposed to be an improvement on letting probability assignments get settled
in any way that happens to go here with other things the scientist thinks, because a given
scientist might just be in a bad mood and have very peculiar probability assignments
for that reason.
Nevertheless, there is a limit to the extent to which the prior probability for the hypothesis
can get tempered.
Because I'm supposed to assign a prior probability to the hypothesis in the way that science
would do so.
I'm supposed to consider how similar hypotheses get treated in science.
Now, that's still supposed to allow some latitude for individual scientific disagreement,
but it's supposed to be a significant constraint.
But now we face the problem of what counts as hypotheses like this one, if I'm to treat
this as like hypotheses have been treated.
So am I to treat all inverse square law hypotheses similarly, no matter what phenomena they're
applied to?
Am I supposed to treat all hypotheses proposed by scientists who got their PhDs from MIT
similarly?
What counts as a similar hypothesis by the standards of science is going to need to be
addressed and it looks like there's going to be a lot of room for a kind of subjective
probability in that assessment.
These aren't insurmountable problems to the extent that you think you can articulate science's
own standards of similarity, but that is a non-trivial problem.
Okay, so the probability of the evidence given the hypothesis is pretty well behaved.
The prior probability of the hypothesis is moderately well behaved.
The hard problem surprisingly concerns the denominator on the right hand side of Bayes
Theorem, the probability of the evidence apart from any hypothesis.
Why?
Because if we unpack this explicitly, the probability of the evidence is equal to the
probability of the evidence on the assumption that our hypothesis is true, plus the probability
of the evidence on the assumption that our hypothesis is false.
If we've got more than one serious hypothesis than we divide it up, it's the probability
of the evidence on the assumption that hypothesis one is true plus the probability of the evidence
on the hypothesis two is true, plus the probability of the evidence on the hypothesis that neither
is true.
So, lemme unpack this with a little expliciteness.
you don't need to understand exactly how all of this works,
I'll focus on the components we're gonna need.
The probability of the evidence,
I'm just gonna use the one hypothesis case
for simplicity's sake,
is the probability of the evidence given the hypothesis
times the probability of the hypothesis.
So that's how probable the,
hypothesis makes the evidence
times how probable the hypothesis itself is.
Same thing if the hypothesis is false,
the probability of the evidence
if the hypothesis is false
times the probability that the hypothesis is false.
That tells me how probable my evidence is, period,
since there are only two possibilities,
the hypothesis is true and that it's false.
And the components there,
the probability of the evidence given the hypothesis
and the prior probability of the hypothesis
aren't too troublesome, we just saw that.
The problem is that the claim that our hypothesis is false
is not itself a hypothesis.
There are many, many ways for a hypothesis to be false.
So the probability that a given hypothesis is false
is not a real hypothesis,
it's called the catch-all hypothesis.
Because there are deeply different ways
in which a hypothesis could be false
and they won't all assign the same probability
to the evidence, right?
The hypothesis could be false in such a way
that the real world doesn't exist,
we're all in the matrix,
or it could be false because a competing hypothesis
that's well known to science turns out to be true,
there are gazillion ways for a hypothesis to be false.
So we're not talking about a particular hypothesis,
we're talking about a catch-all.
The only way to get solid evidence for the values
in this part of the equation in the denominator
would be to claim that all the possible hypotheses
are under consideration.
So that we can ignore the catch-all hypothesis,
all we need to consider is the probability of the evidence
given that say the wave theory of light is true
and the probability of the evidence given
that the particle theory of light is true.
If those really exhausted logical space,
we wouldn't have to worry about the catch-all hypothesis.
And scientists often don't worry
about the catch-all hypothesis,
which reflects their confidence
that they have exhausted all of the serious possibilities.
They think philosophers are only interested
in weird possibilities like we're all dreaming
or we're in the matrix or something like that.
While philosophers generally want to focus on cases
in which scientists in the 19th century all thought
either the wave theory of light
or the particle theory of light had to be true,
turns out they're both false.
One of the gazillion hypotheses under the catch-all
turns out to be true.
So it's logically illegitimate
to ignore the catch-all hypothesis,
but there's no way to get evidence directly to bear
on the catch-all hypothesis
because it's not a particular hypothesis.
So that part of Bayes' theorem,
the denominator on the right hand side,
is one that can only be given
a kind of subjective interpretation.
So this is one of those places where philosophers
are not all that deferential
to what scientists actually do
because we think they generally make a logical mistake
by ignoring the catch-all hypothesis.
So even if I'm able to temper or constrain
many of the probabilities that figure in Bayes' theorem
in the light of evidence,
in the light of scientific practice,
there's no getting around the fact
that the only way to use the whole equation
is to include a kind of psychological guess
about how surprising it would be to get this evidence
if all of my seriously considered hypotheses are wrong.
Subjectivity of this sort cannot be eliminated
if you're going to use Bayes' theorem.
This is often presented as a devastating objection
to Bayesianism, but it's not clear
how bad this subjectivity is.
We've seen Kuhn arguing a few times
that a certain kind of subjectivity is healthy for science.
Disagreement across scientific revolutions
allows serious possibilities to flourish,
and we've seen other people argue
that diversity of subjective views
under conditions of free discussion
and other institutional supports
about scientific criticism
can help increase the objectivity of science.
So the issue should be what's good or bad for science
about the extent to which Bayesianism
allows subjective probabilities.
In addition, Bayesians can argue
that they're just making the role
of subjective factors in science explicit
and hence discussable.
Other approaches they say,
and this will emerge more clearly in a few minutes,
involve subjective factors, but tend not to admit it.
Subjectivity is really the S-word in science.
So let's turn to some major competitors
with the Bayesian program
and see if we can assess the costs
and benefits of each approach.
Lots of statisticians, scientists, philosophers of science
are anxious to avoid the mention of subjectivity
at all costs, and so they don't wanna start
from probabilities for any hypotheses
that are not thoroughly well-behaved.
Non-Bayesians don't like the subjectivity
in the prior probability of the hypothesis
and as we just saw in the probability of the evidence.
So they forego getting any posterior probability value.
You can't get a posterior probability
if you're not willing to plug in values
for these parts of Bayes theorem
that can only be ascertained
via a kind of subjective guess.
So basically that leaves non-Bayesians
using the probability of the evidence
given the hypothesis to do most of their work.
That's the well-behaved probability.
That's the thoroughly well-behaved one,
at least in most cases.
This kind of approach is often paired
with a kind of frequency interpretation of probability
because these people don't wanna rely
on potentially subjective degrees of belief
in the first place.
They want probability statements
to be about something observable.
One way of doing this is to appeal to classical statistics.
Versions of this approach dominated the sciences
for most of the 20th century
and they probably still do, though I'm not sure about that.
Bayesians have been making real inroads here.
Speaking loosely and in a way that I'll explain shortly,
classical statistics is Popperian
while Bayesianism is more Kunian.
Now I'm gonna breeze through
the very basics of classical statistics.
I'm not gonna be able to do full justice to the view.
But I think we'll get as much as is needed for our purposes.
As so often in discussions of probability,
we'll appeal to earns filled with marbles.
Very well-behaved cases.
So suppose we know that one earn contains 75% blue marbles
and 25% red marbles.
The other earn has the proportions reversed.
And we can afford to draw two marbles from one of the earns.
This part of the model is supposed to reflect the fact
that it costs resources to test hypotheses in real science.
So let's say we draw two marbles,
both of which are blue.
And we wanna try to tell
from which earn the marbles were drawn.
This is a very simple problem in classical statistics.
Classical statistics has you state a hypothesis.
Presumably you would guess that these marbles came
from the earn that has 75% blue marbles.
What are you supposed to do?
You assume that your hypothesis is false.
To do that is to adopt what they call the null hypothesis.
That's a term used a few different ways,
but that'll be okay for now.
Now we calculate how likely the evidence would be
on the null hypothesis.
How surprising would it be to get this evidence
if our original hypothesis turns out to be false?
So what's the probability of drawing two blue marbles
from the earn that's mostly got red marbles in it?
Well, it's 75, 25, so we have a 25% chance
for each marble of having drawn it
if it's from the earn that's got three quarters red marbles.
So it's 25% chance times a 25% chance
comes to a 6.25% chance that these marbles came
from the earn of mostly red marbles.
According to classical statistics,
you would be allowed to reject the null hypothesis
if the probability of that evidence
on the null hypothesis is below 5%.
It isn't in this case, it's 6.25%,
so the result is not statistically significant,
so you've learned nothing,
which is a little bit counterintuitive.
A Bayesian by contrast would have appealed to subjective,
at least in some sense, prior probabilities
and would have updated beliefs in light of this modest,
but intuitively genuine information.
And so it would have gone in
with some subjective degree of belief
and modified it by taking this new evidence into account.
This is what I mean when I suggest
that classical statistics is paparian.
We don't adopt hypotheses, we have rules for rejecting them.
The world can tell us we're wrong
in a way that it can't tell us we're right.
That's a broadly paparian approach.
This 5% significance threshold is very interesting.
It is both somewhat arbitrary and somewhat sacred in science.
There was not much argument offered for it,
it was proposed in the 1920s
by the British statistician R.A. Fisher.
And he pretty much just said
that if you had a less than 5% chance of randomness
producing results at least as impressive
as the results you've observed,
then your findings seem to him to be significant.
It's a mathematically convenient standard
and he thought it provided reasonable protection
against error.
That's not a trivial argument,
but it's not exactly a knockdown argument.
But the 5% significance threshold has been institutionalized
in scientific journals, in government agencies,
it's the scientific gold standard to a certain extent.
So though it's semi-arbitrary,
it has one thing going for it.
It's in an important respect, objective.
The data plus this rule provides a decision procedure
that leaves no serious room for judgment or disagreement.
And this is crucial to why it has caught on
to the extent that it does,
because it's not subjective, the S word is banished.
It's replaced, one might think,
by a somewhat arbitrary though objective rule.
And one might wonder why arbitrariness
is so much better than subjectivity.
It's not that I think it's such an objectionable standard,
but we should understand what it is.
It is not God's or science's or the world's own standard
for when you've got a good hypothesis.
It's surprising how many people won't admit
that one could do science differently,
because that seems to introduce the issue of subjectivity.
We now have to admit that we have scientific values
and decide whether the 5% threshold is the right one
to apply.
In addition, this is somewhat controversial,
but not very, I don't think.
The mindless application of the 5% standard
has sometimes led to some reasonably bad science.
It doesn't often happen too vulgarly,
but it's clear that the standard can be abused
if it's combined with the wrong procedures.
And we learned back in our first discussion
of the Raven paradox that procedures can matter.
The procedure of looking at something black
and telling whether it's a Raven is different
from the procedure of looking at something that's a Raven
and telling whether it's black.
How does that apply here?
If you run enough more or less thoughtless experiments,
eventually you're gonna get some statistically
significant findings.
Whether there's any correlation or cause out in the world,
if you perform the procedure of just run enough experiments
and publish when you get a 5% threshold result,
you're gonna get some really bad results published out there.
And at least some people think that some of the real
disappointments in biomedical research,
when drugs that were, scare quotes here,
clinically shown to be effective turn out to have little
or no value, the culprit here is arguably a desire
for objectivity run amok.
A refusal to rely on judgment about whether the procedures
are sensible.
You just run procedures over and over.
And again, I'm not accusing anybody in particular
of doing this, science is generally better than this.
But there's room for the standard to be abused.
Another statistical confusion looms large
in public discussions of evidence outside of science
as well as within it.
All sides agree there's a big difference
between the probability of the evidence given the hypothesis
and the probability of the hypothesis
given the evidence.
But they are confused quite often.
Lawyers have a name for this.
It's sometimes called the prosecutor's fallacy.
Because in a criminal trial, the jury might be told
that there's a very impressive probability
of the evidence given the hypothesis.
So for instance, the forensic evidence
matches the defendant's DNA.
And the chances of the evidence matching a person
chosen at random might be minuscule.
So on the hypothesis that the defendant did it,
this evidence is unsurprising.
And on the hypothesis that the defendant didn't do it,
it's much more surprising.
That suggests that the evidence has a lot of potential power
to confirm the hypothesis that the defendant is guilty.
You can see that if you plug those values
into Bayes' theorem.
But it cannot confirm the hypothesis
that the defendant is guilty unless you've got
a prior probability for that hypothesis.
You cannot get to the probability of the hypothesis
given the evidence without putting all those values,
all the subjective values into Bayes' theorem.
We can see this by realizing that if I knew for sure
I was having dinner with the defendant 1,000 miles away
from the scene of the crime, the evidence,
though it has in an abstract sense,
a lot of confirming power will rightly fail to sway me.
Because my prior probability for the hypothesis
that the defendant did it will be low.
Because I was having dinner with the defendant
1,000 miles away.
But lawyers don't wanna come out and say
that they're appealing to subjective degrees of belief
on the part of jurors.
And of course, many lawyers aren't statisticians anyway.
So they can't quite come out and say,
well, if you have ordinary probabilistic assignments here,
then this evidence should lead you to assign
a high probability to the hypothesis
that the defendant did it.
So they talk as if the probability of the evidence
given the hypothesis by itself can make the probability
of the hypothesis given the evidence high.
And that's a logical fallacy.
It'll work if we assume some background beliefs
which people assume but won't admit that they assume.
And that's a real problem in clear thinking
about evidential matters.
Clear thinking would require that one live by
the paparian scruples of the classical approach
to statistics, which admits that you can't get
a probability for hypotheses.
You only get a rule for rejecting.
So the fear of subjectivity muddles our thinking
about a lot of matters here.
Now I should note to give the classical tradition
more credit than I have.
It hasn't just been sitting on its hands
while Bayesianism has been catching on.
Classical views can adopt a lot of intellectual virtues.
They can do more than just apply
a mechanical rule of rejection.
So what they've done in recent decades
is tried to come up with, while avoiding
prior subjective probabilities,
they've tried to come up with a way
of helping us learn more from experience.
So they cannot attach posterior probabilities
to hypotheses.
You have to have all of the values
for Bayes' theorem to do that.
But they can provide methods that have certain
desirable characteristics.
So one classical approach within classical statistics
allows you to distinguish between errors of two kinds.
One would be accepting the hypothesis if it's false.
The other would be rejecting it if it's true.
You specify the maximum value you're willing to tolerate
for one kind of error, and then the method
will minimize the chance of error of the other kind.
Or there's a kind of 20 questions method
that will, if there's a yes or no answer to a question,
give you one in the smallest number of steps possible.
The point is the classical view has to admit
that they cannot attach probabilities to hypotheses.
But they wanna go beyond popper and do a better job
of letting us learn from experience.
So what we've got is a choice
between two different sorts of views.
If you're willing to fess up to a role
for subjective prior probabilities,
then it's straightforward that you can update your beliefs
and count things as evidence for the truth of hypotheses.
If you're not, you have to take a more direct approach.
The fear of subjectivity costs something.
And I'm not saying it's a mistake
to try to banish subjectivity,
but it's a mistake not to admit that it has costs.
And that's, I think, what vindicates this detour
through so much mathematics.
We turn next time to philosophy of science
within particular sciences, and physics is the first victim.
Hello.
Restyling?
Foundations all together?
Thank you.
I think there's some tonight too.
You know, think aiee I have channels
that let us go out from my stomach.
How do you sort things out,
give me insight of society through my experience?
Maybe it'll work out.
Or maybe it will.
It can make Frances feel completed,
but for tomorrow's trial żebyof a future

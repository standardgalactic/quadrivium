ITERATIVE METHODS FOR
STRUCTURED ALGORITHMIC DATA SCIENCE
A DISSERTATION
SUBMITTED TO THE DEPARTMENT OF COMPUTER SCIENCE
AND THE COMMITTEE ON GRADUATE STUDIES
OF STANFORD UNIVERSITY
IN PARTIAL FULFILLMENT OF THE REQUIREMENTS
FOR THE DEGREE OF
DOCTOR OF PHILOSOPHY
Kevin Tian
June 2022

 
 
 
 
 
 
 
 
 
 
 
 
                      
 
 
 
This dissertation is online at: https://purl.stanford.edu/qy752rq1707
 
© 2022 by Kevin Jimaine Tian. All Rights Reserved.
Re-distributed by Stanford University under license with the author.
ii

I certify that I have read this dissertation and that, in my opinion, it is fully adequate
in scope and quality as a dissertation for the degree of Doctor of Philosophy.
Aaron Sidford, Primary Adviser
I certify that I have read this dissertation and that, in my opinion, it is fully adequate
in scope and quality as a dissertation for the degree of Doctor of Philosophy.
Nima Ahmadipouranari
I certify that I have read this dissertation and that, in my opinion, it is fully adequate
in scope and quality as a dissertation for the degree of Doctor of Philosophy.
Moses Charikar
I certify that I have read this dissertation and that, in my opinion, it is fully adequate
in scope and quality as a dissertation for the degree of Doctor of Philosophy.
Gregory Valiant
Approved for the Stanford University Committee on Graduate Studies.
Stacey F. Bent, Vice Provost for Graduate Education
This signature page was generated electronically upon submission of this dissertation in 
electronic format. An original signed hard copy of the signature page is on file in
University Archives.
iii

Preface
This thesis studies the interplay between two aspects of algorithmic data science: iterative method
theory and structured problem instances arising from applications.
We organize the thesis into
two parts, each of which couples a branch of the modern iterative method toolkit with a family
of related structured problems in data science. Each part of the thesis develops new frameworks
for designing iterative methods, gives new tools for the eﬃcient implementation of said methods,
and oﬀers fresh perspectives on the relationships between our new iterative methods and various
fundamental algorithmic problems in the relevant application domain.
Stochastic Variational Inequalities and Combinatorial Optimization.
The ﬁrst part of the
thesis presents new algorithms and analysis frameworks for solving variational inequalities (VIs) in
monotone operators (which generalizes convex optimization and equilbrium computation in minimax
optimization), with a focus on algorithms for operators admitting a naturally stochastic structure.
In Chapters 2 and 3, we develop diﬀerent aspects of stochastic VI theory: acceleration and vari-
ance reduction. In the case of Chapter 2, as a consequence of our new acceleration frameworks,
we develop state-of-the-art algorithms for (well-conditioned) separable minimax and ﬁnite-sum op-
timization problems. In the case of Chapter 3, we give a host of new runtime tradeoﬀs for bilinear
minimax optimization problems, advancing the frontier of approximate equilibrium computation in
zero-sum matrix games for the ﬁrst time since seminal works of [253, 415].
In Chapters 4, 5, and 6, we build upon the techniques of Chapters 2 and 3 by giving new
algorithms for combinatorial optimization problems such as ﬂow, matching, and transportation
under diﬀerent computational models. We achieve these results by combining advances in primal-
dual acceleration, stochastic estimator design, and novel ways of implementing iterations of our
methods in parallel, streaming, or dynamic settings. In this portion of the thesis, we give a state-of-
the-art approximate solver for undirected maximum ﬂow (Chapter 4), an implementation of low-pass,
optimal space combinatorial optimization algorithms in a semi-streaming model (Chapter 5), and
new reduction frameworks and faster update times for dynamic matching problems (Chapter 6).
iv

Semideﬁnite Programming and High-Dimensional Statistics.
The second part of the thesis
investigates diﬀerent aspects of the design of nearly-linear time semideﬁnite programming (SDP)
solvers. In Chapter 7, we use ideas from the sketching literature and new theory for monotone SDPs
to give a suite of new SDP tools catering to diﬀerent geometries (e.g. Ky Fan or Schatten norm
objectives) and constraint structure (e.g. packing or matrix dictionary recovery instances).
In Chapters 8 and 9, we showcase the power of our new SDP tools by providing robust algo-
rithms for high-dimensional statistical estimation tasks under diﬀerent contamination models. We
consider contamination models capturing adversarial data poisoning and total variation distance
misspeciﬁcation in Chapter 8, where we give new almost-linear time algorithms for heavy-tailed
clustering, generalized linear regression, and principal component analysis. These algorithms obtain
near-optimal recovery guarantees in high dimensions as a function of the corruption parameter, and
are based on combining the ﬁltering paradigm with new SDP regret minimization tools. In Chap-
ter 9, we robustify standard linear system solvers and regression algorithms in both overcomplete
and undercomplete (sparse recovery) settings to run in nearly-linear time under the presence of an
appropriate semi-random adversary which augments the measurement matrix (arbitrarily hindering
standard notions of problem conditioning), without sacriﬁcing statistical performance.
In Chapter 10, we consider a somewhat separate area of algorithmic high-dimensional statistics:
developing an oracle complexity theory for structured logconcave sampling.
Here too, we draw
inspiration from aspects of iterative method theory such as proximal point methods and variance
reduction to give a new framework for structured sampler design. We obtain state-of-the-art query
complexities for well-conditioned, composite, and ﬁnite-sum structured densities; in the ﬁrst case,
we complement our new algorithms with near-matching lower bounds for popular frameworks.
v

Acknowledgments
If the reader chooses to only read one section of this thesis, I sincerely hope it is this one. This thesis
would not have been possible if it were not for the joint eﬀorts of a number of mentors, compatriots,
and friends I have had the tremendous fortune to cross paths with over my lifetime. It is diﬃcult to
capture in words the inﬂuence they have had on both my academic and personal trajectories, but I
will do my best in this section, and any omissions are solely my responsibility.
Mentors.
I would like to start by thanking Aaron Sidford for being an amazing mentor, advisor,
advocate and friend to me over the course of my Ph.D. Aaron took a leap of faith by accepting me as
his student when I had never done research in algorithms or optimization before coming to Stanford.
It took over a year of collaborating with Aaron before we got our ﬁrst result, but Aaron continued to
bring a level of enthusiasm and optimism to every meeting which set the tone for the whole group,
and for how I strived to conduct theory research. Aaron taught me the value of dreaming big but
also questioning our understanding of the basics at every stage. One of the biggest takeaways I have
from working with Aaron is that every time I learn a new technique, it is a worthwhile endeavor
(time permitting) to revisit every related problem armed with this new perspective to see if it yields
deeper understanding. For all the lessons in both math and life you have taught me, and all the
exciting research that has come out of our collaborations, thank you Aaron!
Next, I would like to thank Yin Tat Lee and Jerry Li, who have answered enough of my silly
questions over Slack and Hangouts that they could be considered my informal co-advisors. I was
fortunate to meet both Yin Tat and Jerry during Summer 2019 when I visited Seattle, and my tastes
have substantially beneﬁtted from working with both of them: almost all of the research in Part II
of the thesis is a direct reﬂection of my work with Yin Tat (my guide into the world of sampling)
and Jerry (likewise, for robust statistics). I am excited to continue our collaborations next year as I
move to Seattle. Beyond both being incredibly sharp technically, Yin Tat and Jerry have given me
very helpful advice during a number of tough times. Thank you, Yin Tat and Jerry!
Going back a bit further, I would like to thank James Zou and Greg Valiant for serving as my
rotation mentors during my ﬁrst few months at Stanford. Even though our research interests have
diverged somewhat since then, they taught me about what it meant to do world-class research, and
vi

have continued to provide guidance — I look up to both of them a lot.
Next, I thank my mentors from my time as an undergraduate, including Yuchun Guo, David
Giﬀord and Dana Moshkovitz at MIT, and Jennifer Listgarten and Nicolo Fusi at Microsoft Research.
I did quite a bit of ﬂailing around trying to understand what kind of research I liked and how to make
progress in general during those days — thank you all for your patience and mentorship. Yuchun in
particular has been a great friend and advocate, and paved the way for my graduate studies.
I would like to thank Jon Kelner for hosting me at MIT during Summer 2021. Jon is one of the
kindest academics I know, and taught me most of what I know about random matrix theory. I would
also like to thank the other more senior academics I have beneﬁtted from working with for all they
have shared with me, including Tselil Schramm, Nima Anari, Sepehr Assadi, Chris Musco, Sivakanth
Gopi, Adam Bouland, John Duchi, Ilias Diakonikolas, Daniel Kane, and Santosh Vempala.
The Stanford Theory Group is a wonderful place to spend 6 years thinking about hard problems,
and part of what made me want to do TCS research in the ﬁrst place was the universal friendliness I
felt from theory-minded folk. I thank Moses Charikar, Li-Yang Tan, and Jan Vondrak for excellent
courses they have taught, the other Stanford Theory faculty for creating an awesome environment,
and Jay Subramanian, Megan Harris, Ruth Harris, and all the other Stanford CS administrators
who have made the Ph.D. a smooth experience. I also have beneﬁtted from Aaron's Stanford MS&E
and ICME aﬃliations, and would like to thank the administration of these departments as well.
Finally, it is time to revisit my roots as an "academic" and thank all the teachers who shaped
my tastes in math in the early years. I want to give special shoutouts to Mrs. Sarah Griﬃn and Mrs.
Sally Barber, who helped me mature as a mathematician and a student, and have checked in over
the years to make sure I am happy in life. I also want to thank Dr. Weizhen Gu for being one of
my ﬁrst extracurricular math teachers, and Sam Baethge, Dr. Max Warshauer, Dr. Nathaniel Dean,
and Dr. Edward Early for letting me take my ﬁrst steps into research at Texas Mathworks.
Academic compatriots.
Theoretical research (and research of any form) can be an isolating
experience if one is not careful, given the amount of time spent "in the trenches." I have been lucky
to work with an amazing group of fellow students, each of whom has been incredibly helpful to talk
to over the years as we jointly seek a deeper understanding. The Ph.D. would have been a lot less
enjoyable without all of them, so I will use this space to shout them out.
Arun Jambulapati was one of the ﬁrst students I worked closely with, and since then we have
usually not gone very long before getting interested in a new direction together, having roped
each other into a number of projects over the years. Arun has an incredible breadth and depth of
knowledge, and our wide-ranging conversations have provided me stronger intuition for many topics.
It's been a lot of fun, Arun, and I'm sure this is only the beginning.
Weihao Kong was the ﬁrst senior student I worked on a project with, and has been very generous
with his advice and friendship since then. Yair Carmon has a deep understanding of many diﬀerent
aspects of optimization theory, and was kind enough to include me on our ﬁrst project [108] together.
vii

He also has taught me a lot about writing style, and it never feels like you can be stuck on a problem
for too long when you work with Yair. Those early days during the Ph.D. are the most important
towards building a research network, and I am very grateful it started with Weihao and Yair.
I have been very fortunate to work closely with two junior students, Yujia Jin and Ruoqi Shen,
on several projects throughout my Ph.D. Yujia and Ruoqi are both rising stars in the ﬁeld, and I
am thankful to them for all their eﬀorts and insights, which have gotten us far.
Next, I would like to thank Swati Padmanabhan and Ewin Tang, close friends from my time in
Seattle. Swati went through the exhilarating roller coaster that was the project [285, 286] with me
and Arun; although our work there is not done yet, I'm sure we'll understand someday. Ewin has
ﬁelded many of my questions as I try to learn about quantum computing (which resulted in my ﬁrst
brief cameo on Wikipedia), and I also want to shout out her music recommendations.
I thank my other student collaborators over the years for our joint research eﬀorts, including
Teng Zhang, Qijia Jiang, Daniel Kongsgaard, Allen Liu, June Vuong, Sinho Chewi, and Daogao Liu.
All are exceptional researchers and friends, and it has been a pleasure learning from them.
I also want to thank some of my other friends in graduate school, who helped make the academic
grind a much more enjoyable process, including Shivam Garg (my gym buddy for many years), Kiran
Shiragur, Yang Liu, Jay Mardia, Mark Sellke, Isabella Huang, Judy Shen, and Sitan Chen.
From the before times.
The start of my Ph.D. was a very lonely experience at times, and the
anxiety caused from the experience of dramatically switching areas during my ﬁrst year would have
been diﬃcult to bear alone. Although it has become a much smoother process since then, when
things get tough I still feel very fortunate to be able to lean on my friends "from the before times."
Jeﬀrey Chan has stuck with me as a "big brother" and role model for many diﬀerent stages of
my life. He edited all of my grad school and fellowship applications early on, as well as my ﬁrst
paper submission, hosted me in Berkeley on numerous occasions, and has provided an uncountable
amount of advice about navigating the Ph.D. and adult life in general. Without his guidance and
friendship, I would have surely been much more lost in this journey.
George Qi has been a stalwart friend to me since we were kids, and we have similarly stuck
together through each stage of life since then. I am very grateful that we got to live together during
the COVID times, though I am less grateful for all the ping pong balls he made me chase. Thanks
for all the great memories, and the Rudy's, George.
Kevin Li and Zach Izzo were an absolute blast to live with during the Ph.D. and made Stanford
feel like home away from home, especially during the long nights before deadlines. I am very thankful
to have known them for so long (Kevin has been my roommate for periods during middle school,
high school, college, and graduate school), and that we went through the Ph.D. journey together.
Moving to the Bay Area was an experience made much more comfortable by the presence of
great friends nearby. For that, I thank Thom Lu, Jodie Chen, Connie Liu, Mike Wu, JeﬀSun,
Kevin Peng, Lisa Huang, Serena Wang, Leonel Drukker, Rue Park, Joel Schneider, and Ben Bell.
viii

I took multiple "academic vacations" to Seattle during the Ph.D. and those times were made
even better by being able to hang out with Julia Guo, Matt Basile, Maddie Zhang, and Emmanuel
Azuh. I'm really excited to move there and see you all again soon.
I would also like to thank Alex Jaﬀe, Kai Xiao, Weilian Chu, Tony Zeng, Eric Wang, Sonya Han,
David Wang, Alex Hong, and Sameer Deshpande for being lifelong friends from college who I have
kept in touch with over the years, even though I don't see them nearly as much as I'd like these
days. Similarly, Ding Zhou, Lilly Shen, Leon Otis, Josh Dong, Danny Chen, Frances Chen, James
Cong, Jackie Chen, Aaron Hui, Thaonhi Cung, Mary Jiang, David Yu, Priyanka Deshpande, Bobby
Shen, Aditya Jain, Daniel (Shenghao) Wang, Jason Pang, Peter Hong, Ricky Chiang, Jessica Xiao,
Jarry Xiao, Zeyi Lin, Prerna Bhat, Divya Ramamoorthy, Patrick Haley, and Connie Yan have been
friends from my childhood who have kept in touch with me throughout all this time, which I deeply
appreciate. During the COVID times, life was always more colorful when Lisa Hsiao and Kelly Zhou
dropped by to visit us in Austin. Catching up with you all is always time well spent.
Finally, I thank the 1997-2016 San Antonio Spurs for giving me something to look forward to
every year of my childhood. Pop, Tim, and Manu remain some of my biggest heroes in life.
The most important part.
This is the most diﬃcult part of the acknowledgments section for
me to write, because I feel I will inevitably be unable to do it justice. I want to dedicate this thesis
to all of the following amazing individuals, who have shaped me into the person I am.
From the very beginning, my parents Tian Hong and Xu Xiaoli have done everything they can to
ensure that I live a good life. They sacriﬁced so much (more than I know) to give my sister and me
a solid education and a happy childhood, putting our well-being ﬁrst always. They taught me the
value of honest, hard work, but also to always be proud of my eﬀorts, rather than losing sleep over
things outside of my control. They have been my biggest cheerleaders throughout my life, and I hope
they know how much I look up to them as role models. Although the COVID times were diﬃcult in
some ways, I am grateful to have spent a year of my Ph.D. eating my parents' home-cooked meals
every weekend. It is my great fortune of a lifetime to be able to call you my parents.
Sunny, thank you for all the fun memories we had growing up, and for being the most thoughtful
sister I could ever ask for. Even though we're on diﬀerent coasts now, it's always like a breath of
fresh air when we get to see each other again. Despite being the older sibling, I feel like I still learn
so much from you. I want you to know how proud I am to be your brother.
I would also like to acknowledge my extended family, especially my grandparents, Li Guirong,
Xu Jintang, Zhang Fulan, and Tian Qingzhong. My maternal grandparents helped raise me during
my early childhood, and were some of the most dedicated teachers I have ever had. My grandma Li
Guirong patiently taught me times tables over long walks to and from elementary school, and my
grandpa Xu Jintang painstakingly hand-translated math contests from English to Chinese so that
he could teach me. Despite speaking no English, they spent over a decade in America taking care of
my sister and me. I can only hope to give back to others a fraction of what they have given to me.
ix

Amy, the most important event of my time in graduate school is when you came into my life.
During the happy times, there is no one else I would rather celebrate with; during the hard times,
you always know how to make me feel like everything will be alright. Long distance hasn't always
been easy, but I could imagine no better partner through it all. You are my best friend, and life is
sweeter now because I can always look forward to the next great adventure with you.
x

"When nothing seems to help, I go and look at a stonecutter hammering away
at his rock perhaps a hundred times without as much as a crack showing in it.
Yet at the hundred and ﬁrst blow it will split in two, and I know
that it was not that blow that did it, but all that had gone before."
— Jacob Riis
xi

Contents
Preface
iv
Acknowledgments
vi
1
Introduction
1
1.1
Results: Theory of Stochastic Variational Inequalities
. . . . . . . . . . . . . . . . .
9
1.1.1
Chapter 2: Acceleration via Primal-Dual Extragradient Methods . . . . . . .
10
1.1.2
Chapter 3: Stochastic Methods for Matrix Games
. . . . . . . . . . . . . . .
11
1.2
Results: Combinatorial Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.2.1
Chapter 4: Faster Approximate ℓ∞Regression and Maximum Flow
. . . . .
14
1.2.2
Chapter 5: Semi-Streaming Combinatorial Optimization . . . . . . . . . . . .
15
1.2.3
Chapter 6: Dynamic Decremental Bipartite Matching
. . . . . . . . . . . . .
16
1.3
Results: Structured Semideﬁnite Programming Solvers . . . . . . . . . . . . . . . . .
16
1.3.1
Chapter 7: Matrix Multiplicative Weights and Friends . . . . . . . . . . . . .
17
1.4
Results: Algorithmic High-Dimensional Statistics . . . . . . . . . . . . . . . . . . . .
20
1.4.1
Chapter 8: High-Dimensional Robust Statistics in Almost-Linear Time . . . .
20
1.4.2
Chapter 9: Semi-Random Linear Systems . . . . . . . . . . . . . . . . . . . .
22
1.4.3
Chapter 10: Towards an Oracle Complexity of Sampling . . . . . . . . . . . .
24
1.5
Bibliography of Represented Material
. . . . . . . . . . . . . . . . . . . . . . . . . .
25
I
Stochastic Variational Inequalities and Combinatorial Optimization 28
2
Acceleration via Primal-Dual Extragradient Methods
29
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.1.1
Relative Lipschitzness in extragradient methods
. . . . . . . . . . . . . . . .
30
2.1.2
Sharper rates for separable minimax and ﬁnite sum optimization . . . . . . .
33
2.1.3
Additional related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.2
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
xii

2.3
Extragradient convergence under relative Lipschitzness . . . . . . . . . . . . . . . . .
44
2.4
Acceleration via relative Lipschitzness . . . . . . . . . . . . . . . . . . . . . . . . . .
45
2.5
Area convexity rates for box-simplex games via relative Lipschitzness . . . . . . . . .
48
2.6
Randomized coordinate acceleration via expected relative Lipschitzness
. . . . . . .
51
2.7
Separable minimax optimization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
2.7.1
Setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.7.2
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
2.7.3
Convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
2.7.4
Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
2.8
Finite sum optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
2.8.1
Setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
2.8.2
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
2.8.3
Convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
2.8.4
Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
2.9
Minimax ﬁnite sum optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
2.9.1
Setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
2.9.2
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
2.9.3
Inner loop convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . .
78
2.9.4
Outer loop convergence analysis
. . . . . . . . . . . . . . . . . . . . . . . . .
80
2.9.5
Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
3
Stochastic Methods for Matrix Games
83
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.1.1
Our results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
3.1.2
Our approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
3.1.3
Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
3.1.4
Chapter organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
3.2
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
3.2.1
Local norm setups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
3.2.2
The problem and optimality criterion
. . . . . . . . . . . . . . . . . . . . . .
101
3.2.3
Matrix access models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
3.2.4
Data structure interfaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
3.3
Framework
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
3.3.1
Sublinear coordinate methods . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
3.3.2
Variance-reduced coordinate methods
. . . . . . . . . . . . . . . . . . . . . .
107
3.4
Matrix games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
3.4.1
ℓ1-ℓ1 sublinear coordinate method
. . . . . . . . . . . . . . . . . . . . . . . .
111
3.4.2
ℓ1-ℓ1 variance-reduced coordinate method . . . . . . . . . . . . . . . . . . . .
113
xiii

3.5
Data structure implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
3.5.1
IterateMaintainerp
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
3.5.2
ApproxExpMaintainer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
3.5.3
ScaleMaintainer
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
3.6
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
3.6.1
Maximum inscribed ball . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
3.6.2
Minimum enclosing ball . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
3.6.3
Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
4
Faster Approximate ℓ∞Regression and Maximum Flow
143
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
4.1.1
Regression results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
4.1.2
Maximum ﬂow results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
4.1.3
Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
4.1.4
Organization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
4.2
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
4.2.1
Basic deﬁnitions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
4.2.2
Overview of our algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
4.3
ℓ∞regression subject to a box constraint
. . . . . . . . . . . . . . . . . . . . . . . .
158
4.3.1
Constructing the smooth approximation to regression
. . . . . . . . . . . . .
158
4.3.2
Acceleration via proximal point method . . . . . . . . . . . . . . . . . . . . .
159
4.3.3
Constructing the subproblem oracle
. . . . . . . . . . . . . . . . . . . . . . .
163
4.3.4
Putting it all together: accelerated ℓ∞regression . . . . . . . . . . . . . . . .
168
4.3.5
Cheap iterations for ℓ∞regression in column-sparse matrices . . . . . . . . .
172
4.4
Accelerating maximum ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
4.4.1
Maximum ﬂow preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . .
174
4.4.2
From maximum ﬂow to constrained ℓ∞regression
. . . . . . . . . . . . . . .
175
4.4.3
Runtimes for accelerated maximum ﬂow . . . . . . . . . . . . . . . . . . . . .
175
4.4.4
Exact maximum ﬂows in uncapacitated graphs . . . . . . . . . . . . . . . . .
177
4.5
Improved ﬂow runtimes via primal-dual coordinate regression . . . . . . . . . . . . .
178
4.5.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
4.5.2
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
4.5.3
Runtime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
5
Semi-Streaming Combinatorial Optimization
204
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
5.1.1
Problem setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
206
5.1.2
Our results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
xiv

5.1.3
Our techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
5.1.4
Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
5.2
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
5.3
Box-simplex games in low space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214
5.4
Approximate maximum cardinality matching
. . . . . . . . . . . . . . . . . . . . . .
220
5.4.1
Reducing MCM to a box-simplex problem . . . . . . . . . . . . . . . . . . . .
221
5.4.2
Additional tools
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223
5.4.3
Approximate MCM in fewer passes and optimal space . . . . . . . . . . . . .
224
5.5
Further matching applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
5.5.1
Exact maximum cardinality matching . . . . . . . . . . . . . . . . . . . . . .
225
5.5.2
Weighted bipartite matching under an ℓ1 constraint
. . . . . . . . . . . . . .
227
5.5.3
Optimal transportation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228
5.5.4
Maximum weight matching . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
5.6
Transshipment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
5.6.1
Constructing stretch approximators
. . . . . . . . . . . . . . . . . . . . . . .
231
5.6.2
Reduction to box-simplex game . . . . . . . . . . . . . . . . . . . . . . . . . .
233
5.6.3
Recovering a sparse ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
5.6.4
Semi-streaming transshipment
. . . . . . . . . . . . . . . . . . . . . . . . . .
236
6
Dynamic Decremental Bipartite Matching
238
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
6.1.1
Our results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
6.1.2
Prior work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
245
6.2
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
246
6.3
Dynamic decremental bipartite matching . . . . . . . . . . . . . . . . . . . . . . . . .
247
6.3.1
DDBM framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
247
6.3.2
DDBM solvers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
6.4
Regularized box-simplex games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
252
6.4.1
Algorithmic framework
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
253
6.4.2
Helper lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
255
6.4.3
Regularized box-simplex solver and its guarantees
. . . . . . . . . . . . . . .
256
II
Semideﬁnite Programming and High-Dimensional Statistics
260
7
Matrix Multiplicative Weights and Friends
261
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
7.1.1
Sketching matrix multiplicative weights
. . . . . . . . . . . . . . . . . . . . .
261
7.1.2
Ky Fan matrix multiplicative weights
. . . . . . . . . . . . . . . . . . . . . .
265
xv

7.1.3
Matrix dictionary recovery SDPs . . . . . . . . . . . . . . . . . . . . . . . . .
266
7.1.4
Schatten packing SDPs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
269
7.2
A rank-1 sketch for matrix multiplicative weights . . . . . . . . . . . . . . . . . . . .
270
7.2.1
Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
270
7.2.2
Analyzing the average mirror projection . . . . . . . . . . . . . . . . . . . . .
274
7.2.3
Eﬃcient computation of matrix exponential-vector products . . . . . . . . . .
278
7.2.4
Application to semideﬁnite programming
. . . . . . . . . . . . . . . . . . . .
280
7.2.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
283
7.3
Ky Fan matrix multiplicative weights . . . . . . . . . . . . . . . . . . . . . . . . . . .
284
7.3.1
Regret bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
284
7.3.2
Reﬁned divergence bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
286
7.3.3
Reﬁned k-PCA guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
290
7.3.4
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
7.4
Matrix dictionary recovery SDP solvers
. . . . . . . . . . . . . . . . . . . . . . . . .
302
7.4.1
Identity constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
302
7.4.2
General constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
7.5
Schatten packing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
317
7.5.1
Mirror descent interpretation of [379] . . . . . . . . . . . . . . . . . . . . . . .
317
7.5.2
ℓp-norm packing linear programs . . . . . . . . . . . . . . . . . . . . . . . . .
318
7.5.3
Schatten-norm packing semideﬁnite programs . . . . . . . . . . . . . . . . . .
320
7.5.4
Schatten packing with a ℓ∞constraint . . . . . . . . . . . . . . . . . . . . . .
321
8
High-Dimensional Robust Statistics in Almost-Linear Time
322
8.1
Organization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
322
8.2
Clustering mixture models in almost-linear time . . . . . . . . . . . . . . . . . . . . .
323
8.2.1
Our results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
326
8.2.2
Technical overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
8.3
Preliminaries: clustering mixture models . . . . . . . . . . . . . . . . . . . . . . . . .
331
8.3.1
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
332
8.3.2
Technical tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
8.3.3
Potential function approach to fast ﬁltering . . . . . . . . . . . . . . . . . . .
335
8.4
Fast bounded covariance multiﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . .
338
8.4.1
Reducing Partition to SplitOrCluster . . . . . . . . . . . . . . . . . . . . . . . .
339
8.4.2
Reducing SplitOrCluster to SplitOrTailBound and Fixing . . . . . . . . . . . . .
343
8.4.3
Implementation of SplitOrTailBound
. . . . . . . . . . . . . . . . . . . . . . .
347
8.4.4
Fixing a cluster via fast ﬁltering
. . . . . . . . . . . . . . . . . . . . . . . . .
350
8.4.5
Runtime analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
356
8.4.6
Full bounded covariance algorithm . . . . . . . . . . . . . . . . . . . . . . . .
357
xvi

8.4.7
Cleaning up the list
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
360
8.4.8
(Slightly) improving the error rate . . . . . . . . . . . . . . . . . . . . . . . .
363
8.5
Clustering mixture models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
364
8.5.1
Clustering uniform (sub-)Gaussian mixture models . . . . . . . . . . . . . . .
364
8.5.2
Robustly clustering (sub-)Gaussian mixture models . . . . . . . . . . . . . . .
367
8.5.3
Mixture models with bounded fourth moments . . . . . . . . . . . . . . . . .
369
8.5.4
Bounded-covariance mixture models . . . . . . . . . . . . . . . . . . . . . . .
371
8.6
Robust regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
373
8.6.1
Our results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
374
8.6.2
Prior work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
378
8.6.3
Techniques
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
379
8.7
Preliminaries: robust regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
381
8.7.1
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
381
8.7.2
Our statistical models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
384
8.7.3
Linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
385
8.7.4
Regularity assumptions: Lipschitz and smooth stochastic optimization . . . .
389
8.7.5
Robustly decreasing the covariance operator norm
. . . . . . . . . . . . . . .
391
8.8
Linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
392
8.8.1
Filtering under (ϵ, ϵ2
α )-goodness . . . . . . . . . . . . . . . . . . . . . . . . . .
392
8.8.2
Identiﬁability proof for linear regression . . . . . . . . . . . . . . . . . . . . .
393
8.8.3
Halving the distance to θ⋆. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
395
8.8.4
Last phase analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
8.8.5
Full algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
400
8.9
Robust acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
401
8.9.1
Noisy gradient oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
402
8.9.2
Proximal subproblems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
404
8.9.3
Halving the distance to θ⋆
F
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
8.9.4
Full accelerated algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
411
8.10 Lipschitz generalized linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . .
413
8.10.1 Noisy gradient oracle for the Moreau envelope . . . . . . . . . . . . . . . . . .
413
8.10.2 Accelerated optimization of the regularized Moreau envelope
. . . . . . . . .
416
8.11 Robust sub-Gaussian principal component analysis . . . . . . . . . . . . . . . . . . .
416
8.11.1 Robust sub-Gaussian PCA via ﬁltering
. . . . . . . . . . . . . . . . . . . . .
420
8.11.2 Robust sub-Gaussian PCA in nearly-linear time
. . . . . . . . . . . . . . . .
421
9
Semi-Random Linear Systems
427
9.1
Organization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
427
9.2
Diagonal preconditioning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
428
xvii

9.2.1
Inner scaling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
429
9.2.2
Outer scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
430
9.3
Overcomplete semi-random linear systems . . . . . . . . . . . . . . . . . . . . . . . .
437
9.3.1
Semi-random linear systems . . . . . . . . . . . . . . . . . . . . . . . . . . . .
438
9.3.2
Statistical linear regression
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
439
9.4
Semi-random sparse recovery
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
444
9.4.1
Our techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
449
9.4.2
Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
453
9.5
Preliminaries: semi-random sparse recovery . . . . . . . . . . . . . . . . . . . . . . .
454
9.6
Exact recovery
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
455
9.6.1
Radius contraction using step oracles . . . . . . . . . . . . . . . . . . . . . . .
456
9.6.2
Designing a step oracle
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
459
9.6.3
Equivalence between Assumption 11 and RIP . . . . . . . . . . . . . . . . . .
465
9.6.4
Putting it all together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
468
9.7
Noisy recovery
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
468
9.7.1
Radius contraction above the noise ﬂoor using step oracles . . . . . . . . . . .
469
9.7.2
Designing a strong step oracle . . . . . . . . . . . . . . . . . . . . . . . . . . .
475
9.7.3
Equivalence between Assumption 12 and RIP . . . . . . . . . . . . . . . . . .
477
9.7.4
Putting it all together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
478
10 Towards an Oracle Complexity of Sampling
480
10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
480
10.1.1 Our results: upper bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
482
10.1.2 Our results: lower bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
486
10.1.3 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
490
10.1.4 Technical overview: upper bounds
. . . . . . . . . . . . . . . . . . . . . . . .
493
10.1.5 Technical overview: lower bounds . . . . . . . . . . . . . . . . . . . . . . . . .
499
10.1.6 Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
501
10.2 Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
501
10.2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
501
10.2.2 Technical facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
503
10.2.3 Metropolis-adjusted Langevin algorithm . . . . . . . . . . . . . . . . . . . . .
504
10.2.4 Hamiltonian Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
505
10.3 Proximal reduction framework
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
505
10.4 Tighter runtimes for structured densities . . . . . . . . . . . . . . . . . . . . . . . . .
510
10.4.1 Well-conditioned logconcave sampling: proof of Corollary 45 . . . . . . . . . .
510
10.4.2 Composite logconcave sampling: proof of Corollary 46 . . . . . . . . . . . . .
513
10.4.3 Sampling logconcave ﬁnite sums: proof of Corollary 47 . . . . . . . . . . . . .
514
xviii

10.5 Composite logconcave sampling with a restricted Gaussian oracle . . . . . . . . . . .
516
10.5.1 Reduction from Composite-Sample to Composite-Sample-Shared-Min
. . .
516
10.5.2 Reduction from Composite-Sample-Shared-Min to Sample-Joint-Dist . . .
519
10.5.3 Implementing Sample-Joint-Dist . . . . . . . . . . . . . . . . . . . . . . . .
519
10.5.4 Putting it all together: proof of Theorem 69 . . . . . . . . . . . . . . . . . . .
519
10.6 Logconcave ﬁnite sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
520
10.6.1 Approximate Metropolis-Hastings
. . . . . . . . . . . . . . . . . . . . . . . .
521
10.6.2 Conductance analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
525
10.7 Lower bound for MALA on Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . .
527
10.8 Lower bound for MALA on well-conditioned distributions . . . . . . . . . . . . . . .
534
10.9 Mixing time lower bound for MALA . . . . . . . . . . . . . . . . . . . . . . . . . . .
541
10.9.1 Mixing time lower bound for small h . . . . . . . . . . . . . . . . . . . . . . .
542
10.9.2 Proof of Theorem 73 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
543
10.10Lower bounds for HMC
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
544
10.10.1Structure of HMC: a detour to Chebyshev polynomials . . . . . . . . . . . . .
544
10.10.2HMC lower bound for all K . . . . . . . . . . . . . . . . . . . . . . . . . . . .
548
10.11Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
553
A Deferred proofs from Chapter 2
554
A.1 Convex analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
554
A.2 Unaccelerated smooth convex optimization via mirror prox
. . . . . . . . . . . . . .
557
A.3 Minimax optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
558
A.4 Additional extragradient methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
560
A.4.1
Strongly monotone mirror prox . . . . . . . . . . . . . . . . . . . . . . . . . .
560
A.4.2
Dual extrapolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
561
A.5 Extragradient acceleration in non-Euclidean norms . . . . . . . . . . . . . . . . . . .
562
A.6 Missing proofs from Section 2.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
565
A.7 Optimism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
568
A.8 Reducing strongly monotone problems to regularized subproblems
. . . . . . . . . .
568
A.8.1
Convex optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
568
A.8.2
Convex-concave optimization . . . . . . . . . . . . . . . . . . . . . . . . . . .
570
A.9 Helper facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
571
A.10 Proofs for Section 2.9
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
573
A.10.1 Proofs for Section 2.9.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
573
A.10.2 Proofs for Section 2.9.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
576
A.10.3 Proofs for Section 2.9.4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
580
xix

B Deferred proofs from Chapter 3
584
B.1
Deferred proofs from Section 3.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
584
B.2
Deferred proofs from Section 3.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
586
B.2.1
Proof of Proposition 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
586
B.2.2
Proof of Proposition 10
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
588
B.2.3
Proof of Proposition 11
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
589
B.3
Deferred proofs for sublinear methods
. . . . . . . . . . . . . . . . . . . . . . . . . .
593
B.3.1
ℓ2-ℓ2 sublinear coordinate method
. . . . . . . . . . . . . . . . . . . . . . . .
593
B.3.2
ℓ2-ℓ1 sublinear coordinate method
. . . . . . . . . . . . . . . . . . . . . . . .
597
B.4
Deferred proofs for variance-reduced methods . . . . . . . . . . . . . . . . . . . . . .
601
B.4.1
Helper proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
601
B.4.2
ℓ2-ℓ2 variance-reduced coordinate method . . . . . . . . . . . . . . . . . . . .
603
B.4.3
ℓ2-ℓ1 variance-reduced coordinate method . . . . . . . . . . . . . . . . . . . .
608
B.5
Additional results on variance-reduced methods . . . . . . . . . . . . . . . . . . . . .
612
B.5.1
Row-column sparsity variance-reduced methods . . . . . . . . . . . . . . . . .
612
B.5.2
Extensions with composite terms . . . . . . . . . . . . . . . . . . . . . . . . .
614
B.6
Deferred proofs from Section 3.6
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
618
B.6.1
Proofs from Section 3.6.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
618
B.6.2
Proofs from Section 3.6.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
619
B.6.3
Proofs from Section 3.6.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
619
B.7
Strongly monotone proximal method . . . . . . . . . . . . . . . . . . . . . . . . . . .
622
B.8
IterateMaintainer2: numerical stability and variations . . . . . . . . . . . . . . . .
625
B.8.1
Numerical stability of IterateMaintainer1.
. . . . . . . . . . . . . . . . . .
625
B.8.2
WeightedIterateMaintainer2 . . . . . . . . . . . . . . . . . . . . . . . . . .
626
B.8.3
CenteredIterateMaintainer2 . . . . . . . . . . . . . . . . . . . . . . . . . .
628
C Deferred proofs from Chapter 4
630
C.1 Missing proofs from Section 4.1 and Section 4.2 . . . . . . . . . . . . . . . . . . . . .
630
C.1.1
Folklore bound on size of ℓ∞-strongly-convex functions . . . . . . . . . . . . .
630
C.1.2
Reduction from general box-constrained ℓ∞regression to Deﬁnition 8
. . . .
631
C.1.3
Convergence rates of ﬁrst-order methods . . . . . . . . . . . . . . . . . . . . .
632
C.1.4
Proof of Lemma 239 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
633
C.1.5
Proof of Lemma 240 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
634
C.2 Missing proofs from Section 4.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
634
C.2.1
Reducing undirected maximum ﬂow to ℓ∞regression . . . . . . . . . . . . . .
634
C.2.2
Reducing directed maximum ﬂow to undirected maximum ﬂow . . . . . . . .
637
xx

D Deferred proofs from Chapter 5
639
D.1 Area convexity and optimal transport
. . . . . . . . . . . . . . . . . . . . . . . . . .
639
D.1.1
Optimal transport . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
639
D.1.2
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
643
D.1.3
Main algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
646
D.1.4
Rounding to Ur,c . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
649
D.1.5
Missing proofs from Section D.1.3
. . . . . . . . . . . . . . . . . . . . . . . .
650
D.1.6
Experiments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
657
D.1.7
Deferred proofs from Section 5.3
. . . . . . . . . . . . . . . . . . . . . . . . .
661
D.2 Matching tools from the literature
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
663
D.3 Cycle cancelling in low space
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
665
D.3.1
Link/cut tree description
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
666
D.3.2
Implementation of BCCO . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
667
D.4 Sampling for rounding linear programming solutions . . . . . . . . . . . . . . . . . .
668
D.4.1
Concentration bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
668
D.4.2
Random sampling guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . .
670
D.4.3
Application: rounding MCM solutions . . . . . . . . . . . . . . . . . . . . . .
674
D.5 Approximate MCM via box-constrained Newton's method . . . . . . . . . . . . . . .
675
E Deferred proofs from Chapter 6
684
E.1
Proofs for Section 6.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
684
E.1.1
Proofs for Section 6.3.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
684
E.1.2
DDBM via regularized box-simplex games . . . . . . . . . . . . . . . . . . . .
686
E.1.3
DDBM via matrix scaling and box-constrained Newton's method . . . . . . .
689
E.1.4
DDBM via Sinkhorn objective solver in [124] . . . . . . . . . . . . . . . . . .
691
E.2
Proofs for Section 6.4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
692
E.2.1
Proofs for Section 6.4.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
694
E.2.2
Proofs for Section 6.4.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
698
E.2.3
Proofs for Section 6.4.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
701
E.3
Approximating Sinkhorn distances . . . . . . . . . . . . . . . . . . . . . . . . . . . .
708
E.3.1
Regularized box-simplex solver for computing Sinkhorn distances: Theorem 85 709
E.3.2
Box-constrained Newton for computing Sinkhorn distances: Theorem 86 . . .
711
E.3.3
Unaccelerated rate of Sinkhorn's algorithm
. . . . . . . . . . . . . . . . . . .
713
F Deferred proofs from Chapter 7
716
F.1
Deferred proofs from Section 7.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
716
F.1.1
Dual averaging regret bounds . . . . . . . . . . . . . . . . . . . . . . . . . . .
716
F.1.2
High probability regret bounds . . . . . . . . . . . . . . . . . . . . . . . . . .
717
xxi

F.1.3
Proof of Lemma 7.2.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
718
F.1.4
Proof of Lemma 7.2.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
719
F.1.5
Facts about the Beta distribution . . . . . . . . . . . . . . . . . . . . . . . . .
726
F.1.6
Eﬃcient computation of matrix exponential-vector products . . . . . . . . . .
727
F.1.7
Description of the Lanczos method . . . . . . . . . . . . . . . . . . . . . . . .
728
F.2
Deferred proofs from Sections 7.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
734
F.2.1
Proof of Proposition 26
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
734
F.3
Deferred proofs from Section 7.5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
735
F.3.1
Proofs from Section 7.5.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
735
F.3.2
Proofs from Section 7.5.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
736
F.3.3
Proof of Proposition 30
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
739
G Deferred proofs from Chapter 8
744
G.1 List-decodable mean estimation for α−1 = Ω(d) . . . . . . . . . . . . . . . . . . . . .
744
G.2 Filtering in k dimensions: SIFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
745
G.2.1
General preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
746
G.2.2
Filtering preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
747
G.2.3
Analysis of SIFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
750
G.3 Fast ﬁltering in k dimensions under a diameter bound . . . . . . . . . . . . . . . . .
753
G.3.1
Analysis of DecreaseKFNorm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
756
G.3.2
Analysis of ProduceGoodTuple . . . . . . . . . . . . . . . . . . . . . . . . . . .
761
G.4 Cleanup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
764
G.4.1
Merging candidate means . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
764
G.4.2
Bounding dataset diameter
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
766
G.4.3
Putting it all together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
767
G.4.4
Trading oﬀaccuracy for runtime . . . . . . . . . . . . . . . . . . . . . . . . .
769
G.5 Warmup: fast Gaussian multiﬁlter
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
770
G.5.1
Reducing GaussianPartition to GaussianSplitOrCluster . . . . . . . . . . . . . .
771
G.5.2
Implementation of GaussianSplitOrCluster
. . . . . . . . . . . . . . . . . . . .
775
G.5.3
Full Gaussian algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
778
G.6 Deferred proofs from Section 3.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
782
G.6.1
Proof of Proposition 33
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
782
G.6.2
Proof of Proposition 36
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
785
G.7 Concentration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
789
G.7.1
Sub-Gaussian concentration . . . . . . . . . . . . . . . . . . . . . . . . . . . .
789
G.7.2
Concentration under weightings in Sn
ϵ . . . . . . . . . . . . . . . . . . . . . .
790
G.8 Deferred proofs from Section 8.11.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
792
G.8.1
Robust univariate variance estimation . . . . . . . . . . . . . . . . . . . . . .
792
xxii

G.8.2
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
795
G.8.3
Analysis of PCAFilter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
796
G.9 Deferred proofs from Section 8.11.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
799
G.9.1
Proof of Proposition 40
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
799
G.9.2
Proof of Lemma 149 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
800
G.9.3
Proof of Lemma 150 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
800
H Deferred proofs from Chapter 9
801
H.1 Deferred proofs from Section 9.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
801
H.1.1
Polynomial approximation to the square root . . . . . . . . . . . . . . . . . .
801
H.1.2
Deferred proofs from Section 9.2.2
. . . . . . . . . . . . . . . . . . . . . . . .
802
H.1.3
Normalizing the diagonal
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
803
H.1.4
Faster scalings with a conjectured subroutine . . . . . . . . . . . . . . . . . .
806
H.2 Greedy and non-convex methods fail in the semi-random setting
. . . . . . . . . . .
813
H.2.1
Iterative hard thresholding
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
814
H.2.2
Orthogonal matching pursuit . . . . . . . . . . . . . . . . . . . . . . . . . . .
814
H.2.3
Convex methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
815
H.3 Proof of Lemma 171 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
816
I
Deferred proofs from Chapter 10
820
I.1
Discussion of inexactness tolerance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
820
I.2
Deferred proofs from Section 10.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
821
I.2.1
Deferred proofs from Section 10.5.2 . . . . . . . . . . . . . . . . . . . . . . . .
821
I.2.2
Deferred proofs from Section 10.5.3 . . . . . . . . . . . . . . . . . . . . . . . .
827
I.3
Mixing time ingredients
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
830
I.3.1
Warm start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
830
I.3.2
Transitions of nearby points . . . . . . . . . . . . . . . . . . . . . . . . . . . .
832
I.3.3
Isoperimetry
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
835
I.3.4
Correctness of Y-Oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
836
I.4
Structural results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
838
I.5
Equivalence of HMC and Metropolis-adjusted Langevin dynamics . . . . . . . . . . .
841
I.6
Gradient concentration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
842
I.7
Necessity of ﬁxing a scale
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
846
I.8
HMC lower bounds beyond κ
√
d
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
846
I.8.1
Relaxation time lower bound for small K
. . . . . . . . . . . . . . . . . . . .
847
I.8.2
Mixing time lower bound for small K
. . . . . . . . . . . . . . . . . . . . . .
853
Bibliography
856
xxiii

List of Tables
3.1
Dependence on A for diﬀerent methods in diﬀerent geometries. Comments: Ai: and
A:j denote the ith row and jth column of A, respectively. Numerically sparse instances
satisfy Lco = O(Lrc).
† In the ℓ2-ℓ1 setting we can also achieve Lco = Lrc
√rcs and
Lco = max{maxi ∥Ai:∥1 ,
p
maxi ∥Ai:∥1 maxj ∥A:j∥1}.
. . . . . . . . . . . . . . . . .
86
3.2
Comparison of iterative methods for bilinear problems. Comments: nnz denotes the
number of nonzeros in A ∈Rm×n and rcs ≤max{m, n} denotes the maximum number
of nonzeros in any row and column of A. The quantities Lmv, Lco and Lrc depend on
problem geometry (see Table 3.1).
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
3.3
Comparison of complexity for diﬀerent applications. Comments: ρ denotes the radii
ratio of the minimum ball enclosing the rows of A and maximum ball inscribed in
them.
† For MaxIB and MinEB, we refer the reader to Section 3.6.2 for a more
ﬁne-grained runtime bound. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
3.4
The distributions p, q used in our coordinate gradient estimator. Comments: The estimator
is of the form ˜g(x, y) =
  1
pij yiAij · ej, −1
qlk Alkxk · el

where i, j ∼p and l, k ∼q.
. . . . .
96
3.5
The distributions p, q used for our reduced variance coordinate gradient estimator. Com-
ments: The estimator is of the form ˜g(x, y) =
 A⊤y+ 1
pij (yi −y0,i)Aij·ej, −Ax−1
qlk Alk(xk −x0,k)·
el

where i, j ∼p and l, k ∼q and x0, y0 is a reference point. . . . . . . . . . . . . . . . .
96
3.6
Local norm setups. Comments: In each case, Z = X × Y, ∆n is the probability
simplex {x | x ∈Rn
≥0, 1⊤
n x = 1}, Bn is the Euclidean ball {x | x ∈Rn, ∥x∥2 ≤1}, the
operations sign, min, and | · | are performed entrywise on a vector, and ◦stands for
the entrywise product between vectors.
. . . . . . . . . . . . . . . . . . . . . . . . .
100
4.1
Dependencies of algorithms for ℓ∞regression in A ∈Rn×m on various problem pa-
rameters. Note that there is up to an O(√m) discrepancy between the ℓ2 and ℓ∞
norms. Here, d is the maximum number of nonzero entries in any column of A.
. .
149
4.2
Complexity of maximum ﬂow since [242] for undirected graphs with n vertices, m
edges, where s is the ℓ2
2 of the maximum ﬂow's congestion, and F is the maximum
ﬂow value. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
xxiv

8.1
Robust linear regression results in the κ ≫1 regime. All listed results assume 2-
to-4 hypercontractivity and independent noise (although some also give rates for
non-independent noise).
Error guarantees are in Mahalanobis distance.
We omit
polylogarithmic factors for simplicity. . . . . . . . . . . . . . . . . . . . . . . . . . . .
378
10.1 Complexity of sampling from e−F (x) where F(x) = 1
n
P
i∈[n] fi(x) on Rd is µ-strongly
convex, each fi is convex and L-smooth, and κ = L
µ . For relevant lines, M is the
Lipschitz constant of the Hessian ∇2F, which our algorithm has no dependence on.
Complexity is measured in terms of the number of calls to fi or ∇fi for summands
{fi}i∈[n]. We hide polylog( nκd
ϵ ) factors for simplicity. . . . . . . . . . . . . . . . . . .
491
D.1 Optimal transport algorithms. Algorithms using second-order information use potentially-
expensive SDD system solvers; the runtime analysis of Sink/Greenkhorn is due to
[208, 365].
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
642
xxv

List of Figures
9.1
The eﬀect of ℓ1 projection on iterate progress. The dashed line represents a facet of
the ℓ1-ball around xt of radius ∥xt −x⋆∥1. . . . . . . . . . . . . . . . . . . . . . . . .
452
10.1 Second derivative of our hard function f1d, κ = 10, h = 0.01. Starting from inside
the hard region, on average over g ∼N(0, I), a move by
√
2hg decreases the second
derivative. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
500
C.1 The reduction from solving the approximate maximum ﬂow problem to solving ˜O(1)
approximate regression problems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
636
C.2 Recovering a maximum ﬂow in directed G via a maximum ﬂow in undirected G′. . .
638
D.1 Edge-incidence matrix A of a 3 × 3 bipartite graph and uniform demands. . . . . . .
643
xxvi

Chapter 1
Introduction
Iterative methods are, in a broad sense, algorithmic procedures which make progress towards solving
a computational task by performing a sequence of update rules, which are potentially much less
cumbersome to implement than an "all-at-once" solution to the original problem. The analysis of
iterative methods typically proceeds by ﬁrst deﬁning a global progress measure (a "potential"), and
showing that local iterative updates advance this potential. In some cases, the potential used for
analysis purposes directly reﬂects the problem the algorithm designer is trying to solve (e.g. when
minimizing a loss function, the potential may be the function value itself). In other cases, e.g. when
the most direct potential of interest either is diﬃcult to directly measure or does not accurately
reﬂect the type of progress being made by local iterates, designing an appropriate potential for
analysis purposes can become a much more challenging endeavor.
Simple iterative methods such as (stochastic) gradient descent have emerged as a powerful
workhorse enabling many recent advances in data science, operations research, and machine learn-
ing. The practical appeal of iterative methods is clear: by synthesizing local update rules into an
algorithm with global performance guarantees, iterative methods eﬀectively decouple the eﬃcient
implementation of the local update (such as matrix-vector multiplications, gradient computations,
or other vectorizable operations) from the analysis of the overall algorithm. From an algorithm de-
sign perspective, what remains is to establish an understanding of performance measures obtained
by the potpourri of iterative methods which practitioners may choose between. However, in many
situations, there is not a "one-size-ﬁts-all" type of analysis which generically applies to the myriad
use cases of an iterative method framework. To advance the algorithmic frontier for a speciﬁc task at
hand, an algorithm designer needs to take into account speciﬁc structural aspects of the problem in
choosing an iterative method (and conducting a corresponding analysis, e.g. designing a potential).
Ubiquitous algorithms and algorithmic primitives building upon iterative update rules, such as
stochastic gradient methods, convex programming, regression, and clustering, have well-understood
1

CHAPTER 1. INTRODUCTION
2
analyses in classical "textbook" regimes, under simplifying assumptions such as moderate dimension-
ality, knowledge of problem parameters, and an independently-sampled dataset. Frequently, these
analyses are applied black-box in designing methodology for speciﬁc applications to obtain theo-
retical performance guarantees. However, in prominent modern applications going beyond these
well-studied regimes, the theoretical guarantees of basic algorithms and their classical analyses may
leave much on the table. This is particularly true when pursuing tradeoﬀs between goals such as ro-
bustness, accuracy, and computational eﬃciency as is often demanded by the practice of data-driven
applications. This state of aﬀairs suggests the following directions towards developing a modern
theory of iterative methods.
First, can we develop a principled understanding of the power of classical frameworks for designing
iterative methods, under design constraints imposed by the modern practice of data science?
Moreover, in cases when classical frameworks fall short, can we expand the modern computa-
tional theory of data science by proposing new algorithmic frameworks which provably address the
shortcomings of the existing toolkit in the context of these aforementioned design constraints?
In many of the case studies contained in this thesis, the answer to these questions is yes, with an
important qualiﬁcation: to go beyond the black-box performance guarantees of analysis frameworks
for iterative methods, we will crucially rely on a deeper understanding of speciﬁc structural properties
of the algorithmic problem we study. This type of problem-speciﬁc structural understanding can be
viewed as a guiding force in the theory of algorithm design in two ways. Most directly, it guides our
selection of an appropriate iterative method and analysis for solving our desired problem. Taking a
more comprehensive view, if the type of structure in question is suﬃciently fundamental (e.g. shared
across multiple natural computational problems), it may motivate the development of new iterative
methods which are capable of taking advantage of said structure, and which thus have important
implications for solving future problems with similar properties.
The goal of this thesis is to investigate algorithmic problems at the interface of iterative method
design and the practice of modern data science through the lens of these two questions. Admittedly,
the problems this thesis studies are fairly diverse, and at ﬁrst glance may not seem particularly
cohesive. However, the design of algorithms for solving these problems becomes a substantially more
uniﬁed undertaking when approached through the principles discussed thus far, namely furthering
the interplay between iterative method theory and structured data science. We organize the thesis
into two distinct parts, which take diﬀerent perspectives on the types of iterative method frameworks
they investigate, the structure and mathematical properties of the problems they capitalize on, and
the analytical toolkits they borrow from in synthesizing complete algorithms.
Each part of this thesis will begin with our developments of an aspect of modern iterative method
theory: stochastic variational inequality solvers in Part I, and eﬃcient semideﬁnite program solvers
in Part II. In each case, the theory we develop aims to answer the following questions for the family
of structured optimization problems at hand.

CHAPTER 1. INTRODUCTION
3
1. Convergence analysis. How can we develop analysis frameworks which eﬀectively capture the
convergence of our iterative methods (e.g. by choosing an appropriate potential function)?
2. Eﬃcient iterations. How can we couple our framework with cheaply-implementable iterations
(e.g. by replacing complex updates with simpler or more ﬂexible alternatives, which aﬀord a
provably comparable iteration quality in terms of relevant progress measures)?
In each of Parts I and II, we will oﬀer fresh perspectives for both of Questions 1 and 2: in the latter
case of answering Question 2, we will frequently combine our developments in optimization theory
with advancements in numerical linear algebra (i.e. streaming and sketching) or data structures.
Finally, the latter portion of each part of the thesis develops a close interplay between the new
aspects of optimization theory we build and algorithms with improved performance guarantees for
a fundamental application domain: combinatorial optimization in Part I, and high-dimensional
statistics in Part II. By pushing forth this interplay with new iterative methods which are capable
of taking advantage of structural aspects of the relevant application domain, our goal in this thesis
is to showcase the power and ﬂexibility of application-mindful iterative method design for solving
problems in structured algorithmic data science.
Part I: Stochastic Variational Inequalities and Combinatorial Optimization
In the ﬁrst part of this thesis, we develop new iterative methods for solving variational inequalities
admitting stochastic formulations, as well as reﬁned frameworks for analyzing these new methods.
In Chapters 2 and 3, we directly apply these new methods to answer several open questions in
the theory of ﬁrst-order optimization methods.
In Chapters 4, 5, and 6, we improve the state-
of-the-art for a variety of well-studied problems in combinatorial optimization, such as maximum
ﬂow, optimal transport, and bipartite matching in diﬀerent computational models, by combining
our new iterative machinery with insights from the theoretical computer science literature, such as
combinatorial constructions and eﬃcient data structures.
We now brieﬂy motivate how these developments came to be.
Our approach to solving the
algorithmic graph theory problems we study in Chapters 4, 5, and 6 is to ﬁrst relax the underly-
ing combinatorial optimization problem to an appropriate continuous formulation. The resulting
continuous formulations share the following properties.
1. They are naturally formulated as zero-sum games between two players with geometrically-
constrained domains (e.g. a Euclidean ball, a probability simplex, or a coordinatewise box).
2. They have combinatorial structure which can be harnessed to obtain tighter runtime char-
acterizations, and are eﬀectively captured by subsampling schemes (e.g. they are regression
problems in an adjacency matrix with rows that are sparse or bounded, or become such a
problem after applying a graph-structured preconditioner).

CHAPTER 1. INTRODUCTION
4
In order to capitalize upon these structural aspects, we develop the theory of stochastic variational
inequalities.
Variational inequalities (VIs) are a well-studied family of continuous optimization
problems parameterized by a vector-valued operator g : Z →Z∗, where Z∗is the dual space of Z,
see e.g. [458, 460, 459]. A strong solution to a VI in an operator g is a point z⋆∈Z such that
⟨g(z⋆), z⋆−z⟩≤0 for all z ∈Z.
(1.1)
The attentive reader well-versed in convex optimization may recognize (1.1) as the ﬁrst-order opti-
mality condition [458, 98]; indeed, when g = ∇f for a diﬀerentiable convex function f deﬁned over
a domain Z, (1.1) is a necessary and suﬃcient condition for z⋆to minimize f. This phenomenon is
not unique to convex optimization: (1.1) has important implications for more general VIs, particu-
larly when the operator g is monotone (for all z, w ∈Z, ⟨g(z) −g(w), z −w⟩≥0). Indeed, convex
function gradients are monotone, and further another important example of a monotone operator
is an appropriate gradient operator of a convex-concave function f : Z →R, where Z = X × Y is
a product domain.1 In this case, (1.1) implies that z⋆is an equilibrium of f, and hence solves an
appropriate minimax optimization problem.
In the special case of convex optimization, a substantial amount of eﬀort in the continuous
optimization community has been devoted to the understanding of optimal query complexities for
solving (1.1) via ﬁrst-order iterative methods, which in each iteration, query g at a point in the
domain and perform a computationally-cheap update rule (e.g. direct vector addition). This has
led to a comprehensive understanding of the complexity of solving convex optimization problems in
various structured forms. A particularly important type of structure in the practice of optimization
is stochasticity. Informally, this property is captured when g admits a natural formulation as an
expectation over a random operator ˜g : Z →Z∗, and either updates or queries to ˜g are substantially
cheaper than the analogs in g. A canonical example is when f is an average loss function over a
sample or a population, and ˜g = ∇fi is the loss gradient evaluated at a speciﬁc individual i.
However, much less is known about the complexity of solving stochastic variational inequalities
beyond convex minimization.
This becomes highly relevant in our applications to combinatorial
optimization, which are often encapsulated by an appropriate stochastic minimax problem, as al-
luded to by the structural properties discussed previously. Correspondingly, in Chapters 2 and 3, we
develop new iterative methods for analyzing and solving more general stochastic variational inequal-
ities. More concretely, Chapter 2 investigates the power of primal-dual formulations of variational
inequalities for designing accelerated algorithms, making progress towards understanding Question 1
from the perspective of designing analysis frameworks for acceleration. Further, Chapter 3 builds
theories of variance reduction and numerical sparsity for stochastic minimax problems, providing
new tools for answering Question 2 for a suite of structured sparse updates.
1More precisely, in the case when f(·, y) is convex and f(x, ·) is concave for any x ∈X, y ∈Y, we will deﬁne the
monotone operator g(x, y) = (∇xf(x, y), −∇yf(x, y)).

CHAPTER 1. INTRODUCTION
5
Beyond yielding important primitives and design frameworks which will later be used in Chap-
ters 4, 5, and 6, we note that results in Chapters 2 and 3 represent independently important progress
in optimization theory, answering several basic open questions some of which have seen no improved
bounds in over a decade (e.g. approximating a Nash equilbrium in a zero-sum game). More broadly,
both minimax and stochastic optimization problems have become increasingly important for mod-
eling purposes in modern data science, representing robustness to an adversary and scalability to
large datasets respectively. As a result, we are optimistic that developing a general understanding
of stochastic VI theory will have important downstream implications, a major motivation for the
work in Part I of this thesis beyond speciﬁc applications in combinatorial optimization.
Finally, we brieﬂy discuss the types of combinatorial optimization problems which are considered
in Part I of the thesis, and the modern algorithmic design considerations they aim to address.
Our results in Chapter 4, 5, and 6 showcase the interplay between iterative method design and
combinatorial problem structure, as well as the ﬂexibility of the stochastic VI solvers we build in
adapting to various design constraints.
1. In Chapter 4, we consider the undirected maximum ﬂow problem on capacitated graphs. We
develop stochastic algorithms based on coordinate subsampling which, on a graph with m
edges and n vertices, yield a ﬂow of quality 1 −ϵ times the optimum with a runtime2 of
eO(m + √mnϵ−1), which remains the state-of-the-art for moderately small ϵ (and is improved
further for graphs with sparse maximum ﬂows).
Our methods take advantage of inherent
sparsity structure in graphs: by routing ﬂows on a size-m graph via a smaller object (of
dimension eO(n)), after a small number of passes over the graph, the method has a convergence
rate which scales sublinearly in graph size, an appealing property for large-scale applications.
2. In Chapter 5, we consider combinatorial optimization problems such as transportation and
matching in a semi-streaming setting, where we can only access the description of the graph
using read-only passes over its adjacency matrix. We improve the pass complexities of semi-
streaming iterative methods by applying acceleration tools to an appropriate continuous re-
laxation of the optimization problem. Moreover, by opening up the recursions governing our
iterative method iterates, we demonstrate that our new algorithms can be implemented using
an optimal O(n) space. To achieve this, we combine an implicit representation of our iterative
method iterates with dynamic data structure tools for sparsifying solutions on the ﬂy.
3. In Chapter 6, we consider the application of iterative methods to dynamic combinatorial
optimization problems, where one wishes to maintain an (approximate) solution on a slowly-
changing graph, without recomputing from scratch.
We demonstrate a simple framework
for a dynamic model of bipartite matching, which reduces the solution maintenance problem
2Throughout the thesis, for ease of presentation, e
O hides polylogarithmic factors in appropriate problem parame-
ters; in speciﬁc sections, we will be more precise in quantifying the relevant paramters.

CHAPTER 1. INTRODUCTION
6
to solving a small number of regularized regression subproblems. Intuitively, regularization
equips the subproblem solutions with robustness properties which persist in a dynamic model,
even against strong (adversarial) models of how the problem changes over time. Our hope
is that this insight opens the door towards further applications of the powerful continuous
optimization toolkit for dynamic algorithmic questions.
Part II: Semideﬁnite Programming and High-Dimensional Statistics
In the second part of this thesis, we present novel solvers achieving nearly-linear runtimes for various
structured semideﬁnite programs, and their further algorithmic implications. Semideﬁnite program-
ming (SDP) is a powerful primitive in convex optimization, and generalizes linear programming (LP)
to matrix-valued variables and constraints. Formally, semideﬁnite programs (SDPs) are concerned
with optimization problems of the form
min ⟨C, X⟩subject to X ∈Sd
≥0, ⟨Ai, X⟩≥bi for all i ∈[n],
(1.2)
where C, X, {Ai}i∈[n] are real, symmetric d × d matrices and Sd
≥0 is the d × d positive semideﬁnite
cone. However, the ﬂexibility of using SDP solvers to attack an algorithmic problem can come at a
signiﬁcant computational price. The implementation of update rules for semideﬁnite programming
solvers is notoriously expensive, often requiring full-matrix spectral operations (e.g. inversion or
eigendecomposition), or careful analysis of the error propagation of approximate operations (see
e.g. [399] for an extensive discussion of the diﬃculty of implicitly exponentiating a matrix). This
computational overhead is a substantial challenge to address when deciding whether to use a SDP
solver in practice. In Chapter 7, we give a suite of new SDP tools which both run in nearly-linear
time, and eﬀectively capture diﬀerent types of problem structure. Two important types of structure
we investigate are monotonicity (e.g. where the constraints {Ai}i∈[n] in (1.2) are all nonnegative
or nonpositive when viewed in an operator sense) and geometry (e.g. variants of (1.2) which have
constraint geometry captured by an appropriate matrix norm, such as Schatten or Ky Fan norms,
and hence can be phrased as regret minimization over an appropriate dual matrix set).
The solvers we give in Chapter 7 advance our understanding of Questions 1 and 2 on multiple
fronts.
From the perspective of Question 1, we suggest a new approach towards understanding
width-independent convergence rates for monotone SDPs based on opening up potentials used in
regret minimization, which results in multiple novel solvers. From the perspective of Question 2,
we demonstrate new mathematical properties of low-rank sketches and the resulting approximate
projections they implement, which allows for their use in iterates of our SDP solvers.
In Chapters 8 and 9, we build upon these new solvers either directly or indirectly (drawing
inspiration from their design principles) to develop new methods for modern algorithmic questions
in high-dimensional statistics. A common theme in these two chapters is the investigation of the

CHAPTER 1. INTRODUCTION
7
robustness of algorithms to model misspeciﬁcation. Classical analyses of estimators and learning
algorithms in statistics theory typically assume a "best-case" scenario from a modeling standpoint,
which makes obtaining provable guarantees more tractable: a few examples follow.
1. Assume that the dataset is drawn independently from the same distribution (homogeneity).
2. Assume that the underlying distribution is "well-behaved" and "explicit" in various senses,
such as belonging to a known family (e.g. Gaussian), exhibiting strong concentration properties
(e.g. light tail behavior or bounded moments), or satisfying an explicit parameterization (e.g.
being in approximately isotropic position, with a near-identity covariance).
However, due to the large-scale nature of modern data collection, imposing such strong assump-
tions about a statistical task may lead to meaningless algorithm performance guarantees, resulting
in estimates which reveal very little about the actual problem at hand. As a result, it is important
to develop algorithms which are ﬂexible enough to datasets we are likely to actually observe, which
may break typical assumptions. These assumption violations may occur at several diﬀerent places
along the data collection pipeline, including heterogeneity of sampling and (potentially adversarial)
dataset contamination; alternatively, the ground truth may simply not have the properties we would
like to posit (e.g. it is non-Gaussian, or satisﬁes weak concentration).
The ﬁeld of robust statistics [31, 512, 278, 513] has classically addressed exactly these consid-
erations, and indeed has resulted in robust estimators which can handle various forms of dataset
contamination. Unfortunately, yet another aspect of modern data science throws a new wrinkle into
the eﬀectiveness of these classical robust estimators: high dimensionality. In particular, classical
robust estimators when viewed as algorithms often result in runtimes exponential in the problem
dimension. This has been alleviated in part by recent advances by the theoretical computer science
community, e.g. [175, 337], but the resulting runtimes of many of these newly proposed algorithms
are still far from linear in the problem size, and remain potentially prohibitive in high dimensions.
This thesis advances the state-of-the-art of robust algorithms for performing statistical tasks in
high dimensions. We consider two fairly distinct examples of algorithmic robustness criteria "beyond
the best case" motivated by practical modeling considerations.
1. In Chapter 8, we develop new algorithms for estimating generalized linear models, clustering
mixture models, and performing principal component analysis under the strong contamination
model. In this model, an adversary is allowed to replace a bounded fraction of (independent)
draws with arbitrary points in the support of the underlying distribution, reﬂecting either
(potentially adversarial) data poisoning or model misspeciﬁcation in total variation distance.
2. In Chapter 9, we give new algorithms for overcomplete linear regression and sparse recovery,
in the presence of a monotone semi-random adversary. Such an adversary can augment a well-
behaved dataset with additional consistent information, which cannot aﬀect the information-
theoretic tractability of the problem, but can arbitrarily hinder the computational performance

CHAPTER 1. INTRODUCTION
8
of iterative methods which succeed under best case assumptions. This adversary captures the
presence of heterogeneous data sources or the breaking of independence assumptions.
All of the algorithms presented in Chapters 8 and 9 crucially both run in almost-linear time in
the dataset size, and further obtain provably near-optimal statistical guarantees under the relevant
contamination model. Our algorithms build heavily upon the SDP solvers we develop in Chapter 7,
to capture structural properties of the underlying problem instance. Intuitively, many of the nice-
ness properties which are used by classical "best-case analyses" of the statistical problems we study
are inherently spectral. For example, bounded moments may be certiﬁed through a small covariance
matrix, and isotropicity and variants thereof are inherently statements about the conditioning of a
dataset (how "spherical" the covariance matrix is). Moreover, algorithms which only take advantage
of independence of samples through matrix concentration arguments can frequently be simulated
through appropriate spectral certiﬁcates of concentration. As a result, even in contaminated situ-
ations beyond the best case, by carefully designing and solving appropriate SDP instances we can
eﬃciently locate enough structure present in the problem to yield high-quality solutions.
Finally, in Chapter 10, we consider an area of algorithmic high-dimensional statistics which is
somewhat of a departure from the other problems in Part II of this thesis: sampling from structured
distributions under weak oracle access to the density.
Speciﬁcally, the goal in Chapter 10 is to
develop an oracle complexity theory for "logconcave sampling" problems of the following ﬂavor:
produce a sample from the density µ ∝exp(−f), where f : Rd →R is convex.
(1.3)
Problems of the form (1.3) frequently arise in computational Bayesian statistics (i.e. posterior sam-
pling), as well as wide-ranging branches of modern data science such as diﬀerential privacy [63] and
reinforcement learning [464]. In many applications, our access to µ is best modeled through a fairly
implicit oracle, which queries gradients or values of the negative log-likelihood f at points (i.e. does
not assume explicit knowledge of the density π or its normalizing constant). The goal of an oracle
complexity theory is then to bound the number of oracle queries required to solve the problem (1.3).
Despite similarities between (1.3) and questions in convex optimization such as min f under value
or gradient oracle access to f, which have been well-understood from an oracle complexity perspective
for decades [414, 418], our understanding of the problem (1.3) from an analogous perspective is
remarkably nascent. In Chapter 10, we push forward this research program by providing state-of-
the-art samplers for basic structured formulations of (1.3) under standard oracle access and weak
regularity assumptions, as well as lower bounds for well-studied approaches to sampler design.
Admittedly, the algorithms we develop for solving (1.3) are fairly distinct from the applications
of SDP technology to robust statistical estimation found in the remainder of Part II of this thesis.
However, the principles we use to design our new sampling frameworks draw heavy inspiration
from the theory of iterative methods, and our analysis synthesizes tools from convex optimization

CHAPTER 1. INTRODUCTION
9
with structural insights from the extensive literature on high-dimensional statistics and applied
probability. In this sense, Chapter 10 is very reﬂective of the themes of Part II at both a philosophical
level, and also in the mathematical tools it uses. We hope the reader is understanding of the technical
non sequitur the inclusion of Chapter 10 presents: the problem it studies is a personal favorite of
the author, and we believe the resulting algorithms we design are suﬃciently reﬂective of the goals
of Part II that they merit representation in the thesis.
Organization
In Sections 1.1, 1.2, 1.3, and 1.4, we describe results contained in this thesis at a more detailed
resolution. For brevity, we concentrate on deﬁning the problems we solve and our results, providing
only abbreviated discussions of motivations and techniques (deferring a more extensive discussion to
the relevant chapters). Section 1.5 provides a bibliography on the content that this thesis represents.
1.1
Results: Theory of Stochastic Variational Inequalities
In Chapters 2 and 3, we give new algorithms for solving various fundamental stochastic VIs (1.1)
in monotone operators. As a result, we obtain improved complexity bounds for several diﬀerent
optimization problems which admit appropriate stochastic VI formulations.
Throughout this discussion, ﬁx a monotone operator g : Z →Z∗, and a convex regularizer
r : Z →R. The classical proximal point method [459] is an implicit method for solving a VI by
recursively performing the update
zt+1 ←argminz∈Z

⟨ηg(z), z −zt⟩+ V r
zt(z)
	
,
(1.4)
where η > 0 is a step size parameter, and V r the Bregman divergence in r, a function which rewards
proximity to zt (e.g. when r is a standard squared Euclidean regularizer, V r is simply the squared
Euclidean distance). When r is strongly convex, it is well-known that (1.4) converges at a rate of
roughly
1
ηT in T iterations. However, the update (1.4) is considered implicit because it typically
does not aﬀord closed form solutions, and hence can only be approximately implemented. Indeed,
as η →∞solving (1.4) can be shown to solve the original VI in g as well. This framework hence
presents a variety of tradeoﬀs an algorithm designer must consider.
1. How large should we choose the step size η to be able to solve the subproblems (1.4)? A large
value of η leads to a stronger convergence rate, but more diﬃcult subproblems to solve.
2. What optimization method should we use to implement solutions to each subproblem, and
how can we take advantage of structure (e.g. stochastic formulations) present in g?

CHAPTER 1. INTRODUCTION
10
3. In cases when (1.4) may be diﬃcult to directly implement, can we deﬁne an approximation to
the update (1.4) whose analysis is more readily analyzed?
In Chapter 2, we provide a novel ﬁne-grained analysis of the convergence rate of extragradi-
ent methods, a discretization strategy for simulating the subproblem (1.4) via ﬁxed-point iteration
[329, 415, 420] as suggested by Question 3. We then highlight the power of primal-dual formula-
tions of optimization problems (which are well-captured by our new convergence criteria, termed
"relative Lipschitzness"), and recover accelerated algorithms for smooth convex optimization and
ℓ∞regression using classical extragradient methods. Finally, we demonstrate the ﬂexibility of the
relative Lipschitzness framework by adapting it to yield state-of-the-art accelerated algorithms for
smooth ﬁnite-sum convex and separable minimax optimization problems.
In Chapter 3, in the spirit of Questions 1 and 2, we directly implement solvers for (1.4) for bilin-
ear minimax problems over geometric domains (such as Euclidean balls and probability simplices)
whose convergence rates are parameterized by η. Importantly, the resulting solvers obtain sublinear
runtimes after a linear amount of preprocessing, and are based on a variance-reduced scheme for
stochastic minimax optimization. By tuning the parameter η, the overall proximal point method
yields improved rates for regression, classiﬁcation, and equilbrium computation. We couple this
new variance reduction framework for minimax optimization with gradient estimators based on sub-
sampling coordinates of the constraint matrix, which results in signiﬁcantly-improved runtimes for
numerically sparse bilinear minimax problems.
1.1.1
Chapter 2: Acceleration via Primal-Dual Extragradient Methods
Extragradient methods are algorithms which approximately implement the update rule (1.4) by
performing a linearized ﬁxed-point iteration:
z′
t ←argminz∈Z

⟨ηg(zt), z −zt⟩+ V r
zt(z)
	
,
zt+1 ←argminz∈Z

⟨ηg(z′
t), z −zt⟩+ V r
zt(z)
	
.
(1.5)
For suﬃciently simple (e.g. coordinatewise separable) regularizers r, the updates (1.5) are cheaply
implementable. Classical analyses by [415, 420] show that when g is Lipschitz in a norm ∥·∥and r is
strongly convex in the same norm, the approximation (1.5) converges at a
1
ηT rate for any η ≤L−1.
A direct application of the [415, 420] result to the case of smooth, strongly convex optimization
(minimization of f where ∇f is L-Lipschitz and f is µ-strongly convex) yields a convergence rate
of eO( L
µ ) gradient queries to a high-accuracy optimizer. Mysteriously, this convergence rate is not
optimal: Nesterov's famous accelerated gradient descent algorithm [417] attains a convergence rate
of eO(
q
L
µ ) for the same problem, which is the tight rate up to constants [418].
In Chapter 2, we ﬁrst show that applying the rule (1.5) to smooth, strongly convex optimization
does indeed yield the optimal rate of eO(
q
L
µ ), when one deﬁnes g as the gradient operator resulting

CHAPTER 1. INTRODUCTION
11
from a primal-dual minimax formulation of the optimization problem:
min
x∈X max
y∈X ∗
µ
2 ∥x∥2
2 + ⟨y, x⟩−˜f ∗(y),
(1.6)
where ˜f ∗is the convex conjugate of ˜f := f −µ
2 ∥·∥2
2. We give a tighter convergence guarantee for
the updates (1.5) which goes beyond the standard Lipschitz g, strongly convex r setup considered
in [415, 420]. We then show that running extragradient methods on the formulation (1.6) phrased
as a variational inequality obtains an accelerated optimization rate through our tighter analysis.
Beyond recovering known results, we consider two families of modern optimization problems:
separable minimax problems of the form
min
x∈X max
y∈Y f(x) + h(x, y) −g(y) where f, g are convex and h is convex-concave,
(1.7)
and ﬁnite-sum convex optimization problems of the form
min
x∈X
1
n
X
i∈[n]
fi(x).
(1.8)
For each of (1.7) and (1.8), we design appropriate primal-dual formulations, and adaptations of
(stochastic, in the latter case) extragradient methods which achieve accelerated rates of optimization.
In the case of (1.7), we obtain the ﬁrst algorithm to match a lower bound of [545] under smoothness
and strong convexity. In the case of (1.8), we use our primal-dual formulation of (1.8) to design an
accelerated method which improves upon the prior best rate obtained in [17] by up to a √n factor.
1.1.2
Chapter 3: Stochastic Methods for Matrix Games
The theory of variance reduction for convex minimization has resulted in various improved algorithms
for stochastic optimization problems [477, 472, 17]. The principle behind variance reduction is to
use a small number of (more-expensive) full gradient computations as preprocessing to improve
the quality of later stochastic gradient estimates. In particular, letting e∇f be a (potentially poor)
unbiased estimator for ∇f, the idea is to reduce its variance by using the estimate
e∇f(x) −e∇f(¯x) + ∇f(¯x),
which is similarly unbiased, but whose variance improves the closer ¯x is to a minimizer of f.
However, for minimax problems, a similar theory of variance reduction was largely unexplored
(except in the Euclidean regression setting: see [432]). This lack of development was an impediment,
in some cases, to further improvements to the complexity of stochastic minimax optimization. To
further the theory of minimax variance reduction, we consider in Chapter 3 the basic setting of

CHAPTER 1. INTRODUCTION
12
bilinear saddle point problems:
min
x∈X max
y∈Y y⊤Ax −b⊤y + c⊤x,
(1.9)
for some A ∈Rm×n and geometrically-constrained domains X, Y. For example, in an ℓ1-ℓ1 "zero-
sum game," X = ∆n and Y = ∆m are probability simplices with bounded ℓ1 norm, which represent
mixed strategies (we additionally consider analogous ℓ2-ℓ1 games, and ℓ2-ℓ2 games, which encapsulate
linear classiﬁcation and programming, and obtain similar improvements, but omit discussion here).
As a starting point, we develop a way of combining ideas from the theory of variance reduction
with the tradeoﬀs aﬀorded by proximal point methods (1.4), to obtain new algorithms with improved
complexities for zero-sum games. Roughly speaking, when A is entrywise bounded, prior algorithms
by [415] and [253] respectively obtained runtimes of
eO
nnz(A)
ϵ

and eO
m + n
ϵ2

for computing an ϵ-approximate equilbrium to (1.9), where A has nnz(A) nonzero entries.
We
instead show that we can solve the subproblem (1.4) at a rate of eO((m + n)η2), after a nnz(A)-time
preprocessing step, for any η < ϵ−1. Altogether, choosing the best η yields a rate of
eO
 nnz(A) + (m + n)η2
· 1
ηϵ

=⇒eO
 
nnz(A) +
p
nnz(A)(m + n)
ϵ
!
,
which generically improves upon [415] and improves [253] for moderately small ϵ. We give analogous
speedups for nonlinear stochastic minimax problems under a gradient oracle model.
By designing new stochastic gradient estimators based on subsampling coordinates, we show how
to further improve upon this rate for numerically sparse A, namely dense matrices with few large
entries. Intuitively, an estimator which samples coordinates avoids density issues by targeting small
entries of A less frequently, and hence presents a ﬁner-grained, more continuous characterization of
A's inherent sparsity. For example, when every row and column of A has Euclidean norm eO(1), we
obtain an improved runtime of
eO

(nnz(A) + η2) · 1
ηϵ

=⇒eO
 
nnz(A) +
p
nnz(A)
ϵ
!
.
Direct uses of our stochastic methods for matrix games imply similar speedups for linear classiﬁca-
tion, linear regression, and various problems in computational geometry.

CHAPTER 1. INTRODUCTION
13
1.2
Results: Combinatorial Optimization
In Chapters 4, 5, and 6 we draw inspiration from our developments on stochastic VI theory, and
obtain new algorithms for approximately solving constrained ℓ1 and ℓ∞regression. The resulting
methods have immediate implications for a variety of combinatorial optimization problems, which
are reducible to regression. As an example, consider the box-constrained ℓ∞-regression problem
min
x∈[−1,1]m ∥Ax −b∥∞+ c⊤x,
(1.10)
where A ∈Rn×m has rows with bounded ℓ1 norms.
Solving (1.10) at an accelerated rate, i.e.
computing an ϵ-approximate minimizer within a runtime scaling linearly or sublinearly in nnz(A)
but depending only linearly on ϵ−1, was a notorious open problem in the continuous optimization
community (it is straightforward to obtain a rate of nnz(A) · ϵ−2 using unaccelerated gradient
methods). One concrete reason for this is lower bounds on the size of ℓ∞-strongly convex regularizers
over a box [483, 486], which typically govern the convergence rate of accelerated methods.
In a breakthrough work, Sherman in [483] showed how to formulate (1.10) as a "box-simplex
game"
min
x∈[−1,1]m max
y∈∆n y⊤Ax −b⊤y + c⊤x
(1.11)
and design a somewhat ad hoc accelerated algorithm for solving (1.11) via a primal-dual extragradi-
ent method utilizing a coupled regularizer. Intuitively, Sherman's regularizer uses the dual variable
y to perform a careful reweighting of a primal regularizer on x, bypassing the size lower bound.
To further our understanding of these local reweighting strategies and their implications for
accelerating structured regression, we show in Chapter 2 how to construct a simpler primal-dual
regularizer which retains Sherman's rate for solving (1.10), (1.11). Moreover, we give a ﬂexible anal-
ysis strategy for our regression algorithm through the framework of a strengthening of our relative
Lipschitzness criterion for extragradient algorithms, which we term local relative Lipschitzness.
In Chapter 4, we propose alternatives to Sherman's regularization strategy for solving (1.10)
based on reweighted accelerated coordinate methods. The resulting runtimes improve upon [483] in
many structured instances, and we show how to apply them to obtain faster approximate undirected
maximum ﬂow algorithms by using advances in combinatorial preconditioning [482, 318, 441].
In Chapter 5, we show that by recasting graph-structured problems such as bipartite match-
ing, optimal transport, and transshipment as box-simplex games, we can obtain improved parallel
complexities for approximately solving each by building upon our solver from Chapter 2. Moreover,
we demonstrate that the iterates of our solver admit recursions which are amenable to implemen-
tation in low space in a semi-streaming model of access to the graph. As a result, we improve the
state-of-the-art pass and/or space complexities for all of these problems as well.
In Chapter 6, we give a generic reduction framework from a decremental variant of bipartite

CHAPTER 1. INTRODUCTION
14
matching to solving a sequence of appropriately regularized box-simplex games, i.e. (1.11) with addi-
tional (small) regularization terms which encourage the simplex variable y to be "uniformly spread."
We complete this picture by providing new accelerated solvers for said regularized games, via our
relative Lipschitzness framework from Chapter 2.
1.2.1
Chapter 4: Faster Approximate ℓ∞Regression and Maximum Flow
The (undirected, capacitated) maximum ﬂow problem on a graph G = (V, E, w) (where u ∈RE
≥0 are
edge capacities) asks to ﬁnd a ﬂow f ∈RE which satisﬁes entrywise capacity constraints |f| ≤u,
and routes as many units as possible from a source s to a sink t. Letting B ∈{−1, 0, 1}E×V be the
(signed) edge-vertex incidence matrix of G, and letting d ∈RV be a 2-sparse demand vector with
ds = −1, dt = 1, it is straightforward to see that solving the "minimum congestion ﬂow" problem
min
B⊤f=d
diag (u)−1 f

∞
recovers the maximum ﬂow. In [482, 318, 441] it was shown that an appropriate "soft relaxation" of
the hard constraints B⊤f = d can be used without loss in approximation quality: it suﬃces to solve a
box-constrained instance (1.10) instead. Intuitively, the box constraint on x := diag (u)−1 f follows
from a reparameterization and binary search on the value of ∥x∥∞, and A is B after left-multiplying
by a combinatorial preconditioner, and has bounded row norms. The preconditioner is obtained by
a routing scheme from G onto a smaller graph. Our runtime improvements for maximum ﬂow follow
by designing optimization procedures which capture the structure of the problem more tightly. In
particular, our methods directly leverage sparsity properties of the preconditioner.
We begin by developing a faster algorithm for solving (1.10) via an accelerated coordinate descent
algorithm with dynamic sampling probabilities, providing an alternative to [483].
The resulting
algorithm yields a runtime of
eO

m +
√mn
ϵ

for solving a maximum ﬂow instance with n vertices and m edges, to 1 −ϵ multiplicative accuracy,
generically improving upon the eO(mϵ−1) runtime of [483] for slightly-dense graphs. We further build
upon a synthesis of stochastic VI acceleration methodology from Chapter 2, and data structures for
supporting coordinate updates from Chapter 3, to obtain an improved maximum ﬂow algorithm.
When the optimal x⋆, the solution to (1.10), has ∥x⋆∥2
2 = s, our improvement runs in time
eO

m + n + √ns
ϵ

.
Since s ≤m generically, this is never worse than our previous runtime (up to logarithmic factors),
and represents an improvement for sparse maximum ﬂows.

CHAPTER 1. INTRODUCTION
15
1.2.2
Chapter 5: Semi-Streaming Combinatorial Optimization
We demonstrate that various combinatorial optimization problems can be eﬃciently reduced to
box-simplex games, following similar "soft penalization schemes" as were used in prior works on
maximum ﬂow. For example, consider the problem of maximum cardinality bipartite matching on
an unweighted bipartite graph G = (V, E), where we wish to ﬁnd a ﬂow vector f ∈RE
≥0 which solves
max
B⊤f≤1V
∥f∥1 .
(1.12)
Here, we overload notation from the prior section and let B ∈{0, 1}E×V be the (unsigned) incidence
matrix, and 1V is the all-ones vector on V . The condition B⊤f ≤1V means there is no more than
one total unit of ﬂow on any vertex, and ∥f∥1 is the matching size. It is well-known that an optimal
f is integral, i.e. it is 0-1 valued, so this continuous relaxation is without loss.
By using the existence of combinatorial rounding schemes which greedily remove overﬂow (vio-
lations of the constraints B⊤f = 1) without substantially aﬀecting matching size, we show that the
matching problem (1.12) is captured by an ℓ1 regression problem
min
x∈∆E −⟨1E, x⟩+
max(B⊤x −1V , 0)

1
(1.13)
which penalizes constraint violations directly in the objective. A primal-dual formulation of (1.13)
can then be framed as a box-simplex game, and is hence amenable to the accelerated extragradient
solvers developed in [483] or Chapter 2 of this thesis. Similar observations for the optimal transport
and transshipment problems (e.g. shortest path) result in state-of-the-art parallel algorithms for
each, obtaining depth scaling as eO(ϵ−1) where ϵ governs approximation quality, and near-linear work.
Prior parallel complexities of near-linear work solvers for these problems scaled as eO(ϵ−2) [365, 358],
and were based on unaccelerated methods. Our results follow because extragradient methods rely
on only ﬁrst-order ("gradient") information about the problem, which can be implemented using
only eﬃciently parallelizable matrix-vector multiplications.
We next turn to solving these problems under a semi-streaming model of computation. In this
model, the matrix B is not stored explicitly due to space considerations, and can only be accessed
through read-only streams where edges are presented one at a time, termed passes over the graph.
This presents a potential obstacle to solving (1.13) in low space: the iterates are naturally |E|-
dimensional, and hence writing them is as expensive as storing the entire matrix B, which we were
trying to avoid! As our main result, we show our accelerated box-simplex game solver from Chapter 2
can entirely be implemented and deterministically sparsiﬁed in O(|V |) space for these combinatorial
applications, implying a eO(ϵ−1) pass complexity in a semi-streaming model and improving upon
prior works on semi-streaming bipartite matching (e.g. [12, 13]) in either space, passes, or both.

CHAPTER 1. INTRODUCTION
16
1.2.3
Chapter 6: Dynamic Decremental Bipartite Matching
In recent years (cf. Chapters 4 and 5), a variety of static combinatorial optimization problems have
beneﬁtted tremendously from continuous perspectives and iterative method machinery. However,
the development of dynamic graph algorithms, an important and widely-studied undertaking in the
theoretical computer science literature, has remained fairly separate from continuous optimization
developments. This begs the question: can we directly use faster solvers for continuous formulations
of combinatorial problems to obtain improved complexities in a dynamic setting?
As a proof-of-concept, in Chapter 6 we show how to obtain such a reduction for a natural
dynamic decremental model of bipartite matching (undergoing possibly adversarial edge deletions),
abbreviated DDBM. We prove that to maintain a fractional matching with value at least 1 −ϵ the
optimum, it suﬃces to solve eO(ϵ−2) regularized subproblems roughly of the form
min
x∈∆E −⟨1E, x⟩+
max(B⊤x −1V , 0)

1 + ϵ
X
e∈E
xe log xe
to high accuracy.
By directly plugging in high-precision accelerated solvers for regularized box-
simplex games inspired by our extragradient analysis framework in Chapter 2, we deterministically
solve the DDBM problem at an amoritized cost of eO(ϵ−3), over m := |E| edge deletions. This
improves upon the prior state-of-the-art of eO(ϵ−4) from [73] for the same problem.
Our framework is ﬂexible enough that it decouples the DDBM problem from the cost of im-
plementing subproblems, and hence is compatible with powerful techniques for solving variants of
regularized box-simplex games. By using a recent breakthrough in the maximum ﬂow literature
[124] as our subproblem solver, we obtain an amoritized cost of eO(mo(1)ϵ−2) over m edge deletions
for the DDBM problem, an improvement upon our earlier result for suﬃciently small ϵ.
1.3
Results: Structured Semideﬁnite Programming Solvers
In Chapter 7, we present nearly-linear time approximation algorithms for structured semideﬁnite
programs (1.2), several of which are directly used in our applications in Chapters 8 and 9. Our algo-
rithms are based on adaptations and generalizations of the matrix multiplicative weights framework,
which we brieﬂy overview here before stating our new results.
Matrix multiplicative weights (MMW) is an instantiation of regret minimization techniques over
a subset of the positive semideﬁnite matrix cone, denoted Sd
≥0 (where the matrices in question are
d × d). To give a typical example, let X := {X ∈Sd
≥0 | Tr(X) = 1} be the "spectraplex." Given a
sequence of operator-norm bounded "gain matrices" {Gt}t≥1, MMW produces a sequence of response
matrices {Xt}t≥1 ⊂X which aim to minimize the cumulative regret in linear measurements through

CHAPTER 1. INTRODUCTION
17
{Gt}t≥1 compared to the best action U ∈X in hindsight:
sup
U∈X

X
t∈[T ]
⟨Gt, U⟩

−

X
t∈[T ]
⟨Gt, Xt⟩

.
(1.14)
Crucially, the response matrices {Xt}t≥1 must be chosen in an online fashion, without knowledge
of future gain matrices Gs for s > t. When all gain matrices satisfy ∥Gt∥op ≤1, the matrix H¨older
inequality implies that the regret can never be more than T, the number of iterations. The key
guarantee of MMW is that it beats this trivial bound by obtaining vanishing average regret: it
plays a sequence of response matrices such that the total regret (1.14) scales as eO(
√
T). A standard
application of MMW, based on reducing (1.2) to feasibility problems which can be captured by
regret minimization over X, yields approximation algorithms for semideﬁnite programming: this
reduction is detailed in Section 7.2. However, this situation comes with a few caveats.
1. Although the update rule employed by MMW is simple to state (based on solving entropy-
regularized problems over X, which admit explicit characterizations), actually implementing
these updates exactly requires matrix eigendecomposition, which is an expensive operation.
Are there cheaper alternatives which do not sacriﬁce performance?
2. There have been relatively few explorations of instantiating MMW for subsets of Sd
≥0 beyond
the spectraplex deﬁned previously. Regret minimization over various other structured subsets
of interest may entail more complicated update rules, and the resulting methods necessitate
careful analyses when combined with faster, approximate implementations of said updates
bypassing explicit eigendecomposition. Can we develop approximation-tolerant MMW variants
over more complex sets?
3. The reduction of SDPs (1.14) to regret minimization over the spectraplex can be very lossy
from the perspective of problem parameterization; typically, one needs to pay an overhead
depending on the problem width, such as the size of constraints {Ai}i∈[n] or the optimizer X⋆.
For structured SDPs, can we design more ﬁne-grained reductions which bypass this lossiness via
stronger guarantees (e.g. multiplicative value approximations with no dependence on width)?
In Chapter 7, we provide new variants of the MMW framework which make progress towards
addressing each of these questions.
1.3.1
Chapter 7: Matrix Multiplicative Weights and Friends
We summarize the performance guarantees of our new variants of the MMW framework.

CHAPTER 1. INTRODUCTION
18
A rank-1 sketch for MMW. Standard MMW iterates can be expressed in closed form as Xt =
exp(Yt)
Tr exp(Yt), for a matrix Yt eﬃciently computable from the sequence of {Gs}s≤t. However, computing
Xt explicitly can be prohibitively expensive for applications. Towards addressing Question 1, one
cheaper way to approximate Xt is to let Q ∈Rk×d be a Johnson-Lindenstrauss (approximately
norm-preserving) sketch matrix, and instead compute
eXt := exp( 1
2Yt)Q⊤Q exp( 1
2Yt)
⟨exp(Yt), Q⊤Q⟩
.
Both the numerator and denominator of eXt can be eﬃciently approximated using the Lanczos
method [339] and polynomial approximations to the exponential. However, black-box applications of
Johnson-Lindenstrauss constructions require choosing k ≈log d·ϵ−2 to guarantee an ϵ-approximation
to inner products. We show that, perhaps surprisingly, when each gain matrix Gt is chosen obliv-
iously to randomness used in computing the current iterate Xt, a rank-1 sketch matrix suﬃces to
retain standard MMW guarantees up to constant factors (letting Q = q⊤for a uniformly random
unit vector q). As a result of our simple sketch, we improve upon prior state-of-the-art runtimes for
regret minimization over the spectraplex by factors of Ω(log5 d) [20].
Ky Fan MMW. Let k ∈[d]. Towards addressing Question 2, we develop new regret minimization
procedures over the "k-Fantope,"
Fk := {X ∈Sd
≥0 | Tr(X) = k, X ⪯I},
with iteration complexity dominated by an approximate k-PCA procedure, implementable in eO(k)
matrix-vector multiplications through gain matrices. The set Fk is important in applications because
it is the set of dual certiﬁcates against the Ky Fan norm ∥G∥k, deﬁned to be the sum of the k largest
eigenvalues (in magnitude) of G: namely, for G ∈Sd
≥0, ∥G∥k = supX∈Fk ⟨G, X⟩. When k = 1, our
method recovers the standard MMW guarantee.
A key diﬃculty in designing a regret minimization procedure over Fk is the relatively complicated
nature of entropic projections onto Fk, which involves thresholding eigenvalues. As a side result
required by our analysis, we give new performance guarantees on approximate k-PCA procedures
such as simultaneous power iteration for approximating the spectrum of a matrix, reﬁning results
from [404, 136]. By opening up the interplay between our MMW variant and approximate spectrum
computation, we obtain end-to-end approximation-tolerant algorithms for regret minimization over
Fk, and improve prior bounds in [136] for the same problem by factors of Ω(k3).
Structured positive SDP. We consider variants of (1.2) with monotonicity structure, also referred to
in the literature as mixed packing-covering (MPC) SDPs. The feasibility variant of MPC SDPs, in

CHAPTER 1. INTRODUCTION
19
full generality, asks to determine whether there exists w ∈Rn
≥0 such that
X
i∈[n]
wiPi ⪯
X
i∈[n]
wiCi, for {Pi}i∈[n] , {Ci}i∈[n] ⊂Sd
≥0.
(1.15)
We can think of {Pi}i∈[n] as a set of matrices we wish to "pack" in a weighted sense, such that they
are "covered" by the set of {Ci}i∈[n]. The monotone linear programming specialization of (1.15),
where all of {Pi}i∈[n], {Ci}i∈[n] are diagonal matrices, admit approximation algorithms with very
strong guarantees: they can test feasibility of (1.15) up to a 1 + ϵ multiplicative factor, depending
polynomially on ϵ−1 and only polylogarithmically on width parameters of the problem. One com-
pelling answer to Question 3 would thus be designing similar "width-independent" approximation
algorithms for testing (1.15). However, a solution to this problem in full generality has resisted
eﬀorts from the theoretical computer science and continuous optimization communities, including a
recent attempt by the author of this thesis [286].
Towards developing general solvers for (1.15), we give improved algorithms for various special-
izations of MPC SDPs in Chapter 7. We ﬁrst consider the case when each Ci is a κ multiple of
Pi for some κ > 1, which we term a "matrix dictionary recovery SDP." This specialization has
immediate implications to structured preconditioning tasks (detailed in Section 9.2), and intuitively
asks to approximate I with a linear combination of our matrix dictionary of {Pi}i∈[n]. We give a
nearly-linear time algorithm for solving matrix dictionary SDPs with iteration complexity scaling
only linearly in κ, the natural width parameter of the problem. We also design methods which
generalize this result to approximating an arbitrary constraint matrix B ∈Sd
≥0 using a speciﬁed
matrix dictionary which admits eﬃcient linear system solvers, e.g. graph Laplacians [491].
Finally, in the special case of (1.15) where all the Ci are scalars (also known as "pure pack-
ing SDPs"), we give an improved solver whose runtime roughly scales as ϵ−5, improving the ϵ−6
dependency of [19, 442]. We obtain our improvement by interpreting width-independent solvers
for pure packing SDPs through the lens of mirror descent, and show that this viewpoint extends
straightforwardly to solve "Schatten norm packing SDPs" of the form
max ∥w∥1 subject to

X
i∈[n]
wiPi

p
≤1
at a width-independent rate, where ∥·∥p of a matrix argument is the Schatten-p norm. Our result is
the ﬁrst width-independent, nearly-linear time solver for Schatten norm packing SDPs.

CHAPTER 1. INTRODUCTION
20
1.4
Results: Algorithmic High-Dimensional Statistics
1.4.1
Chapter 8: High-Dimensional Robust Statistics in Almost-Linear
Time
We design algorithms for three types of high-dimensional statistical estimation problems under
dataset contamination models: clustering, generalized linear regression, and principal component
analysis (PCA). Our contamination models are variants of the following: we observe a dataset
{Xi}i∈[n] such that an α ∈(0, 1) fraction is independently drawn from a "ground truth" D supported
on Rd, and the remainder of the data is arbitrary (potentially adversarially chosen) points in Rd.
Several algorithms in Chapter 8 are variants of ﬁltering. Instantiations of the ﬁltering paradigm in
algorithmic robust statistics (originally introduced in [177, 360, 492]) combine two ideas: certiﬁable
corruption and iterative dataset sanitization. The ﬁrst idea asks to produce a (problem-speciﬁc)
example of a certiﬁcate of corruption: in the absence of the certiﬁcate, a na¨ıve solution (e.g. an
empirical mean) should suﬃce for the estimation problem. Often, the proof of correctness of this
certiﬁcate comes with a contrapositive which states that whenever it is present, it must be because
of dataset contamination, and can hence be used as a "spectral signature" [508, 360] to deﬁne scores
for each data point which are ampliﬁed on the adversarial points. For example, when performing
mean estimation and the ground truth D is assumed to have a bounded covariance, an appropriate
certiﬁcate is an ampliﬁed-variance direction v and the corresponding scores are correlations with v
(relative to the empirical mean). These scores can then be used to iteratively sanitize the dataset,
e.g. by downweighting a set of soft "inlier" scores being maintained over the data points.
Once an appropriate certiﬁcate is identiﬁed from statistical properties of the problem, basic
instances of ﬁltering strategies frequently suﬃce for obtaining polynomial-time algorithms for robust
statistics. However, a much more challenging endeavor is developing robust estimators which both
attain optimal statistical guarantees, and run in nearly-optimal time. Designing such a "best-of-
both-words" estimator requires satisfactory answers to the following questions.
1. How can we compute scores through a certiﬁcate of corruption in nearly-linear time?
2. How can design a scheme which synthesizes iterations downweighting based on corruption
certiﬁcates, and provably terminates in a near-constant number of iterations?
To answer these questions in the setting of robust mean estimation, [194] used an approximate
top eigenvector computation to answer Question 1. Moreover, they developed a way of using the
guarantees of regret minimization procedures over the spectraplex to ensure fast termination of the
method. More speciﬁcally, [194] used the response matrices of MMW as their certiﬁcates of corrup-
tion, and used MMW regret guarantees to repeatedly decrease the operator norm of an empirical
covariance matrix by a constant factor. Chapter 8 builds upon the program initiated by [194], and
gives cheaply-computable robust estimators for multiple tasks beyond mean estimation.

CHAPTER 1. INTRODUCTION
21
Clustering heavy-tailed mixture models. Given a dataset of size n drawn from a mixture model D :=
P
i∈[k]
1
kDi, where each mixture component Di is Gaussian and has identity-bounded covariance, we
wish to recover which data point was drawn from which component to arbitrary constant precision
(e.g. correctly classifying a .95 fraction of points). This problem is information-theoretically tractable
assuming a suﬃcient separation condition between component means {µi}i∈[k], and there exist
eﬃcient algorithms for solving it dominated by eO(1) approximate k-PCA computations as long
as mini̸=j∈[k] ∥µi −µj∥2 = Ω(k−1
2 ) [520, 7]. However, these algorithms break down in the face of
contamination, do not generalize to heavy-tailed (non-Gaussian) data, and do not run in linear time
(incurring a k-factor overhead).
We present a new clustering algorithm which bypasses all of these barriers assuming a similar
separation level. In particular, we provide an almost-linear time algorithm (with runtime O(n1+ϵ0d)
for any constant ϵ0) for the following problem, termed list-decodable mean estimation.
We are
given a dataset {Xi}i∈[n], an α ∈(0, 1
2) fraction of which is drawn from D (with identity-bounded
covariance), and the remainder of which are arbitrary in Rd. We wish to produce a list of length
O(α−1) containing at least one element at distance eO(−α−1
2 ) from the mean of D. By letting D
vary through mixture components, and using the list-decoding procedure as a preprocessing step
in a separated mixture model estimation problem, we obtain an almost-linear runtime for robustly
clustering heavy-tailed mixture models as well. Even from just a runtime perspective (i.e. assuming
uncorrupted Gaussian components), this is the ﬁrst improvement upon [520, 7].
Our starting point is a simple, polynomial-time reduction from list-decodable mean estimation to
approximate k-PCA subroutines, which we use to deﬁne certiﬁcates of corruption. We then combine
this reduction with a Ky Fan norm SDP solver we develop in Chapter 7 to solve the general list-
decodable mean estimation problem in eO(ndk) time. Finally, to go beyond this k-PCA computation
runtime barrier, we design an O(n1+ϵ0d)-time algorithm for solving list-decodable mean estimation
using "split-or-cluster" steps based on solely one-dimensional projections.
This ﬁnal method is
inspired by both ﬁltering based on MMW responses, and the multiﬁlter algorithm of [179].
In
particular, the algorithm maintains a partial clustering over the dataset which is iteratively reﬁned,
where we bound the total number of points across all clusters.
Parameter estimation in generalized linear models. Consider the following parameter estimation
problem: we observe independent {(Xi, yi)}i∈[n] ∼D supported on Rd ×R, and given a link function
γ : R2 × R, we wish to estimate
θ⋆:= argminθ∈RdE(X,y)D [γ(⟨θ, X⟩, y)] .
This regression problem arises as a maximum likelihood estimation problem in exponential families.
When γ is regular (i.e. convex, and Lipschitz or smooth), and D has appropriately bounded moments,
the empirical risk minimizer attains good performance. We consider a robust variant of this problem

CHAPTER 1. INTRODUCTION
22
where an ϵ fraction of our dataset is contaminated.
By combining a gradient ﬁltering scheme based on semideﬁnite programming, and a new noise-
tolerant accelerated gradient method we develop, we solve the Lipschitz link function variant of the
robust regression problem to √ϵ Euclidean norm accuracy (up to a scaling by problem regularity
parameters) in time eO(nd√κ). Here, κ is a conditioning parameter of the regression problem. This
is a generic improvement upon the "robust gradient descent" method of [447] which attained runtime
eO(ndκ), and applies to both Lipschitz, smooth link functions such as the logistic loss, as well as the
Moreau envelope approximations to solely Lipschitz link functions such as the hinge loss.
We further specialize to the case of robust linear regression, when the X-marginal of D is 2-to-4
hypercontractive (has fourth moments relatively bounded by second moments). Here, we obtain both
an accelerated nearly-linear time algorithm, and another nearly-linear time algorithm which obtains
an improved estimation rate, albeit at a quadratically larger sample complexity.
Our algorithm
estimates at a Euclidean error rate scaling as √κϵ (where κ is the conditoning of the X-marginal
covariance matrix).
This can be seen as an interpolation between (accelerated) robust gradient
methods, which estimate at rate κ√ϵ, and sum-of-squares methods [53], which attain a rate ϵ
3
4 but
require exactly solving a large SDP and impose slightly stronger problem regularity assumptions.
Sub-Gaussian principal component analysis.
Given an ϵ-corrupted set of n samples from a d-
dimensional, mean-zero sub-Gaussian distribution, return a (1 −δ)-approximate top eigenvector
of the covariance matrix. We give a simple ﬁltering-based polynomial-time algorithm which solves
this problem for δ = O(ϵ log ϵ−1), which is essentially optimal. Moreover, we give an application of
a Schatten-p norm packing SDP we develop in Chapter 7 to the robust PCA problem. The result is
a near-linear time algorithm for mildly-"gapped" covariances, achieving δ = O(
p
ϵ log ϵ−1 log d).
We ﬁnd these results interesting for two additional conceptual reasons.
First, they draw an
explicit connection between Schatten norm SDP objectives and robust PCA. We establish this
connection by noting that, while an adversary can elevate a single large direction (fooling operator
norm SDPs), it can only create a bounded number of such large directions, which a Schatten norm
objective can detect. Second, they are an example of covariance property estimation without using
the strong algebraic relationships aﬀorded by assuming exactly Gaussian samples or other strong
algebraic properties, which to our knowledge had remained open until our work.
1.4.2
Chapter 9: Semi-Random Linear Systems
Consider the problem of solving a consistent linear system Ax⋆= b, given (A, b) ∈Rn×d×Rn. When
A is full-rank and n ≥d, the system is overconstrained, and fast iterative methods (e.g. accelerated
gradient descent) converge in nearly-linear time if A⊤A is O(1)-multiplicatively close to a multiple
of the identity. Interestingly, by giving additional consistent linear measurements (augmenting A
with more rows), we can arbitrarily ruin the condition number of A, and correspondingly hinder the
performance of gradient-based iterative methods.

CHAPTER 1. INTRODUCTION
23
We also consider the underconstrained sparse recovery counterpart of this problem, where n < d.
To make the problem meaningful (i.e. for the solution to be uniquely determined), we must ensure
that there are no candidate solutions in the nullspace of A. This is typically formulated as follows: x⋆
is assumed to be s-sparse, and A is assumed to satisfy the O(s)-restricted nullspace property (RNP),
which means it has no O(s)-sparse vectors in its kernel.
The RNP is essentially the minimum
assumption on A required for the sparse recovery problem to be information-theoretically tractable.
When A is assumed to satisfy the stronger property of being O(1)-approximately norm preserving
on all O(s)-sparse vectors, it is said to satisfy the stronger restricted isometry property (RIP). Under
this stronger RIP assumption, there exist fast iterative methods which converge in nearly-linear time.
However, a similar state of aﬀairs (as in the overconstrained case) occurs when applying nearly-
linear time solvers to "augmented" RIP matrices: by adding new consistent measurements, the
problem does not become information-theoretically harder, but the convergence of fast iterative
methods which work well on fully-RIP matrices becomes poor. This latter phenomenon is in fact
reported in practical applications of fast iterative methods to sparse recovery problems [166, 280,
446], motivating the development of nearly-linear time algorithms tolerant to some amount of model
misspeciﬁcation.
We investigate both of these problems through the lens of a monotone semi-
random adversary. Concretely, in the overconstrained setting, we assume that there exists a diagonal
reweighting matrix W⋆= diag (w⋆) , w⋆∈Rn
≥0 such that A⊤W⋆A is well-conditioned. This is an
even weaker assumption on "planted structure" than the presence of a subset of well-conditioned
rows, since we may set w⋆to be a zero-one indicator vector.
In the underconstrained (sparse
recovery) setting, we similarly assume there exists a W⋆such that A⊤W⋆A satisﬁes RIP. In both
cases, we develop nearly-linear time solvers for the exact recovery problem (where we observe Ax⋆),
as well as variants of the noisy recovery problem (where we only observe Ax⋆+ ξ for some noise
vector ξ ∈Rn).
In the overconstrained case, our method is based on approximately solving a more general optimal
diagonal preconditioning problem, which asks to ﬁnd a nonnegative diagonal matrix W such that
the condition number of A⊤WA is as small as possible. To do so, we apply the matrix dictionary
recovery SDP solver from Chapter 7 to a dictionary consisting of outer products of A's rows. We
also solve a similar "outer preconditioning" problem from numerical linear algebra, by using the
diagonal basis dictionary and A⊤A as the constraint matrix.
In the setting of sparse recovery, there are known hardness results (i.e. NP-hardness of certi-
fying RIP [57]) which suggest it may be computationally diﬃcult to obtain a global reweighting
of A which is RIP. We sidestep this hardness result by designing a "locally reweighted" projected
gradient descent method based on ℓ1-constrained optimization. Our algorithm progresses whenever
we can compute a reweighting of g = A⊤∆satisfying certain decomposition properties which can
be easily computationally veriﬁed, where ∆= b −Ax are the residuals of a current iterate x. Such
a reweighting always exists, because when A is fully RIP, we prove such a decomposition always

CHAPTER 1. INTRODUCTION
24
succeeds, and hence we can let the weighting be
√
w∗. We rephrase the computation of each local
reweighting as a convex subproblem, which we solve eﬃciently using stochastic gradient methods.
1.4.3
Chapter 10: Towards an Oracle Complexity of Sampling
We deﬁne three variants of the meta-problem (1.3), each described by a density family parameterized
by a type of structure the negative log-likelihood (denoted f throughout this exposition) is assumed
to satisfy, and a type of oracle access to the density. In each case, we provide results which yield
a further understanding of the oracle complexity of sampling from said density family. We brieﬂy
remark that each family we consider has a counterpart in convex optimization theory and is hence
similarly motivated; further, each optimization counterpart has a well-established oracle complexity
theory, with nearly-matching upper and lower bounds.
Well-conditioned sampling.
We consider the setting where the function f is assumed to have a
condition number bounded by κ (i.e. ∇2f everywhere has all eigenvalues in [µ, L], and κ := L
µ ). We
assume gradient and value oracle access to f.
On the upper bound side, we prove new concentration bounds on the norm of the gradient,
∥∇f(x)∥2 where x ∼µ, for well-conditioned f. This allows us to design samplers attaining an oracle
query complexity of eO(κd) for sampling to high accuracy in total variation from an explicit warm
start, improving a line of recent results which had previously achieved eO(κd + κ1.5√
d) [210, 128].
On the lower bound side, we provide hardness results for sampling from well-conditioned log-
concave distributions using two popular frameworks: the Metropolis-adjusted Langevin algorithm
(MALA), and multi-step Metropolized Hamlitonian Monte Carlo (HMC). We show that when ini-
tialized at suitably-adversarial exponentially warm start, no parameterization of MALA can obtain
a better mixing rate than eΩ(κd), and give similar (slightly weaker) hardness results for HMC. Be-
cause it is currently unknown how to construct subexponentially warm starts, our result may be
interpreted positively as further motivation to study the construction of such high-quality starting
distributions, which would have further implications beyond speciﬁc algorithms e.g. MALA or HMC.
Composite logconcave sampling with a restricted Gaussian oracle. We consider the setting where
the function f is decomposable as f = h + g, where h has a condition number bound of κ and g is
"simple" in a precise sense, but possibly non-smooth. Even in very special cases of this composite
sampling problem where g was e.g. the indicator of a coordinatewise box in the standad basis,
designing nontrivial high-accuracy samplers3 was an outstanding open problem. Our deﬁnition of
simplicity of g assumes that we can query a "restricted Gaussian oracle" (RGO) O(η, v), which
produces a sample from the density ∝exp

−g(x) −1
2η ∥x −v∥2
2

.
3Any logconcave density can be sampled to high accuracy using polynomially many queries to a value oracle [373];
we use "nontrivial" to mean a runtime going beyond a black-box citation to this general result.

CHAPTER 1. INTRODUCTION
25
We give an algorithm which samples from µ to high accuracy in total variation using eO(κd) iterations,
each querying the RGO for g and the gradient oracle for h once.
The astute reader may note a similarity between the RGO deﬁnition and the proximal point
method described in (1.4). In fact, our composite sampler follows from combining two pieces. First,
we develop a basic sampler which achieves eO(κ2d) iteration complexity. Next, we give a generic
"proximal point method" for logconcave sampling based on the notion of RGO access, which we use
to improve the condition number dependence of our basic sampler.
Logconcave ﬁnite sums. We consider the setting where the function f is expressible as a ﬁnite sum:
f =
1
n
P
i∈[n] fi, and all fi are κ well-conditioned. We assume gradient oracle access to each fi.
Note that we can always simulate gradient oracle access to f via n gradient queries; however, by
subsampling gradients, it is possible that we can do even better than a naive "full gradient" method.
We ﬁrst give a basic method which achieves a gradient oracle query complexity of eO(n + κ2d).
We then boost this algorithm through our aforementioned proximal reduction framework, to obtain
an overall query complexity of eO(n + κ
√
nd + κd). Up to a eO(max(1, p n
d )) factor, this is essentially
the best one could hope for without an improvement in the n = 1 case. Finally, we show that the
n = 1 case of our ﬁnite sum sampler actually implies that well-conditioned densities are sampleable
in eO(κd) calls to only a value oracle, suggesting that we may not yet have a mature understanding
why gradient oracles are helpful in the well-conditioned setting.
1.5
Bibliography of Represented Material
Part I
• Chapter 2 of this thesis is based on the works "Relative Lipschitzness in Extragradient Methods
and a Direct Recipe for Acceleration" [147] (published in ITCS 2021) and "Sharper Rates for
Separable Minimax and Finite Sum Optimization via Primal-Dual Extragradient Methods"
[299] (published in COLT 2022), joint with Michael B. Cohen, Yujia Jin, and Aaron Sidford.
• Chapter 3 of this thesis is based on the works "Variance Reduction for Matrix Games" [111]
(published in NeurIPS 2019) and "Coordinate Methods for Matrix Games" [112] (published
in FOCS 2020), joint with Yair Carmon, Yujia Jin, and Aaron Sidford.
• Chapter 4 of this thesis is based on the work "Coordinate Methods for Accelerating ℓ∞Re-
gression and Faster Approximate Maximum Flow" [486] (published in FOCS 2018), joint with
Aaron Sidford.
• Chapter 5 of this thesis is based on the works "A Direct eO(1/ϵ) Iteration Parallel Algorithm
for Optimal Transport" [293] (published in NeurIPS 2019) and "Semi-Streaming Bipartite

CHAPTER 1. INTRODUCTION
26
Matching in Fewer Passes and Optimal Space" [39] (published in SODA 2022), joint with
Sepehr Assadi, Arun Jambulapati, Yujia Jin, and Aaron Sidford.
• Chapter 6 of this thesis is based on the work "Regularized Box-Simplex Games and Dynamic
Decremental Bipartite Matching" [284] (published in ICALP 2022), joint with Arun Jambula-
pati, Yujia Jin, and Aaron Sidford.
Part II
• Chapter 7 of this thesis is based on (parts of) the works "A Rank-1 Sketch for Matrix Mul-
tiplicative Weights" [108] (published in COLT 2019), "List-Decodable Mean Estimation in
Nearly-PCA Time" [180] (published in NeurIPS 2021), "Fast and Near-Optimal Diagonal Pre-
conditioning" [287] (preprint under submission), and "Robust Sub-Gaussian Principal Compo-
nent Analysis and Width-Independent Schatten Packing" [289] (published in NeurIPS 2020),
joint with Yair Carmon, Ilias Diakonikolas, John C. Duchi, Arun Jambulapati, Daniel M.
Kane, Daniel Kongsgaard, Jerry Li, Christopher Musco, and Aaron Sidford.
• Chapter 8 of this thesis is based on (parts of) the works "Clustering Mixture Models in Almost-
Linear Time via List-Decodable Mean Estimation" [181] (published in STOC 2022), "Robust
Regression Revisited: Acceleration and Improved Estimation Rates" (published in NeurIPS
2021), [180], and [289], joint with Ilias Diakonikolas, Arun Jambulapati, Daniel M. Kane,
Daniel Kongsgaard, Jerry Li, and Tselil Schramm.
• Chapter 9 of this thesis is based on (parts of) the works "Semi-Random Sparse Recovery in
Nearly-Linear Time" [319] (preprint under submission) and [287], joint with Arun Jambulapati,
Jonathan A. Kelner, Jerry Li, Allen Liu, Christopher Musco, and Aaron Sidford.
• Chapter 10 of this thesis is based on the works "Logsmooth Gradient Concentration and
Tighter Runtimes for Metropolized Hamiltonian Monte Carlo" [344] (published in COLT 2020),
"Structured Logconcave Sampling with a Restricted Gaussian Oracle" [346] (published in
COLT 2021), and "Lower Bounds on Metropolized Sampling Methods for Well-Conditioned
Distributions" [345] (published in NeurIPS 2021), joint with Yin Tat Lee and Ruoqi Shen.
Copyright notices
As per IEEE copyright requirements, we note excerpts from [486] and [112] are used, from works
which are respectively ©2018 IEEE and ©2020 IEEE. As per ACM copyright requirements, we
note excerpts from [181] are used, from a work which is ©2022 ACM. As per SIAM copyright
requirements, we note excerpts from [39] are used, from a work which is ©2022 SIAM.

CHAPTER 1. INTRODUCTION
27
Other
We acknowledge the author's other research collaborations throughout the Ph.D. which have taught
the author a variety of technical tools and perspectives and inﬂuenced his research style. A few of
these collaborations have resulted in the works "Learning Populations of Parameters" [503] (pub-
lished in NeurIPS 2017), "CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor
Decompositions" [504] (published in ICML 2018), "Acceleration with a Ball Optimization Ora-
cle" [109] (published in NeurIPS 2020), and "Improved Sampling-to-Counting Reductions in High-
Dimensional Expanders and Faster Parallel Determinantal Sampling" [26], joint with Nima Anari,
Callum Burgess, Yair Carmon, Arun Jambulapati, Qijia Jiang, Yujia Jin, Weihao Kong, Yin Tat
Lee, Aaron Sidford, Gregory Valiant, Thuy-Duong Vuong, Teng Zhang, and James Zou.

Part I
Stochastic Variational Inequalities
and Combinatorial Optimization
28

Chapter 2
Acceleration via Primal-Dual
Extragradient Methods
This chapter is based on [147, 299], with Michael B. Cohen, Yujia Jin, and Aaron Sidford.
2.1
Introduction
In this chapter, we study the classic extragradient algorithms of mirror prox [415] and dual ex-
trapolation [420] for solving variational inequalities (VIs) in monotone operators. This family of
problems includes convex optimization and ﬁnding the saddle point of a convex-concave game. Due
to applications of the latter to adversarial and robust training, extragradient methods have received
signiﬁcant recent attention in the machine learning community, see e.g. [121, 391, 277]. Further,
extragradient methods have been the subject of increasing study by the theoretical computer sci-
ence and optimization communities due to recent extragradient-based runtime improvements for
problems including maximum ﬂow [483] and zero-sum games [111, 112].
Given a Lipschitz monotone operator and a bounded strongly-convex regularizer, mirror prox
[415] and dual extrapolation [420] achieve O(T −1) regret for solving the associated VI after T iter-
ations. This rate is worst-case optimal when the Lipschitzness of the operator and strong convexity
of the regularizer are with respect to the Euclidean norm [431]. However, in certain structured
problems related to VIs, alternative analyses and algorithms can yield improved rates. For instance,
when minimizing a smooth convex function (i.e. one with a Lipschitz gradient), it is known that
accelerated rates of O(T −2) are attainable, improving upon the standard O(T −1) extragradient
rate for the naive associated VI (see Appendix A.2). Further, algorithms inspired by extragradient
methods have been developed recovering the O(T −2) rate [192, 530].
29

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
30
Additionally, alternative analyses of extragradient methods, such as optimism [452] and area con-
vexity [483] have shown that under assumptions beyond a Lipschitz operator and a strongly convex
regularizer, improved rates can be achieved. These works leveraged modiﬁed algorithms which run
eﬃciently under such non-standard assumptions. Further, the area convexity-based methods of [483]
have had a number of implications, including faster algorithms for ℓ∞regression, maximum ﬂow and
multicommodity ﬂow [483] as well as improved parallel algorithms for work-eﬃcient positive linear
programming [91] and optimal transport [293].
In this chapter we seek a better understanding of these structured problems and the somewhat
disparate-seeming analyses and algorithms for solving them. We ask, are the algorithmic changes
enabling these results necessary? Can standard mirror prox and dual extrapolation be leveraged to
obtain these results? Can we unify analyses for these problems, and further clarify the relationship
between acceleration, extragradient methods, and primal-dual methods?
Finally, can we use these
novel perspectives to broaden the optimization toolbox through new methods with improved properties?
2.1.1
Relative Lipschitzness in extragradient methods
Towards addressing these questions, we provide a general condition, which we term relative Lips-
chitzness (cf. Deﬁnition 1), and analyze the convergence of mirror prox and dual extrapolation for a
monotone relatively Lipschitz operator.1 This condition is derived directly from the standard anal-
ysis of the methods and is stated in terms of a straightforward relationship between the operator g
and the regularizer r which deﬁne the algorithm. Our condition is inspired by both area convexity
as well as the "relative smoothness" condition in convex optimization [64, 375], and can be thought
of as a generalization of the latter to variational inequalities (see Lemma 3). Further, through this
analysis we show that standard extragradient methods directly yield accelerated rates for smooth
minimization and recover the improved rates of [483] for box-constrained ℓ∞regression, making
progress on the questions outlined above. We also show our methods recover certain randomized
accelerated rates and have additional implications, outlined below.
Extragradient methods directly yield acceleration. In Section 2.4, we show that applying algo-
rithms of [415, 420] to a minimax formulation of minx∈Rd f(x), when f is smooth and strongly
convex, yields accelerated minimization rates when analyzed via relative Lipschitzness. Speciﬁcally,
we consider the following problem, termed the Fenchel game in [530]:
min
x∈Rd max
y∈Rd ⟨y, x⟩−f ∗(y),
(2.1)
and show that when f is µ-strongly convex and L-smooth, O(
p
L/µ) iterations of either mirror
prox [415] or dual extrapolation [420] produces an average iterate which halves the function error
1A somewhat similarly-named property appeared in [374], which also studied mirror descent algorithms under
relaxed conditions; however, their property ∥g(x)∥2
∗≤MVx(y)
∥y−x∥2 for all x, y, is diﬀerent than the one we propose.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
31
of f. By repeated application, this yields an accelerated linear rate of convergence and the optimal
O(T −2) rates for non-strongly convex, smooth function minimization by a reduction [551]. Crucially,
to attain this rate we give a sharpened bound on the relative Lipschitzness of the gradient operator
of (2.1) with respect to a primal-dual regularizer.
Our result advances a recent line of research, [3, 4, 192], on applying primal-dual analyses to
shed light on the mysterious nature of acceleration.
Speciﬁcally, [3, 4] show that the classical
algorithm of [417] can be rederived via applying primal-dual "optimistic" dynamics, inspired by the
framework of [452]. Further, [192] showed that an appropriate discretization of dynamics inspired
by extragradient algorithms yields an alternative accelerated algorithm. While these results clarify
the primal-dual nature of acceleration, additional tuning is ultimately required to obtain their ﬁnal
algorithms and analysis. We obtain acceleration as a direct application of known frameworks, i.e.
standard mirror prox and dual extrapolation, applied to the formulation (2.1), and hope this helps
demystify acceleration.
In Appendix A.5, we further show that analyzing extragradient methods tailored to strongly
monotone operators via relative Lipschitzness, and applying this more ﬁne-grained analysis to a
variant of the objective (2.1), also yields an accelerated linear rate of convergence. The resulting
proof strategy extends readily to accelerated minimization of smooth and strongly convex functions
in general norms, as we discuss at the end of Section 2.4, and we believe it may be of independent
interest.
Finally, we remark that there has been documented diﬃculty in accelerating the minimization of
relatively smooth functions [265]; this was also explored more formally by [197]. It is noted in [265],
as well as suggested in others (e.g. in the development of area convexity [483]) that this discrepancy
may be due to acceleration fundamentally requiring conditions on relationships between groups of
three points, rather than two. Our work, which presents an alternative three-point condition yielding
accelerated rates, sheds light on this phenomenon and we believe it is an interesting future direction
to explore the relationships between our condition and other alternatives in the literature which are
known to yield acceleration.
Area convexity for bilinear box-simplex games. In Section 2.5, we draw a connection between rela-
tive Lipschitzness and the notion of an "area convex" regularizer, proposed by [483]. Area convexity
is a property which weakens strong convexity, but is suitable for extragradient algorithms with a
linear operator. It was introduced in the context of solving a formulation of approximate undirected
maximum ﬂow via box-constrained ℓ∞regression, or more generally approximating bilinear games
between a box variable and a simplex variable. The algorithm of [483] applied to bilinear games
was a variant of standard extragradient methods and analyzed via area convexity, which was proven
via solving a subharmonic partial diﬀerential equation.
We show that mirror prox, as analyzed
by a local variant of relative Lipschitzness, yields the same rate of convergence as implied by area
convexity, for box-simplex games. Our proof of this rate is straightforward and based on a simple

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
32
Cauchy-Schwarz argument after demonstrating local stability of iterates.
Randomized extragradient methods via local variance reduction. In general, the use of stochas-
tic operator estimates in the design of extragradient algorithms for solving general VIs is not as
well-understood as their use in the special case of convex function minimization. The best-known
known stochastic methods for solving VIs [302] with bounded-variance stochastic estimators obtain
O(T −1/2) rates of convergence; this is by necessity, from known classical lower bounds on the rate of
the special case of stochastic convex optimization [414]. Towards advancing the randomized extra-
gradient toolkit, we ask: when can improved O(T −1) rates of convergence be achieved by stochastic
algorithms for solving speciﬁc VIs and ﬁne-grained bounds on estimator variance (i.e. more local
notions of variance)? This direction is inspired by analogous results in convex optimization, where
reduced-variance and accelerated rates have been obtained, matching and improving upon their
deterministic counterparts [300, 472, 18, 347, 425, 553].
For the special case of bilinear games, this question was recently addressed by the works [432, 111],
using proximal reductions to attain improved rates. In this work, we give a framework for direct
stochastic extragradient method design bypassing the variance bottleneck limiting prior algorithms to
a O(T −1/2) rate of convergence for problems with block-separable structure. We identify a particular
criterion of randomized operators used in the context of extragradient algorithms (cf. Proposition 2)
which enables O(T −1) rates of convergence. Our approach is a form of "local variance reduction",
where estimators in an iteration of the method share a random seed and we take expectations over
the whole iteration in the analysis. Our improved estimator design exploits the separable structure
of the problem; it would be interesting to design a more general variance reduction framework for
randomized extragradient methods.
Formally, we apply our local variance reduction framework in Section 2.6 to show that an instance
of our new randomized extragradient methods recover acceleration for coordinate-smooth functions,
matching the known tight rates of [553, 425]. Along the way, we give a variation of relative Lips-
chitzness capturing an analagous property between a locally variance-reduced randomized gradient
estimator and a regularizer, which we exploit to obtain our runtime. We note that a similar approach
was taken in [486] to obtain faster approximate maximum ﬂow algorithms in the bilinear minimax
setting; here, we generalize this strategy and give conditions under which our variance reduction
technique obtains improved rates for extragradient methods more broadly.
Additional contributions. A minor contribution of our work is that we show (Appendix A.3) that
relative Lipschitzness implies new rates for minimax convex-concave optimization, taking a step
towards closing the gap with lower bounds with ﬁne-grained dependence on problem parameters.
Under operator-norm bounds on blocks of the Hessian of a convex-concave function, as well as
blockwise strong convexity assumptions, [545] showed a lower bound on the convergence rate to
obtain an ϵ-approximate saddle point.
When the blockwise operator norms of the Hessian are
roughly equal, [366] gave an algorithm matching the lower bound up to a polylogarithmic factor,

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
33
using an alternating scheme repeatedly calling an accelerated proximal point reduction. Applying
our condition with a strongly monotone variant of the mirror prox algorithm of [415] yields a new ﬁne-
grained rate for minimax optimization, improving upon the runtime of [366] for a range of parameters.
Our algorithm is simple and the analysis follows directly from a tighter relative Lipschitzness bound;
we note the same result can also be obtained by considering an operator norm bound of the problem
after a rescaling of space, but we include this computation because it is a straightforward implication
of our condition. We also remark that these results serve as a baseline rate for smooth minimax
optimization, which is later sharpened by developments in Section 2.7.
Finally, in Appendix A.7, we discuss the relation of relative Lipschitzness to another framework
for analyzing extragradient methods: namely, we note that our proof of the suﬃciency of relative
Lipschitzness recovers known bounds for optimistic mirror descent [452].
2.1.2
Sharper rates for separable minimax and ﬁnite sum optimization
In the second part of this chapter, we further study several fundamental families of optimization
problems, namely (separable) minimax optimization, ﬁnite sum optimization, and minimax ﬁnite
sum optimization (which generalizes both), and provide new state-of-the-art rates for these problem
families using the framework developed in Section 2.1.1. These families have received widespread
recent attention from the optimization community due to their prevalence in modeling tasks arising
in modern data science. For example, minimax optimization has been used in both convex-concave
settings and beyond to model robustness to (possibly adversarial) noise in many training tasks
[378, 451, 244].
Moreover, ﬁnite sum optimization serves as a fundamental subroutine in many
of the empirical risk minimization tasks of machine learning today [92]. Nonetheless, and perhaps
surprisingly, there remain gaps in our understanding of the optimal rates for these problems. Toward
closing these gaps, we provide new accelerated algorithms improving upon the state-of-the-art for
each family of problems.
Our results build upon our earlier framework, in particular its application of primal-dual extra-
gradient methods to recover accelerated rates for smooth, convex optimization in Appendix A.5.
This application considers the problem2
min
x∈X f(x) + µ
2 ∥x∥2 for L-smooth and convex f,
(2.2)
and its equivalent primal-dual formulation as an appropriate "Fenchel game"
min
x∈X max
x∗∈X ∗
µ
2 ∥x∥2 + ⟨x∗, x⟩−f ∗(x∗), where f ∗is the convex conjugate of f .
(2.3)
As discussed earlier, our framework shows that applying extragradient methods [415, 420] and an-
alyzing them through a condition known as relative Lipschitzness recovers an accelerated gradient
2Throughout, X, Y are unconstrained, Euclidean spaces (see Section 2.2).

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
34
query complexity for computing (2.2), which was known to be optimal [418].
Both the Fenchel game [3, 530] and the relative Lipschitzness property (independently proposed
in [496]) have a longer history, discussed in Section 2.1.3. This chapter is particularly motivated by
their synthesis in the earlier sections of this chapter, which used these tools to provide a general
recipe for designing accelerated methods. This recipe consists of the following ingredients.
1. Choose a primal-dual formulation of an optimization problem and a regularizer, r.
2. Bound iteration costs, i.e. the cost of implementing mirror steps with respect to r.
3. Bound the relative Lipschitzness of the gradient operator of the problem with respect to r.
In our basic accelerated algorithm in Appendix A.5, this recipe was applied with (2.3) as the primal-
dual formulation and r(x, x∗) := µ
2 ∥x∥2 + f ∗(x∗). Further, it was shown that each iteration could
be implemented (implicitly) with O(1) gradient queries and that the gradient operator Φ of the
objective (2.3) is O(
p
L/µ)-relatively Lipschitz with respect to r. Combining these ingredients gave
the accelerated rate for (2.3).
We broaden this primal-dual extragradient approach and add new recipes to the optimization
cookbook. As a result, we obtain methods with improved rates for minimax optimization, ﬁnite
sum optimization, and minimax ﬁnite sum optimization. We follow similar approaches as our basic
recipe but change the ingredients with diﬀerent primal-dual formulations, regularizers, extragradient
methods, and analyses. In the following sections, we discuss each problem family, our new results
and approach, and situate them in the relevant literature.
Separable minimax optimization.
In Section 2.7, we study separable convex-concave minimax optimization problems of the form3
min
x∈X max
y∈Y Fmm(x, y) := f(x) + h(x, y) −g(y),
(2.4)
where f is Lx-smooth and µx-strongly convex, g is Ly-smooth and µy-strongly convex, and h is
convex-concave and twice-diﬀerentiable with
∇2
xxh
 ≤Λxx,
∇2
xyh
 ≤Λxy, and
∇2
yyh
 ≤Λyy.
Our goal is to compute a pair of points (x, y) with bounded duality gap with respect to Fmm:
GapFmm(x, y) ≤ϵ (see Section 2.2 for deﬁnitions).
The problem family (2.4) contains as a special case the following family of convex-concave min-
imax optimization problems with bilinear coupling (with Λxx = Λyy = 0 and Λxy = ∥A∥):
min
x∈X max
y∈Y f(x) +
 y⊤Ax −⟨b, y⟩+ ⟨c, x⟩

−g(y).
(2.5)
3Our results in Section 2.7 apply generally to non-twice diﬀerentiable, gradient Lipschitz h, but we use these
assumptions for simplicity in the introduction. All norms are Euclidean (see Section 2.2 for relevant deﬁnitions).

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
35
Problem (2.5) has been widely studied in the optimization literature, dating at least to the classic
work of [116], which used (2.5) to relax convex optimization with aﬃne constraints related to imaging
inverse problems. Problem (2.5) also encapsulates convex-concave quadratics and has been used to
model problems in reinforcement learning [201] and decentralized optimization [333].
Our results. We give the following result on solving (2.4).
Theorem 1 (informal, cf. Theorem 25). There is an algorithm that, given (x0, y0) ∈X ×Y satisfying
GapFmm(x0, y0) ≤ϵ0, returns (x, y) with GapFmm(x, y) ≤ϵ using T gradient evaluations to f, h, and
g for
T = O

κmm log
κmmϵ0
ϵ

, for κmm :=
s
Lx
µx +
s
Ly
µy + Λxx
µx +
Λxy
√µxµy + Λyy
µy .
In the special case of (2.5), Theorem 1 matches a lower bound of [545], which applies to the
family of quadratic minimax problems obeying our smoothness and strong convexity bounds. More
generally, Theorem 1 matches the lower bound whenever Λxx and Λyy are suﬃciently small compared
to the remaining parameters, improving prior state-of-the-art rates [533] in this regime.
By applying reductions based on explicit regularization used in [366], Theorem 1 also yields
analogous accelerated rates depending polynomially on the desired accuracy when we either f, g, or
both are not strongly convex. For conciseness, in this paper we focus on the strongly convex-concave
regime discussed previously in this section.
Our approach. Our algorithm for solving (2.4) is based on the simple observation that minimax
problems with the separable structure can be eﬀectively "decoupled" by using convex conjugation on
the components f and g. In particular, following a similar recipe as our smooth convex optimization
application in Appendix A.5, we rewrite (an appropriate regularized formulation of) the problem
(2.4) using convex conjugates as follows:
min
x∈X,y∗∈Y∗
max
y∈Y,x∗∈X ∗
µx
2 ∥x∥2 −µy
2 ∥y∥2 + ⟨x∗, x⟩−⟨y∗, y⟩+ h(x, y) −f ∗(x∗) + g∗(y∗).
Further, we deﬁne the regularizer r(x, y, x∗, y∗) := µx
2 ∥x∥2+ µy
2 ∥y∥2+f ∗(x∗)+g∗(y∗). Finally, we ap-
ply an extragradient method for strongly monotone operators to our problem, using this regularizer.
We demonstrate eﬃcient implementability, and analyze the relative Lipschitzness of the problem's
gradient operator with respect to r, yielding Theorem 1. In the ﬁnal gradient oracle complexity, our
method obtains the accelerated trade-oﬀbetween primal and dual blocks for µx
2 ∥x∥2+⟨x∗, x⟩−f ∗(x∗)
and its Y analog, for the separable parts f and g respectively. It also obtains an unaccelerated rate
for the h component, by bounding the relative Lipschitzness corresponding to h via our assumptions.
Prior work.
Many recent works obtaining improved rates for minimax optimization under
smoothness and strong convexity restrictions (including Appendix A.3 of this thesis) concentrate

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
36
on a more general family of problems of the form:
min
x∈X max
y∈Y F(x, y).
(2.6)
Typically, these works assume (for simplicity, assuming F is twice-diﬀerentiable), ∇2
xxF is bounded
between µxI and ΛxxI everywhere, ∇2
yyF is bounded between µyI and ΛyyI everywhere, and ∇2
xyF
is operator norm bounded by Λxy. It is straightforward to see that (2.6) contains (2.4) as a special
case, by setting f ←µx
2 ∥·∥2, g ←µy
2 ∥·∥2, and h ←F −f + g.
For (2.6), under gradient access to F, the works [366, 533], and Appendix A.3 presented diﬀerent
approaches yielding a variety of query complexities.
Letting Λmax := max (Λxx, Λxy, Λyy), these
complexities scaled respectively as4
eO


s
max (Λxx, Λxy, Λyy)2
µxµy

, eO
 s
Λxx
µx +
s
Λyy
µy +
s
ΛxyΛmax
µxµy
!
, eO
Λxx
µx + Λyy
µy +
Λxy
√µxµy

.
The state-of-the-art rate (ignoring logarithmic factors) is due to [533], which obtained the middle
gradient query complexity above.
Theorem 1 matches the rate obtained by Appendix A.3 and
improves upon [366] in some regimes. Notably, Theorem 1 never improves upon [533] in the general
regime, up to logarithmic factors. On the other hand, the method in Theorem 1 uses only a single
loop, as opposed to the multi-loop methods in [366, 533] which lose logarithmic factors.
Up to logarithmic factors, there is a gap between [533] and the lower bound of [545] only when
Λxy ≪Λmax. We close this gap for minimax problems admitting the separable structure (2.5). In the
special case of quadratic problems, prior work, [533], proposed a recursive approach which obtained
a rate comparable to that of Theorem 1, albeit larger by subpolynomial factors.
Concurrent work. A pair of independent and concurrent works [332, 501] obtained variants of
Theorem 1. Their results were stated under the restricted setting of bilinear coupling (2.5), but they
each provided alternative results under (diﬀerent) weakenings of our strong convexity assumptions.
The algorithm of [501] is closer to the one developed in this chapter (also going through a primal-
dual lifting), although the ultimate methods and analyses are somewhat diﬀerent.
Though our
results were obtained independently, our presentation was informed by a reading of [332, 501] for a
comparison.
Finite sum optimization
In Section 2.8, we study ﬁnite sum optimization problems of the form
min
x∈X Ffs(x) := 1
n
X
i∈[n]
fi(x),
(2.7)
4 e
O hides logarithmic factors throughout, see Section 2.2.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
37
where fi is Li-smooth for each i ∈[n], and 1
n
P
i∈[n] fi is µ-strongly convex. We focus on the strongly
convex regime; through generic reductions [551], our results yield accelerated rates depending poly-
nomially on the desired accuracy, without the strong convexity assumption.
Methods for solving (2.7) have garnered substantial interest because of their widespread appli-
cability to empirical risk minimization problems over a dataset of n points, which encapsulate a
variety of (generalized) regression problems in machine learning (see [92] and references therein).
Our results. We give the following result on solving (2.7).
Theorem 2 (informal, cf. Theorem 8, Corollary 4). There is an algorithm that, given x0 ∈X
satisfying Ffs(x0) −Ffs(x⋆) ≤ϵ0 where x⋆minimizes Ffs, returns x ∈X with EFfs(x) −Ffs(x⋆) ≤ϵ
using T gradient evaluations (each to some fi) for
T = O

κfs log
κfsϵ0
ϵ

, for κfs := n +
X
i∈[n]
√Li
√nµ.
Our approach. Our algorithm for solving (2.7) builds upon an accelerated coordinate descent
variant developed in Section 2.6, which developed an analysis of a randomized extragradient method
to do so. We consider an equivalent primal-dual formulation of (a regularized variant of) (2.7),
inspired by analogous developments in the ERM literature [477, 478]:
min
x∈X
max
{x∗
i }i∈[n]⊂X ∗
µ
2 ∥x∥2 + 1
n
X
i∈[n]
(⟨x∗
i , x⟩−f ∗
i (x∗
i )) .
Our algorithm then solves this regularized primal-dual game to high precision.
A key building block of our method is a randomized extragradient method which is compatible
with strongly monotone problems. To this end, we extend the way the randomized extragradient
method is applied in Section 2.6, which does not directly yield a high-precision guarantee.
We
proceed as follows: for roughly κfs iterations (deﬁned in Theorem 2) of our method, we run the non-
strongly monotone randomized mirror prox method of Section 2.6 to obtain a regret bound. We then
subsample a random iterate, which we show halves an appropriate potential in expectation via our
regret bound and strong monotonicity. We then recurse on this procedure to obtain a high-precision
solver.
Prior work. Developing accelerated algorithms for (2.7) under our regularity assumptions has
been the subject of a substantial amount of research eﬀort in the community, see e.g. [364, 227,
478, 17] and references therein. Previously, the state-of-the-art gradient query complexities (up to

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
38
logarithmic factors) for (2.7) were obtained by [364, 227, 17],5 and scaled as
eO

n +
sP
i∈[n] Li
µ

.
(2.8)
Rates such as (2.8), which scale as functions of P
i∈[n]
Li
µ , arise in known variance reduction-based
approaches [300, 168, 472, 17] due to their applications of a "dual strong convexity" lemma (e.g.
Theorem 1, [300] or Lemma 2.4, [17]) of the form
∥∇fi(x) −∇fi(¯x)∥2 ≤2Li (fi(¯x) −fi(x) −⟨∇fi(x), ¯x −x⟩) .
The analyses of e.g. [300, 17] sample i ∈[n] proportional to Li, allowing them to bound the variance
of a resulting gradient estimator by a quantity related to the divergence in Ffs.
The rate in (2.8) is known to be optimal in the uniform smoothness regime [539], but in a
more general setting its optimality is unclear. Theorem 2 shows that the rate can be improved for
suﬃciently non-uniform Li. In particular, Cauchy-Schwarz shows that the quantity κfs is never worse
than (2.8), and improves upon it by a factor asymptotically between 1 and √n when the {Li}i∈[n]
are non-uniform. Moreover, even in the uniform smoothness case, Theorem 2 matches the tightest
rate in [17] up to an additive log κfs term, as opposed to an additional multiplicative logarithmic
overhead incurred by the reduction-based approaches of [364, 227].
Our rate's improvement over (2.8) is comparable to a similar improvement that was achieved
previously in the literature on coordinate descent methods.
In particular, [347] ﬁrst obtained a
(generalized) partial derivative query complexity comparable to (2.8) under coordinate smoothness
bounds, which was later improved to a query complexity comparable to Theorem 2 by [553, 425].
Due to connections between coordinate-smooth optimization and empirical risk minimization (ERM)
previously noted in the literature [477, 478], it is natural to conjecture that the rate in Theorem 2
is achieveable for ﬁnite sums (2.7) as well. However, prior to our work (to our knowledge) this rate
was not known, except in special cases e.g. linear regression [10].
Our method is based on using a primal-dual formulation of (2.7) to design our gradient estimators.
It attains the rate of Theorem 8 by sampling summands proportional to √Li, trading oﬀprimal
and dual variances through a careful coupling. It can be viewed as a modiﬁed dual formulation
to the coordinate descent algorithm in Section 2.6, which used primal-dual couplings inspired by
the ﬁne-grained accelerated algorithms of [553, 425]. We believe our result sheds further light on
the duality between coordinate-smooth and ﬁnite sum optimization, and gives an interesting new
approach for algorithmically leveraging primal-dual formulations of ﬁnite sum problems.
5There have been a variety of additional works which have also attained accelerated rates for either the problem
(2.7) or its ERM specialization, see e.g. [167, 547, 338, 549]. However, to the best of our knowledge these do not
improve upon the state-of-the-art rate of [17] in our setting.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
39
Minimax ﬁnite sum optimization
In Section 2.9, we study a family of minimax ﬁnite sum optimization problems of the form
min
x∈X max
y∈Y Fmmfs(x, y) := 1
n
X
i∈[n]
(fi(x) + hi(x, y) −gi(y)) .
(2.9)
We assume fi is Lx
i-smooth, gi is Ly-smooth, and hi is convex-concave and twice-diﬀerentiable with
blockwise operator norm bounds Λxx
i , Λxy
i , and Λyy
i
for each i ∈[n]. We also assume the whole
problem is µx-strongly convex and µy-strongly concave.
We propose the family (2.9) because it encapsulates (2.6) and (2.7), and is amenable to techniques
from solving both. Moreover, (2.9) is a natural description of instances of (2.6) which arise from
primal-dual formulations of empirical risk minimization problems, e.g. [547, 529]. It also generalizes
natural minimax ﬁnite sum problems previously considered in e.g. [111].
Our results. We give the following result on solving (2.9).
Theorem 3 (informal, cf. Theorem 9). There is an algorithm that, given (x0, y0) ∈X ×Y satisfying
GapFmmfs(x0, y0) ≤ϵ0, returns (x, y) with EGapFmmfs(x, y) ≤ϵ using T gradient evaluations, each to
some fi, gi, or hi, where
T = O

κmmfs log (κmmfs) log
κmmfsϵ0
ϵ

,
for κmmfs := n +
1
√n
X
i∈[n]


s
Lx
i
µx +
s
Ly
i
µy + Λxx
i
µx +
Λxy
i
√µxµy + Λyy
i
µy

.
The rate in Theorem 3 captures (up to a logarithmic factor) both of the rates in Theorems 1
and 2, when (2.9) is appropriately specialized. It can be more generally motivated as follows. When
n is not the dominant term in Theorem 2's bound, the remaining term is √n times the average rate
attained by Nesterov's accelerated gradient method [417] on each summand in (2.7). This improves
upon the factor of n overhead which one might naively expect from computing full gradients. In
similar fashion, Theorem 3 attains a rate (up to an additive n, and logarithmic factors) which is √n
times the average rate attained by Theorem 1 on each summand in (2.9).
Our approach. Our algorithm for solving (2.9) is a natural synthesis of the earlier algorithms
suggested in Section 2.1.2. However, to obtain our results we apply additional techniques to by-
pass complications which arise from the interplay between the minimax method and the ﬁnite sum
method, inspired by [111]. In particular, to obtain our tightest rate we would like to subsample the
components in our gradient operator corresponding to {fi}i∈[n] , {gi}i∈[n] , {hi}i∈[n] all at diﬀerent
frequencies when applying the randomized extragradient method. These diﬀerent sampling distri-
butions introduce dependencies between iterates, and make our randomized estimators no longer
"unbiased" for the true gradient operator to directly incur the randomized extragradient analysis.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
40
To circumvent this diﬃculty, we obtain our result via a partial decoupling, treating components
corresponding to {fi}i∈[n], {gi}i∈[n] and those corresponding to {hi}i∈[n] separately. For the ﬁrst two
aforementioned components, which are separable and hence do not interact, we pattern an expected
relative Lipschitzness analysis for each block, similar to the ﬁnite sum optimization. For the remain-
ing component {hi}i∈[n], we develop a variance-reduced stochastic method which yields a relative
variance bound. We put these pieces together in Proposition 5, a new randomized extragradient
method analysis, to give a method with a convergence rate of roughly
n +
1
√n
X
i∈[n]


s
Lx
i
µx +
s
Ly
i
µy

+ (κh
mmfs)2, where κh
mmfs := 1
n
X
i∈[n]
Λxx
i
µx +
Λxy
i
√µxµy + Λyy
i
µy

.
The dependence on all pieces above is the same as in Theorem 3, except for the term corresponding
to the {hi}i∈[n].
To improve this dependence, we wrap our solver in an "outer loop" proximal
point method which solves a sequence of γ-regularized variants of (2.9). We obtain our ﬁnal claim
by trading oﬀthe terms n and (κh
mmfs)2 through our choice of γ, which yields the accelerated
convergence rate of Theorem 3.
Prior work. To our knowledge, there have been relatively few results for solving (2.9) under our
ﬁne-grained assumptions on problem regularity, although various stochastic minimax algorithms
have been developed in natural settings [302, 432, 277, 111, 121, 112, 16]. For the general problem of
solving minx∈X maxy∈Y 1
n
P
i∈[n] Fi(x, y) where Fi is Li-smooth and convex-concave, and the whole
problem is µx-strongly convex and µy-strongly concave, perhaps the most direct comparisons are
Section 5.4 of [111] and Theorem 15 of [507]. In particular, [111] provided a high-precision solver
using roughly
eO

n +
1
√n
X
i∈[n]
Li
µ


gradient queries, when µx = µy = µ. This is recovered by Theorem 9 (possibly up to logarithmic
factors) in the special setting of fi = gi ←0, µx = µy ←µ, and Λxx
i
= Λxy
i
= Λyy
i
←Li. More
generally, [111] gave a result depending polynomially on the desired accuracy without the strongly
convex-concave assumption, which follows from a variant of Theorem 9 after applying the explicit
regularization in [366] that reduces to the strongly convex-concave case.
Moreover, Theorem 15 of [507] provided a high-precision solver using roughly
eO

n +
1
√n
X
i∈[n]
Li
√µxµy


gradient queries. Our work recovers (and sharpens dependences in) this result for minimax ﬁnite
sum problems where each summand has the bilinear coupling (2.5). In the more general setting

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
41
where each summand only has a uniform smoothness bound, the [507] result can be thought of as
the accelerated ﬁnite sum analog of the main claim in [366], which is incomparable to our Theorem 1.
In a similar way, the rate of [507] is incomparable to Theorem 3, and each improves upon the other
in diﬀerent parameter regimes. We believe designing a single algorithm which obtains the best of
both worlds for (2.9) is an interesting future direction.
2.1.3
Additional related work
We give a brief discussion of several lines of work which our results build upon, and their connection
with the techniques used in this paper.
Acceleration via primal-dual extragradient methods. Our algorithms are based on extragradient
methods, a framework originally proposed by [329] which was later shown to obtain optimal rates for
solving Lipschitz variational inequalities in [415, 420]. There have been various implementations of
extragradient methods including mirror prox [415] and dual extrapolation [420]; we focus on adapting
the former in this work. Variations of extragradient methods have been studied in the context of
primal-dual formulations of smooth convex optimization [3, 530], and are known to obtain optimal
(accelerated) rates in this setting. In particular, the relative Lipschitzness analysis of acceleration
in this chapter is motivated by developments in the bilinear setting, namely the area convexity
framework of [483]. We further build upon these works by using primal-dual formulations to design
accelerated algorithms in various settings beyond smooth convex optimization, namely (2.6), (2.7),
and (2.9).
Acceleration under relative regularity assumptions. This chapter builds upon a framework for
analyzing extragradient methods known as relative Lipschitzness, which was proposed independently
by [496]. We demonstrate that this framework (and randomized variants thereof) obtains improved
rates for primal-dual formulations beyond those studied in prior works.
Curiously, our applications of the relative Lipschitzness framework reveal that the regularity
conditions our algorithms require are weaker than standard assumptions of smoothness in a norm.
In particular, several technical requirements of speciﬁc components of our algorithms are satis-
ﬁed by setups with regularity assumptions generalizing and strengthening the relative smoothness
assumption of [64, 375]. This raises interesting potential implications in terms of the necessary regu-
larity assumptions for non-Euclidean acceleration, because relative smoothness is known to be alone
insuﬃcient for obtaining accelerated rates in general [197]. Notably, [265] also developed an acceler-
ation framework under a strengthened relative smoothness assumption, which requires strengthened
bounds on divergences between three points. We further elaborate on these points in Section 2.7.3,
when deriving relative Lipschitzness bounds through weaker assumptions in Lemma 13. We focus
on the Euclidean setup in this paper, but we believe an analogous study of non-Euclidean setups is
interesting and merits future exploration.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
42
2.2
Preliminaries
General notation. We use eO to hide logarithmic factors in problem regularity parameters, initial
radius bounds, and target accuracies when clear from context. We denote [n] := {i ∈N | i ≤n}.
Variables are in Rd unless otherwise noted. ei is the ith standard basis vector. Throughout the
chapter, X (and Y, when relevant) represent Euclidean spaces, and ∥·∥will mean the Euclidean
norm in appropriate dimension when applied to a vector, unless stated otherwise in the context.
Regardless, when a norm ∥·∥is clear from context, the dual norm is ∥·∥∗, deﬁned as ∥x∥∗:=
max∥y∥≤1 y⊤x.
For a variable on a product space, e.g. z ∈X × Y, we refer to its blocks as (zx, zy) when clear
from context. For a bilinear operator A : X →Y∗, unless speciﬁed otherwise, ∥·∥will mean the
(Euclidean) operator norm, i.e.
∥A∥:= sup
∥x∥=1
∥Ax∥= sup
∥x∥=1
sup
∥y∥=1
y⊤Ax.
Complexity model.
Throughout the paper, we evaluate the complexity of methods by their
gradient oracle complexity, and do not discuss the cost of vector operations (which typically are
subsumed by the cost of the oracle). In Section 2.7, the gradient oracle returns ∇f, ∇g, or ∇h at
any point; in Section 2.8 (respectively, Section 2.9), the oracle returns ∇fi at a point for some i ∈[n]
(respectively, ∇fi, ∇gi, or ∇hi at a point for some i ∈[n]).
Divergences. The Bregman divergence induced by diﬀerentiable, convex r is V r
x (x′) := r(x′) −
r(x)−⟨∇r(x), x′ −x⟩, for any x, x′ ∈X. For all x, V r
x is nonnegative and convex. Whenever we use
no superscript r, we assume r = 1
2 ∥·∥2 so that Vx(x′) = 1
2 ∥x −x′∥2. Bregman divergences satisfy
the equality
⟨∇r(w) −∇r(z), w −u⟩= V r
z (w) + V r
w(u) −V r
z (u).
(2.10)
We deﬁne the proximal operation in r by
Proxr
x(Φ) := argminx′∈X {⟨Φ, x′⟩+ V r
x (x′)} .
Functions and operators. We say h : X × Y →R is convex-concave if its restrictions h(·, y) and
h(x, ·) are respectively convex and concave, for any x ∈X and y ∈Y. The duality gap of a pair
(x, y) is Gaph(x, y) := maxy′∈Y h(x, y′) −minx′∈X h(x′, y); a saddle point is a pair (x⋆, y⋆) ∈X × Y
with zero duality gap.
We call operator Φ : Z →Z∗monotone if ⟨Φ(z) −Φ(z′), z −z′⟩≥0 for all z, z′ ∈Z.
We
say z⋆solves the variational inequality (VI) in Φ if ⟨Φ(z⋆), z⋆−z⟩≤0 for all z ∈Z. We equip
diﬀerentiable convex-concave h with the "gradient operator" Φ(x, y) := (∇xh(x, y), −∇yh(x, y)).
The gradient of convex f and the gradient operator of convex-concave h are both monotone (see
Appendix A.9). Their VIs are respectively solved by any minimizers of f and saddle points of h.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
43
Finally, for a function f, we call any x with f(x) ≤f(x⋆) + ϵ an ϵ-approximate minimizer, where x⋆
is the minimizing argument.
Regularity. We say function f : X →R is L-smooth if ∥∇f(x) −∇f(x′)∥∗≤L ∥x −x′∥for all
x, x′ ∈X; if f is twice-diﬀerentiable, this is equivalent to (x′ −x)⊤∇2f(x)(x′ −x) ≤L ∥x′ −x∥2 for
all x, x′ ∈X. We say diﬀerentiable function f : X →R is µ-strongly convex if V f
x (x′) ≥µ
2 ∥x −x′∥2
for all x, x′ ∈X; if f is twice-diﬀerentiable, this is equivalent to (x′−x)⊤∇2f(x)(x′−x) ≥µ ∥x′ −x∥2
for all x, x′ ∈X. We also say f is µ-strongly convex with respect to a distance-generating function
r if V f
x (y) ≥µV r
x (y) for all x, y ∈X.
Finally, we say operator Φ : Z →Z∗is m-strongly monotone with respect to convex r : Z →R
if for all z, z′ ∈Z,
⟨Φ(z) −Φ(z′), z −z′⟩≥m ⟨∇r(z) −∇r(z′), z −z′⟩= m (V r
z (z′) + V r
z′(z)) .
Convex conjugates. The (Fenchel dual) convex conjugate of a convex f : X →R is denoted
f ∗(x∗) := max
x∈X ⟨x, x∗⟩−f(x).
We allow f ∗to take the value ∞. We recall the following facts about convex conjugates.
Fact 1. Let f : X →R be diﬀerentiable.
1. For all x ∈X, ∇f(x) ∈argmaxx∗∈X ∗⟨x∗, x⟩−f ∗(x∗).
2. (f ∗)∗= f.
3. If f ∗is diﬀerentiable, for all x ∈X, ∇f ∗(∇f(x)) = x.
4. If f is L-smooth, then for all x, x′ ∈X,
f(x′) −f(x) −⟨∇f(x), x′ −x⟩≥1
2L ∥∇f(x′) −∇f(x)∥2 .
If f is µ-strongly convex, f ∗is 1
µ-smooth.
Proof. The ﬁrst three items all follow from Chapter 11 of [458]. The ﬁrst part of the fourth item is
shown in Appendix A.1, and the second part is shown in [303].
For a function f : X →R, we deﬁne the set X ∗
f ⊂X ∗to be the set of points realizable as a
gradient, namely X ∗
f := {∇f(x) | x ∈X}. This will be come relevant in applications of Item 4 in
Fact 1 throughout the paper, when ∇f is not surjective (onto X ∗).
Finally, we defer a review and several additional facts about convex conjugates to Appendix A.1.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
44
Algorithm 1: Mirror-Prox(z0, T): Mirror prox [415]
1 Input: Distance generating r, λ-relatively Lipschitz monotone g : Z →Z∗, initial point
z0 ∈Z;
2 for 0 ≤t < T do
3
wt ←Proxr
zt( 1
λg(zt));
4
zt+1 ←Proxr
zt( 1
λg(wt));
2.3
Extragradient convergence under relative Lipschitzness
We give a brief presentation of mirror prox [415], and a convergence analysis under relative Lips-
chitzness. Our results also hold for dual extrapolation [420], which can be seen as a "lazy" version
of mirror prox updating a state in dual space (see [98]); we defer details to Appendix A.4.
Deﬁnition 1 (Relative Lipschitzness). For convex r : Z →R, we call operator g : Z →Z∗
λ-relatively Lipschitz with respect to r if for every three z, w, u ∈Z,
⟨g(w) −g(z), w −u⟩≤λ (V r
z (w) + V r
w(u))
We say g is λ-relatively Lipschitz with respect to r over Zalg ⊆Z if the above inequality holds for
all z, w, u ∈Zalg.
For example, we have the following bound when g = ∇r, which follows directly from nonnega-
tivity of Bregman divergences and (2.10).
Lemma 1. Let r : Z →R be convex. Then, ∇r is 1-relatively Lipschitz with respect to r over Z.
Deﬁnition 1 can be thought of as an alternative to a natural nonlinear analog of the area convexity
condition of [483] displayed below:
⟨g(w) −g(z), w −u⟩≤λ

r(z) + r(w) −r(u) −3r
z + w + u
3

.
Our proposed alternative is well-suited for the standard analyses of extragradient methods such as
mirror prox and dual extrapolation. For the special case of bilinear minimax problems in a matrix
A, the left hand side of Deﬁnition 1 measures the area of a triangle in a geometry induced by A.
Relative Lipschitzness encapsulates the more standard assumptions that g is Lipschitz and r is
strongly convex (Lemma 2), as well as the more recent assumptions that f is convex and relatively
smooth with respect to r [64, 375] (Lemma 3). We defer proofs to Appendix A.1.
Lemma 2. If g is L-Lipschitz and r is µ-strongly convex in ∥·∥, g is L/µ-relatively Lipschitz with
respect to r.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
45
Lemma 3. If f is L-relatively smooth with respect to r, i.e. V f
x (y) ≤LV r
x (y) for all x and y, then
g, deﬁned by g(x) := ∇f(x) for all x, is L-relatively Lipschitz with respect to r.
We now give an analysis of Algorithm 1 showing the average "regret" ⟨g(wt), wt −u⟩of iterates
decays at a O(T −1) rate. This strengthens Lemma 3.1 of [415].
Proposition 1. The iterates {wt} of Algorithm 1 satisfy for all u ∈Z,
X
0≤t<T
⟨g(wt), wt −u⟩≤λV r
z0(u).
Proof. First-order optimality conditions of wt, zt+1 with respect to u imply (see Lemma 216)
1
λ ⟨g(zt), wt −zt+1⟩≤V r
zt(zt+1) −V r
wt(zt+1) −V r
zt(wt),
1
λ ⟨g(wt), zt+1 −u⟩≤V r
zt(u) −V r
zt+1(u) −V r
zt(zt+1).
(2.11)
Adding and manipulating gives, via relative Lipschitzness (Deﬁnition 1),
1
λ ⟨g(wt), wt −u⟩≤V r
zt(u) −V r
zt+1(u) + 1
λ ⟨g(wt) −g(zt), wt −zt+1⟩−V r
wt(zt+1) −V r
zt(wt)
≤V r
zt(u) −V r
zt+1(u).
(2.12)
Finally, summing and telescoping (2.12) yields the desired conclusion.
We brieﬂy comment on how to use Proposition 1 to approximately solve convex-concave games
in a function f(x, y). By applying convexity and concavity appropriately to the regret guarantee
(and dividing by T, the iteration count), one can replace the left hand side of the guarantee with the
duality gap of an average point ¯w against a point u, namely f(wx, uy) −f(ux, wy). By maximizing
the right hand side over u, this can be converted into an overall duality gap guarantee. For some of
our applications in following sections, u will be some ﬁxed point (rather than a best response) and
the regret statement will be used in a more direct manner to prove guarantees.
2.4
Acceleration via relative Lipschitzness
We show that directly applying Algorithm 1 to the optimization problem (2.1) recovers an accelerated
rate for ﬁrst-order convex function minimization (for simplicity, we focus on the ℓ2 norm here;
our methods extend to general norms, discussed in Appendix A.5).
Our main technical result,
Lemma 4, shows the gradient operator of (2.1) is relatively Lipschitz in the natural regularizer
induced by f, which combined with Proposition 1 gives our main result, Theorem 4. Crucially, our
method regularizes the dual variable with f ∗, the Fenchel dual of f, which we show admits eﬃcient
implementation, allowing us to obtain our improved bound on the relative Lipschitzness parameter.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
46
Lemma 4 (Relative Lipschitzness for the Fenchel game). Let f : Rd →R be L-smooth and µ-strongly
convex in the Euclidean norm ∥·∥2. Let g(x, y) = (y, ∇f ∗(y) −x) be the gradient operator of the
convex-concave problem (2.1), and deﬁne the distance-generating function r(x, y) := µ
2 ∥x∥2
2 +f ∗(y).
Then, g is 1 +
q
L
µ -relatively Lipschitz with respect to r.
Proof. Consider three points z = (zx, zy), w = (wx, wy), u = (ux, uy). By direct calculation,
⟨g(w) −g(z), w −u⟩= ⟨wy −zy, wx −ux⟩+ ⟨−wx + zx + ∇f ∗(wy) −∇f ∗(zy), wy −uy⟩.
(2.13)
By Cauchy-Schwarz and L−1-strong convexity of f ∗(cf. Lemma 214) respectively, we have
⟨wy −zy, wx −ux⟩+ ⟨zx −wx, wy −uy⟩≤∥wy −zy∥2 ∥wx −ux∥2 + ∥zx −wx∥2 ∥wy −uy∥2
≤
s
L
µ
µ
2 ∥wx −zx∥2
2 + µ
2 ∥wx −ux∥2
2 + 1
2L ∥wy −zy∥2
2 + 1
2L ∥wy −uy∥2
2

≤
s
L
µ (V r
z (w) + V r
w(u)) .
(2.14)
The second line used Young's inequality twice. Furthermore, by convexity of f ∗from zy to uy,
⟨∇f ∗(wy) −∇f ∗(zy), wy −uy⟩
= ⟨∇f ∗(zy), uy −zy⟩−⟨∇f ∗(wy), uy −wy⟩−⟨∇f ∗(zy), wy −zy⟩
≤f ∗(uy) −f ∗(zy) −⟨∇f ∗(wy), uy −wy⟩−⟨∇f ∗(zy), wy −zy⟩
= V f ∗
zy (wy) + V f ∗
wy (uy) ≤V r
z (w) + V r
w(u).
(2.15)
The last inequality used separability of r and nonnegativity of divergences. Summing the bounds
(2.14) and (2.15) and recalling (2.13) yields the conclusion, where we use Deﬁnition 1.
We also state a convenient fact about the form our iterates take.
Lemma 5. In the setting of Lemma 4, let zt = (xt, yt), wt = (xt+ 1
2 , yt+ 1
2 ) be iterates produced by
running Algorithm 1 on the pair g, r. Suppose y0 = ∇f(v0) for some v0. Then, yt+ 1
2 and yt+1 can
be recursively expressed as yt+ 1
2 = ∇f(vt+ 1
2 ), yt+1 = ∇f(vt+1), for
vt+ 1
2 ←vt + 1
λ(xt −vt), vt+1 ←vt + 1
λ

xt+ 1
2 −vt+ 1
2

.
Proof. We prove this inductively; consider some iteration t. Assuming yt = ∇f(vt), by deﬁnition
yt+ 1
2 = argminy
 1
λ (∇f ∗(yt) −xt) , y

+ V f ∗
yt (y)

= argmaxy
 1
λ(xt −vt) + vt, y

−f ∗(y)

= ∇f

vt + 1
λ(xt −vt)

.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
47
Here, we used standard facts about convex conjugates (see Lemma 212). A similar argument shows
that we can compute implicitly yt+1 = ∇f(vt + 1
λ(xt+ 1
2 −vt+ 1
2 )).
We now prove Theorem 4, i.e. that we can halve function error in O
q
L
µ

iterations of Algo-
rithm 1. Simply iterating Theorem 4 yields a linear rate of convergence for smooth, strongly convex
functions, yielding an ϵ-approximate minimizer in O
q
L
µ log f(x0)−f(x∗)
ϵ

iterations.
Theorem 4. In the setting of Lemma 4, run T ≥4λ iterations of Algorithm 1 initialized at z0 =
(x0, ∇f(x0)) on the pair g, r with λ = 1 +
q
L
µ , and deﬁne
¯v = 1
T
X
0≤t<T
vt+ 1
2 where wt =

xt+ 1
2 , ∇f(vt+ 1
2 )

.
Then we have f(¯v) −f(x∗) ≤1
2(f(x0) −f(x∗)), where x∗minimizes f.
Proof. First, we remark that this form of wt follows from Lemma 5, and correctness of λ follows
from Lemma 4. By an application of Proposition 1, letting u = (x∗, ∇f(x∗)),
1
T
X
0≤t<T
⟨g(wt), wt −u⟩≤λ
T · V r
z0(u) ≤1
4
µ
2 ∥x0 −x∗∥2
2 + V f ∗
∇f(x0)(∇f(x∗))

= 1
4
µ
2 ∥x0 −x∗∥2
2 + f(x0) −f(x∗)

≤1
2 (f(x0) −f(x∗)) .
The second line used the deﬁnition of divergence in f ∗(see Lemma 213) and strong convexity of f,
which implies f(x0) ≥f(x∗) + µ
2 ∥x0 −x∗∥2
2. Moreover, by the deﬁnition of g and ∇f(x∗) = 0,
1
T
X
0≤t<T
⟨g(wt), wt −u⟩= 1
T
X
0≤t<T
D
∇f(vt+ 1
2 ), xt+ 1
2 −x∗E
+
D
vt+ 1
2 −xt+ 1
2 , ∇f(vt+ 1
2 )
E
≥1
T
X
0≤t<T
f(vt+ 1
2 ) −f(x∗) ≥f(¯v) −f(x∗).
The last line used convexity twice. Combining these two derivations yields the conclusion.
For convenience, we state the full algorithm of Theorem 4 as Algorithm 2.
In Appendix A.5, we give an alternative proof of acceleration leveraging relative Lipschitzness, as
well as a variant of extragradient methods suited for strongly monotone operators (cf. Appendix A.4),
by applying these tools to the saddle point problem (to be contrasted with (2.1))
min
x∈Rd f(x) = min
x∈Rd max
y∈Rd
µ
2 ∥x∥2
2 + ⟨y, x⟩−h∗(y), where h(x) := f(x) −µ
2 ∥x∥2
2 .
This alternative proof strategy readily generalizes the accelerated rate of Theorem 4 to general
norms. While the rates attained in Appendix A.5 are slightly less sharp (losing a L
µ factor in the

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
48
Algorithm 2: EG-Accel(x0, ϵ): Extragradient accelerated smooth minimization
1 Input: x0 ∈Rd, f L-smooth and µ-strongly convex in ∥·∥2, and ϵ0 ≥f(x0) −f(x∗);
2 Output: ϵ-approximate minimizer of f;
3 λ ←1 +
p
L/µ, x(0) ←x0, T ←4⌈λ⌉, K ←⌈log2
ϵ0
ϵ ⌉;
4 for 0 ≤k < K do
5
x0 ←x(k), v0 ←x0;
6
for 0 ≤t < T do
7
xt+ 1
2 ←xt −
1
µλ∇f(vt) and vt+ 1
2 ←vt + 1
λ(xt −vt);
8
xt+1 ←xt −
1
µλ∇f(vt+ 1
2 ) and vt+1 ←vt + 1
λ(xt+ 1
2 −vt+ 1
2 ) ;
9
x(k+1) ←1
T
P
0≤t<T vt+ 1
2 ;
10 Return: x(K);
logarithm) when compared to Theorem 4, the analysis is arguably simpler. This is in the sense that
Appendix A.5 shows a potential function decreases at a linear rate in every iteration, rather than
requiring O(
p
L/µ) iterations to halve it.
2.5
Area convexity rates for box-simplex games via relative
Lipschitzness
In this section, we show that a local variant of Deﬁnition 1 recovers the improved convergence rate
achieved by [483] for box-constrained ℓ∞-regression, and more generally box-simplex bilinear games.
Speciﬁcally, we will use the following result, a simple extension to Proposition 1 which states that
relative Lipschitzness only must hold with respect to triples of algorithm iterates.
Corollary 1. Suppose Algorithm 1 is run with a monotone operator g and a distance generating r
satisfying, for all iterations t,
⟨g(wt) −g(zt), wt −zt+1⟩≤λ
 V r
zt(wt) + Vwt(zt+1)

.
(2.16)
Then, the conclusion of Proposition 1 holds.
Proof. Observe that the only applications of relative Lipschitzness in the proof of Proposition 1 are
of the form (2.16) (namely, in (2.12)). Thus, the same conclusion still holds.
In other words, letting Zalg be a superset of all the iterates of the algorithm, it suﬃces for
relative Lipschitzness to hold only over Zalg (see Deﬁnition 1).
We use Corollary 1 to give an
alternative algorithm and analysis recovering the rates implied by the use of area convexity in [483],
for box-simplex games, which we now deﬁne.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
49
Problem 1 (Box-simplex game). Let A ∈Rm×n be a matrix and let b ∈Rm, c ∈Rn be vectors.
The associated box-simplex game, and its induced monotone operator g, are
min
x∈[−1,1]n max
y∈∆m f(x, y) := y⊤Ax −⟨b, y⟩+ ⟨c, x⟩, g(x, y) :=
 A⊤y + c, b −Ax

.
(2.17)
Here, ∆m := {y ∈Rm
≥0 : P
i∈[m] yi = 1} is the nonnegative probability simplex in m dimensions.
By a simple reduction that at most doubles the size of the input (stacking A, b with negated
copies, cf. Section 3.1 of [486]), Problem 1 is a generalization of the box-constrained ℓ∞-regression
problem
min
x∈[−1,1]m ∥Ax −b∥∞.
The work of [483] proposed a variant of extragradient algorithms, based on taking primal-dual
proximal steps in the following regularizer:6
r(x, y) := y⊤|A|(x2) + 10 ∥A∥∞→∞
X
i∈[m]
yi log yi.
(2.18)
Here, |A| is the entrywise absolute value of A. The convergence rate of this algorithm was proven
in [483] via an analysis based on "area convexity" of the pair (g, r), which required a somewhat
sophisticated proof based on solving a partial diﬀerential equation over a triangle. We now show
that the same rate can be obtained by the extragradient algorithms of [415, 420], and analyzed
via local relative Lipschitzness (2.16).7
We ﬁrst make the following simplication without loss of
generality.
Lemma 6. For all x ∈[−1, 1]n the value of maxy∈∆m f(x, y) in (2.17) is unchanged if we remove all
coordinates of b with bi ≥mini∗∈[m] bi∗+ 2 ∥A∥∞→∞, and the corresponding rows of A. Therefore,
in designing an algorithm to solve (2.17) to additive error with linear pre-processing it suﬃces to
assume that bi ∈[0, 2 ∥A∥∞→∞] for all i ∈[m].
Proof. For any x ∈[−1, 1]n, letting i∗∈argmini∈[m]bi we have
max
y∈∆m y⊤(Ax −b) = max
i∈[m][Ax −b]i ≥−∥A∥∞→∞∥x∥∞−min
i∗∈[m] bi ≥−∥A∥∞→∞−bi∗.
However, [Ax −b]i ≤∥A∥∞→∞−bi for all i ∈[m]. Consequently, any coordinate i ∈[m] that
satisﬁes bi ≥bi∗+ 2 ∥A∥∞→∞has [Ax −b]i ≤[Ax −b]i∗and the value of maxy∈∆m f(x, y) is
unchanged if this entry of bi and the corresponding row of A is removed. Further, note that ⟨y, 1⟩
is a constant for all y ∈∆m. Consequently, in linear time we can remove all the coordinates i with
6We let ∥A∥∞→∞:= sup∥x∥∞=1 ∥Ax∥∞, i.e. the ℓ∞operator norm of A or max ℓ1 norm of any row.
7Although our analysis suﬃces to recover the rate of [483] for ℓ∞regression, the analysis of [483] is in some sense
more robust (and possibly) more broadly applicable than ours, as it does not need to reason directly about how much
the iterates vary in a step. Understanding or closing this gap is an interesting open problem.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
50
bi ≥mini∗∈[m] bi∗+ 2 ∥A∥∞→∞and shift all the coordinates by an additive constant so that the
minimum coordinate of a remaining bi is 0 without aﬀecting additive error of any x.
We now prove our main result regarding the use of mirror prox to solve box-simplex games, using
the area convex regularizer analyzed (with a slightly diﬀerent algorithm) in [483].
Theorem 5. Assume the preprocessing of Lemma 6 so that b ∈[0, 2 ∥A∥∞→∞]m. Consider running
Algorithm 1 or Algorithm 65 on the operator in (2.17) with λ = 3, using the regularizer in (2.18).
The resulting iterates satisfy (2.16), and thus satisfy the conclusion of Proposition 1.
Proof. Fix a particular iteration t. We ﬁrst claim that the simplex variables wy
t and zy
t+1 obey the
following multiplicative stability property: entrywise,
wy
t , zy
t+1 ∈
1
2zy
t , 2zy
t

.
(2.19)
We will give the proof for wy
t as the proof for zy
t+1 follows from the same reasoning. Recall that
wt = argminw∈∆n×[−1,1]m
 1
λg(zt), w

+ V r
zt(w)

,
and therefore, deﬁning (x)2 and (zx
t )2 as the entrywise square of these vectors,
wy
t = argminy∈∆m ⟨γy
t , y⟩+ 10 ∥A∥∞→∞
X
i∈[m]
yi log
yi
[zy
t ]i
where γy
t := 1
λ (b −Azx
t ) + |A|
h
(x)2 −(zx
t )2i
.
Consequently, applying log and exp entrywise we have
wy
t ∝exp

log zy
t −
1
10 ∥A∥∞→∞
γy
t

.
This implies the desired (2.19), where we use that ∥γy
t ∥∞≤3 ∥A∥∞→∞, and exp(0.6) ≤2. Next,
we have by a straightforward calculation (Lemma 3.4, [483] or Lemma 6, [293]) that
∇2r(x, y) ⪰

diag
 |A:j|⊤y

0
0
∥A∥∞→∞diag

1
yi


.
(2.20)
By expanding the deﬁnition of Bregman divergence, we have
V r
zt(wt) =
Z 1
0
Z α
0
∥wt −zt∥2
∇2r(zt+β(wt−zt)) dβdα.
Fix some β ∈[0, 1], and let zβ := zt +β(wt −zt). Since the coordinates of zβ also satisfy the stability

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
51
property (2.19), by the lower bound of (D.12), we have
∥wt −zt∥2
∇2r(zβ) ≥
X
i∈[m],j∈[n]
|Aij|
 
[zy
β]i [wx
t −zx
t ]2
j +
1
[zy
β]i
[wy
t −zy
t ]2
i
!
≥1
2
X
i∈[m],j∈[n]
|Aij|

[zy
t ]i [wx
t −zx
t ]2
j +
1
[zy
t ]i
[wy
t −zy
t ]2
i

.
By using a similar calculation to lower bound V r
wt(zt+1), we have by Young's inequality the desired
V r
zt(wt) + V r
wt(zt+1) ≥1
4
X
i∈[m],j∈[n]
|Aij|

[zy
t ]i [wx
t −zx
t ]2
j +
1
[zy
t ]i
[wy
t −zy
t ]2
i

+ 1
4
X
i∈[m],j∈[n]
|Aij|

[zy
t ]i

wx
t −zx
t+1
2
j +
1
[zy
t ]i

wy
t −zy
t+1
2
i

≥1
3
X
i∈[m],j∈[n]
Aij

[wy
t −zy
t ]i

wx
t −zx
t+1

j −

wy
t −zy
t+1

i [wx
t −zx
t ]j

= 1
λ ⟨g(wt) −g(zt), wt −zt+1⟩.
The range of the regularizer r is bounded by O(∥A∥∞→∞log m), and hence the iteration com-
plexity to ﬁnd an ϵ additively-approximate solution to the box-simplex game is O( ∥A∥∞→∞log m
ϵ
).
Finally, we comment that the iteration complexity of solving the subproblems required by extragra-
dient methods in the regularizer r to suﬃciently high accuracy is logarithmically bounded in problem
parameters via a simple alternating minimization scheme proposed by [483]. Here, we note that the
error guarantee e.g. Proposition 1 is robust up to constant factors to solving each subproblem to
ϵ additive accuracy, and appropriately using approximate optimality conditions (for an example of
this straightforward extension, see Corollary 1 of [293]).
2.6
Randomized coordinate acceleration via expected rela-
tive Lipschitzness
We show relative Lipschitzness can compose with randomization. Speciﬁcally, we adapt Algorithm 2
to coordinate smoothness, recovering the accelerated rate ﬁrst obtained in [553, 425]. We recall f
is Li-coordinate-smooth if its coordinate restriction is smooth, i.e. |∇if(x + cei) −∇if(x)| ≤Li|c|
∀x ∈X, c ∈R; for twice-diﬀerentiable coordinate smooth f, ∇2
iif(x) ≤Li.
Along the way, we build a framework for randomized extragradient methods via "local variance
reduction" in Proposition 2. In particular, we demonstrate how for separable domains our tech-
nique can yield O(T −1) rates for stochastic extragradient algorithms, bypassing a variance barrier

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
52
encountered by prior methods [302]. Throughout, let f : Rd →R be Li-smooth in coordinate i, and
µ-strongly convex in ∥·∥2, and deﬁne the distance generating function r(x, y) = µ
2 ∥x∥2
2 + f ∗(y).
Our approach modiﬁes that of Section 2.4 in the following ways. First, our iterates are deﬁned
via stochastic estimators which "share randomness" (use the same coordinate in both updates).
Concretely, ﬁx some iterate zt = (xt, ∇f(vt)). For a distribution {pi}i∈[d], sample i ∼pi and let
gi(zt) :=
 1
pi
∇if(vt), vt −xt

, w(i)
t
=

x(i)
t+ 1
2 , ∇f(vt+ 1
2 )

:= Proxr
zt
 1
λgi(zt)

,
gi(w(i)
t ) :=
 1
pi
∇if(vt+ 1
2 ), vt+ 1
2 −

xt + 1
pi
∆(i)
t

for ∆(i)
t
:= x(i)
t+ 1
2 −xt,
z(i)
t+1 =

x(i)
t+1, ∇f(v(i)
t+1)

:= Proxr
zt
 1
λgi(w(i)
t )

.
(2.21)
By observation, gi(zt) is unbiased for g(zt); however, the same cannot be said for gi(w(i)
t ), as
the random coordinate was used in the deﬁnition of w(i)
t .
Nonetheless, examining the proof of
Proposition 1, we see that the conclusion
⟨g( ¯wt), ¯wt −u⟩≤V r
zt(u) −E

V r
z(i)
t+1(u)

still holds for some point ¯wt, as long as
E
hD
gi(w(i)
t ), w(i)
t
−u
Ei
= ⟨g( ¯wt), ¯wt −u⟩,
E
hD
gi(w(i)
t ) −gi (zt) , w(i)
t
−z(i)
t+1
Ei
≤λE
h
V r
zt(w(i)
t ) + V r
w(i)
t (z(i)
t+1)
i
.
(2.22)
We make this concrete in the following claim, a generalization of Proposition 1 which handles
randomized operator estimates as well as an expected variant of relative Lipschitzness. We remark
that as in Corollary 1, the second condition in (2.22) only requires relative Lipschitzness to hold for
the iterates of the algorithm, rather than globally.
Proposition 2. Suppose in every iteration of Algorithm 1, steps are conducted with respect to
randomized gradient operators
n
gi(zt), gi(w(i)
t )
o
satisfying (2.22) for some { ¯wt}.
Then, for all
u ∈Z,
E

X
0≤t<T
⟨g ( ¯wt) , ¯wt −u⟩

≤λV r
z0(u).
Proof. The proof follows identically to that of Proposition 1, where we iterate taking expectations
over (2.12), each time applying the two conditions in (2.22).
For the rest of this section, we overload gi to mean the choices used in (2.21). This choice is
motivated via the following two properties, required by (2.22) (and shown in Appendix A.6).

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
53
Lemma 7. Let ¯wt := (xt + P
i∈[d] ∆(i)
t , ∇f(vt+ 1
2 )). Then ∀u, taking expectations over iteration t,
E
hD
gi(w(i)
t ), w(i)
t
−u
Ei
= ⟨g( ¯wt), ¯wt −u⟩.
Lemma 8 (Expected relative Lipschitzness). Let λ = 1 + S1/2/√µ, where S1/2 := P
i∈[d]
√Li.
Then, for the iterates (2.21) with pi = √Li/S1/2, taking expectations over iteration t,
E
hD
gi(w(i)
t ) −gi (zt) , w(i)
t
−z(i)
t+1
Ei
≤λE
h
V r
zt(w(i)
t ) + V r
w(i)
t (z(i)
t+1)
i
.
Crucially, our proof of these results uses the fact that our randomized gradient estimators are
1-sparse in the x component, and the fact that we "shared randomness" in the deﬁnition of the
gradient estimators. Moreover, our iterates are eﬃciently implementable, under the "generalized
partial derivative oracle" of prior work [347, 553, 425], which computes ∇if(ax + by) for x, y ∈Rd
and a, b ∈R. In many settings of interest, these oracles can be implemented with a dimension-
independent runtime; we defer a discussion to previous references.
Lemma 9 (Iterate maintenance). We can implement each iteration of Algorithm 1 using two gen-
eralized partial derivative oracle queries and constant additional work.
We defer a formal statement to Appendix A.6, as Lemma 222.
Combining Lemma 7 and
Lemma 8, (2.22) is satisﬁed with λ = 1 + S1/2/√µ.
Finally, all of these pieces directly imply
the following, via the proof of Theorem 4 and iterating expectations. We give our full method as
Algorithm 1.
Theorem 6 (Coordinate acceleration). Algorithm 1 produces an ϵ-approximate minimizer of f in
O

X
i∈[d]
s
Li
µ log
f(x0) −f(x∗)
ϵ

iterations in expectation,
with iteration complexity given by Lemma 9.
Proof. This follows from the proof of Theorem 4, using Proposition 2 in place of Proposition 1.
2.7
Separable minimax optimization
In this section, we provide eﬃcient algorithms for computing an approximate saddle point of the
following separable minimax optimization problem:
min
x∈X max
y∈Y Fmm(x, y) for Fmm := f(x) + h(x, y) −g(y).
(2.23)

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
54
Here and throughout this section f : X →R and g : Y →R are diﬀerentiable, convex functions
and h : X × Y →R is a diﬀerentiable, convex-concave function. For the remainder, we focus on
algorithms for solving the following regularized formulation of (2.23):
min
x∈X max
y∈Y Fmm-reg(x, y) for Fmm-reg(x, y) := f(x) + h(x, y) −g(y) + µx
2 ∥x∥2 −µy
2 ∥y∥2 .
(2.24)
To instead solve an instance of (2.23) where f is µx-strongly convex and g is µy-strongly convex, we
may instead equivalently solve (2.24) by reparameterizing f ←f −µx
2 ∥·∥2, g ←g −µy
2 ∥·∥2. As it is
notationally convenient for our analysis, we focus on solving the problem (2.24) and then give the
results for (2.23) at the end of this section in Corollary 3.
In designing methods for solving (2.24) we make the following additional regularity assumptions.
Assumption 1 (Minimax regularity). We assume the following about (2.24).
1. f is Lx-smooth and g is Ly-smooth.
2. h has the following blockwise-smoothness properties: for all u, v ∈X × Y,
∥∇xh(u) −∇xh(v)∥≤Λxx ∥ux −vx∥+ Λxy ∥uy −vy∥,
∥∇yh(u) −∇yh(v)∥≤Λxy ∥ux −vx∥+ Λyy ∥uy −vy∥.
(2.25)
Note that when h is twice-diﬀerentiable, (2.51) equates to everywhere operator norm bounds on
blocks of ∇2h. Namely, for all w ∈X × Y,
∇2
xxh(w)

op ≤Λxx,
∇2
xyh(w)

op ≤Λxy, and
∇2
yyh(w)

op ≤Λyy.
In the particular case when h(x, y) = y⊤Ax −b⊤y + c⊤x is bilinear, clearly Λxx = Λyy = 0 (as
remarked in the introduction). In this case, we may then set Λxy := ∥A∥op.
The remainder of this section is organized as follows.
1. In Section 2.7.1, we state a primal-dual formulation of (2.24) which we will apply our methods
to, and prove that its solution yields a solution to (2.24).
2. In Section 2.7.2, we give our algorithm and prove it is eﬃciently implementable.
3. In Section 2.7.3, we prove the convergence rate of our algorithm.
4. In Section 2.7.4, we state and prove our main result, Theorem 25.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
55
2.7.1
Setup
To solve (2.24), we will instead ﬁnd a saddle point to the expanded primal-dual function
Fmm-pd(z) :=
D
zf∗, zxE
−
D
zg∗, zyE
+ µx
2 ∥zx∥2 −µy
2 ∥zy∥2 + h(zx, zy) −f ∗(zf∗) + g∗(zg∗).
(2.26)
We denote the domain of Fmm-pd by Z := X × Y × X ∗× Y∗. For z ∈Z, we refer to its blocks by
(zx, zy, zf∗, zg∗). The primal-dual function Fmm-pd is related to Fmm-reg in the following way.
Lemma 10. Let z⋆be the saddle point to (2.26). Then, (zx
⋆, zy
⋆) is a saddle point to (2.24).
Proof. By performing the maximization over zf∗and minimization over zg∗, we see that the problem
of computing a saddle point to the objective in (2.26) is equivalent to
min
zx∈X max
zy∈Y
µx
2 ∥zx∥2 −µy
2 ∥zy∥2 + h(zx, zy) +

max
zf∗∈X ∗
D
zf∗, zxE
−f ∗(zf∗)

−

max
zg∗∈Y∗
D
zg∗, zyE
−g∗(zg∗)

.
By Item 2 in Fact 1, this is the same as (2.24).
We next deﬁne Φ, the gradient operator of Fmm-pd. Before doing so, it will be convenient to
deﬁne r : Z →R, which combines the (unsigned) separable components of Fmm-pd:
r(z) := µx
2 ∥zx∥2 + µy
2 ∥zy∥2 + f ∗(zf∗) + g∗(zg∗).
(2.27)
The function r will also serve as a regularizer in our algorithm. With this deﬁnition, we decompose
Φ into three parts, roughly corresponding to the contribution from r, the contributions from the
bilinear portion of the primal-dual representations of f and g, and the contribution from h. In
particular, we deﬁne
Φr(z) := ∇r(z) =

µxzx, µyzy, ∇f ∗(zf∗), ∇g∗(zg∗)

Φbilin(z) := (zf∗, zg∗, −zx, −zy),
Φh(z) := (∇xh(zx, zy), −∇yh(zx, zy), 0, 0) .
(2.28)
It is straightforward to check that Φ, the gradient operator of Fmm-pd, satisﬁes
Φ(z) := Φr(z) + Φbilin(z) + Φh(z).
(2.29)
Finally, we note that by construction Φ is 1-strongly monotone with respect to r.
Lemma 11 (Strong monotonicity). The operator Φ (as deﬁned in (I.20)) is 1-strongly monotone
with respect to the function r : Z →R as in (7.66).

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
56
Proof. Consider the decomposition of Φ = Φr +Φbilin+Φh deﬁned in (2.28) and (I.20). By deﬁnition
and Items 1 to 3 from Fact 35, we know the operators Φh and Φbilin are monotone, and Φr = ∇r
is 1-strongly monotone with respect to r. Combining the three operators and using additivity of
monotonicity in Item 4 of Fact 35 yields the claim.
2.7.2
Algorithm
Our algorithm will be an instantiation of strongly monotone mirror prox, stated as Algorithm 3 below
and analyzed in Appendix A.4, an alternative to the mirror prox algorithm originally proposed by
[415].
Algorithm 3: SM-Mirror-Prox(λ, T, z0): Strongly monotone mirror prox
1 Input: Convex r : Z →R, m-strongly monotone Φ : Z →Z∗(with respect to r), z0 ∈Z
2 Parameter(s): λ > 0, T ∈N
3 for 0 ≤t < T do
4
zt+1/2 ←Proxr
zt( 1
λΦ(zt))
5
zt+1 ←argminz∈Z{ 1
λ

Φ(zt+1/2), z

+ m
λ V r
zt+1/2(z) + V r
zt(z)}
We prove the following result in Appendix A.4.
Proposition 3. If Φ is λ-relatively Lipschitz with respect to r over Zalg containing all iterates of
Algorithm 3, and its VI is solved by z⋆, the iterates of Algorithm 3 satisfy
V r
zt(z⋆) ≤

1 + m
λ
−t
V r
z0(z⋆), for all t ∈[T].
Our algorithm in this section, Algorithm 4, will simply apply Algorithm 3 to the operator-
regularizer pair (Φ, r) deﬁned in (I.20) and (7.66).
We give this implementation as pseudocode
below, and show that it is a correct implementation of Algorithm 3 in the following lemma.
Lemma 12. Algorithm 4 implements Algorithm 3 with m = 1 on (Φ, r) deﬁned in (I.20), (7.66).
Proof. Let {zt, zt+1/2}0≤t≤T be the iterates of Algorithm 3. We will inductively show that Algo-
rithm 4 preserves the invariants
zt =
 zx
t, zy
t, ∇f
 zf
t

, ∇g (zg
t )

, zt+1/2 =

zx
t+1/2, zy
t+1/2, ∇f

zf
t+1/2

, ∇g

zg
t+1/2

,
for the iterates of Algorithm 4. Once we prove this claim, it is clear from inspection that Algorithm 4
implements Algorithm 3, upon recalling the deﬁnitions (I.20), (7.66).
The base case of our induction follows from our initialization so that (∇f(zf
0), ∇g(zg
0)) ←
(∇f(x0), ∇f(y0)). Next, suppose for some 0 ≤t < T, we have zf∗
t = ∇f(zf
t) and zg∗
t
= ∇g(zg
t ). By

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
57
Algorithm 4: Minimax-Solve(Fmm-reg, x0, y0): Separable minimax optimization
1 Input: (2.24) satisfying Assumption 1, (x0, y0) ∈X × Y
2 Parameter(s): λ > 0, T ∈N
3 (zx
0, zy
0) ←(x0, y0), (zf
0, zg
0) ←(x0, y0)
4 for 0 ≤t < T do
5
Φx ←µxzx
t + ∇f(zf
t) + ∇xh(zx
t, zy
t)
Φy ←µyzy
t + ∇g(zg
t ) −∇yh(zx
t, zy
t).
▷gradient step:
6
zx
t+1/2 ←zx −
1
λµx Φx
7
zy
t+1/2 ←zy −
1
λµy Φy
8
zf
t+1/2 ←(1 −1
λ)zf
t + 1
λzx
t and zg
t+1/2 ←(1 −1
λ)zg
t + 1
λzy
t
9
Φx ←µxzx
t+1/2 + ∇f(zf
t+1/2) + ∇xh(zx
t+1/2, zy
t+1/2)
Φy ←µyzy
t+1/2 + ∇g(zg
t+1/2) −∇yh(zx
t+1/2, zy
t+1/2)
▷extragradient step:
10
zx
t+1 ←
1
1+λzx
t+1/2 +
λ
1+λzx
t −
1
(1+λ)µx Φx
11
zy
t+1 ←
1
1+λzy
t+1/2 +
λ
1+λzy
t −
1
(1+λ)µy Φy
12
zf
t+1 ←
λ
1+λzf
t +
1
1+λzx
t+1/2 and zg
t+1 ←
λ
1+λzg
t +
1
1+λzy
t+1/2
the updates in Algorithm 3,
zf∗
t+1/2 ←argminzf∗∈X ∗
 1
λ
D
∇f ∗(zf∗
t ) −zx
t, zf∗E
+ V f ∗
zf∗
t (zf∗)

= argminzf∗∈X ∗
 1
λ
D
zf
t −zx
t, zf∗E
−
D
zf
t, zf∗E
+ f ∗(zf∗)

= argmaxzf∗∈X ∗

1 −1
λ

zf
t + 1
λzx
t, zf∗
−f ∗(zf∗)

= ∇f

1 −1
λ

zf
t + 1
λzx
t

.
The second line used our inductive hypothesis and Item 3 in Fact 1, and the last used Item 1 in
Fact 1. Hence, the update to zf
t+1/2 in Algorithm 4 preserves our invariant; a symmetric argument
yields zg∗
t+1/2 = ∇g(zg
t+1/2) where zg
t+1/2 := (1 −1
λ)zg
t + 1
λzy
t.
Similarly, we show we may preserve this invariant for zt+1:
zf∗
t+1 ←argminzf∗∈X ∗
 1
λ
D
zf
t+1/2 −zx
t+1/2, zf∗E
−1
λ
D
zf
t+1/2, zf∗E
−
D
zf
t, zf∗E
+

1 + 1
λ

f ∗(zf∗)

= argmaxa∈X ∗

zf
t + 1
λzx
t+1/2, zf∗
−

1 + 1
λ

f ∗(zf∗)

= ∇f

λ
1 + λzf
t +
1
1 + λzx
t+1/2

.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
58
Hence, we may set zf
t+1 :=
λ
1+λzf
t +
1
1+λzx
t+1/2 and similarly, zg
t+1 :=
λ
1+λzg
t +
λ
1+λzy
t+1/2.
As an immediate corollary of Lemma 12, we have the following characterization of our iterates,
recalling the deﬁnitions of X ∗
f and Y∗
g from Section 2.2.
Corollary 2. Deﬁne the product space Zalg := X × Y × X ∗
f × Y∗
g , where X ∗
f := {∇f(x) | x ∈X}
and Y∗
g := {∇g(y) | y ∈Y}. Then all iterates of Algorithm 4 lie in Zalg.
For a point z ∈Zalg, we deﬁne the points zf := ∇f ∗(zf∗) and zg := ∇g∗(zg∗). By Item 3 of
Fact 1, this implies zf, zg ∈X since zf∗and zg∗are appropriate gradients.
2.7.3
Convergence analysis
In order to use Proposition 3 to analyze Algorithm 4, we require a strong monotonicity bound and
a relative Lipschitzness bound on the pair (Φ, r); the former is already given by Lemma 11. We
build up to the latter bound by ﬁrst giving the following consequences of Assumption 1, inspired by
a similar proof strategy as used in Lemma 4.
Lemma 13 (Minimax smoothness implications). Let convex f : X →R and g : Y →R, and
convex-concave h : X × Y →R satisfy Assumption 1. Then, the following hold.
1. |⟨∇f (v) −∇f (w) , x −y⟩| ≤αLxV f
v (w) + α−1Vx(y) for all v, w, x, y ∈X and α > 0.
2. |⟨∇g (v) −∇g (w) , x −y⟩| ≤αLyV g
v (w) + α−1Vx(y) for all v, w, x, y ∈Y and α > 0.
3. Φh is 1-relatively Lipschitz with respect to rh
α : Z →R deﬁned for all z ∈Z and α > 0 by
rh
α(z) := 1
2 (Λxx + αΛxy) ∥zx∥2 + 1
2
 Λyy + α−1Λxy
∥zy∥2.
Proof. We will prove Items 1 and 3, as Item 2 follows symmetrically to Item 1.
Proof of Item 1.
We compute:
|⟨∇f(v) −∇f(w), x −y⟩| ≤∥∇f(v) −∇f(w)∥∥x −y∥
≤α
2 ∥∇f(v) −∇f(w)∥2 + 1
2α ∥x −y∥2
≤αLxV f ∗
∇f(w)(∇f(v)) + α−1Vx(y) = αLxV f
v (w) + α−1Vx(y).
The ﬁrst inequality was Cauchy-Schwarz, the second was Young's inequality, and the third used
Items 3 and 4 in Fact 1. The last equality follows from Fact 1.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
59
Proof of Item 3.
Let w, v, z ∈Z be arbitrary. We have,

Φh(w) −Φh(z), w −v

= ⟨∇xh(wx, wy) −∇xh(zx, zy), wx −vx⟩−⟨∇yh(wx, wy) −∇yh(zx, zy), wy −vy⟩.
Applying Cauchy-Schwarz, Young's inequality, and Assumption 1 yields
⟨∇xh(wx, wy) −∇xh(zx, zy), wx −vx⟩≤∥∇xh(wx, wy) −∇xh(zx, zy)∥∥wx −vx∥
≤(Λxx ∥wx −zx∥+ Λxy ∥wy −zy∥) ∥wx −vx∥
≤Λxx
2 ∥wx −zx∥2 + Λxx
2 ∥wx −vx∥2 + Λxy ∥wy −zy∥∥wx −vx∥.
Symmetrically,
⟨∇yh(wx, wy) −∇yh(zx, zy), wy −vy⟩≤Λyy
2 ∥wy −zy∥2 + Λyy
2 ∥wy −vy∥2 +Λxy ∥wx −zx∥∥wy −vy∥.
Applying Young's inequality again yields
Λxy ∥wy −zy∥∥wx −vx∥≤αΛxy
2
∥wx −vx∥2 + Λxy
2α ∥wy −zy∥2 ,
and Λxy ∥wx −zx∥∥wy −vy∥≤αΛxy
2
∥wx −zx∥2 + Λxy
2α ∥wy −vy∥2 .
Combining these inequalities yields the desired bound of

Φh(w) −Φh(z), w −v

≤(Λxx + αΛxy) (Vzx(wx) + Vwx(vx)) + (Λyy + αΛxy) (Vzy(wy) + Vwy(vy))
= V rh
α
z (w) + V rh
α
w (v).
Leveraging Lemma 13 and Lemma 1, we prove relative Lipschitzness of Φ with respect to r in
Lemma 14. Interestingly, the implications in Lemma 13 are suﬃcient for this proof, and this serves as
a (potentially) weaker replacement for Assumption 1 in yielding a convergence rate for our method.
This is particularly interesting when the condition in Item 1 is replaced with a non-Euclidean
divergence, namely |⟨∇f (v) −∇f (w) , x −y⟩| ≤αLxV f
v (w)+α−1V ω
x (y) for some convex ω : X →R.
Setting, setting v = y, w = x, α =
1
Lx in this condition yields V f
x (y) ≤LV ω
x (y). Hence, this extension
to Item 1 generalizes relative smoothness between f and ω, a condition introduced by [64, 375]. It has
been previously observed [265, 197] that relative smoothness alone does not suﬃce for accelerated
rates. Item 1 provides a new strengthening of relative smoothness which, as shown by its (implicit)
use in Section 2.4, suﬃces for acceleration. We believe a more thorough investigation comparing
these conditions is an interesting avenue for future work.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
60
Lemma 14 (Relative Lipschitzness). Deﬁne Φ : Z →Z∗as in (I.20), and deﬁne r : Z →R as in
(7.66). Then Φ is λ-relatively Lipschitz with respect to r over Zalg deﬁned in Corollary 2 for
λ = 1 +
s
Lx
µx +
s
Ly
µy + Λxx
µx +
Λxy
√µxµy + Λyy
µy .
(2.30)
Proof. Let w, v, z ∈Zalg. We wish to show (cf. Deﬁnition 1)
⟨Φ(w) −Φ(z), w −v⟩≤λ (V r
z (w) + V r
w(v)) .
Since Φ = Φr + Φbilin + Φh (cf. (I.20)), we bound the contribution of each term individually. The
conclusion follows from combining (2.31), (2.32), and (2.33).
Bound on Φr:
By applying Lemma 1 to r,
⟨Φr(w) −Φr(z), w −v⟩= ⟨∇r(w) −∇r(z), w −v⟩≤V r
z (w) + V r
w(v).
(2.31)
Bound on Φbilin:
For all a ∈Zalg, we may write for some af ∈X and ag ∈Y,
Φbilin(a) = (af∗, ag∗, −ax, −ay) = (∇f(af), ∇g(ag), −ax, −ay)
and a = (ax, ay, af∗, ag∗) = (ax, ay, ∇f(af), ∇g(ag)).
Consequently,

Φbilin(w) −Φbilin(z), w −v

=

∇f(wf) −∇f(zf), wx −vx
+

∇g(wf) −∇g(zf), wy −vy
−

wx −zx, ∇f(wf) −∇f(vf)

−⟨wy −zy, ∇g(wg) −∇g(vg)⟩.
Applying Lemma 13 (Item 1 and Item 2) to each term, with α = (µxLx)−1
2 for terms involving f
and α = (µyLy)−1
2 for terms involving g yields

Φbilin(w) −Φbilin(z), w −v

≤
s
Lx
µx

V f
wf(zf) + V f
vf (wf)

+
s
Lx
µx (µxVwx(vx) + µxVzx(wx))
+
s
Ly
µy (V g
wg(zg) + V g
vg(wg)) +
s
Ly
µy (µyVwy(vy) + µyVzy(wy)) .
Applying Item 3 in Fact 1 and recalling the deﬁnition of r (7.66) yields

Φbilin(w) −Φbilin(z), w −v

≤
 s
Lx
µx +
s
Ly
µy
!
(V r
z (w) + V r
w(v)) .
(2.32)

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
61
Bound on Φh:
Applying Lemma 13 (Item 3 with α =
p
µx/µy), we have that Φh is 1-relatively
Lipschitz with respect to rh
α : Z →R deﬁned for all z ∈X and α > 0 by
rh
α(z) := 1
2 (Λxx + αΛxy) ∥zx∥2 + 1
2
 Λyy + α−1Λxy
∥zy∥2
=
Λxx
µx +
Λxy
√µxµy

· µx
2 ∥zx∥2 +
Λyy
µy +
Λxy
√µxµy

· µy
2 ∥zy∥2 .
Leveraging the nonnegativity of Bregman divergences, we conclude

Φh(w) −Φh(z), w −v

≤V rh
α
z (w) + V rh
α
w (v)
≤
Λxx
µx +
Λxy
√µxµy + Λyy
µy

(V r
z (w) + V r
w(v)) .
(2.33)
Finally, we provide simple bounds regarding initialization and termination of Algorithm 4.
Lemma 15. Let (x0, y0) ∈X × Y, and deﬁne
z0 := (x0, y0, ∇f(x0), ∇g(y0)) .
(2.34)
Suppose GapFmm-reg(x0, y0) ≤ϵ0. Then, letting z⋆be the solution to (2.26),
V r
z0(z⋆) ≤

1 + Lx
µx
+ Ly
µy

ϵ0.
Proof. By the characterization in Lemma 10, we have by Item 1 in Fact 1:
z⋆= (x⋆, y⋆, ∇f(x⋆), ∇g(y⋆)) .
Hence, we bound
V r
z0(z⋆) = µxVx0(x⋆) + V f
x⋆(x0) + µyVy0(y⋆) + V g
y⋆(y0)
≤µxVx0(x⋆) + Lx
2 ∥x0 −x⋆∥2
X + µyVy0(y⋆) + Ly
2 ∥y0 −y⋆∥2
Y
=
Lx
µx + 1

µxVx0(x⋆) +
Ly
µy + 1

µyVy0(y⋆)
≤
Lx
µx + Ly
µy + 1

ϵ0.
The ﬁrst line used Item 3 in Fact 1, and the second used smoothness of f and g (Assumption 1).

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
62
To obtain the last line, deﬁne the functions
F x
mm-reg(x) := max
y∈Y Fmm-reg(x, y) and F y
mm-reg(y) := min
x∈X Fmm-reg(x, y).
Fact 36 shows F x
mm-reg is µx-strongly convex and F y
mm-reg is µy-strongly concave, so
GapFmm-reg(x0, y0) =
 F x
mm-reg(x0) −F x
mm-reg(x⋆)

+
 F y
mm-reg(y⋆) −F y
mm-reg(y0)

≥µxVx0(x⋆) + µyVy0(y⋆).
Lemma 16. Let z ∈Z have
V r
z (z⋆) ≤
µx + Lx + Λxx
µx
+ µy + Ly + Λyy
µy
+ (Λxy)2
µxµy

· ϵ
2,
for z⋆the solution to (2.26). Then,
GapFmm-reg(zx, zy) ≤ϵ.
Proof. We follow the notation of Lemma 15. From Fact 36 we know F x
mm-reg is Lx-smooth and
F y
mm-reg is Ly-smooth, where
Lx := µx + Lx + Λxx + (Λxy)2
µy
and Ly := µy + Ly + Λyy + (Λxy)2
µx
,
under Assumption 1. Moreover, by Lemma 10 and the deﬁnition of saddle points, x⋆:= zx
⋆is the
minimizer to F x
mm-reg, and y⋆:= zy
⋆is the maximizer to F y
mm-reg. We conclude via
GapFmm-reg(zx, zy) =
 F x
mm-reg(x) −F x
mm-reg(x⋆)

+
 F y
mm-reg(y⋆) −F y
mm-reg(zy)

≤

µx + Lx + Λxx + (Λxy)2
µy

∥x −x⋆∥2
+

µy + Ly + Λyy + (Λxy)2
µx

∥y −y⋆∥2
≤2
µx + Lx + Λxx
µx
+ µy + Ly + Λyy
µy
+ (Λxy)2
µxµy

V r
z (z⋆) ≤ϵ.
The ﬁrst inequality was smoothness of F x
mm-reg and F y
mm-reg (where we used that the gradients at x⋆
and y⋆vanish because the optimization problems they solve are over unconstrained domains), and
the last inequality was nonnegativity of Bregman divergences.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
63
2.7.4
Main result
We now state and prove our main claim.
Theorem 7. Suppose Fmm-reg in (2.24) satisﬁes Assumption 1, and suppose we have (x0, y0) ∈X ×Y
such that GapFmm-reg(x0, y0) ≤ϵ0. Algorithm 4 with λ as in (2.30) returns (x, y) ∈X × Y with
GapFmm-reg(x, y) ≤ϵ in T iterations, using a total of O(T) gradient calls to each of f, g, h, where
T = O

κmm log
κmmϵ0
ϵ

, for κmm :=
s
Lx
µx +
s
Ly
µy + Λxx
µx +
Λxy
√µxµy + Λyy
µy .
(2.35)
Proof. By Lemma 10, the points x⋆and y⋆are consistent between (2.24) and (2.26). The gradient
complexity of each iteration follows from observation of Algorithm 4.
Next, by Lemma 12, Algorithm 4 implements Algorithm 3 on the pair (I.20), (7.66). By substi-
tuting the bounds on λ and m in Lemmas 14 and 11 into Proposition 3 (where we deﬁne Zalg as in
Corollary 2), it is clear that after T iterations (for a suﬃciently large constant in the deﬁnition of
T), we will have V r
zT (z⋆) is bounded by the quantity in Lemma 16, where we use the initial bound
on V r
z0(z⋆) from Lemma 15. The conclusion follows from setting (x, y) ←(zx
T , zy
T ).
As an immediate corollary, we have the following result on solving (2.23).
Corollary 3. Suppose for Fmm in (2.23) solved by (x⋆, y⋆), (f −µx
2 ∥·∥2 , g −µy
2 ∥·∥2 , h) satisﬁes
Assumption 1. There is an algorithm taking (x0, y0) ∈X × Y satisfying GapFmm(x0, y0) ≤ϵ0, which
performs T iterations for T in (2.35), returns (x, y) ∈X × Y satisfying GapFmm(x, y) ≤ϵ, and uses
a total of O(T) gradient calls to each using O(1) gradient calls to each of f, g, h.
2.8
Finite sum optimization
In this section, we give an algorithm for eﬃciently ﬁnding an approximate minimizer of the following
ﬁnite sum optimization problem:
Ffs(x) := 1
n
X
i∈[n]
fi(x).
(2.36)
Here and throughout this section fi : X →R is a diﬀerentiable, convex function for all i ∈[n]. For
the remainder, we focus on algorithms for solving the following regularized formulation of (2.36):
min
x∈X Ffs-reg(x) for Ffs-reg(x) := 1
n
X
i∈[n]
fi(x) + µ
2 ∥x∥2 .
(2.37)
As in Section 2.7, to solve an instance of (2.36) where each fi is µ-strongly convex, we may instead
equivalently solve (2.37) by reparameterizing fi ←fi −µ
2 ∥·∥2 for all i ∈[n]. We further remark

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
64
that our algorithms extend to solve instances of (2.36) where Ffs is µ-strongly convex in ∥·∥, but
individual summands are not. We provide this result at the end of the section in Corollary 4.
In designing methods for solving (2.37) we make the following additional regularity assumptions.
Assumption 2. For all i ∈[n], fi is Li-smooth.
The remainder of this section is organized as follows.
1. In Section 2.8.1, we state a primal-dual formulation of (2.37) which we will apply our methods
to, and prove that its solution also yields a solution to (2.37).
2. In Section 2.8.2, we give our algorithm and prove it is eﬃciently implementable.
3. In Section 2.8.3, we prove the convergence rate of our algorithm.
4. In Section 2.8.4, we state and prove our main result, Theorem 8.
2.8.1
Setup
To solve (2.37), we instead ﬁnd a saddle point to the primal-dual function
Ffs-pd (z) := 1
n
X
i∈[n]
D
zf∗
i , zxE
−f ∗
i (zf∗
i )

+ µ
2 ∥zx∥2 .
(2.38)
We denote the domain of Ffs-pd by Z := X × (X ∗)n.
For z ∈Z, we refer to its blocks by
(zx,

zf∗
i 	
i∈[n]). The primal-dual function Ffs-pd is related to Ffs-reg in the following way.
Lemma 17. Let z⋆be the saddle point to (2.38). Then, zx
⋆is a minimizer of (2.37).
Proof. By performing the maximization over each zf∗
i , we see that the problem of computing a
minimizer to the objective in (2.38) is equivalent to
min
zx∈X
µ
2 ∥zx∥2 + 1
n
X
i∈[n]

max
zf∗
i ∈X ∗
D
zf∗
i , zxE
−f ∗
i (zf∗
i )

.
By Item 2 in Fact 1, this is the same as (2.37).
As in Section 2.7.1, it will be convenient to deﬁne the convex function r : Z →R, which combines
the (unsigned) separable components of Ffs-pd:
r (z) := µ
2 ∥zx∥2 + 1
n
X
i∈[n]
f ∗
i (zf∗
i ).
(2.39)

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
65
Again, r serves as a regularizer in our algorithm. We next deﬁne Φ, the gradient operator of Ffs-pd:
Φ(z) :=

1
n
X
i∈[n]
zf∗
i + µzx,
 1
n

∇f ∗
i (zf∗
i ) −zx
i∈[n]

.
(2.40)
By construction, Φ is 1-strongly monotone with respect to r.
Lemma 18 (Strong monotonicity). Deﬁne Φ : Z →Z∗as in (2.40), and deﬁne r : Z →R as in
(2.39). Then Φ is 1-strongly-monotone with respect to r.
Proof. The proof is identical to Lemma 11 without the Φh term: the bilinear component cancels in
the deﬁnition of strong monotonicity, and the remaining part is exactly the gradient of r.
2.8.2
Algorithm
Our algorithm is an instantiation of randomized mirror prox stated as Algorithm 5 below, an ex-
tension to mirror prox allowing for randomized gradient estimators. This algorithm was also used
in our developments in Section 2.6, but we restate it explicitly here for convenience. We note that
the operators Φi need only be deﬁned on iterates of the algorithm.
Algorithm 5: Rand-Mirror-Prox({Φi}i∈[n], w0): Randomized mirror prox [147]
1 Input: Convex r : Z →R, probability distribution p : [n] →R≥0 with P
i∈[n] pi = 1,
operators {Φi}i∈[n] : Z →Z∗, z0 ∈Z;
2 Parameter(s): λ > 0, S ∈N ;
3 for 0 ≤s < S do
4
Sample i ∼p;
5
ws+1/2 ←Proxr
wt( 1
λΦi(ws));
6
ws+1 ←Proxr
wt( 1
λΦi(ws+1/2));
We restate the following result (originally given as Proposition 2 earlier) in slightly more gener-
ality, giving a guarantee on Algorithm 5.
Proposition 4. Suppose {Φi}i∈[n] are deﬁned so that in each iteration s, for all u ∈Z, there exists
a point ¯ws ∈Z and a monotone operator Φ : Z →Z∗such that (where all expectations ﬁx ws, and
condition only on the randomness in iteration s)
Ei∼p

Φi(ws+1/2), ws+1/2 −u

= ⟨Φ( ¯ws), ¯ws −u⟩for all u ∈Z,
Ei∼p

Φi(ws+1/2) −Φi(ws), ws+1/2 −ws+1

≤λEi∼p
h
V r
ws(ws+1/2) + V r
ws+1/2(ws+1)
i
.
(2.41)

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
66
Then (where the expectation below is taken over the randomness of the entire algorithm):
E

1
S
X
0≤s<S
⟨Φ( ¯ws), ¯ws −u⟩

≤λV r
w0(u)
S
, for all u ∈Z.
The ﬁrst condition in (2.41) is an "unbiasedness" requirement on the operators {Φi}i∈[n] with
respect to the operator Φ, for which we wish to conclude a regret guarantee. The second posits that
relative Lipschitzness (Deﬁnition 1) holds in an expected sense. We recall that Algorithm 5 requires
us to specify a set of sampling probabilities {pi}i∈[n]. We deﬁne
pi :=
√Li
2 P
j∈[n]
p
Lj
+ 1
2n for all i ∈[n].
(2.42)
This choice crucially ensures that all pi ≥
1
2n, and that all
√Li
pi
≤2 P
j∈[n]
p
Lj.
Our algorithm, Algorithm 6, recursively applies Algorithm 5 to the operator-pair (Φ, r) deﬁned
in (2.40) and (2.39), for an appropriate speciﬁcation of {Φi}i∈[n]. We give this implementation as
pseudocode in Algorithms 6 and 7 below, and show that Algorithm 7 is a correct implementation of
Algorithm 5 with respect to our speciﬁed {Φi}i∈[n] in the remainder of the section.
Algorithm 6: Finite-Sum-Solve(Ffs-reg, x0): Finite sum optimization
1 Input: (2.37) satisfying Assumption 2, x0 ∈X;
2 Parameter(s): T ∈N;
3 zx
0 ←x0, zfi
0 ←x0, zf∗
i
0 ←∇fi(x0) for all i ∈[n];
4 for 0 ≤t < T do
5
zt+1 ←Finite-Sum-One-Phase(Ffs-reg, zt);
We next describe the operators {Φi}i∈[n] used in our implementation of Algorithm 5. Fix some
0 ≤s < S, and consider some iterates {w, waux(j)} := {ws, ws+1/2} of Algorithm 5 (where we use
the notation (j) to mean the iterate that would be taken if j ∈[n] was sampled in iteration s, and
we drop the subscript s for simplicity since we only focus on one iteration). We denote the X block
of waux(j) by wx
aux, since (as made clear in the following) conditioned on w, wx
aux is always the same
regardless of the sampled j ∈[n]. For all j ∈[n], we then deﬁne the operators
Φj (w) :=

1
n
X
i∈[n]
wf∗
i + µwx,
 1
npj
(∇f ∗
j (wf∗
j ) −wx) · 1i=j

,
Φj (waux(j)) :=

1
n
X
i∈[n]
wf∗
i +
1
npj

w
f∗
j
aux(j) −wf∗
j

+ µwx
aux,
 1
npj

∇f ∗
j

w
f∗
j
aux(j)

−wx
aux

· 1i=j

,
(2.43)

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
67
Algorithm 7: Finite-Sum-One-Phase(Ffs-reg, w0): Finite sum optimization subroutine
1 Input: (2.37) satisfying Assumption 2, w0 ∈Z speciﬁed by wx
0, {wfi
0}i∈[n] ∈X;
2 Parameter(s): λ ≥2, S ∈N;
3 Sample 0 ≤σ < S uniformly at random;
4 for 0 ≤s ≤σ do
5
Sample j ∈[n] according to p deﬁned in (2.42);
6
wx
s+1/2 ←wx
s −
1
λµ(µwx
s + 1
n
P
i∈[n] ∇fi(wfis));
7
wfj
s+1/2 ←(1 −
1
λnpj )wfjs +
1
λnpj wx
s;
8
wfi
s+1/2 ←wfis for all i ̸= j;
9
∆s ←∇fj(wfj
s+1/2) −∇fj(wfjs);
10
wx
s+1 ←wx
s −
1
λµ(µwx
s+1/2 + 1
n
P
i∈[n] ∇fi(wfis) +
1
npj ∆s);
11
wfj
s+1 ←wfjs +
1
λnpj (wx
s+1/2 −wfj
s+1/2);
12
wfi
s+1 ←wfis for all i ̸= j;
13 Return: (wx
σ+1/2, {∇fi((1 −
1
λnpi )wfiσ +
1
λnpi wx
σ)}i∈[n])
where 1i=j is a zero-one indicator.
In other words, Φj(w) and Φj(waux(j)) both only have two
nonzero blocks, corresponding to the X and jth X ∗blocks. We record the following useful obser-
vation about our randomized operators (2.43), in accordance with the ﬁrst condition in (2.41). To
give a brief interpretation of our "aggregate point" deﬁned in (2.44), the X coordinate is updated
deterministically from wx according to the corresponding block of Φ, and every dual block j ∈[n]
of ¯w is set to the corresponding dual block had j been sampled in that step.
Lemma 19 (Expected regret). Deﬁne {Φj}j∈[n] : Z →Z∗as in (2.43), and the "aggregate point"
¯w :=

wx
aux,
n
w
f∗
j
aux(j)
o
j∈[n]

.
(2.44)
Then, for all u ∈Z, deﬁning Φ as in (2.40),
Ej∼p [⟨Φj(waux(j)), waux(j) −u⟩] = ⟨Φ( ¯w), ¯w −u⟩.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
68
Proof. We expand the expectation, using (2.43) and taking advantage of the sparsity of Φj:
Ej∼p [⟨Φj(waux(j)), waux(j) −u⟩]
=
* X
j∈[n]
pj

1
n
X
i∈[n]
wf∗
i +
1
npj

w
f∗
j
aux(j) −wf∗
j

+ µwx
aux

, wx
aux −ux
+
+
X
j∈[n]
pj
 1
npj

∇f ∗
j

w
f∗
j
aux(j)

−wx
aux

, w
f∗
j
aux(j) −uf∗
j

=
*
1
n
X
j∈[n]
w
f∗
j
aux(j) + µwx
aux, wx
aux −ux
+
+
X
j∈[n]
 1
n

∇f ∗
j

w
f∗
j
aux(j)

−wx
aux

, w
f∗
j
aux(j) −uyj

= ⟨Φ( ¯w), ¯w −u⟩.
We conclude this section by demonstrating that Algorithm 7 is an appropriate implementation
of Algorithm 5.
Lemma 20 (Implementation). Algorithm 7 implements Algorithm 5 on ({Φi}i∈[n] , r) deﬁned in
(2.43), (2.39), for σ iterations, and returns ¯wσ, following the deﬁnition (2.44). Each iteration s > 0
is implementable in O(1) gradient calls to some fi, and O(1) vector operations on X.
Proof. Let {ws, ws+1/2}0≤s≤σ be the iterates of Algorithm 5. We will inductively show that Algo-
rithm 7 preserves the invariants
ws =

wx
s,

∇fi(wfis)
	
i∈[n]

, ws+1/2 =

wx
s,
n
∇fi(wfi
s+1/2)
o
i∈[n]

for all 0 ≤s ≤σ. Once we prove this claim, it is clear from inspection that Algorithm 7 implements
Algorithm 5 and returns ¯wσ, upon recalling the deﬁnitions (2.43), (2.39), and (2.44).
The base case of our induction follows from the initialization guarantee of Line 3 in Algorithm 7.
Next, suppose for some 0 ≤s ≤σ, we have wf∗
is
= ∇f(wfis) for all i ∈[n]. By the updates in
Algorithm 5, if j ∈[n] was sampled on iteration s,
w
f∗
j
s+1/2 ←argminw
f∗
j ∈X ∗

1
λnpj
D
w
f∗
js −wx
s, wf∗
j
E
−
D
w
f∗
js , wf∗
j
E
+ f ∗
j

wf∗
j

= argmaxw
f∗
j ∈X ∗

1 −
1
λnpj

w
f∗
js +
1
λnpj
wx
s, wf∗
j

−f ∗
j

wf∗
j

= ∇fj

1 −
1
λnpj

w
f∗
js +
1
λnpj
wx
s

.
Here, we used the ﬁrst item in Fact 1 in the last line. Hence, the update to w
f∗
j
s+1/2 in Algorithm 7

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
69
preserves our invariant, and all other wf∗
i
s+1/2, i ̸= j do not change by sparsity of Φj. An analogous
argument shows the update to each wf∗
i
s+1 preserves our invariant. Finally, in every iteration s > 0,
the updates to wx
s+1/2 and wx
s+1 only require evaluating one new gradient each, by 1-sparsity of the
dual block updates in the prior iteration.
2.8.3
Convergence analysis
In this section, we prove a convergence result on Algorithm 7 via an application of Proposition 4.
To begin, we require a bound on the quantity λ in (2.41). We remark that our derivation of this
bound follows the derivation of Lemma 8 closely.
Lemma 21 (Expected relative Lipschitzness). Deﬁne {Φj}j∈[n] : Z →Z∗as in (2.43), and deﬁne
r : Z →R as in (2.39). Letting w+(j) be ws+1 in Algorithm 5 if j ∈[n] was sampled in iteration s,
Ej∼p [⟨Φj(waux(j)) −Φj(w), waux(j) −w+(j)⟩] ≤Ej∼p
h
V r
w (waux(j)) + V r
waux(j) (w+(j))
i
for
λ = 2n +
2 P
j∈[n]
p
Lj
√nµ
.
(2.45)
Proof. We begin by expanding the expectation of the left-hand side:
Ej∼p [⟨Φj(waux(j)) −Φj(w), waux(j) −w+(j)⟩] = Ej∼p

µwx
aux −µwx, wx
aux −wx
+(j)

+ Ej∼p
 1
npj
D
∇f ∗
j

w
f∗
j
aux(j)

−∇f ∗
j

wf∗
j

, w
f∗
j
aux(j) −w
f∗
j
+ (j)
E
+ Ej∼p
 1
npj
D
w
f∗
j
aux(j) −wf∗
j , wx
aux −wx
+(j)
E
+ Ej∼p
 1
npj
D
wx −wx
aux, w
f∗
j
aux(j) −w
f∗
j
+ (j)
E
.
(2.46)
To bound the ﬁrst two lines of (2.46), ﬁx some j ∈[n]. We apply Lemma 1 to the functions µ
2 ∥·∥2
and 1
n∇f ∗
j , and use nonnegativity of Bregman divergences, to conclude

µwx
aux −µwx, wx
aux −wx
+(j)

+
1
npj
D
∇f ∗
j

w
f∗
j
aux(j)

−∇f ∗
j

wf∗
j

, w
f∗
j
aux(j) −w
f∗
j
+ (j)
E
≤2n

V r
w (waux(j)) + V r
waux(j) (w+(j))

.
In particular, we used
1
pj ≤2n by assumption, and noted we only need to handle the case where the
second inner product term above is positive (in the other case, the above inequality is clearly true).
Hence, taking expectations the ﬁrst two lines in (2.46) contribute 2n to λ in the ﬁnal bound.
To bound the last two lines of (2.46), ﬁx j ∈[n]. By applying Item 1 in Lemma 13 to the pair

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
70
( µ
2 ∥·∥2 , nfi), we have
1
n
D
w
f∗
j
aux(j) −wf∗
j , wx
aux −wx
+(j)
E
+ 1
n
D
wx −wx
aux, w
f∗
j
aux(j) −w
f∗
j
+ (j)
E
≤1
n
s
nLj
µ

µVwxaux
 wx
+(j)

+ V
f ∗
j
w
f∗
j

w
f∗
j
aux(j)

+ 1
n
s
nLj
µ

µVwx (wx
aux) + V
f ∗
j
w
f∗
j
aux(j)

w
f∗
j
+ (j)

=
s
Lj
nµ

V r
w (waux(j)) + V r
waux(j) (w+(j))

.
Using
√Li
pi
≤2 P
j∈[n]
p
Lj and taking expectations over the above display,
Ej∼p
 1
npj
D
w
f∗
j
aux(j) −wf∗
j , wx
aux −wx
+(j)
E
+
1
npj
D
wx −wx
aux, w
f∗
j
aux(j) −w
f∗
j
+ (j)
E
≤
2 P
j∈[n]
p
Lj
√nµ
Ej∼p
h
V r
w (waux(j)) + V r
waux(j) (w+(j))
i
.
Hence, the last two lines in (2.46) contribute
2 P
j∈[n]
√
Lj
√nµ
to λ in the ﬁnal bound.
We next apply Proposition 4 to analyze the convergence of Algorithm 7.
Lemma 22. Let w0 := (wx
0, {∇fi(wfi
0)}i∈[n]), which is the input zt to Algorithm 7 at iteration t. If
S ≥2λ in Algorithm 7 with λ as in (2.45), then Algorithm 7 returns ew ←¯wσ as deﬁned in (2.44)
such that for z⋆as the saddle point to (2.38),
EV r
e
w(z⋆) ≤1
2V r
w0(z⋆).
Proof. We apply Proposition 4, where (2.41) is satisﬁed via Lemmas 19 and 21. By Proposition 4
with u = z⋆and S ≥2λ,
E

1
S
X
0≤s<S
⟨Φ( ¯ws), ¯ws −z⋆⟩

≤1
2V r
w0(z⋆).
Moreover, since σ is uniformly chosen in [0, S −1], we have
E [⟨Φ( ¯wσ), ¯wσ −z⋆⟩] ≤1
2V r
w0(z⋆).
Finally, Lemma 20 shows that (an implicit representation of) ¯wσ is indeed returned. We conclude
by applying Lemma 18 and using that z⋆solves the VI in Φ, yielding
E [⟨Φ( ¯wσ), ¯wσ −z⋆⟩] ≥E [⟨Φ( ¯wσ) −Φ(z⋆), ¯wσ −z⋆⟩] ≥V r
¯
wσ(z⋆).

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
71
Finally, we provide a simple bound regarding initialization of Algorithm 6.
Lemma 23. Let x0 ∈X, and deﬁne
z0 :=

x0, {∇fi(x0)}i∈[n]

.
(2.47)
Moreover, suppose that for x⋆the solution to (2.37), Ffs-reg(x0) −Ffs-reg(x⋆) ≤ϵ0. Then, letting z⋆
be the solution to (2.38), we have
V r
z0(z⋆) ≤
 
1 +
P
i∈[n] Li
nµ
!
ϵ0.
Proof. By the characterization in Lemma 17, we have by Item 1 in Fact 1:
z⋆=

x⋆, {∇fi(x⋆)}i∈[n]

.
Hence, we bound analogously to Lemma 15:
V r
z0(z⋆) ≤µVx0(x⋆) + V
1
n
P
i∈[n] fi
x⋆
(x0)
≤µVx0(x⋆) +
P
i∈[n] Li
2n
∥x0 −x⋆∥2
≤
 
1 +
P
i∈[n] Li
nµ
!
µVx0(x⋆) ≤
 
1 +
P
i∈[n] Li
nµ
!
ϵ0.
The last line applied strong convexity of Ffs-reg.
2.8.4
Main result
We now state and prove our main claim.
Theorem 8. Suppose Ffs-reg satisﬁes Assumption 2 and has minimizer x⋆, and suppose we have
x0 ∈X such that Ffs-reg(x0) −Ffs-reg(x⋆) ≤ϵ0. Algorithm 6 using Algorithm 7 with λ as in (2.45)
returns x ∈X with EFfs-reg(x) −Ffs-reg(x⋆) ≤ϵ in Ntot iterations, using a total of O(Ntot) gradient
calls each to some fi for i ∈[n], where
Ntot = O

κfs log
κfsϵ0
ϵ

, for κfs := n +
P
i∈[n]
√Li
√nµ
.
(2.48)
Proof. By Lemma 17, the point x⋆is consistent between (2.36) and (2.38). We run Algorithm 6
with
T = O

log
κfsϵ0
ϵ

.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
72
By recursively applying Lemma 22 for T times, we obtain a point z such that
EV r
z (z⋆) ≤ϵµ
L for L = µ + 1
n
X
i∈[n]
Li,
and hence applying L-smoothness of Ffs-reg and optimality of zx
⋆yields the claim. The complexity
follows from Lemma 12, and spending O(n) gradient evaluations on the ﬁrst and last iterates of each
call to Algorithm 7 (which is subsumed by the fact that S = Ω(n)).
By applying a generic reduction we derive in Appendix A.8, we then obtain the following corollary.
Corollary 4. Suppose the summands {fi}i∈[n] in (2.36) satisfy Assumption 2, and Ffs is µ-strongly
convex with minimizer x⋆. Further, suppose we have x0 ∈X such that Ffs(x0) −Ffs(x⋆) ≤ϵ0.
Algorithm 66 using Algorithm 6 to implement steps returns x ∈X with EFfs(x) −Ffs(x⋆) ≤ϵ in
Ntot iterations, using a total of O(Ntot) gradient calls each to some fi for i ∈[n], where
Ntot = O

κfs log
κfsϵ0
ϵ

, for κfs := n +
X
i∈[n]
√Li
√nµ.
2.9
Minimax ﬁnite sum optimization
In this section, we provide eﬃcient algorithms for computing an approximate saddle point of the
following minimax ﬁnite sum optimization problem:
min
x∈X max
y∈Y Fmmfs(x, y) := 1
n
X
i∈[n]
(fi(x) + hi(x, y) −gi(y)) .
(2.49)
Here and throughout this section {fi : X →R}i∈[n], {gi : Y →R}i∈[n] are diﬀerentiable convex
functions, and {hi : X ×Y →R}i∈[n] are diﬀerentiable convex-concave functions. For the remainder,
we focus on algorithms for solving the following regularized formulation of (2.49):
min
x∈X max
y∈Y Fmmfs-reg(x, y) := 1
n
X
i∈[n]
(fi(x) + hi(x, y) −gi(y)) + µx
2 ∥x∥2 −µy
2 ∥y∥2 .
(2.50)
As in Section 2.7 and Section 2.8, to instead solve an instance of (2.49) where each fi is 2µx-
strongly convex and each gi is 2µy-strongly convex, we may instead equivalently solve (2.50) by
reparameterizing fi ←fi −µx ∥·∥2, gi ←gi −µy ∥·∥2 for each i ∈[n]. The extra factor of 2 is so
we can make a strong convexity assumption in Assumption 3 about separable summands, which
only aﬀects our ﬁnal bounds by constants. We further remark that our algorithms extend to solve
instances of (2.49) where f, g is µx and µy-strongly convex in ∥·∥, but individual summands are not.
We provide this result at the end of the section in Corollary 5.
In designing methods for solving (2.50) we make the following additional regularity assumptions.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
73
Assumption 3. We assume the following about (2.50) for all i ∈[n].
1. fi is Lx
i-smooth and µx
i-strongly convex and gi is Ly
i-smooth and µy
i-strongly convex.
2. hi has the following blockwise-smoothness properties: for all u, v ∈X × Y,
∥∇xhi(u) −∇xhi(v)∥≤Λxx
i ∥ux −vx∥+ Λxy
i ∥uy −vy∥and
∥∇yhi(u) −∇yhi(v)∥≤Λxy
i ∥ux −vx∥+ Λyy
i ∥uy −vy∥.
(2.51)
The remainder of this section is organized as follows.
1. In Section 2.9.1, we state a primal-dual formulation of (2.50) which we will apply our methods
to, and prove that its solution also yields a solution to (2.50).
2. In Section 2.9.2, we give our algorithm, which is composed of an outer loop and an inner loop,
and prove it is eﬃciently implementable.
3. In Section 2.9.3, we prove the convergence rate of our inner loop.
4. In Section 2.9.4, we prove the convergence rate of our outer loop.
5. In Section 2.9.5, we state and prove our main result, Theorem 9.
2.9.1
Setup
To solve (2.50), we will instead ﬁnd a saddle point to the primal-dual function
Fmmfs-pd (z) := µx
2 ∥zx∥2 −µy
2 ∥zy∥2
+ 1
n
X
i∈[n]

hi(zx, zy) +
D
zf∗
i , zxE
−
D
zg∗
i , zyE
−f ∗
i

zf∗
i

+ g∗
i (zg∗
i )

.
(2.52)
We denote the domain of Fmmfs-pd by Z := X × Y × (X ∗)n × (Y∗)n. For z ∈Z, we refer to its
blocks by (zx, zy, {zf∗
i }i∈[n], {zg∗
i }i∈[n]). The primal-dual function Fmmfs-pd is related to the original
function Fmmfs in the following way; we omit the proof, as it follows analogously to the proofs of
Lemmas 10 and 17.
Lemma 24. Let z⋆= (zx
⋆, zy
⋆, {zf∗
i⋆}i∈[n], {zg∗
i⋆}i∈[n]) be the saddle point to (2.52). Then, (zx
⋆, zy
⋆) is a
saddle point to (2.50).
As in Section 2.7.1, it will be convenient to deﬁne the convex function r : Z →R, which combines
the (unsigned) separable components of Fmmfs-pd:
r

zx, zy,
n
zf∗
i
o
i∈[n] ,
n
zg∗
i
o
i∈[n]

:= µx
2 ∥zx∥2+ µy
2 ∥zy∥2+ 1
n
X
i∈[n]
f ∗
i

zf∗
i

+ 1
n
X
i∈[n]
g∗
i

zg∗
i

. (2.53)

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
74
Again, r serves as a regularizer in our algorithm. We next deﬁne Φmmfs-pd, the gradient operator
of Fmmfs-pd. We decompose Φmmfs-pd into three parts, roughly corresponding to the contribution
from r, the contributions from the primal-dual representations of {fi}i∈[n] and {gi}i∈[n], and the
contribution from {hi}i∈[n]. In particular, we deﬁne
Φmmfs-pd(z) := ∇r(z) + Φh(z) + Φbilin(z),
∇r

zx, zy,
n
zf∗
i
o
i∈[n] ,
n
zg∗
i
o
i∈[n]

:=
 
µxzx, µyzy,
 1
n∇f ∗
i

zf∗
i

i∈[n]
,
 1
n∇g∗
i

zg∗
i

i∈[n]
!
,
Φh

zx, zy,
n
zf∗
i
o
i∈[n] ,
n
zg∗
i
o
i∈[n]

:=

1
n
X
i∈[n]
∇xhi(zx, zy), −1
n
X
i∈[n]
∇yhi(zx, zy), {0}i∈[n] , {0}i∈[n]

,
Φbilin

zx, zy,
n
zf∗
i
o
i∈[n] ,
n
zg∗
i
o
i∈[n]

:=

1
n
X
i∈[n]
zf∗
i , 1
n
X
i∈[n]
zg∗
i ,

−1
nzx

i∈[n]
,

−1
nzy

i∈[n]

.
(2.54)
2.9.2
Algorithm
In this section we present our algorithm which consists of the following two parts; its design is
inspired by a similar strategy used in prior work [111, 112].
1. Our "outer loop" is based on a proximal point method (Algorithm 8, adapted from [415]).
2. Our "inner loop" solves each proximal subproblem to high accuracy via a careful analysis of
randomized mirror prox (Algorithm 9, adapted from Algorithm 5).
At each iteration t of the outer loop (Algorithm 8), we require an accurate approximation
zt+1 ≈z⋆
t+1 which solves the VI in Φ := Φmmfs-pd(z) + γ (∇r(z) −∇r(zt)) ,
(2.55)
where we recall the deﬁnitions of gtot and r from (2.54) and (2.53), and when zt is clear from context
(i.e. we are analyzing a single implementation of the inner loop).
To implement our inner loop (i.e. solve the VI in Φ), we apply randomized mirror prox (Algo-
rithm 5) with a new analysis. In particular, we will not be able to obtain the expected relative
Lipschitzness bound required by Proposition 4 for our randomized gradient estimators, so we de-
velop a new "partial variance" analysis of Algorithm 5 to obtain our rate. We use this terminology
because we use variance bounds on a component of Φ for which we cannot directly obtain expected
relative Lipschitzness bounds. We prove Proposition 5 in Appendix A.10.1.
Proposition 5 (Partial variance analysis of randomized mirror prox). Suppose (possibly random)
eΦ is deﬁned so that in each iteration s, for all u ∈Z and all ρ > 0, there exists a (possibly random)

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
75
point ¯ws ∈Z and a γ-strongly monotone operator Φ : Z →Z∗(with respect to r) such that
E
hD
eΦ(ws+1/2), ws+1/2 −w⋆
Ei
= E [⟨Φ( ¯ws), ¯ws −w⋆⟩] ,
E
hD
eΦ(ws+1/2) −eΦ(ws), ws+1/2 −ws+1
Ei
≤

λ0 + 1
ρ

E
h
V r
ws(ws+1/2) + V r
ws+1/2(ws+1)
i
+ ρλ1E

V r
w0(w⋆) + V r
¯
ws(w⋆)

,
(2.56)
where w⋆solves the VI in Φ. Then by setting
ρ ←
γ
5λ1
, λ ←λ0 + 1
ρ, T ←5λ
γ = 5λ0
γ
+ 25λ1
γ2 ,
in Algorithm 5, and returning ¯wσ for 0 ≤σ < S sampled uniformly at random,
E

V r
¯
wσ (w⋆)

≤1
2V r
w0(w⋆).
For simplicity in the following we denote ¯z := zt whenever we discuss a single proximal subprob-
lem. We next introduce the gradient estimator eΦ we use in each inner loop, i.e. ﬁnding a solution
to the VI in Φ deﬁned in (2.55). We ﬁrst deﬁne three sampling distributions p, q, r, via
pj :=
pLx
j
2 P
i∈[n]
p
Lx
i
+ 1
2n for all j ∈[n],
qk :=
p
Ly
k
2 P
i∈[n]
p
Ly
i
+ 1
2n for all k ∈[n],
and rℓ:=
Λtot
ℓ
2 P
i∈[n] Λtot
i
+ 1
2n for all ℓ∈[n], where Λtot
i
:= Λxx
i
µx +
Λxy
i
√µxµy + Λyy
i
µy for all i ∈[n].
(2.57)
Algorithm 9 will run in logarithmically many phases, each initialized at an "anchor point" w0
(cf. Line 18). We construct gradient estimators for Algorithm 5 of Φ(w) = Φmmfs-pd(w)+γ(∇r(w)−
∇r(¯z)) as deﬁned in (2.55) as follows. In each iteration, for a current anchor point w0, we sample
four coordinates j ∼p, k ∼q, and ℓ, ℓ′ ∼r, all independently. We believe that it is likely that
other sampling schemes, e.g. sampling j and k non-independently, will also suﬃce for our method
but focus on the independent scheme for simplicity. We use gxy to refer to the X × Y blocks of a
vector g in Z∗, and f∗g∗to refer to all other blocks corresponding to (X ∗)n × (Y∗)n. Then we deﬁne

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
76
for an iterate w = ws of Algorithm 9 (where Φh is as in (2.54)):
eΦ(w) := Φjkℓ(w) := Φh
jkℓ(w) + Φsep
jkℓ(w) + Φbilin
jkℓ(w),

Φh
jkℓ(w)
x :=

Φh(w0)
x +
1
nrℓ
(∇xhℓ(wx, wy) −∇xhℓ(wx
0, wy
0)) ,

Φh
jkℓ(w)
y :=

Φh(w0)
y −
1
nrℓ
(∇yhℓ(wx, wy) −∇yhℓ(wx
0, wy
0)) ,

Φh
jkℓ(w)
f∗g∗
:=

{0}i∈[n] , {0}i∈[n]

,
h
Φsep
jkℓ(w)
ixy
:= (1 + γ) (µxwx, µywy) −γ (µx¯zx, µy¯zy) ,
h
Φsep
jkℓ(w)
if∗g∗
:= (1 + γ)
  1
npj
∇f ∗
j

wf∗
j

· 1i=j

i∈[n]
,
 1
nqk
∇g∗
k

wf∗
k

· 1i=k

i∈[n]
!
−γ
  1
npj
∇f ∗
j

¯zf∗
j

· 1i=j

i∈[n]
,
 1
nqk
∇g∗
k

¯zg∗
k

· 1i=k

i∈[n]
!
,

Φbilin
jkℓ(w)
xy :=

1
n
X
i∈[n]
wf∗
i , 1
n
X
i∈[n]
wg∗
i

,

Φbilin
jkℓ(w)
f∗g∗
:=
 
−1
npj
wx · 1i=j

i∈[n]
,

−1
nqk
wy · 1i=k

i∈[n]
!
.
(2.58)
In particular, the estimator Φjkℓ(w) only depends on the sampled indices j, k, ℓ, and not ℓ′. Next,
consider taking the step waux(jkℓ) ←Proxr
w( 1
λgjkℓ(w)) as in Algorithm 5, where we use the shorthand
waux(jkℓ) = ws+1/2 to indicate the iterate of Algorithm 5 taken from ws assuming j, k, ℓwere
sampled. Observing the form of gjkℓ, we denote the blocks of waux(jkℓ) by
waux(jkℓ) :=

wx
aux(ℓ), wy
aux(ℓ),
n
wf∗
iaux(j)
o
i∈[n] ,
n
wg∗
iaux(k)
o
i∈[n]

,
where we write wx
aux(ℓ) to indicate that it only depends on the random choice of ℓ(and not j or k);
we use similar notation for the other blocks. We also deﬁne
∆x(j) := w
f∗
j
aux(j) −wf∗
j (j), ∆y(k) := wg∗
k
aux(k) −wg∗
k (k),

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
77
and then set (where we use the notation Φjkℓ′ to signify its dependence on j, k, ℓ′, and not ℓ):
eΦ(waux(jkℓ)) := Φjkℓ′(waux(jkℓ)) := Φh
jkℓ′(waux(jkℓ)) + Φsep
jkℓ′(waux(jkℓ)) + Φbilin
jkℓ′ (waux(jkℓ)),

Φh
jkℓ′(waux(jkℓ))
x :=

Φh(w0)
x +
1
nrℓ′ (∇xhℓ′(wx
aux(ℓ), wy
aux(ℓ)) −∇xhℓ′(wx
0, wy
0)) ,

Φh
jkℓ′(waux(jkℓ))
y :=

Φh(w0)
y −
1
nrℓ′ (∇yhℓ′(wx
aux(ℓ), wy
aux(ℓ)) −∇yhℓ′(wx
0, wy
0)) ,

Φh
jkℓ′(waux(jkℓ))
f∗g∗
:=

{0}i∈[n] , {0}i∈[n]

,
h
Φsep
jkℓ′(waux(jkℓ))
ixy
:= (1 + γ) (µxwx
aux(ℓ), µywy
aux(ℓ)) −γ (µx¯zx, µy¯zy) ,
h
Φsep
jkℓ′(waux(jkℓ))
if∗g∗
:= (1 + γ)
  1
npj
∇f ∗
j

w
f∗
j
aux

· 1i=j

i∈[n]
,
 1
nqk
∇g∗
k

wf∗
kaux

· 1i=k

i∈[n]
!
−γ
  1
npj
∇f ∗
j

¯zf∗
j

· 1i=j

i∈[n]
,
 1
nqk
∇g∗
k

¯zg∗
k

· 1i=k

i∈[n]
!
,

Φbilin
jkℓ′ (waux(jkℓ))
xy :=

1
n
X
i∈[n]
wf∗
i +
1
npj
∆x(j), 1
n
X
i∈[n]
wg∗
i +
1
nqk
∆y(k)

,

Φbilin
jkℓ′ (waux(jkℓ))
f∗g∗
:=
 
−1
npj
wx
aux(ℓ) · 1i=j

i∈[n]
,

−1
nqk
wy
aux(ℓ) · 1i=k

i∈[n]
!
.
(2.59)
We also deﬁne the random "aggregate point" we will use in Proposition 5:
¯w(ℓ) := w +
 wx
aux(ℓ) −wx, wy
aux(ℓ) −wy, {∆x(j)}j∈[n], {∆y(k)}k∈[n]

.
(2.60)
Notably, ¯w(ℓ) depends only on the randomly sampled ℓ. We record the following useful observation
about our randomized operators (2.58), (2.59), in accordance with the ﬁrst condition in (2.56).
Lemma 25. Deﬁne {Φjkℓ, Φjkℓ′} : Z →Z∗as in (2.58), (2.59), and the random "aggregate point"
¯w(ℓ) as in (2.60). Then, for all u ∈Z, recalling the deﬁnition of Φ = Φmmfs-pd + γ(∇r −∇r(¯z))
from (2.55),
E [⟨Φjkℓ′(waux(jkℓ)), waux(jkℓ) −u⟩] = Eℓ∼r [⟨Φ( ¯w(ℓ)), ¯w(ℓ) −u⟩] .
We prove Lemma 25 in Appendix A.10.1. Finally, we give a complete implementation of our
method as pseudocode below in Algorithms 8 (the outer loop) and 9 (the inner loop). We also show
that it is a correct implementation in the following Lemma 26, which we prove in Appendix A.10.1.
Lemma 26. Lines 6 to 18 of Algorithm 9 implement Algorithm 5 on ({eΦ}, r) deﬁned in (2.58),
(2.59), (2.53), for σ iterations, and returns ¯wσ, following the deﬁnition (2.60). Each iteration s > 0
is implementable in O(1) gradient calls to some {fj, gk, hl}, and O(1) vector operations on X and
Y.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
78
Algorithm 8: Minimax-Finitesum-Solve(Fmmfs-reg, x0, y0): Minimax ﬁnite sum opti-
mization
1 Input: (2.50) satisfying Assumption 3, (x0, y0) ∈X × Y
2 Parameter(s): T ∈N
3 zx
0 ←x0, zy
0 ←y0, zfi
0 ←x0, zf∗
i
0 ←∇fi(x0), zgi
0 ←y0, zg∗
i
0 ←∇gi(y0) for all i ∈[n]
4 for 0 ≤t < T do
5
zt+1 ←Minimax-Finitesum-Inner(Fmmfs-reg, {zx
t, zy
t, {zfi
t }i∈[n], {zgi
t }i∈[n]})
6 Return: (zx
T , zy
T )
2.9.3
Inner loop convergence analysis
We give a convergence guarantee on Algorithm 9 for solving the VI in Φ := gtot + γ(∇r −∇r(¯z)).
In order to use Proposition 5 to solve our problem, we must prove strong monotonicity of Φ and
specify the parameters λ0, λ1 and ρ in (2.56); note that Lemma 25 handles the ﬁrst condition in
(2.56). To this end we give the following properties on Φ, eΦ as deﬁned in (2.58) and (2.59); proofs
of Lemmas 28 and 29 are deferred to Appendix A.10.2.
Strong monotonicity. We begin by proving strong monotonicity of Φ.
Lemma 27 (Strong monotonicity). Deﬁne Φ : Z →Z∗as in (2.55), and deﬁne r : Z →R as in
(2.53). Then Φ is (1 + γ)-strongly monotone with respect to r.
Proof. We decompose Φ(z) = (1 + γ)∇r(z) + Φbilin(z) + Φh(z) −γ∇r(¯z), using the deﬁnitions in
(2.54). By a similar argument as Lemma 11, we obtain the claim.
Expected relative Lipschitzness. We next provide bounds on the components of (2.56) correspond-
ing to Φsep and Φbilin, where we use the shorthand Φsep := (1 + γ)∇r −γ∇r(¯z) in the remainder of
this section. In particular, we provide a partial bound on the quantity λ0.
Lemma 28. Deﬁne {Φjkℓ, Φjkℓ′} : Z →Z∗as in (2.58), (2.59), and deﬁne r : Z →R as in (2.53).
Letting w+(jkℓℓ′) be ws+1 in Algorithm 9 if j, k, ℓ, ℓ′ were sampled in iteration s, deﬁning
Φfg
jkℓ(w) := Φsep
jkℓ(w) + Φbilin
jkℓ(w),
Φfg
jkℓ′(waux(jkℓ)) := Φsep
jkℓ′(waux(jkℓ)) + Φbilin
jkℓ′ (waux(jkℓ)),
we have
E
hD
Φfg
jkℓ′(waux(jkℓ)) −Φfg
jkℓ(w), waux(jkℓ) −w+(jkℓℓ′)
Ei
≤λfgE
h
V r
w (waux(jkℓ)) + V r
waux(jkℓ) (w+(jkℓℓ′))
i
,

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
79
Algorithm 9: Minimax-Finitesum-Inner(Fmmfs-reg, ¯zx, ¯zy, {¯zfi}i∈[n], {¯zgi}i∈[n]):
Mini-
max ﬁnite sum optimization subroutine
1 Input: (2.50) satisfying Assumption 3, ¯zx, {¯zfi}i∈[n] ∈X, ¯zy, {¯zgi}i∈[n] ∈Y
2 Parameter(s): γ ≥1, λ > 0, N, S ∈N
3 w0 ←¯z
4 for 0 ≤τ < N do
5
Sample 0 ≤σ < S uniformly at random
6
for 0 ≤s ≤σ do
7
Sample j, k, ℓ, ℓ′ ∈[n] independently according to p, q, r, r respectively deﬁned in
(2.57), and deﬁne
[Φsep]x := (1 + γ) µxwx
s −γµx¯zx,
[Φsep]y = (1 + γ) µywy
s −γµy¯zy,

Φbilinx :=
P
i∈[n] ∇fi
 wfis

n
,

Φbiliny :=
P
i∈[n] ∇gi (wgis )
n
,
Φx :=

Φh(w0)
x + ∇xhℓ(wx
s, wy
s) −∇xhℓ(wx
0, wy
0)
nrℓ
+ [Φsep]x +

Φbilinx ,
Φy :=

Φh(w0)
y −∇yhℓ(wx
s, wy
s) −∇yhℓ(wx
0, wy
0)
nrℓ
+ [Φsep]y +

Φbiliny
8
wx
s+1/2 ←wx
s −
1
λµx Φx, wy
s+1/2 ←wy
s −
1
λµy Φy
9
wfj
s+1/2 ←wfjs −
1
nλpj

(1 + γ) wfjs −γ¯zfj −wx
s

10
wgk
s+1/2 ←wfks −
1
nλpk ((1 + γ) wgk
s −γ¯zgk −wy
s)
11
Deﬁne
[Φsep]x := (1 + γ) µxwx
s+1/2 −γµx¯zx,
[Φsep]y := (1 + γ) µywy
s+1/2 −γµy¯zy,

Φbilinx :=
P
i∈[n] ∇fi
 wfis

n
+
∇fj

wfj
s+1/2

−∇fj

wfjs

npj
,

Φbiliny :=
P
i∈[n] ∇gi (wgis )
n
+
∇gk

wgk
s+1/2

−∇gk (wgk
s )
nqk
,
Φx :=

Φh(w0)
x +
∇xhℓ′(wx
s+1/2, wy
s+1/2) −∇xhℓ′(wx
0, wy
0)
nrℓ′
+ [Φsep]x +

Φbilinx ,
Φy :=

Φh(w0)
y −
∇yhℓ′(wx
s+1/2, wy
s+1/2) −∇yhℓ′(wx
0, wy
0)
nrℓ′
+ [Φsep]y +

Φbiliny
12
13
wx
s+1 ←wx
s −
1
λµx Φx, wy
s+1 ←wy
s −
1
λµy Φy
14
wfj
s+1 ←wfjs −
1
nλpj

(1 + γ) wfj
s+1/2 −γ¯zfj −wx
s+1/2

15
wgk
s+1 ←wfks −
1
nλpk

(1 + γ) wgk
s+1/2 −γ¯zgk −wy
s+1/2

16
¯wfi ←wfiσ −
1
nλpi
 (1 + γ) wfiσ −γ¯zfi −wx
σ

for each i ∈[n]
17
¯wgi ←wgiσ −
1
nλqi ((1 + γ) wgiσ −γ¯zgi −wy
σ) for each i ∈[n]
18
wxy
0 ←wxy
σ+1/2, wfi
0 ←¯wfi, wgi
0 ←¯wgi for all i ∈[n]
19 Return: (wx
0, wy
0, {∇fi(wfi
0)}i∈[n], {∇gi(wgi
0 )}i∈[n])

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
80
for
λfg = 2n(1 + γ) +
P
i∈[n]
p
Lx
i
√nµx
+
P
i∈[n]
p
Ly
i
√nµy
.
Partial variance bound. Finally, we provide bounds on the components of (2.56) corresponding
to Φh. Namely, we bound the quantity λ1, and complete the bound on λ0 within Proposition 5.
Lemma 29. Following notation of Lemma 28, and recalling the deﬁnition (2.61), for
λ1 := 32(λh)2,
where we deﬁne
λh := 1
n
X
i∈[n]
Λxx
i
µx +
Λxy
i
√µxµy + Λyy
i
µy

.
(2.61)
we have for any ρ > 0,
E

Φh
jkℓ′(waux(jkℓ)) −Φh
jkℓ(w), waux(jkℓ) −w+(jkℓℓ′)

≤

2λh + 1
ρ

E
h
V r
w (waux(jkℓ)) + V r
waux(jkℓ) (w+(jkℓℓ′))
i
+ ρλ1E
h
V r
w0(w⋆) + V r
¯
w(ℓ)(w⋆)
i
.
(2.62)
Combining the properties we prove above with Proposition 5, we obtain the following convergence
guarantee for each loop 0 ≤τ < N of Lines 6 to 18 in Algorithm 9.
Proposition 6. Consider a run of Lines 6 to 18 in Algorithm 9 initialized at w0 ∈Z, with
λ ←
 
2n(1 + γ) +
2 P
i∈[n]
p
Lx
i
√nµx
+
2 P
i∈[n]
p
Ly
i
√nµy
+ 2λh
!
+ 160(λh)2
γ
, S ←5λ
γ ,
(2.63)
where λh is deﬁned in (2.61). Letting ew be the new setting of w0 in Line 18 at the end of the run,
E [V r
e
w(w⋆)] ≤1
2V r
w0(w⋆),
where w⋆solves the VI in Φ (deﬁned in (2.55)).
2.9.4
Outer loop convergence analysis
We state the following convergence guarantee on our outer loop, Algorithm 8. The analysis is a some-
what technical modiﬁcation of the standard proximal point analysis for solving VIs [415], to handle
approximation error. As a result, we state the claim here and defer a proof to Appendix A.10.3.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
81
Proposition 7. Consider a single iteration 0 ≤t < T of Algorithm 8, and let z⋆is the saddle point
to Fmmfs-pd (deﬁned in (2.52)). Setting S as in (2.63) and
N := O (log (γλ)) ,
(2.64)
for an appropriately large constant in our implementation of Algorithm 9 and λ as in (2.63), we
have
EV r
zt+1(z⋆) ≤
4γ
1 + 4γ V r
zt(z⋆).
2.9.5
Main result
We now state and prove our main claim.
Theorem 9. Suppose Fmmfs in (2.50) satisﬁes Assumption 3, and has saddle point (x⋆, y⋆). Further,
suppose we have (x0, y0) ∈X ×Y such that GapFmmfs-reg(x0, y0) ≤ϵ0. Algorithm 8 using Algorithm 9
with λ as in (2.63) returns (x, y) ∈X × Y with EGapFmmfs-reg(x, y) ≤ϵ in Ntot iterations, using a
total of O(Ntot) gradient calls each to some fi, gi, or hi for i ∈[n], where
Ntot = O

κmmfs log (κmmfs) log
κmmfsϵ0
ϵ

,
for κmmfs := n +
1
√n
X
i∈[n]


s
Lx
i
µx +
s
Ly
i
µy + Λxx
i
µx +
Λxy
i
√µxµy + Λyy
i
µy

.
(2.65)
In particular, we use Ntot = NTS for
T = O

γ log
κfsϵ0
ϵ

, N = O (log (κmmfs)) , S = O

n + κmmfs
γ
+ (λh)2
γ2

, γ = λh
√n.
Proof. By Lemma 24, the point (x⋆, y⋆) is consistent between (2.50) and (2.52). The complexity of
each iteration follows from observation of Algorithm 8 and 9.
Next, by Proposition 6 and Proposition 7, and our choices of T, N, and S for appropriately large
constants, we obtain a point (x, y) ∈X × Y such that
EV r
(x,y)(z⋆) ≤ϵ
4

1
κmmfs
2
.
Here we used an analogous argument to Lemma 15 to bound the initial divergence. We then use a
similar bound as in Lemma 16 to obtain the desired duality gap bound.
We now revisit the problem (2.49).
We apply a generic reduction framework we develop in
Appendix A.8 to develop a solver for this problem under a relaxed version of Assumption 3, without
requiring strong convexity of individual summands.

CHAPTER 2. ACCELERATION VIA PRIMAL-DUAL EXTRAGRADIENT METHODS
82
Assumption 4. We assume the following about (2.49) for all i ∈[n].
1. fi is Lx
i-smooth, and gi is Ly
i-smooth.
2. h has the following blockwise-smoothness properties: for all u, v ∈X × Y,
∥∇xhi(u) −∇xhi(v)∥≤Λxx
i ∥ux −vx∥+ Λxy
i ∥uy −vy∥,
∥∇yhi(u) −∇yhi(v)∥≤Λxy
i ∥ux −vx∥+ Λyy
i ∥uy −vy∥.
(2.66)
For minimax ﬁnite sum optimization problems with this set of relaxed conditions, we conclude
with the following corollary of Theorem 9.
Corollary 5. Suppose the summands {fi, gi, hi}i∈[n] in (2.49) satisfy Assumption 4, and Fmmfs is
µx-strongly convex in x, µy-strongly convex in y, with saddle point (x⋆, y⋆). Further, suppose we
have (x0, y0) ∈X × Y such that GapFmmfs(x0, y0) ≤ϵ0. Algorithm 66 using Algorithm 8 and 9 to
implement steps returns (x, y) ∈X × Y with EGap(x, y) ≤ϵ in Ntot iterations, using a total of
O(Ntot) gradient calls each to some fi, gi, or hi for i ∈[n], where
Ntot = O

κmmfs log(κmmfs) log
κmmfsϵ0
ϵ

,
for κmmfs := n +
1
√n
X
i∈[n]


s
Lx
i
µx +
s
Ly
i
µy + Λxx
i
µx +
Λxy
i
√µxµy + Λyy
i
µy

.

Chapter 3
Stochastic Methods for Matrix
Games
This chapter is based on [111, 112], with Yair Carmon, Yujia Jin, and Aaron Sidford.
3.1
Introduction
Bilinear minimax problems of the form
min
x∈X max
y∈Y y⊤Ax where A ∈Rm×n,
(3.1)
are fundamental to machine learning, economics and theoretical computer science [393, 525, 157].
We focus on three important settings characterized by diﬀerent domain geometries. When X and
Y are probability simplices—which we call the ℓ1-ℓ1 setting—the problem (3.1) corresponds to a
zero-sum matrix game and also to a linear program in canonical feasibility form. The ℓ2-ℓ1 setting,
where X is a Euclidean ball and Y is a simplex, is useful for linear classiﬁcation (hard-margin support
vector machines) as well as problems in computational geometry [140]. Further, the ℓ2-ℓ2 setting,
where both X and Y are Euclidean balls (with general center), includes linear regression.
Many problems of practical interest are sparse, i.e., the number of nonzero elements in A, which
we denote by nnz, satisﬁes nnz ≪mn. Examples include: linear programs with constraints involving
few variables, linear classiﬁcation with 1-hot-encoded features, and linear systems that arise from
physical models with local interactions. The problem description size nnz plays a central role in
several runtime analyses of algorithms for solving the problem (3.1).
However, sparsity is not an entirely satisfactory measure of instance complexity: it is not con-
tinuous in the elements of A and consequently it cannot accurately reﬂect the simplicity of "nearly
83

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
84
sparse" instances with many small (but nonzero) elements. Measures of numerical sparsity, such as
the ℓ1 to ℓ2 norm ratio, can ﬁll this gap [260]. Indeed, many problems encountered in practice are
numerically sparse. Examples include: linear programming constraints of the form x1 ≥1
n
P
i xi,
linear classiﬁcation with neural network activations as features, and linear systems arising from
physical models with interaction whose strength decays with distance.
Existing bilinear minimax solvers do not exploit the numerical sparsity of A and their runtime
guarantees do not depend on it—the basic limitation of these methods is that they do not directly
access the large matrix entries, and instead sample the full columns and rows in which they occur.
To overcome this limitation, we propose methods that access A a single entry at a time, leverage
numerical sparsity by accessing larger coordinates more frequently, and enjoy runtime guarantees
that depend explicitly on numerical sparsity measures. For numerically sparse large-scale instances
our runtimes are substantially better than the previous state-of-the-art. Moreover, our runtimes
subsume the previous state-of-the-art dependence on nnz and rcs, the maximum number of nonzeros
in any row or column.
In addition to proposing algorithms with improved runtimes, we develop three techniques that
may be of broader interest. First, we design non-uniform sampling schemes that minimize regret
bounds; we use a general framework that uniﬁes the Euclidean and (local norms) simplex geometries,
possibly facilitating future extension. Next, we propose novel stochastic variance reduced methods
for saddle point problems, which are capable of interpolating between the runtimes achieved by
accelerated gradient methods and those achieved by sublinear (fully-)stochastic gradient methods
using a proximal optimization framework of [415].
Lastly, we build a data structure capable of
eﬃciently maintaining and sampling from multiplicative weights iterations (i.e. entropic projection)
with a ﬁxed dense component. This data structure overcomes limitations of existing techniques for
maintaining entropic projections and we believe it may prove eﬀective in other settings where such
projections appear.
3.1.1
Our results
Table 3.2 summarizes our runtime guarantees and puts them in the context of the best existing
results. We consider methods that output (expected) ϵ-accurate solutions of the saddle-point prob-
lem (3.1), namely a pair x, y satisfying
E

max
v∈Y v⊤Ax −min
u∈X y⊤Au

≤ϵ.
The algorithms in Table 3.2 are all iterative solvers for the general problem minx∈X maxy∈Y f(x, y),
specialized to f(x, y) = y⊤Ax. Each algorithm presents a diﬀerent tradeoﬀbetween per-iteration
complexity and the required iteration count, corresponding to the matrix access modality: exact
gradient methods compute matrix-vector products in each iteration, row-column stochastic gradient

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
85
methods sample a row and a column in each iteration, and our proposed coordinate methods take
this tradeoﬀto an extreme by sampling a single coordinate of the matrix per iteration.1 In addition,
variance reduction (VR) schemes combine both fast stochastic gradient computations and infrequent
exact gradient computations, maintaining the amortized per-iteration cost of the stochastic scheme
and reducing the total iteration count for suﬃciently small ϵ.
The runtimes in Table 3.2 depend on the numerical range of A through a matrix norm L that
changes with both the problem geometry and the type of matrix access; we use Lmv, Lrc and Lco to
denote the constants corresponding to matrix-vector products, row-column queries and coordinated
queries, respectively. Below, we describe these runtimes in detail. In the settings we study, our results
are the ﬁrst theoretical demonstration of runtime gains arising from sampling a single coordinate of
A at a time, as opposed to entire rows and columns.
Variance reduction for matrix games. In recent years, minimax formulations for neural network
training rose to prominence [244, 378], leading to intense interest in algorithms for solving large
scale minimax games [198, 237, 323, 161, 297, 391]. However, the algorithmic toolbox for minimax
optimization is not as complete as the one for minimization. Variance reduction, a technique for
improving stochastic gradient estimators by introducing control variates, stands as a case in point. A
multitude of variance reduction schemes exist for ﬁnite-sum minimization [300, 472, 18, 92, 217], and
their impact on complexity is well-understood [539]. In contrast, only a few works apply variance
reduction to ﬁnite-sum minimax problems [55, 484, 121, 394], and the potential gains from variance
reduction are not well-understood.
We take a step towards closing this gap by designing variance-reduced minimax game solvers
that oﬀer strict runtime improvements over non-stochastic gradient methods, similar to that of
optimal variance reduction methods for ﬁnite-sum minimization. Our starting point is Nemirovski's
"conceptual prox-method" [415] for solving minx∈X maxy∈Y f(x, y), where f : X × Y →R is convex
in x and concave in y. The method solves a sequence of subproblems parameterized by α > 0, each
of the form
ﬁnd x, y s.t. ∀x′, y′ ⟨∇xf(x, y), x −x′⟩−⟨∇yf(x, y), y −y′⟩≤αVx0(x′) + αVy0(y′)
(3.2)
for some (x0, y0) ∈X × Y, where Va(b) is a norm-suitable Bregman divergence from a to b: squared
Euclidean distance for ℓ2 and KL divergence for ℓ1. Combining each subproblem solution with an
extragradient step, the prox-method solves the original problem to ϵ accuracy by solving eO (α/ϵ)
1Interior point methods oﬀer an alternative tradeoﬀbetween iteration cost and iteration count: the number of
required iterations depends on 1/ϵ only logarithmically, but every iteration is costly, requiring a linear system solution
which at present takes time Ω(min{m, n}2). In the ℓ1-ℓ1 geometry, the best known runtimes for interior point methods
are e
O((nnz + min{m, n}2)
p
min{m, n}) [349], e
O(max{m, n}ω) [144], and e
O(mn + min{m, n}3) [517]. In this chapter
we are mainly interested in the large-scale low-accuracy regime with L/ϵ < min(m, n) where the runtimes described
in Table 3.2 are favorable (with the exception of [517] in certain cases). Our methods take only few passes over the
data, which are not the case for many interior-point methods [349, 144]. Also, our methods do not rely on a general
(ill-conditioned) linear system solver, which is a key ingredient in interior point methods.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
86
Lmv (matrix-vector)
Lrc (row-column)
Lco (coordinate)
ℓ1-ℓ1
maxi,j |Aij|
maxi,j |Aij|
max
n
maxi ∥Ai:∥2 , maxj ∥A:j∥2
o
ℓ2-ℓ1
maxi ∥Ai:∥2
maxi ∥Ai:∥2
max
n
maxi ∥Ai:∥1 , ∥A∥F
o†
ℓ2-ℓ2
∥A∥op
∥A∥F
max
nqP
i ∥Ai:∥2
1,
qP
j ∥A:j∥2
1
o
Table 3.1:
Dependence on A for diﬀerent methods in diﬀerent geometries.
Comments:
Ai:
and A:j denote the ith row and jth column of A, respectively.
Numerically sparse instances
satisfy Lco = O(Lrc).
† In the ℓ2-ℓ1 setting we can also achieve Lco = Lrc
√rcs and Lco =
max{maxi ∥Ai:∥1 ,
p
maxi ∥Ai:∥1 maxj ∥A:j∥1}.
Method
Iteration cost
Total runtime
Exact gradient [415, 420]
O(nnz)
eO

nnz · Lmv · ϵ−1
Row-column [253, 140, 55]
O(n + m)
eO

(m + n) · L2
rc · ϵ−2
Row-column VR [55, 111]
O(n + m)
eO

nnz +
p
nnz · (m + n) · Lrc · ϵ−1
Sparse row-col (folklore)
eO (rcs)
eO

rcs · L2
rc · ϵ−2
Sparse row-col VR (Appendix B.5)
eO (rcs)
eO

nnz + √nnz · rcs · Lrc · ϵ−1
Coordinate (Section 3.3.1)
eO (1)
eO

nnz + L2
co · ϵ−2
Coordinate VR (Section 3.3.2)
eO (1)
eO

nnz + √nnz · Lco · ϵ−1
Table 3.2: Comparison of iterative methods for bilinear problems.
Comments: nnz denotes the
number of nonzeros in A ∈Rm×n and rcs ≤max{m, n} denotes the maximum number of nonzeros
in any row and column of A. The quantities Lmv, Lco and Lrc depend on problem geometry (see
Table 3.1).
Task
Method
Runtime
MaxIB
[22]
eO
 mn + ρm√n · ϵ−1
Our method (Theorem 12)
eO
 nnz + ρ√nnz · rcs · ϵ−1†
MinEB
(when m ≥n)
[22]
eO
 mn + m√n · ϵ−1/2
Our method (Theorem 13)
eO
 nnz + √nnz · rcs · ϵ−1/2†
Regression
(A⊤A ⪰µI)
AGD [417]
eO

nnz · ∥A∥op
1
√µ

[260]
eO

nnz + nnz2/3 ·
 P
i∈[n] ∥A∥F · ∥Ai:∥1 · ∥Ai:∥2
1/3 1
√µ

Our method (Theorem 14)
eO

nnz + √nnz · max
npP
i ∥Ai:∥2
1,
qP
j ∥A:j∥2
1
o
1
√µ

Table 3.3: Comparison of complexity for diﬀerent applications. Comments: ρ denotes the radii ratio
of the minimum ball enclosing the rows of A and maximum ball inscribed in them. † For MaxIB and
MinEB, we refer the reader to Section 3.6.2 for a more ﬁne-grained runtime bound.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
87
subproblems.2 (Solving (3.2) with α = 0 is equivalent to to solving minx∈X maxy∈Y f(x, y).) A basic
form of our ﬁrst contribution is showing that if a stochastic unbiased gradient estimator ˜g satisﬁes
the "variance" bound
E ∥˜g(x, y) −∇f(x0, y0)∥2
∗≤L2 ∥x −x0∥2 + L2 ∥y −y0∥2
(3.3)
for some L > 0, then O(L2/α2) regularized stochastic mirror descent steps using ˜g solve (3.2) in
a suitable probabilistic sense. We call unbiased gradient estimators that satisfy (3.3) "centered."
We will shortly discuss the design of such centered gradient estimators in various application, which
constitutes another contribution of this chapter.
As per the discussion above, to achieve accuracy ϵ our algorithm solves eO (α/ϵ) subproblems.
Each subproblem takes O(nnz(A)) time for computing two exact gradients (one for variance reduction
and one for an extragradient step), plus an additional (m+n)L2/α2 time for the inner mirror descent
iterations, with L an appropriate Lipschitz constant of the problem. The total runtime is therefore
eO

nnz(A) + (m + n)L2
α2
α
ϵ

.
By setting α optimally to be max{ϵ, L
p
(m + n)/nnz(A)}, we obtain the runtime
eO

nnz(A) +
p
nnz(A) · (m + n) · L · ϵ−1
.
(3.4)
Some problem instances admit a structured form of sparsity where every row and column has at
most rcs nonzero elements. In these settings, we develop sampling distributions and data structures
to allow the above framework to leverage row-column sparsity, reducing the amortized per-iteration
cost from O (m + n) to eO (rcs), and yielding improved tradeoﬀs of problem parameters.
Coordinate stochastic gradient methods. In conjunction with our novel variance reduction frame-
work, we develop coordinate stochastic gradient estimators which allow per-iteration cost eO (1) and
iteration count eO
 n + m + ( Lco
ϵ )2
.
We deﬁne Lco in Table 3.1; for each domain geometry, the
quantity Lco
Lrc is a measure of the numerical sparsity of A, satisfying
1 ≤L2
co
L2rc
≤rcs.
Every iteration of our method requires sampling an element in a row or a column with probability
proportional to its entries. Assuming a matrix access model that allows such sampling in time eO (1)
2More precisely, the required number of subproblem solutions is at most Θ· α
ϵ , where Θ is a "domain size" parameter
that depends on X, Y, and the Bregman divergence V . In the ℓ1 and ℓ2 settings considered in this chapter, we have
the bound Θ ≤log(nm) and we use the e
O notation to suppress terms logarithmic in n and m. However, in other
settings—e.g., ℓ∞-ℓ1 games [483, 486]—making the parameter Θ scale logarithmically with the problem dimension is
far more diﬃcult.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
88
(similarly to [47, 487, 238]), the total runtime of our method is eO
 n + m + ( Lco
ϵ )2
. In this case, for
numerically sparse problems such that Lco = O(Lrc), the proposed coordinate methods outperform
row-column sampling by a factor of m + n. Moreover, the bound L2
co ≤L2
rc(m + n) implies that
our runtime is never worse than that of row-column methods. When only coordinate access to the
matrix A is initially available, we may implement the required sampling access via preprocessing in
time O(nnz). This changes the runtime to eO
 nnz + ( Lco
ϵ )2
, so that the comparison above holds only
when ( Lco
ϵ )2 = ˜Ω(nnz). In that regime, the variance reduction technique we describe below provides
even stronger guarantees.
Coordinate methods with variance reduction. By combining the two aforementioned contribu-
tions, we design a variance reduction algorithm with amortized per-iteration cost eO (1), required
iteration count of eO
 √nnz · Lco
ϵ

and total running time eO
 nnz + √nnz · Lco
ϵ

. In the numerically
sparse regime Lco = O(Lrc), our runtime improves on row-column VR by a factor of
p
nnz/(m + n),
and in general the bound Lco ≤Lrc
√m + n guarantees it is never worse. Since variance reduction
methods always require a single pass over the data to compute an exact gradient, this compari-
son holds regardless of the matrix access model. In the ℓ2-ℓ2 setting we note that for elementwise
non-negative matrices, Lco = max{∥A1∥2, ∥A⊤1∥2} ≤Lmv
√m + n, and consequently our method
outperforms exact gradient methods by a factor of
p
nnz/(m + n), even without any numerical or
spectral sparsity in A. Notably, this is the same factor of improvement that row-column VR achieves
over exact gradient methods in the ℓ1-ℓ1 and ℓ2-ℓ1 regimes.
Optimality of the constant Lco. For the ℓ1-ℓ1 and ℓ2-ℓ2 settings, we argue that the constant Lco in
Table 3.1 is optimal in the restricted sense that no alternative sampling distribution for coordinate
gradient estimation can have a better variance bound than Lco (a similar sense of optimality also
holds for Lrc in each geometry). In the ℓ2-ℓ1 setting, a diﬀerent sampling distribution produces an
improved (and optimal) constant max{maxi ∥Ai:∥1 , ∥|A|∥op}, where Ai: is the ith row of A, and
|A|ij = |Aij| is the elementwise absolute value of A. However, it is unclear how to eﬃciently sample
from this distribution.
Applications. We illustrate the implications of our results for two problems in computational
geometry, minimum enclosing ball (Min-EB) and maximum inscribed ball (Max-IB), as well as
linear regression. For Min-EB and Max-IB in the non-degenerate case m ≥n, we apply our ℓ2-ℓ1
results to obtain algorithms whose runtime bounds coincide with the state-of-the-art [22] for dense
problems, but can be signiﬁcantly better for sparse or row-column sparse instances.
For linear
regression we focus on accelerated linearly converging algorithms, i.e., those that ﬁnd x such that
∥Ax −b∥2 ≤ϵ in time proportional to µ−1
2 log 1
ϵ where µ is the smallest eigenvalue of A⊤A. Within
this class and in a number of settings, our reduced variance coordinate method oﬀers improvement
over the state-of-the-art: for instances where ∥Ai:∥1 = O(∥Ai:∥2) and ∥A:j∥1 = O(∥A:j∥2) for all i, j
it outperforms [260] by a factor of nnz1/6, and for elementwise nonnegative instances it outperforms
accelerated gradient descent by a factor of
p
nnz/(m + n). See Table 3.3 for a detailed runtime

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
89
comparison.
3.1.2
Our approach
We now provide a detailed overview of our algorithm design and analysis techniques, highlighting our
main technical insights. We focus on the ℓ1-ℓ1 geometry, since it showcases all of our developments.
Beyond our proposal of a new variance-reduced scheme for minimax optimization, our technical
contributions have two central themes:
1. Sampling schemes design.
The key to obtaining eﬃcient coordinate methods is carefully
choosing the sampling distribution. Here, local norms analysis of stochastic mirror descent [474]
on the one hand enables tight regret bounds, and on the other hand imposes an additional design
constraint since the stochastic estimators must be bounded for the analysis to apply. We achieve
estimators with improved variance bounds meeting this boundedness constraint by leveraging a
"clipping" operation introduced by [140]. Speciﬁcally, in the simplex geometry, we truncate large
coordinates of our estimators, and show that our method is robust to the resulting distortion.
2. Data structure design. Our goal is to perform iterations in eO (1) time, but our mirror descent
procedures call for updates that change m + n variables in each step. We resolve this tension via
data structures that implicitly maintain the iterates. Variance reduction poses a considerable
challenge here, because every reduced-variance stochastic gradient contains a dense component
that changes all coordinates in a complicated way. In particular, existing data structures cannot
eﬃciently compute the normalization factor necessary for projection to the simplex. We design
a data structure that overcomes this hurdle via Taylor expansions, coordinate binning, and a
binomial heap-like construction. The data structure computes approximate mirror projections,
and we modify the standard mirror descent analysis to show it is stable under the particular
structure of the resulting approximation errors.
At the intersection of these two themes is a novel sampling technique we call "sampling from the
sum," which addresses the same variance challenges as the "sampling from the diﬀerence" technique
of an earlier version of this work [111], but is more amenable to eﬃcient implementation with a data
structure.
Coordinate stochastic gradient method
Our algorithm is an instance of stochastic mirror descent [416], which in the ℓ1-ℓ1 setting produces
a sequence of iterates (x1, y1), (x2, y2), . . . according to
xt+1 = Π∆(xt ◦exp{−η˜gx(xt, yt)})
and yt+1 = Π∆(yt ◦exp{−η˜gy(xt, yt)}) ,
(3.5)

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
90
where Π∆(v) =
v
∥v∥1 is the projection onto the simplex (exp and log are applied to vectors elemen-
twise, and elementwise multiplication is denoted by ◦), η is a step size, and ˜gx, ˜gy are stochastic
gradient estimators for f(x, y) = y⊤Ax satisfying
E˜gx(x, y) = ∇xf(x, y) = A⊤y and E˜gy(x, y) = −∇yf(x, y) = −Ax.
We describe the computation and analysis of ˜gx; the treatment of ˜gy is analogous. To compute
˜gx(x, y), we sample i, j from a distribution p(x, y) on [m] × [n] and let
˜gx(x, y) =
yiAij
pij(x, y)ej,
(3.6)
where pij(x, y) denotes the probability of drawing i, j from p(x, y) and ej is the jth standard basis
vector—a simple calculation shows that E˜gx = A⊤y for any p. We ﬁrst design p(x, y) to guarantee
an eO
 ( Lco
ϵ )2
iteration complexity for ﬁnding an ϵ-accurate solution, and then brieﬂy touch on how
to compute the resulting iterations in eO (1) time.
Local norms-informed distribution design. The standard stochastic mirror descent analysis [416]
shows that if E ∥˜gx(x, y)∥2
∞≤L2 for all x, y (and similarly for ˜gy), taking η =
ϵ
L2 and a choice of
T = eO
 ( L
ϵ )2
suﬃces to ensure that the iterate average 1
T
PT
t=1(xt, yt) is an ϵ-accurate solution in
expectation. Unfortunately, this analysis demonstrably fails to yield suﬃciently tight bounds for
our coordinate estimator: there exist instances for which any distribution p produces L ≥nLrc. We
tighten the analysis using a local norms argument [?, cf.]Section 2.8]Shalev-Shwartz12, showing that
eO
 ( L
ϵ )2
iterations suﬃce whenever ∥η˜gx∥∞≤1 with probability 1 and for all x, y
E ∥˜gx(x, y)∥2
x ≤L2,
where
∥γ∥2
x =
X
j
xjγ2
j
is the local norm at x ∈X. We take
pij = yi
A2
ij
∥Ai:∥2
2
(3.7)
(recalling that x, y are both probability vectors). Substituting into (3.6) gives
E ∥˜gx(x, y)∥2
x =
X
i,j
y2
i A2
ijxj
pij
=
X
i,j
yi ∥Ai:∥2
2 xj =
X
i
yi ∥Ai:∥2
2 ≤max
i
∥Ai:∥2
2 ≤L2
co,
with Lco = max{maxi ∥Ai:∥2 , maxj ∥A:j∥2} as in Table 3.1.
While this is the desired bound on E ∥˜gx(x, y)∥2
x, the requirement ∥η˜gx∥∞≤1 does not hold when
A has suﬃciently small elements. We address this by clipping ˜g: we replace η˜gx with clip(η˜xx), where
[clip(v)]i := min{|vi|, 1} sign(vi),

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
91
the Euclidean projection to the unit box. The clipped gradient estimator clearly satisﬁes the desired
bounds on inﬁnity norm and local norm second moment, but is biased for the true gradient. Following
the analysis of [140], we account for the bias by relating it to the second moment via
| ⟨γ −clip(γ), x⟩| ≤∥γ∥2
x ,
which allows to absorb the eﬀect of the bias into existing terms in our error bounds. Putting together
these pieces yields the desired bound on the iteration count.
Eﬃcient implementation. Data structures for performing the update (3.5) and sampling from
the resulting iterates in eO (1) time are standard in the literature [?, e.g.,]]ShalevW16. We add to
these the somewhat non-standard ability to also eﬃciently track the running sum of the iterates. To
eﬃciently sample i, j ∼p according to (3.7) we ﬁrst use the data structure to sample i ∼y in eO (1)
time and then draw j ∈[n] with probability proportional to A2
ij in time O(1), via either O(nnz)
preprocessing or an appropriate assumption about the matrix access model. The "heavy lifting" of
our data structure design is dedicated for supporting variance reduction, which we describe in the
next section.
Sampling distributions beyond ℓ1-ℓ1. Table 3.4 lists the sampling distributions we develop for
the various problem geometries. Note that for the ℓ2-ℓ1 setting we give three diﬀerent distributions
for sampling the simplex block of the gradient (i.e., ˜gy); each distribution corresponds to a diﬀerent
parameter Lco (see comments following Table 3.1).
The distribution qij ∝√yi |Aijxj| yields a
stronger bound L in the ℓ2-ℓ1 setting, but we do not know how to eﬃciently sample from it.
Coordinate variance reduction
To accelerate the stochastic coordinate method we apply our variance reduction framework. This
framework operates in α
ϵ epochs, where α is a design parameter that trades between full and stochas-
tic gradient computations. Each epoch consists of three parts: (i) computing the exact gradient at
a reference point (x0, y0), (ii) performing T iterations of regularized stochastic mirror descent to
produce the sequence (x1, y1), . . . , (xT , yT ) and (iii) taking an extra-gradient step from the average
of the iterates in (ii). Setting κ = 1/(1 + ηα/2), the iterates xt follow the recursion
xt+1 = Π∆
 xκ
t ◦x1−κ
0
◦exp{−ηκ[gx
0 + ˜δx(xt, yt)]}

, where Π∆(v) =
v
∥v∥1
,
(3.8)
and gx
0 = A⊤y0 is the exact gradient at the reference point, and ˜δx is a stochastic gradient diﬀerence
estimator satisfying
E˜δx(x, y) = ∇xf(x, y) −∇xf(x0, y0) = A⊤(y −y0).

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
92
The iteration for yt is similar. As discussed earlier, we show that if ˜δx satisﬁes
E∥˜δx(x, y)∥2
∞≤L2 
∥x −x0∥2
1 + ∥y −y0∥2
1

∀x, y
(3.9)
and a similar bound holds on E∥˜δy(x, y)∥2
∞, then T = O( L2
α2 ) iterations per epoch with step size
η =
α
L2 suﬃce for the overall algorithm to return a point with expected error below ϵ.
We would like to design a coordinate-based estimator ˜δ such that the bound (3.9) holds for
L = Lco as in Table 3.1 and each iteration (3.8) takes eO (1) time. Since every epoch also requires
O(nnz) time for matrix-vector product (exact gradient) computations, the overall runtime would be
eO((nnz + L2
co
α2 ) · α
ϵ ). Choosing α = Lco/√nnz then gives the desired runtime eO(nnz + √nnz · Lco
ϵ ).
Distribution design (sampling from the diﬀerence). We start with a straightforward adaptation
of the general estimator form (3.6). To compute ˜δx(x, y), we sample i, j ∼p, where p may depend
on x, x0, y and y0, and let
˜δx(x, y) = (yi −[y0]i)Aij
pij
ej,
(3.10)
where ej is the jth standard basis vector. As in the previous section, we ﬁnd that the require-
ment (3.9) is too stringent for coordinate-based estimators. Here too, we address this challenge
with a local norms argument and clipping of the diﬀerence estimate. Using the "sampling from the
diﬀerence" technique from an earlier version of this work [111], we arrive at
pij = |yi −[y0]i|
∥y −y0∥1
·
A2
ij
∥Ai:∥2
2
.
(3.11)
This distribution satisﬁes the local norm relaxation of (3.9) with L2 = L2
co.
Data structure design.
Eﬃciently computing (3.8) is signiﬁcantly more challenging than its
counterpart (3.5). To clarify the diﬃculty and describe our solution, we write
xt = Π∆(ˆxt) = ˆxt/ ∥ˆxt∥1
and break the recursion for the unnormalized iterates ˆxt into two steps
ˆx′
t = ˆxκ
t ◦exp{v}, and
(3.12)
ˆxt+1 = ˆx′
t ◦exp{st},
(3.13)
where v = (1 −κ) log x0 −ηκgx
0 is a ﬁxed dense vector, and st = −η˜δx(xt, yt) is a varying 1-sparse
vector. The key task of the data structures is maintaining the normalization factor ∥ˆxt∥1 in near-
constant time. Standard data structures do not suﬃce because they lack support for the dense
step (3.12).
Our high-level strategy is to handle the two steps (3.12) and (3.13) separately. To handle the

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
93
dense step (3.12), we propose the data structure ScaleMaintainer that eﬃciently approximates
∥ˆxt∥1 in the "homogeneous" case of no sparse updates (i.e. st = 0 for all t). We then add support
for the sparse step (3.13) using a binomial heap-like construction involving O(log n) instances of
ScaleMaintainer.
The ScaleMaintainer data structure. When st = 0 for all t the iterates ˆxt admit closed forms
ˆxt+τ = ˆxκτ
t
◦exp
(
v
τ−1
X
t′=0
κt′
)
= ˆxκτ
t
◦exp
n1 −κτ
1 −κ v
o
= ˆxt ◦exp {[1 −κτ]¯v} ,
where ¯v =
v
1−κ −log xt.
Consequently, we design ScaleMaintainer to take as initialization ¯n-
dimensional vectors ¯x ∈R¯n
≥0, and ¯v ∈R¯n and provide approximations of the normalization factor
Zτ(¯x, ¯v) = ∥¯x ◦exp{(1 −κτ)¯v})∥1
(3.14)
for arbitrary values of τ ≥1.
We show how to implement each query of Zτ(¯x, ¯v) in amortized
time eO (1). The data structure also supports initialization in time eO (¯n) and deletions (i.e., setting
elements of ¯x to zero) in amortized time eO (1).
To eﬃciently approximate the quantity Zτ(¯x, ¯v) we replace the exponential with its order p =
O(log n) Taylor series. That is, we would like to write
Zτ(¯x, ¯v) =
X
i∈[¯n]
[¯x]ie(1−κτ )[¯v]i ≈
X
i∈[¯n]
[¯x]i
p
X
q=0
1
q!(1 −κτ)q[¯v]q
i =
p
X
q=0
(1 −κτ)q
q!
⟨¯x, ¯vq⟩.
The approximation Pp
q=0
(1−κτ )q
q!
⟨¯x, ¯vq⟩is cheap to compute, since for every τ it is a linear combina-
tion of the p = eO (1) numbers {⟨¯x, ¯vq⟩}q∈[p] which we can compute once at initialization. However,
the Taylor series approximation has low multiplicative error only when |(1 −κτ)[¯v]i| = O(p), which
may fail to hold, as we may have ∥¯v∥∞= poly(n) in general. To handle this, suppose that for a ﬁxed
τ we have an oﬀset µ ∈R and "active set" A ⊆[¯n] such that the following conditions hold for a
threshold R = O(p): (a) the Taylor approximation is valid in A, e.g. we have |(1−κτ)(¯vi −µ)| ≤2R
for all i ∈A, (b) entries outside A are small; (1 −κτ)[¯vi −µ] ≤−R for all i /∈A, and (c) at least
one entry in the active set is large; (1 −κτ)[¯vi −µ] ≥0 for some i ∈A. Under these conditions, the
entries in Ac are negligibly small and we can truncate them, resulting in the approximation
e(1−κτ )µ
" p
X
q=0
(1 −κτ)q
q!
⟨¯x, (¯v −µ)q⟩A + e−R ⟨¯x, 1⟩Ac
#
,
which we show approximates Zτ(¯x, ¯v) to within eO(R+log n)−Ω(p) multiplicative error, where we used
⟨a, b⟩S := P
i∈S aibi; here, we also require that log maxi ¯xi
mini ¯xi = O(R), which we guarantee when
choosing the initial ¯x.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
94
The challenge then becomes eﬃciently mapping any τ to {⟨¯x, (¯v −µ)q⟩A}q∈[p] for suitable µ
and A. We address this by jointly bucketing τ and ¯v. Speciﬁcally, we map τ into a bucket index
k = ⌊log2
1−κτ
1−κ ⌋, pick µ to be the largest integer multiple of R/((1−κ)2k) such that µ ≤maxi ¯vi, and
set A = {i | |(1 −κ)2k(¯vi −µ)| ≤R}. Since k ≤kmax = ⌊log2
1
1−κ⌋= O(log n), we argue that pre-
computing ⟨¯x, (¯v −µ)q⟩A for every possible resulting µ and A takes at most O(¯np log
1
1−κ) = eO (¯n)
time, which we can charge to initialization. We further show how to support deletions in eO (1) time
by carefully manipulating the precomputed quantities.
Supporting sparse updates.
Building on ScaleMaintainer, we design the data structure
ApproxExpMaintainer that (approximately) implements the entire mirror descent step (3.8) in time
eO (1).3 The data structure maintains vectors ¯x ∈∆n and ¯v ∈Rn and K = ⌈log2(n + 1)⌉instances of
ScaleMaintainer denoted {ScaleMaintainerk}k∈[K]. The kth instance tracks a coordinate subset
Sk ⊆[n] such that {Sk}k∈[K] partitions [n], and has initial data [¯x]Sk and [¯v]Sk. We let τk ≥0
denote the "time index" parameter of the kth instance. The data structure satisﬁes two invariants;
ﬁrst, the unnormalized iterate ˆx satisﬁes
[ˆx]Sk = [¯x ◦exp{(1 −κτk)¯v}]Sk , for all k ∈[K].
(3.15)
Second, the partition satisﬁes
|Sk| ≤2k −1 for all k ∈[K],
(3.16)
where at initialization we let SK = [n] and Sk = ∅for k < K, ¯x = x0, ¯v =
v
1−κ −log x0 and τK = 0.
The invariant (3.15) allows us to eﬃciently (in time eO (K) = eO (1)) query coordinates of xt =
ˆxt/ ∥ˆxt∥1, since ScaleMaintainer allows us to approximate ∥ˆxt∥1 = P
k∈[K] Zτk([¯x]Sk, [¯v]Sk) with
Z as deﬁned in (3.14). To implement the dense step (3.12), we simply increment τk ←τk + 1 for
every k. Let j be the nonzero coordinate of st in the sparse step (3.13), and let k ∈[K] be such
that j ∈Sk. To implement (3.13), we delete coordinate j from ScaleMaintainerk, and create a
singleton instance ScaleMaintainer0 maintaining S0 = {j} with initial data [¯x]S0 = est ˆxj, [¯v]S0 =
vj/(1−κ)−log(est ˆxj) and τ0 = 0. Going from k = 1 to k = K, we merge ScaleMaintainerk−1 into
ScaleMaintainerk until the invariant (3.16) holds again. For example, if before the sparse step we
have |S1| = 1, |S2| = 3 and |S3| = 2, we will perform 3 consecutive merges, so that afterwards we
have |S1| = |S2| = 0 and |S3| = 7.
To merge two ScaleMaintainerk−1 into ScaleMaintainerk, we let S′
k = Sk−1∪Sk and initialize
a new ScaleMaintainer instance with [¯x]S′
k = [ˆx]S′
k,4 [¯v]S′
k = [v]S′
k/(1 −κ) −log[ˆx]S′
k and τk = 0;
this takes eO (|S′
k|) = eO
 2k
time due to the invariant (3.16). Noting that a merge at level k can
3The data structures ApproxExpMaintainer and ScaleMaintainer structure support two additional operations
necessary for our algorithm: eﬃcient approximate sampling from xt and maintenance of a running sum of ˆxτ. Given
the normalization constant approximation, the implementation of these operations is fairly straightforward, so we do
not discuss them in the introduction.
4More precisely, for every j ∈S′
k we set ¯xj = ˆxj + ε maxi∈S′
k ˆxi, where ε is a small padding constant that ensures
the bounded multiplicative range necessary for correct operation of ScaleMaintainer.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
95
only happen once in every Ω(2k) updates, we conclude that the amortized cost of merges at each
level is eO (1), and (since K = eO (1)), so is the cost of the sparse update.
Back to distribution design (sampling from the sum). Our data structure enables us to compute
the iteration (3.8) and query coordinates of the iterates xt and yt in eO (1) amortized time. However,
we cannot compute ˜δx using the distribution (3.11) because we do not have an eﬃcient way of
sampling from |yt −y0|; Taylor approximation techniques are not eﬀective for approximating the
absolute value because it is not smooth. To overcome this ﬁnal barrier, we introduce a new design
which we call "sampling from the sum,"
pij(x, y) =
1
3yi + 2
3[y0]i

·
A2
ij
∥Ai:∥2
2
.
(3.17)
Sampling from the modiﬁed distribution is simple, as our data structure allows us to sample from yt.
Moreover, we show that the distribution (3.17) satisﬁes a relaxed version of (3.9) where the LHS is
replaced by a local norm as before, and the RHS is replaced by L2(Vx0(xt) + Vy0(yt)), where Vx(x′)
is the KL divergence between x and x′. In Table 3.5 we list the sampling distributions we design for
variance reduction in the diﬀerent domain geometries.
3.1.3
Related work
Variance reduction for matrix games. Matrix games, the canonical form of discrete zero-sum games,
have long been studied in economics [426]. It is well-known that the classical mirror descent (i.e. no-
regret) method yields an algorithm with running time eO
 nnz(A)L2ϵ−2
[414]. Subsequent work [253,
415, 420, 140] improve this runtime as described above. Our work builds on the extragradient scheme
of [415] as well as the gradient estimation and clipping technique of [140].
[432] apply standard variance reduction [300] to bilinear ℓ2-ℓ2 games by sampling elements pro-
portional to squared matrix entries. Using proximal-point acceleration they obtain a runtime of
eO

nnz(A) + ∥A∥F
p
nnz(A) max{m, n}ϵ−1 log 1
ϵ

, a rate we recover using our algorithm. However,
in this setting the mirror-prox method has runtime eO

∥A∥op nnz(A)ϵ−1
, which may be better
than the result of [432] by a factor of
p
mn/nnz(A) due to the discrepancy in the norm of A. Naive
application of [432] to ℓ1 domains results in even greater potential losses. [484] extend the method
of [432] to smooth functions using general Bregman divergences, but their extension is unaccelerated
and appears limited to a ϵ−2 rate.
[121] propose a variance-reduced extragradient method with applications to generative adversarial
training. In contrast to our algorithm, which performs extragadient steps in the outer loop, the
method of [121] performs stochastic extragradient steps in the inner loop, using ﬁnite-sum variance
reduction as in [300]. [121] analyze their method in the convex-concave setting, showing improved
stability over direct application of the extragradient method to noisy gradients.
However, their
complexity guarantees are worse than those of linear-time methods. Following up on [121], [394]

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
96
Setting
pij
qij
ℓ1-ℓ1
yi ·
A2
ij
∥Ai:∥2
2
xj ·
A2
ij
∥A:j∥2
2
ℓ2-ℓ1
yi · |Aij|
∥Ai:∥1
A2
ij
∥A∥2
F
ℓ2-ℓ1
yi · |Aij|
∥Ai:∥1
∝x2
j · 1Aij̸=0
ℓ2-ℓ1
yi · |Aij|
∥Ai:∥1
Aij · x2
j
P
k∈[n] ∥A:k∥1 · x2
k
ℓ2-ℓ2
∥Ai:∥2
1
P
k∈[m] ∥Ak:∥2
1
· |Aij|
∥Ai:∥1
∥A:j∥2
1
P
k∈[n] ∥A:k∥2
1
· |Aij|
∥A:j∥1
ℓ2-ℓ2
yi2
∥y∥2
2
· |Aij|
∥Ai:∥1
xj2
∥x∥2
2
· |Aij|
∥A:j∥1
Table 3.4: The distributions p, q used in our coordinate gradient estimator. Comments: The estimator is
of the form ˜g(x, y) =
  1
pij yiAij · ej, −1
qlk Alkxk · el

where i, j ∼p and l, k ∼q.
Setting
pij
qij
ℓ1-ℓ1
yi + 2[y0]i
3
·
A2
ij
∥Ai:∥2
2
xj + 2[x0]j
3
·
A2
ij
∥A:j∥2
2
ℓ2-ℓ1
yi + 2[y0]i
3
· |Aij|
∥Ai:∥1
A2
ij
∥A∥2
F
ℓ2-ℓ1
yi + 2[y0]i
3
· |Aij|
∥Ai:∥1
∝[x −x0]2
j · 1Aij̸=0
ℓ2-ℓ1
yi + 2[y0]i
3
· |Aij|
∥Ai:∥1
|Aij| · [x −x0]2
j
P
k∈[n] ∥A:k∥1 · [x −x0]2
k
ℓ2-ℓ2
∥Ai:∥2
1
P
k∈[m] ∥Ak:∥2
1
· |Aij|
∥Ai:∥1
∥A:j∥2
1
P
k∈[n] ∥A:k∥2
1
· |Aij|
∥A:j∥1
ℓ2-ℓ2
[y −y0]2
i
∥y −y0∥2
2
· |Aij|
∥Ai:∥1
[x −x0]2
j
∥x −x0∥2
2
· |Aij|
∥A:j∥1
Table 3.5: The distributions p, q used for our reduced variance coordinate gradient estimator. Comments:
The estimator is of the form ˜g(x, y) =
 A⊤y + 1
pij (yi −y0,i)Aij · ej, −Ax −1
qlk Alk(xk −x0,k) · el

where
i, j ∼p and l, k ∼q and x0, y0 is a reference point.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
97
propose to reduce the variance of the stochastic extragradient method by using the same stochastic
sample for both the gradient and extragradient steps. In the Euclidean strongly convex case, they
show a convergence guarantee with a relaxed variance assumption, and in the noiseless full-rank
bilinear case they recover the guarantees of [398]. In the general convex case, however, they only
show an ϵ−2 rate of convergence.
Coordinate methods for matrix games. Updating a single coordinate at a time—or more broadly
computing only a single coordinate of the gradient at every iteration—is a well-studied and suc-
cessful technique in optimization [540]. Selecting coordinates at random is key to obtaining strong
performance guarantees: [498] show this for linear regression, [475] show this for ℓ1 regularized linear
models, and [423] shows this for general smooth minimization. Later works [347, 553, 425] propose
accelerated coordinate methods. These works share two common themes: selecting the gradient
coordinate from a non-uniform distribution (see also [455]), and augmenting the 1-sparse stochastic
gradient with a dense momentum term. These techniques play important roles in our development
as well.
To reap the full beneﬁts of coordinate methods, iterations must be very cheap, ideally taking
near-constant time. However, most coordinate methods require super-constant time, typically in the
form of a vector-vector computation. Even works that consider coordinate methods in a primal-dual
context [477, 22, 547, 407, 476] perform the coordinate updates only on the dual variable and require
a vector-vector product (or more generally a component gradient computation) at every iteration.
A notable exception is the work of [531, 532] which develops a primal-dual stochastic coordinate
method for solving Markov decision processes, essentially viewing them as ℓ∞-ℓ1 bilinear saddle-
point problems. Using a tree-based ℓ1 sampler data structure similar to the ℓ1 sampler we use for
simplex domains for the sublinear case, the method allows for eO (1) iterations and a potentially
sublinear runtime scaling as ϵ−2. [499] also consider bilinear saddle-point problems and variance
reduction. Unlike our work, they assume a separable domain, use uniform sampling, and do not
accelerate their variance reduction scheme with extra-gradient steps. The separable domain makes
attaining constant iteration cost time much simpler, since there is no longer a normalization factor
to track, but it also rules out applications to the simplex domain. While [499] report promising
empirical results, their theoretical guarantees do not improve upon prior work.
Our work develops coordinate methods with eO (1) iteration cost for new types of problems.
Furthermore, it maintains the iteration eﬃciency even in the presence of dense components arising
from the update, thus allowing for acceleration via an extra-gradient scheme.
Data structures for optimization. Performing iterations in time that is asymptotically smaller
than the number of variables updated at every iteration forces us to carry out the updates implicitly
using data structures; several prior works employ data structures for exactly the same reason. One
of the most similar examples comes from [347], who design a data structure for an accelerated
coordinate method in Euclidean geometry. In our terminology, their data structure allows performing

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
98
each iteration in time O(rcs) while implicitly updating variables of size O(n). [203] design a data
structure based on balanced search trees that supports eﬃcient Euclidean projection to the ℓ1 ball
of vector of the form u + s where u is in the ℓ1 ball and s is sparse. They apply it in a stochastic
gradient method for learning ℓ1 regularized linear classiﬁer with sparse features. Among the many
applications of this data structure, [407] adapt it to eﬃciently compute Euclidean projections into
the intersection of the simplex and a χ2 ball for 1-sparse updates. [476] and [531, 532], among
others, use binary tree data structures to perform multiplicative weights projection to the simplex
and sampling from the iterates.
A recent work of [486] develops a data structure which is similar to ApproxExpMaintainer, for
updates arising from a primal-dual method to eﬃciently solve ℓ∞regression. Their data structure
was also designed to handle updates to a simplex variable which summed a structured dense compo-
nent and a sparse component. However, the data structure design of that work speciﬁcally exploited
the structure of the maximum ﬂow problem in a number of ways, such as bounding the sizes of
the update components and relating these bounds to how often the entire data structure should be
restarted. Our data structure can handle a broader range of structured updates to simplex variables
and has a much more ﬂexible interface, which is crucial to the development of our variance-reduced
methods as well as our applications.
Another notable use of data structures in optimization appears in second order methods, where
a long line of work uses them to eﬃciently solve sequences of linear systems and approximately
compute iterates [316, 27, 349, 144, 352, 515, 517]. Finally, several works on low rank optimization
make use of sketches to eﬃciently represent their iterates and solutions [141, 543].
Numerical sparsity.
Measures of numerical sparsity, such as the ℓ2/ℓ∞or ℓ1/ℓ2 ratios, are
continuous and dimensionless relaxations of the ℓ0 norm. The stable rank of a matrix A measures
the numerical sparsity of its singular values (speciﬁcally, their squared ℓ2/ℓ∞ratio) [146].
For linear regression, stochastic methods generally outperform exact gradient methods only when
A is has low stable rank, cf. discussion in [111, Section 4.3], i.e., numerically sparse singular values.
In recent work, [260] develop algorithms for linear regression and eigenvector problems for matrices
with numerically sparse entries (as opposed to singular values). Our work further broadens the scope
of matrix problems for which we can beneﬁt from numerical sparsity. Moreover, our results have
implications for regression as well, improving on [260] in certain numerically sparse regimes.
In recent work by [48], the authors develop primal-dual sublinear methods for ℓ1-regularized linear
multi-class classiﬁcation (bilinear games in ℓ1-ℓ∞geometry), and obtain complexity improvements
depending on the numerical sparsity of the problem. Similarly to our work, careful design of the
sampling distribution plays a central role in [48]. They also develop a data structure that allows
iteration cost independent of the number of classes. However, unlike our work, [48] rely on sampling
entire rows and columns, have iteration costs linear in n + m, and do not utilize variance reduction.
We believe that our techniques can yield improvements in their setting.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
99
3.1.4
Chapter organization
In Section 3.2, we set up our terminology, notation, the interfaces of our data structures, and the
diﬀerent matrix access models we consider. In Section 3.3 we develop our algorithmic framework: we
present coordinate stochastic gradient methods in Section 3.3.1 and their reduced variance counter-
parts in Section 3.3.2. In Section 3.4 we apply both methods to solving ℓ1-ℓ1 matrix games; we show
how to implement the method using our data structures and analyze the runtime. In Section 3.5,
we discuss in detail the implementation and analysis of our data structures. Finally, in Section 3.6
we specialize our results to obtain algorithms for minimum enclosing ball and maximum inscribed
ball problems as well as linear regression. Many proof details as well as our algorithms for other
domain setups, i.e. ℓ2-ℓ1 and ℓ2-ℓ2 are deferred to the appendix.
For brevity in the main body of this chapter, we provide our "basic" instantiation of our variance
reduction framework (specialized with row-column sampling estimators, as opposed to coordinate
sampling estimators) in Appendix B.5.
3.2
Preliminaries
In Section 3.2.1, we abstract the properties of the diﬀerent domains we handle into a general notion
of a "local norm" setup under which we develop our results. In Section 3.2.2, we give the deﬁnition
and optimality criterion of the bilinear saddle-point problem we study. In Section 3.2.3, we give
the matrix access models used in the algorithms we design. In Section 3.2.4, we summarize the
interfaces and complexity of the data structures we design, deferring their detailed implementations
to Section 3.5.
3.2.1
Local norm setups
The analyses of our algorithms cater to the geometric of each speciﬁc domain.
To express our
results generically, for each pair of domains Z = X × Y, we deﬁne an associated "local norm setup",
which contains various data tailored for our analyses.
While initially this notation may appear
complicated or cumbersome, later it helps avoid redundancy in the paper. Further, it clariﬁes the
structure necessary to generalize our methods to additional domain geometries.
Deﬁnition 2. A local norm setup is the quintuplet (Z, ∥·∥·, r, Θ, clip) , where
1. Z is a compact and convex subset of Z∗:= Rn × Rm.
2. ∥·∥· is a local norm: for every z ∈Z, the function ∥·∥z : Z∗→R≥0 is a norm on Z∗.
3. r : Z →R is a convex distance generating function: its induced Bregman divergence
Vz(z′) := r(z′) −r(z) −⟨∇r(z), z′ −z⟩.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
100
ℓ1-ℓ1
ℓ2-ℓ1
ℓ2-ℓ2
X
∆n
Bn
Bn
Y
∆m
∆m
Bm
∥δ∥z
qP
k∈[n+m][z]k[δ]2
k
q
∥δx∥2
2 + P
i∈[m][zy]i[δy]2
i
∥δ∥2
r
P
k∈[n+m][z]k log[z]k
1
2 ∥zx∥2
2 + P
i∈[m][zy]i log[zy]i
1
2 ∥z∥2
2
Θ
log(mn)
1
2 + log(m)
1
clip(δ)
sign(δ) ◦min{1, |δ|}
(δx, sign(δy) ◦min{1, |δy|})
δ
Table 3.6: Local norm setups. Comments: In each case, Z = X × Y, ∆n is the probability simplex
{x | x ∈Rn
≥0, 1⊤
n x = 1}, Bn is the Euclidean ball {x | x ∈Rn, ∥x∥2 ≤1}, the operations sign, min,
and |·| are performed entrywise on a vector, and ◦stands for the entrywise product between vectors.
satisﬁes
⟨γ, z −z′⟩−Vz(z′) ≤1
2 ∥γ∥2
∗:= 1
2 max
s∈Z ∥γ∥2
s
for all z, z′ ∈Z and γ ∈Z∗.
(3.18)
4. Θ = maxz,z′∈Z{r(z)−r(z′)} is the range of r. For z∗∈argminz∈Zr(z) we have Θ is an upper
bound on the range of Vz∗(z) ≤Θ for all z ∈Z.
5. clip : Z∗→Z∗is a mapping that enforces a local version of (3.18):
| ⟨clip(γ), z −z′⟩| −Vz(z′) ≤∥γ∥2
z
for all z, z′ ∈Z and γ ∈Z∗,
(3.19)
and satisﬁes the distortion guarantee
| ⟨γ −clip(γ), z⟩| ≤∥γ∥2
z
for all z ∈Z and γ ∈Z∗.
(3.20)
Table 3.6 summarizes the three local norm setups we consider. Throughout the paper,
for a vector z ∈X × Y, we denote its X and Y blocks by zx and zy.
In addition, we write coordinate i of any vector v as [v]i.
Proposition 8. The quintuplets (Z, ∥·∥·, r, Θ, clip) in Table 3.6 satisfy the local norm setup
requirements in Deﬁnition 2.
While Proposition 8 is not new, for completeness and compatibility with our notation we prove it
in Appendix B.1.
In each local norm setup, we slightly overload notation and use ∥·∥(without a subscript) to

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
101
denote the dual norm of ∥·∥∗, i.e., ∥η∥:= maxδ:∥δ∥∗≤1 δ⊤η. In each domain geometry ∥·∥and ∥·∥∗
are as follows:
∥η∥=
q
∥ηx∥2
1 + ∥ηy∥2
1
∥δ∥∗=
q
∥δx∥2
∞+ ∥δy∥2
∞
for ℓ1-ℓ1
∥η∥=
q
∥ηx∥2
2 + ∥ηy∥2
1
∥δ∥∗=
q
∥δx∥2
2 + ∥δy∥2
∞
for ℓ2-ℓ1
∥η∥= ∥η∥2
∥δ∥∗= ∥δ∥2
for ℓ2-ℓ2 .
(3.21)
3.2.2
The problem and optimality criterion
Throughout, we consider the bilinear saddle point problem
min
x∈X max
y∈Y f(x, y), where f(x, y) := y⊤Ax + b⊤x −c⊤y, for A ∈Rm×n, b ∈Rn and c ∈Rm.
(3.22)
We will always assume that every row and column of A has at least one nonzero entry (else removing
said row or column does not aﬀect the problem value), so that the number of nonzeros nnz is at least
m+n−1. To simplify the exposition of the ℓ1-ℓ1 and ℓ2-ℓ1 setups we will assume b = 0n and c = 0m
as is standard in the literature. Adding linear terms to these setups is fairly straightforward and does
not aﬀect the complexity (up to logarithmic factors) of our designed algorithms using data structures
designed in this chapter (speciﬁcally ApproxExpMaintainer in Section 3.2.4); see Section 3.6 for an
example. The gradient mapping associated with (3.22) for z = (zx, zy) ∈Z = X × Y is
g(z) := (∇xf(z), −∇yf(z)) = (A⊤zy + b, −Azx + c).
(3.23)
The mapping g is continuous and monotone, where we call g monotone if and only if
⟨g(z′) −g(z), z′ −z⟩≥0, ∀z, z′ ∈Z.
This holds due to the convexity-concavity (indeed, bilinearity) of function f. Our goal is to de-
sign randomized algorithms for ﬁnding an (expected) ϵ-accurate saddle point z ∈Z such that, in
expectation,
EGap(z) := E

max
y′∈Y f(zx, y′) −min
x′∈X f(x′, zy)

≤ϵ.
(3.24)
In order to do so, we aim to ﬁnd a sequence z1, z2, . . . , zK with (expected) low average regret,
i.e., such that E maxu∈Z
n
1
K
PK
k=1 ⟨g(zk), zk −u⟩
o
≤ϵ. Due to bilinearity of f we have
EGap
 
1
K
K
X
k=1
zk
!
= E max
u∈Z
(
1
K
K
X
k=1
⟨g(zk), zk −u⟩
)
≤ϵ.
Finally, we make the explicit assumption that whenever we are discussing an algorithm in this

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
102
chapter with a simplex domain (e.g. in ℓ1-ℓ1 or ℓ2-ℓ1 case), the quantity Lco/ϵ is bounded by (m+n)3,
as otherwise we are in the high-accuracy regime where the runtimes of interior point methods or
cutting-plane methods [350, 296] are favorable. Speciﬁcally for ℓ1-ℓ1 matrix games in this regime,
interior-point methods [349, 144, 517] are always faster, see footnote in Section 3.1.1. We make this
assumption for notational convenience when discussing logarithmic factors depending on multiple
quantities, such as m, n, Lco, and ϵ−1.
3.2.3
Matrix access models
We design randomized algorithms which require accessing and sampling from the matrix A ∈Rn×m
in a variety of ways. Here, we list these operations, where we assume each takes constant time.
Speciﬁc algorithms only require access to a subset of this list; we make a note of each algorithm's
requirements when presenting it.
A1. For i, j ∈[m] × [n], return Aij.
A2. For i ∈[m] and p ∈{1, 2}, draw j ∈[n] with probability |Aij|p/ ∥Ai:∥p
p.
A3. For j ∈[n] and p ∈{0, 1, 2}, draw i ∈[m] with probability |Aij|p/ ∥A:j∥p
p.
A4. For i ∈[m] (j ∈[n]) and p ∈{1, 2}, return ∥Ai:∥p (∥A:j∥p).
A5. For p ∈{1, 2}, return maxi∈[m] ∥Ai:∥p, maxj∈[n] ∥A:j∥p, nnz, rcs, and ∥A∥F.
Given any representation of the matrix as a list of nonzero entries and their indices, we can always
implement the access modes above (in the assumed constant time) with O(nnz) time preprocessing;
see e.g. [526] for an implementation of the sampling (in a unit cost RAM model). Our variance-
reduced algorithms have an additive O(nnz) term appearing in their runtimes due to the need to
compute at least one matrix-vector product to implement gradient estimators. Thus, their stated
runtime bounds hold independently of matrix access assumptions.
3.2.4
Data structure interfaces
We rely on data structures to maintain and sample from the iterates of our algorithms. Below, we
give a summary of the operations supported by our data structures and their runtime guarantees.
We show how to implement these data structures in Section 3.5.
IterateMaintainerp
Given p ∈{1, 2}, we design a data structure IterateMaintainerp which maintains an implicit
representation of the current iterate x ∈Rn and a running sum s of all iterates. At initialization,

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
103
Category
Function
Runtime
initialize
Init(x0, v): x ←x0, s ←0
O(n)
update
Scale(c): x ←cx
O(1)
AddSparse(j, c): [x]j ←[x]j + c (if p = 1, we require c ≥−[x]j)
O(log n)†
AddDense(c): x ←x + cv (supported if p = 2)
O(1)
UpdateSum(): s ←s + x
O(1)
query
Get(j): Return [x]j
O(1)
GetSum(j): Return [s]j
O(1)
GetNorm(): Return ∥x∥p
O(1)
sample†
Sample(): Return j with probability [x]p
j/ ∥x∥p
p
O(log n)
† An alternative implementation does not support Sample, but performs AddSparse in time O(1).
this data structure takes as input the initial iterate x0 to be maintained and for p = 2, the data
structure also takes as input a ﬁxed vector v. It then supports the following operations.
The implementation of IterateMaintainerp is given in Section 3.5.1. In Sections B.3.2 and B.4.3
we use variants of this data structure WeightedIterateMaintainer2 and CenteredIterateMaintainer2,
and defer the detailed discussions of their implementations to Appendix B.8.
ApproxExpMaintainer
To maintain multiplicative weights updates with a ﬁxed dense component, we design a data structure
ApproxExpMaintainer initialized with an arbitrary point x0 ∈∆n, a direction v ∈Rn, a decay
constant κ ∈[0, 1] and an approximation error parameter ε. In order to specify the implementation
of our data structure, we require the following deﬁnition.
Deﬁnition 3 (β-padding). For x, x′ ∈∆n, we say x′ is a β-padding of x if x′ = ˜x/ ∥˜x∥1, for a
point ˜x ∈Rn
≥0 with ˜x ≥x entrywise and ∥˜x −x∥1 ≤β.
Notions similar to β-padding appear in previous literature [331]. A key technical property of β-
paddings is that they do not increase entropy signiﬁcantly (see Lemma 34).
ApproxExpMaintainer has maintains two vectors x, ˆx ∈∆n that, for an error tolerance parameter
ε, satisfy the invariant
ˆx is a ε-padding of x.
(3.25)
an error tolerance parameter ε. We now specify the interface, where ◦denotes elementwise product,
[xκ]j = [x]κ
j denotes elementwise power, Π∆(z) = z/ ∥z∥1 normalizes z ∈Rn
≥0 to lie in the simplex,
and ∥s∥0 denotes the number of nonzeroes in vector s. To state our runtimes, we deﬁne
ω := max

1
1 −κ, n
λε

.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
104
For most of our applications of ApproxExpMaintainer, ω is a polynomial in m and n (our iterate
dimensions), so log(ω) = O(log(mn)) (with the exception of our maximum inscribed ball application,
where our runtimes additionally depend polylogarithmically on the size of the hyperplane shifts b;
see Remark 2). We defer a more ﬁne-grained runtime discussion to Section 3.5.3.
Category
Function
Runtime
initialize
Init(x0, v, κ, ε, λ) : κ ∈[0, 1), ε > 0, minj[x0]j ≥λ
O(n log n log2 ω)
update
MultSparse(g): x ←ε-padding of Π∆(x ◦exp(g))
O(∥g∥0 log2 n log2 ω)
DenseStep(): x ←Π∆(xκ ◦exp(v))
O(log n)
UpdateSum(): s ←s + ˆx (recall invariant (3.25))
O(log n log ω)
query
Get(j): Return [ˆx]j
O(log n log ω)
GetSum(j): Return [s]j
O(log2 ω)
sample
Sample(): Return j with probability [ˆx]j
O(log n log ω)
The role of ApproxExpMaintainer is in to eﬃciently implement the regularized and reduced-
variance stochastic mirror descent steps of the form (3.8). To do this, we initialize the data structure
with v = (1 −κ) log x0 −ηκgx
0. Then, the iteration (3.8) consists of calling DenseStep() followed by
MultSparse(−ηκ˜δx).
3.3
Framework
In this section, we develop our algorithmic frameworks. The resulting algorithms have either sub-
linear or variance-reduced complexities. We develop our sublinear coordinate method framework in
Section 3.3.1, and its variance-reduced counterpart in Section 3.3.2.
3.3.1
Sublinear coordinate methods
In Section 3.3.1 we introduce the concept of a local gradient estimator, which allow stronger guar-
antees for stochastic mirror descent with clipping (Algorithm 10) via local norms analysis. Then,
in Section 7 we state the form of the speciﬁc local gradient estimators we use in our coordinate
methods, and motivate the values of Lco in Table 3.1.
Convergence analysis
Deﬁnition 4. For local norm setup (Z, ∥·∥·, r, Θ, clip) , we call a stochastic gradient estimator
˜g : Z →Z∗an L-local estimator if it satisﬁes the following properties for all z ∈Z:
1. Unbiasedness: E[˜g(z)] = g(z).
2. Second moment bound: for all w ∈Z, E[∥˜g(z)∥2
w] ≤L2.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
105
The following lemma shows that L-local estimators are unbiased for L-bounded operators.
Lemma 30. A gradient mapping that admits an L-local estimator satisﬁes ∥g(z)∥∗≤L for all
z ∈Z.
Proof. For every z ∈Z, the function ∥·∥2
z is convex. Thus by Jensen's inequality,
∥g(z)∥2
z = ∥E˜g(z)∥2
z ≤E ∥˜g(z)∥2
z ≤L2.
Taking supremum over z ∈Z gives ∥g(z)∥2
∗≤L2.
We note that the same result does not hold for ˜g because maximum and expectation do not commute.
That is, E ∥˜g∥2
∗is not bounded by L2. This fact motivates our use of local norms analysis.
Below, we state Algorithm 10, stochastic mirror descent with clipping, and a guarantee on its rate
of convergence using local gradient estimators. We defer the proof to Appendix B.2 and note here
that it uses the "ghost iterates" technique due to [416] in order to rigorously bound the expected
regret with respect to the best response to our iterates, rather than a pre-speciﬁed point. This
technique is purely analytical and does not aﬀect the algorithm.
We also note that the second
inequality in Proposition 9 holds with any convex-concave function f; the ﬁrst uses bilinearity of
our problem structure.
Algorithm 10: Stochastic mirror descent
1 Input: Matrix A ∈Rm×n, L-local gradient estimator ˜g, clipping function clip(·);
2 Output: A point with O( Θ
ηT + ηL2) expected duality gap ;
3 Parameters: Step-size η, number of iterations T;
4 z0 ←argminz∈Zr(z);
5 for t = 1, . . . , T do
6
zt ←arg minz∈Z

⟨clip(η˜g(zt−1)), z⟩+ Vzt−1(z)
	
;
7 Return:
1
T +1
PT
t=0 zt;
Proposition 9. Let (Z, ∥·∥·, r, Θ, clip) be a local norm setup, let L, ϵ > 0, and let ˜g be an L-local
estimator. Then, for η ≤
ϵ
9L2 and T ≥6Θ
ηϵ ≥54L2Θ
ϵ2
, Algorithm 10 outputs a point ¯z such that
EGap(¯z) ≤E
"
sup
u∈Z
1
T + 1
T
X
t=0
⟨g(zt), zt −u⟩
#
≤ϵ.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
106
Coordinate gradient estimators
We now state the general form which our local gradient estimators ˜g take. At a point z ∈Z, for
speciﬁed sampling distributions p(z), q(z), sample ix, jx ∼p(z) and iy, jy ∼q(z). Then, deﬁne
˜g(z) :=
Aixjx[zy]ix
pixjx(z) ejx, −Aiyjy[zx]jy
qiyjy(z)
eiy

+ g(0) where g(0) = (b, c).
(3.26)
It is clear that regardless of the distributions p(z), q(z), for the gradient operator in (I.20), ˜g(z) is
an unbiased gradient estimator (E[˜g(z)] = g(z)) and ˜g(z) −g(0) is 2-sparse.
Optimal values of Lco. In the remainder of this section we assume for simplicity the g(0) = 0
(i.e. the objective f in (3.22) has not linear terms). Here we compute the optimal values of L for
local gradient estimators (see Deﬁnition 4) of the form (3.26) for each of the local norm setups we
consider. This motivates the values of Lco we derive in the following sections. First, in the ℓ1-ℓ1
case, the second moment of ∥˜gx(z)∥2
wx (the local norm of the X block of ˜g(z) at point w) is
E
"
[wx]jx
Aixjx[zy]ix
pixjx(z)
2#
=
X
i∈[m],j∈[n]
A2
ij[zy]2
i [wx]j
pij(z)
≥


X
i∈[m],j∈[n]
|Aij|[zy]i
q
[wx]j


2
.
Since zy ∈∆m and
√
wx ∈Bn with
√
wx
2 = 1, the above lower bound is in the worst case
maxi ∥Ai:∥2
2. Similarly, the best possible bound on the Y is maxj ∥A:j∥2
2. Therefore, in the ℓ1-ℓ1
setup, no local estimator has parameter L smaller than Lco in Table 3.1.
Next, in the ℓ2-ℓ2 case, the (ℓ2) second moment of the X block is
E
"Aixjx[zy]ix
pixjx(z)
2#
=
X
i∈[m],j∈[n]
A2
ij[zy]2
i
pij(z)
≥


X
i∈[m],j∈[n]
|Aij|[zy]i


2
=

X
i∈[m]
∥Ai:∥1 [zy]i


2
.
In the worst case, this is at least (P
i∈[m] ∥Ai:∥1)2; similarly, the best second moment bound for the
Y block is (P
j∈[n] ∥A:j∥1)2, which means that Lco is similarly unimprovable in the ℓ2-ℓ2 setup.
Finally, in the ℓ2-ℓ1 case, where X = Bn and Y = ∆m, we again have that the ℓ2 second moment
of the X (ball) block is at least
E
"Aixjx[zy]ix
pixjx(z)
2#
≥


X
i∈[m],j∈[n]
|Aij|[zy]i


2
.
Here, since zy ∈∆m, the worst-case lower bound of the variance is maxi ∥Ai:∥2
1. Further, the local

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
107
norm (at w) second moment of the Y (simplex) block is at least
E
"
[wy]iy
Aiyjy[zx]jy
qiyjy(z)
2#
≥


X
i∈[m],j∈[n]
|Aij|[zx]j
p
[wy]i


2
.
Since zx ∈Bn and
√
wy ∈Bm, in the worst case this second moment can be as high as ∥|A|∥op, where
we use |A| to denote the elementwise absolute value of A. This is better than the Lco in Table 3.1,
suggesting there is room for improvement here. However, the sampling probabilities inducing this
optimal variance bound are of the form
qij(z; w) ∝|Aij|
p
[wy]i · [zx]j,
and it unclear how to eﬃciently sample from this distribution. Improving our ℓ2-ℓ1 gradient estimator
(or proving that no improvement is possible) remains an open problem.
3.3.2
Variance-reduced coordinate methods
In this section, we develop the algorithmic framework we use in our variance-reduced methods. We
ﬁrst deﬁne a type of "centered-local" gradient estimator, modifying the local gradient estimators of
the previous section. We then give the general form of a variance-reduced method and analyze it in
the context of our gradient estimators and the error incurred by our data structure maintenance.
General convergence result
Deﬁnition 5. For local norm setup (Z, ∥·∥·, r, Θ, clip), and given a reference point w0 = (wx
0, wy
0),
we call a stochastic gradient estimator ˜gw0 : Z →Z∗an L-centered-local estimator if it satisﬁes the
following properties:
1. Unbiasedness: E[˜gw0(z)] = g(z).
2. Relative variance bound: for all w ∈Z, E[∥˜gw0(z) −g(w0)∥2
w] ≤L2Vw0(z).
Remark 1. Similarly to Lemma 30, a gradient mapping that admits an L-centered-local estimator
also satisﬁes ∥g(z) −g(w0)∥2
∗≤L2Vw0(z), by Jensen's inequality.
Algorithm 11 below is an approximate variant of the variance reduction algorithm in an earlier
version of this work [111] which closely builds upon the "conceptual prox-method" of [415]. How-
ever, for self-containment of this chapter we specialize our presentation of our variance reduction
framework to our implementation in the coordinate sampling setting.
The algorithm repeatedly calls a stochastic oracle O : Z →Z to produce intermediate iterates,
and then performs an extragradient (linearized) proximal step using the intermediate iterate. The

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
108
main modiﬁcation compared to [111] is Line 8, which accommodates slight perturbations to the extra-
gradient step results. These perturbations arise due to input requirements of our data structures:
we slightly pad coordinates in simplex blocks to ensure they are bounded away from zero.
Algorithm 11: OuterLoop(O) (conceptual prox-method [415])
1 Input: Target approximation quality εouter, (α, εinner)-relaxed proximal oracle O(z) for
gradient mapping g and some εinner < εouter, distance-generating r;
2 Parameters: Number of iterations K;
3 Output: Point ¯zK with E Gap(¯z) ≤αΘ
K + εouter;
4 z0 ←argminz∈Zr(z) ;
5 for k = 1, . . . , K do
6
zk−1/2 ←O(zk−1)
▷We implement O(zk−1) by calling InnerLoop(zk−1, ˜gzk−1, α) ;
7
z⋆
k := Proxα
zk−1(g(zk−1/2)) = argminz∈Z

g
 zk−1/2

, z

+ αVzk−1(z)
	
;
8
zk ←any point satisfying Vzk(u) −Vz⋆
k(u) ≤εouter−εinner
α
, for all u ∈Z
9 Return: ¯zK = 1
K
PK
k=1 zk−1/2;
The following deﬁnition summarizes the key property of the oracle O.
Deﬁnition 6. Let operator g be monotone and α, εinner > 0. An (α, εinner)-relaxed proximal oracle
for g is a (possibly randomized) map O : Z →Z such that z′ = O(z) satisﬁes
E

max
u∈Z

⟨g(z′), z′ −u⟩−αVz(u)
	
≤εinner.
The following proposition shows that despite the error permitted tolerated in Line 8, the algo-
rithm still converges with rate 1/K. We defer its proof to Appendix B.2.
Proposition 10. Let O be an (α, εinner)-relaxed proximal oracle with respect to gradient mapping
g, distance-generating r with range at most Θ and some εinner ≤εouter. Let z1/2, z3/2, . . . , zK−1/2 be
iterates of Algorithm 11 and let ¯zK be its output. Then
E Gap(¯zK) ≤E max
u∈Z
1
K
K
X
k=1

g(zk−1/2), zk−1/2 −u

≤αΘ
K + εouter.
Algorithm 12 is a variant of the variance-reduced inner loop of an earlier version of this work [111],
adapted for local norms and inexact iterates (again, due to approximations made by the data struc-
ture). It tolerates error in three places:
1. Instead of estimating the gradient at the previous iterate wt−1, we estimate it at a point ˆwt−1
such that wt−1−ˆwt−1 has small norm and similar divergence from the reference point w0 (Line 5).
2. Instead of letting the next iterate be the exact mirror descent step w⋆
t , we let be a point wT that
is close to w⋆
t in norm and has similar divergences to from w0 and to any any point in Z (Line 7).

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
109
3. The output ˜w can be an approximation of the average of the iterates, as long as its diﬀerence to
the true average has bounded norm (Line 8).
We quantify the eﬀect of these approximations in Proposition 11, which gives a runtime guarantee
for Algorithm 12 (where we recall the deﬁnition of ∥·∥as the dual norm of ∥·∥∗, see (3.21)). The
proof is deferred to Appendix B.2.
Algorithm 12: InnerLoop(w0, ˜gw0, ϕ)
1 Input: Initial w0 ∈Z, L-centered-local gradient estimator ˜gw0, oracle quality α > 0;
2 Parameters: Step size η, number of iterations T, approximation tolerance ϕ;
3 Output: Point ˜w satisfying Deﬁnition 6;
4 for t = 1, . . . , T do
5
ˆwt−1 ≈wt−1 satisfying (a) Vw0( ˆwt−1) −Vw0(wt−1) ≤ϕ
α and (b) ∥ˆwt−1 −wt−1∥≤
ϕ
LD ;
6
w⋆
t ←argminw∈Z

⟨w, clip(η˜gw0( ˆwt−1) −ηg(w0)) + ηg(w0)⟩+ αη
2 Vw0(w) + Vwt−1(w)
	
;
7
wt ≈w⋆
t satisfying
(a) maxu

Vwt(u) −Vw⋆
t (u)

≤ηϕ,
(b) Vw0(wt) −Vw0(w⋆
t ) ≤ϕ
α, and
(c) ∥wt −w⋆
t ∥≤
ϕ
2LD
8 Return: ˜w ≈1
T
PT
t=1 wt satisfying
 ˜w −1
T
PT
t=1 wt
 ≤
ϕ
LD.
Proposition 11. Let (Z, ∥·∥·, r, Θ, clip) be any local norm setup. Let w0 ∈Z, α ≥εinner > 0,
and ˜gw0 be an L-centered-local estimator for some L ≥α.
Assume the domain is bounded by
maxz∈Z ∥z∥≤D, that g is L-Lipschitz, i.e. ∥g(z) −g(z′)∥∗≤L ∥z −z′∥, that g is LD-bounded, i.e.
maxz∈Z ∥g(z)∥∗≤LD, and that ˆw0 = w0. Then, for η =
α
10L2 , T ≥
6
ηα ≥60L2
α2 , and ϕ = εinner
6
,
Algorithm 12 outputs a point ˆw ∈Z such that
E max
u∈Z [⟨g( ˜w), ˜w −u⟩−αVw0(u)] ≤εinner,
(3.27)
i.e. Algorithm 12 is an (α, εinner)-relaxed proximal oracle.
Remark 2 (Assumption of boundedness on g). The assumption that g is LD-bounded in the dual
norm is immediate from other assumptions used in Proposition 11 in the case of the applications in
Section 3.4, where we develop methods for solving ℓ1-ℓ1 matrix games and assume that g(0) = 0.
In applications in Section 3.6, due to the existence of extra linear terms b, c ̸= 0, all complexity
bounds will have an additional dependence on log(∥[b; c]∥∗) which we pay in the implementation of
data structure ApproxExpMaintainer (i.e. the parameter L in the bound on g is larger if ∥[b; c]∥∗is
large). We hide this extra polylogarithmic factor in the eO notation.
We also remark that (up to constants) the bounds on the range of εinner ≤α ≤L in the statement
of Proposition 11 correspond to the cases where the inner and outer loop consist of a single iteration.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
110
Variance-reduced coordinate gradient estimators
We now state the general form which our centered-local estimators ˜gw0 take, given a reference
point w0 ∈Z. At a point z, for sampling distributions p(z; w0), q(z; w0) to be speciﬁed, sample
ix, jx ∼p(z; w0) and iy, jy ∼q(z; w0). Then, deﬁne
˜gw0(z) =
Aixjx[zy −wy
0]ix
pixjx(z; w0)
ejx, −Aiyjy[zx −wx
0]jy
qiyjy(z; w0)
eiy

+ g(w0).
(3.28)
It is clear that regardless of the distributions p(z; w0), q(z; w0), this is an unbiased gradient
estimator (E[˜gw0(z)] = g(z)). Furthermore, ˜gw0(z) −g(w0) is always 2-sparse.
Finally, we remark that we give another instantiation of our variance reduction framework using
gradient estimators based on sampling rows and columns in Appendix B.5. While the resulting
complexity bounds do not improve upon our coordinate methods (up to logarithmic factors), the
estimator design and analysis is conceptually simpler, and the interested reader may ﬁrst read
Appendix B.5 as a precursor to our coordinate-based sampling methods.
3.4
Matrix games
In this section we instantiate the algorithmic framework of Section 3.3 in ℓ1-ℓ1 setup without linear
terms, i.e. b = c = 0 in the objective (3.22). This is the fundamental "matrix game" problem
min
x∈∆m max
y∈∆n y⊤Ax.
We give two algorithms for approximately solving matrix games. In Section 3.4.1 we develop a
stochastic coordinate method based on Algorithm 10 with potentially sublinear runtime eO
 (L1,1
co /ϵ)2
.
In Section B.3 we develop a coordinate variance-reduction based on Algorithm 11 with runtime
eO
 nnz + √nnz · L1,1
co /ϵ

that improves on the former runtime whenever it is Ω(nnz). In both cases
we have
L1,1
co := max

max
i
∥Ai:∥2 , max
j
∥A:j∥2

(3.29)
as in Table 3.1.
Instantiations for the ℓ2-ℓ1 and ℓ2-ℓ2 setups follow similarly. We carry them out in Appendices B.3
(for stochastic coordinate methods) and B.4 (for variance reduction methods).
Remark 3. For simplicity in this section (and the remaining implementations in Appendices B.3, B.4),
we will set g(0) = 0 whenever the setup is not ℓ2-ℓ2, as is standard in the literature. We defer a
discussion of how to incorporate arbitrary linear terms in simplex domains to Section 3.6; up to
additional logarithmic terms in the runtime, this extension is supported by ApproxExpMaintainer.
Assumptions. Throughout (for both Sections 3.4.1 and 3.4.2), we assume access to entry queries,

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
111
ℓ2 norms of rows and columns, and ℓ2 sampling distributions for all rows and columns. We use the
ℓ1-ℓ1 local norm setup (Table 3.6). We also deﬁne Lmax := ∥A∥max = maxi∈[m],j∈[n] |Aij|.
3.4.1
ℓ1-ℓ1 sublinear coordinate method
Gradient estimator
For z ∈∆n × ∆m and desired accuracy ϵ > 0, we specify the sampling distributions p(z), q(z):
pij(z) := [zy]i
A2
ij
∥Ai:∥2
2
and
qij(z) := [zx]j
A2
ij
∥A:j∥2
2
.
(3.30)
We ﬁrst state and prove the local properties of this estimator.
Lemma 31. In the ℓ1-ℓ1 setup, estimator (3.26) using the sampling distribution in (3.30) is a
√
2L1,1
co -local estimator.
Proof. Unbiasedness holds by deﬁnition. For arbitrary wx, we have the variance bound:
E
h
∥˜gx(z)∥2
wx
i
≤
X
i∈[m],j∈[n]
pij(z) ·
 
[wx]j ·
Aij[zy]i
pij(z)
2!
=
X
i∈[m],j∈[n]
[wx]j
A2
ij[zy]2
i
pij(z)
≤
X
i∈[m],j∈[n]
[wx]j[zy]i ∥Ai:∥2
2 ≤max
i∈[m] ∥Ai:∥2
2 ≤(Lco
1,1)2.
Similarly, we have
E
h
∥˜gy(z)∥2
wy
i
≤(Lco
1,1)2.
The deﬁnition ∥˜g(z)∥2
w = ∥˜gx(z)∥2
wx + ∥˜gy(z)∥2
wy yields the claimed variance bound.
Implementation details
In this section, we discuss the details of how to leverage the IterateMaintainer1 data structure
to implement the iterations of our algorithm. The algorithm we analyze is Algorithm 10, using the
local estimator deﬁned in (3.26), and the distribution (3.30). We choose
η =
ϵ
18

L1,1
co
2 and T =
6Θ
ηϵ

≥108
 L1,1
co
2 log(mn)
ϵ2
.
Lemma 31 implies that our estimator satisﬁes the remaining requirements for Proposition 9, giving
the duality gap guarantee in T iterations. In order to give a runtime bound, we claim that each
iteration can be implemented in log(mn) time, with O(m + n) additional runtime.
Data structure initializations and invariants. At the start of the algorithm, we spend O(m + n)
time initializing data structures via IMx
1.Init( 1
n1n, 0n) and IMy
1.Init( 1
m1m, 0m), where IMx
1, IMy
1 are

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
112
appropriate instantiations of IterateMaintainer1 data structures. Throughout, we preserve the
invariant that the points maintained by IMx
1, IMy
1 correspond to the x and y blocks of the current
iterate zt at iteration t of the algorithm.
Iterations. For simplicity, we only discuss the runtime of updating the x block as the y block
follows symmetrically. We divide each iteration into the following substeps, each of which we show
runs in time O(log mn). We refer to the current iterate by z = (zx, zy), and the next iterate by
w = (wx, wy).
Sampling. Recall that
pij(z) := [zy]i
A2
ij
∥Ai:∥2
2
.
We ﬁrst sample coordinate i via IMy
1.Sample() in O(log m). Next, we sample j ∈[n] with probability
proportional to A2
ij using the data structure corresponding to Ai: in O(1) by assumption of the
matrix access model.
Computing the gradient estimator. To compute c := clip(Aij[zy]i/pij), it suﬃces to compute Aij,
[zy]i, and pij. Using an entry oracle for A we obtain Aij, and we get [zy]i by calling IMy
1.Get(i).
Computing pij using the precomputed ∥Ai:∥2 and the values of Aij, [zy]i therefore takes O(1) time.
Performing the update. For the update corresponding to a proximal step, we have
wx ←ΠX (zx ◦exp(−η˜gx(z))) =
zx ◦exp(−η˜gx(z))
∥zx ◦exp(−η˜gx(z))∥1
.
We have computed ˜gx(z), so to perform this update, we call
ξ ←IMx
1.Get(j);
IMx
1.AddSparse(j, (exp(−ηc) −1)ξ);
IMx
1.Scale(IterateMaintainerx.GetNorm()−1);
IMx
1.UpdateSum().
By assumption, each operation takes time O(log n), giving the desired iteration complexity. It is
clear that at the end of performing these operations, the invariant that IMx
1 maintains the x block
of the iterate is preserved.
Averaging. After T iterations, we compute the average point ¯zx:
[¯zx]j ←1
T · IMx
1.GetSum(j), ∀j ∈[n].
By assumption, this takes O(n) time.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
113
Algorithm guarantee
Theorem 10. In the ℓ1-ℓ1 setup, the implementation in Section 3.4.1 has runtime
O
  L1,1
co
2 log2(mn)
ϵ2
+ m + n
!
,
and outputs a point ¯z ∈Z such that EGap(¯z) ≤ϵ..
Proof. The runtime follows from the discussion in Section 3.4.1. The correctness follows from Propo-
sition 9.
Remark 4. Using our IterateMaintainer1 data structure, the ℓ1-ℓ1 algorithm of [253] runs in time
O(rcs ∥A∥2
max log2(mn)/ϵ2), where rcs is the maximum number of nonzeros in any row or column.
Our runtime universally improves upon it since (Lco
1,1)2 ≤rcs∥A∥2
max.
3.4.2
ℓ1-ℓ1 variance-reduced coordinate method
Gradient estimator
Given reference point w0 ∈∆n × ∆m, for z ∈∆n × ∆m and a parameter α > 0, we specify the
sampling distributions p(z; w0), q(z; w0):
pij(z; w0) := [zy]i + 2[wy
0]i
3
·
A2
ij
∥Ai:∥2
2
and qij(z; w0)
:= [zx]j + 2[wx
0]j
3
·
A2
ij
∥A:j∥2
2
.
(3.31)
We remark that this choice of sampling distribution, which we term "sampling from the sum" (of
the current iterate and reference point), may be viewed as a computationally-eﬃcient alternative to
the distribution speciﬁed in [111], which was based on "sampling from the diﬀerence". In particular,
sampling from the diﬀerence is an operation which to the best of our knowledge is diﬃcult to
implement in sublinear time, so we believe that demonstrating that this alternative distribution
suﬃces may be of independent interest. In order to show its correctness, we need the following
claim, whose proof we defer to Appendix B.4.1.
Lemma 32. For y, y′ ∈∆m, divergence Vy(y′) generated by r(y) = P
i∈[m][y]i log[y]i −[y]i satisﬁes
Vy(y′) ≥1
2 ∥y′ −y∥2
3
2y+y′ = 1
2
X
i∈[m]
([y]i −[y′]i)2
2
3[y]i + 1
3[y′]i
.
We now show the local properties of this estimator.
Lemma 33. In the ℓ1-ℓ1 setup, estimator (3.28) using the sampling distribution in (3.31) is a
√
2L1,1
co -centered-local estimator.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
114
Proof. Unbiasedness holds by deﬁnition. For arbitrary wx, we have the variance bound:
E
h˜gx
w0(z) −gx(w0)
2
wx
i
=
X
i∈[m],j∈[n]
pij(z; w0) ·
 
[wx]j ·
Aij ([zy]i −[wy
0]i)
pij(z; w0)
2!
=
X
i∈[m],j∈[n]
[wx]j
A2
ij ([zy]i −[wy
0]i)2
pij(z; w0)
≤
X
i∈[m],j∈[n]
[wx]j
([zy]i −[wy
0]i)2
1
3[zy]i + 2
3[wy
0]i
∥Ai:∥2
2
≤2

max
i
∥Ai:∥2
2

Vwy
0(zy),
where in the last inequality we used Lemma 32. Similarly, we have for arbitrary wy,
E
h˜gy
w0(z) −gy(w0)
2
wy
i
≤2

max
j
∥A:j∥2
2

Vwx
0(zx).
Combining these and using
∥˜gw0(z) −g(w0)∥2
w :=
˜gx
w0(z) −gx(w0)
2
wx +
˜gy
w0(z) −gy(w0)
2
wy
yields the desired variance bound.
Implementation details
In this section, we discuss the details of how to leverage the ApproxExpMaintainer data structure
to implement the iterations of our algorithm.
We ﬁrst state one technical lemma on the eﬀect
of β-padding (Deﬁnition 3) on increasing entropy, used in conjunction with the requirements of
Proposition 11 to bound the error tolerance required by our ApproxExpMaintainer data structure.
The proof is deferred to Appendix B.4.1.
Lemma 34. Let x′ ∈∆n be a β-padding of x ∈∆n. Then,
X
j∈[n]
x′
j log x′
j −
X
j∈[n]
xj log xj ≤βn
e + β(1 + β).
This leads to the following divergence bounds which will be used in this section.
Lemma 35. Let x′ ∈∆n be a β-padding of x ∈∆n. Then
Vx′(u) −Vx(u) ≤β, ∀u ∈Z

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
115
and if ∥log(x0)∥∞≤M, then
Vx0(x′) −Vx0(x) ≤β

2M + n
e + 1 + β

Proof. Throughout this proof, let ˜x be the point in Deﬁnition 3 such that ∥˜x −x∥1 ≤β and x′ =
˜x/ ∥˜x∥1.
The ﬁrst claim follows from expanding
Vx′(u) −Vx(u) =
X
j∈[n]
uj log xj
x′
j
=
X
j∈[n]
uj log
xj
˜xj
· ∥˜x∥1

≤log(∥˜x∥1) ≤β.
The ﬁrst inequality used u ∈∆n and ˜x ≥x entrywise, and the last inequality used log(1 + β) ≤β.
For the second claim, we have by the triangle inequality
∥x −x′∥1 ≤∥x −˜x∥1 + ∥˜x −x′∥1 ≤β + (∥˜x∥1 −1) ∥x′∥1 ≤2β.
The claim then follows from expanding
Vx0(x′) −Vx0(x) =
X
j∈[n]
x′
j log x′
j −
X
j∈[n]
xj log xj + ⟨log x0, x −x′⟩,
and applying Lemma 34.
The algorithm we analyze is Algorithm 11 with K = 3αΘ/ϵ, εouter = 2ϵ/3, εinner = ϵ/3 using
Algorithm 12 as an (α, εinner)-relaxed proximal oracle.
The speciﬁc modiﬁcation we perform to
deﬁne the iterates {zk} of Algorithm 11 as modiﬁcations of the ideal iterates {z⋆
k} uses the following
deﬁnition.
Deﬁnition 7. For a simplex variable x′ ∈∆n, we deﬁne truncate(x′, δ) to be the point x ∈∆n with
xj ∝max(x′
j, δ) for all j ∈[n]. For a variable z on two blocks, we overload notation and deﬁne
truncate(z, δ) to be the result of applying truncate(·, δ) to each simplex block of z.
For our implementation, in each step of Algorithm 11, we will compute the point z⋆
k exactly,
and apply the operation zk ←truncate(z⋆
k, δ), for δ = εouter−εinner
α(m+n)
. We now quantify the eﬀect of
truncation in terms of Bregman divergence to an arbitrary point.
Lemma 36 (Eﬀect of truncation). Let x′ ∈∆n, and let x = truncate(x′, δ). Then, for any u ∈∆n,
and where divergences are with respect to entropy,
Vx(u) −Vx′(u) ≤δn.
Proof. Note that x is a δn-padding of x′, as it is the result of adding at most δ to each coordinate
and renormalizing to lie in the simplex. Consequently, the result follows from Lemma 35.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
116
Lemma 255 thus implies our iterates satisfy the requirements of Algorithm 11. Our implemen-
tation of Algorithm 12 will use approximation tolerance ϕ = εinner/6 = ϵ/18, where we always
set
L1,1
co ≥α ≥εinner.
(3.32)
This matches the requirements of Proposition 11. In the implementation of Algorithm 12, we use
the centered-local gradient estimator deﬁned in (3.28), using the sampling distribution (3.31). For
each use of Algorithm 12, we choose
η =
α
20

L1,1
co
2 and T =
 6
ηα

≥120
 L1,1
co
2
α2
.
Our discussion will follow in four steps: ﬁrst, we discuss the complexity of all executions in
Algorithm 11 other than calls to the oracles. Next, we discuss the complexity of all initializations
of ApproxExpMaintainer data structures. Then, we discuss the complexity of all other iterations
of Algorithm 12. For simplicity, when discussing Algorithm 12, we will only discuss implementation
of the x-block, and the y-block will follow symmetrically, while most runtimes are given considering
both blocks. Lastly, we discuss complexity of computing the average iterate in the end of the inner
loop. Altogether, the guarantees of Proposition 10 and Proposition 11 imply that if the guarantees
required by the algorithm hold, the expected gap of the output is bounded by ϵ.
Outer loop extragradient steps. Overall, we execute K = 3αΘ/ϵ iterations of Algorithm 11, with
εouter = 2ϵ/3, εinner = ϵ/3 to obtain the desired gap, where Θ = log(mn) in the ℓ1-ℓ1 setup. We
spend O(nnz) time executing each extragradient step in Algorithm 11 exactly to compute iterates z⋆
k,
where the dominant term in the runtime is computing each g(zk−1/2), for k ∈[K]. We can maintain
the average point ¯z throughout the duration of the algorithm, in O(m+n) time per iteration. Finally,
we spend an additional O(m + n) time per iteration applying truncate to each iterate z⋆
k.
Data structure initializations and invariants.
We consider the initialization of data structures for implementing an (α, εinner = ϵ/3)-relaxed
proximal oracle with error tolerance ϕ = ϵ/18. First, note that the point wx
0 used in the initial-
ization of every inner loop, by the guarantees of truncate operation, has no two coordinates with
multiplicative ratio larger than δ = ϵ/(3α(m + n)) ≥(m + n)−4, by our choice α ≤L1,1
co (3.32) and
our assumptions on L1,1
co /ϵ (cf. Section 3.2.2). Since clearly a simplex variable in ∆n has a coordinate
at least 1/n, the entries of w0 are lower bounded by λ = (m + n)−5.
Next, we discuss the initial parameters given to AEMx, an instance of ApproxExpMaintainer
which will support necessary operations for maintaining the x variable (we will similarly initialize
an instance AEMy). Speciﬁcally, the invariant that we maintain throughout the inner loop is that in
iteration t, the"exact vector" x maintained by AEMx corresponds to the x block of the current iterate
wt, and the "approximate vector" ˆx maintained corresponds to the x block of the approximate

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
117
iterate ˆwt, as deﬁned in Algorithm 12.
We will now choose ˜ε so that if AEMx is initialized with error tolerance ˜ε, all requirements of
Proposition 11 (e.g. the bounds stipulated in Algorithm 12) are met. We ﬁrst handle all divergence
requirements. In a given iteration, denote the x blocks of w⋆
t , wt and ˆwt by x⋆
t , xt and ˆxt respectively,
and recall AEMx guarantees xt is a ˜ε-padding of x⋆
t , and ˆxt is a ˜ε-padding of x⋆
t . The former of these
guarantees is true by the speciﬁcation of MultSparse (which will be used in the implementation of
the step, see "Performing the update" below), and the latter is true by the invariant on the points
supported by AEMx. Lines 5 and 7 of Algorithm 12 stipulate the divergence requirements, where
x0 := wx
0,
max {Vx0(xt) −Vx0 (x⋆
t ) , Vx0 (ˆxt) −Vx0 (xt)} ≤ϕ
2α =
ϵ
36α
(3.33)
and
max
u

Vxt(u) −Vx⋆
t (u)

≤ηϕ
2 = ηϵ
36.
(3.34)
Clearly, combining this guarantee with a similar guarantee on the y blocks yields the desired bound.
Since we derived ∥log w0∥∞≤5 log(mn), we claim that choosing
˜ε ≤
ϵ
36α(m + n)
suﬃces for the guarantees in (3.33). By the ﬁrst part of Lemma 35, for all suﬃciently large m + n,
max {Vx0(xt) −Vx0 (x⋆
t ) , Vx0 (ˆxt) −Vx0 (xt)} ≤˜ε

10 log(mn) + n
e + 1 + ˜ε

≤
ϵ
36α.
Similarly for guarantees in (3.34), by the second part of Lemma 35 we know it suﬃces to choose
˜ε ≤
ϵ2
720

L1,1
co
2 ≤
ϵα
720

L1,1
co
2 = ηϵ
36.
Here, we used the restriction εinner ≤α ≤L1,1
co . Next, the norm requirements of Algorithm 12 (the
guarantees in Lines 5, 7, and 8) imply we require
˜ε ≤
ϵ
18
√
2L1,1
co
,
where we used that g is ∥A∥max ≤L1,1
co -Lipschitz and the diameter of Z is bounded by
√
2. Using
our assumptions on the size of parameters in Section 3.2.2, it suﬃces to set the error tolerance
˜ε = (m + n)−8.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
118
To give the remainder of speciﬁed parameters, AEMx is initialized via Init(wx
0, v, κ, ˜ε) for
κ :=
1
1 + ηα/2, v := (1 −κ) log wx
0 −ηκgx(w0).
To motivate this form of updates, note that each iteration of Algorithm 12 requires us to compute
argmin

⟨ctej + gx(w0), x⟩+ α
2 Vwx
0(x) + 1
η Vwx
t(x)

.
We can see that the solution to this update is given by

w⋆
t+1
x ←Π∆([wx
t]κ ◦exp ((1 −κ) log wx
0 −ηκgx(w0)) ◦exp(−ηκctej)) .
(3.35)
This form of update is precisely supported by our choice of κ and v, as well as the DenseStep and
MultSparse operations. By the choice of parameters, we note that 1 −κ ≥(m + n)−8.
Finally, in order to support our sampling distributions and gradient computations, we compute
and store the vectors w0 and g(w0) in full using O(nnz(A)) time at the beginning of the inner loop.
In O(m + n) time, we also build two data structures which allow us to sample from entries of the
given ﬁxed vectors wx
0, and wy
0, in constant time respectively.
Following Section 3.2.4, we deﬁned the parameter
ω := max

1
1 −κ, n
λ˜ϵ

≤(m + n)13, so that log(ω) = O(log(mn)).
Altogether, these initializations take time O(nnz + (m + n) log3(mn)), following Section 3.2.4.
Inner loop iterations. We discuss how to make appropriate modiﬁcations to the x-block. For
simplicity we denote our current iterate as z, and the next iterate as w. Also, we denote ˆz as the con-
catenation of implicit iterates that the two ApproxExpMaintainer copies maintain (see Section 3.2.4
for more details), which is ˜ε close in ℓ1 distance to z, the prior iterate, so that we can query or sample
entries from ˆz using AEMx and AEMy. Each inner loop iteration consists of using a gradient estimator
at ˆz satisfying ∥ˆz −z∥1 ≤˜ε, sampling indices for the computation of ˜gw0(ˆz), computing the sparse
part of ˜gw0(ˆz), and performing the approximate update to the iterate. We show that we can run each
substep using data structure AEMx in time O(log4(mn)), within the error tolerance of Proposition 11
due to the deﬁnition of ˜ε. Combining with our discussion of the complexity of initialization, this
implies that the total complexity of the inner loop, other than outputting the average iterate, is
O(T log4(mn) + nnz + (m + n) log3(mn)) = O
  L1,1
co
2 · log4(mn)
α2
+ nnz + (m + n) log3(mn)
!
.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
119
Sampling. Recall that the distribution we sample from is given by
pij(ˆz; w0) := [ˆzy]i + 2[wy
0]i
3
·
A2
ij
∥Ai:∥2
2
.
First, with probability 2/3, we sample a coordinate i from the precomputed data structure for sam-
pling from wy
0 in constant time; otherwise, we sample i via AEMy.Sample(). Then, we sample an entry
of Ai: proportional to its square via the precomputed data structure (cf. Section 3.2.3) in constant
time. This takes in total O(log mn) time.
Computing the gradient estimator. Proposition 11 requires us to compute the sparse component
of the gradient estimator (3.28) at point ˆz. To do this for the x block, we ﬁrst query [wx
0]j and
[ˆzy]i ←AEMy.Get(i), and then access the precomputed norm ∥Ai:∥2 and entry Aij. We then compute
c = clip
 
Aij [ˆzy −wy
0]i ·
3
[ˆzy]i + 2[wy
0]i
· ∥Ai:∥2
2
A2
ij
!
.
By the guarantees of ApproxExpMaintainer, this takes total time bounded by O(log(mn)).
Performing the update. To perform the update, by observing the form of steps in Algorithm 12 with
our choice of entropy regularizer, the update form given by the regularized mirror-descent step is
(as derived in the discussion of the initialization of AEMx, see (3.35))
[w⋆x ←Π∆((wx)κ ◦exp((1 −κ) log wx
0 −ηκgx(w0) −ηκcej)).
To implement this, recalling our choice of the vector v in the initialization of AEMx, it suﬃces to call
AEMx.DenseStep();
AEMx.MultSparse(−ηκcej);
AEMx.UpdateSum().
By assumption, each operation takes time bounded by O(log4(mn)), where we note the vector used
in the MultSparse operation is 1-sparse.
The implementation of this update is correct up to a
˜ε-padding, whose error we handled previously. By the discussion in the data structure initialization
section, this preserves the invariant that the x block of the current iterate is maintained by AEMx.
Average iterate computation. At the end of each run of Algorithm 12, we compute and return
the average iterate via calls AEMx.GetSum(j) for each j ∈[n], and scaling by 1/T, and similarly query
AEMy. The overall complexity of this step is O((m + n) log2(mn)). The correctness guarantee, i.e.
that the output approximates the average in ℓ1 norm up to ϕ/LD, is given by the choice of ˜ε and

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
120
the guarantees of ApproxExpMaintainer, where in this case the domain size D is bounded by
√
2.
This is never the dominant factor in the runtime, as it is dominated by the cost of initializations.
Algorithm guarantee
Theorem 11. In the ℓ1-ℓ1 setup, let nnz′ := nnz + (m + n) log3(mn).
The implementation in
Section 3.4.2 with the optimal choice of α = max

ϵ/3, L1,1
co log2 (mn) /
√
nnz′

has runtime
O
  
nnz′ +
 L1,1
co
2 log4(mn)
α2
!
α log(mn)
ϵ
!
= O
 
nnz′ +
√
nnz′L1,1
co log3(mn)
ϵ
!
and outputs a point ¯z ∈Z such that
EGap(¯z) ≤ϵ.
Proof. The correctness of the algorithm is given by the discussion in Section 3.4.2 and the guarantees
of Proposition 10 with K = 3αΘ/ϵ, εouter = 2ϵ/3, εinner = ϵ/3, Proposition 11 with ϕ = ϵ/15, and
the data structure ApproxExpMaintainer with our choice of
˜ε := (m + n)−8,
to meet the approximation conditions in Line 5, 7 and 8 of Algorithm 12. The runtime bound is
given by the discussion in Section 3.4.2, and the optimal choice of α is clear.
3.5
Data structure implementation
In this section, we give implementations of our data structures, fulﬁlling the interface and runtime
guarantees of Section 3.2.4. In Section 3.5.1 we provide the implementation of IterateMaintainerp
for p ∈{1, 2} used for sublinear coordinate methods. In Section 3.5.2, we provide an implementa-
tion of ApproxExpMaintainer used in variance-reduced coordinate methods for simplex domains,
provided we have an implementation of a simpler data structure, ScaleMaintainer, which we then
provide in Section 3.5.3.
3.5.1
IterateMaintainerp
The IterateMaintainerp, p ∈{1, 2} data structure is described in Section 3.2.4 and used for
tracking the iterates in our fully stochastic methods and the Euclidean part of our the iterates in our
variance-reduced methods. The data structure maintains an internal representation of x, the current
iterate, and s, a running sum of all iterates. The main idea behind the eﬃcient implementation of
the data structure is to maintain x and s as a linear combination of sparsely-updated vectors. In
particular, the data structure has the following state: scalars ξu, ξv, σu, σv, ι, ν; vectors u, u′, v,

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
121
and the scalar ∥v∥2
2; the vector v is only relevant for variance reduction and is therefore set 0 for the
non-Euclidean case p = 1.
We maintain the following invariants on the data structure state at the end of every operation:
• x = ξuu + ξvv, the internal representation of x
• s = u′ + σuu + σvv, the internal representation of running sum s
• ι = ⟨x, v⟩, the inner product of the iterate with ﬁxed vector v
• ν = ∥x∥p, the appropriate norm of the iterate
In addition, to support sampling, our data structure also maintains a binary tree distx of depth
O(log n). Each leaf node is associated with a coordinate j ∈[n], and each internal node is associated
with a subset of coordinates corresponding to leaves in its subtree. For the node corresponding to
S ⊆[n] (where S may be a singleton), we maintain the sums P
j∈S[u]p
j, P
j∈S[u]j[v]j, and P
j∈S[v]p
j.
We now give the implementation of each operation supported by IterateMaintainerd, followed
by proofs of correctness and of the runtime bounds when applicable.
Initialization
• Init(x0, v). Runs in time O(n).
If p = 1 set v ←0n; otherwise we compute and store ∥v∥2
2. Initialize the remaining data struc-
ture state as follows: (ξu, ξv, u) ←(1, 0, x0), (σu, σv, u′) ←(0, 0, 0n), (ι, ν) ←(⟨x0, v⟩, ∥x0∥p).
Initialize distx, storing the relevant sums in each internal node.
It is clear that x = ξuu + ξvv, s = u′ + σuu + σvv, and that the invariants of ι, ν hold. Each
step takes O(n) time; for the ﬁrst 4 steps this is immediate, and the ﬁnal recursing upwards from
the leaves spends constant time for each internal node, where there are O(n) nodes.
Updates
• Scale(c): x ←cx. Runs in time O(1).
Multiply each of ξu, ξv, ν, ι by c.
• AddSparse(j, c): [x]j ←[x]j + c, with the guarantee c ≥−[x]j if p = 1. Runs in time O(log n).
1. u ←u +
c
ξu ej.
2. u′ ←u′ −cσu
ξu ej.
3. If p = 1, ν ←ν + c. If p = 2, ν ←
p
ν2 + 2c[ξuu + ξvv]j + c2.
4. ι ←ι + c[v]j.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
122
5. For internal nodes of distx on the path from leaf j to the root, update P
j∈S[u]p
j,
P
j∈S[u]j[v]j appropriately.
• AddDense(c): x ←x + cv. Runs in time O(1). (Supported only for p = 2).
Set ξv ←ξv + c, ν ←
q
ν2 + 2cι + c2 ∥v∥2
2, and ι ←ι + c ∥v∥2
2.
• UpdateSum(): s ←s + x. Runs in time O(1).
Set σu ←σu + ξu and σv ←σv + ξv.
Each of the runtime bounds clearly hold; we now demonstrate that the necessary invariants are
preserved. Correctness of Scale and UpdateSum are clear. Regarding correctness of AddSparse,
note that (ignoring the v terms when p = 1)
ξu

u + c
ξu
ej

+ ξvv = ξuu + ξvv + cej,

u′ −cσu
ξu
ej

+ σu

u + c
ξu
ej

+ σvv = u′ + σuu + σvv.
When p = 1, the update to ν is clearly correct. When p = 2, because only [x]j changes,
[ξuu + ξvv + cej]2
j = [ξuu + ξvv]2
j + 2c[ξuu + ξvv]j + c2,
([ξuu + ξvv + cej]j) · [v]j = ([ξuu + ξvv]j) · [v]j + c[v]j.
Thus, the updates to the norm and inner product are correct. Regarding correctness of AddDense
when p = 2, we have
ξuu + (ξv + c)v = ξuu + ξvv + cv,
∥x + cv∥2
2 = ν2 + 2cι + c2 ∥v∥2
2 ,
⟨x + cv, v⟩= ι + c ∥v∥2
2 .
Here, we use the invariants that ν = ∥x∥2 and ι = ⟨x, v⟩.
Queries
• Get(j): Return [x]j. Runs in time O(1).
Return ξu[u]j + ξv[v]j.
• GetSum(j): Return [s]j. Runs in time O(1).
Return [u′]j + σu[u]j + σv[v]j.
• Norm(): Return ∥x∥p. Runs in time O(1).

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
123
Return ν.
By our invariants, each of these operations is correct.
Sampling
The method Sample returns a coordinate j with probability proportional to [x]p
j in time O(log n).
To implement it, we recursively perform the following procedure, where the recursion depth is at
most O(log n), starting at the root node and setting S = [n]:
1. Let S1, S2 be the subsets of coordinates corresponding to the children of the current node.
2. Using scalars ξu, ξv, and the maintained P
j∈Si[u]p
j, P
j∈Si[u]j[v]j, P
j∈Si[v]p
j when appropri-
ate, compute P
j∈Si[x]p
j = P
j∈Si[ξuu + ξvv]p
j for i ∈{1, 2}.
3. Sample a child i ∈{1, 2} of the current node proportional to P
j∈Si[x]p
j by ﬂipping an appro-
priately biased coin. Set S ←Si.
It is clear that this procedure samples according to the correct probabilities. Furthermore, step
2 can be implemented in O(1) time using precomputed values, so the overall complexity is O(log n).
3.5.2
ApproxExpMaintainer
In this section, we give the implementation of ApproxExpMaintainer which supports dense update
to simplex mirror descent iterates. For convenience, we restate its interface, where we recall the
notation ∥g∥0 for the number of nonzero entries in g, Deﬁnition 3 of an ε-padding, the invariant
ˆx is a ε-padding of x,
(3.36)
and the notation
ω := max

1
1 −κ, n
λϵ

.
Category
Function
Runtime
initialize
Init(x0, v, κ, ε, λ) : κ ∈[0, 1), ε > 0, minj[x0]j ≥λ
O(n log n log2 ω)
update
MultSparse(g): x ←ε-padding of Π∆(x ◦exp(g))
O(∥g∥0 log2 n log2 ω)
DenseStep(): x ←Π∆(xκ ◦exp(v))
O(log n)
UpdateSum(): s ←s + ˆx (recall invariant (3.25))
O(log n log ω)
query
Get(j): Return [ˆx]j
O(log n log ω)
GetSum(j): Return [s]j
O(log2 ω)
sample
Sample(): Return j with probability [ˆx]j
O(log n log ω)

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
124
We build ApproxExpMaintainer out of a simpler data structure called ScaleMaintainer, which
maintains the simplex projection of ﬁxed vectors raised elementwise to arbitrary powers; this suﬃces
to support consecutive DenseStep calls without MultSparse calls between them. To add support
for MultSparse, we combine O(log n) instances of ScaleMaintainer in a formation resembling a
binomial heap: for every entry updated by MultSparse we delete it from the ScaleMaintainer
instance currently holding it, put it in a new singleton ScaleMaintainer instance (after appropriate
scaling due to MultSparse), and merge this singleton into existing instances. We now give a brief
description of the ScaleMaintainer interface, and based on it, describe the implementation of
ApproxExpMaintainer. We will provide the implementation of ScaleMaintainer in Section 3.5.3.
ScaleMaintainer is initialized with vectors ¯x ∈Rn′
≥0 and ¯δ ∈Rn′ (with n′ ≤n) and supports
eﬃcient approximate queries on vectors of the form
x[σ] := ¯x ◦exp
 σ¯δ

,
for any scalar σ ∈[σmin, 1]. More speciﬁcally, the data structure allows eﬃcient computation of
∥x[σ]∥1 (to within small multiplicative error εscm), as well as entry queries, sampling and running
sum accumulation from a vector ˆx[σ] satisfying
ˆx[σ] is a εscm-padding of Π∆
 ¯x ◦exp
 σ¯δ

=
x[σ]
∥x[σ]∥1
.
(3.37)
We make the following assumptions on the input to the data structure:
λscm ≤[¯x]i ≤1 for all i ∈[n′] and σ ∈(σmin, 1).
The upper bounds on ¯x and σ are arbitrary, and we may choose λscm and σmin to be very small since
the data structure runtime depends on them only logarithmically. To summarize this dependence,
we deﬁne
ωscm := max

1
σmin
,
n
λscmεscm

.
With these assumptions and notation, we deﬁne the formal interface of ScaleMaintainer.
Category
Function
Runtime
initialize
Init(¯x, ¯δ, σmin, εscm, λscm)
O(n′ log n log2 ωscm)
update
Del(j): Remove coordinate j from ¯x, ¯δ
O(1)
UpdateSum(γ, σ): s ←s + γˆx[σ], with ˆx[σ] deﬁned in (3.37)
O(log ωscm)
query
Get(j): Return [ˆx[σ]]j
O(log ωscm)
GetSum(j): Return [s]j.
O(log2 ωscm)
GetNorm(σ): Return 1 ± ε approx. of
¯x ◦exp(σ¯δ)

1
O(log ωscm)
sample
Sample(σ): Return j with probability [ˆx[σ]]j
O(log n log ωscm)

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
125
ApproxExpMaintainer state
Throughout this section, we denote K := ⌈log n⌉. ApproxExpMaintainer maintains a partition of
[n] into K sets S1 . . . , SK (some of them possibly empty) that satisfy the invariant
|Sk| ≤2k for all k ∈[K].
(3.38)
We refer to the index k as "rank" and associate with each rank k ∈[K] the following data
1. Scalar γk ≥0 and nonnegative integer τk.
2. Vectors ¯xk, ¯δk ∈R|Sk| such that λscm ≤[¯xk]i ≤1 for all i ∈[|Sk|], where λscm = min(ε/n, λ).
3. A ScaleMaintainer instance, denoted ScaleMaintainerk, initialized with ¯xk, ¯δk and λscm
deﬁned above, σmin = 1 −κ and εscm = ε/10, so that log ωscm = O(log ω).
ApproxExpMaintainer also maintains a vector u ∈Rn for auxiliary running sum storage.
Deﬁne the vector δ ∈Rn by
[δ]Sk = log
 γk

¯xk ◦exp
 (1 −κτk)¯δk

, k ∈{1, . . . , K},
(3.39)
where [δ]Sk denotes the coordinates of δ in Sk. Recall that x denotes the point in ∆n maintained
throughout the operations of ApproxExpMaintainer; we maintain the key invariant that the point
x is proportional to exp(δ), i.e.,
x =
exp(δ)
∥exp(δ)∥1
.
(3.40)
Speciﬁcally, we show in Section 3.5.2 that our implementation of DenseStep modiﬁes ¯x, ¯δ, {τk}K
k=0,
{γk}K
k=0 so that the resulting eﬀect on δ, per deﬁnition (3.40), is
δ ←κδ + v.
(3.41)
Similarly, our implementation of MultSparse modiﬁes the state so that the resulting eﬀect on δ is
exp(δ)
∥exp(δ)∥1
←ε-padding of
exp(δ + v)
∥exp(δ + v)∥1
.
(3.42)
We remark that the role of γk is to scale ¯xk so that it lies coordinatewise in the range [λscm, 1],
conforming to the ScaleMaintainer input requirement.
This is also the reason we require the
ε-padding operation in the deﬁnition of MultSparse.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
126
ε-padding point ˆx
We now concretely deﬁne the point ˆx, which is the ε-padding of x that ApproxExpMaintainer
maintains. Let
Γ :=
K
X
k=1
γkScaleMaintainerk.GetNorm(1 −κτk),
(3.43)
be the approximation of exp(δ) derived from the ScaleMaintainer instances. For any j ∈[n], let
kj be such that j ∈Skj, and let ij be the index of j in Skj. The jth coordinate of ˆx is
[ˆx]j := γkjScaleMaintainerkj.GetNorm(1 −κτkj )
Γ
· ScaleMaintainerkj.Get(ij, 1 −κτkj ).
(3.44)
Since for each k, P
j∈Sk ScaleMaintainerk.Get(j, 1 −κτk) = ScaleMaintainerk.GetNorm(1 −κτk)
we have that ˆx ∈∆n. We now prove that ˆx is a ε-padding of x. To do so, we prove the following
lemma.
Lemma 37. Let εscm ≤
1
10 and {Sk}K
k=1 be a partition of [n]. Suppose for each k ∈[K], ˆxk ∈∆|Sk|
is an εscm-padding of xk ∈∆|Sk|.
Further, suppose we have positive scalars {νk}K
k=1, {ˆνk}K
k=1
satisfying
(1 −εscm)νk ≤ˆνk ≤(1 + εscm)νk, for all 1 ≤k ≤K.
Then, for N = PK
k=1 νk and ˆN = PK
k=1 ˆνk, we have that ˆx := PK
k=1
ˆνk
ˆ
N ˆxk is a 10εscm-padding of
x := PK
k=1
νk
N xk.
Proof. For every k ∈[K], let ˜xk to be such that ˜xk ≥xk elementwise, ˆxk = ˜xk/ ∥˜xk∥1, and
∥˜xk −xk∥1 ≤εscm. Consider the point
˜x :=
K
X
k=1
˜νk
ˆN
˜xk,
where ˜νk := ˆνk
maxk∈[K] ∥˜xk∥1
∥˜xk∥1
· 1 + εscm
1 −εscm
,
so that ˆx = ˜x/ ∥˜x∥1. Since ˜xk ≥xk elementwise, ˆνk ≥(1 −εscm)νk and ˆN ≤(1 + εscm)N, we have
that ˜x ≥x elementwise. Furthermore, we have ˆνk ≤(1 + εscm)νk and ˆN ≥(1 −εscm)N, and the
properties ˜xk ≥xk and ∥˜xk −xk∥1 imply 1 ≤∥˜xk∥1 ≤1 + εscm as well as
maxk∈[K]∥˜xk∥1
∥˜xk∥1
≤1 + εscm.
Therefore
∥˜x −x∥1 ≤
K
X
k=1
νk
N

(1 + εscm)3
(1 −εscm)2 ˜xk −xk

1
≤
(1 + εscm)3
(1 −εscm)2 −1

(1 + εscm) + εscm ≤10εscm,
where the ﬁnal bound is veriﬁed numerically for εscm ≤1/10.
The ScaleMaintainer interface guarantees that calls to ScaleMaintainerk.GetNorm return
¯xk ◦exp((1 −κτk)¯δk)

1 to within a 1 ± εscm multiplicative factor, and moreover that Get returns

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
127
entries from an εscm-padding of Π∆(¯xk ◦exp((1 −κτk)¯δk)). Thus, applying Lemma 37 with our
deﬁnition of ˆx in (3.44) yields that ˆx is a 10εscm = ε-padding of x.
ApproxExpMaintainer initialization and updates
We give the implementation and prove runtimes of Init, MultSparse, DenseStep, and UpdateSum.
Init.
Upon initialization of ApproxExpMaintainer, we set γK = maxj∈[n][x0]j and τk = 0
for all k. We let SK = [n] (so that Sk = ∅for all k < K) and instantiate a single instance of
ScaleMaintainer of rank K with parameters
¯xK = x0
γK
, ¯δK =
v
1 −κ −log x0, εscm = ε
10, λscm = min
 ε
n, λ

.
It is clear that the invariant (3.40) holds at initialization, and that the coordinates of ¯xK lie in the
appropriate range, since we assume that x0 ∈[λ, 1]n. We will use the same choices of εscm, λscm for
every ScaleMaintainer instance. The overall complexity of this operation is O(n log n log2 ω).
MultSparse. We state the implementation of MultSparse, prove that the resulting update is
(3.42), and ﬁnally give its runtime analysis. We perform MultSparse(g) in sequence for each nonzero
coordinate of g. Let j denote such nonzero coordinate and let kj be such that j ∈Skj; the operation
consists of the following steps.
1. Remove j from Skj and delete the corresponding coordinate from ScaleMaintainerkj (via a
call to Del).
2. Let S0 = j and initialize ScaleMaintainer0 with initial data ¯x and ¯δ described below.
3. For k going from 1 to K, set Sk ←Sk ∪Sk−1 and Sk−1 = ∅, merging ScaleMaintainerk and
ScaleMaintainerk−1 as described below. If the new set Sk satisﬁes |Sk| ≤2k, break the loop;
else, proceed to the next k.
We now state the initial data given to each ScaleMaintainer upon initialization in the steps
above. Whenever a ScaleMaintainerk is created supporting Sk ⊆[n], we ﬁrst compute δi for each
i ∈Sk according to (3.40). When creating the singleton instance ScaleMaintainer0 we perform the
update
δj ←δj + gj;
(3.45)
this implements multiplication of the jth coordinate by exp(gj). To instantiate ScaleMaintainerk,
we set τk = 0, γk ←maxi∈Sk exp([δ]i) and modify δ according to
[δ]Sk ←max {[δ]Sk, log (λscm · γk)} .
(3.46)
In other words, we raise very small entries of [δ]Sk to ensure that the ratio between any two entries

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
128
of [exp(δ)]Sk is in the range [λ−1
scm, λscm]. We then give ScaleMaintainerk the initial data
¯xk = 1
γk
[exp (δ)]Sk , ¯δk =

v
1 −κ −δ

Sk
.
(3.47)
It is clear that entries of ¯xk are in the range [λscm, 1], and invariant (3.40) holds at initialization,
as τk = 0. Therefore, the operation (3.45) implements x ←Π∆(x ◦exp(g)) exactly; it remains to
show that the operation (3.46) amounts to an ε-padding of exp(δ)/ ∥exp(δ)∥1. We do so by invoking
the following lemma, substituting the values of δ before (3.45) for δ−and δ+ respectively, and
λscm ≤ε/n for ρ.
Lemma 38. Let δ−, δ+ ∈Rn satisfy
[δ+]i = max

[δ−]i, max
j [δ−]j + log ρ

for all j ∈[n] and ρ ≤1.
Then, exp(δ+)/ ∥exp(δ+)∥1 is a ρn-padding of exp(δ−)/ ∥exp(δ−)∥1.
Proof. Let x = exp(δ−)/ ∥exp(δ−)∥1, x′ = exp(δ+)/ ∥exp(δ+)∥1, and ˜x = exp(δ+)/ ∥exp(δ−)∥1.
Clearly x′ = ˜x/ ∥˜x∥1 and ˜x ≥x element-wise. Moreover, letting M = maxj exp([δ−]j), we have
∥˜x −x∥1 = ∥exp(δ+) −exp(δ−)∥1
∥exp(δ−)∥1
≤ρM|{i | [δ+]i ̸= [δ−]i}|
∥exp(δ−)∥1
≤
ρM · n
∥exp(δ−)∥1
≤ρn,
establishing the ρn-padding property.
Finally, we discuss runtime. Recall that the cost of initializing a ScaleMaintainer with |S|
elements is O(|S| log n log2 ω). So, step 1 of our implementation of MultSparse, i.e., calling Del
once and initializing a rank-1 ScaleMaintainer per nonzero element, costs O(∥g∥0 log n log2 ω). We
now discuss costs of merging in step 2. We show these merges cost an amoritized O(log2 n log2 ω) per
nonzero coordinate of g, leading to the claimed bound. Speciﬁcally, we show the cost of T deletions
and initializations due to nonzero entries of MultSparse arguments is O(T log2 n log2 ω). Consider
the number of times a rank-k set can be created through merges: we claim it is upper bounded by
O
 T/2k
. It follows that the overall complexity of step 2 is
O
 K
X
k=0
2k T
2k log n log2 ω
!
= O(T log2 n log2 ω).
The claimed bound on the number of rank-k merges holds because at least 2k−1 deletions (and hence
that many MultSparse calls) must occur between consecutive rank-k merges. To see this, for each
k, maintain a potential Φk for the sum of cardinalities of all rank ℓsets for ℓ< k. Each deletion
increases Φk by at most 1. For an insertion merge to create a rank-k set, Φk must have been at least

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
129
2k−1 + 2; after the merge, it is 0, as in its creation, all rank-ℓsets for ℓ< k must have been merged.
So, there must have been at least 2k−1 deletions in between merges.
DenseStep. To implement DenseStep we simply increment τk ←τk + 1 for all k; clearly, this
takes time O(log n). We now show that (3.40) is maintained, i.e., that the resulting update to the
variable δ under DenseStep is (3.41). Recall that ScaleMaintainerk is initialized according (3.47).
Clearly, (3.40) holds at initialization for the set Sk, as 1 −κ0 = 0. We now show that it continues
to hold after any number of DenseStep calls. Let δ0 be the value of [δ]Sk when ScaleMaintainerk
is initialized, and let δτ be the value of [δ]Sk after τ calls to DenseStep, each performing the update
x ←Π∆(xκ ◦exp(v)). This is consistent with the update δτ+1 = κδτ + [v]Sk, which requires
δτ = κτδ0 +
τ−1
X
τ ′=0
κτ ′[v]Sk = κτδ0 + 1 −κτ
1 −κ [v]Sk = log(γk¯xk) + (1 −κτ)¯δk,
where in the ﬁnal transition we substituted δ0 = log(γk¯xk) and [v]Sk = (1 −κ)[¯δk + δ0] according
to (3.47). We see that the required of form of δτ is identical to its deﬁnition (3.39) and consequently
that (3.40) holds.
UpdateSum. We maintain the running sum s via the invariant
[s]Sk = [u]Sk + ScaleMaintainerk.GetSum(), ∀k ∈[K],
(3.48)
which we preserve in two separate procedures. First, whenever UpdateSum() is called, we compute
the quantity Γ deﬁned in (3.43), and for each k ∈[K] call
ScaleMaintainerk.UpdateSum
γkScaleMaintainerk.GetNorm(1 −κτk)
Γ

.
It is straightforward to see that this indeed preserves the invariant (3.48) for our deﬁnition of ˆx in
(3.44), and takes time O(log n log ω). Next, whenever a coordinate is deleted from a ScaleMaintainerk
instance, or an entire ScaleMaintainerk instance is deleted due to a merge operation, we update
uj ←uj + ScaleMaintainerk.GetSum(j)
for every deleted coordinate j, or j involved in the merge, respectively. We charge the cost of this
operations to that of new ScaleMaintainer instance, which we accounted for in the analysis of
MultSparse.
Queries
Get(j).
Recalling our deﬁnition of ˆx (3.44), we compute Γ in time O(log n log ω) by obtaining
GetNorm(1 −κτk) for each k, and then call Get(j, 1 −κτk) in time O(log ω) for the relevant k.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
130
GetSum(j). Recalling our deﬁnition of s (3.48), we implement GetSum(j) in O(log ω) time via a
single call to GetSum on the relevant ScaleMaintainer instance, and querying a coordinate of u.
Sample. Recalling (3.44), we ﬁrst compute Γ, as well as all γkScaleMaintainerk.GetNorm(1 −
κτk), in O(log n log ω) time.
We then sample an instance ScaleMaintainerk, for 0 ≤k ≤K,
proportional to the value γkScaleMaintainerk.GetNorm(1 −κτk), in O(log n) time. Finally, for the
sampled instance, we call ScaleMaintainerk.Sample(1−κτk) to output a coordinate in O(log n log ω)
time. By the deﬁnition of ˆx used by ApproxExpMaintainer, as well as the deﬁnitions used by each
ScaleMaintainerk instance, it is clear this preserves the correct sampling probabilities.
3.5.3
ScaleMaintainer
Finally, we provide a self-contained treatment of ScaleMaintainer, the main building block in the
implementation of ApproxExpMaintainer described above.
Interface
For ease of reference we restate the interface of ScaleMaintainer, where for the sake of brevity we
drop the subscript scm from ε and λ, and use n rather than n′ to denote the input dimension. Recall
that for the vectors ¯x and ¯δ given at initialization, the data structure keeps track of vectors of the
form
ˆx[σ] := a ε-padding of Π∆
 ¯x ◦exp
 σ¯δ

,
(3.49)
where σ is any scalar in the range {0} ∪[σmin, 1].
The implementation of the data structure relies on three internal parameters: polynomial ap-
proximation order p ∈N, truncation threshold R ≥0, and σ discretization level K ∈N. To satisfy
the accuracy requirements we set these as
R = Θ(1) log 1
ελ,
p = Θ(1) log 1
ελ,
and K =

log
1
σmin

;
we give the runtime analysis in terms of these parameters.
Category
Function
Runtime
initialize
Init(¯x, ¯δ, σmin, ε, λ): require ¯x ∈[λ, 1]n
O(npK log n)
update
Del(j): Remove coordinate j from ¯x, ¯δ
O(1)
UpdateSum(γ, σ): s ←s + γˆx[σ]
O(p)
query
Get(j, σ): Return [ˆx[σ]]j
O(p)
GetSum(j): Return [s]j.
O(pK)
GetNorm(σ): Return 1 ± ε approx. of
¯x ◦exp(σ¯δ)

1
O(p)
sample
Sample(σ): Return j with probability [ˆx[σ]]j
O(p log n)

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
131
Overview
We now outline our design of ScaleMaintainer, where the main challenge is supporting eﬃcient
GetNorm operations under no assumptions on the numerical range of the input ¯δ.
Exponential approximation via Taylor expansion. Our main strategy is to replace the exponential
in the deﬁnition of ˆx[σ] with its Taylor expansion of order p = O(log n
ελ), giving the following
approximation to the GetNorm(σ)
¯x ◦exp
 σ¯δ

1 ≈
*
¯x,
p
X
q=0
1
q!(σ¯δ)q
+
=
p
X
q=0
σq
q!

¯x, ¯δq
,
where qth powers are applied to ¯δ elementwise. By pre-computing all the inner products {

¯x, ¯δq
}p
q=0
at initialization, we may evaluate this Taylor approximation of GetNorm in time O(p). The validity
of the approximation relies on the following well-known fact.
Fact 2 (Theorem 4.1 in [466]). Let ε′, R ≥0. A Taylor series fp(t) = Pp
q=0
tq
q! of degree p =
O(R + log 1
ε′ ) satisﬁes
| exp(t) −fp(t)| ≤exp(t)ε′ for all t ∈[−R, 0].
Truncating small coordinates and σ discretization.
For Fact 2 to directly imply the desired
approximation guarantee for GetNorm, the entries of σ¯δ must all lie in [−R, 0] for some R = eO (1).
However, this will not hold in general, as our data structure must support any value of ¯δ. For a
ﬁxed value of σ, we can work instead with a shifted and truncated version of ¯δ, i.e.,
˜δ[σ, µ] := max{¯δ −µ, −R/σ},
where the oﬀset µ is roughly the maximum element of ¯δ.
Fact 2 allows us to approximate the
exponential of σ˜δ[σ, µ], and for R = Θ(log( n
ελ)) we argue that the truncation of the smallest entries
of δ results in small multiplicative error. Unfortunately, the dependence of ˜δ[σ, µ] on σ would defeat
the purpose of eﬃcient computation, because it is impossible to precompute {

¯x, ˜δ[σ, µ]q
}p
q=0 for
every σ ∈[σmin, 1]. To address this, we argue that truncation of the form ˜δ[ˆσ, µ] is accurate enough
for any σ ∈[ˆσ/2, ˆσ]. Therefore, it suﬃces to to discretize [σmin, 1] into K = ⌈log
1
σmin ⌉levels
ˆσk := 2k−1σmin
and precompute

¯x, ˜δ[ˆσk, µ]q
for every k ∈[K] in q ≤p. This allows us to compute GetNorm(σ) in
O(p) = eO (1) time, with O(npK) = eO (n) preprocessing time.
Supporting deletion via lazy oﬀset selection. Had the dataset not supported deletions, we could
have simply set µ to be the largest entry of ¯δ (independent of k). However, with deletions the

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
132
largest entry of ¯δ could change, potentially invalidating the truncation. To address this, we maintain
a diﬀerent threshold µk for every k ∈[K], and argue that the approximation remains valid if the
invariant
¯δmax ≤µk ≤¯δmax + R
2ˆσk
for every k ∈[K]
(3.50)
holds, where ¯δmax := maxj ¯δj. Writing
˜δ[k] := ˜δ[ˆσk, µk] = max

¯δ −µk, −R
ˆσk

for every k ∈[K],
(3.51)
the data structure only needs to maintain µk and

¯x, ˜δ[k]q
for every k ∈[K] in q ≤p.
When deleting coordinate j, for every k we test whether the invariant (3.50) remains valid.5 If
it does, we keep µk the same and implement deletion (for this value of k) in time O(p) = eO (1)
by subtracting [¯x]j[˜δ[k]]q
j from

¯x, ˜δ[k]q
for every q ≤p. If the invariant is no longer valid, we
reset µk to the new value of ¯δmax and recompute

¯x, ˜δ[k]q
for every q ≤p.
Note that the re-
computation time is proportional to the number of un-truncated coordinates in the newly deﬁned
˜δ[k]. The key observation here is that every re-computation decreases µk by at least R/(2ˆσk) and
so no element of ¯δ can remain un-truncated for more than two re-computation. Therefore, the cost
of recomputing inner products due to deletions, for the entire lifetime of the data structure, is at
most O(npK) = eO (n), which we charge to the cost of initialization.
Explicit expression for ˆx[σ]. Following the preceding discussion, for any σ ≥σmin we set
k⋆=

log2
σ
σmin

,
so that σ ∈
 ˆσk⋆
2 , ˆσk⋆

,
and deﬁne
Z[σ] := eσµk⋆
p
X
q=0
σq
q!
D
¯x, ˜δ[k⋆]qE
≈
¯x ◦exp
 σ¯δ

1
(3.52)
ˆx[σ] := eσµk⋆
Z[σ]
p
X
q=0
σq
q! ¯x ◦˜δ[k⋆]q ≈Π∆
 ¯x ◦exp
 σ¯δ

,
(3.53)
with ˜δ as deﬁned in (3.51).
Correctness
We now prove that the approximation guarantees of ScaleMaintainer hold.
Proposition 12. There exist R = O(1)·log n
ελ and p = O(1)·log n
ελ such that for all σ ∈[σmin, 1], if
the invariant (3.50) holds we have that Z[σ] is an ε multiplicative approximation of
¯x ◦exp
 σ¯δ

1
5We can query the maximum entry of ¯δ under deletions in O(1) time via a standard data structure, e.g. a doubly-
linked list of the sorted entries of ¯δ.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
133
and ˆx[σ] is a ε-padding of Π∆
 ¯x ◦exp
 σ¯δ

, with Z[σ] and ˆx[σ] deﬁned in Eq.s (3.52) and (3.53)
respectively.
Proof. To simplify notation, we write µ = µk⋆and ˆσ = ˆσk⋆. We begin by noting that the inequal-
ities (3.50) and σ ≤ˆσ imply that σ˜δi[k⋆] ∈[−R, 0] for every i ∈[n] and we may therefore apply
Fact 2 to obtain
p
X
q=0
σq
q! ¯xj˜δi[k⋆]q ≥(1 −ε′)¯xi exp(σ˜δi[k]) ≥(1 −ε′)e−σµ¯xi exp(σ¯δi[k])
(3.54)
for every i ∈[n]. Therefore, we have
Z[σ] ≥(1 −ε′)
¯x ◦exp
 σ¯δ

1 .
(3.55)
Similarly, we have
p
X
q=0
σq
q! ¯xj˜δi[k⋆]q ≤(1 + ε′)¯xi exp(σ˜δi[k]) ≤(1 + ε′)e−σµ¯xi(exp(σ¯δi[k]) + exp(−σR/ˆσ))
Note that the condition (3.50) also implies that ˜δj[k⋆] ≥−R/(2ˆσ) for some j ∈[n] (namely the
maximal element of ¯δ). Using also ¯xj ≥λ, we have
eσµ exp(−σR/ˆσ) ≤exp(−σR/(2ˆσ)) ¯xj
λ exp(σ¯δj).
Taking R ≥2 log 2n
λε′ and recalling that σ ≥ˆσ/2, we have exp(−σR/(2ˆσ)) ≤λε′/(2n) and conse-
quently
eσµ exp(−σR/ˆσ) ≤ε′¯xj exp(σ¯δj) ≤ε′
n
¯x ◦exp
 σ¯δ

1 .
Substituting back and using ¯xi ≤1 and ε′ < 1 gives
eσµ
p
X
q=0
σq
q! ¯xj˜δi[k⋆]q ≤(1 + ε′)¯xi exp(σ¯δi) + ε′
n
¯x ◦exp
 σ¯δ

1 .
(3.56)
Summing over i ∈[n], we obtain
Z[σ] ≤(1 + 2ε′)
¯x ◦exp
 σ¯δ

1
(3.57)
Therefore, Z[σ] is a 2ε′-multiplicative approximation of
¯x ◦exp
 σ¯δ

1.
It remains to show that ˆx[σ] is a ε-padding of x[σ] := Π∆
 ¯x ◦exp
 σ¯δ

. First, if we deﬁne
˜x =
1+2ε′
1−ε′ ˆx[σ] then the bounds (3.54) and (3.57) imply that ˜x ≥x[σ] elementwise.
Also, the

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
134
bounds (3.55) and (3.56) imply that
ˆxi[σ] −xi[σ] ≤(1 + ε′)xi[σ] + ε′/n
1 −ε′
for every i ∈[n]. Therefore, for ε′ < 1/10,
∥˜x −x[σ]∥1 ≤
1 + 2ε′
1 −ε′
2
−1 ≤10ε′,
so that ˆx[σ] is a 10ε′ padding of x[σ]. Taking ε′ = ε/10 concludes the proof.
Implementation: data structure state and initialization
Besides storing ¯x and ¯δ, the data structure maintains the following ﬁelds.
1. An oﬀset µk ∈R for every k ≤K =
l
log
1
σmin
m
, initialized as µk = maxj[¯δ]j for all k.
2. A balanced binary tree with n leaves. For node v in the tree, k ∈[K] and q ∈{0, . . . , p}, we
store
Av[k, q] :=
D
¯x, ˜δ[k]qE
Sv
,
where ˜δ[k] = max{¯δ −µk, −R/ˆσk} as before, the set Sv contains the leaves in the subtree
rooted in v, and ⟨a, b⟩S := P
i∈S aibi. When referring to the root of the tree we omit the
subscript, i.e., we write
A[k, q] :=
D
¯x, ˜δ[k]qE
.
3. A vector u ∈Rn and coeﬃcients ck,q ∈R for every k ∈[K] and q ∈{0, . . . , p}, for maintaining
the running sum. We initialize them all to be 0. The running sum obeys the following invariant:
s = u +
K
X
k=1
p
X
q=0
ck,q
q! ¯x ◦˜δ[k]q.
(3.58)
4. A doubly linked list of the sorted entries of ¯δ, with a pointer to the maximal element of ¯δ as
well as pointers to the largest element smaller than µk −R/ˆσk for every k ∈[K].
Initializing the data structure for maintaining the maximum element takes time O(n log n) due
to the need to sort ¯δ. With it, initializing µk is trivial and so is the initialization of u and cq,k.
Initializing the data stored in the binary tree takes time O(npK), since for every value k and q and
internal node v with children v′, v′′ we can recursively compute Av[k, q] as Av′[k, q] + Av′′[k, q]. We
will also charge some additional deletion costs to the initialization runtime, resulting in the overall
complexity O(npK log n).

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
135
Implementation: queries and sampling
GetNorm(σ). We compute k⋆=
l
log2
σ
σmin
m
and return Z[σ] = eσk⋆Pp
q=0
σq
q! A[k⋆, q]. Clearly, this
takes O(p) time and Proposition 12 provides the claimed approximation guarantee.
Get(j, σ). We compute Z[σ] and k⋆as described above and return eσµk⋆
Z[σ]
Pp
q=0
σq
q! ¯xj˜δj[k⋆]q in
accordance with the form (3.53) of ˆx[σ]. Again, this takes O(p) time and Proposition 12 provides
the claimed approximation guarantee.
GetSum(j). Recalling the invariant (3.58), we return uj +PK
k=1
Pp
q=0
ck,q
q! ¯xj˜δj[k]q in time O(pK).
Sample(σ). We perform a random walk from the root of our binary tree data structure to a leaf.
A each internal node v with children v′ and v′′, we select node v′ with probability
⟨1, ˆx[σ]⟩Sv′
⟨1, ˆx[σ]⟩Sv
=
Pp
q=0
σq
q! Av′[k⋆, q]
Pp
q=0
σq
q! Av[k⋆, q] ,
and otherwise select v′′, where k⋆=
l
log2
σ
σmin
m
. We return the index associated with the leaf in
which we end the walk; the probability of returning index j is exactly [ˆx[σ]j. Each step in the walk
takes time O(p) and there are O(log n) steps, so the total time is O(p log n).
Implementation: updates
UpdateSum(σ). Recalling the invariant (3.58) and the form (3.53) of ˆx[σ], we compute k⋆and Z[σ]
as in the GetNorm implementation, and update update
ck⋆,q ←ck⋆,q + eσµk⋆σq
Z[σ]
for every q ∈{0, . . . , p}. This takes time O(p).
Del(j). We set [¯δj] ←−∞, remove the element corresponding to index j from the doubly linked
list, and perform the following operations for each k ∈[K] separately. First, we check if the new
maximum element of ¯δ is a least µk −R/(2ˆσk). If it is, we leave µk unchanged and we simply update
Av[k, q] ←Av[k, q] −¯xj˜δj[k]q
for every q ≤p and node v on the path from the root to the leaf corresponding to index j. Since
the length of the path is O(log n), this update takes time O(p log n).
Otherwise, the new maximum element is less than µk −R/(2ˆσk), and we must change µk in order
to maintain the invariant (3.50). Let µnew
k
be the new maximum element of ¯δ, and let
Uk =

i
 [¯δ]i ≥µnew
k
+ R
ˆσk

be the new set of un-truncated indices. (We ﬁnd the elements in this set when we update the pointer

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
136
to the ﬁrst element smaller than µk −R/ˆσk). We recompute Av[k, q] =
D
¯x, ˜δ[k]qE
Sv for every q ≤p
and every node v with a child in Uk. Performing the computation recursively from leaf to root, this
take at most O(|Uk|p log n) time. To maintain the invariant (3.58) as the deﬁnition of ˜δ[k] changes,
we update
uj ←uj +
p
X
q=0
ck,q
q! ¯xj
 [¯δj −µnew
k
]q −[max
¯δj −µk, −R/ˆσk
	
]q
for every j ∈Uk;
this update takes O(|Uk|p) time. Finally, we update µk ←µnew
k
.
Summing over k ∈[K], deletion operations of the ﬁrst kind (with µk unchanged) take at most
O(Kp log n) time per call to Del. Operations of the second kind (with µk decreased) take time
O(Np log n) throughout the data structure lifetime, where N = P
t≥1
PK
k=1 |U (t)
k | and for each
k ∈[K] we write U (1)
k , U (2)
k , . . . to denote the diﬀerent sets Uk generated by all calls to Del. For
each k, if µk is decreased at all then it must decrease by at least R/(2ˆσk). Therefore, by deﬁnition
of Uk, an index j can belong to U (t)
k
for at most 2 values of t. Consequently, we have N = O(nK).
Therefore, deletion operations of the second kind contribute at most O(nKp log n) to the total
runtime, which we charge to initialization.
3.6
Applications
In this section, we leverage the techniques of this chapter to obtain improved runtimes for solving
certain structured optimization problems.
In Sections 3.6.1 and 3.6.2, we use a variant of our variance-reduced coordinate method in the
ℓ2-ℓ1 setup to obtain algorithms for solving the maximum inscribed ball (Max-IB) and minimum
enclosing ball (Min-EB) problems. Our algorithms improve upon the runtimes of those in [22] by
a factor depending on the sparsity of the matrix. This improvement stems from a preprocessing
step in [22] where the input is randomly rotated to improve a norm dependence of the algorithm.
Our methods avoid this preprocessing and obtain runtimes dependent on the both the sparsity
and numerical sparsity of the data, providing universal improvements in the sparse regime, in the
non-degenerate case where the span of the points is full-rank.
In Section 3.6.3, we use the results of our variance-reduced algorithm in the ℓ2-ℓ2 setup (cf.
Section B.4.2) to obtain improved regression algorithms for a variety of data matrices, including
when the matrix is numerically sparse or entrywise nonnegative.
Our methods in this section rely on an extension of the outer loop of this chapter (Algorithm 11)
for strongly monotone minimax problems, developed in our previous work [111]. Speciﬁcally, for a
separable regularizer r(x, y) = rx(x) + ry(y) on a joint space for any of our setups, consider the

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
137
following composite bilinear minimax problem:
min
x∈X max
y∈Y f(x, y) := y⊤Ax + µxφ(x) −µyψ(y), where φ = V x
x′, ψ = V y
y′.
(3.59)
We call such problem a (µx, µy)-strongly monotone problem; this is a special case of a general-
ization of the notion of strong convexity, in the case of convex minimization. For general strongly-
monotone problems, [111] provided a variant of Algorithm 11 with the following guarantee.
Proposition 13 (Proposition 5, [111]). For problem (3.59), denote µ := √µxµy and ρ :=
p
µx/µy.
Let O be an (α,ε)-relaxed proximal oracle for operator g(x, y) := (∇xf(x, y), −∇yf(x, y)), let Θ be
the range of r, and let ∥(∇xf(z), −∇yf(z′))∥∗≤G, for all z, z′ ∈Z. Let zK be the output of K
iterations of OuterLoopStronglyMonotone, Algorithm 7 of [111]. Then
EGap(zK) ≤
√
2G
v
u
u
t
 
α
µ + α
K 
ρ + 1
ρ

Θ + ε
µ
!
.
Each iteration k ∈[K] consists of one call to O, producing a point zk−1/2, and one step of the form
zk ←
n
g(zk−1/2), z

+ α ˆVzk−1(z) + µ ˆVzk−1/2(z)
o
,
(3.60)
where ˆV := ρV x + ρ−1V y. In particular, by setting
ε = µϵ2
4G2 ,
using K = eOα/µ iterations, we have the guarantee EGap(zK) ≤ϵ.
For self-containment, we give a proof of (a generalization of) Proposition 19 in Appendix B.7.
The (α,ε)-relaxed proximal oracle works similarly as in Algorithm 12 except for the additional
composite terms. For completeness we include the algorithm with its theoretical guarantees and
implementation in Section B.5.2 (see Algorithm 68, Proposition 58 and Section 8).
In all of our applications discussed in this section, the cost of each step (3.60) is O(nnz), stemming
from the computation of g(x, y). The resulting algorithms therefore have runtime
˜O

(nnz + (cost of implementing O)) · α
µ

.
3.6.1
Maximum inscribed ball
In the maximum inscribed ball (Max-IB) problem, we are given a polyhedron P ⊂Rn deﬁned by
m halfspaces {Hi}i∈[m], each characterized by a linear constraint Hi = {x ∈Rn : ⟨ai, x⟩+ bi ≥0},
i.e. P = ∩i∈[n]Hi. The goal is to (approximately) ﬁnd a point x∗∈P that maximizes the smallest

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
138
distance to any of the bounding hyperplanes Hi, i.e.
x∗∈argmaxx∈P min
i∈[n]
⟨ai, x⟩+ bi
∥ai∥2
.
More formally, if the optimal radius of the maximum inscribed ball is r∗, the goal is to ﬁnd an
ϵ-accurate solution, i.e. a point in P which has minimum distance to all bounding hyperplanes at
least (1 −ϵ)r∗.
Given halfspace information A, b where Ai: = ai for all i ∈[m], the polytope is deﬁned by
P = {x | Ax + b ≥0}. We use the following notation in this section: B := ∥b∥∞, r∗is the value
of the maximum inscribed ball problem, R is the radius of the minimum enclosing ball, which is
deﬁned as the Euclidean ball containing P with smallest radius possible, x∗is the center of the
maximum inscribed ball, and ρ is an upper bound on the aspect ratio R/r∗. As in [22], we will make
the following assumptions:
1. The polytope is bounded, and thus m ≥n. This is without loss of generality since when the
polytope is unbounded, the aspect ratio ρ = ∞, and our runtime result holds trivially.
2. ∥Ai:∥2
2 = 1 for all i ∈[m], so ∥A∥2→∞= 1, by properly scaling A (one can consider the trivial
case when for some i, ai = 0 separately).
3. The origin is inside polytope P, i.e. O ∈P, by properly shifting P.
We also deﬁne the following constant (see Appendix B.4.3) in this section with respect to the
rescaled matrix A,
L2,1
co := min
n
L2,1,(1)
co
, L2,1,(2)
co
, L2,1,(3)
co
o
≤√rcs · L2,1
rc ≤√rcs,
given the deﬁnitions of L2,1,(1)
co
, L2,1,(2)
co
, L2,1,(3)
co
as in (B.33), (B.34), and (B.35), and the second
assumption above (namely, that L2,1
rc = maxi∈[m] ∥Ai:∥2 = 1).
[22] show that solving Max-IB is equivalent to the following minimax problem:
r∗:= max
x∈Rn min
y∈∆m f(x, y) := y⊤Ax + y⊤b,
(3.61)
and moreover, to solve the problem to ϵ-multiplicative accuracy, it suﬃces to ﬁnd x∗
ϵ that solves the
minimax problem to ϵ-multiplicative accuracy in terms of the one-sided gap of the x block, i.e.
min
y∈∆m f(x∗
ϵ, y) ≥(1 −ϵ)f(x∗, y∗),
where (x∗, y∗) is the optimal saddle point of problem (3.61). We ﬁrst state several bounds on the
parameters of the problem from [22].

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
139
Fact 3 (Geometric properties of Max-IB). We have ∥x∗∥2 ≤2R, and
r∗= max
x∈Rn min
y∈∆m f(x, y) := y⊤Ax + y⊤b ≤B ≤2R.
These facts imply that we can instead consider the constrained minimax problem (where we
overload our deﬁnition of f for the rest of the section):
r∗:= max
x∈Bn min
y∈∆m f(x, y) = y⊤˜Ax + y⊤b,
where ˜A = 2R · A.
(3.62)
We ﬁrst use a "warm start" procedure to ﬁnd a constant multiplicative estimate of r∗, which uses
the strongly monotone algorithm OuterLoopStronglyMonotone of [111] together with Algorithm 68
of Section B.5.2 as a relaxed proximal oracle on the (µ, µ)-strongly monotone problem
max
x∈Bn min
y∈∆m fµ(x, y) := y⊤˜Ax + y⊤b + µ
X
i∈[m]
[y]i log[y]i −µ
2 ∥x∥2
2 ,
and a line search over parameter µ. The following lemma is an immediate consequence of Proposi-
tion 19 and Corollary 58, whose proof we defer to Appendix B.6.1.
Lemma 39. We can spend eOnnz + ρ√nnz · L2,1
co
time preprocessing to obtain a 8-multiplicative
approximation ˆr of r∗, i.e.
ˆr
8 ≤r∗≤ˆr.
Finally, we use our variance-reduced coordinate algorithm, namely Algorithm 68 as a relaxed
proximal oracle in OuterLoopStronglyMonotone together with Proposition 19 once more to solve
(3.62) to the desired accuracy. The implementation in Section 8 and complexity results in Sec-
tion B.4.3 yield the runtime. This implementation crucially uses our development of the ApproxExpMaintainer
data structure in order to obtain a runtime depending directly on rcs rather than dimensions of the
matrix, as well as independence on B. For completeness, a proof can be found in Appendix B.6.1.
Theorem 12. The algorithm of Section B.4.3 can be used to ﬁnd an ϵ-accurate solution x∗
ϵ to
Max-IB satisfying miny∈∆m f(x∗
ϵ, y) ≥(1 −ϵ)r∗with high probability in time 6
eO

nnz + ρ√nnz · L2,1
co
ϵ

= eO

nnz + ρ√nnz · rcs
ϵ

.
Remark 5. Because we assumed m ≥n, in the case A is dense, up to logarithmic terms our runtime
improves upon the runtime of eOρm√n/ϵ in [22] by a factor of at least
rmn
nnz · m
rcs
6Here
e
O is hiding an additional factor of polylog(∥b∥∞) due to the additional cost in the runtime of
ApproxExpMaintainer, caused by the linear term b (see Remark 2).

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
140
generically. This is an improvement when A is sparse or column-sparse, i.e. nnz ≪mn, or rcs ≪m.
Such a saving is larger when A has numerical sparsity so that e.g.
 L2,1
co
2 ≤maxi∈[m] ∥Ai:∥2
1 +
 maxi∈[m] ∥Ai:∥1
  maxj∈[n] ∥A:j∥1

< rcs · maxi∈[m] ∥Ai∥2
2.
3.6.2
Minimum enclosing ball
In the minimum enclosing ball (Min-EB) problem, we are given a set of data points {a1, . . . , am}
with a1 = 0, maxi∈[m] ∥ai∥= 1.7 The goal is to ﬁnd the minimum radius R∗such that there exists
a point x with distance at most R∗to all points. Following the presentation of [22], we consider
Min-EB in an equivalent form. Deﬁne the vector b to have bi = 1
2 ∥ai∥2
2 entrywise. Then, Min-EB is
equivalent to the minimax problem
R∗:= min
x∈Rn max
y∈∆m
1
2
X
i
yi∥x −ai∥2
2 = min
x∈Rn max
y∈∆m f(x, y), where f(x, y) := y⊤Ax + y⊤b + 1
2 ∥x∥2
2 .
(3.63)
By assumption, ∥A∥2→∞= 1. We let (x∗, y∗) be the optimal solution to the saddle point problem.
We ﬁrst state several bounds on the quantities of the problem. These bounds were derived in [22]
and obtained by examining the geometric properties of the problem.
Fact 4. The following bounds hold: ∥x∗∥2 ≤1, and R∗≥1/8.
To achieve a multiplicative approximation, since R∗≥1/8 by Fact 4, it suﬃces to obtain a pair
(x∗
ϵ, y∗
ϵ ) achieving maxy f(x∗
ϵ, y) −minx f(x, y∗
ϵ ) ≤ϵ/8. In light of minimax optimality, Lemma 40
(proved in Section B.6.2) shows that it suﬃces to consider, for ϵ′ = Θ(ϵ/ log m), solving the following
(1, ϵ′)-strongly monotone problem to suﬃcient accuracy:
min
x∈Rn max
y∈∆m fϵ′(x, y) := y⊤Ax + y⊤b −ϵ′ X
i∈[m]
[y]i log[y]i + 1
2 ∥x∥2
2 .
(3.64)
Lemma 40. Setting ϵ′ = ϵ/(32 log m), an ϵ/16-accurate solution or (3.64) is an ϵ/8-accurate solu-
tion to the original problem (3.63).
As an immediate result of the above lemma, the runtime in Section B.4.3 and the correctness
proofs of Proposition 19 and Corollary 58, we obtain the following guarantee.
Theorem 13. The strongly monotone algorithm OuterLoopStronglyMonotone of [111], using Al-
gorithm 68 of Section B.5.2 and the estimator of Section B.4.3 as a relaxed proximal oracle, ﬁnds
an ϵ-accurate solution x∗
ϵ to Min-EB satisfying R∗≤maxy f(x∗
ϵ, y) ≤(1+ϵ)R∗with high probability
in time
eOnnz +
√nnz · L2,1
co
√ϵ
= eOnnz +
√nnz · rcs
√ϵ
.
7This can be assumed without loss of generality by shifting and rescaling as in [22] and considering the trivial case
when all ai, i ∈[m] are equal.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
141
Remark 6. When m ≥n,8 up to logarithmic terms our runtime improves the eOm√n/√ϵ runtime
of [22] by a factor of
rmn
nnz · m
rcs
generically. This is an improvement when A is sparse or column-sparse, i.e. nnz ≪mn, or rcs ≪m.
As in Section 3.6.1, the improvement is larger when A is numerically sparse, i.e. when
 L2,1
co
2 ≤
maxi∈[m] ∥Ai:∥2
1 +
 maxi∈[m] ∥Ai:∥1
  maxj∈[n] ∥A:j∥1

< rcs · maxi∈[m] ∥Ai∥2
2.
3.6.3
Regression
We consider the standard ℓ2 linear regression problem in a data matrix A ∈Rm×n and vector
b ∈Rm, i.e. minx∈Rn ∥Ax −b∥2. In particular, we consider the equivalent primal-dual form,
min
x∈Rn max
y∈Bm f(x, y) := y⊤(Ax −b).
(3.65)
Throughout, we assume the smallest eigenvalue of A⊤A is µ > 0 and denote an optimal solution
to (3.65) by z∗= (x∗, y∗) (where x∗is the unique solution to the regression problem). Our strategy
is to consider a sequence of modiﬁed problems, parameterized by β > 0, x′ ∈Rn:
min
x∈Rn max
y∈Bm f β
x′(x, y) := y⊤(Ax −b) + β
2 ∥x −x′∥2
2 −β
2 ∥y∥2
2 .
(3.66)
We denote the optimal solution to (3.66) by z∗
(β,x′) = (x∗
(β,x′), y∗
(β,x′)); when clear from context, for
simplicity we drop β and write z∗
x′ = (x∗
x′, y∗
x′) (as β = √µ throughout our algorithm). Lemma 41
(proved in Section B.6.3) states a known relation between the optimal solutions for (3.65) and (3.66).
Lemma 41. Letting (x∗, y∗) be the optimal solution for (3.65) and (x∗
x′, y∗
y′) be the optimal solution
for (3.66), the following relation holds:
∥x∗
x′ −x∗∥2 ≤
1
1 + µ
β2
∥x′ −x∗∥2 .
We give a full implementation of the regression algorithm in Algorithm 69 (see Section B.6.3), and
state its correctness and runtime in Theorem 14. The algorithm repeatedly solves problems of the
form (3.66) in phases, each time using Lemma 41 to ensure progress towards x∗. Observing that each
subproblem is (β, β)-strongly monotone, each phase is conducted via OuterLoopStronglyMonotone,
an algorithm of [111], using the ℓ2-ℓ2 algorithms of Section B.4.2 as a proximal oracle. Due to the
existence of composite terms, our inner loop steps are slightly diﬀerent than in Section B.4.2; we
give a more formal algorithm for the relaxed proximal oracle and its implementation in Algorithm 68
and Appendix B.5.2. We remark that by a logarithmic number of restarts per phase, a standard
8When m < n, the runtime of the algorithm in [22] still holds and is sometimes faster than ours.

CHAPTER 3. STOCHASTIC METHODS FOR MATRIX GAMES
142
argument boosts Theorem 14 to a high-probability claim.
Theorem 14. Given data matrix A ∈Rm×n, vector b ∈Rm, and desired accuracy ϵ ∈(0, 1),
assuming A⊤A ⪰µI for µ > 0, Algorithm 69 outputs an expected ϵ-accurate solution ˜x, i.e.
E [∥˜x −x∗∥2] ≤ϵ,
and runs in time
eO

nnz + √nnz ·
max
npP
i ∥Ai:∥2
1,
qP
j ∥A:j∥2
1
o
√µ

.
We give two settings where the runtime of Algorithm 69 improves upon the state of the art.
Entrywise nonnegative A. In the particular setting when all entries of A are nonnegative,9 by
Proposition 55 our complexity as stated in Theorem 14 improves by a factor of
p
nnz/(m + n) the
runtime of accelerated gradient descent [417], which is the previous state-of-the-art in certain regimes
with runtime O
 nnz · ∥A∥op/√µ

. This speedup is most beneﬁcial when A is dense.
Numerically sparse A. For numerically sparse A with ∥Ai:∥1/ ∥Ai:∥2 = O(1), ∥A:j∥1/ ∥A:j∥2 =
O(1) for all i ∈[m], j ∈[n], we can choose α = √µ in Algorithm 69 and obtain the runtime
O
 
nnz +
max
P
i ∥Ai:∥2
1, P
j ∥A:j∥2
1
	
µ
!
= O
 
nnz + ∥A∥2
F
µ
!
using the argument in Theorem 14. Under a (similar, but weaker) numerically sparse condition
∥Ai:∥1/ ∥Ai:∥2 = O(1), the prior state-of-the-art stochastic algorithm [300] obtains a runtime of
O(nnz + rcs · ∥A∥2
F /µ), and the recent state-of-the-art result in the numerically sparse regime [260]
improves this to O(nnz + (∥A∥2
F /µ)1.5) when rcs = Ω(∥A∥F /√µ). Improving universally over both,
our method gives O(nnz + ∥A∥2
F /µ) in this setting.
9More generally, this holds for arbitrary A ∈Rm×n satisfying ∥|A|∥op ≤∥A∥op.

Chapter 4
Faster Approximate ℓ∞Regression
and Maximum Flow
This chapter is based on [486], with Aaron Sidford.
4.1
Introduction
The classic problem of ℓ∞regression corresponds to ﬁnding a point x∗such that
x∗= argminx∈Rm∥Ax −b∥∞, for A ∈Rn×m, b ∈Rn.
In this chapter, we are primarily concerned with developing iterative algorithms for approximately
solving this problem. We use OPT to denote ∥Ax∗−b∥∞and our goal is to ﬁnd an ϵ-approximate
minimizer of the ℓ∞-regression function, i.e. a point x ∈Rm such that
OPT ≤∥Ax −b∥∞≤OPT + ϵ.
This problem has fundamental implications in statistics and optimization [482, 348, 349, 487]. In
many of these settings, it is also useful to design iterative method machinery for the following more
general problem of ﬁnding
x∗= argminx∈S ∥Ax −b∥∞, for A ∈Rn×m, b ∈Rn, S = {x ∈Rm : xj ∈[lj, rj] ∀j ∈[m]}
for some m pairs of scalar lj ≤rj (possibly inﬁnite). Note that this constrained problem is strictly
more general than the standard one as setting lj = −∞, rj = ∞, ∀j ∈[m] recovers the unconstrained
problem. In this chapter, for simplicity, the domain constraint will only be x ∈[−1, 1]m (though
143

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
144
our results apply to the more general case; see Appendix C.1.2 for a formal statement).
Deﬁnition 8 (Box-constrained ℓ∞regression). We call the problem of solving, for regression matrix
A ∈Rn×m and demands b ∈Rn,
min
x∈[−1,1]m ∥Ax −b∥∞,
the box-constrained ℓ∞regression problem. We refer to any x′ ∈[−1, 1]m such that
∥Ax′ −b∥∞−
min
x∈[−1,1]m ∥Ax −b∥∞≤ϵ
as an ϵ-approximate minimizer.
Many natural optimization problems can be written in the form of box-constrained ℓ∞regres-
sion, e.g. the maximum ﬂow problem and more broadly linear programming [349], and thus faster
methods for solving box-constrained ℓ∞regression can imply faster algorithms for common prob-
lems in theoretical computer science. Therefore, the central goal of this chapter is to provide faster
algorithms for computing ϵ-approximate minimizers to ℓ∞-regression, that when specialized to the
maximum ﬂow problem, achieve faster running times.
4.1.1
Regression results
In this chapter we show how to apply ideas from the literature on coordinate descent methods (see
Section 4.1.3) to obtain faster algorithms for approximately solving box-constrained ℓ∞regression.
We show that by assuming particular sampling and smoothness oracles (which are implementable
given sparsity assumptions on A), we obtain a randomized algorithm which improves upon the
the classic gradient descent based methods across a broad range of parameters and attains an ϵ−1
dependence in the runtime. We show the following in Section 4.3.4.
Theorem 15 (Accelerated box-constrained ℓ∞regression). There is an algorithm initialized that
ϵ-approximately minimizes the box-constrained ℓ∞regression problem (Deﬁnition 8) in time
˜O

mc +

min(m, n) +
p
m min(n, s)

c ∥A∥∞
ϵ

,
where each column of A ∈Rn×m has at most c non-zero entries, and the optimizer x∗has ∥x∗∥2 ≤s.
Note that since s ≤m, the runtime is always at most eO(mc ∥A∥∞/ϵ). Moreover, Theorem 15
generically achieves a runtime of eO(√mnc ∥A∥∞/ϵ) in the case n = O(m). We give a proof of the
following simple extension, encapsulating the general box-constrained case as well as the uncon-
strained case, in Appendix C.1.2, which follows via a reduction to Theorem 15. We simpliﬁed the
bounds for easy statement, but we remark that as they follow by a reduction, they admit similar
improvements when e.g. n, s ≪m.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
145
Corollary 6. There is an algorithm that ϵ-approximately minimizes the box-constrained ℓ∞regres-
sion problem
min
x∈[−r,r]m ∥Ax −b∥∞
in ˜O (mcr∥A∥∞/ϵ) time where each column of A ∈Rn×m has at most c non-zero entries. Moreover,
there is an algorithm that ϵ-approximately minimizes the unconstrained ℓ∞regression problem
min
x∈Rm ∥Ax −b∥∞
in ˜O (mcr∥A∥∞/ϵ) time, where the optimizer is x∗, and ∥x0 −x∗∥∞≤r for some given x0.
The only other known box-constrained ℓ∞regression algorithm achieving an ϵ−1 dependence
(improving upon the standard ϵ−2 dependence) without paying a dimension-dependent penalty is
the recent breakthrough result of [483].
Pessimistic bounds on our guarantees attain a runtime
matching that of [483] across a broad range of parameters (for example in the uniform sparsity case
where mc = O(nnz(A))). In instances with more structured regression matrices, with sharper bounds
on parameters n, s, we obtain improved runtimes. These improvements are attainable by modifying
the algorithm to take steps in a nonuniform diagonal norm, obtaining tighter dependences on sparsity
measures of the matrix and optimal solution, which we elaborate on in Sections 4.3 and 4.5. Because
of these tighter dependencies, in many parameter regimes, including those for the maximum ﬂow
problem for even slightly dense graphs, our result improves upon [483].
Our work provides an alternative approach for accelerating ℓ∞gradient descent for certain highly
structured optimization problems, i.e.
ℓ∞regression.
Whereas Sherman's work introduced an
intriguing notion of area convexity and new regularizations of ℓ∞regression, our results are achieved
by working with the classic smoothing of the ℓ∞norm and by providing a new accelerated coordinate
descent method. We achieve our tighter bounds by exploiting local smoothness properties of the
problem and dynamically sampling by these changing smoothnesses.
Our algorithm is inspired by, and builds upon, advances in non-uniform sampling for coordinate
descent [553, 449, 425], as well as extragradient proximal methods [415, 420], and is similar in spirit
to work on accelerated algorithms for approximating packing and covering linear programs [23] which
too works with non-standard notions of smoothness. Our work overturns conventional wisdom that
these techniques do not extend nicely to ℓ∞regression and the maximum ﬂow problem. Interestingly,
our algorithms gain an improved dependence on dimension and sparsity over [483] in certain cases
while losing the parallelism of [483]. It is an open direction for future work as to see whether or not
these approaches can be combined for a more general approach to minimizing ℓ∞-smooth functions.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
146
4.1.2
Maximum ﬂow results
The classic problem of maximum ﬂow roughly asks for a graph G with m (capacitated) edges and
n vertices, how to send as many units of ﬂow can be sent from a speciﬁed "source" vertex to a
speciﬁed "sink" vertex while preserving ﬂow conservation at all other vertices and without violating
edge capacity constraints (i.e. the ﬂow cannot put more units on an edge than the edge's capacity).
The maximum ﬂow problem is known to be easily reducible to the more general problem of
minimum congestion ﬂow. Instead of specifying s and t this problem takes as input a vector d ∈RV
such that d⊤1 = 0, where 1 is the all-ones vector. The goal of minimum congestion ﬂow is to ﬁnd a
ﬂow f ∈RE which routes d meaning, mean that the imbalance of f at vertex v is given by dv, and
subject to this constraint minimizes the congestion,
max
e∈E(G)

fe
ue

where fe is the ﬂow on some edge, and ue is the capacity on that edge. We refer to the vector with
entries fe/ue as the congestion vector. We call any ﬂow which routes an amount within a 1 + ϵ
multiplicative factor to the optimum an ϵ-approximate maximum ﬂow.
A recent line of work beginning in [482, 318] solves the maximum ﬂow problem by further reducing
to constrained ℓ∞regression. To give intuition for the reduction used in this work, broadly inspired
by [482, 318], we note that maximum ﬂow in uncapacitated graphs can be rephrased as asking for
the smallest congestion of a feasible ﬂow, namely to solve the problem
f ∗= argminBf=d∥f∥∞
where the restriction Bf = d for B the edge-vertex incidence matrix of a graph, and d the demands,
enforces the ﬂow constraints. This can be solved up to logarithmic factors in the running time by
ﬁxing some value F for ∥f∥∞and asking to optimally solve the problem
f ∗= argmin∥f∥∞≤F ∥Bf −d∥∞
where we note that the constraint ∥f∥∞≤F can be decomposed as the indicator of a box so that
this objective matches the form of (4.1). The exact reduction we use has a few modiﬁcations: the
box constraint is more simply replaced by ∥f∥∞≤1, and the regression objective is in a matrix
RB, where R is a combinatorially-constructed preconditioner whose goal is to improve the condition
number (and convergence rate) of the problem, and the problem is scaled for capacitated graphs
(for a more detailed description, see Section 4.4.2).
In this chapter we show how to modify our algorithm for structured ℓ∞regression in order to
obtain faster algorithms for maximum ﬂow. We do so by leveraging the tighter dependence on the
domain size (in the ℓ2 norm rather than ℓ∞) and coordinate smoothness properties of the function

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
147
to be minimized (due to the structure of the regression matrix). In particular we show the following.
Theorem 16 (ℓ2 accelerated approximate maximum ﬂow). There is an algorithm that takes time
˜O(m + max(n, √ns)/ϵ) to ﬁnd an ϵ-approximate maximum ﬂow, where s is the squared ℓ2 norm of
the congestion vector of any optimal ﬂow.
Our running time improves upon the previous fastest running time of this problem of ˜O(m/ϵ).
Since s ≤m we achieve a faster running time whenever the graph is slightly dense, i.e. m = Ω(n1+δ)
for any constant δ > 0.
Interestingly our algorithm achieves even faster running times when there is a sparse maximum
ﬂow, i.e. a maximum ﬂow in which the average path length in the ﬂow decomposition of the optimal
ﬂow is small. Leveraging this, in Section 4.4.4 we provide several new results on exact undirected
and directed maximum ﬂow on uncapacitated graphs as well.
Theorem 17 (Improved algorithms for exact maximum ﬂows). There are algorithms for ﬁnding an
exact maximum ﬂow in the following types of uncapacitated graphs.
• There is an algorithm which ﬁnds a maximum ﬂow in an undirected, uncapacitated graph with
maximum ﬂow value F in time ˜O(m + min(√mnF 3/4, m3/4n1/4√
F)).
• There is an algorithm which ﬁnds a maximum ﬂow in an undirected, uncapacitated graph with
a maximum ﬂow that uses at most s edges in time ˜O(m + √msn1/4 max(n, s)1/4).
Each of these runtimes improves upon previous work in some range of parameters. For example,
the bound of ˜O(m+m3/4n1/4√
F) for undirected, uncapacitated graphs improves upon the previous
best running times of ˜O(m
√
F) achievable by [483] whenever n = o(m) and of ˜O(m+nF) achievable
by [314] whenever m = o(nF 2/3).
We also separately include the following result (which has no dependence on the sparsity s) for
ﬁnding exact ﬂows in general uncapcitated directed graphs, as it improves upon the running time
of ˜O(m · max{m1/2, n2/3}) achieved by [242] whenever m = ω(n) and m = o(n5/3).
Theorem 18 (Exact maximum ﬂow for directed uncapacitated graphs). There is an algorithm which
ﬁnds a maximum ﬂow in a directed, uncapacitated graph in time ˜O(m5/4n1/4). When the maximum
ﬂow is s-sparse, there is an algorithm which ﬁnds a maximum ﬂow in a directed, uncapacitated graph
in time ˜O(mn1/4 max(n, s)1/4).
Although the runtime of [242] has been improved by the recent works of [377] achieving runtime
O(m10/7) and of [348] achieving runtime ˜O(m√n), which dominate our ˜O(m5/4n1/4) runtime, they
do it using sophisticated advances in interior point methods, whereas our algorithm operates using
a ﬁrst-order method which only queries gradient information of the objective function, rather than
second-order Hessian information. In particular, our algorithm is the ﬁrst to improve runtimes for
directed graphs while relying only on ﬁrst-order information of the objective function.
We ﬁnd

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
148
it interesting that our result achieves any running time improvement for unit capacity maximum
ﬂow over [242] without appealing to interior point machinery and think this may motivate further
research in this area, namely designing ﬁrst-order methods for structured linear programs.
4.1.3
Previous work
Here we embark on a deeper dive into the context of the problems and tools discussed in this chapter.
Solving the ℓ∞regression problem. For a non-diﬀerentiable function such as f(x) = ∥x∥∞, it is pos-
sible to use the toolkit for linear programming (including interior point and cutting plane [348, 349])
to obtain iterative algorithms for approximate minimization. However, these particular algorithms
have a larger dependence on dimension, and it is widely believed that the iteration complexity is
inherently dimension-dependent. A ﬁrst-order iterative algorithm with a better dependence on di-
mension for approximately solving the regression problem was developed by [419] and proceeds in
two stages. First, the algorithm constructs a smooth approximation to the original function, which
is typically explicitly derived via regularizing the dual function using a regularizer which is both
smooth and bounded in range. The smooth approximation is constructed such that approximately
minimizing the approximate function is suﬃcient to approximately minimize the original function.
Second, a ﬁrst-order method such as gradient descent in a particular norm, or one of its many
variants, is applied to approximately minimize the smoothed function.
One of the earlier works to develop algorithms using ﬁrst-order methods under this framework to
solve the regression problem is [419]. One regularizer used in this work for optimization over a dual
variable in the simplex was the entropy regularizer, which yields the smooth approximation to the
ℓ∞norm deﬁned by smaxα(x) = α log(P
j exp(xj/α)). Essentially, the methods presented in this
work converge to an ϵ-approximate solution in a number of iterations proportional to either O(ϵ−2)
or O(√mϵ−1), hiding problem-speciﬁc dependencies on smoothness and domain size. However, the
cost of each iteration involves computing a whole gradient, which incurs another multiplicative loss
of the dimension in the runtime.
Several other works which aimed to solve the regression problem via considering a smooth min-
imax formulation, including [415] and [420], incurred the same fundamental barrier in convergence
rate. These works aimed to pose the (smooth) regression problem as ﬁnding the saddle point of
a convex-concave function via a specially-constructed ﬁrst-order method. The main barrier to im-
proving prior work up to this point has been the inability to construct regularizers of small range
which are strongly convex with respect to the ℓ∞norm. For some time, these issues posed a barrier
towards ﬁnding faster algorithms for the regression problem, and many related problems.
Very recently, Sherman [483] presented an alternative method which was able to break this barrier
and attain an O(1/ϵ) iteration count for ﬁnding approximate solutions to the regression problem,
where each iteration can be applied in time to compute a gradient.
The algorithm used was a
variation of Nesterov's dual extrapolation method [420] for approximately ﬁnding a saddle point

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
149
Year
Author
Method
Iteration Complexity
Iteration Cost
Norm
2003
[419]
Smoothing
O(ϵ−2)
O(m)
ℓ∞
O(ϵ−1)
O(m)
ℓ2
2004
[415]
Mirror prox
O(ϵ−2)
O(m)
ℓ∞
O(√mϵ−1)
O(m)
ℓ∞
2005
[420]
Dual extrapolation
O(ϵ−2)
O(m)
ℓ∞
O(√mϵ−1)
O(m)
ℓ∞
2017
[483]
Area-convexity
O(ϵ−1)
O(m)
ℓ∞
2018
This chapter
Local smoothness
O(√mϵ−1)
˜O(d)
ℓ2
Table 4.1: Dependencies of algorithms for ℓ∞regression in A ∈Rn×m on various problem parameters.
Note that there is up to an O(√m) discrepancy between the ℓ2 and ℓ∞norms.
Here, d is the
maximum number of nonzero entries in any column of A.
in a convex-concave function, adapted to work for regularizers satisfying a weaker property known
as area convexity, and an analysis of its convergence. As a corollary, this obtained the currently
fastest-known algorithm for approximate maximum ﬂow.
Abbreviated history of ﬁrst-order methods, emphasizing coordinate-based methods. First-order meth-
ods for convex optimization have a long history. Gradient descent methods with error decaying in k
iterations as O(1/
√
k) for Lipschitz functions and O(1/k) for smooth functions have been well stud-
ied (for example, see [418] or [98] for a more detailed exposition), and applied in many important
settings.
Nesterov gave the ﬁrst gradient-based algorithm for minimizing functions smooth in the Euclidean
norm which converged at the rate O(1/k2). The method is optimal in the sense that it matched
known lower bounds for smooth functions. Unfortunately, this method does not apply generically
to functions which are smooth in other norms, in the same way that unaccelerated variants do,
without possibly paying an additional dependence on the dimension. In particular, the accelerated
convergence rate depends on the regularizer that the mirror descent steps use, and thus the analysis
incurs a loss based on the size of the regularizer, which is the barrier in the aforementioned ℓ∞-
smooth function case.
Speciﬁcally, it is a folklore result that any function strongly-convex over
[−1, 1]n in the ℓ∞norm has range at least n/2, which we show in Appendix C.1.1.
There has been much interest in applying randomized ﬁrst order methods to more eﬃciently
obtain an approximate minimizer on expectation, when the convex optimization problem has certain
structure. One example of these randomized methods in the literature is coordinate descent, studied
ﬁrst in [423]. The main idea is that using crude, computationally eﬃcient, approximations to the
full gradient, one is still able to ﬁnd an approximate minimizer on expectation.
One beneﬁt is
that coordinate descent admits a more ﬁne-grained analysis of convergence rate, based on structural
properties of the function, i.e. the smoothness of the function in each coordinate.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
150
Generalizations of standard coordinate descent have received much attention recently, both for
their powerful theoretical and practical implications. [423] provided an accelerated version of the
standard coordinate descent algorithm, but the naive implementation of its steps were ineﬃcient,
taking linear time in the dimension. The study of eﬃcient accelerated coordinate descent methods
(which converge at the rate O(1/k2) without an additional dependence on dimension) was pioneered
by [347], and since then a ﬂurry of other works, including [222, 553, 449] have improved the rate of
convergence and generalized the methods to composite functions with a separable composite term,
of the form F(x) = f(x) + P
j ψj(xj). We remark that our box constraint can be represented as
such a separable composite term in the objective, and our constrained accelerated coordinate descent
algorithm is an adaptation of such composite methods. For a more detailed history of the study of
coordinate descent methods, we refer the reader to [222].
Accelerated coordinate based methods have proven to be useful in many ways when applied to
problems in theoretical computer science. For example, the authors of [347] framed graph Laplacian
system solvers as a coordinate descent problem to give better runtime guarantees. One particularly
interesting example that highlighted the potential for using accelerated coordinate descent in mini-
mizing entropy-based functions was the work of [23] in solving packing and covering LPs, where the
constraint matrix is nonnegative, in which they also attained a O(1/ϵ) method complexity. Con-
ventional wisdom is that these results are speciﬁc to the structure of the particular problem, so any
exploration of accelerated methods in greater generality is particularly interesting.
Maximum ﬂow. The maximum ﬂow problem is a fundamental problem in combinatorial optimiza-
tion that has been studied extensively for several decades. Until recently, the toolkit used to solve
the problem has been primarily combinatorial, culminating in algorithms with runtime roughly
˜O(min{mn2/3, m3/2}) for ﬁnding a maximum ﬂow in graphs with m edges and n vertices and poly-
nomially bounded capacities [242], and ˜O(m+nF) for ﬁnding a maximum ﬂow in undirected graphs
with m edges, n vertices, and a maximum ﬂow value of F [314].
Breakthroughs in the related problem of electrical ﬂow using tools from continuous optimization
and numerical linear algebra were ﬁrst achieved by Spielman and Teng [491] who showed that solving
a linear system in the Laplacian of a graph could be done in nearly linear time, which is equivalent
to computing an electrical ﬂow.
Notably, the electric ﬂow problem corresponds to approximately solving an ℓ2 regression problem
∥Ax −b∥2, and the maximum ﬂow problem corresponds to approximately solving an ℓ∞regression
problem ∥Ax −b∥∞. Accordingly, using the faster algorithms for electric ﬂow combined with a mul-
tiplicative weights approach, the authors of [139] were able to make a breakthrough to approximately
solve maximum ﬂow with a runtime of ˜O(mn1/3), where ˜O hides logarithmic factors. Finally, using
constructions presented in [376], the authors of [482] and [318] were able to reduce this runtime to
almost linear, essentially using variants of preconditioned gradient descent in the ℓ∞norm. This

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
151
Year
Author
Complexity
Weighted
Directed
1998
[242]
˜O(min(m3/2, mn2/3))
Yes
Yes
1998
[313]
˜O(m√nϵ−1)
Yes
No
2002
[314]
˜O(m + nF)
Yes
No
2011
[139]
˜O(mn1/3ϵ−11/3)
Yes
No
2012
[343]
˜O(mn1/3ϵ−2/3)
No
No
2013
[482], [318]
˜O(m1+o(1)ϵ−2)
Yes
No
2013
[377]
˜O(m10/7)
No
Yes
2014
[348]
˜O(mn1/2)
Yes
Yes
2016
[441]
˜O(mϵ−2)
Yes
No
2017
[483]
˜O(mϵ−1)
Yes
No
2018
This chapter
˜O(m + (n + √ns)ϵ−1)
Yes
No
Table 4.2: Complexity of maximum ﬂow since [242] for undirected graphs with n vertices, m edges,
where s is the ℓ2
2 of the maximum ﬂow's congestion, and F is the maximum ﬂow value.
runtime was reduced to ˜O(m/ϵ2) by Peng in [441] by using a recursive construction of the combi-
natorial preconditioner. As previously mentioned, the ϵ−2 dependence in the runtime was a barrier
typical of algorithms for minimizing ℓ∞-smooth functions without worse dimension dependence, and
was broken in [483], who attained a runtime of ˜O(m/ϵ).
4.1.4
Organization
The rest of this chapter is organized as follows. Many proofs are deferred to the appendices.
• Section 4.2: Overview. We introduce the deﬁnitions and notation we use throughout the
chapter, and give a general framework motivating our work.
• Section 4.3: Regression. We ﬁrst give a framework for accelerated randomized algorithms
which minimize the box-constrained ℓ∞regression function based on uniform sampling, as well
as a faster one based on non-uniform sampling which assumes access to a coordinate smoothness
and sampling oracle. To do so, we develop a new analysis of coordinate descent under a box
constraint, amenable to dynamic coordinate sampling distributions, and show how to accelerate
it via a primal-dual proximal point method. We then give eﬃcient implementations for these
oracles for structured problems.
• Section 4.4: Maximum ﬂow. We state the reduction from the maximum ﬂow problem to
box-constrained ℓ∞regression problem. We ﬁrst show how to attain a faster algorithm for
maximum ﬂow by exploiting combinatorial structure of the ﬂow regression problem, using the
regression algorithm we developed in the prior section. We then state the improved runtimes

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
152
which follow from a randomized primal-dual variation of our regression algorithm, given in
Section 4.5. Further, we give the exact maximum ﬂow runtimes achieved via rounding the
resulting approximate ﬂow of our improved method.
• Section 4.5: Primal-dual coordinate acceleration. We develop an algorithm with im-
proved runtimes for the structured ℓ∞regression problem which results from the maximum
ﬂow reduction, and correspondingly yields further-improved ﬂow runtimes.
4.2
Overview
4.2.1
Basic deﬁnitions
First, we deﬁne some basic objects and properties which we use throughout this paper.
General notations. We use ˜O(f(n)) to denote runtimes of the following form: O(f(n) logc f(n))
where c is a constant. With an abuse of notation, we let ˜O(1) denote runtimes hiding polynomials
in log n when the variable n is clear from context, and refer to such runtimes as "nearly constant."
Generally, we work with functions whose arguments are vector-valued variables in m-dimensional
space, and may depend on a linear operator A : Rm →Rn. Correspondingly we use j ∈[m] and
i ∈[n] to index into these sets of dimensions, where [m] is the set {1, 2, . . . m}. We use ej to denote
the jth standard basis vector, i.e. the vector which is 1 in dimension j and 0 everywhere else. We
use u ◦v to denote the vector which is the coordinate-wise product, i.e. its jth coordinate is ujvj.
Matrices. In this work, we deal with matrices A ∈Rn×m unless otherwise speciﬁed. Accordingly,
we index into rows of A with i ∈[n], and into columns with j ∈[m]. We refer to rows of A via Ai:
or ai when it is clear from context, and columns via A:j. We use nnz(A) to denote the number of
nonzero entries of A, and assume nnz(A) ≥n + m −1, else we may drop a row or column.
We use diag(w) to denote the diagonal matrix whose diagonal entries are the coordinates of a
vector w. We call a square symmetric matrix A positive semi-deﬁnite if for all vectors x, x⊤Ax ≥0
holds. For positive semi-deﬁnite matrices A, B we apply the Loewner ordering and write A ⪯B if
for all vectors x, x⊤Ax ≤x⊤Bx holds.
Finally, we say that a matrix is c-column-sparse if no column of A has more than c nonzero
entries.
Norms. We use ∥· ∥to denote an arbitrary norm when one is not speciﬁed. For scalar valued p ≥1,
including p = ∞, we use ∥x∥p := (P
j xp
j)1/p to denote the ℓp norm. For vector valued w ∈Rm
≥0,
we use ∥x∥2
w := P
j wjx2
j to denote the weighted quadratic norm, and for positive semideﬁnite
matrix A, we deﬁne ∥x∥2
A = x⊤Ax.
Further, we let ∆n be the simplex in n dimensions, e.g.
p ∈∆n ⇐⇒∥p∥1 = 1, p ≥0 entrywise.
For a norm ∥· ∥, the dual norm ∥· ∥∗is deﬁned by ∥x∥∗:= max∥y∥≤1y⊤x. It is well known that
the dual norm of ℓp is ℓq for 1/p + 1/q = 1. For matrix A and a vector norm ∥· ∥, we deﬁne the

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
153
matrix norm ∥A∥:= max∥x∥=1 ∥Ax∥. For example, ∥A∥∞is the largest ℓ1 norm of a row of A.
Functions. We will primarily be concerned with minimizing convex functions f(x) subject to the
argument being restricted by a box constraint, where the domain is some scaled box Bc
∞unless
otherwise speciﬁed. Whenever the function is clear from context, x∗will refer to any minimizing
argument of the function. We use the term ϵ-approximate minimizer of a function f to mean any
point x such that f(x∗) ≤f(x) ≤f(x∗) + ϵ. Furthermore, we deﬁne the OPT operator to be such
that OPT(f) is the optimal value of f, when this optimal value is well-deﬁned.
For diﬀerentiable functions f we let ∇f(x) be the gradient and let ∇2f(x) be the Hessian. We
let ∇jf(x) be the value of the jth partial derivative; we also abuse notation and use it to denote the
vector ∇jf(x)ej when it is clear from context.
Properties of functions. We say that a function is L-smooth with respect to some norm ∥· ∥if it
obeys ∥∇f(x) −∇f(y)∥∗≤L∥x −y∥, the dual norm of the gradient is Lipschitz continuous. It is
well known in the optimization literature that when f is convex, this is equivalent to f(y) ≤f(x) +
∇f(x)⊤(y −x) + L
2 ∥y −x∥2 for y, x ∈dom(f) and, for twice-diﬀerentiable f, y⊤∇2f(x)y ≤L∥y∥2.
We say that a function is Lj-coordinate smooth in the jth coordinate if the restriction of the
function to the coordinate is smooth, i.e. |∇jf(x + cej) −∇jf(x)| ≤Lj|c| ∀x ∈dom(f), c ∈R.
Equivalently, for twice-diﬀerentiable convex f, ∇2
jjf(x) ≤Lj.
Finally, we say a function is µ-strongly convex with respect to ∥· ∥if for all x, y, f(y) ≥f(x) +
∇f(x)⊤(y −x) + µ
2 ∥y −x∥2. When f is twice-diﬀerentiable, equivalently y⊤∇2f(x)y ≥µ ∥y∥2.
Graphs. We primarily study capacitated undirected graphs G = (V, E, u) with edge set E ⊆V × V ,
edge capacities u : E →R+. When referring to graphs, we let m = |E| and n = |V |. Throughout
this paper, we assume that G is strongly connected.
We associate the following matrices with the graph G, when the graph is clear from context.
The matrix of edge weights U ∈RE×E is deﬁned as U := diag(u). Orienting the edges of the graph
arbitrarily, the vertex-edge incidence matrix B ∈RV ×E is deﬁned as Bs,(u,v) := −1 if s = u, 1 if
s = v and 0 otherwise.
Divergences. In the analysis of mirror descent variants, a ﬁrst-order method ﬂexible to geometric
constraints on its arguments, we require the concept of a Bregman divergence with respect to a
regularizer r. For a convex function r, we deﬁne the (nonnegative) Bregman divergence to be
V r
x (y) = r(y) −r(x) −∇r(x)⊤(y −x).
We drop the r for convenience when it is clear from context. The Bregman divergence satisﬁes the
well-known equality
⟨∇V r
x (y), u −y⟩= V r
x (u) −V r
x (y) −V r
y (u).
(4.1)

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
154
4.2.2
Overview of our algorithms
Here, we give an overview of the main ideas used in our algorithms for approximately solving ℓ∞
regression problems. The main ideological contribution of this work is that it uses a new variation
of coordinate descent which uses the novel concept of local coordinate smoothness in order to get
tighter guarantees for accelerated algorithms.
ℓ∞regression algorithm
The ﬁrst piece of our algorithm is developed in Section 4.3.2, where we show how to use a primal-dual
proximal point method inspired by the "conceptual mirror-prox" algorithm of [415] to reduce the
task of designing an accelerated scheme for the ℓ∞regression problem to designing an unaccelerated
procedure for minimizing a regularized approximation of the regression objective. Next, we show
in Section 4.3.3 how to improve the complexity of the standard coordinate descent algorithm for an
appropriately regularized ℓ∞-smooth approximation to the regression problem by using the concept
of local coordinate smoothnesses, which we introduce. To analyze its convergence, we develop a novel
analysis of coordinate descent under dynamic sampling probabilities subject to a box constraint.
Finally, in order to implement the steps of the algorithm, it is necessary to eﬃciently compute
overestimates to the local coordinate smoothnesses, and furthermore sample coordinates proportional
to these overestimates; this procedure is given in Lemma 51.
Acceleration via proximal point reduction. In Section 4.3.2, we show how we can reduce minimizing
the original ℓ∞objective to eﬃciently ﬁnding high-precision minimizers to a sequence of regularized
approximations, via a proximal scheme of [415], which we refer to as the primal-dual proximal point
method, or proximal point method for short.1 This reduction constructs a sequence of iterates by
calling a high-precision minimization oracle for each regularized approximation, where the regu-
larization amount is parameterized by a scalar quantity α > 0. A larger α will result in simpler
subproblems, but will require more calls to the oracle; trading oﬀthese complexities via the pa-
rameter α results in our accelerated runtime. More formally, note that we may rewrite the original
regression problem by introducing a dual variable (after appropriately doubling the constraints to
account for signs; see discussion in Section 4.3.1)
min
x∈[−1,1]m ∥Ax −b∥∞=
min
x∈[−1,1]m max
p∈∆n p⊤(Ax −b).
The proximal point method with parameter α constructs a sequence of points {zt} as follows: from
an iterate zt = (xt, pt), deﬁne the next iterate zt+1 = (xt+1, pt+1) to be the solution to a proximal
subproblem (here and throughout, s := ∥x∗∥2
2 where x∗is the optimizer of the box-constrained ℓ∞
1The proximal point method in this paper is slightly diﬀerent than the "conceptual mirror-prox" algorithm of
[415]. In [415], each iteration takes two steps, the ﬁrst of which solves a regularized proximal problem to suﬃciently
high accuracy, and the second of which is an extragradient adjustment step. We bypass the need for this adjustment
step via more stringent requirements on the accuracy level of the solution of the proximal problem.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
155
regression problem)
zt+1 = argminx∈[−1,1]m argmaxp∈∆n p⊤(Ax −b) + α
2s ∥x −xt∥2
2 −α
X
i
pi log pi
[pt]i
.
(4.2)
To explain further, the most prevalent ﬁrst-order method approach to convex optimization, and its
primal-dual generalization (for example found in mirror descent and gradient descent) for solving a
problem of the form minx∈[−1,1]m maxp∈∆n p⊤(Ax−b) with gradient operator g(x, p), is to repeatedly
construct regularized linearizations of the form, for some regularizer function r,
zt+1 = argminz ⟨g(zt), z⟩+ V r
zt(z).
The proximal method instead sets the next iterate zt+1 to be the result of a proximal problem,
without the linearization; we set the regularizer r(x, p) to be
1
2s ∥x∥2
2 + P
i pi log pi. Overall, if the
regularizer r has range bounded by Θ, then the proximal point method converges in roughly αΘ/ϵ
iterations to an ϵ-approximate saddle point, which suﬃces for our purposes.
We give the convergence analysis of the proximal point method under approximate solutions
to the subproblems deﬁning the iterates {zt} in Section 4.3.2.
Therefore, the main algorithmic
workhorse can be reduced to computing high-accuracy saddle points to problems of the form
argminx∈[−1,1]m α log
X
i∈[n]
exp
 1
α [Ax −bt]i

+ α
2s ∥x −xt∥2
2 .
(4.3)
Note that the problem (6.21) is the same as (4.2), where we maximized over p explicitly; the vector
bt is obtained via a linear shift of the vector b (details can be found in Section 4.3.2). Our remaining
algorithmic development deals with this subproblem; combining a fast iterative method for this
subproblem with the optimal choice of α yields the runtime for regression. To obtain our more
ﬁne-grained runtimes in Section 4.4, we also generalize to diagonally-reweighted ℓ2
2 regularizers.
Local coordinate smoothness. In this work, we introduce the concept of local coordinate smoothness
at a point x. This generalizes the concept of global coordinate smoothness to a particular point.
This deﬁnition is crucial to the analysis throughout the rest of the paper.
Deﬁnition 9 (Local coordinate smoothness). Twice-diﬀerentiable function f is Lj(x) locally coor-
dinate smooth in coordinate j at x, if for all |c| ≤|∇jf(x)/Lj(x)|, ∇2
jjf(x + cej) ≤Lj(x).
We state a useful equivalent characterization to Deﬁnition 9; the proof is standard and follows
by integration (once and twice respectively).
Lemma 42. For twice-diﬀerentiable f, f is Lj(x) locally coordinate smooth if and only if |∇jf(y)−
∇jf(y′)| ≤Lj(x)|y −y′| for all y, y′ between x ± ∇jf(x)/Lj(x)ej. If f is Lj(x) locally coordinate
smooth then for all y between x ± ∇jf(x)/Lj(x)ej, f(y) ≤f(x) + ∇fj(x)(yj −xj) + Lj(x)
2
|yj −xj|2.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
156
Note that this says that a coordinate descent step using local smoothnesses at a point exhibits
roughly the same behavior as a single step of coordinate descent with global smoothnesses.
In
particular, for the point which the coordinate descent algorithm would step to, the function values
exhibit the same quadratic upper bound along the coordinate. For a more motivating discussion of
this deﬁnition, we refer the reader to an analysis of coordinate descent presented in Appendix C.1.5.
We will drop the x from the notation Lj(x) when the point we are discussing is clear, i.e. a particular
iterate of one of our algorithms.
Bounding the progress of coordinate descent in ℓ∞-smooth functions. Here, we sketch the main idea
underlying our improved runtime for the problem (6.21), whose ﬁrst component is ℓ∞-smooth. Why
is it possible to hope to improve gradient methods in the ℓ∞norm via coordinate descent? One
immediate reason is that smoothness in this norm is a strong assumption on the sum S of the local
coordinate smoothness values of f.
As we recall in Appendix C.1, gradient descent for an ℓ∞-smooth function initialized at x0 ∈Rm
takes roughly L∥x0−x∗∥2
∞
ϵ
iterations to converge to a solution which has ϵ additive error, whereas
coordinate descent with appropriate sampling probabilities
Lj
S , for S = P
j Lj, takes
S∥x0−x∗∥2
2
ϵ
iterations to converge to the same quality of solution.
When the norm in the gradient descent method is ∥· ∥∞, we have ∥x0 −x∗∥2
2 ≤m∥x0 −x∗∥2
∞,
but the iterates can be m times cheaper because they do not require a full gradient computation.
So, if we can demonstrate S ≤L, we can hope to match and improve the runtime. To be more
concrete, we will demonstrate the following fact.
Lemma 43. Suppose for some point x, f : Rm →R is convex and L-smooth with respect to ∥· ∥∞,
Λj(x) = ∇2
jjf(x), and S = P
j Λj(x). Then S ≤L.
Proof. Fix x, and deﬁne M := ∇2f(x) and S := Tr(M). Consider drawing y uniformly at random
from {−1, 1}m. By the smoothness assumption, we have y⊤My ≤L∥y∥2
∞= L. Also, note that
E[y⊤My] = E

X
i,j
Mijyiyj

= Tr(M) = S
Thus, by the probabilistic method, there exists some y such that S ≤y⊤My ≤L, as desired.
While this gives a bound on the number of iterations required by a coordinate descent algorithm,
it requires being able to compute and sample by the Lj(x); as we take coordinate descent steps, it is
not clear how the local coordinate smoothnesses Lj(xk) will change, and how to update and compute
them. Naively, at each iteration, we could recompute the local smoothnesses, but this requires as
much work as a full gradient computation if not more. Furthermore, we need to implement sampling
the coordinates in an appropriate way, and show how the algorithm behaves under acceleration.
However, a key idea in our work is that if we can take steps within regions where the smoothness
values do not change by much, we can still make iterates computationally cheap, which we will show.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
157
Box-constrained coordinate descent under dynamic sampling. One technical diﬃculty that arises in
the analysis of coordinate descent methods under local coordinate smoothnesses is the fact that the
sampling distribution changes from iteration to iteration. In prior analyses of coordinate descent
subject to a separable convex (i.e. box) constraint [222, 449], a key technical fact of the iterates
was the fact that they could be written as a convex combination of prior iterates. Under dynamic
sampling distributions, this may no longer be the case. In this work, we give a new analysis of coor-
dinate descent under a box constraint, and show that the progress of each iteration can be directly
analyzed by using the geometry of the box constraint. We develop this analysis in Section 4.3.3, and
combining it with our local coordinate smoothness analysis yields the faster oracle for minimizing
problem (6.21).
Implementation of local smoothness estimates. One useful property of coordinate descent is that
as long as we implement the algorithm with overestimates to the local smoothness values, the
convergence rate scales with the sum of the overestimates. Our full algorithm for solving (6.21)
proceeds by showing how to compute and sample proportional to slight overestimates to the local
smoothnesses, for regression problems in a column-sparse matrix. We do so by ﬁrst proving that
the smooth approximation to ℓ∞regression admits local smoothnesses which can be bounded in
a structured way, in Section 4.3.3.
Further, using a lightweight data structure, we are able to
maintain these overestimates and sample by them in nearly-constant time, yielding a very eﬃcient
implementation, which we show in Section 4.3.5.
Maximum ﬂow algorithm
In Section 4.4, we study the maximum ﬂow problem as an example of a problem which can be
reduced to ℓ∞regression in a column-sparse matrix. We ﬁrst describe a reduction from approximate
maximum ﬂow to structured instances of ℓ∞regression, already-present in the literature [482, 318,
441].
We ﬁrst show that a direct application of our accelerated ℓ∞regression algorithm yields
the fastest currently known approximate maximum ﬂow algorithm, roughly giving a runtime of
˜O(m+(n+√ms)/ϵ). We also show that a slight modiﬁcation of our accelerated regression algorithm,
where the norm we measure smoothness and strong-convexity of the box-constrained variable is
weighted by columns of the matrix, yields a runtime of ˜O(m + √mn/ϵ), generically improving upon
the runtime of [483] for slightly-dense graphs.
Finally, in Section 4.5, we show that by opening up the algorithm further into a fully primal-
dual method, we can use a novel analysis of a variance-reduced mirror prox method based on local
coordinate smoothness estimates in order to obtain an improved runtime of ˜O(m + (n + √ns)/ϵ).
Our randomized mirror prox method requires the development of a somewhat more-complicated data
structure, based on eﬃcient polynomial approximations to the exponential, in order to approximately
query and sample from a simplex variable under dense updates.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
158
4.3
ℓ∞regression subject to a box constraint
We now show how to turn the framework presented in the previous section into improved algorithms
for the problem of box-constrained regression in the ℓ∞norm. Recall that our goal is to compute an
ϵ-approximate minimizer of the constrained ℓ∞regression problem with a O(1/ϵ) method complexity
(see Deﬁnition 8).
In the style of previous approaches to solving ℓ∞regression, because ∥x∥∞is not a smooth
function, we choose to minimize a suitable smooth approximation instead. Intuitively, the O(1/ϵ)
rate comes from accelerating gradient descent for a function which is O(1/ϵ)-smooth. One would
then expect the function error of the T th iterate with respect to OPT is proportional to (1/ϵ)/T 2,
so if we wish for an ϵ-approximate minimizer, it suﬃces to pick T = O(1/ϵ). Because our method
is not a typical accelerated method, and is instead based on reducing the proximal point method to
solving a series of subproblems (6.21), the runtime analysis proceeds somewhat diﬀerently. We will
show (roughly speaking) how to solve a subproblem of type (6.21) in
˜O

m + min(m, n)
α
+ s
α2

iterations, where each iteration can be implemented in time ˜O(c), where c is the maximum number
of nonzero entries in any column of A. Because each problem (6.21) results from a regularization
based on a regularizer r of nearly-constant range, it suﬃces to solve ˜O(α/ϵ) such problems to
yield an ϵ-approximate solution. Finally, each reduction to the subproblem is complemented by an
extragradient step, which takes time O(nnz(A)). The accelerated runtime is then roughly
˜O

nnz(A) + min(m, n)
α
+ s
α2
 α
ϵ

= ˜O
 
nnz(A) + min(m, n) +
p
nnz(A)s
ϵ
!
,
where the choice of α was to appropriately balance the terms.
4.3.1
Constructing the smooth approximation to regression
In this section, we deﬁne the smooth approximation for ℓ∞regression we use through the paper
and provide some technical facts about this approximation. Note that these approximations are
standard in the literature.
First, we deﬁne the smax function which is used throughout.
This
function is smooth in the ℓ∞norm, which can be seen because it is the result of the following
conjugate problem
max
p∈∆n ⟨p, x⟩−α
X
i
pi log pi;
because the function r(p) = P
i pi log pi is 1-strongly convex in the ℓ1 norm, its dual, the softmax
function, is smooth in the ℓ∞norm.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
159
Deﬁnition 10 (Softmax). For all real valued vectors x we let smaxα(x) := α log(P
j exp( xj
α )).
Fact 5 (Softmax additive error). ∀x ∈Rm, maxj∈[m] xj ≤smaxα(x) ≤α log m + maxj∈[m]xj.
Proof. It follows from monotonicity of log and positivity of exp: letting j∗be the maximal index of
x, smaxα(x) ≥α log(exp(xj∗/α)) = xj∗, and smaxα(x) ≤α log(m exp(xj∗/α)) = α log m + xj∗.
Note that these properties are about the quality of approximation smax provides on the maximum
element of a vector, instead of its ℓ∞norm. To apply this to an ℓ∞objective, we used the standard
reduction of applying it to the regression problem in twice the original dimension, deﬁned with a
proxy matrix A′ =

A
−A

and a proxy vector b′ =

b
−b

. For notational convenience, we will focus
on minimizing f(x) deﬁned above, but with A ∈Rn×m and b ∈Rn in the original dimensionalities,
which preserves all dependencies on the dimension and structural sparsity assumptions used later
in this work up to a constant. Next, we state some technical properties of our approximation. We
drop the α from many deﬁnitions because the α we choose for all our methods is ﬁxed.
Deﬁnition 11. For x ∈Rm let p(x) ∈Rm be deﬁned as pj(x) :=
exp(xj/α)
P
j′ exp(xj′/α).
Note that for any x the above pj(x) form a probability distribution. Moreover, they are deﬁned
in this way because they directly are used in the calculation of the gradient and Hessian of smax.
The following facts can be veriﬁed by direct calculation.
Fact 6 (Softmax calculus). ∇smaxα(x) = p(x), 0 ⪯∇2smaxα(x) ⪯α−1diag(p(x)).
4.3.2
Acceleration via proximal point method
In this section, we give an analysis of a proximal point method inspired by [415], tailored to our
purposes. The method reduces the problem of ﬁnding an ϵ-approximate saddle point to a minimax
convex-concave objective to iteratively solving a proximal subproblem to suﬃciently high accuracy.
Consider a saddle point problem of the form
min
x∈X max
p∈P f(x, p),
where f is convex in its restriction to the ﬁrst argument and concave in its restricton to the second.
Deﬁne the duality gap of a pair (x, p) to be
max
p′∈P f(x, p′) −min
x′∈X f(x′, p).
Note that when we deﬁne the associated gradient operator
g(x, p) := (∇xf(x, p), −∇pf(x, p)) ,

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
160
convexity-concavity shows we may upper bound the duality gap with respect to some pair (x′, p′)
by the regret ⟨g(x, p), (x, p) −(x′, p′)⟩, in the sense of
f(x, p′) −f(x′, p) ≤⟨∇xf(x, p), x −x′⟩−⟨∇pf(x, p), p −p′⟩= ⟨g(x, p), (x, p) −(x′, p′)⟩.
The proximal point algorithm, with a possibly randomized prox oracle, deﬁnes a sequence {zt},
where each iterate is the result of calling a proximal oracle on the previous iterate. Formally, the
method is deﬁned as follows.
Deﬁnition 12 (Primal-dual proximal point method). Initalize some z0 = (x0, p0), and let q(x) and
r(p) be convex distance generating functions; let Vz(w) be the Bregman divergence on the joint space
with respect to their sum, i.e. for z = (x, p) and z′ = (x′, p′),
Vz(z′) := V q
x (x′) + V r
p (p′).
We deﬁne the primal-dual proximal point method to be the iteration of the following procedure: on
iteration t, from the point zt = (xt, pt), let zt+1 = (xt+1, pt+1) be any point such that
max
u∈X×P

⟨g(zt+1), zt+1 −u⟩−αVzt(u) + αVzt+1(u)
	
≤ϵ.
We remark that this deﬁnition of zt+1 is motivated by the fact that the (exact) solution of
min
x∈X max
p∈P f(x, p) + αV q
xt(x) −αV r
pt(p)
(4.4)
has this property with ϵ = 0; the conceptual prox method implies that any eﬃcient algorithm
for ﬁnding a high-precision saddle point to the prox problem suﬃces. Our algorithm for computing
iterates will ultimately be randomized; we will union bound the probability that the iterate produced
does not have the necessary property over all iterations by an inverse polynomial in n.
Lemma 44. The iterates resulting from running the primal-dual proximal point method for T iter-
ations satisfy, for any u ∈X × P,
1
T
X
t∈[T ]
⟨g(zt), zt −u⟩≤αVz0(u)
T
+ ϵ.
Proof. Consider some particular iterate t. By the deﬁnition of zt+1, we have for all u,
⟨g(zt+1), zt+1 −u⟩≤α
 Vzt(u) −Vzt+1(u)

+ ϵ.
Summing over all iterations, taking an average, and using nonnegativity of V yields the conclusion.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
161
We now specialize the required oracle for computing the {zt} to our particular saddle-point
problem (in the case of ℓ∞regression). In our setting (after the constraints A have been appropriately
doubled to account for sign), we wish to solve
min
x∈[−1,1]m ∥Ax −b∥∞=
min
x∈[−1,1]m max
p∈∆n p⊤(Ax −b).
The associated gradient operator for a point (x, p) is
g(x, p) =
 A⊤p, b −Ax

.
(4.5)
For the rest of this section, whenever we write g(x, p) and the associated A, b in the regression
problem are clear from context, we mean (4.5). We note that to solve the original (primal-only)
regression problem, it suﬃces to obtain duality gap in the primal-dual regression problem with
respect to (x∗, p′) for any p′, where x∗= argminx∈[−1,1]m ∥Ax −b∥∞, as quantiﬁed in the following.
Lemma 45. Let z = (x, p) ∈[−1, 1]m × ∆n be a pair such that for all u = (x∗, p′), where x∗∈
[−1, 1]m is ﬁxed and p′ ∈∆n is arbitrary,
⟨g(z), z −u⟩≤ϵ.
Then, we have
∥Ax −b∥∞−∥Ax∗−b∥∞≤ϵ.
Proof. Choose p′ so that p′⊤(Ax −b) = ∥Ax −b∥∞. Then,
⟨g(z), z −u⟩= p⊤((Ax −b) −(Ax∗−b)) + (b −Ax)⊤(p −p′) ≥∥Ax −b∥∞−∥Ax∗−b∥∞.
The only inequality follows from p⊤(Ax∗−b) ≤∥Ax∗−b∥∞for any p ∈∆n.
In our deﬁnition of the proximal point method (Deﬁnition 12), we choose q(x) =
1
2s ∥x∥2
2, and
r(p) = P
i∈[n] pi log pi, where s := ∥x∗∥2
2. It is simple to compute that from these deﬁnitions,
V q
x (x′) = 1
2s ∥x −x′∥2
2 , V r
p (p′) =
X
i∈[n]
p′
i log p′
i
pi
.
Moreover, it is well-known that when p ∈∆n is the uniform distribution 1
n1, the range of V r
p (p′)
is bounded by log n. Therefore, Lemma 44 and Lemma 45 imply that we only need to take ˜O(α/ϵ)
iterations of the proximal point method to obtain an ϵ-approximate minimizer to the regression
problem. We complete the analysis of this framework by showing that in order to return a sequence
{zt} with the necessary properties, it suﬃces to approximately compute the saddle point to problems
of the form (4.4).

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
162
Lemma 46. From a point z = (x, p), let ¯z = (¯x, ¯p) be the solution to the problem
argminx′∈[−1,1]margmaxp′∈∆np′⊤(Ax′ −b) + α
2s ∥x −x′∥2
2 −α
X
i∈[n]
p′
i log p′
i
pi
.
Then, for ϵ < 1, any x′ with
∥x′ −¯x∥∞≤min
 
ϵ
16 ∥A∥∞
,
ϵs
8αm,
ϵα
64 ∥A∥2
∞
!
,
and setting p′ ∈∆n to be
p′ ∝exp
 1
α (Ax′ −b + α log p)

,
letting z′ = (x′, p′), for all u ∈[−1, 1]m × ∆n,
⟨g(z′), z′ −u⟩−αVz(u) + αVz′(u) ≤ϵ.
Proof. By the optimality conditions of the deﬁnition of ¯z, we see that for all u ∈[−1, 1]m × ∆n,
⟨g(¯z), ¯z −u⟩≤α(Vz(u) −V¯z(u) −Vz(¯z)) ≤α(Vz(u) −V¯z(u)).
Therefore, it suﬃces to show that
⟨g(z′) −g(¯z), ¯z −u⟩+ ⟨g(z′), z′ −¯z⟩+ α(V¯z(u) −Vz′(u)) ≤ϵ.
(4.6)
We ﬁrst derive a simple bound on ∥¯p −p′∥1. Note that by the deﬁnition of ¯p as the optimal response
to ¯x, we have that A¯x −b + α log(p/¯p) is a multiple of the all-ones vector, so
¯p ∝exp
 1
α (A¯x −b + α log p)

.
Therefore, the multiplicative ratio between each entry of p′ and ¯p is bounded by
exp
 2
α ∥A∥∞∥¯x −x′∥∞

≤exp

ϵ
32 ∥A∥∞

≤1 +
ϵ
16 ∥A∥∞
.
(4.7)
This immediately implies that ∥¯p −p′∥1 ≤ϵ ∥p′∥1 /(16 ∥A∥∞) = ϵ/(16 ∥A∥∞). Finally, we conclude
by noting that by ℓ1-ℓ∞H¨older, and ∥x′ −x∥∞≤ϵ/(16 ∥A∥∞),
⟨g(¯z) −g(z′), ¯z −u⟩≤2 ∥A∥∞∥¯x −x′∥∞+ 2 ∥A∥∞∥¯p −p′∥1 ≤ϵ
4,
⟨g(z′), z′ −¯z⟩≤∥A∥∞∥¯x −x′∥1 + ∥A∥∞∥¯p −p′∥1 ≤ϵ
4.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
163
Moreover, by using the deﬁnitions of Bregman divergences and ∥x∥1 ≤m for x ∈[−1, 1]m, and noting
that similarly to the derivation of (4.7), p′/p is entrywise bounded by exp(ϵ/4α) via ∥x′ −x∥∞≤
ϵ/(16 ∥A∥∞),
α(V q
¯x (ux) −V q
x′(ux)) = α
2s ∥¯x −ux∥2
2 −α
2s ∥x′ −ux∥2
2 = α
s ⟨ux, x′ −¯x⟩+ α
2s ⟨x′ + ¯x, x′ −¯x⟩
≤α
s

∥ux∥1 + 1
2 ∥x′ + ¯x∥1

∥x′ −¯x∥∞≤2αm
s
∥x′ −¯x∥∞≤ϵ
4,
α(V r
¯p (up) −V r
p′(up)) = α
X
i∈[n]
[up]i log p′
i
¯pi
≤α max
i∈[n] log p′
i
¯pi
≤ϵ
4.
Finally, (4.6) follows by combining the above bounds.
Finally, note that for ¯z = (¯x, ¯p) the solution to the problem
argminx′∈[−1,1]margmaxp′∈∆np′⊤(Ax′ −b) + α
2s ∥x −x′∥2
2 −α
X
i∈[n]
p′
i log p′
i
pi
,
we can equivalently write that ¯x is the solution to the problem
argminx∈[−1,1]m α log
X
i∈[n]
exp
 1
α
h
Ax −˜b
i
i

+ α
2s ∥x′ −x∥2
2 , ˜b := b −α log p.
We will show in the following section how to eﬃciently compute an approximate minimizer to this
problem with high probability.
4.3.3
Constructing the subproblem oracle
In this section, we develop a new analysis of (unaccelerated) coordinate descent under local coordi-
nate smoothness estimates and a box constraint, and show how to use it to compute a high-accuracy
solution to the subproblems required by our proximal point method. More speciﬁcally, we develop
an eﬃcient iterative method for solving the problem (abusing some notation for simplicity of this
self-contained section)
argminx∈[−1,1]m α log
X
i∈[n]
exp
 1
α [Ax −b]i

+ α
2s ∥x −¯x∥2
2 .
(4.8)
Box-constrained coordinate descent under dynamic sampling
In this section, we ﬁrst develop a general coordinate descent analysis under a box constraint,
amenable to dynamic sampling probabilities.
Let X be an arbitrary box, e.g.
product of one-
dimensional intervals, and let f be an ℓ2 µ-strongly convex function. Suppose at each point x, we

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
164
have local coordinate smoothness estimates {Lj(x)} such that for
x′ = argminx′∈X

f(x) + ⟨∇jf(x), x′ −x⟩+ Lj(x)
2
∥x′ −x∥2
2

,
we have that the upper bound (recalling Deﬁnition 9) holds, e.g.
f(x′) ≤f(x) + ⟨∇jf(x), x′ −x⟩+ Lj(x)
2
∥x′ −x∥2
2 .
Further, deﬁne
S(x) =
X
j∈[m]
Lj(x),
and assume that there is a global upper bound S on S(x). Consider the following "local smoothness"
variant of the standard coordinate descent algorithm.
Deﬁnition 13 (Locally smooth coordinate descent). Given a function f with local coordinate
smoothnesses {Lj(x)} at each point x, deﬁne the local smoothness coordinate descent algorithm
as iteratively performing the following (resetting x′ ←x) every iteration:
1. Sample j ∝Lj(x).
2. Update x′ ←argminx′∈X
n
f(x) + ⟨∇jf(x), x′ −x⟩+ Lj(x)
2
∥x′ −x∥2
2
o
.
We will now prove a bound on its multiplicative progress in a single iteration.
Lemma 47.
E[f(x′)] −f(x∗) ≤

1 −µ
2S

(f(x) −f(x∗)).
Proof. First, deﬁne
Prog↓:= f(x) −min
x↓∈X
n
f(x) +

∇f(x), x↓−x

+ µ
2
x↓−x
2
2
o
,
x↓:= argminx↓∈X
n
f(x) +

∇f(x), x↓−x

+ µ
2
x↓−x
2
2
o
.
We have by strong convexity that f(x) −f(x∗) ≤Prog↓. We also deﬁne g↓:= x −x↓, and note that
g↓agrees with ∇f(x) in the sign of each coordinate. Further, by separability of the box constraint,
0 ≤|g↓
j | ≤1
µ|∇jf(x)|, ∀j ∈[m].
(4.9)
We can explicitly write that
Prog↓=
X
j∈[m]
Prog↓
j, where Prog↓
j := g↓
j

∇jf(x) −µ
2 g↓
j

.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
165
Similarly, we deﬁne for each j ∈[m],
Prog↑
j := f(x) −min
x′∈X

f(x) + ⟨∇jf(x), x′ −x⟩+ Lj(x)
2
∥x′ −x∥2
2

.
We let g↑be the vector such that g↑
j agrees with [x−x′]j if coordinate j was sampled. In particular,
g↑agrees with ∇f(x) in the sign of each coordinate and by separability ∀j ∈[m],
0 ≤|g↑
j | ≤
1
Lj(x)|∇jf(x)|.
(4.10)
We can explicitly write
Prog↑
j = g↑
j

∇jf(x) −Lj(x)
2
g↑
j

.
First, we claim that for each j ∈[m],
Prog↑
j ≥
µ
2Lj(x)Prog↓
j.
(4.11)
Note that if coordinate j was sampled, and x′
j is on the boundary of X, then g↑
j = g↓
j , since the
minimization problem deﬁning g↓involves a larger step size.
Conversely, if [x↓]j is not on the
boundary of X, then neither is x′
j, and the upper bounds of (4.9), (4.10) are tight. In both these
cases and the third where [x↓]j is on the boundary and x′
j is not, the following inequality holds:
|g↑
j | ≥
µ
Lj(x)|g↓
j |.
We further note that
∇jf(x) −Lj(x)
2
g↑
j
 ≥1
2|∇jf(x)| ≥1
2
∇jf(x) −µ
2 g↓
j
 .
Combining these two facts with the deﬁnitions of Prog↑
j, Prog↓
j shows (4.11). Now, we have
E[f(x′)] ≤f(x) −E[Prog↑
j]
= f(x) −
X
j∈[m]
Lj(x)
2S(x)Prog↑
j
≤f(x) −
X
j∈[m]
µ
2S(x)Prog↓
j
= f(x) −µ
2S Prog↓.
Subtracting f(x∗) from both sides and using the lower bound on Prog↓gives the result.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
166
By iteratively applying Lemma 47, and Markov's inequality, we have the following corollary.
Corollary 7. Box-constrained locally smooth coordinate descent initialized at x0, applied to an ℓ2
µ-strongly convex function f converges to an ϵ-approximate minimizer with probability at least 1 −δ
in
O
S
µ log
f(x0) −f(x∗)
ϵδ

iterations.
We also remark that this analysis generalizes easily to strong convexity in any diagonal norm
given by a (nonnegative) diagonal matrix D. In particular, let f be µ-strongly-convex in the D
norm, where D = diag (() d) is some (positive) diagonal matrix, and assume we have the local
coordinate smoothness bounds {Lj(x)}j∈[m] (note that the smoothness bound is still in the ℓ2 norm,
i.e. independent of the strong convexity measurement matrix). We brieﬂy discuss how to modify
the guarantee of Lemma 47. The algorithm is given as follows.
Deﬁnition 14 (Local smoothness coordinate descent in a diagonal norm). Given a function f with
local coordinate smoothnesses {Lj(x)} at each point x, deﬁne the local smoothness coordinate descent
algorithm in the D norm as iteratively performing the following (resetting x′ ←x) every iteration:
1. Sample j ∝κj(x) := Lj(x)
dj
.
2. Update x′ ←argminx′∈X
n
f(x) + ⟨∇jf(x), x′ −x⟩+ Lj(x)
2
∥x′ −x∥2
2
o
.
We also deﬁne S(x) = P
j∈[m] κj(x), and let S be a global upper bound.
We modify the
deﬁnitions
Prog↓:= f(x) −min
x∗∈X
n
f(x) + ⟨∇f(x), x∗−x⟩+ µ
2 ∥x∗−x∥2
D
o
,
x∗:= argminx∗∈X
n
f(x) + ⟨∇f(x), x∗−x⟩+ µ
2 ∥x∗−x∥2
D
o
,
g↓:= x −x∗.
We also clearly have by the same argument that
0 ≤|g↓
j | ≤
1
µdj
|∇jf(x)|.
Therefore, the same arguments allow us to conclude that for each j ∈[m],
Prog↑
j ≥
µdj
2Lj(x)Prog↓
j, where we recall Prog↑
j = g↑
j

∇jf(x) −Lj(x)
2
g↑
j

.
Finally, our given sampling probabilities imply that we have the desired
E[f(x′)] −f(x∗) ≤

1 −µ
2S

(f(x) −f(x∗)).

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
167
This yields the following corollary.
Corollary 8. Box-constrained local smoothness coordinate descent in the diagonal D norm initialized
at x0, applied to a µ-strongly convex function f in the D norm, converges to an ϵ-approximate
minimizer with probability at least 1 −δ in
O
S
µ log
f(x0) −f(x∗)
ϵδ

iterations.
Minimizing the regularized softmax objective
We now use the developments of the prior section to obtain the runtime of an eﬃcient oracle for
solving (4.8) to high precision; we restate the objective here:
h(x) := α log
X
i∈[n]
exp
 1
α [Ax −b]i

+ α
2s ∥x −¯x∥2
2 .
The complexity of minimizing this objective function using the box-constrained coordinate descent
under local coordinate smoothnesses follows from estimates given in the following lemma.
Lemma 48 (Local coordinate smoothnesses of regularized softmax). At a point x ∈[−1, 1]m, and
for all j ∈[m], deﬁne
Lj(x) = 8
α ∥A:j∥∞

⟨|A:j|, p(x)⟩+ 2α
s

+ α
s .
Then, h is Lj(x) locally-coordinate smooth at x for all j ∈[m].
Proof. Recalling Deﬁnition 9, we prove the following: for y = x + γej, γ ∈
h
±
1
Lj(x)|∇jh(x)|
i
,
∇2
jjh(y) ≤Lj(x).
(4.12)
Deﬁning p(x) ∝exp((Ax−b)/α), by Fact 6, ∇2
jjh(y) ≤1
α ∥A:j∥2
p(y) + α
s . Therefore, it clearly suﬃces
to show that p(y) ≤8p(x) entrywise. Note that as long as we show that entrywise
exp
Ay −b
α

∈
1
e, e

exp
Ax −b
α

,
we have the conclusion by e2 < 8. Now, using the bound on γ, this is equivalent to showing for all
i that | ⟨Ai:, x −y⟩| ≤α. Recalling ∇jh(x) = ⟨A:j, p(x)⟩+ α
s (xj −¯xj), the following suﬃces:
|Aij|

∇jh(x)
Lj(x)
 =

|Aij|
 ⟨A:j, p(x)⟩+ α
s (xj −¯xj)

8
α ∥A:j∥∞
 ⟨|A:j|, p(x)⟩+ 2α
s

+ α
s
 ≤α.
The conclusion follows.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
168
Minimizing the diagonally regularized softmax objective
By a simple modiﬁcation of the regularizer q(x) used in the proximal point method, we show how
to obtain improved smoothness parameters in the regime n < m, independent of the sparsity of the
optimal solution. In particular, for D = diag (() {∥A:j∥∞}), the diagonal matrix whose entries are
the {∥A:j∥∞}, consider running the mirror prox procedure with the regularizer q(x) =
1
2n∥A∥∞∥x∥2
D;
the range of q(x) over the box [−1, 1]m is clearly at most a constant, since the sum of (absolute
values of) entries of A is bounded by ˜O(n ∥A∥∞). Therefore, it suﬃces to design an eﬃcient iterative
method for, in the vein of (4.8), solving subproblems
hd(x) := α log
X
i∈[n]
exp
 1
α [Ax −b]i

+
α
2n ∥A∥∞
∥x −¯x∥2
D .
(4.13)
In lieu of Lemma 48, we have the following local smoothness bounds on this subproblem.
Lemma 49 (Local coordinate smoothnesses of diagonally regularized softmax). Let d be the vector
whose entries are ∥A:j∥∞such that D = diag (() d). At a point x ∈[−1, 1]m, and for all j ∈[m],
deﬁne
Lj(x) = 8
α ∥A:j∥∞

⟨|A:j|, p(x)⟩+
2α
n ∥A∥∞
∥A:j∥∞

+
α
n ∥A∥∞
∥A:j∥∞.
Then, hd is Lj(x) locally-coordinate smooth at x for all j ∈[m].
Proof. The proof is similar to that of Lemma 48. Recalling Deﬁnition 9, for y = x + γej, γ ∈
h
±
1
Lj(x)|∇jhd(x)|
i
, we wish to show
∇2
jjhd(y) ≤Lj(x).
(4.14)
Deﬁning p(x) ∝exp((Ax −b)/α), by Fact 6, ∇2
jjhd(y) ≤1
α ∥A:j∥2
p(y) +
α
n∥A∥∞∥A:j∥∞. Therefore,
it clearly suﬃces to show that p(y) ≤8p(x) entrywise.
It suﬃces to show that for all i that
| ⟨Ai:, x −y⟩| ≤α, or recalling the deﬁnition of ∇jh(x) = ⟨A:j, p(x)⟩+
α
n∥A∥∞∥A:j∥∞(xj −¯xj),
|Aij|

∇jh(x)
Lj(x)
 =

|Aij|

⟨A:j, p(x)⟩+
α
n∥A∥∞∥A:j∥∞(xj −¯xj)

8
α ∥A:j∥∞

⟨|A:j|, p(x)⟩+
2α
n∥A∥∞∥A:j∥∞

+
α
n∥A∥∞∥A:j∥∞

≤α.
The conclusion follows.
4.3.4
Putting it all together: accelerated ℓ∞regression
We now state our main runtime result for ℓ∞regression. We combine previous developments to
bound the number of coordinate descent iterations needed under local coordinate smoothness es-
timates needed to ﬁnd an ϵ-approximate minimizer to the box-constrained ℓ∞regression problem

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
169
(Deﬁnition 8). We remark that the theorem statement assumes access to query and sampling oracles
for the local coordinate smoothnesses; we show how to design eﬃcient oracles for column-sparse A
in Section 4.3.5. The combination of the following two theorems formally show Theorem 15.
Theorem 19 (Coordinate acceleration for ℓ∞regression). The proximal point method (Deﬁni-
tion 12) with regularizers q(x) =
1
2s ∥x∥2
2 and r(p) = P
i∈[n] pi log pi, with each iterate deﬁned by the
local smoothness coordinate descent method (Deﬁnition 13) applied to the appropriate subproblem,
results (with high probability) in an ϵ-approximate minimizer to the box-constrained ℓ∞regression
problem in time
˜O
   
s ∥A∥2
∞
α2
+ min(m, n) ∥A∥∞
α
+ m
!
· Titer + nnz(A)
!
· α
ϵ
!
,
where Titer is the cost of sampling proportional to Lj(x) and computing the value of Lj(x) for an
iterate x of local smoothness coordinate descent. For α = max(ϵ,
p
s/m ∥A∥∞) and Titer = O(c log n)
from Section 4.3.5, where c is the column sparsity of A, the runtime is
˜O

mc + (min(m, n) + √ms) c ∥A∥∞
ϵ

.
Proof. We ﬁrst discuss the complexity of returning an iterate of the proximal point method. Lemma 46,
and the discussion following, imply that for a function of form (4.8) with optimal argument x∗
t , it
suﬃces to ﬁnd any point x′ with ∥x′ −x∗
t ∥∞bounded by an inverse polynomial in parameters
∥A∥∞, m, s, ϵ−1, α−1 to implement the proximal point method. By Fact 5, the range of the function
(where the linear term bt is appropriately shifted)
h(x) = α log
X
i∈[n]
exp
 1
α [Ax −bt]i

+ α
2s ∥x −¯x∥2
2
is at most α log n + 2 ∥A∥∞+ αm
2s , where the second term comes from the range of ∥Ax −b∥∞
over [−1, 1]m, and the third from a simple bound ∥x −¯x∥2
2 ≤m for all x ∈[−1, 1]m. Moreover,
strong-convexity of h(x) in the ℓ2 norm, and optimality of x∗
t , yields
h(x′) −h(x∗
t ) ≥⟨∇h(x∗
t ), x′ −x∗
t ⟩+ α
2s ∥x′ −x∗
t ∥2
2 ≥α
2s ∥x′ −x∗
t ∥2
2 ,
which implies
∥x′ −x∗
t ∥∞≤∥x′ −x∗
t ∥2 ≤
r
2s (h(x′) −h(x∗
t ))
α
.
(4.15)

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
170
Note that for any point x, we can deﬁne an upper bound on the sum of values Lj(x) in Lemma 48,
S := 8
α ∥A∥2
∞+ 16 min(m, n)
s
∥A∥∞+ mα
s
≥
X
j∈[m]
8
α ∥A:j∥∞

⟨|A:j|, p(x)⟩+ 2α
s

+ α
s .
(4.16)
Here, we used that the sum of entries in the matrix is at most n ∥A∥∞, and the largest entry in any
column is at most ∥A∥∞. Then, by Corollary 7 with strong convexity parameter µ = α/s, we see
that a suﬃcient x′ may be found with high probability in time
˜O
  
s ∥A∥2
∞
α2
+ min(m, n) ∥A∥∞
α
+ m
!
· Titer
!
.
Finally, due to our choice of the regularizers q and r in the proximal point method and Lemma 44,
eO(α/ϵ) iterations of proximal point suﬃce, for any α ≥ϵ. Combining these bounds yields the ﬁrst
runtime claim. To see the second, we simpliﬁed using nnz(A) ≤mc.
Before we give our result for accelerating ℓ∞regression via a diagonal norm regularization,
we state a technical result on the degree of accuracy required by solutions of the proximal point
subproblems, the analog of Lemma 46 in the diagonal norm (throughout, D := diag({∥A:j∥∞})).
Lemma 50. From a point z = (x, p), let ¯z = (¯x, ¯p) be the solution to the problem
argminx′∈[−1,1]margmaxp′∈∆np′⊤(Ax′ −b) +
α
2n ∥A∥∞
∥x −x′∥2
D −α
X
i∈[n]
p′
i log p′
i
pi
.
Then, for ϵ < 1, any x′ with
∥x′ −¯x∥∞≤min
 
ϵ
16 ∥A∥∞
, ϵn
8αm,
ϵα
64 ∥A∥2
∞
!
,
and setting p′ ∈∆n to be
p′ ∝exp
 1
α (Ax′ −b + α log p)

,
letting z′ = (x′, p′), for all u ∈[−1, 1]m × ∆n, and where divergences are with respect to q(x) :=
1
2n∥A∥∞∥x∥2
D and r(p) = P
i∈[n] pi log pi,
⟨g(z′), z′ −u⟩−αVz(u) + αVz′(u) ≤ϵ.
Proof. By the optimality conditions of the deﬁnition of ¯z, we see that for all u ∈[−1, 1]m × ∆n,
⟨g(¯z), ¯z −u⟩≤α(Vz(u) −V¯z(u) −Vz(¯z)) ≤α(Vz(u) −V¯z(u)).

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
171
Therefore, it suﬃces to show that
⟨g(z′) −g(¯z), ¯z −u⟩+ ⟨g(z′), z′ −¯z⟩+ α(V¯z(u) −Vz′(u)) ≤ϵ.
(4.17)
By exactly the same logic as in Lemma 46, we have the multiplicative ratio between every entry
of p′ and ¯p is bounded by 1 + ϵ/(16 ∥A∥∞), and ∥¯p −p′∥1 ≤ϵ/(16 ∥A∥∞). Finally, we conclude by
noting that by ℓ1-ℓ∞H¨older, and ∥x′ −x∥∞≤ϵ/(16 ∥A∥∞),
⟨g(¯z) −g(z′), ¯z −u⟩≤2 ∥A∥∞∥¯x −x′∥∞+ 2 ∥A∥∞∥¯p −p′∥1 ≤ϵ
4,
⟨g(z′), z′ −¯z⟩≤∥A∥∞∥¯x −x′∥1 + ∥A∥∞∥¯p −p′∥1 ≤ϵ
4.
Moreover, by using the deﬁnitions of Bregman divergences, ∥x∥1 ≤m for x ∈[−1, 1]m, p′/p is
entrywise bounded by exp(ϵ/4α), and ∥A∥∞is larger than every entry of D,
α(V q
¯x (ux) −V q
x′(ux)) =
α
2n ∥A∥∞
∥¯x −ux∥2
D −
α
2n ∥A∥∞
∥x′ −ux∥2
D
=
α
n ∥A∥∞
uxD(x′ −¯x) +
α
2n ∥A∥∞
(x′ + ¯x)D(x′ −¯x)
≤α
n

∥ux∥1 + 1
2 ∥x′ + ¯x∥1

∥x′ −¯x∥∞≤2αm
n
∥x′ −¯x∥∞≤ϵ
4,
α(V r
¯p (up) −V r
p′(up)) = α
X
i∈[n]
[up]i log p′
i
¯pi
≤α max
i∈[n] log p′
i
¯pi
≤ϵ
4.
Finally, (4.17) follows by combining the above bounds.
Theorem 20 (Coordinate acceleration for ℓ∞regression in a diagonal norm). The proximal point
method (Deﬁnition 12) with regularizers q(x) :=
1
2n∥A∥∞∥x∥2
D for D = diag({∥A:j∥∞}) and r(p) =
P
i∈[n] pi log pi, with each iterate deﬁned by the local smoothness coordinate descent method in the
D norm (Deﬁnition 14) applied to the appropriate subproblem, results (with high probability) in an
ϵ-approximate minimizer to the box-constrained ℓ∞regression problem in time
˜O
   
n ∥A∥2
∞
α2
+ n ∥A∥∞
α
+ m
!
· Titer + nnz(A)
!
· α
ϵ
!
,
where Titer is the cost of sampling proportional to Lj(x)/dj and computing the value of Lj(x) for an
iterate x of local smoothness coordinate descent. For α = max(ϵ,
p
n/m ∥A∥∞) and Titer = O(c log n)
from Section 4.3.5, where c is the column sparsity of A, the runtime is
˜O

mc + (n + √mn) c ∥A∥∞
ϵ

.
Proof. We ﬁrst note that without loss of generality, every entry of D is at least ϵ/m; indeed, adding

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
172
ϵ/m to an arbitrary nonzero entry of each column only perturbs the value of ∥Ax −b∥∞over [−1, 1]m
by an additive ϵ. Thus, in lieu of (4.15) in the proof of Theorem 19, it suﬃces to solve to a degree of
accuracy polynomially larger in problem parameters, where we use that the objective is 1/(n ∥A∥∞)-
strongly convex in the D norm, and the D norm is at most ϵ/m times smaller than the ℓ2 norm
(Lemma 50 bounds the accuracy we require in our subproblem solutions in ℓ∞).
We next bound the sum of local smoothnesses (relative to dj) induced by Lemma 49, and the
resulting complexity of solving the subproblems induced by the proximal point method with a
diagonal regularizer; the remainder of the proof follows identically from Theorem 19. Note that
X
j∈[m]
Lj(x)
dj
=
X
j∈[m]
8
α

⟨|A:j|, p(x)⟩+
2α
n ∥A∥∞
∥A:j∥∞

+
α
n ∥A∥∞
= O
∥A∥∞
α
+ 1 +
mα
n ∥A∥∞

.
The strong convexity parameter of the induced subproblems in the diagonal norm, of the form (4.13),
is α/(n ∥A∥∞). Thus, applying Corollary 8 implies each iterate of the mirror prox method can be
found with high probability in time
˜O
  
n ∥A∥2
∞
α2
+ n ∥A∥∞
α
+ m
!
· Titer
!
.
Finally, due to the choice of regularizers, the domain size is still ˜O(1), so ˜O(α/ϵ) iterations of
proximal point suﬃce, giving the ﬁrst claim; the second claim follows by choice of α.
4.3.5
Cheap iterations for ℓ∞regression in column-sparse matrices
In this section, we show how to attain cheap iterations for A whose columns have bounded sparsity.
In particular, suppose A is c-column-sparse. We show how to, for the local coordinate smoothness
estimates
Lj(x) = 8
α ∥A:j∥∞

⟨|A:j|, p(x)⟩+ 2α
s

+ α
s
(4.18)
deﬁned in Lemma 48, implement maintenance of the Lj(x) and sampling by the quantities Lj(x) for
each iteration of the local smoothness coordinate descent procedure applied to the problem (4.8), in
time Titer = O(c log n). This shows that the runtime of the eﬃcient implementation of our algorithm
is, up to a ˜O(c) multiplicative factor, the same as the iteration count; in particular, for c = ˜O(1), we
are able to implement each step in ˜O(1) time, without aﬀecting the number of iterations by more
than a ˜O(1) factor. More formally, in this section we show the following.
Lemma 51 (Eﬃcient implementation of iterates). Suppose we implement local smoothness coordi-
nate descent (Deﬁnition 13) for the problem (4.8) for some c-column-sparse A. Then, with nnz(A)
precomputation cost, throughout the lifetime of the algorithm for local coordinate smoothness esti-
mates Lj(x) (4.18) where x is an iterate, it is possible to (1) maintain the sum P
j∈[m] Lj(x), (2)

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
173
compute for any j the value Lj(x), and (3) sample from the distribution {pj ∝Lj(x)} in time
O(c log n) per iteration.
Proof. We will describe the Lj(x) maintenance and sampling procedures separately.
Maintaining smoothness overestimates.
We ﬁrst show how to (implicitly) maintain the quantities
pi(x) =
exp( 1
α[Ax −b]i)
P
i′∈[n] exp( 1
α[Ax −b]i′)
in O(c) time per iteration. In particular, because each iteration of (local smoothness) coordinate
descent, starting at x and stepping to x′, only aﬀects a single coordinate, and by column-sparsity
this only aﬀects at most c of the values exp( 1
α[Ax −b]i), we can maintain their sum in O(c) time,
and also maintain the vector exp( 1
α(Ax −b)).
Next, we discuss how to maintain P
j∈[m] Lj(x) and query any Lj(x) in O(c) time per iteration.
In O(nnz(A)) time we precompute and store all values
16 ∥A:j∥∞
s
+ α
s ,
and there are at most c entries in A:j, so querying Lj(x) can be performed in O(c) time, because we
can compute any entry of p(x) using the stored exp( 1
α(Ax −b)) and its maintained sum. Moreover,
in computing the sum
X
j∈[m]
Lj(x) =

X
j∈[m]
16 ∥A:j∥∞
s
+ mα
s

+

8
α
X
i∈[n]
pi(x)
X
j∈[m]
|Aij| ∥A:j∥∞

,
all quantities other than the pi(x) can be precomputed; the second summand can be computed with
respect to the unnormalized vector exp( 1
α(Ax −b)), and then scaled uniformly using its sum.
Sampling from the distribution.
In this part of the proof, we describe how to implement sampling from the distribution pro-
portional to Lj(x). First, in the prior discussion note that we maintain the sum of the Lj(x) by
computing the values of the two summands
X
j∈[m]
16 ∥A:j∥∞
s
+ mα
s , 8
α
X
i∈[n]
pi(x)
X
j∈[m]
|Aij| ∥A:j∥∞.
We ﬁrst ﬂip an appropriately biased coin to choose a summand. If the ﬁrst is selected, then we
sample a coordinate j ∈[m] with probability proportional to
16 ∥A:j∥∞
s
+ α
s ;

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
174
this can be done in constant time via precomputation [528].
To sample from the second summand, it clearly suﬃces to ﬁrst sample the rows of A by a distri-
bution proportional to p(x), and then sample the indices of that row proportional to |Aij| ∥A:j∥∞,
the latter of which takes constant time via precomputation [528]. To sample the rows, we use the
well-known strategy that it suﬃces to maintain an augmented binary search tree data structure
whose leaves dynamically maintain the set of exp( 1
α[Ax −b]i) for the current iterate x. As previ-
ously argued, each iteration changes only c of these values, so maintaining the augmented binary
search tree takes O(c log n) per iteration.
4.4
Accelerating maximum ﬂow
The primary goal of this section is to show how to use the development of Section 4.3, tailored
to the regression problem associated with maximum ﬂow, and give tighter analyses on its runtime
guarantees to demonstrate how it yields faster algorithms. The reduction to ℓ∞regression is the
same as introduced in [482], and is included for completeness.
4.4.1
Maximum ﬂow preliminaries
The maximum ﬂow problem is deﬁned as follows: given a graph, and two of its vertices s and t
labeled as source and sink, ﬁnd a ﬂow f ∈Rm which satisﬁes the capacity constraints such that the
discrete divergence at the sink, (Bf)t, is as large as possible, and (Bf)s = −(Bf)t, (Bf)v = 0 for
v ̸= s, t.
Following the framework of [482], we consider instead the equivalent problem of ﬁnding a min-
imum congestion ﬂow; intuitively, if we route 1 unit of ﬂow from s to t and congest edges as little
as possible, we can ﬁnd the maximum ﬂow by just taking the multiple of the minimum congestion
ﬂow which just saturates edges. The congestion incurred by a ﬂow f is
U −1f

∞where U is the
diagonal matrix of edge capacities, and we say f routes demands d if Bf = d. The problem of
ﬁnding a minimum congestion ﬂow for a given demand vector, and its dual, the maximum congested
cut, can be formulated as follows:
min.
f
∥U −1f∥∞
s.t.
Bf = d, f ≥0.
max.
v
d⊤v
s.t.
∥UB⊤v∥1 ≤1.
(4.19)
Let dS := P
u∈S du and c(S, T) denote the total weight of edges from S to T. It is well-known that
for the second problem, one of the threshold cuts with respect to v achieves dS/c(S, V −S) ≥d⊤v.
Whenever the ﬂow problem is clear from context, we will refer to any optimal ﬂow by f OPT.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
175
4.4.2
From maximum ﬂow to constrained ℓ∞regression
First, we show how to transform the maximum ﬂow problem into a constrained regression problem.
The key tool used here is the concept of a good congestion approximator [482], and associated
properties.
Deﬁnition 15 (Congestion approximator). An α-congestion approximator for G is a matrix R such
that for any demand vector d, ∥Rd∥∞≤OPTd ≤α ∥Rd∥∞.
For undirected graphs, it is known that ˜O(1)-congestion approximators can be computed in nearly
linear time [376, 482, 318, 441]. Further, the certain variants of these congestion approximator have
additional nice properties. We use the following construction from [441].
Theorem 21 (Summary of results in [441]). There is an algorithm which given an m-edge n-vertex
undirected graph runs in time ˜O(m) and with high probability produces an α-congestion approximator
R, for α = ˜O(1). Furthermore, the matrix A := 2αRBU has the following properties: (1) each
column of A has at most ˜O(1) nonzero entries, (2) ∥A∥∞= ˜O(1), (3) A has O(n) rows, and (4) A
can be computed in time ˜O(m).
The above theorem is the result of a construction in [441]. Properties 2, 3, and 4 are direct
results of the construction given in the paper (where 3 follows from the fact that the congestion
approximator comes from routing on a graph which is a tree). Property 1 results from the way in
which the tree is constructed, such that the depth of the congestion-approximating tree is ˜O(1), so
each edge in the original graph G is only routed onto a polylogarithmic number of edges.
Our analysis of reducing the ﬂow problem to the regression problem follows that of [482]. In
particular, the reduction is given as follows.
Lemma 52. Let G be an undirected graph and d be a demand vector. Assume we are given an
α-congestion approximator R, and the associated matrix A = 2αRBU. Furthermore, let 2αRd := b.
In order to multiplicatively approximately solve the maximum ﬂow problem given by Equation (4.19),
it suﬃces to solve an associated box-constrained regression problem ∥Ax −b∥∞over x ∈[−1, 1]m a
nearly-constant number of times to an ϵ-additive approximation, and pay an additional ˜O(m) cost,
which under the change of variables x := U −1f recovers a corresponding ﬂow. We call the full
algorithm Flow-To-Regress.
In particular, we are able to use R from the statement of Theorem 21. For completeness, we will
prove Lemma 52 in the appendices, but on a ﬁrst read one may skip the proof and use the reduction
statement as a black box result for the remaining analysis.
4.4.3
Runtimes for accelerated maximum ﬂow
Here, we provide a full description of how to implement relevant machinery for applying the tools
from Section 4.3 for accelerating the minimization of a constrained ℓ∞function to the regression

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
176
problem given in Lemma 52. Due to the arguments presented in Appendix C.2, it suﬃces to bound
the runtime of approximately solving the initial regression problem.
Deﬁnition 16 (Flow regression problem). The maximum ﬂow regression problem asks to ϵ-approximately
minimize the function ∥Ax −b∥∞subject to x ∈[−1, 1]m, ∥b∥∞≤1, and for ˜O(1)-column-sparse A
with ∥A∥∞= ˜O(1).
Lemma 52 implies that the cost of ﬁnding an ϵ-approximate maximum ﬂow is (up to logarithmic
factors) the same as solving the ﬂow regression problem once.
Applications of Section 4.3
We ﬁrst show how to use the methods of Section 4.3 to obtain an improved maximum ﬂow algo-
rithm. First, note that by applying Theorem 19 directly, combining with the properties given in
Theorem 21 of the regression matrix A, we immediately obtain a runtime of ˜O (m + (n + √ms)/ϵ)
for the maximum ﬂow problem, where the additive factor ˜O(m) comes from the preprocessing re-
quired in Section 4.3.5, as well as the cost of computing the matrix A. Here, we used min(m, n) = n
in the case of the ﬂow regression matrix. We further can apply Theorem 20 to obtain a runtime
of ˜O (m + √mn/ϵ), where the dominant term is √mn as m = Ω(n). Taking the better of these
runtimes implies the following.
Theorem 22. There is an algorithm that takes time ˜O(m + (n +
p
m min(n, s))/ϵ) to ﬁnd, with
high probability, an ϵ-approximate maximum ﬂow, where s is the squared ℓ2 norm of the congestion
vector of any optimal ﬂow.
Tighter runtime dependence
We develop an algorithm with an improved runtime for the ﬂow regression problem in Section 4.5,
based on directly applying a randomized mirror prox method to the primal-dual regression objective.
Its runtime guarantee is stated here, and its full details are given in Section 4.5.
Theorem 23. There is an algorithm, initialized at x0, for ﬁnding an ϵ-approximate minimizer to
the ﬂow regression problem (Deﬁnition 16), with high probability, in time ˜O(m + max(n, √ns)/ϵ),
where s = ∥x0 −x∗∥2
2.
By combining this improved algorithm with the reduction procedure of Lemma 52, we obtain
our fastest algorithm for maximum ﬂow, generically improving upon Theorem 22.
Theorem 24. There is an algorithm that takes time ˜O(m + max(n, √ns)/ϵ) to ﬁnd, with high
probability, an ϵ-approximate maximum ﬂow, where s is the squared ℓ2 norm of the congestion
vector of any optimal ﬂow.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
177
4.4.4
Exact maximum ﬂows in uncapacitated graphs
Here, we describe several corollaries of our approach, for rounding to an exact maximum ﬂow for
several types of uncapacitated graphs. In an uncapacitated graph, s = ∥f ∗∥2
2 ≤Fn where F is the
maximum ﬂow value, because the maximum ﬂow is a 0-1 ﬂow, and thus can be decomposed into F
s −t paths with length at most n. We assume here that all the graphs are simple, and thus m ≤n2;
it is not diﬃcult to generalize these results to non-simple graphs. As preliminaries, we state the
following standard techniques for rounding to exact maximum ﬂows.
Lemma 53 (Theorem 5 in [343]). There is a randomized algorithm that runs in expected time ˜O(m)
which takes a fractional ﬂow of value F on an uncapacitated graph, and returns an integral ﬂow of
value ⌊F⌋.
We will thus always assume that we have applied the rounding to an integral ﬂow as a pre-
processing step, as it will not aﬀect our asymptotic runtime.
Lemma 54 (Augmenting paths). There is an algorithm that runs in time O(m) which takes a non-
maximal integral ﬂow of value F on an uncapacitated graph, and returns an integral ﬂow of value
F + 1.
Suppose we have a ﬂow with value (1 −ϵ)F, where the maximum ﬂow value is F. The two
lemmas for rounding and augmenting a ﬂow therefore imply that the additional runtime required to
attain an exact maximum ﬂow is O(ϵFm).
Undirected uncapacitated graphs
We state several corollaries of Theorem 24 which apply to ﬁnding exact maximum ﬂows in various
types of undirected uncapacitated graphs. All of these results only hold with high probability.
Corollary 9 (Undirected graphs). There is an algorithm which ﬁnds a maximum ﬂow in an undi-
rected, uncapacitated graph in time ˜O(m5/4n1/4).
Proof. We run the algorithm from Theorem 24 for ϵ = n1/4/m3/4, and then run augmenting paths
for O(ϵm2) iterations. Note that the maximum ﬂow value and sparsity are bounded by m, and thus
this will yield a maximum ﬂow. Furthermore the runtime of the approximate algorithm is bounded
by m + √nm/ϵ. Putting together these two runtimes yields the result.
Corollary 10 (Undirected graphs with small maximum ﬂow value). There is an algorithm which
ﬁnds a maximum ﬂow in an undirected, uncapacitated graph with maximum ﬂow value F in time
˜O(m + min(√mnF 3/4, m3/4n1/4√
F)).
Proof. The analysis here is the same as in Corollary 9, but instead we note that the bound on
s is min(m, Fn), where the latter factor results from combining F paths of length at most n. If

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
178
the better bound is Fn, our runtime is bounded by ˜O(m + (n +
√
n2F)/ϵ + ϵFm), and choosing
ϵ = n1/2/(F 1/4m1/2) yields the result. If the better bound is m, our runtime is bounded by ˜O(m +
√nm/ϵ + ϵFm), and choosing ϵ = n1/4/(
√
Fm1/4) yields the result.
Corollary 11 (Undirected graphs with sparse optimal ﬂow). There is an algorithm which ﬁnds a
maximum ﬂow in an undirected, uncapacitated graph with a maximum ﬂow that uses at most s edges
in time ˜O(m + √msn1/4 max(n, s)1/4).
Proof. The analysis here is the same as in Corollary 9, but instead we note that the bound on the
maximum ﬂow value is also s. Thus, our runtime is bounded by ˜O(m + max(n, √ns)/ϵ + ϵsm). If
n ≤s, choosing ϵ = n1/4/(s1/4m1/2) yields the result; otherwise, we choose ϵ =
p
n/ms.
Directed graphs
We follow the standard reduction of ﬁnding a maximum ﬂow in a directed graph to ﬁnding a
maximum ﬂow in an undirected graph described in, for example, [363]. In short, an undirected
graph with maximum ﬂow value O(m) is created, such that we can initialize the algorithm in
Theorem 24 at a ﬂow which is oﬀfrom the true maximum ﬂow by s in ℓ2
2 distance. We give this
reduction in Appendix C.2.2, and refer the reader to [363] for a more detailed exposition.
Thus, after applying this reduction, the only diﬀerence in the runtimes given by the previous
section are that the rounding algorithm will always take time O(ϵm2) instead of O(ϵFm). This
immediately yields the following runtimes for exact maximum ﬂows in directed graphs.
Corollary 12 (Directed graphs). There is an algorithm which ﬁnds a maximum ﬂow in a directed,
uncapacitated graph in time ˜O(m5/4n1/4).
Corollary 13 (Directed graphs with a sparse optimal ﬂow). There is an algorithm which ﬁnds a
maximum ﬂow in a directed, uncapacitated graph in time ˜O(mn1/4 max(n, s)1/4).
4.5
Improved ﬂow runtimes via primal-dual coordinate re-
gression
In this section, we prove Theorem 23 by giving the algorithm and analyzing its runtime. Throughout,
as in the statement of the ﬂow regression problem (Deﬁnition 16), A ∈Rn×m has ˜O(1)-sparse
columns, ∥A∥∞≤1, and ∥b∥∞≤1, where we drop logarithmic factors in ∥A∥∞for simplicity. We
describe how to obtain a point ˆx with ∥ˆx∥∞≤1, and
∥Aˆx −b∥∞−ϵ ≤OPT := ∥Ax∗−b∥∞, where x∗:= argminx|∥x∥∞≤1 ∥Ax −b∥∞.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
179
The runtime we will prove for the algorithm (initialized at the origin) is, as in Theorem 24,
˜O

m + n + √ns
ϵ

, where s := ∥x∗∥2
2 .
Note if we wish to supply the algorithm with an initial point which is not the origin, as is the case for
our results on maximum ﬂow in directed graphs, it suﬃces to modify the deﬁnition of b appropriately
and shift by the initial point (see Appendix C.1.2 for a more formal treatment).
4.5.1
Overview
We ﬁrst give an outline of our algorithm. The main motivation for the form it takes is to obtain the
"best of both worlds" runtime of the form √ns/ϵ. In terms of the dependence of Theorem 19 on
s, i.e. the sparsity of the optimal point, a standard (unweighted) Euclidean regularizer is necessary
for the primal point x ∈[−1, 1]m. In terms of the dependence of Theorem 20 trading oﬀan n
factor for an m, we require more ﬁne-grained estimates on local coordinate smoothnesses based on
dual information and properties of the matrix. We obtain both of these improvements in our ﬁnal
runtime via a fully primal-dual coordinate regression algorithm.
Throughout, all divergences on x space are with respect to q(x) =
1
2s ∥x∥2
2, on y space2 are with
respect to r(y) = P
i yi log yi, and on the product space are with respect to the direct sum (we drop
superscripts in deﬁnitions of Bregman divergences in this section, as the regularizer will be ﬁxed).
Regularized subproblem.
The ﬁrst step of our method is to deﬁne the following function, a
regularized variant of the primal-dual formulation of the box-constrained ∥Ax −b∥∞objective:
h(x, y) := y⊤(Ax −b) + ϵ
2q(x) −
ϵ
4 log nr(y).
(4.20)
Throughout, we refer to the saddle point to the regularized objective h by ˜x ∈[−1, 1]m, ˜y ∈∆n.
The motivation for considering the regularized problem is related to technical issues which arise
when generalizing Lemma 44 to interact with a randomized algorithm; as we will see, returning the
average iterate is computationally expensive for our coordinate method. We bypass this by providing
a last-iterate guarantee via regularization, by arguing we can repeatedly return a point in each phase
halving the distance to the saddle point.
The following lemma shows that to solve the box-constrained ℓ∞regression problem, it suﬃces
to solve the regularized problem
min
x∈[−1,1]m max
y∈∆n h(x, y)
to high accuracy. We also show that the regularized optimizer's ℓ2 sparsity is not too large.
Lemma 55. ∥˜x∥2
2 ≤2s, and ∥A˜x −b∥∞≤OPT + ϵ
2.
2In this section, we use y rather than p to denote dual points, as they evolve separately; in our previous algorithms,
p was typically a probability distribution induced by a primal point x.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
180
Proof. Recall that the deﬁnition of smaxα(x) implies
smaxα(x) = max
y∈∆n y⊤(Ax −b) −αr(y).
By Fact 5,
∥Ax −b∥∞≤smaxϵ/4 log n(x) ≤∥Ax −b∥∞+ ϵ
4.
Correspondingly, we have the following chain of inequalities:
h(˜x, ˜y) ≤h(x∗, ˜y) ≤smaxϵ/4 log n(x∗) + ϵ
2q(x∗) ≤OPT + ϵ
4 + ϵ
4 = OPT + ϵ
2.
The ﬁrst inequality follows from minimality of ˜x with respect to ˜y, the second from considering the
terms in h corresponding to y, and the last by the deﬁnition of OPT and s. Now, we also have
h(˜x, ˜y) = smaxϵ/4 log n(˜x) + ϵ
2q(˜x) ≥∥A˜x −b∥∞+ ϵ
4s ∥˜x∥2
2 .
Putting these together and using ∥A˜x −b∥∞≥OPT by deﬁnition,
OPT + ϵ
4s ∥˜x∥2
2 ≤OPT + ϵ
2 ⇒∥˜x∥2
2 ≤2s.
Similarly, the other conclusion follows by nonnegativity of
ϵ
4s ∥˜x∥2
2.
Consequently, an algorithm which is capable of obtaining a high-accuracy saddle point to h
suﬃces for minimizing the original objective.
Randomized mirror prox method. We now describe one phase of our algorithm, which takes an
initial point zk−1,0 = (xk−1,0, yk−1,0), and returns a point zk,0 = (xk,0, yk,0) with
E[Vzk,0(˜x, ˜y)] ≤1
2Vzk−1,0(˜x, ˜y).
(4.21)
Here, the expectation is over randomness used in the kth phase, i.e. the randomness used to deﬁne
the point zk,0. Combining this recursive guarantee via iterating expectations with the following
initial bound (which uses Lemma 55) gives us a logarithmic bound on the number of phases.
Lemma 56. Let x0,0 be the all-zeroes vector and y0,0 = 1
n1. Then, Vx0,0,y0,0(˜x, ˜y) ≤Θ0 := 1+log n.
In order to obtain the guarantee (4.21), our starting point is Nemirovski's mirror prox method
[415], which can be viewed as a ﬁxed-point iteration approximating the proximal point method
(Deﬁnition 12). Note that optimality conditions imply that iterating (4.4) in the proximal point
method produces a sequence of iterates satisfying
zt+1 ←argminz {⟨g(zt+1), z⟩+ αVzt(z).}

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
181
However, this method is not implementable, as zt+1 uses its own gradient operator in its deﬁnition.
Nemirovski's mirror prox approximates this process via a ﬁxed-point iteration, by deﬁning a two-step
sequence
wt ←argminw
 1
κ ⟨g(zt), w⟩+ Vzt(w)

, zt+1 ←argminz
 1
κ ⟨g(wt), z⟩+ Vzt(z)

.
(4.22)
Here, the parameter κ must be chosen to meet certain criteria so that the ﬁxed-point iteration
provably converges to a suﬃcient quality, and also governs the iteration count. Typically, κ depends
on the strong convexity of the regularizers q and r, which leads to a dimension dependence in the
runtime in the case of ℓ∞regression. [483] bypassed this by identifying a weaker criteria for the
sequence (4.22) to converge. We obtain further improvements via a randomized variation of (4.22).
Note that the gradient operator of the problem (4.20) is:
g(x, y) :=

A⊤y + ϵ
2sx, b −Ax +
ϵ
4 log n log y

.
A natural attempt at an unbiased estimator for g, inspired by the algorithm of Section 4.3, is
(for some sampling probabilities {pj}) to randomly sample a coordinate of the primal block of g, i.e.
gj(x, y) :=
 1
pj

A⊤
:jy + ϵ
2sxj

ej, b −Ax +
ϵ
4 log n log y

,
(4.23)
We would then deﬁne a step by: sample j ∼{pj}, then iterate
wt ←argminw
 1
κ ⟨gj(zt), w⟩+ Vzt(w)

, zt+1 ←argminz
 1
κ ⟨gj(wt), z⟩+ Vzt(z)

.
However, in order to obtain our tight runtimes by leveraging a primal-dual analog of local coor-
dinate smoothnesses, we require "sharing randomness" between these iterates, i.e. using the same
coordinate j in both steps. Note that in doing so, it no longer makes sense to say that gj(wt) is an
unbiased estimator for g(wt), as the choice of j was used in the deﬁnition of wt. We bypass this
by deﬁning an "aggregate point" ¯wt which gj(wt) is unbiased for, over the randomness of wt. We
remark that this design strategy is a somewhat bespoke application of developments in Section 2.6,
tailored to our purposes.
We then use a tight characterization of the convergence of our randomized method via local coor-
dinate smoothnesses to argue about the quality of the average iterates ¯wt, and show that randomly
sampling one over eO(m + (n + √ns)/ϵ) iterations halves the divergence to (˜x, ˜y) in expectation.
For this last step, we use the strong monotonicity3 of the objective h to convert regret bounds into
divergence bounds. Our complete algorithm concludes by repeating this procedure for eO(1) phases.
Roadmap. Section 4.5.2 states the algorithm, a randomized variation of mirror prox which uses
3Strong monotonicity is a primal-dual analog of strong convexity.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
182
the local coordinate smoothness ideas developed in Section 4.3 in its analysis. It ﬁrst develops a
one-phase analysis, which leverages strong monotonicity of the objective h in order to halve the
distance to the true saddle point (˜x, ˜y) in eO(m + max(n, √ns)/ϵ) iterations constituting a phase.
It then uses the output of each phase as the starting point for the next phase, culminating in a
high-accuracy saddle point in a logarithmic number of phases.
A key technical hurdle is that the iterates of the algorithm no longer have the sparse update
structure used in the data structure development of Section 4.3.5. In Section 4.5.3, we show how to
carefully use the structure of the updates to design a data structure based around Taylor approx-
imation to perform iterations in batches, using nearly-constant amoritized time per iteration. We
remark that the data structure we develop in this section is in some sense generalized by the data
structure used for ℓ1-ℓ1 games in Chapter 3.
4.5.2
Algorithm
Throughout, we index phases of the algorithm by k ∈[K], and iterates within a phase by t ∈[T].
As discussed in the overview, we will choose T = eO(m + (n + √ns)/ϵ), and K = eO(1).
Section 4.5.2 deﬁnes local coordinate smoothness quantities which will factor into the algorithm.
Section 4.5.2 gives an analysis of a single phase of the algorithm, which outputs a point with
expected divergence halved from the phase input. At the end of this section, we give a complete
implementation of the phase, where we highlight issues with inexact implementation (which will
be treated formally in Section 4.5.3). Section 4.5.2 leverages this single-phase method to give the
complete algorithm, and proves the ﬁnal runtime guarantee.
Preliminaries
We ﬁrst deﬁne some parameters used in the algorithm. For any y ∈∆n we deﬁne for all j,
Lj(y) := s ∥A:j∥∞|A:j|⊤y + ϵ ∥A:j∥∞,
˜Lj(y) :=


q
s ∥A:j∥∞
X
i∈[n]
q
|Aij|yi +
q
ϵ ∥A:j∥∞


2
.
where |A:j| is element-wise.
These quantities will serve the role of local coordinate smoothness
estimates in our algorithm and analysis. It is immediate that for all j,
˜O(1)Lj(y) ≥˜Lj(y) ≥Lj(y),
(4.24)
where the ˜O(1) factor is due to Cauchy-Schwarz and that each A:j has ˜O(1) non-zero entries.
Lemma 57. For any y, P
j
q
˜Lj(y) ≤C√ns + √mnϵ, for some C = ˜O(1).

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
183
Proof. Let all columns of A have at most c = ˜O(1) nonzero entries. Then,

X
j∈[m]
X
i∈[n]
q
∥A:j∥∞yi|Aij|


2
=

X
j∈[m]
q
∥A:j∥∞·

√c ·
s X
i∈[n]
yi|Aij|




2
≤c

X
j∈[m]
∥A:j∥∞



X
j∈[m]
X
i∈[n]
yi|Aij|


≤nc

X
i∈[n]
yi∥Ai∥1

≤nc.
(4.25)
Here, the ﬁrst line follows from Cauchy-Schwarz and using the fact that the sum P
i∈[n]
p
yi|Aij|
is c-sparse, the second line follows from Cauchy-Schwarz again, and the third line follows from the
assumption ∥A∥∞≤1. Thus,
X
j∈[m]
q
˜Lj(y) = √s
X
j∈[m]
X
i∈[n]
q
∥A:j∥∞|Aij|yi +
X
j∈[m]
q
ϵ ∥A:j∥∞
≤√nsc + √ϵ
s
m
X
j∈[m]
∥A:j∥∞≤√nsc + √mnϵ.
It suﬃces to choose C = √c, where we used the column sparsity assumption.
Finally, we deﬁne the following sampling distribution at any point y:
pj(y) =
C√ns
C√ns + √mnϵ ·
p
s ∥A:j∥∞
P
i
p
|Aij|yi
P
j
p
s ∥A:j∥∞
P
i
p
|Aij|yi
+
√mnϵ
C√ns + √mnϵ ·
p
ϵ ∥A:j∥∞
P
j
p
ϵ ∥A:j∥∞
≥
q
˜Lj(y)
C√ns + √mnϵ.
(4.26)
The last inequality follows from the bounds from Lemma 57,
X
j
q
s ∥A:j∥∞
X
i
q
|Aij|yi ≤C√ns,
X
j
q
ϵ ∥A:j∥∞≤√mnϵ.
We also make the simplifying assumption that at any y, all the sampling probabilities pj(y) are
at least 1/(2m). To see why this is a valid assumption, our algorithm ultimately has a runtime
depending linearly on our bound on P
j∈[m]
q
˜Lj(y). By treating each
q
˜Lj(y) as its sum with the
average square root coordinate smoothness, this only doubles the overall sum (and therefore the
bound in Lemma 57), but enforces the lower bound on the sampling probabilities. This can be
always be implemented by uniform sampling with half probability.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
184
Single phase analysis
In this section we give an analysis of the kth phase.
We drop subscript k from all iterates for
simplicity, within the context of this section, until the very end. We deﬁne
κ := mϵ + 8√mnϵ + 8C√ns + 16n,
the parameter which will ultimately govern the iteration count of the phase. We brieﬂy discuss
where each summand comes from in the analysis.
1. The factor of mϵ is used to account for terms of the form
ϵ
2pj [xt]j showing up in the randomized
gradient estimator, which can be as large as mϵ, in Lemma 62. This is the key lemma used to
bound the progress of a single iteration.
2. The factor of 8√mnϵ is used to ensure the stability of the simplex variable in a single iteration,
due to the eﬀects of terms of the form
ϵ
2pj [xt]j, in Lemmas 59 and 60. It is never the leading-
order term, due to the terms mϵ and 16n.
3. The factor of 8C√ns is used for both the error analysis and stability, due to eﬀects of terms
of the form
1
pj A∆(j)
t , in Lemmas 59, 60, and 62.
4. The factor of 16n is used to guarantee that κ = Ω(n). This is necessary in bounding the
movement due to a ﬁxed, dense term in the gradient updates, in the runtime analysis of
Section 4.5.3. In particular, it ensures we do not have to restart the data structure for simplex
variable maintenance too frequently.
We now give one iteration of the phase, starting at a point zt = (xt, yt).
1. Sample j ∝pj(yt)
2. x(j)
t+ 1
2 ←argminx∈[−1,1]m
nD
1
κpj (A⊤
:jyt +
ϵ
2s[xt]j)ej, x
E
+ Vxt(x)
o
.
3. yt+ 1
2 ←argminy∈∆n
nD
1
κ

b −Axt +
ϵ
4 log n log yt

, y
E
+ Vyt(y)
o
.
4. ∆(j)
t
:= x(j)
t+ 1
2 −xt.
5. x(j)
t+1 ←argminx∈[−1,1]m
nD
1
κpj (A⊤
:jyt+ 1
2 +
ϵ
2s[x(j)
t+ 1
2 ]j)ej, x
E
+ Vxt(x)
o
.
6. y(j)
t+1 ←argminy∈∆n
nD
1
κ

b −A

xt + 1
pj ∆(j)
t

+
ϵ
4 log n log yt+ 1
2

, y
E
+ Vyt(y)
o
.
We remark that in all but possibly the jth coordinate, x(j)
t+ 1
2 and x(j)
t+1 are identical to xt. We write
zt = (xt, yt), w(j)
t
=

x(j)
t+ 1
2 , yt+ 1
2

, z(j)
t+1 =

x(j)
t+1, y(j)
t+1

.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
185
We brieﬂy remark on the form of the iterates. The gradient estimators inducing the points x(j)
t+ 1
2 ,
x(j)
t+1 are precisely those described by (4.23), where we note that the point yt+ 1
2 is deterministic
(conditioned on zt). Moreover, the gradient estimator inducing y(j)
t+1 is chosen so that our algorithm
has the following property, which implies in each iteration, there is an "aggregate point" ¯wt whose
regret we can bound. In this sense, the term
1
pj ∆(j)
t
can be viewed as a debiasing step.
Lemma 58. Let ¯xt+ 1
2 = xt + P
j ∆(j)
t , the point taking all coordinate steps from xt, and denote
gj(w(j)
t ) =
 1
pj

A⊤
:jyt+ 1
2 + ϵ
2s
h
x(j)
t+ 1
2
i
j

ej, b −A

xt + 1
pj
∆(j)
t

+
ϵ
4 log n log yt+ 1
2

.
Then, we have for ˜z = (˜x, ˜y),
Ej
hD
gj(w(j)
t ), w(j)
t
−˜z
Ei
= ⟨g( ¯wt), ¯wt −˜z⟩,
where ¯wt = (¯xt+ 1
2 , yt+ 1
2 ).
Proof. Recall that x(j)
t+ 1
2 and ¯xt+ 1
2 agree in the jth coordinate. Then, expanding we have
Ej
hD
gj(w(j)
t ), w(j)
t
−˜z
Ei
=
X
j
pj
 1
pj
A⊤
:jyt+ 1
2 , ¯xt+ 1
2 −˜x

+
 1
pj
ϵ
2s[¯xt+ 1
2 ]j, ¯xt+ 1
2 −˜x

+

b −A

xt + 1
pj
∆(j)
t

, yt+ 1
2 −˜y

+

ϵ
4 log n log yt+ 1
2 , yt+ 1
2 −˜y

=
D
A⊤yt+ 1
2 , ¯xt+ 1
2 −˜x
E
+
D ϵ
2s ¯xt+ 1
2 , ¯xt+ 1
2 −˜x
E
+
D
b −A¯xt+ 1
2 , yt+ 1
2 −˜y
E
+

ϵ
4 log n log yt+ 1
2 , yt+ 1
2 −˜y

= ⟨g( ¯wt), ¯wt −˜z⟩.
Next, we require the following bound on the size of the updates.
Lemma 59. For any t, call the updates to the simplex variables due to the bilinear term
δt := 1
κ(b −Axt), δ(j)
t+ 1
2 := 1
κ

b −A

xt + 1
pj
∆(j)
t

.
Then, we have
max

∥δt∥∞,
δ(j)
t+ 1
2

∞

≤1
4.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
186
Proof. First, the bound on δt follows by ∥b −Axt∥∞≤2 and κ is suﬃciently large. Note that we
may also conclude a stronger bound, that ∥δt∥∞≤1
8. By triangle inequality, it suﬃces to show

1
κpj
A∆(j)
t

∞
≤1
8 for all j ∈[m].
Firstly, observe that ∆(j)
t
is 1-sparse, and can be bounded by noting (where med takes a median)
h
x(j)
t+ 1
2
i
j = med

−1, [xt]j −
1
κpj
 ϵ
2[xt]j + s · A⊤
:jyt

, 1

,
so that by deﬁnition of ∆(j)
t
= x(j)
t+ 1
2 −xt,
∆(j)
t

∞≤
ϵ
2κpj
|[xt]j| +
s
κpj
A⊤
:jyt
 .
Recall that
pj(yt) ≥
q
˜Lj(yt)
C√ns + √mnϵ ≥8
p
Lj(yt)
κ
.
(4.27)
Here, the ﬁrst inequality was from (H.12), and the second was from the deﬁnition of κ and (4.24).
We now bound the size of entries of
1
κpj A∆(j)
t , recalling ∥xt∥∞≤1:
1
κpj
A∆(j)
t

∞≤
ϵ
2κ2p2
j
∥A:j∥∞+
s
κ2p2
j
∥A:j∥∞
A⊤
:jyt

= 1
κ2
1
p2
j
 ϵ
2 ∥A:j∥∞+ s ∥A:j∥∞|A⊤
:jyt|

≤
1
64Lj(yt)
 ϵ
2 ∥A:j∥∞+ s ∥A:j∥∞|A⊤
:jyt|

≤1
64.
The last line follows from (4.27). This yields the claim, as Lj(yt) = s ∥A:j∥∞|A:j|⊤yt+ϵ ∥A:j∥∞.
Leveraging this, the following lemma shows multiplicative stability of the simplex variables within
a single iteration, which allows us to show that local smoothness estimates do not drift signiﬁcantly.
This proof is somewhat technical, and is deferred until the end of Section 4.5.3, as it requires opening
up our implementation, which will yield the fact that the simplex points are not too unstable.
Lemma 60. Coordinate-wise for any j, yt+ 1
2 , y(j)
t+1 multiplicatively approximate yt by a factor of at
most 8. That is (where division is coordinate-wise), max

yt+ 1
2 /yt, y(j)
t+1/yt

≤8.
We also require the following (standard) local norms bound on the divergence of entropy.
Lemma 61 (Local norms). Let y, y′ be on the simplex. Then for V the divergence with respect to
entropy, Vy(y′) ≥1
2 ∥y −y′∥2
diag(() max(y,y′))−1.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
187
Proof. Let yα = (1 −α)y + αy′. By a Taylor expansion, letting h be entropy, we have
Vy(y′) =
Z 1
0
Z β
0
(y′ −y)∇2h(yα)(y′ −y)dαdβ ≥1
2 ∥y −y′∥2
diag(() max(y,y′))−1 .
We now give a one-step convergence analysis of our algorithm, where use the deﬁnitions
gj(zt) :=
 1
pj

A⊤
:jyt + ϵ
2s[xt]j

ej, b −Axt +
ϵ
4 log n log yt

,
gj(w(j)
t ) :=
 1
pj

A⊤
:jyt+ 1
2 + ϵ
2s
h
x(j)
t+ 1
2
i
j

ej, b −A

xt + 1
pj
∆(j)
t

+
ϵ
4 log n log yt+ 1
2

.
Lemma 62. On any iteration t, we have (where expectations are over the randomness of the coor-
dinate j in the iteration)
E
 1
κ
D
gj(w(j)
t ), w(j)
t
−˜z
E
≤E
h
Vzt(˜z) −Vz(j)
t+1(˜z)
i
.
Proof. Applying the ﬁrst-order optimality conditions deﬁning the two steps, as well as (F.1) following
from the deﬁnition of Bregman divergences,
1
κ
D
gj(zt), w(j)
t
−z(j)
t+1
E
≤Vzt(z(j)
t+1) −Vw(j)
t (z(j)
t+1) −Vzt(w(j)
t )
1
κ
D
gj(w(j)
t ), z(j)
t+1 −˜z
E
≤Vzt(˜z) −Vz(j)
t+1(˜z) −Vzt(z(j)
t+1).
Summing and rearranging slightly, we have
1
κ
D
gj(w(j)
t ), w(j)
t
−˜z
E
≤Vzt(˜z) −Vz(j)
t+1(˜z)
+ 1
κ
D
gj(w(j)
t ) −gj(zt), w(j)
t
−z(j)
t+1
E
−Vw(j)
t (z(j)
t+1) −Vzt(w(j)
t ).
Taking an expectation, we have the conclusion up to proving the following claim, where we recall
κ = mϵ + 8√mnϵ + 8C√ns + 16n:
E
hD
gj(w(j)
t ) −gj(zt), w(j)
t
−z(j)
t+1
Ei
≤κE
h
Vw(j)
t (z(j)
t+1) + Vzt(w(j)
t )
i
.
We will instead show the stronger claim that this is true for any particular j ∈[m]:
D
gj(w(j)
t ) −gj(zt), w(j)
t
−z(j)
t+1
E
≤κ

Vw(j)
t (z(j)
t+1) + Vzt(w(j)
t )

.
(4.28)
We will roughly do so by splitting the left hand side into three pieces, and then bounding them

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
188
separately. First, we rewrite it as (recalling ∆(j)
t
= x(j)
t+ 1
2 −xt is 1-sparse)
D
gj(w(j)
t ) −gj(zt), w(j)
t
−z(j)
t+1
E
= 1
pj
D
A⊤
:j(yt+ 1
2 −yt)ej, x(j)
t+ 1
2 −x(j)
t+1
E
+
D
A⊤
:j(y(j)
t+1 −yt+ 1
2 )ej, x(j)
t+ 1
2 −xt
E
+
ϵ
2spj
h
x(j)
t+ 1
2
i
j −[xt]j

ej, x(j)
t+ 1
2 −x(j)
t+1

+
ϵ
4 log n

log
yt+ 1
2
yt
, yt+ 1
2 −y(j)
t+1

.
(4.29)
We bound the ﬁrst term. By Lemma 61, and as Lemma 60 gives coordinatewise y(j)
t+1, yt+ 1
2 ≤8yt,
Vw(j)
t (z(j)
t+1) + Vzt(w(j)
t ) ≥1
16
y(j)
t+1 −yt+ 1
2

2
diag(()y−1
t
) + 1
2s
x(j)
t+1 −x(j)
t+ 1
2

2
2
+ 1
16
yt+ 1
2 −yt

2
diag(()y−1
t
) + 1
2s
x(j)
t+ 1
2 −xt

2
2 .
We see that by a2 + b2 ≥2ab and Cauchy-Schwarz,
q
∥A:j∥∞|A:j|⊤yt
 1
16
y(j)
t+1 −yt+ 1
2

2
diag(()y−1
t
) + 1
2s
x(j)
t+ 1
2 −xt

2
2

≥
q
|A2
:j|⊤yt
 1
√
8s

h
x(j)
t+ 1
2 −xt
i
j

y(j)
t+1 −yt+ 1
2

diag(()y−1
t
)

=
 1
√
8s

h
x(j)
t+ 1
2 −xt
i
j


·
sX
i
A2
ij[yt]i
v
u
u
tX
i
[y(j)
t+1 −yt+ 1
2 ]2
i
[yt]i
≥
 1
√
8s

h
x(j)
t+ 1
2 −xt
i
j


·
X
i
|Aij||[y(j)
t+1 −yt+ 1
2 ]i|
≥
1
√
8s
D
A⊤
:j(y(j)
t+1 −yt+ 1
2 )ej, x(j)
t+ 1
2 −xt
E
.
Similarly, we have
q
∥A:j∥∞|A:j|⊤yt
 1
16
yt+ 1
2 −yt

2
diag(y−1
t ) + 1
2s
x(j)
t+1 −x(j)
t+ 1
2

2
2

≥
1
√
8s
D
A⊤
:j(yt+ 1
2 −yt)ej, x(j)
t+ 1
2 −x(j)
t+1
E
.
Therefore, by the three above equations,
q
∥A:j∥∞|A:j|⊤yt
√
8s
pj

Vw(j)
t (z(j)
t+1) + Vzt(w(j)
t )

≥1
pj
D
A⊤
:j(y(j)
t+1 −yt+ 1
2 )ej, x(j)
t+ 1
2 −xt
E
+
D
A⊤
:j(yt+ 1
2 −yt)ej, x(j)
t+ 1
2 −x(j)
t+1
E
.
(4.30)

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
189
Now, we consider the second term. Directly applying strong-convexity and Cauchy-Schwarz gives
ϵ
2spj
h
x(j)
t+ 1
2
i
j −[xt]j

ej, x(j)
t+ 1
2 −x(j)
t+1

≤
ϵ
2pj
 1
2s
x(j)
t+ 1
2 −xt

2
2 + 1
2s
x(j)
t+ 1
2 −x(j)
t+1

2
2

≤
ϵ
2pj

Vxt(x(j)
t+ 1
2 ) + Vx(j)
t+ 1
2
(x(j)
t+1)

.
(4.31)
Finally, we consider the third term. It is straightforward to note that for any convex r (in this case,
entropy), ⟨∇r(b) −∇r(a), b −c⟩≤Va(b) + Vb(c) for any three points a, b, c. Applying this,
ϵ
4 log n

log
yt+ 1
2
yt
, yt+ 1
2 −y(j)
t+1

≤
ϵ
4 log n

Vyt(yt+ 1
2 ) + Vyt+ 1
2 (y(j)
t+1)

.
(4.32)
Combining (4.30), (4.31), (4.32), we obtain
⟨gj(wt) −gj(zt), wt −zt+1⟩≤


q
∥A:j∥∞|A:j|⊤yt
√
8s
pj
+
ϵ
2pj



Vxt(x(j)
t+ 1
2 ) + Vx(j)
t+ 1
2
(x(j)
t+1)

+


q
∥A:j∥∞|A:j|⊤yt
√
8s
pj
+
ϵ
4 log n



Vyt(yt+ 1
2 ) + Vyt+ 1
2 (y(j)
t+1)

.
Finally, to prove (4.28), it remains to bound the size of the coeﬃcients of the divergences by κ, and
use nonnegativity of divergences. We claim the following holds:
q
∥A:j∥∞|A:j|⊤yt
√
8s
pj
+
ϵ
2pj
≤κ = 16n + 8√mnϵ + 8C√ns + mϵ.
To see this, recall we assumed pj ≥
1
2m, and further that
1
pj ≤C√ns+√mnϵ
√˜Lj(yt)
by (H.12). Therefore,
q
∥A:j∥∞|A:j|⊤yt
√
8s
pj
+
ϵ
2pj
≤
√
8(C√ns + √mnϵ) + mϵ ≤κ.
Similarly, it is easy to see that the following holds (corresponding to the coeﬃcient of the divergences
on the y side), concluding the proof:
q
∥A:j∥∞|A:j|⊤yt
√
8s
pj
+
ϵ
4 log n ≤κ.
We require the following helper lemma which upper bounds divergence via regret, which allows

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
190
us to ﬁnally convert our regret bound into a divergence bound for the output iterate.
Lemma 63. For any point z, we have ⟨g(z), z −˜z⟩≥
ϵ
4 log nVz(˜z), where we recall for z = (x, y),
g(z) =

A⊤y + ϵ
2sx, b −Ax +
ϵ
4 log n log y

.
Proof. Recall that because ˜z is the saddle point of the convex-concave function g is the gradient
operator of, by ﬁrst-order optimality,
⟨g(˜z), z −˜z⟩≥0 ∀z.
Therefore, noting terms

A⊤(y −˜y), x −˜x

−⟨A(x −˜x), y −˜y⟩cancel,
⟨g(z), z −˜z⟩≥⟨g(z) −g(˜z), z −˜z⟩= ϵ
2s ∥x −˜x∥2
2 +
ϵ
4 log n

log y
˜y , y −˜y

≥ϵVx(˜x) +
ϵ
4 log nVy(˜y) ≥
ϵ
4 log nVz(˜z).
The last line used nonnegativity of the Bregman divergence and ⟨∇r(y) −∇r(˜y), y −˜y⟩≥Vy(˜y).
We now give the method in phase k, initialized at (xk,0, yk,0). Here, we brieﬂy comment on
inexactness issues. The algorithm requires maintenance of variable y on the simplex, and various
quantities which are functions of y, which we can only approximately compute (cheaply): the method
outlined in Section 4.3.5 no longer applies, because updates to the variable are dense. Formally, we
deﬁne Y-Oracle, a data structure which maintains an internal representation of the simplex variables.
Y-Oracle supports the following operations in each iteration (k, t), in amoritized eO(1) time:
• Y-Oracle.Sample(): Samples j ∈[m] from pj(yk,t). Returns (j, pj(yk,t)).
• Y-Oracle.Coord(i): Returns [˘yk,t]i such that |[˘yk,t]i −[yk,t]i| < n−100.
• Y-Oracle.Update-Half(v): Updates the internal representation of yk,t+ 1
2 .
• Y-Oracle.Coord-Half(i): Returns [˘yk,t+ 1
2 ]i such that |[˘yk,t+ 1
2 ]i −[yk,t+ 1
2 ]i| < n−100.
• Y-Oracle.Update(v): Updates the internal representation of yk,t+1.
We develop Y-Oracle in Section 4.5.3. The following is the algorithm for phase k.
1. Let κ = mϵ + 8√mnϵ + 8C√ns + 16n where C is the constant of Lemma 57.
2. Let T =
l
8κ log n
ϵ
m
be the number of iterations per phase.
3. Sample a stopping iteration uniformly at random t∗
k ∈[T].

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
191
4. For iteration t ∈[t∗
k −1]:
(a) Call Y-Oracle.Sample to obtain j, pj(yk,t) (for shorthand, denoted pj).
(b) For each non-zero entry Aij of A:j, call Y-Oracle.Coord(i) to obtain [˘yk,t]i.
(c) xk,t+ 1
2 ←argminx∈[−1,1]m
nD
1
κpj (A⊤
:j ˘yk,t +
ϵ
2s[xk,t]j)ej, x
E
+ Vxk,t(x)
o
.
(d) yk,t+ 1
2 ←argminy∈∆n
nD
1
κ

b −Axk,t +
ϵ
4 log n log yk,t

, y
E
+ Vyk,t(y)
o
.
(e) ∆k,t := xk,t+ 1
2 −xk,t.
(f) For each non-zero entry Aij of A:j, call Y-Oracle.Coord(i) to obtain [˘yk,t+ 1
2 ]i.
(g) xk,t+1 ←argminx∈[−1,1]m
nD
1
κpj (A⊤
:j ˘yk,t+ 1
2 +
ϵ
2s[xk,t+ 1
2 ]j)ej, x
E
+ Vxk,t(x)
o
.
(h) yk,t+1 ←argminy∈∆n
nD
1
κ

b −A

xk,t + 1
pj ∆k,t

+
ϵ
4 log n log yk,t+ 1
2

, y
E
+ Vyk,t(y)
o
.
5. For iteration t = t∗
k:
(a) ∀j ∈[m], ∆(j)
k,t := argminx∈[−1,1]m
nD
1
κpj (A⊤
:jyk,t +
ϵ
2s[xk,t]j)ej, x
E
+ Vxk,t(x)
o
−xk,t.
(b) Compute ∆(j)
k,t for all j ∈[m].
(c) Deﬁne xk+1,0 = xk,t + P
j ∆(j)
k,t.
(d) Deﬁne yk+1,0 = argminy∈∆n
nD
1
κ

b −Axk,t +
ϵ
4 log n log yk,t

, y
E
+ Vyk,t(y)
o
.
6. Output (xk+1,0, yk+1,0).
We remark that in each loop of step 4, steps (c), (e) and (g) are implemented directly in ˜O(1)
time, step (d) is implemented implicitly using Y-Oracle.Update-Half, and step (h) is implemented
implicitly using Y-Oracle.Update. We will discuss the eﬃcient implementation of the procedures
supported by Y-Oracle in Section 4.5.3.
We now come to the main export of this section, which shows that the expected divergence to
the saddle point halves in every phase. In this lemma, we assume exact implementation of the steps;
we discuss how to deal with inexactness issues in the analysis in Section 4.5.3.
Lemma 64. Suppose phase k is initialized with zk,0 = (xk,0, yk,0).
Then, the output zk+1,0 =
(xk+1,0, yk+1,0) satisﬁes (where expectations are taken over the randomness used in phase k)
E

Vzk+1,0(˜x, ˜y)

≤1
2Vzk,0(˜x, ˜y).
Proof. Consider running for all of the T =
l
8κ log n
ϵ
m
iterations. Taking an expectation of Lemma 62
over the entire phase, telescoping, and using nonnegativity of divergences, we obtain (for ˜z = (˜x, ˜y))
E

1
T
X
t∈[T ]
⟨gjt(wk,t), wk,t −˜z⟩

≤κVzk,0(˜z)
T
≤
ϵ
8 log nVzk,0(˜z).

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
192
Here, jt is the coordinate sampled in the tth iteration. Applying Lemma 58, we instead have
E

1
T
X
t∈[T ]
⟨g( ¯wk,t), ¯wk,t −˜z⟩

≤
ϵ
8 log nVzk,0(˜z).
Now, because we randomly sampled a t ∈[T] to be the index t∗
k and passed ¯wt∗
k to the k + 1st phase
as zk+1,0 = (xk+1,0, yk+1,0), we obtain
E [⟨g(zk+1,0), zk+1,0 −˜z⟩] ≤
ϵ
8 log nVzk,0(˜z).
The conclusion follows from applying Lemma 63.
Algorithm statement
We now state the full algorithm, which is composed of phases, each of which halves the expected
divergence to the saddle point.
1. Initialize x0,0 = 0, y0,0 = 1
n1.
2. Let Θ0 = 1 + log n be the initial divergence bound (Lemma 56). Let K =

log2
  16sΘ0
ϵ2

.
3. For phase k ∈[K]:
(a) Run the procedure in Section 4.5.2, initialized at zk,0, to produce the point zk+1,0.
4. Return ˆx := xK,0.
We now analyze the correctness and runtime of this algorithm. We assume the following lemma,
which will be proven in Section 4.5.3.
Lemma 65. Every n iterations of each phase can be implemented in ˜O(n) time. Furthermore, for
each phase k, iteration t∗
k can be implemented in ˜O(m) time.
Theorem 25. The algorithm has runtime
˜O

m + n + √ns
ϵ

,
and satisﬁes E[∥ˆx −˜x∥2] ≤ϵ
2, where the expectation is over all randomness in the algorithm.
Proof. To prove the ﬁrst statement, note that the algorithm computes at most K points of the form
¯wk,t∗
k, and takes at most KT steps. Thus, by Lemma 65 this yields a runtime of
˜O (KT) + ˜O(mK) = ˜O
κ
ϵ + m

= ˜O
mϵ + √mnϵ + √ns + n
ϵ

= ˜O

m + n + √ns
ϵ

.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
193
We used that
p
mn/ϵ is never larger than m + n/ϵ, as it is their geometric mean. To prove the
second statement, we apply Lemma 64 for K ≥log
  16sΘ0
ϵ2

:
E [∥ˆx −˜x∥2]2 ≤E
h
∥ˆx −˜x∥2
2
i
= 4sE[Vˆx(˜x)] ≤4sΘ0
2K
≤
 ϵ
2
2
.
The ﬁrst inequality used convexity of the square, and the second inequality repeatedly used Lemma 64
and iterated expectations. This implies the desired bound.
We then see that ˆx is our desired approximate minimizer, in expectation.
Corollary 14. We have ∥ˆx∥∞≤1, and E[∥Aˆx −b∥∞] ≤OPT + ϵ.
Proof. The ﬁrst statement is immediate from the algorithm, since in each iteration xk,t lies in
[−1, 1]m, and for each j, xk,t + ∆(j)
k,t is also deﬁned to lie in [−1, 1]m, and the region decomposes
coordinatewise. The second statement follows from ∥A∥∞≤1, and
E[∥Aˆx −b∥∞] ≤∥A˜x −b∥∞+ E[∥A(ˆx −˜x)∥∞] ≤OPT + ϵ
2 + E[∥ˆx −˜x∥2] ≤OPT + ϵ.
By Markov's inequality, this means that with half probability we have a 2ϵ-approximate mini-
mizer. This can be boosted to probability 1 −δ using log 1
δ independent runs, and it does not aﬀect
runtime asymptotically since computing objective value takes time ˜O(m).
4.5.3
Runtime
This section proves Lemma 65, which states that we can implement each iteration of each phase in
amoritized ˜O(1) time for each t ̸= t∗
k, and that we can implement the last iteration in ˜O(m) time. As
discussed in Section 4.5.2, it suﬃces to show that Y-Oracle. {Sample, Coord, Update, Update-Half}
may be implemented in amoritized time ˜O(n) every n iterations.
In particular, assuming these
operations are supported, it is simple to see that we can implement the updates to the x variables
in ˜O(1) time per iteration by sparsity. Finally, the last iteration can be implemented in ˜O(m) time
simply by performing the updates to the x variable m times.
Reducing sampling from and computing pj to sampling from and computing √y
We ﬁrst reduce the implementation of Y-Oracle.Sample in an iteration (k, t) to being able to eﬃ-
ciently sample proportional to
p
[yk,t]i (we drop (k, t) for simplicity). Recall we sample from
pj =
C√ns
C√ns + √mnϵ ·
p
s ∥A:j∥∞
P
i
p
|Aij|yi
P
j
p
s ∥A:j∥∞
P
i
p
|Aij|yi
+
√mnϵ
C√ns + √mnϵ ·
p
ϵ ∥A:j∥∞
P
j
p
ϵ ∥A:j∥∞
.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
194
First ﬂip a coin which is heads with probability C√ns/(C√ns + √mnϵ). If it comes up tails,
we sample a j proportional to
p
ϵ ∥A:j∥∞; clearly we may precompute all of these probabilities, and
place them at the leaves of a binary tree (along with each of the subtree sums stored at roots of
subtrees), ﬂipping ˜O(1) appropriately biased coins to sample from this distribution. Next, in order
to sample from a distribution over j proportional to
p
s ∥A:j∥∞
P
i
p
|Aij|yi, it clearly suﬃces to
instead sample an i proportional to √yi, and then sample a j proportional to
p
s ∥A:j∥∞|Aij|; this
latter distribution we can precompute.
We now discuss computing a particular pj in ˜O(1) time: we need to in fact compute the true
pj which we sampled from, because otherwise we will not have an unbiased estimator. To do so, it
clearly suﬃces to compute the conditional probabilities
p
s ∥A:j∥∞
P
i
p
|Aij|yi
P
j
p
s ∥A:j∥∞
P
i
p
|Aij|yi
,
p
ϵ ∥A:j∥∞
P
j
p
ϵ ∥A:j∥∞
.
The latter of these is simple to pre-compute. To compute the former, let
qij :=
p
s ∥A:j∥∞|Aij|
P
i
p
s ∥A:j∥∞|Aij|.
We observe that for any j, at most ˜O(1) of the qij are non-zero, and we can also precompute all the
qij. Finally, the conclusion follows from
p
s ∥A:j∥∞
P
i
p
|Aij|yi
P
j
p
s ∥A:j∥∞
P
i
p
|Aij|yi
=
X
i
√yi
P
i
√yi
· qij.
Now, we only need to evaluate ˜O(1) nonzero summands. In conclusion, in order to sample from
and compute pj in time ˜O(1) per iteration, it suﬃces to sample from and compute a probability
distribution proportional to √y (we remark that our sampling procedure will be exact).
Sparse combinations
In this section we describe how to maintain vt, vt+ 1
2 which are log yt, log yt+ 1
2 up to an additive
multiple of the ones vector, via a linear combination of sparsely updated vectors qt, rt, st. The reason
for this representation is so that we may update the representation in time ˜O(1) per iteration, and
further, for any coordinate i, we may compute exp([vt]i) in constant time by simply taking the
appropriate linear combination of the vectors. The word "sparse" in this section denotes any vector
with ˜O(1) nonzero entries. We begin by recalling the notation from Lemma 59,
δt := 1
κ(b −Axt), δ(j)
t+ 1
2 := 1
κ

b −A

xt + 1
pj
∆(j)
t

.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
195
Now we write the updates to yt+ 1
2 , y(j)
t+1 in the following form, for c =
ϵ
4κ log n:
yt+ 1
2 ∝exp (log yt −c log yt −δt) , y(j)
t+1 ∝exp

log yt −c log yt+ 1
2 −δ(j)
t+ 1
2

.
Letting vectors vt, vt+ 1
2 satisfy yt ∝exp(vt), yt+ 1
2 ∝exp(vt+ 1
2 ) for all t, we have the recursion
vt+ 1
2 = (1 −c)vt −δt, vt+1 = vt −cvt+ 1
2 −δ(j)
t+ 1
2 .
Recalling the structure of these updates, we see that we can further decompose δ(j)
t+ 1
2 into δt + ζt,
where ζt = −1
κpj A∆(j)
t
is sparse. Next, observing ∥b −Axt∥∞≤2 and κ ≥16n, we can assume
∥δt∥∞≤
1
8n. We additionally note that δt −δt−1 = −1
κA(xt −xt−1) is sparse, since xt −xt−1 is
1-sparse and A has sparse columns. Altogether, this yields
vt+1 = (1 −c + c2)vt −(1 −c)δt −ζt
⇒vt+1 −vt = (1 −c + c2)(vt −vt−1) −(1 −c)(δt −δt−1) −(ζt −ζt−1)
⇒vt+1 = (2 −c + c2)vt −(1 −c + c2)vt−1 −(1 −c)(δt −δt−1) −(ζt −ζt−1)
⇒vt+1 = c1vt −c2vt−1 −c3µt −νt.
Here, we have deﬁned c1 = 2 −c + c2, c2 = 1 −c + c2, c3 = 1 −c, µt = δt −δt−1, νt = ζt −ζt−1.
Further, c1 ≤2 and c2 ≤1. Similarly, we can compute
vt+ 1
2 = (1 −c)vt −δt ⇒vt+ 1
2 −vt−1
2 = (1 −c)(vt −vt−1) −(δt −δt−1)
⇒vt+ 1
2 = c3vt −c3vt−1 + vt−1
2 −µt.
In matrix-vector multiplication notation, this update is (where M is clearly full rank)

vt+1
vt+ 1
2
vt

=

vt
vt−1
2
vt−1

M −

c3µt + νt
µt
0

,
M =





c1
c3
1
0
1
0
−c2
−c3
0




.
Now, suppose we have maintained a representation

vt
vt−1
2
vt−1

=

qt
rt
st

M t.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
196
We then require the update

qt+1
rt+1
st+1

M t+1 =

qt
rt
st

M t+1 −(

c3µt + νt
µt
0

M −t−1)M t+1.
We can maintain M −t−1 in closed form by simply performing a single matrix multiplication of 3 × 3
matrices each iteration, so the updates to qt+1, rt+1 and st+1 are sparse:

qt+1
rt+1
st+1

=

qt
rt
st

−

c3µt + νt
µt
0

M −t−1.
Maintaining the sum of exponentials
The previous section states that we can maintain a representation of vt in ˜O(1) time per iteration,
such that we can query for any i, the value exp([vt]i) in constant time (respectively, exp([vt+ 1
2 ]i)).
Consequently, in order to support Y-Oracle.Coord (respectively, Y-Oracle.Coord-Half), we need
to be able to approximate
X
i∈[n]
exp([vt]i)
(4.33)
multiplicatively by 1 +
1
n100 . In this section we will discuss how to do so over n iterations in time
˜O(n). We then discuss how to sample from this distribution, and modify this maintenance to also
support approximate coordinate queries from yt+ 1
2 . We will not formally discuss how to extend this
analysis to query and sample from a distribution proportional to √yt, as required by 4.5.3, as it is an
immediate generalization; we simply also implement Y-Oracle with the vectors 1
2vt, which clearly
suﬃces. For the scope of this section, deﬁne the constant
c =
ϵ
4κ log n, κ > 16n.
The implementation problem is: for every iteration t ∈[n], we are given vectors δt, ζt, such that
• ∥ζt∥∞≤1
8, and ζt is sparse.
• ∥δt∥∞≤
1
8n.
• Vectors vt are deﬁned recursively via vt+1 := (1 −c)vt + δt + ζt.
• We are able to maintain a representation of vt as a linear combination αtqt + βtrt + γtst, for
sparsely changing qt, rt, st, and scalars αt, βt, γt.
These bounds follow from the analysis in Lemma 59. We also require the following fact on the eﬀect
of a certain "squishing" operation, which states that we may take any coordinate of vt which is
signiﬁcantly smaller than another, and raise it within a certain range.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
197
Lemma 66. Let v ∈Rn, and let y ∈∆n be such that y ∝exp(v). Consider the following operation:
let i∗, i′ be coordinates of v such that vi′ < vi∗−16 log n
ϵ
, and set ˆv = v in every coordinate, except
ˆvi′ ←vi∗−16 log n
ϵ
. Then, for ˆy ∝exp(ˆv), assuming ϵ < 1
7,
Vy(˜y) −Vˆy(˜y) > −n−100.
Proof. We explicitly compute
Vy(˜y) −Vˆy(˜y) =
X
i
˜yi log ˆyi
yi
.
Note that the only possible i such that ˆyi
yi ≥1 is i = i′. Furthermore, for every other coordinate i,
yi
ˆyi
=
exp(vi)
∥exp(v)∥1
exp(˜vi)
∥exp(˜v)∥1
= ∥exp(˜v)∥1
∥exp(v)∥1
< 1 + n−16
ϵ
1
.
Here, we used that exp(ˆvi′) can be at most exp(−16 log n
ϵ
) of the sum, due to the contribution of the
vi∗term, and all coordinates i ̸= i′ have ˆvi = vi. Finally,
X
i
˜yi log ˆyi
yi
≥−
X
i̸=i′
˜yi log(1 + n−100) ≥−n−100.
We assume that in the ﬁrst iteration, we have spent O(n) time computing v0 explicitly, using
our sparse representation, and squishing so its coordinates lie in the range [0, 16 log n
ϵ
].
The case ζt = 0.
We ﬁrst handle the case when all of the ζt = 0. At iteration 0, suppose we have spent O(n)
time to compute i∗= argmaxi[v0]i. Also, recall we guaranteed [v0]i∗−[v0]i ≤16 log n
ϵ
. We use the
following fact:
Fact 7 (Taylor expansion of exponential). Let |x| ≤1
2. Then, letting Tayd(x) be the degree d Taylor
approximation of the exponential, we can bound |Tayd(x) −exp(x)| ≤
1
2d .
To approximate (4.33) on iteration t, we will maintain a scalar σt with the guarantee
∥v0 −σt1 −vt∥∞≤1
2.
(4.34)
We will explicitly compute [vt]i∗each iteration t, and set
σt+1 = σt + c[vt]i∗.
(4.35)
First of all, we show the invariant (4.34).

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
198
Lemma 67. Every iteration t ≤n, and for all i, |[vt]i −[vt]i∗| ≤17 log n
ϵ
.
Proof. We claim that the range of the coordinates of vt is never larger than 17 log n
ϵ
: certainly, this
implies the conclusion. To show this, we inductively claim that the range satisﬁes
max
i [vt]i −min
j [vt]j ≤16 log n
ϵ
+ t
4n.
Taking t ≤n yields the result. Clearly for t = 0 this is true; now, for t+1, recall vt+1 = (1−c)vt+δt.
Let i = argmaxi[vt]i, j = argminj[vt]j. Then,
[vt+1]i −[vt+1]j = (1 −c)([vt]i −[vt]j) + ([δt]i −[δt]j) ≤16 log n
ϵ
+ t
4n + 1
4n.
Here we used the inductive guarantee and the range of δt (we may clearly assume log n/ϵ > 1).
Lemma 68. Every iteration t ≤n, (4.34) holds.
Proof. For some particular i, we show it holds; this implies the ℓ∞guarantee. Note that
|[v0]i −σt+1 −[vt+1]i| ≤|[v0]i −σt −[vt]i| + |([vt]i −[vt+1]i) −c[vt]i∗|.
Here we used triangle inequality and the deﬁnitions of σ, ¯v. Now, we have
|([vt]i −[vt+1]i) −c[vt]i∗| ≤|c[vt]i −c[vt]i∗| + |[δt]i| ≤3
8n + 1
8n ≤1
2n.
Thus, inductively we have that
|[v0]i −σt −[vt]i| ≤t
2n.
Using t ≤n yields the result.
Finally, we describe how to compute an accurate approximation (4.33) by Taylor expansion in
˜O(1) time per iteration. We approximate, for some d = O(log n),
X
i∈[n]
exp([vt]i) =
X
i∈[n]
exp ([v0]i −σt) exp ([vt]i −([v0]i −σt))
≈exp(−σt)
X
i∈[n]
exp ([v0]i) Tayd (αt[qt]i + βt[rt]i + γt[st]i −([v0]i −σt)) .
(4.36)
We now group by the degree of the Taylor expansion, 0 ≤k ≤d, and each quintuple 0 ≤d1 + d2 +

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
199
d3 + d4 + d5 = k ≤d:
exp(−σt)
X
i∈[n]
exp([¯vt]i)
X
0≤k≤d
(αt[qt]i + βt[rt]i + γt[st]i −([v0]i −σt))k
k!
= exp(−σt)
X
i∈[n]
exp([v0]i)
X
d1,d2,d3,d4,d5
 k
d1,d2,d3,d4,d5

k!
(αt)d1(βt)d2(γt)d3(−1)d4[qt]d1
i [rt]d2
i [st]d3
i [v0]d4
i [σt]d5
= exp(−σt)
X
d1,d2,d3,d4,d5
 k
d1,d2,d3,d4,d5

k!
(αt)d1(βt)d2(γt)d3(−1)d4 X
i∈[n]
exp([v0]i)[qt]d1
i [rt]d2
i [st]d3
i [v0]d4
i [σt]d5.
Consider the complexity of computing the last expression. There are at most (d + 1)5 = O(d5)
quintuplets d1, d2, d3, d4, d5 with 0 ≤d1 + d2 + d3 + d4 + d5 ≤d. For each quintuplet, we maintain
X
i∈[n]
exp([v0]i)[qt]d1
i [rt]d2
i [st]d3
i [v0]d4
i [σt]d5.
Because each of qt, rt, st, are sparsely changing, we can spend ˜O(d5) time updating the relevant
terms in each of these summations. Furthermore, [σt]d5 is simply a scalar so we can rescale its
contribution to the entire sum in constant time. Now, in order to compute the overall sum, we can
spend constant time computing each coeﬃcient
 k
d1,d2,d3,d4,d5

k!
(αt)d1(βt)d2(γt)d3(−1)d4;
this takes ˜O(d5) time altogether. Lastly, updating exp(−σt), the scaling of the entire sum, takes
constant time, and computing the overall sum thus takes ˜O(d5).
Finally, we must argue that performing this procedure for d = O(log n) suﬃces for a multiplicative
guarantee of 1 +
1
nO(1) . Comparing the approximation in (4.36) to the required (4.33), the only
diﬀerence is each of the approximations
exp([vt]i −([v0]i −σt)) ≈Tayd([vt]i −([v0]i −σt)).
Because the left hand side is bounded between exp(± 1
2), an additive approximation is (up to con-
stants) a multiplicative approximation as well. Further, Fact 7 implies that d = O(log n) suﬃces for
this quality of approximation, as desired.
Binomial heap data structures for ζt.
In this section, we reduce the general case to the case where ζt = 0 via a binomial heap data
structure, a fairly general reduction. We note that the analysis in the previous section also clearly
holds when the number of iterations is less than n, and when there are less than n coordinates. The
main idea of the reduction is that we will maintain data structures for sets {Sk} for 0 ≤k ≤⌈log n⌉,

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
200
such that a Sk either contains no elements, or between 2k−1 + 1 and 2k elements. In particular, we
maintain on every iteration
• A hashmap which, for each i ∈[n], tracks which Sk it belongs to.
• For each Sk,
- The cardinality of Sk.
- P
i∈[n] exp([v0]i)[qt]d1
i [rt]d2
i [st]d3
i [v0]d4
i [σt]d5, for each quintuplet 0 ≤d1+d2+d3+d4+d5 ≤
d.
The main diﬃculty is maintaining the invariant that there is at most one set of each rank (we call k
the "rank" of a nonempty Sk). To this end, if there are two sets Sk, S′
k both with cardinality between
2k−1 + 1 and 2k elements, e.g. of rank k, we allow the operation Merge(Sk, S′
k) which creates a new
Sk+1 of rank k + 1, containing all of the coordinates associated with either Sk or S′
k.
Whenever we perform a merge, we explicitly compute all coordinates involved in the merge,
designate the largest as i∗for the updates to σt for that particular set, and squish if necessary
to guarantee that the range of the set is at most 16 log n
ϵ
; clearly, given our sparse representation
qt, rt, st, αt, βt, γt, we can appropriately modify a coordinate of say qt to handle the squishing. We
also instantiate all relevant quintuplet sums for our particular set. Furthermore, if |S′
k|+|Sk| ≤2k+1,
Merge will also spend ˜O(2k+1 −|S′
k|−|Sk|) time to create "initialization credits", so that the sum of
the initialization credits and the size of Sk+1 is always exactly 2k+1; these credits will be useful for
our amoritized analysis. It takes time ˜O(2k+1) to update the hashmap, reinstantiate all the relevant
quintuplet sums, and create credits, for Sk+1. We note we may need to recursively call Merge if
there was already a set of rank k + 1.
At the start of the n iterations, we initialize a single set of rank ⌈log n⌉, and put all of the
coordinates in this set (and pay any additional cost required for initialization credits), in time ˜O(n).
Each iteration t + 1 will proceed in three stages. In the ﬁrst stage, we compute the approximation
(4.36) to the sum of exponentials as in the previous section, ignoring the eﬀect of ζt. The complexity
of this stage is at most O(log n) times its complexity in the previous section, because we may need
to perform updates for each Sk; thus, it can be implemented in amoritized time ˜O(1).
In the second stage, for each coordinate in the support of ζt, we delete it from its corresponding
Sk and instantiate a new set of rank 0, now explicitly factoring in the eﬀect of ζt. Furthermore, if
this causes its corresponding Sk to become rank k −1, e.g. if before the deletion Sk had 2k−1 + 1
elements, and there was already a set of rank k −1, we will call the Merge operation on the two sets
of rank k −1. The amoritized cost of the second stage is ˜O(1). To see this, every time we create a
new set of rank 0, we spend ˜O(1) time to both initialize the rank 0 set, and pay for ˜O(1) "deletion
credits". Now, whenever we must use the Merge operation to create a new set of rank k, we can
pay for the operation (which costs ˜O(2k), both to merge and pay for new initialization credits) by

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
201
using existing credits: between when Sk was initialized and when it needed to be reinitialized due
to becoming rank k −1, the sum of its initialization credits and the deletion credits created by
removing elements is at least ˜O(2k). We call any such merges Type-1 merges.
In the third stage, we recursively call Merge, starting from the rank 0 sets, in order to maintain
the invariant that there is at most one set of any given rank. We call any such merges Type-2
merges. We claim the amoritized cost of all Type-2 merges over all n iterations is ˜O(n). Consider
the number of times a rank k set can be created through Type-2 merges: we claim it is upper
bounded by ˜O
  n
2k

. If this is true, overall the complexity of the third stage is at most
˜O


⌈log n⌉
X
k=0
2k n
2k

= ˜O(n).
The number of deletions due to the ζt throughout n iterations is at most ˜O(n). Thus, it suﬃces to
prove that between creations of rank k sets due to Type-2 merges, there must have been at least
2k−1 deletions. To see this, for each rank k, maintain a potential Φk for the sum of the cardinalities
of all rank l sets for l < k. Each deletion increases Φk by at most 1. Each Type-1 merge does not
increase Φk, because it can only cause coordinates to belong to sets which increase in rank. In order
for a Type-2 merge to be used to create a rank k set, Φk must have been at least 2k−1 + 2; after the
merge, it is 0, because in its creation, all rank l sets for l < k must have been merged. Thus, for
the potential to become large enough to require a merge again, there must have been at least 2k−1
deletions, as desired.
Finally, we remark that the analysis of each constituent data structure, i.e. the case when ζt = 0
for the supported coordinates, remains correct under deletions. In particular, (4.35) may still use
the original value of [vt]i∗in its recursion, even if the coordinate i∗is deleted; it is easy to see that
by the original boundedness of the range of supported coordinates, the analysis still holds.
Maintaining yt+ 1
2 .
In order to compute coordinates of yt+ 1
2 , we discuss approximating the sum
X
i∈[n]
exp
h
vt+ 1
2
i
i

.
It is easy to see that because vt+ 1
2 and (1 −c)vt never vary by more than a small additive constant
1
8n, and furthermore we also maintain a sparsely updated representation of vt+ 1
2 in terms of qt, rt, st,
we may suitably modify the approximation (4.36) to approximate this sum. In particular, we may
compute an appropriate scaling σt+ 1
2 by estimating all coordinates of cvt by scaling some particular
coordinate, and estimate the coeﬃcients of the quintuplet sums in terms of the coeﬃcients in the
linear combination. The complexity of this computation in each step is at most O(d5) in each step,
which never asymptotically dominates.

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
202
Sampling from the sum of exponentials.
Here, we discuss how to sample from the sum of the exponentials. We use the following fact
about rejection sampling.
Fact 8 (Rejection sampling). Suppose P and Q are probability distributions over [n], and P [i]
Q[i] ∈[ 1
2, 2]
for all i ∈[n]. Further, suppose we may sample from P in time ˜O(1). The following strategy samples
exactly from Q in expected O(1) time: sample a coordinate of i according to P, and accept with
probability
Q[i]
2P [i]; repeat until acceptance.
In our setting, in each iteration P is the distribution over coordinates i ∈Sk proportional to
exp([v0]i −σt), where we overload the deﬁnitions of v0, σt to refer to the point the set Sk uses to
approximate the Taylor expansion. We can sample from this distribution P by maintaining for each
of the ˜O(1) sets Sk, P
i∈Sk exp([v0]i −σt), by initializing it with the sum when σt = 0, and then
appropriately scaling the sums each iteration. Further, we may initialize each set Sk with a binary
tree data structure for sampling proportional to [v0]i for each i ∈Sk, because the uniform scaling
exp(−σt) does not aﬀect this distribution. In conclusion, we sample from P by ﬁrst sampling a set
Sk proportional to its weight given by P, and then sampling a coordinate in the set appropriately.
We then rejection sample from P with respect to Q, the true distribution. By the invariant
(4.34), this rejection sampling scheme meets the requirements to succeed in expected time ˜O(1),
which yields the conclusion.
Cleaning up: eﬀects of approximate sums and squishing
We ﬁrst prove Lemma 60, using additional structure aﬀorded by our data structure implementation.
Proof of Lemma 60. Recall the notation and bounds from Lemma 59,
δt := 1
κ(b −Axt), δ(j)
t+ 1
2 := 1
κ

b −A

xt + 1
pj
∆(j)
t

,
∥δt∥∞,
δ(j)
t+ 1
2

∞≤1
4.
We write the updates to yt+ 1
2 , y(j)
t+1 in the form, for c =
ϵ
4κ log n:
yt+ 1
2 ∝exp (log yt −c log yt −δt) , y(j)
t+1 ∝exp

log yt −c log yt+ 1
2 −δ(j)
t+ 1
2

.
Letting vectors vt, vt+ 1
2 satisfy yt ∝exp(vt), yt+ 1
2 ∝exp(vt+ 1
2 ) for all t, the goal of this lemma is to
show that ∥vt+1 −vt∥∞,
vt+ 1
2 −vt

∞are both bounded by 1. Indeed, we have the recursion
vt+ 1
2 = (1 −c)vt −δt, vt+1 = vt −cvt+ 1
2 −δ(j)
t+ 1
2 .

CHAPTER 4. FASTER APPROXIMATE ℓ∞REGRESSION AND MAXIMUM FLOW
203
Based on the bounds on δt and δ(j)
t+ 1
2 , it suﬃces to show that ∥cvt∥∞,
cvt+ 1
2

∞≤
3
4.
By the
squishing operations performed by the data structure, at the beginning of n iterations (when the
data structure is restarted), the range of vt is contained in [0, 16 log n/ϵ].
Over the course of n iterations, this fact is preserved for each particular data structure supporting
a set of coordinates. Moreover, we recall that we used squishing whenever we initialize a new data
structure in the binomial heap to maintain the fact that the additive range over all coordinates is
O(log n/ϵ). The ﬁnal issue which may come up is the additive drift caused by the vectors δt or
δ(j)
t+ 1
2 ; however, over the course of n iterations, this can only shift the largest coordinate of vt by n/4.
Altogether, it is clear we may assume ∥vt∥∞< n
4 + 33 log n
ϵ
≪
3
4c; the conclusion follows. Similarly,
we inductively have
vt+ 1
2

∞≪
3
4c by bounding its diﬀerence to vt.
We now consider the eﬀect of only approximately maintaining the sums of exponentials in our
algorithm, and applying squishing. In particular, the inequality in Lemma 62 only holds up to an
additive constant. The additive error comes into play in two ways: the ﬁrst-order optimality condi-
tion only holds up to the discrepancy between yt and ˘yt, and each time we apply squishing aﬀects
the value of E

Vzt+1(˜z)

. Regarding the former, all problem parameters and the number of phases
of our algorithm are all bounded by a small polynomial in n, so the guarantees of Y-Oracle.Coord
mean that the cumulative error does not amount to more than n−90 ≪ϵ (we assume ϵ > n−3, else
an interior point method achieves our stated runtime). Similarly, regarding the latter, Lemma 255
implies that even if we squish O(n) coordinates each iteration, the cumulative error in the Bregman
divergence does not amount to more than n−90.

Chapter 5
Semi-Streaming Combinatorial
Optimization
This chapter is based on [293, 39], with Sepehr Assadi, Arun Jambulapati, Yujia Jin, and Aaron
Sidford.
5.1
Introduction
We study the fundamental problem of designing semi-streaming algorithms for maximum cardinality
matching in bipartite graphs (MCM). We consider the insertion-only problem where an unknown
n-vertex m-edge undirected bipartite graph is presented as a stream of edge insertions. Our goal
is to compute an ϵ-approximate MCM (a matching with value ≥1 −ϵ times the optimum) in the
semi-streaming model [221], i.e. using eO(n) space,1 and as few passes as possible.
Due to its canonical and prevalent nature, MCM is well-studied in the graph streaming literature.
In a single pass, a simple greedy algorithm achieves a 1
2-approximation in O(n) space (cf. Lemma 71)
and obtaining better than a (
1
1+ln 2) ≈0.59 approximation requires n1+Ω(1/ log log (n)) space [309] (see
also [240, 308]). Correspondingly, there is a line of research on designing multi-pass algorithms for
ϵ-approximating MCM. In this chapter, we focus on eO(poly(ϵ−1))-pass semi-streaming algorithms.
Here, the state-of-the-art includes the O(ϵ−2)-pass O(n)-space algorithm of [42] (see also [12]) and
the O(ϵ−1 log n)-pass eO(n · poly(ϵ−1))-space algorithm of [13].2
For diﬀerent ranges of ϵ, these semi-streaming MCM algorithms provide non-trivial trade-oﬀs
1Throughout, we measure space in machine words of size Θ(log n) and use e
O to hide factors polylogarithmic in n,
ϵ−1, and Cmax, the largest edge weight of the relevant problem. In all our applications, without loss of generality,
ϵ−1 and Cmax are O(poly(n)). Consequently, in our applications an e
O(n) space algorithm is a semi-streaming, i.e.
O(n · poly(log(n)), algorithm for our applications.
2Streaming MCM has been studied under several other variants and parameter settings; see Section 5.1.4 for
details.
204

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
205
between space and pass complexity. While [13] provides an improved eO(ϵ−1) passes compared to
O(ϵ−2) in [42], the space complexity of [13] is larger by poly(ϵ−1) factors3 and therefore only adheres
to the semi-streaming restriction of using eO(n) space when ϵ ≥(polylog (n))−1. The central question
we address in this paper is whether this trade-oﬀis necessary:
Is it possible to obtain an eO(ϵ−1)-pass semi-streaming algorithm for ϵ-approximating
maximum bipartite matchings for the entire range of ϵ > 0?
In this paper we answer this question in the aﬃrmative. We provide multiple ways to lever-
age recent advances in continuous and combinatorial optimization for this goal. For example, in
Appendix D.5 we show how to obtain this result by a careful application of recent results on the
box-constrained Newton method [145] and streaming sparsiﬁcation [388].
Our main result is a simple algorithm that further improves the space complexity. Our starting
point is to demonstrate that the advances in area convexity [483] (see also Section 2.5), which
achieved a state-of-the-art algorithm for approximate maximum ﬂow, are applicable to match-
ing and transportation problems as well through similar "soft Lagrangian" formulations. By di-
rectly applying these tools, we develop a direct, eﬃcient parallel solver for optimal transport in
Appendix D.1.
We further leverage these new optimization methods to provide an O(n)-space
and O(ϵ−1 · log (ϵ−1) · log n)-pass algorithm for semi-streaming bipartite matching.
This space-
dependency is optimal, as Ω(n) space is needed to simply output the ﬁnal matching.
Further,
this algorithm is deterministic, in contrast to the previous eO(ϵ−1)-pass algorithm of [13], and the
method's runtime is eO(m · ϵ−1).
To obtain this result we introduce a more general algorithmic framework of independent interest.
We design semi-streaming algorithms which can approximately determine the value of and produce
low-space implicit fractional solutions to linear programs, given in the form of box-simplex games,
when rows of the constraint matrix are presented in a stream. We obtain our MCM results by
applying this optimization method to a linear programming representation of the problem and show
that this implicit solution can be converted into a sparse solution in low space.
We believe our results demonstrate the power of recent optimization advances for solving stream-
ing problems. Beyond resolving the space complexity of eO(ϵ−1)-pass algorithms, we show that our
techniques extend to yield the following additional results.
• Exact MCM in o(n) passes for all densities. Recent work [368] asked whether the O(n log n)-
pass semi-streaming algorithm for exact MCMs following from a careful implementation of the
classical Hopcroft-Karp algorithm [274] is improvable. The authors designed an eO(n)-space
algorithm based on interior-point methods using eO(√m) passes, which is o(n) passes except
for in dense graphs. By combining our approximate MCM method with recent advances in
3We remark that the introduction and main theorem statements of [13] are written for constant ϵ. However, for
subconstant ϵ it is straightforward to see their space requirement incurs a poly(ϵ−1) dependence.

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
206
streaming reachability [290], we obtain an O(n
3
4 +o(1))-pass, eO(n)-space algorithm for exact
MCM, thereby answering the main open problem of [368] for all densities. Interestingly, this
improvement is a direct byproduct of resolving our motivating question as the space-pass
trade-oﬀs in [13] and [42] prevent either method from obtaining nontrivial semi-streaming
exact MCM algorithms even when combined with [290] (see Remark 7).
• Optimal transport. Closely-related to computing MCMs, the discrete optimal transport prob-
lem has received widespread recent interest due to applications in machine learning [151, 25].
Several recent works have designed diﬀerent optimization algorithms (inﬂuenced by develop-
ments for matching) [450, 82] solving the problem on a support size of n using eO(n2ϵ−1) total
work; we give a presentation of a new, direct method attaining this rate in Appendix D.1.
In this chapter, we further provide the ﬁrst semi-streaming algorithm in this setting within
eO(ϵ−1) passes, O(n) space, and comparable work.
• Transshipment and shortest path. Yet another application of our method is to solving the
transshipment problem, a type of uncapacitated minimum cost ﬂow problem, on undirected
graphs. Several recent works have focused on this problem in streaming and parallel models [67,
358, 29] and used this to obtain approximate shortest path algorithms. We provide a semi-
streaming algorithm that achieves an ϵ-approximation to transshipment within eO(ϵ−1) passes,
eO(n) space, and eO(mϵ−1) work. As a direct corollary of this result, we obtain ϵ-approximate
semi-streaming algorithms for the s-t shortest path problem with same complexities.
Our
results on transshipment and shortest path improve upon the previous best semi-streaming
algorithms of [67] by eO(ϵ−1) factors in passes and work.
5.1.1
Problem setup
The basic (and motivating) problem we consider in this work is that of computing an approximate
MCM in bipartite graph, G = (V, E), given as an insertion-only stream deﬁned as follows.
Deﬁnition 17 (Semi-streaming graph model). In the semi-streaming graph model, a graph G =
(V, E) with vertex set V = [n] is presented to the algorithm as an arbitrarily ordered stream of edges
(u, v) for u, v ∈V (the tuple contains the weight for weighted graphs). The algorithm can read this
stream in sequential passes and is constrained to use eO(n) space.
We typically let n = |V | and m = |E|. M ∗denotes the the size of the MCM in G, and L, R ⊆V
with L ∩R = ∅and L ∪R = V denotes the vertex bipartition. A key goal of this paper is to
eﬃciently compute approximate MCMs in the semi-streaming graph model. Formally, we refer to
any ﬂow x ∈[0, 1]E such that the total ﬂow adjacent to any v ∈V is ≤1 as a fractional matching
and call the matching integral if x ∈{0, 1}E; we say such a matching is a ϵ-approximate MCM if
∥x∥1 ≥(1 −ϵ)M ∗. The outputs of our algorithms typically include both the approximate problem

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
207
value and an approximate solution. For MCM problems with value M ∗, the algorithm returns this
value within a (1 −ϵ) factor and an integral matching that achieves this approximate value.
To obtain our main result, we develop streaming algorithms for a more general set of problems.
These problems are linear programming in the form of box-simplex games, i.e. bilinear minimax
games between a box, or ℓ∞-constrained player, and simplex, or nonnegative ℓ1-constrained player:
min
x∈∆m
max
y∈[−1,1]n y⊤A⊤x + c⊤x −b⊤y.
(5.1)
Maximizing over y for a ﬁxed x, problem (5.1) is equivalent to the following variant of ℓ1-regression:
min
x∈∆m c⊤x +
A⊤x −b

1 .
(5.2)
We call x ∈∆m an ϵ-approximate minimizer for (5.2) if it satisﬁes c⊤x+
A⊤x −b

1 ≤minx′∈∆m c⊤x′+
A⊤x′ −b

1 + ϵ. We relate (5.1), (5.2) to MCM via reduction: we construct an appropriate in-
stance of (5.1), (5.2) such that any approximate minimizer to (5.2) can be eﬃciently converted
into an approximate MCM. We note the semi-streaming solver we develop in Section 5.3 attains an
approximate solution for (5.1), but in applications we only ever use that the x block approximately
minimizes (5.2).
Interestingly, we provide a general streaming algorithm for approximating the value of (5.1),
(5.2) (and implicitly representing an optimal solution) in the following access model.
Deﬁnition 18 (Semi-streaming matrix model). In the semi-streaming matrix model, a matrix
A ∈Rm×n and a vector c ∈Rm are presented to the algorithm respectively as an arbitrarily ordered
stream of rows {Ai:}i∈[m], and a similarly ordered stream of coordinates {ci}i∈[m] (the ordering on
[m] is arbitrary, but each {Ai:, ci} pair is given together). The algorithm can read this stream in
sequential passes and is constrained to use eO(n) space.
By choosing A to be the incidence matrix of a graph and setting c to be the weights if applicable,
Deﬁnition 18 generalizes Deﬁnition 17.
Note that obtaining semi-streaming algorithms for this
problem is nontrivial only when m = ω(n).
Further applications. In Section 5.5, we study additional combinatorial optimization problems which
are also reducible to box-simplex games. In the problem of discrete optimal transportation, a com-
plete bipartite graph G = (V, E) where E = L × R has associated demands ℓ∈∆L, r ∈∆R, and
edge costs c ∈RE. The demands ℓ, r are probability distributions on domains of equal size, and
the goal is to compute the minimum cost transport plan which attains the prescribed marginals
on the vertex sets L and R. The semi-streaming access model gives the cost of each edge as it is
presented; note that the marginals ℓ, r can be stored explicitly in O(n) space. In this setting, our
designed algorithm gives an approximate value of the optimal transportation, as well as a fractional
transportation plan x ∈∆E that meets the demands exactly (has the correct marginals at every

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
208
vertex) and achieves an approximately optimal value. Using our techniques we also give a result
for computing maximum weight matchings (MWMs), which is competitive with the state-of-the-art
when the optimal matching is highly saturated (e.g. when the MWM value is within a constant
factor of ∥w∥∞n in a graph with weights w, the largest possible value).
Finally, in Section 5.6 we go beyond "bipartite matching-type" applications of our framework
and consider the transshipment problem. Given a demand vector d ∈RV on vertices of an undi-
rected graph with non-negative edge weights, the goal of transshipment is to route a ﬂow satisfying
this demand while minimizing the sum of weighted ﬂow magnitude over all edges. Among other
applications, setting demands of any pairs of vertices s, t in the transshipment problem to 1 and
−1 respectively, and other vertices to zero, reduces the s-t shortest path problem to transshipment.
Combining our techniques with standard tools, we give a semi-streaming algorithm for transshipment
improving the state-of-the-art by roughly an ϵ−1-factor in the number of passes and work.
5.1.2
Our results
Here we state several key results of our paper. The following is our main result.
Theorem 26 (Approximate MCM). There is a deterministic semi-streaming algorithm which given
any bipartite G = (V, E) with |V | = n, |E| = m, ﬁnds a ϵ-multiplicatively approximate MCM in
O
 log n · log(ϵ−1) · ϵ−1
passes, O(n) space, and O(m log2 n · ϵ−1) total work.
The result most closely related to our Theorem 26 is the semi-streaming algorithm of [13] that
achieves a ϵ-approximate MCM using O(ϵ−1 · log n) passes and O(n · poly(log n, ϵ−1)) space. Theo-
rem 26 improves the space dependency to the optimal bound of O(n) at a cost of a log ϵ−1 factor in
the number of passes. Additionally, our algorithm is arguably more straightforward than [13] and
is deterministic (the randomized algorithm of [13] works with high probability).
An interesting feature of our algorithm in Theorem 26 its space complexity has no dependence on
the parameter ϵ. Consequently, this algorithm can obtain a very accurate approximation of MCM in
O(n) space, albeit at a cost of a large pass complexity. We leverage this feature and complement the
algorithm with standard augmenting path approaches for MCM, implemented eﬃciently using recent
advances on PRAM (and semi-streaming) algorithms for directed reachability problem in [290]. This
yields the following result for exactly computing an exact MCM, resolving an open problem of [368]
on obtaining o(n)-pass semi-streaming algorithms for exact MCM (note however that [368] handles
weights, while we primarily focus on MCM).
Theorem 27 (Exact MCM). There is a randomized semi-streaming algorithm which given any
bipartite G = (V, E) with |V | = n, ﬁnds an exact MCM with high probability in O(n
3
4 +o(1)) passes.
Our techniques in obtaining Theorem 26 yields results for several other semi-streaming combi-
natorial optimization problems, such as the following.

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
209
Theorem 28 (Optimal transport). There is a deterministic semi-streaming algorithm which given
any optimal transport instance on a complete bipartite graph on V = L ∪R, costs c ∈RE
≥0, and two
sets of demands ℓ∈∆L, r ∈∆R, ﬁnds an ϵ ∥c∥∞-additive approximate optimal transport plan using
O
 ϵ−1 log n log ϵ−1
passes, O (n) space, and O
 n2ϵ−1 log n log ϵ−1
work.
To our knowledge, this is the ﬁrst non-trivial semi-streaming algorithm for optimal transport
(however, it is plausible one can use existing semi-streaming algorithms for b-matching such as [12]
in conjunction with the reduction of [82] to obtain non-trivial semi-streaming algorithms.)
We
further give an algorithm for MWM using similar techniques (see Section 5.5.3).
Finally, yet another application of our techniques is to the transshipment problem. We deﬁne
transshipment and prove the following result in Section 5.6.
Theorem 29 (Approximate transshipment and shortest path). There is a randomized semi-streaming
algorithm which given weighted undirected G = (V, E, w) and demand vector d ∈Rn, ﬁnds an ϵ-
multiplicatively approximate minimizer to the minimum transshipment cost in O(logO(1) n · ϵ−1)
passes, O(n logO(1) n) space, and O(m logO(1) n · ϵ−1) total work with high probability in n. This, in
particular, yields a semi-streaming algorithm that ϵ-multiplicative approximates the s-t shortest path
problem with the same pass, space, and work complexities.
5.1.3
Our techniques
In this section, we overview our approach. We ﬁrst give an overview of a natural reformulation of
MCM as an appropriate ℓ1-regression problem. We then discuss the two main components of our
semi-streaming MCM implementation, which are representative of our overall framework in other
applications. The ﬁrst component is a low-space solver for general ℓ1 regression problems in the form
of box-simplex games, which returns an implicit representation of an approximate (possibly dense)
solution.
The second component is a rounding procedure which sparsiﬁes implicit approximate
solutions and rounds them to feasibility and (in certain contexts) integrality.
Our main contribution is to give eﬃcient semi-streaming implementations and applications of
each of these components. Our semi-streaming methods for optimal transport, MWM, and trans-
shipment similarly follow from our framework, via appropriately formulating these applications as
regression problems, applying our semi-streaming tools to approximate the regression problem, and
rounding the approximate solution to an explicit sparse solution of no worse objective value.
MCM as ℓ1-regression. The ﬁrst piece of our MCM algorithm is to formulate an appropriate
ℓ1-regression problem, whose approximate solution also implies an approximately optimal matching.
Variants of this (fairly natural) reduction have found previous use, but its proof of correctness is
suggestive of our overall approach, so we describe it here for completeness. To obtain this reduction,
we follow a "penalizing overﬂow" approach which has also found recent applications to approximate
maximum ﬂow [482, 318, 441, 483, 486]. We remark that in Appendix D.1, this strategy is also

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
210
followed to develop a solver for approximate optimal transport based on box-simplex games.
Recall that the (fractional) MCM problem asks to ﬁnd a ﬂow x ∈RE
≥0 with maximal ℓ1 norm,
such that no vertex has more than 1 unit of ﬂow adjacent to it (the "matching constraints"). In
linear-algebraic terms, letting B ∈{0, 1}E×V be the (unsigned) incidence matrix for the graph,
the problem asks to maximize ∥x∥1 subject to the matching constraints, B⊤x ≤1 entrywise. The
penalizing overﬂow approach relaxes this problem to an appropriate unconstrained optimization
problem, by appropriately penalizing matching constraint violations (rather than treating them as
hard constraints), and using combinatorial structure to argue solving the unconstrained problem
suﬃces. We now describe how to instantiate this reduction for the MCM problem.
We begin by obtaining M, a 2-approximation to the MCM value M ∗, in a single pass using a
greedy algorithm (Lemma 71), which is used to parameterize our ℓ1-regression problem. The next
step in our reduction is a rounding procedure adapted from [25], which gives an algorithm showing
that any ﬂow ˆx ∈RE
≥0 (not necessarily a feasible matching) can be converted into a feasible (frac-
tional) matching ˜x on the same support, with size loss proportional to the total amount of constraint
violations Overﬂowd(ˆx) :=
(B⊤ˆx −d)+

1, where subscripting + denotes the nonnegative part and
B⊤ˆx −d encodes constraint violations (cf. Lemma 73). Using this observation, we demonstrate that
to ﬁnd a large fractional matching it suﬃces to solve the following ℓ1-regression problem
min
x∈∆e
E −⟨1E, 2Mx⟩+ 2M
(B⊤ˆx −d)+

1
to ϵM additive accuracy, where the graph eG := (eV , eE) modiﬁes our original G = (V, E) by adding
one dummy edge, and the restriction of 2Mx to the original edges E plays the role of our desired
matching; these complications are to ﬁx the ℓ1 norm of our x variable, to put it in a form suitable
for canonical optimization techniques. Further, to explicitly return an approximate MCM, we show
that we can take an implicit representation of our approximate optimizer x, and use dynamic data
structures in O(n) space to return x′ ∈∆e
E on a sparse support satisfying B⊤x = B⊤x′. We can
then explicitly compute an approximately optimal MCM on this sparse support.
We now discuss the technical tools which go into developing both of these pieces: our semi-
streaming ℓ1-regression solver, and obtaining an approximately optimal matching on a sparse sup-
port.
Iterative methods in low space. Our semi-streaming algorithm for approximately solving (5.1),
(5.2) follows recent improved algorithms solving these box-simplex problems in the standard (non-
streaming) setting. Classical ﬁrst-order methods for solving these problems, such as smoothing [419]
or decoupled extragradient methods [415, 420] have iteration counts incurring either a quadratic
dependence on the error ratio ∥A∥∞ϵ−1 or growing polynomially with the dimension (each iteration
running in time linear in the sparsity of the matrix A). A line of work, beginning with a breakthrough
by [483] (which solved a more general problem), has designed improved ﬁrst-order methods bypassing

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
211
both of these bottlenecks, with an iteration count of eO(∥A∥∞ϵ−1). The [483] algorithm is analyzed
in the box-simplex setting we study using a diﬀerent regularizer in Appendix D.1; we recall that
Section 2.5 demonstrates that the classical methods of [415, 420] could also be analyzed using the
regularizer of Appendix D.1, and obtain a matching rate.
Our contribution to this line of work is the crucial observation that the [293] algorithm can
deterministically be implemented implicitly in O(n) space under the semi-streaming matrix model
of Deﬁnition 18. This observation is perhaps surprising, since the simplex variable throughout the
algorithm is typically fully-dense, and we do not subsample to preserve sparsity. We instead utilize
the recursive structure of the box-simplex algorithms to give a low-space representation of simplex
iterates, such that the actual iterate can be accessed in O(1) passes.
We begin by giving a high-level overview of the algorithm of Appendix D.1; we remark that we
will use this algorithm (as opposed to [483]) because its analysis is convenient for obtaining tighter
dependencies on problem parameters for our applications. The algorithm of Appendix D.1 runs in
eO(∥A∥∞ϵ−1) iterations, each computing a box-simplex pair (xt, yt) ∈∆m × [−1, 1]n. Each box-
simplex pair is an approximate solution to a regularized linear objective of the form (hereinafter we
let | · | applied to matrix or vector arguments act entrywise)
min
x∈∆m,y∈[−1,1]n ⟨gx, x⟩+ ⟨gy, y⟩+ r(x, y), where r(x, y) := x⊤|A|(y2) + 10 ∥A∥∞
X
i∈[m]
xi log xi. (5.3)
Typically, closed-form solutions to the above displayed problem are diﬃcult to compute, because the
regularizer r couples the two blocks. However, Appendix D.1 provides an alternating minimization
subroutine which rapidly converges to an approximate solution, which suﬃces for our purposes.
Our ﬁrst observation is that in every iteration, the simplex variable xt, is always proportional
to a vector exp(Avt + |A|ut + λtc) for n-dimensional vectors vt, ut and a scalar λt. This is be-
cause throughout the alternating subroutine for solving (5.3), every simplex iterate is the minimizer
to a entropy-regularized linear objective, where the linear term is with respect to a linear com-
bination of two pieces. The ﬁrst is a gradient operator of the box-simplex problem, of the form
 Ay + c, b −A⊤x

for iterate x, y, and the second is the gradient of our regularizer (which on the
x side is either a linear combination of A and |A| applied to n-dimensional vectors or an entropic
gradient; the latter yields a logarithm of previous iterates, which have an implicit representation
inductively). Combining shows all linear terms have the form Av+|A|u+λc, enabling our recursion.
Our second observation is that for any (vt, λt) pair, computing the vectors A⊤exp(Avt +|A|ut +
λtc) and |A|⊤exp(Avt+|A|ut+λtc) (required in the gradient operator) can be performed in a single
semi-streaming pass, and O(n) space, because the resulting matrix-vector product decouples by edge.
Combining these pieces implies a deterministic semi-streaming iterative method for approximating
the value of box-simplex games, as stated in the following (cf. Section 5.2 for deﬁnitions).
Theorem 30. There is a deterministic algorithm obtaining an ϵ-additive approximation to the value

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
212
of (5.1), (5.2) in O( ∥A∥∞
ϵ
· log(m) log(max (∥A∥∞, ∥b∥1) · ϵ−1)) passes and O(n) space. Moreover,
the algorithm outputs a stream of nonnegative 1-sparse vectors in Rm of length O(m · ∥A∥∞log m
ϵ
)
whose sum is ¯x, an ϵ-approximate optimizer to (5.2).
Theorem 30 gives an O(n) space, eO
 ∥A∥∞ϵ−1
-pass algorithm for obtaining ϵ-additive approx-
imations to the value of box-simplex games. The remainder of our work is in applying this method
to matching problems and providing semi-streaming implementations for rounding the solution.
Detecting a sparse support via cycle cancelling. So far, we have given a way of running a solver
for the problems (5.1), (5.2) entirely in O(n) space. However, it remains to discuss how to extract an
MCM from these iterates; although we have a low-space implicit representation, the iterates them-
selves remain fully dense, and to write them down explicitly for postprocessing is too expensive. To
get around this obstacle, we give a data structure which is compatible with our implicit representa-
tion, and can take a ﬂow x given as a sequence of edge additions and obtain an O(n)-sparse x′ such
that ∥x∥1 = ∥x′∥1 and B⊤x = B⊤x′ (i.e. the "quality of the matching" is unaﬀected), but we can
aﬀord to explicitly write down x′. Our data structure is based on cycle cancelling via link/cut trees,
a standard technique in the literature following e.g. [488, 243].
Ultimately, we show our framework of implicitly solving a box-simplex game and feeding the
implicit representation of its solution into a link/cut tree data structure to detect a sparse support
containing a good-quality approximate matching is quite ﬂexible. We leverage this framework to also
solve semi-streaming variants of optimal transport, weighted matching, and transshipment problems.
5.1.4
Previous work
Maximum matching. MCM and its many variants are arguably the most extensively studied problems
in the graph streaming literature; see, e.g. [221, 387, 240, 327, 213, 308, 12, 310, 41, 439, 40, 305,
326, 506, 13, 37, 228, 218, 72, 312, 309] and references therein.
The ﬁrst multi-pass MCM algorithm was given in [221]—alongside the introduction of the semi-
streaming model itself—achieving a ( 2
3 −ϵ)-approximation in O(ϵ−1 log ϵ−1) passes. The ﬁrst (1 −
ϵ)-approximation was then achieved by [387], for both bipartite and non-bipartite graphs, using
(ϵ−1)O(ϵ−1) passes. The pass-complexity of this result for bipartite graphs was improved in [213, 12,
308, 13, 42], leading to the aforementioned state-of-the-art results of [13, 42]. More recently, [368]
designed a semi-streaming algorithm for this problem with eO(√m · log (ϵ−1)) passes, which achieves
better pass-dependence on ϵ, at a cost of a polynomial dependence on n, m in the number of passes.
Finally, [224] very recently gave the ﬁrst poly(ϵ−1)-pass algorithm for this problem for non-bipartite
graphs.
The aforementioned works focus on insertion-only streams.
When allowing deletions to the
stream, i.e. in the turnstile streaming model, the problem becomes signiﬁcantly harder. In particular,
the best approximation ratio possible to MCM in a single-pass is provably only Ω(n1/3) [41, 158]

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
213
which is achieved by the algorithms of [41, 138] (see also [325]). Nevertheless, many multi-pass
streaming algorithms for MCM can be extended to turnstile streams at the cost of an additional
O(log n) multiplicative factor in their number of passes [14]. It is worth noting that the algorithm
of [13] directly works in the turnstile streaming model.
Alongside MCM, maximum weight matching (MWM) has also been studied extensively. Some
key results include ( 1
2 −ϵ)-approximation in a single pass [439], and (1 −ϵ)-approximation in
(ϵ−1)O(ϵ−2) [228], O(ϵ−2 log ϵ−1) [12], and O(ϵ−1 log n) [13] passes. The exact algorithm of [368]
in eO(√m) also works for the more general MWM problem.
We also note that several other streaming variants of MCM and MWM have also been studied,
including random-order streams [327, 218, 326, 310, 37, 72, 38], n1+Ω(1)-space algorithms [37, 13, 69],
or size estimation in o(n)-space [310, 216, 40, 389]. Reviewing this vast literature is beyond the scope
of this paper and we refer the interested reader to these references.
As in many computational models, MCM and MWM have been a testbed for development and
adaptation of various algorithmic tools for streaming including local-ratio algorithms [439, 228],
augmenting paths [221, 387, 213], water-ﬁlling or auction algorithms [308, 42], iterative methods e.g.
multiplicative weights [12, 13] and interior-point methods [368].
Optimal transport. The design of eﬃcient algorithms for discrete optimal transportation has received
widespread attention in recent years [151, 25, 24, 365], due in large part to its many applications in
modern statistical modeling and machine learning. To our knowledge, ours is the ﬁrst result which
applies to this problem in the semi-streaming setting (though as we remark earlier, it may be possible
to combine prior-work to achieve non-trivial results). Our approach for optimal transport follows
straightforwardly from our algorithms for solving the fractional maximum matching problem, via
known reductions in the literature.
Transshipment and shortest path. The transshipment problem in the semi-streaming was previously
studied in [67] which designed an eO(ϵ−2) pass semi-streaming algorithm for this problem using gradi-
ent descent. This algorithm in turn allowed [67] to obtain ϵ-approximate semi-streaming algorithms
for the (single-source) shortest path problem in eO(ϵ−2) passes, improving upon the no(1)-pass algo-
rithm of [269] that required n1+o(1)-space (more than the restriction of semi-streaming algorithms).
More recently, [117] gave a semi-streaming algorithm for ﬁnding exact single-source shortest paths
in eO(√n) passes. On the lower bound front, [261] showed that ϵ-approximate semi-streaming algo-
rithms for the s-t shortest path problem require Ω(ϵ−1) passes (see also [43, 125] for more advances
on this front).
5.2
Preliminaries
General notation. We use [n] to denote {1, ..., n} and 0n and 1n to denote the all-zeros and all-ones
vectors in Rn. An event holds with high probability if it holds with probability ≥1 −n−c for c > 0

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
214
(in our results, c can be chosen to be arbitrarily large aﬀecting guarantees by constant factors). We
use nnz(A) to denote the number of nonzero entries in a matrix A, and its row i and column j are
denoted Ai: and A:j. We say scalar α > 0 is an ϵ-multiplicative approximation to scalar β > 0 if
| α
β −1| ≤ϵ; similarly, we say it is an ϵ-additive approximation if |α −β| ≤ϵ. For a vector v, we use
v+ or (v)+ to denote the vector which entrywise has [v+]i = max (vi, 0), and use vS ∈Rn to denote
the vector which zeroes out entries of v in [n] \ S for S ⊆[n].
Norms. We let ∥v∥p denote the ℓp norm of vector v, and ∥M∥p denote the ℓp operator norm of a
matrix. In particular, ∥M∥∞is the maximum ℓ1 norm over rows of M, and ∥M∥1 is the maximum ℓ1
norm over columns. We let ∆m ⊂Rm
≥0 denote the nonnegative simplex, so x ∈∆m ⇐⇒∥x∥1 = 1
and entrywise x ≥0. Finally, we let ∥v∥0 be the number of nonzero entries of a vector v.
Graphs. We denote a graph by G = (V, E), and deﬁne n = |V | and m = |E| when context is
clear. We refer to the independent vertex subsets of a bipartite graph by L and R. We denote the
(unsigned edge-vertex) incidence matrix of a graph by B ∈RE×V
≥0
, where Bev = 1, if and only if
v ∈V is an endpoint of e and is zero otherwise. We refer to any assignment of values to edges
f ∈RE as a ﬂow. When the graph is weighted (with associated edge weights w ∈RE
≥0), we refer to
the graph by G = (V, E, w).
Computation model. Throughout, we operate in the standard word RAM model of computation,
where basic arithmetic operations on O(log n)-bit words can be performed in constant time. For
weighted graphs, we assume all weights can be stored in O(1) words, and all results concerning
additional memory overhead count the number of additional words necessary for algorithms. We
also assume always that ϵ−1 = O(poly(n)). We measure space overhead complexity by the number of
words used. In other related computational models, this may increase our space or work complexities
by a logarithmic factor.
5.3
Box-simplex games in low space
In this section, we provide semi-streaming algorithms (cf. Deﬁnition 18) for approximately solving
box-simplex bilinear games of the form (5.1), which will yield approximate minimizers for the ℓ1
regression problem. The complexity of our algorithms depend on the quantity ∥A∥∞. We prove in
this section that we can implement recent improved algorithms for problems of the form (5.1) based
on area-convex regularization in the semi-streaming model in low memory, and give the guarantees
as Theorem 30.
Our algorithm for Theorem 30 is a low-space implementation of a primal-dual algorithm inspired
by [483], specialized to box-simplex games (as analyzed in Appendix D.1). Our main algorithm,
Algorithm 13, follows the framework of Appendix D.1, which analyzes the convergence of a dual
extrapolation scheme for solving (5.1). Lines 8 and 11 of the algorithm are implemented with AltMin

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
215
(Algorithm 14), an alternating minimization subroutine also provided in Appendix D.1. Finally, we
note that we peform pre-processing and post-processing of the problem in Lines 1 and 2, so that
our ﬁnal guarantees only depend on properties of A (and not b or c). The result of these steps only
aﬀect the problem by a constant scalar shift, and can be implemented in one semi-streaming pass
(more detail is given as Lemma 256 in Appendix D.1.7).
Algorithm 13: ACDualEx(A, b, c, ϵ, T)
1 Input: A ∈Rm×n
≥0
, c ∈Rm, b ∈Rn, 0 ≤ϵ ≤∥A∥∞, T ∈N Remove all coordinates i ∈[m]
where ci > mini∗∈[m] ci∗+ 2 ∥A∥∞from c and corresponding rows of A from the problem
(ignoring the entries from the stream), then set c ←c −(mini∗∈[m] ci∗)1 entrywise ;
2 b ←min (max (b, −∥A∥∞) , ∥A∥∞);
3 r(x, y) := x⊤|A|(y2) + 10 ∥A∥∞
P
i∈[m] xi log xi, g(x, y) := (Ay + c, −A⊤x + b);
4 z0 ←( 1
m1m, 0n), s0 ←(0m, 0n);
5 for 0 ≤t < T do
6
γx ←sx
t + 1
3(Ayt + c);
7
γy ←sy
t + 1
3
 b −A⊤xt

;
8
wt := (x′
t, y′
t) ←AltMin(γx, γy, A, K, xt, yt) with K = O(log max(∥A∥∞,∥b∥1)
ϵ
) giving
ϵ
2-approx. minimizer to

 1
3g(zt) −∇r(zt), w

+ r(w) in ∆m × [−1, 1]n ;
9
γx ←sx
t + 1
6(Ay′
t + c);
10
γy ←sy
t + 1
6
 b −A⊤x′
t

;
11
zt+1 := (xt+1, yt+1) ←AltMin(sx
t + γx, sy
t + γy, A, K, xt, yt) with
K = O(log max(∥A∥∞,∥b∥1)
ϵ
) giving ϵ
2-approx. minimizer to

 1
3g(wt) −∇r(zt), z

+ r(z)
in ∆m × [−1, 1]n;
12
st+1 ←(γx, γy) ;
Algorithm 14: AltMin(γx, γy, A, δ, K, xinit, yinit)
1 Input: A ∈Rm×n
≥0
, γx ∈Rm, γy ∈Rn, K ∈N;
2 Output: Approx. minimizer to ⟨(γx, γy), z⟩+ r(z) for r(z) in Line 2, Algorithm 13 ;
3 x(0) ←xinit, y(0) ←yinit;
4 for 0 ≤k < K do
5
x(k+1) ←argminx∈∆m

⟨γx, x⟩+ r(x, y(k))
	
;
6
y(k+1) ←argminy∈[−1,1]n

⟨γy, y⟩+ r(x(k+1), y)
	
;
7 Return: (x(K), y(K));
The guarantees of Algorithms 13 and 14 are demonstrated by Appendix D.1, and summarized
in Proposition 14. The main technical modiﬁcation in it is a tighter characterization of the alter-
nating minimization subroutine, based on bounding the initial error independently of the number of
iterations (whereas Appendix D.1 assumed a worst-case bound on initial error).
Proposition 14. By taking T = O( ∥A∥∞log m
ϵ
) for a suﬃciently large constant, Algorithm 13 results
in iterates {(x′
t, y′
t)}0≤t<T so that ¯x := 1
T
P
0≤t<T x′
t is an ϵ-approximate minimizer to (5.2).

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
216
We defer a proof of Proposition 14 to Appendix D.1.7, as it largely stems from ideas used in
Appendix D.1. We next turn to our main technical contribution, the development of semi-streaming
algorithms for implementing Algorithms 13 and 14 (Lemma 70), the primary innovation of this
section. Concretely, our implementation has the following key property: in every iteration all box
variables are maintained explicitly, and all simplex variables are maintained implicitly as points
proportional to exp(Av + |A|u + λc), for some vectors v, u ∈Rn and scalar λ computed and stored
in O(n) space. We ﬁrst make a crucial technical observation regarding computing gradients with a
single pass.
Lemma 69. There is a one pass semi-streaming algorithm that computes ∥exp(Av + |A|u + λc)∥1,
A⊤exp(Av + |A|u + λc), and |A|⊤exp(Av + |A|u + λc) for input v, u ∈Rn and λ ∈R with O(n)
space and nnz(A) work.
Proof. First, to compute ∥exp(Av + |A|u + λc)∥1, for each i ∈[m] the algorithm sequentially com-
putes each exp(⟨Ai:, v⟩+ ⟨|Ai:|, u⟩+ λci) through the pass and sums these values. Second, note
A⊤exp(Av + |A|u + λc) =
X
i∈[m]
Ai: exp(⟨Ai:, v⟩+ ⟨|Ai:|, u⟩+ λci),
|A|⊤exp(Av + |A|u + λc) =
X
i∈[m]
|Ai:| exp(⟨Ai:, v⟩+ ⟨|Ai:|, u⟩+ λci).
Throughout the pass, the algorithm adds exp(⟨Ai:, v⟩+ ⟨|Ai:|, u⟩+ λci) to the coordinates of the
resulting vector in A and |A| it contributes to (speciﬁed by the row given in the stream).
Using ideas from Lemma 69, we demonstrate that the entirety of Algorithm 13 and its subroutine
Algorithm 14 can be implemented in low space.
Lemma 70. Consider an instance of Algorithm 13 using Algorithm 14 to implement Lines 5 and
6. Throughout, the following invariants on the iterates hold:
xt ∝exp(Avt + |A|ut + λtc), x′
t ∝exp(Av′
t + |A|u′
t + λ′
tc) for some vt, v′
t, ut, u′
t ∈Rn, λt, λ′
t ∈R.
Moreover, we also have
sx
t = Av′′
t + λ′′
t c for some v′′
t ∈Rn, λ′′
t ∈R.
Given their values in the previous iteration, in every iteration of Lines 6-12 of Algorithm 13 we can
compute such (vt, v′
t, v′′
t , ut, u′
t, λt, λ′
t, λ′′
t , yt, y′
t, sy
t) in O(n) space. Each iteration takes O(K) passes
and O(nnz(A) · K) total work, where K is the input parameter to Algorithm 14.
Proof. We proceed by induction on t; in the ﬁrst iteration, we have v0 = u0 = v′′
0 = 0n, λ0 = λ′′
0 = 0.
Preserving the st invariant. Recall that st+1 = st + (Av + 1
6c, 1
6(b −A⊤x′
t)) where v := 1
6y′
t. By
induction on wt, we can explicitly compute v in O(n) space, and Lemma 69 lets us explicitly compute

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
217
1
6(b−A⊤x′
t) in O(n) space as well. Hence, we can inductively perform the following updates in O(n)
space, maintaining the invariant on st+1:
λ′′
t+1 ←λ′′
t + 1
6, v′′
t+1 ←v′′
t + v, sy
t+1 ←sy
t + 1
6(b −A⊤x′
t).
Preserving the wt invariant. Suppose inductively that zt = (xt, yt) where xt ∝exp(Avt+|A|ut+
λtc) for explicitly stored values vt, ut, λt, yt; we will drop the index t for simplicity and refer to
these as ¯v, ¯u, ¯λ, ¯y. Also, we refer to v′′
t , λ′′
t , and sy
t as vs, λs, and sy for simplicity. Consider the
procedure Algorithm 14 initalized with these values, and note that
γx = 1
3 (A¯y + c) + Avs + λsc,
γy = 1
3
 
b −A⊤
exp(A¯v + |A|¯u + ¯λc)
exp(A¯v + |A|¯u + ¯λc)

1
!
+ sy.
Since r(x, y) =

|A|(y2), x

+ 10 ∥A∥∞
P
i∈[m] xi log xi, we can compute that for each 0 ≤k < K,
x(k+1) = argminx∈∆m



D
|A|

(y(k))2
+ γx, x
E
+ 10 ∥A∥∞
X
i∈[m]
xi log xi



∝exp

−
1
10 ∥A∥∞

A
1
3 ¯y + vs

+ |A|(y(k))2 +
1
3 + λs

c

,
y(k+1) = argminy∈[−1,1]n
n
⟨γy, y⟩+
D
|A|⊤x(k+1), y2Eo
=median

−1, 1, −
γy
2|A|⊤x(k+1)

entrywise.
(5.4)
In the last line, the median operation truncates the vector −
γy
2A⊤x(k+1) coordinatewise on the box
[−1, 1]n. Now, suppose at the start of iteration k of Algorithm 14, we have the invariant
x(k) ∝exp

Av(k) + |A|u(k) + λ(k)c

,
(5.5)
and we have explicitly stored the tuple (v(k), u(k), λ(k), y(k)); in the ﬁrst iteration, we can clearly
choose (v(0), u(0), λ(0), y(0)) = (¯v, ¯u, ¯λ, ¯y). By the derivation (5.4), we can update
v(k+1) ←−
1
10 ∥A∥∞
1
3 ¯y + vs

,
u(k+1) ←−
1
10 ∥A∥∞

(y(k))2
,
λ(k+1) ←−
1
10 ∥A∥∞
1
3 + λs

,
preserving representation (5.5) in the next iteration. Moreover, note that γy can be explicitly stored

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
218
(it can be computed in one pass using Lemma 69), and by applying Lemma 69 and the form (5.5),
we can compute the vector |A|⊤x(k+1) explicitly in one pass over the data and O(m) work. This
allows us to explicitly store y(k+1) as well. The ﬁnal point wt is one of the iterates (x(K), y(K)),
proving the desired invariant. Finally, we remark in order to perform these computations we only
need to store the tuple (v(k), u(k), λ(k), y(k)) from the prior iteration, in O(n) memory.
Preserving the zt+1 invariant.
The argument for preserving the invariant on zt+1 is exactly
analogous to the above argument regarding wt; the only modiﬁcation is that the input vector to
Algorithm 14 is
γx = 1
6 (Ay′
t + c) + Avs + λsc,
γy = 1
6
 b −A⊤x′
t

+ sy.
However, the previous argument shows that the vector y′
t can be explicitly computed, and the vector
x′
t satisﬁes the invariant in the lemma statement. The same inductive argument shows that we can
represent every intermediate iterate in the computation of zt+1 in the desired form.
Numerical stability. We make a brief comment regarding numerical stability in the semi-streaming
model, which may occur due to exponentiating vectors with a large range ω(log m) (in deﬁning sim-
plex variables). It was shown in Appendix D.1.5 that Algorithm 13 is stable to increasing the value
of any coordinate of a simplex variable which is m10 multiplicatively smaller than the largest to reach
this threshold, and renormalizing (i.e. this implicit "padding" operation only aﬀects the resulting
minimizer by an inverse-polynomial amount, which we can assume ϵ is larger than since otherwise
the number of passes is super-polynomial). In computations of Lemma 69, we can ﬁrst store the
largest coordinate of Av + |A|u + λc in one pass. Then, for every coordinate more than 10 log m
smaller than the largest coordinate, we will instead treat it as if its value was 10 log m smaller than
the maximum in all computations, requiring one extra pass over the data.
We explicitly state a complete low-space implementation of Algorithms 13 and 14 under our
semi-streaming implementation here for completeness as Algorithm 15.
Corollary 15. Algorithm 15 is an implementation of Algorithms 13 and 14 with parameters given
by Proposition 14 in O(n) space and O(T · log max(∥A∥∞,∥b∥1)
ϵ
) passes.
Proof. It is immediate from Lemma 70 that each loop of Lines 6-10 and Lines 14-18 in Algorithm 15
is an implementation of Algorithm 14 for computing each iterate wt and zt+1 in iteration t of
Algorithm 13, in O(n) space. The pass complexity follows since passes are only used O(1) times in
each run of Lines 6-11 and 15-20 of Algorithm 15.
Finally, we conclude with our proof of Theorem 30.

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
219
Algorithm 15: LowSpaceACDualEx(A, b, c, ϵ)
1 Input: A ∈Rm×n
≥0
, c ∈Rm, b ∈Rn, 0 ≤ϵ ≤∥A∥∞;
2 Output: {v′
t}0≤t<T ⊂Rn, {u′
t}0≤t<T ⊂Rn, {λ′
t}0≤t<T ⊂R, {y′
t}0≤t<T ⊂Rn so that for
¯y := 1
T
X
0≤t<T
y′
t, ¯x := 1
T
X
0≤t<T
exp(Av′
t + |A|u′
t + λ′
tc)
∥exp(Av′
t + |A|u′
t + λ′
tc)∥1
,
the pair (¯x, ¯y) is an ϵ-approximate saddle point to (5.1);
3 T ←O( ∥A∥∞log m
ϵ
), K ←O(log n∥A∥∞
ϵ
);
4 t ←0, λ0 ←0, v0 ←0n, u0 ←0n, y0 ←0n, λ′′
0 ←0, v′′
0 ←0n, sy
0 ←0n ;
5 while t < T do
6
(v(0), u(0), λ(0), y(0)) ←(vt, ut, λt, yt);
7
γy ←1
3(b −A⊤
exp(Avt+|A|ut+λtc)
∥exp(Avt+|A|ut+λtc)∥1 ) + sy
t, computed using Lemma 69;
8
for 0 ≤k < K do
9
v(k+1) ←−
1
10∥A∥∞( 1
3yt + v′′
t );
10
u(k+1) ←−
1
10∥A∥∞((y(k))2);
11
λ(k+1) ←−
1
10∥A∥∞( 1
3 + λ′′
t );
12
d(k+1) ←2|A|⊤
exp(Av(k+1)+|A|u(k+1)+λ(k+1)c)
∥exp(Av(k+1)+|A|u(k+1)+λ(k+1)c)∥1
, computed using Lemma 69;
13
y(k+1) ←med(−1, 1, −
γy
d(k+1) ) entrywise;
14
(v′
t, u′
t, λ′
t, y′
t) ←(v(K), u(K), λ(K), y(K));
15
(v(0), u(0), λ(0), y(0)) ←(vt, ut, λt, yt);
16
γy ←1
6(b −A⊤
exp(Av′
t+|A|u′
t+λ′
tc)
∥exp(Av′
t+|A|u′
t+λ′
tc)∥1 ) + sy
t, computed using Lemma 69;
17
for 0 ≤k < K do
18
v(k+1) ←−
1
10∥A∥∞( 1
6y′
t + v′′
t );
19
u(k+1) ←−
1
10∥A∥∞((y(k))2);
20
λ(k+1) ←−
1
10∥A∥∞( 1
6 + λ′′
t );
21
d(k+1) ←2|A|⊤
exp(Av(k+1)+|A|u(k+1)+λ(k+1)c)
∥exp(Av(k+1)+|A|u(k+1)+λ(k+1)c)∥1
, computed using Lemma 69;
22
y(k+1) ←med(−1, 1, −
γy
d(k+1) ) entrywise ;
23
(vt+1, ut+1, λt+1, yt+1) ←(v(K), u(K), λ(K), y(K));
24
(v′′
t+1, λ′′
t+1, sy
t+1) ←(v′′
t , λ′′
t , sy
t) + ( 1
6y′
t, 1
6, 0n);
25
t ←t + 1;
Proof of Theorem 30. We use the parameter settings in Proposition 14, which implies that it suﬃces
to compute the value of (5.2) with ¯x := 1
T
P
0≤t<T x′
t. By linearity, we have
c⊤¯x = 1
T
X
0≤t<T
⟨c, x′
t⟩= 1
T
X
0≤t<T
X
i∈[m]
ci exp (⟨Ai:, v′
t⟩+ ⟨|Ai:|, u′
t⟩+ λ′
tci) .
Since the summands in the above display (for each 0 ≤t < T) separate componentwise in the

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
220
stream, we can compute each P
i∈[m] ci exp(⟨Ai:, v′
t⟩+⟨|Ai:|, u′
t⟩+λ′
tci) in one pass per iteration and
store their average. Moreover, we can similarly compute the vector A⊤¯x using Lemma 69 in one
pass per iteration and store it explicitly in O(n) space, at which point we can compute
A⊤¯x −b

1.
Combining these two parts, we have the desired approximation to the value.
Finally, to demonstrate the guarantee on streaming ¯x, the approximate minimizer has the form
¯x = 1
T
X
0≤t<T
exp (Av′
t + |A|u′
t + λ′
tc)
∥exp (Av′
t + |A|u′
t + λ′
tc)∥1
.
We can thus take one additional pass per iteration to compute the value of
 1
T
exp (Av′
t + |A|u′
t + λ′
tc)
∥exp (Av′
t + |A|u′
t + λ′
tc)∥1

i
for all i ∈[m],
and produce a stream of these values without aﬀecting the overall pass complexity.
We note that max(∥A∥∞, ∥b∥1) is never larger than n ∥A∥∞by Line 2 of Algorithm 13. Finally,
on a query S ⊆E with |S| = q, using O(n+q) space (and no more passes), for (¯x, ¯y) an ϵ-approximate
saddle point to (5.1) computed by our algorithm, we can output the set of values ({¯xi}i∈S, y). This
is accomplished by calling Lemma 69 and explicitly storing values of x′
t on coordinates in S each
iteration, which is implementable in O(q) additional space.
While we do not use this fact for
ﬁnding an approximate MCM solution or for our applications, this gives the memory requirement
for querying entries of an approximate solution in the most general setting for box-simplex games.
5.4
Approximate maximum cardinality matching
In this section, we give speciﬁc treatment to the problem of approximate MCM and prove our main
result Theorem 26. We prove Theorem 26 by assembling a variety of tools, centered around casting
MCM as an instance of (5.1) and using several helper procedures to complete the result.
1. In Section 5.4.1, we give the speciﬁc box-simplex problem formulation which serves as the
optimization workhorse of this section, and prove that an appropriate approximate solution to
this problem results in an approximate MCM.
2. In Section 5.4.2, we give pre- and post-processing tools for manipulating the box-simplex
problem and its output. Speciﬁcally, we give a vertex-size reduction enabling a tighter runtime
analysis, and a cycle cancelling procedure for sparsifying the support of the approximate MCM.
3. Finally, we put these pieces together to prove Theorem 26 in Section 5.4.3.

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
221
5.4.1
Reducing MCM to a box-simplex problem
Throughout this section, we consider the problem of ﬁnding a ϵ-approximate MCM of a bipartite
graph G = (V, E) with |V | = n, |E| = m, with unsigned incidence matrix B ∈{0, 1}E×V , and
maximum matching size M ∗. Any maximum (possibly fractional) matching solves the problem
max
M ∗>0 M ∗such that ∃x ∈∆E with B⊤(M ∗x) ≤1V entrywise.
Here, the variable M ∗x takes on the role of the matching; observe that since x ∈∆E, the ℓ1 norm of
M ∗x is precisely M ∗. Before giving the box-simplex objective which we will approximate with the
value algorithm of Section 5.3, we recall that we can obtain a 2-approximation to M ∗in one pass.
Lemma 71. The greedy algorithm can be implemented in one semi-streaming pass over a graph
using O(n) space and O(m) work to return a matching of size M with M ≤M ∗≤2M.
The proof is standard and deferred to Appendix D.2. As the ﬁrst step of our approximate MCM
algorithm, we obtain M so that M ≤M ∗≤2M using the above greedy algorithm. We then use
this estimate to construct a problem of the form (5.2), whose approximate solution also yields an
approximate MCM, which we describe now.
From the unweighted graph G = (V, E), we construct a modiﬁed graph eG = (eV , eE) with two
extra vertices and one extra zero-weight edge between them, and refer to its weighted adjacency
matrix as eB, with an all-zero row for the extra edge. We deﬁne dMCM ∈{0, 1}eV to be the vector
which is 1 in every coordinate of V ∈eV , and 0 in the two extra vertices. Finally, we require one
additional piece of notation: for a graph (eV , eE), some demands d ∈ReV
≥0, and some ﬂow ˜x ∈R e
E
≥0,
we deﬁne the overﬂow by
Overﬂowd(˜x) :=
X
j∈eV

eB⊤˜x −d

+

j
.
Combinatorially, this can be interpreted as the total amount the ﬂow ˜x violates d. We show that
to obtain an approximate matching in G, it suﬃces to ﬁnd an ϵM-approximate minimizer to the
following problem (where dMCM as deﬁned earlier is the demands for the MCM problem):
min
x∈∆e
E −2M ∥xE∥1 + OverﬂowdMCM(2Mx).
(5.6)
In particular, note that we can represent any ﬂow on the original edges E with ℓ1 norm at most 2M
(in particular, the maximum cardinality matching) as the restriction of a ﬂow in R e
E
≥0 with ℓ1 norm
exactly 2M. Hence, we can interpret (5.6) combinatorially as attempting to put as much ﬂow on
the edges of the original graph E as possible, while penalizing the overﬂow.
We begin by demonstrating that to ﬁnd approximate minimizers to (5.6), it suﬃces to ﬁnd an

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
222
approximate minimizer to the following ℓ1 regression problem:
min
x∈∆e
E
A⊤x −b

1 , where A := M eB, b := 1
2dMCM.
(5.7)
Lemma 72. Any ∆-additively approximate minimizer to problem (5.6) is a ∆-additively approxi-
mate minimizer to problem (5.7), for all ∆> 0, and vice versa.
Proof. The property of being a ∆-approximate minimizer is preserved if the entire problem is shifted
by a constant scalar, so it suﬃces to show (5.6) and (5.7) diﬀer by a constant scalar. Observe that
for any scalar v, max(v, 0) = 1
2(v + |v|). Thus, we can write
OverﬂowdMCM (2Mx) =
X
j∈V
max

2M eB⊤x −1, 0

= 1
2
2M eB⊤x −dMCM

1 + 1
2
X
j∈V

2M eB⊤x −1

=
A⊤x −b

1 + ⟨MBdMCM, x⟩−1
2|V |.
Comparing with (5.6) yields the conclusion, since MBdMCM is 2M times the indicator on E.
Finally, to conclude this section, we give a simple procedure for rounding an almost-feasible ﬂow
to a feasible matching. This procedure comes with a guarantee which converts an overﬂow bound
on the original ﬂow to a bound on the diﬀerence in quality of the resulting rounded matching. We
remark that a similar algorithm was developed in [25], and reproduced in Appendix D.1; however,
we give a somewhat tighter analysis which is suitable for our purposes here.
Algorithm 16: RemoveOverﬂow(x, G, d)
1 Input: Bipartite graph G = (V, E) with incidence matrix B, x ∈RE
≥0, demands d ∈RV
≥0;
2 Output: ˜x ∈RE
≥0 with ˜x ≤x entrywise, B⊤˜x ≤d, and ∥˜x∥1 ≥∥x∥1 −

 B⊤x −d

+

1;
3 dx ←B⊤x;
4 f ←(dx −d)+;
5 ˜xe ←xe

1 −max

fa
dxa , fb
dx
b

for all e = (a, b) ∈E with xe > 0;
6 Return: ˜x;
Lemma 73 (Overﬂow removal). RemoveOverﬂow is correct, i.e. on input x ∈RE
≥0, it outputs
˜x ∈RE
≥0 with ˜x ≤x, B⊤˜x ≤d, and ∥˜x∥1 ≥∥x∥1 −Overﬂowd(x).
Proof. First, xe > 0 for e = (a, b) implies dx
a > 0 and dx
b > 0 and therefore ˜x is well-deﬁned. As
d, f ≥0 and e ≤dx entrywise we also see that ˜x ≥0 and ˜x ≤x, giving the ﬁrst guarantee.

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
223
Next, note that for all a ∈V if [B⊤x]a ≤d then [B⊤˜x]a ≤da as ˜x ≤x. On the other hand, if
[B⊤˜x]a > da then fa = dx
a −da and
[B⊤˜x]a =
X
(a,b)∈E
˜x ≤
X
(a,b)∈E
xe

1 −fa
dxa

= dx
a

1 −dx
a −da
da

= da.
Consequently, in either case [B⊤˜x]a ≤da so B⊤˜x ≤d. The last claim of the lemma follows from
∥˜x∥1 =
X
e∈E
xe −
X
e=(a,b)∈E|xe̸=0
xe max
fa
dxa
, fb
dx
b

≥∥x∥1 −
X
e=(a,b)∈E|xe̸=0
xe
fa
dxa
+ fb
dx
b

= ∥x∥1 −
X
a∈V
X
e=(a,b)∈E|xe̸=0
xe
ea
dxa
= ∥x∥1 −
X
a∈V
ea = ∥x∥1 −

 B⊤x −d

+

1 .
As a consequence of Lemma 73 with d = dMCM, we have an (algorithmic) proof that any ˜x
satisfying
∥˜x∥1 −Overﬂowd(˜x) ≥(1 −ϵ)M ∗
(5.8)
can be rounded to a feasible matching on the same support with ℓ1 norm at least (1 −ϵ)M ∗.
5.4.2
Additional tools
Before proving Theorem 26, we give a few helper tools building upon prior works in the literature, to
prove Theorem 26, as Propositions 15 and 16. First, when n ≫M ∗, we note that we can reduce the
number of vertices to O(M ∗log(ϵ−1)) while only slightly decreasing the MCM size; a similar result
is given in [12]. We state the reduction here, and defer its proof to Appendix D.2 for completeness.
Proposition 15 (Vertex size reduction). There is a procedure VertexReduction (Algorithm 73) which
takes as input unweighted bipartite G = (V, E) with MCM size M ∗and with O(log ϵ−1) passes,
O(M ∗log ϵ−1) space, and O(m log ϵ−1) work outputs a subset V ′ ⊆V of size O(M ∗log ϵ−1) such
that the induced subgraph G[V ′] has a MCM of size at least (1 −ϵ)M ∗.
We also require a procedure for sparsifying supports based on cycle cancelling. Our cycle can-
celling procedure takes a (possibly infeasible) ﬂow given as an insertion-only stream, and produces
a ﬂow of at least the same value and with no additional overﬂow, with support of size at most n.
The procedure generalizes to weighted bipartite matching problems as well; we remark that similar
procedures have appeared in prior work (see e.g. [306]). We provide a detailed implementation of a
data structure to prove Proposition 16 in Appendix D.3.
Proposition 16 (Cycle cancelling). Consider a (possibly weighted) matching problem on a bipartite
graph G = (V, E, w) (for MCM, we let w = 1). There is an algorithm that has the following property:

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
224
given a stream of length L, consisting of edge-ﬂow tuples (e, fe) where e ∈E and fe ∈R≥0, deﬁne
x ∈RE
≥0 to be the sum of all fe1e in the stream where 1e is the 1-sparse indicator of edge e. Then
the algorithm runs in O(n) space and O(L log n) time, and outputs a ﬂow ˜x supported on O(n) edges
forming a forest, so that ⟨w, ˜x⟩≥⟨w, x⟩, and B⊤x = B⊤˜x.
5.4.3
Approximate MCM in fewer passes and optimal space
We ﬁnally give a proof of Theorem 26 by applying the tools we have built.
Theorem 26 (Approximate MCM). There is a deterministic semi-streaming algorithm which given
any bipartite G = (V, E) with |V | = n, |E| = m, ﬁnds a ϵ-multiplicatively approximate MCM in
O
 log n · log(ϵ−1) · ϵ−1
passes, O(n) space, and O(m log2 n · ϵ−1) total work.
Proof. We ﬁrst deﬁne the steps of the algorithm, where we adjust the error parameter ϵ by an
appropriate constant in each of the following subroutines.
1. Use Lemma 71 to obtain an estimate M ≤M ∗≤2M.
2. If M log 1
ϵ ≤n, use Proposition 15 to reduce to O(M log 1
ϵ ) vertices; overload V , E to be the
vertex and edge sets on the resulting induced subgraph.
3. Solve (5.7) to ϵM additive accuracy using Algorithm 13, with approximate minimizer x. Set
˜x = 2MxE.
4. Apply Proposition 16 to ˜x in streaming fashion to obtain a ﬂow ˆx.
5. Greedily ﬁnd an exact MCM on the support of ˆx.
We next demonstrate correctness. Proposition 14 shows that Algorithm 13 is correct for imple-
menting Step 3. By Proposition 15, the result of Step 3 of the above algorithm satisﬁes (5.8) (by
appropriately adjusting constants), since ϵM ∗≥ϵM by Step 1, and the optimal value of (5.6) is at
most M ∗by choosing 2Mx to be the restriction of the MCM to E, with all overﬂow placed on the
extra edge. Proposition 16 then returns a sparse O(ϵ)-approximate MCM supported on a forest, also
satisfying (5.8). Lemma 73 then implies that this support contains an O(ϵ)-approximate MCM. It
is well-known that to compute a maximum cardinality matching on a tree, the greedy algorithm of
repeatedly taking a leaf edge and removing both endpoints from the graph suﬃces [239]. Applying
this to each tree in the forest yields correctness.
Finally, we discuss pass, space, and work complexities.
Since Proposition 15 gives a vertex
size bound of at most M log 1
ϵ , using Algorithm 13 and 14 in Step 3 of the algorithm implies by
Corollary 15 that the pass complexity is log n · log(ϵ−1) · ϵ−1, since the additive accuracy level is
O(ϵM ∗) and ∥A∥∞= ∥b∥1 = O(M ∗). Next, by applying Proposition 16 to the input stream x
which is the average of the iterates of Algorithm 13, we reduce the support of x to be on O(n) edges

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
225
supported on a forest, while maintaining that (5.8) is satisﬁed. We can give x in a stream of size
O(m log n · ϵ−1) which induces the average iterate, per Theorem 30. This takes O(m log2 n · ϵ−1)
work by Proposition 15, and does not dominate the pass complexity. Finally, we explicitly store the
support of the forest, so it is clear that Step 5 is implementable in O(n) space and work.
5.5
Further matching applications
Here we give a few applications using the semi-streaming matching algorithm proposed in the pa-
per. Section 5.5.1 shows how to apply the semi-streaming matching algorithm to give exact MCM.
Sections 5.5.2, 5.5.3 and 5.5.4 extend our semi-streaming framework and algorithms to the model
where the cost vector c is non-uniform. We ﬁrst state a general result on solving box-simplex games
induced by weighted matching problems in Section 5.5.2. Then in Section 5.5.3 and 5.5.4, we show
how to adapt the model to speciﬁc applications. Speciﬁcally, we develop space-eﬀcient solvers for
optimal transportation and MWM respectively, by reductions to our more general solver.
5.5.1
Exact maximum cardinality matching
Here we show how to leverage Theorem 26 to obtain the following result on computing MCMs.
Theorem 66 (Exact MCM). There is a randomized semi-streaming algorithm which given any
bipartite G = (V, E) with |V | = n, ﬁnds an exact MCM with high probability in O(n
3
4 +o(1)) passes.
We remark that the number of passes in Theorem 27 can be reduced to eO((M ∗)
3
4 +o(1)) for
any MCM problem with MCM size M ∗> 0, using the vertex size reduction of Proposition 15 in
Section 5.4.2. To prove Theorem 27, we use the following result from [290] regarding reachability in
directed graphs. This result was originally proved for parallel (PRAM) algorithms, but its extension
to semi-streaming algorithms is straightforward and has been observed previously, e.g. in [368].
Proposition 17 ([290]). There is a semi-streaming algorithm that given directed G = (V, E) with
|V | = n and vertices s, t, with high probability ﬁnds a path from s to t (assuming one exists) in
O(n
1
2 +o(1)) passes.
We remark that as stated, [290] only solves the decision problem of whether s can reach t,
while in Proposition 17 we require the path. However, here we provide a brief sketch of how it is
straightforward to modify [290] to compute a s-t path and thereby prove Proposition 17.
The algorithm of [290] computes a hopset H of eO(n) edges, i.e. a graph H with eO(n) edges
with the property that any vertex u can reach another vertex v in G ∪H if and only u can reach
v in G also. Further, this hopset has the property that with high probability there exists a path
of length O(n
1
2 +o(1)) from s to t in G ∪H. The hopset H is computed in O(log n) iterations as
H1 ∪H2 ∪. . . HO(log n), where each Hi is computed from O(n
1
2 +o(1))-depth breath-ﬁrst searches in

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
226
vertex-induced subgraphs of Gi = G ∪H1 ∪H2 ∪· · · ∪Hi−1. This computation requires eO(m) work
and O(n
1
2 +o(1)) depth in the PRAM model (as was the motivation in [290]). Similarly, it can also
be implemented in O(n
1
2 +o(1)) passes in the semi-streaming model by performing the operations of
each level of parallel depth within O(1) passes. Furthermore, a directed edge (u, v) is added to Hi
if and only if u is the root of one of the BFS arborescences used to form Hi, and can reach v in the
same BFS computation.
To recover a path from s to t, we run the algorithm of [290] and store the arborescences used to
form H. We then perform one last O(n
1
2 +o(1))-depth breadth-ﬁrst search from s in G ∪H: if a path
P from s to t in G∪H is found, we mark the edges of P and store them in the memory. We can then
replace the edges of P that belong to H to the edges of G stored as parts of arborescences used to
form H. For i from O(log n) down to 1, we replace all edges in this s-t path belonging to Hi with the
corresponding arborescence paths belonging to Gi. In the end we obtain a (not necessarily simple)
path from s to t in G, which we may make simple by removing duplicates (notice that technically
for this step, we only need to run a BFS algorithm oﬄine, i.e., with no more passes on the stream,
from s to t among the stored edges of G that form H ∪P).
Proof of Theorem 27. We set ϵ := n−3
4 and run our algorithm in Theorem 26 on G to obtain a
matching F with |F| ≥(1 −ϵ) · M ∗≥M ∗−n
1
4 (as M ∗≤n). This requires O(n
3
4 +o(1)) passes.
We then augment this approximate MCM F to an exact MCM by repeatedly ﬁnding augmenting
paths for it. We form the (directed) residual graph for F as follows: we take vertices {V, s, t} and
add the edges (s, v) for all v ∈L, (v, t) for all v ∈R, and for any e = (i, j) ∈E(G) for i ∈L, j ∈R,
we add (i, j) if e /∈F and (j, i) if e ∈F. As long as F is suboptimal, we can increase its size by 1
by ﬁnding a path from s to t in this residual graph and augmenting the ﬂow with this path's edges:
such a path can be found in O(n
1
2 +o(1)) passes by applying Proposition 17. Since |F| ≥M ∗−n
1
4
initially, it can be augmented at most n
1
4 times, and the result follows.
Remark 7. The exact MCM in Theorem 27 is a direct byproduct of the simultaneous space and
pass-eﬃciency of our approximate algorithm and cannot be achieved directly by the prior state-of-the-
art. While the space of algorithm of [42] is O(n) like ours, their algorithm requires O(ϵ−2) passes
which combined with the augmentation approach and O(n
1
2 +o(1))-pass algorithm of [290] for each
augmentation, leads to eO(n) passes for exact MCM. On the other hand, while the pass complexity of
the algorithm of [13] is suﬃcient on the surface for this augmentation approach, its space-complexity
exceeds the eO(n) requirement of semi-streaming algorithms when ϵ ≪(poly log (n))−1. More pre-
cisely, by setting ϵ small enough in the algorithm of [13] to obtain an o(n) pass algorithm using the
above approach, the space requirement would become Ω(n2), which matches the trivial bound obtained
by storing the input in just one pass and solving the problem optimally oﬄine.)

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
227
5.5.2
Weighted bipartite matching under an ℓ1 constraint
In this section G = (V, E) is a bipartite graph where V = L ∪R with unweighted incidence matrix
B ∈{0, 1}E×V .
We consider a weighted matching problem parameterized by a (possibly non-
uniform) demand vector d ∈[0, 1]V , and weights w ∈RE
≥0, formally deﬁned as follows:
M ∗:= max
x≥0 w⊤x subject to B⊤x ≤d.
(5.9)
We also assume we know the value S of the ℓ1 norm of an optimal matching which is feasible for
the demands d, yielding the maximum matching weight M ∗; in all our relevant applications, we will
have S ≥1. The assumption that we exactly know S may seem restrictive; in all our applications
(cf. Section 5.5.3 and 5.5.4), it will suﬃce to know an upper bound and pad the graph with a dummy
edge appropriately. We will refer to the optimal matching as Sx∗, for some x∗∈∆E.
Next, we state the ℓ1 regression problem we solve to compute an approximate solution to (5.9).
Deﬁne the extended graph with vertex and edge sets eV , eE and incidence matrix eB, as in the
reduction of Section 5.4.1 (as a reminder, it simply adds an extra isolated edge with no constraints
on endpoints). The overﬂow formulation we will solve to obtain an approximate minimizer to (5.9)
is:
min
x∈∆e
E −⟨wE, Sx⟩+ ∥w∥∞Overﬂowd(Sx).
(5.10)
By the arguments of Lemma 72, (5.10) is equivalent to the following ℓ1 regression problem up to a
constant scalar shift:
min
x∈∆e
E −c⊤x +
A⊤x −b

1 ,
where A := 1
2S ∥w∥∞eB, b := 1
2 ∥w∥∞d, and c := S (∥w∥∞1E −wE) .
(5.11)
We next show we can use Algorithm 16 to obtain a feasible approximate MWM on a weighted
graph.
Lemma 74 (Weighted overﬂow removal). Suppose that for a weighted graph (V, E, w) with MWM
value M ∗, ˆx ∈R e
E
≥0 is a ﬂow satisfying
⟨wE, ˆx⟩−∥w∥∞Overﬂowd(ˆx) ≥M ∗−ϵ.
Applying RemoveOverﬂow to ˆx outputs ˜x ∈RE
≥0 with ˜x ≤x, B⊤˜x ≤d, and ⟨wE, ˜x⟩≥M ∗−ϵ.
Proof. By the ℓ1-ℓ∞H¨older's inequality and the guarantees of Algorithm 16,
∥wE∥˜x ≥∥wE∥ˆx −∥w∥∞∥ˆx −˜x∥1 ≥∥wE∥ˆx −∥w∥∞Overﬂowd(ˆx) ≥M ∗−ϵ.

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
228
Finally, by combining Proposition 14 applied to (5.11), the cycle cancelling procedure of Propo-
sition 16, and the rounding procedure in Algorithm 16, we obtain the following guarantee for solving
(5.9), summarized in Corollary 16.
Corollary 16. There is an algorithm that computes an n-sparse ϵ-additively approximate solution ˆx
to the problem (5.9) satisfying w⊤ˆx−w⊤(Sx∗) ≥ϵ, B⊤ˆx ≤d using O
 γ log n log(γ + ∥w∥∞∥d∥1ϵ−1)

passes, O (n) memory, and O
 mγ log n log(nγ + n∥w∥∞∥d∥1ϵ−1)

work, where γ = S∥w∥∞
ϵ
.
Proof. The proof is analogous to that of Theorem 26. We ﬁrst deﬁne the steps of the algorithm, where
we adjust the error parameter ϵ by an appropriate constant in each of the following subroutines.
1. Solve (5.11) to ϵ additive approximation, and let the approximate solution be x. Set ˜x = SxE.
2. Apply Proposition 16 to ˜x in streaming fashion to obtain a ﬂow ˆx.
3. Greedily ﬁnd an exact MWM on the support of ˆx.
The correctness and runtime follow in the same way as in Theorem 26, where we use Lemma 74 in
place of Lemma 73, which implies the support of ˆx contains a ϵ-additively approximate MWM.
We brieﬂy remark on the utility of Corollary 16. The generality of being able to handle arbitrary
costs has the downside of an additive error guarantee rather than multiplicative. We will show how
to apply this general result in diﬀerent settings where either the optimal solution is supported on
simplex (e.g. S = 1 for optimal transport), or we can modify the graph appropriately to have a
saturated optimal matching (e.g. S = n for maximum weight matching).
5.5.3
Optimal transportation
In this section, we give a semi-streaming implementation for solving the discrete optimal transporta-
tion problem. This problem is parameterized by costs4 c ∈RE
≥0, and two sets of demands ℓ∈∆L,
r ∈∆R where L ∪R = V is the bipartition of the vertices, we wish to ﬁnd a transportation plan
x ∈∆E between the demands with (approximately) minimal cost, as speciﬁed by c; we defer a
further discussion of this formulation to [25]. Appendix D.1 showed that to obtain a transport plan
approximating the optimum to ϵ-additive accuracy, it suﬃces to solve the following problem to ϵ
duality gap, where d is the vertical concatenation of ℓand r, Cmax := ∥c∥∞, and B is the adjacency
matrix of the unweighted complete bipartite graph:
min
x∈∆E −c⊤x + 2CmaxOverﬂowd(x).
(5.12)
The formulation above is an instance of the weighted formulation (5.10) with S = 1, w = c. Note
here we also use the standard adjacency matrix B instead of the extended one ˜B in denoting overﬂows
4Costs are without loss of generality nonnegative, as adding a uniform multiple of ∥c∥∞1 aﬀects the cost of all
transportation plans by a ﬁxed amount and the quantity Cmax by at most a constant factor.

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
229
Overﬂow(·) for simplicity as opposed to the rest of the paper. We thus apply Corollary 16 to solve
the problem and conclude by giving a complete result for semi-streaming optimal transportation.
Theorem 28 (Optimal transport). There is a deterministic semi-streaming algorithm which given
any optimal transport instance on a complete bipartite graph on V = L ∪R, costs c ∈RE
≥0, and two
sets of demands ℓ∈∆L, r ∈∆R, ﬁnds an ϵ ∥c∥∞-additive approximate optimal transport plan using
O
 ϵ−1 log n log ϵ−1
passes, O (n) space, and O
 n2ϵ−1 log n log ϵ−1
work.
Proof. The proof follows by ﬁrst applying Corollary 16 to (5.12), with error parameter ϵ ∥c∥∞, to
obtain an O(n) sparse solution x in the desired work and space budget. For demands d ∈RV set
to be the vertical concatenation of the given ℓand r, this is a transportation plan that satisﬁes
B⊤x ≤d, achieves an additive ϵ approximation in the optimal cost, and can be stored in O(n)
space. To make B⊤x = d, it suﬃces to add a rank-one correction term as in Algorithm 2 of [25],
whose coordinates can be computed in a stream in one pass and O(n) space (by storing the rank-one
components). This can only help the objective, and the resulting exact transport plan can be made
sparse via Proposition 16 within the speciﬁed work budget.
5.5.4
Maximum weight matching
We give a result for computing an approximate maximum weight matching for a bipartite graph in
the semi-streaming model. Using the construction in Section 5.5.2 with a demand vector d = 1V
(i.e. the vector which is all-ones on V and zeroes on eV \ V ), it is clear the ℓ1 norm of a maximum
weight matching is n, simply by putting all additional ﬂow on the extra edge. Applying Corollary 16
then directly gives a complete result for semi-streaming maximum weight matching.
Theorem 31. There is a deterministic semi-streaming algorithm which given any weighted bipartite
graph G = (V, E, w) with |V | = n, |E| = m, and deﬁning γ := n∥w∥∞
ϵ
, ﬁnds an ϵ-additive maximum
weight matching using O (γ log n log(nγ)) passes, O (n) space, and O(mγ log n log(nγ)) total work.
Finally, in the case when we have side information upper bounding the ℓ1 norm of any MWM
by S < n, note that it suﬃces to solve (5.10) with this scale S. This implies analogous wins in
Theorem 31, so that the parameter γ scales linearly in S rather than n.
5.6
Transshipment
To demonstrate the versatility of our approach, we also adapt our method to solve the transshipment
problem on graphs in the semi-streaming model. In this problem we are given access to an undirected
graph with non-negative edge weights G = (V, E, w) and a vector d ∈RV . Here d is a demand vector,
i.e. it speciﬁes the desired ﬂow imbalance on every vertex, and we speciﬁes the cost per routing each
unit of ﬂow (in magnitude) on edge e.

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
230
The goal of the transshipment problem is to compute a ﬂow which routes the demands d of
minimum cost, given as the sum of weighted ﬂow magnitudes. Formally, for a ﬂow f ∈RE the
imbalance of the ﬂow at every vertex is given by B⊤f where B in this section refers to the signed
incidence matrix of G.5 Consequently, the transshipment problem is minf∈RE:B⊤f=d
P
e∈E we|fe|.
Rescaling f ←W−1f for W := diag (w) shows that equivalently we can deﬁne the problem as
follows.
Deﬁnition 19 (Transsshipment problem). For a graph G = (V, E, w) with nonnegative edge weights,
let B ∈{−1, 0, 1}m×n be its oriented (signed) unweighted incidence matrix. For any demand vector
d ∈Rn, we write the transshipment problem, for a diagonal positive6 matrix W ∈Rm×w as
min
f:B⊤W−1f=d ∥f∥1 .
(5.13)
Additionally, let opt(d) refer to an arbitrary minimizer of problem (5.13) for a given demand d.
Applying our algorithmic framework, namely using the semi-streaming box-simplex game solver
in Section 5.3 and our tools for rounding and maintaining sparsity, along with known techniques
previously developed in recent work [358], we prove Theorem 29:
Theorem 29 (Approximate transshipment and shortest path). There is a randomized semi-streaming
algorithm which given weighted undirected G = (V, E, w) and demand vector d ∈Rn, ﬁnds an ϵ-
multiplicatively approximate minimizer to the minimum transshipment cost in O(logO(1) n · ϵ−1)
passes, O(n logO(1) n) space, and O(m logO(1) n · ϵ−1) total work with high probability in n. This, in
particular, yields a semi-streaming algorithm that ϵ-multiplicative approximates the s-t shortest path
problem with the same pass, space, and work complexities.
We prove this result in several parts. We begin by giving a semi-streaming construction of an
ℓ1-stretch approximator matrix R ∈RK×n, deﬁned in the following (a similar deﬁnition is given in
[358]). Our semi-streaming implementation is largely based on known tools developed in [358].
Deﬁnition 20 (Stretch approximator). R ∈RK×n is an (α, β)-stretch approximator if it satisﬁes
the following.
• For any d ∈Rn, ∥opt(d)∥1 ≤∥Rd∥1 ≤α ∥opt(d)∥1.
• P
v∈V deg(v)nnz(R:v) ≤mβ, where deg(v) is the degree of v in G and nnz(R:v) is the number
of nonzero entries in the vth column of R.
• R has at most Kβ nonzero entries.
5We overload the notation B for just this section, the only application other than Appendix D.5 where it is signed.
The signed unweighted incidence matrix is deﬁned in the same way as the unsigned one, except we choose an arbitrary
orientation for every edge e = (u, v) so the corresponding row in B has a 1 in column u and a −1 in column v.
6For simplicity, we assume no zero-weight edges. Otherwise, we can form a spanning forest in one pass to remove
zero-weight edges (since these shortcut the transshipment problem), and decompose into connected components.

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
231
In Section 5.6.1, we provide an algorithm which construct an (α, β)-stretch approximator R ∈
RK×n with K = O(n log n) and α, β = logO(1) n; we remark we did not try to control for the
logarithmic factors, as many are inherited from prior work. In Section 5.6.2, we then use our stretch
approximator to reduce a decision variant of transshipment to an appropriate box-simplex game.
In Section 5.6.3, we demonstrate how to use our cycle-cancelling toolkit to round an approximate
transshipment plan f to be sparse.
In Section 5.6.4 we we put these pieces together to prove
Theorem 29.
5.6.1
Constructing stretch approximators
In this section, we give a semi-streaming construction of a stretch approximator. Our construction
is a straightforward adaptation of a previous parallel construction by [358], proceeding in two steps.
First we compute H, an O(log n)-spanner of G using O(log n) semi-streaming passes. We apply the
construction of [358] to H oﬄine, which no longer requires additional access to the edges of G. We
then prove the approximation and sparsity bounds of the obtained matrix.
Deﬁnition 21 (Spanner). We say subgraph H ⊆G is an σ-spanner of graph G = (V, E) if for all
u, v ∈V , where dG(u, v) denotes the shortest path distance through G,
dG(u, v) ≤dH(u, v) ≤σdG(u, v).
It is well-known that Deﬁnition 21 implies that the optimal value of the objective (5.13) is
preserved up to a multiplicative σ factor when restricting to H. This follows as every path in a
decomposition of the solution to (5.13) has its cost preserved up to a σ factor upon routing through
H.
Lemma 75 (Corollary 5.2 of [67]). Let G = (V, E, w) be a weighted graph given in an insertion-only
stream. An O(log n)-spanner of G with O(n log n) edges can be computed using O(log n) passes,
O(n log n) space, and O(m log n) total work.
Lemma 76 (Theorem 4.2 of [358]). Given a graph G = (V, E, w), we can compute a matrix R ∈
RO(n log n)×n satisfying the following properties with high probability in n:
• ∥opt(d)∥1 ≤∥Rd∥1 ≤O(log4.5 n) ∥opt(d)∥1 for all d ∈Rn.
• For any v ∈V , the vth column of R has O(log5 n(log log n)O(1)) nonzero entries in expectation
over the construction of R.
The algorithm runs in O(n log10 n(log log n)O(1)) work and space.
Proof. The ﬁrst and third condition on R follow directly from Theorem 4.2 of [358]. The second
condition follows from Lemma 4.15 and the proof of Lemma 4.16 of [358].

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
232
Lemma 77. Let G = (V, E, w) be a weighted undirected graph given in the semi-streaming graph
model. Algorithm 17 computes R ∈RK×n, an (α, β)-stretch approximator with K = O(n log n),
α = O(log5.5 n), and β = O(log5 n(log log n)O(1)), in O(log n) passes, O(n log10 n(log log n)O(1))
space, and O(m log10 n(log log n)O(1)) work with high probability in n.
Proof. We ﬁrst show correctness of the algorithm. First, we note that the cost of transshipment over
H is at most O(log n) times greater than the cost over G since dG(u, v) ≤dH(u, v) ≤O(log n)dG(u, v)
for all u, v ∈V . We thus have ∥opt(d)∥1 ≤∥Rd∥1 ≤O(log5.5 n) ∥opt(d)∥1 by the ﬁrst condition of
Lemma 76 applied to the matrix R we return; this guarantee is with high probability, by a union
bound over all the routing constructions. Next, the second condition of Lemma 76 implies that
each column of R computed on line 6 of Algorithm 17 has γ nonzero entries in expectation. Thus
E
P
v∈V nnz(R:v)

≤nγ and
E

X
(u,v)∈E
nnz(R:u) + nnz(R:v)

= 2E
"X
v∈V
deg(v)nnz(R:v)
#
≤2mγ.
By Markov's inequality and a union bound, we thus have S ≤6mγ and nnz(R) ≤3nγ with
probability at least 1
3 over the randomness of Lemma 76. Consequently, with high probability in n
after O(log n) repetitions the algorithm returns a matrix R with all the desired properties.
We now bound the pass, space, and work complexity.
By Lemma 75, the graph H can be
computed using O(log n) passes over G using O(n log n) space and O(m log n) total work. Next, the
matrix R on Line 6 of Algorithm 17 can be computed oﬄine in O(m log10 n(log log n)O(1)) work and
O(n log10 n(log log n)O(1)) space. The condition on Line 7 can be checked in a single pass over G
with O(mγ) extra work (as we can terminate if the check fails).
Algorithm 17: StretchApprox(G)
1 Input: Graph G = (V, E, w) ;
2 Output: ℓ1-stretch approximator R;
3 γ = O(log5 n(log log n)O(1));
4 H = O(log n)-spanner of G computed by Corollary 5.2 of [67];
5 for t ∈[O(log n)] do
6
R = matrix computed by Theorem 4.2 of [358] applied to H;
7
Compute S = P
v∈V deg(v)nnz(Rev) in a stream over G;
8
if S ≤6mγ and nnz(R) ≤3nγ then
9
Return: R;

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
233
5.6.2
Reduction to box-simplex game
In this section, we describe our reduction of transshipment to a box-simplex game. We begin by
deﬁning a ﬂow-constrained variant of the transshipment problem.
Deﬁnition 22. Let G = (V, E, w) be a graph, and let d ∈Rn. Let R be any matrix satisfying
∥opt(d)∥1 ≤∥Rd∥1. For any t ≥0, we deﬁne the ﬂow-constrained transshipment problem as
min
f:∥f∥1≤t
RB⊤W−1f −Rd

1 .
(5.14)
We next relate solutions to the ﬂow-constrained transshipment problem to the original prob-
lem (5.13).
Lemma 78. Let G be a graph, d ∈Rn, and R satisfy ∥opt(d)∥1 ≤∥Rd∥1.
• If t ≥∥opt(d)∥1, the optimal value of (5.14) is 0.
• If t < ∥opt(d)∥1, the optimal value of (5.14) is at least ∥opt(d)∥1 −t.
Proof. For notational convenience, let f ∗= opt(d).
By deﬁnition we have B⊤W−1f ∗= d.
If
t ≥∥f ∗∥1, we note f ∗is feasible for (5.14) and therefore achieves an objective value of 0. If instead
t < ∥f ∗∥1, consider any f with ∥f∥1 ≤t and deﬁne f ′ = opt(d −B⊤W−1f). We have
RB⊤W−1f −Rd

1 =
R(d −B⊤W−1f)

1 ≥∥f ′∥1 ,
where for the last inequality we use the deﬁnition of R and the optimality of f ′. On the other hand
we have B⊤W−1f ′ = d −B⊤W−1f and so B⊤W−1(f + f ′) = d. By optimality of f ∗for (5.13),
∥f ∗∥1 ≤∥f + f ′∥1 ≤∥f∥1 + ∥f ′∥1 .
Combining these inequalities, we have the desired
RB⊤W−1f −Rd

1 ≥∥f ∗∥1 −∥f∥1 ≥∥f ∗∥1 −t.
We remark that any (α, β)-stretch approximator R satisﬁes the assumption of Lemma 78. Finally,
we demonstrate how to express problems of the form (5.14) as box-simplex games.
Lemma 79. Let G be a graph, d ∈Rn, and t ≥0. Let R ∈RK×n be an (α, β)-stretch approximator.
Then (5.14) is equivalent to
min
f ′∈∆2E
tA⊤f ′ −b

1 =
min
f ′∈∆2E
max
y∈[−1,1]K ty⊤A⊤f ′ −b⊤y

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
234
for b = Rd and
A =

W−1BR⊤
−W−1BR⊤

.
Additionally, ∥A∥∞≤α and nnz(A) = O(mβ). If R is stored explicitly, we may simulate access to
a stream of the rows of A and |A| using O(mβ) total work and a single pass over G.
Proof. By duality of the ℓ1 norm and the ℓ∞norm, (5.14) is equivalent to
min
f:∥f∥1≤t
RB⊤W−1f −Rd

1 ≡
min
f:∥f∥1≤t
max
y:∥y∥∞≤1 y⊤ RB⊤W−1f −Rd

.
Next, we can write any f ∈RE as f = f+ −f−, where f+ = max{f, 0} and f−= −min{f, 0}
are entrywise nonnegative. Further, if ∥f∥1 ≤t there exists nonnegative f+ and f−satisfying this
condition with 1⊤f+ + 1⊤f−= t: one can simply select an arbitrary edge in G and increase the
corresponding entries in f+ and f−by the same amount, which increases ∥f∥1 without aﬀecting
f+ −f−. Thus, for any f with ∥f∥1 ≤t there exists ˆf = 1
t [f+; f−] ∈∆2E such that
f = t

I
−I


⊤
ˆf.
Applying this variable substitution, our problem is equivalent to
min
ˆ
f∈∆2E
max
∥y∥∞≤1 ty⊤A⊤ˆf −y⊤b
as desired. The bound on ∥A∥∞follows from the observation
RB⊤W−1u

1 ≤α
opt
 B⊤W−1u

1 ≤α ∥u∥1
for any u ∈RE, by the ﬁrst condition of Deﬁnition 20 and deﬁnitions of B, W. This yields
∥A∥∞=
A⊤
1 = max
∥v∥1=1
A⊤v

1 ≤
max
∥v1∥1+∥v2∥1=1
RB⊤W−1v1

1 +
RB⊤W−1v2

1 ≤α
as claimed. We conclude by bounding nnz(A) and showing that we can simulate streaming access
to the columns of A and |A| in O(mβ) total work. Let δv denote the number of nonzero entries in
the vth column of R. The column of RB⊤W−1 corresponding to e = (u, v) ∈E is of the form
w−1
e
(R:u −R:v) ,
and thus has O(δu + δv) nonzero entries. By the second assumed condition on R, summing over

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
235
all columns gives O(mβ) nonzero entries together, thus the bound on nnz(A) follows. Finally, we
may simulate access to columns of A⊤and |A⊤| via a stream over G and forming the corresponding
columns of A⊤and |A⊤| directly using O(mβ) work, from the above characterization of RB⊤W−1.
5.6.3
Recovering a sparse ﬂow
We next give a simple procedure for taking a ﬂow f and rounding it to a sparse ﬂow f ′, such that the
weighted cost is no larger and the marginal imbalances are preserved. At a high level, our algorithm
(Algorithm 18) performs the following steps.
1. We form the "double cover graph" of G = (V, E, w), a bipartite graph consisting of vertices
Vin ∪Vout, which are two copies of V . For every (u, v) ∈E, the double cover graph contains
edges (uin, vout), denoting positive ﬂow, and (uout, vin), denoting negative ﬂow.
2. We sparsify both the positive ﬂow f+ and the negative ﬂow f−using Proposition 16.
3. We identify the sparsiﬁed ﬂows into the original graph by using the signs appropriately, pre-
serving marginal demands and not hurting the weighted cost.
We make the transformation f ←Wf in the following algorithm for consistency with Proposi-
tion 16, so the marginal imbalances in accordance with transshipment are B⊤f and the ℓ1 weight is
∥Wf∥1.
Algorithm 18: RoundStream(f+, f−)
1 Input: Incremental stream of f+, f−∈RE
≥0 on disjoint supports, graph G = (V, E, w);
2 Output: Flow f ′ ∈RE with
∥f ′∥0 = O(n), B⊤f ′ = B⊤(f+ −f−), ∥Wf ′∥1 ≤∥Wf+∥1 + ∥Wf−∥1 .
;
3 Form G′
+ = (V ′ = Vin ∪Vout, E′
+ = ∅) and G′
−= (V ′ = Vin ∪Vout, E′
−= ∅) ;
4 for x(u,v)1(u,v) in stream over f+ do
5
Add edge (uin, vout) with weight x(u,v) to G′
+;
6
Apply Proposition 16 to G′
+;
7 for x(u,v)1(u,v) in stream over f−do
8
Add edge (uout, vin) with weight x(u,v) to G′
−;
9
Apply Proposition 16 to G′
−;
10 Let f ′
+ be the output of Proposition 16 on G′
+ and f ′
−the output of Proposition 16 on G′
−;
11 Return: f ′
+ −f ′
−;
Lemma 80. Algorithm 18 satisﬁes its output statement using a single pass over the streams f+ and
f−, O(n) space, and O(nnz(f+ + f−) log n) work.

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
236
Proof. The pass, space, and work complexities follow from Proposition 16. The sparsity of f ′ and
the guarantee on weighted ℓ1 norm follows from the two applications of Proposition 16, since f+ and
f−have disjoint supports. Finally, by construction of G′
+ and G′
−, the signed marginals of f+ are
preserved by f ′
+, and the signed marginals of f−are preserved by f ′
−. The conclusion follows since
f+ −f−then places the same amount of net ﬂow on any vertex as f ′
+ −f ′
−.
5.6.4
Semi-streaming transshipment
We ﬁnally give our full algorithm to solve the transshipment problem, and a proof of Theorem 29.
Algorithm 19: ApproxTransshipment(G, d, ϵ)
1 Input: Graph G = (V, E, w), demand vector d ∈Rn, error tolerance ϵ ∈(0, 1);
2 Output: Flow f with ∥f∥0 = O(n), B⊤W−1f = d, and ∥f∥1 ≤(1 + ϵ) ∥opt(d)∥1.;
3 H = O(log n)-spanner of G computed by Corollary 5.2 of [67];
4 R = StretchApprox(G);
5 tmax ←2-approximation to minB⊤
HW−1
H f=d ∥f∥1, scaled to be an overestimate to opt(d);
▷We let BH and WH be the appropriate restrictions of B and W to the subgraph H.
6 tmin ←
tmax
O(log n);
7 while tmax ≥(1 +
ϵ
log n)tmin do
8
t ←1
2(tmin + tmax);
9
A ←t

RB⊤W−1
−RB⊤W−1⊤
, b ←Rd;
10
Z, fstream ←approximate value and streamed solution of (5.2) with A, b as deﬁned
above, and c = 0, to accuracy
ϵt
O(log n);
11
if Z ≤
ϵt
O(log n) then
12
tmax ←t;
13
else
14
tmin ←t;
15
f ′ ←RoundStream([Wfstream]+, [Wfstream]−) where [Wfstream]+, [Wfstream]−are the
restrictions of fstream to the top and bottom rows of A respectively after cancelling any
nonnegative ﬂow on corresponding edges in both parts;
16
d′ ←d −B⊤f ′;
17
fres ←2-approximate solution to minB⊤
HW−1
H f=d′ ∥f∥1;
18
Return: W−1 (RoundStream([f ′ + Wfres]+, [f ′ + Wfres]−));
Proof of Theorem 29. We begin by proving correctness of Algorithm 19, and then discuss implemen-
tation costs to show they ﬁt within the speciﬁed budgets.
First, tmax is an overestimate of opt(d) by the deﬁnition of a spanner in Deﬁnition 21, and
similarly tmin is an underestimate. Next, by solving the problem in Line 8 to the stated accuracy,
Lemma 79 shows we have an
ϵt
O(log n)-additive approximation to the value of the problem (5.14).
Hence, Lemma 78 shows that when the binary search terminates, t is an O(
ϵ
log n)-multiplicative

CHAPTER 5. SEMI-STREAMING COMBINATORIAL OPTIMIZATION
237
approximation to ∥opt(d)∥1. The additive error incurred due to the approximate routing of demands
d′ on Line 15 can only aﬀect the solution by O(ϵ) multiplicatively because of the quality of the
spanner. Moreover, the applications of RoundStream do not aﬀect meeting the demands and can
only improve the ℓ1 value, by Lemma 80. Hence, the output of the algorithm meets the demands
and attains an ϵ-multiplicative approximation to the value of the transshipment problem (5.13).
We next discuss pass, space, and work complexities. The costs of Lines 1 and 2 are given by
Lemma 75 and Lemma 77 respectively. Lines 3 and 15 can be performed without streaming access
to G once H has been stored, e.g. using [358], and do not dominate any of the costs.
There are at most log log n
ϵ
iterations of the binary search, given the multiplicative range on t.
The cost of each run in Line 8 is given by Theorem 30, where ∥A∥∞= tα for the α in Lemma 77,
and where the additive accuracy is given in Line 8. Finally, the costs of calls to RoundStream are
given by Lemma 80 and do not dominate, completing the proof.
To conclude, we describe our algorithm for (1 + ϵ)-approximate shortest paths. We choose the
demands d = 1u −1v for transshipment, and consider the ﬂow f computed by Algorithm 19. We
simply return the shortest s-t path contained in the support of f: as f satisﬁed B⊤f = d, it
corresponds to a linear combination of s-t paths. Returning the shortest path found in its support
must therefore have length at most that of f, which is itself ≤(1 + ϵ)opt(d).

Chapter 6
Dynamic Decremental Bipartite
Matching
This chapter is based on [284], with Arun Jambulapati, Yujia Jin, and Aaron Sidford.
6.1
Introduction
Eﬃcient approximate solvers for graph-structured convex programming problems have led to a va-
riety of recent advances in combinatorial optimization. Motivated by problems related to maximum
ﬂow and optimal transportation, a recent line of work [482, 318, 483], as well as Chapters 4, 2,
and 5 developed near-linear time, accelerated solvers for a particular family of convex programming
objectives we refer to in this paper as box-simplex games:
min
x∈∆m
max
y∈[−1,1]n y⊤Ax + c⊤x −b⊤y where ∆m := {x ∈Rm
≥0| ∥x∥1 = 1} .
(6.1)
Box-simplex games, (6.1), are bilinear problems where a maximization player is constrained to the
box (the ℓ∞ball) and a minimization player is constrained to the simplex (the nonnegative ℓ1 shell).
The problem provides a convenient encapsulation of linear programming problems with ℓ1 or ℓ∞
structure; (6.1) can be used to solve box-constrained ℓ∞regression problems (see e.g. [483, 486])
and maximizing over the box-constrained player yields the following ℓ1 regression problem
min
x∈∆m c⊤x + ∥Ax −b∥1 .
(6.2)
Furthermore, solvers for (6.1) and (6.2) are used in state-of-the-art algorithms for approximate
maximum ﬂow [483], optimal transport (OT) [293], (width-dependent) positive linear programming
[91], and semi-streaming bipartite matching [39].
238

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
239
One of the main goals of our work is to develop eﬃcient algorithms for solving regularized variants
of the problems (6.1) and (6.2). An example of particular interest is the following
min
x∈∆m|B⊤x=d c⊤x + µH(x), where µ ≥0 and H(x) :=
X
i∈[m]
xi log xi.
(6.3)
The particular case of (6.3) when B ∈Rm×n is the (unsigned) edge-vertex incidence matrix of a
complete bipartite graph, and d is a pair of discrete distributions supported on the sides of the
bipartition, is known as the Sinkhorn distance objective. This objective is used in the machine
learning literature [151] as an eﬃciently-computable approximation to optimal transport distances:
c corresponds to pairwise movement costs, and d encodes the prescribed marginals. This objective
has favorable properties, e.g. diﬀerentiability [523], and there has been extensive work by both
theorists and practitioners to solve (6.3) and analyze its properties (see e.g. [151, 25] and references
therein). Choosing A and b to be suﬃciently large multiples of B⊤and d, it can be shown that
solutions to the following regularized variant of (6.2) yield approximate solutions to (6.3),
min
x∈∆m c⊤x + ∥Ax −b∥1 + µH(x) .
(6.4)
Beyond connections to Sinkhorn distances, there are additional reasons why it may be desirable
to solve regularized box-simplex games. For example, regularization could speed up algorithms and
allow high-precision solutions to be computed more eﬃciently. Further, obtaining a high-precision
solution to a regularized version of the problem yields a more canonical and predictable approximate
solution than an arbitrary low-precision approximation to the unregularized problem. Moreover,
regularization potentially makes optimal solutions more stable to input changes. For box-simplex
games stemming from bipartite matching we quantify this stability and show all of these properties
allow regularized solvers to yield faster algorithms for a particular dynamic matching problem.
Altogether, the main contributions of this paper are the following.
1. We give improved running times for the problem of dynamic decremental bipartite matching
(DDBM) with an adaptive adversary, a fundamental problem in dynamic graph algorithms.
Our algorithm follows from a general black-box reduction we develop from DDBM to solving
(variants of) regularized box-simplex games to high precision.
2. We give eﬃcient solvers for (variants of) the regularized box-simplex problems (6.3), (6.4).
3. As a byproduct, we also show how to apply our new solvers (and additional tools from the
literature) to obtain state-of-the-art methods for computing Sinkhorn distances.
Formally, the DDBM problem we consider in this paper is the following: given a bipartite graph
undergoing edge deletions maintain, at all times, an ϵ-approximate (maximum) matching,1 that is
1This is sometimes also referred to as a (1 + ϵ)-multiplicatively approximate matching in the literature.

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
240
a matching which has size at least a (1 −ϵ)-fraction of the maximum (for a pre-speciﬁed) value of
ϵ. Unless speciﬁed otherwise, we consider the adaptive adversary model where edge deletions can
be speciﬁed adaptively to the matching returned. Further, unless speciﬁed otherwise, we allow the
matching output by the algorithm to be fractional, rather than integral.
We show how to reduce solving the DDBM problem to solving a sequence of regularized box-
simplex games. This reduction yields a new approach to dynamic matching; this approach is inspired
by prior work, e.g. [73], but conceptually distinct in that it decouples the solving of optimization
subproblems from characterizing their solutions. For our speciﬁc DDBM problem, the only prior
algorithm achieving an amortized polylogarithmic update time (for constant ϵ) is in the recent
work of [73], which derives their dynamic algorithm as an application of two techniques: congestion
balancing and local ﬂows. Our reduction eschews these combinatorial tools and directly argues, via
techniques from convex analysis, that solutions to appropriate regularized matching problems can be
used dynamically as approximate matchings while requiring few recomputations. We emphasize our
use of fast high-accuracy solvers2 in the context of our reduction to obtain our improved runtimes,
as our approach leverages structural characteristics of the exact solutions which we only show are
inherited by approximate solutions when solved to suﬃcient accuracy.
Our work both serves as a proof-of-concept of the utility of regularized linear programming solvers
as a subroutine in dynamic graph algorithms, and provides the tools necessary to solve said problems
in various structured cases.
This approach to dynamic algorithm design eﬀectively separates a
"stability analysis" of the solution to a suitable optimization problem from the computational burden
of solving that problem to high accuracy: any improved solver would then have implications for faster
dynamic algorithms as well. As a demonstration of this ﬂexibility, we give three uses of our reduction
framework (which proceed via diﬀerent solvers) in obtaining our improved DDBM update time. We
hope our work opens the door to exploring the use of the powerful continuous optimization toolkit,
especially techniques originally designed for non-dynamic problems, for their dynamic counterparts.
Chapter organization.
We give a detailed overview of our contributions in Section 6.1.1, and
overview related prior work in Section 6.1.2. We state preliminaries in Section 6.2. In Section 6.3.1,
we describe our framework for reducing DDBM to a sequence of regularized optimization prob-
lems satisfying certain properties, and in Section 6.3.2 we give three diﬀerent instantiations of the
DDBM framework for obtaining a variety of DDBM solvers. Finally in Section 6.4 we provide our
main algorithm for regularized box-simplex games. We defer proofs for Section 6.3 and Section 6.4
to Appendix E.1 and Appendix E.2 respectively, and provide additional results for approximating
Sinkhorn distances eﬃciently in Appendix E.3.
2Throughout, we typically use the term "high-accuracy" to refer to an algorithm whose runtime scales polyloga-
rithmically in the inverse accuracy (as opposed to e.g. polynomially).

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
241
6.1.1
Our results
A framework for faster DDBM. We develop a new framework for solving the DDBM problem of
computing an ϵ-approximate maximum matching in a dynamic graph undergoing edge deletions
from an adaptive adversary.
Our framework provides a reduction from this DDBM problem to
solving various regularized formulations of box-simplex games.
To illustrate the reduction, suppose we have a bipartite graph G = (V, E), and suppose, for
simplicity, that we know M ∗, the size of the (maximum cardinality) matching. As demonstrated in
Chapter 5, solving the ℓ1 regression problem minx∈M ∗∆m −c⊤x + ∥Ax −b∥1, to ϵM ∗additive accu-
racy for appropriate choices of A, b, and c yields an ϵ-approximate maximum cardinality matching.
Intuitively, A and b penalize violations of the matching constraints, and c is a multiple of the all-ones
vector capturing the objective of maximizing the matching size. However, ℓ1 regression objectives do
not necessarily have unique minimizers: as such the output of directly minimizing these objectives is
diﬃcult to characterize beyond (approximate) optimality. This induces diﬃculty in using solutions
to such problems directly in dynamic graph algorithms.
Our ﬁrst key observation (building upon intuition from congestion balancing [73]) is that, beyond
enabling faster runtime guarantees, regularization provides more robust solutions which are resilient
to edge deletions in dynamic applications. We show that if
x∗
ϵ :=
min
x∈M ∗·∆m −c⊤x + ∥Ax −b∥1 + ϵH(x)
(6.5)
is the solution to the regularized box-simplex formulation of bipartite matching, then x∗
ϵ enjoys
favorable stability properties allowing us to argue about its size under deletions.
The stability of solutions to (6.5) is fairly intuitive: the entropy regularizer H(x) encourages the
objective to spread the matching uniformly among edges, when all else is held equal. For example,
when G is a complete bipartite graph on 2n vertices, standard linear programming relaxations of
matching do not favor either of (i) an integral perfect matching, and (ii) a fractional matching
spreading mass evenly across many edges, over the other. However, using (i) as our approximate
matching on a dynamic graph undergoing deletions is substantially more unstable; an adaptive
adversary can remove edges corresponding to our matching, forcing Ω(n) recomputations. On the
other hand, no edge deletions can cause this type of instability for strategy (ii): as each edge receives
weight 1
n in the fractional matching, the only way to reduce the fractional matching size by ϵn is
to remove O(ϵn2) edges: thus O(ϵ−1) recomputations are (intuitively) suﬃcient for maintaining an
ϵ-approximate maximum matching. This distinction underlies the use of high-accuracy solvers in
our reduction; indeed, while they obtain large matching values in an original graph, approximate
solutions may not carry the same types of dynamic matching value stability. We note similar intuition
motivated the approach in [73].
To make this argument more rigorous, consider using x∗
ϵ as our approximate matching for a

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
242
number of iterations corresponding to edge deletions, until its size restricted to the smaller graph
has decreased by a factor of 1 −O(ϵ).
By using strong convexity of (6.5) in the ℓ1 norm, we
argue that whenever the objective value of x∗
ϵ has worsened, the maximum matching size itself must
have gone down by a (potentially much smaller) amount. A tighter characterization of this strong
convexity argument shows that we only need to recompute a solution to slight variants of (6.5)
roughly eO(ϵ−2) times throughout the life of the algorithm. Combined with accelerated eO( m
ϵ )-time
solvers for regularized box-simplex games (which are slight modiﬁcations of (6.5)), this strategy
yields an overall runtime of eO( m
ϵ3 ), improving upon the recent state-of-the-art decremental result of
[73].
We formalize these ideas in Section 6.3, where we demonstrate that a range of regularization
strategies (see Deﬁnition 23) such as (6.5) are amenable to this reduction.
Roughly, as long as
our regularized objective is "at least as strongly convex" as the entropic regularizer, and closely
approximates the matching value in the static setting, then it can be used in our DDBM algorithm.
Combining this framework with solvers for regularized matching problems, we give three diﬀerent
results. The ﬁrst two obtain amortized update times of roughly eO(ϵ−3), in Theorems 38 and 39 via
box-simplex games and matrix scaling, respectively (though the latter holds only for dense graphs).
We give an informal statement of the former here.
Theorem 32 (informal, see Theorem 38). Let G = (V, E) be bipartite, |V | = n, |E| = m, and
ϵ ≥poly(m−1). There is a deterministic algorithm maintaining an ϵ-approximate matching in a
dynamic bipartite graph with adversarial edge deletions running in time O(m log5 m · ϵ−3).
We note that our algorithm (deterministically) returns a fractional matching. There is a black-box
reduction from dynamic fractional matching maintenance to dynamic integral matching maintenance
contained in [527], but to our knowledge this reduction is bottlenecked at an amortized eO(ϵ−4)
runtime (see e.g. Appendix A.2, [73]). Improving this reduction is a key open problem.
High-accuracy solvers for regularized box-simplex games. To use our DDBM framework, we give
a new algorithm for solving regularized box-simplex games of the form:
min
x∈∆m max
y∈[0,1]n fµ,ϵ(x, y) := y⊤A⊤x + c⊤x −b⊤y + µH(x) −ϵ
2
 y2⊤|A|⊤x,
(6.6)
where ϵ and µ = Ω(ϵ) are regularization parameters and y2, |A| denote entrywise operations. The
regularization terms H(x) and (y2)⊤|A|⊤x in (6.6) are parts of a primal-dual regularizer proposed in
[293] (and a variation of a similar regularizer of [483]) used in state-of-the-art algorithms for approx-
imately solving (unregularized) box-simplex games. This choice of regularization enjoys favorable
regularization properties over the joint box-simplex domain, and thereby sidesteps the infamous
ℓ∞-strong convexity barrier that has limited previous attempts at accelerated algorithms for this
problem. Under relatively mild restrictions on problem parameters (see discussion at the start of
Section 6.4), we develop a high accuracy solver for (6.6), stated informally here.

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
243
Theorem 33 (informal, see Theorem 41). Given an instance of (6.6), with µ = Ω(ϵ), ∥A∥∞≤1,
and σ ≥poly(m−1) Algorithm 23 returns x with maxy∈[0,1]n fµ,ϵ(x, y) −fµ,ϵ(x⋆, y⋆) ≤σ in time
eO(nnz(A) ·
1
√µϵ) where (x⋆, y⋆) is the optimizer of (6.6).
Our solver follows recent developments in solving unregularized box-simplex games. We analyze
an approximate extragradient algorithm based on the mirror prox method of [415], and prove that
iterates of the regularized problem (6.6) enjoy multiplicative stability properties previously shown
for the iterates of mirror prox on the unregularized problem [147]. Leveraging these tools, we also
show the regularizer-operator pair satisﬁes technical conditions known as relative Lipschitzness and
strong monotonicity, thus enabling a similar convergence analysis as in [147]. This yields an eﬃcient
algorithm for solving (6.6).
Roughly, when the scale of the problem (deﬁned in terms of the matrix operator norm ∥A∥∞and
appropriate norms of b and c) is bounded,3 our algorithm for computing a high-precision optimizer
to (6.6) runs in eO(
1
√µϵ) iterations, each bottlenecked by a matrix-vector product through A. When
µ ≈ϵ, the optimizer of the regularized variant is an O(ϵ)-approximate solution to the unregularized
problem (6.1), and hence Theorem 41 recovers state-of-the-art runtimes (scaling as eO(ϵ−1)) for box-
simplex games up to logarithmic factors. We achieve our improved dependence on µ in Theorem 41
by trading oﬀthe scales of the primal and dual domains. This type of argument is well-known
for separable regularizers [111], but a key technical novelty of our paper is demonstrating a similar
analysis holds for non-separable regularizers compatible with box-simplex games e.g. the one from
[293], which has not previously been done. To our knowledge, Theorem 41 is the ﬁrst result for
solving general regularized box-simplex games to high accuracy in nearly-linear time. We develop
our box-simplex algorithm and prove Theorem 41 in Section 6.4.
Improved rates for the Sinkhorn distance objective. We apply our accelerated solver for (6.6)
in computing approximations to the Sinkhorn distance objective (6.3), a fundamental algorithmic
problem in the practice of machine learning, at a faster rate.
It is well-known that solving the
regularized Sinkhorn problem (6.3) with µ scaling much larger than the target accuracy ϵ enjoys
favorable properties in practice [151] (compared to its unregularized counterpart, the standard OT
distance). In [24], the authors show that Sinkhorn iteration studied in prior work solves (6.3) to
additive accuracy ϵ at an unaccelerated rate of eO( 1
µϵ). For completeness we provide a proof of this
result (up to logarithmic factors) in Appendix E.3.3.
As a straightforward application of the solver we develop for (6.6), we demonstrate that we can
attain an accelerated rate of eO(
1
√µϵ) for approximating (6.3) to additive accuracy ϵ via a ﬁrst-order
method. More speciﬁcally, the following result is based on reducing the "explicitly constrained"
Sinkhorn objective (6.3) to a "soft constrained" regression variant of the form (6.4), where our box-
simplex game solver is applicable. We now state our ﬁrst result on improved rates for approximating
Sinkhorn distance objectives.
3Our runtimes straightforwardly extend to depend appropriately on these norms in a scale-invariant way.

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
244
Theorem 34 (informal, see Theorem 85). Let µ ∈[Ω(ϵ), O( ∥c∥∞
log m)] in (6.3) corresponding to a
complete bipartite graph with m edges. There is an algorithm based on the regularized box-simplex
game solver of Theorem 41 which obtains an ϵ-approximate minimizer to (6.3) in time eO(m · ∥c∥∞
√µϵ ).
By leveraging the particular structure of the Sinkhorn distance and its connection to a primitive
in scientiﬁc computing and theoretical computer science known as matrix scaling [367, 145, 552],
we give a further-improved solver for (6.3) in Theorem 86. This solver has a nearly-linear runtime
scaling as eO( 1
µ), which is a high-precision solver for the original Sinkhorn objective.
Our high-
precision Sinkhorn solver applies powerful second-order optimization tools from [145] based on the
box-constrained Newton's method for matrix scaling, yielding our second result on improved Sinkhorn
distance approximation rates.
Theorem 35 (informal, see Theorem 86). Let µ, ϵ = O(∥c∥∞) in (6.3) corresponding to a complete
bipartite graph with m edges. There is an algorithm based on the matrix scaling solver of [145] which
obtains an ϵ-approximate minimizer to (6.3) in time eO(m · ∥c∥∞
µ
).
We present both Theorems 85 and 86 because they follow from somewhat incomparable solver
frameworks. While the runtime of Theorem 85 is dominated by that of Theorem 86, it is a direct
application of a more general solver (Theorem 41), which also applies to regularized regression or
box-simplex objectives where the optimum does not have a characterization as a matrix scaling.
Moreover, the algorithm of Theorem 86 is a second-order method which leverages recent advances
in solving Laplacian systems, and hence may be less practical than its counterpart in Theorem 85.
Finally, we note that due to subtle parameterization diﬀerences for our DDBM applications, the
DDBM runtime attained by using our box-simplex solver within our reduction framework is more
favorable on sparse graphs (m ≪n2), compared to that obtained by the matrix scaling solver.
Since our results on optimizing Sinkhorn distances follow from machinery developed in this
paper and [145], we defer them to Appendix E.3.
For consistency with the optimal transport
literature and ease of presentation, we state our results in Appendix E.3 for instances of (6.3)
corresponding to complete bipartite graphs. However, our algorithms based on Theorem 41 extend
naturally to sparse graphs, as do the matrix scaling algorithms of [145] as discussed in that work.
We provide applications of these subroutines to the DDBM problem on sparse graphs in Section 6.3
and Appendix E.1.
A recent advancement.
Subsequent to the original submission of this paper, a breakthrough
result of [124] provided an algorithm which computes high-accuracy solutions to a variety of graph-
based optimization objectives in almost-linear time. One of the many applications of [124] is faster
algorithms for computing Sinkhorn distances. Using the algorithm designed in [124], in place of [145],
yields a speedup of Theorem 86, removing the polynomial dependence on µ−1 at the cost of an
overhead of mo(1).
Theorem 36 (informal, see Lemma 271). For ϵ = Ω(m−3) in (6.3) corresponding to a bipartite

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
245
graph with m edges, there is an algorithm that obtains an ϵ-approximate minimizer to (6.3) in time
m1+o(1).
Further, we show how to use [124] to obtain improved running times for DDBM. By plugging
in this Sinkhorn distance computation algorithm into our DDBM framework (and showing it is
compatible with the form of our subproblems), we obtain an improved (randomized) DDBM solver
in terms of the ϵ−1 dependence at the cost of a mo(1) overhead.
Theorem 37 (informal, see Theorem 40). Let G = (V, E) be bipartite, |V | = n, |E| = m, and
ϵ ∈[Ω(m−3, 1). There is a randomized algorithm with success probability 1 −n−Ω(1) maintaining an
ϵ-approximate maximum matching in an (adaptive) decremental stream running in time m1+o(1)ϵ−2.
We believe this result highlights the versatility and utility of our framework for solving DDBM.
Given the varied technical machinery involved (and diﬀering runtimes and use of randomness) our
Sinkhorn algorithms, i.e. our new regularized box-simplex solver (see Section 6.4), [145], and [124], we
provide statements of each solver instantiated by these diﬀerent machinery in Section 6.4.3. However,
due to the subsequent nature of [124] with respect to our work, and because DDBM running times
using [145] are no better than those achieved in our paper using our regularized box-simplex solver,
the proofs of our DDBM solver based on [124] are deferred to Appendix E.1.4.
6.1.2
Prior work
Dynamic matching. Dynamic graph algorithms are an active area of research in the theoretical
computer science, see e.g. [264, 204, 73, 320, 230, 247, 5, 262, 245, 248, 408, 246] and references
therein. These algorithms have been developed under various dynamic graph models, including the
oblivious adversary model where the updates to the graph are ﬁxed in advance (i.e. do not depend on
randomness used by the algorithm), and the adaptive adversary model in which updates are allowed
to respond to the algorithm, potentially adversarially. We focus on surveying deterministic dynamic
matching algorithms, which perform equally well under oblivious and adaptive updates; for a more
in-depth discussion and corresponding developments in other settings, see [527].
Many variants of the particular dynamic problem of maintaining matchings in bipartite graphs
have been studied, such as the fully dynamic [259], incremental [258, 250], and decremental [73] cases.
However, known conditional hardness results [270, 328] suggest that attaining a polylogarithmic
update time for maintaining a ϵ-approximate fully dynamic matching may be unattainable even
for constant ϵ, prompting the study of restricted variants. The works most relevant to our paper
are those of [258], which provides a eO(ϵ−4) amortized update time algorithm for computing an
ϵ-approximate matching in incremental bipartite case, and [73], which achieves a similar eO(ϵ−4)
update time for decremental bipartite matching. Our main DDBM results, stated in Theorems 38
and 39, improve upon [73] by roughly a factor of ϵ−1 in the decremental setting.

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
246
Box-simplex games. Box-simplex games, as well as ℓ1 and ℓ∞regression, are equivalent to linear
programs in full generality [349], have widespread utility, and hence have been studied extensively
by the continuous optimization community. Here we focus on discussing near-linear time approx-
imation algorithms, i.e. algorithms which run in time near-linear in the sparsity of the constraint
matrix, potentially depending inverse polynomially on the desired accuracy. Interior point methods
solve these problems with polylogarithmic dependence on accuracy, but are second-order and often
encounter polynomial runtime overhead in the dimension (though there are exceptions, e.g. [516]
and references therein).
A sequence of early works e.g. [415, 419, 420] on primal-dual optimization developed ﬁrst-order
methods for solving games of the form (6.1). These works either directly operated on the objective
(6.1) as a minimax problem, or optimized a smooth approximation to the objective recast as a
convex optimization problem. While these techniques obtained iteration complexities near-linear in
the sparsity of the constraint matrix A, they either incurred an (unaccelerated) ϵ−2 dependence on
the accuracy ϵ, or achieved an ϵ−1 rate of convergence at the cost of additional dimension-dependent
factors. This was due to the notorious "ℓ∞strong convexity barrier" (see Appendix A, [486]), which
bottlenecked classical acceleration analyses over an ℓ∞-constrained domain.
[483] overcame this
barrier by utilizing the primal-dual structure of (6.1) through a technique called "area convexity",
obtaining a eO(ϵ−1)-iteration algorithm. Since then, [147] demonstrated that ﬁne-grained analyses
of the classical algorithms of [415, 420] also obtain comparable rates for solving (6.1).
Finally,
we mention that area convexity has found applications in optimal transport and positive linear
programming [293, 91].
Sinkhorn distances. Since [151] proposed Sinkhorn distances for machine learning applications,
a ﬂurry of work has aimed at developing algorithms with faster runtimes for (6.3). A line of work
by [25, 208, 365] has analyzed the theoretical guarantees of the classical Sinkhorn matrix scaling
algorithm for this problem, due to the characterization of its solution as a diagonal rescaling of a ﬁxed
matrix. These algorithms obtain rates scaling roughly as eO(∥c∥2
∞ϵ−2) for solving (6.3) to additive
accuracy ϵ. Perhaps surprisingly, to our knowledge no guarantees for solving (6.3) which improve as
the regularization parameter µ grows are currently stated in the literature, a shortcoming addressed
by this work. Finally, we remark that Sinkhorn iteration has also received extensive treatment from
the theoretical computer science community, e.g. [367], due to connections with algebraic complexity;
see [233] for a recent overview of these connections.
6.2
Preliminaries
General notation. We denote [n] := {1, 2, . . . , n} and let 0 and 1 denote the all-0 and all-1 vectors.
Given v ∈Rd, vi or [v]i denotes the ith entry of v, and for any subset E ⊆[d] we use vE or [v]E
to denote the vector in Rd zeroing out v on entries outside of E. We use ([v]i)+ = max([v]i, 0) to

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
247
denote the operation truncating negative entries. We use v ◦w to denote elementwise multiplication
between any v, w ∈Rd. Given matrix A ∈Rm×n, we use Aij to denote its (i, j)th entry, and denote
its ith row and jth column by Ai: and A:j respectively; its nonzero entry count is nnz(A). We use
diag (v) to denote the diagonal matrix where [diag (v)]ii = vi, for each i. Given two quantities
M and M ′, for any c > 1 we say M is a c-approximation to M ′ if it satisﬁes 1
cM ′ ≤M ≤cM ′.
For ϵ ≪1, we say M is an ϵ(-multiplicative)-approximation of M ′ if (1 −ϵ)M ′ ≤M ≤(1 + ϵ)M ′.
Throughout the paper, we use |A| to denote taking the elementwise absolute value of a matrix A,
and v2 to denote the elementwise squaring of a vector v when clear from context.
Norms. ∥·∥p denotes the ℓp norm of a vector or corresponding operator norm of a matrix. In
particular, ∥A∥∞= maxi ∥Ai:∥1. We use ∥·∥interchangeably with ∥·∥2. We use ∆m to denote an
m-dimensional simplex, i.e. x ∈∆m ⇐⇒x ∈Rd
≥0, ∥x∥1 = 1.
Graphs. A graph G = (V, E) has vertices V and edges E; we abbreviate n := |V | and m := |E|
whenever the graph is clear from context. For bipartite graphs, V = L ∪R denotes the bipartition.
We let B ∈{0, 1}E×V be the (unsigned edge-vertex) incidence matrix with Bev = 1 if v is an
endpoint of e and Bev = 0 otherwise.
Bregman divergence. Given any convex distance generating function (DGF) q(x), we use V q
x′(x) =
q(x) −q(x′) −⟨∇q(x′), x −x′⟩≥0 as its induced Bregman divergence. When the DGF is clear from
context, we abbreviate V := V q. By deﬁnition, V satisﬁes
⟨−∇Vx′(x), x −u⟩= Vx′(u) −Vx(u) −Vx′(x) for any x, x′, u.
(6.7)
Computational model. We use the standard word RAM model, where one can perform each basic
arithmetic operations on O(log n)-bit words in constant time.
6.3
Dynamic decremental bipartite matching
Here we provide a reduction from maintaining an approximately maximum matching in a decre-
mental bipartite graph to solving regularized matching problems to suﬃciently high precision. In
Section 6.3.1 we give this framework and then, in Section 6.3.2, we provide various instantiations of
our framework based on diﬀerent solvers, to demonstrate its versatility.
6.3.1
DDBM framework
Here we provide our general framework for solving DDBM, which assumes that for bipartite G =
(V, E) and approximate matching value M there is a canonical regularized matching problem with
properties given in Deﬁnition 23; we later provide multiple such examples. Throughout this section,
MCM(E) denotes the size of the maximum cardinality matching on edge set E; the vertex set V is
ﬁxed throughout, so we omit it in deﬁnitions.

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
248
Deﬁnition 23 (Canonical regularized objective). Let G = (V, E0) be a bipartite graph and M ≥0
be an 8-approximation of MCM(E0). For all E ⊆E0 with MCM(E) ≥M
8 , let fM,E : RE
≥0 →R,
νE := min
x∈RE
≥0
fM,E(x) , and xE := argminx∈RE
≥0fM,E(x).
(6.8)
We call the set of {fM,E}MCM(E)≥M
8 a family of (ϵ, β)-canonical regularized objectives (CROs) for
G(E0) and M if the following four properties hold.
1. For all E ⊆E0 with MCM(E) ≥M
8 , −νE is an ϵ
8-approximation of MCM(E).
2. For all E ⊆E0 with MCM(E) ≥M
8 , fM,E is equivalent to fM,E0 with the extra constraint that
xE0\E is ﬁxed to 0 entrywise.
3. For any E′ ⊆E ⊆E0 with MCM(E) ≥M
8 and MCM(E′) ≥M
8 ,
fM,E′(xE′) −fM,E(xE) ≥βV H
xE(xE′) where H(x) :=
X
e
xe log xe
(6.9)
4. For any x ∈RE
≥0 such that 8Mx is a feasible matching on (V, E),
8M ∥xE∥1 −
ϵ
128M ≤−fM,E(x) ≤8M ∥xE∥1 +
ϵ
128M.
(6.10)
We further deﬁne the following notion of a canonical solver for a given CRO, which solves the
CRO to suﬃciently high accuracy, and rounds the approximate solution to feasibility.
Deﬁnition 24 (Canonical solver). For (ϵ, β)-CROs {fM,E}MCM(E)≥M
8 , we call A an (ϵ, T )-canonical
solver if it has subroutines Solve and Round running in O(T ) time, satisfying:
1. Solve ﬁnds an approximate solution ˆxE of fM,E satisfying

1 + ϵ
8

νE ≤fM,E(ˆxE) ≤

1 −ϵ
8

νE.
(6.11a)
ˆxE −xE
1 ≤
ϵ
1100.
(6.11b)
2. Round takes ˆxE and returns ˜xE where 8M ˜xE is a feasible matching for G(E), and:

1 + ϵ
8

νE ≤fM,E(˜xE) ≤

1 −ϵ
8

νE.
(6.12a)
˜xE ≤ˆxE monotonically.
(6.12b)
Our DDBM framework, Algorithm 20, uses CRO solvers satisfying the approximation guaran-
tees of Deﬁnition 24 to dynamically maintain an approximately maximum matching. We state its
correctness and runtime in Proposition 18, and defer a proof to Appendix E.1.1.

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
249
Algorithm 20: DecMatching(ϵ, G = (V, E))
1 Input: ϵ ∈(0, 1
8), graph G = (V, E) ;
2 Parameters: Family of CROs {fM,E}E is MP, an (ϵ, T )-canonical solver (Solve, Round);
3 Compute M with 1
2MCM(E) ≤M ≤MCM(E), via the greedy algorithm;
4 ˆxE ←Solve(fM,E);
5 ˜xE ←Round(ˆxE);
6 Mest ←M;
7 while Mest > 1
4M do
8
Edel ←∅;
9
while edge e is deleted and ∥˜xE
Edel∥1 ≤ϵ
8
˜xE
1 do
; ▷recompute whenever the deleted approximate matching size reaches a factor Θ(ϵ)
10
Edel ←Edel ∪{e};
11
E ←E \ Edel, Edel ←∅;
12
ˆxE ←Solve(fM,E);
▷ﬁnd high-accuracy minimizer of FM,E satisfying (6.11a)
and (6.11b)
13
˜xE ←Round(ˆxE);
▷round to feasible matching satisfying (6.12a) and (6.12b)
14
Compute Mest with 1
2MCM(E) ≤Mest ≤MCM(E), via the greedy algorithm
In the following, we let E0 be the original graph's edge set, and E1, E2, . . . , EK be the sequence
of edge sets recomputed in Line 11, before termination for EK+1 on Line 7.
Proposition 18. Let ϵ ∈(0, 1) and M ≥0. Given a family of (ϵ, β)-CROs {fM,E} for G = (V, E0),
and an (ϵ, T )-canonical solver for the family, Algorithm 20 satisﬁes the following.
1. When Mest > 1
4M on Line 7, where Mest estimates MCM(Ek): at any point in the loop of
Lines 9 to 10, 8M ˜xEk
Ek\Edel is an ϵ-approximate matching of G(V, Ek \ Edel).
2. When Mest ≤1
4M on Line 7, where Mest estimates MCM(E): MCM(E) ≤1
2MCM(E0).
The runtime of the algorithm is O(m + (T + m) · M
βϵ).
Proof sketch. We summarize proofs of the two properties, and our overall runtime bound.
Matching approximation properties. By the greedy matching guarantee in Line 7, it holds that
for any Ek (the edge set recomputed in the kth iteration of Line 11 before termination), its true
matching size MCM(Ek) must be no smaller than M
4 . Consequently, we can use the CRO family to
approximate the true matching size up to O(ϵ) multiplicative factors, and by the guarantee (6.12a),
this implies 8M ˜xEk
Ek\Edel is an O(ϵ) approximation of the true matching size. Also, our algorithm's
termination condition and the guarantee on Mest immediately imply MCM(EK+1) ≤1
2MCM(E0).
Iteration bound. We use a potential argument. Given Ek+1 ⊂Ek, corresponding to consecutive

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
250
edge sets requiring recomputation, we use the following inequalities:
fM,Ek+1
 xEk+1
−fM,Ek
 xEk (i)
≥βV H
xEk
 xEk+1
(ii)
≥β
X
i∈Edel
 [xEk+1]i log[xEk+1]i −[xEk]i log[xEk]i −
 1 + log[xEk]i

·
 [xEk+1]i −[xEk]i

(iii)
= β
X
i∈Edel
[xEk]i
(iv)
≥β
ˆxEk
Edel

1 −
ˆxEk −xEk
1
 (v)
≥β
˜xEk
Edel

1 −
ˆxEk −xEk
1

,
(6.13)
where (i) uses the third property in (6.9), (ii) uses convexity of the scalar function c log c, (iii)
uses that xEk+1
Edel
is 0 entrywise, (iv) uses the triangle inequality, and (v) uses the monotonicity
property (6.12b). Moreover, between recomputations we have that the ℓ1-norm of deleted edges
satisﬁes
˜xEk
Edel

1 = Ω(ϵ), and our solver guarantees
ˆxEk −xEk
1 = O(ϵ). Since the overall function
decrease before termination is O(M) given the stopping criterion in Line 7, the algorithm terminates
after O( M
βϵ) recomputations.
Using this generic DDBM framework, we obtain improved decremental matching algorithms by
deﬁning families of CROs fM,E(G) with associated (ϵ, T )-canonical solvers satisfying Deﬁnition 24.
In Appendix E.1.2, we give a regularized primal-dual construction of fM,E, and adapt the solver
of Section 6.4 to develop a canonical solver for the family (speciﬁcally, as the subroutine Solve).
Similarly, in Appendix E.1.3, we show how to construct an appropriate family of fM,E using Sinkhorn
distances, and apply the matrix scaling method presented in Appendix E.3.2 (based on work of [145])
to appropriately instantiate Solve.
While both algorithms, as stated, only maintain an approximate fractional matchings, this frac-
tional matching can be rounded at any point to an explicit integral matching via e.g. the cycle-
canceling procedure of Proposition 3 in [39] in time O(m log m), or dynamically (albeit at amor-
itized cost eO(ϵ−4) using [527]).
Moreover, our algorithm based on the regularized box-simplex
solver (Theorem 38) is deterministic, and both work against an adaptive adversary. Repeatedly
applying Proposition 18, we obtain the following overall claim.
Corollary 17. Let G = (V, E(G)) be bipartite, and suppose for any subgraph (V, E0 ⊆E(G)),
we are given a family of (ϵ, β)-CROs and an (ϵ, T )-canonical solver for the family.
There is a
deterministic algorithm maintaining a fractional ϵ-approximate matching in a dynamic bipartite
graph with adversarial edge deletions, running in time O

m log3 n + (T + m) · M
βϵ · log n

.
Proof. It suﬃces to repeatedly apply Proposition 18 until we can safely conclude MCM(E) = 0,
which by the second property can only happen O(log n) times.

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
251
6.3.2
DDBM solvers
In this section, we demonstrate the versatility of the DDBM framework in Section 6.3.1 by instantiat-
ing it with diﬀerent classes of CRO families, and applying diﬀerent canonical solvers on these families.
By using regularized box-simplex game solvers developed in this paper (see Section 6.4), we give
an eO(mϵ−3) time algorithm for maintaining a ϵ-multiplicatively approximate fractional maximum
matching in a m-edge bipartite graph undergoing a sequence of edge deletions, improving upon the
previous best running time of eO(mϵ−4) [73]. We also use our framework to obtain diﬀerent decremen-
tal matching algorithms with runtime eO(n2ϵ−3) and O(m1+o(1)ϵ−2), building on recent algorithmic
developments in the literature on matrix scaling. The former method uses box-constrained Newton's
method solvers for matrix scaling problems in [145] (these ideas are also used in Appendix E.3.2),
and the latter uses a recent almost-linear time high-accuracy Sinkhorn-objective solver in [124], a
byproduct of their breakthrough maximum ﬂow solver. We defer readers to corresponding sections
in Appendix E.1 for omitted proofs.
Given a bipartite graph initialized at G = (V, E0) with unsigned incidence matrix B ∈{0, 1}E×V ;
we denote n := |V | and m := |E0|. The ﬁrst family of CROs one can consider is the regularized
box-simplex game objective in form:
min
(x,ξ)∈∆E+1
max
y∈[0,1]V fM,E(x, ξ, y) := −1⊤
E(8Mx) −y⊤ 8MB⊤x −1

+ γxH(x, ξ) + γy  y2⊤B⊤x,
where γx = eΘ (ϵM) , γy = Θ (ϵM) ,
and
fM,E(x) :=
min
ξ|(x,ξ)∈∆E+1
max
y∈[0,1]V fM,E(x, ξ, y)
(6.14)
We prove this is a family of (ϵ, γx)-CROs (Lemma 267). The canonical solver for this family uses
RemoveOverﬂow (Algorithm 4, [39]) as Round and uses the regularized box-simplex games developed
later in this paper (see Section 6.4) as Solve, which ﬁnds an ϵ-approximate solution of (6.14) in time
eO( m
ϵ ). Combining all these components with the DDBM framework in Corollary 17 leads to the
following DDBM solver based on regularized box-simplex games.
Theorem 38. Let G = (V, E) be bipartite and let ϵ ∈[Ω(m−3), 1).
There is a deterministic
algorithm for the DDBM problem which maintains an ϵ-approximate matching, based on solving
regularized box-simplex games, running in time O
 mϵ−3 log5 n

.
Our second CRO family is the following regularized Sinkhorn distance objective:
min
(
x
xdum )∈R e
E
≥0 | 2|R|eB⊤(
x
xdum )=d
f sink
M,E(xtot) := 2|R|1⊤
Ex + γH
 x, xdum
where γ = eΘ (ϵM) ,
f sink
M,E(x) :=
min
xdum∈RE\E0
≥0
f sink
M,E(x, xdum),
(6.15)
where we extend graph G = (V, E) to a balanced bipartite graph eG = (eV , eE) by introducing dummy

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
252
vertices and edges. The extended graph allows us to write the inequality constraint B⊤x = 1V
equivalently as the linear constraint 2|R|eB⊤(
x
xdum ) = d for some deﬁned d ∈ReV as some properly-
extended vector of 1V . This allows us to apply known matrix scaling solver to such approximating
Sinkhorn distance objective in literature.
We prove this is a family of (ϵ, γ)-CROs (Lemma 269). The canonical solver for this family uses
truncation to E as Round and uses the matrix scaling solver from [145] based on box-constrained
Newton's method as Solve, which ﬁnds an ϵ-approximate solution of (6.15) in time eO(n2/ϵ). Com-
bining all these components with the DDBM framework in Corollary 17 leads to the following DDBM
solver based on approximating Sinkhorn distances.
Theorem 39. Let G = (V, E) be bipartite and ϵ ∈[Ω(m−3), 1). There is a randomized algorithm for
the DDBM problem which maintains an ϵ-approximate matching with probability 1 −n−Ω(1), based
on matrix scaling solver of [145], running in time eO
 n2ϵ−3
.
Alternatively, for the same (ϵ, γ)-CRO family as in (6.15), one can use the same Round procedure
and the recent high-accuracy almost-linear time graph ﬂow problems solver of [124] for Solve as
a canonical solver. Since this new solver can ﬁnd high-accuracy solutions of entropic-regularized
problems of the form (6.15) within a runtime of (|E0| + O(|V |))1+o(1) = m1+o(1), this gives a third
DDBM solver, which yields and improved an dependence on ϵ−1.
Theorem 40. Let G = (V, E) be bipartite and ϵ ∈[Ω(m−3), 1). There is a randomized algorithm for
the DDBM problem which maintains an ϵ-approximate matching with probability 1 −n−Ω(1), based
on the Sinkhorn objective solver of [124], running in time m1+o(1)ϵ−2.
6.4
Regularized box-simplex games
In this section, we develop a high-accuracy solver for regularized box-simplex games:
min
x∈∆m max
y∈[0,1]n fµ,ϵ(x, y) := y⊤A⊤x + c⊤x −b⊤y + µH(x) −ϵ
2
 y2⊤|A|⊤x,
where H(x) :=
X
i∈[m]
xi log xi is the standard entropic regularizer,
(6.16)
where we recall absolute values and squaring act entrywise.
For ease of presentation, we make the following assumptions for some δ > 0.
1. Upper bounds on entries: ∥A∥∞≤1, ∥b∥∞≤Bmax, ∥c∥∞≤Cmax. For simplicity, we assume
Bmax ≥Cmax ≥1; else, Cmax ←max(1, Cmax) and Bmax ←max(Cmax, Bmax).
2. Lower bounds on matrix column entries: maxi |Aij| ≥δ for every j ∈[n].

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
253
We defer the detailed arguments of why these assumptions are without loss of generality to Ap-
pendix E.2. Our algorithm acts on the induced (monotone) gradient operator of the regularized
box-simplex objective (6.16), namely (∇xfµ,ϵ(x, y), −∇yfµ,ϵ(x, y)), deﬁned as
gµ,ϵ(x, y) :=

Ay + c + µ(1 + log(x)) −ϵ
2 |A| (y2), −A⊤x + b + ϵ · diag (y) |A|⊤x

.
(6.17)
Further, it uses the following joint (non-separable) regularizer of
rµ,ϵ(x, y) := ρ
X
i∈[m]
xi log xi + 1
ρx⊤|A|
 y2
where ρ =
r
2µ
ϵ ,
(6.18)
variants of which have been used in [483, 293, 39, 147]. When clear from context, we drop subscripts
and refer to these as operator g and regularizer r. Our method is the ﬁrst high-accuracy near-linear
time solver for the regularized problem (6.16), yielding an O(ε)-approximate solution with a runtime
scaling polylogarithmically in problem parameters and ε. We utilize a variant of the mirror prox
(extragradient) [415] method for strongly monotone objectives, which appeared in [111, 147] for
regularized saddle point problems with separable regularizers.
In Section 6.4.1, we present high-level ideas of our algorithm, which uses a mirror prox outer
loop (Algorithm 21) and an alternating minimization inner loop (Algorithm 22) to implement outer
loop steps; we also provide convergence guarantees. In Section 6.4.2, we state useful properties of
the regularizer (6.18), and discuss a technical detail ensuring iterate stability in our method. In
Section 6.4.3, we provide our full algorithm for regularized box-simplex games, Algorithm 23 and
give guarantees in Theorem 41. Omitted proofs are in Appendix E.2.
6.4.1
Algorithmic framework
In this section, we give the algorithmic framework we use to develop our high-precision solver, which
combines an outer loop inspired by mirror prox [415] with a custom inner loop for implementing
each iteration. We ﬁrst deﬁne an approximate solution for a proximal oracle.
Deﬁnition 25 (Approximate proximal oracle solution). Given a convex function f over domain
Z and ε ≥0, we say z′ ∈Z is a ε-approximate solution for a proximal oracle if z′ satisﬁes
⟨∇f(z′), z′ −z⟩≤ε. We denote this approximation property by z′ ←ε argminz∈Zf.
We employ such approximate solutions as the proximal oracle within our "outer loop" method.
Our outer loop is a variant of mirror prox (Algorithm 21) which builds upon both the mirror prox
type method in [483] for solving unregularized box-simplex games and the high-accuracy mirror prox
solver developed in [111, 147] for bilinear saddle-point problems on geometries admitting separable
regularizers.
We ﬁrst give a high-level overview of the analysis, which requires bounds on two

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
254
properties. First, suppose g is ν-strongly monotone with respect to regularizer r, i.e.
for any w, z ∈Z, ⟨g(w) −g(z), w −z⟩≥ν ⟨∇r(w) −∇r(z), w −z⟩.
(6.19)
Further, suppose it is α-relatively Lipschitz with respect to r and Algorithm 23 (see Deﬁnition 1 of
[147]), i.e. for any consecutive iterates zk−1, zk−1/2, zk of our algorithm,4

g(zk−1/2) −g(zk−1)), zk−1/2 −zk

≤α

Vzk−1/2(zk) + Vzk−1(zk−1/2)

.
(6.20)
With these assumptions, we show that the strongly monotone mirror prox step makes progress
by decreasing the divergence to optimal solution Vzk(z⋆) by a factor of
α
α+ν : this implies eO( α
ν )
iterations suﬃce for ﬁnding a high-accuracy solution. We provide the formal convergence guarantee
of MirrorProx in Proposition 19, which also accommodates the error of each approximate proximal
step used in the algorithm. This convergence guarantee is generic and does not rely on the concrete
structure of g, r in our box-simplex problem.
Algorithm 21: MirrorProx()
1 Input:
ε
2-approximate proximal oracle, operator and regularizer pair (g, r) such that g is
ν-strongly monotone and α-relatively Lipschitz with respect to r;
2 Parameters: Number of iterations K;
3 Output: Point zK with VzK(z⋆) ≤(
α
ν+α)KΘ + ε
ν ;
4 z0 ←argminz∈Zr(z) ;
5 for k = 1, . . . , K do
6
zk−1/2 ←ε/2 argminz∈Z

⟨g (zk−1) , z⟩+ αVzk−1(z)
	
;
7
zk ←ε/2 argminz∈Z

g
 zk−1/2

, z

+ αVzk−1(z) + νVzk−1/2(z)
	
;
8 return zK
Proposition 19 (Convergence of Algorithm 21). Given regularizer r with range at most Θ, suppose
g is ν-strongly-monotone with respect to r (see (6.19)), and is α-relatively-Lipschitz with respect to
r (see (6.20)). Let zK be the output of Algorithm 21. Then, V r
zK(z⋆) ≤

α
ν+α
K
Θ + ε
ν .
Given the somewhat complicated nature of our joint regularizer, we cannot solve the proximal
problems required by Algorithm 21 in closed form. Instead, we implement each proximal step to the
desired accuracy by using an alternating minimization scheme, similarly to the implementation of
approximate proximal steps in [483, 293].
To analyze our algorithm, we use a generic progress guarantee for alternating minimization
from [293] to solve each subproblem, stated below.
4This property (i.e. relative Lipschitzness restricted to iterates of the algorithm) was referred to as "local relative
Lipschitzness" in [147], but we drop the term "local" for simplicity.

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
255
Algorithm 22: AltMin(γx, γy, A, θ, T, xinit, yinit)
1 Input: A ∈Rm×n
≥0
, γx ∈Rm, γy ∈Rn, T ∈N, θ > 0, xinit ∈∆m, yinit ∈[0, 1]n;
2 Output: Approximate minimizer to ⟨(γx, γy), z⟩+ θr(z) for r(z) in (6.18);
3 x(0) ←xinit, y(0) ←yinit;
4 for 0 ≤t ≤T do
5
x(t+1) ←argminx∈∆m

⟨γx, x⟩+ θr(x, y(t))
	
;
6
y(t+1) ←argminy∈[0,1]n

⟨γy, y⟩+ θr(x(t+1), y)
	
;
7 Return: (x(T +1), y(T ));
Lemma 81 (Alternating minimization progress, Lemma 5 and Lemma 7, [293]). Let r : X ×Y →R
be jointly convex, θ > 0, and γx and γy be linear operators on X, Y. Deﬁne
xOPT, yOPT = argminx∈X argminy∈Yf(x, y) := ⟨γx, x⟩+ ⟨γy, y⟩+ θr(x, y).
(6.21)
Suppose f(x, y) is twice-diﬀerentiable and satisﬁes: for all x′ ≥1
2x entrywise, x′, x ∈X and y′, y ∈
Y, ∇2f(x′, y′) ⪰1
κf(x, y). Then the iterates of Algorithm 22 satisfy
f(x(t+2), y(t+1)) −f(xOPT, yOPT) ≤

1 −1
2κ
 
f(x(t+1), y(t)) −f(xOPT, yOPT)

.
Combining this lemma with the structure of our regularizer (6.18), we obtain the following
guarantees, showing Algorithm 22 ﬁnds a ε
2-approximate solution to the proximal oracle.
Corollary 18 (Convergence of Algorithm 22). Let δ, ε ∈(0, 1), ρ ≥1.
Suppose we are given
γ ∈Z∗= X∗× Y∗with max(∥γx∥∞, ∥γy∥1) ≤B, and deﬁne the proximal subproblem solution
xOPT, yOPT = argminx∈∆margminy∈[0,1]nf(x, y) := ⟨γx, x⟩+ ⟨γy, y⟩+ θr(x, y) for some θ > 0.
If the Hessian condition in Lemma 81 holds with a constant κ > 0, and all simplex iterates x of
Algorithm 22 satisfy x ≥δ elementwise, then the algorithm ﬁnds a ε
2-approximate solution to the
proximal oracle within T = O

log

ρ(B+mnθ)2
δεθ

iterations.
6.4.2
Helper lemmas
Before providing our full method and analysis, here we list a few helper lemmas, which we rely on
heavily in our later development. The ﬁrst characterizes a useful property of rµ,ϵ, showing that its
Hessian is locally well-approximated by a diagonal matrix, which induces appropriate local norms
for the blocks x ∈X, y ∈Y. We use this to prove the "strong monotonicity" (Lemma 85) and
"relative Lipschitzness" (Lemma 86) bounds required in Section 6.4.3.

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
256
Lemma 82 (Bounds on regularizer). Suppose A ∈Rm×n has ∥A∥∞≤1. For any z = (x, y) ∈
∆m × [0, 1]n, r = rµ,ϵ deﬁned as in (6.18), and ¯x ∈Rm
>0, ⟨x, Ay⟩≤∥x∥diag( 1
¯x) ∥y∥diag(|A|⊤¯x) .
Further, if ρ ≥3, the matrix
D(x) :=


ρ
2diag
  1
x

0
0
1
ρdiag

|A|⊤x



(6.22)
satisﬁes the following relationship with the Hessian matrix of r(z):
D(x) ⪯∇2r(z) =

ρ · diag
  1
x

2
ρAdiag (y)
2
ρdiag (y) A⊤
2
ρdiag

|A|⊤x


⪯4D(x).
(6.23)
We also introduce the following notion of a padding oracle (cf. Deﬁnition 2 of [112]), which helps
us control the multiplicative stability of iterates when running our algorithm.
Deﬁnition 26. Given δ > 0, and any ¯z = (¯x, y) ∈∆m × [0, 1]n, a padding oracle Oδ returns
z = (x, y) by setting ˆxi = max(¯xi, δ) coordinate-wise and letting x =
ˆx
∥ˆx∥1 .
This padding oracle has two merits which we exploit. First, the error incurred due to padding is
small proportional to the padding size δ, which ﬁnds usage in proving the correctness of our main
algorithm, Algorithm 23 (see Proposition 20).
Lemma 83 (Error of padding, cf. Lemma 6, [112]). For δ > 0 and ¯z = (¯x, y) ∈∆m × [0, 1]n
let z = (x, y) ∈∆m × [0, 1]n where x = Oδ(¯x) (Deﬁnition 26), then for r in (6.18), and any
w ∈Z = ∆m × [0, 1]n, V r
z (w) −V r
¯z (w) ≤

ρ + 8
ρ

mδ.
Second, padding ensures that the iterates of our algorithm satisfy x = Ω(δ) entrywise, i.e. no
entries of our simplex iterates x are too small. This helps ensure the stability of iterates throughout
one call of Algorithm 22, formally through the next lemma.
Lemma 84 (Iterate stability in Algorithm 22). Suppose ϵ ≤1, ρ ≥6, and α ≥36
ρ (µ log 4
δ +3Cmax).
Let (xk, yk) denote blocks of zk, the kth iterate of Algorithm 21.
In any iteration k of Algo-
rithm 21, calling Algorithm 22 to implement Line 6, if xk−1 ≥
δ
2 entrywise, x(t+1) ∈xk−1 ·

exp
 −1
9

, exp
  1
9

, for all t ∈[T]. Calling Algorithm 22 to implement Line 7, if xk−1/2 ≥
δ
4
entrywise, x(t+1) ∈x
α
α+ν
k−1 ◦x
ν
α+ν
k−1/2 ·

exp
 −1
9

, exp
  1
9

for all t ∈[T].
6.4.3
Regularized box-simplex solver and its guarantees
We give our full high-accuracy regularized box-simplex game solver as Algorithm 23, which com-
bines Algorithm 21, Algorithm 22, and a padding step to ensure stability.
In order to analyze the convergence of Algorithm 23, we begin by observing that the operator in
(6.17) satisﬁes strong monotonicity with respect to our regularizer (6.18).

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
257
Algorithm 23: RegularizedBS(A, b, c, ϵ, µ, ε)
1 Input: A ∈Rm×n, c ∈Rm, b ∈Rn, accuracy ε ∈(m−10, 1), 72ϵ ≤µ ≤1;
2 Output: Approximate solution pair (x, y) to (6.16);
3 Global: δ ←ϵε2
m2 , ρ ←
q
2µ
ϵ , ν ←1
2
p µϵ
2 , α ←18Cmax + 32
p µϵ
2 log 4
δ ;
4 Global: T ←O

log mnBmaxαρ
δε

, K ←O

α
ν log

ν log m
ε

for appropriate constants;
5 (x0, y0) ←( 1
m · 1m, 0n);
6 for k = 1 to K do
7
(γx, γy) ←GradBS(xk−1, yk−1, xk−1, yk−1, 0);
8
(xk−1
2 , yk−1
2 ) ←AltminBS(γx, γy, α, xk−1, yk−1);
9
(γx, γy) ←GradBS(xk−1
2 , yk−1
2 , xk−1, yk−1, ν);
10
(x(T +1), y(T )) ←AltminBS(γx, γy, α + ν, xk−1
2 , yk−1
2 );
11
xk ←
1
∥max(x(T +1),δ)∥1
· max
 x(T +1), δ

, yk ←y(T );
▷Implement padding Oδ(x(T +1))
12 function GradBS(x, y, x0, y0, Θ)
13
gx ←Ay + c + µ(1 + log(x)) −ϵ
2 |A| (y2);
14
gy ←−A⊤x + b + ϵdiag (y) |A|⊤x ;
15
gx
r ←−αρ(1 + log x0) −α
ρ |A|y2
0 −Θρ(1 + log x) −Θ
ρ |A|y2 ;
16
gy
r ←−2α
ρ diag (y0) |A|⊤x0 −2Θ
ρ diag (y) |A|⊤x) ;
17
return (gx + gx
r, gy + gy
r)
18 function AltminBS(γx, γy, θ, x(0), y(0)) ▷Implement approximate proximal oracle via AltMin
19
for 0 ≤t ≤T do
20
x(t+1) ←
1
exp

−1
θρ γx−1
ρ2 |A|(y(t))
2
1
· exp

−1
θργx −1
ρ2 |A|
 y(t)2
;
21
y(t+1) ←med

0, 1, −ρ
2θ ·
γy
|A|⊤x(t+1)

;
22
Return: (x(T +1), y(T ));
Lemma 85 (Strong monotonicity). Let µ ≥ϵ
2 and ρ :=
q
2µ
ϵ . The gradient operator gµ,ϵ (6.17) is
ν := 1
2
p µϵ
2 -strongly monotone (see (6.19)) with respect to rµ,ϵ deﬁned in (6.18).
Next, we show iterate stability through each loop of alternating minimization (i.e. from Line 7
to Line 8, and Line 9 to Line 10 respectively), via Lemma 84.
Corollary 19 (Iterate stability in Algorithm 23). Assume the same parameter bounds as Lemma 84,
and that δ ∈(0, m−1).
In the kth outer loop of Algorithm 23, xk−1 ≥
δ
2 entrywise.
Further,
for all iterates x(t+1) computed in Line 7 to Line 8 and xOPT as deﬁned in (6.21) with θ = α,
1
2xk−1 ≤x(t+1), xOPT ≤2xk−1, and x(t+1), xOPT ≥δ
4, entrywise. Similarly, for all iterates x(t+1)
computed in Line 9 to Line 10 and xOPT as deﬁned in (6.21) with θ = α+ν, 1
2xk−1/2 ≤x(t+1), xOPT ≤
2xk−1/2 and x(t+1), xOPT ≥δ
4, entrywise.
Under iterate stability, our next step is to prove that our operator gµ,ϵ is relatively Lipschitz
with respect to our regularizer rµ,ϵ (as deﬁned in (6.20)).

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
258
Lemma 86 (Relative Lipschitzness). Assume the same parameter bounds as in Lemma 84.
In
the kth outer loop of Algorithm 23, let ¯zk ←(x(T +1), y(T )) from Line 10 be zk before the padding
operation. Then, xk−1/2, ¯xk ∈[ 1
2xk−1, 2xk−1] elementwise and

g(zk−1/2) −g(zk−1), zk−1/2 −¯zk

≤α
 Vzk−1(zk−1/2) + Vzk−1/2(¯zk)

for α = 4 + 32
rµϵ
2 .
Next, we give a convergence guarantee on the inner loops (from Line 7 to Line 8, and Line 9
to Line 10) in Algorithm 23, as an immediate consequence of Corollary 18.
Corollary 20 (Inner loop convergence in Algorithm 23). Assume the same parameter bounds as in
Lemma 84. For γ deﬁned in Line 7, suppose for an appropriate constant T = Ω

log mnBmaxαρ
δε

.
Then, for all k iterate zk−1/2 = (xk−1/2, yk−1/2) of Line 8 satisﬁes

∇g(zk−1) + α∇Vzk−1(zk−1/2), zk−1/2 −w

≤νε
4 , for all w ∈Z.
Similarly, for γ deﬁned in Line 9, iterate ¯zk = (x(T +1), y(T )) of Line 10 satisﬁes

∇g(zk−1) + α∇Vzk−1(¯zk) + ν∇Vzk−1/2(¯zk), ¯zk −w

≤νε
4 , for all w ∈Z.
We now analyze the progress made by each outer loop of Algorithm 23. The proof is very similar
to that of Proposition 19; the only diﬀerence is controlling the extra error incurred in the padding
step of Line 11, which we bound via Lemma 83.
Proposition 20 (Convergence of Algorithm 23). Assume the same parameter bounds as in Lemma 84,
and that δ ≤
ε
4ραm. Algorithm 23 returns zK satisfying V r
zK(z⋆) ≤3ε
ν , letting (for an appropriate
constant) K = Ω( α
ν log( ν log m
ε
)).
We are now ready to prove the main theorem of this section, which gives a complete convergence
guarantee of Algorithm 23 by combining our previous claims.
Theorem 41 (Regularized box-simplex solver). Given regularized box-simplex game (6.16) with
72ϵ ≤µ ≤1 and optimizer (x⋆, y⋆), and letting ε ∈(m−10, 1), RegularizedBS (Algorithm 23) returns
xK satisfying
xK −x⋆
1 ≤
ε
Cmax log2 m and maxy∈[0,1]n fµ,ϵ(xK, y) −fµ,ϵ(x⋆, y⋆) ≤ε. The total
runtime of the algorithm is O(nnz(A) · ( Cmax
√µϵ + log( m
σϵ)) · log( Cmax log m
ε
) log( mnBmax
ε
)).
As a corollary, we obtain an approximate solver for regularized box-simplex games in the following
form (which in particular does not include a quadratic regularizer):
min
x∈∆m max
y∈[0,1]n fµ(x, y) = y⊤A⊤x + c⊤x −b⊤y + µH(x),
where H(x) :=
X
i∈[m]
xi log xi.
(6.24)

CHAPTER 6. DYNAMIC DECREMENTAL BIPARTITE MATCHING
259
Corollary 21 (Half-regularized approximate solver). Given regularized box-simplex game (6.24)
with regularization parameters 72ϵ ≤µ ≤1 and optimizer (x⋆, y⋆), and letting ϵ ∈(m−10, 1),
Algorithm 23 with ε ←
ϵ
2 returns xK satisfying maxy∈[0,1]n fµ(xK, y) −fµ(x⋆, y⋆) ≤ϵ. The total
runtime of the algorithm is O

nnz(A) · ( Cmax
√µϵ + log
  m
ϵ

) · log

Cmax log m
ϵ

log
  mnBmax
ϵ

.

Part II
Semideﬁnite Programming and
High-Dimensional Statistics
260

Chapter 7
Matrix Multiplicative Weights and
Friends
This chapter is based in part on [108, 289, 180, 287], with Yair Carmon, Ilias Diakonikolas, John C.
Duchi, Arun Jambulapati, Daniel M. Kane, Daniel Kongsgaard, Jerry Li, Christopher Musco, and
Aaron Sidford.
7.1
Introduction
In this chapter, we present several related results on variants of the matrix multiplicative weights
method [534, 34], a meta-algorithm for regret minimization over subsets of the positive semideﬁnite
cone. Using these tools, we develop faster solvers for various approximate formulations of semideﬁnite
programming, as well as new algorithms for applications in high-dimensional statistics in Chapters 8
and 9. Further exposition on the various relationships between the tools developed in this chapter,
as well as their implications, can be found in Chapter 1.
7.1.1
Sketching matrix multiplicative weights
Consider the problem of online learning over the spectrahedron ∆n, the set of n × n symmetric
positive semideﬁnite matrices with unit trace. At every time step t, a player chooses action Xt ∈∆n,
an adversary supplies symmetric gain matrix Gt, and the player earns reward ⟨Gt, Xt⟩:= tr(GtXt).
We seek to minimize the regret with respect to the best single action (in hindsight),
sup
X∈∆n
T
X
t=1

Gt, X

−
T
X
t=1

Gt, Xt

= λmax
 
T
X
t=1
Gt
!
−
T
X
t=1

Gt, Xt

.
(7.1)
261

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
262
[534, 535] solve this problem using the matrix exponentiated gradient algorithm [511], also known
as matrix multiplicative weights (MMW). It is given by
Xt = Pmw
 
η
t−1
X
i=1
Gi
!
,
where Pmw(Y) :=
eY
tr eY ,
(7.2)
and η > 0 is a step size parameter. If the operator norm ∥Gt∥∞≤1 for every t, using the MMW
strategy (7.2) with η =
p
2 log(n)/T guarantees that the regret (7.1) is bounded by
p
2 log(n)T;
this guarantee is minimax optimal up to a constant [33].
Unlike standard (vector) multiplicative weights, MMW is computational expensive to implement
in the high-dimensional setting n ≫1. This is due ot the high cost of computing matrix exponentials;
currently they require an eigen-decomposition which costs Θ(n3) with practical general-purpose
methods and Ω(nω) in theory [433]. This diﬃculty has led a number of researchers to consider a
rank-k sketch of Pmw of the form
PU(Y) := eY/2UUT eY/2
⟨eY, UUT ⟩
,
where U ∈Rn×k
(7.3)
and the elements of U are i.i.d. standard Gaussian. For k ≪n, PU is much cheaper than Pmw to
compute, since its computation requires only k products of the form eAb which can be evaluated
eﬃciently via iterative methods (see Section 7.2.3). Since we play rank-deﬁcient matrices, an adver-
sary with knowledge of Xt may choose the gain Gt to be in its nullspace, incurring regret linear in
T. To rule such an adversary out, we assume that Gt and Xt must be chosen simultaneously. We
formalize this as
Assumption 5. Conditionally on X1, G1, . . . , Xt−1, Gt−1, the gain Gt is independent of Xt.
This assumption is standard in the literature on adversarial bandit problems [99] where it is similarly
unavoidable. While it comes at signiﬁcant loss of generality, Assumption 5 holds in two important
applications, as described below.
The challenge of bias Assumption 5 allows us to write
E
"*
Gt, PUt

η
t−1
X
i=1
Gi

|{Gi}t
i=1
#
=
*
Gt, EUPU

η
t−1
X
i=1
Gi

.
However, even though U satisﬁes EUUU T = I, we have EUPU(Y) ̸= Pmw(Y) for general Y.
Therefore, the guarantees of MMW do not immediately apply to actions chosen according to the
sketch (7.3), even in expectation.
A common solution in the literature [34, 442, 19] is to pick
k = eO
 1/ϵ2
such that, by the Johnson-Lindenstrauss lemma, PU(Y) approximates Pmw(Y) to
within multiplicative error ϵ.
This makes the MMW guarantees applicable again, but requires

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
263
considerable computation per step, that will match the cost of full matrix exponentiation for suﬃ-
ciently small ϵ. [304, 20] prove regret guarantees for sketches of ﬁxed rank k ≤3 with forms diﬀerent
from (7.3); we discuss their approaches in detail later in this section.
Our approach.
In this work we use the sketch (7.3) with k = 1, playing the rank-1 matrix
Xt = Put(η Pt−1
i=1 Gi) where Pu(Y) = vvT /(vT v) for v = eY/2u and ut ∈Rn standard Gaussian.
Instead of viewing Pu as a biased estimator of Pmw, we deﬁne the deterministic function
¯P(Y) := EuPu(Y),
and view Pu as an unbiased estimator for ¯P. Our primary contribution is in showing that
¯P is nearly as good a mirror projection as Pmw.
More precisely, we show that replacing Pmw with ¯P leaves the regret bounds almost unchanged;
if ∥Gt∥∞≤1 for every t, the actions ¯Xt = ¯P(η Pt−1
i=1 Gi) guarantee (with properly tuned η)
regret of at most
p
6 log(4n)T, worse than MMW by only a factor of roughly
√
3. To prove this,
we establish that ¯P possesses the geometric properties necessary for mirror descent analysis: it is
Lipschitz continuous and its associated Bregman divergence is appropriately bounded. Since Pu
is—by deﬁnition—an unbiased estimator of ¯P, we immediately obtain (thanks to Assumption 5)
that Xt = Put(η Pt−1
i=1 Gi) satisﬁes the same regret bound in expectation. High-probability bounds
follow immediately via martingale concentration.
Application to online PCA. As our sketched actions are of the form Xt = xtxT
t , the regret they
incur is λmax
  PT
t=1 Gt

−PT
t=1 xT
t Gtxt. Therefore, the vectors xt can be viewed as streaming
approximations of the principal component1 of the cumulative matrix Pt−1
i=1 Gi. This online coun-
terpart of the classical principal component analysis problem is the topic of a number of prior works
[534, 232, 20]. Our sketch oﬀers regret bounds that are optimal up to constants, with computational
cost per step as low as any known alternative, and overall computational cost better than any in the
literature by a factor of at least log5 n. Our regret bounds hold for gains Gt of any rank or sparsity,
and our computational scheme (Section 7.2.3) naturally leverages low rank and/or sparsity in the
gains.
Application to semideﬁnite programming (SDP). Any feasibility-form SDP is reducible to the
matrix saddle-point game maxX∈∆n miny∈σm ⟨Pm
i=1 yiAi, X⟩, where σm is the simplex in Rm and
A1, . . . , Am ∈Rn×n are symmetric matrices.
A simple procedure for approximating a saddle-
point (Nash equilibrium) for this game is to have each player perform online learning, where the
max-player observes gains Gt = Pm
i=1[yt]iAi and the min-player observes costs [ct]i = ⟨Ai, Xt⟩.
Using standard/matrix multiplicative weights for the min/max players, respectively, we may produce
approximate solutions with additive error ϵ in O(log(nm)/ϵ2) iterations, with each iteration costing
1For this reason we consider gain-maximization rather than loss-minimization, which is generally more conventional.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
264
O(n3) time, due to the MMW computation. In Section 7.2.4 we show that by replacing MMW with
our sketch we guarantee ϵ error in a similar number of iterations, but with each iteration costing
eO (N/√ϵ), where N is the problem description size, which is often signiﬁcantly smaller than n2.
This guarantee matches the state-of-the-art in a number of settings.
Related work
MMW appears in a large body of work spanning optimization, theoretical computer science, and
machine learning [415, 534, 33]. Here, we focus on works that, like us, attempt to relieve the compu-
tational burden of computing the matrix exponential, while preserving the MMW regret guarantees.
To our knowledge, the ﬁrst proposal along these lines is due to [34], who apply MMW with a
Johnson-Lindenstrauss sketch to semideﬁnite relaxations of combinatorial problems. Subsequent
works on positive semideﬁnite programming adopted this technique [442, 19]. To achieve ϵ-accurate
solutions, these works require roughly ϵ−2 matrix exponential vector products per mirror projection.
[50] apply the accelerated mirror-prox scheme of [415] to matrix saddle-point problems and
approximate Pmw using the rank-k sketch (7.3). Instead of appealing to the JL lemma, they absorb
the bias and variance of this approximation directly into the algorithm's error estimates.
This
enables a more parsimonious choice of k; to attain additive error ϵ, they require k = eO
 ϵ−1
.
See Section 7.2.4 for additional discussion of the performance of this method.
A diﬀerent line of work, called Follow the Perturbed Leader (FTPL) [304], eschews matrix
exponentiation, and instead produces rank-1 actions Xt = xtxT
t , where xt is an approximate top
eigenvector of a random perturbation of Pt−1
i=1 Gi.
While a single eigenvector computation has
roughly the same cost as a single matrix-exponential vector product, the regret bounds for FTPL—
and hence also the total work—scale polynomially in the problem dimension n: [232] bound the
regret by eO
√
nT

and [211] improve the bound to eO
√
n1/2T

for gains of rank 1. In contrast,
the regret of MMW and its sketches depends on n only logarithmically.
[20] give the ﬁrst ﬁxed-rank sketch with MMW-like regret, proposing a scheme called Follow the
Compressed Leader (FTCL). Their approach is based on replacing the MMW mirror projection (7.2)
with the projection corresponding to ℓ1−1/q regularization, given by Pq-reg(Y) := (c(Y)I −Y)−q
where c(Y) is the unique c ∈R such that cI −Y ≻0 and tr[(cI −Y)−q] = 1. They use a sketch
of Pq-reg similar in spirit to (7.3) and prove that k = 3 suﬃces to obtain regret bounds within a
polylogarithmic factor of MMW, with q chosen to be roughly log n.
The basis of the FTCL proof strategy is a potential argument used to derive regret bounds for
the exact Pq-reg. Their analysis consists of carefully tracing this argument, and accounting for the
errors caused by sketching in each step of the way. In comparison, we believe our analysis is more
transparent; rather than control multiple series expansion error terms, we establish three simple
geometric properties of our projection ¯P. We also provide tighter bounds; to guarantee ϵ average
regret, FTCL requires a factor of Ω(log5(n/ϵ)) more online learning steps than our method. The

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
265
per-step computational cost of our method is similar to that of FTCL, with better polylogarithmic
dependence on n. On a practical note, the computational scheme we describe in Section 7.2.3 is
signiﬁcantly simpler to implement than the one proposed for FTCL.
7.1.2
Ky Fan matrix multiplicative weights
We give a Ky Fan k-norm (sum of k largest eigenvalues) generalization of MMW, which typically
gives operator norm guarantees. We analyze our algorithm and show that it is tolerant to the error
guarantees of approximate k-PCA procedures such as simultaneous power iteration [404]. Crucial
to our tightest runtime bounds are strengthenings of the analysis of a similar procedure found in
[136] in several places, which save multiple k factors in our guarantees and may be of independent
interest; we now highlight a few here.2
The main idea of our Ky Fan MMW regret guarantee is to bound the cost of actions {Yt}t≥0
against a sequence of positive semideﬁnite "gain matrices" {Gt}t≥0 as measured by inner products.
The actions {Yt}t≥0 are given by the algorithm (depending on the gain matrices), and live in
Y := {Y ∈Rd×d | 0 ⪯Y ⪯I, Tr(Y) = k}.
The reason for this choice of action set, the "k-Fantope," is because it satisﬁes
sup
U∈Y
⟨U, G⟩= ∥G∥k ,
where ∥·∥k is the Ky Fan k-norm, so the best action in hindsight captures this norm. Ultimately,
our ﬁltering scheme requires matrix-vector query access to each Yt, which are deﬁned by Bregman
projections onto the set Y. It was shown in [136] that the natural choice of projection, induced by
a regularizer r(Y) chosen to be matrix entropy, is a truncated exponential, where truncation occurs
on the top-k eigenspace. The bottleneck cost of iterations is computing this space.
To this end, we show new guarantees on the performance of approximate k-PCA, which allow
for their use in this process. One example is that we show roughly 1
ϵ iterations of simultaneous
power iteration on a positive semideﬁnite matrix S ∈Rd×d, resulting in approximate eigenvectors
V ∈Rd×k, are enough to guarantee (cf. Proposition 24)
(1 −ϵ)S ⪯PSP + (I −P) S (I −P) ⪯(1 + ϵ)S, where P := VV⊤.
This improves a similar analysis in [136], which showed an approximation factor of 1 ± kϵ.
The main other technical piece required by our MMW algorithm is a reﬁned divergence bound
2We believe that similar wins following from our tighter analysis apply to the algorithm of [136].

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
266
of the form (cf. Lemma 88 for a formal statement)
V r∗
S (S + ηG) ≤∥ηG∥op ⟨ηG, Y⟩, where Y := ∇r∗(S) ∈Y,
a strengthening of V r∗
S (S + ηG) ≤k ∥ηG∥2
op .
Here, V r∗is the Bregman divergence in the convex conjugate of r. The latter bound follows easily
from strong convexity of r (and hence smoothness of its dual); we require the former strengthening
so that we can use the action matrices {Yt}t≥0 to deﬁne scores, to decrease inner products.3 In
particular, the weaker bound above has no dependence on Y, so without the stronger bound it is
unclear how to use the MMW update structure to downweight.
We prove our reﬁned divergence bound by adapting arguments from previous literature [108, 285]
on using Hessian formulae of spectral functions to prove divergence bounds, whenever the conjugate
r∗is twice-diﬀerentiable, and applying the Alexandrov theorem.
Finally, up to (non-dominant)
approximation error terms, our Ky Fan MMW procedure's main guarantee can be stated as: given
a sequence of positive semideﬁnite matrices {Gt}t≥0, let step size η > 0 satisfy ηGt ⪯I for all t.
The procedure plays a sequence {Yt}t≥0 ∈Y, so that for any T ∈N,

1
T
T −1
X
t=0
Gt

k
≤2
T
T −1
X
t=0
⟨Gt, Yt⟩+ k log d
ηT
.
(7.4)
7.1.3
Matrix dictionary recovery SDPs
Positive semideﬁnite programs are a wide family of structured SDPs with numerous applications
[241, 36, 281, 353, 133, 131, 135, 136], which contain "pure packing" SDPs and "pure covering"
SDPs as special cases. We use the term positive semideﬁnite programming in this paper to refer to
the fully general mixed packing-covering SDP problem, which is parameterized by "packing" and
"covering" matrices {Pi}i∈[n], P, {Ci}i∈[n], C ∈Sd
≥0, and asks to ﬁnd the smallest µ > 0 such that
there exists w ∈Rn
≥0 with
X
i∈[n]
wiPi ⪯µP,
X
i∈[n]
wiCi ⪰C.
(7.5)
By redeﬁning Pi ←
1
µP−1
2 PiP−1
2 and Ci ←C−1
2 CiC−1
2 for all i ∈[n] and a given µ > 0, the
optimization problem in (7.5) is equivalent to testing whether there exists w ∈Rn
≥0 such that
X
i∈[n]
wiPi ⪯
X
i∈[n]
wiCi.
(7.6)
The above formulation (7.6) was studied by [283, 285], and an important open problem in the
ﬁeld of structured convex programming is designing a "width-independent" solver for testing the
3It is a strengthening since ∇r∗(S) ∈Y, so we can apply a matrix H¨older's inequality and use Tr(Y) = k, ∀Y ∈Y.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
267
feasibility of (7.6) up to a 1 + ϵ factor (namely, testing whether (7.6) is approximately feasible with
an iteration count polynomial in ϵ−1 and polylogarithmic in other problem parameters), or solving
for the optimal µ in (7.5) to this approximation factor. Up to this point, such width-independent
solvers have remained elusive beyond the case of pure packing SDPs [19, 442, 289].
This work develops an eﬃcient algorithm for solving specializations of (7.5), (7.6) where the
packing and covering matrices {Pi}i∈[n], {Ci}i∈[n], as well as the constraints P, C, are multiples of
each other. In particular, we give eﬃcient algorithms for the following special case of (7.5), (7.6).
Given a set of matrices (a "matrix dictionary") {Mi}i∈[n] ∈Sd
≥0, a constraint matrix B, and a
tolerance ϵ ∈(0, 1), such that there exists a feasible set of weights w⋆∈Rn
≥0 with
B ⪯
X
i∈[n]
w⋆
i Mi ⪯κB,
(7.7)
for some unknown κ ≥1, we wish to return a set of weights w ∈Rn
≥0 such that
B ⪯
X
i∈[n]
wiMi ⪯(1 + ϵ)κB.
(7.8)
While this "matrix dictionary recovery" problem is a restricted case of (7.5), as we demonstrate, it
is already expressive enough to capture many interesting applications.
Our results concerning the problem (7.7), (7.8) will assume the family {Mi}i∈[n] is "simple" in
the following two senses: First, we assume we have an explicit factorization of each Mi as
Mi = ViV⊤
i , Vi ∈Rd×m.
(7.9)
In our applications m = 1, but our solver also handles the general case.
Second, recalling the
shorthand M(w) := P
i∈[n] wiMi, we assume that we can approximately solve systems in M(w)+λI
for any w ∈Rn
≥0, λ ≥0. Concretely, we assume for any ϵ > 0, there is a linear operator f
Mw,λ,ϵ
which we can compute and apply in T sol
M · log 1
ϵ time4, and
 f
Mw,λ,ϵv −(M(w) + λI)−1 v

2 ≤ϵ
(M(w) + λI)−1 v

2 for all v ∈Rd.
(7.10)
Under these assumptions, we prove the following main claims in Section 7.4. We remark that we did
not try to heavily optimize logarithmic factors and the dependence on ϵ−1, as many of these factors
are inherited from subroutines from prior work.
Theorem 42. Given matrices {Mi}i∈[n] with explicit factorizations (7.9), such that (7.7) is feasible
for B = I and some κ ≥1, we can return weights w ∈Rn
≥0 satisfying (7.8) with probability ≥1 −δ
4We use this notation because, if T sol
M is the complexity of solving the system to constant error c < 1, then we can
use an iterative reﬁnement procedure to solve the system to accuracy ϵ in time T sol
M · log 1
ϵ for any ϵ > 0.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
268
in time
O
 
Tmv({Vi}i∈[n]) · κ1.5 · log6( mndκ
δϵ
)
ϵ8
!
.
Here Tmv({Vi}i∈[n]) denotes the computational complexity of multiplying an aribtrary vector by
all matrices in the set {Vi}i∈[n].
Theorem 43. Given matrices {Mi}i∈[n] with explicit factorizations (7.9), such that (7.7) is feasible
for some κ ≥1 and we can solve linear systems in linear combinations of {Mi}i∈[n] to ϵ relative
accuracy in the sense of (7.10) in T sol
M · log 1
ϵ time, and B satisfying
B ⪯M(1) ⪯αB, I ⪯B ⪯βI,
(7.11)
we can return weights w ∈Rn
≥0 satisfying (7.8) with probability ≥1 −δ in time
O
 
 Tmv
 {Vi}i∈[n] ∪{B}

+ T sol
M

· κ2 · log11( mndκβ
δϵ
)
ϵ8
· log α
ϵ
!
.
The ﬁrst condition in (10.32) is no more general than assuming we have a "warm start" reweight-
ing w0 ∈Rn
≥0 (not necessarily 1) satisfying B ⪯P
i∈[n][w0]iMi ⪯αB, by exploiting scale invariance
of the problem and setting Mi ←[w0]iMi. The second bound in (10.32) is equivalent to κ(B) ≤β
up to constant factors, since given a bound β, we can use the power method (cf. Fact 40) to shift
the scale of B so it is spectrally larger than I spectrally. The operation requires just a logarithmic
number of matrix vector multiplications with B, which does not impact the runtime in Theorem 43.
Our algorithm for Theorem 42 is based on matrix multiplicative weights (MMW) [534, 34, 33],
a popular ﬁrst-order meta-algorithm for approximately solving SDPs, with carefully chosen gain
matrices formed by using packing SDP solvers as a black box. In this sense, it is an eﬃcient reduction
from structured mixed positive instances of the form (7.7), (7.8) to pure packing instances. We note
similar ideas were previously used in [353] (repurposed in [133]) for solving graph-structured matrix
dictionary recovery problems. Our Theorems 42 and 43 improve upon these results both in generality
(prior works only handled κ = 1 + ϵ where ϵ is suﬃciently small) and eﬃciency (our reduction calls
a packing solver ≈log d times for constant ϵ, κ, while [353] used ≈log2 d calls). Moreover, our
method is a natural application of MMW, and is arguably simpler than prior work. This simplicity
is useful in diagonal scaling applications, as it allows us to obtain a tighter characterization of our
dependence on κ, the primary quantity of interest.
Our algorithm for proving Theorem 43 is new, and based on combining Theorem 42 with a
multi-level iterative preconditioning scheme we refer to as a "homotopy method." In particular,
our algorithm for Theorem 42 recursively calls Theorem 42 and preconditioned linear system solvers
as black boxes, to provide near-optimal reweightings M(w) which approximate B + λI for various
values of λ. We then combine our access to linear system solvers in M(w) with eﬃcient rational

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
269
approximations to various matrix functions, yielding our overall algorithm. This homotopy method
framework is reminiscent of techniques used by other recent works in the literature on numerical
linear algebra and structured continuous optimization, such as [362, 311, 100, 8].
7.1.4
Schatten packing SDPs
We consider a natural generalization of packing semideﬁnite programs (SDPs) which we call Schatten
packing. Given symmetric positive semideﬁnite A1, . . . , An and parameter p ≥1, a Schatten packing
SDP asks to solve the optimization problem
min

X
i∈[n]
wiAi

p
subject to w ∈∆n.
(7.12)
Here, ∥M∥p is the Schatten-p norm of matrix M and ∆n is the probability simplex. When p = ∞,
(7.12) is the well-studied (standard) packing SDP objective [282, 19, 442], which asks to ﬁnd the most
spectrally bounded convex combination of packing matrices. For smaller p, the objective encourages
combinations more (spectrally) uniformly distributed over directions.
The specialization of (7.12) to diagonal matrices is a smooth generalization of packing linear
programs, previously studied in the context of fair resource allocation [385, 191]. For the ℓ∞case
of (7.12), packing SDPs have the desirable property of admitting "width-independent" approxima-
tion algorithms via exploiting positivity structure. Speciﬁcally, width-independent solvers obtain
multiplicative approximations with runtimes independent or logarithmically dependent on size pa-
rameters of the problem. This is a strengthening of additive notions of approximation typically used
for approximate semideﬁnite programming. Our work gives the ﬁrst width-independent solver for
Schatten packing. We state an informal guarantee here.
Theorem 44. Let {Ai}i∈[n] ∈Sd
≥0, and ϵ > 0. There is an algorithm taking O( p log( nd
ϵ )
ϵ
) iterations,
returning a 1 + ϵ multiplicative approximation to the problem (7.12). For odd p, each iteration can
be implemented in time nearly-linear in the number of nonzeros amongst all {Ai}i∈[n].
We remark that by setting p appropriately, Theorem 44 also results in the state-of-the-art ap-
proximate (ℓ∞) packing SDP solver. It will be used in our development of Theorems 42 and 43.
Previous work
Semideﬁnite programming (SDP) and its linear programming specialization are fundamental com-
putational tasks, with myriad applications in learning, operations research, and computer science.
Though general-purpose polynomial time algorithms exist for SDPs ([424]), in practical settings in
high dimensions, approximations depending linearly on input size and polynomially on error ϵ are
sometimes desirable. To this end, approximation algorithms based on entropic mirror descent have

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
270
been intensely studied [534, 34, 232, 20, 108], obtaining ϵ additive approximations to the objective
with runtimes depending polynomially on ρ/ϵ, where ρ is the "width", the largest spectral norm of
a constraint.
For structured SDPs, stronger guarantees can be obtained in terms of width. Speciﬁcally, several
algorithms developed for packing SDPs ((7.12) with p = ∞) yield (1 + ϵ)-multiplicative approx-
imations to the objective, with logarithmic dependence on width [282, 442, 19, 285]. As ρ upper
bounds objective value in this setting, in the worst case runtimes of width-dependent solvers yielding
ϵρ-additive approximations have similar dependences as width-independent counterparts. Width-
independent solvers simultaneously yield stronger multiplicative bounds at all scales of objective
value, making them desirable in suitable applications. In particular, ℓ∞packing SDPs have found
great utility in robust statistics algorithm design [133, 131, 132, 171].
Beyond ℓ∞packing, width-independent guarantees in the SDP literature are few and far between;
to our knowledge, other than the covering and mixed solvers of [285], ours is the ﬁrst such guarantee
for a broader family of objectives5. Our method complements analogous ℓp extensions in the width-
dependent setting, e.g. [21], as well as width-independent solvers for ℓp packing linear programs
[385, 191]. We highlight the fair packing solvers of [385, 191], motivated by problems in equitable
resource allocation, which further solved ℓp packing variants for p ̸∈[1, ∞).
We ﬁnd analogous
problems in semideﬁnite settings interesting, and defer to future work.
7.2
A rank-1 sketch for matrix multiplicative weights
We present our main contribution in Section 7.2.1: regret bounds for our rank-1 randomized projec-
tions Pu and their proof via the geometry of ¯P. In Section 7.2.3 we describe how to compute Xt in
eO (√ηt) matrix-vector products using the Lanczos method. In Section 7.2.4 we present in detail the
application of our sketching scheme to semideﬁnite programming, as described above. We conclude
the section in Section 7.2.5 by discussing a number of possible extensions of our results along with
the challenges they present.
7.2.1
Main result
In this section, we state and prove our main result: regret bounds for a rank-1 sketch of the matrix
multiplicative weights method.
Let us recall our sketch.
At time step t, having observed gain
matrices G1, . . . , Gt−1 ∈Sn, we independently draw6 ut ∼Uni(Sn−1) and play the rank-1 matrix
Xt := Put
 
η
t−1
X
i=1
Gi
!
, where Pu(Y) := eY/2uuT eY/2
uT eYu
= vvT
vT v for v = eY/2u.
(7.13)
5In concurrent and independent work, [136] develops width-independent solvers for Ky-Fan packing objectives, a
diﬀerent notion of generalization than the Schatten packing objectives we consider.
6Since Pu is invariant to scaling of u, it has the same distribution for u standard Gaussian or uniform on a sphere.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
271
We call Pu : Sn →∆n the randomized mirror projection. The key computational consideration is
that we can evaluate Pu(Y) eﬃciently, while on the analytic side, we show that the update (7.13)
deﬁnes on average an eﬃcient mirror descent procedure. The regret bounds for Xt then follow.
Expected regret bounds
The focus of our analysis is the average mirror projection
¯P(Y) := EuPu(Y) and action sequence
¯Xt := ¯P
 
η
t−1
X
i=1
Gi
!
,
(7.14)
where Eu denotes expectation w.r.t. to u ∼Uni(Sn−1). As we show in Section 7.2.2 to come, ¯P is
the gradient of the function
¯p(Y) := Eu log
 
eY, uuT 
= Eu log
 uT eYu

,
which we also show7 is a convex spectral function [356]. As a consequence, we can write the average
action ¯Xt in the familiar dual averaging [422] or Follow the Regularized Leader [268] form
¯Xt = argmaxX∈∆n
(
η
t−1
X
i=1
⟨Gi, X⟩−¯r(X)
)
where ¯r(X) = supY∈Sn {⟨Y, X⟩−¯p(Y)} is the convex conjugate of ¯p. In this standard approach, the
regularizer ¯r deﬁnes the scheme, and regret analysis proceeds by showing that ¯r is strongly convex
and has bounded range. The former property is equivalent to the smoothness of ¯p.
In contrast, our starting point is the deﬁnition (7.14) of the projection ¯P, and we ﬁnd it more
convenient to argue about ¯P and ¯p directly. Toward that end, for any Y, Y′ ∈Sn we let
¯VY(Y′) := ¯p(Y′) −¯p(Y) −

Y′ −Y, ¯P(Y)

(7.15)
denote the Bregman divergence induced by ¯p. We show that ¯VY(·) has the properties—analogous
to those arising from duality in analyses of dual averaging [422]—necessary to establish our regret
bounds.
Proposition 21. The projection ¯P and divergence ¯V satisfy
1. Smoothness: for every Y, D ∈Sn, ¯VY(Y + D) ≤3
2 ∥D∥2
∞.
2. Reﬁned smoothness for positive shifts: for every Y, D ∈Sn such that D ⪰0 and ∥D∥∞≤1
6,
¯VY(Y + D) ≤3 ∥D∥∞

D, ¯P(Y)

.
7For ﬁxed u ∈Rn, however, Pu ̸= ∇log
 uT eYu

and we do not know if it is the gradient of any other function.
Moreover, Y 7→log
 uT eYu

is not convex.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
272
3. Diameter bound: for every Y, Y′ ∈Sn, ¯VY(0) −¯VY(Y′) ≤log(4n).
4. Surjectivity: for every X ∈relint ∆n there exists Y ∈Sn such that ¯P(Y) = X.
We return to Proposition 21 and prove it in Section 7.2.2. The proposition gives the following
regret bounds for the averaged actions ¯Xt.
Theorem 45. Let G1, . . . , GT be any sequence of gain matrices in Sn and let ¯Xt = ¯P(η Pt−1
i=1 Gi)
as in Eq. (7.14). Then, for every T ∈N,
λmax
 T
X
t=1
Gt
!
−
T
X
t=1

Gt, ¯Xt

≤log(4n)
η
+ 3η
2 ·
T
X
t=1
∥Gt∥2
∞.
(7.16)
If additionally 0 ⪯Gt ⪯I for every t and η ≤1
6,
λmax
 T
X
t=1
Gt
!
−
T
X
t=1

Gt, ¯Xt

≤log (4n)
η
+ 3η · λmax
 T
X
t=1
Gt
!
.
(7.17)
We prove Theorem 45 in Appendix F.1.1. The proof is essentially the standard dual averaging
telescoping argument [422], which we perform using only the properties in Proposition 21. Indeed,
matrix multiplicative weights satisﬁes a version of Proposition 21 with slightly smaller constant
factors, and its regret bounds follow similarly.
The projection ¯P is no easier to compute than the matrix multiplicative weights projection.
However, Pu is easily computed and is unbiased for ¯P. Consequently—under Assumption 5—the
sketch Pu inherits the regret guarantees in Theorem 45. To argue this formally, we deﬁne the σ-ﬁelds
Ft := σ(G1, X1, . . . , GtX1, Gt+1),
so that Gt ∈Ft−1 and ¯Xt ∈Ft−1, while, under Assumption 5, E[X1 | Ft−1] = ¯Xt because
ut ∼Uni(Sn−1), independent of Ft−1. Consequently, we have the following
Corollary 22. Let G1, . . . , GT be symmetric gain matrices satisfying Assumption 5 and let Xt be
generated according to Eq. (7.13). Then
E

λmax

T
X
t=1
Gt

−
T
X
t=1
⟨Gt, Xt⟩

≤log(4n)
η
+ 3η
2 ·
T
X
t=1
E

∥Gt∥2
∞

.
If additionally 0 ⪯Gt ⪯I for every t and η ≤1
6,
E

λmax

T
X
t=1
Gt

−
T
X
t=1
⟨Gt, Xt⟩

≤log (4n)
η
+ 3η · E

λmax

T
X
t=1
Gt

.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
273
Proof. Using Gt ∈Ft−1 and E[Xt | Ft−1] = ¯Xt, we have E ⟨Gt, Xt⟩= E [E[⟨Gt, Xt⟩| Ft−1]] =
E

Gt, ¯Xt

, and so the result is immediate from taking expectation in Theorem 45.
It is instructive to compare these guarantees to those for the full (non-approximate) matrix
multiplicative weights algorithm. Let
R[T] := E

λmax
 1
T
T
X
t=1
Gt

−1
T
T
X
t=1
⟨Gt, Xt⟩

denote the expected average regret at time T. If ∥Gt∥∞≤1 for every t, the bound (7.16) along
with Corollary 22 imply, for η = (2 log(4n)/(3T))1/2,
R[T] ≤
r
6 log(4n)
T
,
i.e. R[T] ≤ϵ for T ≥6 log(4n)
ϵ2
.
In contrast, the matrix multiplicative weights procedure (7.2) guarantees average regret below ϵ in
2 log(n)/ϵ2 steps, so our guarantee is worse by a factor of roughly 3.
The bound (7.17) guarantees smaller relative average regret when we additionally assume 0 ⪯
Gt ⪯I for every t and an a-priori upper bound of the form λ⋆:= λmax( 1
T
PT
t=1 Gt) ≥λ0. Here, a
judicious choice of η guarantees R[T]/λ⋆≤ε for T ≥12 log(4n)/(λ0ε2). Again, this is slower than
the corresponding guarantee for matrix multiplicative weights by a factor of roughly 3. Relative
regret bounds of the form (7.17) are useful in several application of multiplicative weights and its
matrix variant [33], e.g. width-independent solvers for linear and positive semideﬁnite programs [442].
High-probability regret bounds
Using standard martingale convergence arguments [114, 416], we can provide high-probability con-
vergence guarantees for our algorithm.
Indeed, we have already observed in Corollary 22 that
E [⟨Gt, Xt⟩| Ft−1] =

Gt, ¯Xt

and therefore

Gt, Xt −¯Xt

is a martingale diﬀerence sequence
adapted to the ﬁltration Ft. As |⟨Gt, Xt⟩| ≤∥Gt∥∞∥Xt∥1 = ∥Gt∥∞, the martingale Pt
i=1

Gi, Xti −¯Xi

has bounded diﬀerences whenever ∥Gt∥∞is bounded, so that the next theorem is an immediate con-
sequence of the Azuma-Hoeﬀding inequality and its multiplicative variant [20]
Corollary 23. Let G1, . . . , GT be symmetric gain matrices satisfying Assumption 5 and let Xt be
generated according to Eq. (7.13). If ∥Gt∥∞≤1 for every t, then for every T ∈N and δ ∈(0, 1),
with probability at least 1 −δ,
λmax
 T
X
i=1
Gt
!
−
T
X
t=1
⟨Gt, Xt⟩≤log(4n)
η
+ 3η
2 T +
q
2T log 1
δ .
(7.18)

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
274
If additionally 0 ⪯Gt ⪯I for every t and η ≤1
6, then with probability at least 1 −δ,
λmax
 T
X
i=1
Gt
!
−
T
X
t=1
⟨Gt, Xt⟩≤log (4n/δ)
η
+ 4η λmax
 T
X
i=1
Gt
!
.
(7.19)
We give the proof of Corollary 23 in Appendix F.1.2.
Our development uses Assumption 5 only through its consequence E[Xtt | Ft−1] = ¯Xt. Therefore,
our results apply to any adversary that produces gains with such martingale structure, a weaker
requirement than Assumption 5.
7.2.2
Analyzing the average mirror projection
In this section we outline the proof of Proposition 21, which constitutes the core technical contribu-
tion of our work. Our general strategy is to relate the average mirror projection to the multiplicative
weights projection, which satisﬁes a version of Proposition 21. Our principal mathematical tool is
the theory of convex, twice-diﬀerentiable spectral functions [356, 357].
We begin with the vector log-sum-exp, or softmax, function
lse(v) := log

n
X
j=1
evj

and its gradient ∇lse(v) =
ev
1T ev ,
where we write ev for exp(·) applied elementwise to v and 1 for the all-ones vector.
Note that
∇lse : Rn →σn is the mirror projection associated with (vector) multiplicative weights. Let Y ∈Sn
have eigen-decomposition Y = Q diag(λ)QT . The matrix softmax function is
pmw(Y) := log tr eY = lse(λ) and Pmw(Y) = ∇pmw(Y) =
eY
tr eY = Q diag(∇lse(λ))QT
is the matrix multiplicative weights mirror projection.
We now connect the function ¯p(Y) = Eu[log tr(eYuuT )] and the projection ¯P(Y) = Eu eY/2uuT eY/2
uT eYu
to their counterparts pmw, Pmw and lse.
Lemma 87. Let Y ∈Sn have eigen-decomposition Y = Q diag(λ)QT . Let w ∈σn be drawn from
a Dirichlet( 1
2, . . . , 1
2) distribution. Then
¯p(Y) = Ew [lse(λ + log w)] = Ewpmw(Y + Q diag(log w)QT )
(7.20)
where log is applied elementwise. The function ¯p is convex and its gradient is
¯P(Y) = ∇¯p(Y) = Q diag (Ew[∇lse(λ + log w)]) QT = EwPmw(Y + Q diag(log w)QT ).
(7.21)
Proof. Let u be uniformly distributed over the unit sphere in Rn and note that u and QT u are

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
275
identically distributed. Therefore, for Λ = diag(λ),
¯p(Y) = Eu log
 uT eYu

= Eu log
 (QT u)T eΛ(QT u)

= Eu log
 uT eΛu

= ¯p(Λ).
Further, a vector w with coordinates8 wi = u2
i has a Dirichlet( 1
2, . . . , 1
2) distribution. Hence,
¯p(Λ) = Eu log
 
n
X
i=1
u2
i eλi
!
= Ew log
 
n
X
i=1
eλi+log wi
!
= Ewlse(λ + log w),
establishing the identity (7.20).
Evidently, ¯p(Y) is a spectral function—a permutation-invariant function of the eigenvalues of Y.
Moreover, since lse is convex, λ 7→Ewlse(λ + log w) is also convex, and Corollary 2.4, [356] shows
that ¯p is convex. Moreover, Corollary 3.2, [356] gives
∇¯p(Y) = Q diag(∇Ew[lse(λ + log w)])QT = EwPmw(Y + Q log(w)QT ).
It remains to show that ¯P(Y) = ∇¯p(Y). Here we again use the rotational symmetry of u to write
¯P(Y) = Eu
eY/2uuT eY/2
uT eYu
= Q

Eu
eΛ/2(QT u)(QT u)T eΛ/2
(QT u)T eΛ(QT u)

QT = Q ¯P(Λ)QT .
Moreover,
¯P(Λ)ij = Eu
uiuje(λi+λj)/2
Pn
k=1 u2
keλk
(⋆)
= Eu
u2
i eλiI{i=j}
Pn
k=1 u2
keλk = Ew∇ilse(λ + log w)I{i=j}
where the equality (⋆) above follows because ui has a symmetric distribution, even conditional on
uj, j ̸= i, so E

uiuj | u2
1, . . . , u2
n, uj

= 0 for i ̸= j.
Lemma 87 is all we need in order to prove parts 3 and 4 of Proposition 21.
Proof. (Proposition 21, parts 3 and 4) We ﬁrst observe the following simple lower bound on ¯p,
immediate from identity (7.20) in Lemma 87,
¯p(Y) = Ew log

n
X
i=1
eλi(Y)+log wi
≥λmax(Y) + Ew1 log w1 ≥λmax(Y) −log(4n),
(7.22)
where Ew1 log w1 ≥−log(4n) comes from noting that w1 ∼Beta( 1
2, n−1
2 ) (see Lemma 286 in Ap-
pendix F.1.5). For matrices Y ∈Sn and Xt ∈∆n,
⟨Y, Xt⟩= ⟨Y −λmin(Y)I, Xt⟩+λmin(Y) tr Xt ≤∥Y −λmin(Y)I∥∞∥Xt∥1+λmin(Y) tr Xt = λmax(Y),
8The letter w naturally denotes a vector of 'weights' in the simplex. Here, it is also double-u.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
276
where the ﬁnal equality is due to ∥Y −λmin(Y)I∥∞= λmax(Y) −λmin(Y) for every Y ∈Sn and
∥Xt∥1 = tr Xt = 1 for every Xt ∈∆n.
⟨Y, Xt⟩−¯p(Y) ≤log(4n)
(7.23)
for every Y ∈Sn and Xt ∈∆n. Part 3 follows since
¯VY(0) −¯VY(Y′) = ¯p(0) +

Y′, ¯P(Y)

−¯p(Y′) ≤¯p(0) + log(4n) = log(4n),
where we used the bound (7.23) with Xt = ¯P(Y) and the fact that ¯p(0) = Ew log(1T w) = 0.
To show Part 4, let ¯r(Xt) := supY∈Sn{⟨Y, Xt⟩−¯p(Y)} be the convex conjugate of ¯p. Eq. (7.23)
implies that ¯r(Xt) < ∞for all Xt ∈∆n, and therefore relint ∆n ⊆relint dom¯r. Every convex
function has nonempty subdiﬀerential on the relative interior of its domain as shown in Theorem
X.1.4.2 of [271], and thus for X ∈relint ∆n there exists Y ∈∂¯r(Xt). By deﬁnition of ¯r, any such Y
satisﬁes Xt = ∇¯p(Y) = ¯P(Y), as required.
Proving parts 1 and 2 requires second order information on ¯p. For twice diﬀerentiable function
f, we denote ∇2f(A)[B, B] = ∂2
∂t2 f(A + tB)|t=0. It is easy to verify that, for every λ, δ ∈Rn,
δT ∇2lse(λ)δ = ∇2lse(λ)[δ, δ] ≤(δ2)T ∇lse(λ),
where [δ2]i = δ2
i ; this concisely captures the pertinent second order structure of the multiplicative
weights mirror projection. [421] shows that this property extends to the matrix case.
For any Y, D ∈Sn, ∇2pmw(Y)[D, D] ≤

D2, Pmw(Y)

.
In Appendix F.1.3 we explain how to
ﬁnd this result in [421], as it is not explicit there. In view of Lemma 87, it is natural to hope that
∇2¯p and ∇2pmw are also related via simple expectation. Unfortunately, this fails; we can, however,
derive a bound.
For any Y, D ∈Sn, orthogonal eigenbasis Q for Y , and w ∼Dirichlet( 1
2, . . . , 1
2),
∇2¯p(Y)[D, D] ≤3 · Ew∇2pmw(Y + Q diag(log w)QT )[D, D]
(7.24)
≤3

D2, ¯P(Y)

.
(7.25)
Our proof of Lemma 7.2.2 is technical; we sketch it here brieﬂy and give it in full Appendix F.1.4.
The key ingredient in the proof is a formula for the Hessian of spectral functions [357]. Using the
spectral characterization (7.20), the formula gives that
∇2¯p(Y)[D, D] = diag( ˜D)T 
Ew∇2lse(λ + log w)

diag( ˜D) +
D
EwAw(λ), ˜D ◦˜D
E
.
where ˜D = QT DQ, diag( ˜D) ∈Rn is the vector containing the diagonal entries of ˜D, A ◦B denotes

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
277
elementwise multiplication of A and B, and Aw
ij(λ) := ∇ilse(λ+log(w))−∇jlse(λ+log(w))
λi−λj
I{i̸=j}. With the
shorthand Y{w} := Y+Q diag(log w)QT , we use the formula of [357] again to express ∇2pmw(Y{w})
as
∇2pmw(Y{w})[D, D] = diag( ˜D)T 
∇2lse(λ + log w)

diag( ˜D) +
D
A1(λ + log w), ˜D ◦˜D
E
,
where A1 = A ˜
w evaluated at ˜w = 1. The bulk of the proof is dedicated to establishing the entry-wise
bounds
EwAw
ij(λ) ≤Ew



1 +
tanh
  λi−λj
2
 log wi
wj

λi −λj

A1
ij(λ + log w)

≤3 · EwA1
ij(λ + log w).
The ﬁrst inequality follows from pointwise analysis of a symmetrized version of Aw
ij. The second
inequality follows from piecewise monotonicity of Aw
ij as a function of log wi
wj ∼logit Beta( 1
2, 1
2),
combined with tight exponential tail bounds for the latter. Substituting the bound on EwAw
ij(λ)
into the expression for ∇2¯p(Y) and comparing with Ew∇2pmw(Y{w}) yields the desired result (7.24).
Applying Lemma 7.2.2 and recalling the identity (7.21) yields
Ew∇2pmw(Y + Q diag(log w)QT )[D, D] ≤

D2, EwPmw(Y + Q diag(log w)QT )

=

D2, ¯P(Y)

,
establishing the ﬁnal bound (7.25).
The bound (7.25) gives the remaining parts of Proposition 21.
Proof. (Proposition 21, parts 1 and 2) Fix Y, D ∈Sn and let p(t) := ¯p(Y + tD). The Bregman
divergence (7.15) admits the integral form
¯VY(Y + D) = p(1) −p(0) −p′(0) =
Z 1
0
(p′(t) −p′(0))dt =
Z 1
0
Z t
0
p′′(τ)dτdt
=
Z 1
0
Z t
0
∇2¯p(Y + τD)[D, D]dτdt.
(7.26)
Note that since ¯P(Y) ∈∆n for every Y ∈Sn,

D2, ¯P(Y)

≤∥D2∥∞∥¯P(Y)∥1 = ∥D∥2
∞. Therefore,
the bound (7.25) gives
∇2¯p(Y + τD)[D, D] ≤3 ∥D∥2
∞.
Substituting back into (7.26) and using
R 1
0
R t
0 dτdt = 1
2 gives Proposition 21.1.
When D ⪰0, we have

D2, ¯P(Y)

=
D
D, D1/2¯P(Y)D1/2E
≤∥D∥∞∥D1/2¯P(Y)D1/2∥1 = ∥D∥∞⟨D, ∇¯p(Y)⟩.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
278
Plugging the bound above into the bound (7.25) and substituting back into (7.26) gives
¯VY(Y + D) ≤3 ∥D∥∞
Z 1
0
Z t
0
⟨D, ∇¯p(Y + τD)⟩dτdt.
(7.27)
Moreover,
Z t
0
⟨D, ∇¯p(Y + τD)⟩dτ =
Z t
0
p′(τ)dτ = p(t) −p(0) = ¯VY(Y + tD) +

tD, ¯P(Y)

,
(7.28)
where the ﬁnal equality uses the deﬁnition (7.15) of the Bregman divergence. Note also that v(t) :=
¯VY(Y + tD) is increasing for t ≥0 due to convexity of ¯p; tv′(t) = ⟨tD, ∇¯p(Y + tD) −∇¯p(Y)⟩≥0.
Therefore, the equality (7.28) implies
R t
0 ⟨D, ∇¯p(Y + τD)⟩dτ ≤¯VY(Y +D)+t·

D, ¯P(Y)

for every
0 ≤t ≤1. Substituting this back into (7.27) and rearranging gives

1 −3 ∥D∥∞

¯VY(Y + D) ≤3
2 ∥D∥∞

D, ¯P(Y)

.
establishing part 2 of the proposition, as 1 −3 ∥D∥∞≥1
2 by assumption.
7.2.3
Eﬃcient computation of matrix exponential-vector products
The main burden in computing the randomized mirror projections (7.13) lies in computing eAb
for A ∈Sn and b ∈Rn. Matrix exponential-vector products have widespread use in solutions of
diﬀerential equations [465, 272], and also appear as core components in a number of theoretical
algorithms [34, 429, 291]. Following a large body of literature [399], we approximate eAb via the
classic Lanczos method [339], an iterative process for computing f(A)b for general real functions f
applied to matrix A. The Lanczos approximation enjoys strong convergence guarantees upon which
we base our analysis [466]. It is also eminently practical: the only tunable parameter is the number
of iterations, and each iteration accesses A via a single matrix-vector product.
Let g
expk(A, b) be the result of k iterations of the Lanczos method for approximating eAb. We
provide a precise description of the method in Appendix F.1.6. Let
˜Xt;k = ePut;k
 
η
t−1
X
i=1
Gi
!
,
where ePu;k(Y) = vvT
vT v
for v = g
expk(Y/2, u)
(7.29)
denote the approximate randomized mirror projection.
Using the Lanczos method to compute
full eigen-decompositions has well-documented numerical stability issues [392].
In contrast, the
approximation (7.29) appears to be numerically stable.
To provide a theoretical basis for this
observation, we exhibit error bounds under ﬁnite ﬂoating point precision, leveraging the results
of [405], which in turn build on [199, 200]. To account for computational cost, we denote by mv(Y)
the cost of multiplying matrix Y by any vector.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
279
Proposition 22. Let ϵ, δ ∈(0, 1) and Y ∈Sn, and set M := max{∥A∥op , log( n
ϵδ), 1}. Let u be
uniformly distributed on the unit sphere in Rn and independent of Y. If the number of Lanczos
iterations k satisﬁes k ≥Θ(1)
q
M log( nM
ϵδ ) then the approximation (7.29) satisﬁes
∥Pu(Y) −ePu;k(Y)∥1 ≤ϵ with probability ≥1 −δ over u ∼Uni(Sn−1)
when implemented using ﬂoating point operations with B = Θ(1) log nM
ϵδ
bits of precision. The time
to compute ePu;k(Y) is O(mv(Y)k + k2B).
We prove Proposition 22 in Appendix F.1.6 and describe here the main ingredients in the proof.
First, we show by calculation that
∥Pu(Y) −ePu;k(Y)∥1 ≤
√
8
eY/2u −g
expk(Y/2, u)

2
eY/2u

2
.
Therefore, a multiplicative error guarantee for g
expk would imply our result. Unfortunately, for such
a guarantee to hold for all vectors u we must have k = Ω(∥Y∥∞) [429]. We circumvent that by
using the randomness of u to argue that w.h.p. ∥eY/2u∥2 ≳
1
√neλmax(Y/2) ∥u∥2. This allows us to
use existing additive error guarantees for g
expk to obtain our result.
We connect the approximation to regret in the following corollary.
Corollary 24. Let G1, . . . , GT be symmetric gain matrices satisfying ∥Gt∥∞≤1 for every t. There
exists a numerical constant k0 < ∞, such that for every T ∈N and δ ∈(0, 1), ˜Xt;kt deﬁned in (7.29)
with kt =

k0(√1 + ηt) log( nT
δ )

, and Xt deﬁned in (7.13) satisfy
T
X
t=1
D
Gt, ˜Xt;kt
E
≥−1 +
T
X
t=1
⟨Gt, Xt⟩
w.p. ≥1 −δ/2.
(7.30)
Let ϵ ∈(0, 1], T = 16 log(4en/δ)
ϵ2
and η =
q
2 log(4en)
3T
. If Assumption 5 holds with respect to the actions
˜Xt;kt, then with probability at least 1 −δ, 1
T λmax
PT
i=1 Gt

−1
T
PT
t=1
D
Gt, ˜Xt;kt
E
≤ϵ. Computing
the actions ˜X1;k1, . . . , ˜XT ;kT requires O(ϵ−2.5 log2.5( n
ϵδ)) matrix-vector products.
Finally, as we discuss in detail in Appendix F.1.6, computing matrix exponential-vector products
(and hence Pu) reduces to solving eO (1) linear systems. Since [20] propose to compute their sketch
using a similar reduction, the running time guarantees they establish for their sketch are also valid
for ours.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
280
7.2.4
Application to semideﬁnite programming
Here we describe how to use our rank-1 sketch to solve semideﬁnite programs (SDPs). The standard
SDP formulation is, given ˜C, ˜
A1, . . . , ˜A ˜m ∈S˜n and ˜b ∈R ˜m,
minimize
Z⪰0

 ˜C, Z

subject to

 ˜
Ai, Z

= ˜bi ∀i ∈[ ˜m].
A binary search over the optimum value reduces this problem to a sequence of feasibility problems.
When the constraints imply tr Z ≤r for some r < ∞, every intermediate feasibility problem is
equivalent to deciding whether there exists X in the spectrahedron ∆n s.t. ⟨Ai, X⟩≤0 for all
i ∈[m], with n, m and Ai ∈Sn constructed from ˜n, ˜m, ˜Ai,˜b, ˜C and r. This decision problem is in
turn equivalent [231] to determining the sign of
s = min
y∈σm max
X∈∆n ⟨A⋆y, X⟩, where A⋆y :=
X
i∈[m]
[y]iAi.
(7.31)
and σm is the simplex in Rm. For every y ∈σm and X ∈∆n, we have that
min
y′∈σm

A⋆y′, X

≤s ≤max
X′∈∆n

A⋆y, X′
.
Therefore, to determine s to additive error ϵ, it suﬃces to ﬁnd y, X with Gap(X, y) ≤ϵ, where
Gap(X, y) := max
X′∈∆n

A⋆y, X′
−min
y′∈σm

A⋆y′, X

= λmax (A⋆y) −min
i∈[m] ⟨Ai, X⟩.
(7.32)
A basic approach to solving convex-concave games such as (7.31) is to apply online learning for X
and y simultaneously, where at each round the gains/costs to the max/min player are determined
by the actions of the opposite player in the previous round. Importantly, such dynamics satisfy
Assumption 5, and we use our rank-1 sketch as the online learning strategy of the (matrix) max
player, and standard multiplicative weights for the (vector) min player. Algorithm 24 describes
the resulting scheme. The algorithm entertains a convergence guarantee that depends on the width
parameter
ω := max
i∈[m] ∥Xi∥∞
and has the following form.
Theorem 46. Let {Xt, yt}T
t=1 be the actions produced by Algorithm 1 and, deﬁne Xavg
T
= 1
T
PT
t=1 Xt,
yavg
T
= 1
T
PT
t=1 yt. Then
E [Gap (Xavg
T , yavg
T
)] ≤log(4mn)
ηT
+ 2ηω2.
Proof. Recalling the deﬁnition (7.32) of the duality gap, and that Gt = A⋆yt and [ct]i = ⟨Ai, Xt⟩,

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
281
Algorithm 24: Primal-dual SDP feasibility
1 Let G0 := 0 and c0 := 0
2 for t = 1, . . . , T do
3
Sample vector ut uniformly at random from the unit sphere
4
Play matrix Xt := Put
  Pt−1
i=1 ηGi

5
Play vector yt := ∇lse
 −η Pt−1
i=1 ci

=
yt−1◦e−ηct−1
1T (yt−1◦e−ηct−1).
6
Form gain matrix Gt = A⋆yt = P
i∈[m][yt]iAi
7
Form cost vector [ct]i := ⟨Xt, Ai⟩, i ∈[m]
we have
Gap(Xavg
T , yavg
T
) = 1
T λmax

T
X
t=1
Gt

−1
T min
i∈[m]

T
X
t=1
[ct]i

.
Note that yt = ∇lse(−η Pt−1
i=1 ci) is a function of X1, . . . , Xt−1. Therefore, Gt = A⋆yt satisﬁes
Assumption 5 and we may use Corollary 22 to write
E

λmax

T
X
t=1
Gt

−
T
X
t=1
⟨Gt, Xt⟩

≤log(4n)
η
+ 3η
2 ·
T
X
t=1
E

∥Gt∥2
∞

≤log(4n)
η
+ 3ηω2T
2
,
where in the second inequality we used ω = maxi∈[m] ∥Ai∥∞and y ∈σm to bound ∥Gt∥∞=
∥A⋆yt∥∞≤ω · 1T yt = ω. Similarly, we use the standard multiplicative weights regret bound [474]
to write
T
X
t=1
cT
t yt −min
i∈[m]

T
X
t=1
[ct]i

≤log(m)
η
+ η
2 ·
T
X
t=1
∥ct∥2
∞≤log(m)
η
+ ηω2T
2
,
where the second inequality again follows from |[ct]i| = |⟨Ai, Xt⟩| ≤∥Ai∥∞≤ω since Xt ∈∆n.
Finally,
cT
t yt =
m
X
i=1
[yt]i ⟨Ai, Xt⟩= ⟨A⋆yt, Xt⟩= ⟨Gt, Xt⟩.
Hence, summing the two regret bounds and dividing by T gives the result.
For
η
=
log(4mn)/
√
2ω2T
and
T
=
8 log(4mn)ω2/ϵ2,
Theorem
46
guarantees
E [Gap(Xavg
T , yavg
T
)] ≤ϵ.
A high-probability version of this guarantee follows readily via Corol-
lary 23.
Let us now discuss the computational cost of Algorithm 24. Let mv(M) denote the time re-
quired to multiply the matrix M by any vector, and let mv(A) := P
i∈[m] mv(Ai). Except for the
computation of Xt, every step in the for loop in Algorithm 24 takes O(mv(A)) work to execute (we
may assume mv(A) ≥max{n, m} without loss of generality). Let Yt = η Pt−1
i=1 Gi = A⋆(Pt−1
i=1 ηyi),
and note that, with the values of η and T above, ∥Yt∥∞≤ηTω = eO (ω/ϵ) for every t ≤T.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
282
Per Section 7.2.3, the computation of Xt costs eO

∥Yt∥0.5
∞mv(Yt)

= eO
 (ω/ϵ)0.5 mv(Yt)

. Writing
mv(A⋆) := maxα∈Rm{mv(A⋆α)} ≤min{mv(A), n2}, the total computational cost of our algorithm
is
eO
 
(ω/ϵ)0.5 mv(A⋆) + mv(A)

T

= eO
 (ω/ϵ)2.5 mv(A⋆) + (ω/ϵ)2 mv(A)

.
In many settings of interest—namely when the Ais have mostly non-overlapping sparsity patterns
and yet the Yts are sparse—we have mv(A⋆) ≈mv(A), so that the computational cost is dominated
by the ﬁrst term.
Comparison with other algorithms
Let nnz(M) denote the number of nonzero entries of matrix M, and let nnz(A) := P
i∈[m] nnz(Ai) ≥
mv(A). If in Algorithm 24 we replace the randomized projection Pu with the matrix multiplicative
weights projection Pmw, the regret bound of Theorem 46 still holds, but the overall computational
cost becomes eO
 (ω/ϵ)2 (n3 + nnz(A))

due to full matrix exponentiation.
[415] accelerates this
scheme using extra-gradient steps, guaranteeing duality gap below ϵ in eO (ω/ϵ) iterations, with each
iteration involving two full matrix exponential computations. The overall computational cost of such
scheme is consequently eO
 (ω/ϵ) (n3 + nnz(A))

. [421] attains the same rate by using accelerated
gradient descent on a smoothed version of the dual problem. Our scheme improves on this rate for
suﬃciently sparse problems, with n3/nnz(A) ≫(ω/ϵ)−1.5.
[163] applies a subgradient method to the dual problem, approximating the subgradients using
the Lanczos method to compute a leading eigenvector of A⋆y. The method solves the dual problem to
accuracy ϵ with total work eO
 (ω/ϵ)2.5 mv(A⋆) + (ω/ϵ)2 mv(A)

, essentially the same as us. However,
it is not clear how to eﬃciently recover a primal solution from this method. Moreover, the surrogate
duality gap [163] proposes will not always be 0 at the global optimum, whereas with our approach
the true duality gap is readily computable.
[50] replace the full matrix exponentiation in the accelerated scheme of [415] with a rank-k
sketch of the form (7.3), where k = eO (ω/ϵ). Similarly to [415], they require eO (ω/ϵ) iterations to
attain duality gap below ϵ. [50] approximate matrix exponential vector products by truncating a
Taylor series, costing eO (k(ω/ϵ) mv(A⋆)) = eO
 (ω/ϵ)2 mv(A⋆)

work per iteration. With the Lanczos
method, the cost improves to eO
 (ω/ϵ)1.5 mv(A⋆)

work per iteration. Every step of their method
also computes ⟨Ai, X⟩for all i ∈[m] and a rank-k matrix X = Pk
j=1 vjvT
j ; this costs either k·mv(A)
work (computing ⟨Ai, vj⟩for every i, j) or nnz(A) + n2k (when forming X explicitly). The former
option yields total complexity identical to our method. The latter option is preferable only when
nnz(A) ≫n2 ≥mv(A⋆), and can result in an improvement over the running time of our method if
mv(A⋆) ≪nnz(A) (ω/ϵ)−1.5 +n2 (ω/ϵ)−0.5. [50] report that k = 1 often gave the best result in their
experiment, which is not predicted by their theory. A hypothetical explanation for this ﬁnding is
that, with k = 1, they are essentially running Algorithm 24.
Finally, [163] and [231] propose sub-sampling based algorithms for approximate SDP feasibility

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
283
with runtimes potentially sublinear in mv(A⋆). However, because of their signiﬁcantly worse depen-
dence on ω/ϵ, as well as dependence on Frobenius norms, we match or improve upon their runtime
guarantees in a variety of settings; see [231] for a detailed comparison.
7.2.5
Discussion
We conclude with a discussion of a number of additional settings where our sketch—or some variation
thereof—might be beneﬁcial. In the ﬁrst two settings we discuss, the naturally arising online learning
problem involves adversaries that violate Assumption 5, demonstrating a limitation of our analysis.
Online convex optimization. In the online convex optimization problem, at every time step t the
adversary provides a convex loss ℓt, the players pays a cost ℓt(Xt) and wishes to minimize the regret
PT
t=1 ℓt(Xt) −minX
PT
t=1 ℓt(X). The standard reduction to the online learning problem is to con-
struct an adversary with gains Gt = −∇ℓt(Xt). However, even if the losses ℓt follow Assumption 5,
the constructed gains Gt clearly violate it. Therefore, extensions of our results to online convex
optimization will require additional work and probably depend on ﬁner problem structure.
Positive semideﬁnite programming. [442] and [19] propose algorithms for solving positive (pack-
ing/covering) semideﬁnite programs with width independent running time, meaning that the com-
putational cost of solving the problems to ϵ multiplicative error depends only logarithmically on the
width parameter (ω in Section 7.2.4). Both algorithms rely on matrix exponentiation, which they
approximate with a rank eO
 ϵ−2
sketch using the Johnson-Lindenstrauss lemma. The algorithm
of [442] uses matrix multiplicative weights in essentially a black-box fashion, so one could hope to
replace their high-rank sketch with our rank-1 technique. Unfortunately, the gain matrices that they
construct violate Assumption 5 and so our results do not immediately apply. A rank-1 sketch for
this setting remains an intriguing open problem.
Improved computational eﬃciency against an oblivious adversary. An oblivious adversary pro-
duces gain matrices G1, . . . , GT independent of the actions X1, . . . , XT ; this is a stronger version of
Assumption 5. For such an adversary, if we draw u ∼Uni(Sn−1) and set u1 = u2 = · · · = uT = u,
the average regret guarantee of Corollary 22 still applies, as [20] explain. In this setting, it may be
possible to make the computation of Xt more eﬃcient by reusing Xt−1. Such savings exist in the
stochastic setting (when Gt are i.i.d.) via Oja's algorithm [20], and would be interesting to extend
to the oblivious setting.
Online k eigenvectors.[427] show that a variant of matrix multiplicative weights is also capable
of learning online the top k-dimensional eigenspace, with similar regret guarantees. As our rank-1
sketch solves the k = 1 leading eigenvector problem, it is interesting to study whether a rank-k
sketch solves the k leading eigenvectors problem.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
284
7.3
Ky Fan matrix multiplicative weights
We give a regret guarantee for a Ky Fan matrix multiplicative weights procedure, as well as its
eﬃcient implementation. We ﬁrst state a general-purpose regret bound in Section 7.3.1, using a key
divergence bound shown in Section 7.3.2. We then show how to use a more ﬁne-grained analysis
of simultaneous power iteration developed in Section 7.3.3 to prove correctness and a complexity
bound on our overall method (tolerant to approximation error), given in Section 7.3.4.
Throughout this entire section, all variables (unless otherwise speciﬁed) will be either d-dimensional
vectors or d × d matrices, and we let k ∈[d] be some smaller dimensionality.
7.3.1
Regret bound
Throughout this section, we deﬁne a "dual set" and regularizer inducing dual variables as follows:
Y := {Y | 0 ⪯Y ⪯I, Tr(Y) = k} , r(Y) := ⟨Y, log Y⟩−Tr(Y).
(7.33)
Finally, we deﬁne the projection operator for any symmetric matrix S,
∇r∗(S) := argminY∈Y {⟨−S, Y⟩+ r(Y)} , where r∗(S) := max
Y∈Y {⟨S, Y⟩−r(Y)} .
(7.34)
Here, we remark that it is a direct application of convex duality and the following fact (which is
standard, and follows from e.g. the arguments of [542]) that ∇r∗is unique, and is the gradient of
r∗, the Fenchel dual of r over the set Y.
Fact 9. Function r deﬁned in (7.33) is 1
k-strongly convex over Y in ∥·∥tr, and has range k log d
k.
We prove a helper lemma about the structure of ∇r∗, using its closed form derived in [136].
Fact 10 ([136], Lemma 7.3). Given symmetric matrix S with eigenvalues λ1 ≥λ2 ≥. . . ≥λd and
corresponding eigenvectors {vj}j∈[d], we can compute ∇r∗(S) as follows. Deﬁne
τ(S) := max
(
τ
 τ > 0,
exp(τ)
P
j∈[d] exp(min(τ, λj)) ≤1
k
)
.
(7.35)
Then,
∇r∗(S) =
X
j∈[d]
k exp(min(τ(S), λj))
P
j′∈[d] exp(min(τ(S), λj′))vjv⊤
j .
In other words, ∇r∗exponentiates its argument and normalizes the trace to be k, with the
exception of "large" coordinates which are truncated so that the resulting matrix is operator norm
bounded (as in the deﬁnition of Y). We now give a "reﬁned regret bound" for Algorithm 25 when
all gain matrices {Gt}t≥0 are positive and bounded.
The bound is reﬁned in the sense that it

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
285
Algorithm 25: KFMMW(k, {Gt}t≥0, η)
1 Input: Gain matrices {Gt}t≥0, step size η > 0;
2 Y0 ←k
dI, S0 ←∇r(Y0) = log( k
d)I;
3 for t ≥0 do
4
St+1 ←St + ηGt;
5
Yt+1 ←∇r∗(St+1);
depends directly on the inner products ⟨Gt, Yt⟩rather than a looser, more standard bound such as
k ∥Gt∥op (cf. discussion in [21]). In proving Proposition 23, we will rely on a new bound on Bregman
divergences with respect to r∗, which is stated here, and proven in the following Section 7.3.2.
Lemma 88. For symmetric matrix S, positive semideﬁnite G, and scalar η > 0 let S′ = S + ηG.
Suppose that ∥ηG∥op ≤1
2. Then,
V r∗
S (S′) ≤⟨ηG, ∇r∗(S)⟩.
Proposition 23. Suppose the input gain matrices to Algorithm 25 satisfy the bound, for all t ≥0,
0 ⪯ηGt ⪯1
2I.
Then, we have the guarantee for all T ≥1, and all U ∈Y,
1
T
T −1
X
t=0
⟨Gt, U⟩≤2
T
T −1
X
t=0
⟨Gt, Yt⟩+ k log d
ηT
.
Proof. Fix some U ∈Y throughout this proof, and note that by Fact 9, V r
Y0(U) ≤k log d as
Y0 minimizes r. Moreover, ﬁx Ψ := ∇r(U); it is a straightforward computation that the inverse
mapping ∇r∗(Ψ) = U holds, via Fact 10. For each iteration t,
⟨ηGt, U −Yt⟩= ⟨St+1 −St, ∇r∗(Ψ) −∇r∗(St)⟩
= V r∗
Ψ (St) −V r∗
Ψ (St+1) + V r∗
St (St+1) ≤V r∗
Ψ (St) −V r∗
Ψ (St+1) + ⟨ηGt, Yt⟩.
(7.36)
The second equality is the well-known three-point equality of Bregman divergence and follows from
expanding deﬁnitions, and in the last inequality we used Lemma 88. Telescoping (7.36) across all
iterations and dividing by ηT, we arrive at the bound
1
T
T −1
X
t=0
⟨Gt, U −Yt⟩≤1
T
T −1
X
t=0
⟨Gt, Yt⟩+ V r∗
Ψ (S0)
ηT
.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
286
The conclusion follows by rearrangement and using that (from Fact 9 and ∇r(Y0) = S0)
V r∗
Ψ (S0) = r∗(S0) −r∗(Ψ) −⟨U, S0 −Ψ⟩
= (⟨Y0, S0⟩−r(Y0)) −(⟨U, Ψ⟩−r(U)) −⟨U, S0 −Ψ⟩
= r(U) −r(Y0) −⟨∇r(Y0), U −Y0⟩= V r
Y0(U) ≤k log d.
In Section 7.3.4, where we will only have approximate access to the {Yt}t≥0, we give a simple
bound showing that the guarantee in Proposition 23 does not signiﬁcantly deteriorate as Corollary 26.
7.3.2
Reﬁned divergence bound
In this section, we prove Lemma 88. The proof is patterned from calculations in [108, 285] tailored
towards the speciﬁc properties of the functions r, r∗in (7.33), (7.34). We deﬁne the vector variants
of these functions, denoted rvec : Yvec →R and r∗
vec : Rd →R, by
rvec(y) := ⟨y, log y⟩−∥y∥1 , r∗
vec(s) := min
y∈Yvec {⟨−s, y⟩+ r(y)} ,
where Yvec is the set of nonnegative vectors with ℓ1 norm k and maximum entry bounded by 1.
Here, we use log y to denote the entrywise logarithm of a vector.
Lemma 89. For s ∈Rd, overload τ(s) to mean (7.35) applied to a matrix whose eigenvalues are
given by s. Then, r∗
vec is twice-diﬀerentiable at s if and only if no coordinate of s is equal to τ(s).
Proof. Suppose without loss throughout this proof that s is sorted so s1 ≥. . . ≥sd; we may do this
since r∗
vec is symmetric in its arguments. Also, deﬁne (overloading (7.35) appropriately for vectors)
N(s) :=
X
j∈[d]
exp(min(τ(s), sj)) =⇒[∇r∗
vec(s)]j = k exp(min(τ(s), sj))
N(s)
.
(7.37)
This implication is via a direct modiﬁcation of the calculations leading to Fact 10 (alternatively, this
follows from Corollary 3.3 of [356] since r∗is a spectral function).
Twice-diﬀerentiable case. We ﬁrst prove that r∗
vec(s) is twice-diﬀerentiable when no coordinate
of s is τ(s); suppose that for some 0 ≤ℓ≤k −1, exactly ℓcoordinates of s are (strictly) larger than
τ(s).9 If ℓ= 0, it is clear that r∗
vec is twice-diﬀerentiable, so we focus on the case ℓ̸= 0; in this case,
by the deﬁnition of τ(s) (summing over indices larger and smaller than τ separately),
N(s) = k exp(τ(s)) = ℓexp(τ(s)) +
X
j̸∈[ℓ]
exp(sj) =⇒exp(τ(s)) =
P
j̸∈[ℓ] exp(sj)
k −ℓ
.
9From the deﬁnition of τ, we cannot have ℓ≥k since otherwise the sum of the k largest elements is too large.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
287
We thus compute
∂
∂sj
exp(τ(s)) =



0
j ∈[ℓ]
exp(sj)
k−ℓ
j ̸∈[ℓ]
,
∂
∂sj
N(s) =



0
j ∈[ℓ]
k exp(sj)
k−ℓ
j ̸∈[ℓ]
.
(7.38)
It is then a straightforward calculation that ∇2
ijr∗
vec(s) exists in all cases, upon diﬀerentiating coor-
dinates of ∇r∗
vec as computed in (7.37). In particular,
∇2
ijr∗
vec(s) =









k exp(si)
N(s)
−k exp(si)2
N(s)2
i = j ̸∈[ℓ]
−k exp(si) exp(sj)
N(s)2
i, j ̸∈[ℓ], i ̸= j
0
otherwise
.
(7.39)
This also shows that all ∇2
ijr∗
vec are continuous in a small neighborhood of s, so we conclude r∗
vec is
twice-diﬀerentiable at s.
Non-twice-diﬀerentiable case. Next, suppose we are in the case where some coordinate sℓ= τ(s).
We claim that
∂
∂sℓ
∂
∂sℓr∗
vec(s) does not exist. In particular, perturbing sℓin a positive direction does
not aﬀect τ(s), and thus does not aﬀect N(s) either, so the derivative from above of
∂
∂sℓr∗
vec(s) with
respect to sℓvanishes. To compute the derivative from below, suppose without loss of generality
that sℓ≥τ(s) but sℓ+1 < τ(s). We handle the case where ℓ≥2 here, and discuss ℓ= 1 at the end.
We ﬁrst compute the eﬀect on negatively perturbing sℓon τ(s); for vanishing δ > 0, let s′ = s −δeℓ.
Since τ is weakly monotone in its argument, clearly sj ≥τ(s) > s′
ℓfor j ∈[ℓ−1], so since
k exp(s′
ℓ) ≤ℓexp(s′
ℓ) + (k −ℓ) exp(τ(s)) = ℓexp(s′
ℓ) +
X
j̸∈[ℓ]
exp(sj) =
X
j∈[d]
exp
 min(s′
ℓ, s′
j)

,
we have by the deﬁnition (7.35) that τ(s′) ≥s′
ℓ. Next, by
k exp(τ(s′)) = (ℓ−1) exp(τ(s′)) + exp (s′
ℓ) +
X
j̸∈[ℓ]
exp(sj)
=⇒exp(τ(s′)) =
exp (s′
ℓ) + P
j̸∈[ℓ] exp(sj)
k −(ℓ−1)
= exp (s′
ℓ) + (k −ℓ) exp(τ(s))
k −(ℓ−1)
,
we see that τ(s′) < τ(s) since s′
ℓdecreased. It is straightforward to see from this that since
 ∂
∂sℓ

−
exp(τ(s)) =
exp(sℓ)
k −(ℓ−1) =⇒
 ∂
∂sℓ

−
X
j∈[d]
exp (min(τ(s), sj)) =
k exp(sℓ)
k −(ℓ−1),

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
288
where [ ∂
∂sℓ]−is the derivative from below, we have
 ∂
∂sℓ

−
k exp(sℓ)
P
j∈[d] exp (min(τ(s), sj))
=
k
P
j∈[d] exp (min(τ(s), sj))
2

exp(sℓ)
X
j∈[d]
exp (min(τ(s), sj)) −k exp(sℓ)2
k −(ℓ−1)

̸= 0.
The last inequality is by
X
j∈[d]
exp (min(τ(s), sj)) = k exp(τ(s)) ̸=
k
k −(ℓ−1) exp(τ(s)) =
k
k −(ℓ−1) exp(sℓ).
Thus, the derivatives from above and below do not agree as desired. Finally, consider when ℓ= 1;
the above calculations imply that τ(s′) = ∞(since then no element needs to be truncated). Hence,
 ∂
∂sℓ

−
k exp(sℓ)
P
j∈[d] exp (min(τ(s), sj)) =
k
P
j∈[d] exp (sj)
2

exp(sℓ)
X
j∈[d]
exp (sj) −exp(sℓ)2

̸= 0.
We next prove a bound on quadratic forms with respect to the (matrix) Hessian of r∗, at symmet-
ric matrices S where the function is twice-diﬀerentiable. We crucially use formulas for the derivatives
of spectral functions (permutation-invariant scalar-valued functions on symmetric matrices which
depend only on the eigenvalues), from [356, 357].
Lemma 90. Let S = U⊤diag (s) U be a symmetric matrix with eigenvalues s sorted so that s1 ≥
. . . ≥sd, and U is an orthonormal basis. Then, r∗is twice-diﬀerentiable at S if and only if no
coordinate of s equals τ(S). Further, when r∗is twice-diﬀerentiable at S, for any positive semideﬁnite
G,
∇2r∗(S)[G, G] ≤

∇r∗(S), G2
.
Proof. The ﬁrst claim is a direct consequence of Lemma 89 and the ﬁrst part of Theorem 3.3 of
[357], which states that when r∗is a spectral function of S, it is twice-diﬀerentiable at S if and only
if r∗
vec is twice-diﬀerentiable at s. Moreover, Theorem 3.3 of [357] gives the formula
∇2r∗(S)[G, G] = ∇2r∗
vec(s)
h
diagvec( eG), diagvec( eG)
i
+
D
A, eG ◦eG
E
,
where eG = UGU⊤, Aij =



0
i = j
∇ir∗
vec(s)−∇jr∗
vec(s)
si−sj
i ̸= j
,
(7.40)
◦is the Hadamard (entrywise) product, and diagvec : Rd×d →Rd returns the vector whose entries

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
289
are the diagonal of the input matrix. Here, we assume that no two entries of s are identical since it is
clear that the scalar-valued Hessian is continuous at s by the formula (7.39), so Theorem 4.2 of [357]
shows that ∇2r∗is also continuous at S (thus we can perturb S inﬁnitesimally so the eigenvalues
are unique). Now, let ˜s = min(τ(s), s) entrywise. We ﬁrst have
∇2r∗
vec(s)
h
diagvec( eG), diagvec( eG)
i
≤diag
 k exp(si)
N(s)

si≤τ(s)
! h
diagvec( eG), diagvec( eG)
i
≤
k
N(s)
X
i∈[d]
exp(˜si)

eGii
2
.
(7.41)
Here, we used that ∇2r∗
vec(s) is a diagonal matrix minus a rank-one term, restricted to eigenvalues
which are at most τ(s) as calculated in (7.39). Next, we claim that for any tuple i ̸= j ∈[d],
exp(˜si) −exp(˜sj)
si −sj
≤exp(˜si) + exp(˜sj)
2
.
Without loss of generality assume si > sj. This claim is obvious for any tuple where si > sj ≥τ(s).
For all other cases, we recall the identity exp(a)−exp(b)
a−b
≤exp(a)+exp(b)
2
for all a ̸= b (cf. Lemma B.3,
[285]). Then, if sj ≤si < τ(s), a direct application of this identity yields the claim; for the ﬁnal
case where sj < τ(s) ≤si, this follows from also using si −sj ≥˜si −˜sj. Continuing,
D
A, eG ◦eG
E
=
k
N(S)
X
i̸=j∈[d]
exp(˜si) −exp(˜sj)
si −sj

eGij
2
≤
k
N(S)
X
i̸=j∈[d]
exp(˜si) + exp(˜sj)
2

eGij
2
.
(7.42)
Combining (7.41) and (7.42) in the formula (7.40),
∇2r∗(S)[G, G] ≤
k
N(S)
X
i,j∈[d]
exp(˜si) + exp(˜sj)
2

eGij
2
=
k
N(S)
X
i,j∈[d]
exp(˜si)

eGij
2
=
k
N(S)
X
i∈[d]
exp(˜si)

X
j∈[d]

eGij
2


=
X
i∈[d]
k exp(˜si)
N(S)
h
eG2i
ii =
D
diag (∇r∗
vec(s)) , eG2E
.
Finally, note that eG2 = UG2U⊤, so the last expression is equal to

U⊤diag (∇r∗
vec(s)) U, G2
by
the cyclic property of trace. We conclude by the fact that ∇r∗(S) = U⊤diag (∇r∗
vec(s)) U, since r∗
is a spectral function, due to Corollary 3.3 of [356].
We conclude with the desired proof of Lemma 88.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
290
Lemma 88. For symmetric matrix S, positive semideﬁnite G, and scalar η > 0 let S′ = S + ηG.
Suppose that ∥ηG∥op ≤1
2. Then,
V r∗
S (S′) ≤⟨ηG, ∇r∗(S)⟩.
Proof. We ﬁrst claim that without loss of generality, everywhere on the straight-line path from S
to S′ except for a measure-zero set (in R1), r∗is twice-diﬀerentiable. To see this, the Alexandrov
theorem says that since r∗is convex, it is twice-diﬀerentiable everywhere except a measure-zero
set in the space of its argument.
However, by perturbing S and S′ by a random matrix with
eigenvalues distributed uniformly at random ∈[−δ, δ], for vanishing δ > 0, with probability one the
line between perturbed matrices only intersects the non-twice-diﬀerentiable set on a measure-zero
set (this follows from the disintegration theorem). Thus, by continuity of V r∗in both arguments
(since ∇r∗is Lipschitz by Lemma 15.3 of [473], as r∗is the dual of a strongly convex function), we
assume S, S′ have this property, so we may write
V r∗
S (S′) =
Z 1
0
Z s
0
∇2r∗(St)[G, G]dtds
≤
Z 1
0
Z s
0

∇r∗(St), η2G2
dtds ≤1
2
Z 1
0
Z s
0
⟨∇r∗(St), ηG⟩dtds.
(7.43)
Here, for t ∈[0, 1] we deﬁne St = S+tηG, and used Lemma 90 in the second line (almost everywhere)
as well as the assumed bound on ∥ηG∥op so that η2G2 ⪯1
2ηG. Deﬁne p(t) := r∗(St) and v(t) :=
V r∗
S (St); then,
Z s
0
⟨∇r∗(St), ηG⟩dt = p(s) −p(0) = v(s) + ⟨∇r∗(S), sηG⟩≤v(1) + ⟨∇r∗(S), sηG⟩.
In the last inequality, we used that v is increasing, which can be seen via
tv′(t) = ⟨tηG, ∇r∗(St) −∇r∗(S)⟩≥0.
Substituting back into (7.43),
V r∗
S (S′) ≤1
2
Z 1
0
(v(1) + ⟨∇r∗(S), sηG⟩) ds ≤1
2v(1) + 1
2 ⟨∇r∗(S), ηG⟩.
Rearranging and using that V r∗
S (S′) = v(1) yields the desired bound.
7.3.3
Reﬁned k-PCA guarantees
We show a reﬁned bound on the guarantees of simultaneous power iteration for approximately
learning the top k eigenvectors of a positive semideﬁnite matrix (i.e. k-PCA). In particular, the

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
291
main result of this section (Proposition 24) strengthens Theorem 6.1 in [136] by a factor of k.
Algorithm 26: Power(A, λmax, λmin, k, ϵ, δ)
1 Input: Positive semideﬁnite A ∈Rd×d with λminI ⪯A ⪯λmaxI, accuracy ϵ ∈(0, 1),
k ∈[d], δ ∈(0, 1);
2 N ←Θ

1
ϵ log

d
δϵ · λmax
λmin

for a suﬃciently large universal constant;
3 G ∈Rd×k entrywise ∼N(0, 1);
4 Return: V ∈Rd×k, an orthonormal basis for the column span of ANG;
For the remainder of this section, we will ﬁx a particular positive semideﬁnite matrix A =
U⊤diag (λ) U, where U ∈Rd×d is orthonormal and λ1 ≥λ2 ≥. . . ≥λd are the ordered eigenvalues
of A. We will also deﬁne three sets which partition [d]:
L := {j ∈[d] | λj > (1 + ϵ
4)λk+1},
M := {j ∈[d] | (1 + ϵ
4)λk+1 ≥λj ≥(1 −ϵ
4)λk+1},
S := {j ∈[d] | λj < (1 −ϵ
4)λk+1}.
(7.44)
In particular, L, M, and S are the "large", "medium", and "small" eigenvalues of A. We ﬁrst give
two key structural results, which say that with high probability, the span of V contains essentially
all the ℓ2 mass of any vector in L, and essentially none of the ℓ2 mass of any vector in S.
Lemma 91. Let P := VV⊤where V is the output of Algorithm 26.
With probability at least
1 −δ
3 −exp(−Ck) for a universal constant C, for all j ∈S, ∥Puj∥2 ≤λ2
d
λ2
1 ·
ϵ2
64d2 , where uj is row j
of U, and we follow notation in (7.44).
Proof. By rotational invariance of Gaussian matrices, it suﬃces to consider the case where A is
diagonal and U is the identity; henceforth in this lemma, uj is the jth standard basis vector. Recall
that P is the projection onto the column span of ANG. We explicitly compute
P = ANG
 G⊤A2NG
−1 G⊤AN =⇒∥Puj∥2
2 = u⊤
j ANG
 G⊤A2NG
−1 G⊤ANuj.
(7.45)
Here, we used that P2 = P. Now, notice that (where Gj: is row j of G)
G⊤A2NG =
X
j∈[d]
λ2N
j
Gj:G⊤
j: ⪰λ2N
k
X
j∈[k]
Gj:G⊤
j:.
However, Theorem 1.1 of [463] shows that with probability δ
6 + exp(−Ck) for some constant C,
the smallest eigenvalue of a k × k Gram matrix for independent Gaussian entries is at least
δ
6
√
k.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
292
Assuming that this happens, we then continue to bound
λ2N
k
X
j∈[k]
Gj:G⊤
j: ⪰λ2N
k δ
6
√
k
I =⇒∥Puj∥2
2 ≤6
√
k
δλ2N
k
u⊤
j ANGG⊤ANuj
= 6
√
k
δ
 λj
λk
2N 
GG⊤
jj
≤6
√
k
δ
exp

−ϵN
2
 X
i∈[k]
G2
ji.
In the ﬁrst implication, we combined the lower bound we just derived with (7.45). Using standard
chi-squared concentration bounds (cf. Lemma 1, [340]), the probability that P
i∈[k] G2
ji ≥2k+3 log 6
δ
is no more than δ
6. Performing a union bound, with failure probability at most δ
3 + exp(−Ck), we
have that for suﬃciently large N = Θ

1
ϵ log

d
δϵ · λmax
λmin

, since k ≤d,
∥Puj∥2
2 ≤12k1.5 + 18
√
k log 6
δ
δ
exp

−ϵN
2

≤λ2
min
λ2max
·
ϵ2
64d2 ≤λ2
d
λ2
1
·
ϵ2
64d2 .
Finally, adjusting the failure probability of the chi-squared tail bound by a factor of d, the conclusion
follows by union bounding over all j ∈S.
Lemma 92. Let P := VV⊤where V is the output of Algorithm 26.
With probability at least
1 −δ
3 −d exp(−Ck) for a universal constant C, for all j ∈L, ∥Puj∥2
2 ≥1 −λ2
d
λ2
1 ·
ϵ2
64d2 , where uj is
row j of U, and we follow notation in (7.44).
Proof. By deﬁnition of L, it is clear that j ∈[k]. Again we consider the case where A is diagonal
and U is the identity without loss of generality. Let G[k]: be the ﬁrst k rows of G, and let
eG := G
 G[k]:
−1 .
Observe that the ﬁrst k rows of eG are exactly I. Also, with probability at least 1 −δ
6 −exp(−Ck),
the largest singular value of
 G[k]:
−1 is bounded above by 6
√
k
δ , again by Theorem 1.1 of [463].
Condition on this event for the remainder of the proof. Since in this case G[k]: is invertible, where
Span denotes column span,
Span

eG

= Span (G) =⇒Span

AN eG

= Span
 ANG

.
Fix some j ∈L. To show the conclusion, it suﬃces to show that there exists a unit vector v∗in
the span of AN eG with (⟨uj, v∗⟩)2 ≥1 −λ2
d
λ2
1 ·
ϵ2
64d2 . To see this, let {vi}i∈[k] be any orthonormal basis

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
293
for Span

AN eG

with v1 = v∗; then
∥Puj∥2
2 = u⊤
j Puj =
X
i∈[k]
(⟨uj, vi⟩)2 ≥(⟨uj, v∗⟩)2 ≥1 −λ2
d
λ2
1
·
ϵ2
64d2 .
We will choose v∗to be the normalization of AN eG:j which has unit ℓ2 norm, where eG:j is column j
of eG. By standard chi-squared concentration bounds, with probability at least 1−δ
6, all rows i ̸∈[k]
of the matrix G have squared ℓ2 norm at most
2k + 3 log 6d
δ .
Here, we adjusted the failure probability of Lemma 1 in [340] by a factor of d and union bounded
over all i ̸∈[k]. Now, this implies that for all i ̸∈[k],

 G[k]:
−1⊤
G⊤
i:

2
2
≤72k2 + 108k log 6d
δ
δ2
=⇒eG2
ij ≤72k2 + 108k log 6d
δ
δ2
for all j ∈[k].
We conclude that the column vector eG:j has the property that
eG2
ij









= 1
i = j
= 0
i ̸= j, i ∈[k]
≤72k2+108k log 6d
δ
δ2
i ̸∈[k].
Here, the ﬁrst two cases are by design, and the last is by our earlier derivation. Thus, by choosing
N = Θ

1
ϵ log

d
δϵ · λmax
λmin

to be suﬃciently large (as in the ending of the proof of Lemma 91), we
see that AN eG:j places all but a negligible amount of ℓ2
2 mass on coordinate j, where we use that
λN
j ≥(1 + ϵ
4)NλN
i
for all i ̸∈[k].
We also give a simple helper calculation for demonstrating Loewner orderings.
Lemma 93. Let A, B ∈Rd×d be positive semideﬁnite and suppose for any ﬁxed unit test vector
v ∈Rd and some ϵ ∈(0, 1),
v⊤(A −B) v
 ≤ϵv⊤Bv.
Then, (1 −ϵ)B ⪯A ⪯(1 + ϵ)A.
Proof. The upper bound follows from
v⊤Av ≤v⊤Bv +
v⊤(A −B) v
 ≤(1 + ϵ)v⊤Bv.
The lower bound follows similarly.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
294
Our main bound follows from an application of the above three results.
Proposition 24. Let P := VV⊤where V is the output of Algorithm 26. With probability at least
1 −2δ
3 −2 exp(−Ck) for a universal constant C,
(1 −ϵ) (PAP + (I −P) A (I −P)) ⪯A ⪯(1 + ϵ) (PAP + (I −P) A (I −P)) .
(7.46)
Proof. Condition on the conclusions of Lemmas 91 and 92 holding for the rest of this proof. We
note
A −(PAP + (I −P) A (I −P)) = PA (I −P) + (I −P) AP.
Hence, applying Lemma 93, for any ﬁxed unit test vector v ∈Rd, this proposition asks to show
2
y⊤Ax
 ≤ϵ
 x⊤Ax + y⊤Ay

, where x := Pv and y := (I −P) v.
Recall that A = P
j∈[d] λjuju⊤
j . Letting ˜x := Ux and ˜y := Uy, it suﬃces to show

X
j∈[d]
λj ˜xj ˜yj

≤ϵ
2
X
j∈[d]
λj
 ˜x2
j + ˜y2
j

.
(7.47)
Since ⟨˜x, ˜y⟩= ⟨x, y⟩= 0 by the deﬁnition of x, y,

X
j∈[d]
λj ˜xj ˜yj

=

X
j∈[d]
(λj −λk+1)˜xj ˜yj

≤ϵ
4
X
j∈M
λk+1|˜xj ˜yj| +

X
j̸∈M
(λj −λk+1)˜xj ˜yj

.
Here we used the deﬁnition of j ∈M, so that |λj −λk+1| ≤ϵ
4λk+1. We ﬁrst bound
ϵ
4
X
j∈M
λk+1|˜xj||˜yj| ≤
ϵ
4(1 −ϵ
4)
X
j∈M
λj|˜xj||˜yj| ≤ϵ
4
X
j∈M
λj
 ˜x2
j + ˜y2
j

≤ϵ
4
X
j∈[d]
λj
 ˜x2
j + ˜y2
j

.
(7.48)
Moreover, by Lemma 91, for each j ∈S, we have
|˜xj| =
u⊤
j Pv
 ≤∥Puj∥2 ∥v∥2 ≤λd
λ1
· ϵ
8d,
and similarly for each j ∈L, ˜yj ≤λd
λ1 ·
ϵ
8d by Lemma 92. Thus, since all |˜xj| and |˜yj| are at most 1,

X
j∈S
(λj −λk+1)˜xj ˜yj

≤λ1
X
j∈S
|˜xj| ≤ϵ
8λd ≤ϵ
8
X
j∈[d]
λj
 ˜x2
j + ˜y2
j

,

X
j∈L
(λj −λk+1)˜xj ˜yj

≤λ1
X
j∈L
|˜yj| ≤ϵ
8λd ≤ϵ
8
X
j∈[d]
λj
 ˜x2
j + ˜y2
j

.
(7.49)

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
295
Finally, combining (7.48) and (7.49), we have the desired bound (7.47).
An unfortunate consequence of Proposition 24 is that its failure probability is exponentially
related to k, rather than d. However, for suﬃciently small k = O(log 1
δ ), we can use an alternative
analysis of the power method due to [136] to conclude that the desired bound (7.46) holds.
Corollary 25. There is an algorithm (either Algorithm 26 of this paper, or Algorithm 5 of [136])
which takes as input positive semideﬁnite A ∈Rd×d with λminI ⪯A ⪯λmaxI, k ∈[d], and accuracy
parameter ϵ ∈(0, 1), and returns with probability at least 1−δ a set of orthonormal vectors V ∈Rd×k
such that for P := VV⊤, (7.46) holds. The number of matrix-vector products to A required is
O
k
ϵ log2
 d
δϵ
λmax
λmin

.
Proof. In the case where k ≥log 6/δ
C
for C the universal constant in Proposition 24, the conclusion
is immediate from Proposition 24. In the other case, we have that k = O(log 1
δ ). Hence, we can
run Algorithm 5 of [136] with an accuracy parameter which is O(k) times smaller, and use their
Theorem 6.1 to obtain the desired conclusion. The iteration complexity of Algorithm 5 of [136]
depends linearly on the inverse accuracy, so the bound loses an additional logarithmic factor.
7.3.4
Implementation
In this section, we give an algorithm (Algorithm 27) which takes as input a matrix S and produces
a matrix bY such that for some choice of input ∆≥0, we have
 bY −∇r∗(S)

tr ≤k∆.
(7.50)
We will use this at the end of the section to give a complete (computationally eﬃcient) implemen-
tation of an approximate variant of Algorithm 25, and give its guarantees as Corollary 26.
Proposition 25. With probability at least 1−δ, the output bY of Algorithm 27 satisﬁes (7.50). The
complexity of Lines 3-8 of the algorithm is
O

ndk · λmax
∆2 log2
 d
∆δ
λmax
λmin

.
Moreover, for any ϵ ∈(0, 1), the complexity of providing (1 ± ϵ)-approximate access to quadratic
forms through bY for any n ﬁxed vectors {vi}i∈[n] ⊂Rd with probability at least 1 −δ is
O

nd · λmax
ϵ2
log
1
ϵ

log
nd
δ

.
Proof. We will show correctness and complexity of Algorithm 27 separately.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
296
Algorithm 27: ApproxProject(S, λmax, λmin, k, ∆, δ)
1 Input: Positive semideﬁnite S = M⊤M ∈Rd×d for some explicitly given M ∈Rn×d with
λminI ⪯S ⪯λmaxI, k ≤d ≤n, accuracy ∆∈(0, 1), k ∈[d], δ ∈(0, 1);
2 Output: bY satisfying
 bY −∇r∗(S)

tr ≤k∆with probability ≥1 −δ;
3 V ←Power(S, λmax, λmin, k,
∆
8λmax , δ
2) (or when k ≤log 12/δ
C
, use Algorithm 5 of [136]);
4 {uj}j∈[k] ←eigenvectors of V⊤M⊤MV ∈Rk×k, left-multiplied by V;
5 For j ∈[k], ˜λj ←u⊤
j PSPuj where P := VV⊤;
6 bS ←P
j∈[k] ˜λjuju⊤
j + (1 −
∆
4λmax )(I −P)S(I −P);
7 bT ←(1 ± ∆
8 )-approximation to Tr exp

(1 −
∆
4λmax )(I −P)S(I −P)

with probability 1 −δ
2;
8 ˆτ ←ﬁxed point of k exp(ˆτ) = P
j∈[k] exp(min(ˆτ, ˜λj)) + bT;
9 bY ←
k
P
j∈[k] exp(min(ˆτ,˜λj))+ b
T
P
j∈[k] exp(min(ˆτ, ˜λj))uju⊤
j + exp

(1 −
∆
4λmax )(I −P)S(I −P)

;
10 Return: bY;
Correctness guarantee. We begin with proving correctness, which we complete in two parts. In
particular, we show that the following two bounds hold:
∇r∗(S) −∇r∗(bS)

tr ≤k∆
2 ,
 bY −∇r∗(bS)

tr ≤k∆
2 .
(7.51)
By combining the two parts of (7.51) and applying the triangle inequality, we have the desired
conclusion.
To show the former bound, because the convex conjugate of any
1
k-strongly convex
function in ∥·∥tr is k-smooth in ∥·∥op (cf. Lemma 15.3, [473]), and Fact 9 states that r is strongly
convex, it suﬃces to show that
bS −S

op ≤∆
2
=⇒
∇r∗(bS) −∇r∗(S)

tr ≤k
bS −S

op ≤k∆
2 .
(7.52)
Assume ﬁrst that Power was used in computing Line 3. By Proposition 24, we have that
∥S −(PSP + (I −P) S (I −P))∥op ≤
∆
8λmax
∥S∥op ≤∆
8 .
(7.53)
Next, we claim that {uj}j∈[k] are the eigenvectors of PSP, so that
X
j∈[k]
˜λjuju⊤
j = PSP.
To see this, let wj ∈Rk be an eigenvector of V⊤M⊤MV with eigenvalue ˜λj, and let uj = Vwj, as

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
297
in Line 4 of Algorithm 27. Then indeed we have (since V⊤V is the identity)
PSPuj = VV⊤SVV⊤Vwj = V
 V⊤M⊤MVwj

= ˜λjVwj = ˜λjuj.
We then compute, using the deﬁnition of bS in Line 6,
(PSP + (I −P) S (I −P)) −bS

op =
∆
4λmax
∥(I −P)S(I −P)∥op ≤
3∆
8λmax
∥S∥op ≤3∆
8 .
(7.54)
Here, we used Proposition 24 once more to (loosely) upper bound (I−P)S(I−P) by 1.5S. Combining
(7.52), (7.53), and (7.54) gives the ﬁrst conclusion in (7.51).
We next claim the top eigenvalue of (I −P)S(I −P) is at most (1 +
∆
4λmax )˜λk; this follows from
the second and third parts of Theorem 1 of [404]. Thus, by scaling down (I−P)S(I−P) by a factor
1 −
∆
4λmax , we have that its largest eigenvalue is smaller than the smallest of PSP. Let {˜λj}j∈[d]\[k],
{uj}j∈[d]\[k] complete an eigendecomposition of bS. We conclude that none of the eigenvalues of
bS −PSP will be truncated in the projection since they are not in the top k, so Fact 10 yields that
∇r∗(bS) =
k
P
j∈[k] min(σ, αj) + T

X
j∈[k]
min(σ, αj)uju⊤
j +
X
j̸∈[k]
αjuju⊤
j

,
where T := Tr exp

1 −
∆
4λmax

(I −P)S(I −P)

,
and σ := exp(τ(˜λ)), αj := exp(˜λj) for all j ∈[d].
Speciﬁcally, this form is clear for the ﬁrst k eigenvectors, and for the remainder the ∇r∗operation
applies an exponentiation and scaling (since they will not be truncated), which does not aﬀect
the relevant basis. Finally, by applying the following Lemma 94 with γ = ∆
6 , and using that the
eigenvectors of our returned bY align exactly with those of bS, we have the desired second bound in
(7.51). We remark that the only place we used the fact that Power was used in Line 3 thus far in this
proof was in citing Theorem 1 of [404]; however, if Algorithm 5 of [136] is used, a similar statement
on the top eigenvalue of bS −PSP follows by their Remark 6.9.
Complexity guarantee. When S is given in the form M⊤M, the cost of a matrix-vector product
in S is O(nd). So, from Corollary 26 the cost of Line 3 is bounded by
O

ndk · λmax
∆
log2
 d
∆δ
λmax
λmin

.
In Line 4, the cost of forming the matrix MV is O(ndk), and forming its Gram matrix and performing
an eigendecomposition takes time O(k2d + k3); left-multiplying all resulting vectors by V also takes
time O(k2d). The cost of Line 5 for each j ∈[k] is O(nd + kd), so the overall cost is also O(ndk).
Line 8 is a scalar optimization problem and will not dominate the complexity (tolerance to error in

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
298
a binary search is guaranteed via Lemma 94). The only remaining cost is in Line 6.
To estimate Tr exp(A) to 1±γ accuracy for a positive semideﬁnite matrix 0 ⪯A ⪯λmaxI (here,
we note Proposition 24 guarantees A = (1 −
∆
4λmax )(I −P)S(I −P) ⪯S ⪯λmaxI), we will use
two facts well-known in the approximate semideﬁnite programming literature. First, Theorem 4.1
of [466] shows that a degree-O(λmax log 1
γ ) polynomial p has the property that

1 −γ
3

exp(A) ⪯p(A) ⪯

1 + γ
3

exp(A).
Moreover, the Johnson-Lindenstrauss lemma (e.g. the implementation given in [6]) shows that to
estimate Tr exp(A) it suﬃces to sample a random ± 1
√r matrix G ∈Rd×r for some r = O(log( d
δ )γ−2)
and then compute
X
j∈[r]
p
1
2A

G:j

2
2
≈1± γ
3
X
j∈[r]
exp
1
2A

G:j

2
2
= Tr

exp
1
2A

GG⊤exp
1
2A

.
This last quantity is a 1 ± γ
3 approximation of Tr exp(A) with probability 1 −δ
2. The cost of this
whole procedure is dominated by O(rλmax log 1
γ ) matrix-vector multiplies to A; for our choice of A,
each multiplication costs O(nd) time, leading to an overall complexity of (as γ = Θ(∆))
O

nd · λmax
∆2 log
 1
∆

log
d
δ

.
For any v ∈Rd, essentially the same strategy of sampling a random G ∈Rd×r and computing
v⊤p
1
2A

GG⊤p
1
2A

v =
X
j∈[r]

G:j, p
1
2A

v
2
.
suﬃces for estimating the quadratic form in exp(A) to 1±ϵ accuracy, where now r and the polynomial
degree depend on ϵ rather than γ. We can ﬁrst apply the polynomial to each column of G and then
compute inner products with v. To compute approximate quadratic forms in bY, every part of bY is
explicitly given except for the component exp(A) for A = (1 −
∆
4λmax )(I −P)S(I −P), which we can
approximate with the above strategy in the desired time.
Finally, for a batch of n vectors {vi}i∈[n], note that we can ﬁrst compute all the vectors p( 1
2A)G:j
in the desired time, at which point the cost of computing each quadratic form is reduced to O(dr).
Thus, the overall complexity is O(ndr), where we adjust the logarithm in the deﬁnition of r by a
factor of n to union bound the failure probability.
We now provide the helper Lemma 94, which we remark crucially improves the error analysis in
Section 7.3 of [136] by a factor of k, allowing us to avoid an additional poly(k) dependence.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
299
Lemma 94. Let γ ∈(0, 1), k ∈[d]. Given nonnegative {αj}j∈[d] sorted with α1 ≥. . . ≥αd, let
T := P
j̸∈[k] αj, and let bT ∈[(1 −γ)T, (1 + γ)T]. Deﬁne σ and ˆσ to be ﬁxed points of
kσ =
X
j∈[k]
min (σ, αj) + T, kˆσ =
X
j∈[k]
min (ˆσ, αj) + bT.
Then, we have
X
j∈[d]

k min(σ, αj)
P
i∈[k] min (σ, αi) + T −
k min(ˆσ, αj)
P
i∈[k] min (ˆσ, αi) + bT
 ≤3kγ.
Proof. We ﬁrst comment brieﬂy on the existence of σ, ˆσ. Note that in the setting of the lemma,
f(ˆσ) :=
ˆσ
P
i∈[k] min(ˆσ, αi) + bT
is an increasing, continuous function of ˆσ in the range [0, ∞) which satisﬁes f(0) = 0 and f(∞) = ∞,
so there must be a unique ˆσ satisfying f(ˆσ) = 1
k. Existence of σ is proven similarly. Next, we claim
ˆσ ∈[(1 −γ)σ, (1 + γ)σ].
(7.55)
By our earlier argument, it suﬃces to show that f((1 + γ)σ) ≥1
k and f((1 −γ)σ) ≤1
k, so that an
appeal to continuity and monotonicity of f yields (7.55). To see the former bound, note that
f((1 + γ)σ) =
(1 + γ)σ
P
i∈[k] min((1 + γ)σ, αi) + bT
≥
(1 + γ)σ
(1 + γ) P
i∈[k] min(σ, αi) + (1 + γ)T
=
σ
P
i∈[k] min(σ, αi) + T = 1
k .
The last equality used the deﬁnition of σ; the only inequality used bT ≤(1 + γ)T by assumption,
and min((1 + γ)σ, αi) ≤min((1 + γ)σ, (1 + γ)αi) = (1 + γ) min(σ, αi). Similarly,
f((1 −γ)σ) =
(1 −γ)σ
P
i∈[k] min((1 −γ)σ, αi) + bT
≤
(1 −γ)σ
(1 −γ) P
i∈[k] min(σ, αi) + (1 −γ)T = 1
k .

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
300
Here we used (1−γ) min(σ, αi) ≤min((1−γ)σ, αi). Now, we claim (7.55) implies that for all j ∈[d],

min(σ, αj)
P
i∈[k] min (σ, αi) + T −
min(ˆσ, αj)
P
i∈[k] min (ˆσ, αi) + bT
 ≤
3γ min(σ, αj)
P
i∈[k] min (σ, αi) + T .
(7.56)
To see this, we may upper and lower bound for each j ∈[d],
(1 −γ) min(σ, αj) ≤min(ˆσ, αj) ≤(1 + γ) min(σ, αj)
=⇒(1 −γ)

X
i∈[k]
min(σ, αi) + T

≤
X
i∈[k]
min(ˆσ, αi) + bT ≤(1 + γ)

X
i∈[k]
min(σ, αi) + T


=⇒
(1 −3γ) min(σ, αj)
P
i∈[k] min (σ, αi) + T ≤
min(ˆσ, αj)
P
i∈[k] min (ˆσ, αi) + bT
≤
(1 + 3γ) min(σ, αj)
P
i∈[k] min (σ, αi) + T .
(7.57)
This yields (7.56), which upon summing and using that min(σ, αj) = αj for all j ̸∈[k], and the
deﬁnition of T, yields the ﬁnal conclusion after multiplying by k.
We additionally state one helper guarantee on the properties of bY.
Lemma 95. With probability at least 1 −δ, the output bY of Algorithm 27 satisﬁes
 bY

tr ≤

1 + ∆
2

k,
 bY

op ≤1 + ∆
2 .
Proof. By the deﬁnition of ∇r∗, we have that ∇r∗(bS) ∈Y so has operator norm at most 1 and trace
norm at most k. For the ﬁrst conclusion, (7.51) implies
 bY

tr ≤
∇r∗(bS)

tr + k∆
2 .
For the second conclusion, (7.57) and the operator norm bound on Y imply
λmax

bY

≤(1 + 3γ) λmax

∇r∗(bS)

≤1 + ∆
2 .
Finally, we state our complete algorithm, an approximate version of the KFMMW method.
Corollary 26. Suppose the input gain matrices to Algorithm 28 satisfy the bound, for all t ≥0,
0 ⪯ηGt ⪯1
2I.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
301
Algorithm 28: ApproxKFMMW(k, {Gt}0≤t≤T , η, ∆, δ)
1 Input: Gain matrices {Gt}0≤t≤T , step size η > 0, accuracy ∆∈(0, 1), δ ∈(0, 1);
2 Y0 ←k
dI, S0 ←∇r(Y0) = log( k
d)I;
3 for 0 ≤t < T do
4
St+1 ←St + ηGt;
5
bYt+1 ←ApproxProject(St+1 + (1 + log( d
k))I, t + 2, 1, k, ∆, δ
T );
Further, suppose that the {Gt}t≥0 are weakly decreasing in Loewner order. With probability 1 −δ,
∥GT ∥k ≤2
T
T −1
X
t=0
D
Gt, bYt
E
+ k log d
ηT
+ k∆
η .
The complexity of Algorithm 28 is
O

ndk · T 2
∆2 log2
 dT
∆δ

,
and the cost of providing (1 ± ϵ)-approximate access to quadratic forms for any ﬁxed n vectors
{vi}i∈[n] ⊂Rd through any bYt for any ϵ ∈(0, 1), with failure probability at most δ, is
O

nd · T
ϵ log
1
ϵ

log
nd
δ

.
Proof. We claim ﬁrst that it suﬃces to show that the conclusion of Proposition 25 holds in each
iteration t. To see why this is enough, matrix H¨older on the conclusion of Proposition 23 yields
⟨Gt, Yt⟩≤
D
Gt, bYt
E
+ ∥Gt∥op
 bYt −Yt

tr ≤
D
Gt, bYt
E
+ k∆∥Gt∥op ≤
D
Gt, bYt
E
+ k∆
2η
=⇒1
T
T −1
X
t=0
⟨Gt, U⟩≤2
T
T −1
X
t=0
D
Gt, bYt
E
+ k log d
ηT
+ k∆
η .
In the last inequality in the ﬁrst line, we used the assumption that ηG ⪯1
2I. Supremizing over
U, and using monotonicity of the gain matrices, yields the conclusion. Next, we prove that calling
ApproxProject is valid. Note ∇r∗is invariant under shifts by the identity, so it suﬃces to ﬁrst shift
S0 to be I and hence λmin = 1 is a valid bound. Since for all t ≥0, the change in St (i.e. ηGt)
is positive semideﬁnite and bounded by I, we can set λmax = t + 2 in the call. Finally, the failure
probability comes from a union bound over all iterations, and the overall complexity is T times the
cost of a single ApproxProject operation, given by Proposition 25.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
302
7.4
Matrix dictionary recovery SDP solvers
In this section, we develop general solvers for the types of structured "mixed packing-covering"
problems deﬁned in Section 7.1.3, which we collectively call matrix dictionary recovery SDPs.
In Section 7.4.1, we solve a basic version of this problem where B = I, i.e. the constraints are
multiples of the identity. In Section 7.4.2, we give a more general solver able to handle arbitrary
constraints, whose runtime depends polylogarithmically on the conditioning of said constraints. Our
main results Theorems 42 and 43 are stated in Section 7.1.3, and we provide full proofs here.
7.4.1
Identity constraints
In this section, we consider the special case of the problem (7.7), (7.8) in which B = I. To solve this
problem, we ﬁrst develop a framework for solving the decision variant of the problem (7.7), (7.8).
Given a set of matrices {Mi}i∈[n] ∈Sd
≥0 and a parameter κ ≥1, we wish to determine
does there exist w ∈Rn
≥0 such that λmax

X
i∈[n]
wiMi

≤κλmin

X
i∈[n]
wiMi

?
(7.58)
We note that the problem (7.58) is a special case of the more general mixed packing-covering
semideﬁnite programming problem deﬁned in [285], with packing matrices {Mi}i∈[n] and covering
matrices {κMi}i∈[n]. We deﬁne an ϵ-tolerant tester for the decision problem (7.58) to be an algorithm
which returns "yes" whenever (7.58) is feasible for the value (1 −ϵ)κ (along with weights w ∈
Rn
≥0 certifying this feasibility), and "no" whenever it is infeasible for the value (1 + ϵ)κ (and can
return either answer in the middle range). After developing such a tester, we apply it to solve the
(approximate) optimization variant (7.7), (7.8) by incrementally searching for the optimal κ.
To develop a tolerant tester for (7.58), we require access to an algorithm for solving the opti-
mization variant of a pure packing SDP,
OPT(v) :=
max
w∈Rn
≥0 : P
i∈[n] wiMi⪯I v⊤w.
(7.59)
The algorithm is based on combining a solver for the testing variant of (7.59), developed in Sec-
tion 7.1.4 with a binary search. We state its guarantees as Proposition 26, and defer a proof to
Appendix F.2.
Proposition 26. Let OPT+ and OPT−be known upper and lower bounds on OPT(v) as in (7.59).
There is an algorithm, Apack, which succeeds with probability ≥1 −δ, whose runtime is
O

Tmv

{Mi}i∈[n]

· log2(ndT(δϵ)−1) log2 d
ϵ5

· T for T = O

log log OPT+
OPT−
+ log 1
ϵ

,

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
303
and returns an ϵ-multiplicative approximation to OPT(v), and w attaining this approximation.
We require one additional tool, a regret analysis of matrix multiplicative weights from [21].
Proposition 27 (Theorem 3.1, [21]). Consider a sequence of gain matrices {Gt}0≤t<T ⊂Sd
≥0,
which all satisfy for step size η > 0, ∥ηGt∥2 ≤1. Then iteratively deﬁning (from S0 := 0)
Yt :=
exp(St)
Tr exp(St), St+1 := St −ηGt,
we have the bound for any U ∈Sd
≥0 with Tr(U) = 1,
1
T
X
0≤t<T
⟨Gt, Yt −U⟩≤log d
ηT
+ 1
T
X
t∈[T ]
η ∥Gt∥2 ⟨Gt, Yt⟩.
Finally, we are ready to state our ϵ-tolerant tester for the decision problem (7.58) as Algorithm 29.
Lemma 96. Algorithm 29 meets its output guarantees (as speciﬁed on Line 2).
Proof. Throughout, assume all calls to Apack and the computation of approximations as given by
Lines 6 and 13 succeed. By union bounding over T iterations, this gives the failure probability.
We ﬁrst show that if the algorithm terminates on Line 15, it is always correct. By the deﬁnition
of Apack, all Gt ⪯κI, so throughout the algorithm, −St+1 ⪯ηκTI ⪯11κ log d
ϵ
. If the check on Line
14 passes, we must have −St+1 ⪰11 log d
ϵ
I, and hence the matrix −
1
t+1St+1 has condition number
at most κ. The conclusion follows since the reweighting ¯x induces St+1.
We next prove correctness in the "no" case. Suppose the problem (7.60) is feasible; we show
that the check in Line 9 will never pass (so the algorithm never returns "no").
Let v⋆
t be the
vector which is entrywise exactly {⟨Mi, Yt⟩}i∈[n], and let v′
t be a
ϵ
10-multiplicative approximation
to v⋆
t such that vt is an entrywise
ϵ
10n-additive approximation to v′
t.
By deﬁnition, it is clear
OPT(κv′
t) ≥(1 −
ϵ
10)OPT(κv⋆
t ). Moreover, by the assumption that all λmax(Mi) ≥1, all wi ≤1
in the feasible region of the problem (7.59). Hence, the combined additive error incurred by the
approximation ⟨κvt, w⟩to ⟨κv′
t, w⟩for any feasible w is
ϵ
10. All told, by the guarantee of Apack,
κ ⟨vt, xt⟩≥

1 −ϵ
10
2
OPT(κv⋆
t ) −ϵ
10, where OPT(κv⋆
t ) =
max
P
i∈[n] wiMi⪯I
w∈Rn
≥0
κ
*
Yt,
X
i∈[n]
wiMi
+
.
(7.62)
However, by feasibility of (7.60) and scale invariance, there exists a w ∈Rn
≥0 with P
i∈[n] wiMi ⪯I
and (1 −ϵ)κ P
i∈[n] wiMi ⪰I. Since Yt has trace 1, this certiﬁes OPT(κv⋆
t ) ≥
1
1−ϵ, and thus
κ ⟨vt, xt⟩≥

1 −ϵ
10
2
·
1
1 −ϵ −ϵ
10 > 1 −ϵ
5.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
304
Algorithm 29: DecideStructuredMPC({Mi}i∈[n], κ, Apack, δ, ϵ)
1 Input: {Mi}i∈[n] ∈Sd×d
≥0
such that 1 ≤λmax(Mi) ≤2 for all i ∈[n], κ > 1, Apack which on
input v ∈Rn
≥0 returns w ∈Rn
≥0 satisfying (recalling deﬁnition (7.59))
X
i∈[n]
wiMi ⪯I, v⊤w ≥

1 −ϵ
10

OPT(v), with probability ≥1−δ
2T for some T = O
κ log d
ϵ2

,
failure probability δ ∈(0, 1), tolerance ϵ ∈(0, 1);
2 Output: With probability ≥1 −δ: "yes" or "no" is returned. The algorithm must return
"yes" if there exists w ∈Rn
≥0 with
λmax

X
i∈[n]
wiMi

≤(1 −ϵ)κλmin

X
i∈[n]
wiMi

,
(7.60)
and if "yes" is returned, a vector w is given with
λmax

X
i∈[n]
wiMi

≤(1 + ϵ)κλmin

X
i∈[n]
wiMi

.
(7.61)
;
3 η ←
ϵ
10κ, T ←
l
10 log d
ηϵ
m
, Y0 ←1
dI, S0 ←0;
4 for 0 ≤t < T do
5
Yt ←
exp(St)
Tr exp(St);
6
vt ←entrywise nonnegative ( ϵ
10,
ϵ
10κn)-approximations to {⟨Mi, Yt⟩}i∈[n], with
probability ≥1 −
δ
4T ;
7
xt ←Apack(κvt);
8
Gt ←κ P
i∈[n][xt]iMi;
9
if κ ⟨xt, vt⟩< 1 −ϵ
5 then
10
Return: "no";
11
St+1 ←St −ηGt;
12
τ ←log d
ϵ -additive approximation to λmin(−St+1), with probability ≥1 −
δ
4T ;
13
if τ ≥12 log d
ϵ
then
14
Return: ("yes", ¯x) for ¯x :=
1
t+1
P
0≤s≤t xs;
15 Return: ("yes", ¯x) for ¯x := 1
T
P
0≤t<T xt;

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
305
Hence, whenever the algorithm returns "no" it is correct. Assume for the remainder of the proof
that "yes" is returned on Line 18.
Next, we observe that whenever A succeeds on iteration t,
P
i∈[n][xt]iMi ⪯I, and hence in every iteration we have ∥Gt∥2 ≤κ. Proposition 27 then gives
1
T
X
0≤t<T
⟨Gt, Yt −U⟩≤log d
ηT
+ 1
T
X
t∈[T ]
η ∥Gt∥2 ⟨Gt, Yt⟩, for all U ∈Sd
≥0 with Tr(U) = 1.
Rearranging the above display, using η ∥Gt∥2 ≤
ϵ
10, and minimizing over U yields
λmin

1
T
X
0≤t<T
Gt

≥1 −
ϵ
10
T
X
0≤t<T
⟨Gt, Yt⟩−log d
ηT
≥1 −
ϵ
10
T
X
0≤t<T
⟨Gt, Yt⟩−ϵ
10.
The last inequality used the deﬁnition of T. However, by deﬁnition of vt, we have for all 0 ≤t < T,
⟨Yt, Gt⟩= κ
X
i∈[n]
[xt]i ⟨Mi, Yt⟩≥

1 −ϵ
10

κ ⟨xt, vt⟩≥

1 −ϵ
10
 
1 −ϵ
5

≥1 −3ϵ
10.
(7.63)
The second-to-last inequality used that Line 9 did not pass. Combining the previous two displays,
κλmin

X
i∈[n]
¯xiMi

= λmin

1
T
X
0≤t<T
Gt

≥

1 −ϵ
10
 
1 −3ϵ
10

−ϵ
10 ≥1 −ϵ
2.
On the other hand, since all 0 ≤t < T have P
i∈[n][xt]iMi ⪯I, by convexity P
i∈[n] ¯xiMi ⪯I.
Combining these two guarantees and (1 + ϵ)(1 −ϵ
2) ≥1 shows ¯x is correct for the "yes" case.
We bound the runtime complexities of Lines 6, 7, and 13 of Algorithm 29 in the following sections.
Approximating inner products
In this section, we bound the complexity of Line 6 of Algorithm 29. We will use the following two
standard helper results on random projections and approximation theory.
Fact 11 (Johnson-Lindenstrauss [160]). For 0 ≤ϵ ≤1, let k = Θ
  1
ϵ2 log d
δ

for an appropriate
constant. For Q ∈Rk×d with independent uniformly random unit vector rows in Rd scaled down by
1
√
k, with probability ≥1 −δ for any ﬁxed v ∈Rd,
(1 −ϵ) ∥Qv∥2
2 ≤∥v∥2
2 ≤(1 + ϵ) ∥Qv∥2
2 .
Fact 12 (Polynomial approximation of exp [466], Theorem 4.1). Let M ∈Sd
≥0 have M ⪯RI. Then
for any δ > 0, there is an explicit polynomial p of degree O(
q
R log 1
δ + log2 1
δ ) with
exp(−M) −δI ⪯p(M) ⪯exp(−M) + δI.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
306
We also state a simple corollary of Fact 12.
Corollary 27. Given R > 1, M ∈Sd
≥0, and κ with M ⪯κI, we can compute a degree-O(
√
κR + R)
polynomial p such that for P = p(M),
exp (−M) −exp (−R) I ⪯P ⪯exp (−M) + exp (−R) I.
Using these tools, we next demonstrate that we can eﬃciently approximate the trace of a negative
exponential of a bounded matrix, and quadratic forms through it.
Lemma 97. Given M ∈Sd
≥0, R, κ, ϵ > 0 such that λmin(M) ≤R and λmax(M) ≤κR, δ ∈(0, 1),
we can compute an ϵ-multiplicative approximation to Tr exp(−M) with probability ≥1 −δ in time
O
 
Tmv(M) · √κR · log d
δ
ϵ2
!
.
Proof. First, with probability at least 1 −δ, choosing k = O( 1
ϵ2 log d
δ ) in Fact 11 and taking a union
bound guarantees that for all rows j ∈[d], we have
Q

exp

−1
2M

j:

2
2
is a ϵ
3-multiplicative approximation of


exp

−1
2M

j:

2
2
.
Condition on this event in the remainder of the proof. The deﬁnition
Tr exp(−M) =
X
j∈[d]


exp

−1
2M

j:

2
2
,
and the sequence of equalities
X
j∈[d]
Q

exp

−1
2M

j:

2
2
= Tr

exp

−1
2M

Q⊤Q exp

−1
2M

= Tr
 Q exp (−M) Q⊤
=
X
ℓ∈[k]
exp

−1
2M

Qℓ:

2
2
,
implies that it suﬃces to obtain a ϵ
3-multiplicative approximation to the last sum in the above display.
Since Tr exp(−M) ≥exp(−R) by the assumption on λmin(M), it then suﬃces to approximate each
term
exp(−1
2M)Qℓ:
2
2 to an additive
ϵ
3k exp(−R).
For simplicity, ﬁx some ℓ∈[k] and denote
q := Qℓ:; recall ∥q∥2
2 = 1
k from the deﬁnition of Q in Fact 11.
By rescaling, it suﬃces to demonstrate that on any unit vector q ∈Rd, we can approximate
exp(−1
2M)q
2
2 to an additive ϵ
3 exp(−R). To this end, we note that (after shifting the deﬁnition of

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
307
R by a constant) Corollary 27 provides a matrix P with Tmv(P) = O(Tmv(M) · √κR) and
exp (−M) −ϵ
3 exp (−R) I ⪯P ⪯exp (−M) + ϵ
3 exp (−R) I,
which exactly meets our requirements by taking quadratic forms. The runtime follows from the cost
of applying P to each of the k = O( 1
ϵ2 log d
δ ) rows of Q.
Lemma 98. Given M ∈Sd
≥0 and κ with M ⪯κI, 1 > c > 0, δ ∈(0, 1), ϵ > 0, and a set of
matrices {Mi}i∈[n] with decompositions of the form (7.9) and 1 ≤λmax(Mi) ≤2 for all i ∈[n], we
can compute (ϵ, c)-approximations to all
{⟨Mi, exp(−M)⟩}i∈[n] ,
with probability ≥1 −δ in time
O

Tmv
 M, {Vi}i∈[n]

· √κ log c
m · log mn
δ
ϵ2

.
Proof. First, observe that for all i ∈[n], letting {v(i)
j }j∈[m] be columns of Vi ∈Rd×m, we have
⟨Mi, exp(−M)⟩=
X
j∈[m]

v(i)
j
⊤
exp(−M)

v(i)
j

.
Hence, to provide an (ϵ, c) approximation to ⟨Mi, exp(−M)⟩it suﬃces to provide, for all i ∈[n],
j ∈[m], an (ϵ, c
m)-approximation to

v(i)
j
⊤
exp(−M)

v(i)
j

. As in the proof of Lemma 97, by
taking a union bound it suﬃces to sample a Q ∈Rk×d for k = O( 1
ϵ2 log mn
δ ) and instead compute all
∥Q exp(−1
2M)v(i)
j ∥2
2 to additive error
c
m. We will instead show how to approximate, for arbitrary
vectors q, v with norm at most 1,

q, exp

−1
2M

v
2
to additive error
c
2m.
By letting q range over rows of Q renormalized by
√
k, and scaling all v(i)
j
by a factor of
√
2, this yields
the desired result. To this end, consider using ⟨q, Pv⟩2 for some P with −c
6mI ⪯P −exp(−1
2M) ⪯
c
6mI. Letting the diﬀerence matrix be D := P −exp(−1
2M), we compute

q⊤exp

−1
2M

v
2
−
 q⊤Pv
2 = 2

q⊤exp

−1
2M

v
  q⊤Dv

+
 q⊤Dv
2
≤2 ∥D∥2 + ∥D∥2
2 ≤
c
2m.
We used q and v have ℓ2 norm at most 1, exp(−1
2M) ⪯I, and ∥D∥2 ≤
c
6m ≤1. Hence, ⟨q, Pv⟩2 is a

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
308
valid approximation. The requisite P is given by Corollary 27 with Tmv(P) = O(Tmv(M)·√κ log c
m).
Finally, the runtime follows from ﬁrst applying P to rows of Q to explicitly form eQ with k rows,
and then computing all ∥eQv(i)
j ∥2
2 for all i ∈[n], j ∈[m].
Combining Lemmas 97 and 98, we bound the cost of Line 6 in Algorithm 29.
Lemma 99. We can implement Line 6 of Algorithm 29 in time
O
 
Tmv
 {Vi}i∈[n]

· √κ · log3( mndκ
δϵ
)
ϵ3
!
.
Proof. Since −St is an explicit linear combination of {Mi}i∈[n], we have Tmv(St) = O(Tmv({Vi}i∈[n])).
We ﬁrst obtain a
ϵ
30 approximation to the denominator in Line 6 within the required time by apply-
ing Lemma 97 with κ ←O(κ), R ←O( log d
ϵ ), ϵ ←
ϵ
30, and adjusting the failure probability by O(T).
The bound on λmin(−St) comes from the check on Line 13 and the algorithm yields the bound
on λmax(−St). Next, we obtain a ( ϵ
30,
ϵ
30κn) approximation to each numerator in Line 6 within
the required time by using Lemma 98 with κ ←O( κ log d
ϵ
) and adjusting constants appropriately.
Combining these approximations to the numerators and denominator yields the result.
Implementing a packing oracle
In this section, we bound the complexity of Line 7 of Algorithm 29 by using Proposition 26.
Lemma 100. We can implement Line 7 of Algorithm 29 in time
O
 
Tmv
 {Vi}i∈[n]

· log5( ndκ
δϵ )
ϵ5
!
.
Proof. First, we observe that the proof of the "no" case in Lemma 96 demonstrates that in all calls
to A, we can set our lower bound OPT−= 1 −O(ϵ), since the binary search of Proposition 26
will never need to check smaller values to determine whether the test on Line 9 passes. On the
other hand, the deﬁnition of OPT(κv⋆
t ) in (7.62), as well as OPT(κvt) ≤(1 +
ϵ
10)OPT(κv⋆
t ) by the
multiplicative approximation guarantee, shows that it suﬃces to set OPT+ ≤(1 + O(ϵ))κ.
We will use the algorithm of Proposition 26 as Apack in Algorithm 29. In our setting, we argued
OPT+
OPT−= O(κ), giving the desired runtime bound via Proposition 26.
Approximating the smallest eigenvalue
In this section, we bound the complexity of Line 13 of Algorithm 29, which asks to approximate
the smallest eigenvalue of a matrix M to additive error. At a high level, our strategy is to use the
power method on the negative exponential exp(−M), which we approximate to additive error via
Corollary 27. We ﬁrst state a guarantee on the power method from [404].

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
309
Fact 13 (Theorem 1, [404]). For any δ ∈(0, 1) and M ∈Sd
≥0, there is an algorithm, Power(M, δ),
which returns with probability at least 1 −δ a value V such that λmax(M) ≥V ≥0.9λmax(M). The
algorithm runs in time O(Tmv(M) log d
δ ), and is performed as follows:
1. Let u ∈Rd be a random unit vector.
2. For some ∆= O(log d
δ ), let v ←
M∆u
∥M∆u∥2 .
3. Return ∥Mv∥2.
Lemma 101. We can implement Line 13 of Algorithm 29 in time
O
 
Tmv
 {Vi}i∈[n]

· √κ · log2( dκ
δϵ )
ϵ
!
.
Proof. Throughout, denote M := −St+1, L := λmin(M) and R := log d
ϵ , and note that (assuming all
previous calls succeeded), we must have L ≤14R since the previous iteration had L ≤13R and St
is changing by an O(ϵ)-spectrally bounded matrix each iteration. It suﬃces to obtain V with
0.9(exp(−L) −exp(−20R)) ≤V ≤exp(−L),
(7.64)
and then return −log(V ), to obtain an R-additive approximation to L. To see this, it is immediate
that −log(V ) ≥L from the above display. Moreover, for the given ranges of L and R, it is clear
exp (−20R + L) ≤exp(−6R) ≤1 −3
2R
=⇒exp(−L) −exp(−20R) ≥exp(−L) · 3
2R
=⇒exp(−L) −exp(−20R) ≥exp

−L −2R
3

=⇒log

1
exp(−L) −exp(−20R)

≤L + 2R
3 .
Combining with
−log(V ) ≤log

1
exp(−L) −exp(−20R)

+ log 10
9 ≤log

1
exp(−L) −exp(−20R)

+ R
3
yields the claim. It hence suﬃces to provide V satisfying (7.64) in the requisite time. To do so, we
ﬁrst use Corollary 27 with κ ←O( κ log d
ϵ
) to produce P with Tmv(P) = O(Tmv(M) · √κ · log d
ϵ ) and
exp(−M) −1
2 exp(−20R)I ⪯P ⪯exp(−M) + 1
2 exp(−20R)I.
The conclusion follows by applying Fact 40 to P −1
2 exp(−20R)I and adjusting δ.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
310
Runtime of the optimization variant
Finally, we put these pieces together to solve the optimization variant of (7.7), (7.8). We begin by
stating the runtime of Algorithm 29, which follows from combining Lemmas 99, 100, and 101.
Corollary 28. Algorithm 29 can be implemented in time
O
 
Tmv({Vi}) · κ1.5 · log6( mndκ
δϵ
)
ϵ7
!
.
Theorem 42. Given matrices {Mi}i∈[n] with explicit factorizations (7.9), such that (7.7) is feasible
for B = I and some κ ≥1, we can return weights w ∈Rn
≥0 satisfying (7.8) with probability ≥1 −δ
in time
O
 
Tmv({Vi}i∈[n]) · κ1.5 · log6( mndκ
δϵ
)
ϵ8
!
.
Proof. First, to guarantee all Mi satisfy 1 ≤λmax(Mi) ≤2, we exploit the scale-invariance of the
problem (7.7), (7.8) and rescale each matrix by a 2-approximation to its largest eigenvalue. This
can be done with Fact 40 and does not bottleneck the runtime.
Next, we perform an incremental search on κ initialized at 1, and increasing in multiples of
1 + ϵ
3, returning the ﬁrst guess of κ such that Algorithm 29 returns "yes." This will yield an ϵ-
approximation to the optimal κ, and the runtime follows from combining Corollary 28 with the
overhead of this incremental search, which runs O( log κ
ϵ ) times; we do not lose an extra logarithmic
factor from the search due to the geometric sequence κ1.5+κ1.5(1−O(ϵ))+. . . summing to O( κ
ϵ ).
7.4.2
General constraints
In this section, we consider a more general setting in which there is a constraint matrix B in the
problem (7.7), (7.8) which we have matrix-vector product access to, but we cannot eﬃciently invert
B. We show that we can obtain a runtime similar to that in Theorem 42 (up to logarithmic factors
and one factor of √κ), with an overhead depending polylogarithmically on α and β deﬁned in (10.32)
and restated here for convenience:
B ⪯M(1) ⪯αB, I ⪯B ⪯βI.
Homotopy method preliminaries
Our algorithm will be a "homotopy method" which iteratively ﬁnds reweightings of the {Mi}i∈[n]
which (1+ϵ)κ-approximate B+λM(1), for a sequence of λ values. More speciﬁcally, we ﬁrst bound
the required range of λ via two simple observations.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
311
Lemma 102. Let λ ≥1
ϵ . Then,
B + λM(1) ⪯
X
i∈[n]
(1 + λ)Mi ⪯(1 + ϵ) (B + λM(1)) .
Proof. The ﬁrst inequality is immediate from (10.32). The second follows from B ⪰0.
Lemma 103. Let λ ≤ϵκ
2α, and let w ∈Rn
≥0 satisfy
B + λM(1) ⪯
X
i∈[n]
wiMi ⪯(1 + ϵ)κ (B + λM(1)) .
Then, the same w satisﬁes
B ⪯
X
i∈[n]
wiMi ⪯(1 + 2ϵ)κB.
Proof. The ﬁrst inequality is immediate. The second is equivalent to
ϵκB ⪰(1 + ϵ)λM(1)
which follows from ϵκ ≥(1 + ϵ)λα and (10.32).
Our homotopy method is driven by the observation that if (7.7) is feasible for a value of κ, then
it is also feasible for the same κ when B is replaced with B + λM(1) for any λ ≥0.
Lemma 104. For any λ ≥0, if (7.7) is feasible for κ ≥1, there also exists w⋆
λ ∈Rn
≥0 with
B + λM(1) ⪯
X
i∈[n]
[w⋆
λ]iMi ⪯κ (B + λM(1)) .
Proof. It suﬃces to choose w⋆
λ = w⋆+ λ1 where w⋆is feasible for (7.7).
To this end, our algorithm will proceed in K phases, where K = ⌈log2
2α
ϵ2κ⌉, setting
λ(0) = 1
ϵ , λ(k) = λ(0)
2k
for all 0 ≤k ≤K.
In each phase k for 0 ≤k ≤K, we will solve the problem (7.7), (7.8) for B ←B + λ(k)M(1).
To do so, we will use the fact that from the previous phase we have access to a matrix which is a
3κ-spectral approximation to B which we can eﬃciently invert.
Lemma 105. Suppose for some λ ≥0, κ ≥1, and 0 < ϵ ≤1
2, we have w ∈Rn
≥0 such that
B + λM(1) ⪯
X
i∈[n]
wiMi ⪯(1 + ϵ)κ(B + λM(1)).

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
312
Then deﬁning D := M(w), we have
B + λ
2 M(1) ⪯D ⪯3κ

B + λ
2 M(1)

.
Proof. This is immediate from the assumption and
B + λM(1) ⪯2

B + λ
2 M(1)

.
Now, Lemma 102 shows we can access weights satisfying (7.8) for B ←B + λ(0)M(1), and
Lemma 103 shows if we can iteratively compute weights satisfying (7.8) for each B + λ(k)M(1),
0 ≤k ≤K, then we solve the original problem up to a constant factor in ϵ. Moreover, Lemma 104
implies that the problem (7.7) is feasible for all phases assuming it is feasible with some value of κ for
the original problem. Finally, Lemma 105 shows that we start each phase with a matrix M(w) which
we can eﬃciently invert using the linear operator in (7.10), which is a 3κ-spectral approximation
to the constraint matrix. We solve this self-contained subproblem in the following section using
rational approximations to the square root, and an appropriate call to Theorem 42.
Inverse square root approximations with a preconditioner
In this section, we show how to eﬃciently approximate the inverse square root of some B ∈Sd
≻0
given access to a matrix M(w) ∈Sd
≻0 satisfying
B ⪯M(w) ⪯κB
and supporting eﬃcient inverse access in the form of (7.10). For brevity in this section we will use
f
Mλ,ϵ to denote the linear operator guaranteeing (7.10) for M(w) + λI. Given this access, we will
develop a subroutine for eﬃciently applying a linear operator R ∈Sd
≻0 such that
(R −B−1
2 )v

2 ≤ϵ
B−1
2 v

2 for all v ∈Rd.
(7.65)
We will also use Tmv to denote Tmv(B) + Tmv(M(w)) + T sol
M for brevity in this section, where T sol
M
is the cost of solving a system in M(w) to constant accuracy (see (7.10)). Our starting point is the
following result in the literature on preconditioned accelerated gradient descent, which shows we can
eﬃciently solve linear systems in combinations of B and I.
Lemma 106. Given any λ ≥0 and ϵ > 0, we can compute a linear operator c
Mλ,ϵ such that
( c
Mλ,ϵ −(B + λI)−1)v

2 ≤ϵ
(B + λI)−1v

2 for all v ∈Rd,

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
313
and
Tmv( c
Mλ,ϵ) = O

Tmv · √κ log κ log 1
ϵ

.
Proof. This follows from Theorem 4.4 in [292], the fact that M(w)+λI is a κ-spectral approximation
to B + λI, and our assumed linear operator f
Mλ,(10κ)−1 which solves linear systems in M(w) + λI
to relative error
1
10κ which can be applied in time O(Tmv · log κ) (the assumption (7.10)).
We hence can apply and (approximately) invert matrices of the form B + λI. We leverage this
fact to approximate inverse square roots using the following approximation result.
Proposition 28 (Lemma 13, [298]). For any matrix B and ϵ ∈(0, 1), there is a rational function
r(B) of degree L = O(log 1
ϵ log κ(B)) such that for all vectors v ∈Rd,
(r(B) −B−1
2 )v

2 ≤ϵ
B−1
2 v

2 .
The rational function r(B) has the form, for {λℓ, νℓ}ℓ∈[L] ∪{νL+1} ⊂R≥0 computable in O(L) time,
r(B) =

Y
ℓ∈[L]
(B + λℓI)




Y
ℓ∈[L+1]
(B + νℓI)−1

.
(7.66)
We note that Proposition 28 as it is stated in [298] is for the square root (not the inverse square
root), but these are equivalent since all terms in the rational approximation commute. We show
how to use Lemma 106 to eﬃciently approximate the inverse denominator in (7.66).
Lemma 107. For any vector v ∈Rd, {νℓ}ℓ∈[L+1] ⊂R≥0 and ϵ > 0, we can compute a linear
operator bDϵ such that for the denominator of (7.66), denoted
D :=
Y
ℓ∈[L+1]
(B + νℓI) ,
we have
( bDϵ −D−1)v

2 ≤ϵ
D−1v

2 ,
and
Tmv( bDϵ) = O

Tmv · √κ · L2 log(κ) log
κL · κ(B)
ϵ

.
Proof. We give our linear operator bDϵ in the form of an algorithm, which applies a sequence of linear
operators to v in the alloted time. Denote Bℓ:= B + νℓI for all ℓ∈[L + 1]. We also deﬁne
Πℓ:=
Y
i∈[ℓ]
B−1
i
for all ℓ∈[L + 1], and Π0 := I.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
314
We deﬁne a sequence of vectors {vℓ}0≤ℓ≤L+1 as follows: let v0 := v, and for all ℓ∈[L + 1] deﬁne
vℓ←c
MνL+2−ℓ,∆vℓ−1 for ∆:=
ϵ
3(L + 1)κ(B)2(L+1) .
Here we use notation from Lemma 106. In particular, for the given ∆, we have for all ℓ∈[L + 1],
vℓ−B−1
L+2−ℓvℓ−1

2 ≤∆∥vℓ−1∥2 .
(7.67)
Our algorithm will simply return vL+1, which is the application of a linear operator to v within the
claimed runtime via Lemma 106. We now prove correctness. We ﬁrst prove a bound on the sizes of
this iterate sequence. By the triangle inequality and (7.67), for each ℓ∈[L + 1] we have
∥vℓ∥2 ≤
vℓ−B−1
L+2−ℓvℓ−1

2 +
B−1
L+2−ℓvℓ−1

2 ≤(1 + ∆)
B−1
L+2−ℓ

2 ∥vℓ−1∥2 .
Applying this bound inductively, we have that
∥vℓ∥2 ≤(1 + ∆)ℓ

Y
i∈[ℓ]
B−1
L+2−ℓ

2

∥v0∥2 ≤3

Y
i∈[ℓ]
B−1
L+2−ℓ

2

∥v0∥2 .
(7.68)
Next, we expand by the triangle inequality that
∥vL+1 −ΠL+1v0∥2 ≤
X
ℓ∈[L+1]
∥ΠL+1−ℓvℓ−ΠL+2−ℓvℓ−1∥2
≤
X
ℓ∈[L+1]
∥ΠL+1−ℓ∥2
vℓ−B−1
L+2−ℓvℓ−1

2
≤
X
ℓ∈[L+1]
∆∥ΠL+2−ℓ∥2 ∥vℓ−1∥2
≤
X
ℓ∈[L+1]
3∆∥ΠL+1∥2 ∥v0∥2 ≤3∆(L + 1) ∥ΠL+1∥2 ∥v0∥2 .
The second inequality used commutativity of all {Bℓ}ℓ∈[L+1] and the deﬁnition of ΠL+2−ℓ, the third
used the guarantee (7.67), the fourth used (7.68) and that all {Bℓ}ℓ∈[L+1] have similarly ordered
eigenvalues, and the last is straightforward.
Finally, correctness of returning vL+1 follows from
D = ΠL+1 and the bound
κ(ΠL+1) ≤κ(B)L+1 =⇒3∆(L + 1) ∥ΠL+1∥2 ∥v0∥2 ≤ϵ ∥ΠL+1v0∥2 .
To see the ﬁrst claim, every Bℓclearly has κ(Bℓ) ≤κ(B), and we used the standard facts that for
commuting A, B ∈Sd
≻0, κ(A) = κ(A−1) and κ(AB) ≤κ(A)κ(B).

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
315
At this point, obtaining the desired (7.65) follows from a direct application of Lemma 107 and
the fact that the numerator and denominator of (7.66) commute with each other and B.
Lemma 108. For any vector v ∈Rd and ϵ > 0, we can compute a linear operator Rϵ such that
(Rϵ −B−1
2 )v

2 ≤ϵ
B−1
2 v

2 ,
and
Tmv(Rϵ) = O

Tmv · √κ · log6
κ · κ(B)
ϵ

.
Proof. Let ϵ′ ←ϵ
3, and let D and N be the (commuting) numerator and denominator of (7.66) for
the rational function in Proposition 28 for the approximation factor ϵ′. Let u ←Nv, which we can
compute explicitly within the alloted runtime. We then have from Proposition 28 that
D−1u −B−1
2 v

2 ≤ϵ′ B−1
2 v

2 .
Moreover by Lemma 107 we can compute a vector w within the allotted runtime such that
D−1u −w

2 ≤ϵ′ D−1u

2 ≤ϵ′ B−1
2 v

2 +
D−1u −B−1
2 v

2

≤2ϵ′ B−1
2 v

2 .
The vector w follows from applying an explicit linear operator to v as desired. Finally, by combining
the above two displays we have the desired approximation quality as well:
B−1
2 v −w

2 ≤3ϵ′ B−1
2 v

2 = ϵ
B−1
2 v

2 .
Implementing the homotopy method
In this section, we implement the homotopy method outlined in Section 7.4.2 by using the inverse
square root access given by Lemma 108. We require the following helper result.
Lemma 109. Suppose for some ϵ ∈(0, 1), and M, N ∈Sd
≻0 such that κ(N) ≤β, that
−ϵN−1 ⪯M−1 −N−1 ⪯ϵN−1.
Then,
−9ϵβN2 ⪯M2 −N2 ⪯9ϵβN2.
Proof. The statement is scale-invariant, so suppose for simplicity that I ⪯N ⪯βI.
Also, by
rearranging the assumed bound, we have
−3ϵN ⪯M −N ⪯3ϵN.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
316
Write D = M−N such that ∥D∥2 ≤3ϵ. We bound the quadratic form with an arbitrary unit vector
v ∈Rd:
v⊤(M2 −N2)v
 =
v⊤(N2 −(N + D)2)v

=
v⊤(ND + DN + D2)v

≤3 ∥D∥2 ∥N∥2 ≤9ϵβ.
This completes the proof upon using N2 ⪰I.
We now state our main claim regarding solving (7.7), (7.8) with general B.
Theorem 43. Given matrices {Mi}i∈[n] with explicit factorizations (7.9), such that (7.7) is feasible
for some κ ≥1 and we can solve linear systems in linear combinations of {Mi}i∈[n] to ϵ relative
accuracy in the sense of (7.10) in T sol
M · log 1
ϵ time, and B satisfying
B ⪯M(1) ⪯αB, I ⪯B ⪯βI,
(7.11)
we can return weights w ∈Rn
≥0 satisfying (7.8) with probability ≥1 −δ in time
O
 
 Tmv
 {Vi}i∈[n] ∪{B}

+ T sol
M

· κ2 · log11( mndκβ
δϵ
)
ϵ8
· log α
ϵ
!
.
Proof. We follow Section 7.4.2, which shows that it suﬃces to solve the following problem O(log α
ϵ )
times. We have an instance of (7.7), (7.8) for the original value of κ, and some matrix B with
κ(B) ≤β such that we know w ∈Rn
≥0 with B ⪯M(w) ⪯3κB. We wish to compute w′ with
B ⪯M(w′) ⪯(1 + ϵ)κB.
(7.69)
To do so, we use Lemma 108, which yields a matrix R such that
ϵ
45β B−1
2 ⪯R −B−1
2 ⪯
ϵ
45β B−1
2 and Tmv(R) = O

Tmv
 {Vi}i∈[n] ∪{B}

· √κ · log6
κβ
ϵ

.
By Lemma 109, this implies that
−ϵ
5B ⪯R−2 −B ⪯ϵ
5B.
(7.70)
Now, if we solve (7.7), (7.8) with R−2 in place of B to accuracy 1 + ϵ
5, since the optimal value of κ
has changed by at most a factor of 1 + ϵ
5, it suﬃces to compute w′ such that
R−2 ⪯M(w′) ⪯

1 + 3ϵ
5

κR−2,

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
317
since combined with (7.70) this implies (7.69) up to rescaling w′. Finally, to compute the above
reweighting it suﬃces to apply Theorem 42 with Mi ←RMiR for all i ∈[n]. It is clear these
matrices satisfy (7.9) with the decompositions given by Vi ←RVi, and we can apply these matrices
in time Tmv(R) + Tmv(Vi). It is straightforward to check that throughout the proof of Theorem 42,
this replaces each cost of Tmv({Vi}i∈[n]) with Tmv({Vi}i∈[n]) + Tmv(R), giving the desired runtime.
7.5
Schatten packing
7.5.1
Mirror descent interpretation of [379]
We begin by reinterpreting the [379] solver, which achieves the state-of-the-art parallel runtime for
packing LPs10. An (ℓ∞) packing LP algorithm solves the following decision problem.11.
Problem 2 (ℓ∞packing linear program). Given entrywise nonnegative A ∈Rd×n
≥0 , either ﬁnd
primal solution x ∈∆n with ∥Ax∥∞≤1 + ϵ or dual solution y ∈∆d with A⊤y ≥(1 −ϵ)1.
Algorithm 30: PackingLP(A, ϵ)
1 Input: A ∈Rd×n
≥0 , ϵ ∈[0, 1
2];
2 K ←3 log(d)
ϵ
, η ←K−1, T ←4 log(d) log(nd/ϵ)
ϵ2
;
3 [w0]i ←
ϵ
n2d for all i ∈[n], z ←0, t ←0;
4 while Awt ≤K1, ∥wt∥1 ≤K do
5
vt ←
exp(Awt)
∥exp(Awt)∥1 ;
6
gt ←max(0, 1 −A⊤vt) entrywise;
7
wt+1 ←wt ◦(1 + ηgt), z ←z + vt, t ←t + 1;
8
if t ≥T then
9
Return: y ←1
T z;
10 Return: x ←
wt
∥wt∥1 ;
The following result is shown in [379].
Proposition 29. PackingLP (Algorithm 30) solves Problem 2 in O(nnz(A) · log(d) log(nd/ϵ)
ϵ2
) time.
Oour interpretation of the analysis of [379], combines two ingredients: a potential argument and
mirror descent, which yields a dual feasible point if ∥wt∥1 did not grow suﬃciently.
Potential argument. The potential used by [379] is log(P
j∈[d] exp([Awt]j)) −∥wt∥1, well-known
to be a O(log d)-additive approximation of ∥Awt∥∞−∥wt∥1. As soon as ∥Awt∥∞or ∥wt∥1 reaches
10The [379] solver also generalizes to covering and mixed objectives; we focus on packing in this work.
11Packing linear programs are sometimes expressed as the optimization problem maxx≥0,Ax≤1 ∥x∥1, similarly to
(7.12); these problems are equivalent up to a standard binary search, see e.g. discussion in [285].

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
318
the scale O( log d
ϵ ), by nonnegativity this becomes a multiplicative guarantee, motivating the set-
ting of threshold K. To prove the potential is monotone, [379] uses step size K−1 and a Taylor
approximation; combining with the termination condition yields the desired claim.
Mirror descent. To certify that wt grows suﬃciently (e.g. the method terminates in few iterations,
else dual feasibility holds), we interpret the step wt+1 ←wt◦(1+ηgt) as approximate entropic mirror
descent. Speciﬁcally, we track the quantity P
0≤t<T ⟨ηgt, u⟩, and show that if ∥wt∥1 has not grown
suﬃciently, then it must be bounded for every u ∈∆n, certifying dual feasibility. Formally, for any
gt sequence and u ∈∆n, we show
O(log(nd/ϵ)) + log
∥wT ∥1
∥w0∥1

≥
X
0≤t<T
⟨ηgt, u⟩≥η
X
0≤t<T

1 −A⊤vt, u

.
The last inequality followed by gt being an upwards truncation. If ∥wT ∥1 is bounded (else, we have
primal feasibility), we show the entire above expression is bounded O(log nd
ϵ ) for any u. Thus, by
setting T = O( log(nd/ϵ)
ηϵ
) and choosing u to be each coordinate indicator, it follows that the average
of all vt is coordinatewise at least 1 −ϵ, and solves Problem 2 as a dual solution.
Our gt is chosen as the (truncated) gradient of the function used in the potential analysis, so
its form allows us to interpret dual feasibility (e.g. vt has ℓ1 norm 1 and is a valid dual point).
Our analysis patterns standard mirror descent, complemented by side information which says that
lack of a primal solution can transform a regret guarantee into a feasibility bound. We apply this
framework to analyze ℓp variants of Problem 2, via diﬀerent potentials; nonetheless, our proofs are
quite straightforward upon adopting this perspective, and we believe it may yield new insights for
instances with positivity structure.
7.5.2
ℓp-norm packing linear programs
In this section, we give a complete self-contained example of the framework proposed in Section 7.5.1,
for approximately solving ℓp norm packing linear programs. Speciﬁcally, we now consider the gen-
eralization of Problem 2 to ℓp norms; throughout, q =
p
p−1 is the dual norm.
Problem 3 (ℓp packing linear program). Given entrywise nonnegative A ∈Rd×n
≥0 , either ﬁnd primal
solution x ∈∆n with ∥Ax∥p ≤1 + ϵ or dual solution y ∈Rd
≥0, ∥y∥q = 1 with A⊤y ≥(1 −ϵ)1.
For p = log d
ϵ , Problem 3 recovers Problem 2 up to constants as ℓp multiplicatively approximates
ℓ∞by 1 + ϵ. We now state our method for solving Problem 3 as Algorithm 31.
Other than changing parameters, the only diﬀerence from Algorithm 30 is that v is a point with
unit ℓq norm induced by the gradient of our potential Φt. We state our main potential fact, whose
proof is based straightforwardly on Taylor expanding ∥·∥p, and deferred to Appendix F.3 for brevity.
Lemma 110. In all iterations t of Algorithm 31, deﬁning Φt := ∥Awt∥p −∥wt∥1, Φt+1 ≤Φt.

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
319
Algorithm 31: PNormPacking(A, ϵ, p)
1 Input: A ∈Rd×n
≥0 , ϵ ∈[0, 1
2], p ≥2;
2 η ←p−1, T ←4p log( nd
ϵ )
ϵ
;
3 [w0]i ←
ϵ
n2d for all i ∈[n], z ←0, t ←0;
4 while ∥wt∥1 ≤ϵ−1 do
5
gt ←max(0, 1 −A⊤(vt)p−1) entrywise, for vt ←
Awt
∥Awt∥p ;
6
wt+1 ←wt ◦(1 + ηgt), z ←z + (vt)p−1, t ←t + 1;
7
if t ≥T then
8
Return: y =
z
∥z∥q ;
9 Return: x =
wt
∥wt∥1 ;
We now prove our main result, which leverages the potential bound following the framework of
Section 7.5.1. In the proof, we assume that entries of A are bounded by nϵ−1; this does not incur
more loss than a constant multiple of ϵ in the guarantees, and a proof can be found as Lemma 291.
Theorem 47. Algorithm 31 runs in time O(nnz(A) · p log(nd/ϵ)
ϵ
). Further, its output solves Prob-
lem 3.
Proof. The runtime follows from Line 7 (each iteration cost is dominated by multiplication through
A), so we prove correctness. Deﬁne potential Φt as in Lemma 110, and note that as w0 =
ϵ
n2d1,
Φ0 ≤∥Aw0∥p ≤1
n ∥1∥p ≤1.
The second inequality followed from our assumption on A entry sizes (Lemma 291). If Algorithm 31
breaks out of the while loop of Line 4, we have by Lemma 110 that for x returned on Line 11,
∥Awt∥p −∥wt∥1 ≤1 =⇒∥Ax∥p ≤1 + ∥wt∥1
∥wt∥1
≤1 + ϵ.
Thus, primal feasibility is always correct. We now prove correctness of dual feasibility. First, let
Vx(u) = P
i∈[n] ui log( ui
xi ) be the Kullback-Leibler divergence from x to u, for x, u ∈∆d. Deﬁne the
normalized points xt =
wt
∥wt∥1 in each iteration. Expanding deﬁnitions,
Vxt+1(u) −Vxt(u) =
X
i∈[n]
ui log
[xt]i
[xt+1]i
=
X
i∈[n]
ui

log
∥wt+1∥1
∥wt∥1

+ log

1
1 + η[gt]i

≤−η(1 −η) ⟨gt, u⟩+ log
∥wt+1∥1
∥wt∥1

.
(7.71)

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
320
The only inequality used the bounds, for g, η ∈[0, 1],
log

1
1 + ηg

≤g log

1
1 + η

≤−η(1 −η)g.
Telescoping (7.71) over all T iterations, and using Vx0(u) ≤log n for all u ∈∆n since x0 is uniform,
we have that whenever Line 4 is not satisﬁed before the check on Line 7 (i.e. t ≥T),
η(1 −η)
X
0≤t<T
⟨gt, u⟩≤log
∥wT ∥1
∥w0∥1

+ Vx0(u) ≤log
nd
ϵ2

+ log n ≤2 log
nd
ϵ

.
(7.72)
The last inequality used ∥wT ∥1 ≤ϵ−1 by assumption, and ∥w0∥1 =
ϵ
nd. Next, since each gt ≥
1 −A⊤(vt)p−1 entrywise, deﬁning ¯z = z
T ,
X
0≤t<T
⟨gt, u⟩≥
X
0≤t<T

1 −A⊤(vt)p−1, u

= T

1 −A⊤¯z, u

, for all u ∈∆n.
(7.73)
Combining (7.72) and (7.73), and rearranging, yields by deﬁnition of T,

1 −A⊤¯z, u

≤2 log( nd
ϵ )
Tη(1 −η) ≤4p log( nd
ϵ )
T
≤ϵ =⇒A⊤¯z ≥1 −ϵ entrywise.
The last claim follows by setting u to be each coordinate-sparse simplex vector.
Finally, since
¯z
∥¯z∥q =
z
∥z∥q , to show that y is a correct dual solution to Problem 3 it suﬃces to show ∥¯z∥q ≤1.
This follows as ¯z is an average of the (vt)p−1, convexity of ℓq norms, and that for all t,
(vt)p−1q
q =
X
j∈[d]
vp
t =
X
j∈[d]
[Awt]p
j
∥Awt∥p
p
= 1.
7.5.3
Schatten-norm packing semideﬁnite programs
We generalize Algorithm 31 to solve Schatten packing semideﬁnite programs, which we now deﬁne.
Problem 4. Given {Ai}i∈[n] ∈Sd
≥0, either ﬁnd primal solution x ∈∆n with
P
i∈[n] xiAi

p ≤1+ϵ
or dual solution Y ∈Sd
≥0, ∥Y∥q = 1 with ⟨Ai, Y⟩≥1 −ϵ for all i ∈[n].
We assume that p is an odd integer for simplicity (suﬃcient for our applications), and leave for
interesting future work the cases when p is even or noninteger. The potential used in the analysis
and an overall guarantee are stated here, and deferred to Appendix F.3. The proofs are simple
modiﬁcations of Lemma 110 and Theorem 47 using trace inequalities (similar to those in [285]) in
place of scalar inequalities, as well as eﬃcient approximation of quantities in Line 5 via the standard

CHAPTER 7. MATRIX MULTIPLICATIVE WEIGHTS AND FRIENDS
321
Algorithm 32: SchattenPacking({Ai}i∈[n], ϵ, p)
1 Input: {Ai}i∈[n] ∈Sd
≥0, ϵ ∈[0, 1
2], p ≥2;
2 η ←p−1, T ←4p log( nd
ϵ )
ϵ
;
3 [w0]i ←
ϵ
n2d for all i ∈[n], z ←0;
4 while ∥wt∥1 ≤ϵ−1 do
5
gt ←max

0, 1 −
D
Ai, Vp−1
t
E
entrywise, for Vt ←
P
i∈[n][wt]iAi
∥
P
i∈[n][wt]iAi∥p
;
6
wt+1 ←wt ◦(1 + ηgt), Z ←Z + (Vt)p−1, t ←t + 1;
7
if t ≥T then
8
Return: Y =
Z
∥Z∥q ;
9 Return: x =
wt
∥wt∥1 ;
technique of Johnson-Lindestrauss projections.
Lemma 111. In all iterations t of Algorithm 32, deﬁning Φt :=

P
i∈[n][wt]iAi

p −∥wt∥1, Φt+1 ≤
Φt.
Theorem 48. Let p be odd. Algorithm 32 runs in O( p log(nd/ϵ)
ϵ
) iterations, and its output solves
Problem 4.
Each iteration is implementable in O(nnz · p log(nd/ϵ)
ϵ2
), where nnz is the number of
nonzero entries amongst all {Ai}i∈[n], losing O(ϵ) in the quality of Problem 4 with probability 1 −
poly((nd/ϵ)−1).
7.5.4
Schatten packing with a ℓ∞constraint
We remark that the framework outlined in Section 7.5.1 is ﬂexible enough to handle mixed-norm
packing problems. Speciﬁcally, we give the following guarantee which will used in an application.
Proposition 30. Following Theorem 48's notation, let p be odd, {Ai}i∈[n] ∈Sd
≥0, 0 < ϵ = O(α),
and
min
x∈∆n
∥x∥∞≤1+α
n
∥A(x)∥p = OPT.
(7.74)
for A(x) := P
i∈[n] xiAi. Given estimate of OPT exponentially bounded in nd
ϵ , there is a procedure
calling Algorithm 76 O(log nd
ϵ ) times giving x ∈∆n with ∥x∥∞≤(1+α)(1+ϵ)
n
, ∥A(x)∥p ≤(1+ϵ)OPT.
Algorithm 76 runs in O( log(nd/ϵ) log n
ϵ2
) iterations, each implementable in time O(nnz · p log(nd/ϵ)
ϵ2
).
Our method, found in Appendix F.3, approximately solves (7.74) by ﬁrst applying a stan-
dard binary search to place A(x) on the right scale, for which it suﬃces to solve an approxi-
mate decision problem.
Then, we apply a truncated mirror descent procedure on the potential
Φ(w) = log(exp(∥A(w)∥p) + exp(
n
1+α ∥w∥∞)) −∥w∥1, and prove correctness for solving the decision
problem following the framework we outlined in Section 7.5.1.

Chapter 8
High-Dimensional Robust
Statistics in Almost-Linear Time
This chapter is based on [289, 180, 288, 181] with Ilias Diakonikolas, Arun Jambulapati, Daniel M.
Kane, Daniel Kongsgaard, Jerry Li, and Tselil Schramm.
8.1
Organization
In this chapter, we present several related results on eﬃciently performing statistical estimation or
learning tasks in high dimensions, under the presence of strong contamination (an adversary who
is allowed to replace a bounded fraction of the dataset with arbitrary points from the support of
the distribution one wishes to learn about). Performing these tasks in high dimensions with an
explicit (polynomial-time) algorithm is a challenging endeavor, and it was not until recently that
the polynomial-time tractability of the tasks considered in this chapter was settled [175, 337, 176].
The main contributions of this chapter are to go even further, demonstrating that several basic
variants of these tasks are in fact tractable in almost-linear time. To attain these results, we will
build heavily upon SDP technology introduced in Chapter 7.
Because many of the results in this chapter directly build upon similar techniques (utilizing
variants of matrix multiplicative weights and other tools inspired by SDP solvers for certifying and
ﬁltering corrupted datasets), we present them in one cohesive section. We now give an overview of
the organization of this chapter, as well as the problems it studies.
1. Clustering mixture models. We consider the list-decodable mean estimation problem [118],
and its application to robust clustering tasks, in Sections 8.2, 8.3, 8.4, and 8.5.
2. Parameter estimation in generalized linear models.
We consider learning a latent
322

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 323
parameter in GLMs from corrupted observations in Sections 8.6, 8.7, 8.8, 8.9, and 8.10.
3. Principal component analysis. We consider the problem of robustly recovering an approx-
imate top eigenvector of a sub-Gaussian distribution in Section 8.11.
8.2
Clustering mixture models in almost-linear time
We develop novel algorithms achieving almost-optimal runtimes for two closely related fundamental
problems in high-dimensional statistical estimation: clustering well-separated mixture models and
mean estimation in the list-decodable learning ("majority-outlier") regime. Before we formally state
our contributions, we provide the necessary background and motivation for this work.
Clustering well-separated mixture models. Mixture models are a well-studied class of generative
models used widely in practice. Given a family of distributions F, a mixture model M with k
components is speciﬁed by k distributions dist1, . . . , distk ∈F and nonnegative mixing weights
α1, . . . , αk summing to one, and its law is given by P
i∈[k] αi disti.
That is, to draw a sample
from M, we ﬁrst choose i ∈[k] with probability αi, and then draw a sample from disti. When the
weights are all equal to 1
k, we call the mixture uniform. Mixture models, especially Gaussian mixture
models, have been widely studied in statistics since pioneering work of Pearson in 1894 [440], and
more recently, in theoretical computer science [159, 35, 520, 7, 307, 97, 45, 454].
A canonical learning task for mixture models is the clustering problem. Namely, given inde-
pendent samples drawn from M, the goal is to approximately recover which samples came from
which component. To ensure that this inference task is information-theoretically possible, a com-
mon assumption is that M is "well-separated" and "well-behaved": for example, we may assume
each component disti is suﬃciently concentrated (with sub-Gaussian tails or bounded moments),
and that component means have pairwise distance at least ∆, for suﬃciently large ∆. The goal is
then to eﬃciently and accurately cluster samples from M with as small a separation as possible.
The prototypical example is the case of uniform mixtures of bounded-covariance Gaussians, i.e.
mixtures of the form M = P
i∈[k]
1
kN(µi, Σi), where each Σi is unknown and satisﬁes ∥Σi∥op ≤σ2.
Prior to the current work, the fastest known algorithm for this learning problem was due to [7],
building on [520]. Notably, [7] gave a polynomial-time clustering algorithm when ∆= Ω(σ
√
k).
Interestingly, the algorithmic approach of [520, 7] is surprisingly simple and elegant: ﬁrst, run k-PCA
on the set of n samples in Rd to ﬁnd a k-dimensional subspace (which can be shown to approximately
capture the span of the component means), and then perform a distance-based clustering algorithm
in this subspace. The runtime of this algorithm is dominated by eΩ(ndk) - the cost of (approximate)
k-PCA.1 The idea of using k-PCA as a subroutine to solve the clustering problem is very natural
and has also been useful in practice. Indeed, using PCA as a preprocessing step before applying
1Throughout, when convenient, we hide polylogarithmic factors in the sample size and algorithm failure probabil-
ities with the e
O notation. We reserve the terminology "almost-linear" to mean linear up to subpolynomial factors,
and the terminology "nearly-linear" to mean linear up to polylogarithmic factors.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 324
further learning algorithms (such as clustering) is so ubiquitous that it is commonly suggested by
introductory textbooks on machine learning, see e.g. [403].
However, in our setting, since the size of the description this problem is O(nd), the runtime of
k-PCA is oﬀfrom linear time by a factor of roughly k. In many real-world settings, this factor of k is
quite signiﬁcant. For instance, modern image datasets such as ImageNet [170] often have hundreds
or thousands of diﬀerent classes and subclasses [470]. As a result, many clustering tasks on these
datasets often have k of the same order. The resulting overhead would cause many tasks to be
infeasible at scale on these datasets. Yet, despite considerable attention over the last two decades,2
no faster algorithm has been developed for the clustering task. In particular, the runtime of k-PCA
has remained a bottleneck in this setting.
The preceding discussion motivates the following natural question.
Question 1. Can we cluster mixtures of k "well-separated" structured distributions without the use
of k-PCA? More ambitiously, is there a clustering algorithm that runs in (almost)-linear time?
Prior to the current work, this question remained open even for uniform k-mixtures of identity
covariance Gaussians with pairwise mean separation as large as poly(k). In addition to its funda-
mental interest, a runtime improvement of this sort may have signiﬁcant practical implications for
clustering at scale in real-world applications, see e.g. [438, 538], where spectral methods are com-
monly used. As our main contribution, we resolve Question 1 for the general class of mixtures of
bounded-covariance distributions under information-theoretically near-optimal separation.
List-decodable mean estimation. In many statistical settings, including machine learning secu-
rity [62, 81, 494, 176] and exploratory data analysis e.g. in biology [461, 361, 436], datasets contain
arbitrary — and even adversarially chosen — outliers. The central question of the ﬁeld of robust
statistics is to design estimators tolerant to a small amount of unconstrained contamination. Classi-
cal work in this ﬁeld [31, 512, 278, 513, 263, 279] developed robust estimators for many basic tasks,
although with computational costs scaling exponentially in the problem dimension. More recently, a
line of work in computer science, starting with [175, 337], developed the ﬁrst computationally-eﬃcient
learning algorithms (attaining near-optimal error) for various estimation problems. Subsequently,
there has been signiﬁcant progress in algorithmic robust statistics in a variety of settings (see [182]
for a survey).
In many of these works, it is typically assumed that the fraction of corrupted points is less than
1
2. Indeed, when more than half the points are corrupted, the problem is ill-posed: there is not
necessarily a uniquely-deﬁned notion of "uncorrupted samples." While outputting a single accurate
hypothesis in this regime is information-theoretically impossible, one may be able to compute a
small list of hypotheses with the guarantee that at least one of them is accurate. This relaxed notion
of estimation is known as list-decodable learning [56, 118].
2We note that a recent line of work has developed sophisticated polynomial-time clustering algorithms under
smaller separation assumptions, see e.g. [186, 275, 330]. These algorithms leverage higher moments of the distribution
and consequently require signiﬁcantly higher sample and computational complexity.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 325
Deﬁnition 27 (List-decodable learning). Given a parameter 0 < α < 1
2 and a distribution family
F on Rd, a list-decodable learning algorithm takes as input α and a multiset T of n points such
that an unknown α fraction of T are independent samples from an unknown distribution dist ∈F,
and no assumptions are made on the remaining samples. Given T and α, the goal is to output a
"small" list of hypotheses at least one of which is close to the target parameter of dist.
Arguably the most fundamental problem in the list-decodable learning setting is mean estimation,
wherein the goal is to output a small list of hypotheses, one of which is close to the true mean. A
natural problem in its own right, list-decodable mean estimation generalizes the problem of learning
well-separated mixture models (as explained below) and can model important applications such
as crowdsourcing [495, 390] or semi-random community detection in stochastic block models [118].
Moreover, it is particularly useful in the context of semi-veriﬁed learning [118, 390], where a learner
can audit a small amount of trusted data. An important remark is that the parameter α ∈(0, 1
2) can
be quite small in some of these applications and should not necessarily be thought of as a constant.
In addition to applications in clustering mixture models, a concrete example is the crowdsourcing
setting with many unreliable responders studied in [390], where the parameter α is tiny, depending
inversely-polynomially on other problem parameters such as the dimension.
The parameter α in the list-decodable mean estimation setting plays a very similar role to
the parameter 1
k in learning (uniform) mixture models. This is no coincidence: list-decodable mean
estimation can be thought of as a natural robust generalization of clustering well-separated mixtures.
Indeed, if we run a list-decodable mean estimation algorithm on a dataset drawn from a uniform
mixture of k suﬃciently nice and well-separated distributions with α set to 1
k, the output list must
contain a candidate mean which is close to the mean of each component. This is because from the
perspective of the list-learning algorithm, each component could be the "true" unknown distribution
dist, and thus the list must contain a hypothesis close to the mean of this "true" distribution. This
small list of hypotheses can then typically be used to cluster the original dataset. One conceptually
important implication of this observation is that list-decodable mean estimation algorithms also
naturally lead to algorithms for clustering well-separated mixture models (even in the presence of a
small fraction of corrupted samples) — a reduction we formalize in this work.
The ﬁrst polynomial-time algorithm for list-decodable mean estimation, when F is the family of
bounded-covariance distributions, was by [118]. The [118] algorithm was based on black-box calls to
semideﬁnite program solvers and had a large polynomial runtime. Since then, a sequence of works
[179, 136] (as well as developments contained in this thesis) have obtained substantially improved
runtimes for this problem, while retaining the (near-optimal) statistical guarantees of [118].
In
particular, in Sections G.2, G.3, and G.4 we develop an algorithm which runs in time eO( nd
α ) and
achieves near-optimal error (within a polylogarithmic factor).
Interestingly, as in the case of clustering mixture models, the eΩ( nd
α ) runtime dependence of
the algorithm of Sections G.2, G.3, and G.4 is also due to running a k-PCA subroutine — for

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 326
k = Ω( 1
α) — to reduce the problem to a k-dimensional subspace. In more detail, the algorithm of
Sections G.2, G.3, and G.4 can be viewed as a reduction from list-decodable mean estimation to
polylogarithmically many calls to k-PCA (for carefully chosen matrices). Thus, the cost of k-PCA
appears as a runtime barrier in state-of-the-art algorithms for list-decodable mean estimation as
well. In regimes where α is small, the eΩ( nd
α ) runtime is signiﬁcantly sub-optimal in the input size.
This leaves open whether the extraneous linear dependence on α−1 is improvable, and brings us to
our second main question.
Question 2. Can we perform list-decodable mean estimation with near-optimal statistical guarantees
in (almost)-linear time?
In this chapter, we similarly resolve Question 2 for the class of bounded-covariance distributions.
8.2.1
Our results
We answer both Question 1 and Question 2 in the aﬃrmative, up to subpolynomial factors. Perhaps
surprisingly, to resolve the longstanding open problem of clustering mixture models in almost-linear
time, we develop an almost-linear time algorithm for the (much more general) problem of list-
decodable mean estimation. To then solve the clustering problem, we develop a fast post-processing
technique that eﬃciently reduces the clustering task to list-decodable mean estimation. In light of
this development, we begin by presenting our list-decodable estimation result.
Theorem 49 (informal, see Theorem 51). For any ﬁxed constant ϵ0 > 0, there is an algorithm
FastMultiﬁlter with the following guarantee. Let dist be a distribution over Rd with unknown mean
µ∗and unknown covariance Σ with ∥Σ∥op ≤σ2, and let α ∈(0, 1). Given α and a multiset of
n = Ω( d
α) points on Rd such that an α-fraction are i.i.d. draws from dist, FastMultiﬁlter runs in
time O(n1+ϵ0d) and outputs a list L of O(α−1) hypotheses so that with high probability we have
min
ˆµ∈L ∥ˆµ −µ∗∥2 = O
σ log α−1
√α

.
Notably, in the setting of Theorem 49, a sample complexity of Ω( d
α), error of Ω(σα−1
2 ), and list size
Ω(α−1) are all information-theoretically necessary [186]. Hence, up to a log(α−1) factor in the error,
Theorem 49 achieves optimal statistical guarantees for this problem in almost-linear time.
Leveraging Theorem 49, and combining it with a new almost-linear time post-processing proce-
dure of the resulting list, we achieve our almost-linear runtime for clustering well-separated mixtures
under only a second moment bound assumption — even in the presence of a small fraction of outliers.
In more detail, our algorithm can tolerate a fraction of outliers proportional to the relative size of
the smallest true cluster. For brevity, in this introduction, we will state the natural special case
of our clustering result for uniform bounded-covariance mixtures without outliers. We also achieve

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 327
similar (indeed, slightly stronger) guarantees when the mixture components are sub-Gaussian or
have bounded fourth moments.
Theorem 50 (informal, see Corollaries 33, 35, 36). For any ﬁxed constant ϵ0 > 0, there is an
algorithm with the following guarantee. Given a multiset of n = Ω(dk) i.i.d. samples from a uniform
mixture model M = P
i∈[k]
1
k disti, where each component disti has unknown mean µi, unknown
covariance matrix Σi with ∥Σi∥op ≤σ2, and mini,i′∈[k],i̸=i′ ∥µi −µi′∥2 = eΩ(
√
k) σ, the algorithm
runs in time O(n1+ϵ0 max(k, d)), and with high probability correctly clusters 99% of the points.
Some remarks are in order. First, we note that pairwise mean separation of Ω(
√
k σ) is information-
theoretically necessary for accurate clustering to be possible for bounded covariance components.
The algorithm establishing Theorem 50 nearly achieves the optimal separation. Secondly, and cru-
cially, our clustering algorithm runs in almost-linear time. Finally, as previously alluded to, our
clustering method is robust to outliers, and can handle mixtures with arbitrary weights, with guar-
antees depending on the smallest weight (see Corollary 36 for a precise statement).
It is worth commenting on the max(k, d) term appearing in the running time of Theorem 50.
Our algorithm runs in almost-linear time as long as k ≤d. For the extreme regime where k ≫d,
our algorithm has running time O(n1+ϵ0k). In this parameter regime, it is plausible that Ω(nk) is a
runtime bottleneck for the following reason: even if we are given (exactly) the centers µi, i ∈[k] for
free, Ω(nk) time seems to be required to simply assign each of the n points to its closest center.
Remark 8 (Prior work). The prior works [7, 45] obtained polynomial-time clustering algorithms
with similar statistical guarantees as Theorem 50, under the (much stronger) assumption that each
component distribution disti has sub-Gaussian tails. For bounded covariance distributions, these
algorithms require the stronger mean separation of Ω(kσ) [44]. On the other hand, the clustering
methods obtained in [118] (as an application of their list-decodable mean estimator) (i) require
sub-Gaussian components, and (ii) partition the dataset into C · k for some constant C > 2 — as
opposed to k — clusters. In summary, prior work has not explicitly obtained even a polynomial-time
clustering algorithm in the bounded covariance setting with separation o(k)σ.
8.2.2
Technical overview
Here, we describe the techniques developed for our problem at a high level, and how they circumvent
several conceptual runtime barriers encountered by prior approaches to list-decodable mean estima-
tion and clustering mixture models. Our full proofs are quite technically challenging, and involve
several additional steps which we omit here for clarity of exposition. Throughout this section, we
assume that the "scale" of the problem is σ = 1 for simplicity (e.g. distribution covariances are
bounded by I).

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 328
Prior approaches and their limitations
In this section, we brieﬂy describe two recent fast algorithms for list-decodable mean estimation,
developed by [179] and Sections G.2, G.3, and G.4,3 focusing on tools used in their analyses and
bottlenecks in extending their techniques to obtain (almost)-linear runtimes. For brevity, we will cite
the algorithm of Sections G.2, G.3, and G.4 as [180] in this discussion, where it was ﬁrst presented.
Multiﬁltering. Filtering is one of the most popular techniques for robust estimation [175, 177,
360, 492, 182]. In the minority-outlier setting, ﬁltering is based on the idea of designing certiﬁcates
of corruption, which either ensure that a current estimate suﬃces, or can be used to identify a set of
points to ﬁlter on containing more outliers (corrupted points) than inliers (clean points). Iterating
this process terminates in polynomial time, because (roughly speaking) it eventually removes all
outliers.
In the context of list-decodable mean estimation, standard ﬁltering guarantees are insuﬃcient,
because we cannot aﬀord to remove as many inliers as outliers. To overcome this diﬃculty, [186]
introduced the "multiﬁlter" in the context of Gaussian mean estimation, which was extended to
bounded covariance distributions in [179]. At a high level, a multiﬁlter iterates through a tree of
candidate subsets, and looks for ways to either "cluster" a subset or "split" it into multiple (over-
lapping) subsets.4 To ensure an eﬃcient runtime, a multiﬁlter maintains a potential guaranteeing
that the tree size does not blow up (i.e. there are never too many candidate subsets), and carefully
chooses to split or cluster based on subset sample statistics, thus ensuring that some tree node al-
ways retains a large fraction of inliers. Previous multiﬁlters chose to split or cluster subsets based on
one-dimensional projections along top eigenvectors of sample covariances, which can be dominated
by a single outlier. In the worst case, this leads to an iteration count scaling polynomially with the
dimension.
Filtering via matrix multiplicative weights. The approach taken by the fastest algorithms for mean
estimation in both majority-inlier [194] and majority-outlier [180] settings is heavily motivated by
ﬁltering. In the majority-inlier case, every iteration of the ﬁlter is nearly-linear time, so the only
bottleneck to an overall fast runtime is the number of iterations. However, simple hard instances show
that only projecting onto the worst directions of empirical covariances may lead to an Ω(d) runtime
overhead. The main idea of [194] was to choose scores capturing multiple bad directions at a time,
preventing this worst-case behavior. These scores were based on quadratic forms with certain trace-
one matrices derived from the matrix multiplicative weights (MMW) regret minimization framework
from semideﬁnite programming [534, 34]. By using MMW regret bounds, [194] designs a ﬁlter that
eﬃciently decreases the empirical covariance operator norm, which is used as a potential to yield
convergence in polylogarithmically many iterations.
In the majority-outlier setting, the story is somewhat murkier. To overcome complications of
3We focus on [180] instead of [136] in this technical exposition, as they both apply Ky Fan semideﬁnite programming
machinery to obtain fast runtimes, but the [180] approach is more relevant to us.
4In [179], these subsets were replaced by weight functions, but the intuition is very similar in both cases.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 329
prior list-decodable mean estimation algorithms (e.g. the multiﬁlter), which interleaved "ﬁltering"
and "clustering" steps, [180] designed a "k-dimensional ﬁlter", for k = Θ( 1
α), that they called SIFT,
decoupling the two goals. Speciﬁcally, SIFT uses scores based on k-dimensional projections to hone
in on a subspace outside of which the empirical mean is accurate. It then eﬃciently clusters in just
this subspace; combined with appropriate Ky Fan norm generalizations of MMW, the number of
iterations is then improved to polylogarithmic. However, this approach of decoupling ﬁltering and
clustering appears to inherently use k-dimensional PCA as a subroutine, for k = Θ( 1
α), even just
to learn an "important" subspace a single time. Hence, this approach encounters a similar runtime
bottleneck as prior algorithms for clustering mixture models [520, 7].
Challenges in combining techniques. As mentioned, the approach of [180] seems to inherently
run into a runtime barrier at Ω( nd
α ) due to its reliance on k-PCA. This suggests that to overcome
this barrier, we need to develop a new algorithm which both (1) does not disentangle ﬁltering and
clustering steps, and (2) relies on univariate projections. It is natural to then try to merge the
multiﬁlter with a MMW-based potential to ensure rapid convergence.
Unfortunately, there are several obstacles towards combining these frameworks. A primary com-
plication is that the regret minimization approach of [194] requires multiple consecutive rounds
before it can ensure an appropriate potential decreases. This is because of its reliance on MMW, a
"mirror descent" algorithm which typically does not provide monotone guarantees on iterates (and
hence requires multiple iterations to bound regret) [161]. It is unclear how to make these arguments
work within the multiﬁlter framework, which interleaves two types of steps (splitting and clustering)
that may have incompatible guarantees across iterations.
Finally, even if it were possible to combine the multiﬁlter with a MMW-based potential analysis,
there are still various diﬃculties towards obtaining an almost-linear runtime coming from the size of
our hypothesis tree. For example, making the decision to split or cluster at a node typically requires
Ω(nd) time (e.g. to compute scores), which we cannot aﬀord to perform more than subpolynomially
often. This is problematic because our multiﬁlter tree certainly contains Ω( 1
α) nodes: in the uniform
mixture model case, our tree must contain hypotheses corresponding to every true cluster.
Our techniques
One-shot potential framework. In order to deal with the ﬁrst of the two obstacles discussed (the non-
monotonicity of MMW regret guarantees), our starting point is a framework for fast robust mean
estimation (cf. Section 8.3.3), essentially matching the guarantees of [194] with a more transparent
analysis.
Crucially, our new framework comes with a "one-shot" potential function that shows
monotone progress at every iteration, making it more amenable to combination with a multiﬁlter
(which needs to argue how potentials evolve between diﬀerent types of steps).
In more detail, our new fast algorithm in the majority-inlier setting guarantees monotone progress
on the "Schatten-norm" potential Tr(Y2
t ), where Yt := Mlog(d)
t
and Mt = P
i∈T [wt]i(Xi −µt)(Xi −

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 330
µt)⊤is the weighted empirical covariance with respect to the current weight vector wt. We then use
Yt to sample carefully chosen Gaussian random vectors to locate outliers in multiple univariate di-
rections. By using the guarantees of Johnson-Lindenstrauss projections, we can use these univariate
ﬁlters to ensure the next (weighted) empirical covariance matrix satisﬁes

Y2
t , Mt+1

≤O(1)Tr(Y2
t ) .
(8.1)
Combining (8.1) with a fact from [289] shows that our potential decays geometrically, resulting in
rapid convergence. Fortunately, we can use the same potential in the multiﬁlter context, as long
as we guarantee that (8.1) holds for every child of a node (whether a split or cluster step is used).
In particular, applying (8.1) repeatedly for any path in the multiﬁlter tree implies that the depth
is polylog(d). It remains to bound the width of the tree (the computational cost per layer), while
maintaining the invariant that at least one node on every level preserves enough inliers.
Warmup: fast Gaussian multiﬁlter via indicator weights. Recall that our other obstacle towards
an almost-linear runtime is that each of the Ω(α−1) nodes of our multiﬁlter tree requires Ω(nd) time
to decide on a multiﬁltering step. Our strategy is to reduce the total number of nodes across each
layer of the tree, so that the total cost of multiﬁltering on all of them is roughly nd. We achieve this
goal by ensuring that our multiﬁlter always maintains nodes which specify subsets of our original
data (i.e. 0-1 weights rather than soft weights ∈[0, 1]). Hence, each layer of our new multiﬁlter trades
oﬀthe number of subsets with the cost of multiﬁltering on each subset. Considering the two extreme
layers is illustrative of this tradeoﬀ: at the root, our algorithm performs a single one-dimensional
projection on the entire dataset; at the leaves, it performs O(α−1) one-dimensional projections, each
on a subset consisting of roughly an α-fraction of the original dataset.
As a warmup, we ﬁrst show how to achieve this in the case where the ground-truth, dist, is
a bounded-covariance Gaussian (see Section G.5), so we can exploit strong concentration bounds.
In particular, we know that in any linear projection almost all of the inliers will lie in an interval
of logarithmic length. If almost all of our sample points in a subset are clustered within such an
interval, we can explicitly remove all samples outside of it. On the other hand, if our samples are
spread out, we can split them into two (unweighted) subsets with suﬃcient overlap to ensure that
at least one of the children subsets will contain almost all the inliers, as long as the parent did. We
can in fact apply such a partitioning strategy iteratively along each univariate projection, until each
remaining subset is contained in a short interval; this suﬃces to imply (8.1).
From Gaussians to bounded-covariance distributions. Substantially more technical care is re-
quired in the bounded-covariance setting to achieve an almost-linear runtime without sacriﬁcing the
error rate. Notably, we will no longer be able to guarantee that the subsets lie in short intervals,
due to weaker concentration properties. This also means that we cannot deterministically remove
points, making it more challenging to ensure the weight functions we keep are indicators.
We overcome these challenges in Section 8.4 through several new technical developments. We

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 331
ﬁrst weaken the outcome guarantee of our recursive partitioning strategy, from ensuring each cluster
lies in a short interval, to requiring bounded variance, which we show suﬃces to advance on the
potential (8.1). Furthermore, we use a randomized dropout strategy in place of the "clustering"
step of the multiﬁlter, and design fast quantile checks to ensure the "split" step can be conducted in
nearly-constant time. By carefully combining these subroutines, we can indeed ensure every child of
nodes in a layer satisﬁes (8.1), and that the total computational cost of splitting or clustering on the
entire layer is almost-linear. With our earlier depth bound, this yields our full runtime guarantee.
Reducing clustering to list-decodable learning. In Section 8.5, we demonstrate that several mix-
ture model clustering tasks enjoy beneﬁts from the speedups aﬀorded by our list-decodable learn-
ing methods. In the following, assume we have a list L of size O(α−1) and L ⊇{ˆµi}i∈[k] with
∥ˆµi −µi∥2 = eO(
√
α−1) for all i ∈[k], where µi is the mean of the mixture component disti.
For sub-Gaussian components, we build on a clustering algorithm of [186] and improve it to run
in nearly-linear time via randomized distance comparisons. The main idea of the [186] algorithm
is to exploit concentration, which implies that with high probability, all points drawn from disti
have a closest hypothesis in L at distance eO(
√
α−1) from µi.
By rounding every sample to its
nearest hypothesis, and assuming separation eΩ(
√
α−1) between component means, we can perform
an eﬃcient equivalence class partitioning which clusters the data. We observe that this framework is
tolerant to a small amount of poorly-behaved points or outliers and generalizes to cluster components
with bounded fourth moments.
For our most general application of clustering mixtures under only bounded component covari-
ances, as stated in Theorem 50, the same framework does not apply as a constant fraction of all
points may be misbehaved due to weak concentration. To address this, we develop a new post-
processing technique, relying on the following observation: letting P be the projection onto the
O(α−1)-dimensional subspace spanned by L, any sample hit by P will lie within distance O(
√
α−1)
of its corresponding cluster mean in the low-dimensional subspace with constant probability. We
use this observation to drop hypotheses which are too far away from the true means, and then an
appropriate equivalence relation suﬃces for clustering. The runtime bottleneck of this strategy is
the computation and application of P to our dataset, which can be quite expensive. We show that
by instead measuring distances in a O(log d)-dimensional subspace formed by random projections
within P, and clustering based on these estimates, we obtain similar clustering performance by
exploiting guarantees of Johnson-Lindenstrauss transforms.
8.3
Preliminaries: clustering mixture models
In Section 8.3.1, we deﬁne the notation used throughout the sections relevant to clustering mixture
models.
Next, we recall some technical tools (primarily from prior work) which we draw upon
in Section 8.3.2.
We conclude with a sketch of our potential function approach to ﬁltering in

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 332
Section 8.3.3 by demonstrating how it works for robust mean estimation in the minority-outlier
regime, giving an alternative approach to obtaining the runtimes of [194].
This new approach
bypasses an explicit matrix multiplicative weights argument in favor of a one-step potential. We
ultimately generalize this technique to the list-decodable setting by interlacing it with clustering
steps, inspired by the multiﬁlter algorithm of [186, 179].
8.3.1
Notation
General notation. For mean µ ∈Rd and positive semideﬁnite covariance matrix Σ ∈Rd×d, we let
N(µ, Σ) be the standard multivariate Gaussian. For d ∈N we let [d] := {j | j ∈N, 1 ≤j ≤d}. We
refer to the ℓp norm of a vector argument by ∥·∥p, and overload this to mean the Schatten-p norm
of a symmetric matrix argument. The all-ones vector (when the dimension is clear from context) is
denoted 1. The (solid) probability simplex is denoted ∆n := {x ∈Rn
≥0, ∥x∥1 ≤1}. We refer to the
ith coordinate of a vector v by [v]i.
Matrices. Matrices will be in boldface throughout, and when the dimension is clear from context
we let 0 and I be the zero and identity matrices. The set of d × d symmetric matrices is Sd and the
d × d positive semideﬁnite cone is Sd
≥0. We use the Loewner partial ordering ⪯on Sd
≥0. The largest
eigenvalue, smallest eigenvalue, and trace of a matrix are given by λmax(·), λmin(·), Tr(·) respectively.
We use ∥·∥op to mean the (ℓ2-ℓ2) operator norm, which is the largest eigenvalue for arguments in
Sd
≥0. The inner product on A, B ∈Sd is given by ⟨A, B⟩:= Tr(AB).
Distributions. We often associate a weight vector w ∈∆n with a set of points T ⊂Rd with
|T| = n. Typically we denote this set by {Xi}i∈T , where we overload T to mean the indices as well
as the points. For any T ′ ⊆T we let wT ′ ∈∆n be the vector which agrees with w on T ′ and is 0
elsewhere. The empirical mean and covariance matrix on any subset are denoted
µw(T ′) :=
X
i∈T ′
wi
∥w′
T ∥1
Xi, Covw(T ′) :=
X
i∈T ′
wi
∥w′
T ∥1
(Xi −µw(T ′)) (Xi −µw(T ′))⊤.
For convenience, we also deﬁne the unnormalized convariance matrix by
g
Covw(T ′) :=
X
i∈T ′
wi (Xi −µw(T ′)) (Xi −µw(T ′))⊤.
We say distribution dist with EX∼dist[X] = µ∗has sub-Gaussian parameter σ in all directions if
EX∼dist[exp(s ⟨X −µ∗, v⟩)] ≤exp( σ2s2
2 ) for all unit vectors v. In Section 8.5 we use concentration
properties of sub-Gaussian random variables, which are well-known and can be found e.g. in the
reference [456].
List-decodable mean estimation. We state the model of list-decodable mean estimation we use
throughout; the setting we consider is standard from the literature. Fix a parameter 0 < α < 1
2.
Then a set T := {Xi}i∈T ⊂Rd of size |T| = n = Θ(dα−1) is given, containing a subset S ⊂T such

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 333
that the following assumption holds.
Assumption 6. There is a subset S ⊆T ⊂Rd of size αn = Θ(d), and a vector µ∗∈Rd, such that
1
|S|
X
i∈S
(Xi −µ∗)(Xi −µ∗)⊤⪯I.
We remark that this assumption is motivated by the statistical model where there is an underlying
distribution dist supported on Rd with mean µ∗and covariance bounded by I, and the dataset T
is formed by αn independent draws from dist combined with (1 −α)n arbitrary points.
Up to
constants in the "good" fraction α and the covariance bound, Proposition B.1 of [118] guarantees
Assumption 6 holds with inverse-exponential failure probability.
We also note that Proposition
5.4(ii) of [186] shows that the information-theoretic optimal guarantee for list-decodable estimation
in the setting of Assumption 13 is to return a list L of candidate means, where |L| = Θ(α−1), and
min
µ∈L ∥µ −µ∗∥2 = Θ
 1
√α

(8.2)
This setup handles the more general case where the upper bound matrix in Assumption 6 is σ2I
for some positive parameter σ by rescaling the space appropriately, and the error guarantee (8.2)
becomes worse by a factor of σ. Because of this, we will set σ = 1 throughout for simplicity.
Finally, throughout Sections G.5 and 8.4 we will make the explicit assumption that d ≥α−1;
for the regime where this is not the case, the algorithm in Section G.1 obtains optimal error rates
in time eO(α−2) (and in fact, if we tolerate a list size of O(α−1 log 1
δ ) where δ ∈(0, 1) is the failure
probability of the algorithm, obtains optimal error in time eO(α−1)).
8.3.2
Technical tools
We will use a number of technical tools throughout this work which we list here for convenience.
The ﬁrst few are standard facts about covariance matrices which follow directly from computation.
Fact 14 (Convexity of covariance). For any w ∈∆n associated with T ⊂Rd,
µw(T)µw(T)⊤⪯
X
i∈T
wi
∥w∥1
XiX⊤
i .
This implies that for any v ∈Rd,
(µw(T) −v)(µw(T) −v)⊤⪯
X
i∈T
wi
∥w∥1
(Xi −v)(Xi −v)⊤.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 334
Fact 15 (Eﬀect of mean shift). For any w ∈∆n associated with T ⊂Rd, and any v ∈Rd,
X
i∈T
wi
∥w∥1
(Xi −v)(Xi −v)⊤= Covw(T) + (µw(T) −v)(µw(T) −v)⊤⪰Covw(T).
Fact 16 (Alternate covariance characterization). For any w ∈∆n associated with T ⊂Rd,
1
2 ∥w∥2
1

X
i,j∈T
wiwj(Xi −Xj)(Xi −Xj)⊤

= Covw(T).
Next, we need the notion of safe weight removal in the list-decodable setting, adapted from
Section G.2. The idea behind safe weight removal is that repeatedly performing a downweighting
operation with respect to scores satisfying a certain condition results in weights which preserve some
invariant, which we call saturation. We deﬁne our notions of safety and saturation, and state a key
technical lemma which lets us reason about when saturation is preserved. In the following discussion
assume we are in the list-decodable mean estimation setting we deﬁned in Section 8.3.1.
Deﬁnition 28 (γ-saturated weights). We call weights w ∈∆n γ-saturated, for some γ > 1, if
w ≤1
n1 entrywise, and
∥wS∥1 ≥α ∥w∥
1
γ
1 .
Deﬁnition 29 (γ-safe scores). We call scores {si}i∈T ∈Rn
≥0 γ-safe with respect to w if w ∈∆n
and
X
i∈S
wi
∥wS∥1
si ≤1
γ
X
i∈T
wi
∥wT ∥1
si.
Roughly speaking, we require this alternative notion of safe scores in the majority-outlier regime
because there are less good points we can aﬀord to throw away. The key property connecting these
two deﬁnitions is the following.
Lemma 112. Let w(0) ∈∆n be γ-saturated, and consider any algorithm of the form:
1. For 0 ≤t < N:
(a) Let {s(t)
i }i∈T be γ-safe with respect to w(t).
(b) Update for all i ∈T:
w(t+1)
i
←
 
1 −s(t)
i
s(t)
max
!
w(t)
i , where s(t)
max :=
max
i∈T |w(t)
i
̸=0
s(t)
i .
Then, w(N) is also γ-saturated.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 335
Proof. It suﬃces to prove this in the case N = 1 and then use induction. Deﬁne
δS :=
X
i∈S
w(0)
i
−w(1)
i
w(0)
S

1
, δT :=
X
i∈T
w(0)
i
−w(1)
i
w(0)
1
.
By using the assumption that s(0) is γ-safe, we conclude
δS =
1
s(0)
max
X
i∈S
w(0)
i
w(0)
S

1
s(0)
i
≤1
γ ·
1
s(0)
max
X
i∈T
w(0)
i
w(0)
1
s(0)
i
= 1
γ δT .
Now, using γ-saturation of w(0) and 1 −γ−1δ ≥(1 −δ)γ−1 for all δ ∈[0, 1] and γ > 1,
w(1)
S

1 = (1 −δS)
w(0)
S

1 ≥(1 −δT )γ−1 w(0)
S

1 ≥α

(1 −δT )
w(0)
1
γ−1
= α
w(1)
γ−1
1
.
Finally, we include a technical lemma proved in [136].
Lemma 113 (Restatement of Lemma 296). Let w ∈∆n have w ≤1
n1 entrywise, and ∥w∥1 ≥α2.
Then,
∥µw(T) −µ∗∥2 ≤
s
2 ∥Covw(T)∥op
∥w∥1
∥wS∥1
+ 2
α.
In light of the lower bound of [186], Lemma 113 shows to learn the mean near-optimally in the
list-decodable setting, it suﬃces to ensure ∥wS∥1 = Ω(α) (i.e. we retain a constant fraction of the
"good" weight) and ∥Covw(T)∥op = eO(1) (i.e. the weighted covariance of the dataset is bounded).
8.3.3
Potential function approach to fast ﬁltering
In this section, we outline an example of a potential function approach to fast ﬁltering, an alternative
to ﬁltering based on matrix multiplicative weights (MMW) used in recent literature [194, 359].5 This
replacement is very useful in the list-decodable setting, as it greatly simpliﬁes the requirements of
our fast multiﬁlter which interlaces clustering and ﬁltering steps.
The example problem we consider in this expository section is the minority-outlier regime for
robust mean estimation, when the "ground truth" distribution has covariance norm bounded by I.
We brieﬂy describe the approach of [194] for this problem, and explain how it can be replaced with
our new potential function framework. Throughout this section, ﬁx some 0 < ϵ < 1
2 and assume
that amongst the dataset T ⊂Rd of n points, there is a majority subset S ⊂T of size |S| = (1 −ϵ)n
with bounded empirical covariance: Cov 1
n 1(S) ⪯I.
5MMW guarantees are also implicitly used in approaches based on packing SDPs, see e.g. [131, 132, 136]. However,
[194, 359] use MMW guarantees in a non-black-box way to design ﬁlters.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 336
The main algorithmic step in [194] is an eﬃcient subroutine for halving the operator norm of
the empirical covariance while ﬁltering more weight from T \ S than from S. It is well-known in
the literature that whenever the operator norm is O(1), the empirical mean is within O(√ϵ) in ℓ2
norm from the ground truth mean (for an example of this derivation, see Lemma 3.2 of [194]). Thus,
the key technical challenge is to provide a nearly-linear time implementation of this subroutine.
This was accomplished in [194] using MMW-based regret guarantees, with "gain matrices" given by
covariances and iterative ﬁltering based on MMW responses. The result was a procedure which either
terminates with a good mean estimate, or halves the covariance operator norm after O(log d) rounds
of ﬁltering. The latter is an artifact of many regret minimization techniques, which only guarantee
progress after multiple rounds. It is natural to ask instead whether an alternative one-shot potential
decrease guarantee exists; we now describe such a guarantee.
One-shot potential decrease. Our algorithm will proceed in a number of iterations, where we
modify a weight vector in ∆n associated with T. We initialize w(0) ←1
n1. In iteration t, we will
downweight w(t) ∈∆n to obtain a new vector w(t+1) as follows. Deﬁne the matrices
Mt := g
Covw(t) (T) =
X
i∈T
w(t)
i (Xi −µw(t)(T))(Xi −µw(t)(T))⊤, Yt := Mlog d
t
.
The potential we will track is Φt := Tr(Y2
t ). In order to analyze Φt, we require two helper facts.
Fact 17 (Lemma 7, [289]). Let A ⪰B be matrices in Sd
≥0, and let p ∈N. Then
Tr(Ap−1B) ≥Tr(Bp).
Fact 18. For any γ ≥0 and A ∈Sd
≥0,
γTr(A2 log d) ≤Tr(A2 log d+1) + dγ2 log d+1.
Proof. This is immediate since each of the d eigenvalues of A2 log d is either at least γ2 log d or not,
and both of these cases are accounted for on the right hand side of the conclusion.
We now give the potential analysis. Our main goal will be ensuring that

Y2
t , Mt+1

≤20Tr(Y2
t ).
(8.3)
The speciﬁc constant in the above equation is not particularly important, but is used for illustration.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 337
We now show how (8.3) implies a rapid potential decrease. Observe that
Φt+1 = Tr

M2 log d
t+1

≤1
40Tr

M2 log d+1
t+1

+ d(40)2 log d
≤1
40Tr

M2 log d
t
Mt+1

+ d(40)2 log d
≤1
2Tr(Y2
t ) + d(40)2 log d = 1
2Φt + d(40)2 log d.
(8.4)
The ﬁrst line used Fact 18 with γ = 40, the second used Fact 17 with A = Mt and B = Mt+1 (noting
that if w(t+1) ≤w(t) entrywise, the unnormalized covariance matrices respect the Loewner order by
Fact 15), and the third line used the assumption (8.3). This implies that we decrease Φt by a constant
factor in every iteration, until it is roughly d(40)2 log d, at which point the deﬁnition Φt = Tr(M2 log d
t
)
implies that ∥Mt∥op is bounded by a constant. By using a na¨ıve ﬁltering preprocessing step, we can
guarantee that Φ0 = dO(log d), and hence the process will terminate in O(log2 d) rounds.
Meeting the ﬁlter criterion (8.3). To complete the outline of this algorithm, we need to explain
how to satisfy (8.3) via downweighting, while ensuring that we remove more weight from T \ S than
S. To do so, we deﬁne scores
s(t)
i
:= (Xi −µw(t)(T))⊤M2 log d
t
(Xi −µw(t)(T)) for all i ∈T.
We remark that (randomized) constant-factor approximations can be computed to all s(t)
i
via
Johnson-Lindenstrauss projections in eO(nd) time, but for this discussion we assume we exactly
know all scores. Then, by linearity of trace the condition (8.3) is implied by
X
i∈T
w(t+1)
i
s(t)
i
≤20Tr(Y2
t ),
(8.5)
since Fact 15 implies that

Y2
t , Mt+1

≤P
i∈T w(t+1)
i
s(t)
i . Finally, we note that whenever (8.5) does
not hold, it must be primarily due to the eﬀect of the outliers T \ S, because the covariance of S is
bounded. Hence, we can simply set
w(t+1)
i
=
 
1 −s(t)
i
s(t)
max
!K
w(t)
i ,
where K is the smallest natural number which passes the criterion (8.5). For any smaller K, it can
be shown that downweighting "one more time" preserves the invariant that more outlier mass is
removed, precisely because (8.5) has not been met. Finally, binary searching to ﬁnd the smallest
value of K meeting (8.3) yields a complete algorithm running in eO(nd) time (for further details on
the implementation of this binary search on K, see Theorem 2.4 of [194]).
Generalizing to the majority-outlier regime. Our algorithms for the list-decodable setting marry

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 338
this potential function argument with a multiﬁlter, which produces multiple candidate ﬁltered weight
vectors on an input weight vector. We will instead show that for the tree of weight vectors, where
a node has children given by the candidates produced from the multiﬁlter, at least one child both
halves the potential and performs only γ-safe weight removal, for some γ. After polylogarithmic
layers, we will return all empirical means of leaf nodes as our list of estimates. The child on the
"safe branch" will then have a bounded potential and a γ-saturated weight vector, which suﬃces to
guarantee an accurate mean estimate.
There are a number of additional complications which arise in this extension, which we brieﬂy
mention here as a preface to the following Sections G.5 and 8.4. In order to process every layer of
the multiﬁlter tree in almost-linear time, we need to ensure that the number of datapoints across
all the nodes, including repetitions, has not grown by more than a constant factor. The multiﬁlter
of [179] gives a variant of this guarantee by tracking the sums of the squared ℓ1 norms of weights
associated with diﬀerent nodes as a nonincreasing potential, i.e.
X
i∈S
w(i)
2
1 ,
where S is the set of nodes and each w(i) is a current candidate weight function in S. This is not
suﬃciently strong of a guarantee in our setting, since even points with very small weights need to be
factored into calculations and thus aﬀect runtime. We modify this approach in two ways. First, we
replace the downweighting step with a randomly subsampled ﬁlter, which we show preserves various
safety conditions such as those in Lemma 112 with high probability. Next, we replace the squared
ℓ1 potential with one involving 1 + β powers, for some β ∈(0, 1), which we prove is compatible
with the multiﬁlter. Overall, our ﬁlter tree contains polylogarithmically many layers, each of which
accounts for sets with total cardinality O(n1+O(β)), giving us our ﬁnal runtime.
8.4
Fast bounded covariance multiﬁlter
In this section, we give our algorithm for list-decodable mean estimation under only Assumption 13.
As before, we can assume without loss of generality that α ∈[1/d, 1/ logC d], for some constant
C > 0. We begin by giving our main subroutine, Partition, in Section 8.4.1. The goal of Partition will
be to produce child subsets {cℓ}ℓ∈[k] of a given input set p, which each satisfy the potential criterion
in (G.14), reproduced here:

Y2
p, Mcℓ

≤R2Tr
 Y2
p

.
(8.6)
Recall that in Section G.5, the way we produced children satisfying condition (8.6) was by ensuring
that along logarithmically many random directions, each child cℓlied entirely in short intervals. We
will satisfy this guarantee in this section by more directly working with the deﬁnition of (8.6), which
requires each child to have small variance along the random directions, a looser condition.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 339
To bound the variance of the child subsets, in Section 8.4.2 we develop an algorithm, SplitOrCluster,
which is patterned oﬀour earlier GaussianSplitOrCluster. It either certiﬁes that the input set is al-
ready "close" to having bounded variance in an input direction, or identiﬁes a split point which
produces two subsets which are closer to having this property, while maintaining at least one sub-
set retains most points in S. In the ﬁrst case (the "cluster" case), we develop a postprocessing
procedure Fixing in Section 8.4.4 which randomly ﬁlters points according to safe outlier scores (see
Deﬁnition 29) to make the remaining cluster have truly bounded variance. In the second case (the
"split" case), we develop a fast threshold checking procedure SplitOrTailBound in Section 8.4.3 which
identiﬁes a valid split in polylogarithmic time, whenever one exists; here, we note the key diﬃculty
is that we can no longer use a ﬁxed radius for splits, because Gaussian concentration does not hold.
We discuss runtimes of all of these algorithms in Section 8.4.5, and in particular give a runtime
bound on Partition. Finally, we use Partition to develop our full algorithm, FastMultiﬁlter, which we
analyze in Section 8.4.6 through a potential argument similar to our analysis of FastGaussianMultiﬁlter.
A post-processing step used in FastMultiﬁlter is analyzed in Section 8.4.7.
8.4.1
Reducing Partition to SplitOrCluster
The goal of this section is to develop Partition, the main subroutine of FastMultiﬁlter. Partition has
very similar guarantees to the algorithm GaussianPartition developed in Section G.5.1. It takes as
input a "parent set" Tp ⊆T and produces a number of "children subsets" {Tcℓ}ℓ∈[k] such that every
child satisﬁes

Y2
p, Mcℓ

≤R2Tr
 Y2
p

, where we follow the deﬁnitions (G.13), reproduced here:
Mp := g
Cov 1
n 1 (Tp) , Yp := Mlog d
p
,
Mcℓ:= g
Cov 1
n 1 (Tcℓ) , Ycℓ:= Mlog d
cℓ
for all ℓ∈[k],
(8.7)
This will allow us to conduct a potential analysis to bound the depth of the multiﬁlter tree in
Section 8.4.6. Moreover, we require two additional guarantees of Partition.
1. The ﬁrst is the same as (G.12); namely, for some parameter β ∈(0, 1], we have
X
ℓ∈[k]
|Tcℓ|1+β ≤|Tp|1+β .
(8.8)
This will help us bound the total work done in each layer of the multiﬁlter tree.
2. The second is ensures at least one child preserves most points in S, assuming that the parent
Tp has this property. To this end, the tools of Section 8.3.2 will vastly simplify the language
of this section. In particular, we will ensure that for γ = O(log( 1
α)), every ﬁlter step in this
entire section will be with respect to γ-safe weights in at least one branch. We then apply
Lemma 112 to conclude that some node at every level of the multiﬁlter tree is γ-saturated.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 340
For the remainder of Section 8.4, we will deﬁne
γ := 8 log
 1
α

.
We demonstrate one important consequence of a set being γ-saturated.
Lemma 114. Suppose for a set T ′ ⊂T, the weights w := 1
n1T ′ ∈∆n which place 1
n on coordinates
in T ′ and 0 otherwise are γ-saturated (cf. Deﬁnition 28). Then,
|T ′ ∩S| ≥αn
2 .
Proof. Recall that Deﬁnition 28 gives
∥w∥1 ≥∥wS∥1 ≥α ∥w∥
1
γ
1
=⇒∥w∥
1−1
γ
1
≥α =⇒∥w∥1 ≥3α
4 .
The ﬁrst implication was by rearrangement, and the second used α
1
1+γ−1 ≥α1+ 2
γ ≥3α
4 . Next,
∥wS∥1 ≥α ∥w∥
1
γ
1 ≥α
3α
4
 1
γ
≥α
2 .
The conclusion follows since ∥wS∥1 counts the elements of T ′ ∩S, normalized by 1
n.
Lemmas 112 and 114 imply that as long as we can guarantee that at every ﬁltering step, at least
one child was produced with respect to γ-safe scores, that child retains half the elements of S. We
are now ready to state the algorithm Partition, which heavily relies on a subroutine 1DPartition.
As in the Gaussian case, we develop an algorithm 1DPartition which in turn is based on a subrou-
tine SplitOrCluster, which we implement in Section 8.4.2. The algorithm 1DPartition takes an input
direction v and guarantees that along this direction, every child subset produced has small variance
(scaled by the length of v). 1DPartition is implemented by recursively calling SplitOrCluster, which
takes as input a set T ′′ and produces either one or two subsets, analogously to GaussianSplitOrCluster.
Lemma 115. The output of Partition satisﬁes the guarantees given in Line 2 of Algorithm 33,
assuming correctness of 1DPartition.
Proof. We will follow the proof of Lemma 307. First, to demonstrate that the subsets satisfy (8.8),
inducting on the guarantee (8.9) of 1DPartition suﬃces. Similarly, γ-saturation of some child follows
from inducting on the corresponding guarantee of 1DPartition.
Finally, using the guarantee (8.10) of 1DPartition, every Tcℓ∈SNdir satisﬁes
D
vjv⊤
j , g
Cov 1
n 1(Tcℓ)
E
≤1
2R2 ∥vj∥2
2 , for all j ∈[Ndir].

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 341
Algorithm 33: Partition(Tp, α, δ, β, R)
1 Input: Tp ⊂T, α ∈(0, 1
2), δ ∈(0, 1), β ∈(0, 1], R ∈R≥0 satisfying (for a suﬃciently large
constant)
R = Ω
 
max
 
1
β ·
s
γ log
 1
αβ

,
s
γ log
log d
δ
!!
.
;
2 Output: With failure probability ≤δ: subsets {Tcℓ}ℓ∈[k] of Tp, satisfying (8.8). Every child
satisﬁes (8.6) (using notation (8.7)). If w := 1
n1Tp is γ-saturated, then w′ := 1
n1Tcℓis
γ-saturated for at least one child Tcℓ.;
3 Sample Ndir = Θ(log d
δ ) vectors {uj}j∈[Ndir] ∈Rd each with independent entries ±1.
Following notation (8.7), let vj ←Ypuj for all j ∈[Ndir].;
4 S0 ←Tp;
5 for j ∈[Ndir] do
6
Sj ←∅;
7
for T ′ ∈Sj−1 do
8
T ←1DPartition(T ′, α, vj,
δ
2Ndir , β, R);
9
Sj ←Sj ∪T ;
10 Return: SNdir;
In other words, the variance is small along all directions {Ypuj = vj}j∈[Ndir]. We conclude

Y2
p, Mcl

≤
1.4
2n |Tcℓ| Ndir
X
i,i′∈Tcl
X
j∈[Ndir]
⟨Ypuj, Xi −Xi′⟩2
= 1.4
Ndir
X
j∈[Ndir]
D
vjv⊤
j , g
Cov 1
n 1(Tcℓ)
E
≤
1.4
2Ndir
X
j∈[Ndir]
R2 ∥vj∥2
2
=
1.4
2Ndir
X
j∈[Ndir]
R2 ∥Ypuj∥2
2 ≤R2Tr
 Y2
p

,
with probability at least 1 −δ; the ﬁrst two lines and the last line follow the proof of Lemma 307,
and we used the variance guarantee of 1DPartition in the third line. We remark that we make sure
to take the number of directions Ndir to depend logarithmically on δ (as opposed to just d), so we
can apply the guarantees of [6] with probability 1 −δ
2 on the ﬁrst and last lines.
Next, we state a correctness guarantee for 1DPartition, assuming correctness of its main subrou-
tine, SplitOrCluster. The guarantees of SplitOrCluster are summarized in Line 2 of Algorithm 35.
The salient features are that it takes a set Tin and either produces one set satisfying (8.10) deter-
ministically, or two sets which each are strict subsets of Tin (and hence remove at least one point)

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 342
Algorithm 34: 1DPartition(T ′, α, v, δ, β, R)
1 Input: T ′ ⊂T, α ∈(0, 1
2), v ∈Rd, δ ∈(0, 1), β ∈(0, 1], R ∈R≥0 satisfying (for a
suﬃciently large constant)
R = Ω
 
max
 
1
β ·
s
γ log
 1
αβ

,
s
γ log
log d
δ
!!
.
;
2 Output: Subsets {T ′′
ℓ}ℓ∈[k] of T ′, such that
X
ℓ∈[k]
|T ′′
ℓ|1+β ≤|T ′|1+β.
(8.9)
Every child T ′′
ℓfor ℓ∈[k] has
D
g
Cov 1
n 1(T ′′
ℓ), vv⊤E
≤1
2R2 ∥v∥2
2 .
(8.10)
If w = 1
n1T ′ is γ-saturated, then w′ = 1
n1T ′′
ℓis γ-saturated for at least one child T ′′
ℓ, with
failure probability ≤δ.;
3 Sin ←T ′, Sout ←∅;
4 while Sin ̸= ∅do
5
T ′′ ←the ﬁrst element of Sin;
6
Sin ←Sin \ {T ′′};
7
if SplitOrCluster(T ′′, α, v, δ, β, R) returns one set T (0)
out then
8
Sout ←Sout ∪
n
T (0)
out
o
;
9
else
10
T (1)
out, T (2)
out ←SplitOrCluster(T ′′, α, v, δ, β, R);
11
Sin ←Sin ∪
n
T (1)
out, T (2)
out
o
;
12 Return: Sout;

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 343
deterministically, and (8.9) is always maintained. In the two set case, if 1
n1Tin is γ-saturated then so
is at least one output deterministically; in the one set case, the output is saturated with probability
at least 1 −δ. We will use only these features to analyze 1DPartition in Lemma 116.
Lemma 116. The output of 1DPartition satisﬁes the guarantees given in Line 2 of Algorithm 34,
assuming correctness of SplitOrCluster.
Proof. Each run of Lines 4-13 results in either one set (which we call a "cluster step") or two sets
(which we call a "split step"). We can view this process as a tree, where a leaf node corresponds
to the result of a cluster step, and every node on the leaf-to-node path corresponds to a split step.
Every time a split step occurs, it increases |Sin| + |Sout| by one, so there are at most (n′)1+β calls
to SplitOrCluster where |T ′| = n′, and thus the algorithm terminates in ﬁnite time.
Next, if T ′ is not saturated, then there is no failure probability, since the conditions (8.9) and
(8.10) deterministically succeed. Otherwise, from the root of this partition tree, consider the root-
to-leaf path which at each node corresponding to a split step takes any child which corresponds to
a saturated child (one always exists because split steps deterministically succeed). The only failure
probability comes from the success of the leaf-parent to leaf cluster step, which fails with probability
δ. We can ignore all other bad events, because we only need to ensure one child is saturated.
8.4.2
Reducing SplitOrCluster to SplitOrTailBound and Fixing
In this section, we state and analyze SplitOrCluster, the main subroutine of 1DPartition.
SplitOrCluster uses two subroutines, Fixing and SplitOrTailBound, which are respectively used to
handle the one child and two children cases. Roughly speaking, Fixing takes as input a set Tin which
"almost" has bounded variance in the direction v, and slightly ﬁlters extreme outliers in a way so that
the result has truly bounded variance. On the other hand, SplitOrTailBound is used at a candidate
threshold τ to either check that it induces sets T (1)
out, T (2)
out satisfying (8.12) or satisﬁes a certain tail
bound. By stitching together tail bounds at a small number of quantiles, SplitOrCluster guarantees
that at least one of these quantiles was a valid threshold, else we would attain a contradiction as
Line 6 of SplitOrCluster would have passed. We state guarantees of SplitOrTailBound and Fixing
as Lemmas 117 and 118, and prove them in Sections 8.4.3 and 8.4.4 respectively. We then use
Lemmas 117 and 118 to prove Lemma 119, which demonstrates correctness of SplitOrCluster.
Lemma 117. There is an algorithm, SplitOrTailBound (Algorithm 36), which takes as input Tin ⊆T,
v ∈Rd, β ∈(0, 1], R ∈R≥0, and τ0 ∈R, and returns in one of two cases we call the "split" case
and the "tail bound" case. In the split case, it returns (τ, r) such that the induced sets (8.11) satisfy
(8.12), and if
1
n1Tin is γ-saturated then one of the induced sets Tout has
1
n1Tout is γ-saturated.
Otherwise, for all t ∈R deﬁne the upper and lower tail probabilities
ρ+(t) :=
Pr
i∼unifTin [Yi ≥t] , ρ−(t) :=
Pr
i∼unifTin [Yi ≤t] .
(8.13)

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 344
Algorithm 35: SplitOrCluster(Tin, α, v, δ, β, R)
1 Input: Tin ⊆T, α ∈(0, 1
2), v ∈Rd, δ ∈(0, 1), β ∈(0, 1], R ∈R≥0 satisfying (for a
suﬃciently large constant)
R = Ω
 
max
 
1
β ·
s
γ log
 1
αβ

,
s
γ log
log d
δ
!!
.
;
2 Output: Either one subset T (0)
out ⊂Tin, or two subsets T (1)
out, T (2)
out ⊂Tin. In the one subset
case, T (0)
out has
D
g
Cov 1
n 1

T (0)
out

, vv⊤E
≤1
2R2 ∥v∥2
2. In the two subsets case, they take the
form, for some threshold value τ ∈R and r ∈R≥0
T (1)
out := {Xi | ⟨v, Xi⟩≤τ + r ∥v∥2}, T (2)
out := {Xi | ⟨v, Xi⟩≥τ −r ∥v∥2},
(8.11)
and satisfy
T (1)
out

1+β
+
T (2)
out

1+β
< |Tin|1+β ,
min
 
1 −abs T (1)
out
abs Tin
, 1 −abs T (2)
out
abs Tin
!
≥2γ
r2 .
(8.12)
In either case if 1
n1Tin is γ-saturated then 1
n1Tout is γ-saturated for at least one child Tout,
with failure probability ≤δ only in the case one set is returned (deterministically
otherwise).;
3 Yi ←⟨v, Xi⟩for all i ∈Tin;
4 τmed ←med ({Yi | i ∈Tin}), where med returns the median;
5 I ←[τmed −c, τmed + c] is the smallest interval containing the 1 −α
4 quantiles of
{Yi | i ∈Tin} for c ∈R≥0 and 2I ←[τmed −2c, τmed + 2c];
6 if
D
g
Cov 1
n 1 (Tmid) , vv⊤E
≤1
8R2 ∥v∥2
2 where Tmid := {Xi ∈Tin | Yi ∈2I} then
7
Return: Fixing(Tin, α, v, δ, R);
8 else
9
Run both SplitOrTailBound

Tin, v, β, τmed ±
1
2k ·
q
2048'
β2α ∥v∥2

for integers k with
0 ≤k ≤log2
2048
β2α

until one returns

T (1)
out, T (2)
out

satisfying (8.11), (8.12);
10
Return: T (1)
out, T (2)
out ;

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 345
Then, in the tail bound case SplitOrTailBound certiﬁes
ρ+(τ0) ≤
128γ
β2 |τ0 −τmed|2 ∥v∥2
2 if τ0 ≥τmed, and ρ−(τ0) ≤
128γ
β2|τ0 −τmed|2 ∥v∥2
2 if τ0 ≤τmed.
(8.14)
Lemma 118. There is an algorithm, Fixing (Algorithm 38), which takes as input Tin ⊆T, v ∈
Rd, and R ∈R≥0 and produces Tout with the following guarantee with probability at least 1 −δ.
Deﬁne Tmid as in Line 5 of Algorithm 35.
Then if
1
n1Tin is γ-saturated, so is
1
n1Tout, and if
D
g
Cov 1
n 1(Tmid), vv⊤E
≤1
8R2 ∥v∥2
2, then
D
g
Cov 1
n 1(Tout), vv⊤E
≤1
2R2 ∥v∥2
2.
Finally, we are ready to prove Lemma 119, the main export of this section.
Lemma 119. The output of SplitOrCluster satisﬁes the guarantees given in Line 2 of Algorithm 35.
Proof. If the check in Line 6 passes (the one subset case), correctness of SplitOrCluster follows
immediately from the guarantees of Fixing in Lemma 118. We now focus on the two subset case.
Assume throughout this proof that the {Yi}i∈Tmid are ordered by distance to τmed, so |Y1−τmed| ≤
. . . ≤|Ym −τmed|, where m := |Tmid|; we order the remaining elements i ∈Tin \ Tmid arbitrarily. We
also deﬁne τmed, Tmid, c, I, and 2I as in Lines 4-6 of SplitOrCluster. If Line 9 outputs a split for any
k, then by Lemma 117, (8.11), (8.12) are satisﬁed, and the saturation condition is met for one of the
children. It remains to show that some value of k will result in the split case of SplitOrTailBound.
We show this by contradiction; if all k resulted in SplitOrTailBound returning with a tail bound
guarantee, we prove Tmid would have passed Line 6. Assume for the remainder of the proof that
SplitOrTailBound failed to ﬁnd a split for all k.
First, we bound the length of the interval 2I. Because SplitOrCluster failed to return a split for
k = 1, by combining the corresponding tail bounds (8.14),
Pr
i∼unifTin

|Yi −τmed| >
r 512
β2α ∥v∥2

≤α
4 .
Thus, we conclude
2I ⊂[τmed −C ∥v∥2 , τmed + C ∥v∥2] , for C :=
r2048
β2α .

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 346
We proceed to bound
D
g
Cov 1
n 1(Tmid), vv⊤E
to obtain our desired contradiction. Observe that
g
Cov 1
n 1(Tmid) ⪯Cov 1
n 1(Tmid)
=
1
|Tmid|
X
i∈Tmid

Xi −µ 1
n 1 (Tmid)
 
Xi −µ 1
n 1 (Tmid)
⊤
⪯
1
|Tmid|
X
i∈Tmid
 Xi −¯X
  Xi −¯X
⊤where

v, ¯X

= τmed
=⇒
D
g
Cov 1
n 1(Tmid), vv⊤E
≤
1
|Tmid|
X
i∈Tmid
(Yi −τmed)2 .
(8.15)
Here, we used the deﬁnitions of g
Cov, Cov in the ﬁrst two lines, and Fact 15 in the third. The last
follows by deﬁnition of {Yi}i∈Tin and τmed. Deﬁne the random variable Z to be the realization of
abs Yi −τmed for i a uniform draw from Tmid, let Zi := abs Yi −τmed, and let G(t) = Pr[Z ≥t] be
the inverse cumulative density function of Z. Notice that directly expanding implies that (where we
let m := |Tmid|, Z0 := 0, and recall we argued Zm ≤C ∥v∥2 earlier)
E[Z2] =
X
i∈[m]
(Z2
i −Z2
i−1)G(Yi) =
Z Zm
0
2tG(t)dt ≤
Z C∥v∥2
0
2tG(t)dt.
Deﬁne now K(t) for each ∥v∥2 ≤t ≤C ∥v∥2 to be the smallest k such that t(k) := C
2k ∥v∥2 ≤t, so
K(C ∥v∥2) = 0, K(t) = 1 for t ∈
 C
2 ∥v∥2 , C ∥v∥2

, and so on. By construction, for all relevant t,
t(K(t)) ≤t < 2t(K(t)).
Since G is decreasing in its argument, we can write
Z C∥v∥2
0
2tG(t)dt ≤
Z ∥v∥2
0
2tdt +
Z C∥v∥2
∥v∥2
2tG

t(K(t))
dt ≤∥v∥2
2 +
Z C∥v∥2
∥v∥2
2tG

t(K(t))
dt.
Now, for all 0 ≤k ≤log2( 80
β2α), we recall that we assumed both calls to SplitOrTailBound with
thresholds τmed ± C
2k ∥v∥2 failed to produce a split, and hence certify a tail bound (8.14). Thus,
G

t(k)
≤
512γ
β2  t(k)2 ∥v∥2
2 ≤2048γ
β2t2 ∥v∥2
2 , for any t with K(t) = k.
Here, the ﬁrst inequality used both tail bounds in (8.14) and accounted for the fact that the quan-
tizations ρ+, ρ−are deﬁned over Tin, and G is deﬁned over Tmid with |Tmid| ≥1
2|Tin|; the second

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 347
inequality used that for any such t, t < 2t(k). Putting all these pieces together,
E[Z2] ≤∥v∥2
2 +
Z C∥v∥2
∥v∥2
4096γ
β2t
∥v∥2
2 dt ≤∥v∥2
2 + 4096γ
β2
log(C) ∥v∥2
2 = O
 γ
β2 · log
 1
αβ

∥v∥2
2 .
Finally, recall (8.15) shows that
D
g
Cov 1
n 1(Tmid), vv⊤E
≤E[Z2]. Thus, the set Tmid should have passed
the check in Line 6 under the assumed lower bound on R, yielding the desired contradiction.
8.4.3
Implementation of SplitOrTailBound
In this section, we prove Lemma 117 by providing SplitOrTailBound and giving its analysis.
Lemma 117. There is an algorithm, SplitOrTailBound (Algorithm 36), which takes as input Tin ⊆T,
v ∈Rd, β ∈(0, 1], R ∈R≥0, and τ0 ∈R, and returns in one of two cases we call the "split" case
and the "tail bound" case. In the split case, it returns (τ, r) such that the induced sets (8.11) satisfy
(8.12), and if
1
n1Tin is γ-saturated then one of the induced sets Tout has
1
n1Tout is γ-saturated.
Otherwise, for all t ∈R deﬁne the upper and lower tail probabilities
ρ+(t) :=
Pr
i∼unifTin [Yi ≥t] , ρ−(t) :=
Pr
i∼unifTin [Yi ≤t] .
(8.13)
Then, in the tail bound case SplitOrTailBound certiﬁes
ρ+(τ0) ≤
128γ
β2 |τ0 −τmed|2 ∥v∥2
2 if τ0 ≥τmed, and ρ−(τ0) ≤
128γ
β2|τ0 −τmed|2 ∥v∥2
2 if τ0 ≤τmed.
(8.14)
Proof. This proof proceeds in two parts which we show separately. First, we demonstrate that if
τ0 ≥τmed and none of the runs of Lines 9-17 in Algorithm 36 return with a valid split, then indeed
we can certify the tail bound (8.14) holds (and a similar guarantee holds for τ0 < τmed). Second, we
show whenever a split is returned and Tin is γ-saturated, then one of the output sets will be as well.
Correctness of tail bound.
We consider the case τ0 ≥τmed here, as the other case follows
symmetrically. Let K + 1 be the ﬁrst index such that τK+1 < τmed, so the algorithm checks all pairs
(τj −rj ∥v∥2 , rj) for 0 ≤j ≤K. Suppose that all such induced splits fail to satisfy (8.12). For all
0 ≤j ≤K, let T (1)
j
, T (2)
j
be the induced sets by the pair (τj −rj ∥v∥2 , rj). Then by construction
abs T (1)
j
abs Tin
= 1 −gj,
abs T (2)
j
abs Tin
= gj+1.
For all 0 ≤j ≤K−1, recall (τj −rj ∥v∥2 , rj) did not pass the check (8.12), but the second expression
reads gj ≥2γ
r2
j (since gj ≤1
2 ≤1 −gj+1), which is true by construction. Thus the ﬁrst check did not
pass and we conclude
g1+β
j+1 + (1 −gj)1+β > 1 =⇒g1+β
j+1 > gj.
(8.16)

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 348
Algorithm 36: SplitOrTailBound(Tin, v, β, τ0)
1 Input: Tin ⊆T, v ∈Rd, β ∈(0, 1], τ0 ∈R;
2 Output: Either outputs (τ, r) such that
T (1)
out := {Xi | ⟨v, Xi⟩≤τ + r ∥v∥2}, T (2)
out := {Xi | ⟨v, Xi⟩≥τ −r ∥v∥2} satisfy
T (1)
out

1+β
+
T (2)
out

1+β
< |Tin|1+β , min
 
1 −abs T (1)
out
abs Tin
, 1 −abs T (2)
out
abs Tin
!
≥2γ
r2 ,
or returns "Tail bound" guaranteeing that for τmed := med ({⟨v, Xi⟩| i ∈Tin}) (following
(8.14))
ρ+(t) ≤
128γ
β2 |τ0 −τmed|2 ∥v∥2
2 if τ0 ≥τmed, and ρ−(t) ≤
128γ
β2|τ0 −τmed|2 ∥v∥2
2 if τ0 ≤τmed.
;
3 Yi ←⟨v, Xi⟩for all i ∈Tin, τmed ←med({Yi | i ∈Tin});
4 j ←0;
5 if τ0 > maxi∈Tin Yi or τ0 < mini∈Tin Yi then
6
"Tail bound";
7 if τ0 ≥τmed then
8
while τj ≥τmed do
9
gj ←ρ+(τj) and rj ←
q
2γ
gj ;
10
if T (1)
out, T (2)
out induced by (τj −rj ∥v∥2 , rj) satisfy (8.12) then
11
Return: (τj −rj ∥v∥2 , rj);
12
else
13
τj+1 ←τj −2rj;
14
j ←j + 1;
15 else
16
while τj ≤τmed do
17
ℓj ←ρ−(τj) and rj ←
q
2γ
ℓj ;
18
if T (1)
out, T (2)
out induced by (τj + rj ∥v∥2 , rj) satisfy (8.12) then
19
Return: (τj + rj ∥v∥2 , rj);
20
else
21
τj+1 ←τj + 2rj;
22
j ←j + 1;
23 Return: "Tail bound";

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 349
By applying this inequality inductively with j = K −1, and recalling gK ≤1
2, we have
1
2
(1+β)K
≥g(1+β)K
K
> g0 =⇒2(1+β)K < 1
g0
=⇒K = O
 1
β log log
 1
g0

.
(8.17)
Next, since τK+1 = τ0 −2 P
0≤j≤K rj ∥v∥2 < τmed, we have
τ0 −τmed < 2
K
X
j=0
rj ∥v∥2 =
p
8γ ∥v∥2
K
X
j=0
s
1
gj
<
p
8γ ∥v∥2
K
X
j=0
A
1
(1+β)j , for A :=
r 1
g0
.
where we used our earlier guarantee (8.16) inductively. By Lemma 120, we have the desired tail
bound:
τ0 −τmed <
p
8γ ∥v∥2 · 4A
β ≤
√
128
β
r γ
g0
=⇒ρ+(τ0) = g0 <
128γ
β2 abs τ0 −τmed2 ∥v∥2
2 .
Correctness of split. Suppose that we ﬁnd (τ, r) such that for
T (1)
out := {Xi | ⟨v, Xi⟩≤τ + r ∥v∥2}, T (2)
out := {Xi | ⟨v, Xi⟩≥τ −r ∥v∥2},
we have
min

1 −
T (1)
out

|Tin| , 1 −
T (2)
out

|Tin|

≥2γ
r2 .
(8.18)
We show that the downweighting 1
n1Tin →1
n1T (i)
out is a weight removal with respect to γ-safe scores
for one of i = 1, 2.
Let τ ∗:= ⟨v, µ∗⟩where µ∗is the "true mean vector" in Assumption 13.
Clearly, either τ ∗≥τ or τ ∗≤τ; suppose without loss of generality τ ∗≥τ as the other case
follows symmetrically. Deﬁne the scores {si}i∈Tin to be 1 if i ∈Tin \ T (2)
out and 0 otherwise; then the
downweighting 1
n1Tin →1
n1T (2)
out is of the form in Lemma 112, with respect to these scores. Note that
1
γ
X
i∈Tin
1
|Tin|si = 1
γ
 
1 −abs T (2)
out
abs Tin
!
≥2
r2
by the assumption (8.18). To apply Lemma 112, it remains to show that
X
i∈S∩Tin
1
abs S ∩Tin
si ≤2
r2 .
(8.19)
However, we can extend the deﬁnition of the scores {si}i∈S to include points in S \ Tin, so that si is
the indicator function of ⟨v, Xi⟩< τ −r ∥v∥2 for all i ∈S, which is consistent with our deﬁnitions

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 350
{si}i∈Tin. Then by Chebyshev's inequality and Assumption 13, using ⟨v, µ∗⟩≥τ,
X
i∈S
1
|S|si ≤
Pr
i∼unifS
h
⟨v, Xi −µ∗⟩2 > r2 ∥v∥2
2
i
≤1
r2 .
(8.20)
By Lemma 114, if Tin is γ-saturated then abs S ∩Tin ≥1
2|S|; combining with (8.20) yields (8.19).
Lemma 120. Let A >
√
2, β ∈(0, 1], and let K be such that A
1
(1+β)K >
√
2. Then, we have
K
X
j=0
A
1
(1+β)j ≤4A
β .
Proof. Deﬁne f(x) = A
1
(1+β)x for any 0 ≤x ≤K, and note this is a decreasing function in x.
Thus, since by direct computation the antiderivative of ABx is
1
log B Ei(Bx log(A)) where Ei is the
exponential integral,
K
X
j=0
A
1
(1+β)j ≤A +
Z K
0
f(x)dx = A +
1
log(1 + β)

Ei (log A) −Ei

log A
(1 + β)K

≤A + 2
β (Ei (log A) + 1) .
In the last line, we used log(1 + β) ≥β
2 for β ∈(0, 1],
log A
(1+β)K > log(
√
2) by assumption, and Ei is
increasing with Ei(log(
√
2)) > −1. The conclusion follows from Ei(log A) + 1 ≤3
2A for A >
√
2.
8.4.4
Fixing a cluster via fast ﬁltering
In this section, we prove Lemma 118 by providing Fixing and giving its analysis. Before stating the
algorithm, we provide a helper result which analyzes the eﬀect of a "randomized dropout scheme"
with respect to safe scores, and shows with high probability it is still safe.
In other words, RandDrop removes points from T ′ with probability proportional to their score.
Lemma 121. The output of RandDrop satisﬁes the guarantees given in Line 2 of Algorithm 37.
Proof. For all i ∈T ′, let Zi be the random variable deﬁned as
Zi =



1
with probability
si
smax
0
with probability 1 −
si
smax
.
Note that the number of points removed from T ′ and T ′∩S are respectively P
i∈T ′ Zi and P
i∈T ′∩S Zi.
We now obtain high-probability bounds on both of these totals.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 351
Algorithm 37: RandDrop(T ′, δrd, s)
1 Input: T ′ ⊆T, δrd ∈(0, 1), 4γ-safe scores {si}i∈T ′ with respect to w := 1
n1T ′ such that
smax := maxi∈T ′ si ≤24|T ′ ∩S|, and
X
i∈T ′
1
|T ′|si ≥288γ log
 2
δrd

.
(8.21)
;
2 Output: With failure probability ≤δrd, outputs T ′′ ⊆T ′ such that if w is γ-saturated,
then 1
n1T ′′ is γ-saturated.;
3 T ′′ ←∅;
4 for i ∈T ′ do
5
T ′′ ←T ′′ ∪{Xi} with probability 1 −
si
smax ;
6 Return: T ′′;
First, we lower bound P
i∈T ′ Zi.
Observe that E
P
i∈T ′ Zi

= P
i∈T ′
si
smax , and each Zi is
Bernoulli. Thus we can apply a Chernoﬀbound to obtain
Pr
"X
i∈T ′
Zi < 1
2
X
i∈T ′
si
smax
#
≤exp
 
−1
8
X
i∈T ′
si
smax
!
≤δrd
2 ,
where we used smax ≤24|T ′| and the assumed lower bound (8.21) to conclude
X
i∈T ′
si
smax
≥
X
i∈T ′
si
24|T ′| ≥8 log
 2
δrd

.
Next, we upper bound P
i∈T ′∩S Zi. We claim with failure probability at most δrd
2 ,
X
i∈T ′∩S
Zi ≤1
2γ
abs T ′ ∩S
|T ′|
X
i∈T ′
si
smax
.
(8.22)
Deﬁne µ := P
i∈T ′∩S
si
smax to be the expectation of the left hand side of (8.22), and set
∆:= 1
µ
 
1
2γ
abs T ′ ∩S
|T ′|
X
i∈T ′
si
smax
!
−1
so that (1 + ∆)µ is the right hand side of (8.22). Recall that we assumed that {si}i∈T were 4γ-safe;
rearranging this deﬁnition (cf. Deﬁnition 29) yields
µ ≤1
4γ · abs T ′ ∩S
|T ′|
X
i∈T ′
si
smax
=⇒∆µ = 1
2γ
abs T ′ ∩S
|T ′|
X
i∈T ′
si
smax
−µ ≥1
4γ · abs T ′ ∩S
|T ′|
X
i∈T ′
si
smax
.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 352
However, since abs T ′∩S
smax
≥
1
24 by assumption, we use (8.21) and the above equation to conclude
∆µ ≥3 log
 2
δrd

.
Finally, a Chernoﬀbound shows the failure probability of (8.22) is at most exp

−∆µ
3

≤δrd
2 , as
desired. Thus with probability at least 1 −δrd,
X
i∈T ′∩S
1
|T ′ ∩S|Zi ≤1
γ
X
i∈T ′
1
|T ′|Zi.
Now observe that the {Zi}i∈T ′ meet the deﬁnition of γ-safe scores (Deﬁnition 29). Thus, Lemma 112
applies with weights 1
n1T ′ and 1
n1T ′′ and we obtain the conclusion.
We are now ready to state the algorithm Fixing and prove its guarantees in Lemma 118.
Lemma 118. There is an algorithm, Fixing (Algorithm 38), which takes as input Tin ⊆T, v ∈
Rd, and R ∈R≥0 and produces Tout with the following guarantee with probability at least 1 −δ.
Deﬁne Tmid as in Line 5 of Algorithm 35.
Then if
1
n1Tin is γ-saturated, so is
1
n1Tout, and if
D
g
Cov 1
n 1(Tmid), vv⊤E
≤1
8R2 ∥v∥2
2, then
D
g
Cov 1
n 1(Tout), vv⊤E
≤1
2R2 ∥v∥2
2.
Proof. This proof proceeds in three parts. First, we show that whenever the average score is small:
1
|Tout|
X
i∈Tout
si ≤288γ log
 2
δrd

∥v∥2
2 ,
then the check in Line 11 will fail and the algorithm will terminate. Next, we show calls to RandDrop
meet its input criteria so its conclusion holds inductively (correctness of Line 10 is also handled here).
Finally, we show that Fixing fails with probability at most δ. Assume throughout that 1
n1Tin is γ-
saturated; else there is nothing to prove. We also use the following notation throughout:
Varv (T ′) :=
1
abs T ′
X
i∈T ′
(Yi −µv (T ′))2 , where µv (T ′) :=
D
v, µ 1
n 1 (T ′)
E
, for all T ′ ⊆T.
(8.23)
Small average score implies termination. We show that whenever the average score is small:
1
|Tout|
P
i∈Tout si ≤288γ log

2
δrd

∥v∥2
2, we terminate since this implies
D
g
Cov 1
n 1(Tout), vv⊤E
≤1
2R2 ∥v∥2
2 .

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 353
Algorithm 38: Fixing(Tin, α, v, δ, R)
1 Input: Tin ⊆T, α ∈(0, 1
2), v ∈Rd, δ ∈(0, 1), R ∈R≥0 satisfying (for a suﬃciently large
constant)
R = Ω
 s
γ log
log d
δ
!
,
such that for Tmid deﬁned in Algorithm 35,
D
g
Cov 1
n 1(Tmid), vv⊤E
≤1
8R2 ∥v∥2
2;
2 Output: Outputs Tout ⊂Tin with
D
g
Cov 1
n 1(Tout), vv⊤E
≤1
2R2 ∥v∥2
2 .
If 1
n1Tin is γ-saturated, so is 1
n1Tout, with failure probability ≤δ.;
3 if
D
g
Cov 1
n 1(Tin), vv⊤E
≤1
2R2 ∥v∥2
2 then
4
Return: Tin;
5 Yi ←⟨v, Xi⟩for all i ∈Tin, τmed ←med ({Yi | i ∈Tin});
6 I ←[τmed −c, τmed + c] is the smallest interval containing the 1 −α
4 quantiles of
{Yi | i ∈Tin} for c ∈R≥0 and 2I ←[τmed −2c, τmed + 2c];
7 Deﬁne scores {si}i∈Tin by
si ←





0
Yi ∈I
(Yi −(τmed −c))2
Yi ≤τmed −c
(Yi −(τmed + c))2
Yi ≥τmed + c
;
8 δrd ←
δ
Ω(log d·log d
δ) for a suﬃciently large constant;
9 Tout ←Tin \ {Xi | si ≥12 ∥v∥2
2 |S|};
10 while
D
g
Cov 1
n 1(Tout), vv⊤E
> 1
2R2 ∥v∥2
2 do
11
Tout ←RandDrop(Tout, δrd,
s
∥v∥2
2 );
12 Return: Tout;
To show this, we ﬁrst prove
si > 1
16 (Yi −µ2I)2 for all i ∈Tout, Yi ̸∈2I,
where µ2I :=
1
abs Tin ∩{i | Yi ∈2I}
X
i∈Tout∩{i|Yi∈2I}
Yi.
(8.24)
In other words, µ2I is the mean of points in 2I. To see this, if Yi = τmed −2c −∆for ∆> 0,
si = (c + ∆)2 , (Yi −µ2I)2 ≤(4c + ∆)2 < 16si.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 354
The case when Yi = τmed + 2c + ∆is handled similarly, which covers all Yi ̸∈2I. Then, following
notation (8.23),
Varv(Tout) =
X
i∈Tout
1
abs Tout
(Yi −µv(Tout))2 ≤
X
i∈Tout
1
abs Tout
(Yi −µ2I)2
=
X
i∈Tout∩{i|Yi∈2I}
1
|Tout| (Yi −µ2I)2 +
X
i∈Tout∩{i|Yi /∈2I}
1
|Tout| (Yi −µ2I)2
≤
X
i∈Tout∩{i|Yi∈2I}
1
|Tout| (Yi −µ2I)2 + 16
X
i∈Tout
1
|Tout|si
≤
X
i∈Tout∩{i|Yi∈2I}
1
|Tout| (Yi −µ2I)2 + O

γ log
 1
δrd

∥v∥2
2 .
Here, the ﬁrst line used Fact 15, the third used (8.24) and that all scores are nonnegative, and the
last used our assumption on the average score in Tout. Thus,
D
g
Cov 1
n 1 (Tout) , vv⊤E
=
X
i∈Tout
1
n (Yi −µv(Tout))2
≤
X
i∈Tout∩{i|Yi∈2I}
1
n(Yi −µ2I)2 + O

γ log
 1
δrd

∥v∥2
2
=
D
g
Cov 1
n 1 (Tout ∩{i | Yi ∈2I}) , vv⊤E
+ O

γ log
 1
δrd

∥v∥2
2
≤1
8R2 ∥v∥2
2 + O

γ log
 1
δrd

∥v∥2
2 ≤1
2R2 ∥v∥2
2 .
In the second line we used n ≥|Tout| to handle the second term, the third line used the deﬁnition
of g
Cov, and the fourth line used the assumed bound on
D
g
Cov 1
n 1(Tmid), vv⊤E
, and
g
Cov 1
n 1 (Tout ∩{i | Yi ∈2I}) ⪯g
Cov 1
n 1 (Tmid)
since Tout ∩{i | Yi ∈2I} ⊆Tmid and Fact 15 implies that dropping terms from the covariance
formula and shifting to the mean only decreases Loewner order. The last line used the lower bound
on R.
Correctness of calls to RandDrop. We ﬁrst bound the average score in Tin ∩S at the beginning
of the algorithm. Let Varv(Tin ∩S) denote the variance of Tin ∩S in the direction v following (8.23).
We claim that the mean of Tin ∩S lies close to I: in particular,
µ 1
n 1(Tin ∩S) ∈
h
τmed −c −
p
2Varv (Tin ∩S), τmed + c +
p
2Varv (Tin ∩S)
i
.
(8.25)

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 355
If this were not the case, we would have a contradiction:
Varv (Tin ∩S) ≥
1
abs Tin ∩S
X
i∈Tin∩S|Yi∈I
(Yi −µv (Tin ∩S))2
> abs Tin ∩S ∩{i | Yi ∈I}
abs Tin ∩S
(2Varv (Tin ∩S)) ≥Varv (Tin ∩S) .
The second inequality used that every summand is at least 2Varv (Tin ∩S) if (8.25) does not hold,
and the last used that I contains a 1 −α
4 proportion of the points in Tin, and by Lemma 114 Tin ∩S
contains at least α
2 of the points in Tin. Now using (8.25) and the deﬁnition of the scores,
X
i∈Tin∩S
1
abs Tin ∩S si ≤
X
i∈Tin∩S
1
abs Tin ∩S

abs Yi −µv (Tin ∩S) +
p
2Varv (Tin ∩S)
2
≤6Varv (Tin ∩S) ≤12 ∥v∥2
2 .
(8.26)
In the last line, we used that Tin ∩S contains at least half the points in S by Lemma 114, so
Assumption 13 applies with a normalizing factor at most twice as large. This shows that Line 10 of
Algorithm 38 preserves saturation, since it can only remove points in Tin \ S (if any point in Tin ∪S
had a score larger than 12|S| ∥v∥2
2, it would violate (8.26)). This also shows that if at any point in
running Algorithm 38 we have a γ-saturated subset Tout ⊂Tin, then
X
i∈Tout∩S
1
abs Tout ∩S si ≤24 ∥v∥2
2 .
(8.27)
This is because compared to (8.26), we can at most double the normalizing factor by Lemma 114,
and all scores are nonnegative. By combining with the ﬁrst part of this proof, whenever Line 11
passes,
1
|Tout|
X
i∈Tout
si > 288γ log
 2
δrd

∥v∥2
2 ,
(8.28)
and hence the scores are 4γ-safe as required by RandDrop. The second requirement of RandDrop is
that smax ≤24|Tout ∩S| ∥v∥2
2, which is taken care of by Line 10 as |S| ≤2|Tout ∩S| by Lemma 114.
Finally, Line 11 implies (8.28) by the ﬁrst part of this proof, which is the third condition of RandDrop.
Bounding failure probability. We bound the failure probability in two steps. First, we show with
probability at least 1 −δ
2, there are at most (for a suitable constant)
N := O

log d · log d
δ

calls to RandDrop.
Then, we union bound to show that all these calls to RandDrop pass with
probability at least 1 −δ
2. Combining gives the overall failure probability to Fixing.
To see the bound on N, observe that after Line 10, the largest score is at most O(∥v∥2
2 d), and

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 356
the algorithm ends when the largest score is at most a constant (since then (8.28) clearly fails, at
which point we terminate on Line 11 by the ﬁrst part of this proof). Thus, the largest score can
only halve at most O(log d) times. However, observing the implementation of RandDrop, any point
with score at least half the largest is dropped with probability at least 1
2, and hence after O(log d
δ )
rounds, the largest score will halve with probability at least 1 −
δ
Ω(log d). Union bounding over all
the phases of halving the max score implies after N loops the algorithm terminates with probability
1 −δ
2.
Since there are at most N calls to RandDrop, it suﬃces to set δrd =
δ
2N to check that all calls to
RandDrop pass with probability 1 −δ
2. If all calls pass, we have the desired conclusion.
8.4.5
Runtime analysis
We now give a runtime bound for 1DPartition, and use it to obtain a similar bound on Partition.
Lemma 122. Let n′ := |T ′|, where T ′ is the input to 1DPartition. Then 1DPartition can be imple-
mented to run in time
O

n′d + (n′)1+β
 1
β log log d · log
 1
αβ

+ log d log d
δ

.
Proof. We begin by computing all the points Yi := ⟨v, Xi⟩for i ∈T ′ and sorting them, and store all
quantiles (i.e. the number of points less than any given Yi), which takes time O(n′d + n′ log n′).
Next, we bound the cost of running Fixing on an input Tin of size nin. Given access to quantile
information, and since Fixing is only ever called on a set which is formed after applying some number
of splits to the original dataset T ′, it is straightforward to implement Lines 3-10 in time O(nin).
Moreover, each loop in Lines 11-13 costs O(nin) time, and by the proof of Lemma 118, there are at
most O(log d log d
δ ) loops. Thus overall the runtime of Fixing is
O

nin log d log d
δ

.
We now consider the cost of running SplitOrTailBound with a given threshold τ0. If τ0 does not lie in
the interval of {Yi}i∈Tin, then the runtime is O(1). Otherwise, consider the case τ0 ≥τmed (note τmed
can be computed in constant time given quantile information). Since at least one point is larger than
τ0, g0 ≥1
n, and hence (8.17) shows the number of threshold checks is bounded by O( 1
β log log d).
Each threshold check takes constant time (we just need to compute the cardinalities of the induced
T (1)
out, T (2)
out) and computing the next gj and rj takes constant time given quantile information, so the
cost of SplitOrTailBound is
O
 1
β log log d

.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 357
Correspondingly, the cost of each run of Lines 8-11 of SplitOrCluster is bounded by
O
 1
β log log d · log
 1
αβ

.
Now, consider the structure of 1DPartition. Lemma 116 shows that there are at most (n′)1+β split
steps total, so the total cost of all split steps (which run Lines 8-11 of SplitOrCluster) is
O
(n′)1+β
β
log log d · log
 1
αβ

.
Finally, consider all nodes in the 1DPartition which are parents of leaves. The sums of cardinalities
of all such nodes is bounded by (n′)1+β, so the cost of running Fixing on all these nodes is
O

(n′)1+β log d log d
δ

.
As an immediate corollary, we obtain a runtime bound on Partition.
Corollary 29. Let np := |Tp| for some Tp ⊆T. Partition called on input Tp with parameter C can
be implemented to run in time
O

n1+β
p
d log d log d
δ + n1+β
p
 1
β log log d · log
 1
αβ

log d
δ + log d log2 d
δ

.
Proof. The proof is identical to Corollary 68, where we use Lemma 122 to bound the cost over all
elements of each Sj, and there are Ndir = Θ(log d
δ ) calls to 1DPartition.
8.4.6
Full bounded covariance algorithm
Finally, we give our full algorithm for list-decodable mean estimation under Assumption 13. As
in Section G.5.3, we will reduce to the bounded diameter case via the algorithm NaiveCluster (cf.
Lemma 311); we reproduce its guarantees for arbitrary failure probabilities as NaiveClusterPlus.
Lemma 123 (Restatement, Lemma 306). There is a randomized algorithm, NaiveClusterPlus(T, δ),
which takes as input T ⊂Rd satisfying Assumption 13 and partitions it into disjoint subsets {T ′
i}i∈[k]
such that with probability at least 1 −δ, all of S is contained in the same subset, and every subset
has diameter bounded by O( d8
δ2 ). The runtime of NaiveClusterPlus is O(nd + n log n).
We also require a post-processing procedure to reduce the list size, which we call IteratePostProcess.
We state its guarantees in Lemma 124, and defer the description and analysis to Section 8.4.7.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 358
Lemma 124. There is an algorithm, IteratePostProcess (Algorithm 41), which takes as input T
satisfying Assumption 13 and a list L ⊂Rd of length m ≤n such that
min
ˆµ∈L ∥ˆµ −µ∗∥2 ≤∆, ∆= Ω
 1
√α

and returns with probability at least 1−δ a subset L′ ⊂L of size O( 1
α) such that minˆµ∈L′ ∥ˆµ −µ∗∥2 =
O(∆), within runtime
O
 (m + n)d + αm2n

log d
δ

.
Algorithm 39: FastMultiﬁlter(T, α, δ, β)
1 Input: T ⊂Rd, |T| = n satisfying Assumption 13 with parameter α ∈(0, 1
2), δ ∈(0, 1),
β ∈(0, 1] ;
2 Output: With failure probability ≤δ: L with |L| = O( 1
α) such that some ˆµ ∈L satisﬁes
∥ˆµ −µ∗∥2 = O


s
log
  1
α

α
· max
 
1
β
s
log
 1
αβ

,
p
log log d
!
.
(8.29)
;
3 δouter ←1
2;
4 Nruns ←⌈2 log 2
δ ⌉;
5 L ←∅;
6 for j ∈[Nruns] do
7
{T ′
i}i∈[k] ←NaiveClusterPlus(T, δouter
3
);
8
αi ←|T |
|T ′
i |α for all i ∈[k];
9
L ←
L ∪IteratePostProcess

T, S
i∈[k] FastMultiﬁlterBoundedDiameter(T ′
i, αi, δouter
3
, β), δouter
3

;
10 Return: IteratePostProcess
 T, L, δ
2

;
Proposition 31. FastMultiﬁlterBoundedDiameter meets its output speciﬁcations with probability at
least 1 −δ, within runtime
O

n1+βd log2 d log2 d
δ + n1+β
 1
β log log d · log
 1
αβ

log d log2 d
δ + log2 d log3 d
δ

.
Proof. The proof of the error rate is identical to that in Proposition 73, where the initial potential
Φ0 is bounded by ( d
δ )O(log d) via Lemma 123, which implies the operator norm of g
Cov 1
n 1(T ′) for
every node T ′ on layer D is O(R2), and inductively at least one such node has |T ′ ∩S| ≥1
2|S| by
virtue of being γ-saturated and applying Lemma 114. The failure probability follows since Partition
is called at most n1+βD times, as there are at most n1+β elements of each L(ℓ). Finally, the list size

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 359
Algorithm 40: FastMultiﬁlterBoundedDiameter(T, α, δ, β)
1 Input: T ⊂Rd, |T| = n satisfying Assumption 13 with parameter α ∈(0, 1
2), δ ∈(0, 1),
β ∈(0, 1];
2 Output: With failure probability ≤δ: Lout with |Lout| = O( nβ
α ) such that some ˆµ ∈Lout
satisﬁes
∥ˆµ −µ∗∥2 = O


s
log
  1
α

α
· max
 
1
β
s
log
 1
αβ

,
s
log
log d
δ
!
.
;
3 L(0) ←{T}, Lout ←∅;
4 For suﬃciently large constants,
R ←Θ
 
max
 
1
β ·
s
log
 1
α

log
 1
αβ

,
s
log
 1
α

log
d
δ
!!
, D ←Θ

log d log d
δ

;
5 for ℓ∈[D] do
6
L(ℓ) ←∅;
7
for T ′ ∈L(ℓ−1) do
8
Append all elements of Partition(T ′, α,
δ
n1+βD, β, R) to L(ℓ);
9 Return: List of empirical means of all sets in L(D) with size at least αn
2 ;
follows since Lemma 114 implies every leaf node contains αn
2 , but the total size across leaves is at
most n1+β.
Finally, to obtain the runtime bound we can sum the guarantee of Corollary 29 across each of
the D layers, and use the potential to bound the sum of all n1+β
p
across the layer.
We are now ready to state our main claim on list-decodable mean estimation. For simplicity, we
state the result for β ≥
1
log d, as otherwise there are no runtime or statistical gains asymptotically.
Theorem 51. For
1
log d ≤β ≤1, and logΩ(1)(d) ≤α−1 ≤d, FastMultiﬁlter returns a list of size
O( 1
α) such that
min
ˆµ∈L ∥ˆµ −µ∗∥2 = O
 
1
β · log
  1
α

α
!
,
with probability at least 1 −δ, within runtime
O

n1+2βd log4 d log 1
δ + nd log2 1
δ log d
δ

.
Proof. We ﬁrst analyze Lines 6-10 of FastMultiﬁlter. We claim that each of the Nruns times these

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 360
lines run, there is a ≥1
2 probability that some ˆµ will be added to L satisfying (G.2), within runtime
O

n1+2βd log4 d + n1+β
 1
β log log d · log
 1
αβ

log3 d + log5 d

= O
 n1+2βd log4 d

.
To see this, we apply Proposition 31 to the relevant call of FastMultiﬁlterBoundedDiameter. The
correctness follows identically to the proof of Theorem 91, except that the size of the list of candidate
means S
i∈[k] FastMultiﬁlterBoundedDiameter(T ′
i, αi, δouter
3
, β) is m = O( nβ
α ). By Lemma 124, after
applying IteratePostProcess the error rate is not aﬀected by more than a constant, and the list size
is O( 1
α). The runtime of this last step is dominated by O(αm2n log d) = O(n1+2βd log d).
Next, this implies that after all runs of Lines 6-10 have ﬁnished running (with independent
internal randomness), there is a ≥1 −δ
2 probability that L contains an element ˆµ satisfying (G.2).
At this point, the size of the list is m = O( log δ−1
α
), so Line 11 takes time O(nd log2 1
δ log d
δ ) by
Lemma 124.
This theorem, combined with the previously discussed fact that we can assume that α ∈[1/d, 1/ logΩ(1) d],
gives our desired conclusion.
8.4.7
Cleaning up the list
In this section, we provide the subroutine IteratePostProcess used in FastMultiﬁlter, and prove
Lemma 124, which shows correctness of this subroutine. At a high level, IteratePostProcess ﬁrst
ﬁnds a greedy cover of the input list L at distance O(∆). Then, while the greedy cover has size at
least 4k for k := ⌈1
α⌉, it iteratively prunes away 2k out of 4k hypotheses by testing that there are
enough datapoints closest to retained hypotheses; otherwise, it returns the greedy cover.
Lemma 124. There is an algorithm, IteratePostProcess (Algorithm 41), which takes as input T
satisfying Assumption 13 and a list L ⊂Rd of length m ≤n such that
min
ˆµ∈L ∥ˆµ −µ∗∥2 ≤∆, ∆= Ω
 1
√α

and returns with probability at least 1−δ a subset L′ ⊂L of size O( 1
α) such that minˆµ∈L′ ∥ˆµ −µ∗∥2 =
O(∆), within runtime
O
 (m + n)d + αm2n

log d
δ

.
Proof. We ﬁrst prove correctness, and then prove the runtime bound.
Correctness. By the Johnson-Lindenstrauss lemma as analyzed in [6], with probability at least
1 −δ every pair of points in L ∪T ∪{µ∗} has their distance preserved to a 1.1 multiplicative factor
under multiplication by G⊤. Condition on this event for the remainder of the proof.
Let ¯µ be the element of the input L which is guaranteed to be within distance ∆of µ∗. We will
ﬁrst show that ¯µ is never removed from L by the loop in Lines 6-11. If ¯µ is not a part of Lhead in a

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 361
Algorithm 41: IteratePostProcess(T, α, L, δ, ∆)
1 Input: T ⊂Rd, |T| = n satisfying Assumption 13 with parameter α ∈(0, 1
2), δ ∈(0, 1), L
with |L| = m ≤n such that
min
ˆµ∈L ∥ˆµ −µ∗∥2 ≤∆, ∆= Ω
 1
√α

.
;
2 Output: With failure probability ≤δ: L′ ⊂L with |L′| = O( 1
α) such that
min
ˆµ∈L′ ∥ˆµ −µ∗∥2 = O(∆).
;
3 G ∈Rd×c ←entrywise ± 1
√c uniformly at random, for c = Θ(log d
δ ) (Johnson-Lindenstrauss
matrix [6]);
4 k ←⌈1
α⌉;
5 L′ ←maximal subset of L such that ∀ˆµ ̸= ˆµ′ ∈L′,
G⊤(ˆµ −ˆµ′)

2 ≥5∆;
6 while |L′| ≥4k do
7
Lhead ←ﬁrst 4k elements of L′;
8
Lprune ←elements of Lhead which are nearest neighbors of < αn
2 elements of T, where
ˆµ ∈Lhead is the nearest neighbor of Xi ∈T if
G⊤(ˆµ −Xi)

2 is minimal amongst
Lhead ;
9
L ←L \ Lprune;
10
L′ ←maximal subset of L such that ∀ˆµ ̸= ˆµ′ ∈L′,
G⊤(ˆµ −ˆµ′)

2 ≥5∆;
11 Return: L′;

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 362
given loop, clearly this is true, so suppose ¯µ ∈Lhead, and let ˆµ be some other element in Lhead with
G⊤(¯µ −ˆµ)

2 ≥5∆; by deﬁnition of Lhead as a subset of L′, all such ˆµ satisfy this. Our goal will
be to show that at least half of the points in S have nearest neighbor ¯µ; to do so, it suﬃces to show
that
G⊤(Xi −ˆµ))

2 ≥
G⊤(Xi −¯µ))

2 with probability at most
1
8k over i ∼S for each ˆµ ̸= ¯µ,
so the < 4k other hypotheses in Lhead can only remove αn
2 of the points in S from having nearest
neighbor ¯µ, and hence ¯µ will not be pruned.
We now show the key claim: that for all ˆµ ̸= ¯µ ∈Lhead,
Pr
i∼unifS
G⊤(Xi −ˆµ)

2 ≤
G⊤(Xi −¯µ)

2

≤1
8k .
Observe that by the triangle inequality, for any i ∈S satisfying the event above,
2
G⊤(Xi −¯µ)

2 ≥
G⊤(Xi −¯µ)

2 +
G⊤(Xi −ˆµ)

2
≥
G⊤(ˆµ −¯µ)

2 ≥5∆
=⇒∥Xi −µ∗∥2 ≥∥Xi −¯µ∥2 −∥¯µ −µ∗∥2 ≥∆.
Here we used ∥Xi −¯µ∥2 ≥
1
1.1
G⊤(Xi −¯µ)

2 ≥2∆, and ∥¯µ −µ∗∥2 ≤∆by assumption.
By
Chebyshev's inequality and Assumption 13, we conclude for suﬃciently large ∆= Ω( 1
√α),
Pr
i∼unifS
G⊤(Xi −ˆµ)

2 ≤
G⊤(Xi −¯µ)

2

≤
Pr
i∼unifS [∥¯µ −µ∗∥2 ≥∆] ≤1
8k .
Finally, we have shown that when the algorithm exits on Line 12, L′ is a maximal separated subset
of a pruned list L containing ¯µ. If ¯µ ∈L′, the guarantee is immediate; otherwise, there must have
been some other ˆµ ∈L′ with
G⊤(ˆµ −¯µ)

2 ≤5∆, else ¯µ would have been added. For this ˆµ,
∥ˆµ −µ∗∥2 ≤1.1
G⊤(ˆµ −µ∗)

2 ≤1.1
G⊤(ˆµ −¯µ)

2 + 1.1
G⊤(¯µ −µ∗)

2 = O(∆).
The list size bound follows from Line 6, as the returned L′ has at most 4k elements.
Runtime. First, the cost of computing all projections G⊤X for X ∈L ∪T is O((m + n)d log d
δ ).
Next, Lines 6-11 can only be looped over at most O(αm) times, since every loop removes 2k elements
from L which originally has size m. It remains to argue about the complexity of each loop.
The cost of computing a maximal subset in Lines 5 and 10 is O(m2 log d
δ ), since distance compar-
isons under multiplication by G take O(log d
δ ) and it suﬃces to greedily loop over the list. Similarly,
the cost of computing nearest neighbors of all elements in T in Line 8 is O(mn log d
δ ), which is the
dominant term. Combining these components yields the claim.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 363
8.4.8
(Slightly) improving the error rate
We give a brief discussion of how it is possible to shave a
p
log α−1 factor from the error guarantees
of Theorem 51, bringing it to within a
p
log α−1 factor from optimal when β is a constant. At a
high level, this extraneous factor is due to our insistence that all weight removals be γ-safe, for some
γ = Θ(log α−1). This causes the thresholds required for termination of our subroutines (e.g. for
SplitOrCluster to enter the Fixing stage) to be inﬂated by roughly a γ factor.
We can remove this factor by using 2-safe scores instead of Θ(log α−1)-safe scores, an idea
introduced by Section G.2 to obtain improved estimation rates over the multiﬁlter of [179]. The idea
is to restart the algorithm in phases, where each phase corresponds to the total maintained weight
being stable up to a factor of 2 (in our case, this means subset sizes are stable up to factors of 2).
We now summarize the changes to our algorithm.
We will run the "outer loop" subroutine
FastMultiﬁlterBoundedDiameter (which can be viewed as constructing a multiﬁlter tree) up until a
depth of O(log2 d log 1
α) is reached, in batches of O(log2 d) each corresponding to a stable phase.
Each batch will either meet the relevant termination condition (bounded covariance, such that e.g.
(8.10) is trivially satisﬁed), or make progress by entering the next phase via safe weight removals.
Correspondingly, the condition (8.6) required to make improvements on the potential will be
scaled diﬀerently, according to the size of the relevant set Tp at some node p. In particular, suppose
we are in a phase when 1
2n′ < |Tp| ≤n′. Then we will aim to guarantee

Y2
p, Mcℓ

≤R2
r
n′
n Tr(Y2
p),
where R has the same value as in FastMultiﬁlterBoundedDiameter up to removing a
p
log α−1. This
allows us to terminate when the operator norm of some (unnormalized) g
Cov matrix is O(R2
q
n′
n ),
at which point Lemma 113 concludes a distance of
O


s
R2
r n
n′ ·
n′
|Tp ∩S|

= O
 
R ·
4√
n′n
p
|Tp ∩S|
!
= O
 R
√α

,
where we use that 2-saturation of Tp implies |Tp∩S|
n
≥α
q
n′
2n.
Because of the complications this type of argument introduces, e.g. every one of our subroutines
needs an extra exit condition (when the maintained subset enters the next phase), we omit a formal
treatment here. However, we remark that to remove the entire log α−1 factor from our error likely
requires new ideas. This is because both branches of our key subroutine SplitOrCluster, namely
SplitOrTailBound and Fixing, require this overhead. The former is because integrating variance tail
bounds decaying as t · 1
t2 out to O(α) quantiles (cf. Lemma 119) introduces a gap of log( 1
α). The
latter is because we employ randomize dropout to maintain subsets (rather than weights); our
dropout method requires a threshold of roughly log log d (cf. Lemma 121) to obtain high-probability

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 364
guarantees after union bounding polylog(d) times. For α−1 = logΩ(1) d, this is again a log( 1
α) gap.
8.5
Clustering mixture models
We deﬁne a mixture model to be a mixture P
i∈[k] αi disti where {αi}i∈[k] ∈Rk
≥0, P
i∈[k] αi = 1, and
all disti are supported on Rd. In Sections 8.5.1 and 8.5.2 we handle the case where all distributions
are sub-Gaussians: disti has mean µi, and sub-Gaussian parameter ≤1 in all directions (cf. Sec-
tion 8.3.1). We begin with the uncorrupted, uniform mixture case as a warmup in Section 8.5.1, and
show how our method tolerates non-uniformity and adversarial outliers in Section 8.5.2. We then
give a simple extension of our algorithm to handle mixtures where each component has bounded
fourth moment in Section 8.5.3, and ﬁnally tackle the case of bounded-covariance mixture models
in Section 8.5.4.
Broadly, all of our clustering algorithms follow the same design framework. We ﬁrst demonstrate
using concentration and existence of a good hypothesis (the list-decodable learning guarantee), that
the "nearest hypothesis" to every non-adversarial point is close to the true mean. We next prune
our hypotheses down by only keeping those with a substantial number of nearby points; by arguing
that the number of adversarial points (or points that appear adversarial due to anti-concentration)
is small, no large "coalition" of bad points can be formed, and hence all kept hypotheses are near a
true mean. Finally, assuming enough separation between true means, we can deﬁne a partition of the
points based on their nearest hypotheses. In Section 8.5.4, we will use a more direct clustering process
in the subspace spanned by candidates, combined with fast projected distance approximations, in
order to obtain a tighter separation guarantee.
Throughout, we will frequently use that by Chernoﬀ, the sum of any Bernoulli random variables
whose expectation is Ω(d) will deviate from its expectation by at most any multiplicative constant
with probability at least 1 −exp(−Ω(d)). For example, for a dataset of size n = Ω(dk) drawn from
a uniform mixture P
i∈[k]
1
k disti, each component disti will contribute between 0.99 n
k and 1.01 n
k
points with probability at least 1 −k exp(−Ω(d)), or more simply 1 −exp(−Ω(d)) for k = O(d).
8.5.1
Clustering uniform (sub-)Gaussian mixture models
We ﬁrst consider the simple setting where all αi = 1
k and all disti has mean µi and sub-Gaussian
parameter ≤1 in all directions. We assume access to a list-decoding algorithm A which returns a
list L of length O(k), such that for each i ∈[k], L contains ˆµi such that ∥ˆµi −µi∥2 ≤∆, for some
∆= Ω(
√
k) (in particular, FastMultiﬁlter suﬃces for A). Finally, we assume access to a dataset
X = {Xj}j∈[n] of size n = Θ(dk) drawn from the mixture model independently of A, where we say
that each Xj is "associated with" an index i ∈[k] (designating the component it is drawn from).
We will now demonstrate how to cluster a dataset using calls to A, assuming a suﬃciently large
separation between the means of any two mixture components.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 365
Algorithm 42: ClusterUniformGMM(X, L, ∆, k, δ)
1 Input: X = {Xj}j∈[n] ∼P
i∈[k]
1
k disti where disti has mean µi and sub-Gaussian
parameter ≤1 in all directions, and n = Θ(dk), L of size O(k) containing (for all i ∈[k])
ˆµi ∈L with ∥ˆµi −µi∥2 ≤∆for ∆= Ω(
√
k), δ ∈(0, 1);
2 G ∈Rd×c ←entrywise ± 1
√c uniformly at random, for c = Θ(log n
δ ) (Johnson-Lindenstrauss
matrix [6]);
3 Let m : [n] →L map each Xj to the element ˆµ ∈L minimizing
G⊤(Xj −ˆµ)

2;
4 Deﬁne an equivalence relation ∼on X by Xi ∼Xj iﬀ
G⊤(m(i) −m(j))

2 ≤18∆; if this is
not an equivalence relation, then return any labeling;
5 Return: Labeling of X associated with ∼;
We begin with the following observation.
Lemma 125. Consider some Xj ∈X associated with i ∈[k].
With probability at least 1 −
exp(−Ω(∆2))), for every pair ˆµ, ˆµ′ ∈L, ˆµ ̸= ˆµ′ letting vˆµˆµ′ be the unit vector in the direction
ˆµ −ˆµ′,
⟨vˆµˆµ′, Xj⟩< ⟨vˆµˆµ′, µi⟩+ ∆.
Proof. This is a standard application of sub-Gaussian concentration (on the one-dimensional distri-
bution N(⟨vˆµˆµ′, µi⟩, ⟨Σi, vˆµˆµ′v⊤
ˆµˆµ′⟩)), where we union bound across O(k2) pairs of elements in L. We
simplify by using ∆= Ω(
√
k), so the exponential term dominates the k2 union bound overhead.
Next, we give our key structural lemma regarding the map m.
Lemma 126. Following notation of Algorithm 42, with probability at least 1 −δ −n exp(−Ω(∆2)),
every Xj associated with i ∈[k] satisﬁes ∥m(j) −µi∥2 ≤7∆.
Proof. With probability at least 1−δ, all pairwise distances between X∪L and itself are preserved by
multiplication through G⊤up to a 1 ± 0.1 factor [6] (which we will call the "Johnson-Lindenstrauss
guarantee" henceforth); condition on this event for the remainder of the proof. Suppose for contra-
diction that ∥m(j) −µi∥2 > 7∆, and let ˆµi ∈L denote any (ﬁxed) hypothesis which is promised to
satisfy ∥ˆµi −µi∥2 ≤∆.6 By the triangle inequality, ∥m(j) −ˆµi∥2 > 6∆. Then letting v be the unit
vector in the direction of m(j) −ˆµi,
⟨v, µi⟩≤⟨v, ˆµi⟩+ ∆, ⟨v, m(j)⟩> ⟨v, ˆµi⟩+ 6∆
=⇒⟨v, µi⟩< ⟨v, m(j)⟩−5∆.
6In the case multiple such hypotheses exist, any satisfactory (but ﬁxed) {ˆµi}i∈[k] ⊆L will do.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 366
Next,
G⊤(Xj −m(j))

2 ≤
G⊤(Xj −ˆµi)

2 implies ∥Xj −m(j)∥2 ≤2 ∥Xj −ˆµi∥2 by the Johnson-
Lindenstrauss guarantee, or ⟨v, Xj⟩≥
D
v, 2ˆµi+m(j)
3
E
. Combining with the above displayed equation,
⟨v, Xj⟩≥2
3 ⟨v, ˆµi⟩+ 1
3 ⟨v, m(j)⟩> ⟨v, µi⟩+ ∆.
Applying Lemma 125 and union bounding over all j ∈[n] concludes the proof.
This implies that with high probability, Algorithm 42 (given a list L meeting its prerequisites)
succeeds in correctly labelling all data points, assuming Ω(∆) separation between component means.
Lemma 127. Suppose every pair i, i′ ∈[k], i ̸= i′′ satisﬁes ∥µi −µi′∥2 > 34∆. Then with probability
at least 1−δ−n exp(−Ω(∆2)), Algorithm 42 (assuming its preconditions) outputs a correct clustering
of all points (up to label permutation).
Proof. Assume the result of Lemma 126 and that multiplication through G⊤preserves all pairwise
distances between X ∪L and itself up to a 1 ± 0.1 factor throughout this proof.
We ﬁrst prove that for any two Xj, Xj′ associated to the same i ∈[k], Line 4 of Algorithm 42
sets Xj ∼Xj′. To see this, Lemma 126 and the triangle inequality give ∥m(j) −m(j′)∥2 ≤14∆, so
this will pass Line 4 by the Johnson-Lindenstrauss guarantee. Next, suppose Xj is associated with
i ∈[k] and Xj′ is associated with i′ ∈[k] with i ̸= i′, and suppose for contradiction Xj ∼Xj′. By
the Johnson-Lindenstrauss guarantee, ∥m(j) −m(j′)∥2 ≤20∆, which yields by Lemma 126 and the
triangle inequality that ∥µi −µi′∥2 ≤34∆, contradicting the separation assumption.
We conclude with the following guarantee on ClusterUniformGMM.
Corollary 30. Suppose every pair i, i′ ∈[k], i ̸= i′ satisﬁes ∥µi −µi′∥2 = Ω(
√
k log k) for an appro-
priate constant. There is an algorithm drawing n = Θ(dk) samples from the mixture P
i∈[k]
1
k disti
where disti has mean µi and sub-Gaussian parameter ≤1 in all directions, and returns a correct
clustering of all points (up to label permutation) with probability at least
1 −δ −n exp
 −Ω
 k log2 k

−k exp(−Ω(d)).
The algorithm runs in time, for any ﬁxed ϵ0 > 0,
O

n1+ϵ0d log4 n log4 n
δ + k2 log4 n
δ + nk log n
δ

.
Proof. We begin by stating the algorithm. We take
1
10 of the dataset and run FastMultiﬁlter7 on
it to produce L satisfying the prerequisites of Algorithm 42, and then cluster the remaining
9
10 of
7If α−1 = logo(1) d, we instead run the algorithm in Section G.4 to obtain the desired error guarantee, which ﬁts
within the runtime budget. Similarly, if α−1 = Ω(d), we instead run the algorithm in Section G.1 which ﬁts within
the runtime budget by Proposition 9 of that paper.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 367
the dataset using L via ClusterUniformGMM; then, we take a disjoint
1
10 and cluster the remaining
9
10 using ClusterUniformGMM. We then match labels based on which clusters overlap on at least 1
2
of their points between these two runs. The runtime follows from Theorem 91, and the runtime of
ClusterUniformGMM, which is clearly O(nk log n
δ ) since Line 3 dominates, as Line 4 can be greedily
implemented using distance comparisons between only L once the map m has been formed.
Next, for correctness, Theorem 51 and Proposition B.1 of [118] (which says Assumption 13 is
met for both FastMultiﬁlter runs with probability ≥1−exp(Ω(d))) imply both runs of FastMultiﬁlter
correctly return lists satisfying the precondition of Algorithm 42; here we note that the dataset
partition ensures independence of lists used and datasets clustered. Then, Lemma 127 implies both
clusterings are completely correct on
9
10 of the data. The conclusion follows from standard binomial
concentration, which implies that the
8
10 of the data which was held-out contains at least 1
2 the points
associated with each i ∈[k] in the overall dataset, with probability at least 1 −exp(−Ω(d)).
We remark that for n which grows super-exponentially in k (such that the failure probabil-
ity guarantee of Corollary 30 becomes vacuous), it is straightforward to obtain an appropriate
high-probability guarantee for clustering all points by assuming that the minimum pairwise cluster
separation scales as √log n. A similar remark also applies to Corollary 33.
8.5.2
Robustly clustering (sub-)Gaussian mixture models
In this section, we generalize Corollary 30 to non-uniform corrupted mixture models. In particular,
we consider an adversarially corrupted mixture model
M = (1 −ϵ)
X
i∈[k]
αi dist
i
+ϵ dist
adv ,
(8.30)
where for all i ∈[k], disti has mean µi and sub-Gaussian parameter ≤1 in all directions. Moreover,
for some ﬁxed known α, we assume all αi ≥α and ϵ ≤α
4 . By deﬁnition of α, note that we must have
α = O( 1
k). In this section, we assume our list-decoding subroutine A returns a list of size O(α−1),
and guarantees estimation error ∆. We now state our algorithm.
We again refer to Xj as associated with some i ∈[k] if it is a draw from disti, and as "adversarial"
if it is drawn from distadv. We begin with the following consequences of Lemma 126.
Corollary 31. With probability at least 1−δ −n exp(−Ω(∆2))−k exp(−Ω(d)), both of the following
events hold. For all i ∈[k], every ˆµ ∈L with ∥ˆµ −µi∥2 ≤7∆is in L′. Moreover, every ˆµ ∈L′ has
∥ˆµ −µi∥2 ≤25∆for some i ∈[k].
Proof. By standard binomial concentration, for every i ∈[k], the set of j ∈[n] associated with i
has size at least 0.9αn with probability 1 −k exp(−Ω(d)). Condition on this event, all pairwise
distances between X ∪L and itself being preserved up to 1 ± 0.1 by multiplication through G⊤, and
the conclusion of Lemma 125 for the remainder of this proof.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 368
Algorithm 43: ClusterRobustGMM(X, L, ∆, k, δ, α)
1 Input: X = {Xj}j∈[n] ∼(1 −ϵ) P
i∈[k] αi disti +ϵ distadv where disti has mean µi and
sub-Gaussian parameter ≤1 in all directions, all αi ≥α, ϵ ≤α
4 , and n = Θ( d
α), L of size
O(α−1) containing (for all i ∈[k]) ˆµi ∈L with ∥ˆµi −µi∥2 ≤∆for ∆= Ω(
√
α−1), δ ∈(0, 1);
2 G ∈Rd×c ←entrywise ± 1
√c uniformly at random, for c = Θ(log n
δ ) (Johnson-Lindenstrauss
matrix [6]);
3 Let m : [n] →L map each Xj to the element ˆµ ∈L minimizing
G⊤(Xj −ˆµ)

2;
4 Sˆµ ←{j ∈[n] | m(j) = ˆµ} for all ˆµ ∈L, Bˆµ ←S
ˆµ′∈L|∥G⊤(ˆµ−ˆµ′)∥2≤16∆Sˆµ′;
5 L′ ←{ˆµ ∈L | |Bˆµ| ≥0.9αn};
6 Deﬁne an equivalence relation ∼on X′ by Xi ∼Xj iﬀ
G⊤(m(i) −m(j))

2 ≤55∆, for
X′ := {Xi ∈X | m(i) ∈L′}; if this is not an equivalence relation, then return any labeling;
7 Return: Labeling of X′ associated with ∼, along with X \ X′ as "unlabeled";
We begin with the ﬁrst claim: let ∥ˆµ −µi∥2 ≤7∆. For every Xj associated with i, Lemma 126
shows ∥m(j) −µi∥2 ≤7∆, implying by the Johnson-Lindenstrauss guarantee and triangle inequality,
G⊤(m(j) −ˆµ)

2 ≤16∆. Hence Xj counts towards Bˆµ, which then captures all points associated
with i, so ˆµ ∈L′. For the second claim, suppose ∥ˆµ −µi∥2 > 25∆; then, no ˆµ′ ∈L with ∥ˆµ′ −µi∥2 ≤
7∆will count towards Bˆµ, since such ˆµ′ has
G⊤(ˆµ −ˆµ′)

2 > 16∆.
By Lemma 126, the only
points that can contribute to Bˆµ are then the ones drawn from Dadv, which by standard binomial
concentration will be less than 0.6αn with probability 1 −exp(−Ω(d)), and hence ˆµ ̸∈L′.
The following conclusion then follows immediately in the vein of Lemma 127.
Corollary 32. Suppose every pair i, i′ ∈[k], i ̸= i′ satisﬁes ∥µi −µi′∥2 > 55∆. Then with probability
at least 1 −δ −n exp(−Ω(∆2)) −k exp(−Ω(d)), Algorithm 43 (assuming its preconditions) outputs
a correct clustering of all points associated with some component i ∈[k] (up to label permutation).
Proof. The proof is identical to Lemma 127, where we note for each ˆµ ∈L′, there is a unique i ∈[k]
promised by Corollary 31 where ∥ˆµ −µi∥2 ≤25∆(by our separation assumption). Thus, ∼deﬁnes
an equivalence relation, since every ˆµ ∈L′ is mapped to the unique equivalence class associated with
ˆµi. This successfully recovers the true clusters, since every point Xj associated with some i ∈[k]
will be in X′ by Corollary 31 and Lemma 126, and thus it will be classiﬁed correctly by ∼.
Finally, we give a complete guarantee on ClusterRobustGMM.
Corollary 33. Suppose every pair i, i′ ∈[k], i ̸= i′ satisﬁes ∥µi −µi′∥2 = Ω(
√
α−1 log α−1) for
an appropriate constant. There is an algorithm drawing n = Θ( d
α) samples from the mixture (1 −
ϵ) P
i∈[k] αi disti +ϵ distadv where disti has mean µi and sub-Gaussian parameter ≤1 in all directions,
and returns a correct clustering of all points drawn from some disti (up to label permutation) with
probability at least
1 −δ −n exp
 −Ω
 α−1 log2 α−1
−k exp (−Ω(d)) .

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 369
The algorithm runs in time, for any ﬁxed ϵ0 > 0,
O

n1+ϵ0d log4 n log4 n
δ + 1
α2 log4 n
δ + n
α log n
δ

.
Proof. The proof is the same as Corollary 30, where we note that the mislabeled points from distadv
can only aﬀect the overlapping labels between the two runs by at most a 1.1ϵ ≤1.1
4 α fraction, a
minority compared to the number of correct labels (due to the true mixture), which in both runs will
contribute at least 4
5αi of the points assigned by ∼to the true clustering. Hence, between runs the
clusters assigned by ∼to the points drawn from disti will overlap on at least half their points, and
we can use this agreement to correctly label all points not drawn from distadv (where throughout,
we conditioned on all high-probability events in Corollary 30's proof holding).
8.5.3
Mixture models with bounded fourth moments
In this section, we consider non-uniform corrupted mixture models under a weaker distributional
assumption based on bounded fourth moments. We will no longer be able to correctly cluster all
(non-adversarial) points, but will instead guarantee that amongst non-adversarial points, at least a
1−o(α) fraction are correctly classiﬁed. Concretely, we consider the adversarially corrupted mixture
model (8.30), where for all i ∈[k], and some constant C = O(1),
EX∼disti
h
⟨v, X −µi⟩4i
≤C for all v ∈Rd, ∥v∥2 = 1.
(8.31)
We remark that this fourth-moment bound also implies the covariance is bounded by O(1)I for all
components, by Jensen's inequality. We again assume that all αi ≥α, and ϵ ≤α
4 . Our algorithm for
this setting will be exactly the same as ClusterRobustGMM, but for convenience we restate it under
the new distributional assumptions. We also assume ∆= ω(
√
α−1) for notational simplicity.
Algorithm 44: ClusterRobustBFMM(X, L, ∆, k, δ, α)
1 Input: X = {Xj}j∈[n] ∼(1 −ϵ) P
i∈[k] αi disti +ϵ distadv where disti has mean µi and
satisﬁes (8.31), all αi ≥α, ϵ ≤α
4 , and n = Θ( d
α), L of size O(α−1) containing (for all
i ∈[k]) ˆµi ∈L with ∥ˆµi −µi∥2 ≤∆for ∆= ω(
√
α−1), δ ∈(0, 1);
2 G ∈Rd×c ←entrywise ± 1
√c uniformly at random, for c = Θ(log n
δ ) (Johnson-Lindenstrauss
matrix [6]);
3 Let m : [n] →L map each Xj to the element ˆµ ∈L minimizing
G⊤(Xj −ˆµ)

2;
4 Sˆµ ←{j ∈[n] | m(j) = ˆµ} for all ˆµ ∈L, Bˆµ ←S
ˆµ′∈L|∥G⊤(ˆµ−ˆµ′)∥2≤15∆Sˆµ′;
5 L′ ←{ˆµ ∈L | |Bˆµ| ≥0.9αn};
6 Deﬁne an equivalence relation ∼on X′ by Xi ∼Xj iﬀ
G⊤(m(i) −m(j))

2 ≤55∆, for
X′ := {Xi ∈X | m(i) ∈L′}; if this is not an equivalence relation, then return any labeling;
7 Return: Labeling of X′ associated with ∼, along with X \ X′ as "unlabeled";

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 370
Our analysis of ClusterRobustBFMM follows from the following key observation: deﬁne Xj ∈X
as "adversarial" if it is drawn from distadv, and say it is "pseudo-adversarial" if it is drawn from
some component i ∈[k], but satisﬁes ∥m(j) −µi∥2 > 7∆. Then, by the bounded fourth moment
assumption (8.31), there are few pseudo-adversarial points, made rigorous as follows.
Lemma 128. With probability at least 1−δ −exp(−Ω(d)), the number of pseudo-adversarial points
in X is o(αn).
Proof. We ﬁrst bound the probability that some Xj ∼disti will be pseudo-adversarial. By the
proof of Lemma 126, if Xj were pseudo-adversarial, it must be the case that in the direction v
corresponding to m(j) −ˆµi, ⟨v, Xj⟩> ⟨v, µi⟩+ ∆. However, by (8.31) and Markov,
Pr
X∼disti
h
⟨v, X −µi⟩4 ≥∆4i
≤C
∆4 = o
 α2
.
Union bounding over all possible directions v (one for each of the O(α−1) elements of L other than
ˆµi), this implies the chance that X drawn from any disti is pseudo-adversarial is o(α). Hence, the
expected number of pseudo-adversarial points in our entire dataset is o(αn), and the conclusion
follows by applying standard binomial concentration.
Now, Corollary 31 holds for ClusterRobustBFMM simply by lumping together the pseudo-adversarial
points and the adversarial points in its proof (in particular, no hypothesis more than 25∆away from
a true mean can capture any points in the dataset other than adversarial and pseudo-adversarial
ones). We now state analogs of Corollaries 32 and 33.
Corollary 34. Suppose every pair i, i′ ∈[k], i ̸= i′ satisﬁes ∥µi −µi′∥2 > 55∆. Then with probability
at least 1 −δ −n exp(−Ω(∆2)) −k exp(−Ω(d)), Algorithm 44 (assuming its preconditions) outputs
a correct clustering of a 1 −o(α) proportion of points associated with some component i ∈[k] (up to
label permutation).
Proof. The proof is identical to Corollary 32, where we only guarantee correctly labeling points which
are not pseudo-adversarial; ∼will correctly cluster all such points by separation and Corollary 31.
Corollary 35. Suppose every pair i, i′ ∈[k], i ̸= i′ satisﬁes ∥µi −µi′∥2 = Ω(
√
α−1 log α−1) for
an appropriate constant.
There is an algorithm drawing n = Θ( d
α) samples from the mixture
(1 −ϵ) P
i∈[k] αi disti +ϵ distadv where disti has mean µi and satisﬁes the bounded fourth moment
condition (8.31), and returns a correct clustering of a 1 −o(α) fraction of all points drawn from
some disti (up to label permutation) with probability at least
1 −δ −k exp (−Ω(d)) .

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 371
The algorithm runs in time, for any ﬁxed ϵ0 > 0,
O

n1+ϵ0d log4 n log4 n
δ + 1
α2 log4 n
δ + n
α log n
δ

.
Proof. The proof is identical to Corollary 33, using Corollary 34 instead of Corollary 32, and using
that the pseudo-adversarial points will not substantially aﬀect cluster overlap guarantees.
8.5.4
Bounded-covariance mixture models
In this section, we ﬁnally handle the case of non-uniform corrupted mixture models under only
a bounded-covariance assumption; in particular, we study (8.30) where every component disti for
i ∈[k] has covariance bounded by I. We again assume that all αi ≥α, and ϵ ≤α
4 .
Algorithm 45: ClusterRobustBCMM(X, L, ∆, k, δ, α)
1 Input: X = {Xj}j∈[n] ∼(1 −ϵ) P
i∈[k] αi disti +ϵ distadv where disti has mean µi and has
covariance bounded by I, all αi ≥α, ϵ ≤α
4 , and n = Θ( d
α), L of size O(α−1) containing
(for all i ∈[k]) ˆµi ∈L with ∥ˆµi −µi∥2 ≤∆for ∆= Ω(
√
α−1 log α−1), δ ∈(0, 1);
2 G ∈Rd×c ←entrywise ± 1
√c uniformly at random, for c = Θ(log n
δ ) (Johnson-Lindenstrauss
matrix [6]);
3 P ←L⊤(LL⊤)−1L (i.e. projection matrix onto the subspace spanned by L) where
L ∈RO(α−1)×d is the vertical concatenation of L;
4 e
Xj ←G⊤PXj for all j ∈[n], ˜µi ←G⊤ˆµi for all ˆµi ∈L;
5 eL ←
n
ˆµi ∈L
 |S(ˆµi)| ≥αn
2
o
, where S(ˆµi) :=
n
Xj ∈X

 e
Xj −˜µi

2 ≤2.5∆
o
;
6 Deﬁne an equivalence relation ∼on eL by ˆµi ∼ˆµj iﬀ∥˜µi −˜µj∥2 ≤20∆; if this is not an
equivalence relation, then return any labeling;
7 Return: Labeling of X where e
Xj and e
Xj′ have the same label iﬀe
Xj ∈S(ˆµi) and
e
Xj′ ∈S(ˆµi′) for ˆµi ∼ˆµi′;
We brieﬂy describe Algorithm 45. Line 3 forms the projection matrix onto the span of L. Line
5 "prunes" the set L to a set eL where we only keep candidates with enough data points close by
(in the subspace spanned by L). Line 6 then appropriately partitions the candidates, and Line 7
partitions the dataset based on the candidate partition. In the following proof we use Pˆµ = ˆµ for
all ˆµ ∈L, and that all µi have a point in the subspace projected to by P at most ∆away.
Lemma 129. With probability at least 1 −δ −k exp(−Ω(d)), if ∆= Ω(
√
α−1 log α−1), and every
pair i, i′ ∈[k], i ̸= i′ satisﬁes ∥µi −µi′∥2 > 20∆, Algorithm 45 returns a correct clustering of a
1 −o(1) fraction of all pointns drawn from some disti. The algorithm runs in time
O

n ·

d + 1
α

· log n
δ

.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 372
Proof. Throughout this proof, condition on the event that distances between PX ∪L are preserved
up to 1 ± 0.1 by multiplication through G⊤and that there are at most αn
3
adversarial points; we
will union bound this conditioning with an event of probability 1 −k exp(−Ω(d)).
We ﬁrst claim that any ˆµ ∈L satisfying ∥ˆµ −µi∥2 ≤∆for some i ∈[k] is contained in eL,
with probability at least 1 −exp(−Ω(d)): call this claim †.
To see †, with probability at least
1 −exp(−Ω(d)), a
9
10 fraction of Xj ∈X sampled from disti satisfy ∥P(Xj −µi)∥2 ≤∆. This
follows since Chebyshev's inequality, ∆= ω(
√
α−1), and P projecting onto a O(α−1)-dimensional
subspace imply the probability ∥P(Xj −µi)∥2 ≤∆is o(1). By triangle inequality and P ⪯I, for
such Xj,
∥P(Xj −ˆµ)∥2 ≤∥P(Xj −µi)∥2 + ∥P(ˆµ −µi)∥2 ≤∆+ ∥ˆµ −µi∥2 ≤2∆.
Since G⊤preserves distances to a 1.1 factor, this implies
 e
Xj −˜µ

2 ≤2.5∆as desired.
We next claim that any ˆµ ∈L satisfying ∥ˆµ −µi∥2 > 7∆for all i ∈[k] will not be contained in
eL, with probability at least 1 −exp(−Ω(d)). To see this, we claim S(ˆµ) can only contain an o(α)
fraction of the points drawn from disti; summing over all disti, and adding in all ≤αn
3 adversarial
points, shows |S(ˆµ)| < αn
2 . This latter claim follows by ﬁrst observing that for Xj ∈S(ˆµ),
∥P(Xj −µi)∥2 ≥∥µi −ˆµ∥2 −∥µi −Pµi∥2 −∥P(Xj −ˆµ)∥2
> 7∆−∆−3∆= 3∆≥∥P(Xj −ˆµ)∥2 .
The ﬁrst inequality used that Pˆµ = ˆµ, and the triangle inequality twice. In the second inequality,
we used ∥µi −Pµi∥2 ≤∆since the subspace projected to by P contains a point at most ∆away
from µi, and ∥P(Xj −ˆµ)∥2 ≤3∆by the assumption on G and Xj ∈S(ˆµ).
This implies that
the projection of Xj is closer to Pˆµ = ˆµ than Pµi. Letting dist′
i be the projection of disti by P,
X ∼dist′
i is closer to ˆµ than Pµi with probability at most
1
Ω(∆2) = o(α) by arguments in Lemma 126
and Chebyshev. Hence, the probability that Xj ∈S(ˆµ) is o(α), as desired.
Now, under the success of all above conditioning events, by the minimum separation assumption
of 20∆, each surviving ˆµ ∈eL has a unique µi such that ∥ˆµ −µi∥2 ≤7∆, and each designated ˆµi
with ∥ˆµi −µi∥2 ≤∆survives. Hence, the equivalence partition succeeds and captures all ˆµ ∈eL at
distance at most 7∆from a µi. Moreover, for every pair ˆµ, ˆµ′ ∈eL such that ∥ˆµ −µi∥2 ≤7∆and
∥ˆµ′ −µi′∥2 ≤7∆for some i ̸= i′,
X ∈S(ˆµ) ∩S(ˆµ′) =⇒∥ˆµ −ˆµ′∥2 ≤6∆.
Since the minimum separation between µi and µi′ is 20∆, this is a contradiction by the triangle
inequality. So, no sets S(ˆµ) and S(ˆµ′) intersect, where the hypotheses are close to diﬀerent means.
Thus the labeling in Line 7 is well-deﬁned. It remains to show that if Xj ∼disti, we will have
Xj ∈S(ˆµi) with probability 1 −o(1) (and hence a 1 −o(1) fraction of points is labeled correctly).

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 373
This was in fact shown earlier, when we proved the claim †.
Finally, it is clear that all operations can be performed in the desired runtime: G⊤P can be
computed explicitly in time O(d2α−1 + d2 log n
δ ) = O(nd + d2 log n
δ ) via na¨ıve matrix multiplica-
tion, and all projection and distance computations (multiplying all points and hypotheses by G⊤P,
computing all S(ˆµ), and checking all pairwise distances in Line 6) take time O(n·(d+ 1
α)·log n
δ ).
Corollary 36. Suppose every pair i, i′ ∈[k], i ̸= i′ satisﬁes ∥µi −µi′∥2 = Ω(
√
α−1 log α−1) for
an appropriate constant. There is an algorithm drawing n = Θ( d
α) samples from the mixture (1 −
ϵ) P
i∈[k] αi disti +ϵ distadv where disti has mean µi and covariance bounded by I, and returns a
correct clustering of a 1 −ϵ1 fraction of all points drawn from some disti (up to label permutation)
for any ﬁxed ϵ1 > 0, with probability at least
1 −δ −k exp(−Ω(d)).
The algorithm runs in time, for any ﬁxed ϵ0 > 0,
O

n1+ϵ0d log4 n log4 n
δ + 1
α2 log4 n
δ + n
α log n
δ

.
Proof. Since the goal is to correctly label a 1 −ϵ1 fraction of all points, we can use a ϵ1
2 fraction of
our dataset as holdout for learning an independent list L. We then use this list to cluster a 1 −ϵ1
2
fraction of the remaining points via Lemma 129.
8.6
Robust regression
Parameter estimation in generalized linear models, such as linear and logistic regression problems,
is among the most fundamental and well-studied statistical optimization problems. It serves as the
primary workhorse in statistical studies arising from a variety of disciplines, ranging from economics
[489], biology [524], and the social sciences [249]. Formally, given a link function γ : R2 →R and
a dataset of covariates and labels {(Xi, yi)}i∈[n] ⊂Rd × R drawn from an underlying distribution
DXy, the problem of statistical (generalized linear) regression asks to
estimate θ⋆:= argminθ∈Rd

E(X,y)∼DXy [γ(⟨θ, X⟩, y)]
	
.
(8.32)
For example, when γ(v, y) = 1
2(v −y)2, (8.32) corresponds to (statistical) linear regression. The
problem (8.32) also has an interpretation as computing a maximum likelihood estimate for a pa-
rameterized distributional model for data generation, and indeed is only tractable under certain
distributional assumptions, since we only have access to samples from DXy rather than the under-
lying distribution itself (see e.g. [53] for tractability results in the linear regression setting).

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 374
However, in modern settings, these strong distributional assumptions may fail to hold. In practi-
cally relevant settings, regression is often performed on massive datasets, where the data comes from
a poorly-understood distribution and has not been thoroughly vetted or cleaned of outliers. This
has prompted the study of robust regression, i.e. regression under weak distributional or corruption
assumptions. In this work, we study the problem of regression (8.32) in the strong contamination
model. In this model, we assume the data points we receive are independently drawn from DXy,
but that an arbitrary ϵ-fraction of the samples are then adversarially contaminated or replaced. The
strong contamination model has recently drawn interest in the algorithmic statistics and learning
communities for several reasons. Firstly, it is a ﬂexible model of corruption and can be used to
study both truly adversarial data poisoning attacks (where e.g. part of the dataset is sourced from
malicious respondents), as well as model misspeciﬁcation, where the generative DXy does not ex-
actly satisfy our distributional assumptions, but is close in total variation to a distribution that
does. Furthermore, a line of work building upon [175, 337] (discussed in our survey of prior work
in Section 8.6.2) has achieved remarkable positive results for mean estimation and related problems
under strong contamination, with statistical guarantees scaling independently of the dimension d.
This dimension-free error promise is important in modern high-dimensional settings.
8.6.1
Our results
We give multiple nearly-linear time algorithms8 for problem (8.32) under the strong contamination
model, with improved statistical or runtime guarantees compared to the state-of-the-art.
Prior
algorithms for (8.32) under the strong contamination model typically followed one of two frameworks.
The ﬁrst, which we refer to as robust gradient descent, was pioneered by [447], and is based on
reframing (8.32) as a problem where we have noisy gradient access to an unknown function we wish
to optimize, coupled with the design of a noisy gradient oracle based on a robust mean estimation
primitive. The second, which we refer to as Sever, originated in work of [176], and uses the guarantees
of stationary point ﬁnders such as stochastic gradient descent to repeatedly perform outlier removal.
In this work, we show that both approaches can be sped up dramatically, and give two complementary
types of algorithms within these frameworks.
Robust acceleration.
Our ﬁrst contribution is to demonstrate that within the noisy gradient
estimation framework for minimizing well-conditioned regression problems of the form (8.32), an
accelerated rate of optimization can be achieved, answering an open question asked by [447]. We
demonstrate the following result for smooth statistical regression problems, where we assume the
uncorrupted data is drawn from DXy with marginals DX and Dy, DX has support in Rd, and eO hides
polylogarithmic factors in problem parameters (cf. Section 8.3.1 for technical deﬁnitions). Finally,
we remove κ dependences in sample complexity statements, as they are subsumed by ϵ dependences
8Throughout, we reserve the description "nearly-linear" for runtimes scaling linearly in the dataset size nd, and
polynomially in ϵ−1 and the condition number, up to a polylogarithmic overhead in problem parameters.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 375
due to assumed bounds on ϵκ or ϵκ2.
Theorem 52 (informal, see Theorem 58). Suppose γ : R2 →R is such that γy(v) := γ(v, y) is
convex and has (absolute) ﬁrst and second derivatives at most 1 for all y in the support of Dy, and
DX has second moment matrix Σ⋆⪯L · I. For some µ ≥0, let κ = max(1, L
µ ) and let
θ⋆
reg := argminθ∈Rd
n
E(X,y)∼DXy {γ (⟨θ, X⟩, y)} + µ
2 ∥θ∥2
2
o
be the solution to the true regularized statistical regression problem.9 There is an algorithm that
given n := eO( d
ϵ ) ϵ-corrupted samples from DXy, for ϵκ2 at most an absolute constant, runs in time
eO(nd√κ) and obtains θ with
θ −θ⋆
reg

2 = O
q
κϵ
µ

with probability at least 1 −δ.
A canonical example of a link function γ satisfying the assumptions of Theorem 52 is the logit
function γ(v, y) = log(1 + exp(−vy)), when the labels y are ±1. To contextualize Theorem 52, the
earlier work [447] obtains a similar statistical guarantee in its setting, using eO(κ) calls to a noisy
gradient oracle, which they implement via a subroutine inspired by works on robust mean estimation.
At the time of its initial dissemination, nearly-linear time robust mean estimation algorithms were
not known; since then, [134] showed that for the case of linear regression (see Theorem 54 for the
formal setup, as the linear regression link function is not Lipschitz), the framework was amenable to
mean estimation techniques of [131], and gave an algorithm running in time eO(ndκϵ−6). Theorem 52
represents an improvement to these results on two fronts: we apply tools from the mean-estimation
algorithm of [194] to remove the poly(ϵ−1) runtime dependence for a general class of regression
problems, and we achieve an iteration count of eO(√κ), matching the accelerated gradient descent
runtime of [417] for smooth optimization in the non-robust setting. We remark that the application
of tools inspired by [194] is fairly straightforward, and not a primary contribution of our work
compared to the accelerated dependence on κ.
We demonstrate the generality of our acceleration framework by demonstrating that it applies to
optimizing the Moreau envelope for Lipschitz, but possibly non-smooth, link functions γ; a canonical
example of such a function is the hinge loss γ(v, y) = max(0, 1−vy) with ±1 labels, used in training
support vector machines. The Moreau envelope is a well-studied smooth approximation to a non-
smooth function which everywhere additively approximates the original function if it is Lipschitz (see
e.g. [485]), and in the non-robust setting many state-of-the-art rates for Lipschitz optimization are
known to be attained by accelerated optimization of an appropriate Moreau envelope [502]. We show
that even without explicit access to the Moreau envelope, we can obtain approximate minimizers to
it through our robust acceleration framework.
9To simplify our bounds and avoid estimation error for non-strongly convex statistical regression problems scaling
with the initial search radius (which may be dimension-dependent), we focus on regularized regression problems.
There is a substantial line of work on reductions between rates for strongly convex and convex smooth optimization
in the non-robust setting, see e.g. [551], and we defer an analogous exploration in the robust setting to future work.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 376
Theorem 53 (informal, see Theorem 59). Suppose γ : R2 →R is such that γy(v) := γ(v, y) is
convex and has (absolute) ﬁrst derivative at most 1 for all y in the support of Dy, and DX has
bounded second moment matrix. For some µ, λ ≥0, let κ = max(1,
1
λµ) and let
θ⋆
env := argminθ∈Rd
n
F ⋆
λ(θ) + µ
2 ∥θ∥2
2
o
, where F ⋆
λ(θ) := inf
θ′

F ⋆(θ′) + 1
2λ ∥θ −θ′∥2
2

is the Moreau envelope of F ⋆(θ) := E(X,y)∼DXy {γ (⟨θ, X⟩, y)} .
There is an algorithm that given n := eO( d
ϵ ) ϵ-corrupted samples from DXy, for ϵκ2 at most an
absolute constant, runs in time eO( nd√κ
ϵ
) and obtains θ with
θ −θ⋆
reg

2 = O
q
κϵ
µ

with probability
at least 1 −δ.
To obtain this result, we give a nearly-linear time construction of a noisy gradient oracle for the
Moreau envelope, which may be of independent interest; we note similar gradient oracle constructions
in diﬀerent settings have been developed in the optimization literature (see e.g. [110]).
Robust linear regression. The speciﬁc problem of robust linear regression is perhaps the most
ubiquitous example of statistical regression [321, 315, 188, 550, 134, 53]. Amongst the algorithms
developed for this problem, the only nearly-linear time algorithm is the recent work of [134]. For
an instance of robust linear regression with noise variance bounded by σ2 and covariate second
moment matrix Σ⋆:= EX∼DX[XX⊤], the algorithms of [176, 447, 134] attain distance to the true
regression minimizer θ⋆scaling as σκ√ϵ in the Σ⋆norm (the "Mahalanobis distance") under a
bounded 4th moment assumption. We measure error in the Σ⋆norm as it is scale invariant and
the natural norm in which to measure the underlying (quadratic) statistical regression error.10 We
give one result (Theorem 54) which improves the runtime of [176, 447, 134] under the noisy gradient
descent framework, and one result (Theorem 55) which improves its estimation rate, under the Sever
framework.
We ﬁrst demonstrate that directly applying our robust acceleration framework leads to a similar
estimation guarantee as [176, 447, 134] under the same assumptions.
Theorem 54 (informal, see Theorem 57). Suppose DX is a 2-to-4 hypercontractive distribution with
second moment matrix Σ⋆= EX∼DX[XX⊤] satisfying µ · I ⪯Σ⋆⪯L · I, and y ∼Dy is generated
as ⟨θ⋆, X⟩+ δ, for δ ∼Dδ with variance at most σ2 independent of X. Let κ := L
µ . There is an
algorithm, RobustAccel, that given n := eO( d
ϵ ) ϵ-corrupted samples from DXy, for ϵκ2 at most an
absolute constant, runs in time eO(nd√κ) and obtains θ with ∥θ −θ⋆∥Σ⋆= O(σκ√ϵ) with probability
1 −δ.
We give a formal deﬁnition of 2-to-4 hypercontractivity in Section 8.3.1; as a lower bound of [53]
shows, attaining estimation rates for robust linear regression scaling polynomially in ϵ is impossible
under only bounded second moments, and such a 4th moment bound is the minimal assumption under
10Some prior works gave ℓ2 norm guarantees, which we have translated for comparison.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 377
which robust estimation is known to be possible. Theorem 57 matches the distribution assumptions
and error of [134], while obtaining an accelerated runtime.
Interestingly, under the 4th moment bound used in Theorem 57, [53] showed that the information-
theoretically optimal rate of estimation in the Σ⋆norm is independent of κ, and presented a matching
upper bound under an analogous, but more stringent, distributional assumption.11 We remark that
thus far robust linear regression algorithms have broadly fallen under two categories: The ﬁrst
category (e.g. [321, 550, 53]), based on the sum-of-squares paradigm for algorithm design, sacriﬁces
practicality to obtain improved error rates by paying a large runtime and sample complexity overhead
(as well as requiring stronger distributional assumptions). The second (e.g. [176, 447, 134]), which
opts for more practical approaches to algorithm design, has been bottlenecked at Mahalanobis
distance O(σκ√ϵ) and the requirement that ϵκ2 = O(1).
We present a nearly-linear time method for robust linear regression overcoming this bottleneck
for the ﬁrst time amongst non-sum-of-squares algorithms, and attaining improved statistical perfor-
mance compared to Theorem 54 while only requiring ϵκ = O(1).
Theorem 55 (informal, see Theorem 56). Suppose DX is a 2-to-4 hypercontractive distribution
with second moment matrix Σ⋆= EX∼DX[XX⊤] satisfying µ · I ⪯Σ⋆⪯L · I, and y ∼Dy is
generated as ⟨θ⋆, X⟩+ δ, for δ ∼Dδ, a 2-to-4 hypercontractive distribution with variance at most σ2
independent of X. Let κ := L
µ . There is an algorithm, FastRegression, that given n := eO(( d2
ϵ3 + d
ϵ4 )) ϵ-
corrupted samples from DXy, for ϵκ at most an absolute constant, uses eO( 1
ϵ ) calls to an empirical risk
minimization routine12 and eO( nd
ϵ ) additional runtime, and obtains θ with ∥θ −θ⋆∥Σ⋆= O(σ√κϵ)
with probability at least
9
10.
This second algorithm does require more resources than that of Theorem 54: the sample com-
plexity scales quadratically in d, and the runtime is never faster. Further, we make the slightly
stronger assumption of hypercontractive noise for the uncorrupted samples. On the other hand, the
improved dependence on the condition number in the error can be signiﬁcant for distributions in
practice, which may be far from isotropic. All told, Theorem 55 presents an intermediate tradeoﬀ
inheriting some statistical gains of the sum-of-squares approach (albeit still depending on κ) with-
out sacriﬁcing a nearly-linear runtime. Interestingly, we obtain Theorem 55 by reinterpreting an
identiﬁability proof used in the algorithm of [53], and combining it with tools inspired by the Sever
framework. Our sample complexity for Theorem 55 dramatically improves that of [176]'s original
linear regression algorithm in the Sever framework for moderate ϵ, which used eO( d5
ϵ2 ) samples (in
addition to beating their weaker error guarantee). We elaborate on these points further in Sec-
tion 8.2.2; we believe it is an interesting open problem to understand if the worse sample complexity
11The algorithm of [53] requires DX to be certiﬁably hypercontractive, an algebraic condition frequently required
by the sum-of-squares algorithmic paradigm to apply to robust statistical estimation problems.
12The empirical risk minimization algorithm used is up to the practitioner; its runtime will never scale worse than
e
O(nd√κ) by applying (non-robust) accelerated gradient descent, but can be substantially better if recent advances in
stochastic gradient methods are used, e.g. [18].

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 378
of Theorem 55 is truly necessary for the improved statistical guarantees.
Table 8.1: Robust linear regression results in the κ ≫1 regime. All listed results assume 2-to-4
hypercontractivity and independent noise (although some also give rates for non-independent noise).
Error guarantees are in Mahalanobis distance. We omit polylogarithmic factors for simplicity.
Reference
Runtime
Error guarantee
Range of ϵ
Comments
[321]
poly(d)
σϵ
1
4
N/A
Certiﬁable hypercontractivity
[550]
poly(d)
σϵ
1
2
N/A
Certiﬁable hypercontractivity
[53]
poly(d)
σϵ
3
4
N/A
Certiﬁable hypercontractivity
[176]
poly(d)
σκϵ
1
2
O(κ−2)
Linear sample size
[134]
ndκ · poly(ϵ−1)
σκϵ
1
2
O(κ−2)
Linear sample size
Theorem 54
nd√κ
σκϵ
1
2
O(κ−2)
Linear sample size
Theorem 55
nd · poly(ϵ−1)
σ(κϵ)
1
2
O(κ−1)
Quadratic sample size
8.6.2
Prior work
We give a general overview contextualizing our work in this section, and defer the comparison of
speciﬁc technical components we develop in this work to relevant discussions.
The study of learning in the presence of adversarial noise is known as robust statistics, with a long
history dating back over 60 years [31, 512, 278, 513, 279]. Despite this, the ﬁrst eﬃcient algorithms
with near-optimal error for many fundamental high dimensional robust statistics problems were only
recently developed [175, 337, 177]. Since these works, eﬃcient robust estimators have been developed
for a variety of more complex problems; a full survey of this ﬁeld is beyond our scope, and we defer
a more comprehensive overview to [182, 360, 492].
Our results sit within the line of work in this ﬁeld on robust stochastic optimization. The ﬁrst
works which achieved dimension-independent error rates with eﬃcient algorithms for the problems
we consider are the aforementioned works of [447, 176]. Similar problems were previously considered
in [118, 54]. In [118], the authors consider a setting where a majority of the data is corrupted, and the
goal is to output a short list of hypotheses so that at least one is close to the true regressor. However,
because most of their data is corrupted, they achieve weaker statistical rates; in particular, their
techniques do not achieve vanishing error as the fraction of error goes to zero. In [54], the authors
consider a somewhat diﬀerent model with stronger assumptions on the structure of the functions. In
particular, they assume that the uncorrupted covariates are Gaussian, and are primarily concerned
with the case where the regressors are sparse.
Their main goal is to achieve sublinear sample
complexities by leveraging sparsity. We also remark that the algorithms in [118, 54] are also much
more cumbersome, requiring heavy-duty machinery such as black-box SDP solvers and cutting plane
methods, and as a result are more computationally intense than those considered in [447, 177].

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 379
There has been a large body of subsequent work on the special case of robust linear regres-
sion [321, 315, 188, 550, 134, 53]; however, the majority of this line of work focuses on achieving
improved error rates under additional distributional assumptions by using the sum-of-squares hier-
achy. As a result, their algorithms are likely impractical in high dimensions, and require large (albeit
polynomial) sample complexity and runtime. Of particular interest to us is [134], who combine the
framework of [447] with the robust mean estimation algorithm of [131] to achieve nearly-linear run-
times in the problem dimension and the number of samples. Our Theorem 54 can be thought of
as the natural accelerated version of [134], with an additional ϵ−6 runtime overhead removed using
more sophisticated mean estimation techniques.
8.6.3
Techniques
We now describe the techniques we use to obtain the accelerated rates of Theorems 52, 53, and 54
as well as the robust linear regression algorithm of Theorem 55.
Robust acceleration. Our robust acceleration framework is based on the following abstract for-
mulation of an optimization problem: there is an unknown function F ⋆: Rd →R with minimizer
θ⋆which is L-smooth and µ-strongly convex, and we wish to estimate θ⋆, but our only mode of
accessing F ⋆is through a noisy gradient oracle Ong. Namely, for some σ, ϵ, we can query Ong at
any point θ ∈Rd with an upper bound R ≥∥θ −θ⋆∥2 and receive an estimate G(θ) such that
∥G(θ) −∇F ⋆(θ)∥2 = O
√
Lϵσ + L√ϵR

.
(8.33)
In other words, we receive gradients perturbed by both ﬁxed additive noise, and multiplicative noise
depending on the distance to θ⋆. The prior works [176, 447, 134] observed that by using tools from
robust mean estimation, appropriate noisy gradient oracles could be constructed for the functions
F ⋆(θ) = E(X,y)∼DXy[σ(⟨θ, X⟩−y)] arising from the distributional assumptions in Theorems 52, 53,
and 54. Our ﬁrst contribution is speeding up the implementation of Ong to run in nearly-linear time
eO(nd), leveraging recent advances by [194] for robust mean estimation.
Our second, and more technically involved, contribution is demonstrating that accelerated run-
times are achievable under the noisy gradient oracle access model of (8.33). Designing accelerated
algorithms under noisy gradient access is an extremely well-studied problem, and there are both
strong positive results [162, 400, 209, 142, 395, 102] as well as negative results [172] showing that
under certain noise models, accelerated gradient descent may be outperformed by unaccelerated
methods. Indeed, it was asked (motivated by these negative results) as an open question in [447]
whether an accelerated rate was possible under the noise model (8.33).
Our accelerated algorithm runs in logarithmically many phases, where we halve the distance to
the optimizer (while it is above a certain noise ﬂoor depending on the additive error in (8.33)) in
each phase. The subroutine we design for implementing each phase is a robust accelerated "outer

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 380
loop" tolerant to noisy gradient access in the manner provided by our oracle Ong. By carefully
balancing the accuracy of subproblem solutions, the multiplicative error in our gradient estimates
within the accelerated outer loop, and the drift of the phase's iterates (which may venture further
from θ⋆than our initial iterate upper bound), we show that above the noise ﬂoor we can halve the
distance to θ⋆in eO(√κ) queries to Ong; recursing on this guarantee yields our complete algorithm.
To obtain Theorem 53, we demonstrate that for Lipschitz functions F ⋆admitting a radiusless
noisy gradient oracle, i.e. one which satisﬁes (8.33) with no dependence on R, we can further eﬃ-
ciently construct a noisy gradient oracle for the Moreau envelope of F ⋆using projected subgradient
descent. This construction enables applying our robust accelerated method to Lipschitz regression
problems.
Our acceleration framework crucially tolerates both additive and multiplicative guarantees for
gradient estimation. While it is possible that arguments of other noisy acceleration frameworks
e.g. [142] may be extended to capture our gradient noise model, we give a self-contained derivation
specialized to our speciﬁc oracle access for convenience. We view our result as a proof-of-concept
that acceleration is possible under this noise model; we believe a uniﬁed study of acceleration under
noise models encompassing (8.33) warrants further exploration, and defer it to interesting future
work.
Robust linear regression. For the special case of linear regression, as discussed earlier, the robust
optimization methods of [176, 447, 134] attain Mahalanobis distance scaling as σκ√ϵ from the true
minimizer. Directly plugging in deterministic conditions proven by [134] to hold under an appro-
priate statistical model into our robust gradient descent framework, we obtain a similar guarantee
(Theorem 54) at an accelerated rate. In this technical overview, we now focus on how we obtain the
improvements of Theorem 55.
At a high level, prior works lose two factors of √κ in their error guarantees because of two norm
conversions from the Σ⋆norm to the ℓ2 norm: one in gradient space, and one in parameter space.
Because we do not have access to the true covariance Σ⋆, it is natural to perform both gradient
estimation and the gradient descent procedure itself in the ℓ2 norm. When the ℓ2 guarantees of both
subroutines are converted back to the Σ⋆norm, the error rate is lossy by a factor of κ.
We give a diﬀerent approach to robust linear regression which bypasses this barrier in parameter
space, saving a factor of √κ in our error rate. In particular, we measure progress of our parameter
estimates entirely in the Σ⋆norm in our analysis, which removes the need for an additional norm
conversion.
Our starting point is the following identiﬁability proof guarantee of [53], which we
slightly repurpose for our needs. Let w ∈∆n be entrywise less than 1
n 1,1 such that ∥wG∥1 ≥1 −4ϵ
where G is our "uncorrupted" data, and let θ ∈Rd. We demonstrate in Proposition 37 that
∥θ −θ⋆∥Σ⋆= O

σ√κϵ +
rCovw

{gi(θ)}i∈[n]

op
ϵ
µ + √ϵ ∥∇Fw(θ)∥(Σ⋆)−1

.
(8.34)

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 381
In the above display, gi(θ) is the empirical gradient of the squared loss at our ith data point,
Covw(·) is the empirical second moment matrix of its argument under the weighting w, and Fw is
the empirical risk under w. The guarantee (8.34) suggests a natural approach to estimation: if we
can simultaneously verify that θ is an approximate minimizer to Fw, and that the empirical second
moment of gradients at θ (according to w) are small, then we have a proof that θ and θ⋆are close.
Prior work by [53] used this approach to obtain a polynomial-time estimator by solving a joint
optimization problem in (w, θ), via an appropriate semideﬁnite program relaxation. However, in
designing near-linear time algorithms, we cannot aﬀord to use said relaxation. This raises a chicken-
and-egg issue: for ﬁxed w, it is straightforward to make ∥∇Fw(θ)∥(Σ⋆)−1 small, by setting θ to the
empirical risk minimizer (ERM) of Fw. Likewise, for ﬁxed θ, known ﬁltering techniques rapidly
decrease ∥Covw({gi(θ)})i∈[n]∥op while preserving most of wG, by using that the second moment
restricted to uncorrupted points has a small operator norm as a certiﬁcate for outlier removal.
However, performing either of these subroutines to guarantee one of our suﬃcient conditions passes
(small operator norm or gradient norm) may adversely aﬀect the quality of the other.
We circumvent this chicken-and-egg problem by introducing a third potential, namely the actual
function value Fw(θ). In particular, notice that the two subroutines we described earlier (down-
weighting w or setting θ to the ERM) both decrease this third potential. Our linear regression
algorithm is an alternating procedure which iteratively ﬁlters w based on the gradients at the cur-
rent θ (to make the operator norm small), and sets θ to the ERM of Fw (to zero out the gradient
norm). We further show that if the ERM step does not make signiﬁcant function progress (our third
potential), then it was not strictly necessary to make progress according to (8.34), since the gradient
norm was already small. This gives a dimension-independent bound on the number of times we
could have alternated, via tracking function progress, yielding Theorem 55.
8.7
Preliminaries: robust regression
We give the notation used throughout the sections dedicated to robust regression in Section 8.7.1, and
set up the statistical model we consider in Section 8.7.2. In Section 8.7.3, we give the deterministic
regularity assumptions used by our regression algorithm in Section 8.8. In Section 8.7.4, we give the
deterministic regularity assumptions used by our stochastic optimization algorithms in Sections 8.9
and 8.10. Finally, in Section 8.7.5, we state a nearly-linear time procedure for robustly decreasing
the operator norm of the second moment matrix of a set of vectors. Some proofs are deferred to the
appendices.
8.7.1
Notation
General notation.
For d ∈N we let [d] := {j | j ∈N, 1 ≤j ≤d}.
The ℓp norm of a vector
argument is denoted ∥·∥p, where ∥·∥∞is the element with largest absolute value; when the argument

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 382
is a symmetric matrix, we overload this to mean the Schatten-p norm.
The all-ones vector (of
appropriate dimension from context) is denoted 1,1.
The (solid) probability simplex is denoted
∆n := {w ∈Rn
≥0, ∥w∥1 ≤1}. We use eO to suppress logarithmic factors in dimensions, distance
ratios, the problem condition number κ, the inverse corruption parameter ϵ−1, and the inverse
failure probability. For v ∈Rn and S ⊆[n], we let vS ∈Rn denote v with coordinates in [n] \ S
zeroed out. For a set S, we call S1, S2 a bipartition of S if S1 ∩S2 = ∅and S1 ∪S2 = S.
Matrices.
Matrices are denoted in boldface.
We denote the zero and identity matrices (of
appropriate dimension) by 0 and I. The d × d symmetric matrices are Sd, and the d × d positive
semideﬁnite cone is Sd
≥0. For A, B ∈Sd we write A ⪯B to mean B −A ∈Sd
≥0. The largest
and smallest eigenvalue and trace of a symmetric matrix are respectively denoted λmax(·), λmin(·),
and Tr(·). The inner product on Sd is ⟨A, B⟩:= Tr(AB). For positive deﬁnite M, we deﬁne the
induced norm ∥v∥M :=
√
v⊤Mv. We use ∥·∥op to mean the ℓ2-ℓ2 operator norm of a matrix; when
the argument is symmetric, it is synonymous with λmax, and otherwise is the largest singular value.
Functions. The gradient and Hessian of a twice-diﬀerentiable function are denoted ∇and ∇2.
We say diﬀerentiable f : Rd →R is λ-Lipschitz in a quadratic norm ∥·∥M if ∥∇f(θ)∥M−1 ≤λ for all
θ ∈Rd. We say twice-diﬀerentiable f : Rd →R is L-smooth and µ-strongly convex in ∥·∥M if
µM ⪯∇2f(θ) ⪯LM, for all θ ∈Rd.
When M is not speciﬁed, we assume M = I (i.e. the norm in question is ℓ2). For any M, smoothness
and strong convexity imply the following bounds for all θ, θ′ ∈Rd,
f(θ) + ⟨∇f(θ), θ′ −θ⟩+ µ
2 ∥θ′ −θ∥2
M ≤f(θ′) ≤f(θ) + ⟨∇f(θ), θ′ −θ⟩+ L
2 ∥θ′ −θ∥2
M .
It is well-known that L-smoothness of function f implies L-Lipschitzness of the function gradient
∇f, i.e. ∥∇f(θ) −∇f(θ′)∥M−1 ≤L ∥θ −θ′∥M for all θ, θ′ ∈Rd. For any f which is L-smooth and
µ-strongly convex in ∥·∥M, with θ∗:= argminθ∈Rdf(θ), it is straightforward to show
1
2L ∥∇f(θ)∥2
M−1 ≤f(θ) −f(θ⋆) ≤1
2µ ∥∇f(θ)∥2
M−1 for all θ ∈Rd.
Distributions. The multivariate Gaussian distribution with mean µ and covariance Σ is denoted
N(µ, Σ). For weights w ∈Rn
≥0 and a set of vectors X := {Xi}i∈[n], we let
µw (X) :=
X
i∈[n]
wi
∥w∥1
Xi, Covw, ¯
X (X) :=
X
i∈[n]
wi
∥w∥1
 Xi −¯X
  Xi −¯X
⊤.
be the empirical mean and (centered) covariance matrix; when ¯X is not speciﬁed, it is the zeroes vec-
tor. Draws from the uniform distribution on {Xi}i∈[n] are denoted X ∼unif X. We say distribution

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 383
D supported on Rd is 2-to-4 hypercontractive with parameter C2→4 if for all v ∈Rd,
EX∼D
h
⟨X, v⟩4i
≤C2→4EX∼D
h
⟨X, v⟩2i2
.
We will refer to this property as being C2→4-hypercontractive for short; by massaging the deﬁnition,
we observe C2→4-hypercontractivity is preserved under linear transformations of the distribution.
Filtering.
We will make much use of the following algorithmic technique, which refer to as
ﬁltering.
In the ﬁltering paradigm, we have an index set [n], and a ﬁxed, unknown bipartition
[n] = G ∪B, G ∩B = ∅. The set G is a "good" set of indices that we wish to keep, and the set B
is a set of "bad" indices which we would like to remove. The algorithm maintains a set of weights
w ∈∆n (with the goal of producing a weight vector which is close to the uniform distribution on G).
These weights are iteratively updated according to "scores" τ ∈Rn
≥0; the goal of ﬁltering is to assign
large scores to coordinates in B and small scores to coordinates in G, so that the bad coordinates
can be ﬁltered out according to their scores. Concretely, we use the following deﬁnition.
Deﬁnition 30 (saturated weights). We say weights w ∈∆n are c-saturated with respect to the
bipartition G ∪B = [n] if w ≤1
n 1,1 entrywise, and

 1
n 1,1 −w

G

1
≤

 1
n 1,1 −w

B

1
+ c.
If c = 0, we refer to w as simply saturated.
In words, w is saturated if its diﬀerence from the uniform distribution has more weight on B
than G (in the context of our algorithm, if we have started with uniform weights and produced a
saturated w, then we have removed more mass from B than G). We allow for a "fudge factor"
of an additive c to relax the above deﬁnition, which will come in handy in our linear regression
applications.
Deﬁnition 31 (safe scores). Suppose G ∪B is a bipartition of [n], and suppose w ∈∆n is a set of
weights. We call a set of scores τ = {τi}i∈[n] ∈Rn
≥0 safe with respect to w if it satisﬁes
⟨wG, τ⟩≤⟨wB, τ⟩.
The following simple lemma (implicit in prior works [177, 118, 360, 492]) is the crux of the
ﬁltering paradigm, relating these two deﬁnitions.
Lemma 130. Suppose w ∈∆n is saturated, and τ ∈Rn
≥0 is safe with respect to w. Deﬁning w′ by
w′
i ←

1 −
τi
τmax

wi for all i ∈[n], and τmax :=
max
i∈[n]|wi̸=0 τi,
then w′ ∈∆n is also saturated.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 384
Proof. If G ∪B = [n] is the bipartition with good coordinates G, then by deﬁnition of safe scores,
∥[w −w′]G∥1 =
1
τmax
⟨wG, τ⟩≤
1
τmax
⟨wG, τ⟩≤∥[w −w′]B∥1 .
Now, since w is saturated, and since w′ ≤w ≤1
n 1,1 by deﬁnition of saturation,

h
1
n 1,1 −w′i
G

1 =

h
1
n 1,1 −w
i
G

1+∥[w −w′]G∥1 ≤

h
1
n 1,1 −w
i
B

1+∥[w −w′]B∥1 =

h
1
n 1,1 −w′i
G

1 .
We will also frequently using the following simple fact.
Lemma 131. Suppose w ∈∆n is c-saturated with respect to bipartition [n] = G ∪B, and suppose
the bad set |B| ≤ϵn. Let ˜w =
w
∥w∥1 be the distribution with probabilities proportional to w and
w⋆
G =
1
|G|1G be uniform over G. Then, ∥˜w −w⋆
G∥1 ≤6ϵ + 2c.
Proof. By the deﬁnition of saturation, since there is only ϵ mass to remove from the coordinates of
B on 1
n 1,1, clearly ∥w∥1 ≥1 −2ϵ −c. By the triangle inequality, we have
∥˜w −w⋆
G∥1 ≤∥˜w −w∥1 +
w −1
n 1,1

1 +
 1
n 1,1 −w⋆
G

1 .
By deﬁnition of ˜w, ∥˜w −w∥1 = 1 −∥w∥1 ≤2ϵ + c. By saturation,
w −1
n 1,1

1 ≤2ϵ + c. Finally,
since |G| ≥(1 −ϵ)n,
 1
n 1,1 −w⋆
G

1 ≤2ϵ. Combining these pieces yields the claim.
8.7.2
Our statistical models
We provide provable guarantees for optimization problems captured by the following statistical
model.
Model 1 (stochastic optimization in the strong contamination model). For Df a distribution over
functions f : Rd →R, our goal is to optimize Ef∼Df [f(θ)]. We are given access to n samples
{fi}i∈[n] produced as follows:
1. Functions { ˜fi}i∈[n] are drawn independently from Df.
2. An arbitrary subset B ⊂[n] of the samples is replaced with arbitrary functions from supp(Df).
3. For each i ∈[n], if i ∈B we observe fi as the corrupted sample, otherwise, we observe fi = ˜fi.
We call B the corrupted samples. When |B| = ϵn, we say {fi}i∈[n] is drawn ϵ-corrupted from Df.
Throughout we use the convention that G∪B = [n] is the bipartition of sample coordinates with
B the corrupted samples and G the "good" samples. For simplicity, we assume throughout that

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 385
ϵ = |B|
n is smaller than some globally ﬁxed constant. We will also frequently use the notation gi to
mean ∇fi for all i ∈[n]. When Df is clear from context, we denote the "true average function" by
F ⋆(θ) := Ef∼Df [f(θ)] .
For w ∈∆n and functions {fi}i∈[n], the (unnormalized) weighted empirical average function is
Fw(θ) :=
X
i∈[n]
wifi(θ).
(8.35)
We will use w⋆
G to denote the uniform distribution over G, w⋆
G :=
1
|G|1G, and we use FG as shorthand
for the function Fw⋆
G. The goal of robust parameter estimation is to estimate the true optimizer,
which we always denote by θ⋆:= argminθ∈RdF ⋆(θ). For example, the problem estimating the mean
of D can be expressed by choosing fi(θ) = 1
2 ∥θ −Xi∥2
2 for Xi ∼D. In the uncorrupted setting (i.e.
ϵ = 0), a typical strategy (given reasonable regularity assumptions on Df) is to choose the estimator
θG := argminθ∈RdFG(θ). The challenge is to obtain comparable estimation performance to θG which
is robust to an ϵ-fraction of unknown corruptions.
Our focus is optimizing generalized linear models. In particular, throughout we will work only
with Df of the following form.
Model 2 (generalized linear model). A generalized linear model is a distribution over functions
fi : Rd →R which is deﬁned by a joint distribution DXy over pairs {Xi, yi} ∈Rd × R and a link
function γ : R2 →R, so that samples fi ∼Df are generated as
fi(θ) := γ (⟨Xi, θ⟩, yi) ,
for (Xi, yi) ∼DXy.
(8.36)
Note that observing {fi}i∈[n] is equivalent to observing the dataset {Xi, yi}i∈[n] when γ is known.
For instance, when γ(v, y) =
1
2(v −y)2, this is the problem of (statistical) linear regression.
Further, when γ(v, y) = log(1 + exp(−vy)), our problem is logistic regression, and when γ(v, y) =
max(0, 1 −vy), it is ﬁtting a support vector machine. We refer to the X and y marginals over DXy
respectively by DX and Dy, and we denote Σ⋆:= EX∼DX

XX⊤
when DX is clear from context.
8.7.3
Linear regression
In Section 8.8 and (part of) Section 8.9, we develop algorithms for the well-studied special case of
the generalized linear model, Model 2 wherein γ(v, y) = 1
2(v −y)2, i.e. a statistical variant of linear
regression. We obtain guarantees under the following model and regularity assumptions for DXy.
Model 3 (distributional regularity for linear regression). Given distributions DX and Dδ over
Rd, Rd respectively, the distribution DXy over Rd ×R is sampled as follows: for an underlying vector

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 386
θ⋆∈Rd, independently sample X ∼DX and δ ∼Dδ, and set y ←⟨θ⋆, X⟩+ δ. Further, DX and Dδ
satisfy the following regularity assumptions.
1. For Σ⋆= EDX[XX⊤] and 0 < µ < L, we have µI ⪯Σ⋆⪯LI.
2. DX is C2→4-hypercontractive for a constant C2→4.
3. Dδ is a C2→4-hypercontractive distribution with mean zero and variance ≤σ2.
For {(Xi, yi)}i∈[n] (in particular, overloading to include i ∈B), we use the notation δi :=
yi −⟨Xi, θ⋆⟩. We also use the following notation when discussing linear regression:
fi(θ) := 1
2 (⟨Xi, θ⟩−yi)2 , gi(θ) := ∇fi(θ) = Xi (⟨Xi, θ⟩−yi) .
(8.37)
We will denote the condition number of Σ⋆by κ := L
µ throughout. Under Model 3, it is immediate
from the ﬁrst-order optimality condition that for
F ⋆(θ) := EX,y∼DXy
1
2 (⟨X, θ⟩−y)
2
,
the optimizer argminθ∈RdF ⋆(θ) is exactly θ⋆.
In our setting, following the description in Section 8.7.2 we independently draw {(Xi, yi)}i∈G ∼
DXy for |G| = (1−ϵ)n and observe {(Xi, yi)}i∈[n] where [n] = G∪B and {(Xi, yi)}i∈B are arbitrarily
chosen. We will frequently refer to {Xi}i∈[n] as X ∈Rn×d. Under Model 3, recent work [53] obtained
the following results.
Proposition 32 ([53], Theorem 1.7, Theorem 1.9, Theorem 1.2). For Models 1 and 3, the minimax
optimal error rate for estimators ˆθ is
ˆθ −θ⋆
Σ⋆= O

σϵ
3
4

.
When the distribution DX is further certiﬁably hypercontractive in the sum-of-squares proof system,
there is a poly(d)-time estimator requiring poly(d) samples achieving this rate with high probabil-
ity. Moreover, without the hypercontractivity condition in Model 3, even when µ, L = Θ(1) it is
information-theoretically impossible to attain an error rate depending polynomially on ϵ.
The algorithmic result of [53] (and all known techniques with error rate ≪√ϵ) crucially requires
that the distributions are sum-of-squares certiﬁably hypercontractive, which is a stronger assump-
tion than (standard) hypercontractivity. There is evidence that the problem of certifying 2 →4
hypercontractivity is computationally intractable in general (under e.g. the small-set expansion hy-
pothesis, see [58, 78]). Even for certiﬁably hypercontractive distributions, known algorithms require
use spectral estimators of higher-order moment matrices, and thus more samples and increased
runtime complexity. Hence, error √ϵ is a standing barrier for fast algorithms.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 387
In Section 8.8, whenever we discuss robust linear regression we operate in Models 1 and 3. These
assumptions imply that the data {Xi, yi}i∈[n] will satisfy the following deterministic conditions with
probability
9
10. For convenience we work with these deterministic conditions directly in our proofs.
Assumption 7 (deterministic regularity for linear regression). Let ϵ be suﬃciently small, and let
r ∈(0, ϵ2). Assume {(Xi, yi)}i∈[n] ⊂Rd × R is (ϵ, r)-good for linear regression (or (ϵ, r)-good if
context is clear), which means there is a partition [n] = G ∪B with |G| ≥(1 −ϵ)n which satisﬁes:
1. For any w ∈∆n with ∥wG∥1 ≥1 −4ϵ, 1
2Σ⋆⪯CovwG(X) ⪯3
2Σ⋆.
2. There is a constant Cest such that for all θ ∈Rd, there exists a G′ ⊆G satisfying |G′| ≥
(1 −r)|G| such that for all ϵ-saturated w ∈∆n, if we let ˜w :=
wG′
∥wG′∥1 ,
∥∇F ˜
w(θ) −∇F ⋆(θ)∥2 ≤Cest
√
Lϵ (σ + ∥θ −θ⋆∥Σ⋆) ,
(8.38)
Cov ˜
w
 {gi(θ)}i∈G′

op ≤CestL

∥θ −θ⋆∥2
Σ⋆+ σ2
.
(8.39)
3. There is a constant Cub such that
X
i∈G
1
|G|fi(θ⋆) =
1
2|G|
X
i∈G
(⟨Xi, θ⋆⟩−yi)2 ≤Cubσ2.
We defer the proof of the following claim, which establishes the probabilistic validity of As-
sumption 7 (up to adjusting constants in deﬁnitions) under the statistical Models 1 and 3, to
Appendix G.6.
Proposition 33. Let α ≥1 and let ϵ > 0 be suﬃciently small. Let {(Xi, yi)}i∈[n] ⊂Rd × R be an
ϵ-corrupted set of samples from a distribution DXy as in Model 3. Then, if
n = O
dα2 log d
ϵ4
+ d2α1.5 log(d/ϵ)
ϵ3

,
the set {(Xi, yi)}i∈[n] is (2ϵ, ϵ2
α )-good for linear regression with probability at least
9
10.
Remark 9. We remark that the gaurantees of Proposition 33 may be recovered with n = O(d log d/ϵ4+
d2 log d log 1
ϵ ) samples when the Xi are further assumed to be subgaussian.
We observe that Assumption 7 implies the following useful bound.
Lemma 132. Let w ∈∆n be ϵ-saturated with respect to bipartition [n] = G ∪B, let G⋆⊆G be the
subset in Assumption 7.2 corresponding to θ⋆, and let ˜w =
wG⋆
∥wG⋆∥1 . Let θ ˜
w = argminθ∈RdF ˜
w(θ) be
the empirical minimizer of F ˜
w. Then,
∥θ ˜
w −θ⋆∥Σ⋆≤4Cestσ√κϵ.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 388
Proof. Applying Assumption 7.2 with θ = θ⋆, we have that
∥∇F ˜
w(θ⋆)∥2 ≤Cest
√
Lϵσ.
However, applying Assumption 7.1 on the weights wG⋆implies that F ˜
w is 1
2-strongly convex in the
Σ⋆norm (since wG⋆removes at most ϵ2 mass from wG) and minimized by θ ˜
w, and hence by using
consequences of strong convexity and (Σ⋆)−1 ⪯1
µI,
∥∇F ˜
w(θ⋆)∥2 ≥√µ ∥∇F ˜
w(θ⋆)∥(Σ⋆)−1 ≥
√µ
4 ∥θ ˜
w −θ⋆∥Σ⋆.
Finally, in Section 8.9, when we develop an alternative approach to linear regression based on the
robust gradient descent framework, we require a slightly weaker set of distributional assumptions
and deterministic implications, which we now state.
Model 4 (distributional regularity for linear regression, gradient descent setting). Given distribu-
tions DX and Dδ over Rd, Rd respectively, the distribution DXy over Rd×R is sampled as follows: for
an underlying vector θ⋆∈Rd, independently sample X ∼DX and δ ∼Dδ, and set y ←⟨θ⋆, X⟩+ δ.
Further, DX and Dδ satisfy the following regularity assumptions.
1. For Σ⋆= EDX[XX⊤] and 0 < µ < L, we have µI ⪯Σ⋆⪯LI.
2. DX is C2→4-hypercontractive for a constant C2→4.
3. Dδ is a distribution with mean zero and variance ≤σ2.
The main diﬀerence between Model 3 and Model 4 is that the latter no longer requires hyper-
contractive noise. This corresponds to the following deterministic assumption.
Assumption 8 (deterministic regularity for linear regression, gradient descent setting). Let ϵ be
suﬃciently small. For every ﬁxed θ ∈Rd, there is a partition [n] = Gθ ∪Bθ with |Gθ| ≥(1 −ϵ)n
which satisﬁes: there is a constant Cest such that for ˜w :=
wGθ
∥wGθ∥1
,
∥∇F ˜
w(θ) −∇F ⋆(θ)∥2 ≤Cest
√
Lϵ (σ + ∥θ −θ⋆∥Σ⋆) ,
(8.40)
Cov ˜
w
 {gi(θ)}i∈Gθ

op ≤CestL

∥θ −θ⋆∥2
Σ⋆+ σ2
.
(8.41)
The main diﬀerence between Assumption 7 and Assumption 8 is that the latter provides gradient
bounds using a diﬀerent set Gθ for each θ (as opposed to the former, which uses the same set G for
all θ). The upshot is that the corresponding required sample complexity is lower.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 389
Proposition 34 ([134], Lemma 5.5). Let ϵ > 0 be suﬃciently small. Let {(Xi, yi)}i∈[n] ⊂Rd × R
be an O(ϵ)-corrupted set of samples from a distribution DXy as in Model 8. Then if
n = O
d log(d/ϵ)
ϵ

,
Assumption 8 holds with probability at least
9
10.
8.7.4
Regularity assumptions: Lipschitz and smooth stochastic optimiza-
tion
In Section 8.9, we develop algorithms for the special case of Model 2 when all γyi(v) := γ(v, yi), as
viewed as a function of its ﬁrst variable, satisfy
γ′
yi(v)
 ≤1, 0 ≤γ′′
yi(v) ≤1 for all v ∈R, i ∈[n].
In other words, all γyi are 1-smooth and 1-Lipschitz. A canonical example is when all yi = ±1 are
positive or negative labels, and γ is the logistic loss function, γ(v, y) = log(1 + exp(−vy)). In this
setting, we will focus on approximating the (population) regularized optimizer,
θ⋆
reg := argminθ∈Rd
n
F ⋆(θ) + µ
2 ∥θ∥2
2
o
, where F ⋆(θ) := Ef∼Df [f(θ)] .
(8.42)
Here, µ ∈R≥0 controls the amount of regularization, and is used to introduce some amount of strong
convexity in the problem. Following Section 8.7.2, the distribution Df over sampled functions is
directly dependent on a dataset distribution, DXy, through the relationship in Model 2. Concretely,
we make the following regularity assumptions about the distribution DXy and its induced Df.
Model 5 (distributional regularity for smooth GLMs). DXy, supported on Rd × R, its marginals
DX, Dy, and its induced function distribution Df, have the following properties.
1. Letting the second moment matrix of DX be Σ⋆and 0 < L, we have Σ⋆⪯LI.
2. There is a link function γ : R2 →R, such that for all y in the support of Dy, γy(v) := γ(v, y)
satisﬁes
γ′
y(v)
 ≤1 and 0 ≤γ′′
y (v) ≤1 for all v ∈R.
3. The distribution of f ∼Df is generated as follows: for (X, y) ∼DXy, f(θ) = γ(⟨X, θ⟩, y).
In Section 8.10, we further develop algorithms for the special case of Model 2 when all γyi(v) :=
γ(v, yi) satisfy only a ﬁrst-derivative bound,
γ′
yi(v)
 ≤1 for all v ∈R, i ∈[n].

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 390
In other words, all γyi are 1-Lipschitz (but possibly non-smooth). A canonical example is when all
yi = ±1 are positive or negative labels, and γ is the support vector machine loss function (hinge
loss), γ(v, y) = max(0, 1 −vy). In this setting, we will focus on approximating the (population)
regularized optimizer of the Moreau envelope,
θ⋆
env := argminθ∈Rd
n
F ⋆
γ (θ) + µ
2 ∥θ∥2
2
o
, where F ⋆(θ) = Ef∼Df [f(θ)],
and F ⋆
λ(θ) := inf
θ′

F ⋆(θ′) + 1
2λ ∥θ −θ′∥2
2

.
The Moreau envelope is extremely well-studied [435], and can be viewed as a smooth approximation
to a non-smooth function. We choose to focus on optimizing the Moreau envelope because it is
amenable to acceleration techniques, trading oﬀapproximation for smoothness.
Model 6 (distributional regularity for Lipschitz GLMs). DXy, supported on Rd × R, its marginals
DX, Dy, and its induced function distribution Df, have the following properties.
1. Letting the second moment matrix of DX be Σ⋆and 0 < L, we have Σ⋆⪯LI.
2. There is a link function γ : R2 →R, such that for all y in the support of Dy, γy(v) := γ(v, y)
satisﬁes
γ′
y(v)
 ≤1 for all v ∈R.
3. The distribution of f ∼Df is generated as follows: for (X, y) ∼DXy, f(θ) = γ(⟨X, θ⟩, y).
In other words, Model 6 is Model 5 without the smoothness assumption. Under the weaker
Model 6, [176] showed that we can make the following simplifying deterministic assumptions about
our observed dataset (which also extends to Model 5, as it a superset of conditions).
Assumption 9 (deterministic regularity for Lipschitz regression). The set {(Xi, yi)}i∈[n] ⊂Rd ×R,
and the link function γ : R2 →R, have the following properties, for [n] = G ∪B.
1. Letting w⋆
G :=
1
|G|1G, Covw⋆
G(X) ⪯3
2LI.
2. There is a constant Cest such that for all θ ∈Rd and all saturated w ∈∆n, letting ˜w :=
wG
∥wG∥1 ,
∥∇F ˜
w(θ) −∇F ⋆(θ)∥2 ≤Cest
√
Lϵ,
Cov ˜
w
 {gi(θ)}i∈G

op ≤CestL.
Proposition 35 ([176], Proposition C.3). Let ϵ > 0 be suﬃciently small. Let {(Xi, yi)}i∈[n] ⊂Rd×R
be an ϵ-corrupted set of samples from a distribution DXy as in Model 6. Then, if
n = O
d log (d/ϵ)
ϵ

,
Assumption 9 holds with probability at least
9
10.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 391
Finally, we make the useful observation that F ⋆under Model 6 is also Lipschitz.
Lemma 133. Under Model 6, F ⋆= Ef∼Df [f] is
√
L-Lipschitz.
Proof. We wish to prove ∥∇F ⋆(θ)∥2 ≤
√
L for all θ ∈Rd. By nonnegativity of covariance,
Ef∼Df
h
(∇f(θ)) (∇f(θ))⊤i
⪰
 Ef∼Df [∇f(θ)]
  Ef∼Df [∇f(θ)]
⊤= (∇F ⋆(θ)) (∇F ⋆(θ))⊤.
Since the right-hand side of the above display is rank-one, ∥∇F ⋆(θ)∥2
2 is at most the largest eigenvalue
of the gradient second moment matrix, so it suﬃces to show the left-hand side is ⪯LI: assuming
f ∼Df is associated with (X, y) ∼DXy,
Ef∼Df
h
(∇f(θ)) (∇f(θ))⊤i
= EX,y∼DXy
h
X (γ′ (⟨X, θ⟩, y))2 X⊤i
⪯Σ⋆⪯LI.
8.7.5
Robustly decreasing the covariance operator norm
In this section, we describe a procedure, FastCovFilter, which takes as input a set of vectors V :=
{vi}i∈[n] ∈Rn×d such that for an (unknown) bipartition [n] = G∪B, it is promised that Ei∼unifG

viv⊤
i

is bounded in operator norm by R. Given this promise, FastCovFilter takes as input a set of saturated
weights w ∈∆n and performs a sequence of safe weight removals to obtain a new saturated w′ ∈∆n,
such that P
i∈[n] w′
iviv⊤
i
is bounded in operator norm by O(R). Moreover, the procedure runs in
nearly-linear time in the description size of V. We formally describe the guarantees of FastCovFilter
here as Proposition 36, and defer the proof to Appendix G.6.
Proposition 36. There is an algorithm, FastCovFilter (Algorithm 92), taking inputs V := {vi}i∈[n] ∈
Rn×d, saturated weights w ∈∆n with respect to bipartition [n] = G ∪B with |B| = ϵn, δ ∈(0, 1],
and R ≥0 with the promise that

X
i∈G
1
|G|viv⊤
i

op
≤R.
Then, with probability at least 1 −δ, FastCovFilter returns saturated w′ ∈∆n such that

X
i∈[n]
w′
iviv⊤
i

op
≤5R.
The runtime of FastCovFilter is
O

nd log3(n) log
n
δ

.
An algorithm with similar guarantees to FastCovFilter is implicit in the work [194], but we give
a self-contained exposition in this work for completeness. Our approach in designing FastCovFilter

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 392
is to use a matrix multiplicative weights-based potential function, along with the ﬁltering approach
suggested by Lemma 130, to rapidly decrease the quadratic form of the empirical second moment
matrix in a number of carefully-chosen directions, and argue this quickly decreases the potential. We
remark that this potential-based approach was developed earlier in this chapter (see Section 8.3).
8.8
Linear regression
Throughout this section, we operate under Models 1 and 3, and Assumption 7. Namely, there is
a dataset {Xi, yi}i∈[n] and an unknown bipartition [n] = G ∪B, such that {Xi, yi}i∈G were draws
from a distribution DXy, and we wish to estimate
θ⋆:= argminθ∈RdF ⋆(θ), where F ⋆(θ) := EX,y∼DXy
1
2 (⟨X, θ⟩−y)2

.
We follow notation (8.35), (8.37) in this section, and deﬁne G⋆⊆G as the subset given by Assump-
tion 7.2 for the true minimizer θ⋆. We also deﬁne B⋆:= [n] \ G⋆, so B⋆⊇B.
We begin in Section 8.8.1 with a preliminary on ﬁltering under a weaker assumption than the
safety condition in Deﬁnition 29; in particular, Assumption 7 is not quite compatible with Deﬁni-
tion 29 because G′ can change based on θ, but will not aﬀect saturation by more than constants. In
Section 8.8.2, we then state a general "identiﬁability proof" showing that controlling certain quanti-
ties such as the operator norm of the gradient covariances and near-optimality of a current estimate
θ, with respect to some weights w ∈∆n, suﬃces to bound closeness of θ and θ⋆. This identiﬁability
proof is motivated by an analogous argument in [53], and will guide our algorithm design. Next,
we give a self-contained oracle which rapidly halves the distance to θ⋆outside of a suﬃciently large
radius in Section 8.8.3, and analyze the ﬁnal phase (once this radius is reached) in Section 8.8.4. We
put the pieces together to give our main result and full algorithm in Section 8.8.5.
8.8.1
Filtering under (ϵ, ϵ2
α )-goodness
Throughout this section, we will globally ﬁx a value (for a suﬃciently large constant)
α = O

max

1, ϵκ log R0
σ

.
(8.43)
In this deﬁnition, R0 is an initial distance bound on ∥θ0 −θ⋆∥Σ⋆we will provide to our ﬁnal algorithm
(see the statement of Theorem 56). We operate under Assumption 7 such that our dataset is (ϵ, ϵ2
α )-
good, which inﬂates the sample complexity of Proposition 33 by a factor depending on α.
At a high level, this technical complication is to ensure that throughout the algorithm we never
remove more than 3ϵ mass from G, and that our weights are always ϵ-saturated with respect to
G ∪B, allowing for inductive application of Assumption 7. Formally, we demonstrate the following

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 393
(simple) generalization of Lemma 130, using safety deﬁnitions on diﬀerent sets.
Lemma 134. Suppose Assumption 7 holds with r = ϵ2
α . Consider any algorithm which performs
the following weight removal.
1. w0 ←1
n 1,1
2. For 0 ≤t < T:
(a) [wt+1]i ←

1 −
τi
τmax

[wt]i for all i ∈[n] and τmax := maxi∈[n]|wi̸=0 τi, for {τi}i∈[n] safe
with respect to wt and a bipartition G′
t, [n] \ G′
t where G′
t ⊆G, |G′
t| ≥(1 −ϵ2
α )|G|.
Suppose the number of distinct sets G′
t throughout the algorithm is bounded by
α
2ϵ. Then throughout
the algorithm, wt is ϵ-saturated with respect to the bipartition G ∪B.
Proof. Consider some distinct set G′.
The proof of Lemma 130 demonstrates that under these
assumptions, in every iteration t using G′ for weight removal, the amount of mass removed from G′
is less than the amount removed from [n] \ G′. Moreover, the amount of mass removed from G in
these iterations can be at most the amount removed from G′, plus the weight assigned to the entire
diﬀerence G \ G′, which is at most a ϵ2
α fraction. Similarly, the amount of mass removed from B
is at least the amount of mass removed from [n] \ G′, minus their set diﬀerence, which is again at
most ϵ2
α . Combining over all distinct sets, this deviation is at most ϵ.
It will be straightforward to verify that throughout this section, all weight removals we perform
will be of the form in Lemma 134, and that we never perform weight removals with respect to more
than
α
2ϵ distinct sets. Hence, we will always assume that any weights w we discuss are ϵ-saturated
with respect to the bipartition G ∪B, and thus has ∥wG∥1 ≥1 −3ϵ, allowing for application of
Assumption 7. Finally, we remark we sometimes will apply Assumption 7.1 with weight vectors wG⋆
instead of wG; since their diﬀerence is O(ϵ2) < ϵ, this is a correct application for any ∥wG∥1 ≥1−3ϵ.
8.8.2
Identiﬁability proof for linear regression
In this section, we give an identiﬁability proof similar to that appearing in [53] which demonstrates,
for a given weight-parameter estimate pair (w, θ) ∈∆n × Rd, veriﬁable technical conditions on this
pair which certify a bound on ∥θ −θ⋆∥Σ⋆. In short, Proposition 37 will show that if both of the
quantities
∥∇Fw(θ)∥(Σ⋆)−1 ,
Covw

{gi(θ)}i∈[n]

op
(8.44)
are simultaneously controlled, then we obtain a distance bound to θG⋆.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 394
Proposition 37. Let w ∈∆n be ϵ-saturated with respect to the bipartition [n] = G ∪B, and let
θ ∈Rd. Assuming ϵκ is suﬃciently small, there is a universal constant Cid such that
∥θ −θ⋆∥Σ⋆≤Cid




√ϵ



σ√κ +
v
u
u
t
Covw

{gi(θ)}i∈[n]

op
µ



+ ∥∇Fw(θ)∥(Σ⋆)−1



.
Proof. Let G′ be the set promised by Assumption 7.2 for the point θ. Throughout this proof, we
deﬁne Gθ := G′∩G⋆, and we let w⋆
θ :=
1
|Gθ|1Gθ be the uniform weights on Gθ. Finally, let ˆθ minimize
Fw⋆
θ. By applying Lemma 132 on the weights w⋆
θ, we have that ∥ˆθ −θ⋆∥Σ⋆= O(σ√κϵ). Thus, in
the remainder of the proof we focus on bounding ∥θ −ˆθ∥Σ⋆by the required quantity.
Let C(w⋆
θ, ˜w) supported on [n] × [n] be an optimal coupling between i ∼w⋆
θ and j ∼˜w, where
˜w :=
w
∥w∥1 is the distribution proportional to w; we denote the coupling as C for short. For a pair
(i, j) ∈[n] × [n] sampled from C, we let 1i=j be the indicator of the event i = j, and similarly deﬁne
1i̸=j. From the total variation characterization of coupling, we have by Lemma 131 that
Ei,j∼C [1i̸=j] = ∥˜w −w⋆
θ∥1 ≤9ϵ.
Here we used that ∥w⋆
θ −w⋆
G∥1 = O(ϵ2), where w⋆
G is the uniform distribution on G, as in Lemma 131.
Now, let v = θ −ˆθ. We have by Assumption 7.1 that
Ei∼w⋆
θ
h
⟨v, Xi⟩
D
Xi, θ −ˆθ
Ei
=
D
θ −ˆθ, Ei∼w⋆
θ

XiX⊤
i

(θ −ˆθ)
E
≥1
2
θ −ˆθ

2
Σ⋆.
(8.45)
On the other hand,
Ei∼w⋆
θ
h
⟨v, Xi⟩
D
Xi, θ −ˆθ
Ei
= Ei∼w⋆
θ [⟨v, Xi⟩(⟨Xi, θ⟩−yi)] + Ei∼w⋆
θ
h
⟨v, Xi⟩

yi −
D
Xi, ˆθ
Ei
= Ei∼w⋆
θ [⟨v, gi(θ)⟩] −Ei∼w⋆
θ
hD
v, gi(ˆθ)
Ei
= Ei∼w⋆
θ [⟨v, gi(θ)⟩] .
Here, we used that Ei∼w⋆
θ
h
gi(ˆθ)
i
= ∇Fw⋆
θ(ˆθ) = 0 by deﬁnition of ˆθ. Continuing,
Ei∼w⋆
θ
h
⟨v, Xi⟩
D
Xi, θ −ˆθ
Ei
= Ei,j∼C [⟨v, gi(θ)⟩1i=j] + Ei,j∼C [⟨v, gi(θ)⟩1i̸=j]
= Ej∼˜
w [⟨v, gj(θ)⟩] −Ei,j∼C [⟨v, gj(θ)⟩1i̸=j] + Ei,j∼C [⟨v, gi(θ)⟩1i̸=j]
≤⟨v, ∇F ˜
w (θ)⟩+ 3√ϵ

Ej∼˜
w
h
⟨v, gj(θ)⟩2i 1
2 + Ei∼w⋆
θ
h
⟨v, gi(θ)⟩2i 1
2 
.
(8.46)
In the last line, we used Cauchy-Schwarz and Ei,j∼C[12
i̸=j] ≤9ϵ to deal with the second and third

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 395
terms. To bound the term corresponding to i ∼w⋆
θ,
Ei∼w⋆
θ
h
⟨v, gi(θ)⟩2i
≤∥v∥2
2
Covw⋆
θ

{gi(θ)}i∈[n]

op ≤1.1Cestκ ∥v∥2
Σ⋆

∥θ −θ⋆∥2
Σ⋆+ σ2
.
The ﬁrst inequality is by deﬁnition of ∥·∥op, and the second used ∥v∥2
2 ≥1
µ ∥v∥2
Σ⋆and Assumption 7.2,
since Gθ is a subset of the set G′ promised by Assumption 7.2 (where we adjusted by a constant for
normalization). We similarly arrive at the bound
Ej∼˜
w
h
⟨v, gj(θ)⟩2i
≤∥v∥2
Σ⋆
µ
Covw

{gi(θ)}i∈[n]

op .
Now plugging the above displays into (8.46) and combining with (8.45), as well as using ∥θ −θ⋆∥2
Σ⋆+
σ2 = O(∥θ −ˆθ∥2
Σ⋆+ σ2) by the triangle inequality and our earlier bound on ∥θ⋆−ˆθ∥Σ⋆,
θ −ˆθ

2
Σ⋆≤
θ −ˆθ

Σ⋆∥∇F ˜
w(θ)∥(Σ⋆)−1 + O
 √ϵκ
 
σ
θ −ˆθ

Σ⋆+
θ −ˆθ

2
Σ⋆

+ O
 √ϵ





θ −ˆθ

Σ⋆
v
u
u
t
Covw

{gi(θ)}i∈[n]

op
µ



.
In the ﬁrst line, we used Cauchy-Schwarz to bound the term ⟨v, ∇F ˜
w(θ)⟩. Dividing through by
∥θ −ˆθ∥Σ⋆, and using that ϵκ is suﬃciently small, yields the conclusion.
Proposition 37 suggests a natural approach. On the one hand, if θ is an (approximate) minimizer
to Fw, the ﬁrst quantity in (8.44) will be small. On the other hand, for a ﬁxed θ ∈Rd, we can
ﬁlter on w using our subroutine FastCovFilter until the second quantity in (8.44) is small. The main
challenge is accomplishing both bounds simultaneously. To this end, we show that the number of
times we have to repeat this process of ﬁltering and then computing an approximate minimizer is
bounded, using Fw as a potential function; by preprocessing Fw so that is smooth at all points, if θ
is an approximate minimizer for the Fw attained after ﬁltering on gradients at θ, then we can exit
the subroutine. Otherwise, we argue we make substantial function progress by calling an empirical
risk minimizer to terminate quickly. We make this strategy formal in the following sections.
8.8.3
Halving the distance to θ⋆
In this section, we design a procedure, HalfRadiusLinReg, with the following guarantee. Suppose that
we have ϵ-saturated ¯w ∈∆n and a parameter ¯θ ∈Rd, as well as scalar R with the promise
¯θ −θ⋆
Σ⋆≤R.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 396
The goal of HalfRadiusLinReg is to return a new θ with ∥θ −θ⋆∥Σ⋆≤1
2R; we require that R = Ω(σ)
for a suﬃciently large constant in this section. We do so by using saturated weights to guide a
potential analysis, crucially using Proposition 37. Before stating HalfRadiusLinReg, we require two
additional helper tools. The ﬁrst is an approximate optimization procedure.
Deﬁnition 32. We call OERM a γ-approximate ERM oracle if on input F : Rd →R it returns a
point ˆθ such that F(ˆθ) −F(θ⋆
F ) ≤γ, for θ⋆
F := argminθ∈RdF(θ).
The second controls the initial error, which we use to yield distance bounds via strong convexity.
Lemma 135. There is an algorithm, FunctionFilter, which takes as input ϵ-saturated w ∈∆n,
θ ∈Rd, and R ≥∥θ −θ⋆∥Σ⋆, and produces ϵ-saturated w′ ∈∆n such that Fw′(θ) ≤2Cub(σ2 + R2),
in time O(nd + n log D
R2 ), where D is a bound on the largest 1
2(⟨Xi, θ⟩−yi)2 for any nonzero wi.
Proof. Deﬁne τi := fi(θ) = 1
2(⟨Xi, θ⟩−yi)2. Assumption 7.1 shows that FwG⋆is 3
2-smooth in the Σ⋆
norm, since its Hessian is exactly ∥wG⋆∥1 CovwG⋆(X) ⪯3
2Σ⋆. Moreover, letting ˆθ minimize FwG⋆,
by Lemma 132, the triangle inequality and the assumed bound R,
θ −ˆθ

Σ⋆≤R +
θ⋆−ˆθ

Σ⋆≤R + σ.
Hence, assuming Cub is large enough and R ≥σ, smoothness demonstrates
X
i∈G⋆
wiτi = FwG⋆(θ) ≤FwG⋆(ˆθ) + 3
4
θ −ˆθ

2
Σ⋆≤Cub(σ2 + R2),
Next, letting τmax := maxi∈[n]|wi̸=0 τi and K ∈N ∪{0} be smallest such that
X
i∈[n]

1 −
τi
τmax
K
wiτi ≤2Cub(σ2 + R2),
each of the ﬁrst K weight removals according to the scores τ are safe with respect to G⋆∪B⋆, and
Lemma 134 implies we can output ϵ-saturated w′
i =

1 −
τi
τmax
K
wi entrywise. It remains to binary
search for K; given access to the scores τ, checking if a given K passes the above display takes O(n)
time. We can upper bound K by the following inequality:
X
i∈[n]

1 −
τi
τmax
K
wiτi ≤
X
i∈[n]
exp

−Kτi
τmax

wiτi ≤
1
eK
X
i∈[n]
wiτmax ≤τmax
K .
Here the second inequality used x exp(−Cx) ≤
1
eC for all nonnegative x, C, where we chose C =
K
τmax
and x = τi. Hence, K = O( D
R2 ). The runtime follows as computing scores takes time O(nd).
We remark that every time we use FunctionFilter is a weight removal of the form in Lemma 134,
which is safe with respect to the bipartition G⋆∪B⋆. This accounts for one distinct set throughout.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 397
Algorithm 46: HalfRadiusLinReg(X, y, ¯w, ¯θ, R, D, OERM, δ)
1 Input: Dataset X = {Xi}i∈[n] ∈Rn×d, y = {yi}i∈[n] ∈Rn, satisfying Assumption 7,
ϵ-saturated ¯w ∈∆n with respect to bipartition [n] = G∪B such that X⊤diag ( ¯w) X ⪯8LI,
¯θ ∈Rd with
¯θ −θ⋆
Σ⋆≤R, δ ∈(0, 1), O( σ2
κ )-approximate ERM oracle OERM,
D ≥max
i∈[n]
1
2(

Xi, ¯θ

−yi)2.
(8.47)
;
2 Output: With probability ≥1 −δ, saturated w with respect to G ∪B, and θ with
∥θ −θ⋆∥Σ⋆≤1
2R.
;
3 t ←0, w(0) ←FunctionFilter(w, ¯θ, R, D), ∆0 ←∞;
4 while ∆t >
R2
512κC2
id do
5
θ(t) ←OERM(Fw(t));
6
w(t+1) ←FastCovFilter

gi
 θ(t)	
i∈[n] , w(t),
δ
O(κ), 40CestCubLR2
;
7
∆t+1 ←Fw(t+1)
 θ(t)
−Fw(t+1)
 θ(t+1)
;
8
t ←t + 1;
9 Return:
 w(t), θ(t−1)
Lemma 136. HalfRadiusLinReg is correct, i.e. if its preconditions are met, it successfully returns
(w, θ) such that w ∈∆n is ϵ-saturated and ∥θ −θ⋆∥Σ⋆≤1
2R. It runs in O(κ) calls to OERM, plus
O

n log
 D
R2

+ ndκ log3(n) log
nκ
δ

additional time.
Proof. We discuss correctness and runtime separately.
Correctness. The ﬁrst step of our correctness proof is to show that throughout the algorithm,
θ(t) −θ⋆
Σ⋆≤6
p
CubR.
(8.48)
To see this, consider iteration t and suppose w(t) is ϵ-saturated. Let G′ ⊆G be the set promised by
Assumption 7.2 for the pair (w(t), θ(t)), and let Gt = G′ ∩G⋆. At the beginning of the algorithm we
applied FunctionFilter (see Lemma 135), and the minimum value of Fw is monotone nonincreasing
as w is decreasing, and OERM only decreases function value (else Line 4 would fail), so since R ≥σ,
Fw(t)
Gt

θ(t)
≤Fw(t)

θ(t)
≤4CubR2.
On the other hand, Fw(t)
Gt is 1
3-strongly convex in the Σ⋆norm by Assumption 7.1 (adjusting for the

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 398
normalization factor), and hence letting θ⋆
t be the minimizer of Fw(t)
Gt, we have
4CubR2 ≥Fw(t)
Gt

θ(t)
≥Fw(t)
Gt

θ(t)
−Fw(t)
Gt (θ⋆
t ) ≥1
6
θ(t) −θ⋆
t

2
Σ⋆=⇒
θ(t) −θ⋆
t

Σ⋆≤5
p
CubR.
Finally, by using Lemma 132 on θ⋆
t , we have ∥θ⋆
t −θ⋆∥Σ⋆≤4Cestσ√κϵ ≤R. From this and the
above display, the triangle inequality yields (8.48). Now, (8.48) with Assumption 7.2 shows that for
all t,
Cov
1
|Gt| 1Gt
n
gi(θ(t))
o
i∈Gt

op
≤40CestCubLR2.
We argue later in this proof that there are at most O(κ) loops throughout the algorithm. This shows
all calls to FastCovFilter are safe with respect to the bipartition Gt, [n] \ Gt (adjusting the deﬁnition
of ϵ by a constant in Proposition 36), and thus taking a union bound the algorithm succeeds with
probability at least 1 −δ. Condition on this for the remainder of the proof.
The success of all calls to FastCovFilter implies that in every iteration t (following Proposition 36),
Covw(t+1)
n
gi(θ(t))
o
i∈[n]

op
= O
 LR2
.
(8.49)
Next, in any iteration where ∆t+1 ≤
R2
512κC2
id , we claim that
∇Fw(t+1)

θ(t)
(Σ⋆)−1 ≤
R
4Cid
.
(8.50)
This is because Fw(t+1) is 8κ-smooth in the Σ⋆norm, since ∇2Fw(t+1) = X⊤diag
 w(t+1)
X ⪯8LI
by assumption, and 8κΣ⋆⪰8LI by assumption, so by the guarantees of OERM,
1
16κ
∇Fw(t+1)

θ(t)
2
(Σ⋆)−1 ≤Fw(t+1)

θ(t)
−min
θ∈Rd Fw(t+1) (θ)
≤∆t+1 + O
σ2
κ

≤
R2
256κC2
id
.
Rearranging indeed yields (8.50). Now by combining (8.49) and (8.50) in Proposition 37, we see
that if ∆t+1 ≤
R2
512κC2
id in an iteration, we obtain the desired
θ(t) −θ⋆
Σ⋆≤1
2R.
Runtime.
We ﬁrst observe that the loop in Lines 4-9 of Algorithm 46 can only run O(κ)
times.
This is because FunctionFilter decreases the initial function value until it is O(R2), and
every loop decreases the function value by Ω( R2
κ ). Hence, the cost of the whole algorithm is one
call to FunctionFilter, and O(κ) calls to OERM, FastCovFilter, and two function value computations,
which ﬁt into the allotted runtime budget by Lemma 135 and Proposition 36.
Finally, we remark that throughout Algorithm 46, we only ﬁltered weight based on FunctionFilter

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 399
and FastCovFilter, with respect to at most O(κ) distinct sets (in the manner described by Lemma 134):
the sets {Gt} used in correctness calls to FastCovFilter, and the set G⋆for the one call to FunctionFilter.
8.8.4
Last phase analysis
In this section, we give a slight variant of Algorithm 46 which applies when R ≤Clpσ for a universal
constant Clp. It will have a somewhat more stringent termination condition, because we require the
gradient term in Proposition 37 to be O(σ√ϵκ), but otherwise is identical.
Algorithm 47: LastPhase(X, y, ¯w, ¯θ, D, OERM, δ)
1 Input: Dataset X = {Xi}i∈[n] ∈Rn×d, y = {yi}i∈[n] ∈Rn, satisfying Assumption 7,
ϵ-saturated ¯w ∈∆n with respect to bipartition [n] = G∪B such that X⊤diag ( ¯w) X ⪯8LI,
¯θ ∈Rd with
¯θ −θ⋆
Σ⋆≤Clpσ, δ ∈(0, 1), O(σ2ϵ)-approximate ERM oracle OERM,
D ≥max
i∈[n]
1
2(

Xi, ¯θ

−yi)2.
(8.51)
;
2 Output: With probability ≥1 −δ, saturated w with respect to G ∪B, and θ with
∥θ −θ⋆∥Σ⋆= O
 σ√κϵ

.
;
3 t ←0, w(0) ←FunctionFilter(w, ¯θ, Clpσ, D), ∆0 ←∞;
4 while ∆t > σ2ϵ do
5
θ(t) ←OERM(Fw(t));
6
w(t+1) ←FastCovFilter

gi
 θ(t)	
i∈[n] , w(t), O (δϵ) , 40CestCubC2
lpLσ2
;
7
∆t+1 ←Fw(t+1)
 θ(t)
−Fw(t+1)
 θ(t+1)
;
8
t ←t + 1;
9 Return:
 w(t), θ(t−1)
;
Lemma 137. LastPhase is correct, i.e. if its preconditions are met, it successfully returns (w, θ)
such that w ∈∆n is ϵ-saturated and ∥θ −θ⋆∥Σ⋆= O (σ√κϵ). It runs in O( 1
ϵ ) calls to OERM, plus
O

n log
 D
R2

+ nd
ϵ log3(n) log
 n
δϵ

additional time.
Proof. On the correctness side, the analysis is nearly identical to Lemma 136; the same logic applies
to yield an analogous bound to (8.48), which shows that all iterates are within O(σ) from the
minimizer, so all calls to FastCovFilter are correct.
This implies that (analogous to (8.49)) the
gradient operator norm is always bounded by O(Lσ2). Similarly, since the threshold for termination

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 400
is when the function decrease is σ2ϵ, we have that on the terminating iteration,
1
16κ
∇Fw(t+1)

θ(t)
2
(Σ⋆)−1 = O(σ2ϵ),
and combining this with the operator norm bound in Proposition 37 yields the conclusion. On the
runtime side, the analysis is the same as Lemma 136, except that there are now O( 1
ϵ ) iterations.
Again, we remark here that throughout Algorithm 47, we ﬁltered weight with respect to at most
O(ϵ−1) distinct sets (in the manner described by Lemma 134).
8.8.5
Full algorithm
Before we give our full algorithm, we require some preliminary pruning procedures on the dataset.
Lemma 138. Under Assumption 7, for all i ∈G, ∥Xi∥2 ≤
√
2Ln.
Proof. Suppose otherwise for some i ∈G. Then, since CovwG (X) ⪰
1
|G|XiX⊤
i ⪰1
nXiX⊤
i , CovwG (X)
has an eigenvalue larger than 3
2L (certiﬁed by
Xi
∥Xi∥2 ), contradicting Assumption 7.1.
We next give a bound on D required by Algorithms 46 and 47, assuming a bound on
¯θ −θ⋆
Σ⋆.
Lemma 139. Suppose all {Xi}i∈[n] satisfy ∥Xi∥2 ≤
√
2Ln, and we have a bound
¯θ −θ⋆
Σ⋆≤R.
Under Assumption 7, it suﬃces to set D in HalfRadiusLinReg or LastPhase to
D = 2nCubσ2 + 2κnR2.
(8.52)
Proof. First, under Assumption 7 we have that for all i ∈G,
|⟨Xi, θ⋆⟩−yi| ≤
p
2nCubσ2.
Applying Cauchy-Schwarz, we have that for all i ∈G, since
¯θ −θ⋆
2 ≤
1
√µ
¯θ −θ⋆
Σ⋆≤
R
√µ,

Xi, ¯θ

−yi
 ≤
p
2nCubσ2 + ∥Xi∥2
¯θ −θ⋆
2 ≤
p
2nCubσ2 +
√
2κnR.
Finally, we give our full algorithm for regression, FastRegression, below.
Theorem 56. In Models 1 and 3, under Assumption 7 with r = ϵ2
α for α as in (8.43), supposing ϵκ is
suﬃciently small, given θ0 ∈Rd and ∥θ0 −θ⋆∥Σ⋆≤R0, FastRegression returns θ with ∥θ −θ⋆∥Σ⋆=
O(σ√κϵ) with probability at least 1 −δ. The algorithm runs in O
  1
ϵ + κ log R0
σ

calls to a O(σ2ϵ)-
approximate ERM oracle, and
O

nd log3(n) log
nR0
σδϵ
 1
ϵ + κ log R0
σ

additional time.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 401
Algorithm 48: FastRegression(X, y, θ0, R0, OERM, δ)
1 Input: Dataset X = {Xi}i∈[n] ∈Rn×d, y = {yi}i∈[n] ∈Rn, satisfying Models 1 and 3, and
satisfying Assumptions 7, θ0 ∈Rd with ∥θ0 −θ⋆∥Σ⋆≤R0, δ ∈(0, 1), O(σ2ϵ)-approximate
ERM oracle OERM.;
2 Output: With probability ≥1 −δ, θ with
∥θ −θ⋆∥Σ⋆= O
 σ√κϵ

.
;
3 Remove all (Xi, yi) with ∥Xi∥2 >
√
2nL, n ←new dataset size;
4 w ←FastCovFilter(X, 1
n 1,1, δ
3, 3
2L, 2nL), R ←R0 + 4Cestσ√κϵ, θ ←θ0;
5 T ←O(log R0
σ ) for a suﬃciently large constant;
6 while R > Clpσ do
7
D ←value in (8.52) with current setting of R ;
8
Remove all (Xi, yi) not satisfying bound (8.47), n ←new dataset size;
9
(w, θ) ←HalfRadiusLinReg(X, y, w, θ, R, D, OERM,
δ
3T );
10
R ←1
2R;
11 D ←value in (8.52) with current setting of R ;
12 Remove all (Xi, yi) not satisfying bound (8.47), n ←new dataset size;
13 (w, θ) ←HalfRadiusLinReg(X, y, w, θ, D, OERM, δ
3);
14 Return: θ;
Proof. First, correctness of Lines 3, 8, and 13 of the algorithm follow from Lemmas 138 and 139. Also,
Line 4 ensures that throughout the algorithm we have X⊤diag (w) X ⪯8LI, so the preconditions of
HalfRadiusLinReg and LastPhase are met. Finally, the initial setting of R is correct by Lemma 132
and the assumed bound R0. The correctness and runtime then follow from applying Lemma 136 T
times and Lemma 137 once. The failure probability comes from union bounding over the one call
to FastCovFilter, the T calls to HalfRadiusLinReg, and the one call to LastPhase.
Finally, for completeness we check that the promise of Section 8.8.1 is kept by Algorithm 48.
There are at most log( R0
σ ) calls to HalfRadiusLinReg, and one call to LastPhase. Combined, this
accounts for at most O( 1
ϵ +κ log R0
σ ) distinct sets we ﬁltered with respect to, in the manner described
by Lemma 134. For a suﬃciently large α in (8.43), this is indeed at most α
2ϵ distinct sets.
We conclude this section by noting that the guarantees of Proposition 33 imply the sample
complexity required for Algorithm 48 to succeed with probability at least
9
10 −δ is n = eO( d
ϵ4 + d2
ϵ3 ).
8.9
Robust acceleration
In this section, we give a general-purpose algorithm for solving statistical optimization problems
with a ﬁnite condition number κ under the strong contamination model. We study the following
abstract problem: we wish to minimize a function F : Rd →R which is L-smooth and µ-strongly

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 402
convex with minimizer θ⋆
F , but we only have black-box access to F through a noisy gradient oracle
Ong. In particular, we can query Ong at any point θ ∈Rd with a parameter R ≥∥θ −θ⋆
F ∥2 and
receive G(θ) such that for a universal constant Cng,
∥G(θ) −∇F(θ)∥2 ≤Cng
√
Lϵσ + L√ϵR

.
Notably, our algorithm is accelerated, running in a number of iterations depending on √κ rather
than κ. It applies to both the regression setting of Section 8.7.3 and the smooth stochastic opti-
mization setting of Section 8.7.4. We demonstrate in Section 8.9.1 how to build a noisy gradient
oracle for regression and smooth stochastic optimization settings. We then build in Section 8.9.2 a
simple subroutine based on the robust gradient descent framework of [447] to approximately solve
regularized subproblems encountered by our ﬁnal algorithm. We put the pieces together and give our
complete algorithm in Sections 8.9.3 and 8.9.4. Throughout we assume ϵκ2 < 1 is suﬃciently small.
8.9.1
Noisy gradient oracle
In this section, we build noisy gradient oracles for the problems in Sections 8.7.3 and 8.7.4. We now
give a formal deﬁnition below; the remaining sections will access F through this abstraction.
Deﬁnition 33 (Noisy gradient oracle). We call Ong(θ, R) a (L, σ, δ)-noisy gradient oracle for F :
Rd →R with minimizer θ⋆
F if on query θ ∈Rd and given R ≥∥θ −θ⋆
F ∥2, with probability ≥1 −δ it
returns G(θ) satisfying for a universal constant Cng,
∥G(θ) −∇F(θ)∥2 ≤Cng
√
Lϵσ + L√ϵR

.
If the returned G(θ) always satisﬁes the stronger bound ∥G(θ) −∇F(θ)∥2 ≤Cng
√
Lϵσ, we call Ong
a (L, σ, δ)-radiusless noisy gradient oracle.
Before developing our implementations, we state a useful identiﬁability result relating gradient
estimation to controlling operator norms of gradient second moments for ﬁnite sum functions.
Lemma 140. Suppose FG(θ) =
1
|G|
P
i∈G fi(θ) for some functions {fi}i∈G, and let w ∈∆n be
saturated with respect to bipartition [n] = G ∪B. For ˜w :=
w
∥w∥1 and w⋆
G =
1
|G|1G, we have
Ei∼w⋆
G [∇fi(θ)] −Ej∼˜
w [∇fj(θ)]

2 ≤
√
24ϵ
Covw⋆
G

{∇fi(θ)}i∈[n]

1
2
op +
Cov ˜
w

{∇fi(θ)}i∈[n]

1
2
op

.
Proof. Throughout this proof, we let C supported on [n]×[n] be an optimal coupling between i ∼w⋆
G

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 403
and j ∼˜w, and follow notation from Proposition 37. For some unit vector v ∈Rd, we have

v, Ei∼w⋆
G [∇fi(θ)] −Ej∼˜
w [∇fj(θ)]

= Ei,j∼C [⟨v, ∇fi(θ) −∇fj(θ)⟩]
= Ei,j∼C [⟨v, ∇fi(θ) −∇fj(θ)⟩1i̸=j]
≤
√
6ϵEi,j∼C
h
⟨v, ∇fi(θ) −∇fj(θ)⟩2i 1
2
≤
√
24ϵ

Ei∼w⋆
G
h
⟨v, ∇fi(θ)⟩2i 1
2 + Ej∼˜
w
h
⟨v, ∇fj(θ)⟩2i 1
2 
.
The conclusion follows from choosing v to be in the direction of Ei∼w⋆
G [∇fi(θ)]−Ej∼˜
w [∇fj(θ)], and
using the deﬁnition of the operator norm.
Lemma 140 implies that for approximating gradients of functions F which are "closely approx-
imated" by an (unknown) ﬁnite sum function FG, it suﬃces to ﬁnd a weighting ˜w such that the
operator norm of Cov ˜
w applied to gradients is bounded. We now demonstrate applications of this
strategy to linear regression and smooth stochastic optimization.
Corollary 37. Consider a robust linear regression instance where we have sample access to datasets
X = {Xi}i∈[n] ∈Rn×d and y = {yi}i∈[n] ∈Rn under Models 1, 4, with sample size n corresponding
to Proposition 34. For
F(θ) = F ⋆(θ) = EX,y∼DXy
1
2(⟨Xi, θ⟩−yi)2

,
we can construct a (L, σ, δ)-noisy gradient oracle for F in O
 nd log3(n) log2   n
δ

time, using O(log 1
δ )
queries of samples from Proposition 34.
Proof. We ﬁrst demonstrate how to construct a noisy gradient oracle with success probability ≥
8
10.
The algorithm is as follows: ﬁrst, sample a dataset under Models 1, 4, according to Proposition 34.
Then, at point θ ∈Rd, with probability
9
10 Assumption 8 gives us a set G := Gθ (where we drop the
subscript for simplicity, as this proof only discusses a single θ) with |G| = (1 −ϵ) such that (8.41)
holds. If we have the promise ∥θ −θ∗∥2 ≤R, let w ∈∆n be the output of
FastCovFilter

{gi(θ)}i∈[n], 1
n 1,1, 9
10, Cest
 Lσ2 + LR2
, where gi(θ) := Xi (⟨Xi, θ⟩−yi) .
where FastCovFilter (Algorithm 92) is the algorithm of Proposition 36. We then output Ej∼˜
w [gj(θ)],
where ˜w =
w
∥w∥1 . The runtime is O(nd log4 n) from the bottleneck operation of running FastCovFilter.
The assumptions of FastCovFilter, namely a bound on Covw⋆
G
 {gi(θ)}i∈[n]

, are satisﬁed by Assump-
tion 8.2. Guarantees of FastCovFilter and Lemma 140 then imply
Ei∼w⋆
G [gi(θ)] −Ej∼˜
w [gj(θ)]

2 = O
√
Lϵσ + L√ϵR

.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 404
The conclusion follows from Assumption 8.2 which bounds
Ei∼w⋆
G [gi(θ)] −∇F(θ)

2.
We now describe how to boost the success probability, by calling our sample access T = O(log 1
δ )
times. Let G⋆:= ∇F(θ) be the true gradient, and run the procedure described above T times,
producing {Gt}t∈[T ], such that each Gt satisﬁes ∥Gt −G⋆∥2 ≤C(
√
Lϵσ + L√ϵR) with probability
at least 4
5, for some constant C. By standard binomial concentration, with probability at least 1−δ,
at least 3
5 of the {Gt}t∈[T ] will satisfy this bound; call such a satisfying Gt "good." We return any
Gt which is within distance 2C(
√
Lϵσ + L√ϵR) from at least 3
5 of the gradient estimates. Note
this will never return any Gt with ∥Gt −G⋆∥2 > 4C(
√
Lϵσ + L√ϵR), since the triangle inequality
implies this Gt will miss all the good estimates, a contradiction since there is at most a 2
5 fraction
which is not good. Thus, this procedure satisﬁes the requirements with Cng = 4C; the additional
runtime overhead is O(log2( 1
δ )) distance comparisons between our gradient estimates.
Corollary 38. Consider a robust Lipschitz (not necessarily smooth) stochastic optimization instance
where we have sample access to datasets X = {Xi}i∈[n] ∈Rn×d and y = {yi}i∈[n] ∈Rn under
Models 1, 2, 6 with sample size n corresponding to Proposition 35. For
F(θ) = F ⋆(θ) + µ
2 ∥θ∥2
2 = Ef∼Df [f(θ)] + µ
2 ∥θ∥2
2 ,
we can construct a (L, 1, δ)-radiusless noisy gradient oracle for F in O
 nd log3(n) log2   n
δ

time,
using O(log 1
δ ) queries of samples from Proposition 35.
Proof. The proof follows identically to that of Corollary 37, where we use Assumption 9.2 in place
of Assumption 8.2.
In most of Sections 8.9.2, 8.9.3, and 8.9.4, we will no longer discuss any speciﬁcs of the unknown
function F : Rd →R we wish to optimize, except that it is L-smooth, µ-strongly convex, has
minimizer θ⋆
F , and supports a noisy gradient oracle Ong. We will apply Corollaries 37 and 38 to
derive concrete rates and sample complexities for speciﬁc applications at the conclusion of this
section.
8.9.2
Proximal subproblems
In this section, we develop a subroutine which obtains approximate minimizers of certain regularized
problems which we call proximal subproblems. Concretely, we now formally deﬁne the notion of a
noisy proximal oracle, an abstraction we will use in the following Section 8.9.4.
Deﬁnition 34 (Noisy proximal oracle). We call Onp a (σ, δ)-noisy proximal oracle for L-smooth
F : Rd →R with minimizer θ⋆
F if on query ¯θ ∈Rd and given R ≥
¯θ −θ⋆
F

2, with probability ≥1−δ

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 405
it returns ˆθ satisfying for a universal constant Cnp,
ˆθ −θ⋆
¯θ

2 ≤Cnp

σ
r ϵ
L + √ϵR

, where θ⋆
¯θ := argminθ∈Rd

F(θ) + L
2
θ −¯θ
2
2

.
(8.53)
The main goal of this section is to give a reduction from implementation of a noisy proximal oracle
to implementation of a noisy gradient oracle. We ﬁrst require a standard helper result bounding the
distance from θ⋆
F to θ⋆
¯θ by R, which we later use to show stability of our iterates.
Lemma 141. Following deﬁnition (8.53),
θ⋆
¯θ −θ⋆
F

2 ≤R.
Proof. The ﬁrst-order optimality condition of θ⋆
¯θ and the three-point identity
⟨c −b, a −c⟩= 1
2 ∥b −a∥2
2 −1
2 ∥c −a∥2
2 −1
2 ∥c −b∥2
2
(8.54)
yield the standard ℓ2 mirror descent guarantee
0 ≤F(θ⋆
¯θ) −F(θ⋆
F ) ≤

∇F
 θ⋆
¯θ

, θ⋆
¯θ −θ⋆
F

≤L
2
¯θ −θ⋆
F
2
2 −L
2
θ⋆
¯θ −θ⋆
F
2
2 −L
2
θ⋆
¯θ −¯θ
2
2 .
The ﬁrst two inequalities used convexity of F and that θ⋆
F is the minimizer of F. Rearranging yields
the desired conclusion, via
θ⋆
¯θ −θ⋆
F

2 ≤
¯θ −θ⋆
F

2 ≤R.
We can now deﬁne our noisy proximal oracle implementation, Algorithm 49.
Algorithm 49: NoisyProximalOracle(¯θ, R, Ong, σ, δ)
1 Input: Ong, a (L, σ, δ
T )-noisy gradient oracle for L-smooth F : Rd →R with minimizer θ⋆
F
for T = O

log

1
√ϵ + R
√
L
σ√ϵ

, ¯θ ∈Rd with
¯θ −θ⋆
F

2 ≤R, δ ∈(0, 1);
2 Output: With probability ≥1 −δ, ˆθ satisfying (8.53) for a universal constant Cnp;
3 T ←O

log

1
√ϵ + R
√
L
σ√ϵ

for a suﬃciently large constant, t ←0, θ0 ←¯θ;
4 while t < T and F(θt) + L
2
θt −¯θ
2
2 ≤2
3(F(θt−1) + L
2
θt−1 −¯θ
2
2) do
5
gt ←Ong(θt, 4R, δ
T ) + L(θt −¯θ);
6
θt+1 ←θt −
1
2Lgt;
7
t ←t + 1;
8 Return: θt−1;
Proposition 38. NoisyProximalOracle correctly implements a (σ, δ)-noisy proximal oracle for F in
T = O

log

1
√ϵ + R
√
L
σ√ϵ

calls to a (L, σ, δ
T )-noisy gradient oracle Ong, and O(dT) additional time.
Proof. The runtime is immediate from the description of Algorithm 49, so we focus on correctness.
For notational simplicity, denote the function we wish to minimize (with minimizer θ⋆
¯θ) by
F
¯θ(θ) := F(θ) + L
2
θ −¯θ
2
2 .

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 406
By Lemma 141 and the assumption on θ0 = ¯θ, we have that
θ0 −θ⋆
¯θ

2 ≤2R via the triangle
inequality. We ﬁrst claim that for all 0 ≤t < T, it is the case that
θt −θ⋆
¯θ

2 ≤3R =⇒∥θt −θ⋆
F ∥2 ≤4R.
(8.55)
In order to show this bound, since F ¯θ is L-strongly convex, it suﬃces to show
F
¯θ (θt) −F
¯θ  θ⋆
¯θ

≤4LR2 =⇒L
2
θt −θ⋆
¯θ
2
2 ≤4LR2 =⇒
θt −θ⋆
¯θ

2 ≤3R.
Thus, it suﬃces to show that the function error at all points θt is bounded by 4LR2; this is true
initially since
θ0 −θ⋆
¯θ

2 ≤2R and F ¯θ is 2L-smooth by L-smoothness of F, and the termination
condition guarantees that while the algorithm runs it is always a descent algorithm. Hence, all calls
to Ong are successful. This implies for the error of the gradient estimation, et := gt −∇F ¯θ(θt), that
∥et∥2 =
Ong

θt, 3R, δ
T

−∇F(θt)

2
≤Cng
√
Lϵσ + 4L√ϵR

.
(8.56)
Next, we expand: since F ¯θ is 2L-smooth,
F
¯θ (θt+1) −F
¯θ  θ⋆
¯θ

≤F
¯θ (θt) +
D
∇F
¯θ(θt), θt+1 −θt
E
+ L ∥θt+1 −θt∥2
2 −F
¯θ  θ⋆
¯θ

= F
¯θ (θt) −1
2L
D
∇F
¯θ(θt), ∇F
¯θ(θt) + et
E
+ 1
4L
∇F
¯θ(θt) + et

2
2 −F
¯θ  θ⋆
¯θ

= F
¯θ (θt) −F
¯θ  θ⋆
¯θ

−1
4L
∇F
¯θ (θt)

2
2 + 1
4L ∥et∥2
2
≤1
2

F
¯θ (θt) −F
¯θ  θ⋆
¯θ

+ 1
4L ∥et∥2
2 .
(8.57)
In the last line, we used that L-strong convexity of F ¯θ implies
1
2L
∇F
¯θ (θt)

2
2 ≥F
¯θ (θt) −F
¯θ  θ⋆
¯θ

.
Continuing from (8.57), we have from (8.56) that
F
¯θ (θt+1) −F
¯θ  θ⋆
¯θ

≤1
2

F
¯θ (θt) −F
¯θ  θ⋆
¯θ

+ 1
4L ∥et∥2
2
≤1
2

F
¯θ (θt) −F
¯θ  θ⋆
¯θ

+ C2
ng
2
 σ2ϵ + 16LR2ϵ

.
Thus, we will continue decreasing the suboptimality gap by a 1
3 factor so long as
C2
ng
2
 σ2ϵ + 16LR2ϵ

≤1
6

F
¯θ (θt) −F
¯θ  θ⋆
¯θ

.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 407
Suppose Line 4 terminates because the function value failed to improve. From the above,
L
2
θt−1 −θ⋆
¯θ
2
2 ≤F
¯θ (θt−1) −F
¯θ  θ⋆
¯θ

≤3C2
ng
 σ2ϵ + 16LR2ϵ

.
Rearranging yields the desired conclusion. Otherwise, in T iterations we improve the function error
from at most 4LR2 in the ﬁrst iteration, to at most 3C2
ng
 σ2ϵ + 16LR2ϵ

(since it is decreasing by a
constant factor each time), which again yields the desired conclusion by the above calculation.
8.9.3
Halving the distance to θ⋆
F
In this section, we give a subroutine used in our full algorithm, which halves the distance to θ⋆
F , the
minimizer of F, outside of a suﬃciently large radius. Suppose that we have an initial point ¯θ ∈Rd,
as well as a suﬃciently large scalar R ≥0 with the promise that (for a universal constant Clp)
¯θ −θ⋆
F

2 ≤R, where R ≥Clpσ
rκϵ
µ .
(8.58)
We now state a procedure, HalfRadiusAccel, which returns a new θ with ∥θ −θ⋆
F ∥2 ≤1
2R. In its
statement, we deﬁne a sequence of scalars {at, At}0≤t<T given by the recursions
A0 = 0, At = La2
t, At+1 = At + at+1.
(8.59)
The following fact is well-known (see for instance Chapter 2.2 of [418]).
Fact 19. For all 0 ≤t < T, At = Θ( t2
L ) and at = Θ( t
L).
Algorithm 50: HalfRadiusAccel(¯θ, R, Onp, δ)
1 Input: Onp, a (σ,
δ
O(√κ))-noisy proximal oracle for L-smooth, µ-strongly convex
F : Rd →R with minimizer θ⋆
F , ¯θ ∈Rd and R ∈R≥0 satisfying (8.58), δ ∈(0, 1);
2 Output: With probability ≥1 −δ, θ ∈Rd with ∥θ −θ⋆
F ∥2 ≤1
2R;
3 T ←O (√κ) for a suﬃciently large constant, t ←0, θ0 ←¯θ, v0 ←¯θ;
4 while t < T do
5
yt ←
At
At+1 θt + at+1
At+1 vt ;
6
θt+1 ←Onp(yt, 3R);
7
vt+1 ←argminv∈B
n
at+1 ⟨yt −θt+1, v⟩+
1
2L ∥v −vt∥2
2
o
, where B := {v |
v −¯θ

2 ≤R};
8
t ←t + 1;
9 Return: θt;
We assume for simplicity that we can exactly solve the constrained subproblem in Line 7 in each
iteration in O(d) time; by a standard binary search on a Lagrange multiplier, we can solve this
problem to high accuracy (cf. Proposition 8, [109]) and it is straightforward to verify this will not

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 408
be the bottleneck operation compared to calling Onp. Before analyzing the algorithm, we require a
helper argument which shows that all iterates lie close to θ⋆
F .
Lemma 142. In every iteration 0 ≤t ≤T, ∥yt −θ⋆
F ∥2 ≤3R.
Proof. Because yt is a convex combination of θt and vt, and vt is constrained to lie at distance at
most 2R from θ⋆
F by the triangle inequality since vt and θ⋆
F both lie in B, it suﬃces to show that all
∥θt −θ⋆
F ∥2 ≤3R. We show a slightly stronger statement: deﬁning Dt := 2R + tCnp(σp ϵ
L + 3√ϵR)
where Cnp is the constant from Deﬁnition 34, we claim that
∥θt −θ⋆
F ∥2 ≤Dt.
(8.60)
The conclusion then follows from
DT = 2R + O

σ
rϵκ
L + √κϵR

≤3R,
assuming κϵ is small enough and using the lower bound on R from (8.58). To show (8.60), assume
inductively that ∥θt −θ⋆
F ∥2 ≤Dt for some t, and observe by convexity that we also have ∥yt −θ⋆
F ∥2 ≤
Dt. Letting θ⋆
t be the optimal solution to the proximal subproblem deﬁning Onp(yt, 3R), we have
from the proof of Lemma 141 that ∥θ⋆
t −θ⋆
F ∥2 ≤∥yt −θ⋆
F ∥2 ≤Dt. The conclusion follows from the
bound ∥θt −θ⋆
t ∥2 ≤Cnp(σp ϵ
L + 3√ϵR), given by the guarantees of Onp.
The distance bound of Lemma 142 implies all calls to Onp are correct. We now give the main
technical lemma of this section, which shows a potential bound on iterates of HalfRadiusAccel.
Lemma 143. In every iteration 0 ≤t ≤T, deﬁne
Et := F(θt) −F(θ⋆
F ), Dt := 1
2 ∥vt −θ⋆
F ∥2
2 , Φt := AtEt + Dt.
Then, for all 0 ≤t < T, for a universal constant Cpot,
Φt+1 −Φt ≤Cpot

tσ
r ϵ
LR + t√ϵR2 + t2σ2 ϵ
L + t2ϵR2

.
Proof. Throughout this proof, let θ⋆
t be the solution to the subproblem deﬁning Onp(yt, 3R), namely
θ⋆
t := argminθ

F(θ) + L
2 ∥θ −yt∥2
2

.
Fix an iteration t. Deﬁne for notational convenience ρ := Cnp(σ
q
1
L + 3R), so the error guarantee
of Onp is that
θ⋆
t+1 −θt+1

2 ≤√ϵρ. The optimality conditions of θ⋆
t+1 and vt+1 respectively show

∇F
 θ⋆
t+1

, θ⋆
t+1 −u

≤L
2 ∥yt −u∥2
2 −L
2
θ⋆
t+1 −u
2
2 −L
2
yt −θ⋆
t+1
2
2 for all u ∈Rd,
(8.61)

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 409
where we used the identity (F.1), and (since θ⋆
F ∈B)
at+1L ⟨yt −θt+1, vt+1 −θ⋆
F ⟩≤1
2 ∥vt −θ⋆
F ∥2
2 −1
2 ∥vt+1 −θ⋆
F ∥2
2 −1
2 ∥vt −vt+1∥2
2 .
(8.62)
Moreover, deﬁne the point qt =
At
At+1 θt + at+1
At+1 vt+1. Rearranging,
vt+1 =
1
at+1
(At+1qt −Atθt) = θ⋆
t+1 + At
at+1
 θ⋆
t+1 −θt

−At+1
at+1
 θ⋆
t+1 −qt

.
By convexity of F, the above display yields

∇F
 θ⋆
t+1

, vt+1 −θ⋆
F

=

∇F
 θ⋆
t+1

, θ⋆
t+1 −θ⋆
F

+ At
at+1

∇F
 θ⋆
t+1

, θ⋆
t+1 −θt

−At+1
at+1

∇F
 θ⋆
t+1

, θ⋆
t+1 −qt

≥F
 θ⋆
t+1

−F (θ⋆
F ) + At
at+1
 F
 θ⋆
t+1

−F (θt)

−At+1
at+1

∇F
 θ⋆
t+1

, θ⋆
t+1 −qt

= At+1
at+1
 F
 θ⋆
t+1

−F (θ⋆
F )

−At
at+1
(F (θt) −F (θ⋆
F ))
−At+1
at+1

∇F
 θ⋆
t+1

, θ⋆
t+1 −qt

.
(8.63)
Next, since the proximal problem deﬁning θt+1 is 2L-smooth, the guarantees of Onp imply
F (θt+1) + L
2 ∥θt+1 −yt∥2
2 ≤F
 θ⋆
t+1

+ L
2
θ⋆
t+1 −yt
2
2 + L
θt+1 −θ⋆
t+1
2
2
≤F
 θ⋆
t+1

+ L
2
θ⋆
t+1 −yt
2
2 + Lϵρ2.
Substituting the above into (8.63) yields
at+1

∇F
 θ⋆
t+1

, vt+1 −θ⋆
F

≥At+1Et+1 −AtEt −At+1

∇F
 θ⋆
t+1

, θ⋆
t+1 −qt

+ At+1
L
2 ∥θt+1 −yt∥2
2 −L
2
θ⋆
t+1 −yt
2
2 −Lϵρ2

.
(8.64)
Next, since (unconstrained) optimality of θ⋆
t+1 implies θ⋆
t+1 = yt −1
L∇F(θ⋆
t+1),
at+1

∇F(θ⋆
t+1), vt+1 −θ⋆
F

= L

yt −θ⋆
t+1, vt+1 −θ⋆
F

≤at+1L ⟨yt −θt+1, vt+1 −θ⋆
F ⟩+ at+1L
θ⋆
t+1 −θt+1

2 ∥vt+1 −θ⋆
F ∥2
≤at+1L ⟨yt −θt+1, vt+1 −θ⋆
F ⟩+ 2at+1L√ϵρR.
In the last inequality, we used that vt+1 and θ⋆
F both lie in B. Finally, combining the above display,

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 410
(8.64), and (8.62) yields the bound
Φt+1 −Φt ≤At+1

∇F
 θ⋆
t+1

, θ⋆
t+1 −qt

−1
2 ∥vt −vt+1∥2
2 + 2at+1L√ϵρR
−At+1
L
2 ∥θt+1 −yt∥2
2 −L
2
θ⋆
t+1 −yt
2
2 −Lϵρ2

= At+1

∇F
 θ⋆
t+1

, θ⋆
t+1 −qt

+ L
2
θ⋆
t+1 −yt
2
2 −L
2 ∥θt+1 −yt∥2
2 −L
2 ∥yt −qt∥2
2

+ 2at+1L√ϵρR + At+1Lϵρ2.
In the second-to-last line, we used that vt −vt+1 =
At+1
at+1 (yt −qt) and
A2
t+1
a2
t+1 = LAt+1 by using
deﬁnitions. Now applying (8.61) with u = qt gives

∇F
 θ⋆
t+1

, θ⋆
t+1 −qt

+ L
2
θ⋆
t+1 −yt
2
2 −L
2 ∥θt+1 −yt∥2
2 −L
2 ∥yt −qt∥2
2
≤−L
2 ∥θt+1 −yt∥2
2 −L
2
θ⋆
t+1 −qt
2
2 ≤0.
Thus, applying Fact 19 shows the potential change is bounded by the desired
2at+1L√ϵρR + At+1Lϵρ2 = O
 t√ϵρR + t2ϵρ2
= O

tσ
r ϵ
LR + t√ϵR2 + t2σ2 ϵ
L + t2ϵR2

.
Finally, we are ready to analyze the output of HalfRadiusAccel.
Lemma 144. With probability at least 1 −δ, the output θT of HalfRadiusAccel satisﬁes
∥θT −θ⋆
F ∥2 ≤1
2R.
Proof. The failure probability comes from union bounding over the failures of calls to Onp, so we
discuss correctness assuming all calls succeed. By telescoping Lemma 143 over T iterations, we have
ΦT ≤Φ0 + Cpot
X
t∈[T ]

tσ
r ϵ
LR + t√ϵR2 + t2σ2 ϵ
L + t2ϵR2

.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 411
It is clear from deﬁnition that Φ0 ≤1
2R2, so it remains to bound all other terms. By examination,
X
t∈[T ]
tσ
r ϵ
LR = O

σκ
r ϵ
LR

= O
 R2
,
X
t∈[T ]
t√ϵR2 = O
 κ√ϵR2
= O
 R2
,
X
t∈[T ]
t2σ2 ϵ
L = O

κ1.5σ2 ϵ
L

= O
 R2
,
X
t∈[T ]
t2ϵR2 = O
 κ1.5ϵR2
= O
 R2
.
Each of the above lines follows from ϵκ2 being suﬃciently small, and the lower bound in (8.58). Thus
for a large enough value of Clp in (8.58), we have that ΦT ≤R2. Since ΦT = AT ET + DT ≥AT ET ,
choosing a suﬃciently large value of T = √κ combined with Fact 19 yields
ET = F (ΘT ) −F (θ⋆
F ) ≤R2
AT
≤LR2
8κ
= µR2
8 .
The conclusion follows from strong convexity of F, which implies ET ≥µ
2 ∥θT −θ⋆
F ∥2
2.
A note on constants. To check there are no conﬂict of interests hidden in the constants of this
proof, note ﬁrst that conditional on the bound at time T being at most R2, the number of iterations
T can be chosen solely as a function of the constants in Fact 19. From this point, the constant Clp
in (8.58) can be chosen to ensure that the potential bound is indeed R2.
8.9.4
Full accelerated algorithm
We conclude this section with a statement of a complete accelerated algorithm, and its applications.
Proposition 39. RobustAccel correctly returns θ satisfying (8.65) with probability ≥1 −δ. It runs
in O
√κ log2 
R0
√
L
σ√ϵ

calls to Ong, and O
√κd log2 
R0
√
L
σ√ϵ

additional time.
Proof. Correctness is immediate by iterating the guarantees of Lemma 144 T times, and taking a
union bound over all (at most N) calls to Ong, the only source of randomness in the algorithm. The
runtime follows from examining HalfRadiusAccel and NoisyProximalOracle, since all operations take
O(d) time other than calls to Ong.
By combining Proposition 39 with Corollary 37 and 38, we derive the following conclusions.
Theorem 57. Under Models 1, 4, supposing ϵκ2 is suﬃciently small, given θ0 ∈Rd and ∥θ0 −θ⋆∥Σ⋆≤
R0, RobustAccel using the noisy gradient oracle of Corollary 37 returns θ with ∥θ −θ⋆∥Σ⋆=

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 412
Algorithm 51: RobustAccel(θ0, R0, Ong, δ)
1 Input: Ong, a (L, σ, δ
N )-noisy gradient oracle for L-smooth, µ-strongly convex F : Rd →R
with minimizer θ⋆
F for N = O
√κ log2 
R0
√
L
σ√ϵ

, θ0 ∈Rd with ∥θ0 −θ⋆
F ∥2 ≤R0, δ ∈(0, 1);
2 Output: With probability ≥1 −δ, θ ∈Rd with
∥θ −θ⋆
F ∥2 = O

σ
rκϵ
µ

.
(8.65)
;
3 T ←O

log

R0√µ
σ√κϵ

for a suﬃciently large constant, t ←0;
4 Onp ←NoisyProximalOracle(·, ·, Ong, σ, δ
N );
5 while t < T do
6
θt+1 ←HalfRadiusAccel(θt, Rt, Onp, δ
T );
7
Rt+1 ←1
2Rt;
8
t ←t + 1;
9 Return: θt;
O (σκ√ϵ) with probability at least 1 −δ. The algorithm runs in
O
 
nd√κ log2
 R0
σ√ϵ

log3(n) log2
 
n log
  R0
σ

δϵ
!!
time,
where n is the dataset size of Proposition 34. The sample complexity of the method is
O
 
log
 
log
  R0
σ

δϵ
!
·
d log(d/ϵ)
ϵ
!
.
Theorem 58. Under Models 1, 2, and 5, supposing ϵκ2 is suﬃciently small for κ := max(1, L
µ ),
given θ0 ∈Rd and
θ0 −θ⋆
reg

2 ≤R0, RobustAccel using the noisy gradient oracle of Corollary 38
returns θ with
θ −θ⋆
reg

2 = O
q
κϵ
µ

with probability at least 1 −δ. The algorithm runs in
O
 
nd√κ log2
R0
√L + µ
ϵ

log3(n) log2
 
n log
 R0
√L + µ

δϵ
!!
time,
where n is the dataset size of Proposition 35. The sample complexity of the method is
O
 
log
 
log
 R0
√L + µ

δϵ
!
·
d log(d/ϵ)
ϵ
!
.
We remark that Theorem 57 can aﬀord to reuse the same samples to construct gradient estimates
for all θ we query per Assumption 8: though the set Gθ may change per θ, our noisy estimator also

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 413
provides a per-θ guarantee (and hence does not require the set to be consistent across calls).
8.10
Lipschitz generalized linear models
In this section, we give an algorithm for minimizing the regularized Moreau envelopes of Lipschitz
statistical optimization problems under the strong contamination model, following the exposition of
Section 8.7.4. Concretely, we recall we wish to compute an approximation to
θ⋆
env := argminθ∈Rd
n
F ⋆
λ (θ) + µ
2 ∥θ∥2
2
o
, where F ⋆(θ) = Ef∼Df [f(θ)],
and F ⋆
λ(θ) := inf
θ′

F ⋆(θ′) + 1
2λ ∥θ −θ′∥2
2

.
(8.66)
Recall that in Corollary 38, we developed a noisy gradient oracle for F ⋆, as long as our function
distribution is captured by Model 6. However, techniques of Section 8.9 do not immediately apply
to this setting, as F ⋆is not smooth. On the other hand, we do not have direct access to F ⋆
λ.
We ameliorate this by developing a noisy gradient oracle (Deﬁnition 33) for the Moreau envelope
F ⋆
λ in Section 8.10.1, under only Assumption 9; this will allow us to apply the acceleration techniques
of Section 8.9 to the problem (8.66), which we complete in Section 8.10.2. Interestingly, our noisy
gradient oracle for F ⋆
λ will have noise and runtime guarantees independent of the envelope parameter
λ, allowing for a range of statistical and runtime tradeoﬀs for applications.
8.10.1
Noisy gradient oracle for the Moreau envelope
In this section, we give an eﬃcient reduction which enables the construction of a noisy gradient
oracle for F ⋆
λ, assuming a radiusless noisy gradient oracle for F ⋆, and that F ⋆is Lipschitz. We
note that both of these assumptions hold under Model 6: we showed in Lemma 133 that F ⋆is
√
L-Lipschitz, and constructed a radiusless noisy gradient oracle in Corollary 38.
To begin, we recall standard facts about the Moreau envelope F ⋆
λ, which can be found in e.g.
[435].
Fact 20. F ⋆
λ is λ−1-smooth, satisﬁes 0 ≤F ⋆(θ) −F ⋆
λ(θ) ≤Lλ for all θ ∈Rd, and has gradient
∇F ⋆
λ(θ) = 1
λ
 θ −proxλ,F ⋆(θ)

, where proxλ,F ⋆(θ) := argminθ′

F ⋆(θ′) + 1
2λ ∥θ −θ′∥2
2

.
Fact 20 demonstrates that to construct a noisy gradient oracle for F ⋆
λ, it suﬃces to approximate
the minimizer of the subproblem deﬁning the proxλ,F ⋆operator. To this end, we give a simple
algorithm which approximates this proximal minimizer, based on noisy projected gradient descent.
We now begin our analysis of ApproxMoreauMinimizer. In the following discussion, for notational
simplicity deﬁne θ⋆
¯θ := proxλ,F ⋆(¯θ) to be the exact minimizer of the proximal subproblem.
We

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 414
Algorithm 52: ApproxMoreauMinimizer(¯θ, Ong, δ)
1 Input: Ong, a (L, 1, δ
T )-radiusless noisy gradient oracle for
√
L-Lipschitz F ⋆: Rd →R for
T = O
  1
ϵ log 1
ϵ

, ¯θ ∈Rd, δ ∈(0, 1);
2 Output: With probability ≥1 −δ, ˆθ satisfying for a universal constant Cenv,
ˆθ −proxλ,F ⋆(¯θ)

2 ≤Cenv
√
Lϵλ.
(8.67)
;
3 T ←O
  1
ϵ log 1
ϵ

for a suﬃciently large constant, t ←0, θ0 ←¯θ, η ←
4C2
ngϵλ
5
;
4 while t < T do
5
gt ←Ong(θt, ∞, δ
T ) + 1
γ (θt −¯θ);
6
θt+1 ←ProjB(θt −ηgt), where Proj is the ℓ2 projection and
B :=
n
θ |
θ −¯θ

2 ≤2
√
Lλ
o
;
7
t ←t + 1;
8 Return: θt;
require a simple helper bound showing θ⋆
¯θ does not lie too far from ¯θ.
Lemma 145. For B :=
n
θ |
θ −¯θ

2 ≤2
√
Lλ
o
and θ⋆
¯θ := proxλ,F ⋆(¯θ), θ⋆
¯θ ∈B.
Proof. Let R :=
θ⋆
¯θ −¯θ

2. Since θ⋆
¯θ minimizes the proximal subproblem and F ⋆is convex,
F ⋆ θ⋆
¯θ

+ R2
2λ ≤F ⋆ ¯θ

≤F ⋆ θ⋆
¯θ

+

∇F ⋆ ¯θ

, θ⋆
¯θ −¯θ

=⇒R2
2λ ≤
√
LR.
We now prove correctness of ApproxMoreauMinimizer.
Lemma 146. ApproxMoreauMinimizer correctly computes ˆθ satisfying (8.67) in O
  1
ϵ log 1
ϵ

calls to
Ong, with probability ≥1 −δ.
Proof. We assume throughout correctness of all calls to Ong, which follows from a union bound.
Consider some iteration 0 ≤t < T, and let ˆθt+1 := θt −ηgt be the unprojected iterate. Since
Euclidean projections decrease distances to points within a set (see e.g. Lemma 3.1, [98]), letting

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 415
gt = et + ∇F ⋆(θt) + 1
λ(θt −¯θ), where ∥et∥2 ≤Cng
√
Lϵ,
1
2
θt+1 −θ⋆
¯θ
2
2 ≤1
2
ˆθt+1 −θ⋆
¯θ

2
2 = 1
2
θt −ηgt −θ⋆
¯θ
2
2
= 1
2
θt −θ⋆
¯θ
2
2 −η

et + ∇F ⋆(θt) + 1
λ(θt −¯θ), θt −θ⋆
¯θ

+ η2
2 ∥gt∥2
2
≤1
2
θt −θ⋆
¯θ
2
2 −η
λ
θt −θ⋆
¯θ
2
2 + η ∥et∥2
θt −θ⋆
¯θ

2 + η2
2 ∥gt∥2
2
≤1
2
θt −θ⋆
¯θ
2
2 −η
λ
θt −θ⋆
¯θ
2
2 + ηCng
√
Lϵ
θt −θ⋆
¯θ

2 + 5η2L.
(8.68)
In the third line, we lower bounded

∇F ⋆(θt) + 1
λ(θt −¯θ), ¯θ −θ⋆
¯θ

by using strong convexity of
F ⋆+ 1
λ
· −¯θ
2
2; in the last line, we used the assumed bound on et as well as
∥gt∥2 ≤∥∇F ⋆(θt)∥2 + ∥et∥2 + 1
λ
θt −¯θ

2 ≤3
√
L + Cng
√
Lϵ ≤
√
10L,
for suﬃciently small ϵ. Next, consider some iteration t where iterate θt satisﬁes
θt −θ⋆
¯θ

2 ≥4Cng
√
Lϵλ.
(8.69)
On this iteration, we have from the deﬁnition of η that
ηCng
√
Lϵ
θt −θ⋆
¯θ

2 ≤η
4λ
θ −θ⋆
¯θ
2
2 , 5η2L ≤η
4λ
θ −θ⋆
¯θ
2
2 .
Plugging these bounds back into (8.68), on any iteration where (8.69) holds,
θt+1 −θ⋆
¯θ
2
2 ≤

1 −η
λ
 θt −θ⋆
¯θ
2
2 =

1 −4
5C2
ngϵ
 θt −θ⋆
¯θ
2
2 .
Because the squared distance
θt −θ⋆
¯θ
2
2 is bounded by 16Lλ2 initially and decreases by a factor of
O(ϵ) every iteration until (8.69) no longer holds, it will reach an iteration where (8.69) no longer
holds within T iterations.
Finally, by (8.68), in every iteration t after the ﬁrst where (8.69) is
violated, either the squared distance to θ⋆
¯θ goes down, or it can go up by at most
ηCng
√
Lϵ
θt −θ⋆
¯θ

2 + 5η2L = O
 ϵ2λ2L

.
Here we used our earlier claim that the distance can only go up when (8.69) is false. Thus, the
squared distance will never be more than 16C2
ngL(ϵ + O(ϵ2))λ2 within T iterations, as desired.
By using the gradient characterization in Fact 20 and the noisy gradient oracle implementation
of Corollary 38, we conclude this section with our Moreau envelope noisy gradient oracle claim.
Corollary 39. Consider a robust Lipschitz stochastic optimization instance where we have sample

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 416
access to datasets X = {Xi}i∈[n] ∈Rn×d and y = {yi}i∈[n] ∈Rn under Models 1, 2, 6 with sample
size n corresponding to Proposition 35. For
F(θ) = F ⋆
λ(θ) + µ
2 ∥θ∥2
2 ,
we can construct a (L, O(1), δ)-radiusless noisy gradient oracle in O( nd
ϵ log3(n) log2( n
δϵ) log( 1
ϵ )) time.
The sample complexity of our noisy gradient oracle is
O

log
 1
δϵ

·
d log(d/ϵ)
ϵ

.
8.10.2
Accelerated optimization of the regularized Moreau envelope
We conclude by combining Proposition 39, the smoothness bound from Fact 20, and the noisy
gradient oracle implementation of Corollary 39 to give this section's main result, Theorem 59.
Theorem 59. Under Models 1, 2, and 6, supposing ϵκ2 is suﬃciently small for κ := max(1,
1
λµ),
given θ0 ∈Rd and ∥θ0 −θ⋆
env∥2 ≤R0, RobustAccel using the noisy gradient oracle of Corollary 39
returns θ with ∥θ −θ⋆
env∥2 = O
q
κϵ
µ

with probability at least 1 −δ. The algorithm runs in
O

nd√κ
ϵ
log
 
R0
p
λ−1 + µ
ϵ
!
log3(n) log2


n log

R0
p
λ−1 + µ

δϵ

log
1
ϵ

time,
where n is the dataset size of Proposition 35. The sample complexity of the method is
O

log


log

R0
p
λ−1 + µ

δϵ

·
d log(d/ϵ)
ϵ

.
Combined with Fact 20, Theorem 59 oﬀers a range of tradeoﬀs by tuning the parameter λ: the
smaller λ is, the more the Moreau envelope resembles the original function, but the statistical and
runtime guarantees oﬀered by Theorem 59 become correspondingly weaker.
8.11
Robust sub-Gaussian principal component analysis
We consider the following statistical task, which we call robust sub-Gaussian principal component
analysis (PCA). Given samples X1, . . . , Xn from sub-Gaussian13 distribution D with covariance
Σ, an ϵ fraction of which are arbitrarily corrupted, the task asks to output unit vector u with
u⊤Σu ≥(1 −γ) ∥Σ∥∞
14 for tolerance γ. Ergo, the goal is to robustly return a (1 −γ)-approximate
13See Section 3.2 for a formal deﬁnition.
14Throughout we use ∥M∥p to denote the Schatten p-norm.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 417
top eigenvector of the covariance of sub-Gaussian D. This is the natural extension of PCA to the
robust statistics setting.
There has been a ﬂurry of recent work on eﬃcient algorithms for robust statistical tasks, e.g.
covariance estimation and PCA. From an information-theoretic perspective, sub-Gaussian concen-
tration suﬃces for robust covariance estimation. Nonetheless, to date all polynomial-time algorithms
achieving nontrivial guarantees on covariance estimation (including PCA speciﬁcally) in the pres-
ence of adversarial noise require additional algebraic structure. For instance, sum-of-squares certi-
ﬁably bounded moments have been leveraged in polynomial time covariance estimation algorithms
[275, 330]; however, this is a stronger assumption than sub-Gaussianity.
In many applications (see discussion in [177]), the end goal of covariance estimation is PCA. Thus,
a natural question which relaxes robust covariance estimation is: can we robustly estimate the top
eigenvector of the covariance Σ, assuming only sub-Gaussian concentration?
Our work answers
this question aﬃrmatively via two incomparable algorithms. The ﬁrst achieves γ = O(ϵ log ϵ−1) in
polynomial time; the second achieves γ = O(
p
ϵ log ϵ−1 log d), in nearly-linear time under a mild
gap assumption on Σ. Moreover, both methods have nearly-optimal sample complexity.
Previous work
The study of estimators robust to a small fraction of adversarial outliers dates back to foundational
work, e.g. [278, 512]. Following more recent work [337, 175], there has been signiﬁcant interest in
eﬃcient, robust algorithms for statistical tasks in high-dimensional settings. We focus on methods
robustly estimating covariance properties here, and defer a thorough discussion of the (extensive)
robust statistics literature to [492, 360, 182].
There has been quite a bit of work in understanding and giving guarantees for robust covariance
estimation where the uncorrupted distribution is exactly Gaussian [177, 178, 175, 132].
These
algorithms strongly use relationships between higher-order moments of Gaussian distributions via
Isserlis' theorem. Departing from the Gaussian setting, work of [337] showed that if the distribution
is an aﬃne transformation of a 4-wise independent distribution, robust covariance estimation is
possible. This was extended by [330], which also assumed nontrivial structure in the moments of the
distribution, namely that sub-Gaussianity was certiﬁable via the sum-of-squares proof system. To
the best of our knowledge it has remained open to give nontrivial guarantees for robust estimation
of any covariance properties under minimal assumptions, i.e. sub-Gaussian concentration.
All aforementioned algorithms also yield guarantees for robust PCA, by applying a top eigenvec-
tor method to the learned covariance. However, performing robust PCA via the intermediate covari-
ance estimation step is lossy, both statistically and computationally. From a statistical perspective,
Ω(d2) samples are necessary to learn the covariance of a d-dimensional Gaussian in Frobenius norm
(and for known eﬃcient algorithms for spectral norm error [185]); in contrast, O(d) samples suﬃce
for (non-robust) PCA. Computationally, even when the underlying distrubution is exactly Gaussian,

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 418
the best-known covariance estimation algorithms run in time Ω(d3.25); algorithms working in more
general settings based on the sum-of-squares approach require much more time. In contrast, the
power method for PCA in a d × d matrix takes time eO(d2)15. Motivated by this, our work initiates
the direct study of robust PCA, which is often independently interesting in applications.
We remark there is another problem termed "robust PCA" in the literature, e.g. [103], under a
diﬀerent generative model. We defer a detailed discussion to [177], which experimentally shows that
algorithms from that line of work do not transfer well to our corruption model.
Our results
We give two algorithms for robust sub-Gaussian PCA. Both are sample optimal, polynomial-time,
and assume only sub-Gaussianity. We follow the distribution and corruption model described in
Assumption 10.
Assumption 10 (Corruption model, see [175]). Let D be a mean-zero distribution on Rd with
covariance Σ and sub-Gaussian proxy Γ ⪯cΣ for a constant c.
Denote by index set G′ with
|G′| = n a set of (uncorrupted) samples {Xi}i∈G′ ∼D. An adversary arbitrarily replaces ϵn points
in G′; we denote the new index set by [n] = B ∪G, where B, is the (unknown) set of points added
by an adversary, and G ⊆G′ is the set of points from G′ that were not changed.
As we only estimate covariance properties, the assumption that D is mean-zero only loses con-
stants in problem parameters, by pairing samples and subtracting them (cf. [175], Section 4.5.1).
Our ﬁrst method is via a simple ﬁltering approach, as summarized in the following (and developed
in Section 8.11.1).
Theorem 60. Under Assumption 10, let δ ∈[0, 1], and n = Ω

d+log δ−1
(ϵ log ϵ−1)2

. Algorithm 94 runs in
time O( nd2
ϵ log n
δϵ log n
δ ), and outputs u with u⊤Σu > (1−C⋆ϵ log ϵ−1)∥Σ∥∞, for C⋆a ﬁxed multiple
of parameter c in Assumption 10, with probability at least 1 −δ.
Our second algorithm is more eﬃcient under mild conditions, but yields a worse approximation
1 −γ for γ = O(
p
ϵ log ϵ−1 log d). Speciﬁcally, if there are few eigenvalues of Σ larger than 1 −
γ, our algorithm runs in nearly-linear time. Note that if there are many eigenvalues above this
threshold, then the PCA problem itself is not very well-posed; our algorithm is very eﬃcient in
the interesting setting where the approximate top eigenvector is identiﬁable. We state our main
algorithmic guarantee here, and defer details to Section 8.11.2.
Theorem 61. Under Assumption 10, let δ ∈[0, 1], n = Ω

d+log δ−1
(ϵ log ϵ−1)2

, γ = C
p
ϵ log ϵ−1 log d, for
C a ﬁxed multiple of parameter c from Assumption 10, and let t ∈[d] satisfy Σt+1 < (1 −γ) ∥Σ∥∞.
Algorithm 53 outputs a unit vector u ∈Rd with u⊤Σu ≥(1 −γ)∥Σ∥∞in time eO( nd
ϵ4.5 + ndt
ϵ1.5 ).
15We say g = e
O(f) if g = O(f logc f) for some constant c > 0.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 419
We remark that Ω(dϵ−2) samples are necessary for a (1−ϵ)-approximation to the top eigenvector
of Σ via uncorrupted samples from N(0, Σ), so our ﬁrst method is sample-optimal, as is our second
up to a eO(ϵ−1) factor.
Preliminaries
General notation. [n] denotes the set 1 ≤i ≤n. Applied to a vector, ∥·∥p is the ℓp norm; applied to
a symmetric matrix, ∥·∥p is the Schatten-p norm, i.e. the ℓp norm of the spectrum. The dual norm
of ℓp is ℓq for q =
p
p−1; when p = ∞, q = 1. ∆n is the n-dimensional simplex (subset of positive
orthant with ℓ1-norm 1) and we deﬁne Sn
ε ⊆∆n to be the truncated simplex:
Sn
ε :=
(
w ∈Rn
≥0
 ∥w∥1 = 1, w ≤
1
n(1 −ε) entrywise
)
.
(8.70)
Matrices. Sd is d × d symmetric matrices, and Sd
≥0 is the positive semideﬁnite subset. I is the
identity of appropriate dimension. λmax, λmin, and Tr are the largest and smallest eigenvalues and
trace of a symmetric matrix. For M, N ∈Sd, ⟨M, N⟩:= Tr (MN) and we use the Loewner order ⪯,
(M ⪯N iﬀN −M ∈Sd
≥0). The seminorm of M ⪰0 is ∥v∥M :=
√
v⊤Mv.
Fact 21. For A, B with compatible dimension, Tr(AB) = Tr(BA). For M, N ∈Sd
≥0, ⟨M, N⟩≥0.
Fact 22. We have the following characterization of the Schatten-p norm: for M ∈Sd, and q =
p
p−1,
∥M∥p =
sup
N∈Sd, ∥N∥q=1
⟨N, M⟩.
For M = P
j∈[d] λiviv⊤
i , the satisfying N is
P
j∈[d] ±λp−1
i
viv⊤
i
∥M∥p−1
p
, so NM has spectrum
|λ|p
∥M∥p−1
p
.
Distributions. We denote drawing vector X from distribution D by X ∼D, and the covariance
Σ of D is EX∼D

XX⊤
. We say scalar distribution D is γ2-sub-Gaussian if EX∼D[X] = 0 and
EX∼D [exp (tX)] ≤exp
t2γ2
2

∀t ∈R.
Multivariate D has sub-Gaussian proxy Γ if its restriction to any unit v is ∥v∥2
Γ-sub-Gaussian, i.e.
EX∼D

exp
 tX⊤v

≤exp
 
t2 ∥v∥2
Γ
2
!
for all ∥v∥2 = 1, t ∈R.
(8.71)

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 420
8.11.1
Robust sub-Gaussian PCA via ﬁltering
In this section, we sketch the proof of Theorem 60, which gives guarantees on our ﬁltering algorithm
for robust sub-Gaussian PCA. This algorithm obtains stronger statistical guarantees than Theo-
rem 61, at the cost of super-linear runtime; the algorithm is given as Algorithm 94. Our analysis
stems largely from concentration facts about sub-Gaussian distributions, as well as the following
(folklore) fact regarding estimation of variance along any particular direction.
Lemma 147. Under Assumption 10, let δ ∈[0, 1], n = Ω

log δ−1
(ϵ log ϵ−1)2

, and u ∈Rd be a ﬁxed
unit vector. Algorithm 93, 1DRobustVariance, takes input {Xi}i∈[n], u, and ϵ, and outputs σ2
u with
|u⊤Σu −σ2
u| < Cu⊤Σu · ϵ log ϵ−1 with probability at least 1 −δ, and runs in time O(nd + n log n),
for C a ﬁxed multiple of the parameter c in Assumption 10.
In other words, we show that using corrupted samples, we can eﬃciently estimate a 1+O(ϵ log ϵ−1)-
multiplicative approximation of the variance of D in any unit direction16. This proof is deferred
to Appendix G.8 for completeness. Algorithm 94 combines this key insight with a soft ﬁltering
approach, suggested by the following known structural fact found in previous work (e.g. Lemma A.1
of [194], see also [493, 492]).
Lemma 148. Let {ai}i∈[m], {wi}i∈[m] be sets of nonnegative reals, and amax = maxi∈[m] ai. Deﬁne
w′
i =

1 −
ai
amax

wi, for all i ∈[m]. Consider any disjoint partition IB, IG of [m] with P
i∈IB wiai >
P
i∈IG wiai. Then, P
i∈IB wi −w′
i >
1
2amax
P
i∈[m] wiai > P
i∈IG wi −w′
i.
Our Algorithm 94, PCAFilter, takes as input a set of corrupted samples {Xi}i∈[n] following
Assumption 10 and the corruption parameter ϵ.
At a high level, it initializes a uniform weight
vector w(0), and iteratively operates as follows (we denote by M(w) the empirical covariance
P
i∈[n] wiXiX⊤
i ).
1. ut ←approximate top eigenvector of M(w(t−1)) via power iteration.
2. Compute σ2
t ←1DRobustVariance({Xi}i∈[n], ut, ϵ).
3. If σ2
t > (1 −O(ϵ log ϵ−1)) · u⊤
t M(w(t−1))ut, then terminate and return ut.
4. Else:
(a) Sort indices i ∈[n] by ai ←⟨ut, Xi⟩2, with a1 smallest.
(b) Let ℓ≤i ≤n be the smallest set for which Pn
i=ℓwi ≥2ϵ, and apply the downweighting
procedure of Lemma 148 to this subset of indices.
The analysis of Algorithm 94 then proceeds in two stages.
16Corollary 71 gives a slightly stronger guarantee that reusing samples does not break dependencies of u.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 421
Monotonicity of downweighting. We show the invariant criteria for Lemma 148 (namely, that for
the set ℓ≤i ≤n in every iteration, there is more spectral mass on bad points than good) holds
inductively for our algorithm. Speciﬁcally, lack of termination implies M(w(t−1)) puts signiﬁcant
mass on bad directions, which combined with concentration of good directions yields the invariant.
The details of this argument can be found as Lemma 320.
Roughly uniform weightings imply approximation quality. As Lemma 148 then applies, the pro-
cedure always removes more mass from bad points than good, and thus can only remove at most 2ϵ
mass total by the corruption model. Thus, the weights w(t) are always roughly uniform (in Sn
O(ϵ)),
which by standard concentration facts (see Appendix G.7) imply the quality of the approximate top
eigenvector is good. Moreover, the iteration count is bounded by roughly d because whenever the
algorithm does not terminate, enough mass is removed from large spectral directions. Combining
with the termination criteria imply that when a vector is returned, it is a close approximation to
the top direction of Σ. Details can be found as Lemma 322 and in the proof of Theorem 60.
8.11.2
Robust sub-Gaussian PCA in nearly-linear time
We give our nearly-linear time robust PCA method, leveraging developments of Section 7.5. Through-
out, we will be operating under Assumption 10, for some corruption parameter ϵ with ϵ log ϵ−1 log d =
O(1); ϵ = O(
1
log d log log d) suﬃces. We now develop tools to prove Theorem 61.
Algorithm 53 uses three subroutines: our earlier 1DRobustVariance method (Lemma 147), an
application of our earlier Proposition 30 to approximate the solution to
min
w∈Sn
ϵ

X
i∈[n]
wiXiX⊤
i

p
, for p = Θ
 s
log d
ϵ log ϵ−1
!
,
(8.72)
and a method for computing approximate eigenvectors by [404] (discussed in Appendix G.9).
Proposition 40. There is an algorithm Power (Algorithm 1, [404]), parameterized by t ∈[d],
tolerance ˜ϵ > 0, p ≥1, and A ∈Sd
≥0, which outputs orthonormal {zj}j∈[t] with the guarantee



z⊤
j Apzj −λp
j(A)

≤˜ϵλp
j(A)
z⊤
j Ap−1zj −λp−1
j
(A)

≤˜ϵλp−1
j
(A)
for all j ∈[t].
(8.73)
Here, λj(A) is the jth largest eigenvalue of A. The total time required by the method is O(nnz(A) tp log d
ε
).
Algorithm 53 is computationally bottlenecked by the application of Proposition 30 on Line 2 and
the t calls to 1DRobustVariance on Line 5, from which the runtime guarantee of Theorem 61 follows
straightforwardly. To demonstrate correctness, we ﬁrst certify the quality of the solution to (8.72).

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 422
Algorithm 53: RobustPCA({Xi}i∈[n], ϵ, t)
1 Input: {Xi}i∈[n] ϵ = O(
1
log d log log d), t ∈[d] with Σt+1 ≤(1 −γ)Σ for γ in Theorem 61;
2 w ←BoxedSchattenPacking (Proposition 30) on {Ai = XiX⊤
i }i∈[n], α ←ϵ, p as in (8.72);
3 M = P
i∈[n] wiXiX⊤
i ;
4 {zj}j∈[t] = Power(t, ϵ, p, M);
5 αj ←1DRobustVariance({Xi}i∈[n], M
p−1
2 zj/∥M
p−1
2 zj∥2, ϵ) for all j ∈[t];
6 Return: zj∗for j∗= argmaxj∈[t]αj;
Lemma 149. Let n = Ω

d+log δ−1
(ϵ log ϵ−1)2

. With probability 1 −δ
2, the uniform distribution over G
attains value (1+ ˜ϵ
2)∥Σ∥p for objective (8.72), where ˜ϵ = C′ϵ log ϵ−1 for a universal constant C′ > 0.
The proof of this is similar to results in e.g. [175, 360], and combines concentration guarantees
with a union bound over all possible corruption sets B. This implies the following immediately,
upon applying the guarantees of Proposition 30.
Corollary 40. Let w be the output of Line 2 of RobustPCA. Then, we have ∥w∥∞≤
1
(1−2ϵ)n, and

P
i∈[n] wiXiX⊤
i

p ≤(1 + ˜ϵ) ∥Σ∥p under the guarantee of Lemma 149.
Let w be the output of the solver. Recall that M = Pn
i=1 wiXiX⊤
i . Additionally, deﬁne
MG :=
X
i∈G
wiXiX⊤
i , wG :=
X
i∈G
wi, MB :=
X
i∈B
wiXiX⊤
i , wB :=
X
i∈G
wi .
(8.74)
Notice in particular that M = MG + MB, and that all these matrices are PSD. We next prove the
second, crucial fact, which says that MG is a good approximator to Σ in Loewner ordering:
Lemma 150. Let n = Ω

d+log δ−1
(ϵ log ϵ−1)2

. With probability at least 1 −δ
2, (1 + ˜ϵ)Σ ⪰MG ⪰(1 −˜ϵ)Σ.
The proof combines the strategy in Lemma 149 with the guarantee of the SDP solver. Perhaps
surprisingly, Corollary 40 and Lemma 150 are the only two properties about M that our ﬁnal
analysis of Theorem 61 will need. In particular, we have the following key geometric proposition,
which carefully combines trace inequalities to argue that the corrupted points MB cannot create
too many new large eigendirections.
Proposition 41. Let M = MG + MB be so that ∥M∥p ≤(1 + ˜ϵ) ∥Σ∥p, MG ⪰0 and MB ⪰0, and
so that (1 + ˜ϵ)Σ ⪰MG ⪰(1 −˜ϵ)Σ. Following notation of Algorithm 53, let
M =
X
j∈[d]
λjvjv⊤
j , Σ =
X
j∈[d]
σjuju⊤
j
(8.75)
be sorted eigendecompositions of M and Σ, so λ1 ≥. . . ≥λd, and σ1 ≥. . . ≥σd. Let γ be as in

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 423
Theorem 61, and assume σt+1 < (1 −γ)σ1. Then,
max
j∈[t] v⊤
j Σvj ≥(1 −γ) ∥Σ∥∞.
Proof. For concreteness, we will deﬁne the parameters
p = 2
7
r
log(3d)
˜ϵ
, γ = 14
p
˜ϵ log(3d) = 49p˜ϵ.
For these choices of p, γ, we will use the following (loose) approximations for suﬃciently small ˜ϵ:

1 −γ
4
p
=

1 −γ
4
 4
γ log(3d)
≤1
3d, (1 + ˜ϵ)p −(1 −˜ϵ)p ≤exp(p˜ϵ) −(1 −p˜ϵ) ≤3p˜ϵ.
(8.76)
Suppose for contradiction that all v⊤
j Σvj < (1 −γ)σ1 for j ∈[t]. By applying the guarantee of
Corollary 40 and Fact 22, it follows that

M, Mp−1
= ∥M∥p
p ≤(1 + ˜ϵ)p ∥Σ∥p
p .
(8.77)
Let s ∈[d] be the largest index such that σs >
 1 −γ
4

σ1, and note that s ≤t. We deﬁne
N :=
X
j∈[s]
λp−1
j
vjv⊤
j ⪯Mp−1.
That is, N is the restriction of Mp−1 to its top s eigendirections. Then,

M, Mp−1
=

MB, Mp−1
+

MG, Mp−1
≥

MB, Mp−1
+

(1 −˜ϵ)Σ, Mp−1
≥⟨MB, N⟩+ (1 −˜ϵ)p ∥Σ∥p
p .
(8.78)
In the second line, we used Lemma 150 twice, as well as the trace inequality Lemma 151 with A = M
and B = (1 −˜ϵ)Σ. Combining (8.77) with (8.78), and expanding the deﬁnition of MB, yields
((1 + ˜ϵ)p −(1 −˜ϵ)p) ∥Σ∥p
p ≥⟨MB, N⟩=
*
MB,
X
j∈[s]
λp−1
j
vjv⊤
j
+
=
*
M,
X
j∈[s]
λp−1
j
vjv⊤
j
+
−
*
MG,
X
j∈[s]
λp−1
j
vjv⊤
j
+
≥
*
M,
X
j∈[s]
λp−1
j
vjv⊤
j
+
−(1 + ˜ϵ)
*
Σ,
X
j∈[s]
λp−1
j
vjv⊤
j
+
=
X
j∈[s]

λp
j −(1 + ˜ϵ)λp−1
j
v⊤
j Σvj

≥
X
j∈[s]

λp
j −λp−1
j
(1 + ˜ϵ)(1 −γ)σ1

.
(8.79)

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 424
The third line followed from from the spectral bound MG ⪯(1 + ˜ϵ)Σ of Lemma 150, and the
fourth followed from the fact that {λj}j∈[d], {vj}j∈[d] eigendecompose M, as well as the assumption
v⊤
j Σvj ≤(1 −γ)σ1 for all j ∈[t]. Letting S := P
j∈[s] σp
j , and using both approximations in (8.76),
∥Σ∥p
p ≤
X
j∈[s]
σp
j +

1 −γ
4
p
(d −s)σp
1 ≤4
3S =⇒((1 + ˜ϵ)p −(1 −˜ϵ)p) ∥Σ∥p
p ≤4p˜ϵS.
(8.80)
Next, we bound the last term of (8.79). By using (1 + ˜ϵ)(1 −γ) ≤1 −γ
2 ,
X
j∈[s]

λp
j −λp−1
j
(1 + ˜ϵ)(1 −γ)σ1

≥
X
j∈[s]
λp−1
j

λj −

1 −γ
2

σ1

≥γ
6
X
j∈[s]
λp−1
j
σ1 ≥γ
6 (1 −˜ϵ)p−1 X
j∈[s]
σp
j ≥γ
12S.
(8.81)
The second line used λj −(1 −γ
2 )σ1 ≥(1 −˜ϵ)σj −(1 −γ
2 )σ1 ≥γ
6 σ1 by deﬁnition of s, Lemma 152
(twice), and (1 −˜ϵ)p−1 ≥1
2. Combining (8.81) and (8.80) and plugging into (8.79),
4p˜ϵS ≥γ
12S =⇒48p˜ϵ ≥γ.
By the choice of γ and p (i.e. γ = 49p˜ϵ), we attain a contradiction.
In the proof of Proposition 41, we used the following facts.
Lemma 151. Let A ⪰B ⪰0 be symmetric matrices and p a positive integer. Then we have
Tr
 Ap−1B

≥Tr (Bp) .
Proof. For any 1 ≤k ≤p −1,
Tr
 AkBp−k
≥Tr

Ak−1B
p−k
2 AB
p−k
2

≥Tr

Ak−1B
p−k
2 BB
p−k
2

= Tr
 Ak−1Bp−k+1
.
The ﬁrst step used the Extended Lieb-Thirring trace inequality Tr(MN2) ≥Tr(MαNM1−αN) for
α ∈[0, 1], M, N ∈Sd
≥0 (see e.g. Lemma 2.1, [19]), and the second A ⪰B. Finally, induction on k
yields the claim.
Lemma 152. For all j ∈[d], λj ≥(1 −˜ϵ)σj.
Proof. By the Courant-Fischer minimax characterization of eigenvalues,
λj ≥min
k∈[j] u⊤
k Muk.
However, we also have M ⪰MG ⪰(1 −˜ϵ)Σ (Lemma 150), yielding the conclusion.

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 425
The guarantees of Proposition 41 were geared towards exact eigenvectors of the matrix M. We
now modify the analysis to tolerate inexactness in the eigenvector computation, in line with the
processing of Line 5 of our Algorithm 53. This yields our ﬁnal claim in Theorem 61.
Corollary 41. In the setting of Proposition 41, and letting {zj}j∈[t] satisfy (8.73), set for all j ∈[t]
yj :=
M
p−1
2 zj
M
p−1
2 zj

2
.
Then with probability at least 1 −δ,
max
j∈[t] y⊤
j Σyj ≥(1 −γ) ∥Σ∥∞.
Proof. Assume all yj have y⊤
j Σyj ≤(1 −γ)σ1 for contradiction. We outline modiﬁcations to the
proof of Proposition 41. Speciﬁcally, we redeﬁne the matrix N by
N := M
p−1
2

X
j∈[s]
zjz⊤
j

M
p−1
2 .
Because P
j∈[s] zjz⊤
j
is a projection matrix, it is clear N ⪯Mp−1. Therefore, by combining the
derivations (8.77) and (8.78), it remains true that
((1 + ˜ϵ)p −(1 −˜ϵ)p) ∥Σ∥p
p ≥⟨MB, N⟩= ⟨M, N⟩−⟨MG, N⟩.
We now bound these two terms in an analogous way from Proposition 41, with negligible loss;
combining these bounds will again yield a contradiction. First, we have the lower bound
*
M,
X
j∈[s]
M
p−1
2 zjz⊤
j M
p−1
2
+
=
X
j∈[s]
z⊤
j Mpzj ≥(1 −˜ϵ)
X
j∈[s]
λp
j.
Here, the last inequality applied the assumption (8.73) with respect to Mp. Next, we upper bound
*
MG,
X
j∈[s]
M
p−1
2 zjz⊤
j M
p−1
2
+
≤(1 + ˜ϵ)
*
Σ,
X
j∈[s]
M
p−1
2 zjz⊤
j M
p−1
2
+
= (1 + ˜ϵ)
X
j∈[s]
M
p−1
2 zj

2
2 y⊤
j Σyj
≤(1 + ˜ϵ)(1 −γ)σ1
X
j∈[s]
z⊤
j Mp−1zj
≤(1 −γ)(1 + ˜ϵ)2σ1
X
j∈[s]
λp−1
j
,

CHAPTER 8. HIGH-DIMENSIONAL ROBUST STATISTICS IN ALMOST-LINEAR TIME 426
The ﬁrst line used MG ⪯(1+˜ϵ)Σ, the second used the deﬁnition of yj, the third used our assumption
y⊤
j Σyj ≤(1−γ)σ1, and the last used (8.73) with respect to Mp−1. Finally, the remaining derivation
(8.81) is tolerant to additional factors of 1 + ˜ϵ, yielding the same conclusion up to constants.
Finally, we prove Theorem 61 by combining the tools developed thus far.
Proof of Theorem 61. Correctness of the algorithm is immediate from Corollary 41 and the guaran-
tees of 1DRobustVariance. Concretely, Corollary 41 guarantees that one of the vectors we produce
will be a (1−γ)-approximate top eigenvector (say some index j ∈[t]), and 1DRobustVariance will only
lose a negligible fraction O(ϵ log ϵ−1) of this quality (see Lemma 147); the best returned eigenvector
as measured by 1DRobustVariance can only improve the guarantee. Finally, the failure probability
follows by combining the guarantees of Lemmas 147, 149, and 150.
We now discuss runtime. The complexity of lines 2, 4, and 5, as guaranteed by Proposition 30,
Proposition 40, and Lemma 147 are respectively (recalling p = eO(ϵ−0.5))
eO
 nd
ϵ4.5

, eO
ndt
ϵ1.5

, eO (ndt) .
Throughout we use that we can compute matrix-vector products in an arbitrary linear combination
of the XiX⊤
i
in time O(nd); it is easy to check that in all runtime guarantees, nnz can be replaced
by this computational cost. Combining these bounds yields the ﬁnal conclusion.

Chapter 9
Semi-Random Linear Systems
This chapter is based on [287, 319], with Arun Jambulapati, Jonathan A. Kelner, Jerry Li, Allen
Liu, Christopher Musco, and Aaron Sidford.
9.1
Organization
In this chapter, we present several related results on solving various linear systems or regression tasks
under the presence of a semi-random adversary. We will consider both the overcomplete, full-rank
case (when the number of linear measurements is at least the dimensionality of the problem, and
the solution is fully determined) as well as the undercomplete case (when the measurement matrix
is rank-deﬁcient and therefore additional structural assumptions, e.g. sparsity of the solution vector,
must be imposed). In both settings, we will investigate the curious phenomenon that the guarantees
of nearly-linear time algorithms for regression (as typically presented) do not behave well under data
augmentation. Namely, by giving additional consistent information which does not impede upon
the statistical tractability of the problem, an adversary can arbitrarily break the runtime guarantees
of ﬁrst-order methods. We propose two new robust solvers for these respective cases, which are
able to match in appropriate semi-random settings (up to logarithmic factors) the performance of
nearly-linear time algorithms succeeding under fully random, "well-conditioned" data.
Because our solution for the overcomplete case is encapsulated by a natural self-contained prob-
lem in numerical linear algebra, that of designing an optimal diagonal preconditioner for an arbitrary
full-rank matrix, we present this result ﬁrst in two self-contained sections (Sections 9.2, 9.3); the
algorithm follows fairly straightforwardly (with a few tweaks) from our earlier developments in Sec-
tion 7.4. We then give exposition and our algorithm for the undercomplete case in the later portion
of this chapter.
427

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
428
9.2
Diagonal preconditioning
We demonstrate applications of our SDP solvers (Theorems 42, 43) to a classic problem in numerical
linear algebra: how to best reduce the condition number of a PSD matrix K ∈Rd×d via a diagonal
scaling. Speciﬁcally, the goal is to ﬁnd a d × d positive diagonal matrix W that approximately
minimizes κ(W
1
2 KW
1
2 )1. Given such a W, a solution to the linear system Kx = b can then be
obtained by solving the better-conditioned system W
1
2 KW
1
2 y = W
1
2 b and returning x = W
1
2 y.
Diagonal preconditioning has been studied since at least the 1950s [225, 518, 445]. Since the high
computational cost of a generic SDP solver outweighs any computational beneﬁts from reducing the
condition number of K, until our work ﬁnding a near optimal diagonal W has has not been feasible
[448]. Instead, prior work mostly studies heuristics like Jacobi preconditioning, which simply chooses
W to be the inverse-diagonal of K, so W
1
2 KW
1
2 has a constant diagonal [369, 225]2. Another
approach is to choose W to minimize ∥I −W
1
2 KW
1
2 ∥F [254, 71].
We also note that diagonal
preconditioning bears superﬁcial resemblance to the matrix scaling problem, which has received
recent attention in theoretical computer science [552, 145, 234]. The goal in matrix scaling is to
ﬁnd a diagonal scaling that equalizes the row and column norms of K. While sometimes eﬀective
as a heuristic [322], since they target a diﬀerent objective, scaling algorithms do not yield provable
guarantees on κ(W
1
2 KW
1
2 ).
In contrast to prior work on eﬃcient algorithms, our main result yields a provably near optimal
result for any K. Speciﬁcally, letting κ⋆
o(K) := mindiagonal W⪰0 κ(W
1
2 KW
1
2 ), we show that:
Theorem 62. Let ϵ > 0 be a ﬁxed constant.3 There is an algorithm, which given full-rank K ∈Sd
≻0
computes w ∈Rd
≥0 such that κ(W
1
2 KW
1
2 ) ≤(1 + ϵ)κ⋆
o(K) with probability ≥1 −δ in time
O

Tmv(K) · (κ⋆
o(K))1.5 · log7
dκ⋆
o(K)
δ

.
Importantly the runtime in Theorem 62 does not depend on the original condition number,
κ(K). Instead, it scales with the optimal κ⋆
o(K), meaning that there is only a small computational
overhead in comparison to the cost of solving the linear system after preconditioning, which is is
˜O(Tmv(K) ·
p
κ⋆o(K)) using conjugate gradient or accelerate gradient descent. Roughly, whenever
κ(K) ≥(κ⋆
o(K))3, we obtain a faster linear system solver for K. This has applications to e.g. solving
least squares regression problems of the form minx ∥Ax −b∥2. Solving the least squares problem
1The symmetry of the rescaling W
1
2 KW
1
2 is important, as it preserves the symmetry and positive deﬁniteness of
K, which is required for iterative solvers like the conjugate gradient method.
2A well-known result of van der Sluis [518, 251] proves that the Jacobi preconditioner gives an m-factor approx-
imation to the optimal preconditioning problem where m ≤d is the maximum number of non-zeros in any row of
K. We review and slightly strengthen this result in Appendix H.1.3. We also prove a new dimension-independent
baseline result that may be of independent interest: the Jacobi preconditioner always obtains condition number no
worse than (min W
1
2 KW
1
2 )2.
3We do not focus on the ϵ dependence and instead take it to be constant since, in applications involving solving
linear systems, there is little advantage to obtaining better than a two factor approximation (i.e. setting ϵ = 1).

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
429
requires solving a linear system in the PSD matrix K = A⊤A. In this setting, Tmv(K) = O(nnz(A)),
where nnz(A) denotes the number of non-zeros in A.
A result analogous to Theorem 62 but depending quadratically on κ⋆
o(K)) can be obtained by
directly applying Theorem 43 with n = d, Mi = eie⊤
i , κ = κ⋆
o(K), and B =
1
κK (i.e. using the
dictionary of 1-sparse diagonal matrices to approximate K). We obtain an improved (κ⋆
o(K))1.5
dependence via another homotopy method (similar to the one used for our SDP solver), which
allows us to eﬃciently compute matrix-vector products with a symmetric square-root of K. Access
to the square root allows us to reduce the iteration complexity of our SDP solver.
In addition to standard diagonal preconditioning, which is an "outer scaling" problem, we also
introduce an analogous "inner scaling" problem. Given a full rank n × d matrix A with n ≥d,
the inner scaling problem asks to ﬁnd an n × n nonnegative diagonal matrix W that approximately
minimizes κ
 A⊤WA

. Let κ⋆
i (A) := mindiagonal W⪰0 κ
 A⊤WA

denote the minimum (inner)-
rescaled condition number. In contrast to the outer scaling problem applied e.g. to K = A⊤A,
where the goal is to rescale columns of A by W
1
2 , the inner scaling problem asks to rescale A's
rows. To the best of our knowledge, this problem has not been studied previously. Directly applying
Theorem 42 with κ = κ⋆
i (A) and Mi = aia⊤
i for all i ∈[n] yields the following.
Theorem 63. Let ϵ > 0 be a ﬁxed constant. There is an algorithm, which given full-rank A ∈Rn×d
for n ≥d computes w ∈Rn
≥0 such that κ(A⊤WA) ≤(1 + ϵ)κ⋆
i (A) with probability ≥1 −δ in time
O

Tmv(A) · (κ⋆
i (A))1.5 · log6
nκ⋆
i (A)
δ

.
A natural open question is if the κ⋆
o(K) dependence in Theorem 62 (and the corresponding
dependence in Theorem 63) can be reduced even further, ideally to
p
κ⋆o(K). This would match
the most eﬃcient linear system solvers in K under diagonal rescaling, if the best known rescaling
was known in advance. We prove in Appendix H.1.4 that if a suﬃciently approximation-tolerant
width-independent variant of Theorem 42 is developed, it can be used to achieve such a runtime for
both the classical outer scaling problem, and the inner scaling problem we introduce. We also give
generalizations to ﬁnding rescalings which minimize natural average notions of conditioning, under
existence of such a conjectured solver.
9.2.1
Inner scaling
We prove Theorem 63 on computing constant-factor optimal inner scalings.
Proof of Theorem 63. We apply Theorem 42 with κ = κ⋆
i (A) and Mi = aia⊤
i for all i ∈[n], where
rows of A are denoted {ai}i∈[n]. The deﬁnition of κ⋆
i (A) and A⊤WA = P
i∈[n] wiaia⊤
i
(where
W = diag (w)) implies that (7.7) is feasible for these parameters: namely, there exists w⋆∈Rn
≥0

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
430
such that
I ⪯
X
i∈[n]
wiMi ⪯κI.
Moreover, factorizations (7.9) hold with Vi = ai and m = 1. Note that Tmv({Vi}i∈[n]) = O(nnz(A))
since this is the cost of multiplication through all rows of A.
9.2.2
Outer scaling
In this section, we prove a result on computing constant-factor optimal outer scalings of a matrix
K ∈Sd
≻0. We ﬁrst remark that we can obtain a result analogous to Theorem 63, but which scales
quadratically in the κ⋆
o(K), straightforwardly by applying Theorem 43 with n = d, Mi = eie⊤
i for
all i ∈[d], κ = κ⋆
o(K), and B =
1
κK. This is because the deﬁnition of κ⋆
o implies there exists
w⋆∈Rd
≥0 with I ⪯(W⋆)−1
2 K(W⋆)−1
2 ⪯κI, where W⋆= diag (w⋆) and where this w⋆is the
entrywise inverse of the optimal outer scaling attaining κ⋆
o(K). This then implies
1
κK ⪯
X
i∈[d]
w⋆
i Mi ⪯K,
since P
i∈[d] w⋆
i Mi = W⋆and hence Theorem 43 applies, obtaining a runtime for the outer scaling
problem of roughly V(K) · κ⋆
o(K)2 (up to logarithmic factors).
We give an alternative approach in this section which obtains a runtime of roughly Tmv(K) ·
κ⋆
o(K)1.5, matching Theorem 63 up to logarithmic factors. Our approach is to deﬁne
A := K
1
2
to be the positive deﬁnite square root of K; we use this notation throughout the section, and let
{ai}i∈[d] be the rows of A. We cannot explicitly access A, but if we could, directly applying The-
orem 63 suﬃces because κ(W
1
2 KW
1
2 ) = κ(W
1
2 A2W
1
2 ) = κ(AWA) for any nonnegative diagonal
W ∈Rd×d. We show that by using a homotopy method similar to the one employed in Section 7.4.2,
we can implement this strategy with only a polylogarithmic runtime overhead. At a high level, the
improvement from Theorem 43 is because we have explicit access to K = A2. By exploiting can-
cellations in polynomial approximations we can improve the cost of iterations of Algorithm 29 from
roughly κ (where one factor of √κ comes from the cost of rational approximations to square roots
in Section 7.4.2, and the other comes from the degree of polynomials), to roughly √κ.
Finally, throughout this section we will assume κ(A) ≤κ⋆
o(K), which is without loss of generality
by rescaling based on the diagonal (Jacobi preconditioning), as we show in Appendix H.1.3. Also,
for notational convenience we will ﬁx a matrix K ∈Sd
≻0 and denote κ⋆:= κ⋆
o(K).

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
431
Preliminaries
We ﬁrst state a number of preliminary results which will be used in buliding our outer scaling
method. We begin with a polynomial approximation to the square root, proven in Appendix H.1.
It yields a corollary regarding approximating matrix-vector products with a matrix square root.
Fact 23 (Polynomial approximation of √·). Let M ∈Sd
≻0 have µI ⪯M ⪯κµI where µ is known.
Then for any δ ∈(0, 1), there is an explicit polynomial p of degree O(√κ log κ
δ ) with
(1 −δ)M
1
2 ⪯p(M) ⪯(1 + δ)M
1
2 .
Corollary 42. For any vector b ∈Rd, δ, ϵ ∈(0, 1), and M ∈Sd
≻0 with κ(M) ≤κ, with probability
≥1 −δ we can compute u ∈Rd such that
u −M
1
2 b

2 ≤ϵ
M
1
2 b

2 in time O

Tmv (M) ·
√κ log κ
ϵ + log d
δ

.
Proof. First, we compute a 2-approximation to µ in Fact 23 within the runtime budget using the
power method (Fact 40), since κ is given. This will only aﬀect parameters in the remainder of the
proof by constant factors. If u = Pb for commuting P and M, our requirement is equivalent to
−ϵ2M ⪯

P −M
1
2
2
⪯ϵ2M.
Since square roots are operator monotone (by the L¨owner-Heinz inequality), this is true iﬀ
−ϵM
1
2 ⪯P −M
1
2 ⪯ϵM
1
2 ,
and such a P which is applicable within the runtime budget is given by Fact 23.
We next demonstrate two applications of Corollary 42 in estimating applications of products
involving A = K
1
2 , where we can only explicitly access K. We will use the following standard fact
about operator norms, whose proof is deferred to Appendix H.1.
Lemma 153. Let B ∈Rd×d and let A ∈Sd
≻0. Then min (∥AB∥2 , ∥BA∥2) ≥
1
κ(A) ∥B∥2 ∥A∥2.
First, we discuss the application of a bounded-degree polynomial in AWA to a uniformly random
unit vector, where W is an explicit nonnegative diagonal matrix.
Lemma 154. Let u ∈Rd be a uniformly random unit vector, let K ∈Sd
≻0 such that A := K
1
2 and
κ(K) ≤κ, and let P be a degree-∆polynomial in AWA for an explicit diagonal W ∈Sd
≥0. For
δ, ϵ ∈(0, 1), with probability ≥1 −δ we can compute w ∈Rd so ∥w −Pu∥2 ≤ϵ ∥Pu∥2 in time
O

Tmv(K) ·

∆+ √κ log dκ
δϵ

.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
432
Proof. We can write P = ANA for some matrix N which is a degree-O(∆) polynomial in K and W,
which we have explicit access to. Standard concentration bounds show that with probability at least
1−δ, for some N = poly(d, δ−1), ∥Pu∥2 ≥1
N ∥P∥2. Condition on this event for the remainder of the
proof, such that it suﬃces to obtain additive accuracy
ϵ
N ∥P∥2. By two applications of Lemma 153,
we have
∥ANA∥2 ≥1
κ ∥A∥2
2 ∥N∥2 .
(9.1)
Our algorithm is as follows: for ϵ′ ←
ϵ
3Nκ, compute v such that ∥v −Au∥2 ≤ϵ′ ∥A∥2 ∥u∥2 using
Corollary 42, explicitly apply N, and then compute w such that ∥w −ANv∥2 ≤ϵ′ ∥A∥2 ∥Nv∥2; the
runtime of this algorithm clearly ﬁts in the runtime budget. The desired approximation is via
∥w −ANAu∥2 ≤∥w −ANv∥2 + ∥ANv −ANAu∥2
≤ϵ′ ∥A∥2 ∥N∥2 ∥v∥2 + ∥ANv −ANAu∥2
≤2ϵ′ ∥A∥2
2 ∥N∥2 + ∥A∥2 ∥N∥2 ∥v −Au∥2
≤2ϵ′ ∥A∥2
2 ∥N∥2 + ϵ′ ∥A∥2
2 ∥N∥2
≤3ϵ′κ ∥ANA∥2 = ϵ
N ∥P∥2 .
The third inequality used ∥v∥2 ≤∥Au∥2 + ϵ ∥A∥2 ∥u∥2 ≤(1 + ϵ) ∥A∥2 ≤2 ∥A∥2.
We give a similar guarantee for random bilinear forms through A involving an explicit vector.
Lemma 155. Let u ∈Rd be a uniformly random unit vector, let K ∈Sd
≻0 such that A := K
1
2 and
κ(K) ≤κ, and let v ∈Rd. For δ, ϵ ∈(0, 1), with probability ≥1 −δ we can compute w ∈Rd so
⟨w, v⟩is an ϵ-multiplicative approximation to u⊤Av in time
O

Tmv(K) · √κ log dκ
δϵ

.
Proof. As in Lemma 154, for some N = poly(d, δ−1) it suﬃces to give a
ϵ
N ∥Av∥2-additive approxi-
mation. For ϵ′ ←
ϵ
N√κ, we apply Corollary 42 to obtain w such that ∥w −Au∥2 ≤ϵ′ ∥Au∥2, which
ﬁts within the runtime budget. Correctness follows from
|⟨Au −w, v⟩| ≤∥Au −w∥2 ∥v∥2 ≤ϵ′ ∥A∥2 ∥v∥2 ≤ϵ′√κ ∥Av∥2 ≤ϵ
N ∥Av∥2 .
Implementing Algorithm 29 implicitly
In this section, we bound the complexity of Algorithm 29 in the following setting. Throughout this
section denote Mi = aia⊤
i
for all i ∈[d], where A ∈Sd
≻0 has rows {ai}i∈[d], and A2 = K. We

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
433
assume that
κ(K) ≤κscale := 3κ⋆,
and we wish to compute a reweighting w ∈Rd
≥0 such that
κ

X
i∈[d]
wiaia⊤
i

= κ

W
1
2 KW
1
2

≤(1 + ϵ)κ⋆,
assuming there exists a reweighting w⋆∈Rd
≥0 such that above problem is feasible with conditioning
κ⋆. In other words, we assume we start with a matrix whose conditioning is within a 3-factor of the
optimum after rescaling, and wish to obtain a 1 + ϵ-approximation to the optimum. We show in the
next section how to use a homotopy method to reduce the outer scaling problem to this setting.
Our strategy is to apply the method of Theorem 63.
To deal with the fact that we cannot
explictly access the matrix A, we give a custom analysis of the costs of Lines 6, 7, and 13 under
implicit access in this section, and prove a variant of Theorem 63 for this speciﬁc setting.
Estimating the smallest eigenvalue implicitly. We begin by discussing implicit implementation
of Line 13. Our strategy combines the approach of Lemma 101 (applying the power method to the
negative exponential), with Lemma 154 since to handle products through random vectors.
Lemma 156. Given δ ∈(0, 1), constant ϵ > 0, K ∈Sd
≻0 such that A := K
1
2 and κ(K) ≤κscale,
and diagonal W ∈Sd
≥0 such that M := AWA ⪯O(κscale log d)I, we can compute a O(log d)-additive
approximation to λmin(M) with probability ≥1 −δ in time
O

Tmv(K) · √κscale · log2 dκscale
δ

.
Proof. The proof of Lemma 101 implies it suﬃces to compute a 0.2-multiplicative approximation
to the largest eigenvalue of P, a degree-∆= O(√κscale log d) polynomial in M. Moreover, letting
∆′ = O(log d
δ ) be the degree given by Fact 40 with δ ←δ
3, the statement of the algorithm in Fact 40
shows it suﬃces to compute for a uniformly random unit vector u,
P∆u

2 and
P∆+1u

2 to multiplicative accuracy 1
30.
We demonstrate how to compute
P∆u

2 to this multiplicative accuracy with probability at least 1−
δ
3; the computation of
P∆+1u

2 is identical, and the failure probability follows from a union bound
over these three random events. Since P∆is a degree-O(∆∆′) = O(√κscale log d log d
δ ) polynomial
in AWA, the conclusion follows from Lemma 154.
Estimating inner products with a negative exponential implicitly. We next discuss implicit im-
plementation of Line 6. In particular, we give variants of Lemmas 97 and 98 which are tolerant to
implicit approximate access of matrix-vector products.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
434
Lemma 157. Given δ ∈(0, 1), constant ϵ > 0, K ∈Sd
≻0 such that A := K
1
2 and κ(K) ≤κscale,
and diagonal W ∈Sd
≥0 such that M := AWA ⪯O(κscale log d)I and λmin(M) = O(log d), we can
compute an ϵ-multiplicative approximation to Tr exp(−M) with probability ≥1 −δ in time
O

Tmv(K) · √κscale · log2 dκscale
δ

.
Proof. The proof of Lemma 97 shows it suﬃces to compute k = O(log d
δ ) times, an
ϵ
3 exp(−R)-
additive approximation to u⊤Pu where R = O(log d), for uniformly random unit u and P, a
degree-∆= O(√κscale log d)-polynomial in M with ∥P∥2 ≤∥exp(−M)∥2 + ϵ
3 exp(−R) ≤4
3 exp(−R).
Applying Lemma 154 with ϵ ←ϵ
4 to compute w, an approximation to Pu, the approximation follows:
|⟨w, u⟩−⟨Pu, u⟩| ≤∥w −Pu∥2 ≤ϵ
4 ∥P∥2 ≤ϵ
3 exp(−R).
The runtime follows from the cost of applying Lemma 154 to all k random unit vectors.
Lemma 158. Given δ ∈(0, 1), constant ϵ > 0, K ∈Sd
≻0 such that A := K
1
2 and κ(K) ≤κscale,
and diagonal W ∈Sd
≥0 such that M := AWA ⪯O(κscale log d)I, we can compute (ϵ, O(
1
κscaled))-
approximations to all

aia⊤
i , exp(−M)
	
i∈[d] ,
with probability ≥1 −δ in time
O

Tmv(K) · √κscale · log2 dκscale
δ

.
Proof. The proof of Lemma 98 implies it suﬃces to compute k = O(log d
δ ) times, for each i ∈[d],
the quantity ⟨u, Pai⟩to multiplicative error ϵ
2, for uniformly random unit vector u and P, a degree-
∆= O(√κscale log(κscaled))-polynomial in M. Next, note that since ai = Aei and P = ANA for
N an explicit degree-O(∆) polynomial in K and W, we have ⟨u, Pai⟩= u⊤A (NKei). We can
approximate this by some ⟨w, NKei⟩via Lemma 155 to the desired accuracy. The runtime comes
from applying Lemma 155 k times, multiplying each of the resulting vectors w by KN and stacking
them to form a k × d matrix eQ, and then computing all ∥eQei∥2 for i ∈[d].
Implementing a packing oracle implicitly. Finally, we discuss implementation of Line 7 of Algo-
rithm 29. The requirement of Line 7 is a multiplicative approximation (and a witnessing reweighting)
to the optimization problem
max
P
i∈[d] wiMi⪯I
w∈Rd
≥0
v⊤w.
Here, v is explicitly given by an implementation of Line 6 of the algorithm, but we do not have
{Mi}i∈[d] explicitly. To implement this step implicitly, we recall the approximation requirements of

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
435
the solver of Proposition 26, as stated in Section 7.5. We remark that the approximation tolerance
is stated for the decision problem tester of Section 7.5 (Proposition 66); once the tester is implicitly
implemented, the same reduction as described in Appendix F.2 yields an analog to Proposition 26.
Corollary 43 (Approximation tolerance of Proposition 26). Let ϵ > 0 be a ﬁxed constant. The
runtime of Proposition 26 is due to T = O(log( d
δ ) log d · log log OPT+
OPT−) iterations, each of which
requires O(1) vector operations and O(ϵ)-multiplicative approximations to
Tr (Mp) ,

Ai, Mp−1	
i∈[d] for M :=
X
i∈[d]
wiMi for an explicitly given w ∈Rd
≥0,
(9.2)
where p = O(log d) ∈N is odd, and SI ⪯M ⪯RI, for R = O(log d) and S = poly( 1
nd, κ((P
i∈[n] Mi))−1).
We remark that the lower bound S comes from the fact that the initial matrix of the solver of
Section 7.5 is a bounded scaling of P
i∈[n] Mi, and the iterate matrices are monotone in Loewner
order. We now demonstrate how to use Lemmas 154 and 155 to approximate all quantities in (9.2).
Throughout the following discussion, we specialize to the case where each Mi = aia⊤
i , so M in (9.2)
will always have the form M = AWA for diagonal W ∈Sd
≥0, and S = poly((dκscale)−1).
Lemma 159. Given δ ∈(0, 1), constant ϵ > 0, K ∈Sd
≻0 such that A := K
1
2 and κ(K) ≤κscale,
and diagonal W ∈Sd
≥0 such that SI ⪯M := AWA ⪯O(log d)I where S = poly((dκscale)−1), we
can compute an ϵ-multiplicative approximation to Tr(Mp) for integer p in time
O

Tmv(K) ·

p + √κscale log dκscale
δ

· log d
δ

.
Proof. As in Lemma 157, it suﬃces to compute k = O(log d
δ ) times, an
ϵ
N Sp-additive approximation
to u⊤Mpu, for uniformly random unit vector u and N = poly(d, δ−1). By applying Lemma 154 with
accuracy ϵ′ ←
ϵSp
NRp to obtain w, an approximation to Mpu, we have the desired
|⟨u, Mpu⟩−⟨u, w⟩| ≤∥Mpu −w∥2 ≤ϵ′ ∥Mpu∥2 ≤ϵ′Rp ≤ϵ
N Sp.
The runtime follows from k applications of Lemma 154 to the speciﬁed accuracy level.
Lemma 160. Given δ ∈(0, 1), constant ϵ > 0, K ∈Sd
≻0 such that A := K
1
2 and κ(K) ≤κscale,
and diagonal W ∈Sd
≥0 such that M := AWA ⪯O(log d)I, we can compute an ϵ-multiplicative
approximation to all

aia⊤
i , Mp−1	
i∈[d] where {ai}i∈[d] are rows of A,

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
436
where p is an odd integer, with probability ≥1 −δ in time
O

Tmv(K) ·

p + √κscale log dκscale
δ

· log d
δ

.
Proof. First, observe that for all i ∈[d] it is the case that

aia⊤
i , Mp−1
= (Aei)⊤Mp−1 (Aei) ≥Sp−1 ∥A∥2
2 κ−2
scale.
Letting r = 1
2(p −1) and following Lemma 158 and the above calculation, it suﬃces to show how to
compute k = O(log d
δ ) times, for each i ∈[d], the quantity ⟨u, Mrai⟩to multiplicative error ϵ
2, for
uniformly random unit vector u and N = poly(d, δ−1). As in Lemma 158, each such inner product
is u⊤A(NKei) for N an explicit degree-O(p) polynomial in K and W. The runtime follows from
applying Lemma 155 k times and following the runtime analysis of Lemma 158.
Putting it all together.
Finally, we state our main result of this section, regarding rescaling
well-conditioned matrices, by combining the pieces we have developed.
Corollary 44. Given δ ∈(0, 1), constant ϵ > 0, K ∈Sd
≻0 such that κ(K) ≤3κ⋆, and such that
κ⋆
o(K) = κ⋆, we can compute diagonal W ∈Sd
≥0 such that κ(W
1
2 KW
1
2 ) ≤(1+ϵ)κ⋆with probability
≥1 −δ in time
O

Tmv(K) · (κ⋆)1.5 · log6
dκ⋆
δ

.
Proof. The proof is essentially identical to the proof of Theorem 63 by way of Theorem 42. In
particular, we parameterize Theorem 42 with Mi = aia⊤
i
where A = K
1
2 is the positive deﬁnite
square root of K with rows {ai}i∈[d]. Then, running Algorithm 29 with an incremental search for
the optimal κ⋆yields an overhead of eO(κ⋆log(dκ⋆)). The cost of each iteration of Algorithm 29
follows by combining Lemmas 156, 157, 158, 159, 160, and Corollary 43.
Homotopy method
In this section, we use Corollary 44, in conjunction with a homotopy method similar to that of
Section 7.4.2, to obtain our overall algorithm for outer scaling. We state here three simple helper
lemmas which follow almost identically from corresponding helper lemmas in Section 7.4.2; we
include proofs of these statements in Appendix H.1 for completeness.
Lemma 161. For any matrix K ∈Sd
≻0 and λ ≥0, κ⋆
o(K + λI) ≤κ⋆
o(K).
Lemma 162. Let K ∈Sd
≻0. Then, for λ ≥1
ϵ λmax(K), κ(K+λI) ≤1+ϵ. Moreover, given a diagonal
W ∈Sd
≥0 such that κ(W
1
2 (K+λI)W
1
2 ) ≤κscale for 0 ≤λ ≤ϵλmin(K)
1+ϵ
, κ(W
1
2 KW
1
2 ) ≤(1+ϵ)κscale.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
437
Lemma 163. Let K ∈Sd
≻0, and let W ∈Sd
≥0 be diagonal. Then for any λ > 0,
κ

W
1
2 (K + λI) W
1
2

≤2κ

W
1
2

K + λ
2 I

W
1
2

.
Theorem 62. Let ϵ > 0 be a ﬁxed constant.4 There is an algorithm, which given full-rank K ∈Sd
≻0
computes w ∈Rd
≥0 such that κ(W
1
2 KW
1
2 ) ≤(1 + ϵ)κ⋆
o(K) with probability ≥1 −δ in time
O

Tmv(K) · (κ⋆
o(K))1.5 · log7
dκ⋆
o(K)
δ

.
Proof. We will assume we know the correct value of κ⋆
o(K) up to a 1 + O(ϵ) factor throughout this
proof for simplicity, and call this estimate κ⋆. This will add an overall multiplicative overhead of
O(1) by using an incremental search as in Theorem 42. We will also assume that κ(K) = O((κ⋆)2)
by ﬁrst applying the Jacobi preconditioner; see Appendix H.1.3 for a proof.
Our algorithm follows the framework of Section 7.4.2 and runs in phases indexed by k for 0 ≤
k ≤K for some K, each computing a scaling of K + λkI with condition number (1 + ϵ)κ⋆; note that
a scaling with condition number κ⋆is always feasible for any λk ≥0 by Lemma 161. We will deﬁne
λ0 = 1
ϵ V where V is a constant-factor overestimate of λmax(K), which can be obtained by Fact 40
without dominating the runtime. We will then set
λk = λ0
2k , K = O (log κ⋆) .
Lemma 162 shows that we have a trivial scaling attaing condition number (1 + ϵ)κ⋆for K + λ0I,
and that if we can compute rescalings for all λk where 1 ≤k ≤K, then the last rescaling is also a
(1 + ϵ)κ⋆-conditioned rescaling for K up to adjusting ϵ by a constant.
Finally, we show how to implement each phase of the algorithm, given access to the reweighting
from the previous phase. Note that Lemma 163 shows that the reweighting W computed in phase
k yields a rescaling W
1
2 (K + λk+1I)W
1
2 which is 3κ⋆-conditioned. By running the algorithm of
Corollary 44 on K ←W
1
2 (K + λk+1I)W
1
2 , we compute the desired reweighting for phase k + 1.
The ﬁnal runtime loses one logarithmic factor over Corollary 44 due to running for K phases.
9.3
Overcomplete semi-random linear systems
Unlike a diagonal preconditioner, a good inner scaling does not yield a faster solution to a given
least squares regression problem minx ∥Ax −b∥2.
Instead, it allows for a faster solution to the
weighted least squares problem minx ∥W
1
2 Ax −W
1
2 b∥2. It turns out that this has a number of
interesting applications.
Speciﬁcally, we now explore an interesting connection to semi-random
4We do not focus on the ϵ dependence and instead take it to be constant since, in applications involving solving
linear systems, there is little advantage to obtaining better than a two factor approximation (i.e. setting ϵ = 1).

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
438
noise models for least-squares regression. As a motivating example, consider the case when there
is a hidden parameter vector xtrue ∈Rd that we want to recover, and we have a "good" set of
consistent observations Agxtrue = bg, in the sense that κ(A⊤
g Ag) is small. Here, we can think of Ag
as being drawn from a well-conditioned distribution. Now, suppose an adversary gives us a superset
of these observations (A, b) such that Axtrue = b, and Ag are an (unknown) subset of rows of A,
but κ(A⊤A) ≫κ(A⊤
g Ag). Perhaps counterintuitively, by giving additional consistent data, the
adversary can arbitrarily hinder the cost of iterative methods for ﬁnding xtrue. Our inner scaling
methods can be viewed as a way of "robustifying" linear system solvers to such noise models (ﬁnding
W which yields a rescaled condition number that is at least as good as the indicator of the rows of
Ag, which are not known a priori). We also demonstrate additional applications in reducing risk in
more general statistical regression settings.
The semi-random noise model we introduce for linear system solving follows a line of work
originating in [85] for graph coloring.
A semi-random model consists of an (unknown) planted
instance which a classical algorithm performs well against, augmented by additional information
given by a "monotone" or "helpful" adversary masking the planted instance. Conceptually, when
an algorithm fails given this "helpful" information, the algorithm may have overﬁt to its problem
speciﬁcation. This model has been studied in various statistical settings [294, 220, 219, 397, 380]. Of
particular relevance to our work, which studies robustness to semi-random noise in the context of fast
algorithms (as opposed to the distinction between polynomial-time algorithms and computational
intractability) is [133], which developed an algorithm for matrix completion under semi-random noise
extending work of [353].
9.3.1
Semi-random linear systems
Consider the following semi-random noise model for solving an overdetermined, consistent linear
system Axtrue = b where A ∈Rn×d for n ≥d.
Deﬁnition 35 (Semi-random linear systems). In the semi-random noise model for linear systems,
a matrix Ag ∈Rm×d with κ(A⊤
g Ag) = κg, m ≥d is "planted" as a subset of rows of a larger matrix
A ∈Rn×d. We observe the vector b = Axtrue for some xtrue ∈Rd we wish to recover.
We remark that we call the model in Deﬁnition 35 "semi-random" because of the following
motivating example: the rows Ag are feature vectors drawn from some "nice" (e.g. well-conditioned)
distribution, and the dataset is contaminated by an adversary supplying additional data (a priori
indistinguishable from the "nice" data), aiming to hinder conditioning of the resulting system.
Interestingly, Deﬁnition 35 demonstrates in some sense a shortcoming of existing linear system
solvers: their brittleness to additional, consistent information. In particular, κ(A⊤A) can be ar-
bitrarily larger than κg. However, if we were given the indices of the subset of rows Ag, we could

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
439
instead solve the linear system bg = Agxtrue with iteration count dependent on the condition num-
ber of Ag. Counterintuitively, by giving additional rows, the adversary can arbitrarily increase the
condition number of the linear system, hindering the runtime of conditioning-dependent solvers.
The inner rescaling algorithms we develop in Section 9.2 are well-suited for robustifying linear
system solvers to the type of adversary in Deﬁnition 35. In particular, note that
κ⋆
i (A) ≤κ
 A⊤WgA

= κ
 A⊤
g Ag

= κg,
where Wg is the diagonal matrix which is the 0-1 indicator of rows of Ag. Our solvers for reweightings
approximating κ⋆
i can thus be seen as trading oﬀthe sparsity of Ag for the potential of "mixing
rows" to attain a runtime dependence on κ⋆
i (A) ≤κg. In particular, our resulting runtimes scale
with nnz(A) instead of nnz(Ag), but also depend on κ⋆
i (A) rather than κg.
We remark that the other solvers we develop are also useful in robustifying against variations on
the adversary in Deﬁnition 35. For instance, the adversary could instead aim to increase τ(A⊤A),
or give additional irrelevant features (i.e. columns of A) such that only some subset of coordinates
xg are important to recover. For brevity, we focus on the model in Deﬁnition 35 in this work.
9.3.2
Statistical linear regression
The second application we give is in solving noisy variants of the linear system setting of Deﬁnition 35.
In particular, we consider statistical regression problems with various generative models.
Deﬁnition 36 (Statistical linear regression). Given full rank A ∈Rn×d and b ∈Rd produced via
b = Axtrue + ξ, ξ ∼N(0, Σ),
(9.3)
where we wish to recover unknown xtrue ∈Rd, return x so that (where expectations are taken over
the randomness of ξ) the risk (mean-squared error) E[∥x −xtrue∥2
2] is small.
In this section, we deﬁne a variety of generative models (i.e. specifying a covariance matrix Σ of
the noise) for the problem in Deﬁnition 36. For each of the generative models, applying our rescaling
procedures will yield computational gains, improved risk bounds, or both. We give statistical and
computational results for statistical linear regression in both the homoskedastic and heteroskedastic
settings. In particular, when Σ = σ2I (i.e. the noise for every data point has the same variance),
this is the well-studied homoskedastic setting pervasive in stastical modeling. When Σ varies with
the data A, the model is called heteroskedastic (cf. [252]).
In most cases, we do not directly give guarantees on exact mean squared errors via our prepro-
cessing, but rather certify (possibly loose) upper bound surrogates. We leave direct certiﬁcation of
conditioning and risk simultaneously without a surrogate bound as an interesting future direction.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
440
Heteroskedastic statistical guarantees
We specify two types of heteroskedastic generative models (i.e. deﬁning the covariance Σ in (9.3)),
and analyze the eﬀect of rescaling a regression data matrix on reducing risk.
Noisy features.
Consider the setting where the covariance in (9.3) has the form Σ = AΣ′A⊤,
for matrix Σ′ ∈Sd
≥0. Under this assumption, we can rewrite (9.3) as b = A(xtrue + ξ′), where
ξ′ ∼N(0, Σ′). Intuitively, this corresponds to exact measurements through A, under noisy features
xtrue + ξ′. As in this case b ∈Im(A) always, regression is equivalent to linear system solving, and
thus directly solving any reweighted linear system W
1
2 Ax∗= W
1
2 b will yield x∗= xtrue + ξ′.
We thus directly obtain improved computational guarantees by computing a reweighting W
1
2
with κ(A⊤WA) = O(κ⋆
i (A)). Moreover, we note that the risk (Deﬁnition 36) of the linear system
solution x∗is independent of the reweighting:
E
h
∥x∗−xtrue∥2
2
i
= E
h
∥ξ′∥2
2
i
= Tr
 Σ′
.
Hence, computational gains from reweighting the system are without statistical loss in the risk.
Row norm noise.
Consider the setting where the covariance in (9.3) has the form
Σ = σ2diag
n
∥ai∥2
2
o
i∈[n]

.
(9.4)
Intuitively, this corresponds to the setting where noise is independent across examples and the size
of the noise scales linearly with the squared row norm. We ﬁrst recall a standard characterization
of the regression minimizer.
Fact 24 (Regression minimizer). Let the regression problem ∥Ax −b∥2
2 have minimizer x⋆, and
suppose that A⊤A is invertible. Then,
x⋆=
 A⊤A
−1 A⊤b.
Using Fact 24, we directly prove the following upper bound surrogate holds on the risk under
the model (9.3), (9.4) for the solution to any reweighted regression problem.
Lemma 164. Under the generative model (9.3), (9.4), letting W ∈Sn
≥0 be a diagonal matrix and
x⋆
w := argminx
W
1
2 (Ax −b)

2
2

,
we have
E
h
∥x⋆
w −xtrue∥2
2
i
≤σ2 Tr
 A⊤WA

λmin (A⊤WA).

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
441
Proof. By applying Fact 24, we have that
x⋆
w =
 A⊤WA
−1 A⊤W (Axtrue + ξ) = xtrue +
 A⊤WA
−1 A⊤Wξ.
Thus, we have the sequence of derivations
E
h
∥x⋆
w −xtrue∥2
A⊤WA
i
= E

 A⊤WA
−1 A⊤Wξ

2
A⊤WA

= E
hD
W
1
2 ξξ⊤W
1
2 , W
1
2 A
 A⊤WA
−1 A⊤W
1
2
Ei
= σ2 D
diag
n
wi ∥ai∥2
2
o
, W
1
2 A
 A⊤WA
−1 A⊤W
1
2
E
≤σ2Tr
 A⊤WA

.
(9.5)
The last inequality used the ℓ1-ℓ∞matrix H¨older inequality and that W
1
2 A
 A⊤WA
−1 A⊤W
1
2 is
a projection matrix, so ∥W
1
2 A
 A⊤WA
−1 A⊤W
1
2 ∥∞= 1. Lower bounding the squared A⊤WA
norm by a λmin(A⊤WA) multiple of the squared Euclidean norm yields the conclusion.
We remark that the analysis in Lemma 164 of the surrogate upper bound we provide was loose
in two places: the application of H¨older and the norm conversion. Lemma 164 shows that the risk
under the generative model (9.4) can be upper bounded by a quantity proportional to τ(A⊤WA),
the average conditioning of the reweighted matrix.
Directly applying Lemma 164, our risk bounds improve with the conditioningof the reweighted
system. Hence, our scaling procedures improve both the computational and statistical guarantees
of regression under this generative model, albeit only helping the latter through an upper bound.
Homoskedastic statistical guarantees
In this section, we work under the homoskedastic generative model assumption.
In particular,
throughout the covariance matrix in (9.3) will be a multiple of the identity:
Σ = σ2I.
(9.6)
We begin by providing a risk upper bound under the model (9.3), (9.6).
Lemma 165. Under the generative model (9.3), (9.6), let x⋆:= argminx{∥Ax −b∥2
2}. Then,
E
h
∥x⋆−xtrue∥2
A⊤A
i
= σ2d =⇒E
h
∥x∗−xtrue∥2
2
i
≤
σ2d
λmin(A⊤A).
(9.7)

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
442
Proof. Using Fact 24, we compute
x⋆−xtrue =
 A⊤A
−1 A⊤b −xtrue
=
 A⊤A
−1 A⊤(Axtrue + ξ) −xtrue =
 A⊤A
−1 A⊤ξ.
Therefore via directly expanding, and using linearity of expectation,
E
h
∥x∗−xtrue∥2
A⊤A
i
= E
A
 A⊤A
−1 A⊤ξ

2
2

= E
hD
ξξ⊤, A
 A⊤A
−1 A⊤Ei
= σ2 
A
 A⊤A
−1 A⊤
= σ2d.
The ﬁnal implication follows from λmin(A⊤A) ∥x∗−xtrue∥2
2 ≤∥x∗−xtrue∥2
A⊤A.
Lemma 165 shows that in regards to our upper bound (which is loose in the norm conversion at
the end), the notion of adversarial semi-random noise is at odds in the computational and statistical
senses. Namely, given additional rows of the matrix A, the bound (9.7) can only improve, since λmin
is monotonically increasing as rows are added. To address this, we give guarantees about recover-
ing reweightings which match the best possible upper bound anywhere along the "computational-
statistical tradeoﬀcurve." We begin by providing a weighted analog of Lemma 165.
Lemma 166. Under the generative model (9.3), (9.6), letting W ∈Sn
≥0 be a diagonal matrix and
x⋆
w := argminx
W
1
2 (Ax −b)

2
2

,
we have
E
h
∥x⋆
w −xtrue∥2
2
i
≤σ2d ·
∥w∥∞
λmin (A⊤WA).
(9.8)
Proof. By following the derivations (9.5) (and recalling the deﬁnition of x⋆
w),
E
h
∥x⋆
w −xtrue∥2
A⊤WA
i
= E
hD
ξξ⊤, WA
 A⊤WA
−1 A⊤W
Ei
= σ2Tr

WA
 A⊤WA
−1 A⊤W

.
(9.9)
Furthermore, by W ⪯∥w∥∞I we have A⊤W2A ⪯∥w∥∞A⊤WA. Thus,
Tr

WA
 A⊤WA
−1 A⊤W

=
D
A⊤W2A,
 A⊤WA
−1E
≤∥w∥∞Tr(I) = d ∥w∥∞.
Using this bound in (9.9) and converting to Euclidean norm risk yields the conclusion.
Lemma 166 gives a quantitative version of a computational-statistical tradeoﬀcurve. Speciﬁcally,
we give guarantees which target the best possible condition number of a 0-1 reweighting, subject to

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
443
a given level of λmin(A⊤WA). In the following discussion we assume there exists Ag ⊆A, a subset
of rows, satisfying (for known κg, νg, and suﬃciently small constant ϵ ∈(0, 1))
κg ≤κ
 A⊤
g Ag

≤(1 + ϵ)κg,
1
λmin
 A⊤
g Ag
 ≤νg.
(9.10)
Our key observation is that we can use existence of a row subset satisfying (9.10), combined with a
slight modiﬁcation of Algorithm 29, to ﬁnd a reweighting w such that
κ
 A⊤WA

= O (κg) ,
∥w∥∞
λmin (A⊤WA) = O(νg).
(9.11)
Lemma 167. Consider running Algorithm 29, with the modiﬁcation that in Line 7, we set
xt ←an ϵ
10-multiplicative approximation of argmaxP
i∈[n] wi e
Ai⪯I
x∈Rn
≥0
⟨κvt, w⟩,
where for all i ∈[n], eAi :=

Ai
0d×n
0n×d
diag

κg
νg ei


.
(9.12)
Then, if (9.10) is satisﬁed for some A ∈Rn×d and row subset Ag ⊆A, Algorithm 29 run on κ ←κg
and {Ai = aia⊤
i }i∈[n] where {ai}i∈[n] are rows of A will produce w satisfying (9.11).
Proof. We note that each matrix eAi is the same as the corresponding Ai, with a single nonzero
coordinate along the diagonal bottom-right block. The proof is almost identical to the proof of
Lemma 96, so we highlight the main diﬀerences here. The main property that Lemma 96 used was
that Line 9 did not pass, which lets us conclude (7.63). Hence, by the approximation guarantee on
each xt, it suﬃces to show that for any Yt ∈Sd
≥0 with Tr(Yt) = 1, (analogously to (7.62)),
max
P
i∈[n] wi e
Ai⪯I
x∈Rn
≥0
κg
*
Yt,
X
i∈[n]
wiAi
+
≥1 −O(ϵ).
(9.13)
However, by taking w to be the 0-1 indicator of the rows of Ag scaled down by λmax(A⊤
g Ag), we
have by the promise (9.10) that
X
i∈[n]
wi eAi =
1
λmax(A⊤
g Ag) ⪯I ⇐=
1
λmax(A⊤
g Ag)A⊤
g Ag ⪯I, κg
νg
·
1
λmax(A⊤
g Ag) ≤1.
(9.14)
Now, it suﬃces to observe that (9.14) implies our indicator w is feasible for (9.13), so
max
P
i∈[n] wi e
Ai⪯I
x∈Rn
≥0
κg
*
Yt,
X
i∈[n]
wiAi
+
≥λmin
 A⊤
g Ag

λmax
 A⊤
g Ag
 · κg ≥1 −O(ϵ).

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
444
The remainder of the proof is identical to Lemma 96, where we note the output w satisﬁes
X
i∈[n]
wi eAi ⪯I,
X
i∈[n]
wiAi ⪰1 −O(ϵ)
κg
I,
which upon rearrangement and adjusting ϵ by a constant yields (9.11).
By running the modiﬁcation of Algorithm 29 described for a given level of νg, it is straightforward
to perform an incremental search on κg to ﬁnd a value satisfying the bound (9.11) as described in
Theorem 63. It is simple to verify that the modiﬁcation in (9.12) is not the dominant runtime in
any of Theorems 63 or 62 since the added constraint is diagonal and eAi is separable. Hence, for
every "level" of νg in (9.10) yielding an appropriate risk bound (9.8), we can match this risk bound
up to a constant factor while obtaining computational speedups scaling with κg.
9.4
Semi-random sparse recovery
Sparse recovery is one of the most fundamental and well-studied inverse problems, with numerous
applications in prevalent real-world settings [214]. In its most basic form, we are given an entrywise
Gaussian measurement matrix G ∈Rm×d and measurements b = Gx⋆for an unknown s-sparse
x⋆∈Rd; the goal of the problem is to recover x⋆.
Seminal works by Cand`es, Romberg, and
Tao [104, 106, 105] showed that even when the linear system in G is extremely underconstrained,
recovery is tractable so long as m = Ω(s log d). Further they gave a polynomial-time algorithm
known as basis pursuit based on linear programming recovering x⋆in this regime.
Unfortunately, the runtime of linear programming solvers, while polynomial in the size of the
input, can still be prohibitive in many high-dimensional real-world settings.
Correspondingly, a
number of alternative approaches which may broadly be considered ﬁrst-order methods have been
developed. These methods provably achieve similar recovery guarantees under standard generative
models such as Gaussian measurements, with improved runtimes compared to the aforementioned
convex programming methods. We refer to these ﬁrst-order methods through as "fast" algorithms
throughout and they may roughly be placed in the following (potentially non-disjoint) categories.
• Greedy algorithms, e.g. [382, 437, 411], seek to greedily ﬁnd elements in the support of the
true x⋆using diﬀerent combinatorial search criteria.
• Non-convex iterative algorithms, e.g. [410, 86, 87, 381, 226], directly optimize a (poten-
tially non-convex) objective over a non-convex domain.
• Convex ﬁrst-order methods, e.g. [223, 164, 148, 66, 68, 412, 9] quickly solve the convex
objective underlying basis pursuit using ﬁrst-order methods.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
445
We also note that theoretically, when n is suﬃciently large, recent advances by [517, 516], also
obtain fast runtimes for the relevant linear programming objective. The fastest IPM for the noiseless
sparse recovery objective runs in time5 eO(nd + n2.5) which is nearly-linear when A is dense and
n ≪d2/3. For a range of (superlogarithmic, but sublinear) n, these runtimes are no longer nearly-
linear; furthermore, these IPMs are second-order and our focus is on designing ﬁrst-order sparse
recovery methods, which are potentially more practical.6
It has often been observed empirically that fast ﬁrst-order methods can have large error, or
fail to converge, in real-world settings [166, 280, 446] where convex programming-based algorithms
(while potentially computationally cumbersome) perform well statistically [548, 15]. This may be
surprising, given that in theory, fast algorithms essentially match the statistical performance of the
convex programming-based algorithms under standard generative assumptions. While there have
been many proposed explanations for this behavior, one compelling argument is that fast iterative
methods used in practice are more brittle to changes in modeling assumptions. We adopt this view-
point in this chapter, and develop fast sparse recovery algorithms which achieve optimal statistical
rates under a semi-random adversarial model [85, 219], a popular framework for investigating the
robustness of learning algorithms under changes to the data distribution.
Semi-random adversaries. Semi-random adversaries are a framework for reasoning about algo-
rithmic robustness to distributional shift. They are deﬁned in statistical settings, and one com-
mon type of semi-random adversary is one which corresponds to generative models where data has
been corrupted in a "helpful" or "monotone" way. Such a monotone semi-random adversary takes
a dataset from which learning is information-theoretically tractable, and augments it with addi-
tional information; this additional information may not break the problem more challenging from
an information-theoretic perspective,7 but may aﬀect the performance of algorithms in other ways.
In this chapter, we consider a semi-random adversary which makes the computational problem more
diﬃcult without aﬀecting the problem information-theoretically, by returning a consistent superset
of the unaugmented observations. This contrasts with other adversarial models such as gross cor-
ruption [31, 512, 278, 513], where corruptions may be arbitrary, and the corrupted measurements
incorrect. It may be surprising that a "helpful" adversary has any implications whatsoever on a
learning problem, from either an information-theoretic or computational standpoint.
Typically, convex programming methods for statistical recovery problems are robust to these
sorts of perturbations — in brief, this is because constraints to a convex program that are met by
an optimum point does not change the optimality of that point. However, greedy and non-convex
methods — such as popular practical algorithms for sparse linear regression — can be susceptible
to semi-random adversaries. Variants of this phenomenon have been reported in many common
5We use e
O to hide polylogarithmic factors in problem parameters for brevity of exposition throughout.
6We also note that these IPM results do not immediately apply to natural (nonlinear) convex programs for sparse
recovery under noisy observations, see Appendix H.2.
7There are notable exceptions, e.g. the semi-random stochastic block model of [397].

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
446
statistical estimation problems, such as stochastic block models and broadcast tree models [397],
PAC learning [84], matrix completion [396, 133], and principal component regression [77]. This can
be quite troubling, as semi-random noise can be thought of as a relatively mild form of generative
model misspeciﬁcation: in practice, the true distribution is almost always diﬀerent from the models
considered in theory. Consequently, an algorithm's non-robustness to semi-random noise is suggestive
that the algorithm may be more unreliable in real-world settings.
We consider a natural semi-random adversarial model for sparse recovery (see e.g. page 284 of
[46]), which extends the standard restricted isometry property (RIP) assumption, which states that
applying matrix A approximately preserves the ℓ2 norm of sparse vectors. Concretely, throughout
the chapter we say matrix A satisﬁes the (s, c)-restricted isometry (RIP) property if for all s-sparse
vectors v,
1
c ∥v∥2
2 ≤∥Av∥2
2 ≤c ∥v∥2
2 .
We state a basic version of our adversarial model here, and defer the statement of the fully general
version to Deﬁnition 38.8 We defer the introduction of notation used in the remainder of the chapter
to Section 9.5.
Deﬁnition 37 (pRIP matrix). Let m, n, d ∈N be known with n ≥m. We say A ∈Rn×d is ρ-pRIP
(planted RIP) if there is an (unknown) G ∈Rm×d such that each row of G is also a row of A and
1
√mG is (Θ(s), Θ(1))-RIP for appropriate constants, and ∥G∥max ≤ρ. When ρ = eO(1) for brevity
we say A is pRIP.
Under the problem parameterizations used in this chapter, standard RIP matrix constructions
satisfy ρ = eO(1) with high probability.
For example, when G is entrywise Gaussian and m =
Θ(s log d), a tail bound shows that with high probability we may set ρ = O(√log d) to be compatible
with the assumptions in Deﬁnition 37.
pRIP matrices can naturally be thought of as arising from a semi-random adversarial model as
follows. First, an RIP matrix G ∈Rm×d is generated, for example from a standard ensemble (e.g.
Gaussian or subsampled Hadamard). An adversary inspects G, and forms A ∈Rn×d by reshuﬄing
and arbitrarily augmenting rows of G. Whenever we refer to a "semi-random adversary" in the
remainder of the introduction, we mean the adversary provides us a pRIP measurement matrix A.
The key recovery problem we consider in the remainder of this chapter is recovering an unknown
s-sparse vector x⋆∈Rd given measurements b ∈Rn through A. We consider both the noiseless
or exact setting where b = Ax⋆and the noisy setting where b = Ax⋆+ ξ for bounded ξ. In the
noiseless setting in particular, the semi-random adversary hence only gives the algorithm additional
consistent measurements of the unknown s-sparse vector x⋆. In this sense, the adversary is only
"helpful," as it returns a superset of information which is suﬃcient for sparse recovery (formally, this
8When clear from context, as it will be throughout the main sections of the remainder of the chapter, s will always
refer to the sparsity of a vector x⋆∈Rd in an exact or noisy recovery problem through A ∈Rn×d. For example, the
parameter s in Deﬁnition 37 is the sparsity of the vector in an associated sparse recovery problem.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
447
adversary cannot break the standard restricted nullspace condition which underlies the successful
performance of convex programming methods). We note n may be much larger than m, i.e. we
impose no constraint on how many measurements the adversary adds.
Semi-random sparse recovery in nearly-linear time. We devise algorithms which match the nearly-
linear runtimes and optimal recovery guarantees of faster algorithms on fully random data, but which
retain both their runtime and the robust statistical performances of convex programming methods
against semi-random adversaries. In this sense, our algorithms obtain the "best of both worlds."
We discuss and compare more extensively to existing sparse recovery algorithms under Deﬁnition 37
in the following section. We ﬁrst state our result in the noiseless observation setting.
Theorem 64 (informal, see Theorem 66). Let x⋆∈Rd be an unknown s-sparse vector. Let A ∈
Rn×d be pRIP. There is an algorithm, which given A and b = Ax⋆, runs in time eO(nd), and outputs
x⋆with high probability.
Since our problem input is of size nd, our runtime in Theorem 64 is nearly-linear in the problem
size. We also extend our algorithm to handle the noisy observation setting, where we are given
perturbed linear measurements of x⋆from a pRIP matrix.
Theorem 65 (informal, see Theorem 67). Let x⋆∈Rd be an unknown s-sparse vector, and let
ξ ∈Rn be arbitrary. Let A ∈Rn×d be pRIP. There is an algorithm, which given A and b = Ax⋆+ξ,
runs in time eO(nd), and with high probability outputs x satisfying
∥x −x⋆∥2 ≤O
 1
√m
ξ(m)

2

,
where ξ(m) denotes the largest m entries of ξ by absolute value, with other coordinates set to 0.
The error scaling of Theorem 65 is optimal in the semi-random setting. Indeed, when there is no
semi-random noise, the guarantees of Theorem 65 exactly match the standard statistical guarantees
in the fully-random setting for sparse recovery, up to constants; for example, when A = √mI (which
is clearly RIP, in fact an exact isometry, after rescaling), it is information-theoretically impossible to
obtain a better ℓ2 error.9 The error bound of Theorem 65 is similarly optimal in the semi-random
setting because in the worst case, the largest entries of ξ may correspond to the rows of the RIP
matrix from which recovery is information-theoretically possible.
Performance of existing algorithms. To contextualize Theorems 64 and 65, we discuss the per-
formance of existing algorithms for sparse recovery under the semi-random adversarial model of
Deﬁnition 37. First, it can be easily veriﬁed that our semi-random adversary never changes the
9In the literature it is often standard to scale down the sensing matrix A by √m; this is why our error bound is
similarly scaled. However, this scaling is more convenient for our analysis, especially when stating weighted results.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
448
information-theoretic tractability of sparse recovery. In the noiseless setting for example, the per-
formance of the minimizer to the classical convex program based on ℓ1 minimization,
min
Ax=b ∥x∥1 ,
is unchanged in the presence of pRIP matrices (as x⋆is still consistent with the constraint set, and
in particular a RIP constraint set), and hence the semi-random problem can be solved in polynomial
time via convex programming. This suggests the main question we address: can we design a near-
linear time algorithm obtaining optimal statistical guarantees under pRIP measurements?
As alluded to previously, standard greedy and non-convex methods we have discussed may fail
to converge to the true solution against appropriate semi-random adversaries generating pRIP ma-
trices. We give explicit counterexamples to several popular methods such as orthogonal matching
pursuit and iterative hard thresholding in Appendix H.2. Further, it seems likely that similar coun-
terexamples also break other, more complex methods commonly used in practice, such as matching
pursuit [382] and CoSaMP [410].
Additionally, while fast "convex" iterative algorithms (e.g. ﬁrst-order methods for solving ob-
jectives underlying polynomial-time convex programming approaches) will never fail to converge
to the correct solution given pRIP measurements, the analyses which yield fast runtimes for these
algorithms [412, 9] rely on properties such as restricted smoothness and strong convexity (a spe-
cialization of standard conditioning assumptions to numerically sparse vectors). These hold under
standard generative models but again can be broken by pRIP measurements; consequently, standard
convergence analyses of "convex" ﬁrst-order methods may yield arbitrarily poor rates.
One intuitive explanation for why faster methods fail is that they depend on conditions such as
incoherence [196] or the restricted isometry property [106], which can be destroyed by a semi-random
adversary. For instance, RIP states that if S is any subset of m = Θ(s) columns of A, and AS is the
submatrix formed by taking those columns of A, then A⊤
S AS is an approximate isometry (i.e. it is
well-conditioned). While it is well-known that RIP is satisﬁed with high probability when A consists
of Θ(s log d) Gaussian rows, it is not too hard to see that augmenting A with additional rows can
easily ruin the condition number of submatrices of this form. In contrast, convex methods work
under weaker assumptions such as the restricted nullspace condition, which cannot be destroyed
by the augmentation used by pRIP matrices. Though these weaker conditions (e.g. the restricted
nullspace condition) suﬃce for algorithms based on convex programming primitives, known analyses
of near-linear time "fast" algorithms require additional instance structure, such as incoherence or
RIP. Thus, it is plausible that fast algorithms for sparse recovery are less robust to the sorts of
distributional changes that may occur in practice.
Beyond submatrices. Our methods naturally extend to a more general setting (see Deﬁnition 38,
wherein we deﬁne "weighted RIP" (wRIP) matrices, a generalization of Deﬁnition 37). Rather than
assuming there is a RIP submatrix G, we only assume that there is a (nonnegative) reweighting of

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
449
the rows of A so that the reweighted matrix is "nice," i.e. it satisﬁes RIP. Deﬁnition 37 corresponds
to the special case of this assumption where the weights are constrained to be either 0 or 1 (and
hence must indicate a subset of rows). In our technical sections (Sections 9.6 and 9.7), our results are
stated for this more general semi-random model, i.e. sparse recovery from wRIP measurements. For
simplicity of exposition, throughout the introduction, we mainly focus on the simpler pRIP sparse
recovery setting described following Deﬁnition 37.
Towards instance-optimal guarantees. While the performance of the algorithms in Theorems 64
and 65 is already nearly-optimal in the worst case semi-random setting, one can still hope to improve
our runtime and error bounds in certain scenarios. Our formal results, Theorems 66 and 67, provide
these types of ﬁne-grained instance-optimal guarantees in several senses.
In the noiseless setting (Theorem 66), if it happens to be that the entire matrix A is RIP (and not
just G), then standard techniques based on subsampling the matrix can be used to solve the problem
in time eO(sd) with high probability. For example, if A is pRIP where, following the notation of
Deﬁnition 37, G is entrywise Gaussian, and the adversary chose to give us additional Gaussian rows,
one could hope for a runtime improvement (simply by ignoring the additional measurements given).
Theorem 66 obtains a runtime which smoothly interpolates between the two regimes of a worst-case
adversary and an adversary which gives us additional random measurements from an RIP ensemble.
Roughly speaking, if there exists a (a priori unknown) submatrix of A of m ≫eΘ(s) rows which is
RIP, then we show that our algorithm runs in sublinear time eO(nd· s
m), which is eO(sd) when m ≈n.
We show this holds in our weighted semi-random model (under wRIP measurements, Deﬁnition 38)
as well, where the runtime depends on the ratio of the ℓ1 norm of the (best) weight vector to its ℓ∞
norm, a continuous proxy for the number of RIP rows under pRIP.
We show a similar interpolation holds in the noisy measurement setting, both in the runtime
sense discussed previously, and also in a statistical sense. In particular, Theorem 67 achieves (up to
logarithmic factors) the same interpolating runtime guarantee of Theorem 66, but further attains a
squared ℓ2 error which is roughly the average of the m largest elements of the squared noise vector
ξ (see the informal statement in Theorem 65). This bound thus improves as m ≫eΘ(s); we show
it extends to weighted RIP matrices (Deﬁnition 38, our generalization of Deﬁnition 37) in a natural
way depending on the ℓ∞-ℓ1 ratio of the weights.
9.4.1
Our techniques
Our overall approach for semi-random sparse recovery is fairly diﬀerent from two recent works in
the literature which designed fast iterative methods succeeding under a semi-random adversarial
model: [133] and Section 9.3. In particular, these two algorithms were both based on the following
natural framework, which separates the "planted learning" problem (e.g. identifying the planted
benign matrix) from the "estimation" task (e.g. solving a linear system or regression problem).
1. Compute a set of weights for the data (in linear regression for example, these are weights on

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
450
each of the rows of a measurement matrix A), such that after re-weighting, the data ﬁts the
input assumptions of a fast iterative method which performs well on a fully random instance.
2. Apply said fast iterative algorithm on the reweighted data in a black-box manner.
To give a concrete example, Section 9.3 studied the standard problem of overdetermined linear
regression with a semi-random adversary, where a measurement matrix A is received with the
promise that A contains a "well-conditioned core" G. The algorithm of Section 9.3 ﬁrst learned a
re-weighting of the rows of A by a diagonal matrix W
1
2 , such that the resulting system in A⊤WA
is well-conditioned and hence can be solved using standard ﬁrst-order methods.
In the case of semi-random sparse recovery, there appear to be signiﬁcant barriers to reweighting
approaches (which we will shortly elaborate on). We take a novel direction that involves designing a
new nearly-linear time iterative method for sparse recovery tailored to the geometry of the problem.
Why not (globally) reweight the rows?
There are several diﬃculties immediately encountered
when trying to use the aforementioned reweighting framework for sparse recovery. First of all, there
is no eﬀective analog of condition number for an underdetermined linear system. The standard
assumption on the measurement matrix A to make sparse recovery tractable for fast iterative meth-
ods is that A satisﬁes RIP, i.e. A is roughly an isometry when restricted to O(s)-sparse vectors.
However, RIP is NP-hard to verify [57] and this may suggest that it is computationally hard to try,
say, learning a reweighting of the rows of A such that the resulting reweighted matrix is guaranteed
to be RIP (though it would be very interesting if this were achievable). More broadly, almost all
explicit conditions (e.g. RIP, incoherence etc.) which make sparse recovery tractable for fast algo-
rithms are conditions about subsets of the columns of A. Thus, any approach which reweights rows
of A such that column subsets of the reweighted matrix satisfy an appropriate condition results in
optimization problems that seems challenging to solve in nearly-linear time.
We circumvent these diﬃculties in two steps. First, we propose a new analysis of an iterative
method based on (reweighted) projected gradient descent, which obtains a fast convergence rate
whenever each step satisﬁes certain locally veriﬁable properties. Next, our algorithm computes a
sequence of local reweightings (which can be diﬀerent in each iteration) of the rows of our measure-
ment matrix, such that each local reweighting satisﬁes our requisite progress conditions for that
step. We use the existence of a global reweighting satisfying RIP to demonstrate that each local
reweighting subproblem has a good solution, and design an eﬃcient method for computing each lo-
cal reweighting. Our framework of bypassing hardness of computing a global reweighting to recover
planted statistical information, by instead designing an iterative method capable of exploiting local
reweightings with (computationally tractable) certiﬁable progress conditions, is quite general, and
we hope it will ﬁnd uses in semi-random settings beyond our particular problem.
The geometry of sparse recovery. We now explain our new approach, and how we derive de-
terministic conditions on the steps of an iterative method which certify progress by exploiting the
geometry of sparse recovery. We focus on the clean observation setting in this technical overview.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
451
Suppose that we wish to solve a sparse regression problem Ax⋆= b where x⋆is s-sparse, and we
are given A and b. To ﬁx a scale, suppose for simplicity that we know ∥x⋆∥1 = √s and ∥x⋆∥2 = 1.
Also, assume for the purpose of conveying intuition that A is pRIP, and that the planted matrix G
in Deﬁnition 37 is an entrywise random Gaussian matrix.
We next consider a natural family of iterative algorithms for solving the system Ax⋆= b. For
simplicity, assume that our current iterate is xt = 0 (such that b is the residual vector b −Axt;
more generally, in the following description b will be replaced by the current residuals). Inspired by
gradient methods for solving regression, we consider "ﬁrst-order" algorithms of the following form:
yt ←xt + A⊤u
xt+1 ←P(yt)
(9.15)
where u ∈Rn are coeﬃcients to be computed (for example, a natural attempt could be to set u to
be a multiple of Axt −b, resulting in the step being a scaled gradient of ∥Ax −b∥2
2 at xt) and P
denotes projection onto the (convex) ℓ1 ball of radius √s. Our goal will be to make constant-factor
progress in terms of ∥xt −x⋆∥2 in each application of (9.15), to yield a eO(1) iteration method. Note
that xt+1 must at minimum make constant factor progress in the direction x⋆−xt (in terms of
decreasing the projection onto this direction) if we hope to make constant factor progress in overall
distance to x⋆. In other words, we must have
⟨xt+1 −xt, x⋆−xt⟩= Ω(∥x⋆−xt∥2
2) = Ω(1) .
First, observe that by the deﬁnition of our step and shifting so that xt = 0, the point yt satisﬁes
⟨yt −xt, x⋆−xt⟩= ⟨A⊤u, x⋆−xt⟩= u⊤(b −Axt) = u⊤b,
so to obtain a corresponding progress lower bound for the move to yt, we require
u⊤b = Ω(1).
(9.16)
Of course, this condition alone is not enough, for two reasons: the step also moves in directions
orthogonal to x⋆−xt, and we have not accounted for the projection step P. If all of the rows of A
were random, then standard Gaussian concentration implies that we expect ∥b∥2 = p n
d , and thus
to satisfy (9.16) we need ∥u∥2 ≥
q
d
n. This also implies
∥yt −xt∥2 =
A⊤u

2 ≈
r
d
n,
since for Gaussian A, we expect that its rows are roughly orthogonal. Moreover, since
q
d
n ≫1 in

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
452
xt
x⋆
xt+1
yt
q
d
n
1
Figure 9.1: The eﬀect of ℓ1 projection on iterate progress. The dashed line represents a facet of the
ℓ1-ball around xt of radius ∥xt −x⋆∥1.
the typical underconstrained setting, almost all of the step from xt to yt is actually orthogonal to
the desired "progress direction" parallel to x⋆−xt. This appears to be a serious problem, because
in order to argue that our algorithm makes progress, we need to argue that the ℓ1 projection step
xt+1 = P(yt) "undoes" this huge ℓ2 movement orthogonal to the progress direction (see Figure 1).
Our key geometric insight is that the ℓ1 ball is very thin in most directions, but is thick in
directions that correspond to "numerically sparse" vectors, namely vectors with bounded ℓ1 to ℓ2
ratios. Crucially, the movement of our step in the progress direction parallel to x⋆−xt is numerically
sparse because x⋆−xt is itself O(s)-numerically sparse by assumption. However, for Gaussian A,
the motion in the subspace orthogonal to the progress direction ends up being essentially random
(and thus is both not sparse, and is ℓ∞-bounded). Formally, we leverage this decomposition to show
that the ℓ1 projection keeps most of the forward movement in the progress direction x⋆−xt but
eﬀectively ﬁlters out much of the orthogonal motion, as demonstrated by the ﬁgure below.
This geometric intuition is the basis for the deterministic conditions we require of the steps of
our iterative method, to guarantee that it is making progress. More precisely, the main condition
we require of our step in each iteration, parameterized by the coeﬃcients u used to induce (9.15), is
that yt −xt has a "short-ﬂat" decomposition into two vectors p + e where
∥p∥2 = O(1) is "short" and ∥e∥∞= O
 1
√s

is "ﬂat".
Intuitively, the short component is a proxy for the progress component (in the direction towards x⋆),
and the ﬂat component is a proxy for the orthogonal component, which should be ﬁltered out by the

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
453
projection step onto an ℓ1 ball. The above bounds are rescaled appropriately in our actual method.
We state these requirements formally (in the clean observation case, for example) in Deﬁnition 39,
where we deﬁne a "step oracle" which is guaranteed to make constant-factor progress towards x⋆.
By combining the above short-ﬂat decomposition requirement with a progress requirement such as
(9.16), we can show that as long as we can repeatedly implement a satisfactory step, our method is
guaranteed to converge rapidly.
This framework for sparse recovery eﬀectively reduces the learning problem to the problem of
implementing an appropriate step oracle. Note that a valid step always exists by only looking at
the Gaussian rows. To complete our algorithm, we give an implementation of this step oracle (i.e.
actually solving for a valid step) which runs in nearly-linear time even when the data is augmented by
a semi-random adversary (that is, our measurement matrix is pRIP or wRIP rather than RIP), and
demonstrate that our framework readily extends to the noisy observation setting. Our step oracle is
motivated by stochastic gradient methods. In particular, we track potentials corresponding to the
progress made by our iterative method and the short-ﬂat decomposition, and show that uniformly
sampling rows of a pRIP (or wRIP) matrix A and taking steps in the direction of these rows which
maximally improve our potentials rapidly implements a step oracle.
9.4.2
Related work
Sparse recovery. Sparse recovery, and variants thereof, are fundamental statistical and algorithmic
problems which have been studied in many of settings, including signal processing [355, 469, 196,
86, 59], and compressed sensing [104, 106, 105, 195, 462]. A full review of the extensive literature on
sparse recovery is out of the scope of the present chapter; we refer the reader to e.g. [214, 165, 335, 471]
for more extensive surveys.
Within the literature on sparse recovery, arguably the closest line of work to ours is the line of
work which attempts to design eﬃcient algorithms which work when the restricted condition number
of the sensing or measurement matrix is large. Indeed, it is known that many nonconvex methods
fail when the restricted condition number of the sensing matrix is far from 1, which is often the
case in applications [280]. To address this, several works [280, 471] have designed novel non-convex
methods which still converge, when the restricted condition number of the matrix is much larger
than 1. However, these methods still require that the restricted condition number is constant or
bounded, whereas in our setting, the restricted condition number could be arbitrarily large due to
the generality of the semi-random adversary assumption.
Another related line of work considers the setting where, instead of having a sensing matrix
with rows which are drawn from an isotropic Gaussian, have rows drawn from N(0, Σ), for some
potentially ill-conditioned Σ [79, 453, 514, 280, 324, 154, 546, 70, 317]. This setting is related to
our semi-random adversarial model, in that the information-theoretic content of the problem does
not change, but obtaining eﬃcient algorithms which match the optimal statistical rates is very

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
454
challenging. However, there does not appear to be any further concrete connection between this "ill-
conditioned covariance" setting and the semi-random model we consider in this work. Indeed, the ill-
conditioned setting appears to be qualitatively much more diﬃcult for algorithms: in particular, [317]
shows evidence that there are in fact no eﬃcient algorithms that achieve the optimal statistical
rates, without additional assumptions on Σ. In contrast in the semi-random setting, polynomial-
time convex programming approaches, while having potentially undesirable superlinear runtimes,
still obtain optimal statistical guarantees.
Finally as discussed earlier in the introduction, there is a large body of work on eﬃcient algorithms
for sparse recovery in an RIP matrix (or a matrix satisfying weaker or stronger analogous properties).
These works e.g. [104, 106, 105, 382, 437, 411, 410, 86, 87, 381, 226, 223, 164, 148, 66, 68, 412, 9]
are typically based on convex programming or diﬀerent iterative ﬁrst-order procedures.
Semi-random models. Context for semi-random models was previously discussed in Section 9.3.
We also note that the well-studied Massart noise model in PAC learning [386] can be thought of
as a semi-random variant of the random classiﬁcation noise model. However, this setting appears
to be quite diﬀerent from ours: in particular, it was not until quite recently that polynomial-time
algorithms were even known to be achievable for a number of fundamental learning problems under
Massart noise [173, 189, 126, 183, 184, 187, 190, 174, 544].
9.5
Preliminaries: semi-random sparse recovery
General notation. We let [n] := {i ∈N, 1 ≤i ≤n}. The ℓp norm of a vector is denoted ∥·∥p,
and the sparsity (number of nonzero entries) of a vector is denoted ∥·∥0. For a vector v ∈Rd and
k ∈[d], we let v(k) be the vector equalling v on the largest k entries of v in absolute value (with
other coordinates zeroed out). The all-zeroes vector of dimension n is denoted 0n. The nonnegative
probability simplex in dimension n (i.e. ∥p∥1 = 1, p ∈Rn
≥0) is denoted ∆n.
For mean µ ∈Rd and positive semideﬁnite covariance Σ ∈Rd×d, N(µ, Σ) denotes the corre-
sponding multivariate Gaussian. i ∼unif. S denotes a uniform random sample from set S. For N ∈N
and p ∈∆n we we use Multinom(N, p) to denote the probability distribution corresponding to N
independent draws from [n] as speciﬁed by p.
Sparsity. We say v is s-sparse if ∥v∥0 ≤s. We deﬁne the numerical sparsity of a vector by
NS(v) := ∥v∥2
1 / ∥v∥2
2. Note that from the Cauchy-Schwarz inequality, if ∥v∥0 ≤s, then NS(v) ≤s.
Matrices. Matrices are in boldface throughout. The zero and identity matrix of appropriate
dimension from context are 0 and I. For a matrix A ∈Rn×d, we let its rows be Ai:, i ∈[n] and
its columns be A:j, j ∈[d]. The set of d × d symmetric matrices is Sd, and its positive deﬁnite
and positive semideﬁnite restrictions are Sd
≻0 and Sd
≥0. We use the Loewner partial order ⪯on Sd.
The largest entry of a matrix A ∈Rn×d is denoted ∥A∥max := maxi∈[n],j∈[d] |Aij|. When a matrix
A ∈Rn×d is clear from context, we refer to its rows as {ai}i∈[n].

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
455
Short-ﬂat decompositions. Throughout we frequently use the notion of "short-ﬂat decomposi-
tions." We say v ∈Rd has a (C2, C∞) short-ﬂat decomposition if v = p + e for some e ∈Rd with
∥e∥2 ≤C2 and p ∈Rd with ∥p∥∞≤C∞. Further, we use trunc(v, c) ∈Rd for c ∈R≥0 to denote
the vector which coordinatewise [trunc(v, c)]i = sgn(vi) max(|vi| −c, 0) (i.e. the result of adding or
subtracting at most c from each coordinate to decrease the coordinate's magnitude). Note that
v ∈Rd has a (C2, C∞) short-ﬂat decomposition if and only if ∥trunc(v, C∞)∥2 ≤C2 (in which case
p = trunc(v, C∞) and e = v −p is such a decomposition).
Restricted isometry property. We say that matrix A ∈Rn×d satisﬁes the (s, c)-restricted isometry
property (RIP) or (more concisely) A is (s, c)-RIP, if for all s-sparse vectors v ∈Rd,
1
c ∥v∥2
2 ≤∥Av∥2
2 ≤c ∥v∥2
2 .
9.6
Exact recovery
In this section, we give an algorithm for solving the underconstrained linear system Ax⋆= b given
the measurement matrix A ∈Rn×d (for n ≤d) and responses b ∈Rn (i.e. noiseless or "exact"
regression), and x⋆is s-sparse. Our algorithm succeeds when A is weighted RIP (wRIP), i.e. it
satisﬁes Deﬁnition 38, a weighted generalization of Deﬁnition 37.
Deﬁnition 38 (wRIP matrix). Let w⋆
∞∈[0, 1]. We say A ∈Rn×d is (ρ, w⋆
∞)-wRIP if ∥A∥max ≤ρ,
and there exists a weight vector w⋆∈∆n satisfying ∥w⋆∥∞≤w⋆
∞, such that diag (w⋆)
1
2 A is
(Θ(s), Θ(1))-RIP for appropriate constants. When ρ = eO(1) for brevity we say A is w⋆
∞-wRIP.
As discussed after Deﬁnition 37, a wRIP matrix can be thought of as arising from a "semi-random
model" because it strictly generalizes our previously-deﬁned pRIP matrix notion in Deﬁnition 37
with w⋆
∞=
1
m, by setting w⋆to be
1
m times the zero-one indicator vector of rows of G. The main
result of this section is the following theorem regarding sparse recovery with wRIP matrices.
Theorem 66. Let δ ∈(0, 1), r > 0, and suppose R0 ≥∥x⋆∥2 for s-sparse x⋆∈Rd. Then with
probability at least 1 −δ, Algorithm 54 using Algorithm 55 as a step oracle takes as input a (ρ, w⋆
∞)-
wRIP matrix A ∈Rn×d and b = Ax⋆, and computes ˆx satisfying ∥ˆx −x⋆∥2 ≤r in time
O

nd log3(ndρ) log
1
δ · log R0
r

log
R0
r

·
 w⋆
∞sρ2 log d

.
Under the wRIP assumption, Theorem 66 provides a natural interpolation between the fully
random and semi-random generative models. To build intuition, if a pRIP matrix contains a planted
RIP matrix with eO(s) rows (the information-theoretically minimum size), then by setting w⋆
∞≈
1
e
O(s),
we obtain a near-linear runtime of eO(nd). However, in the fully random regime where w⋆
∞≈1
n (i.e.
all of A is RIP), the runtime improves eO(sd) which is sublinear for n ≫s.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
456
The roadmap of our algorithm and its analysis are as follows.
1. In Section 9.6.1, we give an algorithm (Algorithm 54) which iteratively halves an upper bound
on the radius to x⋆, assuming that either an appropriate step oracle (see Deﬁnition 39) based
on short-ﬂat decompositions can be implemented for each iteration, or we can certify that the
input radius bound is now too loose. This algorithm is analyzed in Lemma 170.
2. We state in Assumption 11 a set of conditions on a matrix-vector pair (A, ∆) centered around
the notion of short-ﬂat decompositions, which suﬃce to provide a suﬃcient step oracle im-
plementation with high probability in nearly-linear time.
In Section 9.6.2 we analyze this
implementation (Algorithm 55) in the proof of Lemma 168 assuming the inputs satisfy As-
sumption 11.
3. In Section 9.6.3, we show Assumption 11, with appropriate parameters, follows from A being
wRIP. This is a byproduct of a general equivalence we demonstrate between RIP, restricted
conditioning measures used in prior work [9], and short-ﬂat decompositions.
9.6.1
Radius contraction using step oracles
In this section, we provide and analyze the main loop of our overall algorithm for proving Theorem 66.
This procedure, HalfRadiusSparse, takes as input an s-sparse vector xin and a radius bound R ≥
∥xin −x⋆∥2 and returns an s-sparse vector xout with the guarantee ∥xout −x⋆∥2 ≤
1
2R.
As a
subroutine, it requires access to a "step oracle" Ostep, which we implement in Section 9.6.2 under
certain assumptions on the matrix A.
Deﬁnition 39 (Step oracle). We say that Ostep is a (Cprog, C2, δ)-step oracle for ∆∈Rn and
A ∈Rn×d, if the following holds. Whenever there is v ∈Rd with 1
4 ≤∥v∥2 ≤1 and ∥v∥1 ≤2
√
2s
such that ∆= Av, with probability ≥1 −δ, Ostep returns w ∈Rn
≥0 such that the following two
conditions hold. First,
X
i∈[n]
wi∆2
i ≥Cprog.
(9.17)
Second, there exists a (C2, Cprog
6√s ) short-ﬂat decomposition of A⊤diag (w) ∆:
trunc

A⊤diag (w) ∆, Cprog
6√s

2
≤C2.
(9.18)
Intuitively, (9.18) guarantees that we can write γ = p + e where p denotes a "progress" term
which we require to be suﬃciently short in the ℓ2 norm, and e denotes an "error" term which we
require to be small in ℓ∞. We prove that under certain assumptions on the input A (stated in
Assumption 11 below), we can always implement a step oracle with appropriate parameters.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
457
Assumption 11. The matrix A ∈Rn×d satisﬁes the following. There is a weight vector w⋆∈∆n
satisfying ∥w⋆∥∞≤w⋆
∞, a constant L, ρ ≥1, and a constant K (which may depend on L) such that
for all v ∈Rd with 1
4 ≤∥v∥2 ≤1 and ∥v∥1 ≤2
√
2s we have, deﬁning ∆= Av:
1. A is entrywise bounded by ±ρ, i.e. ∥A∥max ≤ρ.
2.
1
L ≤
X
i∈[n]
w⋆
i ∆2
i ≤L.
(9.19)
3. For W⋆:= diag (w⋆), there is a (L,
1
K√s) short-ﬂat decomposition of A⊤W⋆∆= P
i∈[n] w⋆
i ∆iai:
trunc

A⊤W⋆∆,
1
K√s

2
≤L.
(9.20)
Our Assumption 11 may also be stated in a scale-invariant way (i.e. with (9.19), (9.20) scaling
with ∥v∥2), but it is convenient in our analysis to impose a norm bound on v. Roughly, the second
property in Assumption 11 is (up to constant factors) equivalent to the "restricted strong convexity"
and "restricted smoothness" assumptions of [9], which were previously shown for speciﬁc measure-
ment matrix constructions such as random Gaussian matrices. The use of the third property in
Assumption 11 (the existence of short-ﬂat decompositions for numerically sparse vectors) in design-
ing an eﬃcient algorithm is a key contribution of our work. Interestingly, we show in Section 9.6.3
that these assumptions are up to constant factors equivalent to RIP.
More speciﬁcally, we show that when A is wRIP, we can implement a step oracle for ∆= Av
where v =
1
R(x −x⋆) for some iterate x of Algorithm 54, which either makes enough progress to
advance the algorithm or certiﬁes that v is suﬃciently short, by using numerical sparsity properties
of v. We break this proof into two parts. In Lemma 168, we show that Assumption 11 suﬃces
to implement an appropriate step oracle; this is proven in Section 9.6.2. In Lemma 169, we then
demonstrate the wRIP assumption with appropriate parameters implies our measurement matrix
satisﬁes Assumption 11, which we prove by way of a more general equivalence in Section 9.6.3.
Lemma 168. Suppose A satisﬁes Assumption 11.
Algorithm 55 is a (Cprog, C2, δ) step oracle
StepOracle for (∆, A) with Cprog = Ω(1), C2 = O(1) running in time
O

nd log3(ndρ) log 1
δ

·
 w⋆
∞sρ2 log d

.
Lemma 169. Suppose A ∈Rn×d is (ρ, w⋆
∞)-wRIP with a suitable choice of constants in the RIP
parameters in Deﬁnition 38. Then, A also satisﬁes Assumption 11.
We now give our main algorithm HalfRadiusSparse, assuming access to the step oracle Ostep from
Section 9.6.2 with appropriate parameters, and that A obeys Assumption 11.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
458
Algorithm 54: HalfRadiusSparse(xin, R, Ostep, δ, A, b)
1 Input: s-sparse xin ∈Rd, R ≥∥xin −x⋆∥2 for s-sparse x⋆∈Rd, (Cprog, C2, δ)-step oracle
Ostep for all (∆, A) with ∆∈Rn, δ ∈(0, 1), A ∈Rn×d, b = Ax⋆∈Rn ;
2 Output: s-sparse vector xout that satisﬁes ∥xout −x⋆∥2 ≤1
2R with probability ≥1 −Tδ ;
3 Set x0 ←xin, X ←{x ∈Rd | ∥x −xin∥1 ≤
√
2sR} T ←
l
6C2
2
C2prog
m
, η ←Cprog
2C2
2
;
4 for 0 ≤t ≤T −1 do
5
wt ←Ostep(∆t, A) for ∆t ←1
R(Axt −b), γt ←A⊤diag (wt) ∆t = P
i∈[n][wt]i[∆t]iai ;
6
if P
i∈[n][wt]i[∆t]2
i < Cprog or
trunc(γt, Cprog
6√s )

2 > C2 then
7
Return: xout ←[xt](s)
8
else xt+1 ←argminx∈X ∥x −xt −ηRγt∥2 ;
9 Return: xout ←[xt](s)
Lemma 170. Assume A satisﬁes Assumption 11. With probability at least 1 −Tδ, Algorithm 54
succeeds (i.e. ∥xout −x⋆∥2 ≤1
2R).
Proof. Throughout this proof, condition on the event that all step oracles succeed (which provides
the failure probability via a union bound). We ﬁrst observe that x⋆∈X because of Cauchy-Schwarz,
the 2s-sparsity of xin −x⋆, and the assumption ∥xin −x⋆∥2 ≤R.
Next, we show that in every iteration t of Algorithm 54,
∥xt+1 −x⋆∥2
2 ≤

1 −
C2
2
2C2prog

∥xt −x⋆∥2
2 .
(9.21)
As x⋆∈X, the optimality conditions of xt+1 as minimizing ∥x −(xt −ηRγt)∥2
2 over X imply
2 ⟨xt+1 −xt + ηRγt, xt+1 −x⋆⟩≤0
=⇒∥xt −x⋆∥2
2 −∥xt+1 −x⋆∥2
2 ≥2ηR ⟨γt, xt+1 −x⋆⟩+ ∥xt −xt+1∥2
2 .
(9.22)
Hence, it suﬃces to lower bound the right-hand side of the above expression. Let γt = pt +et denote
the (C2, Cprog
6√s ) short-ﬂat decomposition of γt which exists by Deﬁnition 39 assuming the step oracle
succeeded. We begin by observing
2ηR ⟨γt, xt+1 −xt⟩+ ∥xt −xt+1∥2
2 = 2ηR ⟨et, xt+1 −xt⟩+ 2ηR ⟨pt, xt+1 −xt⟩+ ∥xt −xt+1∥2
2
≥−2ηR ∥et∥∞∥xt+1 −xt∥1 −η2R2 ∥pt∥2
2
≥−ηR2Cprog −η2R2C2
2.
(9.23)
The ﬁrst inequality followed from H¨older on the ﬁrst term and Cauchy-Schwarz on the latter two
terms in the preceding line. The second followed from the ℓ1 radius of X, and the bounds on et and

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
459
pt from (9.18). Next, from Deﬁnition 39, for ∆= ∆t = 1
R(Axt −b) and v = 1
R(xt −x⋆),
2ηR ⟨γt, xt −x⋆⟩= 2ηR
X
i∈[n]
wi∆i ⟨ai, v⟩= 2ηR2 X
i∈[n]
wi∆2
i ≥2ηR2Cprog.
(9.24)
Finally, (9.21) follows from combining (9.22), (9.23), and (9.24), with our choice of η, and the fact
that inducting on this lemma implies the ℓ2 distance to x⋆of the iterates is monotone decreasing.
Next, we claim that regardless of whether Algorithm 54 terminates on Line 7 or Line 11, we
have ∥xt −x⋆∥2 ≤1
4R. Note that the vector v = 1
R(xt −x⋆) satisﬁes Av = ∆:= 1
R(Axt −b). By
assumption the condition ∥v∥1 ≤2
√
2s is met (since xt, x⋆∈X), and upon iterating (9.21) on our
radius bound assumption, this implies that the condition ∥v∥2 ≤1 is met. Hence, if the algorithm
terminated on Line 7, we must have ∥v∥2 ≤1
4R =⇒∥xt −x⋆∥2 ≤1
4R, as otherwise the termination
condition would have been false. On the other hand, by (9.21), after T steps we have
∥xT −x⋆∥2
2 ≤exp

−TC2
2
2C2prog

∥x0 −x⋆∥2
2 ≤1
16R2.
We conclude that at termination, ∥xt −x⋆∥2 ≤1
4R. Now, s-sparsity of x⋆and the deﬁnition of
xout = argmin∥x∥0≤s ∥x −xt∥2 imply the desired
∥xout −x⋆∥2 ≤∥xout −xt∥2 + ∥x⋆−xt∥2 ≤2 ∥x⋆−xt∥2 ≤1
2R.
(9.25)
9.6.2
Designing a step oracle
In this section, we design a step oracle Ostep(∆, A) (see Deﬁnition 39) under Assumption 11 on the
input matrix A ∈Rn×d. Our step oracle iteratively builds a weight vector ¯w ∈Rn
≥0. It will be
convenient to deﬁne
γ ¯
w :=
X
i∈[n]
¯wi∆iai.
(9.26)
Note that a valid step oracle always exists (although it is unclear how to implement the following
solution): namely, setting ¯w = w⋆satisﬁes the oracle assumptions by the second and third conditions
in Assumption 11. In order to ensure Algorithm 58 is indeed a step oracle, we track two potentials
for some µ, C we will deﬁne in Algorithm 55:
Φ2( ¯w) :=
X
i∈[n]
¯wi∆2
i and Φsqmax( ¯w) :=

min
∥p∥2≤L∥¯
w∥1
sqmaxµ (γ ¯
w −p)

+ ∥¯w∥1
4CLs,
where sqmaxµ(x) := µ2 log

X
j∈[d]
exp
 
x2
j
µ2
!
.
(9.27)

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
460
Intuitively, Φ2( ¯w) corresponds to progress on (9.17), and Φsqmax( ¯w) is intended to track the bounds
(9.18). We note the following fact about the sqmax function which follows from direct calculation.
Fact 25. For all x ∈Rd, ∥x∥2
∞≤sqmaxµ(x), and sqmaxµ(x) ≥µ2 log(d).
Also it will be important to note that Φsqmax( ¯w) can be computed to high precision eﬃciently.
We state this claim in the following and defer a full proof to Appendix H.3; we give a subroutine
which performs a binary search on a Lagrange multiplier on the ℓ2 constraint on p, and then solves
for each optimal pj using another binary search based on the Lagrange multiplier value.
Lemma 171. Let δ > 0 and θ ≥0. For any vector γ ∈Rd, we can solve the optimization problem
min
∥p∥2≤θ sqmaxµ(γ −p)
to additive accuracy δ in time
O
 
d log2
 
∥γ∥2
2
µ
√
δ
!!
.
We state the full implementation of our step oracle as Algorithm 55 below.
Algorithm 55: StepOracle(∆, A, δ)
1 Input: ∆∈Rn, A ∈Rn×d satisfying Assumption 11, δ ∈(0, 1) ;
2 Output: w such that if there is v ∈Rd with 1
4 ≤∥v∥2 ≤1 and ∥v∥1 ≤2
√
2s such that
∆= Av, with probability ≥1 −δ, (9.17), (9.18) are satisﬁed with Cp = 1, C2 = O(1).
3 C ←200, µ ←
1
√Cs log d, η ←
1
Kw⋆∞sρ2 log d, N ′ ←⌈log2
1
δ ⌉;
4 for 0 ≤k ≤N ′ do
5
w0 ←0n, N ←⌈5Ln
η ⌉;
6
for 0 ≤t ≤N do
7
if Φ2(wt) ≥1 then Return: w ←wt ;
8
Sample i ∼unif. [n] ;
9
Compute (using Lemma 171) dt ∈[0, ηw⋆
∞] maximizing to additive O( η
n)
Γt(d) := Φ2(wt + dei) −CsΦsqmax(wt + dei)
(9.28)
wt+1 ←wt + dtei ;
10 Return: w ←0n ;
Our main helper lemma bounds the expected increase in Φsqmax from choosing a row of A
uniformly at random, and choosing a step size according to w⋆. We do not know w⋆, but we argue
that our algorithm makes at least this much expected progress. Deﬁne the decomposition promised
by (9.20):
p⋆:= trunc

A⊤W⋆∆,
1
K√s

, e⋆:= A⊤W⋆∆−p⋆.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
461
Furthermore, deﬁne for all i ∈[n],
z(i) := ηw⋆
i (∆iai −p⋆),
(9.29)
where p⋆is given by (9.20). We use {z(i)}i∈[n] as certiﬁcates of Φsqmax's growth in the following.
Lemma 172. Assume that the constant K in Assumption 11 is suﬃciently large, and that ∆=
Av where v satisﬁes the norm conditions in Assumption 11. Then for any ¯w ∈Rn
≥0 such that
Φsqmax( ¯w) ≤C2µ2 log d, and η ≤
1
Kw⋆∞sρ2 log d, we have
Ei∼unif.[n] [Φsqmax( ¯w + ηw⋆
i )] ≤Φsqmax( ¯w) +
1
2CLs · η
n.
Proof. We assume for simplicity L ≥2
√
2 as otherwise we may set L ←max(2
√
2, L) and (9.19)
remains true. Let p ¯
w be the minimizing argument in the deﬁnition of Φsqmax( ¯w) in (9.27). For
any i ∈[n], it is clear that p ¯
w + (ηw⋆
i )p⋆is a valid argument for the optimization problem deﬁning
Φsqmax( ¯w + ηw⋆
i ) by ∥p⋆∥2 ≤L, and since ∥w∥1 grows by ηw⋆
i . Next, deﬁne
F(x) :=
X
j∈[d]
exp
 
x2
j
µ2
!
(9.30)
such that Φsqmax( ¯w) = µ2 log F(x)+ ∥¯
w∥1
4CLs for x = γ ¯
w−p ¯
w. As discussed earlier, since ∥p ¯
w + (ηw⋆
i )p⋆∥2 ≤
∥p ¯
w∥2 + ηw⋆
i L, we conclude
Φsqmax( ¯w + ηw⋆
i ) ≤µ2 log F(x + z(i)) + ∥¯w + ηw⋆
i ∥1
4CLs
.
(9.31)
We next compute
1
n
X
i∈[n]
F(x + z(i)) = 1
n
X
j∈[d]
exp
 
x2
j
µ2
! 
X
i∈[n]
exp
 
2xjz(i)
j
+ (z(i)
j )2
µ2
!

≤1
nF(x) max
j∈[d]

X
i∈[n]
exp
 
2xjz(i)
j
+ (z(i)
j )2
µ2
!
.
(9.32)
We now bound the right-hand side of this expression. For any i ∈[n] and j ∈[d], recalling (9.29),
z(i)
j
 ≤ηw⋆
i (|∆i| ∥ai∥∞+ ∥p⋆∥2) ≤ηw⋆
∞L(√sρ2 + 1).
(9.33)
The second inequality used our bounds from Assumption 11; note that for ∆= Av where v satisﬁes
the norm conditions in Assumption 11, |∆i| ≤ρ ∥v∥1 ≤2
√
2sρ. Hence, if we choose a suﬃciently

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
462
large constant K in Assumption 11, we have
1
µ
z(i)
j
 ≤
√
C
K√s log dρ2 ·
 L(√sρ2 + 1)

≤
1
4C√log d.
Also by the assumption that Φsqmax( ¯w) ≤C2µ2 log d we must have that for all j ∈[d],
|xj|
µ
≤C
p
log d.
Now, using exp(c) ≤1 + c + c2 for |c| ≤1, we get
X
i∈[n]
exp
 
2xjz(i)
j
+ (z(i)
j )2
µ2
!
≤
X
i∈[n]

1 +
2xjz(i)
j
µ2
+
(z(i)
j )2
µ2
+
 
2xjz(i)
j
+ (z(i)
j )2
µ2
!2

≤
X
i∈[n]
 
1 +
2xjz(i)
j
µ2
+ 10C2 log d ·
(z(i)
j )2
µ2
!
.
(9.34)
We control the ﬁrst-order term via the observation that P
i∈[n] z(i) = ηe⋆which is ℓ∞-bounded from
(9.20), so taking the constant K in Assumption 11 suﬃciently large, we have

X
i∈[n]
z(i)
j
µ

≤η
µ ∥e⋆∥∞≤η√C log d
K
=⇒

X
i∈[n]
2xjz(i)
j
µ2

≤2C
p
log d · η√C log d
K
≤η log d
8L
.
(9.35)
In the last inequality we assumed K ≥16C1.5L. We control the second-order term by using (a+b)2 ≤
2a2 + 2b2, ∥p⋆∥∞≤∥p⋆∥2 ≤L, and (9.19):
X
i∈[n]

z(i)
j
2
≤2η2w⋆
∞

X
i∈[n]
w⋆
i [p⋆]2
j +
X
i∈[n]
w⋆
i ∆2
i ρ2

≤2η2w⋆
∞(Lρ2 + L2).
(9.36)
Putting together (9.34), (9.35), and (9.36), with the deﬁnition of µ, we conclude for suﬃciently large
K,
1
n
X
i∈[n]
exp
 
2xjz(i)
j
+ (z(i)
j )2
µ2
!
≤1 + η log d
8Ln + 2η2w⋆
∞(Lρ2 + L2)
nµ2
≤1 + η log d
4Ln .

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
463
Hence, combining the above with (9.32), and using log(1 + c) ≤c for all c,
µ2 log

1
n
X
i∈[n]
F(x + z(i))

≤µ2 log F(x) + µ2η log d
4Ln
= µ2 log F(x) + 1
Cs ·
η
4Ln.
(9.37)
Finally, we compute via (9.31) and concavity of log,
Ei∼unif.[n][Φsqmax( ¯w + ηw⋆
i )] ≤µ2
n
X
i∈[n]
log F(x + z(i)) + ∥¯w∥1
4CLs +
1
4CLs

1
n
X
i∈[n]
ηw⋆
i


≤µ2 log

1
n
X
i∈[n]
F(x + z(i))

+ ∥¯w∥1
4CLs +
1
4CLs · η
n
≤µ2 log F(x) + ∥¯w∥1
4CLs + 1
Cs ·
η
2Ln = B( ¯w) + 1
Cs ·
η
2Ln.
In the last line, we used the bound (9.37).
Finally, we can complete the analysis of Algorithm 55.
Lemma 168. Suppose A satisﬁes Assumption 11.
Algorithm 55 is a (Cprog, C2, δ) step oracle
StepOracle for (∆, A) with Cprog = Ω(1), C2 = O(1) running in time
O

nd log3(ndρ) log 1
δ

·
 w⋆
∞sρ2 log d

.
Proof. It suﬃces to prove Algorithm 55 meets its output guarantees in this time. Throughout this
proof, we consider one run of Lines 5-10 of the algorithm, and prove that it successfully terminates
on Line 7 with probability ≥1
2 assuming A satisﬁes Assumption 11 and that ∆= Av for v satisfying
the norm bounds in Assumption 11. This yields the failure probability upon repeating N ′ times.
For the ﬁrst part of this proof, we assume we can exactly compute ∆t, and carry out the proof
accordingly. We discuss issues of approximation tolerance at the end, when bounding the runtime.
Correctness. We use the notation At := Φ2(wt), Bt := Φsqmax(wt), and Φt := At −CsBt. We
ﬁrst observe that At is 1-Lipschitz, meaning it can only increase by 1 in any given iteration; this
follows from ηw⋆
∞∆2
i ≤
1
8sρ2 ∆2
i ≤1, since ∆2
i = ⟨ai, v⟩2 ≤8sρ2 by ℓ∞-ℓ1 H¨older.
Suppose some run of Lines 5-13 terminates by returning on Line 8 in iteration T, for 0 ≤T ≤N.
The termination condition implies that AT ≥1 = Cprog, so to show that the algorithm satisﬁes
Deﬁnition 39, it suﬃces to show existence of a short-ﬂat decomposition in the sense of (9.18).
Clearly, Φt is monotone non-decreasing in t, since we may always force Γt = 0 by choosing dt = 0.
Moreover, Φ0 = −CsB0 = −Csµ2 log d = −1. The above Lipschitz bound implies that AT ≤2,

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
464
since AT −1 ≤1 by the termination condition; hence,
AT −CsBT = ΦT ≥Φ0 = −1 =⇒BT ≤AT + 1
Cs
≤3
Cs ≤C2µ2 log d.
Note that the above inequality and nonnegativity of sqmaxµ imply that ∥wT ∥1
4LCs ≤
3
Cs, so ∥wT ∥1 ≤
12L. For the given value of C = 200, and the ﬁrst inequality in Fact 25, the deﬁnition of the ﬁrst
summand in B implies there is a short-ﬂat decomposition meeting (9.18) with C2 = L ∥wT ∥1 = O(1).
Hence, we have shown that Deﬁnition 39 is satisﬁed whenever the algorithm returns on Line 7.
We make one additional observation: whenever Φt ≥0, the algorithm will terminate. This follows
since on such an iteration,
At ≥CsBt ≥CsB0 = Csµ2 log d = 1,
since clearly the function B is minimized by the all-zeroes weight vector, attaining value µ2 log d.
Success probability. We next show that with probability at least 1
2, the loop in Lines 5-10 will
terminate. Fix an iteration t. When sampling i ∈[n], the maximum gain in Φt for dt ∈[0, ηw⋆
∞] is
at least that attained by setting dt = ηw⋆
i , and hence
E[Φt+1 −Φt | At ≤1] ≥η
Ln −
η
2Ln =
η
2Ln.
(9.38)
Here, we used that the expected gain in At by choosing dt = ηw⋆
i over a uniformly sampled i ∈[n]
is lower bounded by
η
Ln via (9.19), and the expected gain in CsBt is upper bounded by Lemma 172.
Let Zt be the random variable equal to Φt −Φ0, where we freeze the value of wt′ for all t′ ≥t if
the algorithm ever returns on Line 8 in an iteration t. Notice that Zt ≤2 always: whenever Zt ≥1,
we have Φt ≥0 so the algorithm will terminate, and Zt is 1-Lipschitz because At is. Moreover,
whenever we are in an iteration t where Pr[At ≥1] ≤1
2, applying (9.38) implies
E[Zt+1 −Zt] = E[Zt+1 −Zt | At ≤1] Pr[At ≤1] ≥
η
4Ln.
Clearly, Pr[At ≥1] is a monotone non-decreasing function of t, since At is monotone. After N ≥5Ln
η
iterations, if we still have Pr[At ≥1] ≤1
2, we would obtain a contradiction since recursing the above
display yields E[ZN] > 2. This yields the desired success probability.
Runtime. The cost of each iteration is dominated by the following computation in Line 9: we
wish to ﬁnd d ∈[0, ηw⋆
∞] maximizing to additive O( η
n) the following objective:
Φ2(w + dei) −CsΦsqmax(w + dei).
We claim the above function is a concave function of d.
First, we show Φsqmax is convex (and
the result will then follow from linearity of Φ2). To see this, for two values wi and w′
i, let the

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
465
corresponding maximizing arguments in the deﬁnition of Φsqmax( ¯w + wi) and Φsqmax( ¯w + w′
i) be
denoted p and p′. Then, 1
2(p+p′) is a valid argument for ¯w+ 1
2(wi+w′
i), and by convexity of sqmaxµ
and linearity of the ℓ1 portion, we have the conclusion.
Next, note that all |∆i| are bounded by 2
√
2sρ (proven after (9.33)) and all aij are bounded by ρ
by assumption. It follows that the restriction of Φ2 to a coordinate is 8sρ2-Lipschitz. Moreover the
linear portion of Φsqmax is clearly
1
4CLs-Lipschitz in any coordinate. Finally we bound the Lipschitz
constant of the sqmax part of Φsqmax. It suﬃces to bound Lipschitzness for any ﬁxed p of
sqmaxµ (γ ¯
w −p + di∆iai)
because performing the minimization over p involved in two sqmax(γ ¯
w −p+d∆iai) and sqmax(γ ¯
w −
p + d′∆iai) can only bring the function values closer together. By direct computation the derivative
of the above quantity with respect to di is
X
j∈[d]
∆iaij

2 [γ ¯
w −p + di∆iai]j

qj
for some probability density vector q ∈∆d. Further we have
|γ ¯
w −p + di∆iai|j ≤O
 √sρ2
+ O(1) + 2
√
2sρ2 · (ηw⋆
∞).
Here we used our earlier proof that we must only consider values of ∥p∥2 = O(1) throughout the
algorithm (since ∥wt∥1 = O(1) throughout) and this also implies no coordinate of γ ¯
w can be larger
than (maxi∈[n] |∆i|)(maxi∈[n],j∈[d] |aij|) ∥¯w∥1 by deﬁnition of γ ¯
w. Combined with our bounds on
linear portions this shows Φ2 and Φsqmax are poly(ndρ)-Lipschitz.
Hence, we may evaluate to the desired O( η
n) accuracy by approximate minimization of a Lipschitz
convex function over an interval (Lemma 33, [143]) with a total cost of O(d log3(ndρ)). Here we use
the subroutine of Lemma 171 in Lemma 33 of [143], with evaluation time O(d log2(ndρ)).
The algorithm then runs in NN ′ iterations, each bottlenecked by the cost of approximating
Γt; combining these multiplicative factors yields the runtime. We note that we do not precompute
∆= Av; we can compute coordinates of ∆in time O(d) as they are required by Algorithm 55.
9.6.3
Equivalence between Assumption 11 and RIP
The main result of this section is an equivalence between Assumption 11 and the weighted restricted
isometry property, which requires two helper tools to prove. The ﬁrst is a "shelling decomposition."
Lemma 173. Let v ∈Rd have NS(v) ≤σ. Then if we write v = P
l∈[k] v(l) where v(1) is obtained
by taking the s largest coordinates of v, v(2) is obtained by taking the next s largest coordinates and

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
466
so on (breaking ties arbitrarily so that the supports are disjoint), we have
X
2≤l≤k
v(l)
2 ≤
rσ
s ∥v∥2 .
Proof. Note that the decomposition greedily sets v(l) to be the s largest coordinates (by absolute
value) of v −P
l′∈[l−1] v(l′), zeroing all other coordinates and breaking ties arbitrarily. This satisﬁes
v(l+1)
2 ≤√s
v(l+1)
∞≤1
√s
v(l)
1 .
The last inequality follows since every entry of v(l) is larger than the largest of v(l+1) in absolute
value. Finally, summing the above equation and using disjointness of supports yields
X
2≤l≤k
v(l)
2 ≤1
√s ∥v∥1 ≤
rσ
s ∥v∥2 .
The second bounds the largest entries of image vectors from the transpose of an RIP matrix.
Lemma 174. Let A ∈Rn×d be (s, c)-RIP, and let u ∈Rn. Then,
[A⊤u](s)

2 ≤c ∥u∥2 .
Proof. Let v = [A⊤u](s). The lemma is equivalent to showing ∥v∥2 ≤c ∥u∥2. Note that
∥v∥2
2 =

v, A⊤u

≤∥Av∥2 ∥u∥2 ≤c ∥v∥2 ∥u∥2 .
The ﬁrst inequality used Cauchy-Schwarz, and the second applied the RIP property of A to v, which
is s-sparse by construction. The conclusion follows via dividing by ∥v∥2.
Using these helper tools, we now prove the main result of this section.
Lemma 175. The following statements are true.
1. If A satisﬁes Assumption 11 with weight vector w⋆, then (W⋆)
1
2 A is (s, L)-RIP.
2. If the matrix (W⋆)
1
2 A is RIP with parameters
 
12800L3K2s,
√
L
2
!
for L ≥1, and ∥A∥max ≤ρ, then A satisﬁes Assumption 11.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
467
Proof. We prove each equivalence in turn.
Assumption 11 implies RIP. The statement of RIP is scale-invariant, so we will prove it for
all s-sparse unit vectors v without loss of generality. Note that such v satisﬁes the condition in
Assumption 11, since ∥v∥2 = 1 and ∥v∥1 ≤√s by Cauchy-Schwarz. Then, the second condition of
Assumption 11 implies that for ∆= Av, we have the desired norm preservation:
1
L ≤
(W∗)
1
2 Av

2
2 =
X
i∈[n]
w⋆
i ∆2
i ≤L.
Boundedness and RIP imply Assumption 11. Let v ∈Rd satisfy 1
4 ≤∥v∥2 ≤1 and ∥v∥1 ≤2
√
2s,
and deﬁne ∆:= Av. The ﬁrst condition in Assumption 11 is immediate from our assumed entrywise
boundedness on A, so we begin by demonstrating the lower bound in (9.19). Let
s′ = 12800L3K2s
and let v(1), . . . , v(k) be the shelling decomposition into s′-sparse vectors given by Lemma 173, where
σ = 128s from the ℓ1 and ℓ2 norm bounds on v. By Lemma 173, we have
v(2)
2 + · · · +
v(k)
2 ≤0.1
L ∥v∥2 .
In particular, the triangle inequality then implies 0.9 ∥v∥2 ≤
v(1)
2 ≤∥v∥2.
Next, recall that
P
i∈[n] w⋆
i ∆2
i =
(W⋆)
1
2 Av

2
2. By applying the triangle inequality and since (W⋆)
1
2 A is RIP,
(W⋆)
1
2 Av

2
2 ≥
 (W⋆)
1
2 Av(1)
2 −
k
X
l=2
(W⋆)
1
2 Av(l)
2
!2
≥
 
5
√
L
· 0.9 ∥v∥2 −
√
L
2
· 1
L ∥v∥2
!2
≥16
L ∥v∥2
2 ≥1
L.
In the second inequality, we applied the RIP assumption to each individual term, since all the vectors
are s′-sparse. Similarly, to show the upper bound in (9.19), we have
(W⋆)
1
2 Av

2
2 ≤
 (W⋆)
1
2 Av(1)
2 +
k
X
l=2
(W⋆)
1
2 Av(l)
2
!2
≤
 √
L
2
· ∥v∥2 +
√
L
2
· 1
L ∥v∥2
!2
≤L.
It remains to verify the ﬁnal condition of Assumption 11. First, for u := W
1
2 Av, by applying

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
468
the shelling decomposition to v into s′-sparse vectors {v(l)}l∈[k],
∥u∥2 ≤
X
l∈[k]
(W⋆)
1
2 Av(l)
2 ≤
√
L ∥v∥2 .
(9.39)
Here, we used our earlier proof to bound the contribution of all terms but v(1). Applying Lemma 174
to the matrix (W⋆)
1
2 A and vector u, we have for ∆= Av,

h
A⊤(W⋆)
1
2 u
i
(s′)

2
=


A⊤W⋆∆

(s′)

2 ≤L.
By setting the ℓ2 bounded component in the short-ﬂat decomposition of A⊤W⋆∆to be the top s′
entries by magnitude, it remains to show the remaining coordinates are ℓ∞bounded by
1
K√s. This
follows from the deﬁnition of s′ and (9.39), which imply that the s′ + 1th largest coordinate (in
magnitude) cannot have squared value larger than L2
s′ ≤
1
K2s without contradicting (9.39).
Finally, it is immediate that Lemma 169 follows from Lemma 175.
9.6.4
Putting it all together
At this point, we have assembled the tools to prove our main result on exact recovery.
Theorem 66. Let δ ∈(0, 1), r > 0, and suppose R0 ≥∥x⋆∥2 for s-sparse x⋆∈Rd. Then with
probability at least 1 −δ, Algorithm 54 using Algorithm 55 as a step oracle takes as input a (ρ, w⋆
∞)-
wRIP matrix A ∈Rn×d and b = Ax⋆, and computes ˆx satisfying ∥ˆx −x⋆∥2 ≤r in time
O

nd log3(ndρ) log
1
δ · log R0
r

log
R0
r

·
 w⋆
∞sρ2 log d

.
Proof. With probability at least 1−δ, combining Lemma 168 and Lemma 169 implies that Assump-
tion 11 holds for all v ∈Rd where 1
4 ≤∥v∥2 ≤1 and ∥v∥1 ≤2
√
2s, and that for N = O(log R0
r ),
we can implement a step oracle for N runs of Algorithm 54 in the allotted time, each with fail-
ure probability 1 −δ
N . Moreover, Algorithm 54 returns in O(1) iterations, and allows us to halve
our radius upper bound. By taking a union bound on failure probabilities and repeatedly running
Algorithm 54 N times, we obtain a radius upper bound of r with probability ≥1 −δ.
9.7
Noisy recovery
In this section, we give an algorithm for solving a noisy sparse recovery problem in a wRIP matrix
A ∈Rn×d (where we recall Deﬁnition 38). In particular, we assume that we receive
b = Ax⋆+ ξ,
(9.40)

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
469
for an arbitrary unknown ξ ∈Rn, and x⋆∈Rd is s-sparse. Throughout this section, we will deﬁne
m :=
1
w⋆∞
,
(9.41)
where w⋆
∞is an entrywise bound on w in Deﬁnition 38. We deﬁne the (unknown) "noise ﬂoor"
Rξ :=
1
√m
ξ(m)

2 ,
where we deﬁned ·(m) in Section 9.5. Our goal will be to return x such that ∥x −x⋆∥2 = O(Rξ).
We now formally state the main result of this section here.
Theorem 67. Let δ ∈(0, 1), and suppose R0 ≥∥x⋆∥2 for s-sparse x⋆∈Rd. Further, suppose
A ∈Rn×d is (ρ, w⋆
∞)-wRIP and b = Ax⋆+ ξ, and R1 ≥Rξ :=
ξ(m)

2. Then with probability at
least 1 −δ, Algorithm 56 using Algorithm 56 as a noisy step oracle computes ˆx satisfying
∥ˆx −x⋆∥2 ≤Rﬁnal = Θ(Rξ),
in time
O

ndw⋆
∞s log4(ndρ) log2
d
δ · log
 R0
Rﬁnal

log
 R1
Rﬁnal

· ρ2 log
 R0
Rﬁnal

log
 R1
Rﬁnal

.
Similarly to Theorem 66, Theorem 67 provides a runtime guarantee which interpolates between
the fully random and semi-random settings, and runs in sublinear time when e.g. the entire measure-
ment matrix A satisﬁes RIP. Theorem 67 further provides a reﬁned error guarantee as a function
of the noise vector ξ, which again interpolates based on the "quality" of the weights w. This is
captured through the parameter m =
1
w⋆∞: when m ≈n, the squared error bound R2
ξ scales as the
average squared entry of ξ, and more generally it scales as the average of the largest m entries.
We solve the noisy variant by essentially following the same steps as Section 9.6 and making minor
modiﬁcations to the analysis; we give an outline of the section here. In Section 9.7.1, we generalize
the framework of Section 9.6.1 to the setting where we only receive noisy observations (9.40), while
our current radius is substantially above the noise ﬂoor. We then implement an appropriate step
oracle for this outer loop in Section 9.7.2, and prove that the relevant Assumption 12 used in our
step oracle implementation holds when A is wRIP in Section 9.7.3.
9.7.1
Radius contraction above the noise ﬂoor using step oracles
In this section, we give the main loop of our overall noise-tolerant algorithm, HalfRadiusSparseNoisy,
which takes as input s-sparse xin and a radius bound R ≥∥xin −x⋆∥2. It then returns an s-sparse
vector xout with the guarantee ∥xout −x⋆∥2 ≤
1
2R, as long as R is larger than an appropriate
multiple of Rξ. We give the analog of Deﬁnition 39 in this setting, termed a "noisy step oracle."

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
470
Deﬁnition 40 (Noisy step oracle). We say that Onstep is a (Cprog, C2, Cξ, δ)-noisy step oracle for
e∆∈Rn and A ∈Rn×d if the following holds. Whenever there is v ∈Rd with
1
12 ≤∥v∥2 ≤1 such
that e∆= Av + ξ where
ξ(m)

2 ≤
√m
Cξ , with probability ≥1 −δ, Onstep returns w ∈Rn
≥0 such that
the following two conditions hold. First,
X
i∈[n]
wi e∆i∆i ≥Cprog.
(9.42)
Second, there exists a (C2, Cprog
6√s ) short-ﬂat decomposition of A⊤diag (w) ∆:
trunc

A⊤diag (w) ∆, Cprog
6√s

2
≤C2.
We next characterize how a strengthened step oracle with appropriate parameters also is a noisy
step oracle. First, we will need a deﬁnition.
Deﬁnition 41. For distributions A, B on Rn, we say A stochastically dominates B if there is a
random variable C on Rn whose coordinates are always nonnegative such that the distribution of A
is the same as the distribution of B + C (where C may depend on the realization of B).
We now formalize the properties of the strengthened step oracle that we will construct.
Deﬁnition 42 (Strong step oracle). We say that Ostep is a (Cprog, C2, Cξ, δ)-strong step oracle for
e∆∈Rn and A ∈Rn×d if it satisﬁes all the properties of a standard step oracle (Deﬁnition 39), as
well as the following additional guarantees.
1. For the output weights w, we have
∥w∥1 ≤
CprogC2
ξ
4
· δ.
(9.43)
2. The distribution of w output by the oracle is stochastically dominated by the distribution
δ
4sρ2 log d
δ
Multinom




&
CprogC2
ξ nsρ2 log d
δ
m
'
,




1
n, . . . , 1
n
|
{z
}
n








for some ρ ≥1.
3. Compared to Deﬁnition 39 (the step oracle deﬁnition), we have the stronger guarantees that
A⊤diag (w) ∆admits a (C2, Cprog
24√s ) short-ﬂat decomposition in (9.18), and obtains its guaran-
tees using the bounds
1
12 ≤∥v∥2 ≤1 (instead of a lower bound of 1
4).
We next demonstrate that a strong step oracle is a noisy step oracle.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
471
Lemma 176. Suppose Ostep is a (Cprog, C2, Cξ, δ)-strong step oracle for e∆∈Rn and A ∈Rn×d.
Then, Ostep is also a ( 1
4Cprog, C2, Cξ, 2δ)-noisy step oracle for (e∆, A).
Proof. In the deﬁnition of a noisy step oracle, we only need to check the condition that P
i∈[n] wi e∆i∆i ≥
1
4Cprog for an arbitrary ∆= e∆−ξ where
ξ(m)

2 ≤√mC−1
ξ , as all other conditions are immediate
from Deﬁnition 42. Note that
X
i∈[n]
wi e∆i∆i =
X
i∈[n]
wi e∆i(e∆i −ξi)
≥1
2
X
i∈[n]
wi e∆2
i −1
2
X
i∈[n]
wiξ2
i .
where we used a2 −ab ≥1
2a2 −1
2b2. The ﬁrst sum above is at least 1
2Cprog by assumption. To upper
bound the second sum, we will use the second property in the deﬁnition of a strong step oracle. Let
S ⊂[n] be the set consisting of the m largest coordinates of ξ (with ties broken lexicographically).
Let α be drawn from the distribution
δ
4sρ2 log d
δ
Multinom




&
CprogC2
ξ nsρ2 log d
δ
m
'
,




1
n, . . . , 1
n
|
{z
}
n







.
Note that with 1 −0.1δ probability, by a Chernoﬀbound, we have that P
i∈S αi ≥1
5δCprogC2
ξ . If
this happens, then since S consists of the largest coordinates of ξ, any vector β such that β ≤α
entrywise and ∥β∥1 ≤1
4δCprogC2
ξ must have
X
i∈[n]
βiξ2
i ≤5
4
X
i∈S
αiξ2
i .
Now note that for any S with |S| = m,
E
"X
i∈S
αiξ2
i
#
≤
δCprogC2
ξ
4m
·
ξ(m)
2
2 ≤δCprog
4
.
Combining the above two inequalities and Markov's inequality and the fact that the distribution of
α stochastically dominates the distribution of w, we deduce that with at least 1 −δ probability,
X
i∈[n]
wiξ2
i ≤
1
0.9δ · E


max
β≤α
∥β∥1≤1
4 δCprogC2
ξ
X
i∈[n]
βiξ2
i

≤Cprog
2
.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
472
Putting everything together, we conclude that we have
X
i∈[n]
wi e∆i∆i ≥Cprog
4
with failure probability at most 2δ, completing the proof.
In Section 9.7.2, we prove that if A satisﬁes Assumption 12 (a slightly diﬀerent assumption than
Assumption 11) then with high probability we can implement a strong step oracle with appropriate
parameters. This is stated more formally in the following; recall m is deﬁned in (9.41).
Assumption 12. The matrix A ∈Rn×d satisﬁes the following. There is a weight vector w⋆∈∆n
with ∥w⋆∥∞≤w⋆
∞= 1
m, a constants L, ρ ≥1, and constants K, Cξ (which may depend on L) such
that for all v ∈Rd, ξ ∈Rn with
1
4 ≤∥v∥2 ≤1, ∥v∥1 ≤2
√
2s,
ξ(m)

2 ≤
√m
Cξ
we have, deﬁning e∆= Av + ξ:
1. A is entrywise bounded by ±ρ, i.e. ∥A∥max ≤ρ.
2.
1
L ≤
X
i∈[n]
w⋆
i e∆2
i ≤L.
(9.44)
3. There is a (L,
1
K√s) short-ﬂat decomposition of A⊤W⋆e∆:
trunc

A⊤W⋆e∆,
1
K√s

2
≤L.
(9.45)
Lemma 177. Suppose A satisﬁes Assumption 12. Algorithm 58 is a (Cprog, C2, Cξ, δ) strong step
oracle StrongStepOracle for (e∆, A) with
Cprog = Ω(1), C2 = O (1) , Cξ = O(1), δ = 1
2

C2
105Cprog
2
,
running in time
O

nd log3(ndρ) log 1
δ

·

w⋆
∞sρ2 log2 d
δ

.
Here, in contrast to the noiseless setting, we can only guarantee that the strong step oracle (and
thus also the noisy step oracle) succeeds with constant probability. In our full algorithm, we boost
the success probability of the oracle by running a logarithmic number of independent trials and

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
473
aggregating the outputs. We also show that for an appropriate choice of constants in Deﬁnition 38,
Assumption 12 is also satisﬁed, stated in Lemma 178 and proven in Section 9.7.3.
Lemma 178. Suppose A ∈Rn×d is (ρ, w⋆
∞)-wRIP with a suitable choice of constants in the RIP
parameters in Deﬁnition 38. Then, A also satisﬁes Assumption 12.
Algorithm 56: HalfRadiusSparseNoisy(xin, R, Rξ, Onstep, δ, A, b)
1 Input: s-sparse xin ∈Rd, R ≥∥xin −x⋆∥2 for s-sparse x⋆∈Rd, (Cprog, C2, Cξ, δ′)-noisy
step oracle Onstep for all (∆, A) with ∆∈Rn, δ′ ≤(10−4 Cprog
C2 )2, A ∈Rn×d,
b = Ax⋆+ ξ ∈Rn for
ξ(m)

2 ≤Rξ
√m, with R ≥CξRξ ;
2 Output: s-sparse vector xout that satisﬁes ∥xout −x⋆∥2 ≤1
2R with probability ≥1 −δ
3 x0 ←xin, X ←{x ∈Rd | ∥x −xin∥1 ≤
√
2sR} ;
4 T ←
l
100C2
2
C2prog
m
, η ←Cprog
2C2
2
;
5 Ntrials ←10 log d
δ ;
6 for 1 ≤j ≤Ntrials do
7
xj
0 ←x0 ;
8
for 0 ≤t ≤T −1 do
9
wj
t ←Onstep(∆j
t, A) for ∆j
t ←1
R(Axj
t −b),
γj
t ←A⊤diag

[wj
t]

∆j
t = P
i∈[n][wj
t]i[∆j
t]iai ;
10
if (wj
t, γj
t ) do not meet all of (9.17), (9.18) and the additional criteria in
Deﬁnition 42 then
11
xj
T ←[xj
t](s) ;
12
Break: ;
13
xj
t+1 ←argminx∈X
x −xj
t −ηRγt

2 ;
14 xT ←Aggregate({x1
T , . . . , xNtrials
T
}, R
2 ) ;
15 Return: xout ←[xT ](s) ;
Algorithm 57: Aggregate(S, R)
1 Input: S = {yi}i∈[k] ⊂Rd, R ≥0 such that for some unknown z ∈Rd, at least 0.51k points
yi ∈S have ∥yi −z∥2 ≤R
3 ;
2 Output: ez with ∥ez −z∥2 ≤R ;
3 for 1 ≤i ≤k do
4
if at least 0.51k points yj ∈S satisfy ∥yi −yj∥2 ≤2R
3 then Return: ez ←yi ;
Next, we give a guarantee regarding our geometric post-processing step, Algorithm 57.
Claim 1. Aggregate(S, R) runs in O(k2d) time and meets its output guarantees.
Proof. Let T be the subset of indices i ∈[k] such that ∥yi −z∥≤R
3 . Whenever the algorithm tests
yi for some i ∈T, it will be returned and satisﬁes the desired properties. Now consider any yi

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
474
returned by the algorithm. The ball of radius 2R
3 around yi intersects the ball of radius R
3 around
z, since otherwise it can only contain at most 0.49k points. Thus, ∥yi −z∥2 ≤R. The runtime is
dominated by the time it takes to do k2 distance comparisons of points in Rd.
We remark that is possible that for k = Ω(log 1
δ ) as is the case in our applications, the runtime
of Claim 1 can be improved to have a better dependence on k by subsampling the points and using
low-rank projections for distance comparisons.
Lemma 179. Assume A satisﬁes Assumption 12. Then, Algorithm 56 meets its output guarantees
in time
O
 nd log3(ndρ)

·
 w⋆
∞sρ2 log d

· log2 d
δ

.
Proof. We claim that for each independent trial j ∈[Ntrials], except with probability 1 −Tδ′, the
output xj
T satisﬁes ∥xj
T −x⋆∥2 ≤R
6 . Once we prove this, by Chernoﬀat least 0.51Ntrials of the trials
satisfy ∥xj
T −x⋆∥2 ≤R
6 except with probability at most δ, and then we are done by Claim 1.
It remains to prove the above claim. Fix a trial j, and drop the superscript j for notational
convenience. In every iteration t, e∆:= 1
R(Axt −b) is given to Onstep. Since b = Ax⋆+ ξ, we have
e∆= 1
R(A(x −x⋆) + ξ) = Av + ξ,
for ∥v∥2 ≤1, ∥v∥1 ≤2
√
2s, and ∥ξ∥2,(m) ≤
√m
Cξ , where the last inequality used the assumed bounds
ξ(m)

2 ≤Rξ
√m, Rξ
R ≤1
Cξ
.
Hence, by the assumptions on Onstep, it will not fail for such inputs unless ∥v∥2 ≥
1
12 is violated,
except with probability ≤δ′. If the check in Line 10 fails, then except with probability ≤δ′, the
conclusion ∥xT −x⋆∥2 ≤R
6 follows analogously to Lemma 170, since v = 1
R(x −x⋆).
The other case's correctness follows identically to the proof of Lemma 170, except for one diﬀer-
ence: to lower bound the progress term (9.24), we use the assumption (9.42) which shows
2ηR ⟨γt, xt −x⋆⟩= 2ηR
X
i∈[n]
wi e∆i ⟨ai, v⟩= 2ηR2 X
i∈[n]
wi e∆i∆i ≥2ηR2Cprog.
Hence, following the proof of Lemma 170 (and adjusting for constants), whenever the algorithm
does not terminate we make at least a 50
T fraction of the progress towards x⋆, so in T iterations
(assuming no step oracle failed) we will have ∥xT −x⋆∥2 ≤R
6 .
Finally, the runtime follows from combining Lemma 177 (with constant failure probability) with
a multiplicative overhead of T ·Ntrials due to the number of calls to the step oracle, contributing one
additional logarithmic factor. We adjusted one of the log d terms to become a log d
δ term to account
for the runtime of Aggregate (see Claim 1).

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
475
9.7.2
Designing a strong step oracle
In this section, we design a strong step oracle Ostep(e∆, A) under Assumption 12. As in Section 9.6.2,
our oracle iteratively builds a weight vector ¯w, and sets
γ ¯
w :=
X
i∈[n]
¯wi e∆iai.
We will use essentially the same potentials as in (9.27), deﬁned in the following:
eΦ2( ¯w) :=
X
i∈[n]
¯wi e∆2
i , eΦsqmax( ¯w) :=

min
∥p∥2≤L∥¯
w∥1
sqmaxµ(γ ¯
w −p)

+ ∥¯w∥1
4CLs.
(9.46)
Algorithm 58: StrongStepOracle(e∆, A, δ)
1 Input: e∆∈Rn, A ∈Rn×d satisfying Assumption 11, δ ∈(0, 1) ;
2 Output: (w, γ) such that γ = P
i∈[n] wi e∆iai, and if there is v ∈Rd with
1
12 ≤∥v∥2 ≤1
such that e∆= Av + ξ where
ξ(m)

2 ≤
√m
Cξ , with probability ≥1 −δ, (9.17), (9.18) are
satisﬁed with
Cprog = 1, C2 = O (1) .
Furthermore, the second condition in (9.18) is satisﬁed with the constant 24 rather than 6,
and there is Cξ = O(1) such that (9.43) is also satisﬁed.
3 C ←3200, µ ←
1
√Cs log d, η ←
1
Kw⋆∞sρ2 log d, N ′ ←⌈log2
2
δ ⌉;
4 for 0 ≤k ≤N ′ do
5
w0 ←0n, N ←⌈5Ln
η ⌉;
6
for 0 ≤t ≤N do
7
if eΦ2(wt) ≥1 then Return: γ ←P
i∈[n][wt]i e∆iai, w ←wt ;
8
Sample i ∼unif. [n] ;
9
Compute (using Lemma 171) dt ∈[0, ηw⋆
∞] maximizing to additive O( η
n)
Γt(d) := eΦ2(wt + dei) −CseΦsqmax(wt + dei)
wt+1 ←wt + dtei ;
10 Return: γ ←0d, w ←0n
Algorithm 58 is essentially identical to Algorithm 55 except for changes in constants. We further
have the following which veriﬁes the second property in the deﬁnition of strong step oracle.

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
476
Fact 26. The distribution of w returned by Algorithm 58 is stochastically dominated by the distri-
bution
ηw∗
∞Multinom




5Ln
η ,




1
n, . . . , 1
n
|
{z
}
n








Proof. Every time we inspect a row, we change the corresponding entry of w by at most ηw∗
∞. The
result follows from the number of iterations in the algorithm and uniformity of sampling rows.
To analyze Algorithm 58, we provide appropriate analogs of Lemmas 172 and 168.
Because
Algorithm 58 is very similar to Algorithm 55, we will largely omit the proof of the following statement,
which follows essentially identically to the proof of Lemma 172 up to adjusting constants.
Lemma 180. Assume that the constant K in Assumption 12 is suﬃciently large, and that e∆=
Av + ξ where v, ξ satisfy the norm conditions in Assumption 12. Then for any ¯w ∈Rn
≥0 such that
B( ¯w) ≤C2µ2 log d, we have
Ei∼unif.[n][B( ¯w + ηw⋆
i )] ≤B( ¯w) +
1
2CLs · η
n.
Proof. The analysis is essentially identical to that of Lemma 172; we discuss only the main diﬀerence.
To apply the Taylor expansion of the exponential, Lemma 172 required a bound that
1
µ
z(i)
j
 = O

1
√log d

⇐=
z(i)
j
 = O

1
√s log d

,
for all i ∈[n] and j ∈[d]. Note that in the setting of Lemma 172, we took z(i)
j
= ηw⋆
i (∆iaij −p⋆
j).
Here, we will take z(i)
j
= ηw⋆
i (e∆iaij −p⋆
j); bounds on all of these terms follow identically to in
Lemma 172, except that e∆i = ⟨ai, v⟩+ ξi, so we need to show
ηw⋆
i ρ|ξi| = O

1
√s log d

.
This follows since η ≤
1
log d and |ξi| = O(√m) by assumption. Hence, as w⋆
i ≤1
m by deﬁnition of m,
this is equivalent to m = Ω(sρ2), an explicit assumption we make.
We now give a full analysis of Algorithm 58, patterned oﬀof Lemma 168.
Lemma 177. Suppose A satisﬁes Assumption 12. Algorithm 58 is a (Cprog, C2, Cξ, δ) strong step
oracle StrongStepOracle for (e∆, A) with
Cprog = Ω(1), C2 = O (1) , Cξ = O(1), δ = 1
2

C2
105Cprog
2
,

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
477
running in time
O

nd log3(ndρ) log 1
δ

·

w⋆
∞sρ2 log2 d
δ

.
Proof. The analysis is essentially identical to that of Algorithm 55 in Lemma 168; we discuss diﬀer-
ences here. First, the stochastic domination condition follows from Fact 26 for suﬃciently large Cξ,
K.
For the remaining properties, since the algorithm runs N ′ ≥log2
2
δ times independently, it suﬃces
to show each run meets Deﬁnition 42 with probability ≥
1
2 under the events of Assumption 12,
assuming there exists the desired decomposition e∆= Av + ξ in the sense of Assumption 12. Union
bounding with the failure probability in Fact 26 yields the overall failure probability.
Correctness. As in Lemma 168, it is straightforward to see that eΦ2 is 1-Lipschitz, since the value
of η is smaller than that used in Algorithm 55. The termination condition in iteration T then again
implies eΦ2(wT ) ≥1, and eΦsqmax(wT ) ≤
3
Cs. For C = 3200, this implies the short-ﬂat decomposition
with stronger parameters required by Deﬁnition 42, as well as the ∥wT ∥1 bound.
Success probability.
As in Lemma 168, the expected growth in Φt in any iteration where
Pr[eΦ2(wt) ≥1] ≤
1
2 is ≥
η
4Ln.
Hence, running for ≥
5Ln
η
iterations and using Φt −Φ0 ≤2
yields the claim.
Runtime. This follows identically to the analysis in Lemma 168.
9.7.3
Equivalence between Assumption 12 and RIP
In this section, we prove Lemma 178, restated here for completeness. The proof will build heavily
on our previous developments in the noiseless case, as shown in Section 9.6.3.
Lemma 178. Suppose A ∈Rn×d is (ρ, w⋆
∞)-wRIP with a suitable choice of constants in the RIP
parameters in Deﬁnition 38. Then, A also satisﬁes Assumption 12.
Proof. The analysis is largely similar to the analysis of Lemma 175; we will now discuss the diﬀerences
here, which are introduced by the presence of the noise term ξ. There are three components to
discuss: the upper and lower bounds in (9.44), and the decomposition (9.45).
Regarding the bounds in (9.19), by changing constants appropriately in Deﬁnition 38, we can
assume that A satisﬁes the second property in Assumption 11 with the parameters
4
L and L
4 . In
particular, for ∆= Av, we then have
4
L ≤
X
i∈[n]
w⋆
i ∆2
i ≤L
4 .

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
478
Recall that e∆= ∆+ ξ for some
ξ(m)

2 ≤
√m
Cξ . Hence,
X
i∈[n]
w⋆
i e∆2
i ≤2
X
i∈[n]
w⋆
i ∆2
i + 2
X
i∈[n]
w⋆
i ξ2
i
≤L
2 + 2
 1
m
ξ(m)
2
2

≤L,
for an appropriately large C2
ξ ≥
4
L. Here the ﬁrst inequality used (a + b)2 ≤2a2 + 2b2, and the
second inequality used that the largest P
i∈[n] w⋆
i ξ2
i can be subject to ∥w⋆∥1 = 1 and ∥w⋆∥∞≤1
m is
attained by greedily choosing the m largest coordinates of ξ by their magnitude, and setting w⋆
i = 1
m
for those coordinates. This gives the upper bound in Assumption 12, and the lower bound follows
similarly: for appropriately large C2
ξ ≥L
2 ,
X
i∈[n]
w⋆
i e∆2
i ≥1
2
X
i∈[n]
w⋆
i ∆2
i −1
2
X
i∈[n]
w⋆
i ξ2
i
≥2
L −1
2
 1
m
ξ(m)
2
2

≥1
L.
Lastly, for the decomposition required by (9.45), we will use the decomposition of Lemma 169 for the
component due to P
i∈[n] w⋆
i ∆iai; in particular, assume by adjusting constants that this component
has a ( L
2 ,
1
2K√s) short-ﬂat decomposition. It remains to show that
X
i∈[n]
w⋆
i ξiai = A⊤W⋆ξ.
also admits a ( L
2 ,
1
2K√s) short-ﬂat decomposition, at which point we may conclude by the triangle
inequality. Let u = (W⋆)
1
2 ξ; from earlier, we bounded
∥u∥2
2 ≤1
m
ξ(m)
2
2 =⇒∥u∥2 ≤1
Cξ
.
Hence, applying Lemma 174 using the RIP matrix (W⋆)
1
2 A with appropriate parameters yields the
conclusion, for large enough Cξ. In particular, the ℓ2-bounded part of the decomposition follows
from Lemma 174, and the proof of the ℓ∞-bounded part is identical to the proof in Lemma 175.
9.7.4
Putting it all together
We now prove our main result on noisy recovery.
Theorem 67. Let δ ∈(0, 1), and suppose R0 ≥∥x⋆∥2 for s-sparse x⋆∈Rd. Further, suppose
A ∈Rn×d is (ρ, w⋆
∞)-wRIP and b = Ax⋆+ ξ, and R1 ≥Rξ :=
ξ(m)

2. Then with probability at

CHAPTER 9. SEMI-RANDOM LINEAR SYSTEMS
479
least 1 −δ, Algorithm 56 using Algorithm 56 as a noisy step oracle computes ˆx satisfying
∥ˆx −x⋆∥2 ≤Rﬁnal = Θ(Rξ),
in time
O

ndw⋆
∞s log4(ndρ) log2
d
δ · log
 R0
Rﬁnal

log
 R1
Rﬁnal

· ρ2 log
 R0
Rﬁnal

log
 R1
Rﬁnal

.
Proof. Our algorithm will iteratively maintain a guess Rguess on the value of
1
√m
ξ(m)

2, initialized
at Rguess ←R1. For each value of Rguess ≥Rξ, the hypothesis of Algorithm 56 is satisﬁed, and hence
using a strategy similar to the proof of Theorem 66 (but terminating at accuracy R = O(Rguess)
where the constant is large enough to satisfy the assumption R ≥CξRguess) results in an estimate
at distance R with probability at least 1 −δ, with runtime
O

ndw⋆
∞sρ2 log4(ndρ) log2
d
δ · log
 R0
Rﬁnal

· ρ2 log
 R0
Rﬁnal

.
The runtime above follows from Lemma 179.
Our overall algorithm repeatedly halves Rguess, and outputs the last point returned by a run of
the algorithm where it can certify a distance bound to x⋆of R = CξRguess. We use Rﬁnal to denote
CξRguess on the last run. Clearly for any Rguess ≥Rξ this certiﬁcation will succeed, so we at most
lose a factor of 2 in the error guarantee as we will have Rﬁnal ≤2CξRξ. The ﬁnal runtime follows
from adjusting δ by a factor of O(log
R1
Rfinal ) to account for the multiple runs of the algorithm.

Chapter 10
Towards an Oracle Complexity of
Sampling
This chapter is based on [344, 346, 345], with Yin Tat Lee and Ruoqi Shen.
10.1
Introduction
Since its study was pioneered by the celebrated randomized convex body volume approximation
algorithm of Dyer, Frieze, and Kannan [212], designing samplers for logconcave distributions has been
a very active area of research in theoretical computer science and statistics with many connections
to other ﬁelds. In a generic form, the problem can be stated as: sample from a distribution whose
negative log-density is convex, under various access models to the distribution.
Developing eﬃcient algorithms for sampling from structured logconcave densities is a topic that
has received signiﬁcant recent interest due to its widespread practical applications. There are many
types of structure which densities commonplace in applications may possess that are exploitable
for improved runtimes. Examples of such structure include derivative bounds ("well-conditioned
densities") and various types of separability (e.g. "composite densities" corresponding to possibly
non-smooth regularization or restrictions to a set, and "logconcave ﬁnite sums" corresponding to
averages over multiple data points).1 Building an algorithmic theory for sampling these latter two
families, which are not well-understood in the literature, is a primary motivation of this chapter.
There are strong parallels between the types of structured logconcave families garnering recent
attention and the classes of convex functions known to admit eﬃcient ﬁrst-order optimization algo-
rithms. Notably, gradient descent and its accelerated counterpart [417] are well-known to quickly
1We make this terminology more precise in Section 10.2.1, which contains various deﬁnitions used in this paper.
480

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
481
optimize a well-conditioned function, and have become ubiquitous in both practice and theory. Sim-
ilarly, various methods have been developed for eﬃciently optimizing non-smooth but structured
composite objectives [66] and well-conditioned ﬁnite sums [17].
Logconcave sampling and convex optimization are intimately related primitives (cf. e.g. [75, 2]), so
it is perhaps unsurprising that there are analogies between the types of structure algorithm designers
may exploit. Nonetheless, our understanding of the complexity landscape for sampling is quite a
bit weaker in comparison to counterparts in the ﬁeld of optimization; few lower bounds are known
for the complexity of sampling tasks, and obtaining stronger upper bounds is an extremely active
research area (contrary to optimization, where matching bounds exist in many cases). Moreover
(and perhaps relatedly), the toolkit for designing logconcave samplers is comparatively lacking; for
many important primitives in optimization, it is unclear if there are analogs in sampling, possibly
impeding improved bounds.
Broadly speaking, this chapter presents improved upper and lower bounds aimed towards advanc-
ing the program of understanding the following question for various natural, practically-motivated
types of structure and oracle access.
What is the oracle complexity of sampling from various structured density families?
The upper bounds presented in this chapter broadly fall under the themes of (1) understanding which
types of structured logconcave distributions admit eﬃcient samplers, and (2) leveraging connections
between optimization and sampling for algorithm design. The lower bounds presented in this chapter
aim to characterize the complexity of sampling from logconcave densities exhibiting a basic form of
structure (well-conditioning), by demonstrating hardness for various standard sampling frameworks.
Altogether, we address the oracle complexity of sampling on each of the following fronts, which
constitute the primary technical contributions of this chapter.
1. We give a general reduction framework for bootstrapping samplers with mixing times with
polynomial dependence on a conditioning measure κ to mixing times with linear dependence
on κ. The framework is heavily motivated by a perspective on proximal point methods in
structured convex optimization as instances of optimizing composite objectives, and leverages
this connection via a surprisingly simple analysis (cf. Theorem 68).
2. We develop novel "base samplers" for composite logconcave distributions and logconcave ﬁnite
sums (cf. Theorems 69, 70). The former is the ﬁrst composite sampler with stronger guar-
antees than those known in the general logconcave setting. The latter constitutes the ﬁrst
high-accuracy ﬁnite sum sampler whose gradient query complexity improves upon the na¨ıve
strategy of querying full gradients of the negative log-density in each iteration. Using our
novel base samplers within our reduction framework, we obtain state-of-the-art samplers for
all of the aforementioned structured families, i.e. well-conditioned, composite, and ﬁnite sum,

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
482
as Corollaries 45, 46, and 47.
3. We give lower bounds on the performance of two of the most popular sampling methods
in practice, the Metropolis-adjusted Langevin algorithm (MALA) and multi-step Hamiltonian
Monte Carlo (HMC) with a leapfrog integrator, when applied to well-conditioned distributions.
Our main lower bound result, Theorem 73, is a nearly-tight lower bound of eΩ(κd) on the
mixing time of MALA from an exponentially warm start, matching upper bounds obtained in
this chapter up to logarithmic factors and answering an open question of [137]. We also show
that a polynomial dependence on dimension is necessary for the relaxation time of HMC under
any number of leapfrog steps, and bound the gains achievable by changing the step count (see
Theorem 74 for a representative result).
We formally state our upper bound results in Section 10.1.1, our lower bound results in Sec-
tion 10.1.2, and situate each in the literature in Section 10.1.3. Section 10.1.4 is a technical overview
of our approaches for developing our base samplers for composite and ﬁnite sum-structured densi-
ties, as well as our reduction framework (Section 10.1.4). Section 10.1.5 is a technical overview for
our various lower bound constructions. Finally, Section 10.1.6 gives a roadmap for the rest of the
chapter.
10.1.1
Our results: upper bounds
Before stating our results, we ﬁrst require the notion of a restricted Gaussian oracle, whose deﬁnition
is a key ingredient in giving our reduction framework as well as our later composite samplers.
Deﬁnition 43 (Restricted Gaussian oracle). O(λ, v) is a restricted Gaussian oracle (RGO) for
convex g : Rd →R if it returns
O(λ, v) ←sample from the distribution with density ∝exp

−1
2λ ∥x −v∥2
2 −g(x)

.
In other words, an RGO asks to sample from a multivariate Gaussian (with covariance a multiple
of the identity), "restricted" by some convex function g. Intuitively, if we can reduce a sampling
problem for the density ∝exp(−g) to calling an RGO a small number of times with a small value of
λ, each RGO subproblem could be much easier to solve than the original problem. This can happen
for a variety of reasons, e.g. if the regularized density is extremely well-conditioned, or because it
inherits concentration properties of a Gaussian. This idea of reducing a sampling problem to multiple
subproblems, each implementing an RGO, underlies the framework of Theorem 68. Because the
idea of regularization by a large Gaussian component repeatedly appears in this paper, we make the
following more speciﬁc deﬁnition for convenience, which lower bounds the size of the Gaussian.
Deﬁnition 44 (η-RGO). We say O(λ, v) is an η-restricted Gaussian oracle (η-RGO) if it satisﬁes
Deﬁnition 43 with the restriction that parameter λ is required to be always at most η in calls to O.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
483
Variants of our notion of an RGO have implicitly appeared previously [149, 401], and eﬃcient
RGO implementation was a key subroutine in the fastest sampling algorithm for general logconcave
distributions [149]. It also extends a similar oracle used in composite optimization, which we will
discuss shortly. However, the explicit use of RGOs in a framework such as Theorem 68 is a novel
technical innovation of our work, and we believe this abstraction will ﬁnd further uses.
Proximal reduction framework. In Section 10.3, we prove correctness of our proximal reduction
framework, whose guarantees are stated in the following Theorem 68.
Theorem 68. Let π be a distribution on Rd with
dπ
dx(x) ∝exp(−foracle(x)) such that foracle is
µ-strongly convex, and let ϵ ∈(0, 1). Let η ≤1
µ, T = Θ( 1
ηµ log
d
ηµϵ) for some β ≥1, and O be a
η-RGO for foracle. Algorithm 59, initialized at the minimizer of foracle, runs in T iterations, each
querying O a constant number of times, and obtains ϵ total variation distance to π.
In other words, if we can implement an η-RGO for a µ-strongly convex function foracle in time
TRGO, we can sample from exp(−foracle) in time eO( 1
ηµ · TRGO).
To highlight the power of this
reduction framework, suppose there was an existing sampler A for densities ∝exp(−f) with mixing
time eO(κ10√
d), where f : Rd →R is L-smooth, µ-strongly convex, and has condition number κ = L
µ
(cf. Section 10.2.1 for deﬁnitions).2 Choosing η = 1
L and foracle ←f in Theorem 68 yields a sampler
whose mixing time is eO(κ · TRGO), where TRGO is the cost of sampling from a density proportional
to
exp

−L
2 ∥x −v∥2
2 −f(x)

,
for some v ∈Rd. However, observe that this distribution has a negative log-density with constant
condition number L+L
L+µ ≤2! By using A as our RGO, we have TRGO = eO(
√
d), and the overall
mixing time is eO(κ
√
d). Leveraging Theorem 68 in applications, we obtain the following new results,
improving mixing of various "base samplers" which we bootstrap as RGOs for regularized densities.
Well-conditioned densities.
In an earlier version of part of this chapter [344], it was shown
that a variant of Metropolized Hamiltonian Monte Carlo obtains a mixing time of eO(κd log3 κd
ϵ ) for
sampling a density on Rd with condition number κ. The analysis of [344] was somewhat delicate, and
required reasoning about conditioning on a nonconvex set with desirable concentration properties.
In Section 10.4.1, we prove Corollary 45, improving [344] by roughly a logarithmic factor with a
signiﬁcantly simpler analysis. For completeness, we include the only technical fact required in its
proof from [344], a concentration bound on the norm of the gradient of a logsmooth density, in
Appendix I.6.
Corollary 45. Let π be a distribution on Rd with dπ
dx(x) ∝exp (−f(x)) such that f is L-smooth and
µ-strongly convex, and let ϵ ∈(0, 1), κ = L
µ . Assume access to x∗= argminx∈Rdf(x). Algorithm 59
with η =
1
8Ld log(κ) using Algorithm 60 as a restricted Gaussian oracle for f uses O(κd log κ log κd
ϵ )
gradient queries in expectation, and obtains ϵ total variation distance to π.
2No sampler with mixing time scaling as poly(κ)
√
d is currently known.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
484
We include Corollary 45 as a warmup for our more complicated results, as a way to showcase
the use of our reduction framework in a slightly diﬀerent way than the one outlined earlier. In
particular, in proving Corollary 45, we will choose a signiﬁcantly smaller value of η, at which point
a simple rejection sampling scheme implements each RGO with expected constant gradient queries.
We give another algorithm matching Corollary 45 with a deterministic query complexity bound
as Corollary 49. The algorithm of Corollary 49 is interesting in that it is entirely a zeroth-order
algorithm, and does not require access to a gradient oracle. To our knowledge, in the well-conditioned
optimization setting, no zeroth-order query complexities better than roughly √κd are known, e.g.
simulating accelerated gradient descent with a value oracle; thus, our sampling algorithm has a
query bound oﬀby only ˜O(√κ) from the best-known optimization algorithm. We are hopeful this
result may help in the search for query lower bounds for structured logconcave sampling.
Composite densities with a restricted Gaussian oracle. In Section 10.5, we develop a sampler for
densities on Rd proportional to exp(−f(x) −g(x)), where f has condition number κ and g admits
a restricted Gaussian oracle O. We state its guarantees here.
Theorem 69. Let π be a distribution on Rd with dπ
dx(x) ∝exp (−f(x) −g(x)) such that f is L-
smooth and µ-strongly convex, and let ϵ ∈(0, 1).
Let η ≤
1
32Lκd log(κ/ϵ) (where κ =
L
µ ), T =
Θ( 1
ηµ log( κd
ϵ )), and let O be a η-RGO for g.
Further, assume access to the minimizer x∗=
argminx∈Rd{f(x) + g(x)}. There is an algorithm which runs in T iterations in expectation, each
querying a gradient oracle of f and O a constant number of times, and obtains ϵ total variation
distance to π.
The assumption that the composite component g admits an RGO can be thought of as a measure
of "simplicity" of g. This mirrors the widespread use of a proximal oracle as a measure of simplicity
in the context of composite optimization [66], which we now deﬁne.
Deﬁnition 45 (Proximal oracle). O(λ, v) is a proximal oracle for convex g : Rd →R if it returns
O(λ, v) ←argminx∈Rd
 1
2λ ∥x −v∥2
2 + g(x)

.
Many regularizers g in deﬁning composite optimization objectives, which are often used to enforce
a quality such as sparsity or "simplicity" in a solution, admit eﬃcient proximal oracles. In particular,
if the proximal oracle subproblem admits a closed form solution (or otherwise is computable in O(d)
time), the regularized objective can be optimized at essentially no asymptotic loss. It is readily
apparent that our RGO (Deﬁnition 43) is the extension of Deﬁnition 45 to the sampling setting. In
[401], a variety of regularizations arising in practical applications including coordinate-separable g
(such as restrictions to a coordinate-wise box, e.g. for a Bayesian inference task where we have side
information on the ranges of parameters) and ℓ1 or group Lasso regularized densities were shown
to admit RGOs. Our composite sampling results achieve a similar "no loss" phenomenon for such
regularizations, with respect to existing well-conditioned samplers.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
485
By choosing the largest possible value of η in Theorem 69, we obtain an iteration bound of
eO(κ2d). In Section 10.4.2, we prove Corollary 46, which improves Theorem 69 by roughly a κ factor.
Corollary 46. Let π be a distribution on Rd with dπ
dx(x) ∝exp(−f(x)−g(x)) such that f is L-smooth
and µ-strongly convex, and let ϵ ∈(0, 1), κ = L
µ . Assume access to x∗= argminx∈Rd{f(x)+g(x)} and
let O be a restricted Gaussian oracle for g. There is an algorithm (Algorithm 59 using Theorem 69
as a restricted Gaussian oracle) which runs in O(κd log3 κd
ϵ ) iterations in expectation, each querying
a gradient of f and O a constant number of times, and obtains ϵ total variation distance to π.
To sketch the proof, choosing η = 1
L in Theorem 68 yields an algorithm running in eO( 1
ηµ) = eO(κ)
iterations.
In each iteration, the RGO subproblem asks to sample from the distribution whose
negative log-density is f(x) + g(x) + L
2 ∥x −v∥2
2 for some v ∈Rd, so we can call Theorem 69, where
the "well-conditioned" portion f(x)+ L
2 ∥x −v∥2
2 has constant condition number. Thus, Theorem 69
runs in eO(d) iterations to solve the subproblem, yielding the result. In fact, Corollary 46 nearly
matches Corollary 45 in the case g = 0 uniformly. Surprisingly, this recovers the runtime of [344]
without appealing to strong gradient concentration bounds (e.g. [344], Theorem 3.2).
Logconcave ﬁnite sums.
In Section 10.6, we initiate the study of mixing times for sampling
logconcave ﬁnite sums with polylogarithmic dependence on accuracy. We give the following result.
Theorem 70. Let π be a distribution on Rd with dπ
dx(x) ∝exp(−F(x)), where F(x) = 1
n
Pn
i=1 fi(x)
is µ-strongly convex, fi is L-smooth and convex ∀i ∈[n], κ = L
µ , and ϵ ∈(0, 1). Assume access
to x∗= argminx∈RdF(x). Algorithm 64 uses O
 κ2d log4 nκd
ϵ

value queries to summands {fi}i∈[n],
and obtains ϵ total variation distance to π.
For a zeroth-order algorithm, Theorem 70 serves as a surprisingly strong baseline as it nearly
matches the previously best-known bound for zeroth-order well-conditioned sampling when n = 1;
however, when e.g. κ ≈d, the complexity bound is at least cubic. By using Theorem 70 within the
framework of Theorem 68, we obtain the following improved result.
Corollary 47 (Improved ﬁrst-order logconcave ﬁnite sum sampling). In the setting of Theorem 70,
Algorithm 59 using Algorithm 64 and SVRG [300] as a restricted Gaussian oracle for F uses
O

n log
nκd
ϵ

+ κ
√
nd log3.5
nκd
ϵ

+ κd log5
nκd
ϵ

= eO

n + κ max

d,
√
nd

queries to ﬁrst-order oracles for summands {fi}i∈[n], and obtains ϵ total variation distance to π.
Corollary 47 has several surprising properties. First, its bound when n = 1 gives yet another way
of (up to polylogarithmic factors) recovering the runtime of [344] without gradient concentration.
Second, up to a eO(max(1, p n
d )) factor, it is essentially the best runtime one could hope for without an
improvement when n = 1. This is in the sense that eO(κd) is the best known runtime for n = 1 (with
near-matching lower bounds, developed later in this chapter), and to our knowledge every eﬃcient

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
486
well-conditioned sampler requires minimizer access, i.e. eO(n) gradient queries [539]. Interestingly,
when n = 1, Algorithm 64 can be signiﬁcantly simpliﬁed, and becomes the standard Metropolized
random walk [210]; this yields Corollary 49, an algorithm attaining the iteration complexity of
Corollary 45 while only querying a value oracle for f.
10.1.2
Our results: lower bounds
The restricted problem of sampling from a particular family of distributions, which we call "well-
conditioned distributions," has garnered a substantial amount of recent research eﬀort from the
algorithmic learning and statistics communities. This speciﬁc family is interesting for a number
of reasons. First of all, it is practically relevant: Bayesian methods have found increasing use in
machine learning applications [60], and many distributions arising from these methods are well-
conditioned, such as multivariate Gaussians, mixture models with small separation, and densities
arising from Bayesian logistic regression with a Gaussian prior [210]. Moreover, for several of the most
widely-used sampler implementations in popular packages [1, 113], such as the Metropolis-adjusted
Langevin algorithm (MALA) and Hamiltonian Monte Carlo (HMC), the target density having a
small condition number is in some sense a minimal assumption for known provable guarantees.
Finally, the highly-documented success of ﬁrst-order (gradient-based) methods in optimization
[65], which are particularly favorable in the well-conditioned setting, has driven a recent interest in
connections between optimization and sampling. Exploring this connection has been highly fruitful:
since seminal work of [301], which demonstrated that the continuous-time Langevin dynamics which
MALA and HMC discretize has an interpretation as gradient descent on density space, a ﬂurry of
work including [153, 130, 210, 156, 206, 205, 128, 129, 480, 402, 137], as well as upper bound results
in this chapter (see Section 10.1.1), have obtained improved upper bounds for the mixing of various
discretizations of the Langevin dynamics for sampling from well-conditioned densities.
Many of
these works have drawn inspiration from techniques from ﬁrst-order optimization.
On the other hand, demonstrating lower bounds on the complexity of sampling tasks (in the
well-conditioned regime or otherwise) has proven to be a remarkably challenging problem. To our
knowledge, there are very few unconditional lower bounds for sampling tasks (i.e. the complexity
of sampling from a family of distributions under some query model).
This is in stark contrast
to the theory of optimization, where there are matching upper and lower bounds for a variety of
fundamental tasks and query models, such as optimization of a convex function under ﬁrst-order
oracle access [418]. This gap in the development of the algorithmic theory of sampling is the primary
motivation for our work, wherein we aim to answer the following more restricted question.
What is the complexity of the popular sampling methods, MALA and HMC,
for sampling well-conditioned distributions?

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
487
The problem we study is still less general than unconditional query lower bounds for sampling, in that
our lower bounds are algorithm-speciﬁc; we characterize the performance of particular algorithms
for sampling a distribution family. However, we believe asking this question, and developing an
understanding of it, is an important ﬁrst step towards a theory of complexity for sampling. On the
one hand, lower bounds for speciﬁc algorithms highlight weaknesses in their performance, pinpointing
their shortcomings in attaining faster rates. This is useful from an algorithm design perspective, as
it clariﬁes what the key technical barriers are to overcome. On the other hand, the hard instances
which arise in designing lower bounds may have important structural properties which pave the way
to stronger and more general (i.e. algorithm-agnostic) lower bounds.
For these reasons, in this work we focus on characterizing the complexity of the MALA and HMC
algorithms (see Sections 10.2.3 and 10.2.4 for algorithm deﬁnitions), which are often the samplers of
choice in practice, by lower bounding their performance when they are used to sample from densities
proportional to exp(−f(x)), where f : Rd →R has a ﬁnite condition number. In particular, f is said
to have a condition number of κ < ∞if it is L-smooth and µ-strongly convex (has second derivatives
in all directions in the range [µ, L]), where κ = L
µ . We will also overload this terminology and say
the density itself has condition number κ. We call such a density (with ﬁnite κ) "well-conditioned."
Finally, we explicitly assume throughout that κ = O(d4), as otherwise in light of our lower bounds
the general-purpose logconcave samplers of [373, 295, 127] are preferable.
Our primary contribution is a nearly-tight characterization of the performance of MALA for
sampling from two high-dimensional distribution families without a warm start assumption: well-
conditioned Gaussians, and the more general family of well-conditioned densities. In Sections 10.7
and 10.8, we prove the following two lower bounds on MALA's complexity, which is a one-parameter
algorithm (for a given target distribution) depending only on step size. We also note that we ﬁx
a scale [1, κ] on the eigenvalues of the function Hessian up front, because otherwise the non-scale-
invariance of the step size can be exploited to give much more trivial lower bounds (cf. Appendix I.7).
Theorem 71. For every step size, there is a target Gaussian on Rd whose negative log-density
always has Hessian eigenvalues in [1, κ], such that the relaxation time of MALA is Ω( κ
√
d
√log d).
Theorem 72. For every step size, there is a target density on Rd whose negative log-density always
has Hessian eigenvalues in [1, κ], such that the relaxation time of MALA is Ω( κd
log d).
To give more context on Theorems 71 and 72, MALA is an example of a Metropolis-adjusted
Markov chain, which in every step performs updates which preserve the stationary distribution.
Indeed, it can be derived by applying a Metropolis ﬁlter on the standard forward Euler discretization
of the Langevin dynamics, a stochastic diﬀerential equation with stationary density ∝exp(−f(x)):
dxt = −∇f(xt)dt +
√
2dWt,

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
488
where Wt is Brownian motion. Such Metropolis-adjusted methods typically provide total variation
distance guarantees, and attain logarithmic dependence on the target accuracy.3 The mixing of such
chains is governed by their relaxation time, also known as the inverse spectral gap (the diﬀerence
between 1 and the second-largest eigenvalue of the Markov chain transition operator).
However, in the continuous-space setting, it is not always clear how to relate the relaxation time
to the mixing time, which we deﬁne as the number of iterations it takes to reach total variation
distance 1
e from the stationary distribution from a given warm start (we choose 1
e for consistency
with the literature, but indeed any constant bounded away from 1 will do). There is an extensive
line of research on when it is possible to relate these two quantities (see e.g. [52]), but typically these
arguments are tailored to properties of the speciﬁc Markov chain, causing relaxation time lower
bounds to not be entirely satisfactory in some cases. We thus complement Theorems 71 and 72 with
a mixing time lower bound from an exponentially warm start, as follows.
Theorem 73. For every step size, there is a target density on Rd whose negative log-density al-
ways has Hessian eigenvalues in [1, κ], such that MALA initialized at an exp(d)-warm start requires
Ω(
κd
log2 d) iterations to reach e−1 total variation distance to the stationary distribution.
We remark that Theorem 73 is the ﬁrst mixing time lower bound for discretizations of the
Langevin dynamics we are aware of, as other related lower bounds have primarily been on relaxation
times [129, 344, 137]. Up to now, it is unknown how to obtain a starting distribution for a general
distribution with condition number κ with warmness better than κd (which is obtained by the
starting distribution N(x∗, 1
LI) where L is the smoothness parameter and x∗is the mode).4 A line
of work [210, 128, 344] analyzed the performance of MALA under this warm start, culminating
in a mixing time of eO(κd), where eO hides logarithmic factors in κ, d, and the target accuracy.
On the other hand, a recent work [137] demonstrated that MALA obtains a mixing time scaling
as eO(poly(κ)
√
d), when initialized at a polynomially warm start,5 and further showed that such
a mixing time is tight (in its dependence on d). They posed as an open question whether it was
possible to obtain eO(poly(κ)d1−Ω(1)) mixing from an explicit starting distribution.
We address this question by proving Theorem 73, showing that the eO(κd) rate of [344] for
MALA (recovered by the upper bound developments of this chapter) applied to a κ-conditioned
density is tight up to logarithmic factors from an explicit "bad" warm start. Concretely, to prove
Theorems 71-73, in each case we exhibit an exp(−d)-sized set according to the stationary measure
where either the chain cannot move in poly(d) steps with high probability, or must choose a very
small step size. Beyond exhibiting a mixing bound, this demonstrates the subexponential warmness
assumption in [137] is truly necessary for their improved bound. To our knowledge, this is the ﬁrst
3We note this is in contrast with a diﬀerent family of unadjusted discretizations, which are analyzed by coupling
them with the stochastic diﬀerential equation they simulate (see e.g. [153, 130] for examples), at the expense of a
polynomial dependence on the target accuracy; we focus on Metropolis-adjusted discretizations in this work.
4The warmness of a distribution is the worst-case ratio between the measures it and the stationary assign to a set.
5As discussed, it is currently unknown how to obtain such a warm start generically.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
489
nearly-tight characterization of a speciﬁc sampling algorithm's performance in all parameters, and
improves lower bounds of [137, 344]. It also implies that to go beyond eO(κd) mixing requires a
subexponential warm start.
The lower bound statement of Theorem 73 is warmness-sensitive, and is of the following (some-
what non-standard) form: for β = exp(d), we provide a lower bound on the quantity
inf
algorithm parameters
sup
starts of warmness ≤β
densities in target family
mixing time of algorithm.
In other words, we are allowed to choose both the hard density and starting distribution adaptively
based on the algorithm parameters (in the case of MALA, our choices respond to the step size).
We note that this type of lower bound is compatible with standard conductance-based upper bound
analyses, which typically only depend on the starting distribution through the warmness parameter.
In Section 10.10, we further study the multi-step generalization of MALA, known in the literature
as Hamiltonian Monte Carlo with a leapfrog integrator (which we refer to in this paper as HMC). In
addition to a step size η, HMC is parameterized by a number of steps per iteration K; in particular,
HMC makes K gradient queries in every step to perform a K-step discretization of the Langevin
dynamics, before applying a Metropolis ﬁlter. It was recently shown in [128] that under higher
derivative bounds, balancing η and K more carefully depending on problem parameters could break
the apparent κd barrier of MALA, even from an exponentially warm start.
It is natural to ask if there is a stopping point for improving HMC. We demonstrate that HMC
cannot obtain a better relaxation time than eO(κ
√
dK−1) for any K, even when the target is a
Gaussian. Since every HMC step requires K gradients, this suggests eΩ(κ
√
d) queries are necessary.
Theorem 74. For every step size and count, there is a target Gaussian on Rd whose negative log-
density always has Hessian eigenvalues in [1, κ], such that the relaxation time of HMC is Ω(
κ
√
d
K√log d).
In Appendix I.8, we also give some lower bounds on how much increasing K can help the
performance of HMC in the in-between range κ
√
d to κd.
In particular, we demonstrate that
if K ≤dc for some constant c ≈0.1, then the K-step HMC Markov chain can only improve
the relaxation time of Theorem 74 by roughly a factor K2, showing that to truly go beyond a
κd relaxation time by more than a do(1) factor, the step size must scale polynomially with the
dimension (Proposition 84). We further demonstrate how to extend the mixing time lower bound of
Theorem 73 in a similar manner, demonstrating formally for small K that (up to logarithmic factors)
the gradient query complexity of HMC cannot be improved beyond κd by more than roughly a K
factor (Proposition 85).
Our mixing lower bound technique in Theorem 73 does not directly extend to give a complemen-
tary mixing lower bound for Theorem 74 for all K, but we defer this to interesting future work.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
490
10.1.3
Previous work
Logconcave sampling is a problem that, within the theoretical computer science ﬁeld, has its origins
in convex body sampling (a problem it generalizes). A long sequence of developments have made
signiﬁcant advances in the general model, where only convexity is assumed about the negative log-
density, and only value oracle access is given.
In this prior work discussion, we focus on more
structured cases where all or part of the negative log-density has ﬁnite condition number, and refer
the reader to [519, 370, 150] for an account on progress in the general case.
Well-conditioned densities. Signiﬁcant recent eﬀorts in the machine learning and statistics com-
munities focused on obtaining provable guarantees for well-conditioned distributions, starting from
pioneering work of [153], and continued in e.g. [130, 156, 129, 128, 210, 206, 205, 351, 402, 480, 344].
In this setting, many methods based on discretizations of continuous-time ﬁrst-order processes (such
as the Langevin dynamics) have been proposed. Typically, error guarantees come in two forms:
either in the 2-Wasserstein (W2) distance, or in total variation (TV). The line [210, 128, 344] has
brought the gradient complexity for obtaining ϵ TV distance to eO(κd) where d is the dimension, by
exploiting gradient concentration properties. For progress in complexities depending polynomially
on ϵ−1, attaining W2 guarantees (typically incomparable to TV bounds), we defer to [480], the state-
of-the-art using eO(κ
7
6 ϵ−1
3 +κϵ−2
3 ) queries to obtain W2 distance ϵ
p
dµ−1 from the target.6 We note
incomparable guarantees can be obtained by assuming higher derivative bounds (e.g. a Lipschitz
Hessian); our work uses only the minimal assumption of bounded second derivatives.
Composite densities. Recent works have studied sampling from densities of the form (10.1), or its
specializations (e.g. restrictions to a convex set). Several [443, 96, 74] are based on Moreau envelope
or proximal regularization strategies, and demonstrate eﬃciency under more stringent assumptions
on the structure of the composite function g, but under minimal assumptions obtain fairly large
provable mixing times Ω(d5).
Proximal regularization algorithms have also been considered for
non-composite sampling [537]. Another discretization strategy based on projections was studied by
[101], but obtained mixing time Ω(d7). Finally, improved algorithms for special constrained sampling
problems have been proposed, such as simplex restrictions [276].
Of particular relevance and inspiration to this work is [401].
By generalizing and adapting
Metropolized HMC algorithms of [210, 128], adopting a Moreau envelope strategy, and using (a
stronger version of) the RGO access model, [401] obtained a runtime which in the best case scales as
eO
 κ2d

, similar to the guarantee of our base sampler in Theorem 69. However, this result required a
variety of additional assumptions, such as access to the normalization factor of restricted Gaussians,
Lipschitzness of g, warmness of the start, and various problem parameter tradeoﬀs. The general
problem of sampling from (10.1) under minimal assumptions more eﬃciently than general-purpose
logconcave algorithms is to the best of our knowledge unresolved (even under restricted Gaussian
oracle access), a novel contribution of our mixing time bound. Our results also suggest that the
6Here,
p
dµ−1 is the eﬀective diameter; this accuracy measure allows for scale-invariant W2 guarantees.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
491
Method
Gradient oracle complexity
W2 ≤ϵ, µ = 1
W2 ≤ϵ
p
dµ−1
SAGA-LD [119]
n + κ1.5√
d+κd+Md
ϵ
+ κd4/3
ϵ2/3
n + κ1.5+κ
√
d+M
√
d
ϵ
+ κd2/3
ϵ2/3
SVRG-LD [119]
n + κ1.5√
d+κd+Md
ϵ
+ κd4/3
ϵ2/3
n + κ3
ϵ2 + κ1.5+M
√
d
ϵ
CV-ULD [119]
n + κ4d1.5
ϵ3
n + κ4
ϵ3
SVRG-LD [554]
n + κ1.5√
d+Md
ϵ
+ κ
√
nd
ϵ
n + κ1.5+M
√
d
ϵ
+ κ√n
ϵ
State-of-the-art, n = 1 [480]
κ7/6d1/6
ϵ1/3
+ κd1/3
ϵ2/3
κ7/6
ϵ1/3 +
κ
ϵ2/3
Method
Gradient oracle complexity (TV ≤ϵ)
Corollary 47
n + κd + κ
√
nd
State-of-the-art, n = 1 [344]
κd
Table 10.1: Complexity of sampling from e−F (x) where F(x) = 1
n
P
i∈[n] fi(x) on Rd is µ-strongly
convex, each fi is convex and L-smooth, and κ = L
µ . For relevant lines, M is the Lipschitz constant
of the Hessian ∇2F, which our algorithm has no dependence on. Complexity is measured in terms of
the number of calls to fi or ∇fi for summands {fi}i∈[n]. We hide polylog( nκd
ϵ ) factors for simplicity.
RGO is a natural notion of tractability for the composite sampling problem.
Logconcave ﬁnite sums. Since [536] proposed the stochastic gradient Langevin dynamics, which
at each step stochastically estimates the full gradient of the function, there has been a long line of
work giving bounds for this method and other similar algorithms [155, 229, 467, 61, 413]. These
convergence rates depend heavily on the variance of the stochastic estimates. Inspired by variance-
reduced methods in convex optimization, samplers based on low-variance estimators have also been
proposed [202, 207, 80, 51, 406, 123, 554, 119].
Although our reduction-based approach is not
designed speciﬁcally for solving problems of ﬁnite sum structure, our speedup can be viewed as due
to a lower variance estimator implicitly deﬁned through the oracle subproblems of Theorem 68 via
repeated re-centering.
In Table 10.1, we list prior runtimes [554, 119] for sampling logconcave ﬁnite sums; note these
results additionally require bounded higher derivatives (with the exception of the κ4 dependence),
obtain guarantees only in Wasserstein distance, and have polynomial dependences on ϵ−1. On the
other hand, our reduction-based approach obtains total variation bounds with linear dependence on
κ and polylogarithmic dependence on ϵ−1. Our bound also simultaneously matches the state-of-the-
art bound when n = 1, a feature not shared by prior stochastic algorithms. To our knowledge, no
previous nontrivial7 bounds were known in the high-accuracy regime before our work.
Theoretical analyses of MALA and HMC. MALA was originally proposed in [76], and subse-
quently its practical and theoretical performance in diﬀerent settings has received extensive treat-
ment in the literature (cf. the survey [444]). A number of theoretical analyses related to the well-
conditioned setting we study predate the work of [210], such as [457, 94], but they typically consider
7As mentioned previously, one can always compute the full ∇F in every iteration in a well-conditioned sampler.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
492
more restricted settings or do not state explicit dependences on κ and d.
Recently, a line of work has obtained a sequence of stronger upper bounds on the mixing of
MALA. First, [210] demonstrated that MALA achieves a mixing time of eO(κd + κ1.5√
d) from a
polynomially warm start, and the same set of authors later proved the same mixing time under an
exponentially warm start (which can be explicitly constructed) in [128]. It was later demonstrated
in [344] that under an appropriate averaging scheme, the mixing time could be improved to eO(κd)
from an exponentially warm start with no low-order dependence.
Finally, a recent work [137]
demonstrated that from a polynomially warm start, MALA mixes in time eO(poly(κ)
√
d) for general
κ-conditioned distributions and in time eO(poly(κ)
3√
d) for κ-conditioned Gaussians, and posed the
open question of attaining similar bounds from an explicit (exponentially) warm start. This latter
work was a primary motivation for our exploration.
The HMC algorithm with a leapfrog integrator (which we refer to as HMC for simplicity) can be
viewed as a multi-step generalization of MALA, as it has two parameters (a step size η and a step
count K), and when K = 1 the implementation matches MALA exactly. For larger K, the algorithm
simulates the (continuous-time) Hamiltonian dynamics with respect to the potential f(x) + 1
2 ∥v∥2
2
where f is the target's negative log-density and v is an auxiliary "velocity" variable. The intuition
is that larger K leads to more faithful discretizations of the true dynamics.
However, there are few explicit analyses of the (Metropolis-adjusted) HMC algorithm, applied
to well-conditioned distributions.8
To our knowledge, the only theoretical upper bound for the
mixing of (multi-step) HMC stronger than known analyses of its one-step specialization MALA is
by [128], which gave a suite of bounds trading oﬀthree problem parameters: the conditioning κ,
the dimension d, and the Hessian Lipschitz parameter LH, under the additional assumption that
the log-density has bounded third derivatives. Assuming that LH is polynomially bounded by the
problem smoothness L, they demonstrate that HMC with an appropriate K can sometimes achieve
sublinear dependence on d in number of gradient queries, where the quality of this improvement
depends on κ and d (e.g. if κ ∈[d
1
3 , d
2
3 ] and LH ≤L1.5, κd
11
12 gradients suﬃce). This prompts the
question: can HMC attain query complexity independent of d, assuming higher derivative bounds,
from an explicit warm start? Theorem 74 answers this negatively (at least in terms of relaxation
time) using an exponentially-sized bad set; moreover, our hard distribution is a Gaussian, with all
derivatives of order at least 3 vanishing.
Lower bounds for sampling. The bounds most closely relevant to those in this paper are given
by [344], who showed that the step size of MALA must scale inversely in κ for the chain to have
a constant chance of moving, and [137], who showed that the step size must scale as d−1
2 . Theo-
rem 72 matches or improves both bounds simultaneously, proving that up to logarithmic factors the
relaxation time of MALA scales linearly in both κ and d, while giving an explicit hard distribution
and exp(−d)-sized bad set. Moreover, both [344, 137] gave strictly spectral lower bounds, which are
8There has been considerably more exploration of the unadjusted variant [384, 383, 93], which typically obtain
mixing guarantees scaling polynomially in the inverse accuracy (as opposed to polylogarithmic).

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
493
complemented by our Theorem 73, a mixing time lower bound.
We brieﬂy mention several additional lower bound results in the sampling and sampling-adjacent
literature, which are related to this work. Recently, [107] exhibited an information-theoretic lower
bound on unadjusted discretizations simulating the underdamped Langevin dynamics, whose dimen-
sion dependence matches the upper bound of [480] (while leaving the precise dependence on κ open).
Finally, [235] and [120] give information-theoretic lower bounds for estimating normalizing constants
of well-conditioned distributions and the number of stochastic gradient queries required by ﬁrst-order
sampling methods under noisy gradient access respectively.
10.1.4
Technical overview: upper bounds
Composite logconcave sampling
We study the problem of approximately sampling from a distribution π on Rd, with density
dπ(x)
dx
∝exp (−f(x) −g(x)) .
(10.1)
Here, f : Rd →R is assumed to be "well-behaved" (i.e. has ﬁnite condition number), and g : Rd →R
is a convex, but possibly non-smooth function. This problem generalizes the special case of sampling
from exp(−f(x)) for well-conditioned f, simply by letting g vanish. Even the specialization of (10.1)
where g indicates a convex set (i.e. is 0 inside the set, and ∞outside) is not well-understood; existing
mixing time bounds for this restricted setting are large polynomials in d [96, 101], and are typically
weaker than guarantees in the general logconcave setting [372, 371]. This is in contrast to the convex
optimization setting, where ﬁrst-order methods readily generalize to solve problem families such as
minx∈X f(x), where X ⊆Rd is a convex set, as well as its generalization
min
x∈Rd f(x) + g(x), where g : Rd →R is convex and admits a proximal oracle.
(10.2)
We deﬁned proximal oracles in Deﬁnition 45; in short, they are prodecures which minimize the sum
of a quadratic and g. Deﬁnition 45 is desirable as many natural non-smooth composite objectives
arising in learning settings, such as the Lasso [505] and elastic net [555], admit eﬃcient proximal
oracles. It is clear that the deﬁnition of a proximal oracle implies it can also handle arbitrary sums of
linear functions and quadratics, as the resulting function can be rewritten as the sum of a constant
and a single quadratic. The seminal work [66] extends fast gradient methods to solve (10.2) via
proximal oracles, and has prompted many follow-up studies.
Motivated by the success of the proximal oracle framework in convex optimization, we study
sampling from the family (10.1) through the lens of RGOs, a natural extension of Deﬁnition 45. The
main result of Section 10.5 is a "base" algorithm eﬃciently sampling from (10.1), assuming access
to an RGO for g. We now survey the main components of this algorithm.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
494
Reduction to shared minimizers. We ﬁrst observe that without loss of generality, f and g share
a minimizer: by shifting f and g by linear terms, i.e. ˜f(x) := f(x) −⟨∇f(x∗), x⟩, ˜g(x) := g(x) +
⟨∇f(x∗), x⟩, where x∗minimizes f +g, ﬁrst-order optimality implies both ˜f and ˜g are minimized by
x∗. Moreover, implementation of a ﬁrst-order oracle for ˜f and an RGO for ˜g are immediate without
additional assumptions. This modiﬁcation becomes crucial for our later developments, and we hope
this simple observation, reminiscent of "variance reduction" techniques in stochastic optimization
[300], is broadly applicable to improving algorithms for the sampling problem induced by (10.1).
Beyond Moreau envelopes: expanding the space. A typical approach in convex optimization in
handling non-smooth objectives g is to instead optimize its Moreau envelope, deﬁned by
gη(y) := min
x∈Rd

g(x) + 1
2η ∥x −y∥2
2

.
(10.3)
Intuitively, the envelope gη trades oﬀfunction value with proximity to y; a standard exercise shows
that gη is smooth (has a Lipschitz gradient), with smoothness depending on η, and moreover that
computing gradients of gη reduces to calling a proximal oracle (Deﬁnition 45). It is natural to extend
this idea to the composite sampling setting, e.g. via sampling from the density
exp (−f(x) −gη(x)) .
However, a variety of complications prevent such strategies from obtaining rates comparable to
their non-composite, well-conditioned counterparts, including diﬃculty in bounding closeness of the
resulting distribution, as well as biased drifts of the sampling process due to error in gradients.
Our approach departs from this smoothing strategy in a crucial way, inspired by Hamiltonian
Monte Carlo (HMC) methods [334, 409]. HMC can be seen as a discretization of the ubiquitous
Langevin dynamics, on an expanded space.
In particular, discretizations of Langevin dynamics
simulate the stochastic diﬀerential equation dxt
dt = −∇f(xt)+
√
2 dWt
dt , where Wt is Brownian motion.
HMC methods instead simulate dynamics on an extended space Rd ×Rd, via an auxiliary "velocity"
variable which accumulates gradient information. This is sometimes interpreted as a discretization
of the underdamped Langevin dynamics [130]. HMC often has desirable stability properties, and
expanding the dimension via an auxiliary variable has been used in algorithms obtaining the fastest
rates in the well-conditioned logconcave sampling regime [480, 344]. Inspired by this phenomenon,
we consider the density on Rd × Rd
dˆπ
dz (z) := exp

−f(y) −g(x) −1
2η ∥x −y∥2
2

where z = (x, y).
(10.4)
Due to technical reasons, the family of distributions we use in our ﬁnal algorithms are of slightly
diﬀerent form than (10.4), but this simpliﬁcation is useful to build intuition. Note in particular that
the form of (10.4) is directly inspired by (10.3), where rather than maximizing over x, we directly

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
495
expand the space. The idea is that for small enough η and a set on x of large measure, smoothness of
f will guarantee that the marginal of (10.4) on x will concentrate y near x, a fact we make rigorous.
To sample from (10.1), we then show that a rejection ﬁlter applied to a sample x from the marginal
of (10.4) will terminate in constant steps. Consequently, it suﬃces to develop a fast sampler for
(10.4).
Alternating sampling with an oracle.
The form of the distribution (10.4) suggests a natural
strategy for sampling from it: starting from a current state (xk, yk), we iterate
1. Sample yk+1 ∼exp

−f(y) −
1
2η ∥xk −y∥2
2

.
2. Sample xk+1 ∼exp

−g(x) −
1
2η ∥x −yk+1∥2
2

, via a restricted Gaussian oracle.
When f and g share a minimizer, taking a ﬁrst-order approximation in the ﬁrst step, i.e. sampling
yk+1 ∼exp(−f(xk) −⟨∇f(xk), y −xk⟩−
1
2η ∥y −xk∥2
2), can be shown to generalize the Leapfrog
step of HMC updates. However, for η very small (as in our setting), we observe the ﬁrst step itself
reduces to the case of sampling from a distribution with constant condition number, performable
in ˜O(d) gradient calls by e.g. Metropolized HMC [210, 128, 344]. Moreover, it is not hard to see
that this "alternating marginal" sampling strategy preserves the stationary distribution exactly, so
no ﬁltering is necessary. Directly bounding the conductance of this random walk, for small enough
η, leads to an algorithm running in eO
 κ2d2
iterations, each calling an RGO once, and a gradient
oracle for f roughly eO (d) times. This latter guarantee is by an appeal to known bounds [128, 344]
on the mixing time in high dimensions of Metropolized HMC for a well-conditioned distribution, a
property satisﬁed by the y-marginal of (10.4) for small η.
Stability of Gaussians under bounded perturbations. To obtain our tightest runtime result, we
use that η is chosen to be much smaller than L−1 to show structural results about distributions
of the form (10.4), yielding tighter concentration for bounded perturbations of a Gaussian (i.e. the
Gaussian has covariance 1
ηI, and is restricted by L-smooth f for η ≪L−1). To illustrate, let
dPx(y)
dy
∝exp

−f(y) −1
2η ∥y −x∥2
2

and let its mean and mode be ¯yx, y∗
x. It is standard that ∥¯yx −y∗
x∥2 ≤√dη, by η−1-strong logcon-
cavity of Px. Informally, we show that for η ≪L−1 and x not too far from the minimizer of f, we
can improve this to ∥¯yx −y∗
x∥2 = O(√η); see Proposition 83 for a precise statement.
Using our structural results, we sharpen conductance bounds, improve the warmness of a starting
distribution, and develop a simple rejection sampling scheme for sampling the y variable in expected
constant gradient queries. Our proofs are continuous in ﬂavor and based on gradually perturbing the
Gaussian and solving a diﬀerential inequality; we believe they may of independent interest. These
improvements lead to an algorithm running in eO
 κ2d

iterations; ultimately, we use our reduction
framework, stated in Theorem 68, to improve this dependence to eO (κd).

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
496
Logconcave ﬁnite sums
We initiate the algorithmic study of the following task in the high-accuracy regime: sample x ∼π
within total variation distance ϵ, where dπ
dx(x) ∝exp(−F(x)) and
F(x) = 1
n
X
i∈[n]
fi(x),
(10.5)
all fi : Rd →R are convex and L-smooth, and F is µ-strongly convex. We call such a distribution
π a (well-conditioned) logconcave ﬁnite sum.
In applications (where summands correspond to points in a dataset, e.g. in Bayesian linear and
logistic regression tasks [210]), querying ∇F may be prohibitively expensive, so a natural goal is to
obtain bounds on the number of required queries to summands ∇fi for i ∈[n]. This motivation
also underlies the development of stochastic gradient methods in optimization, a foundational tool
in modern statistics and data processing.
Na¨ıvely, one can complete the task by using existing
samplers for well-conditioned distributions and querying the full gradient ∇F in each iteration, re-
sulting in a summand gradient query complexity of eO(nκd) [344]. Many recent works, inspired from
recent developments in the complexity of optimizing a well-conditioned ﬁnite sum, have developed
subsampled gradient methods for the sampling problem. However, to our knowledge, all such guar-
antees depend polynomially on the accuracy ϵ and are measured in the 2-Wasserstein distance; in
the high-accuracy, total variation case no nontrivial query complexity is currently known.
We show in Section 10.6 that given access to the minimizer of F, a simple zeroth-order algorithm
which queries eO(κ2d) values of summands {fi}i∈[n] succeeds (i.e. it never requires a full value or
gradient query of F).
The algorithm is essentially the Metropolized random walk proposed in
[210] for the n = 1 case with a cheaper subsampled ﬁlter step. Notably, because the random walk is
conducted with respect to F, we cannot eﬃciently query the function value at any point; nonetheless,
by randomly sampling to compute a nearly-unbiased estimator of the rejection probability, we do
not incur too much error. This random walk was shown in [128] to mix in eO(κ2d) iterations; we
implement each step to suﬃcient accuracy using eO(1) function evaluations.
It is natural to ask if ﬁrst-order information can be used to improve this query complexity, perhaps
through "variance reduction" techniques (e.g. [300]) developed for stochastic optimization. The idea
behind variance reduction is to recenter gradient estimates in phases, occasionally computing full
gradients to improve the estimate quality.
One fundamental diﬃculty which arises from using
variance reduction in high-accuracy sampling is that the resulting algorithms are not stateless. By
this, we mean that the variance-reduced estimates depend on the history of the algorithm, and thus
it is diﬃcult to ascertain correctness of the stationary distribution. We take a diﬀerent approach to
achieve a linear query dependence on the conditioning κ, described in the following section.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
497
Proximal point reduction framework
To motivate Theorem 68, we ﬁrst recast existing "proximal point" reduction-based optimization
methods through the lens of composite optimization, and subsequently show that similar ideas
underlying our composite sampler in Section 10.1.4 yield an analagous "proximal point reduction
framework" for sampling. Chronologically, our composite sampler (originally announced in [481])
predates our reduction framework, which was then inspired by the perspective given here. We hope
these insights prove fruitful for further development of proximal approaches to sampling.
Proximal point methods as composite optimization. Proximal point methods are a well-studied
primitive in optimization, developed by [459]; cf. [435] for a modern survey. The principal idea is
that to minimize convex F : Rd →R, it suﬃces to solve a sequence of subproblems
xk+1 ←argminx∈Rd

F(x) + 1
2λ ∥x −xk∥2
2

.
(10.6)
Intuitively, by tuning the parameter λ ≥0, we trade oﬀhow regularized the subproblems (10.6) are
with how rapidly the overall method converges. Smaller values of λ result in larger regularization
amounts which are amenable to algorithms for minimizing well-conditioned objectives.
For optimizing functions of the form (10.5) via stochastic gradient estimates to ϵ error, [300, 168,
472] developed variance-reduced methods obtaining a query complexity of eO(n + κ). To match a
known lower bound of eO(n+√nκ) due to [539], two works [364, 227] appropriately applied instances
of accelerated proximal point methods [257] with careful analyses of how accurately subproblems
(10.6) needed to be solved. These algorithms black-box called the eO(n + κ) runtime as an oracle
to solve the subproblems (10.6) for an appropriate choice of λ, obtaining an accelerated rate.9 To
shed some light on this acceleration procedure, we adopt an alternative view on proximal point
methods.10 Consider the following known composite optimization result.
Proposition 42 (Informal statement of [66]). Let f : Rd →R be L-smooth and µ-strongly convex,
and g : Rd →R admit a proximal oracle O(λ, v) (cf. Deﬁnition 45). There is an algorithm taking
eO(√κ) iterations for κ = L
µ to ﬁnd an ϵ-approximate minimizer to f + g, each querying ∇f and O
a constant number of times. Further, λ = 1
L in all calls to O.
Ignoring subtleties of the error tolerance of O, we show how to use an instance of Proposition 42 to
recover the eO(n + √nκ) query complexity for optimizing (10.5). Let f(x) = µ
2 ∥x∥2
2, and g = F −f.
For any Λ ≥µ, f is both µ-strongly convex and Λ-smooth. Moreover, note that all calls to the
proximal oracle O for g require solving subproblems of the form
argminx∈Rd

F(x) −µ
2 ∥x∥2
2 + Λ
2 ∥x −v∥2
2

.
(10.7)
9We note that an improved runtime without extraneous logarithmic factors was later obtained by [17].
10This perspective can also be found in the lecture notes [342].

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
498
The upshot of choosing a smoothness bound Λ ≥µ is that the regularization amount in (10.7)
increases, improving the conditioning of the subproblem, which is Λ-strongly convex and L + Λ-
smooth. The algorithm of e.g. [300] solves each subproblem (10.7) in eO(n + L+Λ
Λ ) gradient queries,
leading to an overall query complexity (for Proposition 42) of
eO
 s
Λ
µ ·

n + L
Λ
!
.
Optimizing over Λ ≥µ, i.e. taking Λ = max(µ, L
n), yields the desired bound of eO(n + √nκ).
Applications to sampling. In Sections 10.5 and 10.6, we develop samplers for structured families
with quadratic dependence on the conditioning κ. The proximal point approach suggests a strategy
for accelerating these runtimes. Namely, if there is a framework which repeatedly calls a sampler for
a regularized density (analogous to calls to (10.6)), one could trade oﬀthe regularization with the
rate of the outer loop. Fortunately, in the spirit of interpreting proximal point methods as composite
optimization, the composite sampler of Section 10.5 itself meets these reduction framework criteria.
We brieﬂy recall properties of our composite sampler here. Let π be a distribution on Rd with
dπ
dx(x) ∝exp(−fwc(x) −foracle(x)),11 where fwc is well-conditioned (has ﬁnite condition number κ)
and foracle admits an RGO, which solves subproblems of the form
O(η, v) ∼the density proportional to exp

−1
2η ∥x −v∥2
2 −foracle(x)

.
(10.8)
The algorithm of Section 10.5 only calls O with a ﬁxed η; note the strong parallel between the RGO
subproblem and the proximal oracle of Proposition 42. For a given value of η ≥0, our composite
sampler runs in eO( 1
ηµ) iterations, each requiring a call to O. Smaller η improve the conditioning
of the negative log-density of subproblem (10.8), but increase the overall iteration count, yielding
a range of trade-oﬀs. The algorithm of Section 10.5 has an upper bound requirement on η (cf.
Theorem 69); in Section 10.3, we observe that this may be lifted when fwc = 0 uniformly, allowing
for a full range of choices. Moreover, the analysis of the composite sampler becomes much simpler
when fwc = 0, as in Theorem 68. We give the framework as Algorithm 59, as well as a full (fairly
short) convergence analysis. By trading oﬀthe regularization amount with the cost of implementing
(10.8) via bootstrapping base samplers, we obtain a host of improved runtimes.
Beyond our speciﬁc applications, the framework we provide has strong implications as a generic
reduction from mixing times scaling polynomially in κ to improved methods scaling linearly in κ.
This is akin to the observation in [364] that accelerated proximal point methods generically improve
poly(κ) dependences to √κ dependences for optimization. We are optimistic this insight will ﬁnd
further implications in the logconcave sampling literature.
11To disambiguate, we sometimes also use the notation fwc + foracle rather than f + g in deﬁning instances of our
reduction framework or composite samplers, when convenient in the context.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
499
10.1.5
Technical overview: lower bounds
In this section, we give an overview of the techniques we use to show our lower bounds. Throughout
for the sake of ﬁxing a scale, we assume the negative log-density has Hessian between I and κI.
MALA. Our starting point is the observation made in [137] that for a MALA step size h, the
spectral gap of the MALA Markov chain scales no better than O(h+h2), witnessed by a simple one-
dimensional Gaussian. Thus, our strategy for proving Theorems 71 and 72 is to show a dichotomy
on the choice of step size: either h is so large such that we can construct an exp(d)-warm start where
the chain is extremely unlikely to move (e.g. the step almost always is ﬁltered), or it is small enough
to imply a poor spectral gap. In the Gaussian case, we achieve this by explicitly characterizing the
rejection probability and demonstrating that choosing the "small ball" warm start where ∥x∥2
2 is
smaller than its expectation by a constant ratio suﬃces to upper bound h.
Given the result of Theorem 71, we see that if MALA is to move at all with decent probability
from an exponentially warm start, we must take h ≪1, so the spectral gap in this regime is simply
O(h). We now move onto the more general well-conditioned setting. As a thought experiment, we
note that the upper bound analyses of [210, 128, 344] for MALA have a dimension dependence which
is bottlenecked by the noise term only. In particular, the MALA iterates apply a ﬁlter to the move
x′ ←x −h∇f(x) +
√
2hg, where g ∼N(0, I) is a standard Gaussian vector. However, even for the
more basic "Metropolized random walk" where the proposal is simply x′ ←x+
√
2hg, the dimension
dependence of upper bound analyses scales linearly in d. Thus, it is natural to study the eﬀect of
the noise, and construct a hard distribution based around it.
We ﬁrst formalize this intuition, and demonstrate that for step sizes not ruled out by Theorem 71,
all terms in the rejection probability calculation other than those due to the eﬀect of the noise g
are low-order. Moreover, because the eﬀect of the noise is coordinatewise separable (since N(0, I)
is a product distribution), to demonstrate a eO( 1
κd) upper bound on h it suﬃces to show a hard
one-dimensional distribution where the log-rejection probability has expectation −Ω(hκ), and apply
sub-Gaussian concentration to show a product distribution has expectation −Ω(hκd).
At this point, we reduce to the following self-contained problem: let x ∈R, let π∗∝exp(−f1d)
be one-dimensional with second derivative ≤κ, and let xg = x +
√
2hg for g ∼N(0, 1). We wish to
construct f1d such that for x in a constant probability region over exp(−f1d) (the "bad set"),
Eg∼N(0,1)

−f1d(xg) + f1d(x) −1
2 ⟨x −xg, f ′
1d(x) + f ′
1d(xg)⟩

= −Ω(hκ),
(10.9)
where the contents of the expectation in (10.9) are the log-rejection probability along one coordinate
by a straightforward calculation. By forming a product distribution using f1d as a building block,
and combining with the remaining low-order terms due to the drift ∇f(x), we attain an exp(−d)-
sized region where the rejection probability is exp(−Ω(hκd)), completing Theorem 72.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
500
It remains to construct such a hard f1d. The calculation
−f1d(xg) + f1d(x) −1
2 ⟨x −xg, f ′
1d(x) + f ′
1d(xg)⟩= −2h
Z 1
0
1
2 −s

g2f ′′
1d(x + s(xg −s))ds
suggests the following approach: because the above integral places more mass closer to the starting
point, we wish to make sure our bad set has large second derivative, but most moves g result in a
much smaller second derivative. Our construction patterns this intuition: we choose12
f1d(x) = κ
3 x2 −κh
3 cos x
√
h
=⇒f ′′
1d(x) = 2κ
3 + κ
3 cos x
√
h
,
such that our bad set is when cos
x
√
h is relatively large (which occurs with probability →1
2 for small
h in one dimension). The period of our construction scales with
√
h, so that most moves
√
2hg
of size O(
√
h) will "skip a period" and hence hit a region with small second derivative, satisfying
(10.9).
Figure 10.1: Second derivative of our hard function f1d, κ = 10, h = 0.01. Starting from inside the
hard region, on average over g ∼N(0, I), a move by
√
2hg decreases the second derivative.
HMC. We further demonstrate that similar hard Gaussians as the one we use for MALA also
place an upper bound on the step size of HMC for any number of steps K. Our starting point is
a novel characterization of HMC iterates on Gaussians: namely, when the negative log-density is
quadratic, we show that the HMC iterates implement a linear combination between the starting
position and velocity, where the coeﬃcients are given by Chebyshev polynomials. For step size η of
size Ω(
1
K√κ) for speciﬁc constants, we show the HMC chain begins to cycle because of the locations
of the Chebyshev polynomials' zeroes, and cannot move. Moreover, for suﬃciently small step size η
outside of this range, it is straightforward by examining the coeﬃcients of Chebyshev polynomials
to show that they are the same (up to constant factors) as in the MALA case, at which point our
previous lower bound holds. It takes some care to modify our hard Gaussian construction to rule
out all constant ranges in the η ≈
1
K√κ region, but by doing so we obtain Theorem 74.
We remark that the observation that HMC iterates are implicitly implementing a Chebyshev
polynomial approximation appears to be unknown in the literature, and is a novel contribution of
12We note [137] also used a (diﬀerent, but similar) cosine-based construction for their lower bound.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
501
our work. We believe understanding this connection is a worthwhile endeavor, as a similar connection
between polynomial approximation and ﬁrst-order convex optimization has led to various interesting
interpretations of Nesterov's accelerated gradient descent method [266, 49].
10.1.6
Roadmap
We give notations and preliminaries in Section 10.2. In Section 10.3 we give our framework for
bootstrapping fast regularized samplers, and prove its correctness (Theorem 68).
Assuming the
"base samplers" of Theorems 69 and 70, in Section 10.4 we apply our reduction to obtain all of
our strongest guarantees, namely Corollaries 45, 46, and 47. We then prove Theorems 69 and 70 in
Sections 10.5 and 10.6. We present our lower bound constructions (proofs of Theorems 71, 72, 73,
and 74 respectively) in Sections 10.7, 10.8, 10.9, and 10.10. Finally, we give concluding thoughts on
our lower bound results in Section 10.11.
10.2
Preliminaries
10.2.1
Notation
General notation. For d ∈N, [d] refers to the set of naturals 1 ≤i ≤d; ∥·∥2 is the Euclidean norm
on Rd when d is clear from context. N(µ, Σ) is the multivariate Gaussian of speciﬁed mean and
variance, I is the identity of appropriate dimension when clear from context, and ⪯is the Loewner
order on symmetric matrices. For any positive semideﬁnite matrix A, we let ∥·∥A be its induced
seminorm ∥x∥A =
√
x⊤Ax. We let {Wt}t≥0 ⊂Rd denote the standard Brownian motion when
dimensions are clear from context.
Functions. We say twice-diﬀerentiable function f : Rd →R is L-smooth and µ-strongly convex
if µI ⪯∇2f(x) ⪯LI for all x ∈Rd; it is well-known that L-smoothness implies that f has an
L-Lipschitz gradient, and that for any x, y ∈Rd,
f(x) + ⟨∇f(x), y −x⟩+ µ
2 ∥y −x∥2
2 ≤f(y) ≤f(x) + ⟨∇f(x), y −x⟩+ L
2 ∥y −x∥2
2 .
If f is L-smooth and µ-strongly convex, we say it has a condition number κ := L
µ . We call a zeroth-
order oracle, or "value oracle", an oracle which returns f(x) on any input point x ∈Rd; similarly, a
ﬁrst-order oracle, or "gradient oracle", returns both the value and gradient (f(x), ∇f(x)).
For our lower bound results, we explicitly assume that κ is at least a constant for convenience of
stating bounds; a lower bound of 10 suﬃces for all our results.
Distributions. We call distribution π on Rd logconcave if dπ
dx(x) = exp(−f(x)) for convex f; π
is µ-strongly logconcave if f is µ-strongly convex. For A ⊆Rd, Ac is its complement, and we let
π(A) :=
R
x∈A dπ(x). We say distribution ρ is β-warm with respect to π if dπ
dρ (x) ≤β everywhere,
and deﬁne the total variation ∥π −ρ∥TV := supA⊆Rd π(A) −ρ(A). We will frequently use the fact

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
502
that ∥π −ρ∥TV is also the probability that x ∼π and x′ ∼ρ are unequal under the best coupling of
(π, ρ); this allows us to "locally share randomness" when comparing two random walk procedures.
We deﬁne the expectation Eπ and variance Varπ with respect to distribution π in the standard way,
Eπ[h(x)] :=
Z
h(x)dπ(x), Varπ[h(x)] := Eπ

(h(x))2
−(Eπ[h(x)])2 .
Structured distributions. This work considers two types of distributions with additional structure,
which we call composite logconcave densities and logconcave ﬁnite sums. A composite logconcave
density has the form exp(−f(x)−g(x)), where both f and g are convex. In this context throughout,
f will either be uniformly 0 or have a ﬁnite condition number (be "well-conditioned"), and g will
represent a "simple" but possibly non-smooth function, as measured by admitting an RGO (cf.
Deﬁnition 43). We will sometimes refer to the components as f and g as fwc and foracle respectively,
to disambiguate when the functions f and g are already deﬁned in context. In our reduction-based
approaches, we have additional structure on the parameter λ which an RGO is called with (cf.
Deﬁnition 44). Speciﬁcally, in our instances typically λ−1 ≫L (or some other "niceness" parameter
associated with the negative log-density); this can be seen as heavily regularizing the negative log-
density, and often makes the implementation simpler.
Finally, a logconcave ﬁnite sum has density of the form exp(−F(x)) where F(x) = 1
n
P
i∈[n] fi(x).
When treating such densities, we make the assumption that each constituent summand fi is L-
smooth and convex, and the overall function F is µ-strongly convex. We measure complexity of
algorithms for logconcave ﬁnite sums by gradient queries to summands, i.e. ∇fi(x) for some i ∈[n].
Optimization. Throughout this work, we are somewhat liberal with assuming access to minimizers
to various functions (namely, the negative log-densities of target distributions). We give a more
thorough discussion of this assumption in Appendix I.1, but note here that for all function families
we consider (well-conditioned, composite, and ﬁnite sum), eﬃcient ﬁrst-order methods exist for
obtaining high accuracy minimizers, and this optimization query complexity is never the leading-
order term in any of our algorithms assuming polynomially bounded initial error.
Sampling.
Consider a Markov chain deﬁned on Rd with transition kernel {Tx}x∈Rd, so that
R
Tx(y)dy = 1 for all x. Further, denote the stationary distribution of the Markov chain by π∗.
Deﬁne the Dirichlet form of functions g, h : Rd →R with respect to the Markov chain by
E(g, h) :=
Z
g(x)h(x)dπ∗(x) −
ZZ
g(y)h(x)Tx(y)dπ∗(x)dy.
A standard calculation demonstrates that
E(g, g) = 1
2
ZZ
(g(x) −g(y))2Tx(y)dπ∗(x)dy.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
503
The mixing of the chain is governed by its spectral gap, a classical quantity we now deﬁne:
λ ({Tx}x∈Rd) := inf
g
 E(g, g)
Varπ∗[g]

.
(10.10)
The relaxation time is the inverse spectral gap. We also recall a result of Cheeger [122], showing the
spectral gap is O(Φ), where Φ is the conductance of the chain:
Φ ({Tx}x∈Rd) :=
inf
A⊂Rd|π∗(A)≤1
2
R
x∈A Tx(Ac)dπ∗(x)
π∗(A)
(10.11)
Finally, we recall the deﬁnition of a Metropolis ﬁlter. A Markov chain with transitions {Tx}x∈Rd
and stationary distribution π∗is said to be reversible if for all x, y ∈Rd,
dπ∗(x)Tx(y) = dπ∗(y)Ty(x).
The Metropolis ﬁlter is a way of taking an arbitrary set of proposal distributions {Px}x∈Rd and
deﬁning a reversible Markov chain with stationary distribution π∗. In particular, the Markov chain
induced by the Metropolis ﬁlter has transition distributions {Tx}x∈Rd deﬁned by
Tx(y) := Px(y) min

1, dπ∗(y)Py(x)
dπ∗(x)Px(y)

for all y ̸= x.
(10.12)
Whenever the proposal is rejected by the modiﬁed distributions above, the chain does not move.
10.2.2
Technical facts
We will repeatedly use the following results.
Fact 27 (Gaussian integral). For any λ ≥0 and v ∈Rd,
Z
exp

−1
2λ ∥x −v∥2
2

dx = (2πλ)
d
2 .
Fact 28 ([210], Lemma 1). Let π be a µ-strongly logconcave distribution, and let x∗minimize its
negative log-density. Then, for x ∼π and any δ ∈[0, 1], with probability at least 1 −δ,
∥x −x∗∥2 ≤
s
d
µ
 
2 + 2 max
 
4
r
log(1/δ)
d
,
r
log(1/δ)
d
!!
.
Fact 29 ([267], Theorem 1.1). Let π be a µ-strongly logconcave density. Let dγµ(x) be the Gaussian
density with covariance matrix µ−1I. For any convex function h,
Eπ[h(x −Eπ[x])] ≤Eγµ[h(x −Eγµ[x])].

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
504
Fact 30 ([206], Theorem 1). Let π be a µ-strongly logconcave distribution, and let x∗minimize its
negative log-density. Then, Eπ[∥x −x∗∥2
2] ≤d
µ.
Fact 31 (Mill's inequality). For one-dimensional Gaussian random variable Z ∼N(0, σ2),
Pr [Z > t] ≤
r
2
π
σ
t exp

−t2
2σ2

.
Fact 32 (χ2 tail bounds, Lemma 1 [340]). Let {Zi}i∈[n] ∼i.i.d. N(0, 1) and a ∈Rn
≥0. Then
Pr

X
i∈[n]
aiZ2
i −∥a∥2
2 ≥2 ∥a∥2
√
t + 2 ∥a∥∞t

≤exp(−t),
Pr

X
i∈[n]
aiZ2
i −∥a∥2
2 ≤−2 ∥a∥2
√
t

≤exp(−t).
Fact 33 (Bernstein's inequality). Let {Zi}i∈[n] be independent mean-zero random variables with
sub-exponential parameter λ. Then
Pr



X
i∈[n]
Zi

> t

≤exp

−1
2 min
 t2
nλ2 , t
λ

.
10.2.3
Metropolis-adjusted Langevin algorithm
In this section, we formally deﬁne the Metropolis-adjusted Langevin algorithm (MALA) which we
study in Sections 10.7 and 10.8. Throughout this discussion, ﬁx a distribution π on Rd, with density
dπ
dx(x) = exp(−f(x)), and suppose that f is twice-diﬀerentiable for simplicity.
The MALA Markov chain is given by a discretization of the (continuous-time) Langevin dynamics
dxt = −∇f(xt)dt +
√
2dWt,
which is well-known to have stationary density exp(−f(x)). MALA is deﬁned by performing a simple
Euler discretization of the Langevin dynamics up to time h > 0, and then applying a Metropolis
ﬁlter. In particular, deﬁne the proposal distribution at a point x by
Px := N (x −h∇f(x), 2hI) .
We obtain the MALA transition distribution by applying the deﬁnition (10.12), which yields
Tx(y) ∝exp
 
−∥y −(x −h∇f(x))∥2
2
4h
!
min

1,
exp

−f(y) −∥x−(y−h∇f(y))∥2
2
4h

exp

−f(x) −∥y−(x−h∇f(x))∥2
2
4h


.
(10.13)

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
505
The normalization constant above is that of the multivariate Gaussian with covariance 2hI.
10.2.4
Hamiltonian Monte Carlo
In this section, we formally deﬁne the (Metropolized) Hamiltonian Monte Carlo (HMC) method
which we study in Section 10.10. We assume the same setting as Section 10.2.3.
The Metropolized HMC algorithm is governed by two parameters, a step size η > 0 and a step
count K ∈N, and can be viewed as a multi-step generalization of MALA. In particular, when K = 1
it is straightforward to show that HMC is a reparameterization of MALA, see e.g. Appendix I.5.
More generally, from an iterate x, HMC performs the following updates.
1. x0 ←x, v0 ∼N(0, I)
2. For 0 ≤k < K:
(a) vk+ 1
2 ←vk −η
2∇f(xk)
(b) xk+1 ←xk + ηvk+ 1
2
(c) vk+1 ←vk −η
2∇f(xk+1)
3. Return xK
Each loop of step 2 is known in the literature as a "leapfrog" step, and is a discretization of
Hamilton's equations for the Hamiltonian function H(x, v) := f(x) + 1
2 ∥v∥2
2; for additional back-
ground, we refer the reader to [128]. This discretization is well-known to have reversible transition
probabilities (i.e. the transition density is the same if the endpoints are swapped) because it satis-
ﬁes a property known as symplecticity. Moreover, the Markov chain has stationary density on the
expanded space (x, v) ∈Rd × Rd proportional to exp(−H(x, v)). Correspondingly, the Metropolized
HMC Markov chain performs the above algorithm from a point x, and accepts with probability
min

1, exp (−H(xK, vK))
exp (−H(x0, v0))

.
(10.14)
10.3
Proximal reduction framework
The reduction framework of Theorem 68 can be thought of as a specialization of a more general
composite sampler which we develop in Section 10.5, whose guarantees are reproduced here.
Theorem 69. Let π be a distribution on Rd with dπ
dx(x) ∝exp (−f(x) −g(x)) such that f is L-
smooth and µ-strongly convex, and let ϵ ∈(0, 1).
Let η ≤
1
32Lκd log(κ/ϵ) (where κ =
L
µ ), T =
Θ( 1
ηµ log( κd
ϵ )), and let O be a η-RGO for g.
Further, assume access to the minimizer x∗=
argminx∈Rd{f(x) + g(x)}. There is an algorithm which runs in T iterations in expectation, each
querying a gradient oracle of f and O a constant number of times, and obtains ϵ total variation
distance to π.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
506
Our main observation, elaborated on more formally for speciﬁc applications in Section 10.4, is
that a variety of structured logconcave densities have negative log-densities foracle, where we can
implement an eﬃcient restricted Gaussian oracle for foracle via calling an existing sampling method.
Crucially, in these instantiations we use the fact that the distributions which O is required to sampled
from are heavily regularized (restricted by a quadratic with large leading coeﬃcient) to obtain fast
samplers. We further note that the upper bound requirement on η in Theorem 69 can be lifted
when the "well-conditioned" component is uniformly 0. Instead of setting f = 0 and g = foracle
in Theorem 69, and reﬁning the analysis for this special case to tolerate arbitrary η, we provide a
self-contained proof here. This particular structure (the composite setting where fwc is uniformly
zero and foracle is strongly convex) admits signiﬁcant simpliﬁcations from the more general case, so
using slightly diﬀerent proof techniques, we are able to obtain stronger convergence guarantees for
this particular problem allowing for mixing in fewer than d iterations from a feasible start.
Theorem 68. Let π be a distribution on Rd with
dπ
dx(x) ∝exp(−foracle(x)) such that foracle is
µ-strongly convex, and let ϵ ∈(0, 1). Let η ≤1
µ, T = Θ( 1
ηµ log
d
ηµϵ) for some β ≥1, and O be a
η-RGO for foracle. Algorithm 59, initialized at the minimizer of foracle, runs in T iterations, each
querying O a constant number of times, and obtains ϵ total variation distance to π.
For simplicity of notation, we replace foracle in the statement of Theorem 68 with g throughout
just this section. Let π be a density on Rd with dπ
dx(x) ∝exp(−g(x)) where g is µ-strongly convex
(but possibly non-smooth), and let O be a restricted Gaussian oracle for g.
Consider the joint
distribution ˆπ supported on an expanded space z = (x, y) ∈Rd × Rd with density, for some η > 0,
dˆπ
dz (z) ∝exp

−g(x) −1
2η ∥x −y∥2
2

.
Note that the x-marginal of ˆπ is precisely π, so it suﬃces to sample from the x-marginal. We consider
a simple alternating Markov chain for sampling from ˆπ, described in the following Algorithm 59.
Algorithm 59: AlternateSample(g, η, T)
1 Input: µ-strongly convex g : Rd →R, η > 0, T ∈N, x0 = minx g(x);
2 for k ∈[T] do
3
Sample yk ∼πxk−1, where for all x ∈Rd, dπx
dy (y) ∝exp

−1
2η ∥x −y∥2
2

;
4
Sample xk ∼πyk, where for all y ∈Rd, dπy
dx (x) ∝exp

−g(x) −
1
2η ∥x −y∥2
2

;
5 Return: xT ;
By observing that the distributions πx and πy in the above method are precisely the marginal
distributions of ˆπ with one variable ﬁxed, it is straightforward to see that ˆπ is a stationary distribution
of the process. We make this formal in the following lemma.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
507
Lemma 181 (Alternating marginal sampling). Let ˆπ be a density on two blocks (x, y). Sample
(x, y) ∼ˆπ, and then sample ˜x ∼ˆπ(·, y), ˜y ∼ˆπ(˜x, ·). Then, the distribution of (˜x, ˜y) is ˆπ. Moreover,
the alternating marginal sampling Markov chain on either marginal is reversible.
Proof. The density of the resulting distribution at (˜x, y) is proportional to the product of the
(marginal) density at y and the conditional distribution of ˜x | y, which by deﬁnition is ˆπ. Therefore,
(˜x, y) is distributed as ˆπ, and the argument for ˜y follows symmetrically. To see reversibility on the
x marginal, it suﬃces to note that the probability we move from x to x′ is proportional to
Z
y
ˆπ(x, y)ˆπ(x′, y)dy,
which is a symmetric function of x and x′. A similar argument holds for the y marginals.
We also state a simple observation about alternating schemes such as Algorithm 59, which will
be useful later. Let Px be the density of yk after one step of the above procedure starting from
xk−1 = x, and let Tx be the resulting density of xk.
Observation 1. For any two points x, x′ ∈Rd, ∥Tx −Tx′∥TV ≤∥Px −Px′∥TV.
Proof. This follows by the coupling characterization of total variation (see e.g. Chapter 5 of [354]).
Per the optimal coupling of y ∼Px and y′ ∼Px′, whenever the total variation sets y = y′ in Line 2
of AlternateSample, we can couple the resulting distributions in Line 3 as well.
In order to prove Theorem 45, we ﬁrst show that the random walk in Algorithm 59 converges
rapidly in the 2-Wasserstein distance (denoted W2 in this section).
Lemma 182. Let π0 be the starting distribution of x in Algorithm 59. Let πk be the distribution of
xk and π be the x-marginal of ˆπ. For all k ≥0,
W 2
2 (πk+1, π) ≤
1
(1 + ηµ)2 W 2
2 (πk, π).
Hence, for any η ≤1
µ, in T ′ = O

1
ηµ log
d
µ∆

iterations, the random walk mixes to
W2(πT ′, π) ≤∆.
Proof. Let Γxk be the optimal coupling between xk ∼πk and ˆx ∼π according to the W2 distance.
Coupling the Gaussian random variable generating yk+1 ∼πxk and ˆy ∼πˆx gives a coupling Γyk+1
between yk+1 and ˆy such that
EΓyk+1
h
∥yk+1 −ˆy∥2
2
i
= EΓxk
h
∥xk −ˆx∥2
2
i
.
(10.15)

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
508
Then, let πy be the distribution of xk+1 in a run of Line 3 of Algorithm 59 starting from yk+1 = y,
and πˆy be the distribution of ˆx in Line 3 starting from ˆy, respectively. Since πˆy is µ + 1
η strongly
log-concave, πˆy satisﬁes a log-Sobolev inequality with constant µ + 1
η (Theorem 2 of [430]). Hence,
W 2
2 (πy, πˆy) ≤
2
µ + 1
η
dKL(πy∥πˆy)
≤
1

µ + 1
η
2 Eπy
"∇log πy
πˆy

2
2
#
≤
1
(1 + ηµ)2 ∥y −ˆy∥2
2 .
The ﬁrst step used the Talagrand transportation inequality (Theorem 1 of [430]). The second step
used the log-Sobolev inequality. The third step used
∇log πy(x)
πˆy(x) = ∇log
exp(−g(x) −
1
2η ∥x −y∥2
2)
R
x′ exp(−g(x) −
1
2η ∥x −ˆy∥2
2)dx′
exp(−g(x) −
1
2η ∥x −ˆy∥2
2)
R
x′ exp(−g(x) −
1
2η ∥x −y∥2
2)dx′
= 1
2η ∇

∥x −ˆy∥2
2 −∥x −y∥2
2

= 1
η (y −ˆy).
(10.16)
Taking expectation over Γyk+1 and using (10.15) shows that
W 2
2 (πk+1, π) ≤
1
(1 + ηµ)2 W 2
2 (πk, π).
Algorithm 59 starts from the distribution π0 = δx∗, where x∗= minx g(x). Since π is µ-strongly
logconcave, we have (see e.g. Proposition 1 of [206])
W 2
2 (π0, π) = Eˆπ
h
∥x∗−x∥2i
≤d
µ.
Then, for η < 1
µ,
1
1+ηµ ≤1 −ηµ
2 , so after T ′ = O( 1
ηµ log
d
µ∆) iterations, W2(πT ′, π) ≤∆.
Next, we bound the KL divergence between the output of Algorithm 59 and the target distribution
π. We need the following standard lemma regarding KL divergences of marginal distributions.
Lemma 183. Let Pz and Qz be distributions supported on X indexed by z, a random variable
distributed as πz. Let eP be the joint distribution of (x, z) for x ∼Pz and z ∼πz, and eQ be the joint
distribution of (x, z) as x ∼Qz and z ∼πz. Let P and Q be the marginal distribution of eP and eQ
on x, averaged over z. Then,
dKL(P∥Q) ≤Ez∼πz [dKL(Pz∥Qz)] .

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
509
Proof. By the deﬁnition of dKL,
dKL( eP∥eQ) = E(x,z)∼e
P
"
log
eP(x, z)
eQ(x, z)
#
= Ez∼πz
"
Ex∼Pz
"
log
eP(x, z)
eQ(x, z)
##
= Ez∼πz

Ex∼Pz

log Pz(x)
Qz(x)

= Ez∼πz [dKL(Pz∥Qz)] .
Finally, by the data processing inequality,
dKL(P∥Q) ≤dKL( eP∥eQ) = Ez∼πz [dKL(Pz∥Qz)] .
The following lemma shows that a 2-Wasserstein distance bound on the distribution at iteration
k implies a KL divergence bound on iteration k + 1.
Lemma 184. Let πk be the distribution of xk for some k such that W2(πk, π) ≤∆and π be the
x-marginal of ˆπ. Then,
dKL(πk+1∥π) ≤∆2
2η .
Proof. As in Lemma 182, let Γxk be the optimal coupling between xk ∼πk and ˆx ∼π, which yields
a coupling Γyk+1 between yk+1 and ˆy such that
EΓyk+1
h
∥yk+1 −ˆy∥2
2
i
= EΓxk
h
∥xk −ˆx∥2
2
i
≤∆2.
(10.17)
Then,
dKL(πk+1∥π) ≤E(yk+1,ˆy)∼Γyk

dKL(πyk+1∥πˆy)

≤
1
2η2

µ + 1
η
E(yk+1,ˆy)∼Γyk+1
h
∥yk+1 −ˆy∥2
2
i
≤∆2
2η .
The ﬁrst inequality followed from Lemma 184 by taking P = πk+1, Q = π and z = (yk+1, y). The
second inequality used the log-Sobolev inequality and (10.16). The last inequality used (10.17).
Finally, putting the pieces together, Theorem 68 follows from Lemma 182 and Lemma 184.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
510
Proof of Theorem 68. By Lemma 182 and Lemma 184, there is T = O

1
ηµ log
d
ηµϵ

so that dKL(πT ∥π) ≤
2ϵ2. By Pinsker's inequality,
∥πT −π∥TV ≤
r
1
2dKL(πT ∥π) = ϵ.
We note that Theorem 68 is robust to a small amount of error tolerance in the sampler O.
Speciﬁcally, if O has tolerance
ϵ
2T , then by calling Theorem 68 with desired accuracy
ϵ
2 and ad-
justing constants appropriately, the cumulative error incurred by all calls to O is within the total
requisite bound (formally, this can be shown via the coupling characterization of total variation).
We defer a more formal elaboration on this inexactness argument to Appendix I.1 and the proof of
Proposition 46.
10.4
Tighter runtimes for structured densities
In this section, we use applications of Theorem 68 to obtain simple analyses of novel state-of-the-
art high-accuracy runtimes for the well-conditioned densities studied in [210, 128, 344], as well as
the composite and ﬁnite sum densities studied in this work. We will assume the conclusions of
Theorems 69 and 70 respectively in deriving the results of Sections 10.4.2 and 10.4.3.
10.4.1
Well-conditioned logconcave sampling: proof of Corollary 45
In this section, let π be a distribution on Rd with density proportional to exp(−f(x)), where f is L-
smooth and µ-strongly convex (and κ = L
µ ) and has pre-computed minimizer x∗. We will instantiate
Theorem 68 with foracle(x) = f(x), and choose η =
1
8Ld log(κ). We now require an η-RGO O for
foracle = f to use in Theorem 68.
Our implementation of O is a rejection sampling scheme. We use the following helpful guarantee.
Lemma 185 (Rejection sampling). Let π, ˆπ be distributions on Rd with dπ
dx(x) ∝p(x), dˆπ
dx(x) ∝ˆp(x).
Suppose for some C ≥1 and all x ∈Rd, p(x)
ˆp(x) ≤C. The following is termed "rejection sampling":
repeat independent runs of the following procedure until a point is outputted.
1. Draw x ∼ˆπ.
2. With probability
p(x)
C ˆp(x), output x.
Rejection sampling terminates in C
R
ˆp(x)dx
R
p(x)dx
runs in expectation, and the output distribution is π.
Proof. The second claim follows from Bayes' rule which implies the conditional density of the output
point is proportional to ˆp(x) ·
p(x)
C
ˆ
p(x) ∝p(x), so the distribution is π. To see the ﬁrst claim, the

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
511
probability any sample outputs is
Z
x
p(x)
C ˆp(x)dˆπ(x) = 1
C
Z
x
R
x p(x)dx
R
x ˆp(x)dxdπ(x) =
R
x p(x)dx
C
R
x ˆp(x)dx.
The conclusion follows by independence and linearity of expectation.
We further state a concentration bound shown ﬁrst in [344] regarding the norm of the gradient of a
point drawn from a logsmooth distribution. For completeness, a proof can be found in Appendix I.6;
see a formal statement in slightly more generality in Theorem 95.
Proposition 43 (Logsmooth gradient concentration, Corollary 3.3, [344]). Let π be a distribution
on Rd with dπ
dx(x) ∝exp(−f(x)) where f is convex and L-smooth. With probability at least 1 −κ−d,
∥∇f(x)∥2 ≤3
√
Ld log κ for x ∼π.
(10.18)
By the requirements of Theorem 68, the restricted Gaussian oracle O only must be able to draw
samples from densities of the form, for some y ∈Rd,
exp

−foracle(x) −1
2η ∥x −y∥2
2

= exp

−f(x) −4Ld log κ ∥x −y∥2
2

.
(10.19)
We will use the following Algorithm 60 to implement O.
Algorithm 60: XSample(f, y, η)
1 Input: L-smooth, µ-strongly convex f : Rd →R, y ∈Rd, η > 0;
2 if ∥∇f(y)∥2 ≤3
√
Ld log κ then
3
while true do
4
Draw x ∼N(y −∇f(y), ηI);
5
τ ∼Unif[0, 1];
6
if τ ≤exp(f(y) + ⟨∇f(y), x −y⟩−f(x)) then
7
Return: x;
8 Use [128] to sample x from (10.19) to total variation distance
ϵ
Θ(κd2 log3( κd
ϵ )) using
O(d log κd
ϵ ) queries to ∇f (Theorem 1, [128], where (10.19) has constant condition
number);
9 Return: x;
Lemma 186. Let η =
1
8Ld log(κ), and suppose y satisﬁes the bound in (10.18), i.e. ∥∇f(y)∥2 ≤
3
√
Ld log κ. Then, Line 3 of Algorithm 60 runs an expected 2 times, and Algorithm 60 samples
exactly from (10.19), whenever the condition of Line 1 is met.
Proof. Note that when the assumption of Line 1 is met, Algorithm 60 is an instantiation of rejection

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
512
sampling (Lemma 185) with
p(x) = exp

−f(x) −1
2η ∥x −y∥2
2

,
ˆp(x) = exp

−f(y) −⟨∇f(y), x −y⟩−1
2η ∥x −y∥2
2

.
By convexity, we may take C = 1. Next, by applying Fact 27 twice and L-smoothness of foracle,
Z
x
p(x)dx ≥
Z
x
exp

−f(y) −⟨∇f(y), x −y⟩−1 + ηL
2η
∥x −y∥2
2

dx
= exp

−f(y) +
η
2(1 + ηL) ∥∇f(y)∥2
2
 Z
x
exp
 
−1 + ηL
2η
x −y +
η
1 + ηL∇f(y)

2
2
!
dx
= exp

−f(y) +
η
2(1 + ηL) ∥∇f(y)∥2
2
  2πη
1 + ηL
 d
2
,
Z
x
ˆp(x)dx = exp

−f(y) + η
2 ∥∇f(y)∥2
2

(2πη)
d
2 ,
which implies the desired bound (recalling Lemma 185 and our assumed bound on ∥∇f(y)∥2)
R
ˆp(x)dx
R
p(x)dx ≤exp
η
2 −
η
2(1 + ηL)

∥∇f(y)∥2
2

(1 + ηL)
d
2
≤1.5 exp

η2L
2(1 + ηL) ∥∇f(y)∥2
2

≤2.
We are now equipped to prove our main result concerning well-conditioned densities.
Corollary 45. Let π be a distribution on Rd with dπ
dx(x) ∝exp (−f(x)) such that f is L-smooth and
µ-strongly convex, and let ϵ ∈(0, 1), κ = L
µ . Assume access to x∗= argminx∈Rdf(x). Algorithm 59
with η =
1
8Ld log(κ) using Algorithm 60 as a restricted Gaussian oracle for f uses O(κd log κ log κd
ϵ )
gradient queries in expectation, and obtains ϵ total variation distance to π.
Proof. By applying Theorem 68 with the chosen η, and noting that the cumulative error due to all
calls to Line 10 cannot amount to more than ϵ
2 total variation error throughout the algorithm, it
suﬃces to show that Algorithm 60 uses O(1) gradient queries each iteration in expectation. This
happens whenever the condition in Line 1 is met via Lemma 186, so we must show Line 10 is executed
with probability O((d log κd
ϵ )−1).
To show this, note that combining Proposition 43 with the warmness of the start x0 in Algo-
rithm 60, this event occurs with probability at most κ−d
2 in the ﬁrst iteration.13 Since warmness
13Formally, Line 2 of Algorithm 59 has y1 ∼N(x0, ηI), but by smoothness ∥∇f(y1)∥2 ≤∥∇f(x0)∥2 + L ∥x −y∥2
and L ∥x −y∥2 ≤e
O(L√η) with high probability, adding a negligible constant to the bound of Proposition 43.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
513
is monotonically decreasing14 throughout using an exact oracle in Algorithm 59, and the total er-
ror accumulated due to Line 10 throughout the algorithm is O((d log κd
ϵ )−1), we have the desired
conclusion.
We show a bound nearly-matching Corollary 45 using only value access to f, and with a deter-
ministic iteration complexity (rather than an expected one), as Corollary 49 in Section 10.4.3.
10.4.2
Composite logconcave sampling: proof of Corollary 46
In this section, let π be a distribution on Rd with density proportional to exp(−f(x) −g(x)),
where f is L-smooth and µ-strongly convex (and κ = L
µ ), and g is convex and admits a restricted
Gaussian oracle O. Without loss of generality, we assume that f and g share a minimizer x∗which
we have pre-computed; if this is not the case, we can redeﬁne f(x) ←f(x) −⟨∇f(x∗), x⟩and
g(x) ←g(x) + ⟨∇f(x∗), x⟩; see Section 10.5.1 for this reduction.
We will instantiate Theorem 68 with foracle = f + g, which is a µ-strongly convex function. Our
main result of this section follows directly from Theorem 68 and using Theorem 69 as the required
oracle O, stated more precisely in the following.
Corollary 46. Let π be a distribution on Rd with dπ
dx(x) ∝exp(−f(x)−g(x)) such that f is L-smooth
and µ-strongly convex, and let ϵ ∈(0, 1), κ = L
µ . Assume access to x∗= argminx∈Rd{f(x)+g(x)} and
let O be a restricted Gaussian oracle for g. There is an algorithm (Algorithm 59 using Theorem 69
as a restricted Gaussian oracle) which runs in O(κd log3 κd
ϵ ) iterations in expectation, each querying
a gradient of f and O a constant number of times, and obtains ϵ total variation distance to π.
Proof. As discussed at the beginning of this section, assume without loss that f and g both are
minimized by x∗. We apply the algorithm of Theorem 68 with η =
1
L to the µ-strongly convex
function f + g, which requires one call to O to implement. Thus, the iteration count parameter in
Theorem 68 is T = O(κ log κd
ϵ ).
Recall that we chose η = 1
L. To bound the total complexity of this algorithm, it suﬃces to give
an η-RGO O+ for sampling from distributions with densities of the form, for some y ∈Rd,
exp

−f(x) −g(x) −1
2η ∥x −y∥2
2

= exp

−f(x) −g(x) −L
2 ∥x −y∥2
2

to total variation distance
ϵ
Θ(T ) (see discussion at the end of Section 10.3). To this end, we apply
Theorem 69 with the well-conditioned component f(x) + L
2 ∥x −y∥2
2, the composite component
g(x), and the largest possible choice of η. Note that we indeed have access to a restricted Gaussian
oracle for g (namely, O), and this choice of well-conditioned component is 2L-smooth and L-strongly
convex, so its condition number is a constant. Thus, Theorem 69 requires O(d log2 κd
ϵ ) calls to O
14This is a standard fact in the literature, and can be seen as follows: each transition step in the chain is a convex
combination of warm point masses, preserving warmness.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
514
and gradients of f to implement the desired O+ on any query y (where we note
ϵ
Θ(T ) =
1
poly(κ,d,ϵ−1)).
Combining these complexity bounds yields the desired conclusion.
10.4.3
Sampling logconcave ﬁnite sums: proof of Corollary 47
In this section, let π be a distribution on Rd with density proportional to exp(−F(x)), where
F(x) = 1
n
P
i∈[n] fi(x) is µ-strongly convex, and for all i ∈[n], fi is L-smooth (and κ = L
µ ). We will
instantiate Theorem 68 with foracle(x) = F(x), and Theorem 70 as an η-RGO for some choice of η.
More precisely, Theorem 70 shows that given access to the minimizer x∗, only zeroth-order access
to the summands of F is necessary to obtain the iteration bound. In order to obtain the minimizer
to high accuracy however, variance reduced stochastic gradient methods (e.g. [300]) require Ω(n+κ)
gradient queries, which amounts to Ω((n+κ)d) function evaluations. We state a convenient corollary
of Theorem 70 which removes the requirement of accessing x∗, via an optimization pre-processing
step using the method of [300] (see further discussion in Appendix I.1). This is useful to us in
proving Theorem 47 because in the sampling tasks required by the RGO, the minimizer changes
(and thus must be recomputed every time).
Corollary 48 (First-order logconcave ﬁnite sum sampling). In the setting of Theorem 70, using
[300] to precompute the minimizer x∗and running Algorithm 64 uses O(n log κd
ϵ + κ2d log4 nκd
ϵ )
ﬁrst-order oracle queries to summands {fi}i∈[n] and obtains ϵ total variation distance to π.
We now apply the reduction framework developed in Section 10.3 to our Algorithm 64 to obtain
an improved query complexity for sampling from logconcave ﬁnite sums.
Corollary 47 (Improved ﬁrst-order logconcave ﬁnite sum sampling). In the setting of Theorem 70,
Algorithm 59 using Algorithm 64 and SVRG [300] as a restricted Gaussian oracle for F uses
O

n log
nκd
ϵ

+ κ
√
nd log3.5
nκd
ϵ

+ κd log5
nκd
ϵ

= eO

n + κ max

d,
√
nd

queries to ﬁrst-order oracles for summands {fi}i∈[n], and obtains ϵ total variation distance to π.
Proof. We apply Theorem 68 with µ-strongly convex foracle = F(x), using Algorithm 64 as the
required η-RGO O for sampling from distributions with densities of the form
exp

−F(x) −1
η ∥x −y∥2
2

for some y ∈Rd, to total variation
ϵ
Θ(T ) (see Section 10.3) for T the iteration bound of Algorithm 59.
We apply Theorem 70 to the function eF(x) = F(x) + 1
η ∥x −y∥2
2; we can express this in ﬁnite sum
form by adding 1
η ∥x −y∥2
2 to every constituent function, and the eﬀect on gradient oracles is 1
η(x−y).

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
515
Note eF has condition number O(1 + ηL). For a given η, the overall complexity is
log κd
ϵ
ηµ

n log
nκd
ϵ

+ d log4
nκd
ϵ

+ (ηL)2d log4
nκd
ϵ

Here, the inner loop complexity uses Corollary 48 to also ﬁnd the minimizer (for warm starts), and
the outer loop complexity is by Theorem 68. The result follows by optimizing over η, namely picking
η = max( 1
L,
q
n
L2d log3(nκd/ϵ)), and that Algorithm 59 always must have at least one iteration.
Note the only place that Corollary 47 used gradient evaluations was in determining minimizers
of subproblems, via the ﬁrst step of Corollary 48. Consider now the n = 1 case. By running e.g.
accelerated gradient descent for smooth and strongly convex functions, it is well-known [417] that we
can obtain a minimizer in eO(√κ) iterations, each querying a gradient oracle, where κ is the condition
number. By smoothness, we can approximate every coordinate of the gradient to arbitrary precision
using 2 function evaluations, so this is a eO(√κd) value oracle complexity.
Finally, for every optimization subproblem in Corollary 47 where η = (L · polylog κd
ϵ )−1, the
condition number is a constant, which amounts to a eO(d) value oracle complexity for computing
a minimizer.
This is never the dominant term compared to Theorem 70, yielding the following
conclusion.
Corollary 49. In the setting of Corollary 45, Algorithm 59 using Algorithm 64 as a restricted
Gaussian oracle uses O(κd log2 κd
ϵ ) value queries and obtains ϵ total variation distance to π.
We note that the polylogarithmic factor is signiﬁcantly improved when compared to Corollary 47
by removing the random sampling steps in Algorithm 64. A precise complexity bound of the result-
ing Metropolized random walk, a zeroth-order algorithm mixing in O(κ2d log κd
ϵ ) for a logconcave
distribution with condition number κ, is given as Theorem 2 of [128].
Finally, in the case n ≥1, we also exhibit an improved query complexity in terms of an entirely
zeroth-order sampling algorithm which interpolates with Corollary 49 (up to logarithmic factors).
By trading oﬀthe eO(nd + κd) zeroth-order complexity of minimizing a ﬁnite sum function [300],
and the eO(κ2d) zeroth-order complexity of sampling, we can run Theorem 68 for the optimal choice
of η = eO(
√n
L ). The overall zeroth-order complexity can be seen to be eO(nd + √nκd).

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
516
10.5
Composite logconcave sampling with a restricted Gaus-
sian oracle
In this section, we provide our "base sampler" for composite logconcave densities as Algorithm 61,
and give its guarantees by proving Theorem 69. Throughout, ﬁx distribution π with density
dπ
dx(x) ∝exp (−f(x) −g(x)) , where f : Rd →R is L-smooth, µ-strongly convex,
and g : Rd →R admits a restricted Gaussian oracle O.
(10.20)
We will deﬁne κ := L
µ , and assume that we have precomputed x∗:= argminx∈Rd {f(x) + g(x)}. Our
algorithm proceeds in stages following the outline in Section 10.1.4.
1. Composite-Sample is reduced to Composite-Sample-Shared-Min, which takes as input a dis-
tribution with negative log-density f + g, where f and g share a minimizer; this reduction is
given in Section 10.5.1, and the remainder of the section handles the shared-minimizer case.
2. The algorithm Composite-Sample-Shared-Min is a rejection sampling scheme built on top of
sampling from a joint distribution ˆπ on (x, y) ∈Rd × Rd whose x-marginal approximates π.
We give this reduction in Section 10.5.2.
3. The bulk of our analysis is for Sample-Joint-Dist, an alternating marginal sampling algo-
rithm for sampling from ˆπ. To implement marginal sampling, it alternates calls to O and a
rejection sampling algorithm YSample. We prove its correctness in Section 10.5.3.
We put these pieces together in Section 10.5.4 to prove Theorem 69. We remark that for sim-
plicity, we will give the algorithms corresponding to the largest value of step size η in the theorem
statement; it is straightforward to modify the bounds to tolerate smaller values of η, which will cause
the mixing time to become correspondingly larger (in particular, the value of K in Algorithm 63).
Algorithm 61: Composite-Sample(π, x∗, ϵ)
1 Input: Distribution π of form (10.20), x∗minimizing negative log-density of π, ϵ ∈[0, 1];
2 Output: Sample x from a distribution π′ with ∥π′ −π∥TV ≤ϵ;
3 ˜f(x) ←f(x) −⟨∇f(x∗), x⟩, ˜g(x) ←g(x) + ⟨∇f(x∗), x⟩;
4 Return: Composite-Sample-Shared-Min(π, ˜f, ˜g, x∗, ϵ);
10.5.1
Reduction from Composite-Sample to Composite-Sample-Shared-Min
Correctness of Composite-Sample is via the following properties.
Proposition 44. Let ˜f and ˜g be deﬁned as in Composite-Sample.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
517
Algorithm 62: Composite-Sample-Shared-Min(π, f, g, x∗, ϵ)
1 Input: Distribution π of form (10.20), where f and g are both minimized by x∗, ϵ ∈[0, 1];
2 Output: Sample x from a distribution π′ with ∥π′ −π∥TV ≤ϵ;
3 while true do
4
Deﬁne the set
Ω:=
(
x | ∥x −x∗∥2 ≤4
s
d log(288κ/ϵ)
µ
)
(10.21)
;
5
x ←Sample-Joint-Dist(f, g, x∗, O, ϵ
18);
6
if x ∈Ωthen
7
τ ∼Unif[0, 1];
8
y ←YSample(f, x, η);
9
α ←exp

f(y) −⟨∇f(x), y −x⟩−L
2 ∥y −x∥2
2 + g(x) + ηL2
2 ∥x −x∗∥2
2

;
10
ˆθ ←exp

−f(x) −g(x) +
η
2(1+ηL) ∥∇f(x)∥2
2

(1 + ηL)
d
2 α;
11
if τ ≤
ˆθ
4 then
12
Return: x;
1. The density ∝exp(−f(x) −g(x)) is the same as the density ∝exp(−˜f(x) −˜g(x)).
2. Assuming ﬁrst-order (function and gradient evaluation) access to f, and restricted Gaussian
oracle access to g, we can implement the same accesses to ˜f, ˜g with constant overhead.
3. ˜f and ˜g are both minimized by x∗.
Proof. For f and g with properties as in (10.20), with x∗minimizing f + g, deﬁne the functions
˜f(x) := f(x) −⟨∇f(x∗), x⟩, ˜g(x) := g(x) + ⟨∇f(x∗), x⟩,
and observe that ˜f + ˜g = f + g everywhere. This proves the ﬁrst claim. Further, implementation of
a ﬁrst-order oracle for ˜f and a restricted Gaussian oracle for ˜g are immediate assuming a ﬁrst-order
oracle for f and a restricted Gaussian oracle for g, showing the second claim; any quadratic shifted
by a linear term is the sum of a quadratic and a constant. We now show ˜f and ˜g have the same
minimizer. By strong convexity, ˜f has a unique minimizer; ﬁrst-order optimality shows that
∇˜f(x∗) = ∇f(x∗) −∇f(x∗) = 0,
so this unique minimizer is x∗. Moreover, optimality of x∗for f + g implies that for all x ∈Rd,
⟨∂g(x∗) + ∇f(x∗), x∗−x⟩≤0.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
518
Algorithm 63: Sample-Joint-Dist(f, g, x∗, η, O, δ)
1 Input: f, g of form (10.20) both minimized by x∗, δ ∈[0, 1], η > 0, O restricted Gaussian
oracle for g;
2 Output: Sample x from a distribution ˆπ′ with ∥ˆπ′ −ˆπ∥TV ≤δ, where we overload ˆπ to
mean the marginal of (10.22) on the x variable;
3 η ←
1
32Lκd log(16κ/δ);
4 Let ˆπ be the density with
dˆπ
dx(z) ∝exp

−f(y) −g(x) −1
2η ∥y −x∥2
2 −ηL2
2
∥x −x∗∥2
2

(10.22)
;
5 Call O to sample x0 ∼πstart, for
dπstart(x)
dx
∝exp

−L + ηL2
2
∥x −x∗∥2
2 −g(x)

(10.23)
;
6 K ←226·100
ηµ
log

d log(16κ)
4δ

(see Remark 10);
7 for k ∈[K] do
8
Call YSample

f, xk−1, η,
δ
2Kd log( dκ
δ )

to sample yk ∼πxk−1 (Algorithm 3), for
dπx
dy (y) ∝exp

−f(y) −1
2η ∥y −x∥2
2

(10.24)
;
9
Call O to sample xk ∼πyk, for
dπy
dx (x) ∝exp

−g(x) −1
2η ∥y −x∥2
2 −ηL2
2
∥x −x∗∥2
2

(10.25)
;
10 Return: xK;

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
519
Here, ∂g is a subgradient. This shows ﬁrst-order optimality of x∗for ˜g also, so x∗minimizes ˜g.
10.5.2
Reduction from Composite-Sample-Shared-Min to Sample-Joint-Dist
Composite-Sample-Shared-Min is a rejection sampling scheme, which accepts samples from sub-
routine Sample-Joint-Dist in the high-probability region Ωdeﬁned in (10.21). We give a general
analysis for approximate rejection sampling in Appendix I.2.1, and Appendix I.2.1 bounds relation-
ships between distributions π and ˆπ, deﬁned in (10.20) and (10.22) respectively (i.e. relative densities
and normalization constant ratios). Combining these pieces proves the following main claim.
Proposition 45. Let η =
1
32Lκd log(288κ/ϵ), and assume Sample-Joint-Dist(f, g, x∗, O, δ) samples
within δ total variation of the x-marginal on (10.22). Composite-Sample-Shared-Min outputs a
sample within total variation ϵ of (10.20) in an expected O(1) calls to Sample-Joint-Dist.
10.5.3
Implementing Sample-Joint-Dist
Sample-Joint-Dist alternates between sampling marginals in the joint distribution ˆπ, as seen
by deﬁnitions (10.24), (10.25). We showed that marginal sampling attains the correct stationary
distribution as Lemma 181. We bound the conductance of the induced walk on iterates {xk} by
combining an isoperimetry bound with a total variation guarantee between transitions of nearby
points in Appendix I.2.2. Finally, we give a simple rejection sampling scheme YSample as Algorithm 3
for implementing the step (10.24). Since the y-marginal of ˆπ is a bounded perturbation of a Gaussian
(intuitively, f is L-smooth and η−1 ≫L), we show in a high probability region that rejecting from
the sum of a ﬁrst-order approximation to f and the Gaussian succeeds in 2 iterations.
Remark 10. For simplicity of presentation, we were conservative in bounding constants throughout;
in practice, we found that the constant in Line 4 is orders of magnitude too large (a constant < 10
suﬃced), which can be found as Section 4 of [481]. Several constants were inherited from prior
analyses, which we do not rederive to save on redundancy.
We now give a complete guarantee on the complexity of Sample-Joint-Dist.
Proposition 46. Sample-Joint-Dist outputs a point with distribution within δ total variation
distance from the x-marginal of ˆπ. The expected number of gradient queries per iteration is constant.
10.5.4
Putting it all together: proof of Theorem 69
We show Theorem 69 follows from the guarantees of Propositions 44, 45, and 46. Formally, The-
orem 69 is stated for an arbitrary value of η which is upper bounded by the value in Line 1 of
Algorithm 63; however, it is straightforward to see that all our proofs go through for any smaller
value. By observing the value of K in Sample-Joint-Dist, we see that the number of total it-
erations in each call to Sample-Joint-Dist O

1
ηµ log( κd
ϵ )

= O
 κ2d log2   κd
δ

. Proposition 46

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
520
also shows that every iteration, we require an expected constant number of gradient queries and
calls to O, the restricted Gaussian oracle for g, and that the resulting distribution has δ total
variation from the desired marginal of ˆπ. Next, Proposition 45 implies that the number of calls
to Sample-Joint-Dist in a run of Composite-Sample-Shared-Min is bounded by a constant, the
choice of δ is Θ(ϵ), and the resulting point has total variation ϵ from the original distribution π.
Finally, Proposition 44 shows sampling from a general distribution of the form (10.1) is reducible to
one call of Composite-Sample-Shared-Min, and the requisite oracles are implementable.
10.6
Logconcave ﬁnite sums
In this section, we provide our "base sampler" for logconcave ﬁnite sums as Algorithm 64, and give
its guarantees by proving Theorem 70. Throughout, ﬁx distribution π with density
dπ
dx(x) ∝exp(−F(x)), where F(x) = 1
n
X
i∈[n]
fi(x) is µ-strongly convex,
and for all i ∈[n], fi is L-smooth.
We will deﬁne κ := L
µ , and assume that we have precomputed x∗:= argminx∈Rd{F(x)}. We will also
assume explicitly that ∇fi(x∗) = 0 for all i ∈[n] throughout this section (i.e. all fi are minimized
at the same point); this is without loss of generality, by a similar argument as in Proposition 44.
Algorithm 64: FiniteSum-MRW(F, h, x0, p, K)
1 Input: F(x) = 1
n
P
i∈[n] fi(x), step size h > 0, initial x0, p ∈[0, 1], iteration count K ∈N;
2 for 0 ≤k < K do
3
Draw ξk ∼N(0, I);
4
yk+1 ←xk +
√
2hξk;
5
Draw Sk ⊆[n] by including each i ∈Sk independently with probability p;
6
For each i ∈[n],
γ(i)
k
←
(
1
p
q
exp
 −1
nfi(yk+1) + 1
nfi(xk)

−1

+ 1
i ∈Sk
1
i ̸∈Sk
;
7
γk ←Qn
i=1 γ(i)
k , τ ∼Unif[0, 1];
8
if τ ≤3
4γk and |Sk| ≤2pn then
9
xk+1 ←yk+1;
10
else
11
xk+1 ←xk;
12 Return: xK;

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
521
Algorithm 64 is the zeroth-order Metropolized random walk of [210] with an eﬃcient, but biased,
ﬁlter step; the goal of our analysis is to show this bias does not incur signiﬁcant error.
10.6.1
Approximate Metropolis-Hastings
We ﬁrst recall the following well-known fact underlying Metropolis-Hastings (MH) ﬁlters.
Proposition 47. Consider a random walk on Rd with proposal distributions {Px}x∈Rd and accep-
tance probabilities {α(x, x′)}x,x′∈Rd conducted as follows: at a current point x,
1. Draw a point x′ ∼Px.
2. Move the random walk to x′ with probability α(x, x′), else stay at x.
Suppose Px(x′) = Px′(x) for all pairs x, x′ ∈Rd, and further dπ
dx(x)α(x, x′) = dπ
dx(x′)α(x′, x). Then,
π is a stationary distribution for the random walk.
Proof. This follows because the walk satisﬁes detailed balance (reversibility) with respect to π.
We propose an algorithm that applies a variant of the Metropolis-Hastings ﬁlter to a Gaussian
random walk. Speciﬁcally, we deﬁne the following algorithm, which we call Inefficient-MRW.
Deﬁnition 46 (Inefficient-MRW). Consider the following random walk for some step size h > 0:
for each iteration k at a current point xk ∈Rd,
1. Set yk+1 ←xk +
√
2hξ, where ξ ∼N(0, I).
2. xk+1 ←yk+1 with probability α(xk, yk+1) (otherwise, xk+1 ←xk), where
α(x, y) =









1
q
exp(−F (y))
exp(−F (x)) > 4
3,
3
4
q
exp(−F (y))
exp(−F (x))
3
4 ≤
q
exp(−F (y))
exp(−F (x)) ≤4
3,
exp(−F (y))
exp(−F (x))
q
exp(−F (y))
exp(−F (x)) < 3
4.
(10.26)
Lemma 187. Distribution π with dπ
dx(x) ∝exp(−F(x)) is stationary for Inefficient-MRW.
Proof. Without loss of generality, assume that π has been normalized so that dπ
dx(x) = exp(−F(x)).
We apply Proposition 47, dropping subscripts in the following. It is clear that Px(y) = Py(x) for
any x, y, so it suﬃces to check the second condition. When 3
4 ≤
q
exp(−F (y))
exp(−F (x)) ≤4
3, this follows from
dπ
dx(x)α(x, x′) = 3
4
p
exp(−F(x) −F(y)) = dπ
dx(x′)α(x′, x).
The other case is similar (as it is a standard Metropolis-Hastings ﬁlter).

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
522
In Algorithm 64, we implement an approximate version of the modiﬁed MH ﬁlter in Deﬁnition 46,
where we always assume the pair x, y are in the second case of (10.26). In Lemma 188, we show
that if a certain boundedness condition holds, then Algorithm 64 approximates Inefficient-MRW
well. We then show that the output distributions of Inefficient-MRW and our Algorithm 64 have
small total variation distance in Lemma 189.
Lemma 188. Suppose that in an iteration 0 ≤k < K of Algorithm 64, the following three conditions
hold for some parameters Rx, Cξ, Cx ∈R≥0:
1. ∥xk −x∗∥2 ≤Rx.
2. ∥ξk∥2 ≤Cξ
√
d.
3. For all i ∈[n], |∇fi(xk)⊤ξk| ≤Cx ∥∇fi(xk)∥2.
Then, for any
h ≤
1
98C2xL2R2x + 7LC2
ξ d,
(10.27)
3
4 ≤
q
exp(−F (yk+1))
exp(−F (xk))
≤4
3. Moreover, we have E [γk] =
q
exp(−F (yk+1))
exp(−F (xk)) , and when |Sk| ≤2pn, γk ≤4
3.
Proof. We ﬁrst show E [γk] =
q
exp(−F (yk+1))
exp(−F (xk)) . Since each i ∈Sk is generated independently,
E [γk] =
Y
i∈[n]
E
h
γ(i)
k
i
=
Y
i∈[n]
"
(1 −p) + p
 
1
p
 s
exp

−1
nfi(yk+1) + 1
nfi(xk)

−1
!
+ 1
!#
=
Y
i∈[n]
s
exp

−1
nfi(yk+1) + 1
nfi(xk)

=
s
exp(−F(yk+1))
exp(−F(xk)) .
Next, for any i ∈[n], we lower and upper bound −fi(yk+1) + fi(xk). First,
−fi(yk+1) + fi(xk) ≤∇fi(xk)⊤(xk −yk+1)
≤
√
2hCx ∥∇fi(xk)∥2 ≤
√
2hCxLRx.
The ﬁrst inequality followed from convexity of fi, the second from yk+1 −xk =
√
2hξk and our
assumed bound, and the third from smoothness and ∇f(x∗) = 0. To show a lower bound,
fi(yk+1) −fi(xk) ≤∇fi(xk)⊤(yk+1 −xk) + L
2 ∥yk+1 −xk∥2
2
≤
√
2hCxLRx + hLC2
ξ d.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
523
The ﬁrst inequality was smoothness. Repeating this argument for each i ∈[n] and averaging,
−
√
2hCxLRx −hLC2
ξ d ≤−F(yk+1) + F(xk) ≤
√
2hCxLRx.
(10.28)
Then, when h ≤
1
98C2xL2R2x+7LC2
ξ d,
3
4 ≤
s
exp(−F(yk+1))
exp(−F(xk))
≤4
3, and for all i ∈[n], −fi(yk+1) + fi(xk) ≤1
4.
Thus, we can bound each γ(i)
k :
γ(i)
k
≤1
p

exp
 1
8n

−1

+ 1 ≤1 +
1
7pn.
Finally, when |Sk| ≤2pn, γk ≤(1 +
1
7pn)2pn ≤4
3 as desired.
Lemma 189. Draw x0 ∼N(x∗, 1
LI). Let ˆπK be the output distribution of the algorithm of Deﬁni-
tion 46 for K steps starting from x0, and let πK be the output distribution of Algorithm 64 starting
from x0. For any δ ∈[0, 1], let p = 5 log 12K
δ
n
in Algorithm 64. There exist
Cξ = O

1 +
s
log K
δ
d

, Cx = O
 r
log nK
δ
!
,
and Rx = O


s
d log κK
δ
µ

,
so that when h ≤
1
98C2xL2R2x+7LC2
ξ d, we have ∥πK −ˆπK∥TV ≤δ.
Proof. By the coupling deﬁnition of total variation, it suﬃces to upper bound the probability that
the algorithms' trajectories, sharing all randomness in proposing points yk+1, diﬀer. This can happen
for two reasons: either we used an incorrect ﬁltering step (i.e. the pair (xk, yk+1) did not lie in the
second case of (10.26)), or we incorrectly rejected in Line 7 of Algorithm 64 because |Sk| ≥2pn. We
bound the error due to either happening over any iteration by δ, yielding the conclusion.
Incorrect ﬁltering.
Consider some iteration k.
Lemma 188 shows that as long as its three
conditions hold in iteration k, we are in the second case of (10.26), so it suﬃces to show all conditions
hold. By Fact 28 and as ξk is independent of all {∇fi(xk)}i∈[n], with probability at least 1 −
δ
2K ,
both of the conditions ∥ξk∥2 ≤Cξ
√
d and15 |∇fi(xk)⊤ξk| ≤Cx ∥∇fi(xk)∥2 for all i ∈[n] hold for
some
Cξ = O

1 +
s
log K
δ
d

, Cx = O
 r
log nK
δ
!
.
15We recall that the distribution of v⊤ξ for ξ ∼N(0, I) is the one-dimensional N(0, ∥v∥2
2).

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
524
Next, x0 ∼N(x∗, 1
LI) is drawn from a κ
d
2 warm start for π. By Fact 28, we have ∥x0 −x∗∥2 ≤Rx
for x0 drawn from π with probability at least 1 −
δ
4K · κ−d
2 , for some
Rx = O


s
d log κK
δ
µ

.
Since warmness of the exact algorithm of Deﬁnition 46 is monotonic, as long as the trajectories have
not diﬀered up to iteration k, ∥xk −x∗∥2 ≤Rx also holds with probability ≥1 −
δ
4K . Inductively,
the total variation error caused by incorrect ﬁltering over K steps is at most 3δ
4 .
Error due to large |Sk|.
Supposing all the conditions of Lemma 188 are satisﬁed in iteration k, we
show that with high probability, Inefficient-MRW and Algorithm 64 make the same accept or reject
decision. By Lemma 188, Inefficient-MRW (10.26) accepts with probability α′
k = 3
4
q
exp(−F (yk+1))
exp(−F (xk)) .
On the other hand, Algorithm 64 accepts with probability
αk = 3
4E [γk | |Sk| ≤2pn] · Pr[|Sk| ≤2pn].
The total variation between the output distributions is |αk −α′
k|. Further, since by Lemma 188,
α′
k = 3
4E [γk]
= 3
4 (E [γk | |Sk| ≤2pn] · Pr[|Sk| ≤2pn] + E [γk | |Sk| > 2pn] · Pr[|Sk| > 2pn])
= αk + 3
4E [γk | |Sk| > 2pn] · Pr[|Sk| > 2pn],
it suﬃces to upper bound this latter quantity. First, by Lemma 190, when p = 5 log 12K
δ
n
, we have
Pr[|Sk| > 2pn] ≤
δ
12K . Finally, since each i ∈Sk is generated independently,
E [γk | |Sk| > 2pn] ≤
max
S′:|S′|=2pn E

Y
i∈[n]
γ(i)
k
| S′ ⊆Sk


≤2E


Y
i∈[n]\S′
γ(i)
k

= 2
v
u
u
t
Y
[n]\S′
exp

−1
nfi(yk+1) + 1
nfi(xk))

≤4.
Here, we used Lemma 188 applied to the set S′, and the upper bound (10.28) we derived earlier.
Combining these calculations shows that the total variation distance incurred in any iteration k due
to |Sk| being too large is at most
δ
4K , so the overall contribution over K steps is at most δ
4.
We used the following helper lemma in our analysis.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
525
Lemma 190. Let S ⊆[n] be formed by independently including each i ∈[n] with probability p.
Then,
Pr [|S| > 2pn] ≤exp

−3pn
14

.
Proof. For i ∈[n], let 1i∈S be the indicator random variable of the event i ∈S, so E [1i∈S] = p and
Var [1i∈S −p] = p(1 −p)2 + (1 −p)p2 ≤2p.
By Bernstein's inequality,
Pr

X
i∈[n]
1i∈S ≥np + r

≤exp

−
1
2r2
2np + 1
3r

.
In particular, when r = pn, we have the desired conclusion.
10.6.2
Conductance analysis
We next bound the mixing time of Inefficient-MRW, using the following result from prior work.
We remark that in our application, the log β term is non-dominant.
Proposition 48 (Lemma 1, Lemma 2, [128]). Let a random walk with a µ-strongly logconcave
stationary distribution π on x ∈Rd have transition distributions {Tx}x∈Rd. For some ϵ ∈[0, 1], let
convex set Ω⊆Rd have π(Ω) ≥1 −
ϵ2
2β2 . Let πstart be a β-warm start for π, and let the algorithm
be initialized at x0 ∼πstart. Suppose for any x, x′ ∈Ωwith ∥x −x′∥2 ≤∆,
∥Tx −Tx′∥TV ≤7
8.
(10.29)
Then, the random walk mixes to total variation distance within ϵ of π in O(log β +
1
∆2µ log log β
ϵ )
iterations.
Consider an iteration of Inefficient-MRW from xk = x. Let Px be the density of yk+1, and let
Tx be the density of xk+1 after ﬁltering. Deﬁne a convex set Ω⊆Rd parameterized by RΩ∈R≥0:
Ω= {x ∈Rd : ∥x −x∗∥2 ≤RΩ}.
We show that for two close points x, x′ ⊆Ω, the total variation between Tx and Tx′ is small.
Lemma 191. For some h = O(
1
L2R2
Ω+Ld) and x, x′ ⊆Ωwith ∥x −x′∥2 ≤1
8
√
h, ∥Tx −Tx′∥TV ≤7
8.
Proof. By the triangle inequality of total variation distance,
∥Tx −Tx′∥TV ≤∥Tx −Px∥TV + ∥Px −Px′∥TV + ∥Tx′ −Px′∥TV .

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
526
First, by Pinsker's inequality and the KL divergence between Gaussian distributions,
∥Px −Px′∥TV ≤
p
2KL(Px||Px′) = ∥x −x′∥2
√
2h
.
When ∥x −x′∥2 ≤1
8
√
h, ∥Px −Px′∥TV ≤1
8. Next, we bound ∥Tx −Px∥TV: by a standard calcula-
tion (e.g. Lemma D.1 of [344]), we have
∥Tx −Px∥TV = 1 −3
4Eξk+1
"s
exp (−F(yk+1))
exp (−F(xk))
#
.
We show that ∥Tx −Px∥TV ≤3
8. It suﬃces to show that Eξk+1
hp
exp (−F(yk+1) + F(xk))
i
≥5
6.
Since 15
16
q
exp
 −1
16

≥5
6, it suﬃces to show that with probability at least 15
16 over the randomness
of ξk+1, −F(yk+1) + F(xk) ≥−1
16. As ξk+1 ∼N(0, Id), by applying Fact 28 twice,
Pr
h
∥ξk+1∥2
2 > 36d
i
≤exp(−4) ≤1
32,
Pr
h∇F(xk)⊤ξk+1
2 ≥36 ∥∇F(xk)∥2
2
i
≤1
32.
(10.30)
We upper bound the term F(yk+1) −F(xk) by smoothness and Cauchy-Schwarz:
F(yk+1) −F(xk) ≤∇F(xk)⊤(yk+1 −xk) + L
2 ∥yk+1 −xk∥2
2
≤
√
2h
∇F(xk)⊤ξk+1
 + hL∥ξk+1∥2
2.
Then, since ∥∇F(xk)∥≤LRΩwhen x ∈Ω, it is enough to choose h = O(
1
L2R2
Ω+Ld) so that
−F(yk+1) + F(xk) ≥−1
16,
as long as the events of (10.30) hold, which occurs with probability at least 15
16. Similarly, we can
show that ∥Tx′ −Px′∥TV ≤3
8. Combining the three bounds, we have the desired conclusion.
Theorem 70. Let π be a distribution on Rd with dπ
dx(x) ∝exp(−F(x)), where F(x) = 1
n
Pn
i=1 fi(x)
is µ-strongly convex, fi is L-smooth and convex ∀i ∈[n], κ = L
µ , and ϵ ∈(0, 1). Assume access
to x∗= argminx∈RdF(x). Algorithm 64 uses O
 κ2d log4 nκd
ϵ

value queries to summands {fi}i∈[n],
and obtains ϵ total variation distance to π.
Proof. First, N(x∗, 1
LI) yields a β = κ
d
2 -warm start for π (see e.g. [210]). For this value of β, by

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
527
Fact 28 it suﬃces to choose
RΩ= Θ
 s
d log κ
ϵ
µ
!
for π(Ω) ≥1 −
ϵ2
2β2 . Letting δ = ϵ
2, we will choose the step size h and iteration count K so that
1
h = Θ

Lκd log2 nκd
ϵ

, K = Θ

κ2d log3 nκd
ϵ

have constants compatible with Lemma 189. Note that this choice of h is also suﬃciently small to
apply Lemma 191 for our choice of RΩ. By applying Proposition 48 to the algorithm of Deﬁnition 46,
and using the bound from Lemma 191, in K iterations Inefficient-MRW will mix to total variation
distance δ to π.
Furthermore, applying Lemma 189, we conclude that Algorithm 64 has total
variation distance at most 2δ = ϵ from π.
It remains to bound the oracle complexity of Algorithm 64. Note in every iteration, we never
compute more than 4pn values of {fi}i∈[n], since we always reject if |Sk| ≥2pn, and we only
compute values for indices in Sk. For the value of p in Lemma 189, this amounts to O(log nκd
ϵ ) value
queries.
10.7
Lower bound for MALA on Gaussians
In this section, we derive a upper bound on the spectral gap of MALA when the target distribution
is restricted to being a multivariate Gaussian (i.e. its negative log-density is a quadratic in some well-
conditioned matrix A). Throughout this section we will let f(x) = 1
2x⊤Ax for some I ⪯A ⪯κI.
We remark here that without loss of generality, we have assumed that the minimizer of f is the all-
zeros vector and the strong convexity parameter is µ = 1. These follow from invariance of condition
number under linear translations and scalings of the variable.
Next, we deﬁne a speciﬁc hard quadratic function we will consider in this section, fhq : Rd →R.
Speciﬁcally, fhq will be a quadratic in a diagonal matrix A which has A11 = 1 and Aii = κ for
2 ≤i ≤d. We can rewrite this as
fhq(x) :=
X
i∈[d]
fi(xi), where fi(c) =



1
2c2
i = 1
κ
2 c2
2 ≤i ≤d
.
(10.31)
Notice that fhq is coordinate-wise separable, and behaves identically on coordinates 2 ≤i ≤d (and
diﬀerently on coordinate 1). To this end for a vector v ∈Rd, we will denote its ﬁrst coordinate by
v1 ∈R, and its remaining coordinates by v−1 ∈Rd−1. This will help us analyze the behavior of
these components separately, and simplify notation.
We next show that for coordinate-separable functions with well-behaved ﬁrst coordinate, such

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
528
as our fhq, the spectral gap (deﬁned in (10.10)) of the MALA Markov chain is governed by the step
size h. The following is an extension of an analogous proof in [137].
Lemma 192. Consider the MALA Markov chain (10.13), with stationary distribution π∗with
negative log-density f. Suppose f is coordinate-wise separable (i.e. f(x) = P
i∈[d] fi(xi)). If f(x) =
f(−x) for all x ∈Rd, f1 is O(1)-smooth, and Ex1∼exp(−f1)[x2
1] = Θ(1), the spectral gap (10.10) is
O(h + h2).
Proof. Recalling the deﬁnition (10.10), we choose g(x) = x1; note that by symmetry of f around
the origin, we have Eπ∗[g] = 0, and thus by our assumption,
Varπ∗[g] = Ex∼π∗[x2
1] = Θ(1).
Here we used that π∗is a product distribution. Thus it suﬃces to upper bound E(g, g):
E(g, g) = 1
2
ZZ
(x1 −y1)2Tx(y)dπ∗(x)dy
≤1
2
ZZ
(x1 −y1)2Px(y)dπ∗(x)dy
= 1
2Ex∼π∗,ξ∼N(0,1)

hf ′
1(x1) −
√
2hξ
2
≤Ex∼π∗
h
h2 (f ′
1(x1))2i
+ 2Eξ∼N(0,1)

hξ2
≤O(h2)Ex∼π∗
x2
1

+ 2h = O
 h + h2
.
In the second line, we used that whenever the Markov chain rejects the distribution both terms are
zero; in the third, we used the deﬁnition of the MALA proposals; in the fourth, we used (a + b)2 ≤
2a2 + 2b2 for a, b ∈R. Finally, the last line used that symmetry implies that the minimizer of f is
the origin, so applying Lipschitzness and f ′
1(0) = 0 yields the desired bound.
This immediately implies a spectral gap bound on our hard function fhq.
Corollary 50. The spectral gap of the MALA Markov chain for sampling from the density propor-
tional to exp(−fhq), where fhq is deﬁned in (10.31), is O(h + h2).
It remains to give a lower bound on the step size h, which we accomplish by upper bounding the
acceptance probability of MALA. We will give a step size analysis for a fairly general characterization
of Markov chains, where the proposal distribution from a point x is
y =

y1
y−1

, where y1 = (1 −α1)x1 + β1g1
and y−1 = (1 −α−1)x−1 + β−1g−1, for g ∼N(0, I).
(10.32)

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
529
To be concrete, recall that the proposal distribution for MALA (10.13) is given by y = x −hAx +
√
2hg. For the A used in deﬁning fhq, this is of the form (10.32) with the speciﬁc parameters
α1 = h, α−1 = hκ, β1 = β−1 =
√
2h.
However, this more general characterization will save signiﬁcant amounts of recalculation when
analyzing updates of the HMC Markov chain in Section 10.10. Recalling the formula (10.13), we
ﬁrst give a closed form for the acceptance probability.
Lemma 193. For f(x) = 1
2x⊤Ax, we have
f(x) −f(y) + 1
4h

∥y −(x −h∇f(x))∥2
2 −∥x −(y −h∇f(y))∥2
2

= h
4 ∥x∥2
A2 −h
4 ∥y∥2
A2 .
Supposing y is of the form in (10.32) and A is as in (10.31), we have
h
4 ∥x∥2
A2 −h
4 ∥y∥2
A2 = h
4
  2α1 −α2
1

x2
1 −β2
1g2
1 −2(1 −α1)β1x1g1

+ hκ2
4
 2α−1 −α2
−1

∥x−1∥2
2 −β2
−1 ∥g−1∥2
2 −2(1 −α−1)β−1 ⟨x−1, g−1⟩

.
Proof. This is a direct computation which we perform here for completeness: the given quantity is
1
2 ∥x∥2
A −1
2 ∥y∥2
A + 1
4h

∥y −x + hAx∥2
2 −∥x −y + hAy∥2
2

= 1
2 ∥x∥2
A −1
2 ∥y∥2
A + 1
2 ⟨y −x, Ax⟩+ h
4 ∥x∥2
A2 −1
2 ⟨x −y, Ay⟩−h
4 ∥y∥2
A2 = h
4 ∥x∥2
A2 −h
4 ∥y∥2
A2 .
The second equality follows from expanding the deﬁnition of y:
∥x∥2
A2 −∥y∥2
A2 = x2
1 −((1 −α1) x1 + β1g1)2 + κ2 
∥x−1∥2
2 −∥(1 −α−1) x−1 + β−1g−1∥2
2

=
 2α1 −α2
1

x2
1 −β2
1g2
1 −2(1 −α1)β1x1g1
+ κ2  2α−1 −α2
−1

∥x−1∥2
2 −β2
−1 ∥g−1∥2
2 −2(1 −α−1)β−1 ⟨x−1, g−1⟩

.
Corollary 51. For any ﬁxed x ∈Rd, and supposing y is of the form in (10.32) and A is as in
(10.31),
Eg∼N(0,I)

f(x) −f(y) + 1
4h

∥y −(x −h∇f(x))∥2
2 −∥x −(y −h∇f(y))∥2
2

= h
4
  2α1 −α2
1

x2
1 −β2
1

+ hκ2
4
 2α−1 −α2
−1

∥x−1∥2
2 −β2
−1(d −1)

.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
530
Proof. This follows from Lemma 193, independence of g and x, and linearity of trace and expectation
applied on squared coordinates of g, where we recognize Eg∼N(0,I)[gg⊤] = I.
Next, for a ﬁxed x, consider the random variables Rx
i :
Rx
i =



h
4
  2α1 −α2
1

x2
1 −β2
1g2
1 −2(1 −α1)β1x1g1

i = 1
hκ2
4
  2α−1 −α2
−1

x2
i −β2
−1g2
i −2(1 −α−1)β−1xigi

2 ≤i ≤d
where g ∼N(0, I) is a standard Gaussian random vector. Notice that for a given realization of g,
we have by Lemma 193 that
X
i∈[d]
Rx
i = h
4 ∥x∥A2 −h
4 ∥y∥A2 .
(10.33)
We computed the expectation of P
i∈[d] Rx
i in Corollary 51. We next give a high-probability bound
on the deviation of P
i∈[d] Rx
i from its expectation.
Lemma 194. With probability at least 1 −δ over the randomness of g ∼N(0, I),
X
i∈[d]
Rx
i −Eg∼N(0,I)

X
i∈[d]
Rx
i

≤2h|α1 −1|β1|x1|
s
log
4
δ

+ hβ2
1
s
log
4
δ

+ 2hκ2|α−1 −1|β−1 ∥x−1∥2
s
log
4
δ

+ hκ2β2
−1
s
d log
4
δ

.
Proof. In deﬁning {Rx
i }i∈[d], the terms involving {x2
i }i∈[d] are deterministic. Thus, we need to upper
bound the deviations of the remaining terms, namely
S(1)
1
:= h
2 (α1 −1)β1x1g1, S(2)
1
:= hβ2
1
4
 1 −g2
1

,
S(1)
−1 := hκ2
2 (α−1 −1)β−1
X
2≤i≤d
xigi, S(2)
−1 := hκ2β2
−1
4
X
2≤i≤d
 1 −g2
i

.
To motivate these deﬁnitions, S(1)
1
+S(2)
1
+S(1)
−1 +S(2)
−1 is the left hand side of the display in the lemma
statement. We begin with S(1)
−1. Notice that this is a one-dimensional Gaussian random variable
distributed as
N
 0, σ2
1

where σ1 := hκ2
2 |α−1 −1|β−1 ∥x−1∥2 .
Thus, applying Mill's inequality yields
Pr
h
S(1)
−1 > t
i
≤
r
2
π
σ1
t exp

−t2
2σ2
1

≤δ
4, for t = 4σ1
s
log
4
δ

.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
531
Next, to bound the term S(2)
−1, deﬁne
σ2 := hκ2β2
−1
4
√
d −1.
Standard χ2 concentration results (Fact 32) then yield
Pr
h
S(2)
−1 > t
i
≤exp

−t2
4σ2
2

≤δ
4, for t = 2σ2
s
log
4
δ

.
Similar bounds follow for S(1)
1
and S(2)
1 , whose computations we omit for brevity. Taking a union
bound over these four terms yields the desired claim.
Finally, we have a complete characterization of a bad set Ω⊂Rd where, with high probability
over the proposal distribution, the acceptance probability is extremely small.
Proposition 49. Let x ∈Rd satisfy ∥x−1∥2 ≤
q
2d
3κ and |x1| ≤5√log d, and suppose y is of the
form in (10.32) and A is as in (10.31). Also suppose that
|α−1| ≤3
5β2
−1κ, β−1 = ω
 r
log d
κd
!
, |α1| = O(|α−1|), β1 = O(β−1).
Then with probability at least 1 −d−5 over the randomness of g ∼N(0, I), we have
h
4 ∥x∥A2 −h
4 ∥y∥A2 = −Ω
 hκ2β2
−1d

.
Proof. We ﬁrst handle terms involving x−1 and g−1. Combining (10.33), Corollary 51, and Lemma 194,
we have with probability at least 1 −1
2d−5 over the randomness of g ∼N(0, I) that ∥x−1∥2
A2
−1 −
∥y−1∥2
A2
−1 (where A−1 is the Hessian of fhq on the last d −1 coordinates) is upper bounded by
hκ2
4
 2α−1 −α2
−1

∥x−1∥2
2 −β2
−1(d −1)

+ 5hκ2|α−1 −1|β−1 ∥x−1∥2
p
log d + 3hκ2β2
−1
p
d log d
≤−hκ2
4.5 β2
−1d + hκ2
4 (2α−1 −α2
−1) ∥x−1∥2
2 + 5hκ2|α−1 −1|β−1 ∥x−1∥2
p
log d.
(10.34)
Here we dropped the last term in the ﬁrst line by adjusting a constant since it is dominated for
suﬃciently large d. It remains to show that all the terms in the second line other than −hκ2
4.5 β2
−1d
are bounded by O(hκ2β2
−1d). We will perform casework on the size of α−1.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
532
Case 1: |α−1| > 3. In this case, we have for suﬃciently large d, by Young's inequality
5hκ2|α−1 −1|β−1 ∥x−1∥2
p
log d ≤1
40hκ2|α−1|β−1 ∥x−1∥2
√
d
≤1
80hκ2β2
−1d + 1
80hκ2α2
−1 ∥x−1∥2
2 .
Plugging this bound into (10.34), we have the desired
∥x−1∥2
A2
−1 −∥y−1∥2
A2
−1 ≤−hκ2
5 β2
−1d + hκ2
4
 2α−1 −0.9α2
−1

∥x−1∥2
2 ≤−hκ2
5 β2
−1d.
In the last inequality we used 2α−1 −0.9α2
−1 ≤0 for |α−1| > 3.
Case 2: |α−1| ≤3. In this case, we ﬁrst observe by our assumed bounds on ∥x−1∥2 and β−1,
5hκ2|α−1 −1|β−1 ∥x−1∥2
p
log d ≤20hκ1.5β−1
p
d log d = o
 hκ2β2
−1d

.
Thus, substituting into (10.34) and dropping the (nonpositive) term corresponding to α2
−1,
∥x−1∥2
A2
−1 −∥y−1∥2
A2
−1 ≤−hκ2
4.8 β2
−1d + hκ2
2 α−1 ∥x−1∥2
2
≤−hκ2
4.8 β2
−1d + hκα−1d
3
= −Ω
 hκ2β2
−1d

.
In the second inequality, we used the assumed bound on ∥x−1∥2
2, and in the last we used the bound
|α−1| ≤3
5β2
−1κ to reach the conclusion.
To complete the proof we need to show terms involving x1 and g1 are small.
In particular,
combining (10.33), Corollary 51, and Lemma 194 and dropping nonnegative terms, it suﬃces to
argue
h
2 α1x2
1 + 5h|α1 −1|β1|x1|
p
log d + 3hβ2
1
p
log d = o
 hκ2β2
−1d

.
This bound clearly holds for the last term hβ2
1
√log d using β1 = O(β−1). For the ﬁrst term, it
suﬃces to use our assumed bounds on |α1| and x1. Finally, the middle term 5h|α1 −1|β1|x1|√log d
is low-order compared to the term 5hκ2|α−1 −1|β−1 ∥x−1∥2
√log d which we argued about earlier,
and hence does not aﬀect any of our earlier bounds by more than a constant. The left-hand side of
the above display is an upper bound of the ﬁrst coordinate's contribution with probability at least
1 −1
2d−5, so a union bound shows the proof succeeds with probability ≥1 −d−5.
Finally, we are ready to give the main lower bound of this section.
Theorem 71. For every step size, there is a target Gaussian on Rd whose negative log-density
always has Hessian eigenvalues in [1, κ], such that the relaxation time of MALA is Ω( κ
√
d
√log d).
Proof. Let π∗be the Gaussian with log-density −fhq (10.31) throughout this proof. If h = O
 √log d
κ
√
d

,

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
533
then Corollary 50 immediately implies the result, so for the remainder of the proof suppose
h = ω
√log d
κ
√
d

.
(10.35)
We ﬁrst recall that MALA Markov chains are an instance of (10.32) with
α1 = h, α−1 = hκ, β1 = β−1 =
√
2h.
It is easy to see that these parameters satisfy the assumptions in Proposition 49, for the given range
of h. We deﬁne a "bad starting set" as follows:
Ω:=

x
 ∥x−1∥2
2 ≤2d
3κ, x2
1 ≤25 log d

.
(10.36)
For any x ∈Ω, and h satisfying (10.35), Proposition 49 is applicable, and by our deﬁnition of Ω,
any x ∈Ωhas proposals which will be accepted with probability
exp
 −Ω
 hκ2β2
−1d

= exp
 −Ω(h2κ2d)

≤
1
d10 .
The conductance of the Markov chain (10.11) is then at most
2
d5 by the witness set Ωand the failure
probability of Proposition 49, which concludes the proof by Cheeger's inequality [122], where we use
the assumption that κ = O(d4).
Finally, as it clariﬁes the required warmness to escape the bad set in the proof of Theorem 71
(and is used in our mixing time bounds in Section 10.9), we lower bound the measure of Ωaccording
to π∗. Applying Lemma 195 shows with probability at least exp(−1
12d), ∥x−1∥2
2 ≤
d
2κ, and Fact 31
shows that x2
1 ≤25 log d with probability at least 1
2; combining shows that the measure is at least
exp(−d). We required one helper technical fact, a small-ball probability bound for Gaussians.
Lemma 195. Let v ∼N(0, I) be a random Gaussian vector in n dimensions. For large enough n,
Pr
h
∥v∥2
2 ≤n
2
i
≥exp

−n
12

.
Proof. Observe that ∥v∥2
2 follows a χ2 distribution with n degrees of freedom. Thus this probability
is governed by the χ2 cumulative density function, and is
1
Γ(k)γ (k, ck)
where we deﬁne k := n
2 and c := 1
2; here Γ is the standard gamma function, and γ is the lower

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
534
incomplete gamma function. Next, we have the bound from [428]
1
Γ(k)γ (k, ck) ≥(1 −exp (−ℓck))k , ℓ:= (Γ(k + 1))−
1
k−1 .
A direct calculation yields ℓ≥2.5
k
=⇒1 −exp(−ℓck) ≥exp(−1
6) for large enough k. Recalling we
deﬁned k = n
2 yields the conclusion.
10.8
Lower bound for MALA on well-conditioned distribu-
tions
In this section, we derive a lower bound on the relaxation time of MALA for sampling from a
distribution with density proportional to exp(−f(x)), where f : Rd →R is a (non-quadratic) target
function with condition number κ. In particular, by exploiting the structure of non-cancellations
which do not occur for quadratics, we will attain a stronger lower bound.
Our ﬁrst step is to derive an upper bound on the acceptance probability for a general target
function f according to the MALA updates (10.13), analogously to Lemma 193 in the Gaussian
case.
Lemma 196. For any function f : Rd →R, we have
f(x) −f(y) + 1
4h

∥y −(x −h∇f(x))∥2
2 −∥x −(y −h∇f(y))∥2
2

= −f(y) + f(x) −1
2 ⟨x −y, ∇f(x) + ∇f(y)⟩+ h
4 ∥∇f(x)∥2
2 −h
4 ∥∇f(y)∥2
2 .
Proof. This is a direct computation which we perform here for completeness:
f(x) −f(y) + 1
4h

∥y −(x −h∇f(x))∥2
2 −∥x −(y −h∇f(y))∥2
2

= f(x) −f(y) + 1
2h ⟨y −x, h∇f(x)⟩−1
2h ⟨x −y, h∇f(y)⟩+ h
4 ∥∇f(x)∥2
2 −h
4 ∥∇f(y)∥2
2
= −f(y) + f(x) −1
2 ⟨x −y, ∇f(x) + ∇f(y)⟩+ h
4 ∥∇f(x)∥2
2 −h
4 ∥∇f(y)∥2
2 .
Next, recall the proposal distribution of the MALA updates (10.13) sets y = x−h∇f(x)+
√
2hg
where g ∼N(0, I). We further split this update into a random step and a deterministic step, by
deﬁning
xg := x +
√
2hg, where g ∼N(0, I) and y = xg −h∇f(x).
(10.37)
This will allow us to reason about the eﬀects of the stochastic and drift terms separately.
We

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
535
crucially will use the following decomposition of the equation in Lemma 196:
−f(y) + f(x) −1
2 ⟨x −y, ∇f(x) + ∇f(y)⟩+ h
4 ∥∇f(x)∥2
2 −h
4 ∥∇f(y)∥2
2
= −f(xg) + f(x) −1
2 ⟨x −xg, ∇f(x) + ∇f(xg)⟩
+f(xg) −f(y) −1
2 ⟨x −xg, ∇f(y) −∇f(xg)⟩
−1
2 ⟨xg −y, ∇f(x) + ∇f(y)⟩+ h
4 ∥∇f(x)∥2
2 −h
4 ∥∇f(y)∥2
2 .
(10.38)
We will use the following observation, which gives an alternate characterization of the second line
of (10.38), as well as a bound on the third and fourth lines for smooth functions.
Lemma 197. For twice-diﬀerentiable f : Rd →R, letting xs := x + s(xg −x) for s ∈[0, 1], we have
−f(xg) + f(x) −1
2 ⟨x −xg, ∇f(x) + ∇f(xg)⟩= −2h
Z 1
0
1
2 −s

g⊤∇2f(xs)gds.
Moreover, assuming f is κ-smooth,
f(xg) −f(y) −1
2 ⟨x −xg, ∇f(y) −∇f(xg)⟩
−1
2 ⟨xg −y, ∇f(x) + ∇f(y)⟩+ h
4 ∥∇f(x)∥2
2 + h
4 ∥∇f(y)∥2
2
≤2
 h2κ + h3κ2
∥∇f(x)∥2
2 + 3
 h1.5κ + h2.5κ2
∥g∥2 ∥∇f(x)∥2 + h2κ2 ∥g∥2
2 .
Proof. By integrating twice and using the deﬁnition xg = x +
√
2hg,
f(xg) = f(x) +
Z 1
0
⟨∇f(xs), xg −x⟩ds
= f(x) + ⟨∇f(x), xg −x⟩+
Z 1
0
Z s
0
∇2f(xt) (xg −x) dt, xg −x

ds
= f(x) + ⟨∇f(x), xg −x⟩+ 2h
Z 1
0
(1 −s) g⊤∇2f(xs)gds.
(10.39)
Similarly,
f(x) = f(xg) + ⟨∇f(xg), x −xg⟩+ 2h
Z 1
0
sg⊤∇2f(xs)gds.
(10.40)

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
536
The ﬁrst conclusion follows from combining (10.39) and (10.40). Next, assuming f is κ-smooth,
f(xg) −f(y) −1
2 ⟨x −xg, ∇f(y) −∇f(xg)⟩−1
2 ⟨xg −y, ∇f(x) + ∇f(y)⟩
= f(xg) −f(y) +
√
2h
2
⟨g, ∇f(y) −∇f(xg)⟩−⟨xg −y, ∇f(y)⟩−h
2 ⟨∇f(x), ∇f(x) −∇f(y)⟩
≤f(xg) −f(y) −⟨xg −y, ∇f(y)⟩+
√
2h
2
∥g∥2 ∥∇f(xg) −∇f(y)∥2 + h
2 ∥∇f(x)∥2 ∥∇f(x) −∇f(y)∥2
≤κ
2 ∥xg −y∥2
2 +
√
2hκ
2
∥g∥2 ∥xg −y∥2 + hκ
2 ∥∇f(x)∥2 ∥x −y∥2
≤h2κ
2
∥∇f(x)∥2
2 +
√
2
2 h1.5κ ∥g∥2 ∥∇f(x)∥2 + hκ
2 ∥∇f(x)∥2
√
2h ∥g∥2 + h ∥∇f(x)∥2

= h2κ ∥∇f(x)∥2
2 +
√
2h1.5κ ∥g∥2 ∥∇f(x)∥2 .
(10.41)
The second line used the deﬁnitions of xg and y in (10.37), and the third used Cauchy-Schwarz. The
fourth used smoothness (which implies gradient Lipschitzness), and the ﬁfth again used (10.37) and
the triangle inequality. Next, we bound the remaining terms h
4 ∥∇f(x)∥2
2 −h
4 ∥∇f(y)∥2
2:
h
4 ∥∇f(x)∥2
2 −h
4 ∥∇f(y)∥2
2 = h
4 ⟨∇f(x) + ∇f(y), ∇f(x) −∇f(y)⟩
≤hκ
4 (2 ∥∇f(x)∥2 + κ ∥x −y∥2) ∥x −y∥2
≤hκ
4

2 ∥∇f(x)∥2 + hκ ∥∇f(x)∥2 +
√
2hκ ∥g∥2
 
h ∥∇f(x)∥2 +
√
2h ∥g∥2

≤1
2
 h2κ + h3κ2
∥∇f(x)∥2
2 +
√
2
2
 h1.5κ + h2.5κ2
∥g∥2 ∥∇f(x)∥2 + h2κ2 ∥g∥2
2 .
(10.42)
Combining (10.41) and (10.42) yields the conclusion.
We will ultimately use the second bound in Lemma 197 to argue that the third and fourth lines
in (10.38) are low-order, so it remains to concentrate on the remaining term,
−f(xg) + f(x) −1
2 ⟨x −xg, ∇f(x) + ∇f(xg)⟩= −2h
Z 1
0
1
2 −s

g⊤∇2f(xs)gds.
(10.43)
Our goal is to demonstrate this term is −Ω(hκd) over an inverse-exponentially sized region, for
a particular hard distribution.
As it is coordinate-wise separable, our proof strategy will be to
construct a hard one-dimensional function, and replicate it to obtain a linear dependence on d.
We now deﬁne the speciﬁc hard function fhard : Rd →R we work with for the remainder of the
section; it is straightforward to see fhard is κ-smooth and 1-strongly convex.
fhard(x) :=
X
i∈[d]
fi(xi), where fi(c) =



1
2c2
i = 1
κ
3 c2 −κh
3 cos
c
√
h
2 ≤i ≤d
.
(10.44)

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
537
We will now show that sampling from the distribution with density proportional to exp(−fhard) is
hard. First, notice that the function fhard has condition number κ and is coordinate-wise separable.
It immediately follows from Lemma 192 that the spectral gap (deﬁned in (10.10)) of the MALA
Markov chain is governed by the step size h as follows.
Corollary 52. The spectral gap of the MALA Markov chain for sampling from the density propor-
tional to exp(−fhard), where fhard is deﬁned in (10.44), is O(h + h2).
For the remainder of the section, we focus on upper bounding (10.43) over a large region according
to the density proportional to exp(−fhard). Recall {fi}i∈[d] are the summands of fhard. For a ﬁxed
x, consider the random variables Sx
i :
Sx
i = −fi([xg]i) + fi(xi) −1
2(xi −[xg]i)(f ′
i(xi) + f ′
i([xg]i)).
It is easy to check that for a given realization of g, we have
X
i∈[d]
Sx
i = −f(xg) + f(x) −1
2 ⟨x −xg, ∇f(x) + ∇f(xg)⟩,
where the right-hand side of the above display is the left-hand side of (10.43).
We bound the
expectation of P
i∈[d] Sx
i , and its deviation from its expectation, in Lemma 198 and Lemma 199
respectively.
Lemma 198. For any ﬁxed x ∈
n
x
 −1
2π
√
h + 2πki
√
h ≤xi ≤1
2π
√
h + 2πki
√
h, ki ∈N, ∀2 ≤i ≤d
o
and h ≤1, the random variables Sx
i , 1 ≤i ≤d satisfy
Eg∼N(0,I) [Sx
i ] ≤



0
i = 1
−0.08hκ cos xi
√
h
2 ≤i ≤d
.
Proof. We remark that the condition on x simply enforces coordinatewise in 2 ≤i ≤d, cos xi
√
h > 0.
Consider some coordinate 2 ≤i ≤d: since [xg]i = xi +
√
2hgi,
Sx
i = −fi

xi +
√
2hgi

+ fi(xi) +
√
2h
2
gi

f ′
i(xi) + f ′
i

xi +
√
2hgi

= −κ
3

xi +
√
2hgi
2
+ κh
3 cos
 xi
√
h
+
√
2gi

+ κ
3 x2
i −κh
3 cos
 xi
√
h

+
√
2h
2
gi
 
4κ
3 xi + 2
√
2hκ
3
gi + κ
√
h
3
sin
 xi
√
h
+
√
2gi

+ κ
√
h
3
sin
 xi
√
h
!
= κh
3

cos
 xi
√
h
+
√
2gi

−cos
 xi
√
h

+
√
2hκ
6
gi

sin
 xi
√
h
+
√
2gi

+ sin
 xi
√
h

Here, we used that the quadratic terms in the second and third lines cancel (this also follows from

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
538
examining the proof of Lemma 193):
−κ
3

xi +
√
2hgi
2
+ κ
3 x2
i +
√
2h
2
gi
 
4κ
3 xi + 2
√
2hκ
3
gi
!
= 0.
By a direct computation, taking an expectation over gi ∼N(0, 1) yields
Egi∼N(0,1)

cos
 xi
√
h
+
√
2gi

=
cos

xi
√
h

exp (1) ,
Egi∼N(0,1)

sin
 xi
√
h
+
√
2gi

gi

=
√
2 cos

xi
√
h

exp (1)
.
Putting these pieces together,
Egi∼N(0,1) [Sx
i ] = κh
3

2
exp(1) −1

cos
 xi
√
h

≤−0.08κh cos
 xi
√
h

.
Here, we used cos xi
√
h > 0. For i = 1, Lemma 193 shows Eg1∼N(0,1) [Sx
1 ] = 0.
Lemma 199. With probability at least 1 −1
d5 over the randomness of g ∼N(0, I),
X
i∈[d]
Sx
i −Eg∼N(0,I)

X
i∈[d]
Sx
i

≤10hκ
p
d log d.
Proof. By Lemma 197, for coordinate 1 ≤i ≤d,
Sx
i = −2h
Z 1
0
1
2 −s

f ′′
i ([xs]i)dsg2
i , where
2h
Z 1
0
1
2 −s

f ′′
i ([xs]i)ds
 ≤hκ
2 .
We attained the latter bound by smoothness.
Now, each random variable Sx
i −E[Sx
i ] is sub-
exponential with parameter hκ
2 (for coordinates where the coeﬃcient is negative, note the negation
of a sub-exponential random variable is still sub-exponential). Hence, by Fact 33,
Pr

X
i∈[d]
Sx
i −Eg∼N(0,I)

X
i∈[d]
Sx
i

≥10hκ
p
d log d

≤1
d5 .
Now, we build a bad set Ωhard with lower bounded measure that starting from a point x ∈Ωhard,

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
539
with high probability, −Eg∼N(0,I)
hP
i∈[d] Sx
i
i
is negative:
Ωhard =
(
x
 |x1| ≤2, ∀2 ≤i ≤d, ∃ki ∈Z, |ki| ≤⌊
5
π
√
hκ
⌋, such that
−9
20π
√
h + 2πki
√
h ≤xi ≤9
20π
√
h + 2πki
√
h
)
.
(10.45)
In other words, Ωhard is the set of points where cos xi is large for 2 ≤i ≤d, and coordinates are
bounded. We ﬁrst lower bound the measure of Ωhard, and show ∥∇f(x)∥2 is small within Ωhard.
Our measure lower bound will not be used in this section, but will become relevant in Section 10.9.
Lemma 200. Let h ≤
1
10000π2κ.
Let π∗have log-density −fhard (10.44).
Then, π∗(Ωhard) ≥
exp(−d). Moreover, for all x ∈Ωhard, ∥∇f(x)∥2 ≤10
√
κd.
Proof. We ﬁrst consider a superset of Ωhard. We deﬁne the set, for K := ⌊
5
π
√
hκ⌋,
Ω′ =

x
 |x1| ≤2, ∀2 ≤i ≤d, −9
20π
√
h −2πK
√
h ≤xi ≤9
20π
√
h + 2πK
√
h

.
It is easy to verify that Ω′ ⊇Ωhard. We ﬁrst show π∗(Ω′) is lower bounded by 1.1−d. Since fhard is
separable, the coordinates are independent, so it suﬃces to show each one-dimensional measure is
lower bounded by
1
1.1. This is a standard computation of Gaussian measure for the ﬁrst coordinate,
which we omit. For 2 ≤i ≤d, since the marginal distribution is κ
3 -strongly logconcave, it is sub-
Gaussian with parameter
3
κ (see Lemma 1, [210]). It follows from a standard sub-Gaussian tail
bound that the measure of the set |xi| ≤
9
√κ is at least
1
1.1. For our choice of K, by assumption on
h, 2π
√
hK ≥
10
√κ −2π
√
h ≥
9
√κ. Combining across coordinates gives π∗(Ω′) ≥1.1−d.
Next, we lower bound π∗(Ωhard)
π∗(Ω′) . We divide the support of the set Ωhard and Ω′ into small disjoint
regions and bound π∗(Ωhard)
π∗(Ω′)
for each small region and each coordinate separately. For 2 ≤i ≤d,
k ∈
h
−⌊
5
π
√
hκ⌋−1, ⌊
5
π
√
hκ⌋
i
, k ∈Z, let
Ω′(i,k) =

2πk
√
h, 2π(k + 1)
√
h
i
,
and
Ω(i,k)
hard =

2πk
√
h, 2πk
√
h + 9
20π
√
h

∪

2π(k + 1)
√
h −9
20π
√
h, 2π(k + 1)
√
h

.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
540
Then, letting π∗
i be the marginal of π∗on coordinate i, we have
π∗
i

Ω(i,k)
hard

π∗
i
 Ω′(i,k) =
R 2πk
√
h+ 9
20 π
√
h
2πk
√
h
exp

−κ
3 x2
i + κh
3 cos xi
√
h

dxi +
R 2π(k+1)
√
h
2π(k+1)
√
h−9
20 π
√
h exp

−κ
3 x2
i + κh
3 cos xi
√
h

dxi
R 2π(k+1)
√
h
2πk
√
h
exp

−κ
3 x2
i + κh
3 cos xi
√
h

dxi
≥
R 2πk
√
h+ 9
20 π
√
h
2πk
√
h
exp
 −κ
3 x2
i

dxi +
R 2π(k+1)
√
h
2π(k+1)
√
h−9
20 π
√
h exp
 −κ
3 x2
i

dxi
R 2π(k+1)
√
h
2πk
√
h
exp
 −κ
3 x2
i

dxi exp
  κh
3

≥exp

−κh
3

·
9
10π
√
h
2π
√
h
·
exp

−κ
3

2π(k + 1)
√
h
2
exp

−κ
3

2πk
√
h
2
≥0.42.
The second step used cos xi
√
h ≥0 for xi ∈Ω(i,k)
hard. The fourth step used the assumption κh ≤
1
10000π2 .
Finally, letting Ω′(i) and Ω(i)
hard be the projections of Ω′ and Ωhard on the ith coordinate. For any
xi ∈Ω′
i with x /∈Ω′(i,k), and for all integers k ∈[−K −1, K], xi ∈Ω(i)
hard, so
π∗
Ω(i)
hard

π∗(Ω′(i)) ≥0.42. Since
the coordinates are independent under π∗, π∗(Ωhard)
π∗(Ω′)
≥0.42d. Combining our lower bounds,
π∗(Ωhard) = π∗(Ω′)π∗(Ωhard)
π∗(Ω′)
≥
 1.1
0.42
−d
≥exp (−d) .
Finally, we bound ∥∇fhard(x)∥2 for x ∈Ω′, from the deﬁnition of fhard (10.44),
∥∇fhard(x)∥2 =
v
u
u
tf ′
1(x)2 +
d
X
i=2
f ′
i(x)2 =
v
u
u
tx2
1 +
d
X
i=2
 
2κ
3 xi + κ
√
h
3
sin xi
√
h
!2
.
Then directly plugging in the deﬁnition of Ωhard and using | sin c| ≤|c| for all c,
∥∇fhard(x)∥2 ≤
q
1.52 + (d −1)
 9√κ
2 ≤10
√
κd.
Finally, we combine the bounds we derived to show the acceptance probability is small within
Ωhard.
Lemma 201. Let h = o

1
κ log d

. For any x ∈Ωhard, let y = x−h∇fhard(x)+
√
2hg for g ∼N(0, I).
With probability at least 1 −2
d5 , we have
fhard(x) −fhard(y) + 1
4h

∥y −(x −h∇fhard(x))∥2
2 −∥x −(y −h∇fhard(y))∥2
2

= −Ω(hκd) .
Proof. By combining Lemma 196 and the decomposition (10.38), the conclusion is equivalent to

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
541
showing that the following quantity is −Ω(hκd):
−fhard(xg) + fhard(x) −1
2 ⟨x −xg, ∇fhard(x) + ∇fhard(xg)⟩
+fhard(xg) −fhard(y) −1
2 ⟨x −xg, ∇fhard(y) −∇fhard(xg)⟩
−1
2 ⟨xg −y, ∇fhard(x) + ∇fhard(y)⟩+ h
4 ∥∇fhard(x)∥2
2 −h
4 ∥∇fhard(y)∥2
2 .
For x ∈Ωhard, every xi for 2 ≤i ≤d has cos xi
√
h bounded away from 0 by a constant and hence
combining Lemmas 198 and 199 implies the ﬁrst line is −Ω(hκd) with probability at least
1
d5 .
Regarding the second and third lines, Lemma 197 shows that it suﬃces to bound (over the set
Ωhard)
(h2κ + h3κ2) ∥∇fhard(x)∥2
2 +
 h1.5κ + h2.5κ2
∥g∥2 ∥∇fhard(x)∥2 + h2κ2 ∥g∥2
2 = o(hκd).
Fact 32 implies ∥g∥2 ≤
√
2d with probability at least 1 −1
d5 . Combining this bound, the bound on
∥∇fhard(x)∥2 from Lemma 200, and the upper bound on h yields the conclusion.
We conclude by giving the main result of this section.
Proposition 50. For h = o(
1
κ log d), there is a target density on Rd whose negative log-density
always has Hessian eigenvalues in [1, κ], such that the relaxation time of MALA is Ω( κd
log d).
Proof. The proof is identical to that of Theorem 71, where we use Lemma 201 in place of Proposi-
tion 49.
Theorem 72. For every step size, there is a target density on Rd whose negative log-density always
has Hessian eigenvalues in [1, κ], such that the relaxation time of MALA is Ω( κd
log d).
Proof. This is immediate from combining Theorem 71 (with the hard function fhq in the range
h = Ω(
1
κ log d)) and Proposition 50 (with the hard function fhard in the range h = o(
1
κ log d)).
10.9
Mixing time lower bound for MALA
In this section, we derive a mixing time lower bound for MALA. Concretely, we show that for any
step size h, there is a hard distribution π∗∝exp(−f) such that ∇2f always has eigenvalues in [1, κ],
yet there is a exp(d)-warm start π0 such that the chain cannot mix in o(
κd
log2 d) iterations, starting
from π0. We begin by giving such a result for h = O( log d
κd ) in Section 10.9.1, and combine it with
our developments in Sections 10.7 and 10.8 to prove our main mixing time result.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
542
10.9.1
Mixing time lower bound for small h
Throughout this section, let h = O

log d
κd

, and let π∗= N(0, I) be the standard d-dimensional
multivariate Gaussian. We will let π0 be the marginal distribution of π∗on the set
Ω:=

x | ∥x∥2
2 ≤1
2d

.
Recall from Lemma 195 that π0 is a exp(d)-warm start. Our main proof strategy will be to show
that for such a small value of h, after T = O(
κd
log2 d) iterations, with constant probability both of the
following events happen: no rejections occur throughout the Markov chain, and ∥xt∥2
2 ≤
9
10d holds
for all t ∈[T]. Combining these two facts will demonstrate our total variation lower bound.
Lemma 202. Let {xt}0≤t<T be the iterates of the MALA Markov chain with step size h = O

log d
κd

,
for T = o(
κd
log2 d) and x0 ∼π0. With probability at least
99
100, both of the following events occur:
1. Throughout the Markov chain, ∥xt∥2 ≤0.9
√
d.
2. Throughout the Markov chain, the Metropolis ﬁlter never rejected.
Proof. We inductively bound the failure probability of the above events in every iteration by 0.01
T ,
which will yield the claim via a union bound. Take some iteration t + 1, and note that by triangle
inequality, and assuming all prior iterations did not reject,
∥xt+1∥2 ≤∥x0∥2+h
t
X
s=0
∥xs∥2+
√
2h

t
X
s=0
gs

2
≤∥x0∥2+0.9hT
√
d+
√
2h ∥Gt∥2 ≤0.8
√
d+
√
2h ∥Gt∥2 .
Here, we applied the inductive hypothesis on all ∥xs∥2, the initial bound ∥x0∥2 ≤
q
1
2d, and that
hT = o(1) by assumption. We also deﬁned Gt = Pt
s=0 gs, where gs is the random Gaussian used by
MALA in iteration s; note that by independence, Gt ∼N(0, t + 1). By Fact 32, with probability at
least
1
200T , ∥Gt∥2 ≤2
√
Td, and hence 0.8
√
d +
√
2h ∥Gt∥2 ≤0.9
√
d, as desired.
Next, we prove that with probability ≥1−
1
200T , step t does not reject. This concludes the proof
by union bounding over both events in iteration t, and then union bounding over all iterations. By
the calculation in Lemma 193, the accept probability is
min

1, exp
h
4
 2h −h2
∥xt∥2
2 −2h ∥g∥2
2 −2
√
2h (1 −h) ⟨xt, g⟩

.
We lower bound the argument of the exponential as follows. With probability at least 1 −d−5 ≥
1 −
1
400T , Facts 31 and 32 imply both of the events ∥g∥2
2 ≤2d and ⟨xt, g⟩≤10√log d ∥x∥2 occur.
Conditional on these bounds, we compute (using 2h ≥h2 and the assumption ∥xt∥2 ≤0.9
√
d)
 2h −h2
∥xt∥2
2 −2h ∥g∥2
2 −2
√
2h (1 −h) ⟨xt, g⟩≥−4hd −40
p
hd log d ≥−44 log d.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
543
Hence, the acceptance probability is at least
exp (−11h log d) ≥1 −
1
400T ,
by our choice of T with Th log d = o(1), concluding the proof.
Proposition 51. The MALA Markov chain with step size h = O

log d
κd

requires Ω(
κd
log2 d) iterations
to reach total variation distance 1
e to π∗, starting from π0.
Proof. Let ˜π be the distribution of the MALA Markov chain after T = o(
κd
log2 d) steps without
applying a Metropolis ﬁlter in any step, and let ˆπ be the distribution after applying the actual
MALA chain (including rejections). To show ∥ˆπ −π∗∥TV ≥1
e, it suﬃces to show the bounds
∥˜π −π∗∥TV ≥2
5, ∥˜π −ˆπ∥TV ≤0.01,
and then we apply the triangle inequality. By the coupling characterization of total variation, the
second bound follows immediately from the second claim in Lemma 202, wherein we couple the two
distributions whenever a rejection does not occur. To show the ﬁrst bound, the measure of
Ωlarge :=
n
x | ∥x∥2
2 ≥0.81d
o
according to π∗is at least 0.99 by Fact 32, and according to ˜π it can be at most 0.01 by the ﬁrst
conclusion of Lemma 202. This yields the bound via the deﬁnition of total variation.
10.9.2
Proof of Theorem 73
Finally, we put together the techniques of Sections 10.7, 10.8, and 10.9.1 to prove Theorem 73.
Theorem 73. For every step size, there is a target density on Rd whose negative log-density al-
ways has Hessian eigenvalues in [1, κ], such that MALA initialized at an exp(d)-warm start requires
Ω(
κd
log2 d) iterations to reach e−1 total variation distance to the stationary distribution.
Proof. We consider three ranges of h. First, if h = Ω

1
κ log d

, we use the hard function fhq and
the hard set in (10.36), which has measure at least exp(−d) according to the stationary distribution
by Lemma 195. Then, applying Proposition 49 demonstrates that the chance the Markov chain can
move over d5 iterations is o( 1
d), and hence it does not reach total variation 1
e in this time. Next,
if h = o

1
κ log d

∩ω

log d
κd

, we use the hard function fhard and the hard set in (10.45), which has
measure at least exp(−d) by Lemma 200. Applying Lemma 201 again implies the chain does not
mix in d5 iterations. Finally, if h = O

log d
κd

, applying Proposition 51 yields the claim.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
544
10.10
Lower bounds for HMC
In this section, we derive a lower bound on the spectral gap of HMC. We ﬁrst analyze some general
structural properties of HMC in Section 10.10.1, as a prelude to later sections. We then provide a
lower bound for HMC on quadratics in Section 10.10.2, with any number of leapfrog steps K.
10.10.1
Structure of HMC: a detour to Chebyshev polynomials
We begin our development with a bound on the acceptance probability for general HMC Markov
chains. Recall from (10.14) that this probability is (for H(x, v) := f(x) + 1
2 ∥v∥2
2)
min

1, exp (−H(xK, vK))
exp (−H(x0, v0))

.
(10.14)
We ﬁrst state a helper calculation straightforwardly derived from the exposition in Section 10.2.4.
Fact 34. One step of the HMC Markov chain starting from x0 generates iterates {(vk−1
2 , xk, vk)}0≤k≤K
deﬁned recursively by the closed-form equations:
vk−1
2 = v0 −η
2∇f(x0) −η
X
j∈[k−1]
∇f(xj),
vk = v0 −η
2∇f(x0) −η
X
j∈[k−1]
∇f(xj) −η
2∇f(xk),
xk = x0 + ηkv0 −η2k
2 ∇f(x0) −η2
X
j∈[k−1]
(k −j)∇f(xj).
When expanding the acceptance probability (10.14) using the equations in Fact 34, many terms
conveniently cancel, which we capture in Lemma 203. This phenomenon underlies the improved
performance of HMC on densities with highly-Lipschitz Hessians [128].
Lemma 203. For the iterates given by Fact 34,
H (x0, v0) −H (xK, vK) =
X
0≤k≤K−1

f(xk) −f(xk+1) + 1
2 ⟨∇f(xk) + ∇f(xk+1), xk+1 −xk⟩

+ η2
8 ∥∇f(x0)∥2
2 −η2
8 ∥∇f(xK)∥2
2 .

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
545
Proof. Recall H(x0, v0) −H(xK, vK) = f(x0) −f(xK) + 1
2 ∥v0∥2
2 −1
2 ∥vK∥2
2. We begin by expanding
1
2 ∥v0∥2
2 −1
2 ∥vK∥2
2 = 1
2 ∥v0∥2
2 −1
2

v0 −η
2∇f(x0) −η
X
j∈[K−1]
∇f(xj) −η
2∇f(xK)

2
2
= η
*
v0, 1
2∇f(x0) +
X
j∈[K−1]
∇f(xj) + 1
2∇f(xK)
+
−η2
2

1
2∇f(x0) +
X
j∈[K−1]
∇f(xj) + 1
2∇f(xK)

2
2
= η
2
X
0≤k≤K−1
⟨v0, ∇f(xk) + ∇f(xk+1)⟩
−η2
2
X
0≤k≤K−1
*
∇f(xk) + ∇f(xk+1), 1
2∇f(x0) +
X
j∈[k]
∇f(xj)
+
+ η2
8 ⟨∇f(x0) −∇f(xK), ∇f(x0) + ∇f(xK)⟩.
Here the ﬁrst equality used Fact 34. Moreover, for each 0 ≤k ≤K −1, by Fact 34
1
2 ⟨∇f(xk) + ∇f(xk+1), xk+1 −xk⟩= η
2 ⟨∇f(xk) + ∇f(xk+1), v0⟩
−η2
2
*
∇f(xk) + ∇f(xk+1), 1
2∇f(x0) +
X
j∈[k]
∇f(xj)
+
.
Combining yields the result.
We state a simple corollary of Lemma 203 in the case of quadratics.
Corollary 53. For f(x) = 1
2x⊤Ax, the iterates given by Fact 34 satisfy
H(x0, v0) −H(xK, vK) = η2
8 ∥∇f(x0)∥2
2 −η2
8 ∥∇f(xK)∥2
2 .
Proof. It suﬃces to observe that for any two points x, y ∈Rd,
f(x) −f(y) + 1
2 ⟨∇f(x) + ∇f(y), y −x⟩= 1
2x⊤Ax −1
2y⊤Ay + 1
2 ⟨A(x + y), y −x⟩= 0.
Finally, it will be convenient to have a more explicit form of iterates in the case of quadratics,
which follows directly from examining the recursion in Fact 34.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
546
Lemma 204. For f(x) = 1
2x⊤Ax, the iterates {xk}0≤k≤K given by Fact 34 satisfy
xk =

X
0≤j≤k
Dj,k(η2A)j

x0 +

η
X
0≤j≤k−1
Ej,k(η2A)j

v0,
where Dj,k := (−1)j ·
k
k + j ·
k + j
2j

, Ej,k := (−1)j ·
 k + j
2j + 1

.
(10.46)
Proof. This formula can be veriﬁed to match the recursions of Fact 34 by checking the base cases
D0,k = 1, D1,k = −k2
2 , E0,k = k, and (where Dj,k := 0 for j > k and Ej,k := 0 for j ≥k)
Dj,k = −
X
i∈[k−1]
(k −i)Dj−1,i, Ej,k = −
X
i∈[k−1]
(k −i)Ej−1,i.
In particular, by using the third displayed line of Fact 34, the coeﬃcient of (η2A)jx0 in xk for
j ≥2 is the negated sum of the coeﬃcients of (η2A)j−1 in all (k −i)xi. Similarly, the coeﬃcient of
η(η2A)jv0 in xk for j ≥1 is the negated sum of the coeﬃcients of η(η2A)j−1 in all (k −i)xi. The
displayed coeﬃcient identities follow from the binomial coeﬃcient identities
k
k + j
k + j
2j

=
X
j−1≤i≤k−1
(k −i)i
i + j −1
i + j −1
2j −2

,
 k + j
2j + 1

=
X
j≤i≤k−1
(k −i)
i + j −1
2j −1

.
Lemma 204 motivates the deﬁnition of the polynomials
pk(z) :=
X
0≤j≤k
Dj,kzj, qk(z) :=
X
0≤j≤k−1
Ej,kzj.
(10.47)
In this way, at least in the case when A = diag (λ) for a vector of eigenvalues λ ∈Rd, we can
concisely express the coordinates of iterates in (10.46) by
[xk]i = pk(η2λi)[x0]i + ηqk(η2λi)[v0]i.
(10.48)
Interestingly, the polynomial pk turns out to have a close relationship with the kth Chebyshev
polynomial (of the ﬁrst kind), which we denote by Tk. Similarly, the polynomial qk is closely related
to the (k −1)th Chebyshev polynomial of the second kind, denoted Uk−1. The relationship between
the Chebyshev polynomials and the phenomenon of acceleration for optimizing quadratics via ﬁrst-
order methods has been known for some time (see e.g. [266, 49] for discussions), and we ﬁnd it
interesting to further explore this relationship. Concretely, the following identities hold.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
547
Lemma 205. Following deﬁnitions (10.46), (10.47),
pk(z) = Tk

1 −z
2

, qk(z) = Uk−1

1 −z
2

.
Proof. It is easy to check p0(z) = 1 and p1(z) = 1 −z
2, so the former conclusion would follow from
pk+1(z) = (2 −z)pk(z) −pk−1(z) ⇐⇒Dj,k+1 = 2Dj,k −Dj−1,k −Dj,k−1,
following well-known recursions deﬁning the Chebyshev polynomials of the ﬁrst kind. This identity
can be veriﬁed by direct expansion. Moreover, for the latter conclusion, recalling the deﬁnition of
Morgan-Voyce polynomials of the ﬁrst kind Bk(z), we can directly match qk(z) = Bk−1(−z). The
conclusion follows from Section 4 of [30], which shows Bk−1(−z) = Uk−1(1 −z
2) as desired (note
that in the work [30], the indexing of Chebyshev polynomials is oﬀby one from ours).
Now for z = η2λi, we have from (10.48) and Lemma 205 that [xk]i = ±[x0]i precisely when
pk(z) = Tk

1 −z
2

= ±1, qk(z) = Uk−1

1 −z
2

= 0.
Hence, this occurs whenever 1 −z
2 is both an extremal point of Tk in the range [−1, 1] and a root of
Uk−1. Both of these occur exactly at the points cos( j
kπ), for 0 ≤j ≤k.
Proposition 52. For κ ≥π2 and K ≥2, no K-step HMC Markov chain with step size 1 ≥η2 ≥
π2
κK2
can mix in ﬁnite time for all densities on Rd whose negative log-density's Hessian has eigenvalues
between 1 and κ for all points x ∈Rd, initialized at a constant-warm start.
Proof. Fix a value of 1 ≥η ≥
q
π2
κK2 . We claim there exists a 1 ≤j ≤K −1 such that for
λ := 2
 1 −cos
  jπ
K

η2
, 1 ≤λ ≤κ.
Since λ is a monotone function of η, it suﬃces to check the endpoints of the interval [ π2
κK2 , 1]. For
η2 = 1, we choose j = K −1, which using 2x2
π2 ≤1 −cos(x) ≤x2
2 for all −π ≤x ≤π, yields
1 ≤4(K −1)2
K2
≤λ ≤(K −1)2π2
K2
≤π2 ≤κ.
Similarly, for η2 =
π2
κK2 , we choose j = 1, which yields
1 ≤
4
η2K2 ≤λ ≤
π2
η2K2 ≤κ.
Now, consider the quadratic f(x) = 1
2x⊤Ax where A ∈Rd×d is a diagonal matrix, A11 = 1, Aii = κ
for all 3 ≤i ≤d, and A22 = λ :=
2(1−cos( jπ
K ))
η2
for the choice of j which makes 1 ≤λ ≤κ. For

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
548
any symmetric starting set capturing a constant amount of measure along the second coordinate, by
Lemma 204 and the following exposition, xK = ±x0 along the second coordinate regardless of the
random choice of velocity and thus the chain cannot leave the starting set.
10.10.2
HMC lower bound for all K
We now give our HMC lower bound, via improving Proposition 52 by a dimension dependence. We
begin by giving a stronger upper bound on η in the range η2 ≤
1
κK2 . Noting that there is a constant-
sized gap between this range and the bound in Proposition 52, we next rule out this gap. Finally,
we handle the case of extremely large η2 ≥1. We put these pieces together to prove Theorem 74.
Upper bounding η = O(K−1κ−1
2 ) under a constant gap
For this section, we let A be the d × d diagonal matrix which induces the hard quadratic function
fhq, deﬁned in (10.31) and reproduced here for convenience:
fhq(x) :=
X
i∈[d]
fi(xi), where fi(c) =



1
2c2
i = 1
κ
2 c2
2 ≤i ≤d
.
We also let h := η2
2 , x := x0, g := v0, and y := xK throughout for analogy to Section 10.7, so that
we can apply Proposition 49. Next, note that by the closed-form expression given by Lemma 204,
we can write the iterates of the HMC chain in the form (10.32), reproduced here:
y =

y1
y−1

, where y1 = (1 −α1)x1 + β1g1
and y−1 = (1 −α−1)x−1 + β−1g−1, for g ∼N(0, I).
Concretely, we have by Lemma 204 that
α1 = −
X
1≤j≤K
(−1)j (2h)j

K
K + j
 K + j
2j

,
α−1 = −
X
1≤j≤K
(−1)j (2hκ)j

K
K + j
 K + j
2j

,
β1 =
√
2h
X
0≤j≤K−1
(−1)j(2h)j
K + j
2j + 1

,
β−1 =
√
2h
X
0≤j≤K−1
(−1)j(2hκ)j
K + j
2j + 1

.
(10.49)
By a straightforward computation, the parameters in (10.49) satisfy the conditions of Proposition 49.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
549
Lemma 206. Supposing η2 ≤
1
κK2 , α1, α−1, β1, β−1 deﬁned in (10.49) satisfy
|α−1| ≤3
5β2
−1κ, |α1| = O(|α−1|), β1 = O(β−1).
Proof. The proof follows since under η2 ≤
1
10κK2 , all of the parameters in (10.49) are dominated by
their ﬁrst summand. We will argue this for α−1 and β−1; the corresponding conclusions for α1 and
β1 follow analogously since κ ≥1. Deﬁne the summands of α−1 and β−1 by
cj := (−1)j+1(2hκ)j

K
K + j
 K + j
2j

, 1 ≤j ≤K,
dj :=
√
2h(−1)j(2hκ)j
K + j
2j + 1

, 0 ≤j ≤K −1.
Then, we compute that for all 1 ≤j ≤K −1, assuming 2hκK2 ≤1,
0 ≥cj+1
cj
= (−2hκ) (K + j)(K −j)
(2j + 2)(2j + 1) ≥−2hκK2
12
≥−0.1.
(10.50)
Similarly, for all 0 ≤j ≤K −2,
0 ≥dj+1
dj
= (−2hκ) (K + j + 1)(K −j −1)
(2j + 3)(2j + 2)
≥−2hκK2
6
≥−0.2.
(10.51)
By repeating these calculations for α1 and β1, we see that all parameters are given by rapidly
decaying geometric sequences, and thus the conclusion follows by examination from
α1 ∈

0.8hK2, hK2
, α−1 ∈

0.8hκK2, hκK2
,
β1 ∈
h
0.8
√
2hK,
√
2hK
i
, β−1 ∈
h
0.8
√
2hK,
√
2hK
i
.
We obtain the following corollary by combining Lemma 206, Corollary 53, and Proposition 49.
Corollary 54. Let x ∈Rd satisfy ∥x−1∥2 ≤
q
2d
3κ and |x1| ≤5√log d, let (xK, vK) be the result of
the K-step HMC Markov chain with step size η =
√
2h with η2 ≤
1
κK2 from x0 = x, and let A be as
in (10.31). Then with probability at least 1 −d−5 over the randomness of v0 ∼N(0, I), we have
H(x0, v0) −H(xK, vK) = −Ω
 h2κ2K2d

.
Proof. It suﬃces to use the bounds on β−1 = Θ(
√
hK) shown in the proof of Lemma 206 and the
conclusions of Corollary 53 and Proposition 49.

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
550
Removing the constant gap
We show how to improve the bound in Corollary 54 to only require η2 ≤
π2
κK2 , which removes the
constant gap between the requirement of Corollary 54 and the bound in Proposition 52. First, let
Ac be the D × d diagonal matrix which induces the following hard quadratic function fhqc:
fhqc(x) :=
X
i∈[d]
fi(xi), where fi(c) =









1
2c2
i = 1
κ
2π2 c2
2 ≤i ≤d −1
κ
2 c2
i = d
.
(10.52)
In other words, along the ﬁrst d −1 coordinates, fhqc is the same as a d −1-dimensional variant of
fhq with condition number
κ
π2 . We deﬁne a coordinate partition of x and g into x1, x−1d, xd, and
g1, g−1d, gd, and we deﬁne α1, α−1d, αd, β1, β−1d, βd in analogy with (10.32).
We ﬁrst note that because of separability of fhqc, and since the assumption of Corollary 54 holds
on the ﬁrst d −1 coordinates for η2 ≤
π2
κK2 , we can immediately obtain a bound on the change in
the Hamiltonian along these coordinates.
Corollary 55. Let x ∈Rd satisfy ∥x−1∥2 ≤
q
2π2d
3κ
and |x1| ≤5√log d, let (xK, vK) be the result of
the K-step HMC Markov chain with step size η =
√
2h where η2 ≤
π2
κK2 from x0 = x, and let Ac be
as in (10.52). Then with probability at least 1 −2d−5 over the randomness of v0 ∼N(0, I), we have
H

[x0][d−1] , [v0][d−1]

−H

[xK][d−1] , [vK][d−1]

= −Ω
 h2κ2K2d

.
We now move to bounding the contribution of the last coordinate.
Lemma 207. Let (y, vK) be the result of the K-step HMC Markov chain with step size η =
√
2h
where η2 ≤
π2
κK2 , and write yd = (1 −αd)xd + βdgd, for
αd = −
X
1≤j≤K
(−1)j(2hκ)j

K
K + j
 K + j
2j

, βd =
√
2h
X
0≤j≤K−1
(−1)j(2hκ)j
K + j
2j + 1

.
Then, we have |αd| = O(hκK2), |βd| = O(
√
hK).
Proof. After the index j is a suﬃciently large constant, the geometric argument sequence of Lemma 206
applies (since the denominators of the ratios (10.50) and (10.51) grow with the index j); before then,
each coeﬃcient is within a constant factor of the ﬁrst in absolute value. Thus, the coeﬃcients can
be at most a constant factor larger than the ﬁrst in absolute value.
Lemma 208. Let |[x0]d| ≤log d
√κ , |[v0]d| ≤log d, and let (xK, vK) be the result of the K-step HMC
Markov chain with step size η =
√
2h where η2 ≤
π2
κK2 . Then with probability at least 1 −d−5 over

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
551
the randomness of v0 ∼N(0, I), we have
H ([x0]d , [v0]d) −H ([xK]d , [vK]d) = o
 h2κ2K2d

.
Proof. We can assume abs [v0]d = abs gd ≤log d, which passes the high probability bound. By
Corollary 53 and Lemma 193, we wish to bound
hκ2
4
  2αd −α2
d

x2
d −β2
dg2
d −2(1 −αd)βdxdgd

= o
 h2κ2K2d

.
Dropping all clearly negative terms, and since |αd| = O(1) by Lemma 207, it is enough to show
abs hκ2αdx2
d = o
 h2κ2K2d

, abs hκ2βdxdgd = o
 h2κ2K2d

.
The ﬁrst bound is immediate from assumptions. The second follows from assumptions as well since
√
hκK2 is at most a constant, so abs hκ2βdxdgd = O(h1.5κ1.5K log2 d) = O(h2κ2K2 log2 d).
By combining Lemma 208 and Corollary 55, we obtain the following strengthening of Corol-
lary 54.
Corollary 56. Let x ∈Rd satisfy ∥x−1d∥2 ≤
q
2d
3κ, |x1| ≤5√log d, and |xd| ≤log d
√κ , let (xK, vK) be
the result of the K-step HMC Markov chain with step size η =
√
2h with η2 ≤
π2
κK2 from x0 = x, and
let Ac be as in (10.52). Then with probability at least 1 −d−5 over the randomness of v0 ∼N(0, I),
we have
H(x0, v0) −H(xK, vK) = −Ω
 h2κ2K2d

.
Ruling out η ≥1
Finally, we give a short argument ruling out the case η ≥1 not covered by Proposition 52. In this
section, let π∗= N(0, κ−1I), with negative log-density f(x) = κ
2 ∥x∥2
2. For η ≥1 and κ ≥10, (10.48)
and straightforward lower bounds on Chebyshev polynomials outside the range [−1, 1] demonstrate
the proposal distribution is of the form (from starting point x0 ∈Rd)
xK ←αx0 + βv0, v0 ∼N(0, 1), |α| ≥10, |β| ≥1.
(10.53)
Lemma 209. Letting (xK, vK) be the result of K-step HMC from any x0, and f(x) = κ
2 ∥x∥2
2, for
η ≥1, with probability at least 1 −d−5 over the randomness of v0 ∼N(0, I), we have
H(x0, v0) −H(xK, vK) = −Ω(d).

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
552
Proof. Following notation (10.53) and applying Corollary 53, it suﬃces to show
∥x0∥2
2 −∥αx0 + βv0∥2
2 = −Ω(d).
Expanding, it suﬃces to upper bound
 1 −α2
∥x0∥2
2 −2αβ ⟨x0, v0⟩−β2 ∥v0∥2
2 .
With probability at least 1 −d−5, Fact 32 shows ∥v0∥2
2 ≥1
2d and ⟨x0, v0⟩≥−4√log d ∥x0∥2. Hence,
 1 −α2
∥x0∥2
2 −2αβ ⟨x0, v0⟩−β2 ∥v0∥2
2 ≤−0.99α2 ∥x0∥2
2 + 8αβ
p
log d ∥x0∥2 −β2
2 d
≤20β2 log d −β2
2 d = −Ω(d).
Here, we used that α2 ≥100 and took d larger than a suﬃciently large constant.
Proof of Theorem 74
A consequence of Corollary 54 is that if the step size h = ω(
√log d
κK
√
d), initializing the chain from any
x0 in the set Ωdeﬁned in (10.36) leads to a polynomially bad mixing time. We further relate the
step size to the spectral gap of the HMC Markov chain in the following.
Lemma 210. The spectral gap of the K-step HMC Markov chain for sampling from the density
proportional to exp(−fhq), where fhq is deﬁned in (10.31), is O(hK2 + h2K4).
Proof. We follow the proof of Lemma 192; again let g(x) = x1, and π∗be the stationary distribution.
For our function f, it is clear again that Varπ∗[g] = Θ(1). Thus it suﬃces to upper bound E(g, g):
letting Px(y) be the proposal distribution of K-step HMC, and α1, β1 be as in (10.49),
E(g, g) ≤1
2
ZZ
(x1 −y1)2Px(y)dπ∗(x)dy
≤Ex∼π∗
α2
1x2
1

+ Eξ∼N(0,1)

β2
1ξ2
= α2
1 + β2
1 = O
 hK2 + h2K4
.
Finally, by combining Lemma 210 and Corollary 56, we arrive at the main result of this section.
Theorem 74. For every step size and count, there is a target Gaussian on Rd whose negative log-
density always has Hessian eigenvalues in [1, κ], such that the relaxation time of HMC is Ω(
κ
√
d
K√log d).
Proof. For 1 ≥η2 ≥
π2
κK2 it suﬃces to apply Proposition 52. For η2 ≥1, we apply Lemma 209.
Otherwise, in the relevant range of h = 2η2 , the dominant term in Lemma 210 is O(hK2). Applying

CHAPTER 10. TOWARDS AN ORACLE COMPLEXITY OF SAMPLING
553
Corollary 56 with the hard quadratic function fhqc, the remainder of the proof follows analogously
to that of Theorem 71.
We remark that as in Theorem 71, it is straightforward to see that the measure of the bad region
∥x−1d∥2 ≤
q
2d
3κ, |x1| ≤5√log d, and |xd| ≤log d
√κ used in the proof is at least exp(−d).
10.11
Conclusion
In this work, we presented relaxation time lower bounds for the MALA and HMC Markov chains
at every step size and scale, as well as a mixing time bound for MALA from an exponentially warm
start. We highlight in this section a number of unexplored directions left open by our work, beyond
direct strengthenings of our results, which we ﬁnd interesting and defer to a future exploration.
Variable or random step sizes. Our lower bounds were for MALA and HMC Markov chains with
a ﬁxed step size. For variable step sizes which take e.g. values in a bounded multiplicative range, we
believe our arguments can be modiﬁed to give relaxation time lower bounds for the resulting Markov
chains. However, the arguments of Section 10.10 (our HMC lower bound) are particularly brittle to
large multiplicative ranges of candidate step sizes, because they rely on the locations of Chebyshev
polynomial zeroes, which only occur in a bounded range. From an algorithm design perspective,
this suggests that adaptively or randomly choosing step size ranges may be eﬀective in improving
the performance of HMC. Such a result would also give theoretical justiﬁcation to the No-U-Turn
sampler of [273], a common HMC alternative in practice. We state as an explicit open problem: can
one obtain improved upper bounds, such as a √κ dependence or a dimension-independent rate, for
example by using variations of these strategies (variable step sizes)?
Necessity of κ lower bound. All of our witness sets throughout the paper are exp(−d) sized. It was
observed in [210] that it is possible to construct a starting distribution with warmness arbitrarily
close to √κd; the marginal restriction of our witness set falls under this warmness bound for all
κ ≥e2 ≈8. However, recently [346] proposed a proximal point reduction approach to sampling,
which (for mixing bounds scaling at least linearly in κ) shows that it suﬃces to sample a small
number of regularized distributions, whose condition numbers are arbitrarily close to 1.
By adjusting constants, we can modify the proof of the Gaussian lower bounds (Theorems 71
and 74) to have witness sets with measure cd for a constant c arbitrarily close to 1 (the bottleneck
being Lemma 195). However, our witness set for the family of hard distributions in Section 10.8
encounters a natural barrier at measure 2d, since the set is sign-restricted by the cosine function
(and hence can only contain roughly every other period). This bottleneck is encountered in the proof
of Lemma 200. We ﬁnd it interesting to see if a stronger construction rules out the existing warm
starts for all κ ≥1, or if an upper bound can take advantage of the reduction of [346] to obtain
improved dependences on dimension assuming κ ≈1.

Appendix A
Deferred proofs from Chapter 2
A.1
Convex analysis
We use the standard notion of a convex conjugate of a convex function, also known as the Fenchel
dual, which we will refer to as a "dual function" throughout.
Deﬁnition 47 (Fenchel dual). For convex function f, its Fenchel dual is deﬁned by
f ∗(y) := minx {⟨y, x⟩−f(x)}
We state several key facts about dual functions that we will frequently make use of. These are
well-known and we defer proofs to standard texts, e.g. [458].
Lemma 211 (Convex conjugate, Fenchel duality). For any convex f, its Fenchel dual f ∗, and x, y
in their respective domains, f(x) + f ∗(y) ≥⟨y, x⟩.
Lemma 212 (Conjugate of a conjugate, maximizing argument). For convex f, it holds that f ∗∗(x) =
f(x), and if x = argmaxx′{⟨y, x⟩−f(x′)}, then x ∈∂f ∗(y) where ∂is the subgradient operator.
Consequently, if f and f ∗are both diﬀerentiable, ∇f and ∇f ∗are inverse functions.
Lemma 213 (Divergence of f ∗). The Bregman divergences of dual functions are related by
V f ∗
∇f(x)(∇f(x′)) = V f
x′(x).
We prove two statements regarding strong convexity of the dual of a smooth function.
The
ﬁrst has been previously been observed by e.g. [303], but we include a proof for completeness as a
precursor to Lemma 215, a coordinate smoothness generalization.
Lemma 214 (Strong convexity of the dual). Suppose f is convex and L-smooth with respect to ∥·∥.
Then, f ∗is 1
L-strongly convex with respect to ∥·∥∗.
554

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
555
Proof. This is equivalent to showing that, for two dual points ξ1, ξ2, we have
f ∗(ξ2) −f ∗(ξ1) −⟨∇f ∗(ξ1), ξ1 −ξ2⟩≥1
2L ∥ξ1 −ξ2∥2
∗.
Writing y = ∇f ∗(ξ1), x = ∇f ∗(ξ2), we also have ξ1 = ∇f(y), ξ2 = ∇f(x) by the earlier analysis of
maximizing arguments. Furthermore, by Lemma 213 relating the Bregman divergences of conjugate
functions, it suﬃces to show that
f(y) −f(x) −⟨∇f(x), y −x⟩≥1
2L ∥∇f(y) −∇f(x)∥2
∗.
Let
z = argminw ⟨∇f(y) −∇f(x), w −y⟩+ L
2 ∥w −y∥2 ,
where we note that by the equality case of Cauchy-Schwarz, we have
⟨∇f(y) −∇f(x), z −y⟩+ L
2 ∥z −y∥2 = −1
2L ∥∇f(y) −∇f(x)∥2
∗.
Then, the proof follows directly by using convexity and smoothness. Indeed,
0 ≤f(z) −f(x) −⟨∇f(x), z −x⟩
≤f(y) + ⟨∇f(y), z −y⟩+ L
2 ∥z −y∥2 −f(x) −⟨∇f(x), z −x⟩
= f(y) −f(x) −⟨∇f(x), y −x⟩+ ⟨∇f(y) −∇f(x), z −y⟩+ L
2 ∥z −y∥2
= f(y) −f(x) −⟨∇f(x), y −x⟩−1
2L ∥∇f(y) −∇f(x)∥2
∗,
where in the last line, we used the deﬁnition of z. This proves the desired claim.
The following Lemma 215 can be thought of as a limit of Lemma 214 in diagonal quadratic norms
∥·∥where all but one coordinate tends to ∞(and in the dual, only one coordinate is nonzero), which
captures coordinate smoothness. We provide a more direct proof for completeness.
Lemma 215 (Coordinate strong convexity of the dual). For a convex function f which is Li-smooth
in the ith coordinate, we have
f(y) ≥f(x) + ⟨∇f(x), y −x⟩+
1
2Li
|∇if(y) −∇if(x)|2 .
Proof. The proof is essentially the same as that of Lemma 214, with a tighter guarantee given by

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
556
coordinate smoothness. Let ∥·∥be the ℓ2 norm, and
z = argminw:w=y+tei ⟨∇f(y) −∇f(x), w −y⟩+ Li
2 ∥w −y∥2
where we note that by the equality case of Cauchy-Schwarz, and as y and z only diﬀer in the ith
coordinate, we have
⟨∇f(y) −∇f(x), z −y⟩+ Li
2 ∥z −y∥2 = −1
2Li
|∇if(y) −∇if(x)|2 .
Then, the proof follows:
0 ≤f(z) −f(x) −⟨∇f(x), z −x⟩
≤f(y) + ⟨∇f(y), z −y⟩+ Li
2 ∥z −y∥2 −f(x) −⟨∇f(x), z −x⟩
= f(y) −f(x) −⟨∇f(x), y −x⟩+ ⟨∇f(y) −∇f(x), z −y⟩+ Li
2 ∥z −y∥2
= f(y) −f(x) −⟨∇f(x), y −x⟩−
1
2Li
|∇if(y) −∇if(x)|2
where in the last line, we used the deﬁnition of z. This proves the desired claim.
As a corollary of (2.10), we obtain the following.
Lemma 216. For w = Proxr
z(g), ∀u, ⟨g, w −u⟩≤V r
z (u) −V r
w(u) −V r
z (w).
Proof. By ﬁrst order optimality of the function ⟨g, w⟩+ V r
z (w) as a function of w, we have
⟨g + ∇V r
z (w), u −w⟩≥0 =⇒⟨g, w −u⟩≤⟨−∇V r
z (w), w −u⟩= V r
z (u) −V r
w(u) −V r
z (w),
where we used (2.10).
Lemma 2. If g is L-Lipschitz and r is µ-strongly convex in ∥·∥, g is L/µ-relatively Lipschitz with
respect to r.
Proof. By Cauchy-Schwarz, Lipschitzness of g, and strong convexity of r,
⟨g(w) −g(z), w −u⟩≤∥g(w) −g(z)∥∗∥w −u∥≤L ∥w −z∥∥w −u∥
≤L
2

∥w −z∥2 + ∥w −u∥2
≤L
µ (V r
z (w) + V r
w(u)) .
Lemma 3. If f is L-relatively smooth with respect to r, i.e. V f
x (y) ≤LV r
x (y) for all x and y, then
g, deﬁned by g(x) := ∇f(x) for all x, is L-relatively Lipschitz with respect to r.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
557
Proof. By assumption of relative smoothness of f and the deﬁnition of divergence,
L (V r
z (w) + V r
w(u)) ≥V f
z (w) + V f
w (u)
= f(w) −

f(z) + ∇f(z)⊤(w −z)

+ f(u) −

f(w) + ∇f(w)⊤(u −w)

= V f
z (u) −∇f(z)⊤(z −u) −∇f(z)⊤(w −z) + ∇f(w)⊤(w −u)
= V f
z (u) + ⟨g(w) −g(z), u −z⟩.
The result follows from the fact that V f
z (u) ≥0 by convexity of f.
A.2
Unaccelerated smooth convex optimization via mirror
prox
For completeness, we provide here the proof that the 1/T rate of extragradient methods for solving
Lipschitz monotone VIs also yields a 1/T rate for smooth function minimization. Given a smooth,
convex f this rate is achieved by applying these methods to solve the VI induced by g(x) := ∇f(x).
Consequently, this section highlights that the accelerated rates we achieve for minimizing f are via
working in the expanded primal-dual space of the VI induced by the Fenchel game, as well as our
ﬁne-grained notion of relative Lipschitzness, as considered in Section 2.4.
Lemma 217. For L-smooth, convex f : Rd →R deﬁne g : Rd →R by g(x) := ∇f(x), and for
arbitrary x0 let r(x) := 1
2 ∥x0 −x∥2
2. For all T ≥0, both Mirror-Prox(x0, T) (Algorithm 1) and
Dual-Ex(x0, T) (Algorithm 65) produce {wt}0≤t<T such that for any x∗minimizing f,
f

1
T
X
0≤t<T
wt

−f(x∗) ≤L ∥x0 −x∗∥2
2
2T
.
Proof. Since x0 = argminx∈Rdr(x), and since (by Lemma 2 and smoothness) g is L-relatively Lips-
chitz with respect to r, Proposition 1 and Proposition 53 imply
X
0≤t<T
⟨g(wt), wt −u⟩≤LV r
x0(u).
Consequently, applying convexity and letting u = x∗above yields
f

1
T
X
0≤t<T
wt

−f(x∗) ≤1
T
X
0≤t<T
(f(wt) −f(x∗)) ≤1
T
X
0≤t<T
⟨g(wt), wt −x∗⟩≤LV r
x0(x∗)
T
= L ∥x0 −x∗∥2
2
2T
.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
558
We remark that for µ-strongly convex functions, it is well-known that this also implies a O( L
µ )
rate of linear convergence to an approximate minimizer by converting function error back to a bound
on the squared distance to x∗via applying strong convexity.
A.3
Minimax optimization
In this section, we give a new ﬁne-grained complexity bound of minimax optimization under block-
wise strong convexity and smoothnesses of the problem. Speciﬁcally, consider the problem
min
x∈Rd max
y∈Rd f(x, y),
(A.1)
and assume that the objective satisﬁes the following bounds1:
1.
∇2
xxf(x, y)

op ≤Lxx,
∇2
xyf(x, y)

op ≤Lxy,
∇2
yyf(x, y)

op ≤Lyy everywhere
2. f(x, ·) is µy-strongly concave for each x, and f(·, y) is µx-strongly convex for each y
We show an upper bound of
O
  
Lxx
µx
+
s
L2xy
µxµy
+ Lyy
µy
!
· log 1
ϵ
!
queries via a simple application of our relative Lipschitzness framework, and a known strongly
monotone variant of Algorithm 1, for obtaining ϵ Euclidean distance to the saddle point of (A.1),
improving upon the bound of [366] in some cases. This can be converted into a duality gap bound
while only increasing the logarithmic term by problem parameters; see the end of this section for
a discussion. We will use Algorithm 3 and a tightening of its guarantees following Deﬁnition 1, in
Proposition 3, a strengthening of a known derivation (see e.g. Proposition 5 of [111]) under relative
Lipschitzness. We defer its proof to Appendix A.4. Our claimed upper bound follows by combining
Proposition 3 with the following relative Lipschitzness bound. We note that the same rate can be
obtained by directly combining Proposition 3 with a rescaling of the space to make the objective
1-strongly convex in each variable, but we include a proof via relative Lipschitzness because it makes
the calculation more mechanical (and also originally motivated this observation).
Lemma 218. Let r(x, y) = µx
2 ∥x∥2
2 + µy
2 ∥y∥2
2 and g(x, y) = (∇xf(x, y), −∇yf(x, y)). Then with
respect to r, g is 1-strongly monotone and
Lxx
µx
+
s
L2xy
µxµy
+ Lyy
µy
-relatively Lipschitz.
1Here, ∥·∥op is the ℓ2 operator norm.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
559
Proof. First, we prove strong monotonicity: let z = (zx, zy) and w = (wx, wy). Moreover, let J be
the Jacobian of the operator g, and note that
g(w) −g(z) =
Z 1
0
J(zt)(w −z)dt, where zt := (1 −t)z + tw.
Thus, integrating and using antisymmetry of the oﬀ-diagonal blocks of J,
⟨g(w) −g(z), w −z⟩=
Z 1
0
(w −z)⊤J(zt)(w −z)dt
=
Z 1
0
 (wx −zx)⊤∇2fxx(zt)(wx −zx) −(wy −zy)⊤∇2fyy(zt)(wy −zy)

dt
≥
Z 1
0

µx ∥wx −zx∥2
2 + µy ∥wy −zy∥2
2

dt = V r
z (w) + V r
w(z).
In the only inequality, we used our strong convexity and strong concavity assumptions. Next, we
prove the relative Lipschitz bound: consider three points z, w, and u = (ux, uy). We have by triangle
inequality and the assumed operator norm bounds
gx(w) −gx(z) =
Z 1
0
 ∇2
xxf(zt)(wx −zx) + ∇2
xyf(zt)(wy −zy)

dt
=⇒∥gx(w) −gx(z)∥2 ≤Lxx ∥wx −zx∥2 + Lxy ∥wy −zy∥2 ,
gy(w) −gy(z) = −
Z 1
0
 ∇2
yxf(zt)(wx −zx) + ∇2
yyf(zt)(wy −zy)

dt
=⇒∥gy(w) −gy(z)∥2 ≤Lxy ∥wx −zx∥2 + Lyy ∥wy −zy∥2 .
Therefore, it follows from Cauchy-Schwarz that
⟨g(w) −g(z), w −u⟩≤Lxx ∥wx −zx∥2 ∥wx −ux∥2 + Lxy ∥wy −zy∥2 ∥wx −ux∥2
+Lxy ∥wx −zx∥2 ∥wy −uy∥2 + Lyy ∥wy −zy∥2 ∥wy −uy∥2 .
Finally, denoting rx(x) := µx
2 ∥x∥2
2, ry(y) := µy
2 ∥y∥2
2, the conclusion of relative Lipschitzness follows
from nonnegativity of divergences and combining the three bounds
Lxx ∥wx −zx∥2 ∥wx −ux∥2 ≤Lxx
µx

V rx
zx (wx) + V rx
wx(ux)

,
Lyy ∥wy −zy∥2 ∥wy −uy∥2 ≤Lyy
µy

V ry
zy (wy) + V ry
wy(uy)

,
Lxy ∥wy −zy∥2 ∥wx −ux∥2 + Lxy ∥wx −zx∥2 ∥wy −uy∥2 ≤
Lxy
√µxµy
(V r
z (w) + V r
w(u)) .

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
560
We prove the last bound here; the other two follow similarly. By expanding the deﬁnition of r,
Lxy
√µxµy
(V r
z (w) + V r
w(u)) =
Lxy
√µxµy
µx
2 ∥wx −zx∥2
2 + µy
2 ∥wy −zy∥2
2
+µx
2
zx
+ −wx2
2 + µy
2
zy
+ −wy2
2

≥Lxy (∥wx −zx∥2 ∥wy −uy∥2 + ∥wx −ux∥2 ∥wy −zy∥2) .
We give a brief discussion regarding converting a distance bound from a pair (x, y) to the saddle
point (x∗, y∗) of (A.1) into a duality gap bound. Speciﬁcally, let y′ be the best response to x, and
x′ be the best response to y. Further, denote
h1(x) := max
y∈Rd f(x, y), h2(y) := min
x∈Rd f(x, y).
Note the duality gap of the pair (x, y) is precisely
f(x, y′) −f(x′, y) = (f(x, y′) −f(x∗, y∗)) + (f(x∗, y∗) −f(x′, y))
= (h1(x) −h1(x∗)) + (h2(y∗) −h2(y)) .
Denote L = max(Lxx, Lxy, Lyy). Under the setting of this section, it was shown in Lemma B.2 of
[366] that h1 is 2L2
µx smooth, which implies the bound (since x∗minimizes h1 by deﬁnition)
h1(x) −h1(x∗) ≤L2
µx
∥x −x∗∥2
2 .
Consequently, we can convert a distance bound into a duality gap bound, while only aﬀecting the
logarithmic runtime term. A similar argument holds for terms corresponding to h2.
A.4
Additional extragradient methods
A.4.1
Strongly monotone mirror prox
We give a proof of Proposition 3, restated here for convenience.
Proposition 3. If Φ is λ-relatively Lipschitz with respect to r over Zalg containing all iterates of
Algorithm 3, and its VI is solved by z⋆, the iterates of Algorithm 3 satisfy
V r
zt(z⋆) ≤

1 + m
λ
−t
V r
z0(z⋆), for all t ∈[T].

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
561
Proof. As in (2.11), ﬁrst-order optimality with respect to z∗and nonnegativity of Vwt(zt+1) implies
1
λ ⟨g(zt), wt −zt+1⟩≤V r
zt(zt+1) −V r
wt(zt+1) −V r
zt(wt),
1
λ ⟨g(wt), zt+1 −z∗⟩≤V r
zt(z∗) −V r
zt+1(z∗) −V r
zt(zt+1) + m
λ

V r
wt(z∗) −V r
zt+1(z∗)

.
Rearranging and applying relative Lipschitzness (Deﬁnition 1) as in (2.12) yields

1 + m
λ

Vzt+1(z∗) ≤1
λ (⟨g(wt), wt −z∗⟩−mVwt(z∗)) +

1 + m
λ

Vzt+1(z∗) ≤Vzt(z∗).
(A.2)
Here, we used the fact that by the deﬁnition of z∗and strong monotonicity,
⟨g(wt), wt −z∗⟩−mVwt(z∗) ≥⟨g(wt) −g(z∗), wt −z∗⟩−mVwt(z∗) ≥0.
Thus, iterating the inequality (A.2) yields the conclusion.
A.4.2
Dual extrapolation
In this section, we give a simpliﬁed presentation of the dual extrapolation algorithm of [420] for
approximately solving a variational inequality in a monotone operator g, using a regularizer r. It
obtains the same rate of convergence as the mirror prox algorithm, under relative Lipschitzness
(Deﬁnition 1). Speciﬁcally, it is the following algorithm which updates a dual variable st iteratively.
Algorithm 65: Dual-Ex(¯z, T): Dual extrapolation [420]
1 Input: Distance generating r, λ-relatively Lipschitz monotone g : Z →Z∗, initial point
¯z ∈Z;
2 s0 ←0;
3 for 0 ≤t < T do
4
zt ←Proxr
¯z(st);
5
wt ←Proxr
zt( 1
λg(zt));
6
st+1 ←st + 1
λg(wt);
Proposition 53. The iterates {wt} of Algorithm 65 satisfy for all u ∈Z,
X
0≤t<T
⟨g(wt), wt −u⟩≤λV r
¯z (u).
Proposition 53 requires the following helper lemma, the main tool in its proof.
Lemma 219. For every iteration t,
1
λ ⟨g(wt), wt −¯z⟩≤⟨st+1, zt+1 −¯z⟩+ V r
¯z (zt+1) −⟨st, zt −¯z⟩−V r
¯z (zt).

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
562
Proof. We again apply Lemma 216 on the two steps with respect to zt+1, yielding
⟨st, zt −zt+1⟩≤V r
¯z (zt+1) −V r
zt(zt+1) −V r
¯z (zt),
1
λ ⟨g(zt), wt −zt+1⟩≤V r
zt(zt+1) −V r
wt(zt+1) −V r
zt(wt).
Furthermore, note that by relative Lipschitzness, we have
1
λ ⟨g(wt) −g(zt), wt −zt+1⟩≤V r
wt(zt+1) + V r
zt(wt).
Combining these three inequalities and rearranging terms appropriately yields the conclusion:
⟨st, zt −zt+1⟩+ 1
λ ⟨g(wt), wt −zt+1⟩≤V r
¯z (zt+1) −V r
¯z (zt)
=⇒1
λ ⟨g(wt), wt −¯z⟩≤⟨st+1, zt+1 −¯z⟩+ V r
¯z (zt+1) −⟨st, zt −¯z⟩−V r
¯z (zt).
This immediately yields the following:
Corollary 57. Φt = 1
λ
Pt−1
k=0 ⟨g(wk), wk −¯z⟩−⟨st, zt −¯z⟩−V¯z(zt) is nonincreasing in t.
Proof of Proposition 53. Note that for any u,
T −1
X
t=0
⟨g(wt), wt −u⟩=
T −1
X
t=0
⟨g(wt), wt −¯z⟩+
T −1
X
t=0
⟨g(wt), ¯z −u⟩+ (λV¯z(u) −λV¯z(u))
≤
T −1
X
t=0
⟨g(wt), wt −¯z⟩+
T −1
X
t=0
⟨g(wt), ¯z −zT ⟩+ (λV¯z(u) −λV¯z(zT ))
= λΦT + λV¯z(u) ≤λΦ0 + λV¯z(u) = λV¯z(u).
The ﬁrst inequality used the deﬁnition of zT , and the second inequality used Corollary 57.
We note that all our acceleration results are also implementable with dual extrapolation as the
base method, by analogous arguments as in Sections 2.4 and 2.6.
A.5
Extragradient acceleration in non-Euclidean norms
In this section, we generalize the developments of Section 2.4 to general norms, where f is L-smooth
in some ∥·∥. The notion of strong convexity in general norms is slightly diﬀerent; for acceleration
to be achievable, f must be µ-strongly convex with respect to a regularizer ω, where ω is 1-strongly
convex in ∥·∥; see e.g. [18] for a discussion.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
563
We ﬁrst state our general strategy. Let h(x) := f(x) −µω(x), for all x ∈Rd; it is immediate
that h is convex and L-smooth in ∥·∥. To solve the problem minx∈Rd f(x) = minx∈Rd h(x) + µω(x),
it instead suﬃces to solve the equivalent saddle point problem
min
x∈Rd max
y∈Rd µω(x) + ⟨y, x⟩−h∗(y).
(A.3)
It is immediate that the saddle point of (A.3) is z∗:= (x∗, ∇h(x∗)). The key observation is that
(A.3) is strongly monotone and relatively Lipschitz (with an accelerated parameter) with respect to
the natural choice of regularizer,
r(x, y) := µω(x) + h∗(y).
(A.4)
From this point, it suﬃces to apply the strongly monotone extragradient framework of Proposition 3.
We now make this formal by giving the following two helper lemmata.
Lemma 220. Let f : Rd →R be L-smooth in norm ∥·∥, and µ-strongly convex with respect to
ω, a 1-strongly convex function in ∥·∥.
Let h(x) := f(x) −µω(x) ∀x ∈Rd, and let g(x, y) =
(y + µ∇ω(x), ∇h∗(y) −x) be the gradient operator of the objective (A.3).
Then, g is 1-strongly
monotone with respect to the distance generating function r deﬁned in (A.4).
Proof. By deﬁnition, it suﬃces to show that for all w, z ∈Rd × Rd,
⟨g(w) −g(z), w −z⟩≥V r
w(z) + V r
z (w) = ⟨∇r(w) −∇r(z), w −z⟩.
By direct expansion, this is an equality. In particular, for w = (wx, wy) and z = (zx, zy),
⟨g(w) −g(z), w −z⟩= ⟨(wy −zy) + µ (∇ω(wx) −∇ω(zx)) , wx −zx⟩
+ ⟨(∇h∗(wy) −∇h∗(zy)) −(wx −zx) , wy −zy⟩
= µ ⟨∇ω(wx) −∇ω(zx), wx −zx⟩+ ⟨∇h∗(wy) −∇h∗(zy), wy −zy⟩
= ⟨∇r(w) −∇r(z), w −z⟩.
Lemma 221. In the setting of Lemma 220, g is 1 +
q
L
µ -relatively Lipschitz with respect to r.
Proof. The proof is patterned oﬀof Lemma 4. Consider three points z = (zx, zy), w = (wx, wy),
u = (ux, uy). By direct calculation,
⟨g(w) −g(z), w −u⟩= ⟨wy −zy, wx −ux⟩+ ⟨−wx + zx, wy −uy⟩
+ µ ⟨∇ω(wx) −∇ω(zx), wx −ux⟩+ ⟨∇h∗(wy) −∇h∗(zy), wy −uy⟩.
(A.5)
To bound the ﬁrst line of (A.5), we have the following analog to (2.14), where we use that h∗is

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
564
1
L-strongly convex in ∥·∥∗by Lemma 214:
⟨wy −zy, wx −ux⟩+ ⟨zx −wx, wy −uy⟩≤∥wy −zy∥∗∥wx −ux∥+ ∥zx −wx∥∥wy −uy∥∗
≤
s
L
µ
µ
2 ∥wx −zx∥2 + µ
2 ∥wx −ux∥2 + 1
2L ∥wy −zy∥2
∗+ 1
2L ∥wy −uy∥2
∗

≤
s
L
µ (V r
z (w) + V r
w(u)) .
To bound the second line of (A.5), we use that the calculation (2.15) implies that
µ ⟨∇ω(wx) −∇ω(zx), wx −ux⟩+ ⟨∇h∗(wy) −∇h∗(zy), wy −uy⟩= ⟨∇r(w) −∇r(z), w −u⟩
≤V r
z (w) + V r
w(u).
Combining the above two calculations in the context of (A.5) yields the desired claim.
Theorem 75. In the setting of Lemma 220, consider running Algorithm 3 on the monotone operator
g and the distance generating function r initialized at z0 := (x0, ∇h(x0)), for T iterations. Every
iteration consists of solving a constant number of proximal problems in the function ω, and a constant
number of d-dimensional vector operations; moreover, we can always maintain each zt in the form
(xt, ∇h(vt)) for some explicitly computed vt ∈Rd. Finally, we have
T ≥4
s
L
µ log
2L
µ · f(x0) −f(x∗)
ϵ

=⇒f(xT ) −f(x∗) ≤ϵ.
Proof. We ﬁrst demonstrate the claimed implementability of steps of Algorithm 3. Note that from
the form of iterates in Algorithm 3, each x block of variables indeed results from solving proximal
problems in ω. On the y block, the claim of the invariant (that it can be represented as some ∇h(v)
for explicit v) follows identically to the arguments in Lemma 5, where we inductively show that the
y block of each zt, wt can be maintained as a gradient of h.
Next, we prove the desired error bound. We ﬁrst compute, for z∗:= (x∗, ∇h(x∗)),
V r
z0(z∗) = µV ω
x0(x∗) + V h∗
∇h(x0)(∇h(x∗)) = µV ω
x0(x∗) + V h
x∗(x0)
≤V f
x0(x∗) + V f
x∗(x0) = ⟨∇f(x0) −∇f(x∗), x0 −x∗⟩
≤L ∥x0 −x∗∥2 ≤2L
µ (f(x0) −f(x∗)) .
By applying Proposition 3 with the bounds from Lemmas 220 and 221, we have
V r
zT (z∗) ≤µϵ
L .

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
565
The conclusion follows from smoothness of f and strong convexity of ω, i.e.
L
µ V r
zT (z∗) ≥L
µ V µω
xT (x∗) ≥L
2 ∥xT −x∗∥2 ≥f(xT ) −f(x∗).
A.6
Missing proofs from Section 2.6
Lemma 7. Let ¯wt := (xt + P
i∈[d] ∆(i)
t , ∇f(vt+ 1
2 )). Then ∀u, taking expectations over iteration t,
E
hD
gi(w(i)
t ), w(i)
t
−u
Ei
= ⟨g( ¯wt), ¯wt −u⟩.
Proof. Note vt+ 1
2 is deterministic regardless of the sampled i ∈[d]. Expanding for u = (ux, uy),
E
hD
gi(w(i)
t ), w(i)
t
−u
Ei
=
X
i∈[d]
pi
 1
pi
∇if(vt+ 1
2 ), x(i)
t+ 1
2 −ux

+

vt+ 1
2 −

xt + 1
pi
∆(i)
t

, yt+ 1
2 −uy

= ⟨g( ¯wt), ¯wt −u⟩.
Here, we used the fact that ∇if(vt+ 1
2 ) is 1-sparse.
Lemma 8 (Expected relative Lipschitzness). Let λ = 1 + S1/2/√µ, where S1/2 := P
i∈[d]
√Li.
Then, for the iterates (2.21) with pi = √Li/S1/2, taking expectations over iteration t,
E
hD
gi(w(i)
t ) −gi (zt) , w(i)
t
−z(i)
t+1
Ei
≤λE
h
V r
zt(w(i)
t ) + V r
w(i)
t (z(i)
t+1)
i
.
Proof. Equivalently, we wish to show that
E
hD
gi(w(i)
t ) −gi (zt) , w(i)
t
−z(i)
t+1
Ei
≤

1 + S1/2
√µ

E
h
V r
zt(w(i)
t ) + V r
w(i)
t (z(i)
t+1)
i
.
The proof is patterned from Lemma 4. By direct calculation, the left hand side is
E
hD
gi(w(i)
t ) −gi (zt) , w(i)
t
−z(i)
t+1
Ei
=
X
i∈[d]
pi
 1
pi
D
∇if(vt+ 1
2 ) −∇if (vt) , x(i)
t+ 1
2 −x(i)
t+1
E
+ 1
pi
D
xt −x(i)
t+ 1
2 , ∇if(vt+ 1
2 ) −∇if(v(i)
t+1)
E
+
X
i∈[d]
pi
D
vt+ 1
2 −vt, ∇f(vt+ 1
2 ) −∇f(v(i)
t+1)
E
.
(A.6)

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
566
We ﬁrst bound the second and third lines of (A.6):
1
pi
D
∇if(vt+ 1
2 ) −∇if (vt) , x(i)
t+ 1
2 −x(i)
t+1
E
+
D
xt −x(i)
t+ 1
2 , ∇if(vt+ 1
2 ) −∇if(v(i)
t+1)
E
≤S1/2
√µ
µ
2
x(i)
t+ 1
2 −x(i)
t+1

2
2 +
1
2Li
∇if(vt+ 1
2 ) −∇if(v(i)
t+1)

2
2
+µ
2
xt −x(i)
t+ 1
2

2
2 +
1
2Li
∇if(vt+ 1
2 ) −∇if (vt)

2
2

≤S1/2
√µ

V r
zt(w(i)
t ) + V r
w(i)
t (z(i)
t+1)

.
(A.7)
The ﬁrst inequality used the deﬁnition pi =
p
Li/S1/2 and Cauchy-Schwarz, and the second used
strong convexity and Lemma 215. Next, we bound the fourth line of (A.6):
D
vt+ 1
2 −vt, ∇f(vt+ 1
2 ) −E
h
∇f(v(i)
t+1)
iE
≤V f ∗
∇f(vt)

∇f(vt+ 1
2 )

+ V f ∗
∇f(vt+ 1
2 )

E
h
∇f(v(i)
t+1)
i
≤V f ∗
∇f(vt)

∇f(vt+ 1
2 )

+ E

V f ∗
∇f(vt+ 1
2 )

∇f(v(i)
t+1)

≤E
h
V r
zt(w(i)
t ) + V r
w(i)
t (z(i)
t+1)
i
.
The ﬁrst inequality is (2.15), the second is convexity of Bregman divergence, and the third used
nonnegativity of µ
2 ∥·∥2
2. Combining with an expectation over (A.7) yields the claim.
Lemma 222. Suppose at step t, for Bt ∈R2×2, pt, qt ∈Rd, we maintain

xt
vt

=

pt
qt

Bt.
Then, for any sampled i, we can compute Bt+1 ∈R2×2, pt+1, qt+1 ∈Rd such that

xt+1
vt+1

=

pt+1
qt+1

Bt+1,
using two generalized partial derivative oracle queries and constant additional work.
Proof. Suppose on iteration t that coordinate i was sampled.
In closed form, we have by the
deﬁnition of gradient estimators (2.21) the updates to the x component
xt+ 1
2 = xt −
1
µλpi
∇if(vt), xt+1 = xt −
1
µλpi
∇if(vt+ 1
2 ).
Thus, if we can maintain the implicit representation of vt and vt+ 1
2 , we can compute these updates

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
567
Algorithm 1 EG-Coord-Accel(x0, ϵ): Extragradient accelerated coordinate-smooth minimiza-
tion
Input: x0 ∈Rd, f {Li}i∈[d]-coordinate smooth and µ-s.c. in ∥·∥2, and ϵ0 ≥f(x0) −f(x∗)
λ ←1 + P
i∈[d]
p
Li/µ, T ←4⌈λ⌉, K ←⌈log2
ϵ0
ϵ ⌉, A ←
 
1
1
κ −
1
κ2
0
1 −1
κ +
1
κ2
!
p0 ←x0, q0 ←x0, B0 ←I2×2
for 0 ≤k < K do
Sample τ uniformly in [0, T −1]
for 0 ≤t < τ do
Sample i ∝√Li
Compute ∇if(vt), ∇if((1 −λ−1)vt + λ−1xt) via generalized partial derivative oracle
st ←

1
µλpi ∇if((1 −λ−1)vt + λ−1xt)
1
µλ2p2
i ∇if(vt)

Bt+1 ←BtA,

pt+1
qt+1

←

pt
qt

−stBt+1
−1
end for
B0 ←
 
[Bτ]12
[Bτ]12
[Bτ]22
[Bτ]22
!
, p0 ←pτ, q0 ←qτ
end for
return [Bτ]12pτ + [Bτ]22qτ
via a partial derivative oracle. Next, the proof of Lemma 5 implies in closed form
vt+ 1
2 =

1 −1
λ

vt + 1
λxt,
vt+1 = vt + 1
λ

xt + 1
pi
∆(i)
t
−vt+ 1
2

=

1 −1
λ + 1
λ2

vt +
 1
λ −1
λ2

xt −
1
µλ2p2
i
∇if(vt),
where we have already computed ∇if(vt). Therefore, the update can be written as, for 2-sparse st,

xt+1
vt+1

=

xt
vt

A −st
where A :=

1
1
κ −
1
κ2
0
1 −1
κ +
1
κ2

, st :=

1
µλpi ∇if(vt+ 1
2 )
1
µλ2p2
i ∇if(vt)

.
We can therefore maintain this implicitly via the equivalent step (doing constant extra work),
Bt+1 = BtA,

pt+1
qt+1

=

pt
qt

−stBt+1
−1.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
568
A.7
Optimism
In this section, we discuss the relationship of our condition (relative Lipschitzness) with the condition
of "optimism" proposed by [452], which is known to recover the mirror prox algorithm for Lipschitz
operators. Speciﬁcally, the optimistic mirror descent procedure of [452] iterates the following steps
(see Equation 1 of their paper), for strongly convex r:
wt ←Proxr
zt (ηMt) , zt+1 ←Proxr
zt (ηg(wt)) .
(A.8)
Here, Mt is a vector which ideally "resembles" the point ηg(wt), capturing a notion of "predictabil-
ity." Under Lipschitzness of g, [452] notes that simply choosing Mt = g(zt) and η =
1
L yields an
algorithm identical to Algorithm 1, where the notion of predictable sequences comes from stability
of g. Rakhlin and Sridharan prove the following claim about their procedure (Lemma 1, [452]):
Proposition 54 (Optimistic mirror descent). For g : Z →Z∗and any u ∈Z,
X
0≤t<T
⟨g(wt), wt −u⟩≤V r
z0(u)
η
+
X
0≤t<T
∥Mt −g(wt)∥∗∥wt −zt+1∥
−1
2η

∥wt −zt∥2 + ∥wt −zt+1∥2
.
Indeed, Proposition 54 follows by the proofs we give of Proposition 1 and Lemma 2, where we
apply Cauchy-Schwarz and strong convexity of r. However, crucially our proof in Section 2.4 of the
accelerated rate bypasses this direct application of Cauchy-Schwarz to the quantity Mt −g(wt), and
couples terms in a way which can yield a tighter relative Lipschitz parameter (see Lemma 4). In
this sense, relative Lipschitzness can sharpen the rate of convergence for optimistic mirror descent
achieved by [452], which was crucial for our improved guarantees.
A.8
Reducing strongly monotone problems to regularized
subproblems
A.8.1
Convex optimization
We give the following generic reduction for strongly convex optimization in the form of an algorithm.
Similar reductions are standard in the literature [227], but we include the algorithm and full analysis
here for completeness.
Lemma 223. In Algorithm 66, letting x⋆minimize f, we have for every k ∈[K]:
EVxk (x⋆) ≤1
2k Vx0 (x⋆) .

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
569
Algorithm 66: Redx-Convex: Strongly convex optimization reduction
1 Input: µ-strongly convex f : X →R, x0 ∈X
2 Parameter(s): K ∈N
3 for 0 ≤k < K do
4
xk+1 ←any (possibly random) point satisfying
EVxk+1(x⋆
k+1) ≤1
4Vxk(x⋆
k+1), where x⋆
k+1 := argminx∈X f(x) + µ
4 Vxk(x)
Proof. By applying the optimality condition on x⋆
k+1, strong convexity of f, and (2.10),

∇f(x⋆
k+1), x⋆
k+1 −x⋆

≤µ
4

xk −x⋆
k+1, x⋆
k+1 −x⋆

=⇒µVx⋆
k+1(x⋆) ≤f(x⋆
k+1) −f(x⋆)
≤

∇f(x⋆
k+1), x⋆
k+1 −x⋆

≤µ
4 Vxk(x⋆) −µ
4 Vx⋆
k+1(x⋆) −µ
4 Vxk(x⋆
k+1).
Further by the triangle inequality and (a + b)2 ≤2a2 + 2b2, we have
Vxk+1(x⋆) ≤2Vxk+1(x⋆
k+1) + 2Vx⋆
k+1(x⋆).
Hence, combining these pieces,
EVxk+1(x⋆) ≤2Vx⋆
k+1(x⋆) + 2EVxk+1(x⋆
k+1)
≤2Vx⋆
k+1(x⋆) + 1
2Vxk(x⋆
k+1)
≤1
2Vxk(x⋆) −1
2Vx⋆
k+1(x⋆) ≤1
2Vxk(x⋆).
We apply this reduction in order to prove Corollary 4.
Corollary 4. Suppose the summands {fi}i∈[n] in (2.36) satisfy Assumption 2, and Ffs is µ-strongly
convex with minimizer x⋆. Further, suppose we have x0 ∈X such that Ffs(x0) −Ffs(x⋆) ≤ϵ0.
Algorithm 66 using Algorithm 6 to implement steps returns x ∈X with EFfs(x) −Ffs(x⋆) ≤ϵ in
Ntot iterations, using a total of O(Ntot) gradient calls each to some fi for i ∈[n], where
Ntot = O

κfs log
κfsϵ0
ϵ

, for κfs := n +
X
i∈[n]
√Li
√nµ.
Proof. The overhead K is asymptotically the same here as the parameter T in Theorem 8, by

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
570
analogous smoothness and strong convexity arguments. Moreover, we use Theorem 8 to solve each
subproblem required by Algorithm 66; in particular, the subproblem is equivalent to approximately
minimizing Ffs + µ
8 ∥·∥2, up to a linear shift which does not aﬀect any smoothness bounds, and a
constant in the strong convexity. We note that we will initialize the subproblem solver in iteration
k with xk. We hence can set T = 2 and S = O(κfs), yielding the desired iteration bound.
A.8.2
Convex-concave optimization
We give the following generic reduction for strongly convex-concave optimization in the form of an
algorithm. For simplicity in this section, we deﬁne for z = (zx, zy) ∈X × Y,
ω(z) := µx
2 ∥zx∥2 + µy
2 ∥zy∥2 .
Algorithm 67: Redx-Minimax: Reduction for minimax
1 Input: F : X × Y →R such that F(·, y) is µx-strongly convex for all y ∈Y and F(x, ·)
µy-strongly concave for all x ∈X, z0 ∈X × Y
2 Parameter(s): K ∈N
3 for 0 ≤k < K do
4
zk+1 ←any (possibly random) point satisfying
E
h
V ω
zk+1
 z⋆
k+1
i
≤1
4
 V ω
zk
 z⋆
k+1

,
where z⋆
k+1 := argminzx∈X argmaxzy∈YF(zx, zy) + µx
4 Vzx
k (zx) −µy
4 Vzy
k (zy)
Lemma 224. In Algorithm 67, letting (x⋆, y⋆) be the saddle point of F, we have for every k ∈[K]:
E

V ω
zk(z⋆)

≤1
2k V ω
z0(z⋆).
Proof. By applying the optimality conditions on z⋆
k+1, strong convexity-concavity of F, and (2.10),
and letting ΦF be the gradient operator of F,

ΦF (z⋆
k+1), z⋆
k+1 −z⋆

≤µx
4

zx
k −[z⋆
k+1]x, [z⋆
k+1]x −zx
⋆

+ µy
4

zy
k −[z⋆
k+1]y, [z⋆
k+1]y −zy
⋆

=⇒V ω
z⋆
k+1(z⋆) ≤

ΦF (z⋆
k+1), z⋆
k+1 −z⋆

≤1
4V ω
zk(z⋆) −1
4V ω
z⋆
k+1(z⋆) −1
4Vzk(z⋆
k+1).

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
571
Further by the triangle inequality and (a + b)2 ≤2a2 + 2b2, we have
V ω
zk+1(z⋆) ≤2V ω
zk+1(z⋆
k+1) + 2V ω
z⋆
k+1(z⋆).
Hence, combining these pieces,
EV ω
zk+1(z⋆) ≤2V ω
z⋆
k+1(z⋆) + 2EV ω
zk+1(z⋆
k+1)
≤2V ω
z⋆
k+1(z⋆) + 1
2V ω
zk(z⋆
k+1)
≤1
2V ω
zk(z⋆) −1
2V ω
z⋆
k+1(z⋆) ≤1
2V ω
zk(z⋆).
We apply this reduction in order to prove Corollary 5.
Corollary 5. Suppose the summands {fi, gi, hi}i∈[n] in (2.49) satisfy Assumption 4, and Fmmfs is
µx-strongly convex in x, µy-strongly convex in y, with saddle point (x⋆, y⋆). Further, suppose we
have (x0, y0) ∈X × Y such that GapFmmfs(x0, y0) ≤ϵ0. Algorithm 66 using Algorithm 8 and 9 to
implement steps returns (x, y) ∈X × Y with EGap(x, y) ≤ϵ in Ntot iterations, using a total of
O(Ntot) gradient calls each to some fi, gi, or hi for i ∈[n], where
Ntot = O

κmmfs log(κmmfs) log
κmmfsϵ0
ϵ

,
for κmmfs := n +
1
√n
X
i∈[n]


s
Lx
i
µx +
s
Ly
i
µy + Λxx
i
µx +
Λxx
i
√µxµy + Λyy
i
µy

.
Proof. The overhead K is asymptotically the same here as the logarithmic term in the parameter T in
Theorem 9, by analogous smoothness and strong convexity arguments. Moreover, we use Theorem 9
with µx, µy rescaled by constants to solve each subproblem required by Algorithm 67; in particular,
the subproblem is equivalent to approximately ﬁnding a saddle point to Ffs(z)+ µx
8 ∥zx∥2 −µy
8 ∥zy∥2,
up to a linear shift which does not aﬀect any smoothness bounds. We note that we will initialize the
subproblem solver in iteration k with zk. We hence can set T = O(γ), yielding the desired iteration
bound.
A.9
Helper facts
Here we state two helper facts that are used throughout the analysis, for completeness of the paper.
The ﬁrst gives a few properties on monotone operators. We ﬁrst recall by deﬁnition, an operator

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
572
Φ : Z →Z∗is monotone if
⟨Φ(z) −Φ(z′), z −z′⟩≥0,
for all z′, z′ ∈Z.
An operator Φ is m-strongly monotone with respect to convex r : Z →R if for all z, z′ ∈Z,
⟨Φ(z) −Φ(z′), z −z′⟩≥m ⟨∇r(z) −∇r(z′), z −z′⟩,
for all z′, z′ ∈Z.
We state the following standard facts about monotone operators and their specialization to
convex-concave functions, and include references or proofs for completeness.
Fact 35. The following facts about monotone operators hold true:
1. Given a convex function f(x) : X →R, its induced operator Φ = ∇f : X →X ∗is monotone.
2. Given a convex-concave function h(x, y) : X × Y →R, Φ(x, y) = (∇xh(x, y), −∇yh(x, y)) :
X × Y →X ∗× Y∗is monotone.
3. Given a convex function f, its induced operator Φ = ∇f is 1-strongly monotone with respect
to itself.
4. Monotonicity is preserved under addition: For any m, m′ ≥0, if Φ is m-strongly monotone
and Ψ is m′-strongly monotone with respect to convex r, then Φ + Ψ is (m + m′)-strongly
monotone with respect to r.
Proof. The ﬁrst two items are basic fact of convexity and minimax optimization [460]. For the third
item, we note that for any x, x′ ∈X
⟨Φ(x) −Φ(x′), x −x′⟩= ⟨∇f(x) −∇f(x′), x −x′⟩,
which satisﬁes 1-strong monotonicity with respect to f by deﬁnition.
For the fourth item, we note that for any m, m′ ≥0 and assumed Φ, Ψ,
⟨Φ(z) −Φ(z′), z −z′⟩≥m ⟨∇r(z) −∇r(z′), z −z′⟩,
⟨Ψ(z) −Ψ(z′), z −z′⟩≥m′ ⟨∇r(z) −∇r(z′), z −z′⟩,
=⇒⟨Φ(z) + Ψ(z) −(Φ(z′) + Ψ(z′)) , z −z′⟩≥(m + m′) ⟨∇r(z) −∇r(z′), z −z′⟩.
These facts about monotone operators ﬁnd usage in proving (relative) strong monotonicity of
our operators; see Lemma 11, 18 and 27 in the main paper.
The second fact bounds the smoothness of best-response function of some given convex-concave
function h; X × Y →R. We refer readers to Fact 1 of [533] for a complete proof.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
573
Fact 36 (Fact 1, [533]). Suppose h satisﬁes the blockwise-smoothness properties: for all u, v ∈X ×Y,
∥∇xh(u) −∇xh(v)∥≤Λxx ∥ux −vx∥+ Λxx ∥uy −vy∥,
∥∇yh(u) −∇yh(v)∥≤Λxx ∥ux −vx∥+ Λyy ∥uy −vy∥,
(A.9)
and suppose h is µx-strongly convex in x and µy-strongly concave in y. The best response function
hy(x) := maxy∈Y h(x, y) is µx-strongly convex and Λxx + (Λxx)2
µy
-smooth, and hx(y) := minx∈Y h(x, y)
is µy-strongly concave and Λyy + (Λxx)2
µx
-smooth.
We use this fact when converting radius bounds to duality gap bounds in Lemma 15 and 16.
A.10
Proofs for Section 2.9
A.10.1
Proofs for Section 2.9.2
Proposition 5 (Partial variance analysis of randomized mirror prox). Suppose (possibly random)
eΦ is deﬁned so that in each iteration s, for all u ∈Z and all ρ > 0, there exists a (possibly random)
point ¯ws ∈Z and a γ-strongly monotone operator Φ : Z →Z∗(with respect to r) such that
E
hD
eΦ(ws+1/2), ws+1/2 −w⋆
Ei
= E [⟨Φ( ¯ws), ¯ws −w⋆⟩] ,
E
hD
eΦ(ws+1/2) −eΦ(ws), ws+1/2 −ws+1
Ei
≤

λ0 + 1
ρ

E
h
V r
ws(ws+1/2) + V r
ws+1/2(ws+1)
i
+ ρλ1E

V r
w0(w⋆) + V r
¯
ws(w⋆)

,
(2.56)
where w⋆solves the VI in Φ. Then by setting
ρ ←
γ
5λ1
, λ ←λ0 + 1
ρ, T ←5λ
γ = 5λ0
γ
+ 25λ1
γ2 ,
in Algorithm 5, and returning ¯wσ for 0 ≤σ < S sampled uniformly at random,
E

V r
¯
wσ (w⋆)

≤1
2V r
w0(w⋆).
Proof. First, consider a single iteration 0 ≤s < S, and ﬁx the point ws in Algorithm 5. By the
optimality conditions on ws+1/2 and ws+1, we have
1
λ
D
eΦ(ws), ws+1/2 −ws+1
E
≤V r
ws(ws+1) −V r
ws+1/2(ws+1) −V r
ws(ws+1/2),
1
λ
D
eΦ(ws+1/2), ws+1 −w⋆
E
≤V r
ws(w⋆) −V r
ws+1(w⋆) −V r
ws(ws+1).

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
574
Summing the above, rearranging, and taking expectations yields
E
 1
λ ⟨Φ( ¯ws), ¯ws −w⋆⟩

= E
 1
λ
D
eΦ(ws+1/2), ws+1/2 −w⋆
E
≤E
h
V r
ws(w⋆) −V r
ws+1(w⋆)
i
+ E
 1
λ
D
eΦ(ws+1/2) −eΦ(ws), ws+1/2 −ws+1
E
−V r
ws(ws+1/2) + V r
ws+1/2(ws+1)

≤E
h
V r
ws(w⋆) −V r
ws+1(w⋆)
i
+ ρλ1
λ E

V r
w0(w⋆) + V r
¯
ws(w⋆)

.
In the last line we used the assumption (2.56). Since w⋆solves the VI in Φ, adding E 1
λ ⟨Φ(w⋆), w⋆−¯ws⟩
to the left-hand side above and applying strong monotonicity of g in r yields
E
 1
λV r
¯
ws(w⋆)

≤E
h
V r
ws(w⋆) −V r
ws+1(w⋆)
i
+ ρλ1
λ E

V r
w0(w⋆) + V r
¯
ws(w⋆)

.
Telescoping the above for 0 ≤s < S and using nonnegativity of Bregman divergences yields
(γ −ρλ1) E

1
T
X
0≤t<T
V r
¯
ws(w⋆)

≤
 λ
T + ρλ1

V r
w0(w⋆).
Substituting our choices of ¯ws, ρ, λ, and T yields the claim.
Lemma 25. Deﬁne {Φjkℓ, Φjkℓ′} : Z →Z∗as in (2.58), (2.59), and the random "aggregate point"
¯w(ℓ) as in (2.60). Then, for all u ∈Z, recalling the deﬁnition of Φ = Φmmfs-pd + γ(∇r −∇r(¯z))
from (2.55),
E [⟨Φjkℓ′(waux(jkℓ)), waux(jkℓ) −u⟩] = Eℓ∼r [⟨Φ( ¯w(ℓ)), ¯w(ℓ) −u⟩] .
Proof. We demonstrate this equality for the X and (X ∗)n blocks; the others (the Y and (Y∗)n
blocks) follow symmetrically. We will use the deﬁnitions of Φh and Φbilin from (2.54).
X block.
Fix ℓ∈[n]. We ﬁrst observe that
Eℓ′∼r
h
Φh
jkℓ′(waux(jkℓ))
xi
=

Φh( ¯w(ℓ))
x ,
Eℓ′∼r
hh
Φsep
jkℓ′(waux(jkℓ))
ixi
= (1 + γ) [∇r( ¯w(ℓ))]x −γ [∇r(¯z)]x .
Moreover, by expanding the expectation over j ∼p,
Ej∼p
hD
Φbilin
jkℓ′ (waux(jkℓ))
x , wx
aux(ℓ) −uxEi
=
*
1
n
X
j∈[n]
(wf∗
j + ∆x(j)), wx
aux(ℓ) −ux
+
=
D
Φbilin( ¯w(ℓ))
x , wx
aux(ℓ) −uxE
.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
575
Summing, we conclude that for ﬁxed ℓand taking expectations over j, k, ℓ′,
E

[Φjkℓ′(waux(jkℓ))]x , wx
aux(ℓ) −ux
=

[Φ( ¯w(ℓ))]x , wx
aux(ℓ) −ux
.
The conclusion for the X block follows by taking expectations over ℓ.
X ∗blocks.
Note that the [Φh
jkℓ′]f∗blocks are always zero. Next, for the [Φsep
jkℓ′]f∗component, by
expanding the expectation over j ∼p and taking advantage of sparsity, for any ℓ∈[n],
Ej∼p
h
Φsep
jkℓ′(waux(jkℓ))
if∗
, wf∗
aux(jkℓ) −uf∗
= (1 + γ)
X
j∈[n]
 1
n∇f ∗
j

w
f∗
j
aux

, w
f∗
j
aux −uf∗
j

−γ
X
j∈[n]
 1
n∇f ∗
j

¯zf∗
j

, w
f∗
j
aux −uaj

=
D
(1 + γ) [∇r( ¯w(ℓ))]f∗
−γ [∇r(¯z)]f∗
, ¯wf∗(ℓ) −uf∗E
.
Here, we recall f∗
j denotes the block corresponding to the jth copy of X ∗. Finally, for the [Φbilin
jkℓ′ ]f∗
component, ﬁx ℓ∈[n]. Expanding the expectation over j ∼p and taking advantage of sparsity,
Ej∼p
hD
Φbilin
jkℓ′ (waux(jkℓ))
f∗
, [waux(jkℓ)]f∗
−uf∗Ei
=
D
Φbilin( ¯w(ℓ))
f∗
, ¯wf∗(ℓ) −uf∗E
.
Summing, we conclude that for ﬁxed ℓand taking expectations over j, k, ℓ′,
E
D
[gjkℓ′(waux(jkℓ))]f∗
, [waux(jkℓ)]f∗
−uf∗E
=
D
[gtot( ¯w(ℓ))]f∗
, ¯wf∗(ℓ) −uf∗E
.
The conclusion for the X ∗blocks follows by taking expectations over ℓ.
Lemma 26. Lines 6 to 18 of Algorithm 9 implement Algorithm 5 on ({eΦ}, r) deﬁned in (2.58),
(2.59), (2.53), for σ iterations, and returns ¯wσ, following the deﬁnition (2.60). Each iteration s > 0
is implementable in O(1) gradient calls to some {fj, gk, hl}, and O(1) vector operations on X and
Y.
Proof. Let {ws, ws+1/2}0≤s≤σ be the iterates of Algorithm 5. We will inductively show that some
run of Lines 6 to 18 in Algorithm 9 preserves the invariants
ws =

wx
s, wy
s,

∇fi(wfis)
	
i∈[n] , {∇fi(wgis )}i∈[n]

,
ws+1/2 =

wx
s+1/2, wy
s+1/2,
n
∇fi(wfi
s+1/2)
o
i∈[n] ,
n
∇fi(wgi
s+1/2)
o
i∈[n]


APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
576
for all 0 ≤s ≤σ. Once we prove this claim, it is clear that Lines 6 to 18 in Algorithm 9 implements
Algorithm 5 and returns ¯wσ, upon recalling the deﬁnitions (2.58), (2.59), (2.53), and (2.60).
The base case of our induction follows from the way w0 is initialized in Line 18. Next, suppose
for some 0 ≤s ≤σ, our inductive claim holds. By the update in Line 9 of Algorithm 9, if j ∈[n]
was sampled in iteration s, using the ﬁrst item in Fact 1,
w
f∗
j
s+1/2 ←argminw
f∗
j ∈X ∗

1
nλpj

(1 + γ)wfjs −γ¯zfj −wx
s

, wf∗
j

+ V
f ∗
j
w
f∗
j
s

wf∗
j

= ∇fj

wfjs −
1
nλpj

(1 + γ) wfjs −γ¯zfj −wx
s

.
Similarly, by the update in Line 10, if k ∈[n] was sampled in iteration s,
wg∗
k
s+1/2 ←argminwg∗
k

1
nλqk
((1 + γ)wgk
s −γ¯zgk −wy
s) , wg∗
k

+ V g∗
k
w
g∗
k
s

wg∗
k

.
= ∇gk

wgk
s −
1
nλqk
((1 + γ) wgk
s −γ¯zgk −wy
s)

.
Hence, the updates to w
f∗
j
s+1/2 and wg∗
k
s+1/2 preserve our invariant, and all other wf∗
i
s+1/2, i ̸= j and
wg∗
i
s+1/2, i ̸= k do not change by sparsity of Φjkℓ. Analogously the updates to each wf∗
i
s+1 and wg∗
i
s+1
preserve our invariant. Finally, in every iteration s > 0, the updates to wxx
s+1/2 and wxx
s+1 only require
evaluating O(1) new gradients each, by 1-sparsity of the dual block updates.
A.10.2
Proofs for Section 2.9.3
Lemma 28. Deﬁne {Φjkℓ, Φjkℓ′} : Z →Z∗as in (2.58), (2.59), and deﬁne r : Z →R as in (2.53).
Letting w+(jkℓℓ′) be ws+1 in Algorithm 9 if j, k, ℓ, ℓ′ were sampled in iteration s, deﬁning
Φfg
jkℓ(w) := Φsep
jkℓ(w) + Φbilin
jkℓ(w),
Φfg
jkℓ′(waux(jkℓ)) := Φsep
jkℓ′(waux(jkℓ)) + Φbilin
jkℓ′ (waux(jkℓ)),
we have
E
hD
Φfg
jkℓ′(waux(jkℓ)) −Φfg
jkℓ(w), waux(jkℓ) −w+(jkℓℓ′)
Ei
≤λfgE
h
V r
w (waux(jkℓ)) + V r
waux(jkℓ) (w+(jkℓℓ′))
i
,
for
λfg = 2n(1 + γ) +
P
i∈[n]
p
Lx
i
√nµx
+
P
i∈[n]
p
Ly
i
√nµy
.
Proof. This is immediate upon combining the following Lemmas 225 and 226.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
577
Lemma 225. Following notation of Lemma 28, for λsep := 2n(1 + γ), we have
E
hD
Φsep
jkℓ′(waux(jkℓ)) −Φsep
jkℓ(w), waux(jkℓ) −w+(jkℓℓ′)
Ei
≤λsepE
h
V r
w (waux(jkℓ)) + V r
waux(jkℓ) (w+(jkℓℓ′))
i
.
Proof. The proof is similar to (part of) the proof of Lemma 21. We claim that for any j, k, ℓ, ℓ′,
D
Φsep
jkℓ′(waux(jkℓ)) −Φsep
jkℓ(w), waux(jkℓ) −w+(jkℓℓ′)
E
≤λsep 
V r
w (waux(jkℓ)) + V r
waux(jkℓ) (w+(jkℓℓ′))

.
Fix j, k, ℓ, ℓ′. Since all pj and qk are lower bounded by
1
2n by assumption, applying Lemma 1 to the
relevant blocks of r and nonnegativity of Bregman divergences proves the above display.
Lemma 226. Following notation of Lemma 28, for
λcross :=
2 P
i∈[n]
p
Lx
i
√nµx
+
2 P
i∈[n]
p
Ly
i
√nµy
,
we have
E

Φbilin
jkℓ′ (waux(jkℓ)) −Φbilin
jkℓ(w), waux(jkℓ) −w+(jkℓℓ′)

≤λcrossE
h
V r
w (waux(jkℓ)) + V r
waux(jkℓ) (w+(jkℓℓ′))
i
.
Proof. The proof is similar to (part of) the proof of Lemma 21. We claim that for any j, k, ℓ, ℓ′,

Φbilin
jkℓ′ (waux(jkℓ)) −Φbilin
jkℓ(w), waux(jkℓ) −w+(jkℓℓ′)

≤λcross 
V r
w (waux(jkℓ)) + V r
waux(jkℓ) (w+(jkℓℓ′))

.
Fix j, k, ℓ, ℓ′. By applying Item 1 in Lemma 13 with f = fj, α = (Lx
jµx)−1
2 ,
Ej
 1
npj
D
w
f∗
j
aux −wf∗
j , wx
aux(ℓ) −wx
+(jkℓℓ′)
E
+
1
npj
D
wx −wx
aux(ℓ), w
f∗
j
aux −w
f∗
j
+ (jkℓℓ′)
E
≤
2 P
i∈[n]
p
Lx
i
√nµx

V r
w (waux(jkℓ)) + V r
waux(jkℓ) (w+(jkℓℓ′))

.
Similarly, by applying Item 1 in Lemma 13 with f = gk, α = (Ly
kµy)−1
2 ,
Ej
 1
nqk
D
wg∗
k
aux −wg∗
k , wy
aux(ℓ) −wy
+(jkℓℓ′)
E
+
1
nqk
D
wy −wy
aux(ℓ), wg∗
k
aux −wg∗
k
+ (jkℓℓ′)
E
≤
2 P
i∈[n]
p
Ly
i
√nµy

V r
w (waux(jkℓ)) + V r
waux(jkℓ) (w+(jkℓℓ′))

.
Summing the above displays yields the desired claim.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
578
Lemma 29. Following notation of Lemma 28, and recalling the deﬁnition (2.61), for
λ1 := 32(λh)2,
where we deﬁne
λh := 1
n
X
i∈[n]
Λxx
i
µx +
Λxx
i
√µxµy + Λyy
i
µy

.
(2.61)
we have for any ρ > 0,
E

Φh
jkℓ′(waux(jkℓ)) −Φh
jkℓ(w), waux(jkℓ) −w+(jkℓℓ′)

≤

2λh + 1
ρ

E
h
V r
w (waux(jkℓ)) + V r
waux(jkℓ) (w+(jkℓℓ′))
i
+ ρλ1E
h
V r
w0(w⋆) + V r
¯
w(ℓ)(w⋆)
i
.
(2.62)
Proof. The proof is similar to (part of) the proof of Lemma 14. Fix j, k, ℓ, ℓ′. By deﬁnition,

Φh
jkℓ′(waux(jkℓ)) −Φh
jkℓ(w)
xx
=
1
nrℓ′ (∇xhℓ′(wx
aux(ℓ), wy
aux(ℓ)) −∇xhℓ′(wx
0, wy
0), ∇yhℓ′(wx
0, wy
0) −∇yhℓ′(wx
aux(ℓ), wy
aux(ℓ)))
−1
nrℓ
(∇xhℓ(wx, wy) −∇xhℓ(wx
0, wy
0), ∇yhℓ(wx
0, wy
0) −∇yhℓ(wx, wy)) .
We decompose the x blocks of the left-hand side of (2.62) as
D
Φh
jkℓ′(waux(jkℓ)) −Φh
jkℓ(w)
x , wx
aux(ℓ) −wx
+(jkℓℓ′)
E
= 1 + 2 + 3 ,
1 :=
1
nrℓ′

∇xhℓ′(wx
aux(ℓ), wy
aux(ℓ)) −∇xhℓ′(wx
0, wy
0), wx
aux(ℓ) −wx
+(jkℓℓ′)

,
2 :=
1
nrℓ

∇xhℓ(wx
0, wy
0) −∇xhℓ(wx
aux(ℓ), wy
aux(ℓ)), wx
aux(ℓ) −wx
+(jkℓℓ′)

,
3 :=
1
nrℓ

∇xhℓ(wx
aux(ℓ), wy
aux(ℓ)) −∇xhℓ(wx, wy), wx
aux(ℓ) −wx
+(jkℓℓ′)

.
By the Lipschitzness bounds in (2.51) and Young's inequality,
1 ≤
1
nrℓ′ ∥∇xhℓ′(wx
aux(ℓ), wy
aux(ℓ)) −∇xhℓ′(wx
0, wy
0)∥
wx
aux(ℓ) −wx
+(jkℓℓ′)

≤
1
nrℓ′
 Λxx
ℓ′ ∥wx
aux(ℓ) −wx
0∥
wx
aux(ℓ) −wx
+(jkℓℓ′)
 + Λxx
ℓ′ ∥wy
aux(ℓ) −wy
0∥
wx
aux(ℓ) −wx
+(jkℓℓ′)

≤2ρ(Λxx
ℓ′ )2
µxn2r2
ℓ′ ∥wx
aux(ℓ) −wx
0∥2 + 2ρ(Λxx
ℓ′ )2
µxn2r2
ℓ′ ∥wy
aux(ℓ) −wy
0∥2 + µx
4ρ
wx
aux(ℓ) −wx
+(jkℓℓ′)
2 .

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
579
Symmetrically, we bound
2 ≤2ρ(Λxx
ℓ)2
µxn2r2
ℓ
∥wx
aux(ℓ) −wx
0∥2 + 2ρ(Λxx
ℓ)2
µxn2r2
ℓ
∥wy
aux(ℓ) −wy
0∥2 + µx
4ρ
wx
aux(ℓ) −wx
+(jkℓℓ′)
2 .
Finally, we have
3 ≤
1
nrℓ
∥∇xhℓ(wx
aux(ℓ), wy
aux(ℓ)) −∇xhℓ(wx, wy)∥
wx
aux(ℓ) −wx
+(jkℓℓ′)

≤
1
nrℓ
 Λxx
ℓ∥wx
aux(ℓ) −wx∥
wx
aux(ℓ) −wx
+(jkℓℓ′)
 + Λxx
ℓ∥wy
aux(ℓ) −wy∥
wx
aux(ℓ) −wx
+(jkℓℓ′)

≤
1
nrℓ
Λxx
ℓ
µx
µx
2 ∥wx
aux(ℓ) −wx∥2 + µx
2
wx
aux(ℓ) −wx
+(jkℓℓ′)
2

+
1
nrℓ

Λxx
ℓ
√µxµy
µy
2 ∥wy
aux(ℓ) −wy∥2 + µx
2
wx
aux(ℓ) −wx
+(jkℓℓ′)
2

.
We may similarly decompose the y blocks of the left-hand side of (2.62) as
4 + 5 + 6 , where
symmetrically, we have
4 ≤2ρ(Λyy
ℓ′ )2
µyn2r2
ℓ′ ∥wy
aux(ℓ) −wy
0∥2 + 2ρ(Λxx
ℓ′ )2
µyn2r2
ℓ′ ∥wx
aux(ℓ) −wx
0∥2 + µy
4ρ
wy
aux(ℓ) −wy
+(jkℓℓ′)
2 ,
5 ≤2ρ(Λyy
ℓ)2
µyn2r2
ℓ
∥wy
aux(ℓ) −wy
0∥2 + 2ρ(Λxx
ℓ)2
µyn2r2
ℓ
∥wx
aux(ℓ) −wx
0∥2 + µy
4ρ
wy
aux(ℓ) −wy
+(jkℓℓ′)
2 ,
6 ≤
1
nrℓ
Λyy
ℓ
µy
µy
2 ∥wy
aux(ℓ) −wy∥2 + µy
2
wy
aux(ℓ) −wy
+(jkℓℓ′)
2

+
1
nrℓ

Λxx
ℓ
√µxµy
µx
2 ∥wx
aux(ℓ) −wx∥2 + µy
2
wy
aux(ℓ) −wy
+(jkℓℓ′)
2

.
We ﬁrst observe that by deﬁnition of r and nonnegativity of Bregman divergences,
3 + 6 ≤
1
nrℓ
Λxx
ℓ
µx +
Λxx
ℓ
√µxµy + Λyy
ℓ
µy
 
V r
w(waux(jkℓ)) + V r
waux(jkℓ)(w+(jkℓℓ′))

≤2λh 
V r
w(waux(jkℓ)) + V r
waux(jkℓ)(w+(jkℓℓ′))

.
Moreover, since by the triangle inequality and (a + b)2 ≤2a2 + 2b2,
∥wx
aux(ℓ) −wx
0∥2 ≤2 ∥wx
aux(ℓ) −wx
⋆∥2 + 2 ∥wx
0 −wx
⋆∥2 ,
∥wy
aux(ℓ) −wy
0∥2 ≤2 ∥wy
aux(ℓ) −wy
⋆∥2 + 2 ∥wy
0 −wy
⋆∥2 ,

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
580
we have by deﬁnition of r and λ1,
1 + 2 + 4 + 5 ≤1
ρ

V r
w(waux(jkℓ)) + V r
waux(jkℓ)(w+(jkℓℓ′))

+ ρλ1

V r
w0(w⋆) + V r
¯
w(ℓ)(w⋆)

.
Summing the above displays and taking expectations yields the claim.
A.10.3
Proofs for Section 2.9.4
Proposition 7. Consider a single iteration 0 ≤t < T of Algorithm 8, and let z⋆is the saddle point
to Fmmfs-pd (deﬁned in (2.52)). Setting S as in (2.63) and
N := O (log (γλ)) ,
(2.64)
for an appropriately large constant in our implementation of Algorithm 9 and λ as in (2.63), we
have
EV r
zt+1(z⋆) ≤
4γ
1 + 4γ V r
zt(z⋆).
Proof. Fix an iteration t ∈[T] of Algorithm 8, and let z⋆
t+1 be the exact solution to the VI in
Φmmfs-pd + γ∇r −∇r(zt).
By the guarantee of Proposition 6, after the stated number of NS
iterations in Algorithm 9 (for an appropriately large constant), we obtain a point zt+1 such that
E
h
V r
zt+1(z⋆
t+1)
i
≤
1
1 + 3γeκV r
zt(ˆzt+1), where eκ := 10
X
i∈[n]
Lx
i + Λxx
i
µx
+ Ly
i + Λyy
i
µy
+
Λxx
i
√µxµy
2
.
(A.10)
The optimality condition on z⋆
t+1 yields

Φmmfs-pd  z⋆
t+1

, z⋆
t+1 −z⋆

≤γV r
zt (z⋆) −γV r
z⋆
t+1 (z⋆) −γVzt
 z⋆
t+1

.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
581
Rearranging terms then gives:

Φmmfs-pd (zt+1) , zt+1 −z⋆

≤γV r
zt (z⋆) −γV r
zt+1 (z⋆) −γV r
zt
 z⋆
t+1

+ γ

V r
zt+1 (z⋆) −V r
z⋆
t+1 (z⋆)

+

Φmmfs-pd (zt+1) −Φmmfs-pd  z⋆
t+1

, z⋆
t+1 −z⋆

+

Φmmfs-pd (zt+1) , zt+1 −z⋆
t+1

= γV r
zt (z⋆) −γV r
zt+1 (z⋆) −γV r
zt
 z⋆
t+1

+ γV r
zt+1
 z⋆
t+1

+ γ

∇r (zt+1) −∇r
 z⋆
t+1

, z⋆
t+1 −z⋆

+

Φmmfs-pd (zt+1) −Φmmfs-pd  z⋆
t+1

, z⋆
t+1 −z⋆

+

Φmmfs-pd (zt+1) , zt+1 −z⋆
t+1

≤γV r
zt (z⋆) −γV r
zt+1 (z⋆) −γV r
zt
 z⋆
t+1

+ γV r
zt+1
 z⋆
t+1

+ γ

∇r (zt+1) −∇r
 z⋆
t+1

, zt+1 −z⋆

+

Φmmfs-pd (zt+1) −Φmmfs-pd  z⋆
t+1

, zt+1 −z⋆

+

Φmmfs-pd (zt+1) −Φmmfs-pd (z⋆) , zt+1 −z⋆
t+1

.
(A.11)
In the only equality, we used the identity (2.10).
The last inequality used monotonicity of the
operators γ∇r and Φmmfs-pd, as well as Φmmfs-pd(z⋆) = 0 because it is an unconstrained minimax
optimization problem. In the remainder of the proof, we will bound the last three lines of (A.11).
First, for any α > 0, we bound:

∇r (zt+1) −∇r
 z⋆
t+1

, zt+1 −z⋆

= µx 
zx
t+1 −(z⋆
t+1)x, zx
t+1 −zx
⋆

+ µy 
zy
t+1 −(z⋆
t+1)y, zy
t+1 −zy
⋆

+ 1
n
X
i∈[n]
D
∇f ∗
i (zf∗
i
t+1) −∇f ∗
i ((z⋆
t+1)f∗
i ), zf∗
i
t+1 −zf∗
i⋆
E
+ 1
n
X
i∈[n]
D
∇g∗
i (zg∗
i
t+1) −∇g∗
i ((z⋆
t+1)g∗
i ), zg∗
i
t+1 −zg∗
i⋆
E
≤2αµx zx
t+1 −(z⋆
t+1)x2 + µx
8α
zx
t+1 −zx
⋆
2
+ 2αµy zy
t+1 −(z⋆
t+1)y2 + µy
8α
zy
t+1 −zy
⋆
2
+ 1
n
X
i∈[n]
2αLx
i
(µx)2
zf∗
i
t+1 −(z⋆
t+1)f∗
i

2
+
1
8αLx
i
zf∗
i
t+1 −zf∗
i⋆

2
+ 1
n
X
i∈[n]
2αLy
i
(µy)2
zg∗
i
t+1 −(z⋆
t+1)g∗
i

2
+
1
8αLy
i
zg∗
i
t+1 −zg∗
i⋆

2
≤1
4αV r
zt+1(z⋆) + eκαV r
zt+1(z⋆
t+1).
(A.12)
The equality used the deﬁnition of r in (2.53). The ﬁrst inequality used Young's and Cauchy-Schwarz

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
582
on the X × Y blocks, as well as
1
µx
i -smoothness of the f ∗
i from Assumption 3 and Item 4 in Fact 1
(and similar bounds on each g∗
i ). The last inequality used strong convexity of each piece of r.
Similarly, by deﬁnition of Φmmfs-pd (2.54) which we denote for Φ for brevity in the following:

Φ (zt+1) −Φ
 z⋆
t+1

, zt+1 −z⋆

≤1
8V r
zt+1(z⋆) + 2eκV r
zt+1(z⋆
t+1)
+ 1
n
X
i∈[n]

∇xhi(zx
t+1, zy
t+1) −∇xhi((z⋆
t+1)x, (z⋆
t+1)y), zx
t+1 −zx
⋆

+ 1
n
X
i∈[n]

∇yhi(zx
t+1, zy
t+1) −∇yhi((z⋆
t+1)x, (z⋆
t+1)y), zy
t+1 −zy
⋆

+ 1
n
X
i∈[n]
D
zf∗
i
t+1 −(z⋆
t+1)f∗
i , zx
t+1 −zx
⋆
E
+
D
zg∗
i
t+1 −(z⋆
t+1)g∗
i , zy
t+1 −zy
⋆
E
−1
n
X
i∈[n]
D
zx
t+1 −(z⋆
t+1)x, zf∗
i
t+1 −zf∗
i⋆
E
+
D
zy
t+1 −(z⋆
t+1)y, zg∗
i
t+1 −zg∗
i⋆
E
where we used (A.12) to bound the ∇r terms. Consequently,

Φ (zt+1) −Φ
 z⋆
t+1

, zt+1 −z⋆

≤1
8V r
zt+1(z⋆) + 2eκV r
zt+1(z⋆
t+1)
+ 1
n
X
i∈[n]
µx
16Vzx
t+1(zx
⋆) + µy
16Vzy
t+1(zy
⋆)

+ 1
n
X
i∈[n]

16
(Λxx
i )2
µx
+ (Λxx
i )2
µy

Vzx
t+1((z⋆
t+1)x)

+ 1
n
X
i∈[n]

16
(Λxx
i )2
µx
+ (Λyy
i )2
µy

Vzy
t+1((z⋆
t+1)y)

+ 1
n
X
i∈[n]
µx
16Vzx
t+1(zx
⋆) + µy
16Vzy
t+1(zy
⋆)

+ 1
n
X
i∈[n]
 8
µx
zf∗
i
t+1 −(z⋆
t+1)f∗
i

2
+ 8
µy
zg∗
i
t+1 −(z⋆
t+1)g∗
i

2
+ 1
n
X
i∈[n]
1
8V f ∗
i
z
f∗
i
t+1

zf∗
i⋆

+ 1
8V g∗
i
z
g∗
i
t+1

zg∗
i⋆

+ 1
n
X
i∈[n]

8Lx
iVzx
t+1((z⋆
t+1)x) + 8Ly
iVzy
t+1((z⋆
t+1)y)

≤1
4V r
zt+1(z⋆) + eκV r
zt+1(z⋆
t+1).
(A.13)
In the ﬁrst inequality, we used Cauchy-Schwarz, Young's, and our various smoothness assumptions
(as well as strong convexity of each f ∗
i and g∗
i ). The last inequality used strong convexity of each
piece of r.

APPENDIX A. DEFERRED PROOFS FROM CHAPTER 2
583
For the last term, by a similar argument as in the previous bounds, we have

Φ(zt+1) −Φ(z⋆), zt+1 −z⋆
t+1

≤1
4V r
zt+1(z⋆) + eκV r
zt+1(z⋆
t+1).
(A.14)
Plugging the inequalities (A.12) with α = γ, (A.13) and (A.14) back into (A.11), this implies

Φmmfs-pd (zt+1) , zt+1 −z⋆

≤γV r
zt (z⋆) −γV r
zt+1 (z⋆) −γV r
zt
 z⋆
t+1

+ γV r
zt+1
 z⋆
t+1

(A.15)
+ 3
4V r
zt+1(z⋆) + 3eκγ2V r
zt+1(z⋆
t+1).
(A.16)
By strong monotonicity of Φmmfs-pd with respect to r, we also have

Φmmfs-pd (zt+1) , zt+1 −z⋆

≥

Φmmfs-pd (zt+1) −Φmmfs-pd (z⋆) , zt+1 −z⋆

≥V r
zt+1 (z⋆) .
(A.17)
Combining (A.16) and (A.17) with the assumption (A.10), and taking expectations, we obtain
1
4 + γ

EV r
zt+1(z⋆) ≤γV r
zt(z⋆) =⇒EV r
zt+1(z⋆) ≤
4γ
1 + 4γ V r
zt(z⋆).

Appendix B
Deferred proofs from Chapter 3
B.1
Deferred proofs from Section 3.2
Proof of Proposition 8. It is clear that our choices of X, Y are compact and convex, and that the
local norms we deﬁned are indeed norms (in all cases, they are quadratic norms). Validity of our
choices of Θ follow from the well-known facts that for x ∈∆n, the entropy function P
j∈[n] xj log xj
is convex with range log n, and that for x ∈Bn, 1
2 ∥x∥2
2 is convex with range 1
2.
In the Euclidean case we have that Vx(x′) = 1
2 ∥x −x′∥2
2 and (3.18) follows from the Cauchy-
Schwarz and Young inequalities:
⟨γ, x′ −x⟩≤1
2 ∥γ∥2
2 + 1
2 ∥x −x′∥2
2 .
Similarly, for the simplex we have that entropy is 1-strongly-convex with respect to ∥·∥1 and therefore
Vy(y′) ≥1
2 ∥y −y′∥2
1, and we obtain (3.18) from the H¨older and Young inequalities,
⟨γ, y′ −y⟩≤1
2 ∥γ∥2
∞+ 1
2 ∥y −y′∥2
1 ,
where we note that ∥γ∥∞= ∥γ∥∗in this case.
Finally, clip(·) is not the identity only when the corresponding domain is the simplex, in which
case entropy satisﬁes the local norms bound (see the following Lemma 227),
⟨γ, y −y′⟩−Vy(y′) ≤
X
i∈[m]
γ2
i yi for all y, y′ ∈∆m and γ ∈Rm such that ∥γ∥∞≤1.
Noting that ∥clip(γ)∥∞≤1 and that ∥clip(γ)∥y ≤∥γ∥y for all y ∈∆m, we have the desired local
584

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
585
bound (3.19). Finally, for every coordinate i ∈[m] we have
|γi −[clip(γ)]i| = ||γi| −1| I{|γi|>1} ≤|γi| I{|γi|>1} ≤|γi|2.
Consequently, | ⟨γ −clip(γ), z⟩| ≤P
i∈[m] γ2
i zi, giving the distortion bound (3.20).
We now state and give a proof of Lemma 227, our local norm bound. For the remainder of this
subsection, let Y be the m dimensional simplex ∆m, and let r(y) = Pm
i=1 yi log yi be the negative
entropy distance generating function. The corresponding Bregman divergence is the KL divergence,
which is well-deﬁned for any y, y′ ∈Rm
≥0 and has the form
Vy(y′) =
X
i∈[m]

y′
i log y′
i
yi
+ yi −y′
i

=
Z 1
0
dt
Z t
0
X
i∈[m]
(yi −y′
i)2
(1 −τ)yi + τy′
i
dτ.
(B.1)
In the literature, "local norms" regret analysis [474] relies on the fact that r∗(γ) = log(P
i eγi) (the
conjugate of negative entropy in the simplex) is locally smooth with respect to a Euclidean norm
weighted by ∇r∗(γ) =
eγ
∥eγ∥1 . More precisely, the Bregman divergence V ∗
γ (γ′) = r∗(γ′) −r∗(γ) −
⟨∇r∗(γ), γ′ −γ⟩satisﬁes
V ∗
γ (γ + δ) ≤∥δ∥2
∇r∗(γ) :=
X
i
[∇r∗(γ)]i · δ2
i
whenever δi ≤1.79 ∀i.
(B.2)
Below, we state this bound in a form that is directly applicable to our analysis.
Lemma 227. Let y, y′ ∈∆m and δ ∈Rm. If δ satisﬁes δi ≤1.79 for all i ∈[m] then the KL
divergence Vy(y′) satisﬁes
⟨δ, y′ −y⟩−Vy(y′) ≤∥δ∥2
y :=
X
i∈[m]
yiδ2
i
Proof. It suﬃces to consider y in the relative interior of the simplex where r is diﬀerentiable; the
ﬁnal result will hold for any y in the simplex by continuity. Recall the following general facts about
convex conjugates:

γ′, y′
−r(y′) ≤r∗(γ′) for any γ′ ∈Rm, y = ∇r∗(∇r(y)) and r∗(∇r(y)) =
⟨∇r(y), y⟩−r(y). Therefore, we have for all y′ ∈∆m,
⟨δ, y′ −y⟩−Vy(y′) = ⟨∇r(y) + δ, y′⟩−r(y′) −[⟨∇r(y), y⟩−r(y)] −⟨y, δ⟩
≤r∗(∇r(y) + δ) −r∗(∇r(y)) −⟨∇r∗(∇r(y)), δ⟩= V ∗
∇r(y)
 ∇r(y) + δ

.
The result follows from (B.2) with γ = ∇r(y), recalling again that y = ∇r∗(∇r(y)). For completeness

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
586
we prove (B.2) below, following [474]. We have
r∗(γ + δ) −r∗(γ) = log
 P
i∈[m] eγi+δi
P
i∈[m] eγi
!
(i)
≤log
 
1 +
P
i∈[m] eγi(δi + δ2
i )
P
i∈[m] eγi
!
= log(1 +

∇r∗(γ), δ + δ2
)
(ii)
≤⟨∇r∗(γ), δ⟩+

∇r∗(γ), δ2
,
where (i) follows from ex ≤1 + x + x2 for all x ≤1.79 and (ii) follows from log(1 + x) ≤x for all x.
Therefore,
V ∗
γ (γ + δ) = r∗(γ + δ) −r∗(γ) −⟨∇r∗(γ), δ⟩≤

∇r∗(γ), δ2
= ∥δ∥2
∇r∗(γ) ,
completing the proof.
B.2
Deferred proofs from Section 3.3
B.2.1
Proof of Proposition 9
In this section, we provide a convergence result for mirror descent under local norms. We require
the following well-known regret bound for mirror descent.
Lemma 228. Let Q : Z →R be convex, let T ∈N, z0 ∈Z and γ0, γ1, . . . , γT ∈Z∗. The sequence
z1, . . . , zT deﬁned by
zt = argminz∈Z

⟨γt−1, z⟩+ Q(z) + Vzt−1(z)
	
satisﬁes for all u ∈Z (denoting zT +1 := u),
T
X
t=0
⟨γt, zt −u⟩+
T
X
t=1
⟨∇Q(zt), zt −u⟩≤Vz0(u) +
T
X
t=0
{⟨γt, zt −zt+1⟩−Vzt(zt+1)}.
(B.3)
Proof. Fix u ≡zT +1 ∈Z. We note that by deﬁnition zt is the solution of a convex optimization
problem with (sub)gradient γt−1 + ∇Q(·) + ∇Vzt−1(·), and therefore by the ﬁrst-order optimality
condition [271] satisﬁes

γt−1 + ∇Q(zt) + ∇Vzt−1(zt), zt −zT +1

≤0.
By the three-point equality of Bregman divergences we have −

∇Vzt−1(zt), zt −zT +1

= Vzt−1(zT +1)−
Vzt(zT +1) −Vzt−1(zt). Substituting and summing over t ∈[T] gives
T
X
t=1
⟨γt−1 + ∇Q(zt), zt −zT +1⟩≤Vz0(zT +1) −
T
X
t=0
Vzt(zt+1).

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
587
Rearranging the LHS and adding ⟨γT , zT −zT +1⟩to both sides of the inequality gives
T
X
t=1
⟨γt + ∇Q(zt), zt −zT +1⟩≤Vz0(zT +1) +
T
X
t=0
{⟨γt, zt −zt+1⟩−Vzt(zt+1)} ,
which is the bound stated in the lemma.
The proposition follows from this regret bound, the properties of the local norm setup, and the
"ghost iterate" argument due to [416].
Proposition 9. Let (Z, ∥·∥·, r, Θ, clip) be a local norm setup, let L, ϵ > 0, and let ˜g be an L-local
estimator. Then, for η ≤
ϵ
9L2 and T ≥6Θ
ηϵ ≥54L2Θ
ϵ2
, Algorithm 10 outputs a point ¯z such that
EGap(¯z) ≤E
"
sup
u∈Z
1
T + 1
T
X
t=0
⟨g(zt), zt −u⟩
#
≤ϵ.
Proof. Deﬁning
˜∆t := g(zt) −1
η clip(η˜g(zt))
and the ghost iterates
st = argmins∈Z
1
2η ˜∆t−1, s

+ Vst−1(s)

with s0 = w0,
we rearrange the regret as
η
T
X
t=0
⟨g(zt), zt −u⟩≤
T
X
t=0
⟨clip(η˜g(zt)), zt −u⟩+
T
X
t=0
D
η ˜∆t, st −u
E
+
T
X
t=0
D
η ˜∆t, zt −st
E
,
(B.4)
and bound each term in turn.
We ﬁrst apply Lemma 228 with Q = 0 and γt = clip(η˜g(zt)), using (3.19) to conclude that
T
X
t=0
⟨clip(η˜g(zt)), zt −u⟩≤Vz0(u) + η2
T
X
t=0
∥˜g(zt)∥2
zt , for all u ∈Z.
(B.5)
Next, we apply Lemma 228 again, this time with γt = 1
2η ˜∆t, to obtain the regret bound
T
X
t=0

η ˜∆t, st −u

≤2Vz0(u) +
T
X
t=0
nD
η ˜∆t, st −st+1
E
−2Vst(st+1)
o
≤2Vz0(u) + η2
T
X
t=0

∥˜g(zt)∥2
st + 1
2 ∥g(zt)∥2
∗

,
(B.6)

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
588
for all u ∈Z, where we used
D
η ˜∆t, st −st+1
E
−2Vst(st+1) ≤⟨ηg(zt), st −st+1⟩−Vst(st+1)
+ | ⟨clip(η˜g(zt)), st −st+1⟩| −Vst(st+1),
and then appealed to the bounds (3.18) and (3.19) in the deﬁnition of the local norm setup. Now,
substituting (B.5) and (B.6) into (B.4), maximizing over u, and taking an expectation, we obtain
E sup
u∈Z
T
X
t=0
⟨ηg(zt), zt −u⟩≤3Θ + η2E
T
X
t=0
n
∥˜g(zt)∥2
zt + ∥˜g(zt)∥2
st + 1
2 ∥g(zt)∥2
∗
o
+ E
T
X
t=0
D
η ˜∆t, zt −st
E
.
(B.7)
To bound the last term we use the fact that g(zt) = E [˜g(zt) | zt, st] (which follows from the ﬁrst
part of Deﬁnition 4). We then write
E
D
η ˜∆t, zt −st
E ≤E |⟨η˜g(zt) −clip(η˜g(zt)), zt −st⟩| ≤η2 ∥˜g(zt)∥2
zt + η2 ∥˜g(zt)∥2
st ,
(B.8)
where the ﬁrst inequality is by Jensen's inequality, and the last is due to the property (3.20) of the
local norm setup. Substituting (B.8) into (B.7), we obtain
E sup
u∈Z
T
X
t=0
⟨ηg(zt), zt −u⟩≤3Θ + η2E
T
X
t=0

2 ∥˜g(zt)∥2
zt + 2 ∥˜g(zt)∥2
st + 1
2 ∥g(zt)∥2
∗

.
Finally, using the second moment bound of local gradient estimator (Deﬁnition 4) and its consequence
Lemma 30, we may bound each of the expected squared norm terms by L2. Dividing through by
η(T + 1) gives
E sup
u∈Z
"
1
T + 1
T
X
t=0
⟨g(zt), zt −u⟩
#
≤
3Θ
η(T + 1) + 9ηL2
2
.
Our choices η =
ϵ
9L2 and T ≥6Θ
ηϵ imply that the right hand side is at most ϵ, as required.
B.2.2
Proof of Proposition 10
Proposition 10. Let O be an (α, εinner)-relaxed proximal oracle with respect to gradient mapping
g, distance-generating r with range at most Θ and some εinner ≤εouter. Let z1/2, z3/2, . . . , zK−1/2 be
iterates of Algorithm 11 and let ¯zK be its output. Then
E Gap(¯zK) ≤E max
u∈Z
1
K
K
X
k=1

g(zk−1/2), zk−1/2 −u

≤αΘ
K + εouter.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
589
Proof. For some iteration k, we have by the optimality conditions on z⋆
k that

g(zk−1/2), z⋆
k −u

≤α
 Vzk−1(u) −Vz⋆
k(u) −Vzk−1 (z⋆
k)

∀u ∈Z.
Summing, writing

g(zk−1/2), z⋆
k −u

=

g(zk−1/2), zk−1/2 −u

−

g(zk−1/2), zk−1/2 −z⋆
k

, and re-
arranging yields
K
X
k=1

g(zk−1/2), zk−1/2 −u

≤αVz0(u) +
K
X
k=1
α
 Vzk(u) −Vz⋆
k(u)

+
K
X
k=1
 
g(zk−1/2), zk−1/2 −z⋆
k

−αVzk−1 (z⋆
k)

,
(B.9)
for all u ∈Z. Since z0 minimizes r, the ﬁrst term is bounded by Vz0(u) ≤r(u) −r(z0) ≤Θ. The
second term is bounded by the deﬁnition of zk in Algorithm 11:
K
X
k=1
α
 Vzk(u) −Vz⋆
k(u)

≤K(εouter −εinner).
Thus, maximizing (B.9) over u and then taking an expectation yields
E max
u∈Z
K
X
k=1

g(zk−1/2), zk−1/2 −u

≤αΘ + K(εouter −εinner)
+
K
X
k=1
E

g(zk−1/2), zk−1/2 −z⋆
k

−αVzk−1 (z⋆
k)

.
Finally, by Deﬁnition 6, E

g(zk−1/2), zk−1/2 −z⋆
k

−αVzk−1(z⋆
k)

≤εinner for every k, and the result
follows by dividing by K.
B.2.3
Proof of Proposition 11
We provide a convergence result for the variance-reduced stochastic mirror descent scheme in Al-
gorithm 12. We ﬁrst state the following helper bound which is an application of Lemma 230. It
is immediate from the variance bound of local-centered estimators (Property 2 of Deﬁnition 5) and
the fact that all local norms (whether the domains are balls or simplices) are quadratic.
Lemma 229. For any w ∈Z, (L, ϵ)-centered-local estimator ˜gw0 satisﬁes
E ∥˜gw0(z) −g(z)∥2
w ≤L2Vw0(z).
Lemma 230. Let ∥·∥D be a quadratic norm in a diagonal matrix, e.g. for some D = diag(d) and

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
590
d ≥0 entrywise, let ∥x∥2
D = P dix2
i . Then, if X is a random vector, we have
E ∥X −E[X]∥2
D ≤E ∥X∥2
D .
Proof. This follows from the deﬁnition of variance:
E ∥X −E[X]∥2
D = E ∥X∥2
D −∥E[X]∥2
D ≤E ∥X∥2
D .
Proposition 11. Let (Z, ∥·∥·, r, Θ, clip) be any local norm setup. Let w0 ∈Z, α ≥εinner > 0,
and ˜gw0 be an L-centered-local estimator for some L ≥α.
Assume the domain is bounded by
maxz∈Z ∥z∥≤D, that g is L-Lipschitz, i.e. ∥g(z) −g(z′)∥∗≤L ∥z −z′∥, that g is LD-bounded, i.e.
maxz∈Z ∥g(z)∥∗≤LD, and that ˆw0 = w0. Then, for η =
α
10L2 , T ≥
6
ηα ≥60L2
α2 , and ϕ = εinner
6
,
Algorithm 12 outputs a point ˆw ∈Z such that
E max
u∈Z [⟨g( ˜w), ˜w −u⟩−αVw0(u)] ≤εinner,
(3.27)
i.e. Algorithm 12 is an (α, εinner)-relaxed proximal oracle.
Proof. For any u ∈Z, and deﬁning ˜∆t := ˜g( ˆwt) −g(w0) and ∆t := g( ˆwt) −g(w0), we have
X
t∈[T ]
⟨ηg(wt), wt −u⟩=
X
t∈[T ]
D
clip(η ˜∆t) + ηg(w0), wt −u
E
+
X
t∈[T ]
D
η∆t −clip(η ˜∆t), wt −u
E
+
X
t∈[T ]
⟨ηg(wt) −ηg( ˆwt), wt −u⟩.
(B.10)
We proceed to bound the three terms on the right hand side of (B.10) in turn. For the ﬁrst term,
recall the guarantees for the "ideal" iterates of Algorithm 12,
w⋆
t = argminw∈Z
nD
clip(η ˜∆t) + ηg(w0), w
E
+ αη
2 Vw0(w) + Vwt−1(w)
o
.
By using the optimality conditions of these iterates, deﬁning Q(z) := ⟨ηg(w0), z⟩+ αη
2 Vw0(z), γt :=
clip(η ˜∆t), and deﬁning for notational convenience w⋆
T +1 := u,
X
t∈[T ]
⟨γt−1 + ∇Q(w⋆
t ), w⋆
t −u⟩≤
X
t∈[T ]

−∇Vwt−1(w⋆
t ), w⋆
t −u

=
X
t∈[T ]
 Vwt−1(u) −Vw⋆
t (u) −Vwt−1(w⋆
t )

= Vw0(u) +
X
t∈[T ]
 Vwt(u) −Vw⋆
t (u)

−
T
X
t=0
Vwt(w⋆
t+1).
(B.11)

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
591
We thus have the chain of inequalities, recalling γ0 = 0,
X
t∈[T ]
D
clip(η ˜∆t) + ηg(w0), wt −u
E
+ αη
2
X
t∈[T ]
⟨∇Vw0(w⋆
t ), w⋆
t −u⟩
=
T
X
t=0
⟨γt, wt −u⟩+
X
t∈[T ]
⟨∇Q(w⋆
t ), w⋆
t −u⟩+
X
t∈[T ]
⟨ηg(w0), wt −w⋆
t ⟩
(i)
≤Vw0(u) +
X
t∈[T ]
 Vwt(u) −Vw⋆
t (u)

+
T
X
t=0
 ⟨γt, wt −w⋆
t+1⟩−Vwt(w⋆
t+1)

+
X
t∈[T ]
⟨ηg(w0), wt −w⋆
t ⟩
(ii)
≤Vw0(u) + 2ηϕT +
T
X
t=0

⟨clip(η ˜∆t), wt −w⋆
t+1⟩−Vwt(w⋆
t+1)
 (iii)
≤Vw0(u) + 2ηϕT +
T
X
t=0
η2  ˜∆t

2
wt
.
(B.12)
Here, (i) was by rearranging (B.11) via the equality
X
t∈[T ]
⟨γt−1, w⋆
t −u⟩=
T
X
t=0
⟨γt, wt −u⟩−
T
X
t=0

γt, wt −w⋆
t+1

,
(ii) was by the conditions maxu

Vwt(u) −Vw⋆
t (u)

≤ηϕ and ∥wt −w⋆
t ∥≤
ϕ
LD satisﬁed by the
iterates, and (iii) was by the property of clipping (3.18), as deﬁned in the problem setup. Now by
rearranging and using the three-point property of Bregman divergence (i.e. ⟨−∇Vw′(w), w −u⟩=
Vw′(u) −Vw(u) −Vw′(w)), it holds that
X
t∈[T ]
D
clip(η ˜∆t) + ηg(w0), wt −u
E
≤Vw0(u) + 2ηϕT + η2
T
X
t=0
 ˜∆t

2
wt + αη
2
X
t∈[T ]
(Vw0(u) −Vw0(w⋆
t ))
≤Vw0(u) + 3ηϕT + η2
T
X
t=0
 ˜∆t

2
wt
+ αη
2
X
t∈[T ]
(Vw0(u) −Vw0( ˆwt)) ,
(B.13)
where the second inequality follows from the condition Vw0( ˆwt) −Vw0(w⋆
t ) ≤2ϕ
α satisﬁed by iterates
of Algorithm 12. To bound the second term of (B.10), we deﬁne the ghost iterate sequnce {st} by
st = argmins∈Z
1
2

η∆t−1 −clip(η ˜∆t−1), s

+ Vst−1(s)

with s0 = w0.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
592
Applying Lemma 228 with Q = 0 and γt = 1
2(η∆t −clip(η ˜∆t)), and observing that again γ0 = 0,
X
t∈[T ]

η∆t −clip(η ˜∆t), st −u

≤2Vw0(u) +
T
X
t=0
n
⟨η∆t −clip(η ˜∆t), st −st+1⟩−2Vst(st+1)
o
≤2Vw0(u) + η2
T
X
t=0

∥∆t∥2
∗+
 ˜∆t

2
st

.
Here, we used properties (3.18) and (3.19). Consequently,
X
t∈[T ]
D
η∆t −clip(η ˜∆t), wt −u
E
=
X
t∈[T ]
D
η∆t −clip(η ˜∆t), wt −st
E
+
X
t∈[T ]
D
η∆t −clip(η ˜∆t), st −u
E
≤2Vw0(u) + η2
T
X
t=0

∥∆t∥2
∗+
 ˜∆t

2
st

+
X
t∈[T ]
D
η∆t −clip(η ˜∆t), wt −st
E
.
(B.14)
To bound the third term of (B.10), we use the condition ∥wt −ˆwt∥≤
ϕ
LD which implies
X
t∈[T ]
⟨ηg(wt) −ηg( ˆwt), wt −u⟩≤
X
t∈[T ]
∥ηg(wt) −ηg( ˆwt)∥∗∥wt −u∥≤2ηϕT.
(B.15)
Combining our three bounds (B.13), (B.14), and (B.15) in the context of (B.10), using ˆw0 = w0 and
˜g(w0) = g(w0), and ﬁnally dividing through by ηT, we obtain
1
T
X
t∈[T ]
⟨g(wt), wt −u⟩−
 3
ηT + α
2

Vw0(u)
≤5ϕ + 1
T
X
t∈[T ]

η
 ˜∆t

2
wt + η
 ˜∆t

2
st + η ∥∆t∥2
∗+

∆t −1
η clip(η ˜∆t), wt −st

−α
2 Vw0( ˆwt)

.
(B.16)
Since T ≥
6
αη, taking a supremum over u ∈Z in (B.16) and then an expectation yields
E sup
u∈Z

1
T
X
t∈[T ]
⟨g(wt), wt −u⟩−αVw0(u)

≤5ϕ
+ 1
T E

X
t∈[T ]
η
 ˜∆t

2
wt + η
 ˜∆t

2
st + η ∥∆t∥2
∗+

∆t −1
η clip(η ˜∆t), wt −st

−α
2 Vw0( ˆwt)

.
(B.17)
We will show the second line of (B.17) is nonpositive. To do so, observe for each t ∈[T], by the

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
593
property (3.20) of clip(·), since conditional on wt, st, ˜∆t is unbiased for deterministic ∆t,
E

∆t −1
η clip(η ˜∆t), wt −st
 =
E

˜∆t −1
η clip(η ˜∆t), wt −st
 ≤η
 ˜∆t

2
wt + η
 ˜∆t

2
st .
(B.18)
Finally, by using property 2 of the centered-local estimator ˜∆t, as well as Remark 1, we have for
each t ∈[T],
E

η
 ˜∆t

2
wt

≤ηL2Vw0( ˆwt), E

η
 ˜∆t

2
st

≤ηL2Vw0( ˆwt), and η ∥∆t∥2
∗≤ηL2Vw0( ˆwt).
(B.19)
Using bounds (B.18) and (B.19) in (B.17), as well as η ≤
α
10L2 ,
E sup
u∈Z

1
T
X
t∈[T ]
⟨g(wt), wt −u⟩−αVw0(u)

≤5ϕ.
(B.20)
For the ﬁnal claim, denote the true average iterate by ¯w := 1
T
P
t∈[T ] wt. We have ∀u ∈Z,
⟨g( ˜w), ˜w −u⟩
(i)
= −⟨g( ˜w), u⟩= ⟨g( ¯w) −g( ˜w), u⟩+ ⟨g( ¯w), ¯w −u⟩
(ii)
≤ϕ + ⟨g( ¯w), ¯w −u⟩
= 1
T
X
t∈[T ]
⟨g(wt), wt −u⟩+ ϕ.
Here, (i) used the fact that linearity of g gives ⟨g(z), z⟩= 0, ∀z ∈Z, and (ii) used H¨older's inequality
⟨g( ¯w) −g( ˜w), u⟩≤∥g( ¯w) −g( ˜w)∥∗∥u∥≤2LD ∥˜w −¯w∥≤ϕ following from the approximation
guarantee ∥˜w −¯w∥≤
ϕ
2LD. Combining with (B.20) yields the conclusion, as 6ϕ = εinner.
B.3
Deferred proofs for sublinear methods
B.3.1
ℓ2-ℓ2 sublinear coordinate method
Assumptions. The algorithm in this section will assume access to entry queries, ℓ1 norms of rows
and columns, and ℓ1 sampling distributions for rows and columns. Further, it assumes the ability
to sample a row or column proportional to its squared ℓ1 norm; given access to all ℓ1 norms, the
algorithm may spend O(m + n) constructing these sampling oracles in O(m + n) time, which does
not aﬀect its asymptotic runtime. We use the ℓ2-ℓ2 local norm setup (Table 3.6). We deﬁne
L2,2
co :=
s X
i∈[m]
∥Ai:∥2
1 +
X
j∈[n]
∥A:j∥2
1.
(B.21)

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
594
Gradient estimator
For z ∈Bn × Bm, we specify two distinct choices of sampling distributions p(z), q(z) which obtain
the optimal Lipschitz constant. The ﬁrst one is an oblivious distribution:
pij(z) :=
∥Ai:∥2
1
P
k∈[m] ∥Ak:∥2
1
· |Aij|
∥Ai:∥1
and
qij(z) :=
∥A:j∥2
1
P
k∈[n] ∥A:k∥2
1
· |Aij|
∥A:j∥1
.
(B.22)
The second one is a dynamic distribution:
pij(z) := [zy]i
2
∥zy∥2
2
· |Aij|
∥Ai:∥1
and
qij(z) := [zx]j
2
∥zx∥2
2
· |Aij|
∥A:j∥1
.
(B.23)
We now state the local properties of each estimator.
Lemma 231. In the ℓ2-ℓ2 setup, estimator (3.26) using the sampling distribution in (B.22) or (B.23)
is an L2,2
co -local estimator.
Proof. For convenience, we restate the distributions here: they are respectively
pij(z) :=
∥Ai:∥2
1
P
k∈[m] ∥Ak:∥2
1
· |Aij|
∥Ai:∥1
and
qij(z) :=
∥A:j∥2
1
P
k∈[n] ∥A:k∥2
1
· |Aij|
∥A:j∥1
and
pij(z) := [zy]i
2
∥zy∥2
2
· |Aij|
∥Ai:∥1
and
qij(z) := [zx]j
2
∥zx∥2
2
· |Aij|
∥A:j∥1
.
Unbiasedness holds by deﬁnition. We ﬁrst show the variance bound on the x block for distribu-
tion (B.22):
E
h
∥˜gx(z)∥2
2
i
=
X
i∈[m],j∈[n]
pij(z) ·
Aij[zy]i
pij(z)
2
=
X
i∈[m],j∈[n]
A2
ij[zy]2
i
pij(z)
=
X
i∈[m],j∈[n]
|Aij|
∥Ai:∥1
[zy]2
i ·

X
i∈[m]
∥Ai:∥2
1

=
X
i∈[m]
∥Ai:∥2
1.
Similarly, we have
E
h
∥˜gy(z)∥2
2
i
≤
X
j∈[n]
∥A:j∥2
1.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
595
Now, we show the variance bound on the x block for distribution (B.23):
E
h
∥˜gx(z)∥2
2
i
=
X
i∈[m],j∈[n]
pij(z) ·
Aij[zy]i
pij(z)
2
=
X
i∈[m],j∈[n]
A2
ij[zy]2
i
pij(z)
=
X
i∈[m],j∈[n]
|Aij|∥Ai:∥1 ∥zy∥2
2 ≤
X
i∈[m]
∥Ai:∥2
1,
and a similar bound holds on the y block.
We remark that using the oblivious distribution (B.22) saves a logarithmic factor in the runtime
compared to the dynamic distribution, so for the implementation of all of our ℓ2-ℓ2 algorithms we
will use the oblivious distribution.
Implementation details
In this section, we discuss the details of how to leverage the IterateMaintainer2 data structure
to implement the iterations of our algorithm. The algorithm we analyze is Algorithm 10, using the
local estimator deﬁned in (3.26), and the distribution (B.22). We choose
η =
ϵ
9

L2,2
co
2 and T =
6Θ
ηϵ

≥54
 L2,2
co
2
ϵ2
.
Lemma 231 implies that our estimator satisﬁes the remaining requirements for Proposition 9, giving
the duality gap guarantee in T iterations. In order to give a runtime bound, we claim that each
iteration can be implemented in constant time, with O(m + n) additional runtime.
Data structure initializations and invariants. At the start of the algorithm, we spend O(m + n)
time initializing data structures via IMx
2.Init(0n, b), IMy
2.Init(0m, c), where IMx
2, IMy
2 are instanti-
ations of IterateMaintainer2 data structures. Throughout, we preserve the invariant that the
points maintained by IMx
2, IMy
2 correspond to the x and y blocks of the current iterate zt at iteration
t of the algorithm. We note that we instantiate data structures which do not support Sample().
Iterations. For simplicity, we only discuss the runtime of updating the x block as the y block fol-
lows symmetrically. We divide each iteration into the following substeps, each of which we show run
in constant time. We refer to the current iterate by z = (zx, zy), and the next iterate by w = (wx, wy).
Sampling. Because the distribution is oblivious, sampling both i and j | i using precomputed data
structures takes constant time.
Computing the gradient estimator. To compute c := Aij[zy]i/pij, it suﬃces to compute Aij, [zy]i,
and pij. Using an entry oracle for A obtains Aij in constant time, and calling IMy
2.Get(i) takes
constant time. Computing pij using the precomputed row norms and the values of Aij, [zy]i takes

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
596
constant time.
Performing the update. For the update corresponding to a proximal step, we have
wx ←ΠX (zx −η˜gx(z)) =
zx −η˜gx(z)
max{∥zx −η˜gx(z)∥2 , 1}.
We have computed ˜gx(z), so to perform this update, we call
IMx
2.AddSparse(j, −ηc);
IMx
2.AddDense(−η);
IMx
2.Scale(max{IMx.GetNorm(), 1}−1);
IMx
2.UpdateSum().
By assumption, each operation takes constant time because we do not support Sample in our in-
stances of IM2, giving the desired iteration complexity. It is clear that at the end of performing these
operations, the invariant that IMx
2 maintains the x block of the iterate is preserved.
Averaging. After T iterations, we compute the average point ¯zx:
[¯zx]j ←1
T · IMx
2.GetSum(j), ∀j ∈[n].
By assumption, this takes O(n) time.
Algorithm guarantee
Theorem 76. In the ℓ2-ℓ2 setup, the implementation in Section B.3.1 has runtime
O
  L2,2
co
2
ϵ2
+ m + n
!
and outputs a point ¯z ∈Z such that
EGap(¯z) ≤ϵ.
Proof. The runtime bound follows from the discussion in Section B.3.1. The correctness follows from
Proposition 9.
Remark 11. Using our IterateMaintainer2 data structure, the ℓ2-ℓ2 algorithm of [55] runs in
time O
 rcs∥A∥2
F /ϵ2
. Our runtime universally improves upon it since
X
i∈[m]
∥Ai:∥2
1 +
X
j∈[n]
∥A:j∥2
1 ≤2rcs ∥A∥2
F .

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
597
B.3.2
ℓ2-ℓ1 sublinear coordinate method
Assumptions. The algorithm in this section will assume access to every oracle listed in Section 3.2.3.
However, for a speciﬁc matrix A, only one of three sampling distributions will be used in the
algorithm; we describe the speciﬁc oracle requirements of each distribution following their deﬁnition.
We use the ℓ2-ℓ1 local norm setup (Table 3.6). Throughout this section, we will assume that the
linear term in (3.26) is g(0) = 0 uniformly.
Finally, in this section we assume access to a weighted variant of IterateMaintainer2, which
takes a nonnegative weight vector w as a static parameter. WeightedIterateMaintainer2 supports
two modiﬁed operations compared to the data structure IterateMaintainer2: its GetNorm() oper-
ation returns
qP
j[w]j[x]2
j, and its Sample() returns coordinate j with probability proportional to
[w]j[x]2
j (cf. Section 3.2.4). We give the implementation of this extension in Appendix B.8.
Gradient estimator
For z ∈Bn × ∆m and desired accuracy ϵ > 0, we specify three distinct choices of sampling distribu-
tions p(z), q(z). Each of our distributions induces an estimator with diﬀerent properties.
The ﬁrst one is
pij(z) := |Aij|
∥Ai:∥1
· [zy]i and
qij(z) :=
A2
ij
∥A∥2
F
.
(B.24)
The second one is
pij(z) := |Aij|
∥Ai:∥1
· [zy]i and
qij(z) := [zx]2
j · 1{Aij̸=0}
P
l∈[n] csl · [zx]2
l
.
(B.25)
Here, we let csj ≤rcs denote the number of nonzero elements in column A:j. The third one is
pij(z) := |Aij|
∥Ai:∥1
· [zy]i and
qij(z) :=
|Aij| · [zx]2
j
P
l∈[n] ∥A:l∥1 · [zx]2
l
.
(B.26)
For L2,1,(1)
co
, L2,1,(2)
co
, and L2,1,(3)
co
to be deﬁned, the estimators induced by these distributions
are local estimators whose guarantees depend on these constants respectively. Furthermore, these
Lipschitz constants are in general incomparable and depend on speciﬁc properties of the matrix.
Therefore, we may choose our deﬁnition of L2,1
co to be the minimum of these constants, by choosing
an appropriate estimator. We now state the local properties of each estimator.
Lemma 232. In the ℓ2-ℓ1 setup, estimator (3.26) using the sampling distributions in (B.24), (B.25),

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
598
or (B.26) is respectively a L2,1,(k)
co
-local estimator, for k ∈{1, 2, 3}, and
L2,1,(1)
co
:=
r
max
i∈[m] ∥Ai:∥2
1 + ∥A∥2
F,
L2,1,(2)
co
:=
r
2rcs max
i∈[m] ∥Ai:∥2
2,
L2,1,(3)
co
:=
s
max
i∈[m] ∥Ai:∥2
1 +

max
i∈[m] ∥Ai:∥1
 
max
j∈[n] ∥A:j∥1

.
Proof. First, we give the proof for the sampling distribution (B.24). Unbiasedness holds by deﬁnition.
For the x block, we have the variance bound:
E
h
∥˜gx(z)∥2
2
i
=
X
i∈[m],j∈[n]
pij(z) ·
Aij[zy]i
pij(z)
2
=
X
i∈[m],j∈[n]
|Aij|∥Ai:∥1[zy]i
=
X
i∈[m]
∥Ai:∥2
1[zy]i ≤max
i∈[m] ∥Ai:∥2
1.
For arbitrary wy, we have the variance bound on the y block:
E
h
∥˜gy(z)∥2
wy
i
=
X
i∈[m],j∈[n]
qij(z) ·
 
[wy]i ·
Aij[zx]j
qij(z)
2!
=
X
i∈[m],j∈[n]
[wy]i
A2
ij[zx]2
j
qij(z)
=
X
i∈[m],j∈[n]
[wy]i[zx]2
j ∥A∥2
F ≤∥A∥2
F .
Next, we give the proof for the sampling distribution (B.25). Unbiasedness holds by deﬁnition. By
Cauchy-Schwarz and our earlier proof, we have the variance bound for the x block:
E
h
∥˜gx(z)∥2
2
i
≤max
i∈[m] ∥Ai:∥2
1 ≤rcs max
i∈[m] ∥Ai:∥2
2 .
For arbitrary wy, we have the variance bound on the y block, where Si :=

j | 1Aij̸=0 = 1
	
:
E
h
∥˜gy(z)∥2
wy
i
=
X
i∈[m],j∈Si
qij(z) ·
 
[wy]i ·
Aij[zx]j
qij(z)
2!
=
X
i∈[m],j∈Si
[wy]i
A2
ij[zx]2
j
qij(z)
≤
X
i∈[m],j∈Si
[wy]iA2
ijrcs ≤rcs max
k∈[m] ∥Ak:∥2
2 .
Finally, we give the proof for the sampling distribution (B.26). Unbiasedness and the variance bound

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
599
for the x block again hold. For arbitrary wy, we have the variance bound on the y block:
E
h
∥˜gy(z)∥2
wy
i
=
X
i∈[m],j∈[n]
qij(z) ·
 
[wy]i ·
Aij[zx]j
qij(z)
2!
=
X
i∈[m],j∈[n]
[wy]i
A2
ij[zx]2
j
qij(z)
≤


X
i∈[m],j∈[n]
[wy]i|Aij|



X
l∈[n]
∥A:l∥1 [zx]2
l


≤

max
k∈[m] ∥Ak:∥1
 
max
l∈[n] ∥A:l∥1

.
By using the deﬁnitions of L2,1,(1)
co
, L2,1,(2)
co
, and L2,1,(3)
co
, we deﬁne the constant
L2,1
co :=
s
max
i∈[m] ∥Ai:∥2
1 + min

∥A∥2
F , rcs max
i∈[m] ∥Ai:∥2
2 ,

max
i∈[m] ∥Ai:∥1
 
max
j∈[n] ∥A:j∥1

.
(B.27)
In particular, by choosing whichever of the distributions (B.24), (B.25), or (B.26) yields the
minimial Lipschitz constant, we may always ensure we have a L2,1
co -local estimator. We now discuss
the speciﬁc precomputed quantities each estimator requires, among those listed in Section 3.2.3. All
distributions require access to entry queries, ℓ1 norms of rows, and ℓ1 sampling distributions for
rows.
• Using the sampling distribution (B.24) requires additional access to ℓ2 sampling distributions
for rows and columns and the Frobenius norm of A.
• Using the sampling distribution (B.25) requires additional access to uniform sampling nonzero
entries of columns.
• Using the sampling distribution (B.26) requires additional access to ℓ1 norms of columns and
ℓ1 sampling distributions for columns.
Implementation details
In this section, we discuss the details of how to leverage the appropriate IterateMaintainer1 and
IterateMaintainer2 data structures to implement the iterations of our algorithm. The algorithm
we analyze is Algorithm 10, using the local estimator deﬁned in (3.26), and the best choice of
distribution among (B.24), (B.25), (B.26). We choose
η =
ϵ
9

L2,1
co
2 and T =
6Θ
ηϵ

≥54
 L2,1
co
2 log(2m)
ϵ2
.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
600
Lemma 232 implies that our estimator satisﬁes the remaining requirements for Proposition 9, giving
the duality gap guarantee in T iterations. In order to give a runtime bound, we claim that each
iteration can be implemented in O(log mn) time, with O(m + n) additional runtime. For simplicity,
because most of the algorithm implementation details are exactly same as the discussion of Sec-
tion 3.4.1 for the simplex block y ∈Y, and exactly the same as the discussion of Section B.3.1 for
the ball block x ∈X, we discuss the diﬀerences here, namely the implementations of sampling and
gradient computation.
We assume that we have initialized IMy
1, an instantiation of IterateMaintainer1, and IMx
2, an
instantiation of IterateMaintainer2. When the choice of distribution is (B.25), we also assume
access to WIMx
2, an instantiation of WeightedIterateMaintainer2 initialized with the weight vec-
tor of nonzero counts of columns of the matrix; similarly, for distribution (B.26) we instantiate a
WeightedIterateMaintainer2 with the weight vector of ℓ1 norms of each column.
Sampling. Recall that
pij(z) := |Aij|
∥Ai:∥1
· [zy]i.
We ﬁrst sample coordinate i via IMy
1.Sample() in O(log m), and then sample j using the data structure
corresponding to Ai: in O(1). Next, to sample from the distribution
qij(z) :=
A2
ij
∥A∥2
F
required by (B.24), we can sample a coordinate of the matrix proportional to its square in constant
time using our matrix access. To sample from the distribution
qij(z) := [zx]2
j · 1{Aij̸=0}
P
l∈[n] csl · [zx]2
l
required by (B.25), we ﬁrst sample coordinate j via WIMx
2.Sample() in O(log n), and then uniformly
sample a coordinate i amongst the entries of A:j for which the indicator labels as nonzero. Finally,
to sample from the distribution
qij(z) :=
|Aij| · [zx]2
j
P
l∈[n] ∥A:l∥1 · [zx]2
l
required by (B.26), we sample coordinate j via WIMx
2.Sample(), and then sample a coordinate i pro-
portional to its absolute value using a column sampling oracle.
Computing the gradient estimator. By the proofs of Theorem 10 and Theorem 76, it suﬃces to com-
pute pixjx, qiyjy in constant time. Calling IMx
2.Get(j), IMy
1.Get(i), IMx
2.GetNorm(), and WIMx
2.GetNorm()

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
601
when appropriate, and using access to precomputation allows us to obtain all relevant quantities for
the computations in O(1).
Algorithm guarantee
Theorem 77. In the ℓ2-ℓ1 setup, the implementation in Section B.3.2 has runtime
O
  L2,1
co
2 log m log(mn)
ϵ2
+ m + n
!
and outputs a point ¯z ∈Z such that
EGap(¯z) ≤ϵ.
Proof. The runtime bound follows from the discussion in Section B.3.2. The correctness follows from
Proposition 9.
Remark 12. Using our IterateMaintainer1 and IterateMaintainer2 data structures, the ℓ2-ℓ1
algorithm of Clarkson et al. [140] runs in time O(rcs maxi∈[m] ∥Ai:∥2
2 log2(mn)/ϵ2). By noting the
deﬁnition of L2,1,(2)
co
, our runtime universally improves upon it since
 L2,1
co
2 ≤2rcs maxi∈[m] ∥Ai:∥2
2.
B.4
Deferred proofs for variance-reduced methods
B.4.1
Helper proofs
Lemma 32. For y, y′ ∈∆m, divergence Vy(y′) generated by r(y) = P
i∈[m][y]i log[y]i −[y]i satisﬁes
Vy(y′) ≥1
2 ∥y′ −y∥2
3
2y+y′ = 1
2
X
i∈[m]
([y]i −[y′]i)2
2
3[y]i + 1
3[y′]i
.
Proof. Let γ ∈Rm. Note that for every τ ∈[0, 1] (with elementwise multiplication, division and
square root), ⟨γ, y −y′⟩=

γ
p
(1 −τ)y + τy′,
y−y′
√
(1−τ)y+τy′

. Therefore, using 2 ⟨u, w⟩≤∥u∥2
2 +
∥w∥2
2, we have for every τ ∈[0, 1],
2 ⟨γ, y −y′⟩≤
X
i∈[m]
((1 −τ)[y]i + τ[y′]i) [γ]2
i +
X
i∈[m]
([y]i −[y′]i)2
(1 −τ)[y]i + τ[y′]i
.
Applying the double integral
R 1
0 dt
R t
0 dτ to both sides of the inequality, and using
R 1
0 dt
R t
0 1 · dτ = 1
2
and
R 1
0 dt
R t
0 τ · dτ = 1
6 gives
⟨γ, y −y′⟩≤
X
i∈[m]
1
3[y]i + 1
6[y′]i

[γ]2
i +
Z 1
0
dt
Z t
0
X
i∈[m]
([y]i −[y′]i)2
(1 −τ)[y]i + τ[y′]i
dτ.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
602
Identifying the double integral with the expression
Vy(y′) =
X
i∈[m]

y′
i log y′
i
yi
+ yi −y′
i

=
Z 1
0
dt
Z t
0
X
i∈[m]
(yi −y′
i)2
(1 −τ)yi + τy′
i
dτ.
(B.28)
for the divergence induced by entropy, the result follows by choosing [γ]i =
[y]i−[y′]i
2
3 [y]i+ 1
3 [y′]i .
Lemma 34. Let x′ ∈∆n be a β-padding of x ∈∆n. Then,
X
j∈[n]
x′
j log x′
j −
X
j∈[n]
xj log xj ≤βn
e + β(1 + β).
Proof. Letting ˜x be the point inducing x′ in Deﬁnition 3, we have
X
j∈[n]
x′
j log x′
j −
X
j∈[n]
xj log xj =

X
j∈[n]
x′
j log x′
j −
X
j∈[n]
˜xj log ˜xj


+

X
j∈[n]
˜xj log ˜xj −
X
j∈[n]
xj log xj

.
We bound these two terms separately. For the ﬁrst term, let ∥˜x∥1 = 1 + b, for some b ≤β; we
see that entrywise, (1 + b)x′
j = ˜xj. For each j ∈[n],
x′
j log x′
j −˜xj log ˜xj = x′
j log x′
j −(1 + b)x′
j log
 (1 + b)x′
j

= bx′
j log 1
x′
j
−(1 + b)x′
j log(1 + b)
≤bx′
j log 1
x′
j
≤β
e .
The ﬁrst inequality was due to nonnegativity of (1 + b) log(1 + b) and x′
j, and the second was due
to the maximum value of the scalar function z log 1
z over the nonnegative reals being 1/e. Summing
over all coordinates yields that the ﬁrst term is bounded by βn/e.
For the second term, we have by integration that entrywise
˜xj log ˜xj −xj log xj =
Z 1
α=0
(1 + log(xj + α(˜xj −xj)))(˜xj −xj)dα
≤
Z 1
α=0
(1 + log(˜xj))(˜xj −xj)dα
≤
Z 1
α=0
˜xj(˜xj −xj)dα ≤(1 + β)|˜xj −xj|.
The ﬁrst inequality is by ˜xj ≥xj for all j ∈[n] and log(x) is monotone in x > 0; the second is by

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
603
log(x) ≤x −1 for all x > 0; the third again uses ˜xj ≥xj and that ˜xj ≤∥˜x∥1 ≤1 + β, and the
second condition in Deﬁnition 3. Finally, combining yields the desired
X
j∈[n]
x′
j log x′
j −
X
j∈[n]
xj log xj ≤βn
e + β(1 + β).
B.4.2
ℓ2-ℓ2 variance-reduced coordinate method
Assumptions. As in Section B.3.1, the algorithm in this section will assume access to entry queries,
ℓ1 norms of rows and columns, and ℓ1 sampling distributions for rows and columns, and the ability
to sample a row or column proportional to its squared ℓ1 norm. We use the ℓ2-ℓ2 local norm setup
(cf. Table 3.6). Again, we deﬁne
L2,2
co :=
s X
i∈[m]
∥Ai:∥2
1 +
X
j∈[n]
∥A:j∥2
1.
Gradient estimator
Given reference point w0 ∈Bn × Bm, for z ∈Bn × Bm, we specify two distinct sampling distribu-
tions p(z; w0), q(z; w0) which obtain the optimal Lipschitz constant. The ﬁrst one is an oblivious
distribution:
pij(z; w0) :=
∥Ai:∥2
1
P
k∈[m] ∥Ak:∥2
1
· |Aij|
∥Ai:∥1
and
qij(z; w0) :=
∥A:j∥2
1
P
k∈[n] ∥A:k∥2
1
· |Aij|
∥A:j∥1
.
(B.29)
The second one is a dynamic distribution:
pij(z; w0) := [wy
0 −zy]2
i
∥wy
0 −zy∥2
2
· |Aij|
∥Ai:∥1
and
qij(z; w0) := [wx
0 −zx]2
j
∥wx
0 −zx∥2
2
· |Aij|
∥A:j∥1
.
(B.30)
We now state the local properties of each estimator.
Lemma 233. In the ℓ2-ℓ2 setup, estimator (3.28) using the sampling distribution in (B.29) or (B.30)
is a
√
2L2,2
co -centered-local estimator.
Proof. Unbiasedness holds by deﬁnition in both cases. We ﬁrst show the variance bound on the x

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
604
block for distribution (B.29):
E
h˜gx
w0(z) −gx(w0)
2
2
i
=
X
i∈[m],j∈[n]
pij(z; w0) ·
Aij[zy −wy
0]i
pij(z; w0)
2
=
X
i∈[m],j∈[n]
A2
ij[zy −wy
0]2
i
pij(z; w0)
=
X
i∈[m],j∈[n]
|Aij|
∥Ai:∥1
[zy −wy
0]2
i ·

X
k∈[m]
∥Ak:∥2
1


=

X
i∈[m]
∥Ai:∥2
1

∥zy −wy
0∥2
2 .
Similarly, we have
E
h˜gy
w0(z) −gy(w0)
2
2
i
≤

X
j∈[n]
∥A:j∥2
1

∥zx −wx
0∥2
2 .
Combining these and using ∥zx −wx
0∥2
2 + ∥zy −wy
0∥2
2 = 2Vw0(z) yields the desired variance bound.
Now, we show the variance bound on the x block for distribution (B.30):
E
h˜gx
w0(z) −gx(w0)
2
2
i
=
X
i∈[m],j∈[n]
pij(z; w0) ·
Aij[zy −wy
0]i
pij(z; w0)
2
=
X
i∈[m],j∈[n]
A2
ij[zy −wy
0]2
i
pij(z; w0)
=
X
i∈[m],j∈[n]
|Aij|∥Ai:∥1 ∥zy −wy
0∥2
2
=

X
i∈[m]
∥Ai:∥2
1

∥zy −wy
0∥2
2 .
and a similar bound holds on the y block.
Again, for algorithmic considerations (i.e. an additional logarithmic factor in the complexity of
sampling from (B.30)), we will only discuss using the oblivious distribution (B.29) in our algorithm.
Implementation details
In this section, we discuss the details of how to leverage the IterateMaintainer2 data structure
to implement the iterations of our algorithm.
The algorithm we analyze is Algorithm 11 with
K = αΘ/ϵ, using Algorithm 12 as an (α, 0)-relaxed proximal oracle.
In the implementation of
Algorithm 12, we use the centered-local gradient estimator deﬁned in (B.29).
For each use of
Algorithm 12, we choose
η =
α
20

L2,2
co
2 and T =
 6
ηα

≥120
 L2,2
co
2
α2
.
(B.31)

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
605
Our discussion will follow in three steps: ﬁrst, we discuss the complexity of all executions in Algo-
rithm 11 other than the calls to the oracles, as well as the initialization procedure for each inner
loop. Next, we discuss the complexity of each iteration of Algorithm 12. Finally, we discuss the
complexity of computing the average iterate in each run of Algorithm 12.
For simplicity, when
discussing Algorithm 12, we will only discuss implementation of the x-block, and the y-block will
follow symmetrically. Altogether, the guarantees of Proposition 10 and Proposition 11 imply that if
the guarantees required by the algorithm hold, the expected gap of the output is bounded by ϵ.
Outer loop extragradient steps and inner loop data structures. Overall, we execute K = αΘ/ϵ
iterations of Algorithm 11, and let εouter = εinner = 0 to obtain the desired gap, where Θ = 1 in
the ℓ2-ℓ2 setup. We spend O(nnz) time executing each extragradient step in Algorithm 11 exactly,
where the dominant term in the runtime is the computation of each g(zk−1/2), for k ∈[K]. Also,
we can maintain the average point ¯z throughout the duration of the algorithm, in O(m + n) time
per iteration. At the beginning of each inner loop, we initialize a data structure IMx
2 which does not
support sampling, an instance of IterateMaintainer2, with IMx
2.Init(wx
0, v), for
v = (1 −κ)wx
0 −ηκgx(w0),
where κ :=
1
1+ηα/2. The inner loop will preserve the invariant that the point maintained by IMx
2 is
the x block of the current inner loop iterate wt in each iteration t. To motivate this initialization,
we recall the form of the updates,
wx
t+1 ←ΠX

κ

wx
t +
 1
κ −1

wx
0 −η˜gx
w0(wt)

,
(B.32)
where ΠX (w) =
w
max{1,∥w∥2}, and the ﬁxed dense part of ˜gx
w0(wt) is gx(w0).
Therefore, in the
following discussion we will be able to maintain this diﬀerence via a scaling by κ, an appropriate
addition of the scaled dense vector, and a sparse update.
Finally, we also store the vector w0 in full, supporting entry queries.
Inner loop iterations. Each inner loop iteration consists of sampling indices for the computation
of ˜gw0, computing the sparse part of ˜gw0, and performing the update to the iterate. We show that
we can run each substep in constant time. Then, this implies that the total complexity of the inner
loop, other than initializing the data structures and outputting the average iterate, is
O(T) = O
  L2,2
co
2
α2
!
.
We discuss how to make appropriate modiﬁcations to the x-block. For simplicity we denote our

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
606
current iterate as z, and the next iterate as w. Recall that the distribution is given by
pij(z; w0) :=
∥Ai:∥2
1
P
k∈[m] ∥Ak:∥2
1
· |Aij|
∥Ai:∥1
.
Sampling. By using precomputed distributions, we can sample i ∝∥Ai:∥2
1 and then j | i ∝|Aij| in
constant time.
Computing the gradient estimator. Computing the sparse component of the gradient estimator 3.28
requires computing Aij, [zy −wy
0]i, and pij(z; w0). Using appropriate use of precomputed access to
entries and row norms (it is clear we may pay O(m + n) at the beginning of the algorithm to store
the sum P
k∈[m] ∥Ak:∥2
1), entry [wy
0]i, and IMy
2.Get(i) allows us to perform the required computation
of the sparse component
c := [˜gx
w0(z) −g(w0)]j
in constant time, by assumption.
Performing the update. In order to perform the update, we recall the form of the update given by
(B.32). Thus, it suﬃces to call
IMx
2.Scale(κ);
IMx
2.AddDense(1);
IMx
2.AddSparse(j, −κηc);
IMx
2.Scale(max{IMx
2.GetNorm(), 1}−1);
IMx
2.UpdateSum()
By assumption, each operation takes constant time. By the discussion in the data structure initial-
ization section, it is clear that we preserve the invariant that the point maintained by IMx
2 is the x
block of the current iterate.
Average iterate computation.
At the end of each run of Algorithm 12, we spend O(n) time
computing and returning the average iterate via appropriate calls to IMx
2.GetSum(j) for each j ∈[n],
and scaling by 1/T.
This operation is asymptotically dominated by the O(nnz(A)) cost of the
extragradient step.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
607
Algorithm guarantee
Theorem 78. In the ℓ2-ℓ2 setup, the implementation in Section B.4.2 with the optimal choice of
α = max{ϵ, L2,2
co
p
1/nnz} has runtime
O
  
nnz +
 L2,2
co
2
α2
!
α
ϵ
!
= O

nnz +
√nnzL2,2
co
ϵ

and outputs a point ¯z ∈Z such that
EGap(¯z) ≤ϵ.
Proof. The correctness of the algorithm is given by the discussion in Section B.4.2 and the guarantees
of Proposition 10 and Proposition 11. The runtime bound is given by the discussion in Section B.4.2,
and the optimal choice of α is clear.
To better understand the strengths of our runtime guarantee, Proposition 55 shows that Theo-
rem 78 implies a universal improvement for ℓ2-ℓ2 games compared to accelerated gradient descent
for matrices A with nonnegative entries (or more generally, for A with ∥|A|∥op = O(∥A∥op)).
Proposition 55. For any A ∈Rm×n, we have
L2,2
co := max



sX
i
∥Ai:∥2
1,
sX
j
∥A:j∥2
1


≤
√
m + n · ∥|A|∥op .
Proof. Denote 1k as the all 1 vector in Rk. We have the following sequence of inequalities:
s X
i∈[m]
∥Ai:∥2
1 =
|A|⊤1m

2 = max
x∈Bn 1⊤
m|A|x ≤∥1m∥2 max
x∈Bn ∥|A|x∥2 ≤√m ∥|A|∥op .
Similarly, bounding maxy∈Bn y⊤|A|1n implies
qP
j∈[n] ∥A:j∥2
1 ≤√n ∥|A|∥op. Taking a maxi-
mum and using max{√m, √n} ≤√m + n implies the result.
Remark 13. For matrix A ∈Rm×n, combining the guarantees of Theorem 78 with the bound from
Proposition 55 implies a runtime bounded by
O
 
nnz +
p
nnz · (m + n) ∥|A|∥op
ϵ
!
.
Whenever ∥A∥op ≥∥|A|∥op, this is an improvement by a factor of
p
nnz/(m + n) compared to the
accelerated full-gradient method (c.f. Table 3.2), which obtains a runtime of O(nnz · ∥A∥op /ϵ). This
applies without any sparsity or numerical sparsity assumptions, and is the same speedup factor as we
obtain for ℓ1-ℓ1 and ℓ2-ℓ1 games using a variance reduction framework with row and column based

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
608
gradient estimators in Appendix B.5. The ℓ2-ℓ2 variance reduction algorithms of Appendix B.5 and
[55] do not oﬀer such improvements, and our improvement stems from our coordinate-based gradient
estimators and our data structure design.
B.4.3
ℓ2-ℓ1 variance-reduced coordinate method
Assumptions. The algorithm in this section will assume access to entry queries, ℓ1 norms of rows, ℓ2
sampling distributions for rows and columns, and the Frobenius norm of A. We use the ℓ2-ℓ1 local
norm setup (cf. Table 3.6). Again, we deﬁne
L2,1,(1)
co
:=
r
max
i∈[m] ∥Ai:∥2
1 + ∥A∥2
F,
(B.33)
L2,1,(2)
co
:=
r
2rcs max
i∈[m] ∥Ai:∥2
2,
(B.34)
L2,1,(3)
co
:=
s
max
i∈[m] ∥Ai:∥2
1 +

max
i∈[m] ∥Ai:∥1
 
max
j∈[n] ∥A:j∥1

.
(B.35)
Finally, in this section we assume access to a centered variant of WeightedIterateMaintainer2,
which takes a point x0 as a static parameter, where x0 is in the space as the iterates x main-
tained. CenteredIterateMaintainer2 supports two additional operations compared to the data
structure WeightedIterateMaintainer2: Sample() returns coordinate j with probability propor-
tional to [w]j[x −x0]2
j (cf. Section 3.2.4) in O(log n) time, and we may query ∥x −x0∥2
w in constant
time, where w is a speciﬁed weight vector. We give the implementation of this extension in Ap-
pendix B.8.
Gradient estimator
Given reference point w0 ∈Bn × ∆m, for z ∈Bn × ∆m and a parameter α > 0, as in Section B.3.2,
we specify three distinct choices of sampling distributions p(z; w0), q(z; w0).
The ﬁrst one is
pij(z; w0) := [zy]i + 2[wy
0]i
3
· |Aij|
∥Ai:∥1
and
qij(z; w0) :=
A2
ij
∥A∥2
F
.
(B.36)
The second one is
pij(z; w0) := [zy]i + 2[wy
0]i
3
· |Aij|
∥Ai:∥1
and
qij(z) := [zx −wx
0]2
j · 1{Aij̸=0}
P
l∈[n] csl · [zx −wx
0]2
l
.
(B.37)

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
609
As in Section B.3.2, csj is the number of nonzeros of A:j. The third one is
pij(z; w0) := [zy]i + 2[wy
0]i
3
· |Aij|
∥Ai:∥1
and
qij(z) :=
|Aij| · [zx −wx
0]2
j
P
l∈[n] ∥A:l∥1 · [zx −wx
0]2
l
.
(B.38)
We now state the local properties of each estimator.
Lemma 234. In the ℓ2-ℓ1 setup, estimator (3.28) using the sampling distributions in (B.36), (B.37),
or (B.38) is respectively a
√
2L2,1,(k)
co
-centered-local estimator, for k ∈{1, 2, 3}.
Proof. First, we give the proof for the sampling distribution (B.36). Unbiasedness holds by deﬁnition.
For the x block, we have the variance bound:
E
h˜gx
w0(z) −gx(w0)
2
2
i
=
X
i∈[m],j∈[n]
pij(z; w0)
Aij[zy −wy
0]i
pij(z; w0)
2
=
X
i∈[m],j∈[n]
A2
ij[zy −wy
0]2
i
pij(z; w0)
≤2 max
i∈[m] ∥Ai:∥2
1 Vwy
0(zy),
where in the last inequality we used Lemma 32.
For arbitrary wy, we have the variance bound on the y block:
E
h˜gy
w0(z) −gy(w0)
2
wy
i
=
X
i∈[m],j∈[n]
[wy]i
A2
ij[zx −wx
0]2
j
qij(z; w0)
=
X
i∈[m],j∈[n]
[wy]i[zx −wx
0]2
j ∥A∥2
F ≤2 ∥A∥2
F Vwx
0(zx).
Combining these and using
∥˜gw0(z) −g(w0)∥2
w := ∥˜gw0(z)x −g(w0)x∥2
2 + ∥˜gw0(z)y −g(w0)y∥2
wy
yields the desired variance bound. For the remaining two distributions, the same argument demon-
strates unbiasedness and the variance bound for the x block. For sampling distribution (B.37) and
arbitrary wy, we have the variance bound on the y block:
E
h˜gy
w0(z) −gy(w0)
2
wy
i
=
X
i∈[m],j∈[n]
[wy]i
A2
ij[zx −wx
0]2
j
qij(z; w0)
≤


X
i∈[m],j∈[n]
[wy]iA2
ij



rcs
X
j∈[n]
[zx −wx
0]2
j


≤2rcs max
i∈[m] ∥Ai:∥2
2 Vwx
0(zx).

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
610
Finally, for sampling distribution (B.38), we have the variance bound on the y block:
E
h˜gy
w0(z) −gy(w0)
2
wy
i
=
X
i∈[m],j∈[n]
[wy]i
A2
ij[zx −wx
0]2
j
qij(z; w0)
≤


X
i∈[m],j∈[n]
[wy]i|Aij|



X
l∈[n]
∥A:l∥1 · [zx −wx
0]2
l


≤2

max
i∈[m] ∥Ai:∥1
 
max
j∈[n] ∥A:j∥1

Vwx
0(zx).
Finally, as in Section B.3.2, we deﬁne the constant
L2,1
co :=
s
max
i∈[m] ∥Ai:∥2
1 + min

∥A∥2
F , rcs max
i∈[m] ∥Ai:∥2
2 ,

max
i∈[m] ∥Ai:∥1
 
max
j∈[n] ∥A:j∥1

,
and note that Lemma 234 implies that we can obtain a
√
2L2,1
co -centered-local estimator by appro-
priately choosing a sampling distribution depending on the minimizing parameter.
Implementation details
The algorithm we analyze is Algorithm 11 with K = 3αΘ/ϵ, εouter = 2ϵ/3 using Algorithm 12 as an
(α, εinner = ϵ/3)-relaxed proximal oracle with ϕ = ϵ/18. In the implementation of Algorithm 11, we
again apply the truncate(·, δ) operation to each iterate z⋆
k, where the truncate operation only aﬀects
the y block; choosing δ = εouter−εinner
αm
suﬃces for its guarantees (see Section 3.4.2 for the relevant
discussion). In the implementation of Algorithm 12, we use the centered-local gradient estimator
deﬁned in (3.28), using the sampling distribution amongst (B.36), (B.37), or (B.38) which attains
the variance bound L2,1
co . For each use of Algorithm 12, we choose
η =
α
20

L2,1
co
2 and T =
 6
ηα

= 120
 L2,1
co
2
α2
.
For simplicity, because most of the algorithm implementation details are exactly the same as the
discussion of Section 3.4.2 for the simplex block y ∈Y, and exactly the same as the discussion of
Section B.4.2 for the ball block x ∈X, we discuss the diﬀerences here.
Outer loop extragradient steps. We execute 3α log(2m)/ϵ iterations of Algorithm 11 to obtain the
desired gap. We spend O(nnz) time executing each extragradient step exactly, and then O(m + n)
time applying the truncate operation and maintaining the average point ¯z. When we initialize the
inner loop, we also create a data structure supporting sampling from wy
0 in constant time.
Data structure initializations and invariants.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
611
On the simplex block, we follow the strategy outlined in Section 3.4.2. We initialize our simplex
maintenance data structure AEMy(wy
0, v, κ, ˜ε) with parameters
κ :=
1
1 + ηα/2, v := (1 −κ) log wy
0 −ηκgy(w0), ˜ε := (m + n)−8.
We will again maintain the invariant that the data structures maintain "exact" and "approximate"
points corresponding to the iterates of our algorithm. The correctness of this setting with respect
to the requirements of Proposition 11, i.e. the approximation conditions in Line 5, 7 and 8 in
Algorithm 12, follows from the discussion of Section 3.4.2; we note that the condition minj[wx
0]j ≥
(m + n)−5 = λ again holds, and that 1 −κ ≥(m + n)−8. Thus, for the parameter ω used in the
interface of ApproxExpMaintainer, we have
log(ω) = log

max

1
1 −κ, m
λ˜ε

= O(log(mn)).
On the ball block, we follow the strategy outlined in Section B.4.2, but instead of using an
IterateMaintainer2 on the x-block, we use CIMx
2, an instance of CenteredIterateMaintainer2
data structure initialized with the point wx
0, supporting the required sampling operation. For the
sampling distribution (B.37), we use the weight vector of column nonzero counts, and for (B.38)
we use the weight vector of column ℓ1 norms. Overall, the complexity of the initializations on both
blocks is bounded by O(n + m log2(m) log2(mn)).
Inner loop iterations.
We discuss how to sample from each of the distributions (B.36), (B.37), and (B.38) in O(log(m) log(mn)).
Combining with the discussions of implementing the inner loop in Sections 3.4.2 and B.4.2, the total
complexity of the inner loop, other than outputting the average iterate, is
O
 T log2(m) log2(mn) + nnz + m log(m) log2(mn)

= O




L2,1,(1)
co
2
log2(m) log2(mn)
α2
+ nnz + m log(m) log2(mn)


.
As in the variance-reduced ℓ1-ℓ1 setting, the dominant term in the runtime is the complexity of
calling AEMy.AddSparse in each iteration. Recall that the distribution p in every case is given by
pij(z; w0) := [zy]i + 2[wy
0]i
3
· |Aij|
∥Ai:∥1
With probability 2/3 we sample a coordinate i from the precomputed data structure for sampling
from wy
0, and otherwise we sample i via AEMy.Sample(). Then, we sample an entry j proportional to
its magnitude from the ℓ1 sampling oracle for Ai: in constant time. The runtime is dominated by

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
612
O(log(m) log(mn)).
To sample from the distribution q in (B.36), we follow the outline in Section B.4.3. Similarly, for
sampling from distributions (B.37) and (B.38), we follow the outline in Section B.4.3 but replace all
calls to an IterateMaintainer instance with a call to CIMx
2 initialized with an appropriate weight
vector. In all cases, the runtime is O(log m) which does not dominate the iteration complexity.
Finally, it is clear from discussions in previous sections that the iterate maintenance invariants
of our data structures are preserved by the updates used in this implementation.
Algorithm guarantee
Theorem 79. In the ℓ2-ℓ1 setup, let nnz′ := nnz + m log(m) log2(mn). The implementation in
Section B.4.3 with the optimal choice of α = max

ϵ/3, L2,1
co log(m) log (mn) /
√
nnz′

has runtime
O
  
nnz′ +
 L2,1
co
2 log2(m) log2(mn)
α2
!
α log(m)
ϵ
!
= O
 
nnz′ +
√
nnz′L2,1
co log(mn) log2(m)
ϵ
!
and outputs a point ¯z ∈Z such that
E [Gap(z)] ≤ϵ.
Proof. The correctness of the algorithm is given by the discussion in Section B.4.3 and the guarantees
of Proposition 10 with K = 3αΘ/ϵ, εouter = 2ϵ/3, εinner = ϵ/3, Proposition 11 with ϕ = ϵ/18 and
data structure ApproxExpMaintainer with our choice of
˜ε := (m + n)−8
to meet the approximation conditions in Line 5, 7 and 8 in Algorithm 12. The runtime bound is
given by the discussion in Section B.4.3, and the optimal choice of α is clear.
B.5
Additional results on variance-reduced methods
B.5.1
Row-column sparsity variance-reduced methods
By instantiating relaxed proximal oracles with row-column based gradient estimators, implemented
with the data structures we develop in Section 3.5, we obtain the row-column sparsity based runtimes
as stated in Table 3.2. Notably, up to logarithmic factors, we generically replace a dependence on
O(m + n) in a prior version of this work [111] with O(rcs), where rcs is deﬁned as the maximum
number of nonzero entries for any row or column. In this section, we give implementation details,
as well as the instantiation of our framework using row-column gradient estimators.
Our row-column estimators ˜gw0 in this section, parameterized by reference point w0, sample a
full column or row of the matrix (rather than a coordinate). To compute ˜gw0(z) we sample i ∼p(z)

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
613
and j ∼q(z) independently according to a speciﬁed distribution depending on the setup, and use
the estimator
˜gw0(z) :=

A⊤wy
0 + Ai:
[zy]i −[wy
0]i
pi(w)
, −Awx
0 −A:j
[zx]j −[wx
0]j
qj(w)

,
(B.39)
The key diﬀerence between this estimator with that of Section 8 is that its diﬀerence with g(w0)
is O(rcs)-sparse rather than O(1)-sparse, requiring MultSparse steps with O(rcs)-sparse vectors. In
all other respects, the implementation details are exactly the same as those in Section 3.4.2 and
Appendix B.4, so we omit them for brevity. We now state our sampling distributions used with the
estimator form (B.39), and the corresponding centered local variance bounds.
In the ℓ1-ℓ1 setup, we use the sampling distribution (from reference point w0 ∈∆m × ∆n)
pi(z) := [zy]i + 2[wy
0]i
3
and
qj(z) := [zx]j + 2[wx
0]j
3
.
(B.40)
Lemma 235. In the ℓ1-ℓ1 setup, gradient estimator (B.39) using the sampling distribution in (B.40)
is a
√
2 ∥A∥max-centered-local estimator.
Proof. Unbiasedness holds by deﬁnition. For the variance bound, it suﬃces to show that
E ∥˜gw0(z) −g(w0)∥2
∞≤2 ∥A∥2
max Vwx
0(zx);
clearly this implies the weaker relative variance bound statement (along with an analogous bound
on the y block). To this end, we have
E ∥˜gw0(z) −g(w0)∥2
∞≤
X
i∈[m]
∥Ai:∥2
∞[zy −wy
0]2
i
pi(z)
≤2 ∥A∥2
max Vwx
0(zx),
where the last inequality used Lemma 32.
In the ℓ2-ℓ2 setup, we use the oblivious sampling distribution
pi = ∥Ai:∥2
2
∥A∥2
F
and qj = ∥A:j∥2
2
∥A∥2
F
.
(B.41)
We proved that gradient estimator (B.39) using the sampling distribution in (B.41) admits a
∥A∥F-centered estimator in [111], which is an equivalent deﬁnition to Deﬁnition 5 in the ℓ2-ℓ2 setup.
For self-containment, we reproduce this proof here.
Lemma 236. In the ℓ2-ℓ2 setup, gradient estimator (B.39) using the sampling distribution in (B.41)
is a ∥A∥F-centered estimator.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
614
Proof. Unbiasedness holds by deﬁnition. For the variance bound,
E ∥˜gw0(z) −g(w0)∥2
2 =
X
i∈[m]
∥Ai:∥2
2
pi
([zy]i −[wy
0]i)2 +
X
j∈[n]
∥A:j∥2
2
qj
([zx]j −[wx
0]j)2
= ∥A∥2
F ∥z −w0∥2
2 .
In the ℓ2-ℓ1 setup, we use the sampling distribution (from reference point w0 ∈Bn × ∆m)
pi(z) = [zy]i + 2[wy
0]i
3
and
qj(z) = ([zx]j −[wx
0]j)2
∥zx −wx
0∥2
2
.
(B.42)
Lemma 237. In the ℓ2-ℓ1 setup, gradient estimator (B.39) using the sampling distribution in (B.42)
is a
√
2L-centered-local estimator with L = maxi∈[m] ∥Ai:∥2 = ∥A∥2→∞.
Proof. Unbiasedness holds by deﬁnition. For the variance bound, we ﬁrst note
E
h˜gx
w0(z) −gx(w0)
2
2
i
≤
X
i∈[m]
∥Ai:∥2
2
([zy]i −[wy
0]i)2
1
3[zy]i + 2
3[wy
0]i
≤max
i∈[m] ∥Ai:∥2
2

X
i∈[m]
([zy]i −[wy
0]i)2
1
3[zy]i + 2
3[wy
0]i


≤2 max
i∈[m] ∥Ai:∥2
2 Vwy
0(zy),
where for the last inequality we use Lemma 32. On the other block, we have
max
i∈[m] E

˜gy
w0(w) −gy(w0)
2
i ≤max
i∈[m]
X
j∈[n]
A2
ij[wx −wx
0]2
j
qj(w)
= 2 max
i∈[m] ∥Ai:∥2
2 Vwx
0(wx).
Summing these two bounds concludes the proof.
B.5.2
Extensions with composite terms
In this section, we give a brief discussion of how to change Proposition 11 and implementations
of the procedures in Sections 3.3.1 and 3.3.2 to handle modiﬁed regularization in the context of
Proposition 19, and composite regularization terms in the objective in the methods of Section 3.6.
Speciﬁcally we consider a composite optimization problem of the form:
min
x∈X max
y∈Y y⊤Ax + µxφ(x) −µyψ(y) where φ = V x
x′ and ψ = V y
y′.
For simplicity of notation we deﬁne Υ(x, y) := µxφ(x) + µyψ(y). We remark that x′ = 0 recovers
the case of φ = rx when X = Bn, and x′ = 1
n1 recovers the case of φ = rx when X = ∆n (similarly
setting y′ allows us to recover this for the y block).

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
615
Changes to inner loop
In this section, we ﬁrst discuss the necessary changes to Algorithm 12 and Proposition 11. For
simplicity of notation, we denote ρ :=
p
µx/µy, ˆV x := ρV x, ˆV y := 1
ρV y, ˆV := ˆV x + ˆV y.
Algorithm 68: InnerLoop(w0, ˜gw0, ϕ)
1 Input: Initial w0 ∈Z, (L, α)-centered-local gradient estimator ˜gw0, oracle quality α > 0;
2 Parameters: Step size η, number of iterations T, approximation tolerance ϕ;
3 Output: Point ˜w satisfying Deﬁnition 6;
4 for t = 1, . . . , T do
5
ˆwt−1 ≈wt−1 satisfying ˆVw0( ˆwt−1) −ˆVw0(wt−1) ≤ϕ
α and ∥ˆwt−1 −wt−1∥≤
ϕ
LD;
6
w⋆
t ←argmin
n
⟨clip(η˜gw0( ˆwt−1) −ηg(w0)), w⟩+ ηΥ(w) + ηα
2 ˆVw0(w) + ˆVwt−1(w)
o
;
7
wt ≈w⋆
t satisfying maxu
h
ˆVwt(u) −ˆVw⋆
t (u)
i
≤
ϕ
1+√µxµy , ˆVw0(wt) −ˆVw0(w⋆
t ) ≤ϕ
α, and
ˆVz′(wt) −ˆVz′(w⋆
t ) ≤
ϕ
√µxµy ;
8 Return: ˜w ≈1
T
PT
t=1 wt satisfying
 ˜w −1
T
PT
t=1 wt
 ≤
ϕ
LD,
maxu
h
ˆV ˜
w(u) −ˆV ¯
w(u)
i
≤
ϕ
√µxµy , ˆVw′( ˜w) −ˆVw′( ¯w) ≤
ϕ
√µxµy , and ∥wt −w⋆
t ∥≤
ϕ
2LD
Corollary 58. Let (Z, ∥·∥·, r, Θ, clip) be any local norm setup. Let w0 ∈Z, εinner > 0, and ˜gw0
be an L-centered-local estimator for some L ≥α ≥εinner. Assume the problem has bounded domain
size maxz∈Z ∥z∥≤D, g is L-Lipschitz, i.e. ∥g(z) −g(z′)∥∗≤L ∥z −z′∥, that g is LD-bounded, i.e.
maxz∈Z ∥g(z)∥∗≤LD, and ˆw0 = w0. Then, for η =
α
10L2 , T ≥
8
ηα ≥60L2
α2 , ϕ = εinner
10 , Algorithm 68
outputs a point ˆw ∈Z such that
E max
u∈Z [⟨g( ˜w) + ∇Υ( ˜w), ˜w −u⟩−αVw0(u)] ≤εinner,
(B.43)
i.e. Algorithm 68 is an (α, εinner)-relaxed proximal oracle.
Proof sketch. Note that the only change is in the deﬁnition of the regularized mirror descent step
with extra composite terms
w⋆
t ←argmin
n
⟨clip(η˜gw0( ˆwt−1) −ηg(w0)), w⟩+ ηΥ(w) + ηα
2
ˆVw0(w) + ˆVwt−1(w)
o
.
Denote ∇Υ(w) = (µx∇φ(wx), µy∇ψ(wy)), so that for the ﬁnal regret bound there are two additional
error terms. The ﬁrst term comes from the error in regularized mirror descent steps via (denoting

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
616
z′ = (x′, y′))
1
T
X
t∈[T ]
[−⟨∇Υ(w⋆
t ), w⋆
t −u⟩+ ⟨∇Υ(wt), wt −u⟩]
≤
√µxµy
T
X
t∈[T ]

ˆVz′(wt) −ˆVz′(w⋆
t ) + ˆVwt(u) −ˆVw⋆
t (u)

≤2ϕ
following the approximation guarantee in Line 7. The other term comes from averaging error. Denote
the true average iterate by ¯w := 1
T
P
t∈[T ] wt. We have ∀u ∈Z,
⟨g( ˜w), ˜w −u⟩−1
T
X
t∈[T ]
⟨g(wt), wt −u⟩= −⟨g( ˜w), u⟩−⟨g( ¯w), ¯w −u⟩
= ⟨g( ¯w) −g( ˜w), u⟩≤ϕ,
and also
⟨∇Υ( ˜w), ˜w −u⟩= ⟨∇Υ( ˜w) −∇Υ( ¯w), ˜w −u⟩+ ⟨∇Υ( ¯w), ˜w −¯w⟩+ ⟨∇Υ( ¯w), ¯w −u⟩
(i)
= √µxµy

−ˆV ¯
w(u) + ˆV ˜
w(u) + ˆV ¯
w( ˜w)

+ ⟨∇Υ( ¯w), ˜w −¯w⟩+ ⟨∇Υ( ¯w), ¯w −u⟩
(ii)
= √µxµy

−ˆV ¯
w(u) + ˆV ˜
w(u) + ˆVw′( ˜w) −ˆVw′( ¯w)

+ ⟨∇Υ( ¯w), ¯w −u⟩,
(iii)
≤2ϕ + ⟨∇Υ( ¯w), ¯w −u⟩
(iv)
≤2ϕ + 1
T
X
t∈[T ]
⟨∇Υ(wt), wt −u⟩.
where we use (i) the three-point property of Bregman divergence, (ii) the fact that ˆV ¯
w( ˜w) +
⟨∇Υ( ¯w), ˜w −¯w⟩= ˆVw′( ˜w) −ˆVw′( ¯w) again by the three-point property, (iii) the approximation
guarantee of Line 8, and (iv) the fact that ⟨∇Υ(w), w −u⟩is convex in w for our choices of Υ.
Hence incorporating the above extra error terms into the regret bound yields the conclusion, as
10ϕ = εinner by our choice of ϕ.
Changes to implementation
Broadly speaking, all of these modiﬁcations can easily be handled via appropriate changes to the
initial data given to our data structures CenteredIterateMaintainer2 and ApproxExpMaintainer.
We discuss general formulations of iterations with these modiﬁcations in both simplices and Eu-
clidean balls, and provide appropriate modiﬁcations to the inital data given to our data structures.
Finally, it is simple to check that all relevant parameters are still bounded by a polynomial in the di-
mensions of variables, so no additional cost due to the data structure is incurred. For simplicity here
we only considerfor the x-block when φx(x) = µr(x) and remark that the case when φx(x) = µVx′(x)

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
617
for some x′ follows similarly.
ℓ1 domains. For this section, deﬁne a domain X = ∆n, let r(x) = P
j∈[n] xj log xj be entropy,
and let µ, α, η, ρ be nonnegative scalar parameters. Consider a sequence of iterates of the form
xt+1 ←argminx∈X ⟨˜gx0(xt), x⟩+ µr(x) + αρ
2 Vx0(x) + ρ
η Vxt(x).
This update sequence, for the form of gradient estimator
˜gx0(x) = g(x0) + b + g′(x),
where g′(x) is a vector with suitable sparsity assumptions depending on the point x, and b is some
ﬁxed vector, generalizes all of the settings described above used in our various relaxed proximal
oracle implementations. Optimality conditions imply that the update may be rewritten as
xt+1 ←Π∆
 
exp
 ρ
η log xt + αρ
2 log x0 −g(x0) −b −g′(xt)
µ + αρ
2 + ρ
η
!!
.
Thus, initializing an ApproxExpMaintainer instance with
κ =
1
µη
ρ + αη
2 + 1, v =
αρ
2 log x0 −g(x0) −b
µ + αρ
2 + ρ
η
enables DenseStep to propagate the necessary changes to the iterate; we propagate changes due to
g′(xt) via AddSparse and the appropriate sparsity assumptions.
ℓ2 domains. For this section, deﬁne a domain X = Bn, let r(x) = 1
2 ∥x∥2
2 be entropy, and let µ,
α, η, ρ be nonnegative scalar parameters. Consider a sequence of iterates of the form
xt+1 ←argminx∈X ⟨˜gx0(xt), x⟩+ µr(x) + αρ
2 Vx0(x) + ρ
η Vxt(x).
This update sequence, for the form of gradient estimator
˜gx0(x) = g(x0) + b + g′(x),
where g′(x) is a vector with suitable sparsity assumptions depending on the point x, and b is some
ﬁxed vector, generalizes all of the settings described above used in our various relaxed proximal
oracle implementations. Optimality conditions imply that the update may be rewritten as
xt+1 ←ΠBn
 ρ
ηxt + αρ
2 x0 −g(x0) −b −g′(xt)
µ + αρ
2 + ρ
η
!
.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
618
Thus, initializing an CenteredIterateMaintainer instance with
v =
αρ
2 x0 −g(x0) −b
µ + αρ
2 + ρ
η
enables AddDense, Scale, and GetNorm to propagate the necessary changes to the iterate; we prop-
agate changes due to g′(xt) via AddSparse and the appropriate sparsity assumptions.
B.6
Deferred proofs from Section 3.6
B.6.1
Proofs from Section 3.6.1
Proof of Lemma 39. We consider the following (µ, µ)-strongly monotone problem, for various levels
of µ:
max
x∈Bn min
y∈∆m fµ(x, y) := y⊤˜Ax + y⊤b + µ
X
i∈[m]
[y]i log[y]i −µ
2 ∥x∥2
2 .
We claim we can implement an (α, ε)-relaxed proximal oracle for this problem in time
eO
  L2,1
co
2
α2
!
.
The oracle is a composite implementation of Algorithm 12 as in Algorithm 68, using the estimator of
Appendix B.4.3. By an application of Proposition 19, the overall complexity of solving this problem
is (by choosing the optimal α, and overloading the constant L2,1
co to be with respect to ˜A):
eO
  
nnz +
 L2,1
co
2
α2
!
α
µ
!
= eO

nnz +
√nnz · L2,1
co
µ

.
By conducting a line search over the parameter µ via repeatedly halving, the total cost of solving
each of these problems is dominated by the last setting, wherein µ = Θ(r∗/ log m), and R/µ = eO (ρ);
here, we recall that we rescaled ˜A so that L2,1
co = O(R). We defer details of the line search procedure
to Lemma C.3 of [19].
Proof of Theorem 12. We solve the problem (3.61) to duality gap ϵˆr/8 ≤ϵr∗, using the algorithm
of Appendix B.4.3 for ℓ2-ℓ1 games. The complexity of this algorithm is (choosing α optimally)
eO
  
nnz( ˜A) +
 L2,1
co
2
α2
!
· α
ϵˆr
!
= eO

nnz + ρ√nnz · L2,1
co
ϵ

,
as claimed. Here, we used that ˜A is a rescaling of A by 2R, and ˆr is a constant multiplicative approx-
imation of r. The approximate solution (x∗
ϵ′, y∗
ϵ′) obtains the requisite duality gap in expectation;

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
619
Markov's inequality implies that with logarithmic overhead in the runtime, we can obtain a pair of
points satisfying with high probability
max
x
f(x, y∗
ϵ′) −min
y
f(x∗
ϵ′, y) = max
x
f(x, y∗
ϵ′) −f(x∗, y∗) + f(x∗, y∗) −min
y
f(x∗
ϵ′, y) ≤ϵ′.
Because y∗is the best response to x∗, we have f(x∗, y∗
ϵ′) ≥f(x∗, y∗), which implies
max
x
f(x, y∗
ϵ′) −f(x∗, y∗) = max
x
f(x, y∗
ϵ′) −f(x∗, y∗
ϵ′) + f(x∗, y∗
ϵ′) −f(x∗, y∗) ≥0.
Combining yields f(x∗, y∗) −miny f(x∗
ϵ′, y) ≤ϵ′ ≤ϵr∗, so since f(x∗, y∗) = r∗, rearranging implies
miny f(x∗
ϵ′, y) ≥r∗−ϵ′ ≥(1 −ϵ)r∗. Thus, x∗
ϵ′ is an ϵ-approximate solution for Max-IB.
B.6.2
Proofs from Section 3.6.2
Proof of Lemma 40. If (x′, y′) is an approximately optimal solution with duality gap ϵ/16 for (3.64),
by deﬁnition
max
y∈∆m fϵ′(x′, y) −min
x∈Rn fϵ′(x, y′) ≤ϵ
16.
Therefore, the following sequence of inequalities hold:
max
y∈∆m f(x′, y) −min
x∈Rn f(x, y′) =

max
y∈∆m f(x′, y) −max
y∈∆m fϵ′(x′, y)

+

max
y∈∆m fϵ′(x′, y) −min
x∈Rn fϵ′(x, y′)

+

min
x∈Rn fϵ′(x, y′) −min
x∈Rn f(x, y′)

(i)
≤

max
y∈∆m f(x′, y) −max
y∈∆m fϵ′(x′, y)

+ ϵ
16 +

min
x∈Rn fϵ′(x, y′) −min
x∈Rn f(x, y′)

(ii)
≤ϵ
32 + ϵ
16 + ϵ
32 = ϵ
8.
In (i), we used the fact that the pair (x′, y′) has good duality gap with respect to fϵ′, and in (ii)
we used that for the ﬁrst summand, fϵ′(x′, ·) approximates f(x′, ·) to an additive ϵ/32, and for the
third summand, −ϵ′ P
i∈[m][y′]i log[y′]i is bounded by ϵ/32, and all other terms cancel.
B.6.3
Proofs from Section 3.6.3
Proof of Lemma 41. At optimality for (3.65), it holds that



y∗
x′ = 1
β (Ax∗
x′ −b)
x∗
x′ = x′ −1
β A⊤y∗
x′
.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
620
By substituting y∗
x′ and rearranging terms we get

I + 1
β2 A⊤A

(x∗
x′ −x∗) = x′ −x∗,
which in turn gives
∥x∗
x′ −x∗∥2 =


I + 1
β2 A⊤A
−1
(x′ −x∗)

2
≤
1
1 + µ/β2 ∥x′ −x∗∥2 .
For the last inequality we use the fact that
I + 1
β2 A⊤A

−1
2
= λmin

I + 1
β2 A⊤A
−1
=
1
1 + µ/β2 ,
by the deﬁnition of µ and since I and A⊤A commute.
Theorem 14. Given data matrix A ∈Rm×n, vector b ∈Rm, and desired accuracy ϵ ∈(0, 1),
assuming A⊤A ⪰µI for µ > 0, Algorithm 69 outputs an expected ϵ-accurate solution ˜x, i.e.
E [∥˜x −x∗∥2] ≤ϵ,
and runs in time
eO

nnz + √nnz ·
max
npP
i ∥Ai:∥2
1,
qP
j ∥A:j∥2
1
o
√µ

.
Proof. We ﬁrst prove correctness. We bound the progress from x(h) to x(h+1), for some h ∈[H], by
1
2
x(h+1) −x∗
2
2 ≤
x(h+1) −x∗
x(h)

2
2 +
x∗
x(h) −x∗2
2 ≤2Vz(h+1)(z∗
x(h)) +
x∗
x(h) −x∗2
2 .
(B.44)
The ﬁrst inequality used ∥a + b∥2
2 ≤2 ∥a∥2
2 + 2 ∥b∥2
2, and the second used the deﬁnition of the
divergence in the ℓ2-ℓ2 setup. Next, choosing a suﬃciently large value of K = eO (β/µ), we use
Proposition 19 to obtain a point z(h+1) satisfying
Vz(h+1)(z∗
x(h)) ≤ϵ2
80Vz(h)(z∗
x(h)) ≤ϵ2
40Vz(h)(z∗) + ϵ2
40Vz∗(z∗
x(h)).
(B.45)
Further, using Lemma 41 with x′ = x(h), β = √µ yields
x∗
x(h) −x∗
2 ≤1
2
x(h) −x∗
2 .
(B.46)

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
621
Algorithm 69: Coordinate variance reduced method for linear regression
1 Input: Matrix A ∈Rm×n with ith row Ai: and jth column A:j, vector b ∈Rm, accuracy ϵ
Output: A point ˜x with ∥˜x −x∗∥2 ≤ϵ
2 L ←max
npP
i ∥Ai:∥2
1,
qP
j ∥A:j∥2
1
o
, α ←L/√nnz, β = √µ, η ←
α
4L2 ;
3 T ←
l
4
ηα
m
, K ←eO (α/β), H = eO (1), z(0) = (x(0), y(0)) ←(0n, 0m), (zx
0, zy
0) ←(0n, 0m);
4 for h = 1, 2, · · · , H do
5
for k = 1, . . . , K do
▷Relaxed oracle query:
6
(x0, y0) ←(zx
k−1, zy
k−1), (gx
0, gy
0) ←(A⊤y0 + β(x0 −x(h−1)), −Ax0 + βy0);
7
for t = 1, . . . , T do
▷Gradient estimation:
8
Sample i ∼p where pi = ([yt−1]i −[y0]i)2
∥yt−1 −y0∥2
2
;
9
Sample j ∼q where qj = ([xt−1]j −[x0]j)2
∥xt−1 −x0∥2
2
;
10
Set ˜gt−1 = g0 +

Ai:
[yt−1]i −[y0]i
pi
, −A:j
[xt−1]j −[x0]j
qj

;
▷Mirror descent step:
11
xt ←
1
1 + ηα/2

xt−1 + ηα
2 x0 −η˜gx
t−1

12
yt ←ΠY

1
1 + ηα/2

yt−1 + ηα
2 y0 −η˜gy
t−1

▷ΠY(v) =
v
max{1,∥v∥2} ;
13
zk−1/2 ←1
T
T
X
t=1
(xt, yt);
▷Extragradient step:
14
zx
k ←
α
α + 2β zx
k−1 +
2β
α + 2β zx
k−1/2 −
1
α + 2β

A⊤zy
k−1/2 + β(zx
k−1/2 −x(h−1))

;
15
zy
k ←ΠY

α
α + 2β zy
k−1 +
2β
α + 2β zy
k−1/2 +
1
α + 2β

Azx
k−1/2 −βzy
k−1/2

;
▷Reshifting the oracle:
16
z(h) = (x(h), y(h)) ←zK = (zx
K, zy
K);
17 Return: ˜x ←x(H)

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
622
Plugging these two bounds into (B.44), and using the form of the divergence in the ℓ2-ℓ2 setup,
1
2
x(h+1) −x∗
2
2
(B.45)
≤
ϵ2
20Vz(h)(z∗) + ϵ2
20Vz∗(z∗
x(h)) +
x∗
x(h) −x∗2
2
(B.46)
≤
1
2
 ϵ2
20 + ϵ2
20 + 1
2
 x(h) −x∗
2
2 + ϵ2
40
y(h) −y∗
2
2 +
y∗
x(h) −y∗2
2

≤3
4 · 1
2
x(h) −x∗
2
2 + ϵ2
5 .
(B.47)
In the last inequality we use the conditions that ϵ ∈(0, 1) and Y = Bm. Recursively applying this
bound for h ∈[H], and for a suﬃciently large value of H = eO (1), we have the desired
x(H) −x∗
2
2 ≤
3
4
H x(0) −x∗
2
2 + 4ϵ2
5
≤ϵ2.
To bound the runtime, recall the inner loop runs for T = O((L2,2
co )2/α2) iterations, each costing
constant time, and the outer loop runs for K = eO (α/β) iterations, each costing O(T +nnz). Finally,
since H = eO (1), the overall complexity of the algorithm is
eO
  
nnz +
 L2,2
co
2
α2
!
α
β
!
.
Choosing α = max{L2,2
co /√nnz, β} optimally and substituting
β = √µ, L2,2
co = max



sX
i
∥Ai:∥2
1,
sX
j
∥A:j∥2
1


,
we have the desired runtime bound on Algorithm 69.
B.7
Strongly monotone proximal method
We provide a proof of a generalization of Proposition 19, namely Proposition 56. It is straightforward
to deduce Proposition 19 from Proposition 56; by the deﬁnition of our distance-generating function
r in Proposition 19, g is µ-strongly monotone in r, and the range of the regularizer is aﬀected by a
ρ + ρ−1 factor. Finally, we mention that we proved a similar convergence rate in Appendix A.4, but
it used a slightly diﬀerent algorithm (namely, it generated the intermediate step using a ﬁrst-order
extrapolation step rather than a relaxed proximal oracle). For our variance reduction results, we
require the specialization of strongly monotone extragradient methods to proximal oracles developed
in this section.
As a brief refresher, in this section we consider variational inequalities with strongly monotone

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
623
operators. Following [375], we say that an operator g is µ-strongly monotone relative to the distance
generating function r if it satisﬁes
⟨g(z′) −g(z), z′ −z⟩≥µ · Vz′(z) for all z′, z ∈Z.
(B.48)
By strong convexity of r, a µ-strongly monotone operator relative to r is also µ-strongly mono-
tone in the standard sense, i.e., ⟨g(z′) −g(z), z′ −z⟩≥
µ
2 ∥z′ −z∥2 for all z, z′ ∈Z. For saddle
point problems minx∈X maxy∈Y f(x, y) with gradient mapping g(x, y) = (∇xf(x, y), −∇yf(x, y))
and separable r(x, y) = rx(x) + ry(y), the property (B.48) holds whenever f(x, y) −µrx(x) + µry(y)
is convex-concave, i.e., whenever f(·, y) is µ-strongly convex relative to rx for every y and f(x, ·) is
µ-strongly-concave relative to ry for every x.
Strongly monotone operators have a unique saddle point z⋆satisfying
max
u∈Z ⟨g(z⋆), z⋆−u⟩≤0.
(B.49)
We show that Algorithm 70—a small modiﬁcation of Algorithm 11—converges linearly to z⋆(i.e.,
with the distance to z⋆decaying exponentially in the number of iterations). The main diﬀerence
between the algorithms is that the extra-gradient step in Algorithm 70 has an additional regulariza-
tion term around zk−1/2, making it similar in form to the regularized mirror descent steps we use in
Algorithm 12. In addition, Algorithm 70 outputs the last iterate rather the iterate average.
Algorithm 70: OuterLoopStronglyMonotone(O)
1 Input: (α, ε)-relaxed proximal oracle O(z) for gradient mapping g satisfying (B.48);
2 Parameters: Number of iterations K;
3 Output: Point zK with EVzK(z⋆) ≤(
α
µ+α)KΘ + ε
µ;
4 z0 ←argminz∈Zr(z) ;
5 for k = 1, . . . , K do
6
zk−1/2 ←O(zk−1)
▷We implement O(zk−1) by calling InnerLoop(zk−1, ˜gzk−1, α) ;
7
zk ←argminz∈Z

g
 zk−1/2

, z

+ αVzk−1(z) + µVzk−1/2(z)
	
;
8 Return: zK;
Proposition 56. Let O be an (α,ε)-relaxed proximal oracle for a gradient mapping g that is µ-
strongly monotone relative to distance-generating function r with range at most Θ. Let zK be the
output of Algorithm 70. Then
E VzK(z⋆) ≤

α
µ + α
K
Θ + ε
µ.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
624
If in addition g is a gradient mapping for f and ∥(∇xf(z), −∇yf(z′))∥∗≤G for all z, z′ ∈Z, then
E Gap(zK) ≤
√
2G
s
α
µ + α
K
Θ + ε
µ.
Proof. Fix an iteration k. Using µ-strong-monotonicity (B.48) and optimality of z⋆(B.49) yields
µVzk−1/2(z⋆)
(B.48)
≤

g(zk−1/2) −g(z⋆), zk−1/2 −z⋆ (B.49)
≤

g(zk−1/2), zk−1/2 −z⋆
=

g(zk−1/2), zk −z⋆
+

g(zk−1/2), zk−1/2 −zk

.
(B.50)
Next,

g(zk−1/2), zk −z⋆ (i)
≤−⟨α∇Vzk−1(zk) + µ∇Vzk−1/2(zk), zk −z⋆⟩
(ii)
≤µ(Vzk−1/2(z⋆) −Vzk(z⋆) −Vzk−1/2(zk)) + α(Vzk−1(z⋆) −Vzk(z⋆) −Vzk−1(zk))
≤αVzk−1(z⋆) −(µ + α)Vzk(z⋆) + µVzk−1/2(z⋆) −αVzk−1(zk),
(B.51)
where we use (i) the ﬁrst-order optimality condition for zk and (ii) the three-point property of
Bregman divergence. In the last inequality, we also use nonnegativity of Vzk−1/2(zk). Now, combin-
ing (E.8) and (E.10) yields
µVzk−1/2(z⋆) ≤αVzk−1(z⋆) −(µ + α)Vzk(z⋆) + µVzk−1/2(z⋆) + [

g(zk−1/2), zk−1/2 −zk

−αVzk−1(zk)].
Rearranging terms, taking an expectation, and using the Deﬁnition 6 of an (α, ε)-relaxed proximal
oracle yields
EVzk(z⋆) ≤
α
µ + αEVzk−1(z⋆) +
ε
µ + α.
Applying this bound recursively K times and using that Vz0(u) = r(u) −r(z0) ≤Θ for z0 the
minimizer of r, we have
EVzK(z⋆) ≤

α
µ + α
K
Θ +
K−1
X
k=0

α
µ + α
k 
ε
µ + α

≤

α
µ + α
K
Θ + ε
µ.
To bound Gap(zK) (deﬁned in (3.24)), write gap(z; u) = f(zx, uy) −f(ux, zy).
Then we have
∥∇zgap(z; u)∥∗= ∥(∇xf(zx, uy), −∇yf(uy, zx))∥∗≤G by assumption (since Z = X × Y), and
therefore gap(z; u) is G-Lipschitz in z. Consequently, for any u ∈Z,
gap(zK; u) ≤gap(z⋆; u) + G ∥zK −z⋆∥≤G ∥zK −z⋆∥,
where the second transition follows from the optimality of z⋆. Therefore, the deﬁnition (3.24) of

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
625
Gap, strong convexity of r and Jensen's inequality yield
EGap(zK) = E max
u∈Z gap(zK; u) ≤G E ∥zK −z⋆∥≤EG
p
2VzK(z⋆) ≤
√
2G
p
EVzK(z⋆).
Substituting the bound on EVzK(z⋆) concludes the proof.
B.8
IterateMaintainer2: numerical stability and variations
B.8.1
Numerical stability of IterateMaintainer1.
We discuss the implementation of a numerically stable version of IterateMaintainer1, and the
complexity of its operations, for use in our sublinear algorithms in Section 3.4.1 and Section B.3.2.
We discuss this implementation for a simplex block, e.g. a simplex variable of dimension n, as for
an ℓ2 geometry numerical stability is clear. The main modiﬁcations we make are as follow.
• We reinitialize the data structure whenever the ﬁeld ν grows larger than some ﬁxed polynomial
in n, or if n/2 iterations have passed.
• We track the coordinates modiﬁed between restarts.
• Every time we reinitialize, we maintain the invariant that the multiplicative range of coordi-
nates of x is bounded by a polynomial in n, i.e. maxj xj/ minj xj is bounded by some ﬁxed
polynomial in n. We will implement this via an explicit truncation, and argue that such an
operation gives negligible additive error compared to the accuracy of the algorithm.
• We implicitly track the set of truncated coordinates at each data structure restart. We do so
by explicitly tracking the set of non-truncated coordinates whenever a truncation operation
happens (see the discussion below), in constant amortized time.
We now discuss the complexity and implementation of these restarts. First, note that ν can never
decrease by more than a multiplicative polynomial in n between restarts, because of nonnegativity
of the exponential, the fact that the original range at the time of the last restart is multiplicatively
bounded, and we restart every time half the coordinates have been touched. Thus, the only source
of numerical instability comes from when ν grows by more than a multiplicative polynomial in n.
Suppose this happens in τ iterations after the restart. Then,
• If τ < n/2, we claim we can implement the restart in O(τ), so the amortized cost per iteration is
O(1). To see this, for every coordinate touched in these τ iterations, we either keep or explicitly
truncate if the coordinate is too small. For every coordinate not touched in these τ iterations,
the relative contribution is at most inverse polynomial in n; we truncate all such coordinates.
Then, we compute the normalization constant according to all non-truncated coordinates, such

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
626
that the value of all truncated coordinates is set to a ﬁxed inverse polynomial in n. We can
implement this by implicitly keeping track of the set of truncated coordinates as well as their
contribution to the normalization factor, and explicitly setting their value in the data structure
when they are updated by AddSparse. Overall, this does not aﬀect the value of the problem
by more than a small multiple of ϵ, by our assumptions on Lrc/ϵ. To see that we can track the
non-truncated coordinates explicitly, we note that it is a subset of the at most τ coordinates
that were touched, so this can be done in constant amortized time.
• If τ = n/2, we claim we can implement the restart in O(n), so the amortized cost per iteration
is O(1). This is clear: we can do so by explicitly recomputing all coordinates, and truncating
any coordinates which have become too small.
We describe how the data structure implements this through its maintained ﬁelds: for non-truncated
coordinates, we do not do anything other than change the scaling factor ν, and for truncated
coordinates, we reset the values of u, u′ in that coordinate appropriately once they have been sparsely
updated. Overall, this does not aﬀect the amortized runtime of our algorithm.
B.8.2
WeightedIterateMaintainer2
In this section, we give implementation details for a weighted generalization of IterateMaintainer2,
which we will call WeightedIterateMaintainer2. It is used in Section B.3.2, when using the sam-
pling distribution (B.26). At initialization, WeightedIterateMaintainer2 is passed an additional
parameter w ∈Rn
≥0, a nonnegative weight vector. We let
⟨u, v⟩w :=
X
j∈[n]
[w]j[u]j[v]j, ∥v∥w :=
q
⟨v, v⟩w.
WeightedIterateMaintainer2 supports all the same operations as IterateMaintainer2, with two
diﬀerences:
• For the current iterate x, WeightedIterateMaintainer2.Norm() returns weighted norm ∥x∥w.
• For the current iterate x, WeightedIterateMaintainer2.Sample() returns a coordinate j with
probability proportional to [w]j[x]2
j.
Similarly to IterateMaintainer2, WeightedIterateMaintainer2 maintains the following ﬁelds.
• Scalars ξu, ξv, σu, σv, ι, ν
• Vectors u, u′, v, w
• Precomputed value ∥v∥2
w.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
627
We maintain the following invariants on the data structure ﬁelds at the end of every operation:
• x = ξuu + ξvv, the internal representation of x
• s = v + σuu + σvv, the internal representation of running sum s
• ι = ⟨x, v⟩w, the weighted inner product of the iterate with ﬁxed vector v
• ν = ∥x∥w, the weighted norm of the iterate
To support sampling, our data structure also maintains a binary tree distx of depth O(log n).
For the node corresponding to S ⊆[n] (where S may be a singleton), we maintain
• P
j∈S[w]j[u]2
j, P
j∈S[w]j[u]j[v]j, P
j∈S[w]j[v]2
j
We now give the implementation of the necessary operations for WeightedIterateMaintainer2,
giving additional proofs of correctness when applicable.
Initialization.
• Init(x0, v, w). Runs in time O(n).
1. (ξu, ξv, u) ←(1, 0, x0).
2. (σu, σv, u′) ←(0, 0, 0n).
3. (ι, ν) ←(⟨x0, v⟩w , ∥x0∥w).
4. Compute and store ∥v∥2
w.
5. Initialize distx, storing the relevant sums in each internal node.
Updates.
Scale(c) and UpdateSum() follow identically to the analysis of IterateMaintainer2.
• AddSparse(j, c): [x]j ←[x]j + c. Runs in time O(log n).
1. u ←u +
c
ξu ej.
2. u′ ←u′ −cσu
ξu ej.
3. ν ←
p
ν2 + 2c[w]j[ξuu + ξvv]j + c2[w]j.
4. ι ←ι + c[w]j[v]j.
5. For internal nodes of distx on the path from leaf j to the root, update P
j∈S[w]j[u]2
j,
P
j∈S[w]j[u]j[v]j appropriately.
• AddDense(c): x ←x + cv. Runs in time O(1).
1. ξv ←ξv + c.

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
628
2. ν ←
q
ν2 + 2cι + c2 ∥v∥2
w.
3. ι ←ι + c ∥v∥2
w.
We demonstrate that the necessary invariants on ι, ν are preserved. Regarding correctness of
AddSparse, the updates to u and u′ are identical to in the analysis of IterateMaintainer2. Next,
because only [x]j changes, the updates to ν, ι are correct respectively by
[w]j · [ξuu + ξvv + c]2
j = [w]j ·
 [ξuu + ξvv]2
j + 2c[ξuu + ξvv]j + c2
,
[w]j · ([ξuu + ξvv + c]j) · [v]j = [w]j · ([ξuu + ξvv]j · [v]j + c[v]j) .
Regarding correctness of AddDense,
∥x + cv∥2
w = ν2 + 2cι + c2 ∥v∥2
w ,
⟨x + cv, v⟩w = ι + c ∥v∥2
w .
Here, we used that the invariants ν = ∥x∥w and ι = ⟨x, v⟩w held. Queries. Get(j) and GetSum(j)
follow identically to the analysis of IterateMaintainer2.
• Norm(): Return ∥x∥w. Runs in time O(1).
1. Return ν.
Sampling.
To support Sample, we must produce a coordinate j with probability proportional to [w]j[x]2
j. To
do so, we recursively perform the following procedure, where the recursion depth is at most O(log n),
starting at the root node and setting S = [n]: the proof of correctness is identical to the proof in
the analysis of IterateMaintainer2.Sample().
1. Let S1, S2 be the subsets of coordinates corresponding to the children of the current node.
2. Using scalars ξu, ξv, and the maintained P
j∈Si[w]j[u]2
j, P
j∈Si[w]j[u]j[v]j, P
j∈Si[w]j[v]2
j, com-
pute P
j∈Si[w]j[x]2
j = P
j∈Si[w]j[ξuu + ξvv]2
j for i ∈{1, 2}.
3. Sample a child i ∈{1, 2} of the current node proportional to P
j∈Si[w]j[x]2
j by ﬂipping an
appropriately biased coin. Set S ←Si.
B.8.3
CenteredIterateMaintainer2
In this section, we give implementation details for a generalization of WeightedIterateMaintainer2,
which we call CenteredIterateMaintainer2. It is used in Section B.4.3, when using the sampling

APPENDIX B. DEFERRED PROOFS FROM CHAPTER 3
629
distributions (B.37) and (B.38). At initialization, CenteredIterateMaintainer2 is passed an addi-
tional parameter x0 ∈Rn, a reference point. CenteredIterateMaintainer2 supports all the same
operations as WeightedIterateMaintainer2, with two diﬀerences:
• For the current iterate x, CenteredIterateMaintainer2.Sample() returns a coordinate j with
probability proportional to [w]j[x −x0]2
j.
• CenteredIterateMaintainer2 supports querying ∥x −x0∥2
w in constant time.
Because all the other operations, ﬁelds, and invariants supported and maintained by the data struc-
ture are exactly the same as IterateMaintainer2, we only discuss the changes made to the binary
tree distx in this section for brevity. In particular, to support sampling, our data structure also
maintains a binary tree distx of depth O(log n). For the node corresponding to S ⊆[n] (where S
may be a singleton), we maintain
• P
j∈S[w]j[u]2
j, P
j∈S[w]j[u]j[v]j, P
j∈S[w]j[v]2
j
• P
j∈S[w]j[x0]2
j, P
j∈S[w]j[u]j[x0]j, P
j∈S[w]j[v]j[x0]j
At initialization, CenteredIterateMaintainer2 creates this data structure and stores the relevant
sums in each internal node. Upon modiﬁcations to u due to updates of the form AddSparse(j, c),
CenteredIterateMaintainer2 propagates the changes along internal nodes of distx on the path
from leaf j to the root. Thus, using these maintained values and the stored values ξu, ξv, it is clear
that for any appropriate subset S, we are able to compute the quantity
X
j∈S
[w]j[ξu+ξvv−x0]2
j =
X
j∈S
[w]j
 ξ2
u[u]2
j + ξ2
v[v]2
j + 2ξuξv[u]j[v]j + [x0]2
j + 2ξu[u]j[x0]j + 2ξv[v]j[x0]j

in constant time, admitting the sampling oracle in time O(log n) by propagating down the tree
maintained by distx. This proves the desired sampling complexity. Finally, by appropriately querying
the stored values in the root node, we can return ∥x −x0∥2
w in constant time.

Appendix C
Deferred proofs from Chapter 4
C.1
Missing proofs from Section 4.1 and Section 4.2
C.1.1
Folklore bound on size of ℓ∞-strongly-convex functions
In this section, we prove the following claim which occurs in the literature, but does not seem to
usually be formally shown:
Lemma 238. Suppose ψ is 1-strongly convex with respect to the ℓ∞norm on [−1, 1]n. Then,
max
x∈[−1,1]n ψ(x) −
min
x∈[−1,1]n ψ(x) ≥n
2
Furthermore, this lower bound is tight, i.e. there is a 1-strongly convex function in the ℓ∞norm for
which equality holds.
Proof. We will prove this by iteratively constructing a set of points x0, x1, . . . xn ∈[−1, 1]n such that
for all i with 0 ≤i ≤n −1, we have
ψ(xi) ≤ψ(xi+1) −1
2
and consequently,
ψ(x0) ≤ψ(xn) −n
2
Let ei be the ith standard basis vector, namely the n-dimensional vector which is 1 in the ith
coordinate and 0 elsewhere.
Let x0 = (0, 0, . . . 0), the n-dimensional point which is 0 in every
coordinate. Let x+
1 = x0 +e1 and let x−
1 = x0 −e1, such that x0 = 1
2x+
1 + 1
2x−
1 . By strong convexity,
ψ(x0) ≤1
2ψ(x+
1 ) + 1
2ψ(x−
1 ) −1
8
x+
1 −x−
1
2
∞= 1
2ψ(x+
1 ) + 1
2ψ(x−
1 ) −1
2
630

APPENDIX C. DEFERRED PROOFS FROM CHAPTER 4
631
Consequently, it must be the case that at least one of
ψ(x0) ≤ψ(x+
1 ) −1
2
ψ(x0) ≤ψ(x−
1 ) −1
2
holds. Let x1 be the point x+
1 or x−
1 for which this holds.
More generally, suppose we have constructed x0, x1, . . . xi in this fashion, such that xi is 0 in
the coordinates i + 1, i + 2, . . . n. Then, let x+
i+1 = xi + ei+1 and let x−
i+1 = xi −ei+1, such that
xi = 1
2x+
i+1 + 1
2x−
i+1. Again by strong convexity, we have that at least one of
ψ(xi) ≤ψ(x+
i+1) −1
2
ψ(xi) ≤ψ(x−
i+1) −1
2
holds, and therefore we can pick one of the points x+
i+1, x−
i+1 to be the point xi+1. We can clearly
iteratively construct a point xn in this fashion, proving the claim.
To show that the lower bound is tight, consider ψ(x) = 1
2 ∥x∥2
2. Clearly this function has range
n
2 over [−1, 1]n. Furthermore, for all x ∈[−1, 1]n, and arbitrary vector z, we have
z⊤∇2ψ(x)z = z⊤Iz = ∥z∥2
2 ≥∥z∥2
∞
where this second-order condition is well-known to be equivalent to 1-strong convexity, for twice-
diﬀerentiable functions.
C.1.2
Reduction from general box-constrained ℓ∞regression to Deﬁni-
tion 8
In this section, we describe a general reduction from unconstrained ℓ∞regression and more arbitrary
box constraints to the setting where the domain of the argument is [−1, 1]m, proving Corollary 6.
Consider ﬁrst the problem of solving the generalized box-constrained regression problem
min
x∈[−r,r]m ∥Ax −b∥∞,
(C.1)
for some r > 0. By performing the change of variables ˜x = x/r, ˜b = b/r, it suﬃces to ﬁnd an
ϵ/r-approximate minimizer to
min
˜x∈[−1,1]m
A˜x −˜b

∞,
(C.2)

APPENDIX C. DEFERRED PROOFS FROM CHAPTER 4
632
which under the change of variables x ←r˜x recovers an ϵ-approximate minimizer to the original
problem. To see this, let x∗be the minimizer to (C.1); it is clear under a simple rescaling and
linearity of norms that ˜x∗:= x∗/r is the minimizer to (C.2). Next, let ˜x be any point in [−1, 1]m
with
A˜x −˜b

∞−
A˜x∗−˜b

∞≤ϵ
r.
By linearity of norms, we see that x = r˜x has x ∈[−r, r]m and
∥Ax −b∥∞−∥Ax∗−b∥∞≤ϵ,
i.e. x is an ϵ-approximate minimizer to (C.1). To bound the complexity of solving (C.2) to ϵ/r
additive accuracy, it suﬃces to invoke Theorem 15.
Next, to deal with the unconstrained case with the promise ∥x0 −x∗∥∞≤r, it suﬃces to perform
a change of variables b′ ←b −Ax0, x′ ←x −x0, and solve the problem
min
x′∈[−r,r]m ∥Ax′ −b′∥∞=
min
x′∈[−r,r]m ∥A(x −x0) −(b −Ax0)∥∞= min
x∈Rm ∥Ax −b∥∞.
To see the last inequality, we use the guarantee that ∥x0 −x∗∥∞≤r, i.e. x∗−x0 is a valid point
x′. We then can invoke the general box-constrained case with radius r.
Finally, we remark that similar additive shifts and rescalings allow us to handle the more general
box constraint Q
j∈[m][ℓj, rj] with appropriate (weighted) dependences on the quantities rj −ℓj.
C.1.3
Convergence rates of ﬁrst-order methods
In this section, we give guarantees for the convergence rates of the classical unaccelerated ﬁrst-order
methods of gradient descent in general norms and coordinate descent.
Gradient descent in general norms
We brieﬂy review the basic guarantees of gradient descent applied to a convex function f which is
L-smooth in an arbitrary norm ∥· ∥. The general framework of gradient descent initializes at some
point x0 and iteratively maximizes the primal progress using the upper bound guaranteed by the
smoothness. In particular, we perform the following update:
xk+1 ←argminy
n
f(xk) + ∇f(xk)⊤(y −xk) + L
2 ∥y −xk∥2o
The O( 1
T ) convergence rate of gradient descent is well-known in the literature. We state the con-
vergence guarantee here.
Lemma 239. Let xT be the result of running gradient descent for T iterations. Then for the global
minimizer x∗, we have f(xT ) −f(x∗) ≤2LR2
T
, where R = maxy:f(y)≤f(x0)∥y −x∗∥.

APPENDIX C. DEFERRED PROOFS FROM CHAPTER 4
633
Coordinate descent
Next, we brieﬂy review the basic guarantees of randomized coordinate descent when applied to a
convex function f which is Lj-smooth in the jth coordinate.
Here, we analyze the convergence
rate of the simple unaccelerated variant of coordinate descent where coordinate j is sampled with
probability Lj
S , where S := P
j Lj. In particular, we perform the following update after sampling a
coordinate j:
xk+1 ←argminy
n
f(xk) + ∇jf(xk)⊤(y −xk) + Lj
2 |yj −xk
j |2o
= xk −1
Lj
∇jf(xk)
Here, we give the convergence rate of this simple coordinate descent algorithm.
Lemma 240. Let xT be the result of running gradient descent for T iterations. Then for the global
minimizer x∗, we have f(xT ) −f(x∗) ≤2SR2
T
, where R = maxy:f(y)≤f(x0)∥y −x∗∥2.
We remark that for any randomized iterative method for minimizing a convex function which
converges in expectation, it is easy to use Markov's inequality to bound the convergence with constant
probability. For example, if an algorithm terminates with a ϵ-approximate minimizer on expectation,
with probability at least 1
2 it terminates with a 2ϵ-approximate minimizer. Thus, if one desires a
high probability result for the approximate minimization, the runtime only incurs a logarithmic
multiplicative loss in the failure probability.
C.1.4
Proof of Lemma 239
First we give an intermediate progress bound which will be useful in the ﬁnal proof.
Lemma 241. f(xk) −f(xk+1) ≥
1
2L∥∇f(xk)∥2
∗
Proof. We will prove that miny
n
∇f(x)⊤(y −x) + L
2 ∥y −x∥2o
≤−1
2L∥∇f(x)∥2
∗; clearly this yields
the desired claim. Let z be such that ∥z∥= 1 and z⊤∇f(x) = ∥∇f(x)∥∗, by the deﬁnition of dual
norm; let y = x −∥∇f(x)∥∗
L
z. Then,
∇f(x)⊤(y −x) + L
2 ∥y −x∥2 = −
∥∇f(x)∥∗
L

z⊤∇f(x) + L
2
∥∇f(x)∥2
∗
L2
∥z∥2 = −1
2L∥∇f(x)∥2
∗
Thus, the minimizer of the upper bound yields the desired progress result.
Next, we prove Lemma 239.
Proof. Let ϵk := f(xk) −f(x∗). Note that by convexity and Cauchy-Schwarz, we have
f(xk) −f(x∗) ≤(∇f(xk))⊤(f(xk) −f(x∗)) ≤∥∇f(xk)∥∗∥xk −x∗∥

APPENDIX C. DEFERRED PROOFS FROM CHAPTER 4
634
Thus, we have the two equations ϵk −ϵk+1 ≥
1
2L∥∇f(xk)∥2
∗and ϵk ≤R∥∇f(xk)∥∗. Combining the
two, it's easy to see that
ϵ2
k ≤2LR2(ϵk −ϵk+1) ↔

1
ϵk+1
−1
ϵk

≥
ϵk
2LR2ϵk+1
≥
1
2LR2
Thus, telescoping we have
1
ϵT ≥
T
2LR2 , which yields the desired rate of convergence.
C.1.5
Proof of Lemma 240
The progress of a step in the jth coordinate is thus lower bounded by −
1
2Lj |∇jf(xk)|2, which can
be veriﬁed by computing the upper bound on f(xk+1). The analysis of convergence follows directly
from the following result on the expected progress of a single step.
Lemma 242. f(xk) −Ek[f(xk+1)] ≥
1
2S ∥∇f(xk)∥2
2
Proof. We directly compute the expectation. We have
E[f(xk+1)] =
X
j
Lj
S

f(xk) −
1
2Lj
|∇jf(xk)|2
= f(xk) −1
2S
X
j
|∇jf(xk)|2
Thus, we can immediately plug in this expected progress result into the convergence rate proof
of gradient descent, and obtain the desired result.
C.2
Missing proofs from Section 4.4
C.2.1
Reducing undirected maximum ﬂow to ℓ∞regression
In this section, we prove Lemma 52, via giving the reduction and analyzing its convergence. First,
suppose we have a subroutine, Almost-Route, which takes in matrices R (an α = ˜O(1)-congestion
approximator), B (an edge-incidence matrix), U (the capacities of edges), α, an error tolerance ϵ,
and a demand vector d, and returns some x such that
2α∥RBUx −Rd∥∞+ ∥x∥∞≤(1 + ϵ)(2α∥RBUx∗−Rd∥∞+ ∥x∗∥∞) := (1 + ϵ)OPT(d)
(C.3)
Here, under a change of variables we have that x = U −1f.
Note that we are writing with an
ϵ-multiplicative approximation to OPT instead of an additive one.
We do this without loss of
generality: assume we have scaled the problem appropriately so that the optimal value is 1, which
it will be when we ﬁnd the true maximum ﬂow instead of the minimum congestion ﬂow. We can
ﬁnd this optimal value via a binary search, as we argued before, losing a ˜O(1) factor in the runtime.

APPENDIX C. DEFERRED PROOFS FROM CHAPTER 4
635
Now, we show a key property of the function we try to minimize. Intuitively, the next lemma
says that if we are able to ϵ-approximately minimize our regression problem, the cost of routing the
residual demands d−Bf is only an ϵ fraction of routing the original demands, allowing us to quickly
recurse. This is a restatement of Lemma 2.2 in [482].
Lemma 243. Deﬁne the change of variables Ux = f.
Suppose 2α∥RBUx −Rd∥∞+ ∥x∥∞=
2α∥R(d −Bf)∥∞+ ∥U −1f∥∞≤(1 + ϵ)OPT(d). Then, ∥R(d −Bf)∥∞≤ϵ∥Rd∥∞.
Proof. Let f ′ be the optimal routing of the residual demands d −Bf, namely the argument which
achieves OPT(d −Bf). Then, Bf ′ = d −Bf, and by the deﬁnition of a congestion approximator,
OPT(d −Bf) = ∥U −1f ′∥∞+ 2α∥R((d −Bf) −Bf ′)∥∞= ∥U −1f ′∥∞≤α∥R(d −Bf)∥∞
(C.4)
For simplicity we write d′ := d −Bf. Furthermore, we have by assumption of the quality of the
initial solution f,
OPT(d) + α∥Rd′∥∞≤∥U −1(f + f ′)∥∞+ 2α∥R(d −B(f + f ′))∥∞+ α∥Rd′∥∞
(C.5)
≤∥U −1f∥∞+ ∥U −1f ′∥∞+ α∥Rd′∥∞
(C.6)
≤∥U −1f∥∞+ 2α∥Rd′∥∞≤(1 + ϵ)OPT(d)
(C.7)
Here, we used that d = B(f +f ′) and our bound ∥U −1f ′∥∞≤α∥Rd′∥∞. Subtracting OPT(d), and
noting that OPT(d) ≤α∥Rd∥∞, we have the desired claim.
Now, we give the full reduction to calling Almost-Route. Note that it was shown in [482] that
routing through a maximal spanning tree yields an O(m)-congestion approximator.
We now need to prove the correctness of our algorithm. This is a restatement of ideas presented
in [482].
Lemma 244. The output of Flow-To-Regress is an ϵ-approximate solution to the minimum
congestion ﬂow problem.
Proof. By the guarantees of Almost-Route, we have the following guarantees:
∥U −1f 0∥∞+ 2α∥Rd1∥∞≤(1 + ϵ)OPT(d),
(C.8)
∥U −1f k∥∞+ 2α∥Rdk+1∥∞≤3
2OPT(dk) ≤3
2α∥Rdk∥∞, k ≥1.
(C.9)
Now, using the second inequality and repeatedly applying it to the ﬁrst, we have the following
guarantee:
1
2α∥Rd1∥∞+ ∥U −1f 0∥∞+ . . . + ∥U −1f T ∥∞≤(1 + ϵ)OPT(d).
(C.10)

APPENDIX C. DEFERRED PROOFS FROM CHAPTER 4
636
f final = Flow-To-Regress(G, d, ϵ)
1. Let T = log 2m.
2. Initialize d0 = d. Initialize f 0 = UAlmost-Route(R, B, U, d0, α, ϵ).
3. Let f final = f 0.
4. Iterate for k = 1, 2, . . . T:
(a) Let dk = dk−1 −Bf k−1.
(b) Let f k = UAlmost-Route(R, B, U, Dk, α, 1
2).
(c) Let f final = f final + f k.
5. Let f T +1 be an (exact) routing of dk −Bf k in a maximal spanning tree. Let f final + f T +1.
6. Return f final
Figure C.1: The reduction from solving the approximate maximum ﬂow problem to solving ˜O(1)
approximate regression problems.
It suﬃces to note that by our choice of T and seeing that by applying Lemma 243 T times, we
have α∥RdT +1∥∞≤
1
2mα∥Rd1∥∞. Thus because we routed dT +1 exactly through a m-congestion
approximator, we have ∥U −1f T +1∥∞≤1
2α∥Rd1∥∞. Finally, Bf final = d, and
∥U −1f final∥∞≤∥U −1f T +1∥∞+ ∥U −1f 0∥∞+ . . . + ∥U −1f T ∥∞
(C.11)
≤∥U −1f final∥∞≤1
2α∥Rd1∥∞+ ∥U −1f 0∥∞+ . . . + ∥U −1f T ∥∞
(C.12)
≤(1 + ϵ)OPT(d).
(C.13)
Lemma 245. The runtime of our routine Flow-To-Regress is the cost of solving the ﬁrst as-
sociated regression problem, 2α∥RBUx −Rd∥∞+ ∥x∥∞, to an ϵ approximation, plus an additional
˜O(m) additive overhead.
Proof. We analyze the time of each of the calls to Almost-Route. Clearly, the ﬁrst call is the cost
of solving the ﬁrst associated regression problem.
Note that we have ﬂexibility in terms of how to implement Almost-Route; for all remaining
calls, we consider the implementation in the form of unaccelerated gradient descent in the ℓ∞norm.
The runtime as we demonstrated in Appendix C.1.3 for each round k is
m∥f k
∗∥2
∞∥αRBU∥2
∞
( 1
2)2
= ˜O(m)
(C.14)

APPENDIX C. DEFERRED PROOFS FROM CHAPTER 4
637
where f k
∗is the optimal solution to the kth regression problem. Here, we used the known properties
of αRBU, as well as the fact that the implications of Lemma 243 allow us to bound the ℓ∞norm of
the optimal solution by O(1) as well.
As a ﬁnal note in the proof of Lemma 52, observe that to optimize the ﬁrst objective 2α ∥Ax −b∥∞+
∥x∥∞it suﬃces to binary search over values r ≥∥U −1f∥∞= ∥x∥∞, and solve the associated regres-
sion problem ∥Ax−b∥∞over x ∈[−r, r]m. More formally, since r is our guess of OPT to the original
ﬂow problem, we repeatedly solve the problem over [−r, r]m to ϵr additive error; if the conclusion
is that the optimal value cannot be 0 (i.e. the additive approximation is larger than ϵr), then we
conclude that this value of r is not routable. This only incurs a multiplicative loss in the runtime by
a factor of ˜O(1), due to the binary search. By normalizing x, b appropriately, it suﬃces to consider
the case where r = 1, and solve to ϵ additive error (see Appendix C.1.2). Finally, we need only
consider the case where ∥b∥∞≤∥A∥∞, as x ∈[−1, 1]∞the unit box, so the demands are clearly not
routable otherwise.
C.2.2
Reducing directed maximum ﬂow to undirected maximum ﬂow
In this section, we give an overview of the main result in [363]. In particular, we prove the following
statement, which is used in our algorithms for ﬁnding exact maximum ﬂows in unit-capacity graphs.
Lemma 246 (Summary of results in [363]). Suppose we wish to ﬁnd an s −t maximum ﬂow in a
unit-capacity directed (multi)graph G with m edges and maximum ﬂow value F. Then, it suﬃces
to ﬁnd the s −t maximum ﬂow fmax in an undirected (multi)graph G′ with O(m) edges, such
that edges of G′ have capacity
1
2, and the maximum ﬂow in G′ has value F + m
2 . Furthermore,
we are able to initialize the undirected maximum ﬂow algorithm in G′ with some finit such that
∥finit −fmax∥2
2 = F.
Proof. First, we give the construction of the undirected graph G′. For every directed edge (u, v) of
weight 1 in G, G′ has the undirected edges (s, v), (v, u), and (u, t) of weight 1
2. Clearly, G′ has O(m)
edges, since each edge in G is replaced with 3 edges in G′.
Next, we give the (algorithmic) proof that one can recover a maximum ﬂow in G from a maximum
ﬂow in G′, and that the maximum ﬂow in G′ has value F + m
2 . Consider the following algorithm.
We will now prove correctness of the algorithm UMF-to-DMF, namely that ffinal −finit is a
maximum ﬂow in graph G. To do so, we show that ffinal has value m
2 + F, and that ffinal −finit
puts ﬂow only in the (u, v) direction and does not put any ﬂow on any new edges (s, v) or (u, t).
Note that this immediately implies the statement ∥finit −fmax∥2
2 = F.
We begin by showing that ffinal has value m
2 + F. The residual graph of G′ with respect to the
ﬂow finit is the directed graph G. Thus, the maximum ﬂow in the residual graph has value F by
assumption, and the ﬂow finit has value m
2 , yielding the conclusion.

APPENDIX C. DEFERRED PROOFS FROM CHAPTER 4
638
f = UMF-to-DMF(G)
1. Let G′ be the undirected graph with edges (s, v), (v, u), (u, t) of weight 1
2 for every directed
edge (u, v) in G.
2. Let finit be the ﬂow which puts 1
2 units of ﬂow on each of the (s, v), (v, u), (u, t).
3. Compute ffinal, the maximum ﬂow of G′.
4. Return ffinal −finit.
Figure C.2: Recovering a maximum ﬂow in directed G via a maximum ﬂow in undirected G′.
Next, we show that for every edge (u, v) in G′ which resulted from a directed edge (u, v) in G,
ffinal −finit puts ﬂow only in the (u, v) direction, and does not violate the capacity constraint. This
is simple to see because ffinal puts a ﬂow with value in {−1
2, 0, 1
2} in the (u, v) direction, and −finit
puts a ﬂow with value 1
2 in the (u, v) direction; adding yields the result.
Finally, we show that ffinal−finit puts no ﬂow on any of the new edges (s, v) (the same statement
holds for edges (u, t) by a similar argument). Again, ffinal puts a ﬂow with value in {−1
2, 0, 1
2} in
the (v, s) direction, and −finit puts a ﬂow with value 1
2 in the (v, s) direction, thus ffinal −finit puts
a ﬂow with nonnegative value in the (v, s) direction. If this value was strictly positive, it would be
part of a path in the ﬂow decomposition sending ﬂow into s, contradicting the maximality of ffinal.

Appendix D
Deferred proofs from Chapter 5
D.1
Area convexity and optimal transport
In this section, we provide the optimization tools which will be used in our semi-streaming solver
of Section 5.3. These tools were originally developed in our earlier work [293] to obtain a direct
parallel solver for optimal transport. For this reason, we choose to ﬁrst give an overview of the
optimal transport problem in Section D.1.1, and then our general-purpose tools in Section D.1.3.
D.1.1
Optimal transport
Optimal transport is playing an increasingly important role as a subroutine in tasks arising in
machine learning [32], computer vision [90, 490], robust optimization [215, 83], and statistics [434].
Given these applications for large scale learning, designing algorithms for eﬃciently approximately
solving the problem has been the subject of extensive recent research [151, 25, 236, 115, 208, 365,
82, 450].
Given two vectors r and c in the n-dimensional probability simplex ∆n and a cost matrix C ∈
Rn×n
≥0
1, the optimal transportation problem is
min
X∈Ur,c ⟨C, X⟩,
where
Ur,c :=
n
X ∈Rn×n
≥0 , X1 = r, X⊤1 = c
o
.
(D.1)
This problem arises from deﬁning the Wasserstein or Earth mover's distance between discrete
probability measures r and c, as the cheapest coupling between the distributions, where the cost
of the coupling X ∈Ur,c is ⟨C, X⟩. If r and c are viewed as distributions of masses placed on n
points in some space (typically metric), the Wasserstein distance is the cheapest way to move mass
to transform r into c. In (D.1), X represents the transport plan (Xij is the amount moved from ri
1Similarly to earlier works, we focus on square matrices; generalizations to rectangular matrices are straightforward.
639

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
640
to cj) and C represents the cost of movement (Cij is the cost of moving mass from ri to cj).
Throughout, the value of (D.1) is denoted OPT. We call ˆX ∈Ur,c an ϵ-approximate transporta-
tion plan if
D
C, ˆX
E
≤OPT + ϵ. Our goal is to design an eﬃcient algorithm to produce such a
ˆX.
Our contributions
Our main contribution is an algorithm running in ˜O(∥C∥max/ϵ) parallelelizable iterations2 and
˜O(n2∥C∥max/ϵ) total work producing an ϵ-approximate transport plan.
Matching runtimes were given in the recent work of [82, 450]. Their runtimes were obtained
via reductions to matrix scaling and positive linear programming, each well-studied problems in
theoretical computer science. However, the matrix scaling algorithm is a second-order Newton-type
method which makes calls to structured linear system solvers, and the positive LP algorithm is
not parallelizable (i.e. has depth polynomial in dimension). These features potentially limit the
practicality of these algorithms. The key remaining open question this paper addresses is, is there
an eﬃcient ﬁrst-order, parallelizable algorithm for approximating optimal transport? We answer this
aﬃrmatively and give an eﬃcient, parallelizable primal-dual ﬁrst-order method; the only additional
overhead is a scheme for implementing steps, incurring roughly an additional log ϵ−1 factor.
Our approach heavily leverages the recent improvement to the maximum ﬂow problem, and more
broadly two-player games on a simplex (ℓ1 ball) and a box (ℓ∞ball), due to the breakthrough work of
[483]. First, we recast (D.1) as a minimax game between a box and a simplex, proving correctness via
a rounding procedure known in the optimal transport literature. Second, we show how to adapt the
dual extrapolation scheme under the weaker convergence requirements of area-convexity, following
[483], to obtain an approximate minimizer to our primal-dual objective in the stated runtime. En
route, we slightly simplify analysis in [483] and relate it more closely to the existing extragradient
literature.
Finally, we give preliminary experimental evidence showing our algorithm can be practical, and
highlight some open directions in bridging the gap between theory and practice of our method, as
well as accelerated gradient schemes [208, 365] and Sinkhorn iteration.
Previous work
Optimal transport. The problem of giving eﬃcient algorithms to ﬁnd ϵ-approximate transport plans
ˆX which run in nearly linear time3 has been addressed by a line of recent work, starting with [151]
and improved upon in [236, 25, 208, 365, 82, 450]. We brieﬂy discuss their approaches here.
2Our iterations consist of vector operations and matrix-vector products, which are easily parallelizable. Throughout
∥C∥max is the largest entry of C.
3We use "nearly linear" to describe complexities which have an n2polylog(n) dependence on the dimension (where
the size of input C is n2), and polynomial dependence on ∥C∥max , ϵ−1.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
641
Works by [151, 25] studied the Sinkhorn algorithm, an alternating minimization scheme. Regu-
larizing (D.1) with an η−1 multiple of entropy and computing the dual, we arrive at the problem
min
x,y∈Rn 1⊤BηC(x, y)1 −r⊤x −c⊤y
where
BηC(x, y)ij = exi+yj−ηCij.
This problem is equivalent to computing diagonal scalings X and Y for M = exp(−ηC) such that
XMY has row sums r and column sums c. The Sinkhorn iteration alternates ﬁxing the row sums
and the column sums by left and right scaling by diagonal matrices until an approximation of such
scalings is found, or equivalently until XMY is close to being in Ur,c.
As shown in [25], we can round the resulting almost-transportation plan to a transportation plan
which lies in Ur,c in linear time, losing at most 2∥C∥max(∥X1 −r∥1 +
X⊤1 −c

1) in the objective.
Further, [25] showed that ˜O(∥C∥3
max/ϵ3) iterations of this scheme suﬃced to obtain a matrix which
ϵ/∥C∥max-approximately meets the demands in ℓ1 with good objective value, by analyzing it as an
instance of mirror descent with an entropic regularizer. The same work proposed an alternative
algorithm, Greenkhorn, based on greedy coordinate descent. [208, 365] showed that ˜O
 ∥C∥2
max/ϵ2
iterations, corresponding to ˜O
 n2∥C∥2
max/ϵ2
work, suﬃce for both Sinkhorn and Greenkhorn, the
current state-of-the-art for this line of analysis.
An alternative approach based on ﬁrst-order methods was studied by [208, 365]. These works
considered minimizing an entropy-regularized Equation D.1; the resulting weighted softmax function
is prevalent in the literature on approximate linear programming [419], and has found similar appli-
cations in near-linear algorithms for maximum ﬂow [482, 318, 486] and positive linear programming
[541, 23]. An unaccelerated algorithm, viewable as ℓ∞gradient descent, was analyzed in [208] and
ran in ˜O(∥C∥2
max/ϵ2) iterations. Further, an accelerated algorithm was discussed, for which the
authors claimed an ˜O(n1/4∥C∥0.5
max/ϵ) iteration count. [365] showed that the algorithm had an addi-
tional dependence on a parameter as bad as n1/4, roughly due to a gap between the ℓ2 and ℓ∞norms.
Thus, the state of the art runtime in this line is the better of ˜O
 n2.5∥C∥max/ϵ

, ˜O
 n2∥C∥2
max/ϵ2
operations. The dependence on dimension of the former of these runtimes matches that of the linear
programming solver of [348, 349], which obtain a polylogarithmic dependence on ϵ−1, rather than
a polynomial dependence; thus, the question of obtaining an accelerated ϵ−1 dependence without
worse dimension dependence remained open.
This was partially settled in [82, 450], which studied the relationship of optimal transport to
fundamental algorithmic problems in theoretical computer science, namely positive linear program-
ming and matrix scaling, for which signiﬁcantly-improved runtimes have been recently obtained
[23, 552, 145]. In particular, they showed that optimal transport could be reduced to instances of
either of these objectives, for which ˜O (∥C∥max/ϵ) iterations, each of which required linear O(n2)
work, suﬃced. However, both of these reductions are based on black-box methods for which prac-
tical implementations are not known; furthermore, in the case of positive linear programming a
parallel ˜O(1/ϵ)-iteration algorithm is not known. [82] also showed any polynomial improvement to

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
642
Year
Author
Complexity
Approach
1st-order
Parallel
2015
[349]
˜O(n2.5)
Interior point
No
No
2017-19
[25]
˜O(n2∥C∥2
max/ϵ2)
Sink/Greenkhorn
Yes
Yes
2018
[208]
˜O(n2∥C∥2
max/ϵ2)
Gradient descent
Yes
Yes
2018-19
[365]
˜O(n2.5∥C∥max/ϵ)
Acceleration
Yes
Yes
2018
[82]
˜O(n2∥C∥max/ϵ)
Matrix scaling
No
Yes
2018-19
[82, 450]
˜O(n2∥C∥max/ϵ)
Positive LP
Yes
No
2019
This work
˜O(n2∥C∥max/ϵ)
Dual extrapolation
Yes
Yes
Table D.1:
Optimal transport algorithms.
Algorithms using second-order information use
potentially-expensive SDD system solvers; the runtime analysis of Sink/Greenkhorn is due to
[208, 365].
the runtime of our paper in the dependence on either ϵ or n would result in maximum-cardinality
bipartite matching in dense graphs faster than ˜O(n2.5) without fast matrix multiplication [468], a
fundamental open problem unresolved for almost 50 years [274].
Specializations of the transportation problem to ℓp metric spaces or arising from geometric set-
tings have been studied [479, 11, 28]. These specialized approaches seem fundamentally diﬀerent
than those concerning the more general transportation problem.
Finally, we note recent work [24] showed the promise of using the Nystr¨om method for low-rank
approximations to achieve speedup in theory and practice for transport problems arising from speciﬁc
metrics. As our method is based on matrix-vector operations, where low-rank approximations may
be applicable, we ﬁnd it interesting to see if our method can be combined with these improvements.
Remark. During the revision process for this work, an independent result [336] was published to
arXiv, obtaining improved runtimes for optimal transport via a combinatorial algorithm. The work
obtains a runtime of ˜O(n2∥C∥max/ϵ+n∥C∥2
max/ϵ2), which is worse than our runtime by a low-order
term. Furthermore, it does not appear to be parallelizable.
Box-simplex objectives. Our main result follows from improved algorithms for bilinear minimax
problems over one simplex domain and one box domain developed in [483].
This fundamental
minimax problem captures ℓ1 and ℓ∞regression over a simplex and box respectively, and inspired
the development of conjugate smoothing [419] as well as mirror prox / dual extrapolation [415,
420]. These latter two approaches are extragradient methods (using two gradient operations per
iteration rather than one) for approximately solving a family of problems, which includes convex
minimization and ﬁnding a saddle point to a convex-concave function. These methods simulate
backwards Euler discretization of the gradient ﬂow, similar to how mirror descent simulates forwards
Euler discretization [193]. The role of the extragradient step is a ﬁxed point iteration (of two steps)
which is a good approximation of the backwards Euler step when the operator is Lipschitz.
Nonetheless, the analysis of [415, 420] fell short in obtaining a 1/T rate of convergence without

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
643
worse dependence on dimension for these domains, where T is the iteration count (which would
correspond to a ˜O (1/ϵ) runtime for approximate minimization). The fundamental barrier was that
over a box, any strongly-convex regularizer in the ℓ∞norm has a dimension-dependent domain size
(shown in [486]). This barrier can also be viewed as the reason for the worse dimension dependence
in the accelerated scheme of [208, 365].
The primary insight of [483] was that previous approaches attempted to regularize the schemes
of [415, 420] with separable regularizers, i.e. the sum of a regularizer which depends only on the
primal block and one which depends only on the dual. If, say, the domain of the primal block was a
box, then such a regularization scheme would run into the ℓ∞barrier and incur a worse dependence
on dimension. However, by more carefully analyzing the requirements of these algorithms, [483]
constructed a non-separable regularizer with small domain size, satisfying a property termed area-
convexity which suﬃced for provable convergence of dual extrapolation [420].
Interestingly, the
property seems specialized to dual extrapolation and not mirror prox [415].
D.1.2
Overview
In this section, we describe a reformulation of (D.1) as a primal-dual objective, which we solve
approximately in Section D.1.3. We adapt the view of [82, 450] of the objective (D.1) as a positive
linear program. Let d be the (vectorized) cost matrix C associated with the instance and let ∆n2 be
the n2 dimensional simplex4. We recall r, c are speciﬁed row and column sums with 1⊤r = 1⊤c = 1.
The optimal transport problem can be written as, for m = n2, and A ∈{0, 1}2n×m, b ∈R2n
≥0, for A
the (unsigned) edge-incidence matrix of the underlying bipartite graph and b the concatenation of
r and c.
min
x∈∆m,Ax=b d⊤x.
(D.2)
A =










1
1
1
0
0
0
0
0
0
0
0
0
1
1
1
0
0
0
0
0
0
0
0
0
1
1
1
1
0
0
1
0
0
1
0
0
0
1
0
0
1
0
0
1
0
0
0
1
0
0
1
0
0
1










, b =










1/3
1/3
1/3
1/3
1/3
1/3










.
Figure D.1: Edge-incidence matrix A of a 3 × 3 bipartite graph and uniform demands.
In particular, A is the 0-1 matrix on V × E such that Ave = 1 iﬀv is an endpoint of edge e. We
summarize some additional properties of the constraint matrix A and vector b.
4We use d because C often arises from distances in a metric space, and to avoid overloading c.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
644
Fact 37. A, b have the following properties.
1. A ∈{0, 1}2n×m has 2-sparse columns and n-sparse rows. Thus ∥A∥1→1 = 2.
2. b⊤=

r⊤
c⊤

, so that ∥b∥1 = 2.
3. A has 2n2 nonzero entries.
Section D.1.4 recalls the proof of the following theorem, which ﬁrst appeared in [25].
Theorem 80 (Rounding guarantee, Lemma 7 in [25]). There is an algorithm which takes ˜x with
∥A˜x −b∥1 ≤δ and produces ˆx in O(n2) time, with
Aˆx = b, ∥˜x −ˆx∥1 ≤2δ.
We now show how the rounding procedure gives a roadmap for our approach. Consider the
following ℓ1 regression objective over the simplex (a similar penalized objective appeared in [482]):
min
x∈∆m d⊤x + 2 ∥d∥∞∥Ax −b∥1 .
(D.3)
We show that the penalized objective value is still OPT, and furthermore any approximate minimizer
yields an approximate transport plan.
Lemma 247 (Penalized ℓ1 regression). The value of (D.3) is OPT. Also, given ˜x, an ϵ-approximate
minimizer to (D.3), we can ﬁnd ϵ-approximate transportation plan ˆx in O(n2) time.
Proof. Recall OPT = minx∈∆m,Ax=b d⊤x. Let ˜x be the minimizing argument in (D.3). We claim
there is some optimal ˜x with A˜x = b; clearly, the ﬁrst claim is then true. Suppose otherwise, and
let ∥A˜x −b∥1 = δ > 0. Then, let ˆx be the result of the algorithm in Theorem 80, applied to ˜x, so
that Aˆx = b, ∥˜x −ˆx∥1 ≤2δ. We then have
d⊤ˆx + 2 ∥d∥∞∥Aˆx −b∥1 = d⊤(ˆx −˜x) + d⊤˜x ≤d⊤˜x + ∥d∥∞∥ˆx −˜x∥1 ≤d⊤˜x + 2 ∥d∥∞δ.
The objective value of ˆx is no more than of ˜x, a contradiction. By this discussion, we can take any
approximate minimizer to (D.3) and round it to a transport plan without increasing the objective.
Section D.1.3 proves Theorem 81, which says we can eﬃciently ﬁnd an approximate minimizer
to (D.3).
Theorem 81 (Approximate ℓ1 regression over the simplex). There is an algorithm (Algorithm 71)
taking input ϵ, which has O((∥d∥∞log n log γ)/ϵ) parallel depth for γ = log n · ∥d∥∞/ϵ, and total
work O(n2(∥d∥∞log n log γ)/ϵ), and obtains ˜x an ϵ-additive approximation to the objective in (D.3).

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
645
We will approach proving Theorem 81 through a primal-dual viewpoint, in light of the following
(based on the deﬁnition of the ℓ1 norm):
min
x∈∆m d⊤x + 2 ∥d∥∞∥Ax −b∥1 = min
x∈∆m
max
y∈[−1,1]2n d⊤x + 2 ∥d∥∞
 y⊤Ax −b⊤y

.
(D.4)
Further, a low-duality gap pair to (D.4) yields an approximate minimizer to (D.3).
Lemma 248 (Duality gap to error). Suppose x, y is feasible (x ∈∆m, y ∈[−1, 1]2n), and for any
feasible u, v,
 d⊤x + 2 ∥d∥∞
 v⊤Ax −b⊤v

−
 d⊤u + 2 ∥d∥∞
 y⊤Au −b⊤y

≤δ.
Then, we have d⊤x + 2 ∥d∥∞∥Ax −b∥1 ≤δ + OPT.
Proof. The result follows from maximizing over v, and noting that for the minimizing u,
d⊤u + 2 ∥d∥∞
 y⊤Au −b⊤y

≤d⊤u + 2 ∥d∥∞∥Au −b∥1 = OPT.
Correspondingly, Section D.1.3 gives an algorithm which obtains (x, y) with bounded duality gap
within the runtime of Theorem 81.
Additional notation
We recall the deﬁnitions of a Bregman divergence of a regularizer and the proximal operator of a
divergence here, which will be used in Section D.1.3.
Deﬁnition 48 (Bregman divergence). For (diﬀerentiable) regularizer r and z, w in its domain, the
Bregman divergence from z to w is
V r
z (w) := r(w) −r(z) −⟨∇r(z), w −z⟩.
Deﬁnition 49 (Proximal operator). For (diﬀerentiable) regularizer r, z in its domain, and g in the
dual space (when the domain is in Rd, so is the dual space), we deﬁne the proximal operator as
Proxr
z(g) := argminw {⟨g, w⟩+ V r
z (w)} .
Several variables have specialized meaning throughout the remainder of this section. All graphs
considered will be on 2n vertices with m edges, i.e. m = n2. A ∈R2n×m is the edge-incidence
matrix. d is the vectorized cost matrix C. b is the constraint vector, concatenating row and column
constraints r, c. In algorithms for solving (D.4), x and y are primal (in a simplex) and dual (in a

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
646
box) variables respectively. In Section D.1.3, we adopt the linear programming perspective where
the decision variable x ∈∆m is a vector. In Section D.1.4, for convenience we take the perspective
where X is an unﬂattened n × n matrix. Ur,c is the feasible polytope: when the domain is vectors,
Ur,c is x | Ax = b, and when it is matrices, Ur,c is X | X1 = r, X⊤1 = c (by ﬂattening X this is
consistent).
D.1.3
Main algorithm
This section describes our algorithm for ﬁnding a primal-dual pair (x, y) with a small duality gap,
with respect to the objective in (D.4), which we restate here for convenience:
min
x∈X max
y∈Y d⊤x + 2 ∥d∥∞
 y⊤max −b⊤y

, X := ∆m, Y := [−1, 1]2n.
(Restatement of (D.4))
Our algorithm is a specialization of the algorithm in [483]. One of our technical contributions in this
regard is an analysis of the algorithm which more closely relates it to the analysis of dual extrapola-
tion [420], an algorithm for ﬁnding approximate saddle points with a more standard analysis. In the
ﬁrst part of this section, we give the algorithmic framework and convergence analysis. In the second
part of this section, we provide analysis of an alternating minimization scheme for implementing
steps of the procedure. The same procedure was used in [483] which claimed without proof the
linear convergence rate of the alternating minimization; we hope the analysis will make the method
more broadly accessible to the optimization community. We defer many proofs to Appendix D.1.5.
Finally, we remark that the dual extrapolation analysis in this section (based on [483]) is some-
what diﬀerent than its earlier incarnation in Appendix A.4; the main diﬀerence is that we allow
for the subproblems to be solved to a speciﬁed level of relative function error, rather than regret.
This diﬀerence ends up being crucial in improving our parameter dependences in our semi-streaming
solver of Section 5.3.
For an objective F(x, y) convex in x and concave in y, the standard way to measure the duality
gap is to deﬁne the gradient operator g(x, y) = (∇xF(x, y), −∇yF(x, y)), and show that for z = (x, y)
and any u on the product space, the regret, ⟨g(z), z −u⟩, is small. Correspondingly, we deﬁne
g(x, y) :=
 d + 2 ∥d∥∞A⊤y, 2 ∥d∥∞(b −Ax)

.
The dual extrapolation framework [420] requires a regularizer on the product space. The algorithm
is simple to state; it takes two "mirror descent-like" steps each iteration, maintaining a state st in
the dual space5. A typical setup is a Lipschitz gradient operator and a regularizer which is the sum
of canonical strongly-convex regularizers in the norms corresponding to the product space X, Y.
However, recent works have shown that this setup can be greatly relaxed and still obtain similar
5In this regard, it is more similar to the "dual averaging" or "lazy" mirror descent setup [98].

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
647
rates of convergence. In particular, [483] introduced the following deﬁnition.
Deﬁnition 50 (Area-convexity). Regularizer r is κ-area-convex with respect to operator g if for any
points a, b, c in its domain,
κ

r(a) + r(b) + r(c) −3r
a + b + c
3

≥⟨g(b) −g(a), b −c⟩.
(D.5)
Area-convexity is so named because ⟨g(b) −g(a), b −c⟩can be viewed as measuring the "area"
of the triangle with vertices a, b, c with respect to some Jacobian matrix. In the case of bilinear
objectives, the left hand side in the deﬁnition of area-convexity is invariant to permuting a, b, c,
whereas the sign of the right hand side can be ﬂipped by interchanging a, c, so area-convexity
implies convexity. However, it does not even imply the regularizer r is strongly-convex, a typical
assumption for the convergence of mirror descent methods.
We state the algorithm for time horizon T; the only diﬀerence from [420] is a factor of 2 in
deﬁning st+1, i.e. adding a 1/2κ multiple rather than 1/κ. We ﬁnd it of interest to explore whether
this change is necessary or speciﬁc to the analysis of [483].
Algorithm 71: ¯w = Dual-Extrapolation(κ, r, g, T): Dual extrapolation with area-convex
r.
1 Initialize s0 = 0, let ¯z be the minimizer of r;
2 for t < T do
3
zt ←Proxr
¯z(st);
4
wt ←Proxr
¯z
 st + 1
κg(zt)

;
5
st+1 ←st +
1
2κg(wt);
6
t ←t + 1;
7 Return: ¯w := 1
T
P
t∈[T ] wt;
Lemma 249 (Dual extrapolation convergence). Suppose r is κ-area-convex with respect to g. Fur-
ther, suppose for some u, Θ ≥r(u) −r(¯z). Then, the output ¯w to Algorithm 71 satisﬁes
⟨g( ¯w), ¯w −u⟩≤2κΘ
T
.
In fact, by more carefully analyzing the requirements of dual extrapolation we have the following.
Corollary 59. Suppose in Algorithm 71, the proximal steps are implemented with ϵ′/4κ additive
error. Then, the upper bound of the regret in Lemma 249 is 2κΘ/T + ϵ′.
We now state a useful second-order characterization of area-convexity involving a relationship
between the Jacobian of g and the Hessian of r, which was proved in [483].

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
648
Theorem 82 (Second-order area-convexity, Theorem 1.6 in [483]). For bilinear minimax objectives,
i.e. whose associated operator g has Jacobian
J =

0
M⊤
−M
0

,
and for twice-diﬀerentiable r, if for all z in the domain,

κ∇2r(z)
−J
J
κ∇2r(z)

⪰0,
then r is 3κ-area-convex with respect to g.
Finally, we complete the outline of the algorithm by stating the speciﬁc regularizer we use, which
ﬁrst appeared in [483]. We then prove its 3-area-convexity with respect to g by using Theorem 82.
r(x, y) = 2¯δmax

10
X
j∈[n]
xj log xj + x⊤A⊤(y2)

,
(D.6)
where (y2) is entry-wise.
To give some motivation for this regularizer, one ℓ∞-strongly convex
regularizer is 1
2 ∥y∥2
2, but over the ℓ∞ball, this regularizer has large range. The term x⊤A⊤(y2) in
(D.6) captures the curvature required for strong-convexity locally, but has a smaller range due to
the restrictions on x, A. The constants chosen were the smallest which satisfy the assumptions of
the following Lemma 250. We note that similar intuition was used to develop the local coordinate
descent method of Chapter 4.
Lemma 250 (Area-convexity of the Sherman regularizer). For the Jacobian J associated with the
objective in (D.4) and the regularizer r deﬁned in (D.6), we have

∇2r(z)
−J
J
∇2r(z)

⪰0.
We now give the proof of Theorem 81, requiring some claims in Appendix D.1.5 for the complexity
of Algorithm 71. In particular, Appendix D.1.5 implies that although the minimizer to the proximal
steps cannot be computed in closed form because of non-separability, a simple alternating scheme
converges to an approximate-minimizer in near-constant time.
Proof of Theorem 81. The algorithm is Algorithm 71, using the regularizer r in (D.6).
Clearly,
in the feasible region the range of the regularizer is at most 20 ∥d∥∞log n + 4 ∥d∥∞, where the
former summand comes from the range of entropy and the latter
A⊤
∞= 2. We may choose

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
649
Θ = O(∥d∥∞log n) to be the range of r to satisfy the assumptions of Lemma 249, since for all u,
⟨∇r(¯z), ¯z −u⟩≤0 ⇒V r
¯z (u) ≤r(u) −r(¯z).
By Theorem 82 and Lemma 250, r is 3-area-convex with respect to g. By Corollary 59, T =
12Θ/ϵ iterations suﬃce, implementing each proximal step to ϵ/12-additive accuracy. Finally, using
Theorem 83 to bound this implementation runtime concludes the proof.
D.1.4
Rounding to Ur,c
We state the rounding procedure in [25] for completeness here, which takes a transport plan ˜X close
to Ur,c and transforms it into a plan which exactly meets the constraints and is close to ˜X in ℓ1,
and then prove its correctness. Throughout r(X) := X1, c(X) := X⊤1.
Algorithm 72: ˆX = Rounding( ˜X, r, c): Rounding to feasible polytope
1 X′ ←diag

min

r
r( ˜X), 1

˜X;
2 X′′ ←X′diag

min

c
c(X′), 1

;
3 er ←r −1⊤r(X′′), ec ←c −1⊤c(X′′), E ←1⊤er;
4 ˆX ←X′′ + 1
E ere⊤
c ;
5 Return: ˆX;
Theorem 80 (Rounding guarantee, Lemma 7 in [25]). There is an algorithm which takes ˜x with
∥A˜x −b∥1 ≤δ and produces ˆx in O(n2) time, with
Aˆx = b, ∥˜x −ˆx∥1 ≤2δ.
Proof. The algorithm is Algorithm 72. We adopt the alternative view of ˜x as a n × n matrix ˜X in
the simplex, and deﬁne operations r(X) = X1, c(X) = X⊤1, recalling the ﬁrst and last n entries of
b are r, c, i.e. the row and column constraints. Recall we assume we have
r( ˜X) −r

1 +
c( ˜X) −c

1 ≤δ.
Clearly all operations in Algorithm 72 take O(n2) time. To explain brieﬂy, X′ is ﬁxed so that its
row sums are feasible (i.e. X′1 ≤r) and X′′ is ﬁxed so that its column sums are feasible. Further,
entrywise X′′ ≤X′ ≤˜X, so X′′ is feasible. We ﬁrst bound
d :=
X′′ −˜X

1 =


X
i:ri( ˜X)>ri
ri( ˜X) −ri

+


X
j:cj(X′)>cj
cj(X′) −cj

.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
650
Note
r( ˜X) −r

1 ≥P
i:ri( ˜X)>ri ri( ˜X) −ri. Further, by X′ ≤˜X entrywise,
X
j:cj(X′)>cj
cj(X′) −cj ≤
c( ˜X) −c

1 .
Thus d ≤δ. ˆX ∈Ur,c, since er, ec ≥0 and 1⊤er = 1⊤ec = e, so ˆX1 = r, ˆX⊤1 = c. Also,
 ˆX −˜X

1 ≤
X′′ −˜X

1 +
 ˆX −X′′
1 ≤δ + e.
Finally,
e = 1 −1⊤X′′1 = 1 −

1⊤˜X1 −d

= d.
Thus using d ≤δ proves the claim.
D.1.5
Missing proofs from Section D.1.3
In this section, we state missing proofs from Section D.1.3.
We will also provide the eﬃcient
implementation of the proximal steps required by Algorithm 71.
Proof of Lemma 249. Our ﬁrst step is to prove the following inequality:
1
2κ ⟨g(wt), wt −¯z⟩≤⟨st+1, zt+1 −¯z⟩+ V r
¯z (zt+1) −⟨st, zt −¯z⟩−V r
¯z (zt).
(D.7)
Let ct = zt+wt+zt+1
3
. The proof follows from minimality of zt with respect to ct, minimality of wt
with respect to zt+1, and area-convexity (D.5) with respect to zt, wt, and zt+1. Respectively,
⟨st, zt⟩+ r(zt) ≤⟨st, ct⟩+ r(ct)
⟨st, wt⟩+ 1
κ ⟨g(zt), wt⟩+ r(wt) ≤⟨st, zt+1⟩+ 1
κ ⟨g(zt), zt+1⟩+ r(zt+1)
1
κ ⟨g(wt) −g(zt), wt −zt+1⟩≤r(zt) + r(wt) + r(zt+1) −3r (ct) .
(D.8)
Adding three times the ﬁrst equation to the third, rearranging, and using the deﬁnition of ct, we
have
1
κ ⟨g(wt) −g(zt), wt −zt+1⟩≤r(wt) + r(zt+1) −2r(zt) + ⟨st, wt + zt+1 −2zt⟩.
Rearranging the second equation, we have
1
κ ⟨g(zt), wt −zt+1⟩≤r(zt+1) −r(wt) + ⟨st, zt+1 −wt⟩.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
651
Adding these two equations, we have
1
κ ⟨g(wt), wt −zt+1⟩≤2r(zt+1) −2r(zt) + ⟨st, 2zt+1 −2zt⟩.
Dividing by 2 and adding
1
2κ ⟨g(wt), zt+1 −¯z⟩to both sides, we obtain the desired (D.7). Now, deﬁne
the potential function
Φk = 1
2κ
k−1
X
t=0
⟨g(wt), wt −¯z⟩−⟨sk, zk −¯z⟩−V r
¯z (zk)
Then, by (D.7), Φk is nonincreasing in k. Therefore for any u, by the deﬁnition of Θ,
1
T
T −1
X
t=0
⟨g(wt), wt −u⟩≤1
T
T −1
X
t=0
⟨g(wt), wt −¯z⟩+ 1
T
T −1
X
t=0
⟨g(wt), ¯z −u⟩+
2κΘ
T
−2κV¯z(u)
T

≤1
T
T −1
X
t=0
⟨g(wt), wt −¯z⟩+ 1
T
T −1
X
t=0
⟨g(wt), ¯z −zT ⟩+
2κΘ
T
−2κV¯z(zT )
T

= 2κ
T ΦT + 2κΘ
T
≤2κ
T Φ0 + 2κΘ
T
= 2κΘ
T
.
The inequality on the second line used the deﬁnition of zT = Proxr
¯z

1
2κ
P
t∈[T −1] g(wt)

, and the
last inequality is ΦT ≤Φ0. The conclusion follows from the deﬁnition of g (because it is linear).
Proof of Corollary 59. We see that (D.7) now holds up to ϵ′
2κ additive error, so that Φk is increasing
by at most
ϵ′
2κ each step. Thus, we obtain ΦT ≤Φ0 + T ϵ′
2κ , yielding the conclusion.
Proof of Lemma 250. We scale both r and J down by 2 ∥d∥∞, which does not aﬀect positive-
semideﬁniteness. By computation we have (recalling all columns of A have ℓ1 norm of 2)
∇2r(x, y) =

5 ∥A:j∥1 diag

1
xj

2A⊤diag (yi)
2diag (yi) A
2diag
 A⊤
i x


.
It suﬃces to show that for any vector

a
b
c
d

we have

a
b
c
d









5 ∥A:j∥1 diag

1
xj

2A⊤diag (yi)
0
−A⊤
2diag (yi) A
2diag
 A⊤
i x

A
0
0
A⊤
5 ∥A:j∥1 diag

1
xj

2A⊤diag (yi)
−A
0
2diag (yi) A
2diag
 A⊤
i x

















a
b
c
d









APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
652
is nonnegative. Upon simplifying and gathering like terms, it suﬃces to show
X
i,j
Aij
 
5a2
j
xj
+ 4ajbiyi + 2b2
i xj −2ajdi + 2cjbi + 5c2
j
xj
+ 4cjdiyi + 2d2
i xj
!
≥0.
However, this is true for yi ∈[−1, 1], since each coeﬃcient groups into clearly nonnegative terms,
 
4a2
j
xj
+ 4ajbiyi + b2
i xj
!
+
 
a2
j
xj
−2ajdi + d2
i xj
!
+
 
4c2
j
xj
+ 4cjdiyi + d2
i xj
!
+
 
c2
j
xj
+ 2cjbi + b2
i xj
!
.
Alternating minimization analysis
We now give the convergence analysis of an alternating minimization procedure for minimizing a
function of the form (throughout this section, r(x, y) is as in (D.6))
f(x, y) := ⟨ξ, x⟩+ ⟨η, y⟩+ r(x, y)
(D.9)
which is the type of minimization problem arising from steps of the form Proxr
¯z(g). As we will see,
f(x, y) is jointly convex. Throughout this section, let xOPT, yOPT be the minimizer to f. Corollary 59
states that O(ϵ) additive error to f gives the same asymptotic convergence rate in Algorithm 71.
We will show that a simple alternating minimization scheme enjoys a linear rate of convergence in
our setting; thus, roughly O(log ϵ−1) iterations suﬃce. We ﬁrst give a proof of a general condition
which suﬃces for linear convergence.
Lemma 251. Suppose f(x, y) is twice-diﬀerentiable and jointly convex, over the product space X ×Y.
Consider the alternating minimization scheme,
1. xk+1 := argminx∈X f(x, yk)
2. yk+1 := argminy∈Yf(xk+1, y)
Further, suppose there are convex regions Xk+1 ⊆X, Yk ⊆Y which contain xk+1, yk respectively,
such that for any x′ ∈Xk+1, y′, y′′ ∈Yk, and for some σ ≥1,
∇2f(x′, y′) ⪰1
σ ∇2
yyf(xk+1, y′′),
(D.10)
where ∇2
yy is the Hessian with all but the yy block zeroed out. Then, for any x∗∈Xk+1, y∗∈Yk,
f(xk+1, yk) −f(xk+1, yk+1) ≥1
σ (f(xk+1, yk) −f(x∗, y∗)) .

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
653
Proof. Let ˜y =
 1 −1
σ

yk + 1
σy∗. We will prove instead that
f(xk+1, yk) −f(xk+1, ˜y) ≥1
σ (f(xk+1, yk) −f(x∗, y∗)) ,
from which the conclusion will follow since f(xk+1, yk+1) ≤f(xk+1, ˜y). Note by deﬁnition of ˜y, as
well as optimality of xk+1 which implies 0 ≥⟨∇xf(xk+1, yk), xk+1 −x∗⟩,
⟨∇yf(xk+1, yk), yk −˜y⟩= 1
σ ⟨∇yf(xk+1, yk), yk −y∗⟩≥1
σ
D
∇f(xk+1, yk), zk+ 1
2 −z∗E
(D.11)
where zk+ 1
2 := (xk+1, yk) and z∗:= (x∗, y∗). Further, let yα := (1−α)yk +αy∗, ˜yα := (1−α)yk +α˜y,
and xα := (1 −α)xk+1 + αx∗. Then, by Taylor expansion we have f(xk+1, yk) −f(xk+1, ˜y) equals
⟨∇yf(xk+1, yk), yk −˜y⟩−
Z 1
0
Z β
0
(˜y −yk)⊤∇2
yyf(xk+1, ˜yα)(˜y −yk)dαdβ
≥1
σ
D
∇f(xk+1, yk), zk+ 1
2 −z∗E
−1
σ2
Z 1
0
Z β
0
(y∗−yk)⊤∇2
yyf(xk+1, ˜yα)(y∗−yk)dαdβ
≥1
σ
 D
∇f(xk+1, yk), zk+ 1
2 −z∗E
−
Z 1
0
Z β
0
(z∗−zk+ 1
2 )⊤∇2f(xα, yα)(z∗−zk+ 1
2 )dαdβ
!
= 1
σ (f(xk+1, yk) −f(x∗, y∗)) .
In the ﬁrst inequality, we used (D.11) and the deﬁnition of ˜y, and in the second we used (D.10)
(since xα ∈Xk+1, yα, ˜yα ∈Yk by convexity).
We now give a helper lemma specialized to the particular f in (D.9), which will be used in the
proof of convergence.
Lemma 252. For some xk+1, yk, let Xk+1 =

x | x ≥1
2xk+1
	
where the inequality is entrywise,
and let Yk be the entire domain of y (i.e. Y). Then for any x′ ∈Xk+1, y′, y′′ ∈Yk,
∇2r(x′, y′) ⪰1
12∇2
yyr(xk+1, y′′).
Proof. Recall that (since ∥A:j∥1 = 2)
∇2r(x, y) = 2 ∥d∥∞

5 ∥A:j∥1 diag

1
xj

2A⊤diag (yi)
2diag (yi) A
2diag
 A⊤
i x


.
Consider the diagonal approximation
D(x) = 2 ∥d∥∞

∥A:j∥1 diag

1
xj

0
0
diag
 A⊤
i x


.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
654
We claim for any y,
D(x) ⪯∇2r(x, y) ⪯6D(x).
(D.12)
To see this, consider the quadratic forms with respect to some vector

u
v

:

u
v

∇2r(x, y)

u
v

= 2 ∥d∥∞
X
i,j
Aij
 
5u2
j
xj
+ 4ujviyi + 2v2
i xj
!
,

u
v

D(x)

u
v

= 2 ∥d∥∞
X
i,j
Aij
 
u2
j
xj
+ v2
i xj
!
.
Now (D.12) follows because for any yi ∈[−1, 1], it's easy to verify
u2
j
xj
+ v2
i xj ≤5u2
j
xj
+ 4ujviyi + 2v2
i xj ≤6
 
u2
j
xj
+ v2
i xj
!
.
Therefore, to prove the lemma statement we can use
∇2r(x′, y′) ⪰D(x′) ⪰1
2D(xk+1) ⪰1
12∇2
yyr(xk+1, y′′).
The inequality D(x′) ⪰
1
2D(xk+1) followed from the deﬁnition of Xk+1, and the last inequality
followed from D(xk+1) spectrally dominating 1
6∇2r(xk+1, y′′), and restrictions of D(xk+1) to the yy
block can only decrease the quadratic form.
We now give the proof of the linear rate of convergence.
Lemma 253. For f(x, y) deﬁned in (D.9), the alternating minimization scheme
1. xk+1 := argminx∈X f(x, yk).
2. yk+1 := argminy∈Yf(xk+1, y).
decreases the function error f(xk, yk) −f(xOPT, yOPT) by a factor of at least 1/24 in each iteration.
Proof. We can apply Lemma 251 with the sets deﬁned in Lemma 252, with σ = 12. On iteration k,
consider picking the points x∗, y∗= 1
2(xk+1 + xOPT), 1
2(yk + yOPT). Evidently, x∗∈Xk+1, y∗∈Yk.
Therefore, since f(xk+1, yk+1) ≥f(xk+2, yk+1),
f(xk+1, yk) −f(xk+2, yk+1) ≥f(xk+1, yk) −f(xk+1, yk+1) ≥1
12(f(xk+1, yk) −f(x∗, y∗)).
Furthermore, by convexity, we have
f(xk+1, yk) −f(x∗, y∗) ≥1
2(f(xk+1, yk) −f(xOPT, yOPT)).

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
655
Finally, combining these two inequalities and rearranging,
23
24(f(xk+1, yk) −f(xOPT, yOPT)) ≥f(xk+2, yk+1) −f(xOPT, yOPT).
Thus, by taking a y step and then an x step, we decrease the function error by a 1/24 factor.
Finally, we show that steps of the alternating minimization can be implemented in linear time.
Lemma 254. For f(x, y) deﬁned in (D.9), we can implement the steps
1. xk+1 := argminxf(x, yk).
2. yk+1 := argminyf(xk+1, y).
restricted to the relevant domains, in time O(n2).
Proof. Recall A has n2 nonzero entries, so a matrix-vector multiplication can be performed in this
time. Computing x in linear time is straightforward: it is deﬁned by
argminx ⟨γ, x⟩+
X
j∈[n]
xj log xj such that x ∈∆m, γ :=
1
20 ∥d∥∞
ξ + 1
10A⊤(y2).
By examining the KKT conditions, it is clear that the minimizing x is proportional to exp(−γ);
computing γ takes O(n2) time, as does the simplex projection. Similarly, computing y in linear time
is simple for ﬁxed x: it is
argminy ⟨η, y⟩+

2 ∥d∥∞Ax, y2
such that y ∈[−1, 1]2n,
which is coordinate-wise decomposable as minimizing a quadratic over an interval.
Theorem 83 (Complexity of alternating minimization). We can obtain an ϵ/2-approximate mini-
mizer to the proximal steps required by Algorithm 71 to ϵ/2 accuracy, with the regularizer of (D.6)
and κ = 3, in O(log γ) parallelizable iterations for γ = log n · ∥d∥∞· ϵ−1, and O(n2 log γ) total work.
Proof. By Lemmas 253 and 254, we can spend O(n2) parallelizable work to decrease the suboptimal-
ity gap by a 1/24 factor, so it remains to argue that the initial error is at most poly(log n, ∥d∥∞, ϵ−1)
to show that implementing the proximal steps to additive error ϵ/2 can be done in O(log γ) itera-
tions. We show that this is true for implementing the proximal step for zt; a similar argument holds
for wt. To this end, note that by our setting of κ, for any z where we let g(z) = (gx(z), gy(z)),
1
2κ ∥gx(z)∥∞= 1
6
d + 2 ∥d∥∞A⊤y

∞≤∥d∥∞
2
,
1
2κ ∥gy(z)∥1 = 1
6 ∥2 ∥d∥∞(b −Ax)∥1 ≤4 ∥d∥∞
3
.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
656
Therefore, for st = (sx
t , sy
t ), by the triangle inequality, and t ≤12Θ/ϵ the bound on the number of
steps required where Θ is the range of r, we have
∥sx
t ∥∞≤t · 1
2κ ∥gx(z)∥∞≤6 ∥d∥∞Θ
ϵ
,
∥sy
t ∥1 ≤t · 1
2κ ∥gy(z)∥1 ≤16 ∥d∥∞Θ
ϵ
.
A simple calculation yields Θ = 20 ∥d∥∞log n + 4 ∥d∥∞upper bounds the range of r. Finally, let
x∗
t , y∗
t be the minimizer of the proximal objective,
⟨sx
t , x⟩+ ⟨sy
t , y⟩+ r(x, y).
For any initialization xinit, yinit to the alternating minimization, the suboptimality gap is given by
⟨sx
t , xinit −x∗
t ⟩+ ⟨sy
t , yinit −y∗
t ⟩+ r(xinit, yinit) −r(x∗
t , y∗
t )
≤∥xinit −x∗
t ∥1 ∥sx
t ∥∞+ ∥yinit −y∗
t ∥∞∥sy
t ∥1 + Θ ≤
44 ∥d∥∞
ϵ
+ 1

Θ.
Therefore, the total number of iterations required is bounded by 24 log

88∥d∥∞
ϵ2
+ 2
ϵ

Θ

as desired.
Numerical precision.
We also make a brief comment on bit-complexity issues which may arise
when scaling exponentials. In particular, each of our alternating minimization steps of the form
⟨g, x⟩+
X
j
xj log xj
(D.13)
require exponentiating a potentially large vector log x −g, and rescaling the vector to be on the
simplex. The following lemma shows that we can implement this step with O(log n) bit-complexity,
with a small polynomial (say n−90) loss in the objective value. Because we may assume that ϵ−1 is
bounded by say, n2, else an interior point method achieves our stated runtime, the cumulative loss
in objective value over all iterations due to limited precision is signiﬁcantly less than ϵ, and does not
aﬀect our asymptotic convergence rate. More precisely, we maintain x implicitly through a vector
v which is log x up to a scaling, and show that by truncating v to have its range of coordinates
bounded by O(log n), the resulting simplex variable remains a high-precision minimizer to (D.13).
Lemma 255. Let v ∈Rm, and let x ∈∆m be such that x ∝exp(v).
Consider the following
operation: let j∗= argminjvj, and j′ be such that vj′ < vj∗−100 log n.
Set ˆv = v in every

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
657
coordinate, except ˆvj′ ←vj∗−100 log n. Then, for ˆx ∝exp(ˆv) in the simplex,
X
j
ˆxj log ˆxj −
X
j
xj log xj < n−95, ⟨g, ˆx −x⟩< ∥g∥∞n−95.
Proof. Clearly, ∥exp(v)∥1 < ∥exp(ˆv)∥1, since exp is monotone. Moreover,
∥exp(ˆv)∥1 −∥exp(v)∥1 = exp(vj∗−100 log n) −exp(vj′) < n−100 exp(vj∗) < n−100 ∥exp(v)∥1 .
Now, for every coordinate j ̸= j′, this implies that (1 −n−100)xj < ˆxj < xj. Thus,
ˆxj log ˆxj −xj log xj < −n−100xj log xj.
Furthermore, we have
ˆxj′ log ˆxj′ −xj′ log xj′ < −xj′ log xj′ < 100n−100,
by −x log x is increasing for small x and xj′ is bounded by n−100. Combining these estimates,
X
j
ˆxj log ˆxj −
X
j
xj log xj < (100 + log n)n−100 < n−95
for n > 10. Similarly,
⟨g, ˆx −x⟩≤∥g∥∞

ˆxj′ +
X
j̸=j′
n−100xj

≤∥g∥∞n−95
for n > 10.
Thus, repeatedly applying Lemma 255 every time we need to truncate a coordinate of v due to
ﬁnite bit precision, over the course of all iterations of the algorithm and all alternating minimization
steps, the error incurred is negligible compared to the desired accuracy ϵ (where we also note ∥g∥∞
for all g we encounter is bounded by a small polynomial in n).
D.1.6
Experiments
We show experiments illustrating the potential of our algorithm to be useful in practice, by consider-
ing its performance on computing optimal transport distances on the MNIST dataset and comparing
against algorithms in the literature including APDAMD [365] and Sinkhorn iteration. All compar-
isons are based on the number of matrix-vector multiplications (rather than iterations, due to our
algorithm's alternating subroutine), the main computational component of all algorithms considered.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
658
(a) Comparison with Sinkhorn iteration with diﬀer-
ent parameters.
(b) Comparison with APDAMD [365] with diﬀerent
parameters.
(a) Comparison with Sinkhorn iteration on 20 ran-
domly chosen MNIST digit pairs.
(b) Comparison with APDAMD [365] on 20 ran-
domly chosen MNIST digit pairs.
While our unoptimized algorithm performs poorly, slightly optimizing the size of the regularizer
and step sizes used results in an algorithm with competitive performance to APDAMD, the ﬁrst-order
method with the best provable guarantees and observed practical performance. Sinkhorn iteration
outperformed all ﬁrst-order methods experimentally; however, an optimized version of our algorithm
performed better than conservatively-regularized Sinkhorn iteration, and was more competitive with
variants of Sinkhorn found in practice than other ﬁrst-order methods.
As we discuss in our implementation details, we acknowledge that implementations of our al-
gorithm illustrated are not the same as those with provable guarantees in our paper. However, we
believe that our modiﬁcations are justiﬁable in theory, and consistent with those made in practice to
existing algorithms. Further, we hope that studying the modiﬁcations we made (step size, using mir-
ror prox [415] for stability considerations), as well as the consideration of other numerical speedups
such as greedy updates [25] or kernel approximations [24], will become fruitful for understanding
the potential of accelerated ﬁrst-order methods in both the theory and practice of computational
optimal transport.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
659
Implementation details.
Dataset. For the ﬁrst two ﬁgures, we had the following experimental setup. We randomly sampled
a pair of digits from the MNIST dataset corresponding to the digit 1, and added a small amount of
background noise for numerical stability, as is standard in the literature [25]. We downsampled the
28 × 28 pixel images to size 14 × 14 by skipping every other pixel to speed up experiments. Similar
performances were observed across multiple random instances. Finally, the cost metric used was by
Manhattan distance on the 2-dimensional grid.
For the second pair of ﬁgures, we randomly sampled 20 pairs of digits from the MNIST dataset
where each pair corresponds to the same digit. As before we added a small amount of background
noise for stability. As opposed to the previous comparison we ran all three algorithms on the true
28 × 28 pixel images. For each of the digit pairs we ran the Sinkhorn algorithm to high precision to
obtain a baseline solution for comparison, and for each of the three algorithms tested we compared
the value of the solutions obtained to this baseline. The plots compare the number of matrix-vector
multiplies to the objective value error averaged over all 20 digit pairs.
The metric is again the
Manhattan distance over the 2-dimensional grid.
Objective value. For simplicity, in all cases we measured objective value by the overestimate
presented in (D.4). By the proof of Lemma 247, this is an overestimate to the true objective after
performing the rounding procedure in Algorithm 72. In practice, we observed that this overestimate
was negligibly diﬀerent from the objective after rounding.
Sinkhorn implementation details. We implemented the standard Sinkhorn algorithm, using dif-
ferent settings of η−1.
Sinkhorn iteration converges to an ϵ-approximate transportation plan in
theory when η is very large, roughly log n/ϵ. However, in practice, it is observed that much smaller
values of η suﬃce for rapid convergence. We tracked the convergence of Sinkhorn iteration for η = 70
and η = 5, which we considered close to a theoretically guaranteed parameter and a much less con-
servative practical parameter, respectively. The optimized Sinkhorn algorithm converged at rates
much faster than the predicted ϵ−2 rate on all experiments, outperforming all other methods, which
we believe merits further investigation. Signiﬁcantly larger values of η led to numerical stability
issues when computing exp(−ηC).
APDAMD implementation details. We implemented the APDAMD algorithm (Algorithm 4 in
[365]), with the quadratic regularizer (i.e.
1
2γ ∥λ∥2
2). We observed that the amount of the quadratic
regularizer added did not aﬀect the practical convergence of the algorithm. A simple reason for this is
because the algorithm builds in a more aggressive step-size strategy, because the pessimistic γ = O(n)
is often too conservative to be necessary in practice. The ﬁgure tracks APDAMD convergence with
η = 10−2, ϵ = 10−3.
Mirror prox. For numerical stability considerations, we implemented our algorithm as an in-
stance of mirror prox [415], another extragradient method which takes local iterations rather than
accumulating a dual operator and taking steps with respect to some ¯z (i.e. dual extrapolation).

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
660
Although there is not a known proof of mirror prox convergence with an area-convex regularizer, we
ﬁnd this decision reasonable for several reasons. In general, variations of entropic mirror descent are
well-known to be equivalent to their dual averaging versions; it is likely that a similar equivalence
can be drawn between mirror prox and extragradient dual averaging, i.e. dual extrapolation. Fur-
thermore, the standard proofs of dual extrapolation and mirror prox are quite similar; we believe
it is likely that area-convexity results in convergence for mirror prox, although this merits further
investigation.
Termination. We terminated our alternating minimization procedure when the movement of
iterations in ℓ1 was negligible. Typically, we observed that 3-5 alternating steps suﬃced for conver-
gence.
Step sizes. We varied two parameters in our experiments: the step size 1
κ used in our extragra-
dient algorithm, and the amount of entropy used in our regularizer (in the paper, we used 10 times
entropy compared to the quadratic component x⊤A⊤(y2)). One reason this may be reasonable in
practice is similar to the observed behavior of the Sinkhorn iteration tuning the η−1 parameter, and
APDAMD performing a more-aggressive line search for the observed amount of regularizer neces-
sary. We note that the need to tune the amount of entropy used in the regularizer is likely due to
the analysis not being tight in the constants, and with a tighter analysis, it may not be necessary
to tune this parameter. To this end, we plotted the performance of three algorithm settings.
• In the "unoptimized constants", we set the constants to roughly those with theoretical guar-
antees, i.e. 10 times entropy and step size 1.
• In the "reasonably optimized constants", we set the amount of entropy to be 4, and the step
size to be ∥d∥∞/3, to oﬀset the ∥d∥∞multiple of the regularizer used in our iterations. For
smaller values of ϵ, these settings compared favorably with APDAMD.
• In the "optimized constants", we set the amount of entropy at 3, and the step size at ∥d∥∞.
This setting outperformed APDAMD and was more competitive with Sinkhorn iteration.
Discussion. We believe multiple interesting avenues of exploration arise from our experiments.
• Sinkhorn with aggressively chosen η outperformed all other methods we benchmarked against,
and converged at rates faster than suggested by its known analyses. It may prove fruitful to
study if further assumptions about practical instances explain this discrepancy.
• Directly accelerated methods such as APDAMD also exhibit ϵ−1 convergence rates, at the cost
of a worse dependence on dimension. However, this worst-case dependence can be mitigated
if the instance is favorable in practice, i.e. by choosing γ ≈O(1). This was observed to be the
case in our experiments for the MNIST dataset. It is interesting to see if a similar adaptive
tuning applies to our method with provable guarantees.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
661
• Our method did not exhibit instability when changing the amount of entropy in the regularizer,
but it did exhibit vastly-improved convergence. It is possible that the amount of regularizer
needed is not quite so large, perhaps through a more careful analysis.
• We did not benchmark against the greedy Sinkhorn method of [25], or consider numerical
speedups such as those in [24]. It remains open to explore if these practical speedups are
applicable to ﬁrst-order methods such as ours as well.
D.1.7
Deferred proofs from Section 5.3
In this section, we give a proof of Proposition 14, which we restate for convenience. We recall the
following deﬁnition: for convex-concave f : X × Y →R, we say a pair (x, y) is an ϵ-approximate
saddle point if f(x, y′) −f(x′, y) ≤ϵ ∀x′ ∈X, y′ ∈Y. Before embarking on the proof, we require
the following helper lemma for demonstrating correctness of Line 2 of Algorithm 13.
Lemma 256. Consider (5.1) under the transformation described by Lines 1 and 2 of Algorithm 13.
The approximate optimality of any point in ∆m for the corresponding problem (5.2) is unaﬀected
by this transformation (i.e. if some point is ϵ-approximately suboptimal for the new problem, it also
has ϵ-approximate optimality for the old problem). Moreover, using O(n) space and one pass we can
implement this transformation for the remainder of any semi-streaming algorithm under the model
of Deﬁnition 18.
Proof. We discuss correctness of each line separately.
Correctness of Line 1.
Let i∗= argmini∈[m]ci.
We claim that the minimizer of ⟨c, x⟩+
A⊤x −b

1 only puts nonzero values on coordinates i ∈[m] where ci ≤ci∗+ 2 ∥A∥∞. To see
this, consider any x ∈∆m where xi ̸= 0 for some ci ≤ci∗+ 2 ∥A∥∞, and consider taking ˜x = x in
every coordinate, except ˜xi∗= xi∗+ xi and ˜xi = 0 (i.e. we zero out the coordinate i and shift its
mass to i∗). Then,
⟨c, x −˜x⟩> 2 ∥A∥∞xi = ∥A∥∞∥˜x −x∥1 ≥
A⊤(˜x −x)

1 ≥
A⊤˜x −b

1 −
A⊤x −b

1 .
Rearranging implies ˜x obtains better objective value for the problem (5.2). Hence, by ignoring all
large coordinates, any ϵ-approximately suboptimal point in the new problem is also ϵ-approximately
suboptimal for the old problem, because the objective values are the same and any feasible point in
the new problem is also feasible for the old problem. Finally, shifting c by a multiple of the all-ones
vector aﬀects the problem globally by a constant, since x is in the simplex.
Correctness of Line 2. Because c⊤x is unaﬀected, it suﬃces to discuss the eﬀect on
A⊤x −b

1 =
X
j∈[n]


A⊤x

j −bj
 .

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
662
For disambiguation call b′ the result of the truncation, and b the vector in the original problem.
Consider a coordinate where bj ̸= b′
j; suppose without loss that bj > ∥A∥∞(the other case is
handled symmetrically). Then, since

A⊤x

j ≤∥A∥∞by deﬁnition,


A⊤x

j −bj
 = bj −

A⊤x

j = (bj −∥A∥∞)+

∥A∥∞−

A⊤x

j

= (bj −∥A∥∞)+


A⊤x

j −b′
j
 .
Hence the eﬀect is a constant scalar shift, as desired. Finally, regarding the semi-streaming imple-
mentation, b is n-dimensional and hence we can store it in O(n) space. We can also store ∥A∥∞
and Cmax in one pass and constant space, and simulate the post-processed vector c by modifying
any coordinate ci encountered in a stream using these precomputed values.
Proposition 14. By taking T = O( ∥A∥∞log m
ϵ
) for a suﬃciently large constant, Algorithm 13 results
in iterates {(x′
t, y′
t)}0≤t<T so that ¯x := 1
T
P
0≤t<T x′
t is an ϵ-approximate minimizer to (5.2).
Proof. First, the eﬀects of Lines 1 and 2 of Algorithm 13 does not aﬀect the problem (5.2) other
than changing the objective value everywhere by a constant, by Lemma 5 of [147] and Lemma 256
respectively. Next, the claim on the number of iterations required by Algorithm 13 follows immedi-
ately from Corollary 59. It remains to show that the given value of K suﬃces for Algorithm 14 to
solve the subproblems to δ additive accuracy.
For simplicity, we will prove this bound on K suﬃces for the computation of wt; the bound for
zt+1 follows analogously. Lemma 251 demonstrates that in solving the minimization subproblem of
Algorithm 14, every iteration of alternating minimizaton decreases the suboptimality gap for the
subproblem by a constant factor, so we only require a bound on the initial error of each subproblem
in Line 5 of Algorithm 13. The subproblem is of the form: minimize over z = (x, y) ∈∆m ×[−1, 1]n,
⟨sx
t + gx
t, x⟩+ ⟨sy
t + gy
t, y⟩+ x⊤|A|(y2) + 10 ∥A∥∞
X
i∈[m]
xi log xi
[xt]i
=: ⟨γ, z⟩+ r(z).
(D.14)
Call the optimal solution to the above problem z∗. Note that the initial point used by Algorithm 14
in Line 8 is zt, which by assumption was an ϵ
2-approximate minimizer to an objective of the form
(D.14) where γ is replaced by γ′ such that by construction (and our pre-processing of c),
∥(γ −γ′)x∥∞= O (∥A∥∞) , ∥(γ −γ′)y∥1 = O (max (∥A∥∞, ∥b∥1)) .
Hence,
⟨γ, zt⟩+ r(zt) = ⟨γ′, zt⟩+ r(zt) + ⟨γ −γ′, zt⟩
≤⟨γ′, z∗⟩+ r(z∗) + ϵ
2 + ⟨γ −γ′, zt⟩
= ⟨γ, z∗⟩+ r(z∗) + ϵ
2 + ⟨γ −γ′, zt −z∗⟩.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
663
In the only inequality, we used the suboptimality bound on zt. Hence, zt can have initial function
error at most O(∥A∥∞+ ∥b∥1 + ϵ) for the objective (D.14), yielding the required bound on K.
To give the ﬁnal result, we claim that any ϵ-approximate saddle point (¯x, ¯y) to a convex-concave
function f on the domain X, Y has that fX (¯x) approximates the value of the primal-only problem
min
x∈X fX (x), where fX (x) := max
y∈Y f(x, y),
within an additive ϵ. To see this, let (x⋆, y⋆) be the exact saddle point of f so that f(x⋆, y⋆) = OPT.
By deﬁnition of (¯x, ¯y),

max
y∈Y f(¯x, y) −OPT

+

OPT −min
x∈X f(x, ¯y)

≤ϵ.
Moreover, both quantities on the left hand side are nonnegative by deﬁnition of OPT, i.e.
OPT = f(x⋆, y⋆) = min
x∈X

max
y∈Y f(x, y)

= max
y∈Y

min
x∈X f(x, y)

.
Thus, by nonnegativity of the second quantity, we conclude that
fX (¯x) = max
y∈Y f(¯x, y) ≤OPT + ϵ = fX (x⋆) + ϵ.
D.2
Matching tools from the literature
In this section, we prove Lemma 71 and Proposition 15. We begin with the former.
Lemma 71. The greedy algorithm can be implemented in one semi-streaming pass over a graph
using O(n) space and O(m) work to return a matching of size M with M ≤M ∗≤2M.
Proof. It is folklore that the greedy algorithm (adding edges one at a time to a matching, if and only
if neither endpoint is already matched) is a 2-approximation to the MCM, proving the approximation
guarantee. Regarding the implementation, we can keep marking an array of vertex indices and check
if a current edge violates a matched vertex, in O(1) time per edge.
We now proceed to Proposition 15. We ﬁrst state our algorithm, a greedy iterative procedure
which repeatedly adds a maximal matching from a maintained vertex set to the remaining vertices.
We require one key technical claim for analyzing Algorithm 73.
Lemma 257. Fix an iteration 0 ≤i ≤N −1 of Algorithm 73, and suppose the MCM size of G[Vi]
is Mi. Then, the MCM size of G[Vi+1] increases by at least 1
3(M ∗−Mi).

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
664
Algorithm 73: VertexReduction(G, ϵ)
1 Input: Bipartite graph G = (V, E) with MCM size M ∗;
2 Output: Vertex subset V ′ ⊆V with |V ′| = O(M ∗log(ϵ−1)) and G[V ′] that has MCM size
≥(1 −ϵ)M ∗;
3 V0 ←vertices incident to some maximal matching in G;
4 N ←⌈3 log(ϵ−1)⌉;
5 for i ∈[N] do
6
S ←vertices in V \ Vi−1 incident to some maximal matching between Vi−1 and V \ Vi−1;
7
Vi ←Vi−1 ∪S;
8 Return: V ′ ←VN
Proof. The symmetric diﬀerence between the maximum matchings of G[Vi] and G can be decomposed
into even cycles and paths (to see this, every vertex in the symmetric diﬀerence has degree at most
2). Hence, ignoring all even cycles, there is a vertex-disjoint augmenting path decomposition of size
k := M ∗−Mi; call these paths P1, P2, . . . Pk. For each path Pj, j ∈[k], all of its edges except
the ﬁrst and last are contained in G[Vi]; call the ﬁrst and last edges (xj, x′
j) and (yj, y′
j), where
xj, yj ∈G[Vi]. We consider a few cases, "processing" paths sequentially.
1. If both (xj, x′
j) and (yj, y′
j) are included in the maximal matching between Vi and V \ Vi, this
increases the maximum matching size in Vi+1 by one since we can use the augmenting path.
2. If both xj and yj are matched to other vertices x′′
j ̸= x′
j and y′′
j ̸= y′
j, we can still use the
resulting augmenting path in Vi+1. The other vertices x′′
j , y′′
j can only remove two other later
paths from consideration in the process, where we consider paths sequentially. Note that if x′′
j
or y′′
j is not an endpoint from a later path, this can only help the argument here.
3. If one of xj and yj is matched to its corresponding endpoint in Pj but the other is not, we can
use the augmenting path and remove one other path from consideration.
Because we took a maximal matching between Vi and S \ Vi, at least one of the above cases must
happen, so we increase the MCM size by at least 1
3k.
We conclude with our analysis of Algorithm 73.
Proposition 15 (Vertex size reduction). There is a procedure VertexReduction (Algorithm 73) which
takes as input unweighted bipartite G = (V, E) with MCM size M ∗and with O(log ϵ−1) passes,
O(M ∗log ϵ−1) space, and O(m log ϵ−1) work outputs a subset V ′ ⊆V of size O(M ∗log ϵ−1) such
that the induced subgraph G[V ′] has a MCM of size at least (1 −ϵ)M ∗.
Proof. The correctness follows immediately by applying Lemma 257, and the fact that our original
maximal matching (contained in G[V0]) is of size at least 1
2M ∗. We now prove correctness. For
the ﬁrst pass, the complexity follows directly from Lemma 71. For all additional iterations of the

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
665
algorithm, note that we can only add O(M ∗) additional vertices to the current set since it is a valid
matching in the original graph, and the iteration can be implemented in a single pass analogously
to the proof of Lemma 71. The space overhead is only the maintenance of the current set Vi.
D.3
Cycle cancelling in low space
In this section, we provide implementation of a data structure which proves the following claim.
Proposition 16 (Cycle cancelling). Consider a (possibly weighted) matching problem on a bipartite
graph G = (V, E, w) (for MCM, we let w = 1). There is an algorithm that has the following property:
given a stream of length L, consisting of edge-ﬂow tuples (e, fe) where e ∈E and fe ∈R≥0, deﬁne
x ∈RE
≥0 to be the sum of all fe1e in the stream where 1e is the 1-sparse indicator of edge e. Then
the algorithm runs in O(n) space and O(L log n) time, and outputs a ﬂow ˜x supported on O(n) edges
forming a forest, so that ⟨w, ˜x⟩≥⟨w, x⟩, and B⊤x = B⊤˜x.
Speciﬁcally, we reduce its proof to demonstrating the existence of a bipartite cycle-cancelling
oracle.
Deﬁnition 51 (Bipartite cycle-cancelling oracle). We call O a bipartite cycle-cancelling oracle
(BCCO) if it is associated with a (weighted) bipartite graph G = (V, E, w), and given any vector
x ∈RE
≥0 supported on L ≤2n edges, O outputs a vector ˜x ∈RE
≥0 such that B⊤x = B⊤˜x and
⟨w, ˜x⟩≥⟨w, x⟩, so that ˜x is supported on at most n edges.
Lemma 258. A BCCO O is implementable in O(L) space and O(L log n) work.
Proof of Proposition 16. If the stream length is L ≤2n, applying O obtains the desired results.
Otherwise, it suﬃces to divide the stream into chunks of n edges, and repeatedly call O. Specif-
ically, we can input the ﬁrst 2n edges of the stream into O and produce a set of n edges, then input
these n edges with the next n edges of the stream, and so on. Inducting on the properties of O,
we obtain the value and feasibility guarantees on the ﬁnal output. Regarding the space and work
guarantees, it suﬃces to repeatedly apply Lemma 258, and note that we can reuse the space for
O.
The remainder of this section is devoted to proving Lemma 258. Speciﬁcally, we demonstrate
how to use the link/cut tree data structure of [488, 500], with a few small modiﬁcations, to provide
the required oracle. In the following discussion, we state some preliminaries on link/cut trees which
are known from prior work (namely a set of supported operations), and we put these components
together at the end of this section to give an implementation which proves Lemma 258.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
666
D.3.1
Link/cut tree description
The description in this section primarily follows the implementation of link/cut trees given in Chapter
5 of [500], with a few terminology changes following the later lecture notes of [169]. It is primarily
a summary of prior work, where we state the supported operations that we will use.
Data structure representation. The link/cut tree implementation of [488, 500] we will use is main-
tained as a forest, where each (undirected, rooted) tree in the forest is decomposed into "preferred
paths" and each path is stored as a splay tree, along with a parent pointer to the neighbor of the
preferred path endpoint closer to the root. Every edge in the forest has an associated value (which
can change). We refer to the tree representations of preferred paths as "auxiliary trees" and the
tree representation of all connected vertices as a "main tree." The link/cut tree supports queries or
modiﬁcations to main trees, including the following operations.
1. Link(v, w, C): add an edge of value C between v and w in the main tree (if they are in diﬀerent
trees, join them so that w is the parent of v).
2. Cut(v): remove the edge between v and its parent in the main tree.
3. ChangeRoot(r): change the root of the main tree to the vertex r.
4. LCA(v, w): return the least common ancestor of v and w in the main tree.
5. Min(v): return the smallest edge value between v and its root in the main tree.
6. Add(v, C): add C to all edge values between v and its root in the main tree.
7. Sum(v): return the sum of all edge values between v and its root in the main tree.
For a more complete and formal description, we refer the reader to [500, 169].
Space complexity of link/cut trees. Based on the representation described above, it is clear that
as long as the amount of additional information we need to store in the auxiliary trees to implement
path aggregation operations is a constant per vertex, the overall space complexity is O(n) where n
is an upper bound on the number of vertices of all main trees. The only remaining space overhead
is storing all auxiliary trees, and the parent pointers of preferred paths in main trees. We include
this discussion because it is not explicitly stated in the source material.
Work complexity of link/cut trees. We state the guarantees of the link/cut tree in the following
claim, which follows by the analysis of the works [488, 500].
Proposition 57 (Main result of [488, 500]). The amoritized cost of L calls to any of the operations
Link, Cut, ChangeRoot, LCA, Min, Add, or Sum is bounded by O(L log n).

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
667
D.3.2
Implementation of BCCO
We now prove Lemma 258.
We ﬁrst outline our approach in constructing the oracle, and then
describe a concrete implementation by using link/cut trees.
Approach outline.
The main goal is to show how to preserve an acyclic matching ˜x so that
B⊤˜x = B⊤x and ⟨w, ˜x⟩≥⟨w, x⟩after processing every edge, in a total of O(n log n) work using the
link/cut tree. Inductively it is clear that the output will satisfy all the requirements of O.
We now describe how to process a single input edge e = (u, v) where u ∈L and v ∈R are on
opposite sides of the bipartition. First, if u and v are both not in the data structure, create a new
main tree consisting of just this edge (with say u as the root). If only one is in the data structure
then create a new edge with value xe appropriately using Link. If u and v are in diﬀerent main trees,
we can call Link(u, v, xe). In these cases, no cycles are created and no edge values are changed.
The last case is when u and v are in the same main tree. The path between u and v in their
main tree and the new edge (u, v) forms a cycle. In order to preserve B⊤x, it suﬃces to alternate
adding and subtracting some amount C from edges along the cycle. In order to make sure ⟨w, x⟩is
monotone, it suﬃces to compute the alternating sum of weights along the cycle to pick a parity, i.e.
whether we add from every odd edge in the cycle and subtract from every even edge, or vice versa
(since all cycles are even length, one of these will increase the weight). Once we pick a parity, we
compute the minimum value amongst all edges of that parity in the cycle and alternate adding and
subtracting this value; this will result in the tree becoming acyclic and preserve all invariants.
We ﬁnally remark that it suﬃces to divide the support of the input x into connected components,
and remove cycles from each connected component. By designating a "root vertex" in each connected
component and processing the edges in that component in BFS order from the root, it is clear that
the tree corresponding to that component will never become disconnected (edges are only removed
when a cycle is formed, and removing any edge on the cycle will not disconnect the tree). Thus, it
suﬃces to discuss how to implement cycle cancelling for a single connected component.
Implementation. We discuss the implementation of cycle cancelling for a single connected com-
ponent. It will use three link/cut trees, called W, T+, and T−; the topology of these three trees will
always be the same (i.e. they are the same tree up to the edge values). Roughly speaking, W will
contain edge weights with alternating signs (e.g. each edge's weight will be signed (−1)depth of edge),
and will be used to determine the direction of cycle cancelling to preserve ⟨w, ˜x⟩≥⟨w, x⟩. Further,
T+ and T−will each correctly contain half of the edge values of the current fractional matching
(depending on their sign in W), and the other edge values will be set to a large quantity. They are
used to compute the minimum (signed) edge value in cycles and to modify the fractional matching.
Consider processing a new edge e = (u, v) with weight we and value xe. If this edge is in any
of the non-cycle-creating cases described in the outline, i.e. its endpoints do not belong in the same
tree with a path between them, then we add a copy to each of W, T+, and T−. Depending on the
depth of the edge in W (which we can check by looking at the sign of its parent edge), we either

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
668
give it value we or −we. If it is positive in W, then we give it value xe in T+ and value n2 ∥x∥∞in
T−; otherwise, we swap these values. We choose the value n2 ∥x∥∞for one copy of the edge, so that
it never becomes the minimum value edge returned by Min throughout the algorithm.
Finally, we handle the case where we need to remove a cycle (when u and v are in the same
tree, and the edge is not in the tree). We start by determining which half of edges in the cycle we
want to remove value from, and which we should add to. To do this, we query r = LCA(u, v) and
ChangeRoot(r) on the tree in W. Then, we compute the alternating sums of weights in each half of
edges in the cycle. This allows us to determine a direction to preserve ⟨w, ˜x⟩≥⟨w, x⟩. We also call
LCA(u, v) and ChangeRoot(r) for both copies of the main tree in T.
We next determine the amount we wish to alternatingly add and subtract along the u-to-r and v-
to-r paths by calling Min on the appropriate tree, T−or T+ (corresponding to the direction obtained
from querying W). We then call Add on both copies of the main tree in T for these paths, with the
value obtained by the Min queries or its negation appropriately. This zeros one edge, which we Cut
from all three copies of the tree. Finally, we revert to the original root in all three trees to maintain
correctness of signs. Overall, the number of link/cut tree operations per edge in the stream is a
constant, so Proposition 57 bounds the total work by O(n log n).
D.4
Sampling for rounding linear programming solutions
In this section, we give a general procedure for rounding a fractional solution which is returned by
our algorithm for box-simplex games in Section 5.3. This procedure applies to general box-simplex
problems, beyond those with combinatorial structure, so we include this section for completeness. In
particular, directly applying the sparsity bounds of this section to our matching problems directly
imply eO(n · poly(ϵ−1)) space bounds for the various matching-related applications in the paper, up
to width parameters, which roughly match our strongest results up to the ϵ−1 dependence. As an
example, we give an application of this technique to MCM at the end of this section.
Given streaming access to an approximate fractional solution x on the simplex (via an implicit
representation), a natural way of constructing a low-space approximate solution is to randomly
sample each entry of xi and reweight to preserve expected objective value; this is the rounding
strategy we analyze. We use the following Algorithm 2, parameterized by some prescribed {Mi}i∈[m].
D.4.1
Concentration bounds
For proofs in this section, as well as later, we crucially rely on well-known concentration properties
of bounded random variables. We state here a few facts used repeatedly throughout.
Proposition 58 (Chernoﬀbound). For K independent scaled Bernoulli random variables {Xk}k∈[K]

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
669
Algorithm 2 RandomSample(x, K, {Mi}i∈[m])
Input: Coordinates of x ∈Rm
≥0 in streaming fashion, sample count K, parameters {Mi}i∈[m]
9 for i ∈[m] do
10
Draw K independent random variables {Xk
i }k∈[K], where
Xk
i =
(
Mi
with probability
xi
Mi
0
otherwise
.
ˆxi ←1
K
P
k∈[K] Xk
i
11 return ˆx
satisfying Xk = Nk with probability pk, 0 < Nk ≤1 for all k ∈[K], and all 0 < δ < 1,
Pr



X
k∈[K]
Xk −
X
k∈[K]
EXk

≥δ
X
k∈[K]
EXk

≤2 exp
 
−
δ2 P
k∈[K] EXk
3
!
We give a simple generalization of Proposition 58 to the case where the scaled Bernoulli variables
are allowed to take on negative values.
Corollary 60 (Generalized Chernoﬀbound). For K independent scaled Bernoulli random variables
{Xk}k∈[K] satisfying Xk = Nk with probability pk, 0 < |Nk| ≤1 for all k ∈[K], and all 0 < δ < 1,
Pr



X
k∈[K]
Xk −
X
k∈[K]
EXk

≥δ
X
k∈[K]
E|Xk|

≤4 exp
 
−
δ2 P
k∈[K] E|Xk|
3
!
Proof. Divide the set [K] = K+ ∪K−, where we deﬁne K+ := {k ∈[K]|Nk ≥0} and K+ := {k ∈
[K]|Nk < 0}. Applying Proposition 58 to P
k∈K+ Xk and P
k∈K−−Xk yields the result.
Furthermore, the following one-sided Chernoﬀbound holds when 0 < Nk ≤1 and δ ≥1.
Proposition 59 (One-sided Chernoﬀbound). For K independent scaled Bernoulli random variables
{Xk}k∈[K] satisfying Xk = Nk with probability pk, 0 < Nk ≤1 for all k ∈[K], and all δ > 0,
Pr

X
k∈[K]
Xk −
X
k∈[K]
EXk ≥δ
X
k∈[K]
EXk

≤exp
 
−
δ2 P
k∈[K] EXk
2 + δ
!
Proposition 60 (Bernstein's inequality). For K independent random variables {Xk}k∈[K] satisfying
|Xk| ≤C with probability one, let V = P
k∈[K] Var[Xk]. Then for all t ≥0,
Pr



X
k∈[K]
Xk −
X
k∈[K]
EXk

≥t

≤2 exp

−
t2
2V + 2Ct/3

.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
670
D.4.2
Random sampling guarantees
We ﬁrst give a general guarantee on the approximation error incurred by random sampling via
Algorithm 2. While the guarantees are a bit cumbersome to state, they become signiﬁcantly simpler
in applications. For instance, in all our applications, all binary random variables are scaled with Mi
such that maxi∈[n] Mi ≤1 (see Lemma 259 for deﬁnition), and the bounds on A⊤ˆx −A⊤x become
standard multiplicative error approximations when the matrix A is all-positive.
Lemma 259. Consider an instance of problem (5.1) parameterized by A, b, c. For some x ∈∆m
whose coordinates can be computed in streaming fashion, deﬁne for all j ∈[n],
Bj =
h
|A|⊤x
i
j .
Let ˆx be the output of Algorithm 2 on input x with
Mi = min
j∈[n]
Bj
|Aij| and K = 12 log(mn)
ϵ2
.
With probability at least 1 −(mn)−1, ˆx satisﬁes the following properties for B := maxi∈[m] Mi:


A⊤ˆx −A⊤x

j
 ≤ϵBj for all j ∈[n],
| ∥ˆx∥1 −∥x∥1 | ≤ϵ max(1, B),
c⊤ˆx −c⊤x
 ≤ϵ ∥c∥∞max(1, B),
∥ˆx∥0 = O



1 +
X
i∈[m]
xi max
j∈[n]
|Aij|
Bj

· log(mn)
ϵ2

.
Proof. First note that it is immediate by deﬁnition of the Bj to see that all xi
Mi = maxj∈[n]
|Aij|xi
Bj
≤1
are valid sampling probabilities for all i ∈[m]. Also, recall that entrywise
ˆxi = 1
K
X
k∈[K]
Xk
i .
We show the ﬁrst property; ﬁx some j ∈[n] and consider [A⊤ˆx]j −[A⊤x]j. Applying Corollary 60

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
671
to the random variables { 1
Bj AijXk
i }i∈[m],k∈[K] with δ = ϵ, we see by deﬁnition of K that
X
i∈[m],k∈[K]
1
Bj
AijXk
i = K
Bj

A⊤ˆx

j ,
X
i∈[m],k∈[K]
E
 1
Bj
AijXk
i

= K
Bj
[A⊤x]j,
X
i∈[m],k∈[K]
E

1
Bj
AijXk
i
 = K
Bj
[|A|⊤x]j,
=⇒Pr
 K
Bj
[A⊤ˆx]j −[A⊤x]j
 ≥δ K
Bj
[|A|⊤x]j

≤4 exp

−δ2K[|A|⊤x]j
3Bj

≤
4
(nm)4 ,
where for the last inequality we use deﬁnitions of δ, K, and that Bj = [|A|⊤x]j, for all j ∈[n].
This conclusion for a coordinate j ∈[n] is equivalent to
[A⊤ˆx]j −[A⊤x]j
 ≤ϵBj.
Union
bounding over all j ∈[n], we thus have with probability at least 1 −
4
n3m4


A⊤ˆx −A⊤x

j
 ≤ϵBj,
∀j ∈[n].
Now we show the second and third properties. Given B := maxi∈[m] Mi, we ﬁrst apply Proposition 60
on the sum P
i∈[m] Kciˆxi = P
i∈[m],k∈[K] ciXk
i , using the bound
X
i∈[m],k∈[K]
Var

ciXk
i

≤K ∥c∥2
∞
X
i∈[m]
Var

Xk
i

≤K ∥c∥2
∞
X
i∈[m]
xiMi ≤K ∥c∥2
∞B.
Thus, we can choose parameters C = ∥c∥∞B, V = K ∥c∥2
∞B to obtain the following bounds for
K ≥12 log(mn)
ϵ2
depending on whether B > 1 or B ≤1. For B > 1, we have
Pr

K

X
i∈[m]
ciˆxi −
X
i∈[m]
cixi

≥ϵB ∥c∥∞K

≤2 exp
 
−
ϵ2B2 ∥c∥2
∞K2
2K ∥c∥2
∞B + 2 ∥c∥2
∞B2Kϵ/3
!
≤
1
4(mn)2 .
For B ≤1, we have
Pr

K

X
i∈[m]
ciˆxi −
X
i∈[m]
cixi

≥ϵ ∥c∥∞K

≤2 exp
 
−
ϵ2 ∥c∥2
∞K2
2K ∥c∥2
∞B + 2 ∥c∥2
∞BKϵ/3
!
≤
1
4(mn)2 .

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
672
Altogether this implies the third conclusion, and the second conclusion follows as a special case
when speciﬁcally picking c = 1n. Each holds with probability ≥1 −1
4(mn)−2.
Finally, for all i ∈[m], let Yi = 1 if ˆxi ̸= 0 and Yi = 0 otherwise. It is straightforward to see that
Yi = 1 with probability
1 −

1 −max
j∈[n]
|Aij|xi
Bj
K
≤K max
j∈[n]
|Aij|xi
Bj
=⇒
X
i∈[m]
E[Yi] ≤K
X
i∈[m]
xi max
j∈[n]
|Aij|
Bj
≤K max

X
i∈[m]
xi max
j∈[n]
|Aij|
Bj
, 1

.
Thus, by applying the one-sided Chernoﬀbound in Proposition 59 (where we consider the cases
where P
i∈[m] xi maxj∈[n]
|Aij|
Bj
≥1 and P
i∈[m] xi maxj∈[n]
|Aij|
Bj
≤1 separately),
Pr

X
i∈[m]
Yi ≥2K + 2K

X
i∈[m]
xi max
j∈[n]
|Aij|
Bj



≤
1
4(mn)2 .
Finally, applying a union bound, all desired events hold with probability 1 −
1
mn.
Note that in the semi-streaming model, one can choose Bj :=

|A|⊤x

j and compute all such
values in one pass when x is of the form in Lemma 69; however, occasionally we will choose larger
values of Bj to obtain improved sparsity guarantees.
Thus, we provide the following one-sided
guarantee, which holds when Aij ≥0, ∀i, j and Bj ≥[A⊤x]j.
Corollary 61. Consider an instance of problem (5.1) parameterized by A with Aij ≥0 ∀i,j, and
b, c. For some x ∈∆m whose coordinates can be computed in streaming fashion, suppose we have
bounds for all j ∈[n],
Bj ≥

A⊤x

j .
Let ˆx be the output of Algorithm 2 on input x with
Mi = min
j∈[n]
Bj
|Aij|, and K = 12 log(mn)
ϵ2
.
With probability at least 1 −(mn)−1, ˆx satisﬁes the following properties for B := maxi∈[m] Mi:

A⊤ˆx −A⊤x

j ≤ϵBj for all j ∈[n],
| ∥ˆx∥1 −∥x∥1 | ≤ϵ max(1, B),
c⊤ˆx −c⊤x
 ≤ϵ ∥c∥∞max(1, B),
∥ˆx∥0 = O



1 +
X
i∈[m]
xi max
j∈[n]
|Aij|
Bj

· log(mn)
ϵ2

.
Note that the ﬁrst and fourth properties of Corollary 61 are one-sided in that we only wish to

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
673
upper bound a property of ˆx. The proof is identical to Lemma 259, except that we use Proposition 59
with δ = ϵ ·
Bj
[|A|⊤x]j > 0 instead of Proposition 58 in cases where δ ≥1, which suﬃces for the one-
sided bound of the ﬁrst property. Similarly, when δ ≥1 as in the proof of the fourth property,
the one-sided Chernoﬀbound only helps concentration.
Next, we give an end-to-end guarantee
on turning Algorithm 13 into a solver for the fractional problem, via applying Algorithm 2 on the
output.
Lemma 260. Given (x, y), an ϵ
2-approximate saddle point of (5.1), let
Bj = [|A|⊤x]j,
B = max
i∈[m] min
j∈[n]
Bj
|Aij|
and ˆx = RandomSample(x, K, {Mi}i∈[m]) with
Mi = min
j∈[n]
Bj
|Aij|, and K = O




log(mn)

∥c∥2
∞+ ∥A∥2
∞

(1 + B)2 +
P
j∈[n] Bj
2
ϵ2



.
With probability 1−(mn)−1, (
ˆx
∥ˆx∥1 , y) is an ϵ-approximate saddle point to (5.1). Moreover, the total
space complexity of Algorithm 13 and Algorithm 2 to compute and store the output (
ˆx
∥ˆx∥1 , y) is
O



log n

X
i∈[m]
xi max
j∈[n]
|Aij|
Bj

·

∥c∥2
∞+ ∥A∥2
∞

(1 + B)2 +
P
j∈[n] Bj
2
ϵ2
+ n ∥A∥∞
ϵ



.
Proof. Our choice of K is with respect to an accuracy parameter on the order of
ϵ
(∥c∥∞+ ∥A∥∞)(1 + B) + P
j∈[n] Bj
.
Recall that an ϵ-approximate saddle point to a convex-concave function f satisﬁes
max
¯y∈Y f(x, ¯y) −min
¯x∈X f(¯x, y) ≤ϵ.
For our output (
ˆx
∥ˆx∥1 , y), since we keep the same box variable y, it suﬃces to show that the side of
the duality gap due to x and
ˆx
∥ˆx∥1 does not change signiﬁcantly. Namely, we wish to show
max
¯y∈Y f

ˆx
∥ˆx∥1
, ¯y

−max
¯y∈Y f(x, ¯y) =

c⊤
ˆx
∥ˆx∥1
+
A⊤
ˆx
∥ˆx∥1
−b

1

−
 c⊤x +
A⊤x −b

1

≤ϵ
2.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
674
By the triangle inequality, it equivalently suﬃces to show that
c⊤ˆx −c⊤x ≤ϵ
8, c⊤ˆx

1
∥ˆx∥1
−1

≤ϵ
8,
A⊤ˆx −A⊤x

1 ≤ϵ
8,
A⊤ˆx

1

1
∥ˆx∥1
−1
 ≤ϵ
8.
The ﬁrst and third conclusions hold by applying Lemma 259 for the choice of K. The second and
fourth hold by |c⊤ˆx| ≤∥c∥∞∥ˆx∥1 and
A⊤ˆx

1 ≤∥A∥∞∥ˆx∥1 and then applying Lemma 259 for the
choice of K. Finally, the desired sparsity follows by combining the space bound of the output (via
Lemma 259) and the space complexity of implicitly representing the average iterate.
D.4.3
Application: rounding MCM solutions
We give a simple application of our random sampling framework to computing an explicit low-space
approximate MCM. While the space complexity does not match our strongest results based on a
low-space cycle cancelling implementation, we hope it is a useful example of how to more generally
sparsify fractional solutions of box-simplex games without explicit combinatorial structure.
Following the reductions of Section 5.4, we assume in this section that we have a simplex variable
x ∈∆m and a matching size ¯
M ∈[1, n] such that
B⊤( ¯
Mx) ≤1V ,
¯
M ≥(1 −ϵ)M ∗,
where M ∗is the maximum matching size. We now show how to apply our random sampling proce-
dure, Algorithm 2, to sparsify the support of the matching without signiﬁcant loss.
Corollary 62. Suppose for an MCM problem, x ∈∆E satisﬁes B⊤x ≤
1
¯
M 1V , and ϵ ∈(0, 1). Let ˆx
be the output of Algorithm 2 on input x with me =
1
¯
M for all e ∈E, and K := 12 log n
ϵ2
. Then with
probability at least 1 −n−2, the output ˆx satisﬁes the following properties:
B⊤ˆx ≤1 + ϵ
¯
M 1V ,
| ∥ˆx∥1 −1| ≤ϵ,
∥ˆx∥0 = O
 ¯
M log n
ϵ2

.
Proof. It is straightforward to see that by the assumptions, we can take A = ¯
MB and Bv = 1 for all
v ∈V , and choose the accuracy level to be ϵ in Lemma 259. Thus, by the conclusions of Lemma 259,
noting that Me =
1
¯
M ≤1 holds for all e ∈E, we conclude that with probability at least 1 −n−2, all

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
675
desired guarantees hold:
¯
MB⊤ˆx ≤¯
MB⊤x + ϵ1V ≤(1 + ϵ)1V ,
| ∥ˆx∥1 −1| ≤ϵ,
∥ˆx∥0 = O
  X
e∈E
xe max
v∈V
|Aev|
Bv
!
· log(mn)
ϵ2
!
= O
 ¯
M log n
ϵ2

.
D.5
Approximate MCM via box-constrained Newton's method
The goal of this section is to prove the following Theorem 84. In particular, we give an alternate
second-order method to compute (1 −ϵ)-approximate maximum matchings in unweighted graphs
using techniques developed by [145] for solving matrix scaling and balancing problems.
Theorem 84. For a MCM problem on bipartite G = (V, E) with |V | = n, |E| = m and optimal
value M ∗, Algorithm 74 with parameter6 ϵ ∈[Θ

log(mn)
n

, 1
2] obtains a matching of size at least
(1 −ϵ)M ∗using eO(n) space, eO(ϵ−1) passes, and eO(m) work per pass.
We ﬁrst recall the dual (vertex cover) formulation of the standard bipartite matching linear
program
min
v≥0,Bv≥1 1⊤v,
(D.15)
where B is the unoriented incidence matrix. Our method (which is based on the box-constrained
Newton's method of [145]) uses a relaxation of this linear program which makes use of oriented
incidence matrices and Laplacians, which we now deﬁne.
Deﬁnition 52. Let G = (V, E, w) be a weighted undirected bipartite graph with bipartition L, R and
nonnegative edge weights w. We deﬁne the unoriented incidence matrix B ∈RE×V as the matrix
where for any edge e = (i, j), i ∈L, j ∈R the row corresponding to e in eB has eBei = eBej = 1, and
all other entries set to 0. Additionally, we deﬁne the oriented incidence matrix eB ∈RE×V as
eB =

IL
0
0
−IR

B,
and the graph Laplacian LG = eB⊤diag (w) eB.
When the graph is obvious we drop the subscript from LG. The orientation of the edges in eB are
typically chosen to be arbitrary in the literature, but we specify edge orientation as our algorithm
6This lower bound on ϵ is without loss of generality as otherwise we can use the algorithm in Section 5.5.1 which
computes an exact MCM with a larger value of ϵ.

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
676
will distinguish the vertices in L and R, and denote a vector x on these vertices as xL and xR
(or sometimes [x]L, [x]R for clarity). We will ﬁrst describe our regularization scheme for this LP
and prove that approximate minimizers for the regularized objective yield approximate fractional
vertex covers. We then prove some stability properties on the Hessian of our regularized objective
and show that a second-order method can be implemented in eO(n) space and eO(ϵ−1) passes. Our
choice of regularization and notion of stability are heavily based on [145], which employed a similar
regularization scheme to solve matrix scaling and balancing problems. We nevertheless give a self-
contained description of these details for completeness.
Lemma 261 (Properties of regularized vertex cover). Let G be an unweighted bipartite graph with
n nodes and m edges, and let L, R be the vertices on either side of its bipartition. Let M ∗be the size
of the maximum matching in G. Let ε ≥8 log(mn)
n
be a parameter, and set µ =
ε
4 log(mn). Consider
fµ(x) := 1⊤x + µ
 X
e∈E
e
1
µ (1−[Bx]e) +
X
i∈V
e−1
µ (xi+ ε
4 )
!
.
(D.16)
fµ has the following properties.
• For any x with fµ(x) < m we have Bx ≥(1 −ε
2)1 and x ≥−ε
2
• If x⋆is a feasible minimizer to D.15, x := (1 + ε
2)x⋆has fµ(x) ≤(1 + 2ε
3 )M ∗.
• fµ(x) ≥M ∗−εn for any x.
• For any x with fµ(x) < m, let x′ = min{x, 2 · 1} Then fµ(x′) ≤fµ(x).
Proof. We prove the claims in order. For the ﬁrst claim, let i be the index of the smallest entry of
x. If xi ≤−ε
2, then since xi + ε
4 ≤xi
2 and exp is always nonnegative,
fµ(x) ≥1⊤x + µ exp

−xi
2µ

≥nxi + µ exp

−xi
2µ

≥m.
The second inequality followed from the deﬁnition of xi and the third from monotonicity of the ex-
pression in xi. Thus x ≥−ε
21 as claimed. This also implies that 1⊤x ≥−εn
2 , so it is straightforward
to verify that if some 1 −[Bx]i ≥ε
2, then we would have fµ(x) ≥−εn
2 + (mn)2µ ≥m, completing
the ﬁrst claim.
Next, since x⋆is a feasible minimizer to (D.15) we have 1⊤x⋆= M ∗, x⋆≥0, and Bx⋆≥1.
Therefore by construction, 1⊤x = (1 + ε
2)M ∗, x ≥0, and Bx = (1 + ε
2)Bx⋆≥(1 + ε
2). Plugging

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
677
these into fµ(x) yields (since M ∗≥1)
fµ(x) ≤

1 + ε
2

M ∗+ µ

m exp
−ε
2µ

+ n exp
−ε
4µ

≤

1 + ε
2

M ∗+ µm + n
mn
≤

1 + 2ε
3

M ∗.
For the third claim, suppose x satisﬁed fµ(x) ≤M ∗−εn. As M ∗≤n we may apply the ﬁrst
claim and obtain x ≥−ε
2 and Bx ≥1 −ε
2. Let v = x + ε
21, so v ≥0 and Bv ≥1. This is a
contradiction: as v is feasible for (D.15) we have M ∗≤1⊤v = 1⊤x + εn
2 < fµ(x) + εn
2 < M ∗.
Finally, note that fµ(x) < m combined with the ﬁrst property of fµ implies that Bx ≥(1 −ε
2)1,
x ≥−ε
2.
Assume x has xi = α > 2 for some i: we will show that decreasing this coordinate
to 2 decreases fµ. The only terms aﬀected by changing xi are 1⊤x (which decreases by α −2),
µ P
j∈V e−1
µ (xj+ ε
4 ), and µ P
e∈E e
1
µ (1−[Bx]e). The ﬁrst of these sums increases by at most
µ exp

−2 + ε
4
µ

−µ exp

−α + ε
4
µ

≤(α −2)e
2
µ ≤α −2
mn ,
while each of the m terms in the second sum increases by at most
µ exp

−−1 −[xR]j
µ

−µ exp

−1 −α −[xR]j
µ

≤(α −2)e
1−ε
µ
≤α −2
mn ,
for some xj: we use our lower bound on x here. Thus the total change in fµ(x) is at most −(α −
2)
 1 −
1
mn −1
n

< 0.
We next show that derivatives of f satisfy some useful stability properties, which we now deﬁne.
Deﬁnition 53. Let f be a convex function. We say that f is r-second order robust if for any vectors
x, y where ∥x −y∥∞≤r we have (where ⪯is the Loewner order on the positive semideﬁnite cone)
e−1∇2f(x) ⪯∇2f(y) ⪯e∇2f(x).
Lemma 262. Let
gµ(x) = fµ



IL
0
0
−IR

x


where fµ is deﬁned in (D.16). The gradient and Hessian of gµ are
∇gµ(x) =

1L −zL
−1R + zR

−eB⊤y, ∇2gµ(x) = 1
µ

diag (z) + eB⊤diag (y) eB

where y := exp

1
µ

1 −eBx

, zL := exp

−1
µ(xL + ε
41)

, zR := exp

1
µ(xR −ε
41)

, and z :=

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
678
[zL, zR]7.
For all x, ∇2gµ(x) is a nonnegative diagonal matrix plus a weighted graph Laplacian
matrix. Further, gµ is convex and µ
2 -second order robust. Finally, for any x, δ we have
δ⊤∇2gµ(x)δ ≤2
µ · δ⊤ diag
 B⊤y

+ diag (z)

δ.
Proof. The gradient and Hessian claims may be veriﬁed directly as we have via the chain rule
∇gµ(x) =

IL
0
0
−IR

∇fµ



IL
0
0
−IR

x


and
∇2gµ(x) =

IL
0
0
−IR

∇2fµ



IL
0
0
−IR

x



IL
0
0
−IR

.
It is clear that z
µ is nonnegative and eB⊤diag

y
µ

eB is a weighted graph Laplacian. Convexity of
gµ follows from ∇2gµ(x) ⪰0 everywhere.
To prove second order robustness, for any x, x′ with
∥x −x′∥∞≤µ
2 ,
eBx −eBx′
∞≤
eB

∞∥x −x′∥∞≤2 · µ
2 ≤µ.
Thus every matrix forming the Hessian of gµ changes by at most a factor of e multiplicatively, and
so gµ is second order robust. For the ﬁnal claim, let |δ|, δ2 be applied entrywise. Then,
δ⊤eB⊤diag (y) eBδ =
X
i∈[m]
yi[eBδ]2
i ≤
X
i∈[m]
yi[B|δ|]2
i
≤
X
i∈[m]
yi[Bδ2]i[B1]i = 2
X
i∈[m]
yi[Bδ2]i = 2δ⊤diag
 B⊤y

δ.
The second line used Cauchy-Schwarz and B1 = 2 · 1. Since diag (z) ⪰0, this yields the claim.
It remains to implement a low-pass and low-space optimization procedure to minimize gµ. We
make use of a variant of the box-constrained Newton's method of [145] stated below.
Deﬁnition 54. We say that a procedure O is an (r, k)-oracle for a class of matrices M if, for any
A ∈M and vector b, O(A, b) returns a vector z where ∥z∥∞≤rk and
z⊤b + 1
2z⊤Az ≤1
2

min
∥y∥∞≤r y⊤b + 1
2y⊤Ay

.
The proof of the following is patterned from [145], but tolerates multiplicative error in the
Hessian.
7Here, we use the notation xL to refer to the vector x restricted to the entries in L ⊆V .

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
679
Lemma 263. Let f be r-second order robust with minimizer x⋆. For some x, let M be such that
1
2∇2f(x) ⪯M ⪯2∇2f(x). Let O be an (r, k)-oracle for a class of matrices M where M ∈M.
Then if max{∥x⋆∥∞, ∥x∥∞} ≤R and R ≥r, for any vector x, x′ = x+ 1
kO
  1
k∇f(x), 2e
k2 M

satisﬁes
f(x′) −f(x⋆) ≤

1 −
r
160kR

(f(x) −f(x⋆)).
Proof. We note that for any y where ∥y −x∥∞≤r we have
f(y) = f(x) + ∇f(x)⊤(y −x) +
Z 1
0
Z β
0
(y −x)⊤∇2f(αx + (1 −α)y)(y −x)dαdβ.
As the integral is over a triangle of area 1
2, the second order robustness of f yields
f(x) + ∇f(x)⊤(y −x) + 1
4e(y −x)⊤M(y −x) ≤f(y),
f(y) ≤f(x) + ∇f(x)⊤(y −x) + e(y −x)⊤M(y −x).
(D.17)
Let
ˆδ = argmin∥δ∥∞≤r
1
k ∇f(x)⊤δ + e
k2 δ⊤Mδ.
Also, let ∆= O( 1
k∇f(x), 2e
k2 M). By deﬁnition of O we have ∥∆∥∞≤rk and
1
k ∇f(x)⊤∆+ e
k2 ∆⊤M∆≤1
2
1
k ∇f(x)⊤ˆδ + e
k2 ˆδ⊤Mˆδ

.
Thus since x′ = x + 1
k∆we see ∥x′ −x∥∞≤r, and hence
f(x′) ≤f(x) + 1
k ∇f(x)⊤∆+ e
k2 ∆⊤M∆≤f(x) + 1
2
1
k ∇f(x)⊤ˆδ + e
k2 ˆδ⊤Mˆδ

.
(D.18)
Deﬁne ˜x =
r
2R(x⋆−x), we note that ∥˜x∥∞≤
r
2R (∥x⋆∥∞+ ∥x∥∞) ≤r. By the minimality of ˆδ we
observe that ˆδ achieves a smaller value than cex for any c ≤1. For the choice c =
1
4e2 this implies
1
2
1
k ∇f(x)⊤ˆδ + e
k2 ˆδ⊤Mˆδ

≤1
2
 c
k ∇f(x)⊤˜x + c2e
k2 ˜x⊤M˜x

=
1
8e2
1
k ∇f(x)⊤˜x +
1
4ek2 ˜x⊤M˜x

≤
1
8e2

f

x + 1
k ˜x

−f(x)

≤−
r
16ke2R (f(x) −f(x⋆)) ,

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
680
where the third line used (D.17) and the last used convexity of f. Plugging this into (D.18) yields
f(x′) −f(x⋆) ≤

1 −
r
16ke2R

(f(x) −f(x⋆)) .
Next, the class of matrices which are Laplacians plus a nonnegative diagonal admit an eﬃcient
O.
Lemma 264 (Theorem 5.11, [145]). Let M be the class of matrices which consist of a Laplacian
matrix plus a nonnegative diagonal. Let A ∈M be a matrix with m nonzero entries. There is an
algorithm which runs in eO(m) time and space and implements an (r, O(log n))-oracle for A.
We complement this with a known semi-streaming spectral approximation for Laplacians.
Lemma 265 (Section 2.2, [388]). Let G = (V, E, w) be a weighted undirected graph, given as an
insertion-only stream. There is an algorithm which for any ε ∈(0, 1) takes one pass and returns
a graph H with O(ε−2n log3(n)) edges such that LH ⪯LG ⪯(1 + ε)LH using eO(m) work and
O(ε−2n log3 n) space.
Algorithm 74: BoxConstrainedVC(G, ε, O)
1 Input: Bipartite graph G = (V, E) with vertex partition V = L ∪R and oriented
edge-incidence matrix eB given as a stream, ε > 0, (O(
ε
log(n)), k)-oracle O for symmetric
diagonal dominant matrices with nonpositive oﬀdiagonal ;
2 Output: v fractional vertex cover for G;
3 R :=
"
IL
0
0
−IR
#
;
4 x0 ←R1, µ =
ε
4 log(mn), T = O( log µ−1
µk
);
5 fµ(x) := 1⊤x + µ
P
e∈E e
1
µ (1−[Bx]e) + P
i∈V e−1
µ (xi+ ε
4 )
;
6 gµ(x) := fµ (Rx);
7 for 0 ≤t < T do
8
Compute ∇gµ(xt) and M, a 2-approximation of ∇2gµ(xt) with Lemma 265;
9
x′
t+1 = xt + 1
kO
  1
k∇gµ(xt), 2e
k2 M

;
10
xt+1 = max{min{x′
t+1, 2 · 1}, −2 · 1} entrywise;
11 Return: v = RxT + ε
21;
We now assemble these claims to prove the correctness of our algorithm.
Proposition 61. Let G = (V, E) be an unweighted bipartite graph with bipartition L, R. Given
access to a (O(
ε
log n), k)-oracle O for ∇2gµ, Algorithm 74 computes v, a feasible vertex cover of size
M ∗+ εn. For y := exp

1
µ(1 −Bx)

, there exists w with ∥w∥1 ≤µn so y −w is a feasible matching

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
681
with 1⊤y ≥M ∗−5εn. Algorithm 74 requires eO(n) auxiliary space and O( k log µ−1
µ
) passes, each
requiring eO(m) work, plus the work and space required by one O call.
As the space used by O can be reused between runs, the space overhead will be eO(n) throughout.
Proof. By Lemmas 261 and 262, O applies to a matrix family containing the Hessian ∇2gµ. In
addition, we see that ∇gµ may be computed in a single pass since it equals
∇gµ(x) = R (1 −z) −eB⊤y
for z = [zL, zR] and zL, zR, y as in Lemma 262. The ﬁrst term may be computed directly, while the
second can be computed analogously to Lemma 69. Further, we can obtain a 2-spectral approxima-
tion of ∇2gµ(x) in semi-streaming fashion: we compute z in one pass, and sparsify the Laplacian
using Lemma 265 while computing y coordinatewise in one pass. Thus in each iteration we perform
one pass and one call to O: as there are O( k log µ−1
µ
) iterations: the claimed space, pass, and work
bounds follow.
We now prove the correctness of the algorithm. Let x⋆be the minimizer of gµ. By construction,
gµ(x0) = fµ(Rx0) ≤2n ≤m and the function is monotone decreasing. We note that the point xt
has ∥xt∥∞≤2 by construction for all t, and that ∥x⋆∥∞≤2 by the fourth condition of Lemma 261.
Further, by Lemma 262 we see that is r-second order robust with r = µ
2 . By Lemma 263 we obtain
gµ(x′
t+1) −gµ(x⋆) ≤

1 −
µ
640k

(gµ(xt) −gµ(x⋆))
for any t. As the fourth condition of Lemma 261 implies gµ(xt+1) ≤gµ(x′
t+1), the ﬁnal xT has
gµ(xT ) −gµ(x⋆) ≤

1 −
µ
640k
T
(gµ(x0) −gµ(x⋆)) ≤µ3n
9e ,
where the second inequality uses gµ(x0) ≤2n and gµ(x⋆) ≥0. Thus if M ∗is the size of the minimum
vertex cover of G, we obtain by Lemma 261 that gµ(xT ) = fµ(RxT ) ≤(1+ 2ε
3 )M ∗+ µ3n
9e ≤M ∗+ ε
2n,
since M ∗≤
n
2 and µ ≤ε.
To complete the proof, the ﬁrst condition of Lemma 261 implies
v = Rx + ε
21 is nonnegative with Bv ≥1 and 1⊤v ≤M ∗+ ε
2n + ε
2n = M ∗+ εn: it is a feasible
fractional matching as claimed.
For the second claim, we observe that for any δ with ∥δ∥∞≤µ
2 and ˆx = xT + δ,
µ3n
9e ≥gµ(xT ) −gµ(x⋆) ≥gµ(xT ) −gµ(ˆx)
≥−∇gµ(xT )⊤δ −eδ⊤∇2gµ(xT )δ
≥−∇gµ(xT )⊤δ −2e
µ δ⊤ diag
 B⊤y

+ diag (z)

δ;

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
682
the ﬁrst line used optimality of x⋆, the second used second order robustness of gµ, and the third
used Lemma 262. We choose δ = −µ2
4e sign (∇gµ(xT )): this satisﬁes ∥δ∥∞≤µ
2 so
µ2
4e ∥∇gµ(xT )∥1 −µ3
8e
B⊤y + z

1 ≤µ3n
9e .
Now note
B⊤y + z

1 =
[eB⊤y]L + zL

1 +
−[eB⊤y]R + zR

1
≤
[eB⊤y]L + zL −1L

1 +
−[eB⊤y]R + zR −1R

1 + n = ∥∇gµ(xT )∥1 + n.
Plugging this in to the above expression and rearranging, we obtain
µ2
4e ∥∇gµ(xT )∥1 ≤µ3n
9e + µ3n
8e + µ3
8e ∥∇gµ(xT )∥1 ,
so ∥∇gµ(xT )∥1 ≤µn. This implies the existence of w with ∥w∥1 ≤µn where 1 + w = B⊤y + z.
Since z is nonnegative, B⊤y ≤1 + w with ∥w∥1 ≤µn as desired. We ﬁnally lower bound 1⊤y. Let
v′ = Rx −µ1. Taking the inner product of 1 + w = B⊤y + z with v′ and rearranging gives
1⊤xL −1⊤xR −µn −1⊤y + w⊤v′ = y⊤(eBx −1) −µ1⊤y + x⊤Rz −µ1⊤z,
or
fµ(Rx) −µn + w⊤v′ = y⊤(eBx −1) + 1⊤y + x⊤Rz.
Next, w⊤v′ ≥−∥w∥1 ∥v′∥∞≥−2µn. Further, since y = exp

1
µ(1 −eBx)

, for any i either [eBx]i −
1 ≤ε
2 or yi ≤exp

−1
µ
ε
2

=
ε
2mn. Thus y⊤(eBx−1) ≤ε1⊤y+ ε
2M ∗. Similarly, we obtain x⊤Rz ≤εn.
Plugging these in, and using µ ≤ε
4 and M ∗≤n yields
fµ(Rx) −3µn −εn −ε
2M ∗≤(1 + ε)1⊤y =⇒1⊤y ≥
1
1 + εfµ(Rx) −3εn.
Here we used µ ≤ε
4 and M ∗≤n. The claim follows from fµ(Rx) ≥M ∗−εn via Lemma 261.
Proof of Theorem 84. Let M ∗be the size of the maximum matching in G.
We ﬁrst preprocess
the graph G using Proposition 15 within the pass, space, and work budgets to reduce to n =
O(M ∗log(ϵ−1)). Applying Algorithm 74 to eG with ε =
ϵ
12 log(ϵ−1), by the second claim in Proposi-
tion 61 we obtain y ∈RE
≥0 with 1⊤y ≥(1 −ϵ
2)M ∗−5εn ≥(1 −O(ϵ))M ∗, and
X
j∈V
max
 [B⊤y]j −1, 0

≤µn ≤ϵ
12M ∗.
The conclusion follows upon applying Propositions 16 and 73 to round the approximate matching y

APPENDIX D. DEFERRED PROOFS FROM CHAPTER 5
683
to be sparse and feasible, and using the implementation of O from Lemma 258.

Appendix E
Deferred proofs from Chapter 6
E.1
Proofs for Section 6.3
E.1.1
Proofs for Section 6.3.1
Proposition 18. Let ϵ ∈(0, 1) and M ≥0. Given a family of (ϵ, β)-CROs {fM,E} for G = (V, E0),
and an (ϵ, T )-canonical solver for the family, Algorithm 20 satisﬁes the following.
1. When Mest > 1
4M on Line 7, where Mest estimates MCM(Ek): at any point in the loop of
Lines 9 to 10, 8M ˜xEk
Ek\Edel is an ϵ-approximate matching of G(V, Ek \ Edel).
2. When Mest ≤1
4M on Line 7, where Mest estimates MCM(E): MCM(E) ≤1
2MCM(E0).
The runtime of the algorithm is O(m + (T + m) · M
βϵ).
Proof. Throughout this proof, let the edge sets recomputed by Line 11 be denoted E1, E2, . . . , EK
and assume the termination condition Line 7 breaks the loop for EK+1, where E0 is the original edge
set of the graph. Before proving the two claims, we ﬁrst observe that for all k ∈[K], MCM(Ek) ≥M
8 .
The base case of k = 0 clearly holds because of the approximation guarantee of the greedy algorithm,
which yields M ≥1
2MCM(E). Now, for any k ∈[K], let M k
est be the greedily computed estimate of
MCM(Ek). We conclude by
MCM(Ek) ≥M k
est > 1
4M ≥1
8MCM(E).
Matching approximation. For any 0 ≤k ≤K, by item 1 of the CRO deﬁnition applied to Ek, we
have (1 + ϵ
8)MCM(Ek) ≥−νEk ≥(1 −ϵ
8)MCM(Ek), and hence combining with (6.11a) and (6.12a)
684

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
685
implies
−fM,Ek(ˆxEk) ≥−

1 −ϵ
8

νEk ≥

1 −ϵ
8
2 1
4M ≥3
16M,
(E.1)
−fM,Ek(˜xEk) ≥−

1 −ϵ
8

νEk ≥

1 −ϵ
8
2 1
4M ≥3
16M.
(E.2)
By the deﬁnition of the CRO in (6.10) and feasiblity of the matching 8M ˜xEk (which follows by the
assumed guarantees on Round), we have
8M
˜xEk
1 ≥−fM,Ek
 ˜xEk
−
ϵ
128M ≥5
32M =⇒
˜xEk
1 ≥
5
8 · 32.
(E.3)
Also, following the above calculation and (E.2) we have that
8M
˜xEk
1 ≥−fM,Ek
 ˜xEk
−
ϵ
128M ≥

1 −ϵ
8
2
MCM(Ek) −ϵ
32M ≥

1 −ϵ
2

MCM(Ek)
where for the last inequality we use MCM(Ek) ≥
1
4M, as shown in the ﬁrst part of this proof.
Consequently, considering any E between recomputations satisfying Ek+1 ⊂E ⊆Ek such that
E = Ek \ Edel, we have by the condition on Line 9 that
8M
˜xEk
Ek\Edel

1 = 8M
˜xEk
1 −8M
˜xEk
Edel

1 ≥

1 −ϵ
8

8M
˜xEk
1
≥

1 −ϵ
8
 
1 −ϵ
2

MCM(Ek) ≥(1 −ϵ) MCM(Ek \ Edel),
which combined with feasibility of 8M ˜xEk (and hence, feasibility of any restriction) implies that
8M ˜xEk
Ek\Edel is always an ϵ-approximate matching of current graph.
Multiplicative matching decrease. When the algorithm terminates after K + 1 loops, using the
termination condition, and the guarantee of Mest, we have
MCM(EK+1) ≤2Mest ≤1
2M ≤1
2MCM(E0).
Hence, the MCM value decreases by a factor of at least 2.
Iteration bound. We next show that the algorithm will stop after K + 1 loops (from Line 9

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
686
to Line 10) for bounded K. For some loop 0 ≤k ≤K −1, letting Ek+1 = Ek \ Edel,
fM,Ek+1
 xEk+1
−fM,Ek
 xEk (i)
≥βV H
xEk
 xEk+1
(ii)
≥β
X
i∈Edel
 [xEk+1]i log[xEk+1]i −[xEk]i log[xEk]i −
 1 + log[xEk]i

·
 [xEk+1]i −[xEk]i

(iii)
= β
X
i∈Edel
[xEk]i
(iv)
≥β
ˆxEk
Edel

1 −
ˆxEk −xEk
1

(v)
≥β
˜xEk
Edel

1 −
ˆxEk −xEk
1

,
(E.4)
where we used (i) the assumption (6.9), (ii) convexity of the scalar function c log c allowing us to
restrict the divergence to a subset, (iii) the property that [xEk+1]i = 0 on all deleted edges by Item 2
in Deﬁnition 23, (iv) the triangle inequality, and (v) the monotonicity property (6.12b).
We now proceed to bound ∥˜xEk
Edel∥1 and
ˆxEk −xEk
1. By the termination condition on Line 9,
∥˜xEk
Edel∥1 ≥ϵ
8
˜xEk
1. Moreover,
ˆxEk −xEk
1 ≤
ϵ
1000 ≤ϵ
16
˜xEk
1 ,
where we used the ℓ1 bound in (6.11b) and the lower bound on
˜xEk
1 implied by (E.3).
Plugging these two bounds back in (E.4), and once again using (E.3), we thus have
fM,Ek+1
 xEk+1
−fM,Ek
 xEk
≥βϵ
16
˜xEk
1 = Ω(βϵ).
At the beginning of the algorithm, recall by (1+ 1
8)MCM(E0) ≥−νEk = −fM,E0(xE0) due to Deﬁni-
tion 23 we have −fM,E0(xE0) ≤2.25M. Similarly, due to Deﬁnition 23 we also have for EK, it holds
that −νEK ≥(1 −1
8)MCM(EK) ≥
7
32M. Thus, this shows K ≤O

M
βϵ

and thus the algorithm
must terminate within O( M
βϵ) loops. This implies the runtime bound after combining with the cost
of the greedy approximation.
E.1.2
DDBM via regularized box-simplex games
Throughout this section, let ϵ ∈(0, 1), and assume that the DDBM problem is initialized with
bipartite G = (V, E0) with unsigned incidence matrix B ∈{0, 1}E×V ; we denote n := |V | and
m := |E0|.
For E ⊆E0 and M, an 8-approximation to MCM(E), we consider the following

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
687
regularized box-simplex game approximating the matching problem:
min
(x,ξ)∈∆E+1
max
y∈[0,1]V fM,E(x, ξ, y) := −1⊤
E(8Mx) −y⊤ 8MB⊤x −1

+ γxH(x, ξ) + γy  y2⊤B⊤x,
where γx =
ϵM
256 log(m), γy = ϵM
256.
(E.5)
We recall a rounding procedure from [39]; we restate its guarantees below for completeness.
Lemma 266 (Rounding). Given bipartite (V, E), RemoveOverﬂow (Algorithm 4, [39]) takes ℓ∈
RE
≥0, runs in O(|E|) time, and outputs ˜ℓsatisfying ˜ℓ≤ℓentrywise, B⊤˜ℓ≤1, and
˜ℓ

1 ≥∥ℓ∥1 −
X
i∈V
 [B⊤ℓ−1]i

+ .
Next, we show that the set of fM,E(x) = minξ|(x,ξ)∈∆E+1 maxy∈[0,1]V fM,E(x, ξ, y) forms a family
of (ϵ, γx)-CROs.
Lemma 267 (CROs from regularized box-simplex games). Deﬁne (overloading notation)
fM,E(x) :=
min
ξ|(x,ξ)∈∆E+1
max
y∈[0,1]V fM,E(x, ξ, y).
Then, the family of fM,E(x) is a family of (ϵ, γx)-CROs.
Proof. We prove each property in turn. Item 2 (restricting coordinates to zero) is immediate from
the problem deﬁnition, so we focus on showing the other three.
Multiplicative value approximation (Item 1). By assumption, M is an 8-multiplicative approx-
imation to MCM(E), so there is a maximum matching 8MxE
⋆such that
xE
⋆

1 ≤1. For such xE
⋆
and ξ := 1 −
xE
⋆

1, and because the entropic and quadratic regularization terms are nonpositive,
νE ≤
max
y∈[0,1]V fM,E(xE
⋆, ξ, y)
≤
max
y∈[0,1]V −1⊤
E(8MxE
⋆) −y⊤ 8MB⊤xE
⋆−1

= −MCM(E).
The last equality holds because we assumed 8MxE
⋆was a maximum matching. For the other direc-
tion, we proceed by contradiction. Suppose we have a feasible (x, ξ) with
max
y
fM,E(x, ξ, y) < −

1 + ϵ
4

MCM(E).

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
688
Applying RemoveOverﬂow on the approximate matching ℓ:= 8Mx yields ˜ℓ:= 8M ˜x such that
−MCM(E) ≤−∥8M ˜x∥1 ≤−∥8Mx∥1 +
8MB⊤x −1

1
(i)
≤
max
y∈[0,1]V fM,E(x, ξ, y) + ϵM
128
(ii)
< −

1 + ϵ
4

MCM(E) + ϵ
16MCM(E) < −MCM(E),
where we used (i) the deﬁnition of fM,E and bounds on γx, γy leading to an additive range of at most
ϵM
128, and (ii) that M ≤8MCM(E) by assumption, leading to a contradiction. Hence, combining
these bounds yields Item 1.
Relative strong convexity (Item 3). For simplicity, we drop the subscript M. Let (xE, ξE, yE)
and (xE′, ξE′, yE′) be the unique optimal solutions of fE and fE′ respectively. By the ﬁrst order
optimality condition of (xE, ξE), we have
fE′(xE′, ξE′, yE) −fE(xE, ξE, yE) ≥
Z 1
0
(1 −α)v⊤∇2
(x,ξ)(x,ξ)f(xα, ξα, yE)vdα,
where we let v := (xE′ −xE, ξE′ −ξE), xα := αxE′ + (1 −α)xE, and ξα := αξE′ + (1 −α)ξE. By
joint convexity of f as a function of (x, ξ) (with y ﬁxed),
Z 1
0
(1 −α)v⊤∇2
(x,ξ)(x,ξ)f(xα, ξα, yE)vdα ≥
Z 1
0
(1 −α)v⊤∇2
xxf(xα, ξα, yE)vdα
= γxV H
xE(xE′).
Here we let ∇2
xxf zero out blocks of ∇2
(x,ξ)(x,ξ)f appropriately. The conclusion (6.9) follows since
fE′(xE′, ξE′, yE′) ≥fE′(xE′, ξE′, yE) by optimality of yE′.
Additive approximation (Item 4). It suﬃces to note that for any feasible matching 8Mx, the
value of fM,E without the added entropic and quadratic regularization terms is exactly −8M ∥xE∥1,
since 8MB⊤x ≤1 entrywise. Using standard bounds on the ranges of H(x, ξ) ∈[−log(m + 1), 0),
 y2⊤B⊤x ∈[0, 1], we have the additive approximation (6.10) due to the values of γx and γy.
We now apply our high-accuracy box-simplex solver RegularizedBS (Theorem 41), together with
our rounding procedure RemoveOverﬂow, to develop a (ϵ, eO(mϵ−1))-canonical solver for the CRO
family induced by the regularized objectives (E.5).
Lemma 268 (Canonical solver for regularized box-simplex games). For ϵ = Ω(m−3), there is an
(ϵ, T )-canonical solver for the family of fM,E deﬁned in Lemma 267, using RegularizedBS (Algo-
rithm 23) as Solve and RemoveOverﬂow as Round, achieving T = O
 mϵ−1 log3 n

.
Proof. We deﬁne xE as in (6.8). Solve applies the procedure in Theorem 41 to solve
1
16M fM,E to

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
689
additive accuracy O(
ϵ3
log m) in the stated runtime, for a suﬃciently small constant. By the mul-
tiplicative approximation guarantee on νE and MCM(E) in Deﬁnition 23, and since MCM(E) is
8-approximated by M, we obtain the guarantee (6.11a). The ℓ1 bound (6.11b) then follows since
the objective is Ω(
ϵ
log m)-strongly convex in the x coordinates.
To show that RemoveOverﬂow is a valid choice of Round, the monotonicity and feasibility con-
ditions follow immediately from Lemma 266. Moreover, for ℓ←8M ˆxE, the deﬁnition of fM,E and
the guarantees of Lemma 266 imply
˜ℓ

1 ≥∥ℓ∥1 −
X
i∈V
 
B⊤ℓ−1

+
=
min
y∈[0,1]V 1⊤
E(8M ˆxE) −y⊤ 8MB⊤ˆxE −1

≥−νE −O
 ϵ3M
log m

−ϵM
128.
Combined with our approximation guarantees between νE, MCM(E), and M via Items 1 and 4
of Deﬁnition 23, this implies (6.12a), showing all required properties of RemoveOverﬂow.
Theorem 38. Let G = (V, E) be bipartite and let ϵ ∈[Ω(m−3), 1).
There is a deterministic
algorithm for the DDBM problem which maintains an ϵ-approximate matching, based on solving
regularized box-simplex games, running in time O
 mϵ−3 log5 n

.
Proof. It suﬃces to combine Lemma 267, Lemma 268, and Corollary 17.
E.1.3
DDBM via matrix scaling and box-constrained Newton's method
We follow the same notational conventions as in Appendix E.1.2, and further assume |L| ≤|R|
without loss of generality. Moreover, assume for simplicity that both L and R have at least one
vertex which has no adjacent edges, denoted v⋆
L and v⋆
R; this clearly will not aﬀect any matching sizes
if we add such vertices. We will further expand the graph G = (V, E) to a new graph eG = (eV , eE),
with bipartition eV = eL ∪eR. In particular, eL consists of the original left vertices L, a new dummy
vertex set L0 such that |L| + |L0| = |R|, and an extra dummy vertex vdum
L
. Similarly, ˜R consists of
the original right vertices R and a new dummy vertex vdum
R
. The new edge set is
eE := E ∪
 
[
v∈L∪L0
(v, vdum
R
)
!
∪
 [
v′∈R
(v′, vdum
L
)
!
∪
 vdum
L
, vdum
R

.
We deﬁne eB ∈{0, 1} e
E×eV such that eBe,v = 1 whenever v ∈e.
To instantiate our matrix
scaling solver for approximating Sinkhorn distance in Appendix E.3.2, we construct a demand vector
d ∈ReV
≥0 and a cost vector c ∈R e
E as follows.
1. Set dv = 1 for all v ∈L ∪L0 ∪R.
2. Set dvdum
L
= dvdum
R
= |R|.

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
690
3. Set ce = −1 for all e ∈E, and ce = 0 for all eE \ E.
For a vector in R e
E
≥0, we refer to its restrictions to the sets E and eE\E by x and xdum when clear from
context. It is clear that any matching 2|R|x ∈RE
≥0 on the original graph (V, E) can be extended to
a matching
2|R|xtot = 2|R|

x
xdum

∈R
e
E
≥0
such that ∥xtot∥1 = 1, and 2|R|eB⊤xtot = d, by placing additional ﬂow on the edges in eE \ E.
Speciﬁcally, we will extend any ﬂow on (v, v′) ∈E to put the same amount of ﬂow on (v′, vdum
L
),
(v, vdum
R
), and (vdum
L
, vdum
R
), and then for any additional demand not routed to a vertex v ∈L∪L0 ∪R
by the original edges E, we route it arbitrarily over the additional edges eE (similarly extending this
ﬂow to vdum
L
and vdum
R
). This is always feasible, as we can route to v⋆
L and v⋆
R. The total ℓ1 norm of d is
4|R| by construction, and every unit of xtot contributes two units of demand, thus 2|R| ∥xtot∥= 2|R|
and rearrangement gives the ℓ1 guarantee. We then consider the Sinkhorn distance objective
min
xtot=(
x
xdum )∈R e
E
≥0|eB⊤(2|R|xtot)=d
f sink
M,E(xtot) := c⊤ 2|R|xtot
+ γH
 xtot
where γ = Θ

ϵM
log(m)

.
(E.6)
We show such objectives also form a family of CROs.
Lemma 269 (CROs from Sinkhorn distances). Deﬁne (overloading notation)
f sink
M,E(x) =
min
xdum∈RE\E0
≥0
f sink
M,E(x, xdum).
Then, the family of f sink
M,E(x) is a family of (ϵ, γ)-CROs.
Proof. We focus on proving Item 1; Item 2 is immediate from the problem deﬁnition, and Items 3
and 4 follow analogously to the proof in Lemma 267.
Multiplicative value approximation (Item 1). By construction, any optimal matching 2|R|x⋆on
the edge set E can be extended to xtot
⋆
such that ∥xtot
⋆∥1 = 1 and eB⊤(2|R|xtot) = d. By nonpositivity
of entropy and the deﬁnition of c, we obtain
νE ≤f sink
M,E(xE
⋆) = −MCM(E).
For the other direction, we proceed by contradiction. Suppose we have a feasible xtot with x := xtot
E ,
so that f sink
M,E(xtot) < −
 1 + ϵ
4

MCM(E). This implies 2|R|x is a feasible matching, and then an
analogous argument to the one used in Lemma 267 yields the contradiction.
We next construct a canonical solver for the family of f sink
M,E, which makes use of the matrix
scaling solver in Proposition 63 of Appendix E.3.2.

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
691
Lemma 270 (Canonical solver for Sinkhorn objectives). For ϵ = Ω(m−3), there is an (ϵ, T )-
canonical solver for the family of f sink
M,E deﬁned in Lemma (269), using the matrix scaling solver from
[145] as Solve and truncation to E as Round, running in time eO(n2/ϵ).
Proof. Let r be the restriction of
d
2|R| to the vertices in R ∪{vdum
R
}, and let ℓbe the restriction
of
d
2|R| to the vertices in L ∪L0 ∪{vdum
L
}. Finally, let K be the (|R| + 1) × (|R| + 1) matrix with
Kij = exp(−2|R|
γ cij) for all indices corresponding to (i, j) ∈E, and Kij = 0 otherwise. Lemma 2 of
[151] shows that solving the (r, c)-matrix scaling problem on K computes the minimizer to (E.6).
We next apply Proposition 63, similarly to the proof of Theorem 86. Clearly sK ≤m, and by
Lemma 10 of [82], we may bound B by
O

log(n) + |R|
γ

= eO
 |R|
ϵM

= eO
 n2
ϵm

.
Here, we used that M = Ω( m
n ) which follows from the fact that the minimum vertex cover size
is the same as the maximum matching size. If there was a vertex cover of size o( m
n ), this would
be a contradiction since each vertex can cover at most n −1 edges.
The runtime then follows
from Proposition 63, where the required accuracy to satisfy (6.11a) and (6.11b) is a polynomial
in problem parameters, following the proofs of Theorem 86 and Lemma 268 (see Appendix E.3.2
for more details). The requirements of Round are clear from inspection and the additive range of
entropy.
Theorem 39. Let G = (V, E) be bipartite and ϵ ∈[Ω(m−3), 1). There is a randomized algorithm for
the DDBM problem which maintains an ϵ-approximate matching with probability 1 −n−Ω(1), based
on matrix scaling solver of [145], running in time eO
 n2ϵ−3
.
Proof. It suﬃces to combine Lemma 269, Lemma 270, and Corollary 17.
E.1.4
DDBM via Sinkhorn objective solver in [124]
In this section we give an alternative m1+o(1)ϵ−2-time algorithm for decremental bipartite matching,
leveraging a recent breakthrough of a m1+o(1)-time algorithm for general graph ﬂow problems in
[124]. We follow the same notational conventions as in previous sections.
Our approach is to simply leverage the recent algorithm of [124] to solve the regularized sub-
problems (E.6) from the previous section. We recall the following result from [124].
Proposition 62 (Theorem 10.16 from [124]). Given a graph G = (V, E), demands d ∈RV , costs
c ∈RE, and weights w ∈RE
≥0, all entrywise bounded by exp(logO(1) m), let B be its unsigned
adjacency matrix, and h(f) = P
e∈E cefe + wefe log fe. Then in m1+o(1) time we can ﬁnd a ﬂow f
with B⊤f = d, f ≥0, and for any constant C > 0
h(f) ≤
min
B⊤f ⋆=d,f ⋆≥0 h(f ⋆) + m−C.

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
692
Lemma 271 (Canonical solver for Sinkhorn objectives). For ϵ = Ω(m−3), there is an (ϵ, T )-
canonical solver for the family of f sink
M,E deﬁned in Lemma 269, using Proposition 62 as Solve and
truncation to E as Round, running in m1+o(1) time.
Proof. To implement Solve, we apply Proposition 62 to the problem (E.6) and solve the subproblem
to accuracy O( ϵ3
m), for a suﬃciently small constant. We observe that (E.6) is a problem exactly of
the form where Proposition 62 applies: by the multiplicative guarantee on νE and MCM(E) and
since MCM(E) is 8-approximated by M, (6.11a) holds immediately. We additionally obtain (6.11b),
as the function f sink
M,E is Ω(
ϵ
log m)-strongly convex in the coordinates of x. The requirements of Round
are clear by inspection and by the additive range of entropy, as shown in Lemma 270.
With this result, we follow Theorem 39 and obtain an improved runtime for DDBM.
Theorem 40. Let G = (V, E) be bipartite and ϵ ∈[Ω(m−3), 1). There is a randomized algorithm for
the DDBM problem which maintains an ϵ-approximate matching with probability 1 −n−Ω(1), based
on the Sinkhorn objective solver of [124], running in time m1+o(1)ϵ−2.
Proof. It suﬃces to combine Lemma 269, Lemma 271, and Corollary 17.
E.2
Proofs for Section 6.4
We ﬁrst remark on the assumptions made throughout Section 6.4, restated as below: For some δ > 0,
we have
1. upper bounds on entries: ∥A∥∞≤1, ∥b∥∞≤Bmax, ∥c∥∞≤Cmax. For simplicity, we assume
Bmax ≥Cmax ≥1, as otherwise we set Cmax ←max(1, Cmax) and Bmax ←max(Cmax, Bmax).
2. lower bounds on matrix column entries: maxi |Aij| ≥δ for every j ∈[n].
The second assumption can be satisﬁed by padding entries of A by δ, which at most incurs
O(δ) ≪O(ϵ) error in the objective value for any δ ≪ϵ. Our runtimes extend to general A in a
scale-invariant way (i.e. by scaling the whole problem by a factor of
1
∥A∥∞and then scaling up the
resulting error), so the ﬁrst assumption (on ∥A∥∞) is for simplicity. All of our runtimes depend
logarithmically on the quantity Bmax, which is polynomially bounded in all our applications.
Finally, regarding Cmax, it is clear we can assume without loss of generality that c ≤1 entrywise,
since shifting c by a multiple of the all-ones vector changes the objective value in (6.16). In the
unregularized case, i.e. fµ,ϵ with µ = ϵ = 0, [39] shows that we can also lower bound the vector
c entrywise by −1. In the regularized case, we provide the following lemma which shows that by
truncating the entries of c, we do not lose too much in objective value (see Appendix E.2).

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
693
Lemma 272. Let τ ≥0, and deﬁne Sτ = {i ∈[m] | ci ≥mini∈[m] ci + τ} to be the large entries of
c. Deﬁne X ′ := {x ∈∆m | xi = 0 for all i ∈Sτ}. Then,
min
x∈X ′ max
y∈[0,1]n fµ,ϵ(x, y) ≤min
x∈∆m max
y∈[0,1]n fµ,ϵ(x, y) + µm exp

−τ −3
µ

.
In other words, Lemma 272 shows that if our goal is to ﬁnd an x ∈∆m which approximately
minimizes maxy∈[0,1]n fµ,ϵ(x, y), we may restrict ourselves to considering only x supported on co-
ordinates where c is polylogarithmically bounded, without much loss in the error. In particular,
setting τ = Θ(log m
σ ) above, where the ﬁnal desired error is σ, yields this claim for µ ≤1. Our
methods will have runtimes depending linearly on Cmax, which upon applying the preprocessing of
Lemma 272, is an overall polylogarithmic dependence on the ﬁnal target accuracy.
Now we prove Lemma 272. Before we do so, we state two simple helper lemmas used in its proof.
Lemma 273. Let X, Y be compact and convex, let X ′ ⊆X also be compact and convex, and let
f : X × Y →R be convex-concave. Suppose for some ∆> 0 it is the case that for all y ∈Y,
min
x∈X ′ f(x, y) ≤min
x∈X f(x, y) + ∆.
Then,
min
x∈X ′ max
y∈Y f(x, y) ≤min
x∈X max
y∈Y f(x, y) + ∆.
Proof. Let (x⋆, y⋆) be the optimizer to f over X × Y, and let (x′
⋆, y′
⋆) be the optimizer to f over
X ′ × Y. We conclude with the following sequence of inequalities:
f(x′
⋆, y′
⋆) ≤min
x∈X f(x, y′
⋆) + ∆≤f(x⋆, y′
⋆) + ∆≤f(x⋆, y⋆) + ∆.
The three inequalities respectively used our assumption, x⋆∈X, and y⋆= argmaxy∈Yf(x⋆, y).
Lemma 274. For µ > 0, deﬁne the function sminµ : Rm →R by
sminµ(v) := −µ log

X
i∈[m]
exp

−1
µvi

.
Fix v, and let S ⊆[m] be a set such that for all i ∈[m] \ S, we have vi ≥mini∈[m] vi + T for some
T > 0. Further let v′ ∈RS be the restriction of v to the set S. Then,
sminµ (v′) := −µ log
 X
i∈S
exp

−1
µv′
i
!
≤sminµ(v) + µm exp

−T
µ

.

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
694
Proof. Let vmin := mini∈[m] vi. Note that
X
i∈[m]\S
exp

−1
µvi

≤m exp

−1
µ(vmin + T)

≤m exp

−T
µ
 X
i∈S
exp

−1
µvi

.
Here we used that some element in S achieves vi = vmin. Hence,
X
i∈[m]
exp

−1
µvi

≤

1 + m exp

−T
µ
 X
i∈S
exp

−1
µvi

.
Taking logarithms and scaling by −µ yields the claim, where we use log(1 + c) ≤c for c ≥0.
Combining these lemmas allow us to prove Lemma 272 formally.
Proof of Lemma 272. By Lemma 273, it suﬃces to ﬁrst ﬁx y ∈[0, 1]n, and prove that
min
x∈X ′ fµ,ϵ(x, y) ≤min
x∈∆m fµ,ϵ(x, y) + µm exp

−τ −3
µ

.
(E.7)
Clearly the term b⊤y cancels from both sides. Next, deﬁne
v := Ay + c −ϵ
2|A|(y2).
Let v′ be the restriction of v to the indices in Sτ. By explicitly minimizing over x ∈∆m and x ∈X ′
respectively, (E.7) is equivalent to proving
sminµ(v′) ≤sminµ(v) + µm exp

−τ −3
µ

.
Since Ay −ϵ
2|A|(y2) has a range of at most 3, every truncated entry of v′ must be at least (τ −3)
larger than the smallest entry of v′. We conclude by applying Lemma 274 with T = τ −3.
E.2.1
Proofs for Section 6.4.1
Proposition 19 (Convergence of Algorithm 21). Given regularizer r with range at most Θ, suppose
g is ν-strongly-monotone with respect to r (see (6.19)), and is α-relatively-Lipschitz with respect to
r (see (6.20)). Let zK be the output of Algorithm 21. Then, V r
zK(z⋆) ≤

α
ν+α
K
Θ + ε
ν .
Proof. Fix an iteration k. We have
νVzk−1/2(z⋆)
(i)
≤

g(zk−1/2) −g(z⋆), zk−1/2 −z⋆ (ii)
≤

g(zk−1/2), zk−1/2 −z⋆
=

g(zk−1/2), zk −z⋆
+

g(zk−1/2), zk−1/2 −zk

,
(E.8)

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
695
where we used (i) the deﬁnition of ν-strong monotonicity, and (ii) optimality of z⋆.
Next,

g(zk−1/2), zk −z⋆
−ε
2
(i)
≤−⟨α∇Vzk−1(zk) + ν∇Vzk−1/2(zk), zk −z⋆⟩
(ii)
≤ν

Vzk−1/2(z⋆) −Vzk(z⋆) −Vzk−1/2(zk)

(E.9)
+ α

Vzk−1(z⋆) −Vzk(z⋆) −Vzk−1(zk)

≤αVzk−1(z⋆) −(ν + α)Vzk(z⋆) + νVzk−1/2(z⋆) −αVzk−1(zk),
(E.10)
and similarly,

g(zk−1/2), zk−1/2 −zk

−ε
2
(i)
≤−⟨α∇Vzk−1(zk−1/2), zk−1/2 −zk⟩+

g(zk−1/2) −g(zk−1)), zk−1/2 −zk

(ii)
≤α

Vzk−1(zk) −Vzk−1/2(zk) −Vzk−1(zk−1/2)

+

g(zk−1/2) −g(zk−1)), zk−1/2 −zk

(iii)
≤αVzk−1(zk),
(E.11)
where we used (i) the approximate proximal oracle optimality conditions for zk and zk−1/2, (ii) the
three-point property of Bregman divergence (6.7), and (iii) the relative Lipschitzness conddition of
g. In the last inequality, we also use nonnegativity of divergence term V .
Now, combining (E.10) and (E.11) with (E.8) yields
νVzk−1/2(z⋆) ≤αVzk−1(z⋆) −(ν + α)Vzk(z⋆) + νVzk−1/2(z⋆) + ε.
Rearranging terms then yields
Vzk(z⋆) ≤
α
ν + αVzk−1(z⋆) +
ε
ν + α.
Applying this bound recursively K times and using that Vz0(u) ≤r(u) −r(z0) ≤Θ for z0 the
minimizer of r, we have
VzK(z⋆) ≤

α
ν + α
K
Θ +
K−1
X
k=0

α
ν + α
k 
ε
ν + α

≤

α
ν + α
K
Θ + ε
ν .
Corollary 18 (Convergence of Algorithm 22). Let δ, ε ∈(0, 1), ρ ≥1.
Suppose we are given

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
696
γ ∈Z∗= X∗× Y∗with max(∥γx∥∞, ∥γy∥1) ≤B, and deﬁne the proximal subproblem solution
xOPT, yOPT = argminx∈∆margminy∈[0,1]nf(x, y) := ⟨γx, x⟩+ ⟨γy, y⟩+ θr(x, y) for some θ > 0.
If the Hessian condition in Lemma 81 holds with a constant κ > 0, and all simplex iterates x of
Algorithm 22 satisfy x ≥δ elementwise, then the algorithm ﬁnds a ε
2-approximate solution to the
proximal oracle within T = O

log

ρ(B+mnθ)2
δεθ

iterations.
To prove Corollary 18, we ﬁrst provide technical lemma that shows how to transfer the accuracy
desired for transferring from function error in Algorithm 22 guarantees to the duality gap error
desired by an approximate extragradient step in Algorithm 21, in a controllable sense.
Lemma 275. Let δ, ε ∈(0, 1), ρ ≥1.
Suppose we are given γ ∈Z∗= X∗× Y∗satisfying
max(∥γx∥∞, ∥γy∥1) ≤B, and deﬁne the proximal oracle subproblem solution
xOPT, yOPT = argminx∈∆margminy∈[0,1]nf(x, y) := ⟨γx, x⟩+ ⟨γy, y⟩+ θr(x, y) for some θ > 0.
Let z = (x, y) be an approximate solution satisfying f(x, y) ≤f(xOPT, yOPT) +
ε2δ2θ
64ρ(8nθ+B)2 , and
suppose x, xOPT ≥δ holds elementwise. For any w ∈Z = X × Y,
⟨γ + θ∇r(z), z −w⟩≤ε
2.
Proof. Let zOPT = (xOPT, yOPT). Under the assumption that f(z) −f(zOPT) ≤A :=
ε2δ2θ
64ρ(8nθ+B)2 ,
using the lower bound in (6.23) with |A|⊤x ≥δ2 entrywise by the assumption maxi |Aij| ≥δ for
every column j, and since entropy is strongly convex in the ℓ1 norm, we have
θρ
4 ∥x −xOPT∥2
1 + δ2θ
2ρ ∥y −yOPT∥2
∞≤f(z) −f(zOPT) ≤A
=⇒
∥x −xOPT∥1 ≤
ε
4

B + 8nρθ
δ
 and ∥y −yOPT∥∞≤
ε
4

B + 8nθ
ρ
.
(E.12)
For w = (u, v), by deﬁnition of zOPT, ⟨γ + θ∇r(zOPT), zOPT −w⟩≤0. Hence, we bound
⟨γ + θ∇r(z), z −w⟩≤⟨γ + θ∇r(z), z −w⟩−⟨γ + θ∇r(zOPT), zOPT −w⟩
= ⟨γ, z −zOPT⟩+ θ ⟨∇r(z) −∇r(zOPT), zOPT −w⟩+ θ ⟨∇r(z), z −zOPT⟩.
(E.13)

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
697
We now bound the terms on the right-hand side of (E.13):
⟨γ, z −zOPT⟩≤∥γx∥∞∥x −xOPT∥1 + ∥γy∥1∥y −yOPT∥∞
≤B ∥x −xOPT∥1 + B ∥y −yOPT∥∞,
⟨∇xr(z) −∇xr(zOPT), xOPT −u⟩≤∥xOPT −u∥1 ∥∇xr(z) −∇xr(zOPT)∥∞
≤2ρ
log
x
xOPT

∞
+ 2
ρ ∥|A|((y + yOPT) ◦(y −yOPT))∥∞
≤2ρ
δ ∥x −xOPT∥1 + 4
ρ ∥y −yOPT∥∞,
⟨∇yr(z) −∇yr(zOPT), yOPT −v⟩≤∥yOPT −v∥∞∥∇yr(z) −∇yr(zOPT)∥1
≤2n
ρ ∥y −yOPT∥∞+ 2n
ρ ∥x −xOPT∥1 ,
⟨∇r(z), z −zOPT⟩≤∥∇xr(z)∥∞∥x −xOPT∥1 + ∥∇yr(z)∥1∥y −yOPT∥∞
≤

ρ

1 + log
1
δ

+ 1
ρ

∥x −xOPT∥1 + 2
ρ ∥y −yOPT∥∞.
(E.14)
For the second bound, we used log(1 + c) ≤c for whichever of c =
xj
[xOPT]j or c = [xOPT]j
xj
is larger, and
the entrywise lower bounds on x and xOPT. For the third bound, we used
diag (yOPT) |A|⊤xOPT −diag (y) |A|⊤x

1 ≤
X
i∈[m],j∈[n]
|A|ij
[xOPT]i [yOPT]j −xiyj

≤
X
i∈[m],j∈[n]
|A|ij
[yOPT]j −yj
 + |[xOPT]i −xi|

≤n(∥yOPT −y∥∞+ ∥xOPT −x∥1).
Plugging the bounds of (E.14) back in (E.13), we can thus conclude that for ρ ≥1, δ ≤1
⟨γ + θ∇r(z), z −w⟩≤

B + 8nρθ
δ

∥x −xOPT∥1 +

B + 8nθ
ρ

∥y −yOPT∥∞≤ε
2,
where for the last inequality we use conditions in (E.12).
We use this lemma to prove Corollary 18 formally below.
Proof of Corollary 18. We ﬁrst bound f(x(1), y(0)) −f(xOPT, yOPT) ≤maxz∈Z f(z) −f(zOPT) ≤
2 ∥γx∥∞+ 2 ∥γy∥1 + θ

ρ log m + 2
ρ

≤4B + θ

ρ log m + 2
ρ

where the last inequality uses ℓ1-ℓ∞
H¨older, the domain deﬁnitions X = ∆m, Y = [0, 1]n, and the deﬁnition of r = rµ,ϵ as in (6.18).

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
698
Now applying Lemma 81, after O

log

ρ(B+mnθ)2
δεθ

iterations for a suﬃciently large constant,
f(x(T +1), y(T )) −f(xOPT, yOPT) ≤

1 −1
2κ
T 
f(x(1), y(0)) −f(xOPT, yOPT)

≤

1 −1
2κ
T 
4B + θ

ρ log m + 2
ρ

≤
ε2δ2θ
64ρ(8nθ + B)2 ,
which by Lemma 275 in turn implies it implements an approximate proximal oracle to ε
2 accuracy.
E.2.2
Proofs for Section 6.4.2
We give a helpful variant of the Cauchy-Schwarz inequality in appropriate "local norms" and a con-
sequence about approximating the Hessian of our regularizer. These ﬁnd uses in proving Lemma 86.
Lemma 82 (Bounds on regularizer). Suppose A ∈Rm×n has ∥A∥∞≤1. For any z = (x, y) ∈
∆m × [0, 1]n, r = rµ,ϵ deﬁned as in (6.18), and ¯x ∈Rm
>0, ⟨x, Ay⟩≤∥x∥diag( 1
¯x) ∥y∥diag(|A|⊤¯x) .
Further, if ρ ≥3, the matrix
D(x) :=


ρ
2diag
  1
x

0
0
1
ρdiag

|A|⊤x



(6.22)
satisﬁes the following relationship with the Hessian matrix of r(z):
D(x) ⪯∇2r(z) =

ρ · diag
  1
x

2
ρAdiag (y)
2
ρdiag (y) A⊤
2
ρdiag

|A|⊤x


⪯4D(x).
(6.23)
Proof. For the ﬁrst property, it suﬃces to square both sides and use Cauchy-Schwarz:

X
i∈[m]
X
j∈[n]
|Aijuivj|


2
≤

X
i∈[m]
X
j∈[n]
|Aij|
[¯x]i
u2
i



X
i∈[m]
X
j∈[n]
|Aij| [¯x]iv2
j


≤

X
i∈[m]
∥A∥∞
[¯x]i
u2
i



X
j∈[n]
h
|A|⊤¯x
i
j v2
j


≤∥u∥2
diag( 1
¯x) ∥v∥2
diag(|A|⊤¯x) .

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
699
Next, given any w = (u, v), we have
w⊤∇2r(x, y)w = ρ ∥u∥2
diag( 1
x) + 2
ρ ∥v∥2
diag(|A|⊤x) + 4
ρu⊤Adiag (y) v
≤ρ ∥u∥2
diag( 1
x) + 2
ρ ∥v∥2
diag(|A|⊤x) + 4
ρ ∥u∥diag( 1
x) ∥diag (y) v∥diag(|A|⊤x)
≤

ρ + 2
ρ

∥u∥2
diag( 1
x) + 4
ρ ∥v∥2
diag(|A|⊤x)
≤4
ρ
2 ∥u∥2
diag( 1
x) + 1
ρ ∥v∥2
diag(|A|⊤x)

= 4w⊤D(x)w.
Similarly, on the other side we have
w⊤∇2r(x, y)w = ρ ∥u∥2
diag( 1
x) + 2
ρ ∥v∥2
diag(|A|⊤x) + 4
ρu⊤Adiag (y) v
≥ρ ∥u∥2
diag( 1
x) + 2
ρ ∥v∥2
diag(|A|⊤x) −4
ρ ∥u∥diag( 1
x) ∥diag (y) v∥diag(|A|⊤x)
≥

ρ −4
ρ

∥u∥2
diag( 1
x) +
2
ρ −1
ρ

∥v∥2
diag(|A|⊤x)
≥ρ
2 ∥u∥2
diag( 1
x) + 1
ρ ∥v∥2
diag(|A|⊤x) = w⊤D(x)w.
Lemma 83 (Error of padding, cf. Lemma 6, [112]). For δ > 0 and ¯z = (¯x, y) ∈∆m × [0, 1]n
let z = (x, y) ∈∆m × [0, 1]n where x = Oδ(¯x) (Deﬁnition 26), then for r in (6.18), and any
w ∈Z = ∆m × [0, 1]n, V r
z (w) −V r
¯z (w) ≤

ρ + 8
ρ

mδ.
Proof. We write r(x, y) = ρH(x) + 1
ρQ(x, y) where
H(x) =
X
i∈[m]
xi log xi, Q(x, y) =
 y2⊤|A|⊤x,
and bound the Bregman divergences induced by H and Q respectively.
First note that by deﬁnition of the padding oracle we have ∥xk −¯xk∥1 ≤∥xk −ˆxk∥1+∥¯xk −ˆxk∥1 ≤
2mδ, and that xk is a mδ-padding of ¯xk by the deﬁnition of padding in [112] (cf. Deﬁnition 2). Thus
using Lemma 6 of [112], we have
V H
z (w) −V H
¯z (w) ≤mδ.
For the quadratic part, letting w = (u, v), we have
V Q
z (w) −V Q
¯z (w) = Q(¯z) −Q(z) + ⟨∇Q(¯z), w −¯z⟩−⟨∇Q(z), w −z⟩
≤∥x −¯x∥1 +

|A|y2, x −¯x

+

diag (y) |A|⊤(¯x −x), v −y

≤4 ∥x −¯x∥1 ≤8mδ,

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
700
where the ﬁrst inequality used ¯y = y, and both the ﬁrst and second used the assumed bounds
∥y∥∞≤1, ∥A∥∞≤1. Combining these bounds proves the statement.
Lemma 84 (Iterate stability in Algorithm 22). Suppose ϵ ≤1, ρ ≥6, and α ≥36
ρ (µ log 4
δ +3Cmax).
Let (xk, yk) denote blocks of zk, the kth iterate of Algorithm 21.
In any iteration k of Algo-
rithm 21, calling Algorithm 22 to implement Line 6, if xk−1 ≥
δ
2 entrywise, x(t+1) ∈xk−1 ·

exp
 −1
9

, exp
  1
9

, for all t ∈[T]. Calling Algorithm 22 to implement Line 7, if xk−1/2 ≥
δ
4
entrywise, x(t+1) ∈x
α
α+ν
k−1 ◦x
ν
α+ν
k−1/2 ·

exp
 −1
9

, exp
  1
9

for all t ∈[T].
Proof. We ﬁrst handle the case of Line 6 in Algorithm 21. Using Algorithm 22 to implement this
step, we observe that the iterates satisfy
x(t+1) ←argminx∈∆m ⟨γx, x⟩+ θr(x, y(t)) where θ = α,
and γx = gx(xk−1, yk−1) −α∇xr(zk−1)
= Ayk−1 + c + µ(1 + log(xk−1)) −ϵ
2 |A| (y2
k−1) −αρ(1 + log xk−1) −α
ρ |A|y2
k−1,
which implies
x(t+1) ∝xk−1 ◦exp
 1
αρ

−α
ρ |A|

y(t)2
−Ayk−1 −c + ϵ
2|A|y2
k−1 + α
ρ |A|y2
k−1 −µ log(xk−1)

.
Consequently, under the given assumptions on xk−1, ρ, and α, and using ∥c∥∞≤Cmax,
log
x(t+1)
xk−1
 ≤1
αρ

1 + Cmax + α
ρ + ϵ
2 + µ log 2
δ

≤1
18
=⇒x(t+1) ∈xk−1 ·

exp

−1
9

, exp
1
9

.
Next, we handle the case of Line 7 in Algorithm 22. Here, the iterates of Algorithm 22 satisfy
x(t+1) ←argminx∈∆m ⟨γx, x⟩+ θr(x, y(t)) where θ = α + ν,
and γx = gx(xk−1/2, yk−1/2) −α∇xr(zk−1) −ν∇xr(zk−1/2)
= Ayk−1/2 + c + µ(1 + log(xk−1/2)) −ϵ
2 |A| (y2
k−1/2)
−αρ(1 + log xk−1) −α
ρ |A|y2
k−1 −νρ(1 + log xk−1/2) −ν
ρ|A|y2
k−1/2.

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
701
Hence,
x(t+1) ∝
x
α
α+ν
k−1 ◦x
ν
α+ν
k−1/2 ◦τ x
where τ x =
exp

1
(α + ν)ρ

−α
ρ |A|

y(t)2
−ν
ρ|A|

y(t)2
−Ayk−1/2 −c
+ ϵ
2|A|y2
k−1/2 + α
ρ |A|y2
k−1 + ν
ρ|A|y2
k−1/2 −µ log(xk−1/2)

.
Consequently, under a similar calculation as before,
exp

−1
18

≤τ x ≤exp
 1
18

entrywise
=⇒x(k+1) ∈x
α
α+ν
k−1 ◦x
ν
α+ν
k−1/2 ·

exp

−1
9

, exp
1
9

.
E.2.3
Proofs for Section 6.4.3
Lemma 85 (Strong monotonicity). Let µ ≥ϵ
2 and ρ :=
q
2µ
ϵ . The gradient operator gµ,ϵ (6.17) is
ν := 1
2
p µϵ
2 -strongly monotone (see (6.19)) with respect to rµ,ϵ deﬁned in (6.18).
Proof. Throughout the proof, let g := gµ,ϵ, f := fµ,ϵ, and r := rµ,ϵ for notational simplicity. In
order to show the desired bound
⟨g(w) −g(z), w −z⟩≥1
3
rµϵ
2 ⟨∇r(w) −∇r(z), w −z⟩,
(E.15)
we begin by putting (E.15) into a more convenient form. Letting J(z) be the Jacobian of g at the
point z, we have by direct integration (where zt := (1 −t)z + tw for all t ∈[0, 1])
⟨g(w) −g(z), w −z⟩=
Z 1
0
(w −z)⊤J(zt)(w −z)dt.
Since quadratic forms through a square matrix M are preserved by replacing M with its symmetric
part 1
2(M + M⊤), it suﬃces to understand the symmetric part of J(zt). The restriction of Jµ to the
xy and yx blocks is skew-symmetric, since the blocks are respectively ∇2
xyf(x, y) and −∇2
yxf(x, y).

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
702
Similarly, its restrictions to its xx and yy blocks are symmetric. Hence, we have
⟨g(w) −g(z), w −z⟩=
Z 1
0
(w −z)⊤

∇2
xxf(zt)
0
0
−∇2
yyf(zt)

(w −z)dt
=
Z 1
0
(w −z)⊤

µ · diag

1
xt

0
0
ϵ · diag

|A|⊤xt


(w −z)dt.
(E.16)
In the last line, we denoted zt := (xt, yt) for all t ∈[0, 1]. Next, to bound the right hand side of
(E.15), integrating once more yields
⟨∇r(w) −∇r(z), w −z⟩=
Z 1
0
(w −z)⊤∇2r(zt)(w −z)dt
=
Z 1
0
(w −z)⊤

ρ · diag

1
xt

2
ρ |A| diag (yt)
2
ρdiag (yt) |A|⊤
2
ρdiag

|A|⊤xt


(w −z)dt
≤
Z 1
0
(w −z)⊤

2ρ · diag

1
xt

0
0
4
ρdiag

|A|⊤xt


(w −z)dt.
(E.17)
The last line used Lemma 82 for all (xt, yt) ∈∆m ×[0, 1]n. Combining the bounds (E.16) and (E.17)
yields the conclusion.
Corollary 19 (Iterate stability in Algorithm 23). Assume the same parameter bounds as Lemma 84,
and that δ ∈(0, m−1).
In the kth outer loop of Algorithm 23, xk−1 ≥
δ
2 entrywise.
Further,
for all iterates x(t+1) computed in Line 7 to Line 8 and xOPT as deﬁned in (6.21) with θ = α,
1
2xk−1 ≤x(t+1), xOPT ≤2xk−1, and x(t+1), xOPT ≥δ
4, entrywise. Similarly, for all iterates x(t+1)
computed in Line 9 to Line 10 and xOPT as deﬁned in (6.21) with θ = α+ν, 1
2xk−1/2 ≤x(t+1), xOPT ≤
2xk−1/2 and x(t+1), xOPT ≥δ
4, entrywise.
Proof. It suﬃces to prove xk−1 ≥
δ
2 entrywise as all other conclusions are then immediate con-
sequences of Lemma 84 (the conclusions about xOPT follow from taking a limit). When k = 1,
xk−1 ≥δ
2 holds by the assumption that δ ≤
1
2m. When k > 1, xk−1 is the result of padding with
parameter δ. We note for any x ∈∆m we have the desired
1 ≤∥max (x, δ)∥1 ≤1 + δm =⇒Oδ(x) ≥
δ
∥max (x, δ)∥1
≥
δ
1 + δm ≥δ
2.
Lemma 86 (Relative Lipschitzness). Assume the same parameter bounds as in Lemma 84.
In
the kth outer loop of Algorithm 23, let ¯zk ←(x(T +1), y(T )) from Line 10 be zk before the padding

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
703
operation. Then, xk−1/2, ¯xk ∈[ 1
2xk−1, 2xk−1] elementwise and

g(zk−1/2) −g(zk−1), zk−1/2 −¯zk

≤α
 Vzk−1(zk−1/2) + Vzk−1/2(¯zk)

for α = 4 + 32
rµϵ
2 .
Proof. The conclusion that xk−1/2, ¯xk ∈[ 1
2xk−1, 2xk−1] elementwise follows from Corollary 19. Next,
we have for zβ = zk−1 + β(zk−1/2 −zk−1), and ρ ≥6,
Vzk−1(zk−1/2) =
Z 1
0
(1 −β)∥zk−1/2 −zk−1∥2
∇2r(zk−1+β(zk−1/2−zk−1))dβ
≥1
4
zk−1/2 −zk−1
2
D(xk−1) ,
following the fact that ∇2r(zk−1 + β(zk−1/2 −zk−1)) ⪰D(xk−1 + β(xk−1/2 −xk−1)) ⪰1
2D(xk−1)
from Lemma 82 and xk−1/2 ∈[ 1
2xk−1, 2xk−1]. Similarly, we also have
Vzk−1/2(¯zk) ≥1
4
zk−1/2 −¯zk
2
D(xk−1) .
On the other hand, we directly compute

g(zk−1/2) −g(zk−1), zk−1/2 −¯zk

=
*
A(yk−1/2 −yk−1), xk−1/2 −¯xk

+
*
−A⊤(xk−1/2 −xk−1), yk−1/2 −¯yk

+
*
µ log
xk−1/2
xk−1

−ϵ
2|A|(y2
k−1/2 −y2
k−1), xk−1/2 −¯xk

+
*
ϵ · diag
 yk−1/2

|A|⊤xk−1/2 −ϵ · diag (yk−1) |A|⊤xk−1, yk−1/2 −¯yk

.
(E.18)
We now bound the terms on the right-hand side of (E.18). By Lemma 82,
*
A(yk−1/2 −yk−1), xk−1/2 −¯xk

≤1
2ρ∥yk−1/2 −yk−1∥2
diag(|A|⊤xk−1) + ρ
2∥xk−1/2 −¯xk∥2
diag

1
xk−1
,
*
A⊤(xk−1/2 −xk−1), ¯yk −yk−1/2

≤1
2ρ∥yk−1/2 −¯yk∥2
diag(|A|⊤xk−1) + ρ
2∥xk−1/2 −xk−1∥2
diag

1
xk−1
.
(E.19)

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
704
For the rest of the terms we have, letting (xβ, yβ) = zβ = (1 −β)zk−1 + βzk−1/2 for all β ∈[0, 1],
*
µ log
xk−1/2
xk−1

−ϵ
2|A|(y2
k−1/2 −y2
k−1), xk−1/2 −¯xk

+
*
ϵ · diag
 yk−1/2

|A|⊤xk−1/2 −ϵ · diag (yk−1) |A|⊤xk−1, yk−1/2 −¯yk

=
Z 1
0
(zk−1/2 −zk−1)⊤

µ · diag

1
xβ

−ϵ |A| diag (yβ)
ϵdiag (yβ) |A|⊤
ϵdiag

|A|⊤xβ


(zk−1/2 −¯zk)dβ
=
rµϵ
2
Z 1
0
(zk−1/2 −zk−1)⊤

ρ · diag

1
xβ

−2
ρ |A| diag (yβ)
2
ρdiag (yβ) |A|⊤
2
ρdiag

|A|⊤xβ


(zk−1/2 −¯zk)dt
≤8
rµϵ
2

∥zk−1/2 −zk−1∥2
D(xk−1) + ∥zk−1/2 −¯zk∥2
D(xk−1)

,
(E.20)
where for the last inequality we use similar arguments as in Lemma 82 to bound the matrix by
4D(xβ) ⪯8D(xk−1) following the assumption that xk−1/2 ∈[ 1
2xk−1, 2xk−1] elementwise. Putting (E.19)
and (E.20) into (E.18), we conclude that the relative Lipschitz condition (6.20) holds with the stated
α.
Corollary 20 (Inner loop convergence in Algorithm 23). Assume the same parameter bounds as in
Lemma 84. For γ deﬁned in Line 7, suppose for an appropriate constant T = Ω

log mnBmaxαρ
δε

.
Then, for all k iterate zk−1/2 = (xk−1/2, yk−1/2) of Line 8 satisﬁes

∇g(zk−1) + α∇Vzk−1(zk−1/2), zk−1/2 −w

≤νε
4 , for all w ∈Z.
Similarly, for γ deﬁned in Line 9, iterate ¯zk = (x(T +1), y(T )) of Line 10 satisﬁes

∇g(zk−1) + α∇Vzk−1(¯zk) + ν∇Vzk−1/2(¯zk), ¯zk −w

≤νε
4 , for all w ∈Z.
Proof. We ﬁrst claim Lemma 81 holds with κ = 8. This follows from the fact that for any x′ ∈∆m,
y′ ∈[0, 1]n satyisfying x′ ≥1
2x, we have
∇2r(x′, y′) ⪰D(x′) ⪰1
2D(x) ⪰1
8∇2r(x, y) ⪰1
8∇2
yyr(x, y),
where we use both of the Hessian bounds from Lemma 82 and the fact that restrictions to blocks
only decrease a quadratic form.
For each γ = (γx, γy) deﬁned in Algorithm 23 and the fact that xk−1, xk−1/2 ≥δ
4 for all k ∈[K],

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
705
we have by the assumptions ∥A∥∞≤1, ∥c∥∞≤Cmax, and ∥b∥1 ≤nBmax that it suﬃces to take
B = max (∥γx∥∞, ∥γy∥1) = O

(αρ + µ) log 1
δ + nBmax

in Corollary 18. Hence, the stated number of iterations T suﬃces.
Proposition 20 (Convergence of Algorithm 23). Assume the same parameter bounds as in Lemma 84,
and that δ ≤
ε
4ραm. Algorithm 23 returns zK satisfying V r
zK(z⋆) ≤3ε
ν , letting (for an appropriate
constant) K = Ω( α
ν log( ν log m
ε
)).
Proof. Our proof is based on the proof of Proposition 19. Lemma 85 and Lemma 86 showed that our
operator-regularizer pair satisﬁes ν-strong monotonicity and α-relative Lipschitzness. Under these
conditions, and letting ¯zk be the unpadded version of zk, recall that the proof of Proposition 19
showed that
(ν + α)V¯zk(z⋆) ≤αVzk−1(z⋆) + ε.
Moreover, the padding guarantee of Lemma 83 shows
(ν + α) (Vzk(z⋆) −V¯zk(z⋆)) ≤(ν + α)

ρ + 8
ρ

mδ ≤ε,
by the assumption on δ. Following the proof of Proposition 19, we conclude
VzK(z⋆) ≤

α
ν + α
K
Θ + 2ε
ν .
Since Θ = O(ρ log m) for our choice of regularizer, the claim follows.
Finally, we prove the main theorem in this section on the convergence of our regularized box-
simplex solver for (6.16). We ﬁrst state two helper lemmas used to prove the theorem.
Lemma 276 (Entropy bounded by ℓ1-distance). For any x, ˜x ∈∆m, deﬁning H(x) = P
i∈[m] xi log xi,
H(˜x) −H(x) ≤33 log(m) ∥˜x −x∥1 + m−30.
Proof. We deﬁne a partition of the coordinates of ˜x,
L :=

i ∈[m] | ˜xi ≥m−32	
and S :=

i ∈[m] | ˜xi < m−32	
.
First, we have by convexity of entropy that, letting vL ∈R|L| denote the |L|-dimensional restriction

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
706
of a vector v ∈Rm to the coordinates of L ⊆[m],
X
i∈L
˜xi log ˜xi −
X
i∈L
xi log xi ≤⟨log ˜xL + 1L, ˜xL −xL⟩
≤∥log ˜xL + 1L∥∞∥˜xL −xL∥1
≤(32 log(m) + 1) ∥˜x −x∥1 ≤33 log(m) ∥˜xL −xL∥1 ,
for suﬃciently large m. In the second line, we used the ℓ1-ℓ∞H¨older's inequality. Moreover, let
S′ := {i ∈[m] | xi < m−32} be the subset of [m] where x is small. By nonpositivity of entropy,
X
i∈S
˜xi log ˜xi −
X
i∈S
xi log xi ≤−
X
i∈S
xi log xi = −
X
i∈S′
xi log xi −
X
i∈S\S′
xi log xi
≤32|S′| log(m)
m32
+ 32 log(m)
X
i∈S\S′
xi
≤32|S| log(m)
m32
+ 32 log(m)
X
i∈S\S′
 xi −m−32
≤m−30 + 32 log(m)
X
i∈S\S′
(xi −˜xi) ≤m−30 + 32 log(m) ∥˜xS −xS∥1 .
The second line used that −c log c is increasing in the range (0, m−32), and the fourth used that
m−32 > ˜xi for all i ∈S. We conclude by combining the above two displays.
Lemma 277. Let (x⋆, y⋆) be the optimizer to the regularized box-simplex game in (6.16). Suppose
ρ ≥6, µ ≤1, ε ≥m−10 and that z = (x, y) satisﬁes Vz (z⋆) ≤
ε2
C2max log4 m.
Then, x is an ε-
approximate minimizer to
f x
µ,ϵ(x, y) :=
max
y∈[0,1]n fµ,ϵ(x, y).
Proof. We ﬁrst claim that the given divergence condition implies that
∥x −x⋆∥1 ≤
ε
Cmax log2 m.
(E.21)
To see this, we observe that due to the Hessian bound in Lemma 82, the divergence induced by
r(x, y) −ρ
2H(x) is nonnegative, and hence
∥x⋆−x∥2
1 ≤2V H
x (x⋆) ≤ρ
2V H
x (x⋆) ≤V r
z (z⋆).
The ﬁrst inequality above was by Pinsker, the second used ρ ≥6, and the last followed by nonneg-
ativity of divergence. This implies (E.21) by the assumed divergence bound. We next show that

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
707
(E.21) implies that x is an approximate minimizer of f x
µ,ϵ, which we recall means
max
y′∈Y fµ,ϵ(x, y′) −fµ,ϵ(x⋆, y⋆) ≤ε.
(E.22)
To this end, we compute
max
y∈Y y⊤ A⊤x −b

−max
y∈Y y⊤ A⊤x⋆−b

≤
A⊤(x −x⋆)

1 ≤∥x −x⋆∥1 ≤ε
4,
⟨c, x⟩−⟨c, x⋆⟩≤Cmax ∥x −x⋆∥1 ≤ε
4,
µ (H(x) −H(x⋆)) ≤33µ log(m) ∥x −x⋆∥1 + µm−30 ≤ε
4,
max
y∈Y
 y2⊤|A|⊤x −max
y∈Y
 y2⊤|A|⊤x⋆≤
A⊤(x −x⋆)

1 ≤∥x −x⋆∥1 ≤ε
4.
The third line used Lemma 276. Combining these pieces proves the claim.
We are now ready to prove Theorem 41.
Theorem 41 (Regularized box-simplex solver). Given regularized box-simplex game (6.16) with
72ϵ ≤µ ≤1 and optimizer (x⋆, y⋆), and letting ε ∈(m−10, 1), RegularizedBS (Algorithm 23) returns
xK satisfying
xK −x⋆
1 ≤
ε
Cmax log2 m and maxy∈[0,1]n fµ,ϵ(xK, y) −fµ,ϵ(x⋆, y⋆) ≤ε. The total
runtime of the algorithm is O(nnz(A) · ( Cmax
√µϵ + log( m
σϵ)) · log( Cmax log m
ε
) log( mnBmax
ε
)).
Proof. We handle correctness and runtime separately.
Correctness. Let (x⋆, y⋆) be the minimax optimal solution of fµ,ϵ in (6.16). By applying Propo-
sition 20 and Lemma 277, shifting the deﬁnition of ε appropriately so that 3ε
ν ←
ε2
C2max log4 m, we have
with the choice of parameters our returned xK satisﬁes the desired bounds.
Runtime. It is immediate to see each line of Algorithm 23 runs in time O(nnz(A)). Then, the
total runtime follows from combining Proposition 20 and Corollary 20.
We now apply Theorem 41 to prove Corollary 21.
Corollary 21 (Half-regularized approximate solver). Given regularized box-simplex game (6.24)
with regularization parameters 72ϵ ≤µ ≤1 and optimizer (x⋆, y⋆), and letting ϵ ∈(m−10, 1),
Algorithm 23 with ε ←
ϵ
2 returns xK satisfying maxy∈[0,1]n fµ(xK, y) −fµ(x⋆, y⋆) ≤ϵ. The total
runtime of the algorithm is O

nnz(A) · ( Cmax
√µϵ + log
  m
ϵ

) · log

Cmax log m
ϵ

log
  mnBmax
ϵ

.
Proof. We apply Theorem 41 with ε ←ϵ
2 and the same value of µ as in the regularized objective
(6.24). Note that the additive range of the quadratic regularizer is ϵ
2, and hence
fµ,ϵ(x, y) ≤fµ(x, y) ≤fµ,ϵ(x, y) + ϵ
2, for all (x, y) ∈∆m × [0, 1]n.
It thus suﬃces to obtain a ϵ
2 error guarantee from Theorem 41, which yields the runtime guarantee.

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
708
E.3
Approximating Sinkhorn distances
In this section, we consider the problem of approximating Sinkhorn distances [151], a common
computational task in the theory and practice of optimal transport. In this problem, we are given
a product domain on two discrete supports L × R, along with demands d ∈RL+R
≥0
(with marginals
∥dL∥1 = ∥dR∥1 = 1) and a cost matrix c ∈RL×R
≥0
. Letting m = |L||R| and n = |L|+|R|, the optimal
transport (OT) problem asks to ﬁnd a feasible transport plan x ∈∆m minimizing ⟨c, x⟩, concisely
described as
min
x∈∆m|B⊤x=d ⟨c, x⟩,
(E.23)
where B is the unsigned incidence matrix of the complete bipartite graph on L × R. Prior work
by [151] proposed Sinkhorn distances as an eﬃciently computable, diﬀerentiable alternative to (E.23),
where for µ > 0, the Sinkhorn distance asks for an optimal regularized transport plan:
x⋆
µ solves
min
x∈∆m|B⊤x=d ⟨c, x⟩+ µH(x), where H(x) =
X
i∈[m]
xi log xi.
(E.24)
We use the notion of function error in terms of the regularized objective ⟨c, x⟩+ µH(x) as the
approximation criterion for developing our approximate solvers in this section, and we aim to ﬁnd
an ϵ-approximate solution xµ ∈U(dL, dR) := {x ∈∆m | B⊤x = d} satisfying
⟨c, xµ⟩+ µH(xµ) ≤

c, x⋆
µ

+ µH(x⋆
µ) + ϵ.
(E.25)
We call such an xµ an ϵ-approximate minimizer to (E.24). One reason for choosing this deﬁnition
of an approximate solution is that function error bounds the error in transportation cost and ℓ1
distances, i.e.
µ
2
xµ −x⋆
µ
2
1 ≤⟨c, xµ⟩+ µH(xµ) −
 
c, x⋆
µ

+ µH(x⋆
µ)

,
and
⟨c, xµ⟩−

c, x⋆
µ

≤∥c∥∞
xµ −x⋆
µ

1 .
The ﬁrst inequality follows from the ﬁrst-order optimality condition of x⋆
µ and Pinsker's inequality,
which shows the objective minimized by x⋆
µ is µ-strongly convex in ℓ1. The second is ℓ1-ℓ∞H¨older.
We present three diﬀerent approaches to approximate Sinkhorn distances (E.24).
1. In Section E.3.1, we develop a ﬁrst-order method that uses our regularized box-simplex solver,
which ﬁnds an ϵ-approximate minimizer to (E.24) in eO(
1
√µϵ) time for µ = Ω(ϵ) (Theorem 85).
2. In Section E.3.2, we present a high-accuracy solver for the problem by building upon recent
near-linear time box-constrained Newton's method for matrix scaling problems [145], which
ﬁnds an ϵ-approximate minimizer in an improved eO( 1
µ) time (Theorem 86).

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
709
3. Lastly, we provide in Appendix E.3.3 an unaccelerated Sinkhorn distance solver with runtime
eO( 1
µϵ) using the classical Sinkhorn method [208] (Corollary 63).
E.3.1
Regularized box-simplex solver for computing Sinkhorn distances:
Theorem 85
In this section, we summarize our application of Theorem 41 in approximating the Sinkhorn distance
(E.24). We ﬁrst show approximating (E.24) can be reduced to solving a regularized box-simplex game:
min
x∈∆m
max
y∈[0,1]2n
1
4C ⟨c, x⟩+ µ
4C H(x) + y⊤A⊤x −⟨b, y⟩,
where A := 1
4

B, −B

, b := 1
4

d, −d

,
(E.26)
for some C := 2 (∥c∥∞+ 33µ log(m)). This characterization, which is key to our approach, follows
by ﬁrst replacing the equality constraints B⊤x = d with inequality constraints B⊤x ≤d, B⊤x ≥d,
and then developing an analogous reduction to the one used by [293] for solving the standard OT
objective (E.23). In particular, by maximizing over y, problem (E.26) is equivalent to the following
regularized ℓ1-regression problem:
min
x∈∆m
1
4C ⟨c, x⟩+ µ
4C H(x) + 1
4
B⊤x −d

1 .
(E.27)
The quantity
B⊤x −d

1 is naturally interpreted as the deviation of the x marginals from the
demands d. We further require one simple helper fact, which shows that any transport plan x can
be "ﬁxed" to be feasible for demands d at a cost depending on its deviation
B⊤x −d

1.
Lemma 278 (Lemma 7, [25]). There is an algorithm, OTRound (Algorithm 2, [25]), which takes
as input any x ∈Rm
≥0 and produces ˜x ∈∆m such that ∥˜x −x∥1 ≤2
B⊤x −d

1 and B⊤˜x = d. The
algorithm runs in O(m) time.
The guarantees of OTRound (cf. Lemma 278) give an algorithmic proof of equivalence between
(E.27) and (E.24) by relating their approximate solutions: Lemma 279 gives a formal statement.
Lemma 279. Let x be a
1
4C ∆-approximate minimizer to (E.27). Then, letting ˜x be the result of
OTRound(x) (cf. Lemma 278), ˜x is a ∆+ µm−30-approximate minimizer to (E.24).
Proof. Deﬁne OPTSinkhorn to be the value of the Sinkhorn objective (E.24), and OPTSinkhorn-Pen to
be the value of (E.27). It is immediate that OPTSinkhorn-Pen ≤
1
4C OPTSinkhorn, since any feasible
point for (E.24) (in particular the minimizer) is also feasible for (E.27) and obtains the same function
value. Hence, we have
⟨c, x⟩+ µH(x) + C
B⊤x −d

1 ≤4C · OPTSinkhorn-Pen + ∆≤OPTSinkhorn + ∆.

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
710
Moreover, we bound the objective value of ˜x:
⟨c, ˜x⟩+ µH(˜x) = ⟨c, x⟩+ µH(x) + ⟨c, ˜x −x⟩+ µ (H(˜x) −H(x))
≤⟨c, x⟩+ µH(x) + (∥c∥∞+ 33µ log(m)) ∥˜x −x∥1 + µm−30
≤⟨c, x⟩+ µH(x) + 2 (∥c∥∞+ 33µ log(m))
B⊤x −d

1 + µm−30
= ⟨c, x⟩+ µH(x) + C
B⊤x −d

1 + µm−30.
In the second line, we applied ℓ1-ℓ∞H¨older and Lemma 276, in the third we used Lemma 278, and
in the last we used the deﬁnition of C. Combining the above displays, we have the desired
⟨c, ˜x⟩+ µH(˜x) ≤OPTSinkhorn + ∆+ µm−30.
Lemma 279 shows that to obtain an ϵ ≥2µm−30-approximate minimizer of (E.24), it suﬃces to
solve (E.27) to
1
8C · ϵ additive accuracy. Applying Corollary 21 in Section 6.4.3, we thus develop a
solver for minimizing the Sinkhorn distance objective (E.24) with the following guarantees.
Theorem 85 (Regularized box-simplex solver for Sinkhorn distances). Consider the Sinkhorn dis-
tance objective (E.24) on an instance with n = |L| + |R| vertices and m = |L||R| edges, and suppose
ϵ ∈[m−5 ∥c∥∞, ∥c∥∞] , µ ∈[36ϵ, ∥c∥∞]. Using OTRound on the result of Corollary 21 applied to
(E.26) obtains an ϵ-approximate minimizer to (E.24) in time
O

m
rµ
ϵ log(m) + ∥c∥∞
√µϵ

log(m) log
∥c∥∞log m
ϵ

.
Proof. Note that the deviation between the regularized Sinkhorn objective (E.26) and the actual
Sinkhorn objective (E.24) at an approximate minimizer is at most µm−30 ≤ϵ
2 =: ∆by Lemma 279.
Thus, it suﬃces to ﬁnd a ε-approximate minimizer to (E.26) with the choice of parameters
µ′ ←µ
4C =
µ
8 (∥c∥∞+ 33µ log(m)),
and ε ←
ϵ
8C =
ϵ
16 (∥c∥∞+ 33µ log(m)).
It is straightforward to verify that the assumptions at the start of Section 6.4 on ∥A∥∞, ∥b∥1,
and ∥c∥∞are all met for this problem and the desired accuracy parameter.
Thus, by applying
Corollary 21, we obtain the desired runtime.

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
711
E.3.2
Box-constrained Newton for computing Sinkhorn distances: The-
orem 86
In this section, we show how to apply the recent matrix scaling solvers developed in [145] to obtain
second-order high-accuracy solvers for approximating Sinkhorn distances.
Our approach follows
from a direct connection to the scaling problem via the well-known correspondence of matrix scaling
and computing Sinkhorn distances: we provide the following results to make this reduction from
matrix scaling rigorous for completeness, and to prove our claimed runtime in Theorem 86.
We ﬁrst deﬁne the matrix scaling problem.
Deﬁnition 55 (Matrix scaling). Given a nonnegative matrix K ∈Rn1×n2
≥0
, the (r, c)-matrix scaling
problem asks to ﬁnd the vectors u ∈Rn1, v ∈Rn2 such that
diag (exp(u)) · K · diag (exp(v)) 1 = r, 1⊤diag (exp(u)) · K · diag (exp(v)) = c⊤.
In other words, it asks to rescale K by nonnegative diagonal matrices to obtain the speciﬁed row
and column marginals. Prior work by [145] gives a near-linear time matrix scaling solver by using a
box-constrained Newton's method, which we restate in Proposition 63 for our use.
Proposition 63 (Theorem 4.6, [145]). Given K ∈Rn1×n2
≥0
with sK := P
i∈[n1],j∈[n2] Kij, deﬁne
PK(u, v) :=
X
i∈[n1],j∈[n2]
Kij exp(ui −vj) −⟨r, u⟩+ ⟨c, v⟩.
(E.28)
Suppose there is an approximate (r, c)-scaling (u, v) satisfying PK(u, v) ≤infu′,v′ PK(u′, v′) +
ε2
n1+n2
and ∥u∥∞, ∥v∥∞≤B. Then we can compute an approximate (r, c) scaling (u, v) of K satisfying
∥diag (exp(u)) · K · diag (exp(v)) 1 −r∥2
2 +
diag (exp(v)) · K⊤· diag (exp(u)) 1 −c
2
2 ≤ε,
in time eO(nnz (K) · B log2( sK
ε )).
It is well-known that computing Sinkhorn distances is equivalent to ﬁnding the optimal matrix
scaling for K := exp(−1
µC) ∈RL×R, where C = (cij)i∈L,j∈R comes from reshaping the cost vector
(see e.g. Lemma 2, [151]).
Consequently, we can apply Proposition 63 with ε :=
ϵ2
8∥c∥2
∞m and
OTRound (Lemma 278) to obtain a high-precision solver for the Sinkhorn distance.
Theorem 86 (Box-constrained Newton solver for Sinkhorn distances). Consider the Sinkhorn dis-
tance objective (E.24) on an instance with n = |L| + |R| vertices and m = |L||R| edges, and suppose
ϵ ∈[m−5 ∥c∥∞, ∥c∥∞] and µ ∈(0, ∥c∥∞]. The algorithm in Proposition 63 obtains an ϵ-approximate
minimizer to (E.24) in time
eO
m ∥c∥∞
µ

.

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
712
To prove the theorem, we ﬁrst require one helper lemma.
Lemma 280. Let dL ∈∆L, dR ∈∆R be demands, let C ∈RL×R
≥0
be costs, and deﬁne K :=
exp(−1
µC) for some µ ≥0. Let x ∈Rm
≥0 be X ∈RL×R appropriately vectorized, such that X =
diag (u) · K · diag (v) for some u ∈RL
≥0, v ∈RR
≥0. Assume that
∥X1 −dL∥1 +
X⊤1 −dR

1 ≤∆.
Then, ˆx :=
x
∥x∥1 is a 3C∆+ µm−30-approximate minimizer to (E.24) with demands d, where C :=
2(∥c∥∞+ 33µ log(m)).
Proof. Clearly, ˆx is also a diagonal scaling of K, by multiplying u or v by appropriate scalars, and
∥x −ˆx∥1 = |∥x∥1 −1| =
1⊤X1 −1⊤dL
 ≤∥X1 −dL∥1 ≤∆.
This along with Lemma 2 of [151] implies that ˆx, as a diagonal scaling of K, is the optimal solution
to (E.24) for some demands ˆdL = bX1, ˆdR = bX⊤1 such that
 ˆd −d

1 = ∥X1 −dL∥1 +
X⊤1 −dR

1 ≤∆+
(X −bX)1

1 +
(X −bX)⊤1

1 ≤3∆.
The remainder of the proof follows from Lemma 281.
Next, Lemma 280 is key to proving Theorem 86 below.
Proof of Theorem 86. Let C := 2 (∥c∥∞+ 33µ log(m)). We ﬁrst do a preprocessing step to pad the
demands dL, dR to the modiﬁed demands ˜dL = Oδ(dL) and ˜dR = Oδ(dR) (see Deﬁnition 26), where
δ :=
ϵ
48mC . Next, suppose for ε :=
ϵ2
288C2m, we ﬁnd a ε-approximate ( ˜dL, ˜dR)-scaling (u, v) in the
sense of Proposition 63, such that X := diag (u) · K · diag (v) satisﬁes
X1 −˜dL

1 +
X⊤1 −˜dR

1 ≤
√
2m
rX1 −˜dL

2
2 +
X⊤1 −˜dR

2
2 ≤
√
2mε =
ϵ
12C .
Recalling that the guarantees of padding imply
dL −˜dL

1 +
dR −˜dR

1 ≤2mδ + 2mδ =
ϵ
12C ,
we have by the triangle inequality, letting x ∈Rm
≥0 be the appropriately vectorized X, and ˆx :=
x
∥x∥1 ,
B⊤ˆx −d

1 ≤
X1 −˜dL

1 +
dL −˜dL

1 +
X⊤1 −˜dR

1 +
dR −˜dR

1 + 2 ∥ˆx −x∥1
≤
ϵ
3C .

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
713
In the last inequality, we bound (see the proof of Lemma 280) ∥x −ˆx∥1 ≤
ϵ
12C . Next, let x′ be the
plan which results after applying OTRound for ˆx and demands d, and let ˜x be the resulting plan
when applying OTRound for x⋆
µ and demands ˜d, where x⋆
µ is optimal for (E.24) with demands d. By
a similar argument as used in the proof of Lemma 279, we have
⟨c, ˜x⟩+ µH(˜x) ≤

c, x⋆
µ

+ µH(x⋆
µ) + C
B⊤x⋆
µ −˜d

1 + µm−30,
⟨c, x′⟩+ µH(x′) ≤⟨c, ˆx⟩+ µH(ˆx) + C
B⊤ˆx −d

1 + µm−30.
(E.29)
Moreover, letting ˜x⋆
µ be optimal for (E.24) with demands ˜d, applying Lemma 280 with d ←˜d and
∆←
ϵ
12C , we obtain
⟨c, ˆx⟩+ µH(ˆx) ≤

c, ˜x⋆
µ

+ µH(˜x⋆
µ) + ϵ
4 + µm−30 ≤⟨c, ˜x⟩+ µH(˜x) + ϵ
4 + µm−30,
(E.30)
since ˜x is feasible for (E.24) with demands ˜d. Finally,
B⊤x⋆
µ −˜d

1 =
d −˜d

1 ≤
ϵ
12C .
(E.31)
Combining (E.29), (E.30), and (E.31), as well as our assumed ranges on ϵ and µ, we obtain
⟨c, x′⟩+ µH(x′) ≤

c, x⋆
µ

+ µH(x⋆
µ) + C
B⊤x⋆
µ −˜d

1 + C
B⊤ˆx −d

1 + ϵ
4 + 3µm−30
≤

c, x⋆
µ

+ µH(x⋆
µ) + ϵ
12 + ϵ
3 + ϵ
4 + 3µm−30 ≤

c, x⋆
µ

+ µH(x⋆
µ) + ϵ.
This proves the correctness of returning the transport plan x′.
The runtime is bounded by one call to the matrix scaling solver, and one call to OTRound
(which clearly does not dominate). To parameterize Proposition 63, the matrix scaling problem
with K = exp(−1
µC) ∈RL×R satisﬁes sK ≤m since c ≥0 entrywise. Furthermore, there are
optimal ( ˜dL, ˜dR)-scaling vectors (u⋆, v⋆) of K satisfying (see e.g. Lemma 10, [82]):
∥u⋆∥∞, ∥v⋆∥∞= O

∥c∥∞
µ
+ log
m
mini∈L,j∈R

[ ˜dL]i, [ ˜dR]j


= O
∥c∥∞
µ
+ log m ∥c∥∞
ϵ

.
Thus by applying Proposition 63 with ε =
ϵ2
288C2m, the runtime is bounded as desired.
E.3.3
Unaccelerated rate of Sinkhorn's algorithm
In this section, we demonstrate that Sinkhorn iteration obtains a complexity for approximately
optimizing (E.24) scaling as eO( ∥c∥2
∞
µϵ ), the natural unaccelerated counterpart to Theorem 85. We
note that this rate for solving (E.24) (up to logarithmic factors) appeared as Theorem 5 of [24], and

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
714
we provide a proof for completeness. We consider an algorithm and combine an analysis of Sinkhorn
due to [208] and a technical lemma developed in this paper (Lemma 276 from Appendix E.2.3), which
we use to reduce (E.24) to a regularized box-simplex game up to negligible (inverse-polynomial)
additive error.
Throughout, we assume ϵ ≤µ ≤∥c∥∞, and further that ∥c∥∞
ϵ
≤m5; otherwise, the runtime of
cutting-plane methods [296] subsume all runtimes claimed in this paper for approximately solving
(E.24). Finally, throughout this section we will ﬁx the values of ϵ, µ, and c ∈Rm
≥0, and deﬁne for
demands d the optimal value of (E.24) by
OPTµ(d) :=
min
x∈∆m,B⊤x=d ⟨c, x⟩+ µH(x).
We ﬁrst bound the diﬀerence between OPTµ(d) and OPTµ(d′) for "nearby" d and d′.
Lemma 281. For any demand vectors d, d′ ∈RL+R
≥0
with blockwise restrictions in ∆L and ∆R,
OPTµ(d) ≤OPTµ(d′) + (2 ∥c∥∞+ 66µ log(m)) ∥d −d′∥1 + µm−30.
Proof. Throughout the proof deﬁne ∆:= ∥d −d′∥1. Let x satisfying B⊤x = d achieve the value
OPTµ(d), and similarly let x′ with B⊤x = d′ achieve OPTµ(d′). By Lemma 278, there is a plan ˜x
with B⊤˜x = d′ and ∥x −˜x∥1 ≤2∆. Since x′ minimizes the Sinkhorn objective with demands d′,
⟨c, x′⟩+ µH(x′) = ⟨c, ˜x⟩+ µH(˜x)
= ⟨c, x⟩+ µH(x) + (⟨c, ˜x −x⟩+ µH(˜x) −µH(x))
≤⟨c, x⟩+ µH(x) + (2 ∥c∥∞+ 66µ log(m)) ∆+ µm−30.
The only inequality used ℓ1-ℓ∞H¨older's, and Lemma 276.
For the remainder of the section, we will ﬁx a particular d which we wish to optimize the Sinkhorn
objective for (and refer to all other demands by d′, ˜d, etc.). We begin by deﬁning the "padded"
demand vector ˜d ∈RL+R
≥0
obtained from modifying d as follows:
˜dL =
max(dL, m−20)
∥max(dL, m−20)∥1
, ˜dR =
max(dR, m−20)
∥max(dR, m−20)∥1
.
(E.32)
By observation, ∥˜d −d∥1 ≤2m−19. Next, we recall a result from the literature on the performance
of Sinkhorn's algorithm.
Proposition 64 (Theorem 1, [208]). There is an algorithm, Sinkhorn, which given demand vector
˜d entrywise at least m−21, returns x′ such that
x′ = argminx∈∆m,B⊤x=d′ ⟨c, x⟩+ µH(x)

APPENDIX E. DEFERRED PROOFS FROM CHAPTER 6
715
for some d′ with ∥d′ −d∥1 ≤∆, in time
O

m
∥c∥∞
µ∆+ log m
∆

.
Finally, we give our main result on the performance of Sinkhorn's algorithm for optimizing (E.24).
Corollary 63. For (E.24) on an instance with n vertices and m edges, suppose ϵ ≤µ ≤∥c∥∞and
∥c∥∞
ϵ
≤m5. Using Sinkhorn (Proposition 64) on ˜d deﬁned in (E.32) to accuracy
∆:=
ϵ
10 ∥c∥∞+ 330µ log(m)
and then applying OTRound (Lemma 278) with demands d results in a vector x with B⊤x = d and
⟨c, x⟩+ µH(x) ≤OPTµ(d) + ϵ
in time
O
 
∥c∥2
∞
µϵ
+ µ log2 m
ϵ
!
.
Proof. By Proposition 64, we obtain a vector x′ ∈∆m satisfying B⊤x′ = d′, ⟨c, x⟩+ µH(x), and
∥d′ −d∥1 ≤
d′ −˜d

1 +
 ˜d −d

1 ≤∆+ 2m−19. Moreover, letting x ←OTRound(x′) for demands
d, we have by Lemma 278 that ∥x −x′∥1 ≤2∆+ 4m−19, implying
⟨c, x⟩+ µH(x) ≤⟨c, x′⟩+ µH(x′) + (∥c∥∞+ 33µ log(m))
 2∆+ 4m−19
+ µm−30
≤⟨c, x′⟩+ µH(x′) + ϵ
2 = OPTµ(d′) + ϵ
2.
Moreover, Lemma 281 implies that
OPTµ(d′) ≤OPTµ(d) + (2 ∥c∥∞+ 66µ log(m))
 2∆+ 4m−19
+ µm−30 ≤OPTµ(d) + ϵ
2.
The conclusion follows from combining the above two displays.

Appendix F
Deferred proofs from Chapter 7
F.1
Deferred proofs from Section 7.2
F.1.1
Dual averaging regret bounds
Theorem 45. Let G1, . . . , GT be any sequence of gain matrices in Sn and let ¯Xt = ¯P(η Pt−1
i=1 Gi)
as in Eq. (7.14). Then, for every T ∈N,
λmax
 T
X
t=1
Gt
!
−
T
X
t=1

Gt, ¯Xt

≤log(4n)
η
+ 3η
2 ·
T
X
t=1
∥Gt∥2
∞.
(7.16)
If additionally 0 ⪯Gt ⪯I for every t and η ≤1
6,
λmax
 T
X
t=1
Gt
!
−
T
X
t=1

Gt, ¯Xt

≤log (4n)
η
+ 3η · λmax
 T
X
t=1
Gt
!
.
(7.17)
Proof. We start with the well-known Bregman 3-point identity, valid for any Φ0, Φ1, Φ2 ∈Sn,

Φ2 −Φ1, ¯P(Φ0) −¯P(Φ1)

= ¯VΦ0(Φ1) −¯VΦ0(Φ2) + ¯VΦ1(Φ2);
(F.1)
the identity follows from the deﬁnition (7.15) of ¯V by direct substitution. Fix some S ∈relint ∆n
and S ∈Sn such that S = ¯P(Ψ) (which exists by Proposition 21.4). Let Yt = η Pt−1
i=1 Gi so that
¯Xt = ¯P(Yt). For a given t, we use the 3-point identity with Φ0 = Ψ, Φ1 = Yt and Φ2 = Yt+1,
yielding
η

Gt, S −¯Xt

=

Yt+1 −Yt, ¯P(Ψ) −¯P(Yt)

= ¯VΨ(Yt) −¯VΨ(Yt+1) + ¯VYt(Yt+1).
716

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
717
Summing these equalities over t = 1, . . . , T and dividing by η gives
* T
X
t=1
Gt, S
+
−
T
X
t=1

Gt, ¯Xt

=
¯VΨ(Y1) −¯VΨ(YT +1)
η
+ 1
η
T
X
t=1
¯VYt(Yt+1)
(F.2)
≤log 4n
η
+ 3η
2
T
X
t=1
∥Gt∥2
∞.
(F.3)
Above, we used ¯VYt(Yt+1) = ¯VYt(Yt + ηGt) ≤3
2η2 ∥Gt∥2
∞(Proposition 21.1) along with Y1 = 0
and ¯VΨ(0) −¯VΨ(YT +1) ≤log 4n (Proposition 21.3).
Since the bound (F.3) is valid for any S ∈relint ∆n, we may supremize it over S. The result (7.16)
follows from noting that supS∈relint ∆n

 PT
t=1 Gt, S

= λmax
  PT
i=1 Gt

.
To see the second bound (7.17), we return to the identity (F.2) and note that the assumptions
0 ⪯Gt ⪯I and η ≤1
6 imply ∥ηGt∥∞≤1
6. Therefore we may use Proposition 21.2 to obtain
¯VYt(Yt+1) = ¯VYt(Yt + ηGt) ≤3 ∥ηGt∥∞

ηGt, ¯P(Yt)

= 3η2 
Gt, ¯Xt

.
Substituting back into (F.2), rearranging and taking the supremum over S as before, we obtain
λmax
 T
X
i=1
Gt
!
≤(1 + 3η)
T
X
t=1

Gt, ¯Xt

+ log(4n)
η
.
(F.4)
Dividing through by (1 + 3η) and noting that 1 −x ≤
1
1+x ≤1 for every x ≥0, we obtain the
result (7.17), concluding the proof.
F.1.2
High probability regret bounds
Corollary 23. Let G1, . . . , GT be symmetric gain matrices satisfying Assumption 5 and let Xt be
generated according to Eq. (7.13). If ∥Gt∥∞≤1 for every t, then for every T ∈N and δ ∈(0, 1),
with probability at least 1 −δ,
λmax
 T
X
i=1
Gt
!
−
T
X
t=1
⟨Gt, Xt⟩≤log(4n)
η
+ 3η
2 T +
q
2T log 1
δ .
(7.18)
If additionally 0 ⪯Gt ⪯I for every t and η ≤1
6, then with probability at least 1 −δ,
λmax
 T
X
i=1
Gt
!
−
T
X
t=1
⟨Gt, Xt⟩≤log (4n/δ)
η
+ 4η λmax
 T
X
i=1
Gt
!
.
(7.19)
Proof. We start with the ﬁrst claim (7.18). Recall that a random process Dt adapted to a ﬁltration Ft
is σ2-sub-Gaussian if E[exp(λDt) | Ft−1] ≤exp(λ2σ2/2) for all λ ∈R. Then using the boundedness

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
718
assumption that ⟨Gt, Xt⟩≤∥Gt∥∞≤1, Hoeﬀding's lemma on bounded random variables implies
that the martingale diﬀerence sequence ⟨Gt, Xt −¯Xt⟩is 1-sub-Gaussian. Consequently, the Azuma-
Hoeﬀding inequality immediately implies that
T
X
t=1
⟨Gt, Xt⟩≥
T
X
t=1
⟨Gt, ¯Xt⟩−
q
2T log 1
δ
w.p. ≥1 −δ.
The bound (7.16) in Theorem 45 thus gives the result (7.18).
For the multiplicative bound (7.19), we require a slightly diﬀerent relative martingale convergence
guarantee.
Lemma 282 ([20], Lemma G.1). Let {Dt} be adapted to the ﬁltration {Ft} and satisfy 0 ≤Dt ≤1.
Then, for any δ, µ ∈(0, 1), and any T ∈N,
P
 T
X
t=1
Dt ≥(1 −µ)
T
X
t=1
E [Dt | Ft−1] −log 1
δ
µ
!
≥1 −δ.
Similarly, the assumption 0 ⪯Gt ⪯I, along with Xt ∈∆n, imply 0 ≤⟨Gt, Xt⟩≤1. Therefore,
the conditions of Lemma 282 hold for Dt = ⟨Gt, Xt⟩, and we use it with µ = η ≤1, obtaining
T
X
t=1
⟨Gt, Xt⟩≥(1 −η)
T
X
t=1

Gt, ¯Xt

−log 1
δ
η
w.p. ≥1 −δ.
The bound (7.17) in Theorem 45 thus yields that with probability at least 1−δ over the randomness
in Xt and Gt,
T
X
t=1
⟨Gt, Xt⟩≥(1 −η)(1 −3η)λmax
 T
X
t=1
Gt
!
−log(4n/δ)
η
.
Noting that (1 −η)(1 −3η) ≥1 −4η completes the proof.
F.1.3
Proof of Lemma 7.2.2
For any Y, D ∈Sn, ∇2pmw(Y)[D, D] ≤

D2, Pmw(Y)

.
Proof. While the result is evident from the development in [421], it is not stated there formally. We
therefore derive it here using our notation and one key lemma from [421]. First, note that
⟨D, ∇pmw(Y)⟩= ⟨D, Pmw(Y)⟩=

D, eY
tr eY
,
where throughout ∇denotes diﬀerentiation with respect to Y and D is viewed as ﬁxed. Applying

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
719
∇again gives,
∇2pmw(Y)[D, D] =
*
D, ∇
 
D, eY
tr eY
!+
=

D, ∇

D, eY
tr eY
−
 
D, eY
tr eY
!2
≤

D, ∇

D, eY
tr eY
.
Note that ∇

D, eY
̸= DeY when D and Y do not commute. However, using the Taylor series for
the exponential and the formula ∇

D, Yk
= Pk−1
i=0 YiDYk−1−i gives,
∇

D, eY
=
∞
X
k=0
1
k!∇

D, Yk
=
∞
X
k=1
k−1
X
i=0
1
k!YiDYk−1−i.
Consequently, we may write

D, ∇

D, eY
=
∞
X
k=1
k−1
X
i=0
1
k!

D, YiDYk−1−i
=
∞
X
k=1
k−1
X
i=0
1
2(k!)

D, YiDYk−1−i + Yk−1−iDYi
.
Lemma 1 in [420] shows that, when Y ⪰0,

D, YiDYk−1−i + Yk−1−iDYi
≤2

D2, Yk−1
.
Substituting back, this gives

D, ∇

D, eY
≤
∞
X
k=1
1
(k −1)!

D2, Yk−1
=

D2, eY
,
and consequently
∇2pmw(Y)[D, D] ≤

D2, eY
tr eY
=

D2, Pmw(Y)

=

D2, ∇pmw(Y)

as required. Finally, note that the assumption Y ⪰0 is without loss of generality, as Pmw(Y) =
Pmw(Y + cI) for every c ∈R, and therefore ∇2pmw is also invariant to scalar shifts.
F.1.4
Proof of Lemma 7.2.2
For any Y, D ∈Sn, orthogonal eigenbasis Q for Y , and w ∼Dirichlet( 1
2, . . . , 1
2),
∇2¯p(Y)[D, D] ≤3 · Ew∇2pmw(Y + Q diag(log w)QT )[D, D]
(7.24)
≤3

D2, ¯P(Y)

.
(7.25)

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
720
Proof. Let ˜D = QT DQ, where as before Y = QΛQT is an eigen-decomposition and Λ = diag(λ).
Recall that lse : Rn →R denotes the vector softmax function, lse(y) := log(Pn
i=1 eyi) = pmw(diag y).
Similarly, deﬁne lse(y) := Ewlse(y+log w) for w ∼Dirichlet( 1
2, . . . , 1
2). By Lemma 87, ¯p(Y) = lse(λ)
is a spectral function. Theorem 3.3 of [357] prove that
∇2¯p(Y)[D, D] = ∇2lse(λ)[diag ˜D, diag ˜D] +
D
¯A(λ), ˜D ◦˜D
E
,
(F.5)
where ◦denotes elementwise multiplication, diag( ˜D) is a vector comprised of the diagonal of ˜D, and
the matrix ¯A is given by
¯Aij(λ) = ∇ilse(λ) −∇jlse(λ)
λi −λj
= Ew
∇ilse(λ + log w) −∇jlse(λ + log w)
λi −λj
|
{z
}
:=Aw
ij(λ)
for i ̸= j and 0 otherwise, whenever λ has distinct elements. This distinctiveness assumption is
without loss of generality, as ¯p is C2 (Theorem 4.2, [357]) so we may otherwise consider an arbitrarily
small perturbation of λ and appeal to continuity of ∇2¯p.
We now use the spectral function Hessian formula to write down ∇2pmw(Y{w})[D, D] where
Y{w} := Y + Q diag(log w)QT (noting that Y and Y{w} have the same eigenvectors),
∇2pmw(Y{w})[D, D] = ∇2lse(λ + log w)[diag ˜D, diag ˜D] +
D
Amw(λ + log w), ˜D ◦˜D
E
,
(F.6)
where
Amw
ij (λ) := ∇ilse(λ) −∇jlse(λ)
λi −λj
= A1
ij(λ)
for i ̸= j and 0 otherwise.
Taking the expectation over w in (F.6) and recalling the deﬁnition
lse(λ) = Ewlse(λ + log w) gives
Ew∇2pmw(Y{w})[D, D] = ∇2lse(λ)[diag ˜D, diag ˜D] +
D
EwAmw(λ + log w), ˜D ◦˜D
E
.
(F.7)
Comparing Eq. (F.7) to (F.5) and the desired bound (7.24), we see that it remains to upper
bound ¯A(λ) = EwAw(λ) in terms of EwAmw(λ + log w). Fix indices i, j ∈[n] such that i ̸= j, and
let
δ := λi −λj
2
and ρ := 1
2 log wi
wj
.
Since ¯A and Amw are both symmetric matrices, we may assume that λi > λj and so δ > 0 (recall
we assumed λi ̸= λj without loss of generality). Let wi↔j denote a vector identical to w except
coordinates i and j are swapped. With this notation, Lemma 283, which we prove in Section F.1.4,

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
721
yields the bound
Aw
ij(λ) + Awi↔j
ij
(λ) ≤

1 + |ρ| tanh(δ)
δ
 
Amw
ij (λ + log w) + Amw
ij (λ + log wi↔j)

.
Taking the expectation over w and using the fact that Dirichlet( 1
2, . . . , 1
2) is invariant to permuta-
tions, we have
¯Aij(λ) ≤Ew

1 + |ρ| tanh(δ)
δ

Amw
ij (λ + log w)

.
(F.8)
We now focus on the term Ew
|ρ| tanh(δ)
δ
Amw
ij (λ + log w). We have
Ew
|ρ| tanh(δ)
δ
Amw
ij (λ + log w) = Ew
|ρ| tanh(δ)
δ
Amw
ij (λ + log w)

I{|ρ|≤δ} + I{|ρ|>δ}

≤(tanh δ)EwAmw
ij (λ + log w)I{|ρ|≤δ} + tanh δ
δ
Ew|ρ|Amw
ij (λ + log w)I{|ρ|>δ},
(F.9)
where the ﬁnal transition uses |ρ|I{|ρ|≤δ} ≤δI{|ρ|≤δ} and Amw
ij (ζ) ≥0 for every ζ ∈Rn. The latter
is a consequence of the convexity of lse and is also evident from Eq. (F.14) in Section F.1.4.
Since w ∼Dirichlet( 1
2, . . . , 1
2), ρ =
1
2 log wi
wj is independent of w\ij := {wk}k̸=i,j.
Moreover,
wi, wj are completely determined by ρ and w\ij (see explicit expression in Section F.1.4). Therefore,
conditional on w\ij, Amw
ij (λ + log w) is a function of ρ. In Lemma 284 we prove that for every λ
and w\ij, this function is decreasing in ρ for ρ > δ. Hence, conditionally on w\ij and the event
ρ > δ, the random variables |ρ| and Amw
ij (λ + log w) are negatively correlated: the expectation of
their product at most the product of their expectations. Let Eρ denote expectation conditional on
w\ij. Lemma 285, with f(ρ) = |ρ|, g(ρ) = Amw
ij (λ + log w), and S = {ρ | ρ > δ} gives that
Eρ|ρ|Amw
ij (λ + log w)I{ρ>δ} ≤(Eρ [ |ρ| | ρ > δ])
 EρAmw
ij (λ + log w)I{ρ>δ}

.
(F.10)
Similarly, Lemma 284 also gives that (conditional on w\ij) Amw
ij (λ + log w) is increasing in ρ for
ρ < −δ, and therefore, by Lemma 285,
Eρ|ρ|Amw
ij (λ + log w)I{ρ<−δ} ≤(Eρ [ |ρ| | ρ < −δ])
 EρAmw
ij (λ + log w)I{ρ<−δ}

.
(F.11)
Let z ∼Beta( 1
2, 1
2). The random variable ρ = 1
2 log wi
wj is symmetric and distributed as 1
2 log( 1−z
z ).
Therefore
Eρ [ |ρ| | ρ < −δ] = Eρ [ |ρ| | ρ > δ] = 1
2E

log 1−z
z
| log 1−z
z
> 2δ
 (⋆)
≤δ +
p
1 + e−2δ,
where we prove the inequality (⋆) in Lemma 288. Substituting this bound into inequalities (F.10)

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
722
and (F.11) and summing them, we obtain
Eρ|ρ|Amw
ij (λ + log w)I{|ρ|>δ} ≤

δ +
p
1 + e−2δ

EρAmw
ij (λ + log w)I{|ρ|>δ}.
Taking expectation over w\ij and substituting back into (F.9) therefore gives,
Ew
|ρ| tanh(δ)
δ
Amw
ij (λ + log w) ≤

tanh(δ) +
p
1 + e−2δ · tanh(δ)
δ

EwAmw
ij (λ + log w),
where we used again Amw
ij (·) ≥0 in order to increase the multiplier of EwAmw
ij (λ + log w)I{|ρ|≤δ}.
Computation shows that tanh(δ) +
√
1 + e−2δ · tanh(δ)
δ
≤1.58 ≤2 for every δ ≥0. Therefore, by the
bound (F.8) we have
¯Aij (λ) ≤3 · EwAij (λ + log w) .
(F.12)
Returning to (F.5), we write
∇2¯p(Y)[D, D] ≤∇2lse(λ)[diag ˜D, diag ˜D] + 3
D
EwAmw(λ + log w), ˜D ◦˜D
E
≤3
h
∇2lse(λ)[diag ˜D, diag ˜D] +
D
EwAmw(λ + log w), ˜D ◦˜D
Ei
.
In the ﬁrst inequality above, we substituted the bound (F.12), using the fact that all the entries of
˜D◦˜D are nonnegative. In the second inequality, we used that fact that ∇2lse(λ)[diag ˜D, diag ˜D] ≥0
since lse is convex. Recalling the expression (F.7) gives (7.24). The ﬁnal bound (7.25) follows from
applying Lemma 7.2.2 to the right side of (7.24) and using the identity (7.21).
A pointwise bound for Lemma 7.2.2
In this section we prove an elementary inequality that plays a central role in the proof of Lemma 7.2.2.
Let i, j ∈[n] be such that i ̸= j. For λ ∈Rn, we deﬁne
Nij(λ) := ∇ilse(λ) −∇jlse(λ) = eλi −eλj
Pn
k=1 eλk =
sinh

λi−λj
2

cosh

λi−λj
2

+ 1
2
P
k̸=i,j eλk−
λi+λj
2
(F.13)
and
Amw
ij (λ) = Nij(λ)
λi −λj
and Aw
ij(λ) = Nij(λ + log w)
λi −λj
.
(F.14)
Additionally, for any vector w ∈Rn, let wi↔j denote a vector identical to w except coordinates i
and j are swapped. With this notation in hand, we state and prove our bound.

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
723
Lemma 283. Let λ ∈Rn, w ∈Rn
+ and i, j ∈[n], i ̸= j. Set δ = λi−λj
2
and ρ = 1
2 log wi
wj . Then,
Aw
ij(λ) + Awi↔j
ij
(λ) ≤

1 + |ρ| tanh(δ)
δ
 
Amw
ij (λ + log w) + Amw
ij (λ + log wi↔j)

.
Proof. Deﬁne
r = 1
2
X
k/∈{i,j}
eλk+log wk−
λi+log wi+λj +log wj
2
≥0.
Observe that if we swap wi and wj, δ and r remain unchanged and the sign of ρ reverses. For x ∈R,
let f(x) :=
sinh(x)
cosh(x)+r. Using (F.13), we may write
q1 := 2Aw
ij(λ) + 2Awi↔j
ij
(λ) = f(δ + ρ)
δ
+ f(δ −ρ)
δ
and
q2 := 2Amw
ij (λ + log w) + 2Amw
ij (λ + log wi↔j) = f(δ + ρ)
δ + ρ
+ f(δ −ρ)
δ −ρ
.
With these deﬁnitions, our goal is to prove that q1−q2
q2
≤|ρ| tanh(δ)
δ
. Since f(x) is an odd function of
x, the terms q1 and q2 are invariant to sign ﬂips in either δ or ρ. Therefore, we may assume both
δ ≥0 and ρ ≥0
without loss of generality.
Substituting back the expressions for q1, q2 and using that |ρ| = ρ by assumption yields
q1 −q2
q2
= ρ
δ ·
f(δ+ρ)
δ+ρ
−f(δ−ρ)
δ−ρ
f(δ+ρ)
δ+ρ
+ f(δ−ρ)
δ−ρ
= ρ
δ · g(δ + ρ) −g(δ −ρ)
g(δ + ρ) + g(δ −ρ),
(F.15)
where
g(x) := f(x)
x
= tanh(x)
x
·
cosh(x)
cosh(x) + r.
Note that tanh(x)
x
is decreasing in |x|. Since |δ −ρ| ≤|δ + ρ| by the assumption ρ, δ ≥0, we have
g(δ −ρ) ≥tanh(δ + ρ)
δ + ρ
·
cosh(δ −ρ)
cosh(δ −ρ) + r.
and therefore
g(δ + ρ) −g(δ −ρ) ≤tanh(δ + ρ)
δ + ρ

cosh(δ + ρ)
cosh(δ + ρ) + r −
cosh(δ −ρ)
cosh(δ −ρ) + r


APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
724
and similarly,
g(δ + ρ) + g(δ −ρ) ≥tanh(δ + ρ)
δ + ρ

cosh(δ + ρ)
cosh(δ + ρ) + r +
cosh(δ −ρ)
cosh(δ −ρ) + r

.
As g(x) > 0 for every x, we may divide these bounds and obtain via elementary manipulation,
g(δ + ρ) −g(δ −ρ)
g(δ + ρ) + g(δ −ρ) ≤
cosh(δ+ρ)
cosh(δ+ρ)+r −
cosh(δ−ρ)
cosh(δ−ρ)+r
cosh(δ+ρ)
cosh(δ+ρ)+r +
cosh(δ−ρ)
cosh(δ−ρ)+r
=
r [cosh (δ + ρ) −cosh (δ −ρ)]
2 cosh (δ + ρ) cosh (δ −ρ) + r [cosh (δ + ρ) + cosh (δ −ρ)]
≤cosh(δ + ρ) −cosh(δ −ρ)
cosh(δ + ρ) + cosh(δ −ρ) = tanh(ρ) tanh(δ) ≤tanh(δ).
Substituting back into (F.15) establishes the desired bound. Examining the proof, we see that the
bound is tight for large values of r and |ρ|.
Piecewise monotonicity of Amw
Lemma 284. Let λ ∈Rn, w ∈σn (the simplex in Rn), and i, j ∈[n] such that δ := 1
2(λi −λj) > 0,
and set ρ := 1
2 log wi
wj . When λ and {wk}k̸=i,j are held ﬁxed, Amw
ij (λ + log w) is increasing in ρ for
ρ < −δ, and decreasing in ρ for ρ > δ.
Proof. First, we write Amw
ij (λ + log w) explicitly as a function of ρ, with λ and {wk}k̸=i,j as ﬁxed
parameters. By (F.14) we have
Amw
ij (λ + log w) =
sinh(ρ + δ)
2(ρ + δ)
h
cosh(ρ + δ) + 1
2
P
k/∈{i,j}
wk
√wiwj eλk−
λi+λj
2
i.
Let m = wi + wj = 1 −P
k̸=i,j wk. Since wi
wj = e2ρ and w ∈σn, we have that wi =
m
1+e−2ρ and
wj =
m
1+e2ρ . Therefore,
1
√wiwj
= 1
m
p
(1 + e−2ρ)(1 + e2ρ) = 2
m cosh(ρ).
Thus,
Amw
ij (λ + log w) =
sinh(ρ + δ)
2(ρ + δ) [cosh(ρ + δ) + r0 cosh(ρ)],
where r0 = P
k/∈{i,j}
wk
m eλk−
λi+λj
2
is a function of only λ and {wk}k̸=i,j, and therefore Amw
ij (λ+log w)
can be viewed as a function of ρ as claimed.

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
725
Writing x = ρ + δ, showing the desired monotonicity properties is equivalent to showing that
b(x) :=
sinh(x)
x (cosh(x) + r0 cosh(x −δ))
is decreasing for x > 2δ and increasing for x < 0. The derivative of b(x) is
b′(x) =
cosh(x) −1
x sinh(x)
x (cosh(x) + r0 cosh(x −δ)) −sinh(x) [sinh(x) + r0 sinh(x −δ)]
x [cosh(x) + r0 cosh(x −δ)]2
,
and has, for all x ∈R, the same sign as
s := x [cosh(x) + r0 cosh(x −δ)]
sinh(x)
b′(x) = coth(x) −1
x −sinh(x) + r0 sinh(x −δ)
cosh(x) + r0 cosh(x −δ).
(F.16)
For x > 2δ, we have by Dan's favorite inequality ( a1+a2
b1+b2 ≥min{ a1
b1 , a2
b2 } for all a1, a2, b1, b2 ≥0),
sinh(x) + r0 sinh(x −δ)
cosh(x) + r0 cosh(x −δ) ≥min {tanh(x), tanh(x −δ)} = tanh(x −δ) > tanh(x/2),
where in the last transition we used the fact that x > 2δ implies x −δ > x/2. Therefore, for x > 2δ
we have the following bound for s,
s ≤coth(x) −1
x −tanh(x/2) =
1
sinh(x) −1
x < 0,
so we have that b(x) is decreasing for x > 2δ as required, since s has the same sign as b′(x).
Similarly, for x < 0, we have by Dan's favorite inequality,
−sinh(x) −r0 sinh(x −δ)
cosh(x) + r0 cosh(x −δ) ≥min {−tanh(x), −tanh(x −δ)} = −tanh(x).
Therefore, for x < 0 we have
s ≥coth(x) −1
x −tanh(x) =
1
−x −
2
sinh(−2x) > 0,
which shows that b(x) is increasing for x < 0, concluding the proof.
The following Lemma proves the intuitive fact that decreasing and increasing functions of the
same random variable are negatively correlated.
Lemma 285. Let ρ be a real-valued random variable, let f, g be functions from R to R and let S ⊂R
be an interval. If f(x) is non-decreasing in x for x ∈S and g(x) is non-increasing in x for x ∈S,
then
Ef(ρ)g(ρ)I{ρ∈S} ≤(E [f(ρ) | ρ ∈S]) ·
 Eg(ρ)I{ρ∈S}

.

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
726
Proof. For every x, x′ ∈S we have (f(x) −f(x′)) · (g(x) −g(x′)) ≤0. Hence, for every x, x′ ∈R,
the bound (f(x) −f(x′)) · (g(x) −g(x′)) · I{x∈S}I{x′∈S} ≤0 holds as well. Let ρ′ be an independent
copy of ρ, then
E

(f(ρ) −f(ρ′)) · (g(ρ) −g(ρ′)) · I{ρ∈S}I{ρ′∈S}

≤0.
Rearranging and using the fact that ρ, ρ′ are i.i.d., we have
 Ef(ρ)g(ρ)I{ρ∈S}

·
 EI{ρ′∈S}

≤
 E

f(ρ)I{ρ∈S}

·
 E

g(ρ′)I{ρ′∈S}

.
Dividing by EI{ρ′∈S} = P(ρ ∈S) yields the desired bound.
F.1.5
Facts about the Beta distribution
Here we collect properties of Beta-distributed random variables, which we use in our development.
Lemma 286. Let n ∈N and let z ∼Beta( 1
2, n−1
2 ). Then
E log 1
z = ψ
  n
2

−ψ
  1
2

≤log(n) + log(2) + γ ≤log(4n),
where ψ(x) =
d
dx log Γ(x) is the digamma function, and γ is the Euler-Mascheroni constant.
Proof. E log 1
z = ψ
  n
2

−ψ
  1
2

by the well-known formula for expectation of the logarithm of a Beta
random variable. We have ψ(x) ≤log(x) and ψ( 1
2) = −log(4) −γ. Moreover, γ ≤log 2, giving the
ﬁnal bound.
Lemma 287. Let z ∼Beta
  1
2, 1
2

and ℓ≥0. Then
2
π
e−ℓ/2
√
1 + e−ℓ≤P

log 1 −z
z
≥ℓ

≤2
π e−ℓ/2.
Proof. The distribution Beta
  1
2, 1
2

has density 1
πx−1/2 (1 −x)−1/2. Therefore
P

log 1 −z
z
≥ℓ

= P

z ≤
1
1 + eℓ

= 1
π
Z (1+eℓ)
−1
0
x−1/2 (1 −x)−1/2 dx.
To obtain a lower bound, we use (1 −x)−1/2 ≥1 for every x ∈[0, 1], and therefore,
P

log 1 −z
z
≥ℓ

≥1
π
Z (1+eℓ)
−1
0
x−1/2dx =
2
π
√
1 + eℓ= 2
π
e−ℓ/2
√
1 + e−ℓ.

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
727
For the upper bound, we use (1 −x)−1/2 ≤

1 −
1
1+eℓ
−1/2
for every 0 ≤x ≤(1 + eℓ)−1, giving
P

log 1 −z
z
≥ℓ

≤1
π
r
1 + eℓ
eℓ
Z (1+eℓ)
−1
0
x−1/2dx = 2
π e−ℓ/2.
Lemma 288. Let z ∼Beta
  1
2, 1
2

and ℓ≥0. Then
E

log 1 −z
z
| log 1 −z
z
≥ℓ

≤ℓ+ 2
p
1 + e−ℓ.
Proof. Conditional on log 1−z
z
≥ℓ, log 1−z
z
is a nonnegative random variable, and we may therefore
write
E

log 1 −z
z
| log 1 −z
z
≥ℓ

=
Z ∞
x=0
P

log 1 −z
z
≥x
 log 1 −z
z
≥ℓ

dx
= ℓ+
Z ∞
x=ℓ
P
 log 1−z
z
≥x

P
 log 1−z
z
≥ℓ
 dx.
By Lemma 287,
P
 log 1−z
z
≥x

P
 log 1−z
z
≥ℓ
 ≤
p
1 + e−ℓ· e−(x−ℓ)/2.
Integrating, we obtain the desired bound.
Lemma 289. Let 3 ≤n ∈N and let z ∼Beta( 1
2, n−1
2 ). For every δ ∈(0, 1),
P

z ≥δ2
n

> 1 −δ.
Proof. The random variable z has density
Γ( n
2 )
Γ( 1
2)Γ( n−1
2 )x−1/2(1 −x)(n−3)/2 ≤
r n
2πx,
where we used Γ( 1
2) = √π and Gautschi's inequality Γ(m+1)/Γ(m+s) ≤(m+1)1−s with m = n
2 −1
and s = 1
2. Integrating the upper bound on the density, we ﬁnd P(z ≤δ2/n) ≤
q
2
πδ < δ.
F.1.6
Eﬃcient computation of matrix exponential-vector products
In this section we give a more detailed discussion of matrix exponential-vector product approxi-
mation using the Lanczos method, and prove the results stated in Section 7.2.3. We ﬁrst formally
state the Lanczos method. In Next, we survey known approximation guarantees and derive simple

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
728
corollaries. We then show that we can apply the matrix exponential to a random vector with a
multiplicative error guarantee, and then we prove it implies Proposition 22. We next discuss some
possible improvement to our guarantees via modiﬁcations and alternatives to the Lanczos method.
Finally, we prove Corollary 24.
Throughout this section we use mv(A) to denote the time required to multiply the matrix A
with any vector.
F.1.7
Description of the Lanczos method
Algorithm 75: Lanczos method for computing matrix exponential vector product
g
expk(A, b)
input
: A ∈Sn, number of iterations k, vector b ∈Rn
1 q0 ←0 ∈Rn, q1 ←b/ ∥b∥2, β1 ←1
2 for i = 1 to k do
3
qi+1 ←Aqi −βqi−1 and αi ←qT
i+1qi
4
qi+1 ←qi+1 −αqi and βi+1 = ∥qi+1∥2
5
if βi+1 = 0 then break else qi+1 ←qi+1/βi+1
6 end
7 Let
Q = [q1 · · · qk]
and
T =


α1
β2
0
β2
α2
...
...
...
βk
0
βk
αk


8 Compute tridiagonal eigen-decomposition T = V ΛV T
return
: g
expk(A, b) = ∥b∥2 · QV exp(Λ)V T e1
Ignoring numerical precision issues, each iteration in the for loop requires O(mv(A)) time, and
that for a k-by-k tridiagonal matrix, eigen-decomposition requires O(k2) time [256], and so the total
complexity is O(mv(A)k + k2). In practical settings k ≪n ≤mv(A) and the cost of the eigen-
decomposition is negligible. Nevertheless, there are ways to avoid performing it, which we discuss
brieﬂy.
Known approximation results, and some corollaries
We begin with a result on uniform polynomial approximation of the exponential due to [466].
Theorem 87 ([466], Theorem 4.1 Restated). For every b > 0 and every ϵ ∈(0, 1] there exists

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
729
polynomial p : R →R of degree O(
p
max{b, log(1/ϵ)} log(1/ϵ)) such that
sup
x∈[0,b]
| exp(−x) −p(x)| ≤ϵ .
As an immediate corollary of this we obtain the following bounds for approximating exp(x) over
arbitrary values
Corollary 64. For every a < b ∈R and every ϵ ∈(0, 1] there exists polynomial p : R →R of degree
O(
p
max{b −a, log(1/ϵ)} log(1/ϵ)) polynomial such that
sup
x∈[a,b]
| exp(x) −p(x)| ≤ϵ exp(b) .
Proof. For all x ∈[a, b] we have b −x ∈[0, b −a] and therefore by Theorem 87 there is a degree
O(
p
max{b −a, log(1/ϵ)} log(1/ϵ)) polynomial q : R →R such that
sup
x∈[a,b]
| exp(−(b −x)) −q(b −x)| ≤ϵ .
Since exp(−(b −x)) = exp(−b) exp(x), the polynomial p(x) = exp(b)q(b −x) is as desired.
The classical theory on the Lanczos method tells us that its error is bounded by twice that of
any uniform polynomial approximation. However, this theory does not account for ﬁnite precision.
A recent result [405] ties polynomial approximation to the error of the Lanczos method using ﬁnite
bitwidth ﬂoating point operations.
Theorem 88 ([405], Theorem 1). Let A ∈Sn, u ∈Rn, and f : R →R.
Suppose k ∈N,
η ∈(0, ∥A∥op] and a polynomial p for degree < k satisfy,
sup
x∈[λmin(A)−η,λmax(A)+η]
|f(x) −p(x)| ≤ϵk and
sup
x∈[λmin(A)−η,λmax(A)+η]
|f(x)| ≤C.
For any µ ∈(0, 1), let yk,µ be the output of k iterations of the Lanczos method for approximating
f(A)v, using ﬂoating point operations with B ≥c log(
nk∥A∥op
µη
) bits precision (for numerical constant
c < ∞). Then yk,µ satisﬁes
∥f(A)u −yk,µ∥2 ≤(7k · ϵk + µ · C) ∥u∥2 .
If arithmetic operations with B bits of precision can be performed in O(1) time then the method can
be implemented in time O(mv(A)k + kB max{k, B}).
Specializing to the matrix exponential and using the uniform approximation guarantee of Corol-
lary 64, we immediately obtain the following.

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
730
Corollary 65. Let A ∈Sn, u ∈Rn, and ϵ > 0, and set M = max{∥A∥op , log(1/ϵ), 1}. There exists
numerical constants c, c′ < ∞such that, for k ≥c
p
M log(M/ϵ) and B ≥c′ log( nM
ϵ ), computing
y = g
expk(A, u) with B bits of ﬂoating point precision guarantees
∥exp(A)u −y∥2 ≤ϵ exp(λmax(A)) ∥u∥2 .
The computation takes time
O

mv(A)
p
M log(M/ϵ) + M log2(nM/ϵ)

provided Θ(log( nM
ϵ )) bit arithmetic operations can be performed in time O(1).
Proof. Let η = 1. Using λmax(A)−λmin(A) ≤2 ∥A∥, Corollary 64 yields that for all α ∈(0, 1] there
exists a degree O
q
max{1 + ∥A∥op , log( 1
α)} log( 1
α)

polynomial p : R →R such that
sup
x∈[λmin(A)−η,λmax(A)+η]
| exp(x) −p(x)| ≤α exp(η) exp(λmax(A)) .
Further, since | exp(x)| ≤exp(η) exp(λmax(A)) for all x ∈[λmin(A) −η, λmax(A) + η], Theo-
rem 88 with f(x) = ex and η = 1 implies that for all µ ∈(0, 1), after applying Lanczos for
k = O(
q
max{∥A∥op , log(1/α)} log(1/α)) iterations on a ﬂoating point machine with Θ(B) bits of
precision for B = log( nk∥A∥
µ
) returns y with
∥f(A)u −y∥2 ≤

µ + α · O(
q
max{∥A∥op , log(1/α)} log(1/α))

exp(η) exp(λmax(A)))
in time O((mv(A) + n)k + kB max{k, B}). Choosing, α = O(ϵ/(M log(M/ϵ))) and µ = O(ϵ) yields
the result.
Multiplicative approximation for random vectors
We now combine the known results cited in the previous section with the randomness of the vector
fed to the matrix exponential, to obtain a multiplicative guarantee that holds with high-probability
over the choice of u, but not for all u ∈Sn−1.
Proposition 65. Let ϵ ∈(0, 1), δ ∈(0, 1), and A ∈Sn. If u is sampled uniformly at random from
the unit sphere and for k = Ω(
p
M log(nM/(ϵδ)) ∈N for M = max{∥A∥op , log(n/(ϵδ)), 1} we let
y = g
expk(A, u) (See Algorithm 75) then
∥exp(A)u −y∥2 ≤ϵ ∥exp(A)u∥2 with probability ≥1 −δ.
This can be implemented in time O

mv(A)
p
M log(nM/(ϵδ) + M log2(nM/(ϵδ))

on a ﬂoating
point machine with O(log(nM/(ϵδ))) bits of precision where arithmetic operations take O(1) time.

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
731
Proof. Consider an application of Corollary 65 to compute y such that
∥exp(A)u −y∥2 ≤ϵ′ exp(λmax(A)) ∥u∥2 .
Now let v be a unit eigenvector of A with eigenvalue λmax(A). Since v is an eigenvector or the PSD
matrix exp(A) with eigenvalue exp(λmax(A)) we have that ∥exp(A)u∥≥exp(λmax)
vT u
. However,
since u is a random unit vector we have that |vT u|2/ ∥u∥2
2 ∼Beta( 1
2, n−1
2 ). Lemma 289 therefore
gives that |vT u|2/ ∥u∥2
2 ≥δ2
n with probability at least 1 −δ. Consequently, exp(λmax(A)) ∥u∥2 ≤
√n
δ ∥exp(A)u∥2 with the same probability. Choosing ϵ′ = ϵδ/√n and invoking Corollary 65 yields
the result.
Proof of Proposition 22
The following lemma relates the multiplicative approximation error for matrix exponential vector
products with the additive approximation error for Pu(Y) under trace norm. Combining it with
Proposition 65 immediately yields Proposition 22.
Lemma 290. Let Y ∈Sn, u, y ∈Rn and ϵ ∈[0, 1). If y ∈Rn satisﬁes
∥exp(Y/2)u −y∥2 ≤
ϵ
√
8 ∥exp(Y/2)u∥2
then
Pu(Y) −yyT
∥y∥2
2

1
≤ϵ .
Proof. Let z := exp(Y/2)u so that by assumption ∥z −y∥2 ≤ϵ ∥z∥2. Further, let ¯z := z/ ∥z∥2 and
¯y := y/ ∥y∥2. Direct calculation (see e.g. Lemma 27 of [143]) yields that the eigenvalues of ¯z¯zT −¯y¯yT
are ±
p
1 −(¯zT ¯y)2 = ± 1
2 ∥¯z + ¯y∥2 ∥¯z −¯y∥2 and therefore the deﬁnition of Pu(Y) yields
Pu(Y) −yyT
∥y∥2
2

1
=
¯z¯zT −¯y¯yT 
1 = ∥¯z + ¯y∥2 · ∥¯z −¯y∥2 ≤
√
2 ∥¯z −¯y∥2 ,
(F.17)
where in the last inequality we used that ¯z and ¯y are unit vectors. Further, by the triangle inequality
and the deﬁnitions of ¯y and ¯z we have
∥¯z −¯y∥2 ≤

z
∥z∥2
−
y
∥z∥2

2
+

y
∥z∥2
−
y
∥y∥2

2
= ∥z −y∥2
∥z∥2
+ | ∥y∥2 −∥z∥2 |
∥z∥2
≤2∥z −y∥2
∥z∥2
(F.18)

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
732
Combining (F.17) and (F.18) with the fact that ∥z −y∥2 ≤(ϵ/
√
8) ∥z∥2 then yields
Pu(Y) −yyT
∥y∥2
2

1
≤
√
2 · 2 · (ϵ/
√
8) = ϵ.
Therefore, Proposition 22 follows immediately by invoking 65 with slightly smaller ϵ.
Improvements to the Lanczos method
In this chapter we focused on the Lanczos method for approximating matrix exponential vector
products because of its excellent practicality and clean analysis. However, there are several modi-
ﬁcations to the method with appealing features, which we now describe brieﬂy. A common theme
among these modiﬁcations is the use of rational approximations to the exponential, which converge
far faster than polynomial approximations [429, 466]. Consequently, it suﬃces to perform eO (1)
Lanczos iterations on a carefully shifted and inverted version of the matrix. Each of these iterations
then involves solving a linear system, and the eﬃcacy of the shift-invert scheme will depend on how
quickly they are solved.
One basic approach to solving these systems is via standard iterative methods, e.g. conjugate
gradient. We expect such approach to oﬀer little to no advantage over applying the Lanczos ap-
proximation directly, as both methods produce vectors in the same Krylov subspace. However, the
approach renders the number of Lanczos iterations k logarithmic in ∥A∥op, and therefore the cost
k2 will never dominate the cost of the matrix-vector products [429, 405]
There is, however, a simpler way of avoiding the eigen-decomposition—simply use the rational
approximation on the tridiagonal matrix formed by running the ordinary Lanczos method, as [465]
proposes. With an appropriate rational function, computing a highly accurate approximation to
exp(T)e1 requires eO (1) tridiagonal system solves, each costing O(k) time. We leave the derivation
of explicit error bounds for this technique (similar to Corollary 64) to future work. In practice,
the cost O(k2) of tridiagonal eigen-decomposition will often be very small compared to the cost
O(mv(A)k) of the matrix-vector products.
More signiﬁcant improvements are possible if the linear system solving routine is able to exploit
information beyond matrix-vector products. For example, consider the case where the matrix to be
exponentiated is a sum of very sparse matrices—this will happen for our sketch whenever the Gt
matrices are much sparser than their cumulative sum. Then, it is possible to use stochastic variance
reduced optimization methods to solve the linear system, as [20] describe.
Another scenario of
interest is when the input matrix has a Laplacian/SDD structure and in this case the performance of
specialized linear system solvers implies approximation guarantees where the polynomial dependence
on ∥A∥op is removed altogether [429]. A ﬁnal useful structure is a chordal sparsity pattern, which

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
733
enables eﬃcient linear system solving through fast Cholesky decomposition.
Proof of Corollary 24
Corollary 24. Let G1, . . . , GT be symmetric gain matrices satisfying ∥Gt∥∞≤1 for every t. There
exists a numerical constant k0 < ∞, such that for every T ∈N and δ ∈(0, 1), ˜Xt;kt deﬁned in (7.29)
with kt =

k0(√1 + ηt) log( nT
δ )

, and Xt deﬁned in (7.13) satisfy
T
X
t=1
D
Gt, ˜Xt;kt
E
≥−1 +
T
X
t=1
⟨Gt, Xt⟩
w.p. ≥1 −δ/2.
(7.30)
Let ϵ ∈(0, 1], T = 16 log(4en/δ)
ϵ2
and η =
q
2 log(4en)
3T
. If Assumption 5 holds with respect to the actions
˜Xt;kt, then with probability at least 1 −δ, 1
T λmax
PT
i=1 Gt

−1
T
PT
t=1
D
Gt, ˜Xt;kt
E
≤ϵ. Computing
the actions ˜X1;k1, . . . , ˜XT ;kT requires O(ϵ−2.5 log2.5( n
ϵδ)) matrix-vector products.
Proof. To obtain the bound (7.30) we use Proposition 22 with ϵ ←
1
T and δ ←δ/(2T) (since we
will use a union bound). At iteration t, ∥Gi∥∞≤1 for all i < t, the quantity M appearing in
Proposition 22 can be bounded as
M ≤
 
1 +

η
2
t−1
X
i=1
Gi

∞
!
log nT 2
δ
≤O(1)(1 + ηt) log nT
δ .
Therefore, our choice of kt suﬃces to guarantee, for Yt = η Pt−1
i=1 Gi,
∥Put(Yt) −ePut;kt(Yt)∥1 ≤1
T with probability ≥1 −δ
2T ,
and so by the union bound the inequality above holds for all t = 1, . . . , T with probability at least
1 −(δ/2). Note that when using Proposition 22 we use the fact that ut is independent of Yt. Thus,
we have
T
X
t=1
D
Gt, Xt −˜Xt;kt
E
≤
T
X
t=1
∥Gt∥∞∥Xt −˜Xt;kt∥1 ≤
T
X
t=1
∥Put(Yt) −ePut;kt(Yt)∥1 = 1,
giving (7.30), where we have used ∥Gt∥∞≤1 for every t.
Note that if Assumption 5 holds with respect to the actions ˜Xt;kt then we have Gt ⊥ut | Ft−1
and therefore E [⟨Gt, Xt⟩|Ft−1] =

Gt, ¯Xt

so that Corollary 23 holds. Thus, to obtain the second
part of the corollary, we use the bound (7.18) with δ ←δ/2 and η and T as speciﬁed; using a union
bound again we have that (7.30) and (7.18) hold together with probability at least 1 −δ. Note that

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
734
η ≤ϵ ≤1 and therefore 1/T ≤1/(ηT). This gives,
1
T λmax
 T
X
i=1
Gt
!
−1
T
T
X
t=1
D
Gt, ˜Xt;kt
E
≤1
T + 3η
2 + log(4n)
ηT
+
s
2 log 2
δ
T
≤3η
2 + log(4en)
ηT
+
s
2 log 2
δ
T
=
r
6 log(4en)
T
+
s
2 log 2
δ
T
≤ϵ,
as required. Finally note that 1 + ηT = O(ϵ−1 log( n
δ )) and consequently
kT = O

ϵ−1/2 log1/2( n
δ ) log1/2( nT
δ )

= O

ϵ−1/2 log1.5( n
ϵδ)

.
Since k1 ≤k2 ≤· · · kT , the total number of matrix-vector products is bounded by T · kT =
O(ϵ−2.5 log2.5( n
ϵδ)), which concludes the proof.
F.2
Deferred proofs from Sections 7.4
F.2.1
Proof of Proposition 26
We give a proof of Proposition 26 in this section. First, we recall an algorithm for the testing variant
of a pure packing SDP problem given in Section 7.5.
Proposition 66 (Restatement of Theorem 48). There is an algorithm, Atest, which given matrices
{Mi}i∈[n] and a parameter C, is an ϵ-tolerant tester for the decision problem
does there exist w ∈∆n such that
X
i∈[n]
wiMi ⪯CI?
(F.19)
The algorithm Atest succeeds with probability ≥1 −δ and runs in time
O

Tmv

{Mi}i∈[n]

· log2(nd(δϵ)−1) log2 d
ϵ5

.
Proof of Proposition 26. As an immediate result of Proposition 66, we can solve (7.59) to multi-
plicative accuracy ϵ using a binary search. This reduction is derived as Lemma A.1 of [285], but we
give a brief summary here. We subdivide the range [⋆
−,⋆
+ ] into K buckets of multiplicative range
1 + ϵ
3, i.e. with endpoints ⋆
−· (1 + ϵ
3)k for 0 ≤k ≤K and
K = O
1
ϵ · log
 ⋆
+
⋆
−

.
We then binary search over 0 ≤k ≤K to determine the value of ⋆(v) to ϵ-multiplicative accuracy,
returning the largest endpoint for which the decision variant in Proposition 66 returns feasible (with

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
735
accuracy ϵ
3). By the guarantees of Proposition 66, the feasible point returned by Proposition 66 for
this endpoint will attain an ϵ-multiplicative approximation to the optimization variant (7.59), and
the runtime is that of Proposition 66 with an overhead of O(log K).
F.3
Deferred proofs from Section 7.5
F.3.1
Proofs from Section 7.5.2
Since our notion of approximation is multiplicative, we can assume without more than constant loss
that A has bounded entries. This observation is standard, and formalized in the following lemma.
Lemma 291 (Entrywise bounds on A). Feasibility of Problem 3 is unaﬀected (up to constants in
ϵ) by removing columns of A with entries larger than nϵ−1.
Proof. If Aji > nϵ−1 for any entry, then xi ≤ϵ(1+ϵ)
n
, else ∥Ax∥p is already larger than 1+ϵ. Ignoring
all such entries of x and rescaling can only change the objective by a 1 + O(ϵ) factor.
Lemma 110. In all iterations t of Algorithm 31, deﬁning Φt := ∥Awt∥p −∥wt∥1, Φt+1 ≤Φt.
Proof. Fix an iteration t. Deﬁne δ = ηgt, and note wt+1 = wt + δ ◦wt; henceforth in this proof, we
will drop subscripts t when clear. Observe that
∥Awt+1∥p = ∥A((1 + δ) ◦w)∥p =

X
j∈[d]
[Aw]p
j

1 + [A(δ ◦wt)]j
[Awt]j
p


1/p
.
As g ≤1 =⇒δ ≤p−11, A(δ◦wt)
Awt
≤p−1 entrywise. Via (1 + x)p ≤exp(px) ≤1 + px + p2x2 for
x ≤p−1, it follows that
∥A((1 + δ) ◦w)∥p ≤

X
j∈[d]
[Aw]p
j
 
1 + p[A(δ ◦w)]j
[Aw]j
+
p[A(δ ◦w)]j
[Aw]j
2!

1/p
.

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
736
By direct manipulation of the above quantity, and recalling we deﬁned v =
Aw
∥Aw∥p ,

X
j∈[d]

[Aw]p
j + p[Aw]p−1
j
[A(δ ◦w)]j + p2[Aw]p−2
j
[A(δ ◦w)]2
j



1/p
=

∥Aw∥p
p
X
j∈[d]

vp
j + pvp−1
j
[A(δ ◦w)]j
∥Aw∥p
+ p2vp−2
j
 
[A(δ ◦w)]j
∥Aw∥p
!2



1/p
= ∥Aw∥p

1 +
X
j∈[d]
 
pvp−1
j
[A(δ ◦w)]j
∥Aw∥p
+ p2vp−2
j
 
[A(δ ◦w)]j
∥Aw∥p
!!2

1/p
.
Using (1 + x)p > 1 + px, i.e. (1 + px)1/p < 1 + x, we thus obtain
∥A((1 + δ) ◦w)∥p ≤∥Aw∥p +

vp−1, A(δ ◦w)

+ p

vp−1, (A(δ ◦w))2
Aw

.
Cauchy-Schwarz yields that [A(δ ◦w)]2
j ≤[A(δ2 ◦w)]j[Aw]j, ∀j ∈[d]. Substituting into the above,
∥A((1 + δ) ◦w)∥p ≤∥Aw∥p +

vp−1, A(δ ◦w)

+ p

vp−1, A(δ2 ◦w)

= ∥Aw∥p +
X
j∈[d]

A⊤vp−1
j δjwj(1 + pδj).
(F.20)
Finally, to bound this latter quantity, since δ = ηg, we observe that for all j either δj = 0 or
1 + pδj = 1 + gj = 2 −[A⊤vp−1]j, in which case

A⊤vp−1
j (1 + pδj) =

A⊤vp−1
j

2 −

A⊤vp−1
j

≤1.
Thus, plugging this bound into (F.20) entrywise,
∥A((1 + δ) ◦w)∥p −∥Aw∥p ≤
X
j∈[d]
δjwj

A⊤vp−1
j (1 + pδj) ≤
X
j∈[d]
δjwj = ∥wt+1∥1 −∥wt∥1 .
Rearranging yields the desired claim.
F.3.2
Proofs from Section 7.5.3
Our analysis of Algorithm 32 will use the following helper fact.
Lemma 292 (Spectral bounds on {Ai}i∈[n]). Feasibility of Problem 4 is unaﬀected (up to constants
in ϵ) by removing matrices Ai with an eigenvalue larger than nϵ−1.
Proof. The proof is identical to Lemma 291; we also require the additional fact that the Schatten

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
737
norm ∥·∥p is monotone in the Loewner order, forcing the constraint xi ≤ϵ(1+ϵ)
n
.
We remark that we can perform this preprocessing procedure via power iteration on each Ai.
Lemma 111. In all iterations t of Algorithm 32, deﬁning Φt :=
P
i∈[n][wt]iAi

p −∥wt∥1, Φt+1 ≤
Φt.
Proof. Drop t and deﬁne δ = ηg. For simplicity, deﬁne the matrices
M0 :=
X
i∈[n]
wiAi, M1 :=
X
i∈[n]
δiwiAi, M2 :=
X
i∈[n]
δ2
i wiAi.
We recall the Lieb-Thirring inequality Tr((ABA)p) ≤Tr(A2pBp). Applying this, we have
∥M0 + M1∥p
p = Tr ((M0 + M1)p) ≤Tr

Mp
0

I + M
−1
2
0
M1M
−1
2
0
p
.
As g ≤1, we have M
−1
2
0
M1M
−1
2
0
⪯p−1I. Applying the bounds (I + M)p ⪯exp(pM) ⪯I + pM +
p2M2 for M = M
−1
2
0
M1M
−1
2
0
, where we use that I commutes with all M, it follows that
∥M0 + M1∥p
p ≤Tr

Mp
0 + pMp−1
0
M1 + p2Mp−1
0
M1M−1
0 M1

.
Deﬁnitions of M0, M1, M2, and preservation of positiveness under Schur complements imply

M0
M1
M1
M2

⪰0 =⇒M2 −M1M−1
0 M1 ⪰0.
Thus, M1M−1
0 M1 ⪯M2. Applying this and recalling V =
M0
∥M0∥p ,
∥M0 + M1∥p
p ≤Tr

Mp
0 + pMp−1
0
M1 + p2Mp−1
0
M2

= ∥M0∥p
p
 
1 + p
*
Vp−1,
M1
∥M0∥p
+
pM2
∥M0∥p
+!
.
By (1 + px)1/p < 1 + x, taking pth roots we thus have
∥M0 + M1∥p ≤∥M0∥p +

Vp−1, M1 + pM2

.
Finally, the conclusion follows as in Lemma 110; by linearity of trace and g = pδ,

Vp−1, M1 + pM2

=
X
i∈[n]

Vp−1, Ai

δiwi(1 + pδi) ≤
X
i∈[n]
δiwi.

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
738
Here, we used the inequality for all nonzero gi,

Vp−1, Ai

(1 + pδi) =

Vp−1, Ai
  2 −

Vp−1, Ai

≤1.
Theorem 48. Let p be odd. Algorithm 32 runs in O( p log(nd/ϵ)
ϵ
) iterations, and its output solves
Problem 4.
Each iteration is implementable in O(nnz · p log(nd/ϵ)
ϵ2
), where nnz is the number of
nonzero entries amongst all {Ai}i∈[n], losing O(ϵ) in the quality of Problem 4 with probability 1 −
poly((nd/ϵ)−1).
Proof. The proof is analogous to that of Theorem 47; we sketch the main diﬀerences here.
By
applying Lemma 292 and monotonicity of Schatten norms in the Loewner order, we again have
Φ0 ≤1, implying correctness whenever the algorithm terminates on Line 4. Correctness of dual
certiﬁcation again follows from lack of termination and the choice of T, as well as setting u to
indicate each coordinate.
Finally, the returned matrix in Line 8 is correct by convexity of the
Schatten-q norm, and the fact that all Vp−1
t
have unit Schatten-q norm.
We now discuss issues regearding computing gt in Line 5 of the algorithm, the bottleneck step;
these techniques are standard in the approximate SDP literature, and we defer a more formal
discussion to e.g. [285]. First, note that each coordinate of gt requires us to compute
1
P
i∈[n][wt]iAi

p−1
p
·
*
Ai,

X
i∈[n]
[wt]iAi


p−1+
.
(F.21)
We estimate the two quantities in the above expression each to 1 + ϵ multiplicative error with
high probability. Union bounding over iterations, and modifying Lemma 111 to use the potential
P
i∈[n][wt]iAi

p −(1 + O(ϵ)) ∥wt∥1, the analysis remains valid up to constants in ϵ with this
multiplicative approximation quality. We now discuss our approximation strategies.
For shorthand, denote M = P
i∈[n][wt]iAi. To estimate the denominator of (F.21), it suﬃces to
multiplicatively approximate ∥M∥p
p = Tr[Mp] within a 1 + ϵ factor, as raising to the p−1
p
power can
only improve this. To do so, we use the well-known fact (e.g. [160]) that letting Q be a k × d matrix
with independent entries ∼N(0, 1
k), for k = O( log( nd
ϵ )
ϵ2
), with probability 1 −poly(( nd
ϵ )−1),
Tr[Mp] ≈
X
ℓ∈[k]
Q⊤
ℓ:MpQℓ:
to a 1 + ϵ factor. To read this from the standard Johnson-Lindestrauss guarantee, it suﬃces to
factorize Mp and use that each row of the square root's ℓ2 norm is preserved with high probability
under multiplication by Q, and then apply the cyclic deﬁnition of trace. Similarly, for each i ∈[n],

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
739
we can approximate the numerators via
Tr

QM
p−1
2 AiM
p−1
2 Q⊤
.
We can simultaneously compute all such quantities by ﬁrst applying O(p) matrix-vector multi-
plications through M to each row of Q, and then computing all quadratic forms. In total, the
computational cost per iteration of all approximations is O(nnz · p log( nd
ϵ )
ϵ2
) as desired.
F.3.3
Proof of Proposition 30
In this section, following our prior developments, we prove the following claim.
Proposition 30. Following Theorem 48's notation, let p be odd, {Ai}i∈[n] ∈Sd
≥0, 0 < ϵ = O(α),
and
min
x∈∆n
∥x∥∞≤1+α
n
∥A(x)∥p = OPT.
(7.74)
for A(x) := P
i∈[n] xiAi. Given estimate of OPT exponentially bounded in nd
ϵ , there is a procedure
calling Algorithm 76 O(log nd
ϵ ) times giving x ∈∆n with ∥x∥∞≤(1+α)(1+ϵ)
n
, ∥A(x)∥p ≤(1 + ϵ)⋆.
Algorithm 76 runs in O( log(nd/ϵ) log n
ϵ2
) iterations, each implementable in time O(nnz · p log(nd/ϵ)
ϵ2
).
Reduction to a decision problem
Given access to an oracle for the following approximate decision problem, we can implement an
eﬃcient binary search for estimating ⋆. Speciﬁcally, letting the range of ⋆be (µlower, µupper), we can
subdivide the range into O( 1
ϵ log µupper
µlower ) multiplicative intervals of range 1 + ϵ, and then compute a
binary search using our decision oracle. This incurs a multiplicative log( nd
ϵ ) overhead in the setting
of Proposition 30 (see Appendix A, [285], for a more formal treatment).
Problem 5. Given {Ai}i∈[n] ∈Sd
≥0, either ﬁnd primal solution x ∈∆n with ∥A(x)∥p ≤1 + ϵ,
∥x∥∞≤(1+ϵ)(1+α)
n
, or conclude no x ∈∆n satisﬁes ∥A(x)∥p ≤1 −ϵ, ∥x∥∞≤(1−ϵ)(1+α)
n
.
The hard constraint ∥x∥∞≤1+α
n
in the deﬁnition (7.74) can be adjusted by constant factors to
admit the ℓ∞bound in Problem 5, since we assumed ϵ = O(α) is suﬃciently small.
Preliminaries
We use the shorthand S :=
n
1+αI, and p′ := log n
ϵ , so ℓp′ and ℓ∞are interchangeable up to 1 + O(ϵ)
factors. In other words, Problem 5 asks to certify whether there exists x ∈∆n with
max

∥A(x)∥p , ∥Sx∥p′

≤1,
(F.22)

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
740
up to multiplicative 1 + ϵ tolerance on either side. Consider the potential function
Φ(w) := log

exp

∥A(w)∥p

+ exp

∥Sw∥p′

−∥w∥1 .
(F.23)
It is clear that the ﬁrst term of Φ(w) approximates the left hand side of (F.22) up to a log 2 additive
factor, so if any of ∥A(w)∥p, ∥A(w)∥p′, or ∥w∥1 reaches the scale 3ϵ−1 and Φ(w) is bounded by 1,
we can safely terminate. and conclude primal feasibility for Problem 5. Next, we compute
∇iΦ(w) = 1 −
exp

∥A(w)∥p

⟨Ai, Y(w)⟩+ exp

∥Sw∥p′

[Sz(w)]i
exp

∥A(w)∥p

+ exp

∥Sw∥p′

for all i ∈[n],
where Y(w) :=
 
A(w)
∥A(w)∥p
!p−1
, z(w) :=
 
Sw
∥Sw∥p′
!p′−1
(F.24)
The following helper lemma will be useful in concluding dual infeasibility of Problem 5.
Lemma 293. In the setting of Problem 5, suppose there exists x∗∈∆n with
∥A(x∗)∥p ≤1 −ϵ, ∥Sx∗∥p′ ≤1 −ϵ.
Then, for any w,
⟨∇Φ(w), x∗⟩≥ϵ.
Proof. From the deﬁnitions in (F.24), it is clear that ∥Y(w)∥q = ∥z(w)∥q′ = 1, where q, q′ are the
dual norms of p, p′ respectively. Moreover, by the deﬁnition of x∗, we have for all ∥Y∥q = ∥z∥q′ = 1,
⟨Y, A(x)⟩≤1 −ϵ, ⟨z, Sx⟩≤1 −ϵ.
This follows from the dual deﬁnition of the ℓp norm. Now, note that for some nonnegative α(w),
β(w) summing to 1, using the above claim and (F.24),
⟨∇Φ(w), x∗⟩= 1 −(α(w) ⟨Y(w), A(x∗)⟩+ β(w) ⟨z(w), Sx∗⟩) ≥ϵ,
as desired (here, we used positivity of all relevant quantities).
Potential monotonicity
We prove a monotonicity property regarding the potential Φ in (F.23).
Lemma 294. Let w ∈Rn
≥0 satisfy ∥A(w)∥p ≤3ϵ−1, ∥Sw∥p′ ≤3ϵ−1, let g = max(0, ∇Φ(w))
entrywise, and let w′ = (1 + ηg) ◦w, where η = (4p′)−1. Then, Φ(w′) ≤Φ(w).

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
741
Proof. Denote for simplicity the threshold K = 3ϵ−1 and the step vector δ = ηg. First, by prior
calculations in Lemma 110 and Lemma 111, it follows that
∥A(w′)∥p ≤∥A(w)∥p + ∆A, ∥Sw′∥p′ ≤∥Sw∥p′ + ∆S,
where ∆A :=
X
i∈[n]
⟨Ai, Y(w)⟩δiwi(1 + pδi), ∆S :=
X
i∈[n]
[Sz(w)]iδiwi(1 + p′δi).
Next, note that by δ ≤η entrywise and lack of termination (i.e. the threshold K),
∆A ≤(1 + pη)η ⟨Y(w), A(w)⟩≤2η ∥A(w)∥p ≤1.
Therefore, by exp(x) ≤1 + x + x2 for x ≤1,
exp

∥A(w′)∥p

≤exp

∥A(w)∥p
  1 + ∆A + ∆2
A

.
(F.25)
Moreover, by applying Cauchy-Schwarz and the threshold ∥A(w)∥p ≤K once more,
∆2
A ≤(1 + pη)2

X
i∈[n]
⟨Ai, Y(w)⟩δiwi


2
≤2

X
i∈[n]
⟨Ai, Y(w)⟩δ2
i wi

⟨Y(w), A(w)⟩≤2K

X
i∈[n]
⟨Ai, Y(w)⟩δ2
i wi

.
(F.26)
Combining (F.25) and (F.26) (and applying similar reasoning to the term ∆S), we conclude
exp

∥A(w′)∥p

≤exp

∥A(w)∥p


1 +
X
i∈[n]
⟨Ai, Y(w)⟩δiwi(1 + (p + 2K)δi)

,
exp

∥Sw′∥p′

≤exp

∥Sw∥p′


1 +
X
i∈[n]
[Sz(w)]iδiwi(1 + (p′ + 2K)δi)

.
Recall the inequality log(1 + x) ≤x for nonnegative x. Expanding the deﬁnition of Φ and ∇Φ (cf.

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
742
(F.23)), and plugging in the above bounds, we conclude that
Φ(w′) −Φ(w) = log


exp

∥A(w′)∥p

+ exp

∥Sw′∥p′

exp

∥A(w)∥p

+ exp

∥Sw∥p′


−⟨δ, w⟩
≤
X
i∈[n]
(1 −∇iΦ(w))δiwi(1 + (p′ + 2K)δi) −⟨δ, w⟩
=
X
i∈[n]
((1 −∇iΦ(w))(1 + (p′ + 2K)δi) −1) δiwi.
As before, we show that this sum is entrywise nonpositive. For any i ∈[n] with δi ̸= 0, we have
(1 −∇iΦ(w))(1 + (p′ + 2K)δi) −1 = (1 −∇iΦ(w))(1 + (p′ + 2K)η∇iΦ(w)) −1
≤(1 −∇iΦ(w))(1 + ∇iΦ(w)) −1 ≤0,
as desired, where we used that η−1 ≥p′ + 2K. This yields the conclusion Φ(w′) ≤Φ(w).
Algorithm and analysis
Finally, we state Algorithm 76 and prove Proposition 30.
Algorithm 76: BoxedSchattenPacking({Ai}i∈[n], ϵ, p, α)
1 Input: {Ai}i∈[n] ∈Sd
≥0, ϵ ∈[0, 1
2], p ≥2, α ∈[0, n −1];
2 p′ ←log n
ϵ , S ←
n
1+αI;
3 η ←(4p′)−1, K ←3ϵ−1, T ←6 log( nd
ϵ )
ηϵ
;
4 [w0]i ←
ϵ
n2d for all i ∈[n], t ←0;
5 while ∥A(wt)∥p , ∥Swt∥p′ , ∥wt∥1 ≤K do
6
gt ←max (0, ∇Φ(wt)) entrywise, where we use the deﬁnition (F.23);
7
wt+1 ←wt ◦(1 + ηgt), t ←t + 1;
8
if t ≥T then
9
Return: Infeasible;
10 Return: x =
wt
∥wt∥1 ;
Proof of Proposition 30. Correctness of the reduction to deciding Problem 5 follows from the discus-
sion in Section F.3.3. Moreover, by the given Algorithm 76, it is clear (following e.g. the preprocessing
of Lemma 292) that Φ(wt) ≤1 throughout the algorithm, so whenever the algorithm terminates we
have primal feasibility. It suﬃces to prove that whenever the problem admits x∗with
∥A(x∗)∥p ≤1 −ϵ, ∥Sx∗∥p′ ≤1 −ϵ,

APPENDIX F. DEFERRED PROOFS FROM CHAPTER 7
743
then the algorithm terminates on Line 5 in T iterations. Analogously to Theorem 47, we have
η(1 −η)
X
0≤t<T
⟨gt, x∗⟩≤log n −log ∥w0∥1 + log ∥wT ∥1 ≤2 log
nd
ϵ

+ log ∥wT ∥1 .
Next, since gt is an upwards truncation of ∇Φ(wt), applying Lemma 293 implies that
∥wT ∥1 ≥exp
ηϵT
2
−2 log
nd
ϵ

.
The conclusion follows by the deﬁnition of T, as desired. Finally, the iteration complexity follows
analogously to the discussion in Theorem 48's proof, where the only expensive cost is estimating
coordinates of the A component of ∇Φ(wt) every iteration.
Finally, we remark that by opening up the dual certiﬁcates Y(w), Z(w) of our mirror descent
analysis, we can in fact implement a stronger version of the decision Problem 5 which returns
a feasible dual certiﬁcate whenever the primal problem is infeasible. We omit this extension for
brevity, as it is unnecessary for our applications, but it is analogous to the analysis of Theorem 48.

Appendix G
Deferred proofs from Chapter 8
G.1
List-decodable mean estimation for α−1 = Ω(d)
We give a simple algorithm for list-decodable mean estimation in the regime α−1 = Ω(d).
Algorithm 77: SamplePostProcess(T, δ)
1 Input: T ⊂Rd with |T| = n satisfying Assumption 13, α ≤
1
Cd for a universal constant C,
δ ∈(0, 1);
2 Output: L ⊂Rd with |L| ≤3
α satisfying (G.2) with probability ≥1 −δ;
3 N ←
l
36 log(2/δ)
α
m
;
4 eL ←{Xi}i∈[N], where each Xi is an independent uniform sample from T;
5 G ∈Rd×c ←entrywise ± 1
√c uniformly at random, for c = Θ(log( 1
αδ));
6 Let L be a maximal subset of eL such that for each Xi ∈L,
G⊤(Xi −Xj)
2
2 ≤8.8d for at
least αN
3
of the Xj ∈eL, and
G⊤(Xi −Xj)
2
2 ≥35.2d, ∀Xj ∈L;
7 L;
Proposition 67. Algorithm 77, SamplePostProcess, meets its output speciﬁcations in runtime
O
 1
α2 log4
 1
αδ

.
Proof. It is straightforward by Assumption 13 (cf. correctness proof of Theorem 89) that at least
αn
2 of the points Xi ∈T satisfy
∥Xi −µ∗∥2
2 ≤2d.
(G.1)
For each i ∈[N] indexing the set eL, let Ei be the event that Xi satisﬁes the bound (G.1); each
of these events is an independent Bernoulli variable with mean at least α
2 . Thus, by applying a
Chernoﬀbound, with probability at least 1 −δ
2, at least αN
3
of the points in eL satisfy (G.1). Next,
744

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
745
by the Johnson-Lindenstrauss lemma of [6], for a suﬃciently large dimensionality c, with probability
at least 1 −δ
2, all of the
G⊤(Xi −Xj)
2
2 are within a 1.1 factor of the corresponding ∥Xi −Xj∥2
2.
Condition on both of these events for the remainder of the proof.
By deﬁnition of the greedy process in Line 6, we have the output size guarantee, since each
element of eL is associated with a (disjoint) cluster of αN
3
points, by the separation property. So, for
correctness, it suﬃces to prove that (G.2) is met for a universal constant (depending on C). Call eS
the set of points in T satisfying (G.1). If any point in eS is chosen in L, then indeed
∥Xi −µ∗∥2
2 ≤2d ≤
2
Cα,
so (G.2) is met with constant
q
2
C . Further, observe that the only thing preventing any point in eS
from being chosen is the separation condition for L. This is because by triangle inequality and the
deﬁnition (G.1), any pair of points Xi, Xj ∈eS satisﬁes ∥Xi −Xj∥2
2 ≤8d, so after multiplication by
G⊤they pass the clustering requirement. Thus, suppose no point in eS is in L. For any Xi ∈eS ∪eL,
this implies there exists a Xj ∈eL with
G⊤(Xi −Xj)
2
2 ≤35.2d =⇒∥Xi −Xj∥2
2 ≤40d.
By triangle inequality, this implies that (G.2) is met with constant
q
84
C , via
∥Xj −µ∗∥2
2 ≤84d ≤84
Cα.
Finally, the runtime is dominated by the cost of multiplying all points in eL by G⊤, and performing all
pairwise distance comparisons of the {G⊤Xi}i∈[N]. Both of these ﬁt in the allotted time budget.
We make a ﬁnal remark that up to logarithmic factors, the runtime in Proposition 67 is not
larger than nd
α asymptotically, since we take sample size n ≥α−1. Thus, in the regime α−1 = Ω(d),
we obtain the correct list size and error bound up to constants, in time eO( nd
α ) as desired.
G.2
Filtering in k dimensions: SIFT
In this section, we develop a simple, polynomial-time algorithm for solving the list-decodable mean
estimation problem based on a "soft downweighting" approach. We outline some preliminary notions
and bounds used in our algorithms and analysis in Sections G.2.1 and G.2.2, which will also be
used in Sections G.3 and G.4. We then use these tools to analyze our "slow" algorithm, SIFT, in
Section G.2.3.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
746
G.2.1
General preliminaries
We will frequently use the following well-known facts throughout the chapter. In both, w ∈∆n is a
weight vector corresponding to a set of points T ⊆Rd.
Fact 38. We have that
0 ⪯
X
i∈T
wi(Xi −µw(T))(Xi −µw(T))⊤=⇒µw(T)µw(T)⊤⪯
X
i∈T
wi
∥w∥1
XiX⊤
i .
Thus, for any vector v ∈Rd,
(µw(T) −v)(µw(T) −v)⊤⪯
X
i∈T
wi
∥w∥1
(Xi −v)(Xi −v)⊤.
Fact 39. For any vector v ∈Rd,
X
i∈[n]
wi(Xi −v)(Xi −v)⊤=
X
i∈[n]
wi(Xi −µw(T))(Xi −µw(T))⊤+ ∥w∥1 (µw(T) −v)(µw(T) −v)⊤
⪰
X
i∈[n]
wi(Xi −µw(T))(Xi −µw(T))⊤.
In the list-decodable mean estimation problem, we are given a set T of n points {Xi}i∈T in Rd.1
For some known α ∈(0, 1
2], there is a subset S ⊆T of size αn such that all {Xi}i∈S are independent
draws from distribution D with mean µ∗, where the covariance of D is identity-bounded:
Ex∼D
h
(x −µ∗) (x −µ∗)⊤i
⪯I.
It is clear that by scaling the space, this assumption appropriately generalizes to the case when the
covariance bound is σ2I. The goal of list-decodable mean estimation is to output a list L, such that
one of the elements of the list is close to the "true mean" µ∗. Our aim will be to output a list of size
|L| = O( 1
α), which is necessary simply by identiﬁability of the subset S; it was shown as Proposition
5.4(ii) of [186] that for such a list size, the minimax optimal error for the problem scales as
min
µ∈L ∥µ −µ∗∥2 = Θ
 1
√α

.
(G.2)
Regarding the sample size n, we additionally recall the following (note in Assumption 13 that the
matrix of interest is not the covariance of S, as it is centered at the true mean µ∗).
Proposition 68 (Proposition B.1, [118]). For any constant ϵ ∈(0, 1), there are constants c, C > 0
1In an abuse of notation, we will both let T denote the set of points itself, as well as an index set for the points.
Correspondingly, we will interchangeably use Xi ∈T and i ∈T.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
747
such that with probability at least 1 −exp(−Ω(n)), for n = Cd
α , if an (1 + ϵ)α fraction of points in
{Xi}i∈T ⊆Rd is drawn from D with covariance bounded by cI, then Assumption 13 holds.
Assumption 13. There is a subset S ⊆{Xi}i∈T ⊆Rd of size αn = Θ(d) satisfying
1
|S|
X
i∈S
(Xi −µ∗) (Xi −µ∗)⊤⪯I.
In the remainder of Sections G.2, G.3, and G.4, we will operate under Assumption 13. We will
also explicitly assume that 1
α = o(d), and d ≤n = Θ( d
α), for simplicity. The latter assumption is
without loss of generality for any failure probability larger than exp(−Ω(d)); for any smaller failure
probability, Proposition 68 implies that the assumption still holds by adjusting the sample size by a
logarithmic factor. It is also fairly straightforward to see that the former assumption is also without
loss of generality, since in the case 1
α ≫d, it suﬃces to sample O( 1
α log 1
δ ) random points and apply
a variant of the post-processing procedure of Section G.4.1 to obtain the correct list size and error
guarantee; we give a formal treatment of this case in Section G.1.
Finally, throughout the variable k will be reserved for values which are Θ( 1
α) for explicitly stated
constants. In particular, many of our algorithms will rely on performing operations such as principal
components analysis in Θ( 1
α) dimensions. As discussed earlier, this is because a substantial portion
of the challenge in the estimation problem is reducing to the problem of learning the mean in Θ( 1
α)
dimensions, at which point na¨ıve random sampling solves the problem up to logarithmic factors.
G.2.2
Filtering preliminaries
We deﬁne two concepts which will be useful in stating guarantees of our downweighting methods.
Deﬁnition 56 (Saturated weights). We call weights w ∈∆n "saturated" if w ≤1
n1 entrywise, and
∥wS∥1 ≥α
q
∥w∥1.
Deﬁnition 57 (Safe scores). We call scores {τi}i∈T ∈Rn
≥0 "safe with respect to w ∈∆n" if
X
i∈S
wi
∥wS∥1
τi ≤1
2
X
i∈T
wi
∥w∥1
τi.
When the weights w are clear from context, we will simply call the scores τ "safe".
In algorithms based on soft ﬁltering in the presence of a small amount of adversarial noise (see
e.g. [177, 360, 492]), a typical goal is to remove more "good weight" than "bad weight" from an
iteratively updated weight vector. However, when the overwhelming majority of the initial weight is
bad, clearly this is too strong of a goal. The intuition for Deﬁnition 56 is that a weaker goal suﬃces
for the guarantees of our methods; while the amount of good weight is decreasing throughout,

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
748
Deﬁnition 56 requires that the good weight becomes more saturated in the weight vector when more
weight is removed. We now make the connection between these deﬁnitions formal.
Lemma 295. Consider a set of saturated (cf. Deﬁnition 56) weights w(0), and updates of the form:
1. For 0 ≤t < N:
(a) Let
n
τ (t)
i
o
i∈T be safe (cf. Deﬁnition 57) with respect to w(t).
(b) Update for all i ∈T:
w(t+1)
i
←
 
1 −τ (t)
i
τ (t)
max
!
w(t)
i , where τ (t)
max :=
max
i∈T |w(t)
i
̸=0
τ (t)
i
.
(G.3)
Then, the result of the updates w(N) is also saturated.
Proof. First, ﬁx some iteration t, and let w := w(t), τ := τ (t), and w′ := w(t+1). Deﬁne
δS :=
X
i∈S
wi −w′
i
∥wS∥1
, δT :=
X
i∈T
wi −w′
i
∥w∥1
.
Note that by the assumption that τ is safe and the iteration (G.3),
δS =
1
τmax
X
i∈S
wi
∥wS∥1
τi ≤
1
2τmax
X
i∈T
wi
∥w∥1
τi = 1
2δT .
Hence, using 1 −1
2δT ≥√1 −δT for all δT ∈[0, 1], we have
w(t+1)
S

1
w(t)
S

1
= 1 −δS ≥
p
1 −δT =
v
u
u
u
t
w(t+1)
T

1
w(t)
T

1
.
(G.4)
Inductively telescoping (G.4), using that w(0) was assumed to be saturated, and ﬁnally comparing
with Deﬁnition 56, yields the desired conclusion that w(N) is saturated.
We next give three helper lemmas which help reason about how the quality of empirical estimates
based on S deteriorate, as the amount of weight allocated to S is reduced. The ﬁrst shows how the
quality of the empirical mean is related to the empirical covariance and proportion of weight in S
(and is essentially a rephrasing of Fact A.3 in [136]).
Lemma 296. Let w ∈∆n have w ≤1
n1 entrywise. Then,
∥µw(T) −µ∗∥2 ≤
s
2 ∥Covw(T)∥op
∥w∥1
∥wS∥1
+
2α
∥w∥1
.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
749
Proof. Let w∗∈∆n be the weight vector which is
1
|S| on coordinates in S, and zero elsewhere. Note
that by deﬁnition ⟨w, w∗⟩= ∥wS∥1
αn . Next,
∥µw(T) −µ∗∥2
2 = max
∥u∥2=1
* X
i∈T
wiw∗
i
⟨w, w∗⟩(Xi −µw(T))
!
−
 X
i∈T
wiw∗
i
⟨w, w∗⟩(Xi −µ∗)
!
, u
+2
≤2 max
∥u∥2=1
*X
i∈T
wiw∗
i
⟨w, w∗⟩(Xi −µw(T)), u
+2
+ 2 max
∥u∥2=1
*X
i∈T
wiw∗
i
⟨w, w∗⟩(Xi −µ∗), u
+2
.
We bound these two terms separately. First, by applying a quadratic form in u to Fact 38,
max
∥u∥2=1
*X
i∈T
wiw∗
i
⟨w, w∗⟩(Xi −µw(T)), u
+2
≤max
∥u∥2=1
X
i∈T
wiw∗
i
⟨w, w∗⟩⟨Xi −µw(T), u⟩2
=
∥w∥1
αn ⟨w, w∗⟩max
∥u∥2=1
X
i∈T
wi
∥w∥1
⟨Xi −µw(T), u⟩2
= ∥Covw(T)∥op
∥w∥1
∥wS∥1
.
Next, by again applying Fact 38, and recalling Assumption 13,
max
∥u∥2=1
*X
i∈T
wiw∗
i
⟨w, w∗⟩(Xi −µ∗), u
+2
≤max
∥u∥2=1
X
i∈T
wiw∗
i
⟨w, w∗⟩⟨Xi −µ∗, u⟩2
≤∥w∥∞
⟨w, w∗⟩max
∥u∥2=1
X
i∈T
w∗
i ⟨Xi −µ∗, u⟩2 ≤αn ∥w∥∞
∥w∥1
.
The second shows how the empirical covariance of S grows relative to how much of S is kept.
Lemma 297. Let w ∈∆n have w ≤1
n1 entrywise. Then Covw(S) ⪯
α
∥wS∥1 I.
Proof. For any vector u with ∥u∥2 = 1,
u⊤Covw(S)u =
X
i∈S
wi
∥wS∥1
⟨u, Xi −µw(S)⟩2
≤
X
i∈S
αw∗
i
∥wS∥1
⟨u, Xi −µ∗⟩2 ≤
α
∥wS∥1

X
i∈S
w∗
i (Xi −µ∗)(Xi −µ∗)⊤

op
.
In the second line we used Fact 39. Using Assumption 13 yields the conclusion.
The third shows how a bound on the saturation of S in a weight vector can be used to bound
the distance between empirical means in S and T via the empirical covariance matrix.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
750
Lemma 298. We have that
(µw(S) −µw(T))(µw(S) −µw(T))⊤⪯∥w∥1
∥wS∥1
Covw(T).
Proof. This follows from the following observations (via Fact 38)
(µw(S) −µw(T))(µw(S) −µw(T))⊤⪯
X
i∈S
wi
∥wS∥1
(Xi −µw(T))(Xi −µw(T))⊤
⪯∥w∥1
∥wS∥1
X
i∈T
wi
∥w∥1
(Xi −µw(T))(Xi −µw(T))⊤
= ∥w∥1
∥wS∥1
Covw(T).
G.2.3
Analysis of SIFT
We now present SIFT as Algorithm 78. It requires calls to an approximate k-PCA subroutine Power,
the classical simultaneous power iteration method, which is stated as Algorithm 26 in Section 7.3.3,
where we present an improved analysis of its guarantees. However, for analysis in this section it
suﬃces to use the following guarantee. For simplicity in this section we drop the arguments λmax
and λmin as inputs to Power, which do not play a role in Proposition 69.
Proposition 69 (Theorem 1, [404]). For any δ ∈(0, 1) and k ∈[d], there is an algorithm, Power,
which takes as input k, δ, A ∈Sd
≥0 and ϵ ∈(0, 1), and returns with probability 1 −δ a set of
orthonormal vectors V ∈Rd×k such that if V:i is column i of V,
⟨V:i, AV:i⟩∈[1 −ϵ, 1 + ϵ] λi (A) for all i ∈[k],
and
 I −VV⊤
A
 I −VV⊤
op ≤(1 + ϵ)λk+1 (A) .
When A is given in the form M⊤M for some M ∈Rn×d, the runtime of Power is
O
ndk
ϵ
log
 d
δϵ

.
Note that Lines 6 through 10 of Algorithm 78 exactly constitute a weight removal method of the
form given in Lemma 295. Consequently, to use Lemma 295 it suﬃces to prove that the weights τi
used in each iteration are safe with respect to the current set of weights, which we now demonstrate.
Lemma 299. In each iteration t of Algorithm 78 until termination, τ (t) is safe with respect to w(t).
Proof. Throughout this proof, let w := w(t) and τ := τ (t). Furthermore, let V, Σ, and β correspond
to the weights w at the iteration's start. We will inductively prove that τ is safe with respect to w,

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
751
Algorithm 78: SIFT(T, δ)
1 Input: T ⊂Rd with |T| = n satisfying Assumption 13, δ ∈(0, 1);
2 w(0) ←1
n1T , t ←0, β ←1, k ←⌈4
α⌉;
3 V ←Power(Covw(t)(T), k, 0.2, δ
2n);
4 Σ ←V⊤Covw(t)(T)V;
5 while λk(Σ) ≥
4
√β do
6
τ (t)
i
←
Σ−1
2 V⊤(Xi −µw(T))

2
2 for all i ∈T;
7
w(t+1)
i
←

1 −τ (t)
i
τ (t)
max

w(t)
i
for all i ∈T, where τ (t)
max := maxi∈T |w(t)
i
̸=0 τ (t)
i
;
8
t ←t + 1, β ←
w(t)
1;
9
V ←Power(Covw(t)(T), k, 0.2, δ
2n);
10
Σ ←V⊤Covw(t)(T)V;
11 Return:
L := {VV⊤Xi +
 I −VV⊤
µw(t)(T) where i ∈T is sampled uniformly at random}, with
list size |L| = ⌈2
α log 2
δ ⌉
which by applying Lemma 295 implies that at the start of the iteration, w is saturated (since clearly
w(0) is saturated). We ﬁrst compute the average score in S:
X
i∈S
wi
∥wS∥1
τi =
X
i∈S
wi
∥wS∥1
Σ−1
2 V⊤(Xi −µw(T))

2
2
=
X
i∈S
wi
∥wS∥1
Σ−1
2 V⊤(Xi −µw(S))

2
2 +
Σ−1
2 V⊤(µw(S) −µw(T))

2
2

=

Σ−1, V⊤Covw(S)V

+
Σ−1
2 V⊤(µw(S) −µw(T))

2
2
≤

Σ−1,
α
∥wS∥1
I

+ ∥w∥1
∥wS∥1
≤1
4
p
βI, 1
√β I

+
√β
α
≤k
2.
The ﬁrst three equalities follow by expanding deﬁnitions; the ﬁrst inequality is by Lemmas 297
and 298, as well as the deﬁnition of Σ. The second inequality is by using the deﬁnition of saturated
weights (Deﬁnition 56) twice, which implies that ∥wS∥1 ≥α√β, as well as the exit condition in Line
5. The third inequality follows from the deﬁnition of k. Finally, we conclude that τ is indeed safe,
since the average score in T is exactly k by design:
X
i∈T
wi
∥w∥1
τi =
X
i∈T
wi
∥w∥1
Σ−1
2 V⊤(Xi −µw(T))

2
2
=
*
Σ−1, V⊤
 X
i∈T
wi
∥w∥1
(Xi −µw(T)) (Xi −µw(T))⊤
!
V
+
=

Σ−1, Σ

= k.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
752
Finally, we prove a runtime and correctness guarantee on Algorithm 78.
Theorem 89. Under Assumption 13, with probability 1 −δ, the output of Algorithm 78 satisﬁes
min
µ∈L ∥µ −µ∗∥2
2 ≤22
α .
The overall runtime of Algorithm 78 is
O

n2dk log
d
δ

.
Proof. We will show correctness and complexity of Algorithm 78 separately.
Complexity guarantee. It is clear that there are at most n iterations in Algorithm 78, since at least
one weight is zeroed out in Line 7 each iteration. Further, the bottleneck operation in each iteration
is clearly the complexity of Power, since an eigendecomposition of Σ takes time O(k3) = O(ndk).
Since ϵ is a constant in Proposition 69 and n = O(d2), this yields the complexity bound. Using a
union bound, with probability 1 −δ
2, the conclusion of Proposition 69 applies in every iteration; we
will condition on this event for the remainder of the proof.
We ﬁnally note that the algorithm must terminate the while loop before removing all the weight.
This is because throughout the algorithm since w is saturated (by Lemmas 295 and 299), ∥w∥1 ≥α2
holds directly by using Deﬁnition 56 and ∥w∥1 ≥∥wS∥1.
Correctness guarantee. As in Lemma 299, we let w denote the weights on the last iteration of
the algorithm (after exiting on Line 12). Denote P := VV⊤and Yi := PXi for all i ∈T. Since
X
i∈S
1
αn (Yi −Pµ∗) (Yi −Pµ∗)⊤= P
 X
i∈S
1
αn (Xi −µ∗) (Xi −µ∗)⊤
!
P ⪯P,
by Assumption 13, the expectation of ∥Yi −Pµ∗∥2
2 for a uniformly random sample i ∈S is 4
α by lin-
earity of trace. Hence, by Markov with probability at least 1
2 a sample from S has ∥Yi −Pµ∗∥2
2 ≤8
α,
so with probability at least 1−δ
2, one of the random samples in L will have an Xi with ∥Yi −Pµ∗∥2
2 ≤
8
α. For this value of i, we expand via the Pythagorean theorem
∥(PXi + (I −P) µw(T)) −µ∗∥2
2 = ∥Yi −Pµ∗∥2
2 + ∥(I −P) (µw(T) −µ∗)∥2
2
≤8
α + ∥(I −P) (µw(T) −µ∗)∥2
2 .

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
753
To bound this second term, we apply Lemma 296 on the set of points {(I −P)Xi}i∈T . This implies
∥(I −P) (µw(T) −µ∗)∥2
2 ≤
2β
∥wS∥1
∥(I −P)Covw(T)(I −P)∥op + 2α
β
≤12√β
∥wS∥1
+ 2α
β ≤14
α .
Here, the last inequality used the deﬁnition of saturation, which also implies that β ≥α2. The
second inequality used that the guarantees of Power and the termination condition imply that
∥(I −P) Covw(T) (I −P)∥op ≤1.2λk(Covw(T)) ≤1.5λk(Σ) ≤
6
√β .
Here, we use that the eigenvalues of Σ are the same as those of VV⊤Covw(T)VV⊤; this calculation is
given in the correctness proof of Proposition 25. Combining the above bounds yields the conclusion.
While Theorem 89 achieves the desired error guarantee (G.2), it unfortunately has a quadratic
dependence on the sample complexity n, as well as a suboptimal list size by a factor of O(log 1
δ ).
We address the latter issue with a post-processing step in Section G.4.1; regarding the former issue,
Algorithm 78 will play a role in our ﬁnal "fast" algorithm in the following Section G.3, which obtains
a runtime with a linear dependence on n via more sophisticated weight removal.
G.3
Fast ﬁltering in k dimensions under a diameter bound
We now give an algorithm, FastSIFT, with an improved dependence on the sample size n compared
to the method SIFT developed in Section G.2. We use the following assumption in this section.
Assumption 14. All data points in T lie in a Euclidean ball of radius R.
We eventually show how to reduce the more general mean estimation problem to mean estimation
on datasets satisfying Assumption 14 in Section G.4.2 to obtain our ﬁnal algorithm. The primary
goal of this section is to develop a method for quickly ﬁnding a "good" tuple (B, w), deﬁned as
follows.
Deﬁnition 58 (Good tuple). We call (B, w) "good" if it obeys the following conditions.
1. B ∈Rd×k′ has orthogonal columns, for some k′ = O( log R
α ), and w ∈∆n is saturated.
2. Let PB := BB⊤. The restriction of Covw(T) to the complement of PB, denoted by
CovP⊥
B
w (T) := (I −PB) Covw(T) (I −PB)

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
754
satisﬁes for a universal constant c,
CovP⊥
B
w (T)

op ≤
c
p
∥w∥1
.
Intuitively, a good tuple signiﬁes that in all but O( log R
α ) dimensions, we have learned the mean
via the guarantee of Lemma 296. However, in the remaining dimensions we can simply run the
algorithm of Section G.2, which obtains an additive poly(k) runtime dependence. We now make this
rigorous.
Algorithm 79: FastSIFT(T, δ, ProduceGoodTuple)
1 Input: T = Tfast ∪Tslow ⊂Rd with |Tfast| = n satisfying Assumptions 13 and 14,
|Tslow| = O( log R
α2 ) satisfying Assumption 13 for a ﬁxed O( log R
α )-dimensional subspace,
δ ∈(0, 1), subroutine ProduceGoodTuple which returns a good tuple with speciﬁed failure
probability;
2 (B, w) ←ProduceGoodTuple(Tfast, δ
2);
3 µfast ←(I −BB⊤)µw(T);
4 Lslow ←SIFT({BB⊤Xi | Xi ∈Tslow}, δ
2);
5 Return: L ←{µslow + µfast | µslow ∈Lslow}
Lemma 300. With probability 1 −δ, some ˆµ ∈L outputted by Algorithm 79 satisﬁes
∥ˆµ −µ∗∥2
2 ≤48 + 4c
α
.
The overall runtime of Algorithm 79 is the cost of running ProduceGoodTuple(Tfast, δ
2) plus
O
 d
α3 log(R) log
dR
δ

+ 1
α6 log3(R) log
d
δ

additional runtime overhead.
Proof. By the proof of Theorem 89 and the second part of Deﬁnition 58, it is immediate that
∥(I −PB)(µw(T) −µ∗)∥2
2 ≤2c + 2
α
.
Moreover, since the size of Tslow is large enough for Proposition 68 to apply, it satisﬁes Assumption 13
on the k′-dimensional subspace whose projection matrix is PB = BB⊤. Thus, Theorem 89 shows
∥µslow −PBµ∗∥2
2 ≤22
α for some µslow ∈Lslow.
Combining these two bounds and the Pythagorean theorem yields the correctness guarantee. For
the runtime overhead guarantee, it is clear the bottleneck operation is Line 4 since Line 3 can be
implemented in time O( d
α log R). For Line 4, we run Algorithm 78 entirely in the coordinate system

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
755
of the columns of B, which is isomorphic to Rk′, and then left-multiply the resulting list by B.
Forming the input set {B⊤Xi | Xi ∈Tslow} takes time O(|Tslow|k′d) = O( d
α3 log2 R); multiplying
the resulting output list by B cannot be the dominant cost by more than a log 1
δ factor.
Here, we note that because we take n = Ω(dα−1) = Ω(α−2) in accordance with Proposition 68,
the cost of O(dα−3 log R log dR
δ ) incurred by Lemma 300 is no more than the cost of logarithmically
many k-PCAs on the original dataset. Regarding the separation of the original dataset into Tfast
and Tslow, which appropriately satisfy Assumption 13, we make the following comment.
Remark 14. We can form a partitioned dataset T = Tfast∪Tslow of the form required by Algorithm 79
by independently drawing n samples to form Tfast, O( log R
α2 ) samples to form Tslow, and applying
Assumption 13 to Tfast and the projection of Tslow into a k′-dimensional subspace. Up to a log 1
δ
factor in the sample complexity (for error probabilities which are smaller than exp(−Ω(α−1))), these
are valid applications of Assumption 13 because of independence; in particular, the draws Tslow are
independent of the k′-dimensional subspace learned by running ProduceGoodTuple on Tfast, which
only depends on randomness used in Step 2 of FastSIFT.
We now state our strategy for the implementation of ProduceGoodTuple.
Roughly speaking,
ProduceGoodTuple is a composition of three subroutines at diﬀerent levels, named BicriteriaFilter,
DecreaseKFNorm, and KFMMW. Each subroutine is associated with one or more potential functions
which show that the subroutine "one level down" is called O(log d) times.
1. ProduceGoodTuple iteratively calls BicriteriaFilter, an algorithm which takes as input saturated
weights w and either produces saturated weights ∥w′∥1 ≤1
2 ∥w∥1, or a good tuple.
2. BicriteriaFilter iteratively calls DecreaseKFNorm, an algorithm which takes as input saturated
weights w and maintains an updated set of orthogonal vectors B. Each call to DecreaseKFNorm
either (1) halves the ℓ1 norm of w, (2) halves the Ky Fan k norm of the covariance matrix,
or (3) decreases the operator norm of the covariance matrix by a constant factor and adds k
vectors to B, for some k = Θ( 1
α).
3. DecreaseKFNorm is based on a "win-win-win" analysis of the ﬁne-grained guarantees of a Ky
Fan norm matrix multiplicative weights procedure, developed in Section 7.3. We will show
that in O(log d) iterations of KFMMW, either the Ky Fan k norm has halved, or one of the
other two "exit conditions" required by DecreaseKFNorm has been certiﬁably met.
Given the guarantees of DecreaseKFNorm, correctness of ProduceGoodTuple and BicriteriaFilter
follow straightforwardly. Thus, in Section G.3.1, we state and prove a performance guarantee on
DecreaseKFNorm, which we use to give a simple analysis of ProduceGoodTuple in Section G.3.2. Com-
bining our analysis of ProduceGoodTuple with Lemma 300 gives the main export from this section.
Finally, we note that in the following development of ProduceGoodTuple and its subroutines, we will
overload the input set T to be Tfast in Algorithm 79, because it is the input to ProduceGoodTuple.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
756
G.3.1
Analysis of DecreaseKFNorm
We ﬁrst state a guarantee for ApproxKFMMW as Proposition 70, which is a computationally eﬃcient
variant of KFMMW (these methods are both given and analyzed in Section 7.3). Proposition 70 is
a restatement of Corollary 26 and Lemma 95 from Section 7.3 with ∆=
1
200.
Proposition 70. There is an algorithm, ApproxKFMMW (Algorithm 28), which takes as input a
sequence of matrices {Gt}t≥0 ⊂Sd
≥0 each in the form M⊤
t Mt for Mt ∈Rn×d for explicitly given
Mt, and k ∈[d]. Suppose that the matrices {Gt}t≥0 are weakly decreasing in Loewner order, and let
η ≤
1
2∥G0∥op . For any N ≥1, with probability 1 −δ′, ApproxKFMMW deﬁnes a sequence of matrices
{ bYt}0≤t<N, where bYt only depends on {Gs}0≤s<t, such that
∥GN∥k ≤2
T
N−1
X
t=0
D
Gt, bYt
E
+ k log d
ηN
+
k
200η .
Each bYt satisﬁes
 bYt

op ≤1.01 and
 bYt

tr ≤1.01k. The cost of the algorithm is
O

ndkN 2 log2
dN
δ′

.
Furthermore, for any set of n ﬁxed vectors {vi}i∈[n] ⊂Rd and any iteration t, 1.05-approximations
to all v⊤
i bYtvi can be computed in time
O

ndN log
nd
δ′

with probability at least 1 −δ′.
We are now ready to state the algorithm DecreaseKFNorm as Algorithm 80. At a high level, the
goal of DecreaseKFNorm is to implement Proposition 70 in a way so that each of the inner products
D
Gt, bYt
E
is suﬃciently small, via decreasing weights deﬁned in terms of the matrix bYt. We will
be able to successfully do this as long as the ℓ1 norm of the weight remains stable, and the top
eigenvalue of the covariance matrix is not too much larger than the kth largest. When either of these
conditions fail, we will exit the algorithm via a diﬀerent termination condition.
The ﬁrst step in the analysis of Algorithm 80 is to guarantee that any time a weight removal
procedure is performed, it is with respect to safe scores, and hence the weights remain saturated
throughout the course of the algorithm. We give this proof of safe weight removal as Lemma 301,
and then an overall correctness and runtime guarantee in Proposition 71.
Lemma 301. Throughout the course of Algorithm 80, any time weight removal is performed in Line
12, it is with respect to safe scores, and thus w(t) is saturated for all 0 ≤t < N.
Proof. With probability 1 −δ, all executions of Lines 5 and 10 throughout the algorithm succeed,
so we will condition on this event for the remainder of this proof. We also note that in any iteration

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
757
Algorithm 80: DecreaseKFNorm(T, w, γ, δ)
1 Input: T ⊂Rd with |T| = n satisfying Assumptions 13 and 14, saturated w, γ ←
1.05-approximation to ∥Covw(T)∥k with probability at least 1 −
δ
3(N+1) for k := ⌈612
α ⌉
satisfying γ ≥
110k
√
∥w∥1 , δ ∈(0, 1);
2 Output: Saturated w′, satisfying one of the following possibilities with probability ≥1 −δ:
w′ has ∥w′∥1 ≤1
2 ∥w∥1 (marked "Case 1"), V ∈Rd×k is also outputted, and
CovP⊥
V
w′ (T)

op ≤2
3 ∥Covw(T)∥op (marked "Case 2"), w′ has ∥Covw′(T)∥k ≤1
2 ∥Covw(T)∥k
(marked "Case 3");
3 N ←⌈425 log d⌉, w(0) ←w, ¯β ←
w(0)
1, η ←
1
2.1ρ, where ρ is a 1.05-approximation of
g
Covw(0)(T)

op with probability at least 1 −
δ
3(N+1);
4 for 0 ≤t < N do
5
V ←Power(Covw(t)(T), k, 0.05,
δ
3(N+1));
6
˜λ1 ←⟨V:1, Covw(t)(T)V:1⟩, ˜λk ←⟨V:k, Covw(t)(T)V:k⟩;
7
if ˜λ1 ≥3.5˜λk then
8
Return: (w(t), V, "Case 2");
9
τ (t)
i
←1.05-approximation to
D
(Xi −µw(t)(T)), bYt(Xi −µw(t)(T))
E
for all i ∈T, with
probability at least 1 −
δ
3(N+1);
10
if P
i∈T w(t)
i τ (t)
i
> γ ¯β
12 then
11
w(t+1) ←w(t,K), where K ←smallest natural number such that
either
w(t,K)
1 ≤
¯β
2 , or
X
i∈T
w(t,K)
i
τ (t)
i
≤γ ¯β
12 ,
where w(t,K)
i
:=
 
1 −τ (t)
i
τ (t)
max
!K
w(t)
i , and τ (t)
max :=
max
i∈T |w(t)
i
̸=0
τ (t)
i
(G.5)
;
12
if
w(t+1)
1 ≤
¯β
2 then
13
Return: (w(t+1), "Case 1");
14
else
15
w(t+1) ←w(t);
16
Feed Gt ←g
Covw(t+1)(T) into the routine ApproxKFMMW with step size η and δ′ ←δ
3;
17 Return: (w(N), "Case 3");
t where Line 12 is reached, Line 7 did not pass, and thus
λ1 (Covw(t)(T)) ≤1.05˜λ1 < 3.675˜λk ≤4λk (Covw(t)(T)) =⇒∥Covw(t)(T)∥k ≥k
4 ∥Covw(t)(T)∥op
(G.6)
by the guarantees of Power in Proposition 69. Consider now a single iteration 0 ≤t < N, and

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
758
suppose inductively that w(t) is saturated before Line 12 is executed. In every round of weight
removal 0 ≤ℓ< K, assuming that the ℓ1 norm has not halved, we can lower bound the average
score in T by the deﬁnition of K:
X
i∈T
w(t,ℓ)
i
w(t,ℓ)
1
τ (t)
i
≥1
¯β
X
i∈T
w(t,ℓ)
i
τ (t)
i
≥γ
12.
Hence, to prove that the scores are safe in iteration ℓ, it suﬃces to show that the average score in
S is at most
γ
24. Because the weights w(t,ℓ) are monotone in ℓ, and the ℓ1 norm of w(t,ℓ)
S
inductively
does not change by more than a factor of
√
2 by the following Lemma 301, it suﬃces to show that
X
i∈S
w(t,0)
i
w(t,0)
S

1
τ (t)
i
≤γ
34 =⇒
X
i∈S
w(t,ℓ)
i
w(t,ℓ)
S

1
τ (t)
i
≤γ
√
2
34
< γ
24.
We now prove this bound on the average score in S with respect to w(t,0) = w(t), which will conclude
the proof. To see this bound, we have
X
i∈S
w(t)
i
w(t)
S

1
τ (t)
i
≤1.05
*
bYt,
X
i∈S
w(t)
i
w(t)
S

1
(Xi −µw(t)(T)) (Xi −µw(t)(T))⊤
+
= 1.05
D
bYt, Covw(t)(S)
E
+ 1.05
D
bYt, (µw(t)(S) −µw(t)(T)) (µw(t)(S) −µw(t)(T))⊤E
≤1.07k ∥Covw(t)(S)∥op + 1.07
(µw(t)(S) −µw(t)(T)) (µw(t)(S) −µw(t)(T))⊤
op
≤1.07kα
w(t)
S

1
+ 9γ
kα ≤1.6k
p¯β
+ 9γ
kα ≤γ
34.
Here, the ﬁrst inequality is by the approximation guarantees on the scores τ (t)
i
. The second inequality
used matrix H¨older twice, as well as trace and operator norm bounds on bYt due to Proposition 70,
and ﬁnally the fact that the trace and operator norm agree for any rank-1 matrix.
The fourth
inequality is by the helper Lemma 301 and saturation of w(0), and the ﬁfth is by our choices of k
and lower bound on γ ≥110k
√¯β . The third inequality used Lemmas 297 and 298, the latter of which
implies
(µw(t)(S) −µw(t)(T)) (µw(t)(S) −µw(t)(T))⊤
op ≤
w(t)
1
w(t)
S

1
∥Covw(t)(T)∥op
≤1
α · 4
k ∥Covw(t)(T)∥k ≤8.4γ
kα .
The second inequality used our assumption (G.6), and the last used that g
Covw(t)(T) is monotonically
decreasing in the Loewner order, and thus since until termination, the normalization factor
w(t)
1

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
759
does not change by more than a factor of two, and γ is a 1.05-approximation to ∥Covw(0)(T)∥k,
∥Covw(t)(T)∥k =
1
w(t)
1
g
Covw(t)(T)

k ≤2
¯β
g
Covw(0)(T)

k ≤2.1γ.
In proving Lemma 301, we used the following helper lemma.
Lemma 302. Consider any algorithm of the form in Lemma 295. Suppose in some iteration t,
w(t)
1 ≥1
2
w(0)
1. Then,
w(t)
S

1 ≥
1
√
2
w(0)
S

1.
Proof. This is immediate from telescoping (G.4), which was used in the proof of Lemma 295.
Finally, we prove overall correctness of Algorithm 80.
Proposition 71. Algorithm 80 succeeds with probability at least 1 −δ, in the sense that each of
Cases 1-3 returns correctly. The overall complexity is bounded by
O

ndk log2(d) log2
dR
δ

.
Proof. We will show correctness and complexity of Algorithm 80 separately.
Correctness guarantee.
As argued in the proof of Lemma 301, with probability 1 −δ every
weight removal is safe, so Lemma 301 shows that w(t) is saturated throughout the algorithm. By
a union bound, we also assume that all approximations are correct in the remainder of the proof.
It is obvious that if the algorithm terminates in Line 14, the requirement of Case 1 is met. If the
algorithm terminates in Line 8, the guarantees of Power (Proposition 69) imply that
λ1 (Covw(t)(T)) ≥
1
1.05
˜λ1 ≥3.5
1.05
˜λk ≥
3.5
1.052 λk (Covw(t)(T))
≥
3.5
1.053
(I −VV⊤)Covw(t)(T)(I −VV⊤)

op ≥3
CovP⊥
V
w(t)(T)

op .
(G.7)
However, since the algorithm did not terminate on Line 14 in the previous iteration, we also have
λ1 (Covw(t)(T)) =
1
w(t)
1
λ1

g
Covw(t)(T)

≤2
¯β λ1

g
Covw(0)(T)

= 2λ1 (Covw(0)(T)) .
Combining the above two calculations gives the correctness proof for Case 2, as
CovP⊥
V
w(t)(T)

op ≤1
3λ1 (Covw(t)(T)) ≤2
3λ1 (Covw(0)(T)) .
Finally, we show correctness in Case 3, where N iterations of the algorithm have passed without
terminating on either of Lines 8 (which halves operator norm) or 14 (which halves weight).
In

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
760
this case, we apply Proposition 70, which is valid since the Gt are monotonically decreasing, and
ηG0 ⪯1
2I by the approximation guarantee on ρ. Here, we also note that all our matrices Gt are
covariance matrices with known weights, so they can be expressed in the form M⊤
t Mt for explicitly
given Mt ∈Rn×d. Proposition 70 additionally requires a bound on each
D
Gt, bYt
E
; to this end,
D
Gt, bYt
E
=
X
i∈T
w(t+1)
i
D
(Xi −µw(t+1)(T)) , bYt (Xi −µw(t+1)(T))
E
≤
X
i∈T
w(t+1)
i
D
(Xi −µw(t)(T)) , bYt (Xi −µw(t)(T))
E
≤1.05
X
i∈T
w(t+1)
i
τ (t)
i
≤1.05γ ¯β
12
.
In the ﬁrst inequality, we used Fact 39; in the second, we used the assumption on the scores τ (t);
and in the third, we used the second guarantee in (G.5) since we did not terminate on Line 14. Now,
applying this bound in every iteration 0 ≤t < N in Proposition 70, and deﬁning GN = GN−1,
∥GN∥k ≤1.05γ ¯β
6
+ 2.1kρ log d
N
+ 2.1kρ
200
≤1.05γ ¯β
6
+
2.21k
g
Covw(0)(T)

op log d
N
+
2.21k
g
Covw(0)(T)

op
200
≤1.05γ ¯β
6
+
9
g
Covw(0)(T)

k log d
N
+
9
g
Covw(0)(T)

k
200
≤1.052 ∥Covw(0)(T)∥k ¯β
6
+
9
g
Covw(0)(T)

k log d
N
+
9
g
Covw(0)(T)

k
200
.
The ﬁrst inequality was by Proposition 70 and the deﬁnition of η; the second was by the approxi-
mation guarantee on ρ; the third was by the fact that the ﬁrst iteration did not terminate on Line
8, so we can apply the bound (G.6); and the fourth was by the deﬁnition of γ. Next, dividing both
sides by ¯β and using that termination on Line 14 has not occurred,
1
2 ∥Covw(N)(T)∥k =
1
2
w(N)
1
g
Covw(N)(T)

k
≤1
¯β ∥GN∥k ≤∥Covw(0)(T)∥k
1.052
6
+ 9 log d
N
+
9
200

.
Here, we used that
1
¯β
g
Covw(0)(T)

k = ∥Covw(0)(T)∥k twice, by deﬁnition of ¯β. Rearranging and
using the deﬁnition of N ≥425 log d then yields correctness of Case 3.
Complexity guarantee. For N = O(log d), the total cost of running ApproxKFMMW is
O

ndk log2(d) log2
d
δ

,
as given by Proposition 70. It is straightforward to check that the costs of Lines 5 and 10, given by

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
761
Propositions 69 and 70, do not dominate this. Finally, since the cost of checking (G.5) for a value
of K is linear in n, it suﬃces to provide an upper bound on K and then binary search. For this, we
have
X
i∈T
w(t,K)
i
τ (t)
i
≤
X
i∈T
exp
 
−Kτ (t)
i
τ (t)
max
!
w(t)
i τ (t)
i
≤
1
eK
X
i∈T
w(t)
i τ (t)
max ≤τ (t)
max
eK .
Here, the ﬁrst inequality used the deﬁnition of w(t,K)
i
, the second used that x exp(−Cx) ≤
1
eC for
all nonnegative x, where we chose C =
K
τ (t)
max , and the third used w(t) ∈∆n. Since the deﬁnition of
saturated weights implies that
p¯β ≥α, it follows that the threshold in Line 11 satisﬁes
γ ¯β
12 ≥110kα
12
≥5000.
Also, Assumption 14 and
 bYt

op ≤1.01 imply that all scores are bounded by 1.01R2, so we conclude
K ≤R2. Thus, the complexity of the binary search is O(n log R) and does not dominate.
G.3.2
Analysis of ProduceGoodTuple
At this point, the statements and analyses of both BicriteriaFilter and ProduceGoodTuple are straight-
forward, as we have done most of the heavy lifting in proving Proposition 71. We state both here
and prove their correctness and a runtime guarantee in Proposition 72.
Proposition 72. Algorithm 82, ProduceGoodTuple, correctly outputs a good tuple with probability
at least 1 −δ. Its overall complexity is
O
nd
α log2(d) log2
dR
δ

log(R) log
 1
α

.
Proof. We will show correctness and complexity of Algorithm 82 separately.
Correctness guarantee. We ﬁrst claim that if BicriteriaFilter meets its speciﬁcations, then so does
ProduceGoodTuple. This is since every time BicriteriaFilter returns in Case 1, the ℓ1 norm of w is
halved, but it can never be smaller than α2 since w is always saturated, so Case 1 occurs ≤2 log 1
α
times. Finally, note that Case 2 of BicriteriaFilter indeed constitutes a good tuple, with c = 128.
It remains to prove that BicriteriaFilter meets its speciﬁcations. We ﬁrst claim that the while loop
of Lines 5-23 is not run more than M times. To see this, whenever DecreaseKFNorm returns in Case
1, the loop immediately terminates, so it suﬃces to bound the number of times DecreaseKFNorm
returns in Case 2 or Case 3 before exiting on Line 13. Observe that every time Case 2 occurs, the
operator norm of Covw(T) is decreased by 1
3, but by Assumption 14 it is bounded by R2 initially,
and as soon as it is smaller than 100, then the algorithm will exit on Line 13. Thus, the number of
times Case 2 occurs is at most 3 log( R2
100); similarly, Case 3 occurs at most 2 log( R2
100) times since it
halves the Ky Fan norm each time. Combining these yields the claimed bound of M loops.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
762
Algorithm 81: BicriteriaFilter(T, δ, w)
1 Input: T ⊂Rd with |T| = n satisfying Assumptions 13 and 14, δ ∈(0, 1), saturated w
Output: Saturated w′, satisfying one of the following possibilities with probability ≥1 −δ:
1. w′ has ∥w′∥1 ≤1
2 ∥w∥1 (marked "Case 1")
2. B ∈Rd×k′ is also outputted, for k′ = O( log R
α ), and
CovP⊥
B
w′ (T)

op ≤
128
p
∥w′∥1
(marked "Case 2")
;
B ←[], k ←⌈612
α ⌉, ¯β ←∥w∥1;
δ′ ←
δ
M , for M = 5 log( R2
100);
while true do
if ∥w∥1 ≤1
2 ¯β then
Return: (w, "Case 1");
T ←projection of T into orthogonal complement of BB⊤;
γ ←1.05-approximation to ∥Covw(T)∥k with probability ≥1 −
δ′
3(N+1), for
N = ⌈150 log d⌉;
if γ <
110k
√
∥w∥1 then
Append the columns of Power(Covw(T), k, 0.05,
δ′
3(N+1)) to B;
Return: (w, B, "Case 2");
if DecreaseKFNorm(T, w, γ, δ′) returns "Case 1" then
Return: (DecreaseKFNorm(T, w, γ, δ′), "Case 1")
else if DecreaseKFNorm(T, w, γ, δ′) returns "Case 2" then
(w, V) ←DecreaseKFNorm(T, w, γ, δ′);
Append the columns of V to B;
else
w ←DecreaseKFNorm(T, w, γ, δ′);
Algorithm 82: ProduceGoodTuple(T, δ)
1 Input: T ⊂Rd with |T| = n satisfying Assumptions 13 and 14, δ ∈(0, 1);
2 Output: Good tuple (B, w) (cf. Deﬁnition 58) with probability ≥1 −δ;
3 w ←1
n1;
4 while true do
5
if BicriteriaFilter

T,
δ
2 log 1
α , w

returns "Case 1" then
6
w ←BicriteriaFilter

T,
δ
2 log 1
α , w

7
;
8
else
9
Return: BicriteriaFilter

T,
δ
2 log 1
α , w

;

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
763
Thus, the failure probability of BicriteriaFilter is met; it remains to prove that in each case, it
returns correctly.
If the algorithm returns on Line 7, this is clear.
If the algorithm returns on
Line 16, note that its input w has ∥w∥1 ≤¯β by monotonicity of ﬁltering, so it must be that the
output of DecreaseKFNorm has ℓ1 norm at most 1
2 ¯β by Case 1 of DecreaseKFNorm. The only other
place the algorithm can return is in Line 13. However, in this case it is clear that B has at most
k · (3 log( R2
100) + 1) = O( log R
α ) columns, since every time Line 18 is executed only k columns are
appended, and we earlier bounded the number of times Line 18 can occur. Finally, by combining
the deﬁnition of γ, the fact that we always project T into the orthogonal complement of BB⊤in Line
9, and the fact that Proposition 69 implies that λk+1(Covw(T)) ≤(1.05)2 γ
k (see e.g. the calculation
(G.7)), we see that when (w, B) is returned,
CovP⊥
B
w (T)

op ≤(1.05)3γ
k
≤
128
p
∥w∥1
.
In the above equation, we overload T to mean the original dataset (rather than after projection in
Line 9). This proves correctness of BicriteriaFilter in all cases. Finally, we remark that all parts of
DecreaseKFNorm operate correctly after the projection in Line 9. The only place this may cause
diﬃculty is in dependences on smallest eigenvalues in implementing ApproxKFMMW, because the
gain matrices are not full rank. However, it is straightforward to check that the guarantees of the
subroutines ApproxProject and Power as given in Section 7.3 will depend on the smallest eigenvalues
of gain matrices restricted to Span(I −BB⊤), if all operations are performed in this space.
Complexity guarantee. By our earlier analysis, ProduceGoodTuple incurs a multiplicative O(log 1
α)
overhead on the cost of BicriteriaFilter, so it suﬃces to understand this latter complexity.
The
dominant cost is clearly the (at most M) calls to DecreaseKFNorm, and the projection steps in Line
9. Line 9 involves orthogonalizing each of n vectors against O(k log R) vectors in d dimensions, so
its complexity is O(ndk log R · M), which does not dominate. The overall cost bound follows from
combining Proposition 71 with a multiplicative O(M log 1
α) overhead factor.
By combining Lemma 300 with Proposition 72, we have the following guarantee on FastSIFT.
Corollary 66. With probability 1 −δ, some ˆµ ∈L outputted by Algorithm 79 satisﬁes
∥ˆµ −µ∗∥2
2 ≤560
α .
The overall runtime of Algorithm 79 is
O
nd
α log2(d) log2
dR
δ

log(R) log
 1
α

+ 1
α6 log3(R) log
dR
δ

.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
764
G.4
Cleanup
In this section, we give implementations of pre-processing and post-processing procedures on the
dataset which will be used in attaining our ﬁnal guarantees. In particular, Section G.4.1 shows
how to reduce the size of our ﬁnal output list, and Section G.4.2 shows how to na¨ıvely cluster the
dataset to have diameter polynomially bounded in problem parameters. Finally, we put all the
pieces together in giving our ﬁnal result on list decodable mean estimation in Section G.4.3, as well
as a variant on this procedure which obtains a slight runtime-accuracy tradeoﬀ, in Section G.4.4.
G.4.1
Merging candidate means
We give a simple greedy algorithm for taking the output of Algorithm 79 (FastSIFT) and reducing its
size to be O( 1
α), without aﬀecting the guarantee (G.2) by more than a constant factor. The algorithm
and analysis bear some resemblance to the strategy in [179], but we include it for completeness. In
this section, denote k := ⌈4
α⌉as in Algorithm 78. We recall from the description of SIFT that the
output L of FastSIFT has the property that elementwise, all ˆµ ∈L are of the form
µﬁxed + VV⊤BB⊤Xi = µﬁxed + PXi, where Xi ∈Tslow, P := VV⊤,
(G.8)
since columns of V ∈Rd×k are contained in Span(B), and µﬁxed lies in the orthogonal complement
of Span(V).2 To see this, note that all input points to SIFT are of the form BB⊤Xi (Line 4 of
FastSIFT), and because SIFT then works in the coordinate system of B, every element of the output
list will have this form. In particular, µﬁxed is the sum of µfast (Line 3, Algorithm 79) and the
empirical mean in the last iteration of SIFT projected into (I −P) (Line 12, Algorithm 78).
Because the proof of Lemma 300 (with c = 128, cf. Proposition 72) shows that
∥µﬁxed −(I −P)µ∗∥2
2 ≤512 + 28
α
= 540
α , ∥P(Xi −µ∗)∥2
2 ≤8
α for some ˆµ ∈L,
(G.9)
it suﬃces to reduce the number of PXi while maintaining one with squared ℓ2 distance O( 1
α) from
Pµ∗. We now give our post-processing procedure. In the following, deﬁne n′ := |Tslow| = O( log R
α2 ).
Lemma 303. The output of Algorithm 83 has |eL| ≤2
α, and at least one ˆµ ∈eL has
∥ˆµ −µ∗∥2
2 ≤1052
α
.
The overall runtime of the algorithm is
O
 1
α4 log(R) log
1
δ

.
2In the implementation, we will have V ∈Rk′×k where k′ is the column dimensionality of B since it is expressed
in the coordinate system of B, but we write it this way for consistency with the whole algorithm.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
765
Algorithm 83: PostProcess(L, α)
1 Input: L, the output of Algorithm 79 (FastSIFT) decomposed as (G.8), satisfying (G.9);
2 Output: eL, a subset of L with |eL| ≤2
α;
3 eL ←∅;
4 Let eL be a maximal subset of L of points ˆµ = µﬁxed + PXi, such that ∥P(Xi −Xj)∥2
2 ≤32
α
for at least n′α
2
of the Xj ∈Tslow, and ∥ˆµ −ˆµ′∥2
2 ≥128
α , ∀ˆµ′ ∈eL;
5 Return: eL;
Proof. We ﬁrst prove the bound on the list size. Note that every element ˆµ ∈eL is associated with
at least n′α
2
elements in Tslow; call this the "cluster" of ˆµ. By the separation assumption on pairs in
eL, the clusters of all ˆµ, ˆµ′ ∈eL are distinct, so there can only be at most 2
α clusters as desired.
We now show the error guarantee. By the decomposition (G.8), the assumption (G.9), and the
Pythagorean theorem, it suﬃces to show that for some ˆµ = PXj + µﬁxed in the output list,
∥P(Xj −µ∗)∥2
2 ≤512
α .
(G.10)
By assumption, there is a particular ˆµ = PXi + µﬁxed ∈L which satisﬁes the bound (G.9). We
will designate this ˆµ as ˆµgood throughout the proof, and ﬁx the index i to be associated with ˆµgood.
Next, we recall that at least n′α
2
of the points Xj ∈Tslow have
∥P(Xj −µ∗)∥2
2 ≤8
α.
This was shown in the ﬁrst part of Theorem 89, and is a straightforward application of Markov and
Assumption 13. By triangle inequality to µ∗and the deﬁnition of ˆµgood, Xi satisﬁes
∥P(Xi −Xj)∥2
2 ≤32
α for at least n′α
2
of the Xj ∈Tslow.
Now, assume that (G.10) does not occur; this clearly also means that ˆµgood cannot belong to eL.
However, this is a contradiction, since triangle inequality implies that if no point in eL satisﬁes (G.10),
then ˆµgood would be added to the list by maximality of the subset.
Finally, we show the complexity guarantee.
Throughout, we use the assumption that L has
already been decomposed as (G.8), and all components in P are expressed in the coordinate system
of V, so all distance comparisons take time O(k). We can ﬁrst eliminate all points which do not meet
the clustering criteria (e.g. do not have enough points nearby) in one pass, in time O(|L||Tslow|k).
Afterwards, a na¨ıve greedy algorithm suﬃces for forming a list eL in Line 4, e.g. iteratively looping
over L and performing the check against points in |eL| sequentially until a loop adds no elements to
eL. This costs O(|L||eL|2k), which yields the runtime since we argued |eL| = O(k).

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
766
G.4.2
Bounding dataset diameter
In Section G.3, we developed an algorithm for list-decodable mean estimation under Assumption 14.
We now demonstrate how to reduce a general dataset satisfying Assumption 13 to this case. Our
strategy will be to divide the original dataset into multiple portions of bounded diameter, such that
with high probability all of the points in S satisfying Assumption 13 lie in the same set. To do
so, we perform a random one-dimensional projection, which is likely to preserve distances up to a
polynomial factor, and then use an equivalence class partition as our clustering. We state two simple
facts which are helpful in the analysis.
Lemma 304. No two points Xi, Xj ∈S have ∥Xi −Xj∥2 ≥2√n.
Proof. It suﬃces to show that every point in S has distance at most √n from µ∗. If this were not
the case, it is clear Assumption 13 cannot hold by virtue of the corresponding rank-one term.
Lemma 305. Let T be a set of n points in Rd, and sample g ∼N(0, I). With probability at least
1 −δ, for every pair of distinct points Xi, Xj ∈T,
1
4 log n
δ
(⟨g, Xi −Xj⟩)2 ≤∥Xi −Xj∥2
2 ≤n4
δ2 (⟨g, Xi −Xj⟩)2 .
(G.11)
Proof. Fix a pair Xi, Xj ∈T; we show that each of the bounds in (G.11) holds with probability at
least 1 −δ
n2 , and then the conclusion holds by a union bound over both tails and all pairs. Since the
distribution of ⟨g, Xi −Xj⟩is N(0, ∥Xi −Xj∥2
2), the lower bound in (G.11) is a straightforward ap-
plication of sub-Gaussian concentration. The upper bound comes from the fact that the probability
mass of N(0, 1) in the range [−√ϵ, √ϵ] is bounded by
1
√
2π
Z √ϵ
−√ϵ
exp

−1
2t2

dt ≤√ϵ.
Hence, the probability that Z ∼N(0, ∥Xi −Xj∥2
2) has Z2 ≤δ2
n4 is bounded by
δ
n2 .
At this point, we are ready to give our pre-processing procedure.
Lemma 306. PreProcess meets its output speciﬁcations. The overall runtime is
O (nd + n log n) .
Proof. The runtime bound is immediate; Lines 3 and 4 clearly take time O(nd), and Line 5 can be
performed by sorting the values {vi}i∈T and greedily forming clusters, creating disjoint paths from
the smallest value to the largest. To show correctness, condition on the conclusion of Lemma 305
occuring (giving the failure probability). We begin with the claim that all of S is contained in a

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
767
Algorithm 84: PreProcess(T, δ)
1 Input: T ⊂Rd with |T| = n satisfying Assumption 13, δ ∈(0, 1);
2 Output: Partition of T into disjoint clusters {Tj}j∈[m], such that all of S is contained in a
single cluster, and every cluster has radius ≤4n4
δ2 , with probability 1 −δ;
3 g ∼N(0, I);
4 vi ←⟨g, Xi⟩for all Xi ∈T;
5 Partition T into equivalence classes {Tj}j∈[m], where indices i, i′ are in the same Tj if there
is a path of distinct i1 = i, i2, . . . iℓ= i′ so that each consecutive |via −via+1| ≤4pn log n
δ ;
6 Return: Clusters in {Tj}j∈[m] with at least αn points;
single cluster; to see this, if Xi, Xj ∈S, then combining Lemma 304 and Lemma 305 implies that
(vi −vj)2 ≤

4 log n
δ

(4n) =⇒|vi −vj| ≤4
r
n log n
δ .
Furthermore, suppose two points Xi, Xi′ are in the same cluster, witnessed by a path of length
ℓ≤n starting at i1 = i and ending at iℓ= i′. Then, by triangle inequality
|vi −vi′| ≤
ℓ−1
X
a=1
|via −via+1| ≤4
r
n3 log n
δ
=⇒(⟨g, Xi −Xi′⟩)2 ≤16n3 log n
δ =⇒∥Xi −Xi′∥2
2 ≤16n7
δ2
log n
δ ≤16n8
δ4 .
In the last implication, we used the upper bound in Lemma 305.
G.4.3
Putting it all together
Finally, we put together the pieces we have developed to give our ﬁnal algorithm.
Algorithm 85: ListDecodableMeanEstimation(T, δ)
1 Input: T ⊂Rd with |T| = n satisfying Assumption 13, Tslow ⊂Rd with |Tslow| = O( log d/δ
α2
)
satisfying Assumption 13 for 1
α ﬁxed O( log d/δ
α
)-dimensional subspaces (cf. Remark 14,
where we use R = poly(d, δ−1) as below, where n = poly(d)), δ ∈(0, 1);
2 Output: L ⊂Rd with |L| ≤2
α satisfying (G.2) with probability ≥1 −δ;
3 {Tj}j∈[m] ←PreProcess(T, δ
2);
4 Lj ←FastSIFT(Tj, δα
2 , ProduceGoodTuple), for all j ∈[m], with R = 4n4
δ2 , and αj = α|T |
|Tj| ,
reusing the same datapoints Tslow for each call to FastSIFT;
5 Lj ←PostProcess(Lj, αj), for all j ∈[m];
6 Return: L ←S
j∈P Lj, where P = {j ∈[m] | |Lj| ≤
2
αj };
Theorem 90. Under Assumption 13, with probability at least 1 −δ, ListDecodableMeanEstimation

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
768
outputs a list of size at most 2
α, and attains error
min
µ∈L ∥µ −µ∗∥2 = O
 1
√α

.
The overall runtime is
O
nd
α log2(d) log3
d
δ

log
 1
α

+ 1
α6 log4
d
δ

.
Proof. We will show correctness and complexity of Algorithm 85 separately.
Correctness guarantee. First, note there are at most α−1 clusters outputted by PreProcess, so by
a union bound, with probability at least 1 −δ, both PreProcess and all FastSIFT calls succeed. Note
that whichever cluster Tj that contains all of S indeed satisﬁes Assumption 13, with |S| = αj|Tj|,
by deﬁnition of αj. Thus, Corollary 66 and Lemma 303 imply that index j will belong to the output
set P, and an element of Lj will meet the error guarantee (G.2). The list size follows from
|L| ≤
X
j∈[m]
2
αj
= 2
α.
Finally, we remark that we can reuse the same slow dataset Tslow for each of the at most 1
α runs of
FastSIFT in Line 4, corresponding to diﬀerent clusters, up to a
1
α factor in the failure probability
of Proposition 68. This is because (as in Remark 14), the low-dimensional subspaces produced by
ProduceGoodTuple are each independent of any randomness used in generating the set Tslow.
Complexity guarantee. The cost of PostProcess given in Lemma 303 never dominates the cost of
FastSIFT given in Corollary 66; similarly, it is clear that the cost of PreProcess given in Lemma 306
never dominates. Thus, it suﬃces to bound the costs of all calls to FastSIFT in Line 4. To this end,
we bound contributions of the two terms in the runtime of Corollary 66. Because each αj ≥α and
the sum of the sizes of the {Tj}j∈[m] is n,
X
j∈[m]
|Tj|d
αj
≤|T|d
α
= nd
α .
Similarly, denoting kj =
1
αj and k = 1
α, since P
j∈[m] kj = k by design,
X
j∈[m]
k6
j ≤

X
j∈[m]
kj


6
= 1
α6 .

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
769
G.4.4
Trading oﬀaccuracy for runtime
In this section, we give a simple alternative to the algorithm ListDecodableMeanEstimation which
removes the lower-order term in the runtime (so that the complexity is just the cost of polylogarith-
mically many calls to a k-PCA routine), at the cost of a slight loss in the accuracy term. We ﬁrst
note that unless α−1 = ω
√
d

, the term with dependence α−6 will not dominate the complexity
of Theorem 90. This is because we choose our sample complexity (following Assumption 13) to be
on the order of d
α, so that asymptotically,
1
α6 > nd
α
=⇒d2 < 1
α4 .
We now give the main result of this section, which shows in this regime of α−1, it suﬃces to randomly
sample in the last stage of each run of FastSIFT rather than apply SIFT. The following Algorithm 86
(FasterSIFT) is a simple modiﬁcation of FastSIFT, which is the same for the ﬁrst three lines, as well
as the last. The only diﬀerence is that in Line 4, the list Lslow is formed by random sampling points
from Tslow and projecting into the subspace BB⊤.
Algorithm 86: FasterSIFT(T, δ, ProduceGoodTuple)
1 Input: T = Tfast ∪Tslow ⊂Rd with |Tfast| = n satisfying Assumptions 13 and 14,
|Tslow| = O( log R
α2 ) satisfying Assumption 13 for a ﬁxed O( log R
α )-dimensional subspace,
δ ∈(0, 1), subroutine ProduceGoodTuple which returns a good tuple with speciﬁed failure
probability;
2 (B, w) ←ProduceGoodTuple(Tfast, δ
2);
3 µfast ←(I −BB⊤)µw(T);
4 Lslow ←{BB⊤Xi where i ∈Tslow is sampled uniformly at random}, with list size
|Lslow| = ⌈2
α log 4
δα⌉;
5 Return: L ←{µslow + µfast | µslow ∈Lslow};
Corollary 67. Consider running ListDecodableMeanEstimation with a modiﬁcation: in Line 4, use
FasterSIFT (Algorithm 86) in place of FastSIFT (Algorithm 79). The resulting list has size at most
2
α. Under Assumption 13, with probability at least 1 −δ, the overall runtime is
O
nd
α log2(d) log3
d
δ

log
 1
α

,
and the error guarantee is
min
µ∈L ∥µ −µ∗∥2 = O


s
log 1
δα
α

.
Proof. We ﬁrst discuss list size and error guarantee.
It suﬃces to show that for the cluster Tj
containing all of S, we can modify Lemma 303 to obtain a list size
2
αj and error guarantee on

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
770
the order of
p
log(1/δα)/α. To see this, all arguments in Lemma 303 follow identically, except
that the random sampling occured in a O( log R
αj )-dimensional space. Hence, the error guarantee is
correspondingly ampliﬁed, where we recall R = poly(d, δ−1), but the list size argument is the same
(e.g. we only keep means which contain at least O(|Tj|αj) points within their cluster, and all clusters
are disjoint).
We now discuss runtime. The cost of all runs of FastSIFT remains the same, up until the step
where SIFT is run; clearly, the cost of random sampling is cheaper than running ProduceGoodTuple,
once the projections into the coordinate system of B have already been formed. Finally, the only
place that we can lose runtime due to working in a larger-dimensional subspace is in the complexity of
PostProcess, where operations are done in O( log R
α ) dimensions. Mirroring the proof of Lemma 303,
this only adds a log R overhead, and it is straightforward to check that the cost of all runs of
PostProcess do not dominate, since for d ≥α−1 and our choice of n, nd
α ≥
1
α4 .
G.5
Warmup: fast Gaussian multiﬁlter
As a warmup to our (stronger) developments in Section 8.4, we give a complete algorithm for list-
decodable mean estimation in the Gaussian case, i.e. where the "true" distribution D is drawn
from a Gaussian with covariance bounded by I. We encourage the interested reader to read this
section before Section 8.4. Conceptually, the types of statements Gaussian concentration (rather
than heavy-tailed concentration) allow us to make let us simplify several of the technical diﬃculties
alluded to at the end of Section 8.3.3, in particular the following.
1. Instead of a "covariance bound" statement such as (8.3) to use in our potential proof, we will
simply guarantee that the multiﬁlter returns sets of points which lie in short intervals along a
number of random directions given by a Johnson-Lindenstrauss sketch.
2. Instead of a randomly subsampled ﬁltering step to remove outliers without soft downweighting
(to preserve truly small subsets), it will be enough to deterministically set thresholds along
1-dimensional projections to safely remove the outliers.
3. The deﬁnition of the Gaussian multiﬁlter (see Section G.5.2) will be substantially simpler,
since we have more explicit tail bounds to check for outliers.
The strength of the error guarantees of the simpler algorithm in this section are somewhat weaker
than those of Section 8.4 even when specialized to the Gaussian case, but we include this section
as an introductory exposition of our techniques. We will use the stronger Gaussian concentration
assumption in this section, a tightening of Assumption 13.
Throughout this section, we will assume that α ∈[1/d, 1/ logC d], for some constant C > 0
without loss of generality, as discussed in Section 8.5.
We now formally deﬁne the regularity condition which we will use throughout this section.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
771
Assumption 15. There is a subset S ⊆T ⊂Rd of size αn = Θ(d · polylog(d)), and a vector
µ∗∈Rd, such that for all unit vectors v ∈Rd and thresholds t ∈R≥0,
Pr
i∼unifS[⟨Xi −µ∗, v⟩> t] ≤exp(−Ω(t2)) +
1
Ω
 log3 d
.
Here, the notation i ∼unif S means that i is a uniformly random sampled index from S.
This assumption is standard in the literature, and follows when the true distribution which S
is sampled from is Gaussian with identity-bounded covariance (see e.g. Deﬁnition A.4, Lemma A.5
[177]). We remark that the sample complexity of Assumption 15 is worse than that of Assumption 13
by a polylogarithmic factor. This lossiness is just to simplify exposition in this warmup section, and
indeed in the following Section 8.4 we give an algorithm which recovers stronger guarantees than
Theorem 91, this section's main export, under only Assumption 13.
In Section G.5.1, we ﬁrst give our main subroutine, GaussianPartition, which takes a candidate
set and produces a number of children candidate sets which each satisfy a progress guarantee similar
to (8.3). The main diﬃculty will be in guaranteeing that the children sets are suﬃciently small, and
that if the parent set was "good" (had large overlap with S), then at least one child set will as well.
We reduce GaussianPartition to a number of one-dimensional clustering steps, which we implement as
GaussianSplitOrCluster in Section G.5.2. Finally, we use the guarantees of GaussianPartition within our
potential-based framework outlined in Section 8.3.3, giving our ﬁnal algorithm FastGaussianMultiﬁlter
in Section G.5.3. Throughout, sets S and T are ﬁxed and satisfy Assumptions 13 and 15.
G.5.1
Reducing GaussianPartition to GaussianSplitOrCluster
Our ﬁnal algorithm creates a tree of candidate sets. Every node p in the tree is associated with a
subset Tp. In order to progress down the tree, at a given node p we form children {cℓ}ℓ∈[k] with
associated sets {Tcℓ}ℓ∈[k]; we call the procedure which produces the children node GaussianPartition,
and develop it in this section. There are three key properties of GaussianPartition which we need.
1. The sum of the cardinalities of {Tcℓ}ℓ∈[k] is not too large compared to |Tp|. This is to guarantee
that at each layer of the tree, we perform about the same amount of work, namely eO(nd).
We formalize this with a parameter β ∈(0, 1] throughout the rest of this section, and will
guarantee that every time GaussianPartition is called on a parent node p,
X
ℓ∈[k]
|Tcℓ|1+β ≤|Tp|1+β.
(G.12)
2. If the parent vertex p has substantial overlap with S (at least 1
2|S| points), then at least one
of the produced children continues to retain all but a small fraction of points in S.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
772
3. Deﬁning the matrices
Mp := g
Cov 1
n 1 (Tp) , Yp := Mlog d
p
,
Mcℓ:= g
Cov 1
n 1 (Tcℓ) , Ycℓ:= Mlog d
cℓ
for all ℓ∈[k],
(G.13)
every Mcℓsatisﬁes the bound

Y2
p, Mcℓ

≤R2Tr
 Y2
p

,
(G.14)
for some (polylogarithmic) value R we will specify. Note the similarity between this and (8.3);
this will be used in a potential analysis to bound progress on covariance operator norms.
We are now ready to state the algorithm GaussianPartition.
Algorithm 87: GaussianPartition(Tp, α, β, C, R)
1 Input: Tp ⊆T, α ∈(0, 1
2), β ∈(0, 1], C, R ∈R≥0 satisfying (for suﬃciently large constants)
R = Ω
 
p
log (C) · log log
 Cα−1
β
!
, C = Ω
 log2 d

.
;
2 Output: With failure probability ≤
1
d3 : subsets {Tcℓ}ℓ∈[k] of Tp, satisfying (G.12). Every
child satisﬁes (G.14) (using notation (G.13)). If |Tp ∩S| ≥( 1
2 + 1
C )|S|, at least one child
Tcℓsatisﬁes
|Tcℓ∩S| ≥|Tp ∩S| −1
C |S|.
(G.15)
;
3 Sample Ndir = Θ(log d) vectors {uj}j∈[Ndir] ∈Rd each with independent entries ±1.
Following notation (G.13), let vj ←Ypuj for all j ∈[Ndir].;
4 S0 ←Tp;
5 for j ∈[Ndir] do
6
Sj ←∅;
7
for T ′ ∈Sj−1 do
8
T ←Gaussian1DPartition(T ′, α, vj, β, CNdir, R);
9
Sj ←Sj ∪T ;
10 Return: SNdir;
It heavily relies on a subroutine, Gaussian1DPartition(T ′, v) which takes a subset T ′ and a vector
v ∈Rd, and produces children subsets of T ′ satisfying the ﬁrst two conditions above, and also
guarantees that along the direction v, each child subset is contained in a relatively short interval.
Once again, Gaussian1DPartition heavily relies on a subroutine, GaussianSplitOrCluster, which we
implement in Section G.5.2. It takes as input a set T ′′ and either produces one or two subsets of T ′′

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
773
Algorithm 88: Gaussian1DPartition(T ′, α, v, β, C, R)
1 Input: T ′ ⊆T, α ∈(0, 1
2), v ∈Rd, β ∈(0, 1], C, R ∈R≥0 satisfying (for suﬃciently large
constants)
R = Ω
 
p
log (C) · log log
 Cα−1
β
!
, C = Ω
 log3 d

.
;
2 Output: Subsets {T ′′
ℓ}ℓ∈[k] ⊆T ′, such that
X
ℓ∈[k]
|T ′′
ℓ|1+β ≤|T ′|1+β.
(G.16)
If |T ′ ∩S| ≥( 1
2 + 1
C )|S|, at least one child T ′′
ℓsatisﬁes
|T ′′
ℓ∩S| ≥|T ′ ∩S| −1
C |S|.
Every child has all values {⟨v, Xi⟩| i ∈T ′′
ℓ} contained in an interval of length R ∥v∥2.;
3 Sin ←{T ′}, Sout ←∅;
4 while Sin ̸= ∅do
5
T ′′ ←the ﬁrst element of Sin;
6
Sin ←Sin \ T ′′;
7
if GaussianSplitOrCluster(T ′′, α, β, R,
1
Cn) returns one set T (0)
out then
8
Sout ←Sout ∪
n
T (0)
out
o
;
9
else
10
T (1)
out, T (2)
out ←GaussianSplitOrCluster(T ′′, α, β, R,
1
Cn);
11
Sin ←Sin ∪
n
T (1)
out, T (2)
out
o
;

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
774
as output. If it outputs one set, that set has length at most R ∥v∥2 in the direction v; otherwise,
Gaussian1DPartition simply recurses on the additional two sets.
Crucially, GaussianSplitOrCluster
guarantees that if T ′′ has substantial overlap with S, then so does at least one child; moreover,
when GaussianSplitOrCluster returns two sets, they satisfy a size potential such as (G.16). We now
demonstrate correctness of GaussianPartition, assuming that Gaussian1DPartition is correct.
Lemma 307. The output of GaussianPartition satisﬁes the guarantees given in Line 2 of Algo-
rithm 87, assuming correctness of Gaussian1DPartition.
Proof. First, to demonstrate that the subsets satisfy (G.12), we observe that we can view GaussianPartition
as always maintaining a set of subsets, Sj (in the beginning, S0 = Tp).
The set Sj is formed
by calling Gaussian1DPartition on elements of Sj−1, each of which satisfy (G.16), so inductively
SNdir = {Tcl}l∈[k] will satisfy (G.12) with respect to S0 = Tp as desired.
Next, by recursively using the guarantee of Gaussian1DPartition, every Tcl ∈SNdir will satisfy
all values {⟨vj, Xi⟩| i ∈Tcl} are contained in an interval of length R ∥vj∥2 , for all j ∈[Ndir].
In other words, this set is short along all the directions {Ypuj = vj}j∈[Ndir]. This lets us conclude

Y2
p, Mcl

=
1
2n |Tcl|
*
Y2
p,
X
i,i′∈Tcl
(Xi −Xi′)(Xi −Xi′)⊤
+
=
1
2n |Tcl|
X
i,i′∈Tcl
∥Yp(Xi −Xi′)∥2
2
≤
1.4
2n |Tcl| Ndir
X
i,i′∈Tcl
X
j∈[Ndir]
⟨Ypuj, Xi −Xi′⟩2
≤
1.4
2n |Tcl| Ndir
X
i,i′∈Tcl
X
j∈[Ndir]
R2 ∥Ypuj∥2
2
≤
1.4
2Ndir
X
j∈[Ndir]
R2 ∥Ypuj∥2
2 ≤R2Tr
 Y2
p

,
with probability at least 1 −
1
2d3 .
Here, we used Fact 16 in the ﬁrst line and linearity of trace
in the second line. The third line used the Johnson-Lindenstrauss lemma of [6] which says that
for any vector v,
1
Ndir
P
j∈[Ndir] ⟨uj, v⟩2 ∈[0.6, 1.4] ∥v∥2
2 for a suﬃciently large Ndir = Θ(log(d))
with probability at least 1 −
1
2d6 , which we union bound over all |Tcl|2 ≤n2 ≤d4 pairs of points.
The fourth line used the radius guarantee of Gaussian1DPartition, and the ﬁfth used |Tcl| ≤n and
the Johnson-Lindenstrauss lemma guarantee that
1
Ndir
P
j∈[Ndir] ∥Ypuj∥2
2 ∈[0.6, 1.4]Tr(Y2
p) with
probability at least 1 −
1
2d3 , which can be deduced by the guarantee of [6] applied to the rows of Yp.
Union bounding over the two applications of [6] yields the claim.
Finally, to demonstrate that at least one child satisﬁes (G.15), suppose p satisﬁes |Tp ∩S| ≥( 1
2 +

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
775
1
C )|S| (i.e. it has substantial overlap with S). Then by applying the guarantee of Gaussian1DPartition
inductively, every Sj will have at least one element T ′ satisfying |T ′ ∩S| ≥
1
2|S|. Every call to
Gaussian1DPartition only removes
1
CNdir |S| points in S, so overall only 1
C |S| points are removed.
G.5.2
Implementation of GaussianSplitOrCluster
In this section, we ﬁrst state GaussianSplitOrCluster and analyze its correctness. We conclude with
a full runtime analysis of Gaussian1DPartition, using our GaussianSplitOrCluster implementation.
Algorithm 89: GaussianSplitOrCluster(Tin, α, v, β, R, ∆)
1 Input: Tin ⊆T, α ∈(0, 1
2), v ∈Rd, β ∈(0, 1], R ∈R≥0, ∆∈(0, 1);
2 Output: Either one subset T (0)
out ⊂Tin, or two subsets T (1)
out, T (2)
out ⊂Tin. In the one subset
case, T (0)
out has
n
⟨v, Xi⟩| i ∈T (0)
out
o
contained in an interval of length R ∥v∥2. In the two
subsets case, they take the form, for some threshold value τ ∈R and
r :=
R
4kmax , kmax = Θ
 log log(
1
α∆)
β

T (1)
out := {Xi | ⟨v, Xi⟩≤τ + r ∥v∥2}, T (2)
out := {Xi | ⟨v, Xi⟩≥τ −r ∥v∥2},
(G.17)
and satisfy
T (1)
out

1+β
+
T (2)
out

1+β
< |Tin|1+β .
(G.18)
;
3 Yi ←⟨v, Xi⟩for all i ∈Tin;
4 T (0)
out ←indices in the middle 1 −α∆quantiles of {Yi}i∈Tin;
5 if
n
Yi | i ∈T (0)
out
o
is contained in an interval of length R ∥v∥2 then
6
Return: T (0)
out;
7 else
8
τmed ←med ({Yi | i ∈Tin}), where med returns the median;
9
τk ←τmed + 2kr ∥v∥2 for all integers −kmax ≤k ≤kmax;
10
Return: T (1)
out, T (2)
out deﬁned in (G.17) for any threshold τk inducing sets satisfying
(G.18);
To analyze Algorithm 89 we ﬁrst demonstrate that it always returns in at least one case. In
particular, we demonstrate that whenever the set T (0)
out is not suﬃciently short, then there will be a
threshold parameter k such that the induced sets in (G.17) satisfy the size bound (G.18).
Lemma 308. Suppose Algorithm 89 does not return on Line 6. Then, there exists a k ∈Z in the
range −kmax ≤k ≤kmax such that Algorithm 89 is able to return on Line 10.
Proof. We instead prove that if there is no such k, then we will have a contradiction on the length
of the set T (0)
out in the direction v. We ﬁrst lower bound the length of the [ 1
2, 1 −α∆
2 ] quantiles

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
776
of {Yi | i ∈Tin} by
1
2R ∥v∥2; the lower bound for the [ α∆
2 , 1
2] quantiles will follow analogously.
Combining shows that if no threshold works, then the algorithm should have returned T (0)
out.
For any threshold τ, deﬁne g(τ) ∈[0, 1] to be the proportion of {Yi | i ∈Tin} which are ≥τ.
Moreover, deﬁne for all 1 ≤k ≤kmax,
γk := g(τk −r ∥v∥2) = g(τmed + (2k −1)r ∥v∥2),
and note that γ1 ≤1
2 by deﬁnition, since τmed was the median. Now, for each 1 ≤k ≤kmax, since
τk was not a valid threshold, the sets
T (1)
k
:= {Xi | Yi ≤τmed + (2k + 1)r ∥v∥2}, T (2)
k
:= {Xi | Yi ≥τmed + (2k −1)r ∥v∥2}
do not satisfy the size bound (G.18). Normalizing both sides of (G.18) by |Tin|1+β and using the
deﬁnitions of {γk}, we obtain the following recursion:
(1 −γk+1)1+β + γ1+β
k
=


T (1)
k

|Tin|


1+β
+


T (2)
k

|Tin|


1+β
≥1 =⇒γk+1 ≤γ1+β
k
.
(G.19)
To obtain the above implication, we used 1 −(1 −x)1+β > x1+β for all x, β ∈[0, 1]. By repeatedly
applying the recursion (G.19), we have
γkmax ≤γ(1+β)(kmax−1)
1
≤
1
2
(1+β)(kmax−1)
≤α∆
2 ,
where we use the deﬁnition of kmax and γ1 ≤
1
2. Thus, the [ 1
2, 1 −α∆
2 ] quantiles are contained
between τmed and τmed + (2kmax −1)r ∥v∥2 ≤τmed + 1
2R ∥v∥2. By repeating this argument in the
range −kmax ≤k ≤−1, we obtain a contradiction (as Algorithm 89 should have returned T (0)
out).
We next prove that if the input T ′ to Gaussian1DPartition has large overlap with S, then the
algorithm always returns some child T ′′ which removes at most 1
C |S| points from this overlap. This
proof uses the implementation of GaussianSplitOrCluster in a white-box way, as well as Assumption 15.
Lemma 309. Whenever Gaussian1DPartition is called on T ′ with |T ′ ∩S| ≥
  1
2 + 1
C

|S| with pa-
rameters R, C satisfying (for suﬃciently large constants)
R = Ω
p
log(C) · log log(Cd)
β

, C = Ω
 log3 d

,
it produces some child T ′′ satisfying |T ′′ ∩S| ≥|T ′ ∩S| −1
C |S|.
Proof. We ﬁrst discuss the structure of Gaussian1DPartition. We say a call to GaussianSplitOrCluster
is a "split step" if it produces two sets, and otherwise we call it a "cluster step." Every output

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
777
child of Gaussian1DPartition is the result of a consecutive number of split steps, and then one cluster
step.
Also, every split step replaces an interval with its intersections with two half-lines which
overlap by 2r ∥v∥2 = Ω(
p
log(C) ∥v∥2). Assume for simplicity that ∥v∥2 = 1 in this proof; analogous
arguments hold for all v by scaling everything appropriately. Finally, we recall that all calls to
GaussianSplitOrCluster in Gaussian1DPartition are with ∆=
1
Cn.
Our key technical claim is that after any number of split steps forming a partition of the real
line, there is always some interval such that ⟨v, µ∗⟩is r away from both endpoints (in this proof,
we allow intervals to have endpoints at ±∞). This is clearly true at the beginning, since the only
interval is (−∞, ∞). Next, we induct and assume that on the current partition, after some number
of split steps, there is an interval [a, b] in the partition such that ⟨v, µ∗⟩∈[a + r, b −r]. Consider
the intersection of this interval with any split step, parameterized by the half-lines (−∞, τ + r] and
[τ −r, ∞) for some τ ∈R. If ⟨v, µ∗⟩≥τ, then one of the resulting intervals is
[max (a, τ −r) , b]
where we note that this interval is non-degenerate by assumption; τ ≤⟨v, µ∗⟩≤b−r =⇒τ −r ≤b.
If the result of the max is [a, b], then the claim holds; otherwise, the interval is [τ −r, b] and the
claim holds by induction (⟨v, µ∗⟩≤b −r) and the assumption ⟨v, µ∗⟩≥τ. The other case when
⟨v, µ∗⟩≤τ follows symmetrically by considering the interval [a, min(b, τ + r)].
Now, consider the partition of the real line which is induced by the eventual children outputted
by Gaussian1DPartition, right before the last cluster step is applied to them (in other words, this
partition is formed only by split steps). Using the above argument, there is some element of this
partition [a, b] so that ⟨v, µ∗⟩∈[a + r, b −r]. Applying Assumption 15 shows that if we consider
the eﬀects of truncating the set {Yi | i ∈S} at the endpoints of this interval, we remove at most a
1
2C fraction of the points from S. Finally, the interval that is returned is the result of a cluster step
applied to this interval. This can only remove at most an α∆≤
α
2C fraction of the overall points,
which is at most
1
2C |S|. Combining these two bounds yields the claim.
Finally, we conclude with a runtime analysis of Gaussian1DPartition.
Lemma 310. Let n′ := |T ′| for some T ′ ⊆T. Gaussian1DPartition called on input T ′ with parameter
C can be implemented to run in time
O

n′d + (n′)1+β log n′ · log log(Cd)
β

.
Proof. We begin by forming all of the one-dimensional projections ⟨v, Xi⟩for all i ∈T ′, and sorting
these values. We also store the quantile of each point (i.e. the number of points larger than it). The
total cost of these operations is O(n′d + n′ log n′).
Next, given this total ordering, observe that the structure of Gaussian1DPartition means that every

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
778
set in Sin is a subinterval of T ′, since this is inductively preserved by calls to GaussianSplitOrCluster;
hence, we can represent every set implicitly by its endpoints. Moreover, given access to the initial
quantile information we can implement every call to GaussianSplitOrCluster in time O(kmax log n′) =
O(log n′· log log(Cd)
β
), since the cost of checking the length of T (0)
out is constant, and the cost of checking
each candidiate τk is dominated by determining the thresholds of the corresponding induced sets
T (1)
out and T (2)
out. These can be performed via binary searches in O(log n′) time.
It remains to bound the number of calls to GaussianSplitOrCluster throughout the execution of
Gaussian1DPartition. To this end, we bound the number of times GaussianSplitOrCluster can return
one set, and the number of times it can return two sets. Every time GaussianSplitOrCluster returns
one set, it adds it to Sout, and by using the guarantee (G.18) recursively, there can only ever be
(n′)1+β such sets. Similarly, every time it returns two sets it increases |Sin| + |Sout| by one, but
we know at termination this is at most (n′)1+β, and this potential never decreases. Thus, the total
number of calls to GaussianSplitOrCluster is bounded by O((n′)1+β), as desired.
As an immediate corollary, we obtain a runtime bound on GaussianPartition.
Corollary 68. Let np := |Tp| for some Tp ⊆T. GaussianPartition called on input Tp with parameter
C can be implemented to run in time
O

n1+β
p
d log2(d) + n1+β
p
log2(d) · log log(Cd))
β

.
Proof. First, consider the cost of computing all vectors Ypuj. It is straightforward to implement
matrix-vector multiplications through Mp in time O(npd), so this cost is O(npd log2(d)).
We next require a bound on the cost of Ndir = Θ(log d) consecutive calls to Gaussian1DPartition.
The cost of each is given by Lemma 310, and the result follows by summing this cost over all elements
of each Sj, which can be bounded since for all j ∈[Ndir], the cardinalities of all sets contained in Sj
have 1 + β powers bounded by n1+β
p
by repeatedly using the guarantee (G.16).
G.5.3
Full Gaussian algorithm
Finally, we are ready to give our full algorithm for list-decodable mean estimation under Assump-
tions 13 and 15. We begin by reducing the original problem to a number of subproblems of bounded
diameter (following Section G.4), and then showing that for each of these subproblems, polyloga-
rithmic calls to GaussianPartition yield subsets of bounded covariance operator norm. We conclude
by recalling that a covariance operator norm bound suﬃces to yield guarantees on mean estimation.
We begin by restating the guarantees of NaiveCluster, used in Line 3 of FastGaussianMultiﬁlter.
Lemma 311 (Restatement, Lemma 306). There is a randomized algorithm, NaiveCluster, which
takes as input T ⊂Rd satisfying Assumption 13 and partitions it into disjoint subsets {T ′
i}i∈[k] such

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
779
Algorithm 90: FastGaussianMultiﬁlter(T, α)
1 Input: T ⊂Rd, |T| = n satisfying Assumptions 13 and 15 with parameter α ∈(0, 1
2);
2 Output: With failure probability ≤1
d: L with |L| = O( 1
α) such that some ˆµ ∈L satisﬁes
∥ˆµ −µ∗∥2 = O
log(d) log log1.5(d)
√α

.
(G.20)
;
3 {T ′
i}i∈[k] ←NaiveCluster(T);
4 αi ←|T |
|T ′
i |α for all i ∈[k];
5 Return: S
i∈[k] FastGaussianMultiﬁlterBoundedDiameter(T ′
i, αi);
Algorithm 91: FastGaussianMultiﬁlterBoundedDiameter(T, α)
1 Input: T ⊂Rd, |T| = n satisfying Assumptions 13 and 15 with parameter α ∈(0, 1
2);
2 Output: With failure probability ≤1
d: Lout with |Lout| = O( 1
α) such that some ˆµ ∈Lout
satisﬁes
∥ˆµ −µ∗∥2 = O
log(d) log log1.5(d)
√α

.
;
3 L(0) ←{T}, Lout ←∅;
4 For suﬃciently large constants,
R ←Θ
 log(d) log log1.5(d)

, C ←Θ(log2 d), D ←Θ(log2 d)
;
5 for ℓ∈[D] do
6
L(ℓ) ←∅;
7
for T ′ ∈L(ℓ−1) do
8
Append all elements of GaussianPartition(T ′, α,
1
log d, C, R) to L(ℓ) with size at least
αn
2 ;
9 Return: List of empirical means of all sets in L(D);

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
780
that with probability at least 1 −
1
d2 , all of S is contained in the same subset, and every subset has
diameter bounded by O(d12). The runtime of NaiveCluster is O(nd + n log n).
We next demonstrate that if the operator norm of the (unnormalized) covariance matrix of a set
of points T ′ is bounded, and T ′ has suﬃcient overlap with S, then its empirical mean is close to µ∗.
Lemma 312. For T ′ ⊂T with empirical mean ˆµ, if |T ′ ∩S| ≥1
2|S| and g
Cov 1
n 1(T ′) ≤R2,
∥ˆµ −µ∗∥2 = O

(1 + R) ·
1
√α

.
Proof. Let w place weight 1
n on coordinates in T ′, and 0 on all other coordinates. Clearly this w
satisﬁes the assumption of Lemma 113, since its ℓ1 norm is simply |T ′|
|T | ≥1
2α. The conclusion follows
by applying Lemma 113, where we use ∥w∥1 Covw(T) = g
Cov 1
n 1(T ′), and the assumed bound.
We now give a full analysis of FastGaussianMultiﬁlterBoundedDiameter.
Proposition 73. FastGaussianMultiﬁlterBoundedDiameter meets its output speciﬁcations with prob-
ability at least 1 −1
d, within runtime
O
 nd log4(d) + n log5(d) log log(d)

.
Proof. Throughout, we denote β :=
1
log d. There are three main guarantees of the algorithm: that
the list size is O( 1
α), that some list element satisﬁes (G.20), and that the runtime is as claimed.
We ﬁrst bound the list size. We can view FastGaussianMultiﬁlterBoundedDiameter as producing a
tree of subsets, of depth D. Each layer of the tree is composed by the sets in L(ℓ) where 0 ≤ℓ≤D,
and L(0) is the root node. The children of each node are the results of calling GaussianPartition on
the associated subset. Moreover, by repeatedly using the guarantee (G.12) inductively, the total
cardinality of all sets at layer ℓis bounded by n1+β = O(n). Since we only return means from sets
with size at least αn
2 on layer D, there can only be O( 1
α) such sets.
Next, we bound error rate. Consider some leaf node, and its path to the root; call the sets
associated with these vertices T0, T1, . . . TD, where TD is the leaf node and T0 = T is the original
set. Deﬁne the potential function at each layer 0 ≤ℓ≤D,
Φℓ:= Tr

M2 log d
ℓ

, where Mℓ:= g
Cov 1
n 1 (Tℓ) .
Note that every parent-child pair along this path satisﬁes the guarantee (G.14). We thus conclude

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
781
that for each 0 ≤ℓ< D, we have the recurrence (analogously to (8.4))
Φℓ+1 = Tr

M2 log d
ℓ+1

≤
1
2R2 Tr

M2 log d+1
ℓ+1

+ d(2R2)2 log d
≤
1
2R2 Tr

M2 log d
ℓ
Mℓ+1

+ d(2R2)2 log d
≤1
2Tr

M2 log d
ℓ

+ d(2R2)2 log d = 1
2Φℓ+ d(2R2)2 log d.
The ﬁrst line used Fact 18 with γ = 2R2, the second used Fact 17, and the third used the guarantee
(G.14). Thus, as long as at a layer ℓwe have
Φℓ> 4d(2R2)2 log d,
we have Φℓ+1 ≤3
4Φℓ, and so the potential is decreasing by at least a constant factor. The potential
Φ0 is bounded by dO(log d), because we assumed the input set has polynomially bounded diameter,
so within D = Ω(log2 d) layers, every node on layer D must have ΦD ≤4d(2R2)2 log d. This implies
that the operator norm of g
Cov 1
n 1(T ′) for every node T ′ on layer D is O(R2).
We next show at least one node T ′ on every layer has |T ′∩S| ≥1
2|S|. By inductively using (G.15)
with our chosen value of C, summing over the O(log2 d) layers guarantees that we only remove at
most 1
2|S| points from the intersection throughout the root-to-leaf path, for some path. We can
now apply Lemma 312 to guarantee (G.20). To obtain the high-probability bound, note that the
number of times we call GaussianPartition is bounded by O( 1
α log2 d), since at each layer we prune
every node with less than αn
2
points; there can only be O( 1
α) surviving nodes per layer (since the
total cardinalities of the layer is bounded by n1+β = O(n)), and taking a union bound over all calls
to GaussianPartition shows the failure probability is at most 1
d.
Finally, we discuss runtime.
We simply apply Corollary 68 to each layer, which bounds the
runtime of each layer by O(nd log(d)+n log3(d) log log(d)), since the sets on that layer satisfy (G.12)
inductively. Summing over all layers yields the desired runtime guarantee.
Theorem 91. FastGaussianMultiﬁlter meets its output speciﬁcations with probability at least 1 −1
d,
within runtime
O
 nd log4(d) + n log5(d) log log(d)

.
Proof. We apply Proposition 73 to the relevant call of FastGaussianMultiﬁlterBoundedDiameter. Note
that all αi ≥α, giving the error guarantee (G.20), and
X
i∈[k]
1
αi
= 1
α,
giving the list size guarantee. The runtime follows from P
i∈[k] |T ′
i| = |T|.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
782
G.6
Deferred proofs from Section 3.2
G.6.1
Proof of Proposition 33
In this section, we prove Proposition 33, restated here for convenience.
Proposition 33. Let α ≥1 and let ϵ > 0 be suﬃciently small. Let {(Xi, yi)}i∈[n] ⊂Rd × R be an
ϵ-corrupted set of samples from a distribution DXy as in Model 3. Then, if
n = O
dα2 log d
ϵ4
+ d2α1.5 log(d/ϵ)
ϵ3

,
the set {(Xi, yi)}i∈[n] is (2ϵ, ϵ2
α )-good for linear regression with probability at least
9
10.
Before we prove this lemma, we need the following useful technical lemmata. The ﬁrst shows
that given a large enough sample of points from a distribution with bounded second moment, there
is a large subset of points with bounded second moment.
Lemma 313 (Lemma A.20 in [177]). Let X1, . . . , Xn be independent samples from a distribution D
with second moment matrix Σ⋆, and let ϵ > 0 be suﬃciently small. There exists a universal constant
c > 0 so that if n ≥c d log d
ϵ
, we have that with probability 0.99, there exists a subset S of size (1−ϵ)n
satisfying
1
|S|
X
i∈S
XiX⊤
i ⪯3
2Σ⋆.
We note that Lemma A.20 is stated for covariance as opposed to second moment, but the same
proof immediately implies the same result for second moment.
We also require the following bound.
Lemma 314 (Lemma 5.1 in [134]). Let X1, . . . , Xn be independent samples from a distribution D
with second moment Σ⋆, and let ϵ > 0 be suﬃciently small. Assume D is 2-to-4 hypercontractive
with parameter C = O(1). There exists a universal constant c > 0 so that if n ≥cd log(d/ϵ)
ϵ
, then
with probability 1 −1
d2 , we have that for any S ⊂[n] of size (1 −ϵ)n,
1
|S|
X
i∈S
XiX⊤
i ⪰(1 −√ϵ)Σ⋆.
Finally, we show our main helper lemma, which is used to prove Assumption 7.2 holds.
Lemma 315. Let ϵ > 0 be suﬃciently small. Let X1, . . . , Xn be n samples from a 2-to-4 hyper-
contractive distribution D with parameter C and second moment Σ⋆. Then, there exist universal
constants c, Cest > 0 so that if
n ≥c
d log d
ϵ4
+ d2 log(d/ϵ)
ϵ3

,

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
783
then with probability 0.99, for every u ∈Rd, there exists an G ⊆[n] satisfying |G| ≥(1 −ϵ2)n, and

1
|G|
X
i∈G
⟨Xi, u⟩2 XiX⊤
i

op
≤CestL ∥u∥2
Σ⋆.
Proof. Without loss of generality (by scale invariance), it suﬃces to prove this for all u with ∥u∥Σ⋆=
1. First, by Markov's inequality with EX∼D[∥X∥2
2] = Tr[Σ⋆] ≤Ld, we have that
Pr
X∼D

∥X∥2
2 ≥20Ld
ϵ2

≤ϵ2
20 .
Hence, by Bernstein's inequality, we have that with probability 0.999,
|{i : ∥Xi∥2
2 ≥20Ldϵ−2}|
n
≤ϵ2
10 .
(G.21)
Condition on this event holding for the rest of the proof.
For any vector u ∈Rd with ∥u∥Σ⋆= 1, let Hu ⊂Rd be the set given by
Hu =
(
x ∈Rd : ⟨x, u⟩2 ≥10C1/2
2→4
ϵ
)
.
Note that by Chebyshev's inequality, since EX∼D[⟨x, u⟩4] ≤C2→4 ∥u∥4
Σ⋆≤C2→4, PrX∼D[X ∈
Hu] ≤
ϵ2
100.
Furthermore, the collection of sets {Hu}∥u∥Σ⋆=1 has VC dimension O(d), as each
Hu can be expressed as a restricted intersection of parallel halfspaces, and it is well-known that
VC dimension is additive under intersection.
Therefore, by the VC inequality, we know that if
n ≥c d log d
ϵ4
for suﬃciently large constant c, with probability 0.999, we have that
sup
Hu
|{i : Xi ∈Hu}|
n
≤ϵ2
50 .
(G.22)
Condition on this event holding for the rest of the proof. All expectations throughout the remainder
of the proof are taken with respect to X ∼D for notational simplicity.
For any ﬁxed u, we deﬁne the truncated fourth moment (contracted in the direction u) by
Ai = Ai(u) = ⟨Xi, u⟩2 XiX⊤
i 1
"
⟨Xi, u⟩2 ≤20C1/2
2→4
ϵ
and ∥Xi∥2
2 ≤20Ld
ϵ2
#
.
Note that
∥E [Ai]∥op ≤
E
h
⟨Xi, u⟩2 XiX⊤
i
i
op =
sup
∥v∥2=1
E
h
⟨Xi, u⟩2 ⟨Xi, v⟩2i
≤C2→4L ∥u∥2
Σ⋆,

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
784
by Cauchy-Schwarz and hypercontractivity. Moreover, by construction, the spectral norm of Ai
is bounded almost surely by 400LC1/2
2→4d
ϵ3
. Hence, by a matrix Chernoﬀbound, we get that if n ≥
c d2 log(d/ϵ)
ϵ3
for a suﬃciently large constant c, then with probability 0.999, we have that

1
n
n
X
i=1
Ai(u)

op
≤2C2→4L ∥u∥2
Σ⋆.
(G.23)
for all u in a poly( ϵ
d)-net of the unit sphere in the Σ⋆norm (which has cardinality ( d
ϵ )O(d) by
Theorem 1.13 of [456]). Because we are union bounding over poly(d, ϵ−1) samples, we have with
high probability that all ∥Xi∥(Σ⋆)−1 = poly(d, ϵ−1). Hence, it is straightforward to show that the
bound (G.23) over our net implies for all ∥u∥Σ⋆= 1,

1
n
n
X
i=1
⟨Xi, u⟩2 XiX⊤
i 1
"
Xi ̸∈Hu and ∥Xi∥2
2 ≤10C1/2
2→4Ld
ϵ
#
op
≤2C2→4L ,
(G.24)
Combining (G.21), (G.22), and (G.24) implies that for every u, the set G = {i : Xi ∈Hu and ∥Xi∥2
2 ≤
20Ld
ϵ2 } satisﬁes the conditions of the lemma.
We are now ready to prove Proposition 33.
Proof of Proposition 33. Condition 3 of Assumption 7 follows directly from Markov's inequality,
since it is asking about the empirical average over G of δ2 ∼Dδ; the adversary removing points can
only aﬀect this upper bound by a constant factor (due to renormalization).
Next, let [n] = G ∪B be the canonical decomposition of the corrupted set of samples.
By
two applications of Lemma 313, with probability at least 0.99, there exists a set G′ ⊂G of size
|G′| ≥(1 −ϵ)|G| ≥(1 −2ϵ)n so that
1
|G′|
X
i∈G′
XiX⊤
i ⪯3
2Σ⋆,
(G.25)
1
|G′|
X
i∈G′
δ2
i XiX⊤
i ⪯3
2σ2Σ⋆.
(G.26)
Condition on the event that such a G′ exists for the remainder of the proof, and also condition on the
event that Lemma 315 is satisﬁed. By a union bound, these events happen together with probability
at least 0.9. We will show that this G′ will satisfy the conditions of the lemma. The upper bound in
Condition 1 of Assumption 7 is immediate, and similarly, the lower bound follows from Lemma 314
and a standard convexity argument (since the vertices of the polytope deﬁning saturated weights
are subsets of cardinality (1 −O(ϵ))|G|).
It thus remains to prove Condition 2 of Assumption 7. To do so, we will ﬁrst prove (8.39) is
satisﬁed with high probability. By Lemma 315 (adjusting by a factor of α in the deﬁnition of ϵ2),

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
785
there exists a set G′′ ⊆G′ so that |G′′| ≥(1 −ϵ2
α )|G′| so that

1
|G′′|
X
i∈G′′
⟨Xi, θ −θ⋆⟩2 XiX⊤
i

op
≤CestL ∥θ −θ⋆∥2
Σ⋆.
Hence, for this choice of G′′, we have
Cov ˜
w
 {gi(θ)}i∈G′′

=
X
i∈G′′
˜wi (⟨Xi, θ −θ⋆⟩+ δi)2 XiX⊤
i
⪯2
X
i∈G′′
˜wi ⟨Xi, θ −θ⋆⟩2 XiX⊤
i + 2
X
i∈G
˜wiδ2
i XiX⊤
i
⪯2(1 + 2ϵ)
|G′′|
X
i∈G′′
⟨Xi, θ −θ⋆⟩2 XiX⊤
i + 2(1 + 2ϵ)
|G′|
X
i∈G′
δ2
i XiX⊤
i
⪯2(1 + 2ϵ)
|G′|
X
i∈G′′
⟨Xi, θ −θ⋆⟩2 XiX⊤
i + 3σ2Σ⋆
⪯2(1 + 2ϵ)CestL

∥θ −θ⋆∥2
Σ⋆+ σ2
I .
where the last line follows from (G.26).
By suitably adjusting the choice of Cest, this proves
that (8.39) is satisﬁed for this choice of G′′. Finally, we claim that (8.39) implies (8.38) via standard
techniques from the robust mean estimation literature, e.g. in the proof of Lemma 3.2 in [194].
G.6.2
Proof of Proposition 36
In this section, we state FastCovFilter and prove Proposition 36, restated for convenience.
Proposition 36. There is an algorithm, FastCovFilter (Algorithm 92), taking inputs V := {vi}i∈[n] ∈
Rn×d, saturated weights w ∈∆n with respect to bipartition [n] = G ∪B with |B| = ϵn, δ ∈(0, 1],
and R ≥0 with the promise that

X
i∈G
1
|G|viv⊤
i

op
≤R.
Then, with probability at least 1 −δ, FastCovFilter returns saturated w′ ∈∆n such that

X
i∈[n]
w′
iviv⊤
i

op
≤5R.
The runtime of FastCovFilter is
O

nd log3(n) log
n
δ

.
Before proving Proposition 36, we require three helper facts.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
786
Algorithm 92: FastCovFilter(V, w, δ, R)
1 Input: V := {vi}i∈[n] ∈Rn×d, saturated weights w ∈∆n with respect to bipartition
[n] = G ∪B with |B| = ϵn for suﬃciently small ϵ, δ ∈(0, 1), R ≥

P
i∈G
1
|G|viv⊤
i

op;
2 Output: With probability ≥1 −δ, saturated w′ with respect to bipartition G ∪B, with

X
i∈[n]
w′
iviv⊤
i

op
≤5R.
;
3 Remove all i ∈[n] with ∥vi∥2
2 ≥nR, n ←new dataset size;
4 T ←O(log2 n) (for a suﬃciently large constant), t ←0, w(0) ←w;
5 while t < T and Power
P
i∈[n] w(t)
i viv⊤
i ,
δ
2T

> 2R do
6
Mt ←P
i∈[n] w(t)
i viv⊤
i , Yt ←Mlog d
t
;
7
Sample Ndir = O(log n
δ ) (for a suﬃciently large constant) vectors {uj}j∈[Ndir] ∈Rd each
with independent entries ±1. Let ˜uj ←Ytuj for all j ∈[Ndir].;
8
for j ∈[Ndir] do
9
τi ←⟨vi, ˜uj⟩2 for all i ∈[n];
10
τmax ←maxi∈[n]|wi̸=0 τi;
11
while P
i∈[n] w(t)
i τi ≥2R ∥˜uj∥2
2 do
12
w(t)
i
←

1 −
τi
τmax

w(t)
i
for all i ∈[n];
13
w(t+1) ←w(t), t ←t + 1;
14 Return: w(t);
Fact 40 (Theorem 1, [404]). For any δ ∈(0, 1] and M ∈Sd
≥0, there is an algorithm, Power(M, δ),
which returns with probability at least 1 −δ a value V such that λmax(M) ≥V ≥0.9λmax(M). The
algorithm costs O(log d
δ ) matrix-vector products through M plus O(d log d
δ ) additional runtime.
Fact 41 (Lemma 7, [289]). Let A, B ∈Sd
≥0 with A ⪰B, and p ∈N. Then Tr(Ap−1B) ≥Tr(Bp).
Fact 42. For any α ≥0 and A ∈Sd
≥0,
αTr(A2 log d) ≤Tr(A2 log d+1) + dα2 log d+1.
Proof. Every eigenvalue λ of A is either at least α (and hence λ2 log d+1 ≥αλ2 log d) or not (and
hence α2 log d+1 ≥αλ2 log d). Both of these cases are accounted for by the right hand side.
Proof of Proposition 36. We discuss correctness, runtime, and the failure probability separately.
Correctness. First, Line 3 is correct because these indices cannot belong to G as they would
certify a violation to the operator norm bound in the direction of v, so this preserves saturation.
Next, it is clear by Fact 40 that if the algorithm ever ends because Power returns too small a

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
787
number, the output is correct, so it suﬃces to handle the other case. Deﬁne the potential function
Φt := Tr(Y2
t ). Our main goal is to show that in every iteration the algorithm runs, Φt decreases
substantially. To this end, we claim that after all runs of Lines 8-14 of FastCovFilter have ﬁnished,
we have in all randomly sampled directions j ∈[Ndir] the guarantee
*
˜uj ˜u⊤
j ,
X
i∈[n]
w(t)
i viv⊤
i
+
≤2R ∥˜uj∥2
2 .
(G.27)
This is immediate from the termination condition on Line 11 for each j ∈[Ndir], the fact that weights
are monotone nonincreasing throughout the whole algorithm, and that the left hand side of (G.27)
is monotone nonincreasing as a function of the weights. Next, by the Johnson-Lindenstrauss lemma
of [6], for a suﬃciently large Ndir with probability at least 1 −
δ
4T ,
1
Ndir
X
j∈[Ndir]
⟨vi, ˜uj⟩2 =
1
Ndir
X
j∈[Ndir]
v⊤
i Yt˜uj ˜u⊤
j Ytvi ∈[0.95, 1.05]

viv⊤
i , Y2
t

for all i ∈[n].
Condition on this event for all runs of Lines 8-14 throughout the algorithm for the remainder of the
proof. Combining this guarantee with (G.27), we have that after Lines 8-14 terminate,
*
Y2
t ,
X
i∈[n]
w(t)
i viv⊤
i
+
=
X
i∈[n]
w(t)
i

viv⊤
i , Y2
t

≤1.1
Ndir
X
j∈[Ndir]
X
i∈[n]
w(t)
i

vi, ˜u2
j

= 1.1
Ndir
X
j∈[Ndir]
*
˜uj ˜u⊤
j ,
X
i∈[n]
w(t)
i viv⊤
i
+
≤2.2R
Ndir
X
j∈[Ndir]
∥˜uj∥2
2 .
Next, by the Johnson-Lindenstrauss lemma of [6], since all {uj}j∈[Ndir] were sampled independently
of Mt, we have with probability at least 1 −
δ
4T that
1
Ndir
X
j∈[Ndir]
∥˜uj∥2
2 =
1
Ndir
X
j∈[Ndir]
u⊤
j Y2
t uj ≤1.1Tr(Y2
t ).
Conditioning on this event in every iteration, at the start of the next iteration, we will have

Y2
t , Mt+1

≤2.5RTr(Y2
t ).
(G.28)

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
788
We now show how (G.28) implies a rapid potential decrease:
Φt+1 = Tr

M2 log d
t+1

≤
1
2.8RTr

M2 log d+1
t+1

+ d(2.8R)2 log d
≤
1
2.8RTr
 Y2
t Mt+1

+ d(2.8R)2 log d ≤0.9Φt + d(2.8R)2 log d.
In the ﬁrst inequality, we used Lemma 42 with α = 2.8R; in the second, we used Fact 41 with A =
Mt, B = Mt+1, and p = 2 log d + 1. The last inequality applied (G.28). The above display implies
that until Φt ≤20d(2.8R)2 log d, the potential is decreasing by a constant factor every iteration, and
Φ0 ≤dλmax(M0)2 log d ≤d(nR)2 log d, so within O(log2 n) iterations we will have
Φt = Tr(M2 log d
t
) ≤20d(2.8R)2 log d.
At this point, it is clear the operator norm of Mt achieves the desired bound of 5R. It remains
to show that all weight removals in Lines 11-13 were safe throughout the algorithm. Here we use
Lemma 130: it suﬃces to show that throughout the algorithm,
X
i∈G
w(t)
i τi ≤R ∥˜uj∥2
2 ,
(G.29)
because then whenever Line 11 fails, the scores are safe with respect to the weights and Lemma 130
applies. However, (G.29) follows from the assumption on

P
i∈G
1
|G|viv⊤
i

op, yielding
X
i∈G
w(t)
i τi ≤
X
i∈G
1
|G|τi =
*
˜uj ˜u⊤
j ,
X
i∈G
1
|G|viv⊤
i
+
≤R ∥˜uj∥2
2 .
Runtime. The cost of all lines other than Lines 6-7 and the repeated loops of Lines 11-13 clearly
fall within the budget. To implement Lines 6-7, we never need to form the matrices Mt or Yt, and
instead form all {˜uj}j∈[Ndir] in time
O

nd log d log n
δ

implicitly through matrix-vector multiplications with Mt, each of which take time O(nd). To im-
plement Lines 11-13, let ¯w denote the value of w(t) right after an execution of Line 10. We wish to
determine the smallest value K such that
X
i∈[n]

1 −
τi
τmax
K
¯wiτi ≤2R ∥˜uj∥2
2 .
Checking if the above display holds for a particular guess of K clearly takes O(n) time, and we can

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
789
upper bound K by the following inequality:
X
i∈[n]

1 −
τi
τmax
K
¯wiτi ≤
X
i∈[n]
exp

−Kτi
τmax

¯wiτi ≤
1
eK
X
i∈[n]
¯wiτmax ≤τmax
K .
Here the second inequality used x exp(−Cx) ≤
1
eC for all nonnegative x, C, where we chose C =
K
τmax
and x = τi. Now since τmax ≤∥˜uj∥2
2 ∥vi∥2
2 ≤nR ∥˜uj∥2
2 by Cauchy-Schwarz, we have that K = O(n)
as desired. At this point, a binary search on K suﬃces, so all loops take time O(n log n).
Failure probability. The only randomness used in the algorithm appears in the guarantees of
Power and the guarantees of the Johnson-Lindenstrauss projections. Taking a union bound over T
iterations shows these all succeed with probability at least 1 −δ.
G.7
Concentration
G.7.1
Sub-Gaussian concentration
We use the following concentration facts on sub-Gaussian distributions following from standard
techniques, and give an application bounding Schatten-norm deviations.
Lemma 316. Under Assumption 10, there are universal constants C1, C2 such that
Pr

sup
v∈Rd
∥v∥2=1
v⊤
 
1
n
X
i∈G′
XiX⊤
i −Σ
!
v
 −tv⊤Σv > 0

≤exp
 C1d −C2n min(t, t2)

.
Proof. By observing (8.71), it is clear that the random vector e
X = Σ−1
2 X for X ∼D has covariance
I and sub-Gaussian proxy cI. For any ﬁxed unit vector u, by Lemma 1.12 of [456], the random
variable (u⊤e
X)2 −1 is sub-exponential with parameter 16c, so by Bernstein's inequality (Theorem
1.13, [456]), deﬁning e
Xi = Σ−1
2 Xi for each Xi ∼D,
Pr
"u⊤
 
1
n
X
i∈G′
e
Xi e
X⊤
i −I
!
u
 > t
2
#
≤exp

−
n
211c2 min(t, t2)

.
For shorthand deﬁne M :=
1
n
P
i∈G′ e
Xi e
X⊤
i , and let N be a maximal
1
4-net of the unit ball (as
measured in ℓ2 distance). By Lemma 1.18 of [456], |N| ≤12d, so by a union bound,
Pr

sup
u∈N
u⊤(M −I)u
 > t
2

≤exp

3d −
n
211c2 min(t, t2)

.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
790
Next, by a standard application of the triangle inequality (see e.g. Exercise 4.3.3, [522])
sup
v∈Rd
∥v∥2=1
v⊤(M −I)v
 ≤2 sup
u∈N
u⊤(M −I)u
 ≤t
with probability at least 1 −exp
 C1d −C2n min(t, t2)

for appropriate C1, C2.
The conclusion
follows since its statement is scale invariant, so it suﬃces to show as we have that
Pr

sup
v∈Rd
∥v∥Σ=1
v⊤
 
1
n
X
i∈G′
XiX⊤
i −Σ
!
v
 −tv⊤Σv > 0

≤exp
 C1d −C2n min(t, t2)

.
Corollary 69. Let p ≥2. Under Assumption 10, there are universal constants C1, C2 with
Pr



1
n
X
i∈G′
XiX⊤
i −Σ

p
> t ∥Σ∥p

≤exp
 C1d −C2n min(t, t2)

.
Proof. Suppose the event in Lemma 316 does not hold, which happens with probability at least
1 −exp(C1d −C2n min(t, t2)). Deﬁne for shorthand M := 1
n
P
i∈G′ XiX⊤
i −Σ and let its spectral
decomposition be P
j∈[d] λjvjv⊤
j . By the triangle inequality and Fact 22,
∥M∥p ≤
X
j∈[d]
|λj|p−1
∥M∥p−1
p
v⊤
j
 
1
n
X
i∈G′
XiX⊤
i −Σ
!
vj

≤t
X
j∈[d]
|λj|p−1
∥M∥p−1
p
v⊤
j Σvj = t
* X
j∈[d]
|λj|p−1
∥M∥p−1
p
vjv⊤
j , Σ
+
≤t ∥Σ∥p .
In the last inequality, we used that P
j∈[d]
|λj|p−1
∥M∥p−1
p
vjv⊤
j has unit ℓq norm, and applied Fact 22.
G.7.2
Concentration under weightings in Sn
ϵ
We consider concentration of the empirical covariance under weightings which are not far from
uniform, in spectral and Schatten senses.
Lemma 317. Under Assumption 10, let δ ∈[0, 1], p ≥2, and n = Ω

d+log δ−1
(ϵ log ϵ−1)2

for a suﬃciently
large constant. Then for a universal constant C3,
Pr

∃w ∈Sn
ϵ


X
i∈G′
wiXiX⊤
i −Σ

p
> C3 · ϵ log ϵ−1 ∥Σ∥p

≤δ
2.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
791
Proof. Because the vertices of Sn
ϵ are uniform over sets S ⊆G′ with |S| = (1 −ϵ)n (see e.g. Section
4.1, [175]), by convexity of the Schatten-p norm it suﬃces to prove
Pr

∃S with |S| = (1 −ϵ)n


1
(1 −ϵ)n
X
i∈S
XiX⊤
i −Σ

p
> C3 · ϵ log ϵ−1 ∥Σ∥p

≤δ
4.
For any ﬁxed S, and recalling |Sc| = ϵn, we can decompose this sum as
1
(1 −ϵ)n
X
i∈S
XiX⊤
i =
1
1 −ϵ
 
1
n
X
i∈G′
XiX⊤
i
!
−
ϵ
1 −ϵ
 
1
|Sc|
X
i∈Sc
XiX⊤
i
!
.
(G.30)
By applying Corollary 69, it follows that by setting t = 1−ϵ
2
· ϵ log ϵ−1 and our choice of n that
Pr



1
n
X
i∈G′
XiX⊤
i −Σ

p
> 1 −ϵ
2
· ϵ log ϵ−1 ∥Σ∥p

≤δ
4.
(G.31)
Moreover, for any ﬁxed Sc, setting t = 1−ϵ
2
· C3 log ϵ−1 where C3 is a suﬃciently large constant, so
that for suﬃciently small ϵ, t = min(t, t2),
Pr



1
ϵn
X
i∈Sc
XiX⊤
i −Σ

p
> 1 −ϵ
2
· C3 · log ϵ−1 ∥Σ∥p

≤exp (C1d −C2ϵnt)
≤exp
 −
 log δ−1 + nϵ log ϵ−1
≤
δ
4
  n
ϵn
.
(G.32)
Here, we used that log
  n
ϵn

= O
 nϵ log ϵ−1
. Finally, union bounding over all possible sets Sc imply
that with probability at least 1 −δ
2, the following events hold:

1
n
X
i∈G′
XiX⊤
i −Σ

p
< 1 −ϵ
2
· ϵ log ϵ−1 ∥Σ∥p ,

1
|Sc|
X
i∈Sc
XiX⊤
i −Σ

p
< 1 −ϵ
2
· C3 · log ϵ−1 ∥Σ∥p for all S with |S| = (1 −ϵ)n.
Combining these bounds in the context of (G.30) after applying the triangle inequality, we have
with probability at least 1 −δ
2 for all S the desired conclusion,

1
(1 −ϵ)n
X
i∈S
XiX⊤
i −Σ

p
< C3 · ϵ log ϵ−1 ∥Σ∥p .

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
792
Corollary 70. Under Assumption 10, let n = Ω

d+log δ−1
(ϵ log ϵ−1)2

for a suﬃciently large constant. For
universal C3 and all w ∈Sn
ϵ , with probability at least 1 −δ
2,
C3 · ϵ log ϵ−1Σ ⪰
X
i∈G′
wiXiX⊤
i −Σ ⪰−C3 · ϵ log ϵ−1Σ.
Proof. Consider any unit vector v ∈Rd. By similar arguments as in (G.31), (G.32), and applying a
union bound over all S with |S| = (1−ϵ)n, with probability at least 1−δ
2, it follows from Lemma 316
that
v⊤
 
1
n
X
i∈G′
XiX⊤
i −Σ
!
v
 < 1 −ϵ
2
· ϵ log ϵ−1v⊤Σv,
(G.33)
v⊤
 
1
|Sc|
X
i∈Sc
XiX⊤
i −Σ
!
v
 < 1 −ϵ
2
· C3 · log ϵ−1v⊤Σv .
(G.34)
Therefore, again using the formula (G.30) and the triangle inequality yields the desired conclusion
for all directions v, which is equivalent to the spectral bound of the lemma statement.
G.8
Deferred proofs from Section 8.11.1
G.8.1
Robust univariate variance estimation
In this section, we prove Lemma 147, which allows us to robustly estimate the quadratic form of
a vector in the covariance of a sub-Gaussian distribution from corrupted samples. Algorithm 93 is
folklore, and intuitively very simple; it projects all samples onto u, throws away the 2ϵ fraction of
points with largest magnitude in this direction, and takes the mean of the remaining set.
Algorithm 93: Univariate variance estimation: 1DRobustVariance({Xi}i∈[n], ϵ, u)
1 Input: {Xi}i∈[n], ϵ > 0, and a unit vector u;
2 Let ai = ⟨Xi, u⟩2 for i = 1, . . . , n ;
3 Sort the ai in increasing order. WLOG assume a1 ≤a2 ≤. . . ≤an.;
4 Return: σ2
u =
1
(1−2ϵ)n
P(1−2ϵ)n
i=1
ai;
We require the following helper fact.
Fact 43. Let Z be a sub-exponential random variable with parameter at most λ3, and let ϵ ∈[0, 1].
Then, for any event E with Pr[Z ∈E] ≤ϵ, |E [Z ·1,1 [Z ∈E]] | ≤2λϵ log ϵ−1.
3We say mean-zero Z is sub-exponential with parameter λ if ∀|s| ≤λ−1, E[exp(sZ)] ≤exp( s2λ2
2
).

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
793
Proof. We have by H¨older's inequality that for any p, q ≥1 with p−1 + q−1 = 1,
|E [Z ·1,1 [Z ∈E]]| ≤E[|Z|p]1/p · ϵ1/q ≤2λp · ϵ1/q.
The second inequality is Lemma 1.10 [456]. Setting p = log ϵ−1 yields the result.
Lemma 147. Under Assumption 10, let δ ∈[0, 1], n = Ω

log δ−1
(ϵ log ϵ−1)2

, and u ∈Rd be a ﬁxed
unit vector. Algorithm 93, 1DRobustVariance, takes input {Xi}i∈[n], u, and ϵ, and outputs σ2
u with
|u⊤Σu −σ2
u| < Cu⊤Σu · ϵ log ϵ−1 with probability at least 1 −δ, and runs in time O(nd + n log n),
for C a ﬁxed multiple of the parameter c in Assumption 10.
Proof. The runtime claim is immediate; we now turn our attention to correctness. We follow notation
of Assumption 10, and in a slight abuse of notation, also deﬁne ai = ⟨Xi, u⟩2 for i ∈G′. First, for
X ∼D, then ⟨u, X⟩2 −u⊤Σu is sub-exponential with parameter at most 16cu⊤Σu (Lemma 1.12,
[456]). By Bernstein's inequality, we have that if X ∼D, then for all t ≥1,
Pr
h
⟨X, u⟩2 > 32ctu⊤Σu
i
≤exp(−t) .
(G.35)
Using this in a standard Chernoﬀbound, we have that with probability 1 −δ
2,
{i ∈G′ : ai > 64c log ϵ−1 · u⊤Σu}

n
≤ϵ .
(G.36)
Let T = 64c log ϵ−1 · u⊤Σu, and let Y be distributed as (⟨u, X⟩2 −u⊤Σu) ·1,1 [⟨u, X⟩2 ≤T], where
X ∼D. We observe Y −E[Y ] is also sub-exponential with parameter 16cu⊤Σu, and that by Fact 43,
|E[Y ]| ≤32cu⊤Σuϵ log ϵ−1.
(G.37)
Deﬁne the interval I = [0, T] and let S be the set of points in [n] that survive the truncation
procedure, so that σ2
u =
1
|S|
P
i∈S ai. Given event (G.36), ai ∈I for all i ∈S, since there are at
most ϵn points in G outside I, and |B| ≤ϵn. We decompose the deviation as follows:
X
i∈S
ai −|S|u⊤Σu =
X
i∈G∩S
(ai −u⊤Σu) +
X
i∈B∩S
(ai −u⊤Σu)
=
X
i∈G′∩I
(ai −u⊤Σu) +
X
i∈B∩S
(ai −u⊤Σu)
−
X
i∈(G′\G)∩I
(ai −u⊤Σu) −
X
i∈(G∩I)\S
(ai −u⊤Σu).
(G.38)
Here we overloaded i ∈I to mean that ai lies in the interval I, and conditioned on S lying entirely
in I. We bound each of these terms individually. First, for all i ∈G′ ∩I, conditioning on (G.36)
(i.e. all ai ∈I), ai −u⊤Σu is an independent sample from Y . Thus, by (G.37) and Bernstein's

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
794
inequality,

1
|G′ ∩I|
X
i∈G′∩I
(ai −u⊤Σu)
 ≤

1
|G′ ∩I|
X
i∈G′∩I
(ai −u⊤Σu) −E[Y ]
 + 32cu⊤Σuϵ log ϵ−1
≤64c · u⊤Σuϵ log ϵ−1,
(G.39)
with (conditional) probability at least 1 −δ
2. By a union bound, both events occur with probability
at least 1 −δ; condition on this for the remainder of the proof. Under this assumption, we control
the other three terms of (G.38). Observe that |B∩S| ≤ϵn, |(G′\G)∩I| ≤ϵn, and |(G∩I)\S| ≤ϵn.
Further, by deﬁnition of I, every summand is at most 64c log ϵ−1 · u⊤Σu. Thus,

X
i∈B∩S
(ai −u⊤Σu)
 ≤64cϵn log ϵ−1 · u⊤Σu,
(G.40)

X
i∈(G′\G)∩I
(ai −u⊤Σu)

≤64cϵn log ϵ−1 · u⊤Σu.
(G.41)

X
i∈(G′∩I)\S
(ai −u⊤Σu)

≤64cϵn log ϵ−1 · u⊤Σu.
(G.42)
Combining (G.39), (G.40), (G.41), and (G.42) in derivation (G.38) and dividing by |S| yields the
claim.
Finally, we also give an alternative set of conditions under which we can certify correctness of
1DRobustVariance. Speciﬁcally, this assumption will be useful in lifting indpendence assumptions
between u and our samples {Xi}i∈[n] in repeated calls within Algorithm 94.
Assumption 16. Under Assumption 10, let the following conditions hold for universal constant
C4:
C4ϵ log ϵ−1 · Σ ⪰1
n
X
i∈G′
XiX⊤
i −Σ ⪰−C4ϵ log ϵ−1 · Σ,
(G.43)
C4 log ϵ−1 · Σ ⪰
X
i∈G′
wi
 XiX⊤
i −Σ

⪰−C4 log ϵ−1 · Σ for all w ∈Sn
1−ϵ.
(G.44)
Note that (G.44) is a factor ϵ weaker in its guarantee than Corollary 70, and is over weights in a
diﬀerent set Sn
1−ϵ. Standard sub-Gaussian concentration (i.e. an unweighted variant of Corollary 70)
and modifying the proof of Corollary 70 to take the constraint set Sn
1−ϵ and normalizing over vertex
sets of size ϵn yield the following conclusion.
Lemma 318. Let n = Ω

d+log δ−1
(ϵ log ϵ−1)2

for a suﬃciently large constant. Assumption 16 holds with
probability at least 1 −δ
2.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
795
We give a variant of Lemma 147 with slightly stronger guarantees for 1DRobustVariance; speciﬁ-
cally, it holds for all u simultaneously for a ﬁxed set of samples satisfying Assumption 16.
Corollary 71. Under Assumption 16, Algorithm 93 outputs σ2
u with |u⊤Σu −σ2
u| < Cu⊤Σu ·
ϵ log ϵ−1, for C a ﬁxed multiple of the parameter c in Assumption 10, and runs in time O(nd+n log n).
Proof. We discuss how to modify the derivations from Lemma 147 appropriately in the absence of
applications of Bernstein's inequality. First, note that appropriately combining (G.43) and (G.44)
in a derivation such as (G.30) yields the following bound (deterministically under Assumption 16):
C4ϵ log ϵ−1 · Σ ⪰
X
i∈G′
wi
 XiX⊤
i −Σ

⪰−C4ϵ log ϵ−1Σ for all w ∈Sn
3ϵ.
(G.45)
Now, consider the decomposition (G.38). We claim ﬁrst that similarly to (G.40), (G.41), (G.42) we
can bound each summand in the latter three terms by O(u⊤Σu log ϵ−1); to prove this, it suﬃces to
show that at least one ﬁltered ai attains this bound, as then by deﬁnition of the algorithm, each
non-ﬁltered ai will as well. Note that a fraction between ϵ and 2ϵ of points in G ⊂G′ is ﬁltered (since
there are only ϵn points from B). The assumption (G.44) then implies precisely the desired bound
on some ﬁltered ai by placing uniform mass on ﬁltered points from G, and applying pigeonhole.
So, all non-ﬁltered ai are bounded by O(u⊤Σu log ϵ−1), yielding analogous statements to (G.40),
(G.41), (G.42).
Finally, an analogous derivation to (G.39) follows via an application of the bound (G.45), where
we place uniform mass on the set G′∩I and adjust constants appropriately, since the above argument
shows that under the assumption (G.44), we have that at most 2ϵn indices i ∈G′ have ai ̸∈I.
G.8.2
Preliminaries
For convenience, we give the following preliminaries before embarking on our proof of Theorem 60
and giving guarantees on Algorithm 94.
First, we state a set of assumptions which augments
Assumption 16 with one additional condition, used in bounding the iteration count of our algorithm.
Assumption 17. Under Assumption 10, let Assumption 16 hold, as well as the following additional
condition for the same universal constant C4:
∥Xi∥2
2 ≤C4 log n
δ · Tr(Σ) for all i ∈G.
(G.46)
Standard sub-Gaussian concentration inequalities and a union bound, combined with our earlier
claim Lemma 318, then yield the following guarantee.
Lemma 319. Let n = Ω

d+log δ−1
(ϵ log ϵ−1)2

for a suﬃciently large constant. Assumption 17 holds with
probability at least 1 −δ.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
796
G.8.3
Analysis of PCAFilter
For this section, for any nonnegative weights w, deﬁne M(w) := P
i∈[n] wiXiX⊤
i . We now state
our algorithm, PCAFilter. At all iterations t, it maintains a current nonnegative weight vector w(t)
(initialized to be the uniform distribution on [n]), preserving the following invariants for all t:
w(t−1)
i
≥w(t)
i
for all i ∈[n],
X
i∈B
w(t−1)
i
−w(t)
i
≥
X
i∈G
w(t−1)
i
−w(t)
i .
(G.47)
We now state our method as Algorithm 94; note that the update to w(t) is of the form in
Lemma 148.
Algorithm 94: PCAFilter({Xi}i∈[n], ϵ)
1 Remove all points i ∈[n] with ∥Xi∥2
2 > c log( n
δ ) · Tr(Σ);
2 w(0)
i
←1
n for all i ∈[n], t ←1;
3 u1 ←approximate top eigenvector of M(w(0));
4 σ2
1 ←1DRobustVariance({Xi}i∈[n], ϵ, u1);
5 while u⊤
t M(w(t−1))ut > (1 + 5C5ϵ log ϵ−1)σ2
t , where C5 = max(C, C4) from constants in
Assumption 16, Corollary 71 do
6
ai ←⟨ut, Xi⟩2 for all i ∈[n];
7
Sort (permute) the indices [n] so the ai are in increasing order (with a1 smallest, an
largest);
8
Let ℓbe the largest index with Pn
i=ℓwi ≥2ϵ;
9
Deﬁne
w(t)
i
←




1 −ai
an

w(t−1)
i
ℓ≤i ≤n
w(t−1)
i
i < ℓ
;
10
ut ←approximate top eigenvector of M(w(t));
11
σ2
t ←1DRobustVariance({Xi}i∈[n], ut, ϵ);
12
t ←t + 1;
13 Return: ut;
We assume that in Line 8, we also have Pn
i=ℓwi ≤3ϵ, as we can assume at least one point is
corrupted i.e. ϵ ≥1
n (else standard algorithms suﬃce for our setting), so adding an additional wi
can only change the sum by ϵ. We ﬁrst prove invariants (G.47) are preserved; at a high level, we
simply demonstrate that Lemma 148 holds via concentration on G and lack of termination.
Lemma 320. Under Assumption 16, for any iteration t of Algorithm 94, suppose (G.47) held for
all iterations t′ ≤t −1. Then, (G.47) holds at iteration t.
Proof. The ﬁrst part of (G.47) is immediate by observing the update in Line 9, so we show the

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
797
second. We drop subscripts and superscripts for conciseness and focus on a single iteration t. Let
IB = {ℓ, . . . , n} ∩B, and IG = {ℓ, . . . , n} ∩G. By Lemma 148, it suﬃces to demonstrate that
X
i∈IB
wiai >
X
i∈IG
wiai.
(G.48)
First, P
i∈IB wi ≤ϵ, so by deﬁnition of index ℓ, we have ϵ ≤P
i∈IG wi ≤2ϵ. Deﬁne ˜wi =
wi
P
i∈IG wi if
i ∈IG, and 0 otherwise, and observe ew ∈Sn
1−2ϵ. By modifying constants appropriately from (G.44),
it follows from deﬁnition of ai = u⊤XiX⊤
i u that
X
i∈IG
wiai ≤
 X
i∈IG
wi
!
· C4 log ϵ−1 · u⊤Σu ≤2C4ϵ log ϵ−1 · u⊤Σu.
(G.49)
On the other hand, by (G.45) we know that the total quadratic form over G is bounded as
X
i∈G
wiai <
 X
i∈G
wi
!
 1 + C4ϵ log ϵ−1
u⊤Σu <
 1 + C4ϵ log ϵ−1
u⊤Σu.
(G.50)
Here, we applied the observation that the normalized wi restricted to G are in Sn
1−3ϵ (e.g. using
Lemma 321 inductively). However, since we did not terminate (Line 5), we must have by ut being
a top eigenvector and Corollary 71 (we defer discussions of inexactness to Theorem 60) that
X
i∈[n]
wiai ≥(1 + 5C5ϵ log ϵ−1)σ2
t ≥(1 + 4C4ϵ log ϵ−1) · u⊤Σu
=⇒
X
i∈B
wiai > 3C4ϵ log ϵ−1 · u⊤Σu.
To obtain the last conclusion, we used (G.50). Finally, note that for all i ∈B \ IB,
ai ≤aℓ≤
X
i∈IG
ewiai ≤C4 log ϵ−1 · u⊤Σu
by rearranging (G.49). This implies that
X
i∈B\IB
wiai ≤

X
i∈B\IB
wi

· C4 log ϵ−1 · u⊤Σu ≤C4ϵ log ϵ−1 · u⊤Σu.
Thus, the desired inequality (G.48) follows from combining the above derivations, e.g. using (G.49)
and
X
i∈IB
wiai =
X
i∈B
wiai −
X
i∈B\IB
wiai > 2C4ϵ log ϵ−1 · u⊤Σu.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
798
Lemma 320 yields for all t that P
i∈B w(0)
i
−w(t)
i
≥P
i∈G w(0)
i
−w(t)
i
by telescoping. Note that
we can only remove at most 2ϵ mass from w total, as P
i∈B w(0)
i
−w(t)
i
≤ϵ. Denote for shorthand
normalized weights v(t) :=
w(t)
∥w(t)∥1
. Then, the following is immediate by
w(t)
1 ≥1 −2ϵ.
Lemma 321. Under Assumption 16, in all iterations t of Algorithm 94, v(t) ∈Sn
2ϵ.
Using Lemma 321, we show that the output has the desired quality of being a large eigenvector.
Lemma 322. Under Assumption 16, let the output of Algorithm 94 be uT . Then for a universal
constant C⋆, u⊤
T ΣuT ≥(1 −C⋆ϵ log ϵ−1)∥Σ∥∞.
Proof. We assume for now that uT is an exact top eigenvector, and discuss inexactness while proving
Theorem 60. By (G.45) and Lemma 321, as then the normalized restriction of w(T ) to G is in Sn
3ϵ,
M(w(T )) ⪰
X
i∈G
w(T )
i
XiX⊤
i ⪰
 1 −2C4ϵ log ϵ−1
Σ
=⇒u⊤
T M(w(T ))uT ≥
 1 −2C4ϵ log ϵ−1
∥Σ∥∞.
We used the Courant-Fischer characterization of eigenvalues, and that uT is a top eigenvector of
M(w(T )). Moreover, by termination conditions and Corollary 71 (correctness of 1DRobustVariance),
(1 + Cϵ log ϵ−1)u⊤
T ΣuT ≥σ2
T ≥(1 + 5C5ϵ log ϵ−1)−1u⊤
T M(w(T ))uT .
Combining these two bounds and rescaling yields the conclusion.
Finally, we prove our main guarantee about Algorithm 94.
Theorem 60. Under Assumption 10, let δ ∈[0, 1], and n = Ω

d+log δ−1
(ϵ log ϵ−1)2

. Algorithm 94 runs in
time O( nd2
ϵ log n
δϵ log n
δ ), and outputs u with u⊤Σu > (1−C⋆ϵ log ϵ−1)∥Σ∥∞, for C⋆a ﬁxed multiple
of parameter c in Assumption 10, with probability at least 1 −δ.
Proof. First, we will operate under Assumption 17, which holds with probability at least 1 −δ. It is
clear that the analyses of Lemma 320 and 322 hold with 1−Θ(ϵ log ϵ−1) multiplicative approximations
of top eigenvector computation, which the power method approximates with high probability. Thus,
each iteration takes time O
  nd
ϵ log n
δϵ

, where we will union bound over the number of iterations.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
799
We now give an iteration bound: in any iteration where we do not terminate, Lemma 148 implies
n
X
i=1
w(t−1)
i
−w(t)
i
≥
1
2 maxi∈[n] ⟨ut, Xi⟩2
n
X
i=ℓ
wiai
≥
1
2C4 log n
δ · Tr(Σ)
n
X
i=ℓ
wiai
≥
1
2C4 log n
δ · Tr(Σ)
 Pn
i=ℓwi
P
i∈[n] wi
! X
i∈[n]
wiai
= Ω

ϵ ·
∥Σ∥∞
log n
δ · Tr(Σ)

= Ω

ϵ
d log n
δ

.
Here, the second line used Assumption 17, the third used that the ai are in sorted order, and the last
used the deﬁnition of ℓas well as the derivations of Lemma 322 (speciﬁcally, that M(w) spectrally
dominates (1 −O(ϵ log ϵ−1))Σ for roughly uniform w). The conclusion follows since there can be at
most O(d log n
δ ) iterations, as the algorithm terminates when a 2ϵ fraction of the mass is removed,
giving the overall runtime claim.
G.9
Deferred proofs from Section 8.11.2
G.9.1
Proof of Proposition 40
Proposition 40. There is an algorithm Power (Algorithm 1, [404]), parameterized by t ∈[d],
tolerance ˜ϵ > 0, p ≥1, and A ∈Sd
≥0, which outputs orthonormal {zj}j∈[t] with the guarantee



z⊤
j Apzj −λp
j(A)

≤˜ϵλp
j(A)
z⊤
j Ap−1zj −λp−1
j
(A)

≤˜ϵλp−1
j
(A)
for all j ∈[t].
(8.73)
Here, λj(A) is the jth largest eigenvalue of A. The total time required by the method is O(nnz(A) tp log d
ε
).
Proof. We claim that Algorithm 1 in [404] applied to the matrix Ap with a careful choice of exponent
q in their Algorithm 1 yields this guarantee. Speciﬁcally, we choose q1, q2, both of which satisfy the
criteria in their main theorem, such that the iterates produced by simultaneous power iteration Mp
with exponent q1 and Mp−1 with exponent q2 are identical; it suﬃces to choose q a multiple of
p(p −1). Thus, we can also apply their guarantees to Ap−1 and apply a union bound. Notice that
their Algorithm 1 also contains some postprocessing to ensure that they obtain singular values in
the right space, which is unnecessary for us, as our matrices are Hermitian.

APPENDIX G. DEFERRED PROOFS FROM CHAPTER 8
800
G.9.2
Proof of Lemma 149
Lemma 149. Let n = Ω

d+log δ−1
(ϵ log ϵ−1)2

. With probability 1 −δ
2, the uniform distribution over G
attains value (1+ ˜ϵ
2)∥Σ∥p for objective (8.72), where ˜ϵ = C′ϵ log ϵ−1 for a universal constant C′ > 0.
Proof. Lemma 317 implies that letting w∗be the uniform distribution over the uncorrupted samples
amongst X1, . . . , Xn, we have with probability at least 1 −δ
2, and denoting ˜ϵ := 2C3 · ϵ log ϵ−1,

X
i∈[n]
w∗
i XiX⊤
i

p
≤

1 + ˜ϵ
2

∥Σ∥p .
Therefore, the mixed ℓ∞-ℓp packing semideﬁnite program
∃w∗∈∆n with ∥w∗∥∞≤
1
(1 −ϵ)n,

X
i∈[n]
w∗
i XiX⊤
i

p
≤

1 + ˜ϵ
2

∥Σ∥p
is feasible. This completes the proof.
G.9.3
Proof of Lemma 150
Lemma 150. Let n = Ω

d+log δ−1
(ϵ log ϵ−1)2

. With probability at least 1 −δ
2, (1 + ˜ϵ)Σ ⪰MG ⪰(1 −˜ϵ)Σ.
Proof. We follow the notation of (8.74). First, by the guarantees of Corollary 40,
wG = 1 −wB ≥1 −
ϵn
(1 −2ϵ)n = 1 −3ϵ
1 −2ϵ ≥1 −2ϵ.
Therefore, again applying Corollary 40, for all i ∈G,
wi
wG
≤
1
(1 −2ϵ)n · 1 −2ϵ
1 −3ϵ =
1
(1 −3ϵ)n.
We conclude that the set of weights { wi
wG }i∈G belong to S(1−ϵ)n
3ϵ
. By applying Corollary 70 to these
weights and adjusting the deﬁnition of C3 by a constant, we conclude with probability at least 1−δ
2
 1 + C3 · ϵ log ϵ−1
Σ ⪰
X
i∈G
wi
wG
XiX⊤
i ⪰
 1 −C3 · ϵ log ϵ−1
Σ.
The conclusion follows by multiplying through by wG, and using the deﬁnition ˜ϵ = 2C3·ϵ log ϵ−1.

Appendix H
Deferred proofs from Chapter 9
H.1
Deferred proofs from Section 9.2
H.1.1
Polynomial approximation to the square root
We give a proof of Fact 23.
Fact 23 (Polynomial approximation of √·). Let M ∈Sd
≻0 have µI ⪯M ⪯κµI where µ is known.
Then for any δ ∈(0, 1), there is an explicit polynomial p of degree O(√κ log κ
δ ) with
(1 −δ)M
1
2 ⪯p(M) ⪯(1 + δ)M
1
2 .
Proof. We will instead prove the following fact: for any ϵ ∈(0, 1), there is an explicit degree-
O
 √κ log κ
ϵ

polynomial p satisfying
max
x∈[ 1
κ ,1] |p(x) −√x| ≤ϵ.
The conclusion for arbitrary scalars with multiplicative range [µ, κµ] will then follow from setting
ϵ = δκ−1
2 (giving a multiplicative error guarantee), and the fact that rescaling the range [ 1
κ, 1] will
preserve this multiplicative guarantee (adjusting the coeﬃcients of the polynomial as necessary, since
µ is known). Finally, the conclusion for matrices follows since p(M) and M
1
2 commute.
Denote γ = 1
κ for convenience. We ﬁrst shift and scale the function √x to adjust the region of
approximation from [γ, 1] to [−1, 1]. In particular, let h(x) =
q
1−γ
2 x + γ+1
2 . If we can ﬁnd some
degree-∆polynomial g(x) with |g(x) −h(x)| ≤ϵ for all x ∈[−1, 1], then
p(x) = g

2
1 −γ x −1 + γ
1 −γ

801

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
802
provides the required approximation to √x.
To construct g, we take the Chebyshev interpolant of h(x) on the interval [−1, 1]. Since h is
analytic on [−1, 1], we can apply standard results on the approximation of analytic functions by
polynomials, and speciﬁcally Chebyshev interpolants. Speciﬁcally, by Theorem 8.2 in [509], if h(z)
is analytic in an open Bernstein ellipse with parameter ρ in the complex plane, then:
max
x∈[−1,1] |g(x) −h(x)| ≤4M
ρ −1ρ−∆,
where M is the maximum of |h(z)| for z in the ellipse. It can be checked that h(x) is analytic on
an open Bernstein ellipse with parameter ρ = 1+√γ
1−√γ — i.e. with major axis length ρ + ρ−1 = 2 1+γ
1−γ .
We can then check that M = √1 + γ ≤
√
2 and ρ −1 ≥2√γ. Since for all γ < 1,
1 −√γ
1 + √γ
1/2γ
≤1
e,
we conclude that 4M
ρ−1ρ−∆≤ϵ as long as ∆≥
1
2γ log

ϵ
√2γ

, which completes the proof.
H.1.2
Deferred proofs from Section 9.2.2
Lemma 153. Let B ∈Rd×d and let A ∈Sd
≻0. Then min (∥AB∥2 , ∥BA∥2) ≥
1
κ(A) ∥B∥2 ∥A∥2.
Proof. We begin with the ﬁrst entry in the above minimum. Let v be the unit vector with ∥Bv∥2 =
∥B∥∞, and note ∥ABv∥2 ≥
1
κ(A) ∥A∥∞∥Bv∥2 by deﬁnition of κ(A). Hence,
∥AB∥∞≥∥ABv∥2 ≥
1
κ(A) ∥A∥∞∥Bv∥2 =
1
κ(A) ∥A∥∞∥B∥∞.
We move onto the second entry. Let v be a vector such that ∥Av∥2 and ∥BAv∥2 = ∥B∥∞; note
that ∥v∥2 ≤
κ(A)
∥A∥∞. The conclusion follows from rearranging the following display:
κ(A) ∥BA∥∞
∥A∥∞
≥∥BA∥∞∥v∥2 ≥∥BAv∥2 = ∥B∥∞.
Lemma 161. For any matrix K ∈Sd
≻0 and λ ≥0, κ⋆
o(K + λI) ≤κ⋆
o(K).
Proof. By scaling K by λ appropriately (since κ⋆
o is invariant under scalar multiplication), it suﬃces
to take λ = 1. The deﬁnition of κ⋆
o implies there exists a diagonal matrix W such that
I ⪯W
1
2 KW
1
2 ⪯κ⋆
o(K)I ⇐⇒W−1 ⪯K ⪯κ⋆
o(K)W−1.
(H.1)

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
803
Thus, to demonstrate κ⋆
o(K + I) ≤κ⋆
o(K) it suﬃces to exhibit a diagonal f
W such that
f
W ⪯K + I ⪯κ⋆
o(K)f
W.
We choose f
W = W−1 + I; then, the above display follows from (H.1) and I ⪯I ⪯κ⋆
o(K)I.
Lemma 162. Let K ∈Sd
≻0. Then, for λ ≥1
ϵ λmax(K), κ(K+λI) ≤1+ϵ. Moreover, given a diagonal
W ∈Sd
≥0 such that κ(W
1
2 (K+λI)W
1
2 ) ≤κscale for 0 ≤λ ≤ϵλmin(K)
1+ϵ
, κ(W
1
2 KW
1
2 ) ≤(1+ϵ)κscale.
Proof. To see the ﬁrst claim, the largest eigenvalue of K+λI is at most λ+λmax(K) and the smallest
is at least λ, so the condition number is at most 1 + ϵ as desired.
To see the second claim, it follows from the fact that outer rescalings preserve Loewner order,
and then combining
K ⪯K + λI =⇒λmax

W
1
2 KW
1
2

≤λmax

W
1
2 (K + λI) W
1
2

,
K ⪰
1
1 + ϵ (K + λI) =⇒λmin

W
1
2 KW
1
2

≥
1
1 + ϵλmin

W
1
2 (K + λI) W
1
2

.
Lemma 163. Let K ∈Sd
≻0, and let W ∈Sd
≥0 be diagonal. Then for any λ > 0,
κ

W
1
2 (K + λI) W
1
2

≤2κ

W
1
2

K + λ
2 I

W
1
2

.
Proof. First, because outer rescalings preserve Loewner order, it is immediate that
K + λ
2 I ⪯K + λI =⇒λmax

W
1
2

K + λ
2

W
1
2 I

≤λmax

W
1
2 (K + λI) W
1
2

.
Moreover, the same argument shows that
1
2K + λ
2 I ⪯K + λ
2 I =⇒λmin

W
1
2

K + λ
2 I

W
1
2

≥1
2λmin

W
1
2 (K + I) W
1
2

.
Combining the above two displays yields the conclusion.
H.1.3
Normalizing the diagonal
In this section, we analyze a popular heuristic for computing diagonal preconditioners. Given a
positive deﬁnite matrix K ∈Sd
≻0, consider applying the outer scaling
W
1
2 KW
1
2 , where W = diag (w) and wi := K−1
ii
for all i ∈[d].
(H.2)

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
804
In other words, the result of this scaling is to simply normalize the diagonal of K to be all ones;
we remark W has strictly positive diagonal entries, else K is not positive deﬁnite. Also called the
Jacobi preconditioner, a result of Van de Sluis [251, 518] proves that for any matrix this scaling
leads to a condition number that is within an m factor of optimal, where m ≤d is the maximum
number of non-zeros in any row of K. For completeness, we state a generalization of Van de Sluis's
result below. We also require a simple fact; both are proven at the end of this section.
Fact 44. For any A, B ∈Sd
≻0, κ(A
1
2 BA
1
2 ) ≤κ(A)κ(B).
Proposition 74. Let W be deﬁned as in (H.2) and let m denote the maximum number of non-zero's
in any row of K. Then,
κ

W
1
2 KW
1
2

≤min

m,
p
nnz(K)

· κ⋆
o (K) .
Note that m and
p
nnz(K) are both ≤d, so it follows that κ(W
1
2 KW
1
2 ) ≤d·κ⋆
o (K). While the
approximation factor in Proposition 74 depends on the dimension or sparsity of K, we show that
a similar analysis actually yields a dimension-independent approximation. Speciﬁcally, the Jacobi
preconditioner always obtains condition number no worse than the optimal squared. To the best of
our knowledge, this simple but powerful bound has been observed in prior work.
Proposition 75. Let W be deﬁned as in (H.2). Then,
κ

W
1
2 KW
1
2

≤(κ⋆
o (K))2 .
Proof. Let W⋆attain the minimum in the deﬁnition of κ⋆
o, i.e. κ(K⋆) = κ⋆
o(K) for K⋆:= W
1
2⋆KW
1
2⋆.
Note that since [W
1
2 KW
1
2 ]ii = 1 by deﬁnition of W it follows that for all i
[W⋆W−1]ii = [W⋆W−1]ii · [W
1
2 KW
1
2 ]ii = [K⋆]ii = e⊤
i K⋆ei ∈[λmin(K⋆), λmax(K⋆)]
where the last step used that λmin(K⋆)I ⪯K⋆⪯λmax(K⋆)I. Consequently, for f
W := W−1
⋆W it
follows that κ(f
W) = κ(f
W−1) ≤λmax(K⋆)/λmin(K⋆) = κ(K⋆). The result follows from Fact 44 as
κ

W
1
2 KW
1
2

= κ

f
W
1
2 K⋆f
W
1
2

≤κ

f
W

κ (K⋆) ≤(κ⋆
o)2 .
Next, we demonstrate that Proposition 75 is essentially tight by exhibiting a family of matrices
which attain the bound of Proposition 75 up to a constant factor. At a high level, our strategy is
to create two blocks where the "scales" of the diagonal normalizing rescaling are at odds, whereas a
simple rescaling of one of the blocks would result in a quadratic savings in conditioning.

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
805
Proposition 76. Consider a 2d × 2d matrix M such that
K =

A
0
0
B

, A =
√
dI + 11⊤, B = I −
1
√
d + d
11⊤,
where A and B are d × d. Then, deﬁning W as in (H.2),
κ

W
1
2 KW
1
2

= Θ(d), κ⋆
o (K) = Θ
√
d

.
Proof. Because W
1
2 KW
1
2 is blockwise separable, to understand its eigenvalue distribution it suﬃces
to understand the eigenvalues of the two blocks. First, the upper-left block (the rescaling of the
matrix A) is multiplied by
1
√
d+1. It is straightforward to see that the resulting eigenvalues are
√
d
√
d + 1
with multiplicity d −1,
√
d with multiplicity 1.
Similarly, the bottom-right block is multiplied by
d+
√
d
d+
√
d−1, and hence its rescaled eigenvalues are
d +
√
d
d +
√
d −1
with multiplicity d −1,
√
d
d +
√
d −1
with multiplicity 1.
Hence, the condition number of W
1
2 KW
1
2 is d +
√
d −1 = Θ(d). However, had we rescaled the
top-left block to be a
√
d factor smaller, it is straightforward to see the resulting condition number
is O(
√
d). On the other hand, since the condition number of K is O(d), Proposition 75 shows that
the optimal condition number κ⋆
o(K) is Ω(
√
d), and combining yields the claim. We remark that as
d →∞, the constants in the upper and lower bounds agree up to a low-order term.
Finally, we provide the requisite proofs of Fact 44 and Proposition 74.
Fact 44. For any A, B ∈Sd
≻0, κ(A
1
2 BA
1
2 ) ≤κ(A)κ(B).
Proof. It is straightforward from λmin(A)I ⪯A ⪯λmax(A)I that
p
λmin(A) ∥u∥2 ≤
A
1
2 u

2 ≤
p
λmax(A) ∥u∥2 ,
and an analogous fact holds for B. Hence, we can bound the eigenvalues of A
1
2 BA
1
2 :
λmax

A
1
2 BA
1
2

= max
∥u∥2=1 u⊤A
1
2 BA
1
2 Bu ≤λmax(A) max
∥v∥2=1 v⊤Bv = λmax(A)λmax(B),
λmin

A
1
2 BA
1
2

= min
∥u∥2=1 u⊤A
1
2 BA
1
2 Bu ≥λmin(A) min
∥v∥2=1 v⊤Bv = λmin(A)λmin(B).
Dividing the above two equations yields the claim.

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
806
Proposition 74. Let W be deﬁned as in (H.2) and let m denote the maximum number of non-zero's
in any row of K. Then,
κ

W
1
2 KW
1
2

≤min

m,
p
nnz(K)

· κ⋆
o (K) .
Proof. Throughout let κ⋆
o := κ⋆
o(K) for notational convenience. Let W⋆obtain the minimum in
the deﬁnition of κ⋆
o and let B = W
1
2⋆KW
1
2⋆. Also let WB be the inverse of a diagonal matrix with
the same entries as B's diagonal. Note that κ(B) = κ⋆
o and W
1
2
BBW
1
2
B = W
1
2 KW
1
2 . So, to prove
Proposition 74, it suﬃces to prove that
κ

W
1
2
BBW
1
2
B

≤min

m,
p
nnz(K)

· κ⋆
o.
Let dmax denote the largest entry in W−1
B .
We have that dmax ≤λmax(B).
Then let M =
(dmaxWB)
1
2 B(dmaxWB)
1
2 and note that all of M's diagonal entries are equal to dmax and κ (M) =
κ(W
1
2
BBW
1
2
B). Moreover, since dmaxWB has all entries ≥1, λmin(M) ≥λmin(B). Additionally, since
a PSD matrix must have its largest entry on the diagonal, we have that ∥M∥2
F ≤nnz(M)d2
max ≤
nnz(M)λmax(B)2. Accordingly, λmax(M) = ∥M∥2 ≤∥M∥F ≤
p
nnz(M)λmax(B).
From this lower bound on λmin(M) and upper bound on λmax(M), we have that
κ

W
1
2 KW
1
2

= κ (M) ≤
p
nnz(M)λmax(B)
λmin(B)
=
p
nnz(M) · κ(B).
This proves one part of the minimum in Proposition 74. The second, which was already proven
in [518] follows similarly. In particular, by the Gershgorin circle theorem we have that λmax(M) ≤
maxi∈[d] ∥Mi:∥1, where Mi: denotes the ith row for M.
Since all entries in M are bounded by
dmax ≤λmax(B), we have that maxi∈[d] ∥Mi:∥1 ≤mλmax(B), and thus
κ

W
1
2 KW
1
2

= κ (M) ≤mλmax(B)
λmin(B)
= m · κ(B).
H.1.4
Faster scalings with a conjectured subroutine
In this section, we demonstrate algorithms which achieve runtimes which scale as eO(
√
κ⋆)1 matrix-
vector multiplies for computing approximately optimal scalings, assuming the existence of a suﬃ-
ciently general width-independent mixed packing and covering (MPC) SDP solver. Such runtimes
(which improve each of Theorems 62 and 63 by roughly a κ⋆factor) would nearly match the cost
of the fastest solvers after rescaling, e.g. conjugate gradient methods. We also demonstrate that we
1Throughout this section for brevity, we use κ⋆to interchangeably refer to the quantities κ⋆
i or κ⋆
o of a particular
appropriate inner or outer rescaling problem.

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
807
can achieve near-optimal algorithms for computing constant-factor optimal scalings for average-case
notions of conditioning under this assumption.
We ﬁrst recall the deﬁnition of the general MPC SDP feasibility problem.
Deﬁnition 59 (MPC feasibility problem). Given sets of matrices {Pi}i∈[n] ∈Sdp
≥0 and {Ci}i∈[n] ∈
Sdc
≥0, and error tolerance ϵ ∈(0, 1), the mixed packing-covering (MPC) feasibility problem asks to
return weights w ∈Rn
≥0 such that
λmax

X
i∈[n]
wiPi

≤(1 + ϵ)λmin

X
i∈[n]
wiCi

,
(H.3)
or conclude that the following is infeasible for w ∈Rn
≥0:
λmax

X
i∈[n]
wiPi

≤λmin

X
i∈[n]
wiCi

.
(H.4)
If both (H.3) is feasible and (H.4) is infeasible, either answer is acceptable.
Throughout this section, we provide eﬃcient algorithms under Assumption 18: namely, that
there exists a solver for the MPC feasibility problem at constant ϵ with polylogarithmic iteration
complexity and suﬃcient approximation tolerance. Such a solver would improve upon our algorithm
in Section 7.4 both in generality (i.e. without the restriction that the constraint matrices are multiples
of each other) and in the number of iterations.
Assumption 18. There is an algorithm MPC which takes inputs {Pi}i∈[n] ∈Sdp
≥0, {Ci}i∈[n] ∈Sdc
≥0,
and error tolerance ϵ, and solves problem (H.3), (H.4), in poly(log(ndρ), ϵ−1) iterations, where
d := max(dp, dc), ρ := maxi∈[n]
λmax(Ci)
λmax(Pi). Each iteration uses O(1) n-dimensional vector operations,
and for ϵ′ = Θ(ϵ) with an appropriate constant, additionally requires computation of
ϵ′-multiplicative approximations to
*
Pi,
exp
P
i∈[n] wiPi

Tr exp
P
i∈[n] wiPi

+
∀i ∈[n],

ϵ′, e
−log(ndρ)
ϵ′
Tr(Ci)

-approximations to
*
Ci,
exp

−P
i∈[n] wiCi

Tr exp

−P
i∈[n] wiCi

+
∀i ∈[n],
(H.5)
for w ∈Rn
≥0 with λmax
P
i∈[n] wiPi

, λmin
P
i∈[n] wiCi

≤R for R = O( log(ndρ)
ϵ
).
In particular, we observe that the number of iterations of this conjectured subroutine depends
polylogarithmically on ρ, i.e. the runtime is width-independent.2 In our settings computing optimal
2The literature on approximate solvers for positive linear programs and semideﬁnite programs refer to logarithmic
dependences on ρ as width-independent, and we follow this convention in our exposition.

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
808
rescaled condition numbers, ρ = Θ(κ⋆); our solver in Section 7.4 has an iteration count depending
linearly on ρ. Such runtimes are known for MPC linear programs [379], however, such rates have
been elusive in the SDP setting.
While the form of requirements in (H.5) may seem somewhat
unnatural at ﬁrst glance, we observe that this is the natural generalization of the error tolerance
of known width-independent MPC LP solvers [379]. Moreover, these approximations mirror the
tolerances of our width-dependent solver in Section 7.4 (see Line 6 and Corollary 43).
We ﬁrst record the following technical lemma, which we will repeatedly use.
Lemma 323. Given a matrix 0 ⪯M ⪯RI for some R > 0, suﬃciently small constant ϵ, and
δ ∈(0, 1), we can compute ϵ-multiplicative approximations to the quantities

aia⊤
i , exp(M)

for all i ∈[n], and Tr exp(M)
in time O((Tmv(M)R + nnz(A)) log n
δ ), with probability at least 1 −δ.
Proof. We discuss both parts separately. Regarding computing the inner products, equivalently,
the goal is to compute approximations to all
exp( 1
2M)ai
2
2 for i ∈[n]. First, by an application of
Fact 12 with δ = ϵ
8 exp(−2R), and then multiplying all sides of the inequality by exp(R), there is a
degree-O(R) polynomial such that

1 −ϵ
8

exp
1
2M

⪯exp
1
2M

−ϵ
8I ⪯p
1
2M

⪯exp
1
2M

+ ϵ
8I ⪯

1 + ϵ
8

exp
1
2M

=⇒

1 −ϵ
3

exp(M) ⪯p
1
2M
2
⪯

1 + ϵ
3

exp(M).
This implies that
p( 1
2M)ai
2
2 approximates
exp( 1
2M)ai
2
2 to a multiplicative ϵ
3 by the deﬁnition of
Loewner order. Moreover, applying Fact 11 with a suﬃciently large k = O(log n
δ ) implies by a union
bound that for all i ∈[n],
Qp( 1
2M)ai
2
2 is a ϵ-multiplicative approximation to
exp( 1
2M)ai
2
2. To
compute all the vectors Qp( 1
2M)ai, it suﬃces to ﬁrst apply p( 1
2M) to all rows of Q, which takes
time O(Tmv(M) · kR) since p is a degree-O(R) polynomial. Next, once we have the explicit k × d
matrix Qp( 1
2M), we can apply it to all {ai}i∈[n] in time O(nnz(A) · k).
Next, consider computing Tr exp(M), which by deﬁnition has
Tr exp(M) =
X
j∈[d]


exp
1
2M

j:

2
2
.

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
809
Applying the same Q and p as before, we have by the following sequence of equalities
X
j∈[d]
Q

exp
1
2M

j:

2
2
= Tr

exp
1
2M

Q⊤Q exp
1
2M

= Tr
 Q exp (M) Q⊤
=
X
ℓ∈[k]
exp
1
2M

Qℓ:

2
2
,
that for the desired approximation, it instead suﬃces to compute
X
ℓ∈[k]
p
1
2M

Qℓ:

2
2
.
This can be performed in time O(Tmv(M) · kR) as previously argued.
A straightforward modiﬁcation of this proof alongside Lemma 154 also implies that we can
compute these same quantities to p(AWA), when we are only given K = A2, assuming that K
is reasonably well-conditioned. We omit the proof, as it follows almost identically to the proofs of
Lemmas 323, 157, and 158, the latter two demonstrating how to appropriately apply Lemma 154.
Corollary 72. Let K ∈Sd
≻0 such that K = A2 and κ(K) ≤κscale. Let W be a diagonal matrix
such that λmax(AWA) ≤R. For δ, ϵ ∈(0, 1), we can compute ϵ-multiplicative approximations to

aia⊤
i , exp(AWA)

for all i ∈[n], and Tr exp(AWA)
with probability ≥1 −δ in time O
 Tmv(K) · R ·
 R + √κscale log dκscale
δ

log n
δ

.
Approximating κ⋆under Assumption 18
We show that, given Assumption 18, we obtain improved runtimes for all three types of diagonal
scaling problems, roughly improving Theorems 62 and 63 by a κ⋆factor.
Inner scalings. We ﬁrst demonstrate this improvement for inner scalings.
Theorem 92. Under Assumption 18, there is an algorithm which, given full-rank A ∈Rn×d for
n ≥d computes w ∈Rn
≥0 such that κ(A⊤WA) ≤(1 + ϵ)κ⋆
i (A) for arbitrarily small ϵ = Θ(1), with
probability ≥1 −δ in time
O

nnz(A) ·
q
κ⋆
i (A) · poly log nκ⋆
i (A)
δ

.
Proof. For now, assume we know κ⋆
i (A) exactly, which we denote as κ⋆
i for brevity. Let {ai}i∈[n]
denote the rows of A, and assume that ∥ai∥2 = 1 for all i ∈[n]. By scale invariance, this assumption
is without loss of generality. We instantiate Assumption 18 with Pi = aia⊤
i and Ci = κ⋆
i aia⊤
i , for

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
810
i ∈[n]. It is immediate that a solution yields an inner scaling with the same quality up to a 1 + ϵ
factor, because by assumption (H.4) is feasible so MPC cannot return "infeasible."
We now instantiate the primitives in (H.5) needed by Assumption 18. Throughout, note that ρ =
κ⋆
i in this setting. Since we run MPC for poly(log nκ⋆
i ) iterations, we will set δ′ ←δ · (poly(nκ⋆
i ))−1
for the failure probability of each of our computations in (H.5), such that by a union bound all of
these computations are correct.
By Lemma 323, we can instantiate the packing gradients to the desired approximation quality
in time O(nnz(A) · poly log nκ⋆
i
δ ) with probability 1 −δ′. By Lemmas 97 and 98, we can instantiate
the covering gradients in time O(nnz(A)
p
κ⋆
i · poly log nκ⋆
i
δ ) with probability 1 −δ′. In applying
these lemmas, we use the assumption that λmin(P
i∈[n] wiCi) = O(log nκ⋆
i ) as in Assumption 18,
and that the covering matrices are a κ⋆
i multiple of the packing matrices so λmax(P
i∈[n] wiCi) =
O(κ⋆
i log nκ⋆
i ). Thus, the overall runtime of all iterations is
O

nnz(A) ·
p
κ⋆
i · poly log nκ⋆
i
δ

for ϵ = Θ(1). To remove the assumption that we know κ⋆
i (A), we can use an incremental search on
the scaling multiple between {Ci}i∈[n] and {Pi}i∈[n], starting from 1 and increasing by factors of
1 + ϵ, adding a constant overhead to the runtime. Our width will never be larger than O(κ⋆
i (A)) in
any run, since MPC must conclude feasible when the width is suﬃciently large.
Outer scalings. We next discuss the case where we wish to symmetrically outer scale a matrix
K ∈Sd
≻0 near-optimally (i.e. demonstrating an improvement to Theorem 62 under Assumption 18).
Theorem 93. Under Assumption 18, there is an algorithm which, given K ∈Sd
≻0 computes w ∈Rd
≥0
such that κ(W
1
2 KW
1
2 ) ≤(1 + ϵ)κ⋆
o(K) for arbitrarily small ϵ ∈Θ(1), with probability ≥1 −δ in
time
O

Tmv(K) ·
p
κ⋆o(K) · poly log dκ⋆
o(K)
δ

.
Proof. Throughout we denote κ⋆
o := κ⋆
o(K) for brevity. Our proof follows that of Theorem 62, which
demonstrates that it suﬃces to reduce to the case where we have a K ∈Sd
≻0 with κ(K) ≤κscale :=
3κ⋆
o, and we wish to ﬁnd an outer diagonal scaling W ∈Sd
≻0 such that κ(W
1
2 KW
1
2 ) ≤(1 + ϵ)κ⋆
o.
We incur a polylogarithmic overhead on the runtime of this subproblem, by using it to solve all
phases of the homotopy method in Theorem 62, and the cost of an incremental search on κ⋆
o.
To solve this problem, we again instantiate Assumption 18 with Pi = aia⊤
i and Ci = κ⋆
oaia⊤
i ,
where {ai}i∈[d] are rows of A := K
1
2 . As in the proof of Theorem 62, the main diﬃculty is to
implement the gradients in (H.5) with only implicit access to A , which we again will perform
to probability 1 −δ′ for some δ′ = δ · (poly(nκ⋆
o))−1 which suﬃces by a union bound. Applying
Lemmas 157 and 158 with the same parameters as in the proof of Theorem 62 (up to constants)

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
811
implies that we can approximate the covering gradients in (H.5) to the desired quality within time
O

Tmv(K) ·
p
κ⋆o · poly log nκ⋆
o
δ

.
Similarly, Corollary 72 implies we can compute the necessary approximate packing gradients in the
same time. Multiplying by the overhead of the homotopy method in Theorem 62 gives the result.
Average-case conditioning under Assumption 18
A number of recent linear system solvers depend on average notions of conditioning, namely the
ratio between the average eigenvalue and smallest [497, 347, 300, 168, 553, 17, 10]. Normalized by
dimension, we deﬁne this average conditioning as follows: for M ∈Sd
≻0,
τ (M) :=
Tr(M)
λmin(M).
Observe that since Tr(M) is the sum of eigenvalues, the following inequalities always hold:
d ≤τ (M) ≤dκ (M) .
(H.6)
In analogy with κ⋆
i and κ⋆
o, we deﬁne for full-rank A ∈Rn×d for n ≥d, and K ∈Sd
≻0,
τ ⋆
i (A) :=
min
diagonal W⪰0 τ
 A⊤WA

, τ ⋆
o (K) :=
min
diagonal W⪰0 τ

W
1
2 KW
1
2

.
(H.7)
We give an informal discussion on how to use Assumption 18 to develop a solver for approximating
τ ⋆
i to a constant factor, which has a runtime nearly-matching the fastest linear system solvers
depending on τ ⋆
i after applying the appropriate rescalings.3 Qualitatively, this may be thought of
as the average-case variant of Theorem 92. We defer an analogous result on approximating τ ⋆
o (with
or without a factorization) to future work for brevity.
To develop our algorithm for approximating τ ⋆
i , we require several tools. The ﬁrst is the rational
approximation analog of the polynomial approximation in Fact 12.
Fact 45 (Rational approximation of exp [466], Theorem 7.1). Let M ∈Sd
≥0 and δ > 0. There is an
explicit polynomial p of degree ∆= Θ(log(δ−1)) with absolute coeﬃcients at most ∆O(∆) with
exp(−M) −δI ⪯p
 
I + M
∆
−1!
⪯exp(−M) + δI.
We also use the runtime of the fastest-known solver for linear systems based on row subsampling,
3We remark that these problems may be solved to high precision by casting them as an appropriate SDP and
applying general SDP solvers, but in this section we focus on fast runtimes.

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
812
with a runtime dependent on the average conditioning τ. Our goal is to compute reweightings W
which approximately attain the minimums in (H.7), with runtimes comparable to that of Fact 46.
Fact 46 ([10]). There is an algorithm which given M ∈Sd
≻0, b ∈Rd, and δ, ϵ ∈(0, 1) returns v ∈Rd
such that
v −M−1b

2 ≤ϵ
M−1b

2 with probability ≥1 −δ in time
O

n +
p
dτ(M)

· d · poly log nτ(K)
δϵ

.
Remark. The runtime of Fact 46 applies more broadly to quadratic optimization problems in M,
e.g. regression problems of the form ∥Ax −b∥2
2 where A⊤A = M. Moreover, Fact 46 enjoys runtime
improvements when the rows of M (or the factorization component A) are sparse; our methods in
the following discussion do as well as they are directly based on Fact 46, and we omit this discussion
for simplicity. Finally, [10] demonstrates how to improve the dependence on
p
dτ(M) to a more
ﬁne-grained quantity in the case of non-uniform eigenvalue distributions. We defer obtaining similar
improvements for approximating optimal rescalings to interesting future work.
We now give a sketch of how to use Facts 45 and 46 to obtain near-optimal runtimes for computing
a rescaling approximating τ ⋆
i under Assumption 18. Let A ∈Rn×d for n ≥d be full rank, and assume
that we known τ ⋆
i := τ ⋆
i (A) for simplicity, which we can approximate using an incremental search
with a logarithmic overhead. Denote the rows of A by {ai}i∈[n]. We instantiate Assumption 18 with
Pi = ∥ai∥2
2 , Ci = τ ⋆
i aia⊤
i , for all i ∈[n],
(H.8)
from which it follows that (H.4) is feasible using the reweighting W = diag (w) attaining τ ⋆
i :
λmax

X
i∈[n]
wiPi

= λmax

X
i∈[n]
wi ∥ai∥2
2

= Tr
 A⊤WA

,
λmin

X
i∈[n]
wiCi

= τ ⋆
i λmin

X
i∈[n]
wiaia⊤
i

= τ ⋆
i λmin
 A⊤WA

.
(H.9)
Hence, if we can eﬃciently implement each step of MPC with these matrices, it will return a reweight-
ing satisfying (H.3), which yields a trace-to-bottom eigenvalue ratio approximating τ ⋆
i to a 1 + ϵ
factor. We remark that in the algorithm parameterization, we have ρ = τ ⋆
i . Moreover, all of the
packing gradient computations in (H.5) are one-dimensional and hence amount to vector operations,
so we will only discuss the computation of covering gradients.
Next, observe that Assumption 18 guarantees that for all intermediate reweightings W computed
by the algorithm and R = O(log nτ ⋆
i ), λmax(P
i∈[n] wiPi) = Tr(A⊤WA) ≤R. This implies that

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
813
the trace of the matrix involved in covering gradient computations is always bounded:
Tr

X
i∈[n]
wiCi

= τ ⋆
i Tr
 A⊤WA

≤τ ⋆
i R.
(H.10)
To implement the covering gradient computations, we appropriately modify Lemmas 97 and 98 to
use the rational approximation in Fact 45 instead of the polynomial approximation in Fact 12. It is
straightforward to check that the degree of the rational approximation required is ∆= O(log nτ ⋆
i ).
Moreover, each of the ∆linear systems which Fact 45 requires us to solve is in the matrix
M := I +
P
i∈[n] wiCi
∆
,
which by (H.10) and the fact that I has all eigenvalues 1, has τ(M) = O(τ ⋆
i ). Thus, we can apply
Fact 46 to solve these linear systems in time
O

n +
p
dτ ⋆
i

· d · poly log nτ ⋆
i
δ

.
Here, we noted that the main fact that e.g. Lemmas 97 and 98 use is that the rational approximation
approximates the exponential up to a poly(n−1, (τ ⋆
i )−1) multiple of the identity. Since all coeﬃcients
of the polynomial in Fact 45 are bounded by ∆O(∆), the precision to which we need to apply Fact 46 to
satisfy the requisite approximations is ϵ = ∆−O(∆), which only aﬀects the runtime by polylogarithmic
factors. Combining the cost of computing (H.5) with the iteration bound of Assumption 18, the
overall runtime of our method for approximating τ ⋆
i is
O

n +
q
dτ ⋆
i (A)

· d · poly log nτ ⋆
i (A)
δ

,
which matches Fact 46's runtime after rescaling in all parameters up to logarithmic factors.
H.2
Greedy and non-convex methods fail in the semi-random
setting
In this section, we show how a few standard, commonly-used non-convex or greedy methods can fail
(potentially quite drastically) in the semi-random adversary setting. The two algorithms that we
examine are Iterative Hard Thresholding and Orthogonal Matching Pursuit [86, 510]. We believe it
is likely that similar counterexamples can be constructed for other, more complex algorithms such
as CoSaMP [410]. For simplicity in this section, we will only discuss the speciﬁc semi-random model
introduced in Deﬁnition 37, where A is pRIP, i.e. it contains an unknown RIP matrix G as a subset

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
814
of its rows.
H.2.1
Iterative hard thresholding
The iterative hard thresholding algorithm [86] involves initializing x0 = 0 and taking
xt+1 = Hs

xt −1
nA⊤(b −Axt)

where Hs zeroes out all but the s largest entries in magnitude (ties broken lexicographically). We
can break this algorithm in the semi-random setting by simply duplicating one row many times.
Hard semi-random adversary. Let n = Cm for some suﬃciently large constant C. The ﬁrst m
rows of A are drawn independently from N(0, I). Now draw v ∼N(0, I), except set the ﬁrst entry
of v to 1. We set the last (C −1)m rows of A all equal to v. We will set the sparsity parameter
s = 1 and let x⋆= (1, 0, . . . , 0). We let b = Ax⋆.
Proposition 77. With A, b generated as above, with high probability, iterative hard thresholding
does not converge.
Proof. With high probability, some coordinate of v is Ω(√log d). We then have that some entry of
A⊤b has magnitude at least Ω(n√log d) with high probability. Thus, the next iterate x1 must have
exactly one nonzero entry that has magnitude at least Ω(√log d) and furthermore, this entry must
correspond to some coordinate of v that has magnitude at least Ω(√log d). However, this means
that the residuals in all of the rows that are copies of v are at least Ω(log d). In the next step, by the
same argument, we get that the residuals blow up even more and clearly this algorithm will never
converge. In fact, xt will never have the right support because its support will always be on one of
the entries where v is large.
H.2.2
Orthogonal matching pursuit
The orthogonal matching pursuit algorithm [510] involves initializing x0 = 0 and keeping track of a
set S (that corresponds to our guess of the support of x⋆). Each iteration, we choose a column cj of
A that maximizes |⟨cj,rt⟩|
∥cj∥2
2
and then add j to S (where rt = Axt −b is the residual). We then add
j to S and project the residual onto the orthogonal complement of all coordinates in S. We show
that we can again very easily break this algorithm in the semi-random setting.
Hard semi-random adversary. Let n = 3m. First, we draw all rows of A independently from
N(0, I). Next, we modify some of the entries in the last 2m rows. Let s be the sparsity parameter.
Let x⋆= (s−1
2 , . . . , s−1
2 , 0, . . . , 0) be supported on the ﬁrst s coordinates and set b = Ax⋆. Now we
modify the columns of A (aside from the ﬁrst s so Ax⋆is not aﬀected). We set the last 2m entries
of one of these columns cj to match those of b.

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
815
Proposition 78. With A, b generated as above, with high probability, orthogonal matching pursuit
does not recover x⋆.
Proof. With high probability (as long as s ≥10), the column cj is the one that maximizes |⟨cj,b⟩|
∥cj∥2
2
because its last 2m entries exactly match those of b. However, j is not in the support of x⋆so the
algorithm has already failed.
We further make the following observation.
Remark 15. By modifying other columns of A as well, the semi-random adversary can actually
make the algorithm pick all of the wrong columns in the support.
H.2.3
Convex methods
Now we brieﬂy comment on how convex methods are robust, in the sense that they can still be used
in the semi-random setting (but may have substantially slower rates than their fast counterparts).
In the noiseless observations case, this is clear because the additional rows of A are simply additional
constraints that are added to the standard ℓ1 minimization convex program.
In the noisy case, let the target error be θ =
ξ(m)

2. We then solve the modiﬁed problem
min ∥x∥1
subject to
[Ax −b](m)

2 ≤θ.
Note that the above is a convex program and thus can be solved in polynomial time by e.g. cutting
plane methods [255]. Also, note that x⋆is indeed feasible for the second constraint. Now for the
solution bx that we obtain, we must have ∥bx∥1 ≤∥x⋆∥1 and
[A(x⋆−bx)](m)

2 ≤2θ.
Let G be the set of m randomly generated rows of A under our semi-random adversarial model.
The previous two conditions imply
• ∥bx −x⋆∥1 ≤2√s ∥x⋆−bx∥2
• ∥G(x⋆−bx)∥2 ≤2θ
which now by restricted strong convexity of G (see [9]) implies that ∥x⋆−bx∥2 = O(
θ
√m). We can
furthermore round bx to s-sparse to obtain the sparse vector x′, and the above bound only worsens
by a factor of 2 for x′ (see Lemma 170 for this argument).

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
816
H.3
Proof of Lemma 171
Lemma 171. Let δ > 0 and θ ≥0. For any vector γ ∈Rd, we can solve the optimization problem
min
∥p∥2≤θ sqmaxµ(γ −p)
to additive accuracy δ in time
O
 
d log2
 
∥γ∥2
2
µ
√
δ
!!
.
Proof. Let P ⊂Rd be the set of p such that p has the same sign as γ entrywise and |pj| ≤|γj| for
all j ∈[d]. By symmetry of the sqmax and the ℓ2 norm under negation, the optimal p lies in P.
Next we claim that the function smaxµ(γ −p) is 2 ∥γ∥2-Lipschitz in the ℓ2 norm as a function of
p, over P. To see this, the gradient is directly computable as
2(p −γ) ◦x where x ∈∆d with xi =
exp([γi −pi]2/µ2)
P
j∈[n] exp([γj −pj]2/µ2) for all i ∈[n]
where ◦denotes entrywise multiplication. Thus, the ℓ2 norm of the derivative is bounded by 2 ∥γ∥2
over P. In the remainder of the proof, we show how to ﬁnd p ∈P which has ℓ2 error
δ
2∥γ∥2 to the
optimal, which implies by Lipschitzness that the function value is within additive δ of optimal.
Next, since 0 ∈P, we may assume without loss of generality that
θ >
δ
2 ∥γ∥2
.
(H.11)
else we may just output 0, which achieves optimality gap at most 2 ∥γ∥2 θ.
Now, by monotonicity of ln it suﬃces to approximately minimize
X
j∈[d]
exp
 
[γ −p]2
j
µ2
!
.
The sum above is always at least d. First we check if ∥γ∥2 ≤θ +
√
δ. If this is true then clearly we
can set p so that all entries of γ −p have magnitude at most
√
δ. This gives a solution such that
sqmaxµ(γ −p) ≤µ2 log

d exp
 δ
µ2

= µ2 log d + δ
and since the value of sqmax is always at least µ2 log d, this solution is optimal up to additive error
δ. Thus, we can assume ∥γ∥2 ≥θ+
√
δ in the remainder of the proof. We also assume all entries of γ
are nonzero since if an entry of γ is 0 then the corresponding entry of p should also be 0. Finally by
symmetry of the problem under negation we will assume all entries of γ are positive in the remainder

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
817
of the proof, such that each entry of p is also positive.
By monotonicity of sqmax in each coordinate (as long as signs are preserved) and the assumption
that ∥γ∥2 ≥θ +
√
δ, the optimal solution must have ∥p∥2 = θ. By using Lagrange multipliers, for
some scalar ζ and all j,
pj = exp(ζ) · [γ −p]j exp
 
[γ −p]2
j
µ2
!
.
(H.12)
For the optimal ζ by taking ℓ2 norms of the quantity above, we have
θ = ∥p∥2 = ζ ∥γ −p∥2 · C for some C ∈
"
0, exp
 
∥γ∥2
2
µ2
!#
.
Hence taking logarithms of both sides and using both the bounds (H.11) and ∥γ −p∥2 ≥
√
δ at the
optimum, which follows from the previous discussion, we obtain
log
θ
∥γ −p∥2
−ζ ∈
"
0, ∥γ∥2
2
µ2
#
=⇒ζ ∈
"
−∥γ∥2
2
µ2
−log
 
2 ∥γ∥2
2
δ
!
, log
∥γ∥2
√
δ
#
.
We next show how to compute p to high accuracy given a guess on ζ. Observe that if γj > 0, then
the right-hand side of (H.12) is decreasing in pj and hence by the intermediate value theorem, there
is a unique solution strictly between 0 and γj for any ζ. Also, note that the location of this solution
increases with ζ. Let p(ζ) be the solution obtained by exactly solving (H.12) for some given ζ. We
have shown for all ζ that 0 ≤[p(ζ)]j ≤γj entrywise and hence ∥p(ζ)∥2 ≤∥γ∥2 for all ζ.
For a ﬁxed ζ, we claim we can estimate p(ζ) to ℓ2 error β in time O(d log ∥γ∥2
β ). To see this, ﬁx
some ζ, µ, and γj, and consider solving (H.12) for the ﬁxed point pj. We can discretize [0, γj] into
intervals of length
γjβ
∥γ∥2 and perform a binary search. The right-hand side is decreasing in pj and
the left-hand side is increasing so the binary search yields some interval of length
γjβ
∥γ∥2 containing
the ﬁxed point pj via the intermediate value theorem. The resulting ℓ2 error along all coordinates is
then β. We also round this approximate p(ζ) entrywise down in the above search to form a vector
˜p(ζ, β) such that ˜p(ζ, β) ≤p(ζ) entrywise and ∥˜p(ζ, β) −p(ζ)∥2 ≤β. We use this notation and it is
well-deﬁned as the search is deterministic.
In the remainder of the proof we choose the constants
α :=
δ2
192 ∥γ∥4
2
, β := min
 
δ2
192 ∥γ∥3
2
,
δ
4 ∥γ∥2
!
.
We deﬁne ˜p(ζ) := ˜p(ζ, β) for short as β will be ﬁxed. Discretize the range [−∥γ∥2
2
µ2 −log 2∥γ∥2
2
δ
, log ∥γ∥2
√
δ ]
into a grid of uniform intervals of length α. Consider the ζ such that ζ ≤ζ⋆< ζ + α. Because
p(ζ⋆) is entrywise larger than p(ζ) and hence the logarithmic term on the right-hand side of (H.12)

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
818
is smaller for p(ζ⋆) than p(ζ), we have
[p(ζ)]j ≤[p (ζ⋆)]j ≤exp (α) [p (ζ)]j .
Moreover the optimal p(ζ⋆) has ℓ2 norm θ, so |ζ −ζ⋆| ≤α and exp(α) −1 ≤2α imply
∥p(ζ) −p(ζ⋆)∥2 ≤2α ∥p(ζ⋆)∥2 ≤2α ∥γ∥2 ≤∆:=
δ2
96 ∥γ∥3
2
.
Consider the algorithm which returns the ζalg on the search grid which minimizes |∥˜p(ζalg)∥2 −θ|
(we will discuss computational issues at the end of the proof). As we have argued above, there is a
choice which yields ∥p(ζ)∥2 ∈[θ −∆, θ + ∆] and hence
∥˜p(ζalg)∥2 ∈[θ −∆−β, θ + ∆+ β] .
(H.13)
We next claim that
∥p(ζalg) −p(ζ⋆)∥2 ≤
δ
4 ∥γ∥2
.
(H.14)
Suppose (H.14) is false and ζalg > ζ⋆. Then letting u := p(ζalg) and v := p(ζ⋆), note that u, v, and
u −v are all entrywise nonnegative and hence
∥u∥2
2 ≥∥v∥2
2 +
X
i∈[n]
2ui(ui −vi) + (ui −vi)2 > ∥v∥2
2 +

δ
4 ∥γ∥2
2
.
Hence, we have by
p
x2 + y2 ≥x + y2
3x for 0 ≤y ≤x, (H.11), and θ ≤∥γ∥2,
∥p(ζalg)∥2 = ∥u∥2 > ∥v∥2 +

δ
4∥γ∥2
2
3 ∥v∥2
≥θ +
δ2
48 ∥γ∥3
2
≥θ + ∆+ 2β.
So, by triangle inequality ∥˜p(ζalg)∥2 > θ + ∆+ β and hence we reach a contradiction with (H.13).
Similarly, suppose (H.14) is false and ζalg < ζ⋆. Then for the same deﬁnitions of u, v, and using
the inequality
p
x2 −y2 ≤x −y2
3x for 0 ≤y ≤x, we conclude
∥v∥2
2 > ∥u∥2
2 +

δ
4 ∥γ∥2
2
=⇒∥u∥2 ≤
s
∥v∥2
2 −

δ
4 ∥γ∥2
2
< θ −∆−2β.
So we reach a contradiction with (H.13) in this case as well.
In conclusion, (H.14) is true and we obtain by triangle inequality the desired
∥˜p(ζalg) −p(ζ⋆)∥2 ≤
δ
4 ∥γ∥2
+ β
√
d ≤
δ
2 ∥γ∥2
.

APPENDIX H. DEFERRED PROOFS FROM CHAPTER 9
819
The complexity of the algorithm is bottlenecked by the cost of ﬁnding ˜p(ζalg). For each ζ on the grid
the cost of evaluating ˜p(ζ) induces a multiplicative d log( ∥γ∥2
2
δ ) overhead. The cost of performing the
binary search on the ζ grid is a multiplicative log( ∥γ∥2
2
µ
√
δ ) overhead; note that a binary search suﬃces
because ∥˜p(ζalg)∥2 is monotonic by our consistent choice of rounding down, and hence | ∥˜p(ζalg)∥2−θ|
is unimodal.

Appendix I
Deferred proofs from Chapter 10
I.1
Discussion of inexactness tolerance
We brieﬂy discuss the tolerance of our algorithm to approximation error in two places: computation
of minimizers, and implementation of RGOs in the methods of Sections 10.3 and 10.5.
Inexact minimization. For all function classes considered in this work, there exist eﬃcient opti-
mization methods converging to a minimizer with logarithmic dependence on the target accuracy.
Speciﬁcally, for negative log-densities with condition number κ, accelerated gradient descent [417]
converges at a rate O(√κ) with logarithmic dependence on initial error and target accuracy (we
implicitly assumed in stating our runtimes that one can attain initial error polynomial in problem
parameters for negative log-densities; otherwise, there is additional logarithmic overhead in the
quality of the initial point to optimization procedures). For composite functions fwc + foracle where
fwc has condition number κ, the FISTA method of [66] converges at the same rate with each iteration
querying ∇fwc and a proximal oracle for foracle once; typically, access to a proximal oracle is a
weaker assumption than access to a restricted Gaussian oracle, so this is not restrictive. Finally, for
minimizing ﬁnite sums with condition number κ, the algorithm of [17] obtains a convergence rate
linearly dependent on n + √nκ ≤n + κ; alternatively, [300] has a dependence on n + κ. In all our
ﬁnal runtimes, these optimization rates do not constitute the bottleneck for oracle complexities.
The only additional diﬃculty our algorithms may present is if the function requiring minimiza-
tion, say of the form foracle(x)+ 1
2η ∥x −y∥2
2 for some y ∈Rd where we have computed the minimizer
x∗to foracle, has ∥y −x∗∥2
2 very large (so the initial function error is bad). However, in all our set-
tings y is drawn from a distribution with sub-Gaussian tails, so ∥y −x∗∥2
2 decays exponentially
(whereas the complexity of ﬁrst-order methods increases only logarithmically), negligibly aﬀecting
the expected oracle query complexity for our methods.
Finally, by solving the relevant optimization problems to high accuracy as a subroutine in each of
our methods, and adjusting various distance bounds to the minimizer by constants (e.g. by expanding
820

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
821
the radius in the deﬁnition of the sets Ωin Algorithm 62 and Section 10.6.2), this accomodates
tolerance to inexact minimization and only aﬀects all bounds throughout the paper by constants.
The only other place that x∗is used in our algorithms is in initializing warm starts; tolerance to
inexactness in our warmness calculations follows essentially identically to Section 3.2.1 of [210].
Inexact oracle implementation. Our algorithms based on restricted Gaussian oracle access are
tolerant to total variation error inverse polynomial in problem parameters for the restricted Gaussian
oracle for g. We discussed this at the end of Section 10.3, in the case of RGO use for our reduction
framework. To see this in the case of the composite sampler in Section 10.5, we pessimistically
handled the case where the sampler Y-Oracle for a quadratic restriction of f resulted in total
variation error in the proof of Proposition 46, assuming that the error was incurred in every iteration.
By accounting for similar amounts of error in calls to O (on the order of
ϵ
T , where T is the number
of times an RGO was used), the bounds in our algorithm are only aﬀected by constants.
I.2
Deferred proofs from Section 10.5
I.2.1
Deferred proofs from Section 10.5.2
Approximate rejection sampling
We ﬁrst deﬁne the rejection sampling framework we will use, and prove various properties.
Deﬁnition 60 (Approximate rejection sampling). Let π be a distribution, with
dπ
dx(x) ∝p(x).
Suppose set Ωhas π(Ω) = 1 −ϵ′, and distribution ˆπ with dˆπ
dx(x) ∝ˆp(x) has for some C ≥1,
p(x)
ˆp(x) ≤C for all x ∈Ω, and
R
ˆp(x)dx
R
p(x)dx ≤1.
Suppose there is an algorithm A which draws samples from a distribution ˆπ′, such that ∥ˆπ′ −ˆπ∥TV ≤
1 −δ. We call the following scheme approximate rejection sampling: repeat independent runs of the
following procedure until a point is outputted.
1. Draw x via A until x ∈Ω.
2. With probability
p(x)
C ˆp(x), output x.
Lemma 324. Consider an approximate rejection sampling scheme with relevant parameters deﬁned
as in Deﬁnition 60, with 2δ ≤1−ϵ′
C . The algorithm terminates in at most
1
1−ϵ′
C
−2δ
(I.1)
calls to A in expectation, and outputs a point from a distribution π′ with ∥π′ −π∥TV ≤ϵ′ + 2δC
1−ϵ′ .

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
822
Proof. Deﬁne for notational simplicity normalization constants Z :=
R
p(x)dx and ˆZ :=
R
ˆp(x)dx.
First, we bound the probability any particular call to A returns in the scheme:
Z
x∈Ω
p(x)
C ˆp(x)dˆπ′(x) ≥
Z
x∈Ω
p(x)
C ˆp(x)dˆπ(x) −

Z
x∈Ω
p(x)
C ˆp(x)(dˆπ′(x) −dˆπ(x))

=
Z
x∈Ω
Z
C ˆZ
dπ(x) −

Z
x∈Ω
p(x)
C ˆp(x)(dˆπ′(x) −dˆπ(x))

≥1 −ϵ′
C
−
Z
x∈Ω
|dˆπ′(x) −dˆπ(x)| ≥1 −ϵ′
C
−2δ.
(I.2)
The second line followed by the deﬁnitions of Z and ˆZ, and the third followed by triangle inequality,
the assumed lower bound on Z/ ˆZ, and the total variation distance between ˆπ′ and ˆπ. By linearity
of expectation and independence, this proves the ﬁrst claim.
Next, we claim the output distribution is close in total variation distance to the conditional
distribution of π restricted to Ω. The derivation of (I.2) implies
Z
x∈Ω
p(x)
C ˆp(x)dˆπ(x) ≥1 −ϵ′
C
,

Z
x∈Ω
p(x)
C ˆp(x)(dˆπ′(x) −dˆπ(x))
 ≤2δ,
=⇒1 −2δC
1 −ϵ′ ≤
R
x∈Ω
p(x)
C ˆp(x)dˆπ′(x)
R
x∈Ω
p(x)
C ˆp(x)dˆπ(x)
≤1 + 2δC
1 −ϵ′ .
(I.3)
Thus, the total variation of the true output distribution from π restricted to Ωis
1
2
Z
x∈Ω

dπ(x)
1 −ϵ′ −
p(x)
C ˆp(x)dˆπ′(x)
R
x∈Ω
p(x)
C ˆp(x)dˆπ′(x)

≤1
2
Z
x∈Ω

dπ(x)
1 −ϵ′ −
p(x)
C ˆp(x)dˆπ′(x)
R
x∈Ω
p(x)
C ˆp(x)dˆπ(x)

+ 1
2
Z
x∈Ω

p(x)
C ˆp(x)dˆπ′(x)
R
x∈Ω
p(x)
C ˆp(x)dˆπ(x)
−
p(x)
C ˆp(x)dˆπ′(x)
R
x∈Ω
p(x)
C ˆp(x)dˆπ′(x)

≤1
2
Z
x∈Ω

dπ(x)
1 −ϵ′ −
p(x)
C ˆp(x)dˆπ′(x)
R
x∈Ω
p(x)
C ˆp(x)dˆπ(x)

+
δC
1 −ϵ′ = 1
2
Z
x∈Ω
dπ(x)
1 −ϵ′
1 −dˆπ′
dˆπ (x)
 +
δC
1 −ϵ′ .
The ﬁrst inequality was triangle inequality, and we bounded the second term by (I.3). To obtain
the ﬁnal equality, we used
Z
x∈Ω
p(x)
C ˆp(x)dˆπ(x) =
Z
x∈Ω
Z
C ˆZ
dπ(x) = (1 −ϵ′)Z
C ˆZ
=⇒
p(x)
C ˆp(x)dˆπ′(x)
R
x∈Ω
p(x)
C ˆp(x)dˆπ(x)
= p(x)
Z
·
ˆZ
ˆp(x) ·
1
1 −ϵ′ · dˆπ′(x) = dπ(x)
1 −ϵ′ · dˆπ′
dˆπ (x).
We now bound this ﬁnal term. Observe that the given conditions imply that dπ
dˆπ(x) is bounded by

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
823
C everywhere in Ω. Thus, expanding we have
1
2
Z
x∈Ω
dπ(x)
1 −ϵ′
1 −dˆπ′
dˆπ (x)
 ≤
C
2(1 −ϵ′)
Z
x∈Ω
|dˆπ(x) −dˆπ′(x)| ≤
δC
1 −ϵ′ .
Finally, combining these guarantees, and the fact that restricting π to Ωloses ϵ′ in total variation
distance, yields the desired conclusion by triangle inequality.
Corollary 73. Let ˆθ(x) be an unbiased estimator for p(x)
ˆp(x), and suppose ˆθ(x) ≤C with probability
1 for all x ∈Ω. Then, implementing the procedure of Deﬁnition 60 with acceptance probability
ˆθ(x)
C
has the same runtime bound and total variation guarantee as given by Lemma 324.
Proof. It suﬃces to take expectations over the randomness of ˆθ everywhere in the proof of Lemma 324.
Distribution ratio bounds
We next show two bounds relating the densities of distributions π and ˆπ.
We ﬁrst deﬁne the
normalization constants of (10.20), (10.22) for shorthand, and then tightly bound their ratio.
Deﬁnition 61 (Normalization constants). We denote normalization constants of π and ˆπ by
Zπ :=
Z
x
exp (−f(x) −g(x)) dx,
Zˆπ :=
Z
x,y
exp

−f(y) −g(x) −1
2η ∥y −x∥2
2 −ηL2
2
∥x −x∗∥2
2

dxdy.
Lemma 325 (Normalization constant bounds). Let Zπ and Zˆπ be as in Deﬁnition 61. Then,
 2πη
1 + ηL
 d
2 
1 + ηL2
µ
−d
2
≤Zˆπ
Zπ
≤(2πη)
d
2 .
Proof. For each x, by convexity we have
Z
y
exp

−f(y) −g(x) −1
2η ∥y −x∥2
2 −ηL2
2
∥x −x∗∥2
2

dy
≤exp

−g(x) −ηL2
2
∥x −x∗∥2
2
 Z
y
exp

−f(x) −⟨∇f(x), y −x⟩−1
2η ∥y −x∥2
2

dy
= exp

−f(x) −g(x) −ηL2
2
∥x −x∗∥2
2
 Z
y
exp
η
2 ∥∇f(x)∥2
2 −1
2η ∥y −x + η∇f(x)∥2
2

dy
= (2πη)
d
2 exp (−f(x) −g(x)) exp
η
2 ∥∇f(x)∥2
2 −ηL2
2
∥x −x∗∥2
2

≤(2πη)
d
2 exp (−f(x) −g(x)) .
(I.4)

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
824
Integrating both sides over x yields the upper bound on Zˆπ
Zπ . Next, for the lower bound we have a
similar derivation. For each x, by smoothness
Z
y
exp

−f(y) −g(x) −1
2η ∥y −x∥2
2 −ηL2
2
∥x −x∗∥2
2

dy
≥exp

−f(x) −g(x) −ηL2
2
∥x −x∗∥2
2
 Z
y
exp

⟨∇f(x), x −y⟩−1 + ηL
2η
∥y −x∥2
2

dy
= exp

−f(x) −g(x) −ηL2
2
∥x −x∗∥2 +
η
2(1 + ηL) ∥∇f(x)∥2
  2πη
1 + ηL
 d
2
≥exp

−f(x) −g(x) −ηL2
2
∥x −x∗∥2
2
  2πη
1 + ηL
 d
2
.
Integrating both sides over x yields
Zˆπ
Zπ
≥
 2πη
1 + ηL
 d
2
R
x exp

−f(x) −g(x) −ηL2
2 ∥x −x∗∥2
2

dx
R
x exp (−f(x) −g(x)) dx
≥
 2πη
1 + ηL
 d
2 
1 + ηL2
µ
−d
2
.
The last inequality followed from Proposition 82, where we used f + g is µ-strongly convex.
Lemma 326 (Relative density bounds). Let η =
1
32Lκd log(288κ/ϵ). For all x ∈Ω, as deﬁned in
(10.21),
dπ
dˆπ(x) ≤2. Here,
dˆπ
dx(x) denotes the marginal density of ˆπ. Moreover, for all x ∈Rd,
dπ
dˆπ(x) ≥1
2.
Proof. We ﬁrst show the upper bound. By Lemma 325,
dπ
dˆπ (x) =
exp (−f(x) −g(x))
R
y exp

−f(y) −g(x) −
1
2η ∥y −x∥2
2 −ηL2
2 ∥x −x∗∥2
2

dy
· Zˆπ
Zπ
≤
exp (−f(x) −g(x))
R
y exp

−f(y) −g(x) −
1
2η ∥y −x∥2
2 −ηL2
2 ∥x −x∗∥2
2

dy
· (2πη)
d
2 .
(I.5)
We now bound the ﬁrst term, for x ∈Ω. By smoothness, we have
exp (−f(y) −g(x))
exp (−f(x) −g(x)) ≥exp

⟨∇f(x), x −y⟩−L
2 ∥y −x∥2
2

,

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
825
so applying this for each y,
R
y exp

−f(y) −g(x) −
1
2η ∥y −x∥2
2 −ηL2
2 ∥x −x∗∥2
2

dy
exp (−f(x) −g(x))
≥exp

−ηL2
2
∥x −x∗∥2
2
 Z
y
exp

⟨∇f(x), x −y⟩−1 + ηL
2η
∥y −x∥2
2

dy
= exp

−ηL2
2
∥x −x∗∥2
2 +
η
2(1 + ηL) ∥∇f(x)∥2
2
 Z
y
exp
 
−1 + ηL
2η
x −y −
η
1 + ηL∇f(x)

2
2
!
dy
≥exp

−ηL2
2
· 16d log(288κ/ϵ)
µ
  2πη
1 + ηL
 d
2
≥3
4
 2πη
1 + ηL
 d
2
.
In the last line, we used that x ∈Ωimplies ∥x −x∗∥2
2 ≤
16d log(288κ/ϵ)
µ
, and the deﬁnition of η.
Combining this bound with (I.5), we have the desired
dπ
dˆπ (x) ≤4
3 (1 + ηL)
d
2 ≤2.
Next, we consider the lower bound. By combining (I.4) with Lemma 325, we have the desired
dπ
dˆπ (x) =
exp (−f(x) −g(x))
R
y exp

−f(y) −g(x) −
1
2η ∥y −x∥2
2 −ηL2
2 ∥x −x∗∥2
2

dy
· Zˆπ
Zπ
≥(2πη)−d
2 ·
 2πη
1 + ηL
 d
2 
1 + ηL2
µ
−d
2
=

1
1 + ηL
 d
2
(1 + ηLκ)−d
2 ≥1
2.
Correctness of Composite-Sample-Shared-Min
Proposition 45. Let η =
1
32Lκd log(288κ/ϵ), and assume Sample-Joint-Dist(f, g, x∗, O, δ) samples
within δ total variation of the x-marginal on (10.22). Composite-Sample-Shared-Min outputs a
sample within total variation ϵ of (10.20) in an expected O(1) calls to Sample-Joint-Dist.
Proof. We remark that η =
1
32Lκd log(288κ/ϵ) is precisely the choice of η in Sample-Joint-Dist where
δ = ϵ/18, as in Composite-Sample-Shared-Min. First, we may apply Fact 28 to conclude that the
measure of set Ωwith respect to the µ-strongly logconcave density π is at least 1 −ϵ/3.
The
conclusion of correctness will follow from an appeal to Corollary 73, with parameters
C = 4, ϵ′ = ϵ
3, δ = ϵ
18.
Note that indeed we have ϵ′ + 2δC
1−ϵ′ is bounded by ϵ, as 1 −ϵ′ ≥2
3. Moreover, the expected number
of calls (I.1) is clearly bounded by a constant as well.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
826
We now show that these parameters satisfy the requirements of Corollary 73. Deﬁne the functions
p(x) := exp(−f(x) −g(x)),
ˆp(x) := (2πη)−d
2
Z
y
exp

−f(y) −g(x) −1
2η ∥y −x∥2
2 −ηL2
2
∥x −x∗∥2
2

dy,
and observe that clearly the densities of π and ˆπ are respectively proportional to p and ˆp. Moreover,
deﬁne Z =
R
p(x)dx and ˆZ =
R
ˆp(x)dx. By comparing these deﬁnitions with Lemma 325, we have
Z = Zπ and ˆZ = (2πη)−d
2 Zˆπ, so by the upper bound in Lemma 325, ˆZ/Z ≤1. Next, we claim that
the following procedure produces an unbiased estimator for p(x)
ˆp(x).
1. Sample y ∼πx, where dπx(y)
dy
∝exp

−f(y) −
1
2η ∥y −x∥2
2

2. α ←exp

f(y) −⟨∇f(x), y −x⟩−L
2 ∥y −x∥2
2 + g(x) + ηL2
2 ∥x −x∗∥2
2

3. Output ˆθ(x) ←exp

−f(x) −g(x) +
η
2(1+ηL) ∥∇f(x)∥2
2

(1 + ηL)
d
2 α
To prove correctness of this estimator ˆθ, deﬁne for simplicity
Zx :=
Z
y
exp

−f(y) −g(x) −1
2η ∥y −x∥2
2 −ηL2
2
∥x −x∗∥2
2

dy.
We compute, using dπx(y)
dy
=
exp(−f(y)−g(x)−1
2η ∥y−x∥2
2−ηL2
2
∥x−x∗∥2
2)
Zx
, that
Eπx [α] =
Z
y
exp

f(y) −⟨∇f(x), y −x⟩−L
2 ∥y −x∥2
2 + g(x) + ηL2
2
∥x −x∗∥2
2

dπx(y)
= 1
Zx
Z
y
exp

−⟨∇f(x), y −x⟩−L
2 ∥y −x∥2
2 −1
2η ∥y −x∥2
2

dy
= 1
Zx
exp

−
η
2(1 + ηL) ∥∇f(x)∥2
2
  2πη
1 + ηL
 d
2
.
This implies that the output quantity
ˆθ(x) = exp

−f(x) −g(x) +
η
2(1 + ηL) ∥∇f(x)∥2
2

(1 + ηL)
d
2 α
is unbiased for
p(x)
ˆp(x) = exp(−f(x) −g(x))Z−1
x (2πη)
d
2 .
Finally, note that for any y used in the

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
827
deﬁnition of ˆθ(x), by using f(y) −f(x) −⟨∇f(x), y −x⟩−L
2 ∥y −x∥2
2 ≤0 via smoothness, we have
ˆθ(x) = exp

−f(x) −g(x) +
η
2(1 + ηL) ∥∇f(x)∥2
2

(1 + ηL)
d
2 α
≤(1 + ηL)
d
2 exp

η
2(1 + ηL) ∥∇f(x)∥2
2 + ηL2
2
∥x −x∗∥2
2

≤(1 + ηL)
d
2 exp

ηL2 ∥x −x∗∥2
2

≤4.
Here, we used the deﬁnition of η and L2 ∥x −x∗∥2
2 ≤16Lκd log(288κ/ϵ) by the deﬁnition of Ω.
I.2.2
Deferred proofs from Section 10.5.3
Throughout this section, for error tolerance δ ∈[0, 1] which parameterizes Sample-Joint-Dist, we
denote for shorthand a high-probability region Ωδ and its radius Rδ by
Ωδ := {x | ∥x −x∗∥2 ≤Rδ} , for Rδ := 4
s
d log(16κ/δ)
µ
.
(I.6)
The following density ratio bounds hold within this region, by simply modifying Lemma 326.
Corollary 74. Let η =
1
32Lκd log(16κ/δ), and let ˆπ be parameterized by this choice of η in (10.22).
For all x ∈Ωδ, as deﬁned in (I.6), dπ
dˆπ(x) ≤2. Moreover, for all x ∈Rd, dπ
dˆπ(x) ≥1
2.
The following claim follows immediately from applying Fact 28.
Lemma 327. With probability at least 1 −
δ2
8(1+κ)d , x ∼ˆπ lies in Ωδ.
Finally, when clear from context, we overload ˆπ as a distribution on x ∈Rd to be the x component
marginal of the distribution (10.22), i.e. with density
dˆπ
dx(x) ∝
Z
y
exp

−f(y) −g(x) −1
2η ∥y −x∥2
2 −ηL2
2
∥x −x∗∥2
2

dy.
We ﬁrst note that ˆπ is stationary for Sample-Joint-Dist; this follows immediately from Lemma 181.
In Section I.2.2, we bound the conductance of the walk. We then use this bound in Section I.2.2 to
bound the mixing time and overall complexity of Sample-Joint-Dist.
Conductance of Sample-Joint-Dist
We bound the conductance of this random walk, as a process on the iterates {xk}, to show the ﬁnal
point has distribution close to the marginal of ˆπ on x. To do so, we break Proposition 48 into two
pieces, which we will use in a more white-box manner to prove our conductance bound.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
828
Deﬁnition 62 (Restricted conductance). Let a random walk with stationary distribution ˆπ on x ∈
Rd have transition densities Tx, and let Ω⊆Rd. The Ω-restricted conductance, for v ∈(0, 1
2 ˆπ(Ω)),
is
ΦΩ(v) =
inf
ˆπ(S∩Ω)∈(0,v]
TS(Sc)
ˆπ(S ∩Ω), where TS(Sc) :=
Z
x∈S
Z
x′∈Sc Tx(x′)dˆπ(x)dx′.
Proposition 79 (Lemma 1, [128]). Let πstart be a β-warm start for ˆπ, and let x0 ∼πstart. For some
δ > 0, let Ω⊆Rd have ˆπ(Ω) ≥1 −
δ2
2β2 . Suppose that a random walk with stationary distribution ˆπ
satisﬁes the Ω-restricted conductance bound
ΦΩ(v) ≥
s
B log
1
v

, for all v ∈
 4
β , 1
2

.
Let xK be the result of K steps of this random walk, starting from x0. Then, for
K ≥64
B log
log β
2δ

,
the resulting distribution of xK has total variation at most δ
2 from ˆπ.
We state a well-known strategy for lower bounding conductance, via showing the stationary
distribution has good isoperimetry and that transition distributions of nearby points have large
overlap.
Proposition 80 (Lemma 2, [128]). Let a random walk with stationary distribution ˆπ on x ∈Rd
have transition distribution densities Tx, and let Ω⊆Rd, and let ˆπΩbe the conditional distribution
of ˆπ on Ω. Suppose for any x, x′ ∈Ωwith ∥x −x′∥2 ≤∆,
∥Tx −Tx′∥TV ≤1
2.
Also, suppose ˆπΩsatisﬁes, for any partition S1, S2, S3 of Ω, where d(S1, S2) is the minimum
Euclidean distance between points in S1, S2, the log-isoperimetric inequality
ˆπΩ(S3) ≥1
2ψ d(S1, S2) · min (ˆπΩ(S1), ˆπΩ(S2)) ·
s
log

1 +
1
min (ˆπΩ(S1), ˆπΩ(S2))

.
(I.7)
Then, we have the bound for all v ∈(0, 1
2]
ΦΩ(v) ≥min
 
1,
∆
128ψ
s
log
1
v
!
.
To utilize Propositions 79 and 80, we prove the following bounds in Appendices I.3.1, I.3.2,
and I.3.3.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
829
Lemma 328 (Warm start). For η ≤
1
Lκd, πstart deﬁned in (10.23) is a 2(1 + κ)
d
2 -warm start for ˆπ.
Lemma 329 (Transitions of nearby points). Suppose ηL ≤1, ηL2R2
δ ≤1
2, and 400d2η ≤R2
δ. For a
point x, let Tx be the density of xk after sampling according to Lines 6 and 7 of Algorithm 63 from
xk−1 = x. For x, x′ ∈Ωδ with ∥x −x′∥2 ≤
√η
10 , for Ωδ deﬁned in (I.6), we have ∥Tx −Tx′∥TV ≤1
2.
Lemma 330 (Isoperimetry). Density ˆπ and set Ωδ deﬁned in (10.22), (I.6) satisfy (I.7) with ψ =
8µ−1
2 .
We note that the parameters of Algorithm 63 and the set Ωδ in (I.6) satisfy all assumptions of
Lemmas 328, 329, and 330. By combining these results in the context of Proposition 80, we see that
the random walk satisﬁes the bound for all v ∈(0, 1
2]:
ΦΩδ(v) ≥
s
ηµ
220 · 100 · log
1
v

.
Plugging this conductance lower bound, the high-probability guarantee of Ωδ by Lemma 327, and
the warm start bound of Lemma 328 into Proposition 79, we have the following conclusion.
Corollary 75 (Mixing time of ideal Sample-Joint-Dist). Assume that calls to Y-Oracle are exact
in the implementation of Sample-Joint-Dist. Then, for any error parameter δ, and
K := 226 · 100
ηµ
log
d log(16κ)
4δ

,
the distribution of xK has total variation at most δ
2 from ˆπ.
Complexity of Sample-Joint-Dist
We ﬁrst state a guarantee on the subroutine Y-Oracle, which we prove in Appendix I.3.4.
Lemma 331 (Y-Oracle guarantee). For δ ∈[0, 1], deﬁne Rδ as in (I.6), and let η =
1
32Lκd log(16κ/δ).
For any x with ∥x −x∗∥2 ≤
p
κd log(16κ/δ) · Rδ, Algorithm 3 (Y-Oracle) draws an exact sample y
from the density proportional to exp

−f(y) −
1
2η ∥y −x∥2
2

in an expected 2 iterations.
We also state a result due to [128], which bounds the mixing time of 1-step Metropolized HMC
for well-conditioned distributions; this handles the case when ∥x −x∗∥2 is large in Algorithm 3.
Proposition 81 (Theorem 1, [128]). Let π be a distribution on Rd whose negative log-density is
convex and has condition number bounded by a constant. Then, Metropolized HMC from an explicit
starting distribution mixes to total variation δ to the distribution π in O(d log( d
δ )) iterations.
Proposition 46. Sample-Joint-Dist outputs a point with distribution within δ total variation
distance from the x-marginal of ˆπ. The expected number of gradient queries per iteration is constant.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
830
Proof. Under an exact Y-Oracle, Corollary 75 shows the output distribution of Sample-Joint-Dist
has total variation at most δ
2 from ˆπ. Next, the resulting distribution of the subroutine Y-Oracle
is never larger than δ/(2Kd log( dκ
δ )) in total variation distance away from an exact sampler. By
running for K steps, and using the coupling characterization of total variation, it follows that this
can only incur additional error δ/(2d log( dκ
δ )), proving correctness (in fact, the distribution is always
at most O((d log(dκ/δ))−1) away in total variation from an exact Y-Oracle).
Next, we prove the guarantee on the expected gradient evaluations per iteration. Lemma 331
shows whenever the current iterate xk has ∥x −x∗∥2 ≤
p
κd log(16κ/δ) · Rδ, the expected number
of gradient evaluations is constant, and moreover Proposition 81 shows that the number of gradient
evaluations is never larger than O(d log( dκ
δ )), where we use that the condition number of the log-
density in (10.24) is bounded by a constant. Therefore, it suﬃces to show in every iteration 0 ≤
k ≤K, the probability ∥xk −x∗∥2 >
p
κd log(16κ/δ) · Rδ is O((d log(dκ/δ))−1). By the warmness
assumption in Lemma 328, and the concentration bound in Fact 28, the probability x0 does not
satisfy this bound is negligible (inverse exponential in κd2 log(κ/δ)). Since warmness is monotonically
decreasing with an exact sampler,1 and the accumulated error due to inexactness of Y-Oracle is at
most O((d log(dκ/δ))−1) through the whole algorithm, this holds for all iterations.
I.3
Mixing time ingredients
We now prove facts which are used in the mixing time analysis of Sample-Joint-Dist. Throughout
this section, as in the speciﬁcation of Sample-Joint-Dist, f and g are functions with properties as
in (10.20), and share a minimizer x∗.
I.3.1
Warm start
We show that we obtain a warm start for the distribution ˆπ in algorithm Sample-Joint-Dist via
one call to the restricted Gaussian oracle for g, by proving Lemma 328.
Lemma 328 (Warm start). For η ≤
1
Lκd, πstart deﬁned in (10.23) is a 2(1 + κ)
d
2 -warm start for ˆπ.
Proof. By the deﬁnitions of ˆπ and πstart in (10.22), (10.23), we wish to bound everywhere the
quantity
dπstart
dˆπ
(x) =
Zˆπ
Zstart
·
exp

−L
2 ∥x −x∗∥2
2 −ηL2
2 ∥x −x∗∥2
2 −g(x)

R
y exp

−f(y) −g(x) −
1
2η ∥y −x∥2
2 −ηL2
2 ∥x −x∗∥2
2

dy
.
(I.8)
1This fact is well-known in the literature, and a simple proof is that if a distribution is warm, then taking one step
of the Markov chain induces a convex combination of warm point masses, and is thus also warm.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
831
Here, Zˆπ is as in Deﬁnition 61, and we let Zstart denote the normalization constant of πstart, i.e.
Zstart :=
Z
x
exp

−L
2 ∥x −x∗∥2
2 −ηL2
2
∥x −x∗∥2
2 −g(x)

dx.
Regarding the ﬁrst term of (I.8), the earlier derivation (I.4) showed
Z
y
exp

−f(y) −g(x) −1
2η ∥y −x∥2
2 −ηL2
2
∥x −x∗∥2
2

dy ≤(2πη)
d
2 exp (−f(x) −g(x)) .
Then, integrating, we can bound the ratio of the normalization constants
Zˆπ
Zπstart
≤
R
x(2πη)
d
2 exp (−f(x) −g(x)) dx
R
x exp

−L
2 ∥x −x∗∥2
2 −ηL2
2 ∥x −x∗∥2
2 −g(x)

dx
≤
R
x(2πη)
d
2 exp

−f(x∗) −µ
2 ∥x −x∗∥2
2 −g(x)

dx
R
x exp

−L
2 ∥x −x∗∥2
2 −µ
2 ∥x −x∗∥2
2 −g(x)

dx
≤(2πη)
d
2 exp (−f(x∗))

1 + L
µ
 d
2
.
(I.9)
The second inequality followed from f is µ-strongly convex and ηL2 ≤µ by assumption. The last
inequality followed from Proposition 82, where we used µ
2 ∥x −x∗∥2
2 + g(x) is µ-strongly convex.
Next, to bound the second term of (I.8), notice ﬁrst that
exp

−L
2 ∥x −x∗∥2
2 −ηL2
2 ∥x −x∗∥2
2 −g(x)

R
y exp

−f(y) −g(x) −
1
2η ∥y −x∥2
2 −ηL2
2 ∥x −x∗∥2
2

dy
=
exp

−L
2 ∥x −x∗∥2
2

R
y exp

−f(y) −
1
2η ∥y −x∥2
2

dy
.
It thus suﬃces to lower bound exp

L
2 ∥x −x∗∥2
2
 R
y exp

−f(y) −
1
2η ∥y −x∥2
2

dy. We have
exp
L
2 ∥x −x∗∥2
2
 Z
y
exp

−f(y) −1
2η ∥y −x∥2
2

dy
≥exp

−f(x) + L
2 ∥x −x∗∥2
2
 Z
y
exp

−⟨∇f(x), y −x⟩−
 1
2η + L
2

∥y −x∥2
2

dy
= exp

−f(x) + L
2 ∥x −x∗∥2
2
  2πη
1 + Lη
 d
2
exp

η
2(1 + Lη) ∥∇f(x)∥2
2

≥exp(−f(x∗))
 2πη
1 + Lη
 d
2
(I.10)
The ﬁrst and third steps followed from L-smoothness of f, and the second applied the Gaussian

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
832
integral (Fact 27). Combining the bounds in (I.9) and (I.10), (I.8) becomes
dπstart
dˆπ
(x) ≤

1 + L
µ
 d
2
(1 + Lη)
d
2 ≤2(1 + κ)
d
2 ,
where x ∈Rd was arbitrary, which completes the proof.
I.3.2
Transitions of nearby points
Here, we prove Lemma 329. Throughout this section, Tx is the density of xk, according to the steps
in Lines 6 and 7 of Sample-Joint-Dist (Algorithm 63) starting at xk−1 = x. We also deﬁne Px
to be the density of yk, by just the step in Line 6. We ﬁrst make a simplifying observation: by
Observation 1, for any two points x, x′, we have
∥Tx −Tx′∥TV ≤∥Px −Px′∥TV .
Thus, it suﬃces to understand ∥Px −Px′∥TV for nearby x, x′ ∈Ωδ.
Our proof of Lemma 329
combines two pieces: (1) bounding the ratio of normalization constants Zx, Zx′ of Px and Px′ for
nearby x, x′ in Lemma 334 and (2) the structural result Proposition 83. To bound the normalization
constant ratio, we state two helper lemmas. Lemma 332 characterizes facts about the minimizer of
f(y) + 1
2η ∥y −x∥2
2 .
(I.11)
Lemma 332. Let f be convex with minimizer x∗, and yx minimize (I.11) for a given x. Then,
1. ∥yx −yx′∥2 ≤∥x −x′∥2.
2. For any x, ∥yx −x∗∥2 ≤∥x −x∗∥2.
3. For any x with ∥x −x∗∥2 ≤R, ∥x −yx∥2 ≤ηLR.
Proof. By optimality conditions in the deﬁnition of yx,
η∇f(yx) = x −yx.
Fix two points x, x′, and let xt := (1 −t)x + tx′. Letting Jx(yx) be the Jacobian matrix of yx,
d
dtη∇f(yxt) = d
dt (xt −yxt) =⇒η∇2f(yxt)Jx(yxt)(x′ −x) = (I −Jx(yxt))(x′ −x)
=⇒Jx(yxt)(x′ −x) = (I + η∇2f(yxt))−1(x′ −x).

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
833
We can then compute
yx′ −yx =
Z 1
0
d
dtyxtdt =
Z 1
0
Jx(yxt)(x′ −x)dt =
Z 1
0
(I + η∇2f(yxt))−1(x′ −x)dt.
By triangle inequality and convexity of f, the ﬁrst claim follows:
∥yx′ −yx∥2 ≤
Z 1
0
(I + η∇2f(yxt))−1
2 ∥x′ −x∥2 dt ≤∥x′ −x∥2 .
The second claim follows from the ﬁrst by yx∗= x∗. The third claim follows from the second via
∥x −yx∥2 = η ∥∇f(yx)∥2 ≤ηL ∥yx −x∗∥2 ≤ηLR.
Next, Lemma 333 states well-known bounds on the integral of a well-conditioned function h.
Lemma 333. Let h be a Lh-smooth, µh-strongly convex function and let y∗
h be its minimizer. Then
 2πL−1
h
 d
2 exp (−h(y∗
h)) ≤
Z
y
exp (−h(y)) ≤
 2πµ−1
h
 d
2 exp (−h(y∗
h)) .
Proof. By smoothness and strong convexity,
exp

−h(y∗
h) −Lh
2 ∥y −y∗
h∥2
2

≤exp(−h(y)) ≤exp

−h(y∗
h) −µh
2 ∥y −y∗
h∥2
2

.
The result follows by Gaussian integrals, i.e. Fact 27.
We now deﬁne the normalization constants of Px and Px′:
Zx =
Z
y
exp

−f(y) −1
2η ∥y −x∥2
2

dy,
Zx′ =
Z
y
exp

−f(y) −1
2η ∥y −x′∥2
2

dy.
(I.12)
We apply Lemma 332 and Lemma 333 to bound the ratio of Zx and Zx′.
Lemma 334. Let f be µ-strongly convex and L-smooth. Let x, x′ ∈Ωδ, for Ωδ deﬁned in (I.6), and
let ∥x −x′∥2 ≤∆. Then, the normalization constants Zx and Zx′ in (I.12) satisfy
Zx
Zx′ ≤1.05 exp

3LR∆+ L∆2
2

.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
834
Proof. First, applying Lemma 333 to Zx and Zx′ yields that the ratio is bounded by
Zx
Zx′ ≤
exp

−f(yx) −
1
2η ∥yx −x∥2
2
 
2π

µ + 1
η
−1 d
2
exp

−f(yx′) −
1
2η ∥yx′ −x∥2
2
 
2π

L + 1
η
−1 d
2
≤1.05 exp

f(yx′) −f(yx) + 1
2η

∥yx′ −x′∥2
2 −∥yx −x∥2
2

.
Here, we used the bound for η−1 ≥32Ld that
 
L + 1
η
µ + 1
η
!d/2
≤1.05.
Regarding the remaining term, recall x, x′ both belong to Ωδ, and ∥x −x′∥2 ≤∆. We have
f(yx′) −f(yx) + 1
2η

∥yx′ −x′∥2
2 −∥yx −x∥2
2

≤⟨∇f(yx), yx′ −yx⟩+ L
2 ∥yx′ −yx∥2
2 + 1
2η ⟨yx′ −x′ + yx −x, yx′ −yx + x −x′⟩
≤LR∆+ L∆2
2
+ 1
2η (∥yx −x∥2 + ∥yx′ −x′∥2) (∥yx′ −yx∥2 + ∥x′ −x∥2)
≤LR∆+ L∆2
2
+ 2ηLR
2η
(∥yx′ −yx∥2 + ∥x′ −x∥2) ≤3LR∆+ L∆2
2
.
The ﬁrst inequality was smoothness and expanding the diﬀerence of quadratics. The second was by
∥∇f(yx)∥2 ≤L ∥yx −x∗∥2 ≤LR and ∥yx′ −yx∥2 ≤∆, where we used the ﬁrst and second parts of
Lemma 332; we also applied Cauchy-Schwarz and triangle inequality. The third used the third part of
Lemma 332. Finally, the last inequality was by the ﬁrst part of Lemma 332 and ∥x′ −x∥2 ≤∆.
We now are ready to prove Lemma 329.
Lemma 329 (Transitions of nearby points). Suppose ηL ≤1, ηL2R2
δ ≤1
2, and 400d2η ≤R2
δ. For a
point x, let Tx be the density of xk after sampling according to Lines 6 and 7 of Algorithm 63 from
xk−1 = x. For x, x′ ∈Ωδ with ∥x −x′∥2 ≤
√η
10 , for Ωδ deﬁned in (I.6), we have ∥Tx −Tx′∥TV ≤1
2.
Proof. First, by Observation 1, it suﬃces to show ∥Px −Px′∥TV ≤1
2. Pinsker's inequality states
∥Px −Px′∥TV ≤
r
1
2dKL (Px, Px′),

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
835
where dKL is KL-divergence, so it is enough to show dKL (Px, Px′) ≤1
2. Notice that
dKL (Px, Px′) = log
Zx′
Zx

+
Z
y
Px(y) log


exp

−f(y) −
1
2η ∥y −x∥2
2

exp

−f(y) −
1
2η ∥y −x′∥2
2


dy.
By Lemma 334, the ﬁrst term satisﬁes, for ∆:=
√η
10 ,
log
Zx′
Zx

≤3LR∆+ L∆2
2
+ log(1.05).
To bound the second term, we have
Z
y
Px(y) log


exp

−f(y) −
1
2η ∥y −x∥2
2

exp

−f(y) −
1
2η ∥y −x′∥2
2


dy = 1
2η
Z
y
Px(y)

∥y −x′∥2
2 −∥y −x∥2
2

dy
= 1
2η
Z
y
Px(y) ⟨x −x′, 2 (y −x) + (x −x′)⟩dy
≤∆2
2η + ∆
η

Z
y
yPx(y)dy −x

2
.
Here, the second line was by expanding and the third line was by ∥x −x′∥2 ≤∆and Cauchy-
Schwarz. By Proposition 83,

R
y yPx(y)dy −x

2 ≤2ηLR, where by assumption the parameters
satisfy the conditions of Proposition 83. Then, combining the two bounds, we have
dKL (Px, Px′) ≤3LR∆+ L∆2
2
+ ∆2
2η + 2LR∆+ log(1.05) = 5LR∆+ L∆2
2
+ ∆2
2η + log(1.05).
When ∆=
√η
10 , ηL ≤1, and ηL2R2 ≤1
2, we have the desired
dKL (Px, Px′) ≤
√ηLR
2
+ Lη
200 +
1
200 + log(1.05) ≤1
2.
I.3.3
Isoperimetry
In this section, we prove Lemma 330, which asks to show that ˆπΩδ satisﬁes a log-isoperimetric
inequality (I.7). Here, we deﬁne ˆπΩδ to be the conditional distribution of the ˆπ x-marginal on set
Ωδ. We recall this means that for any partition S1, S2, S3 of Ωδ,
ˆπΩδ(S3) ≥1
2ψ d(S1, S2) · min (ˆπΩδ(S1), ˆπΩδ(S2)) ·
s
log

1 +
1
min (ˆπΩδ(S1), ˆπΩδ(S2))

.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
836
The following fact was shown in [128].
Lemma 335 ([128], Lemma 11). Any µ-strongly logconcave distribution π satisﬁes the log-isoperimetric
inequality (I.7) with ψ = µ−1
2 .
Observe that πΩδ, the restriction of π to the convex set Ωδ, is µ-strongly logconcave by the
deﬁnition of π (10.20), so it satisﬁes a log-isoperimetric inequality. We now combine this fact with
the relative density bounds Lemma 326 to prove Lemma 330.
Lemma 330 (Isoperimetry). Density ˆπ and set Ωδ deﬁned in (10.22), (I.6) satisfy (I.7) with ψ =
8µ−1
2 .
Proof. Fix some partition S1, S2, S3 of Ωδ, and without loss of generality let ˆπΩδ(S1) ≤ˆπΩδ(S2).
First, by applying Corollary 74, which shows dπ
dˆπ(x) ∈[ 1
2, 2] everywhere in Ωδ, we have the bounds
1
2πΩδ(S1) ≤ˆπΩδ(S1) ≤2πΩδ(S1), 1
2πΩδ(S2) ≤ˆπΩδ(S2) ≤2πΩδ(S2), and ˆπΩδ(S3) ≥1
2πΩδ(S3).
Therefore, we have the sequence of conclusions
ˆπΩδ(S3) ≥1
2πΩδ(S3)
≥d(S1, S2)√µ
4
· min (πΩδ(S1), πΩδ(S2)) ·
s
log

1 +
1
min (πΩδ(S1), πΩδ(S2))

≥d(S1, S2)√µ
8
· ˆπΩδ(S1) ·
s
log

1 +
1
2ˆπΩδ(S1)

≥d(S1, S2)√µ
16
· ˆπΩδ(S1) ·
s
log

1 +
1
ˆπΩδ(S1)

.
Here, the second line was by applying Lemma 335 to the µ-strongly logconcave distribution πΩδ,
and the ﬁnal line used
p
log(1 + α) ≤2
p
log(1 + α
2 ) for all α > 0.
I.3.4
Correctness of Y-Oracle
In this section, we show how we can sample y eﬃciently in the alternating scheme of the algo-
rithm Sample-Joint-Dist, within an extremely high probability region. Speciﬁcally, for any x with
∥x −x∗∥2 ≤
p
κd log(16κ/δ) · Rδ, where Rδ is deﬁned in (I.6), we give a method for implementing
draw y ∝exp

−f(y) −1
2η ∥y −x∥2
2

dy.
The algorithm is Algorithm 3, which is a simple rejection sampling scheme.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
837
Algorithm 3 Y-Oracle(f, x, η, δ)
Input: L-smooth, µ-strongly convex f : Rd →R with minimizer x∗, η > 0, δ ∈[0, 1], x ∈Rd
Output: If ∥x −x∗∥2 ≤
p
κd log(16κ/δ) · Rδ, return exact sample from distribution with density
∝exp(−f(y) −
1
2η ∥y −x∥2
2) (see (I.6) for deﬁnition of Rδ). Otherwise, return sample within δ TV
from distribution with density ∝exp(−f(y) −
1
2η ∥y −x∥2
2) if ∥x −x∗∥2 ≤
p
κd log(16κ/δ) · Rδ
then
while true do
Draw y ∼N(x −η∇f(x), ηI) τ ∼Unif[0, 1] if τ ≤exp(f(x) + ⟨∇f(x), y −x⟩−f(y)) then
Return: y
Return: Sample x within TV δ from density ∝exp(−f(y) −
1
2η ∥y −x∥2
2) using [128]
We recall that we gave guarantees on rejection sampling procedures in Lemma 185 (an "exact"
version of Lemma 324 and Corollary 73). We now prove Lemma 331 via a direct application of
Lemma 185.
Lemma 331 (Y-Oracle guarantee). For δ ∈[0, 1], deﬁne Rδ as in (I.6), and let η =
1
32Lκd log(16κ/δ).
For any x with ∥x −x∗∥2 ≤
p
κd log(16κ/δ) · Rδ, Algorithm 3 (Y-Oracle) draws an exact sample y
from the density proportional to exp

−f(y) −
1
2η ∥y −x∥2
2

in an expected 2 iterations.
Proof. For ∥x −x∗∥2 ≤
p
κd log(16κ/δ) · Rδ, Y-Oracle is a rejection sampling scheme with
p(y) = exp

−f(y) −1
2η ∥y −x∥2
2

, ˆp(y) = exp

−f(x) −⟨∇f(x), y −x⟩−1
2η ∥y −x∥2
2

.
It is clear that p(y) ≤ˆp(y) everywhere by convexity of f, so we may choose C = 1. To bound the
expected number of iterations and obtain the desired conclusion, Lemma 185 requires a bound on
R
y exp

−f(x) −⟨∇f(x), y −x⟩−
1
2η ∥y −x∥2
2

dy
R
y exp

−f(y) −
1
2η ∥y −x∥2
2

dy
,
(I.13)
the ratio of the normalization constants of ˆp and p. First, by Fact 27,
Z
y
exp

−f(x) −⟨∇f(x), y −x⟩−1
2η ∥y −x∥2
2

dy = exp

−f(x) + η
2 ∥∇f(x)∥2
2

(2πη)
d
2 .
Next, by smoothness and Fact 27 once more,
Z
y
exp

−f(y) −1
2η ∥y −x∥2
2

dy ≥
Z
y
exp

−f(x) −⟨∇f(x), y −x⟩−1 + ηL
2η
∥y −x∥2
2

dy
= exp

−f(x) +
η
2(1 + ηL) ∥∇f(x)∥2
2
  2πη
1 + ηL
 d
2
.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
838
Taking a ratio, the quantity in (I.13) is bounded above by
exp
η
2 −
η
2(1 + ηL)

∥∇f(x)∥2
2

(1 + ηL)
d
2 ≤1.5 exp

η2L
2(1 + ηL) ∥∇f(x)∥2
2

≤1.5 exp
η2L3
2
·
16κd2 log2(16κ/δ)
µ

≤2.
The ﬁrst inequality was (1 + ηL)
d
2 ≤1.5, the second used smoothness and the assumed bound on
∥x −x∗∥2, and the third again used our choice of η.
I.4
Structural results
Here, we prove two structural results about distributions whose negative log-densities are small
perturbations of a quadratic, which obtain tighter concentration guarantees compared to naive
bounds on strongly logconcave distributions. They are used in obtaining our bounds in Section I.3
(and for the warm start bounds in Section 10.4), but we hope both the statements and proof
techniques are of independent interest to the community. Our ﬁrst structural result is a bound on
normalization constant ratios, used throughout the paper.
Proposition 82. Let f : Rd →R be µ-strongly convex with minimizer x∗, and let λ > 0. Then,
R
exp(−f(x))dx
R
exp

−f(x) −
1
2λ ∥x −x∗∥2
2

dx
≤

1 + 1
µλ
 d
2
.
Proof. Deﬁne the function
R(α) :=
R
exp

−f(x) −
1
2λα ∥x −x∗∥2
2

dx
R
exp

−f(x) −
1
2λ ∥x −x∗∥2
2

dx
.
Let dπα(x) be the density proportional to exp

−f(x) −
1
2λα ∥x −x∗∥2
2

dx. We compute
d
dαR(α) =
Z
exp

−f(x) −
1
2λα ∥x −x∗∥2
2

R
exp

−f(x) −
1
2λ ∥x −x∗∥2
2

dx
1
2λα2 ∥x −x∗∥2
2 dx
= R(α)
2λα2
Z exp

−f(x) −
1
2λα ∥x −x∗∥2
2

∥x −x∗∥2
2
R
exp

−f(x) −
1
2λα ∥x −x∗∥2
2

dx
dx
= R(α)
2λα2
Z
∥x −x∗∥2
2 dπα(x) ≤R(α)
2α
·
d
µλα + 1.
Here, the last inequality was by Fact 30, using the fact that the function f(x) +
1
2λα ∥x −x∗∥2
2 is

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
839
µ +
1
λα-strongly convex. Moreover, note that R(1) = 1, and
d
dα log

α
µλα + 1

= 1
α −
µλ
µλα + 1 =
1
µλα2 + α.
Solving the diﬀerential inequality
d
dα log(R(α)) = dR(α)
dα
·
1
R(α) ≤d
2 ·
1
µλα2 + α,
we obtain the bound for any α ≥1 (since log(R(1)) = 0)
log(R(α)) ≤d
2 log
µλα + α
µλα + 1

=⇒R(α) ≤
µλα + α
µλα + 1
 d
2
≤

1 + 1
µλ
 d
2
.
Taking a limit α →∞yields the conclusion.
Our second structural result uses a similar proof technique to show that the mean of a bounded
perturbation f of a Gaussian is not far from its mode, as long as the gradient of the mode is small.
We remark that one may directly apply strong logconcavity, i.e. a variant of Fact 30, to obtain a
weaker bound by roughly a
√
d factor, which would result in a loss of Ω(d) in the guarantees of
Theorem 69. This tighter analysis is crucial in our improved mixing time result.
Before stating the bound, we apply Fact 29 to the convex functions h(x) = (θ⊤x)2 and h(x) =
∥x∥4
2 to obtain the following conclusions which will be used in the proof of Proposition 83.
Corollary 76. Let π be a µ-strongly logconcave density. Then,
1. Eπ[(θ⊤(x −Eπ[x]))2] ≤µ−1, for all unit vectors θ.
2. Eπ[∥x −Eπ[x]∥4
2] ≤3d2µ−2.
Proposition 83. Let f : Rd →R be L-smooth and convex with minimizer x∗, let x ∈Rd with
∥x −x∗∥2 ≤R, and let dπη(y) be the density proportional to exp

−f(y) −
1
2η ∥y −x∥2
2

dy. Suppose
that η ≤min

1
2L2R2 ,
R2
400d2

. Then,
Eπη[y] −x

2 ≤2ηLR.
Proof. Deﬁne a family of distributions πα for α ∈[0, 1], with
dπα(y) ∝exp

−α (f(y) −f(x) −⟨∇f(x), y −x⟩) −f(x) −⟨∇f(x), y −x⟩−1
2η ∥y −x∥2
2

dy.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
840
In particular, π1 = πη, and π0 is a Gaussian with mean x −η∇f(x). We deﬁne ¯yα := Eπα[y], and
y∗
α := argminy

α (f(y) −f(x) −⟨∇f(x), y −x⟩) + f(x) + ⟨∇f(x), y −x⟩+ 1
2η ∥y −x∥2
2

.
Deﬁne the function D(α) := ∥¯yα −x∥2, such that we wish to bound D(1). First, by smoothness
D(0) = ∥Eπ0[y] −x∥2 = ∥η∇f(x)∥2 ≤ηLR.
Next, we observe
d
dαD(α) =

¯yα −x
∥¯yα −x∥2
, d¯yα
dα

≤

d¯yα
dα

2
.
In order to bound
 d¯yα
dα

2, ﬁx a unit vector θ. We have
d¯yα
dα , θ

= d
dα
Z
(y −x)dπα(y), θ

=
Z
⟨y −x, θ⟩(f(x) + ⟨∇f(x), y −x⟩−f(y))dπα(y)
≤
sZ
(⟨y −x, θ⟩)2dπα(y)
sZ
(f(x) + ⟨∇f(x), y −x⟩−f(y))2dπα(y)
≤
sZ
(⟨y −x, θ⟩)2dπα(y)
sZ L2
4 ∥y −x∥4
2 dπα(y).
(I.14)
The third line was Cauchy-Schwarz and the last line used smoothness and convexity, i.e.
−L
2 ∥y −x∥2
2 ≤f(x) + ⟨∇f(x), y −x⟩−f(y) ≤0.
We now bound these terms. First,
Z
(⟨y −x, θ⟩)2dπα(y) ≤2
Z
(⟨y −¯yα, θ⟩)2dπα(y) + 2
Z
(⟨¯yα −x, θ⟩)2dπα(y)
≤2η + 2 ∥¯yα −x∥2
2 = 2η + 2D(α)2.
(I.15)
Here, we applied the ﬁrst part of Corollary 76, as πα is η−1-strongly logconcave, and the deﬁnition
of D(α). Next, using for any a, b ∈Rd, ∥a + b∥4
2 ≤(∥a∥2 + ∥b∥2)4 ≤16 ∥a∥4
2 + 16 ∥b∥4
2, we have
Z L2
4 ∥y −x∥4
2 dπα(y) ≤
Z
4L2 ∥y −¯yα∥4
2 dπα(y) +
Z
4L2 ∥x −¯yα∥4
2 dπα(y)
≤12L2d2η2 + 4L2D(α)4.
(I.16)

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
841
Here, we used the second part of Corollary 76. Maximizing (I.14) over θ, and applying (I.15), (I.16),
d
dαD(α) ≤

d¯yα
dα

2
≤
p
8L2(η + D(α)2)(3d2η2 + D(α)4)
≤4L(√η + D(α)) · max(2ηd, D(α)2).
(I.17)
Assume for contradiction that D(1) > 2ηLR, violating the conclusion of the proposition. By con-
tinuity of D, there must have been some ¯α ∈(0, 1) where D(¯α) = 2ηLR, and for all 0 ≤α < ¯α,
D(α) < 2ηLR. By the mean value theorem, there then exists 0 ≤ˆα ≤¯α such that
dD(ˆα)
dα
= D(¯α) −D(0)
¯α
> ηLR.
On the other hand, by our assumption that 2ηL2R2 ≤1, for any d ≥1 it follows that
2ηd ≥4η2L2R2 > D(ˆα)2,
p
2η ≥2ηLR > D(ˆα).
Then, plugging these bounds into (I.17) and using √η + D(ˆα) ≤5
2
√η as
√
2 ≤3
2,
d
dαD(ˆα) ≤4L · 5
2
√η · 2ηd = 20√η d
R · ηLR ≤ηLR.
We used η ≤
R2
400d2 in the last inequality. This is a contradiction, implying D(1) ≤2ηLR.
I.5
Equivalence of HMC and Metropolis-adjusted Langevin
dynamics
We brieﬂy remark on the equivalence of Metropolized HMC and the Metropolis-adjusted Langevin
dynamics algorithm (MALA), a well-studied algorithm since its introduction in [76]. This equivalence
was also commented on in [128].
The algorithm can be seen as a ﬁltered discretization of the
continuous-time Langevin dynamics,
dxt = −∇f(xt)dt +
√
2dWt,
where Wt is Brownian motion. In short, the Metropolized HMC update is
v ∼N(0, I), ˜x ←x + ηv −η2
2 ∇f(x), accept with probability min

1, exp(−H(˜x, ˜v))
exp(−H(x, v))

.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
842
Similarly, the MALA update with step size h is
˜x ∼N(x−h∇f(x), 2hI), accept with probability min
(
1, exp(−f(˜x) −∥x −˜x + h∇f(˜x)∥2
2 /4h)
exp(−f(x) −∥˜x −x + h∇f(x)∥2
2 /4h)
)
.
It is clear that in HMC the distribution of ˜x is
˜x ∼N

x −η2
2 ∇f(x), η2I

,
so it suﬃces to show for h = η2/2,
∥˜x −x + h∇f(x)∥2
2 −∥x −˜x + h∇f(˜x)∥2
2
4h
= 1
2

∥v∥2
2 −∥˜v∥2
2

.
Indeed, the right hand side simpliﬁes to
η
2 ⟨∇f(˜x) + ∇f(x), v⟩−η2
8 ∥∇f(˜x) + ∇f(x)∥2
2 ,
and the left hand side is
1
2 ⟨∇f(˜x) + ∇f(x), ˜x −x⟩+ h
4

∥∇f(x)∥2
2 −∥∇f(˜x)∥2
2

= 1
2

∇f(˜x) + ∇f(x), ηv −η2
2 ∇f(x)

+ η2
8

∥∇f(x)∥2
2 −∥∇f(˜x)∥2
2

.
Comparing coeﬃcients shows the equivalence.
I.6
Gradient concentration
In this section, we give a bound on how well the norm of the gradient ∥∇f(x)∥concentrates when
f is smooth and x ∼dπ∗(x)/dx ∝exp(−f(x)). First, we recall the following "Hessian-weighted"
variant of the Poincar´e inequality, which ﬁrst appeared in [95].
Theorem 94 (Hessian Poincar´e). For probability density dπ∗(x)/dx ∝exp(−f(x)), and continu-
ously diﬀerentiable function g : Rd →R with bounded variance with respect to π∗,
Varπ∗[g] ≤
Z
Rd
D ∇2f(x)
−1 ∇g(x), ∇g(x)
E
dπ∗(x).
An immediate corollary of Theorem 94 is that the Poincar´e constant of a µ-strongly logconcave
distribution is at most µ−1. While it does not appear to have been previously stated in the literature,
our concentration bound can be viewed as a simple application of an argument of Herbst which
reduces concentration to an isoperimetric inequality such as Theorem 94; an exposition of this

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
843
technique can be found in [341]. We now state the concentration result.
Theorem 95 (Gradient norm concentration). If twice-diﬀerentiable f : Rd →R is L-smooth and
convex, then for dπ∗(x)/dx ∝exp(−f(x)), and all c > 0,
Pr
π∗
h
∥∇f(x)∥≥Eπ∗[∥∇f∥] + c
√
L log d
i
≤3d−c.
Proof. Let G(x) := ∥∇f(x)∥, and let g(x) := exp( 1
2λG(x)). Clearly g is continuously diﬀerentiable.
Moreover, suppose ﬁrst for simplicity that f is strongly convex; then the existence of the variance
of g follows from the well-known fact that f has sub-Gaussian tails (e.g. [210], Lemma 1) and
Lipschitzness of its gradient, from which the sublevel sets of the gradient norm grow more slowly
than the decay of ∥x −x∗∥2. The ﬁnal conclusion has no dependence on the strong concavity of f,
and we can extend this to arbitrary convex functions by regularizing by a small amount of quadratic
regularizer (which only aﬀects smoothness) and taking a limit as the regularizer amount vanishes.
We now apply Theorem 94, which implies (noting that the gradient of ∥∇f∥is ∇2f
∇f
∥∇f∥)
Eπ∗[exp(λG)] −Eπ∗

exp
λG
2
2
≤λ2
4 Eπ∗

(∇2f) ∇f
∥∇f∥, ∇f
∥∇f∥

exp(λG)

≤Lλ2
4 Eπ∗[exp(λG)] .
In the last inequality we used smoothness. Letting H(λ) := Eπ∗[exp(λG)], for λ <
2
√
L,
H(λ) ≤
1
1 −Lλ2
4
H
λ
2
2
.
Using this recursively, we have
H(λ) ≤
∞
Y
k=0
 
1
1 −Lλ2
4k+1
!2k
lim
ℓ→∞H
λ
ℓ
ℓ
.
There are two things to estimate on the right hand side. First, for suﬃciently large ℓ,
Eπ∗

exp
λG
ℓ
ℓ
≈

1 + Eπ∗
λG
ℓ
ℓ
≈exp (λEπ∗[G]) .
Second, letting C = Lλ2
4
< 1, [88] showed that
∞
Y
k=0
 
1
1 −C
4k
!2k
≤1 +
√
C
1 −
√
C
.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
844
For completeness, we show this in Appendix I.2. Altogether, we have that for all λ <
2
√
L,
Eπ∗[exp(λG)] ≤1 + 1
2
√
Lλ
1 −1
2
√
Lλ
exp (λEπ∗[G]) .
By Markov's inequality on the exponential, we thus conclude that
Pr
π∗[G ≥Eπ∗[G] + r] ≤exp(−λr)1 + 1
2
√
Lλ
1 −1
2
√
Lλ
.
Finally, letting λ =
1
√
L and r = c
√
L log d,
Pr
π∗
h
∥∇f∥≥
√
Ld + c
√
L log d
i
≤3d−c.
As an immediate corollary, we obtain the following.
Corollary 77. If twice-diﬀerentiable f : Rd →R is L-smooth and strongly convex, then ∀c > 0,
Pr
π∗
h
∥∇f∥≥
√
Ld + c
√
L log d
i
≤3d−c.
Proof. It suﬃces to show that
Eπ∗[∥∇f∥] ≤
√
Ld.
(I.18)
This was observed in [152, 521]; we adapt a proof here. Observe that because
∇· (∇f(x)π∗(x)) = ∆f(x)π∗(x) −⟨∇f(x), ∇f(x)⟩π(x),
where ∇· is divergence and ∆is the Laplacian operator, integrating both sides and noting that the
boundary term vanishes,
Eπ∗
h
∥∇f∥2i
= Eπ∗[∆f] ≤Ld.
The last equality used smoothness of f. (I.18) then follows from concavity of the square root.
We remark that for densities dπ∗where a log-Sobolev variant of the inequality in Theorem 94
holds, we can sharpen the bound in Corollary 77 to O(d−c2).
Deﬁnition 63 (Hessian log-Sobolev). We say density dπ∗/dx ∝exp(−f(x)) satisﬁes a Hessian
log-Sobolev inequality if for all continuously diﬀerentiable g : Rd →R, and for
Entπ∗[g] :=
Z
g(x) log (g(x)) dπ∗(x)

−
Z
g(x)dπ∗(x)

log
Z
g(x)dπ∗(x)

,

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
845
we have
Entπ∗
g2
≤2
Z
Rd
D ∇2f(x)
−1 ∇g(x), ∇g(x)
E
dπ∗(x).
In general, this is a much more restrictive condition than Theorem 94; some suﬃcient conditions
are given in [89]. We now show an improved concentration result under a Hessian LSI; the proof
follows Herbst's argument, a framework developed in [341].
Theorem 96 (Gradient norm concentration under LSI). Suppose f is L-smooth and strongly convex,
and dπ∗(x)/dx ∝exp(−f(x)) satisﬁes a Hessian log-Sobolev inequality. Then for all c > 0,
Pr
π∗
h
∥∇f(x)∥≥Eπ∗[∥∇f∥] + c
p
2L log d
i
≤d−c2.
Proof. Denote G := ∥∇f∥, where we note ∇G = (∇2f)∇f
∥∇f∥. Let H(λ) := Eπ∗[exp(λG)], such that
H′(λ) = Eπ∗[G exp(λG)]. Then, for g2 = exp(λG),
H(λ) = Eπ∗
g2
, λH′(λ) = Eπ∗
g2 log g2
.
This in turn implies via the LSI that
λH′(λ) −H(λ) log H(λ) = Eπ∗
g2 log g2
−Eπ∗
g2
log Eπ∗
g2
≤2Eπ∗
h
∥∇g∥2
(∇2f)−1
i
.
(I.19)
By smoothness and the deﬁnition of g = exp( 1
2λG), we may bound the right hand side:
Eπ∗
h
∥∇g∥2
(∇2f)−1
i
= λ2
4 Eπ∗
h
∥∇G∥2
(∇2f)−1 exp(λG)
i
≤λ2L
4 H(λ).
(I.20)
In the last inequality we used our calculation of ∇G, and ∇2f ⪯LId. Now, consider the function
K(λ) = 1
λ log H(λ). We handle the deﬁnition of K(0) by a limiting argument (and log(1 + x) ≈x):
K(0) = lim
λ→0
1
λ log Eπ∗
eλG
= H(λ) −H(0)
λ
= H′(0) = Eπ∗[G] .
We compute
K′(λ) = −1
λ2 log H(λ) + H′(λ)
λH(λ) = λH′(λ) −H(λ) log H(λ)
λ2H(λ)
.
This, combined with (I.19) and (I.20) imply K′(λ) ≤L
2 . Therefore, by integrating, we have
K(λ) ≤Eπ∗[G] + Lλ
2 ⇒H(λ) = exp(λK(λ)) ≤exp

λEπ∗[G] + Lλ2
2

.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
846
Finally, we have concentration:
Pr
π∗[G ≥Eπ∗[G] + r] = Pr
π∗[exp(λG) ≥exp(λEπ∗[G] + λr)] ≤exp

−λr + Lλ2
2

,
where the last statement is by Markov. Choosing r = c√2L log d, λ = r/L yields the conclusion.
This sharpening is desirable for reasons related to the warmness of starting distributions for
sampling from π∗. However, the "Hessian log-Sobolev" inequality is strictly stronger than Theo-
rem 94, and does not hold for general strongly logconcave distributions [89]. Correspondingly, the
concentration arguments derivable from Poincar´e inequalities appear to be weaker [341]: we ﬁnd
exploring the tightness of Corollary 77 to be an interesting open question.
I.7
Necessity of ﬁxing a scale
We give a simple argument showing if the step size η of the HMC algorithm does not depend on the
"scale" of the problem, namely the eigenvalues of the function Hessian (as opposed to scale-invariant
quantities, e.g. the condition number κ and the dimension), then the task of proving lower bounds
becomes much more trivial. In particular, we can adaptively pick a scale of the problem in response
to the ﬁxed η. This justiﬁes the additional requirement in Theorems 71, 72, 73 and 74 of the ﬁxed
scale [1, κ], which we remark is a strengthening of an analogous scale-free lower bound.
Concretely, suppose we wished to prove the statement of Theorem 74 but only on functions with
condition number κ (without specifying a range of eigenvalues). Then, for ﬁxed η, K, consider
f(x) = λ
2 x2, where λ := 2
 1 −cos
  π
K

η2
.
Clearly, f : R →R has condition number 1 ≤κ for any κ. Then, the proof of Proposition 52 applies
to show that the HMC Markov chain cannot leave any symmetric set, because the coeﬃcients
encounter extremal points or zeroes of the Chebyshev polynomials.
I.8
HMC lower bounds beyond κ
√
d
Here, we analyze the behavior of HMC on the hard function (10.44). We will use this construction
to demonstrate that when the number of steps K is small, we cannot improve either the relaxation
time (Section I.8.1) or the mixing time (Section I.8.2) of MALA by more than roughly a O(K)
factor.

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
847
I.8.1
Relaxation time lower bound for small K
We ﬁrst give a bound on the acceptance probability (10.14) for general HMC Markov chain. We
expand the term −H(xK, vK) + H(x0, v0) and extend the result given by Lemma 203.
Lemma 336. For the iterates given by Fact 34, write ˜xj := x0 + ηjv0 for 0 ≤j ≤K −1. Then,
for a κ-smooth function f,
−H(xK, vK) + H(x0, v0) ≤
K−1
X
j=0

−f(˜xj+1) + f(˜xj) + 1
2 ⟨ηv0, ∇f(˜xj+1) + ∇f(˜xj)⟩

+ηK ∥v0∥2 max
0≤j≤K ∥∇f(xj) −∇f(˜xj)∥2 + 1
2η2K2
max
0≤j1,j2≤K ∥∇f(˜xK) −∇f(xj2)∥2 ∥∇f(xj1)∥2
+1
2η2K2
max
0≤j1,j2,j3≤K ∥∇f(xj3)∥2 ∥∇f(xj1) −∇f(xj2)∥2 .

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
848
Proof. Expanding H (x0, v0) −H (xK, vK) according to the deﬁnition of H, xK and vK,
H (x0, v0) −H (xK, vK)
= −f(xK) + f(x0) −
v0 −η
2∇f(x0) −η PK−1
j=1 ∇f(xj) −η
2∇f(xK)

2
2
2
+ ∥v0∥2
2
2
= −f(xK) + f(˜xK) −f(˜xK) + f(x0) +
*
v0, η
2∇f(x0) + η
K−1
X
j=1
∇f(xj) + η
2∇f(xK)
+
−1
2

η
2∇f(x0) + η
K−1
X
j=1
∇f(xj) + η
2∇f(xK)

2
2
= −f(xK) + f(˜xK) +
K−1
X
j=0
(−f(˜xj+1) + f(˜xj)) +
*
ηv0, 1
2∇f(˜x0) +
K−1
X
j=1
∇f(˜xj) + 1
2∇f(˜xK)
+
−1
2

η
2∇f(x0) + η
K−1
X
j=1
∇f(xj) + η
2∇f(xK)

2
2
+
*
ηv0,

1
2∇f(x0) +
K−1
X
j=1
∇f(xj) + 1
2∇f(xK)

−

1
2∇f(˜x0) +
K−1
X
j=1
∇f(˜xj) + 1
2∇f(˜xK)


+
=
K−1
X
j=0

−f(˜xj+1) + f(˜xj) + 1
2 ⟨ηv0, ∇f(˜xj+1) + ∇f(˜xj)⟩

−f(xK) + f(˜xK) −1
2

η
2∇f(x0) + η
K−1
X
j=1
∇f(xj) + η
2∇f(xK)

2
2
+
*
ηv0,

1
2∇f(x0) +
K−1
X
j=1
∇f(xj) + 1
2∇f(xK)

−

1
2∇f(˜x0) +
K−1
X
j=1
∇f(˜xj) + 1
2∇f(˜xK)


+
.
(I.21)
Now we bound the last two lines in the decomposition (I.21). For the second-to-last line of (I.21),

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
849
by convexity of f and the Cauchy-Schwarz inequality,
−f(xK) + f(˜xK) −1
2

η
2∇f(x0) + η
K−1
X
j=1
∇f(xj) −η
2∇f(xK)

2
2
≤
*
∇f(˜xK), 1
2Kη2∇f(x0) + η2
K−1
X
j=1
(K −j)∇f(xj)
+
−1
2

η
2∇f(x0) + η
K−1
X
j=1
∇f(xj) + η
2∇f(xK)

2
2
≤1
2η2K2
max
0≤j1,j2,j3≤K
 ∇f(˜xK)⊤∇f(xj1) −∇f(xj2)⊤∇f(xj3)

≤1
2η2K2

max
0≤j1,j2≤K ∥∇f(˜xK) −∇f(xj2)∥2 ∥∇f(xj1)∥2 +
max
0≤j1,j2,j3≤K ∥∇f(xj3)∥2 ∥∇f(xj1) −∇f(xj2)∥2

.
(I.22)
In the third line above, we used that the total "number of gradient inner products" for both terms
is 1
2η2K2, and took the largest such inner product diﬀerence.
Finally, for the last line of (I.21), by the Cauchy-Schwarz inequality,
*
ηv0,

1
2∇f(x0) +
K−1
X
j=1
∇f(xj) + 1
2∇f(xK)

−

1
2∇f(˜x0) +
K−1
X
j=1
∇f(˜xj) + 1
2∇f(˜xK)


+
≤ηK ∥v0∥2 max
0≤j≤K ∥∇f(xj) −∇f(˜xj)∥2 .
(I.23)
Combining (I.21), (I.22) and (I.23) proves the desired claim.
We deﬁne a hard function fhard : Rd →R that is κ-smooth and 1-strongly convex (note it is the
same hard function as in Section 10.10, under the change of variable h = η2
2 ). We will show it is
hard to sample from the density proportional to exp(−fhard) when K is small.
fhard(x) :=
X
i∈[d]
fi(xi), where fi(c) =



1
2c2
i = 1
κ
3 c2 −κη2
6 cos
 √
2c
η

2 ≤i ≤d
.
(I.24)
Lemma 337. For η2 ≤1, let ˜xj := x0 + ηjv0 for 0 ≤j ≤K −1 and v0 ∼N(0, I). Let R(j) be the
random variable with given by R(j) = Pd
i=1 R(j)
i
where
R(j)
i
= −fi([˜xj+1]i) + fi([˜xj]i) + 1
2η[v0]i · (∇fi([˜xj+1]i) + ∇fi([˜xj]i)).
Then,
Ev0∼N(0,1)


K−1
X
j=0
R(j)

≤−0.02κη2
d
X
i=2
cos
√
2[x0]i
η
.
(I.25)

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
850
and
Pr


K−1
X
j=0
R(j) −E


K−1
X
j=0
R(j)

≥10η2Kκ
p
d log d

≤1
d5 .
(I.26)
Proof. In this proof, all expectations E are taken over v0 ∼N(0, I), so we omit them. For i = 1,
E


K−1
X
j=0
R(j)
i

= E

−1
2([x0]1 + ηK[v0]1)2 + 1
2[x0]2
1 + 1
2
K−1
X
j=0
η[v0]1(2[x0]1 + η(2j + 1)[v0]1)


= E

−1
2[x0]2
1 −1
2η2K2[v0]2
1 −ηK[x0]1[v0]1 + 1
2[x0]2
1 + 1
2η2K2[v0]2
1 + ηK[x0]1[v0]1

= 0.
We bound each coordinate 2 ≤i ≤d separately.
E


K−1
X
j=0
R(j)
i


=E


K−1
X
j=0
−fi([˜xj+1]i) + fi([˜xj]i) + 1
2η[v0]i · (∇fi([˜xj+1]i) + ∇fi([˜xj]i))


= −κ
3 E
h
([x0]i + ηK[v0]i)2 −[x0]2
i
i
+ 1
3ηκE

[v0]i ·

2[x0]i + η
K−1
X
j=0
(2j + 1)[v0]i




+κη2
6 E


K−1
X
j=0
cos
√
2 ([x0]i + η(j + 1)[v0]i)
η
−cos
√
2 ([x0]i + ηj[v0]i)
η


+
√
2η2κ
12
E

[v0]i
K−1
X
j=0
 
sin
√
2 ([x0]i + ηj[v0]i)
η
+ sin
√
2 ([x0]i + η(j + 1)[v0]i)
η
!

= −κη2
6
K−1
X
j=0
exp(−j2) −exp(−(j + 1)2) −j exp(−j2) −(j + 1) exp(−(j + 1)2) cos
√
2[x0]i
η
The last line used the computation
E
"
[v0]i sin
√
2 ([x0]i + ηj[v0]i)
η
#
=
√
2jexp(−j2) cos
√
2[x0]i
η
,
E
"
cos
√
2 ([x0]i + ηj[v0]i)
η
#
= exp(−j2) cos
√
2[x0]i
η
.
Next, we bound PK−1
j=0
 exp(−j2) −exp(−(j + 1)2) −j exp(−j2) −(j + 1) exp(−(j + 1)2)

. For j =
0, 1 −
2
exp(1) ≥0.264. For j = 1, the negative terms have −3 exp(−4) ≥−0.06, and the positive

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
851
terms can only help this inequality. For the remaining terms,
K−1
X
j=2
 exp(−j2) −exp(−(j + 1)2) −j exp(−j2) −(j + 1) exp(−(j + 1)2)

≥
K−1
X
j=2
 −j exp(−j2) −(j + 1) exp(−(j + 1)2)

≥−2
K
X
j=2
 j exp(−j2)

≥−2
2
exp(4)
1
1 −2 exp(−5) ≥−0.075.
The last inequality used the ratio between two consecutive terms is bounded by j+1
j
exp(j2 −(j +
1)2) ≤2 exp(−5). Summing over d coordinates proves (I.25).
Next, we prove the concentration property of PK−1
j=0 R(j). Let ˜xj,s = ˜xj + sηv0, for s ∈[0, 1] and
j = 0, ..., K −1. By Lemma 197, we have
K−1
X
j=0
R(j) =
K−1
X
j=0
−η2
Z 1
0
1
2 −s

v⊤
0 ∇2f(˜xj,s)v0ds.
For coordinate 1 ≤i ≤d,
η2 R 1
0
  1
2 −s

f ′′
i ([xj,s]i)ds
 ≤η2κ
2
by smoothness. Then, the random
variables PK−1
j=0 R(j)
i
−E
hPK−1
j=0 R(j)
i
i
for 1 ≤i ≤d are sub-exponential with parameter η2κK
2
(for
coordinates where the coeﬃcient is negative, note the negation of a sub-exponential random variable
is still sub-exponential). Hence, by Fact 33,
Pr

X
i∈[d]
 K−1
X
k=0
R(j)
i
−E
"K−1
X
k=0
R(j)
i
#!
≥10η2Kκ
p
d log d

≤1
d5 .
Now, we build a bad set Ωhard with lower bounded measure that starting from a point x0 ∈Ωhard,
such that with high probability, −E
hPK−1
j=0 R(j)i
is very negative. Let h = 1
2η2 so that we may use
the results from Section 10.8. We use the bad set Ωhard deﬁned in (10.45).
Ωhard =
(
x
 |x1| ≤2, ∀2 ≤i ≤d, ∃ki ∈Z, |ki| ≤⌊
5
π
√
hκ
⌋, such that
−9
20π
√
h + 2πki
√
h ≤xi ≤9
20π
√
h + 2πki
√
h
)
.
We restate Lemma 200 here, which lower bounds π∗(Ωhard) and bounds ∥∇f(x)∥2 for x ∈Ωhard.
Lemma 200. Let h ≤
1
10000π2κ.
Let π∗have log-density −fhard (10.44).
Then, π∗(Ωhard) ≥

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
852
exp(−d). Moreover, for all x ∈Ωhard, ∥∇f(x)∥2 ≤10
√
κd.
We can further show the following, which is used to bound the remaining terms in Lemma 336.
Lemma 338. Let x0 ∈Ωhard, ηK ≤
1
100√κ log d and d ≥8.
Let let xj for 1 ≤j ≤K −1
be given by the iterates in Fact 34 and ˜xK = x0 + ηKv0.Then, with probability at least 1 −
1
d5
over random v0 ∼N(0, I), ∥v0∥2 ≤4
√
d log d and for all 0 ≤j ≤K, ∥∇f(xj)∥2 ≤11
√
κd and
∥∇f(˜xK)∥2 ≤11
√
κd.
Proof. We ﬁrst derive a bound on v0 ∼N(0, I). By a standard Gaussian tail bound, for d ≥8, with
probability at least 1−1
d5 , |[v0]i| ≤4 log d for all 1 ≤i ≤d. Then, ∥v0∥2 ≤
p
16d(log d)2 = 4
√
d log d.
Now, we prove the bound on ∥xj −x0∥2 and ∥∇f(xj)∥2 using induction. First, ∥∇f(x0)∥≤11
√
dκ
holds by Lemma 200. Assume for induction ∥∇f(xk)∥2 ≤11
√
dκ for 1 ≤k < j. Then,
∥xj −x0∥2 ≤
ηjv0 −η2j
2 ∇f(x0) −η2
j−1
X
k=1
(j −k)∇f(xk)

2
≤4ηj
√
d log d + η2j2 · 11
√
κd ≤
r
d
κ.
The last inequality used the assumption ηK ≤
1
100√κ log d. Since f is κ-smooth, we have
∥∇f(xj)∥2 ≤∥∇f(x0)∥2 + κ ∥xj −x0∥2 ≤10
√
κd + κ
r
d
κ ≤11
√
κd.
This completes the induction step. Finally, we have
∥∇f(˜xK)∥2 ≤∥∇f(x0)∥2 + κ ∥ηKv0∥2 ≤10
√
κd + 4ηKκ
√
d log d ≤11
√
κd,
where we used ηK ≤
1
100√κ log d.
Lemma 339. Let η and K satisfy K ≤
√
d
10000√log d, and ηK3 ≤
1
100000√κ log d. For any x0 ∈Ωhard,
let (xK, vK) be given by the iterates in Fact 34 and v0 ∼N(0, I). With probability at least 1 −2
d5 ,
−H(xK, vK) + H(x0, v0) ≤−Ω
 η2κd

.
Proof. We ﬁrst remark that the bound on ηK3 implies we may apply Lemma 200 and Lemma 338.
Next, for x0 ∈Ωhard, cos
√
2[x0]i
η
is bounded away from 0 for all 2 ≤i ≤d. By Lemma 337, when
K ≤
√
d
10000√log d, with probability at least 1 −
1
d5 , PK−1
j=0 R(j) ≤−0.002η2κd (the expectation term

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
853
dominates). By Lemma 338, with probability at least 1 −1
d5 , the other terms in Lemma 336 have
ηK ∥v0∥2 max
0≤j≤K ∥∇f(xj) −∇f(˜xj)∥2 + 1
2η2K2
max
0≤j1,j2≤K ∥∇f(˜xK) −∇f(xj2)∥2 ∥∇f(xj1)∥2
+1
2η2K2
max
0≤j1,j2,j3≤K ∥∇f(xj3)∥2 ∥∇f(xj1) −∇f(xj2)∥2
≤4ηK
√
d log d · κη2

K ∥∇f(x0)∥2 +
X
j∈[K−1]
(K −j) ∥∇f(xj)∥2


+η2K2 · 11
√
κd · κ

ηK ∥v0∥2 + η2K ∥∇f(x0)∥2 + η2
X
j∈[K−1]
(K −j) ∥∇f(xj)∥2


≤44η3K3κ1.5d log d + 44η3K3κ1.5d log d + 121η4K4κ2d ≤0.001η2κd.
The last inequality used the assumption η ≤
1
100000K3√κ log d. Combining the above bounds with
Lemma 336 yields the claim.
Proposition 84. For η2K = O
 √log d
κ
√
d

and K = O
 d0.099
, there is a target density on Rd whose
negative log-density is κ smooth, such that relaxation time of HMC is Ω
  κd
K2

.
Proof. It is straightforward to check that such a range of η and K satisﬁes the assumptions of
Lemma 339. Applying Lemma 339 with the hard function fhard, the remainder of the proof follows
analogously to that of Theorem 74.
We give a brief discussion of the implications of Proposition 84. For η2K = ω(
√log d
κ
√
d ), the proof of
Theorem 74 rules out a polynomial relaxation time. In the remaining range, Proposition 84 implies
that for small K = O
 d0.099
, the most we can improve the relaxation time of MALA (Theorem 72)
by taking multiple steps in HMC is by a K2 factor. Since each iteration takes K gradients, this is
roughly an improvement of K in the query complexity, and strengthens Theorem 74 for small K.
I.8.2
Mixing time lower bound for small K
In this section, we ﬁrst use prior results to narrow down the range of η we consider (assuming K is
small). We then generalize the ideas of Section 10.9, our MALA mixing lower bound, to this setting.
Mixing time lower bound for large η. Suppose K = O
 d0.099
throughout this section. The argu-
ments of Section 10.10, speciﬁcally Proposition 52 and Lemma 209, imply mixing time lower bounds
for all ηK = Ω( 1
√κ) (using the "boosting constants" argument of Section 10.10.2 for suﬃciently
large κ as necessary). For ηK = O( 1
√κ), the proof of Theorem 74 further implies mixing time lower
bounds for all η2K = ω(
√log d
κ
√
d ). Hence, we can assume ηK = O( 1
√κ) and η2K = O(
√log d
κ
√
d ).
Next, under the further assumption that K = O
 d0.099
, it is easy to check under the speciﬁed
assumptions on η and K, the preconditions of Lemma 339 are met. This implies that we can rule

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
854
out η2 = ω( log d
κd ) for polynomial-time mixing. Thus, in the following discussion we assume
K = O
 d0.099
, η2 = O
log d
κd

.
(I.27)
Mixing time lower bound for small η. Let π∗= N(0, I) be the standard d-dimensional multivari-
ate Gaussian. We will let π0 be the marginal distribution of π∗on the set
Ω:=

x | ∥x∥2
2 ≤1
2d

.
Recall from Lemma 195 that π0 is a exp(d)-warm start. Our main proof strategy will be to show
that for small η and K as in (I.27), after T = O(
κd
K2 log3 d) iterations, with constant probability both
of the following events happen: no rejections occur throughout the Markov chain, and ∥xt,K∥2
2 ≤
9
10d
holds for all t ∈[T]. Combining these two facts will demonstrate our total variation lower bound.
Lemma 340. Let {xt,k, vt,k}0≤t<T,0≤k≤K be the sub-iterates generated by the HMC Markov chain
with step size η2 = O

log d
κd

and η2K2 ≤1, for T = O(
κd
K2 log3 d) and x0 ∼π0; we denote the actual
HMC iterates by {xt}0≤t<T . With probability at least
99
100, both of the following events occur:
1. Throughout the Markov chain, ∥xt∥2 ≤0.9
√
d.
2. Throughout the Markov chain, the Metropolis ﬁlter never rejected.
Proof. Let h =
1
2η2. We inductively bound the failure probability of the above events in every
iteration by 0.01
T , which will yield the claim via a union bound. Take some iteration t + 1, and note
that by triangle inequality, and assuming all prior iterations did not reject,
∥xt+1,K∥2 ≤∥x0,0∥2 + ηK

t
X
s=0
vs,0
 + η2K
t
X
s=0
K
X
k=1
∥xs,k∥2 ≤∥x0,0∥2 + 0.9η2K2T
√
d + ηK ∥Gt∥2
≤0.8
√
d + ηK ∥Gt∥2 .
Here, we applied the inductive hypothesis on all ∥xs,k∥2, the initial bound ∥x0,0∥2 ≤
q
1
2d, and
that η2K2T = o(1) by assumption.
We also deﬁned Gt = Pt
s=0 vt,0, where vt,0 is the random
Gaussian used by HMC in iteration k; note that by independence, Gt ∼N(0, t + 1). By Fact 32,
with probability at least
1
200T , ∥Gt∥2 ≤2
√
Td, and hence 0.8
√
d + ηK ∥Gt∥2 ≤0.9
√
d, as desired.
Next, we prove that with probability ≥1−
1
200T , step t does not reject. This concludes the proof
by union bounding over both events in iteration t, and then union bounding over all iterations. By
Corollary 53 and the calculation in Lemma 206, when η2K2 ≤1, the accept probability is
min

1, exp
h
4
 2α −α2
∥xt,0∥2
2 −β2 ∥vt,0∥2
2 −2(1 −α)β ⟨xt,0, vt,0⟩

,

APPENDIX I. DEFERRED PROOFS FROM CHAPTER 10
855
for some α ∈

0.8hK2, hK2
and β ∈
h
0.8
√
2hK,
√
2hK
i
. We lower bound the argument of the
exponential as follows. With probability at least 1 −d−5 ≥1 −
1
400T , Facts 31 and 32 imply both of
the events ∥vt,0∥2
2 ≤2d and ⟨xt,0, vt,0⟩≤10√log d ∥xt,0∥2 occur. Conditional on these bounds, we
compute (using 2α ≥α2 and the assumption ∥xt∥2 ≤0.9
√
d)
 2α −α2
∥xt,0∥2
2 −β2 ∥g∥2
2 −2(1 −α)β ⟨xt,0, g⟩≥−4hK2d −40
√
hK
p
d log d ≥−O(K2 log d).
Hence, the acceptance probability is at least
exp
 −O
 η2K2 log d

≥1 −
1
400T ,
by our choice of T with Tη2K2 log d = o(1), concluding the proof.
Proposition 85. The HMC Markov chain with step size η2 = O

log d
κd

and η2K2 ≤1 requires
Ω(
κd
K2 log3 d) iterations to reach total variation distance 1
e to π∗, starting from π0.
Proof. The proof is identical to Proposition 51, where we use Lemma 340 instead of Lemma 202.

Bibliography
[1] Mart´ın Abadi.
Tensorﬂow: learning functions at scale.
In Proceedings of the 21st ACM
SIGPLAN International Conference on Functional Programming, ICFP 2016, Nara, Japan,
September 18-22, 2016, page 1, 2016.
[2] Jacob D. Abernethy and Elad Hazan. Faster convex optimization: Simulated annealing with
an eﬃcient universal barrier. In Proceedings of the 33nd International Conference on Machine
Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 2520-2528, 2016.
[3] Jacob D. Abernethy, Kevin A. Lai, Kﬁr Y. Levy, and Jun-Kun Wang. Faster rates for convex-
concave games. In 31st Annual Conference on Computational Learning Theory (COLT), pages
1595-1625, 2018.
[4] Jacob D. Abernethy, Kevin A. Lai, and Andre Wibisono. Last-iterate convergence rates for
min-max optimization. CoRR, abs/1906.02027, 2019.
[5] Ittai Abraham, David Durfee, Ioannis Koutis, Sebastian Krinninger, and Richard Peng. On
fully dynamic graph sparsiﬁers. In Irit Dinur, editor, IEEE 57th Annual Symposium on Foun-
dations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick,
New Jersey, USA, pages 335-344. IEEE Computer Society, 2016.
[6] Dimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary
coins. J. Comput. Syst. Sci., 66(4):671-687, 2003.
[7] Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. In
International Conference on Computational Learning Theory, pages 458-469. Springer, 2005.
[8] Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva. Iterative reﬁnement for
p-norm regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages 1405-1424,
2019.
856

BIBLIOGRAPHY
857
[9] Alekh Agarwal, Sahand Negahban, and Martin J Wainwright.
Fast global convergence of
gradient methods for high-dimensional statistical recovery. The Annals of Statistics, pages
2452-2482, 2012.
[10] Naman Agarwal, Sham M. Kakade, Rahul Kidambi, Yin Tat Lee, Praneeth Netrapalli, and
Aaron Sidford. Leverage score sampling for faster accelerated regression and ERM. In Algo-
rithmic Learning Theory, ALT 2020, pages 22-47, 2020.
[11] Pankaj K. Agarwal and R. Sharathkumar. Approximation algorithms for bipartite matching
with metric and geometric costs. In Symposium on Theory of Computing, STOC 2014, New
York, NY, USA, May 31 - June 03, 2014, pages 555-564, 2014.
[12] Kook Jin Ahn and Sudipto Guha. Linear programming in the semi-streaming model with
application to the maximum matching problem. Inf. Comput., 222:59-79, 2013.
[13] Kook Jin Ahn and Sudipto Guha. Access to data and number of iterations: Dual primal
algorithms for maximum matching under resource constraints. ACM Trans. Parallel Comput.,
4(4):17:1-17:40, 2018.
[14] Kook Jin Ahn, Sudipto Guha, and Andrew McGregor. Analyzing graph structure via lin-
ear measurements. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA 2012, Kyoto, Japan, January 17-19, 2012, pages 459-467, 2012.
[15] Abhishek Aich and P Palanisamy. On application of omp and cosamp algorithms for doa esti-
mation problem. In 2017 International Conference on Communication and Signal Processing
(ICCSP), pages 1983-1987. IEEE, 2017.
[16] Ahmet Alacaoglu and Yura Malitsky. Stochastic variance reduction for variational inequality
methods. arXiv e-prints, abs/2102.08352, 2021.
[17] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. J.
Mach. Learn. Res., 18:221:1-221:51, 2017.
[18] Zeyuan Allen Zhu. Katyusha: the ﬁrst direct acceleration of stochastic gradient methods. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC
2017, Montreal, QC, Canada, June 19-23, 2017, pages 1200-1205, 2017.
[19] Zeyuan Allen-Zhu, Yin Tat Lee, and Lorenzo Orecchia. Using optimization to obtain a width-
independent, parallel, simpler, and faster positive sdp solver. In Proceedings of the twenty-
seventh annual ACM-SIAM symposium on Discrete algorithms, pages 1824-1831. Society for
Industrial and Applied Mathematics, 2016.

BIBLIOGRAPHY
858
[20] Zeyuan Allen-Zhu and Yuanzhi Li.
Follow the compressed leader: Faster online learning
of eigenvectors and faster MMWU. In Proceedings of the 34th International Conference on
Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 116-125,
2017.
[21] Zeyuan Allen Zhu, Zhenyu Liao, and Lorenzo Orecchia. Spectral sparsiﬁcation and regret min-
imization beyond matrix multiplicative updates. In Proceedings of the Forty-Seventh Annual
ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17,
2015, pages 237-245, 2015.
[22] Zeyuan Allen-Zhu, Zhenyu Liao, and Yang Yuan. Optimization algorithms for faster compu-
tational geometry. In 43rd International Colloquium on Automata, Languages, and Program-
ming, pages 53:1-53:6, 2016.
[23] Zeyuan Allen Zhu and Lorenzo Orecchia. Nearly-linear time positive LP solver with faster
convergence rate. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory
of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pages 229-236, 2015.
[24] Jason Altschuler, Francis Bach, Alessandro Rudi, and Jonathan Niles-Weed. Massively scalable
sinkhorn distances via the nystr¨om method. In Advances in Neural Information Processing
Systems, pages 4427-4437, 2019.
[25] Jason Altschuler, Jonathan Weed, and Philippe Rigollet.
Near-linear time approximation
algorithms for optimal transport via sinkhorn iteration. In Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017,
4-9 December 2017, Long Beach, CA, USA, pages 1964-1974, 2017.
[26] Nima Anari, Callum Burgess, Kevin Tian, and Thuy-Duong Vuong. Improved sampling-to-
counting reductions in high-dimensional expanders and faster parallel determinantal sampling.
CoRR, abs/2203.11190, 2022.
[27] ED Andersen, C Roos, T Terlaky, T Trafalis, and JP Warners. The use of low-rank updates
in interior-point methods. Numerical Linear Algebra and Optimization, pages 1-12, 1996.
[28] Alexandr Andoni, Aleksandar Nikolov, Krzysztof Onak, and Grigory Yaroslavtsev. Parallel
algorithms for geometric graph problems.
In Symposium on Theory of Computing, STOC
2014, New York, NY, USA, May 31 - June 03, 2014, pages 574-583, 2014.
[29] Alexandr Andoni, Cliﬀord Stein, and Peilin Zhong. Parallel approximate undirected shortest
paths via low hop emulators. In Konstantin Makarychev, Yury Makarychev, Madhur Tulsiani,
Gautam Kamath, and Julia Chuzhoy, editors, Proccedings of the 52nd Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2020, Chicago, IL, USA, June 22-26, 2020, pages
322-335. ACM, 2020.

BIBLIOGRAPHY
859
[30] Richard Andr´e-Jeannin. A generalization of morgan-voyce polynomials. Fibonacci Quarterly,
32(3), 1994.
[31] Frank J Anscombe. Rejection of outliers. Technometrics, 2(2):123-146, 1960.
[32] Mart´ın Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein generative adversarial
networks. In Proceedings of the 34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017, pages 214-223, 2017.
[33] Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a
meta-algorithm and applications. Theory Comput., 8(1):121-164, 2012.
[34] Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semideﬁnite pro-
grams. J. ACM, 63(2):12:1-12:35, 2016.
[35] Sanjeev Arora and Ravi Kannan. Learning mixtures of separated nonspherical gaussians. The
Annals of Applied Probability, 15(1A):69-92, 2005.
[36] Sanjeev Arora, Satish Rao, and Umesh V. Vazirani. Expander ﬂows, geometric embeddings
and graph partitioning. J. ACM, 56(2):5:1-5:37, 2009.
[37] Sepehr Assadi, MohammadHossein Bateni, Aaron Bernstein, Vahab S. Mirrokni, and Cliﬀ
Stein. Coresets meet EDCS: algorithms for matching and vertex cover on massive graphs. In
Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA
2019, San Diego, California, USA, January 6-9, 2019, pages 1616-1635, 2019.
[38] Sepehr Assadi and Soheil Behnezhad. Beating two-thirds for random-order streaming match-
ing. In Nikhil Bansal, Emanuela Merelli, and James Worrell, editors, 48th International Collo-
quium on Automata, Languages, and Programming, ICALP 2021, July 12-16, 2021, Glasgow,
Scotland (Virtual Conference), volume 198 of LIPIcs, pages 19:1-19:13. Schloss Dagstuhl -
Leibniz-Zentrum f¨ur Informatik, 2021.
[39] Sepehr Assadi, Arun Jambulapati, Yujia Jin, Aaron Sidford, and Kevin Tian. Semi-streaming
bipartite matching in fewer passes and optimal space. In Joseph (Seﬃ) Naor and Niv Buch-
binder, editors, Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms,
SODA 2022, Virtual Conference / Alexandria, VA, USA, January 9 - 12, 2022, pages 627-669.
SIAM, 2022.
[40] Sepehr Assadi, Sanjeev Khanna, and Yang Li. On estimating maximum matching size in graph
streams. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19, pages 1723-
1742, 2017.

BIBLIOGRAPHY
860
[41] Sepehr Assadi, Sanjeev Khanna, Yang Li, and Grigory Yaroslavtsev. Maximum matchings
in dynamic graph streams and the simultaneous communication model.
In Proceedings of
the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2016,
Arlington, VA, USA, January 10-12, 2016, pages 1345-1364, 2016.
[42] Sepehr Assadi, CliﬀLiu, and Robert E. Tarjan. An auction algorithm for bipartite matching
in streaming and massively parallel computation models. In 4th Symposium on Simplicity in
Algorithms, SOSA@SODA 2021, January 2021, 2021.
[43] Sepehr Assadi and Ran Raz.
Near-quadratic lower bounds for two-pass graph streaming
algorithms. In 61st IEEE Annual Symposium on Foundations of Computer Science, FOCS
2020, Durham, NC, USA, November 16-19, 2020, pages 342-353. IEEE, 2020.
[44] Pranjal Awasthi. Communication. Personal, September 2021.
[45] Pranjal Awasthi and Or Sheﬀet. Improved spectral-norm bounds for clustering. In Approxi-
mation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages
37-49. Springer, 2012.
[46] Pranjal Awasthi and Aravindan Vijayaraghavan. Towards learning sparsely used dictionaries
with arbitrary supports. In Mikkel Thorup, editor, 59th IEEE Annual Symposium on Foun-
dations of Computer Science, FOCS 2018, Paris, France, October 7-9, 2018, pages 283-296.
IEEE Computer Society, 2018.
[47] Mohammad Gheshlaghi Azar, R´emi Munos, and Hilbert J Kappen. Minimax pac bounds on
the sample complexity of reinforcement learning with a generative model. Machine learning,
91(3):325-349, 2013.
[48] Dmitry Babichev, Dmitrii Ostrovskii, and Francis Bach. Eﬃcient primal-dual algorithms for
large-scale multiclass classiﬁcation. arXiv preprint arXiv:1902.03755, 2019.
[49] Francis Bach.
Polynomial magic i: Chebyshev polynomials.
https://francisbach.com/
chebyshev-polynomials/, 2019.
[50] Michel Baes, Michael B¨urgisser, and Arkadi Nemirovski. A randomized mirror-prox method for
solving structured large-scale matrix saddle-point problems. SIAM J. Optimization, 23(2):934-
962, 2013.
[51] Jack Baker, Paul Fearnhead, Emily B Fox, and Christopher Nemeth. Control variates for
stochastic gradient mcmc. Statistics and Computing, 29(3):599-615, 2019.
[52] Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and Geometry of Markov Diﬀu-
sion Operators. Springer, 2014.

BIBLIOGRAPHY
861
[53] Ainesh Bakshi and Adarsh Prasad. Robust linear regression: Optimal rates in polynomial
time. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2021, 2021.
[54] Sivaraman Balakrishnan, Simon S Du, Jerry Li, and Aarti Singh. Computationally eﬃcient
robust sparse estimation in high dimensions. In Conference on Learning Theory, pages 169-
212, 2017.
[55] Palaniappan Balamurugan and Francis R. Bach. Stochastic variance reduction methods for
saddle-point problems. In Advances in Neural Information Processing Systems, 2016.
[56] Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. A discriminative framework for
clustering via similarity functions. In Proceedings of the fortieth annual ACM symposium on
Theory of computing, pages 671-680, 2008.
[57] Afonso S Bandeira, Edgar Dobriban, Dustin G Mixon, and William F Sawin. Certifying the
restricted isometry property is hard. IEEE transactions on information theory, 59(6):3448-
3450, 2013.
[58] Boaz Barak, Fernando GSL Brandao, Aram W Harrow, Jonathan Kelner, David Steurer, and
Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. In Proceedings
of the forty-fourth annual ACM symposium on Theory of computing, pages 307-326, 2012.
[59] Richard G Baraniuk, Volkan Cevher, Marco F Duarte, and Chinmay Hegde. Model-based
compressive sensing. IEEE Transactions on information theory, 56(4):1982-2001, 2010.
[60] David Barber. Bayesian reasoning and machine learning. Cambridge University Press, 2012.
[61] M Barkhagen, NH Chau, ´E Moulines, M R´asonyi, S Sabanis, and Y Zhang. On stochastic
gradient langevin dynamics with dependent data streams in the logconcave case. arXiv preprint
arXiv:1812.02709, 2018.
[62] Marco Barreno, Blaine Nelson, Anthony D. Joseph, and J. D. Tygar. The security of machine
learning. Mach. Learn., 81(2):121-148, 2010.
[63] Raef Bassily, Adam D. Smith, and Abhradeep Thakurta. Private empirical risk minimization:
Eﬃcient algorithms and tight error bounds. In 55th IEEE Annual Symposium on Foundations
of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014, pages 464-
473. IEEE Computer Society, 2014.
[64] Heinz H. Bauschke, J´erˆome Bolte, and Marc Teboulle. A descent lemma beyond lipschitz gra-
dient continuity: First-order methods revisited and applications. Math. Oper. Res., 42(2):330-
348, 2017.

BIBLIOGRAPHY
862
[65] Amir Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathe-
matics, 2017.
[66] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear
inverse problems. SIAM J. Imaging Sciences, 2(1):183-202, 2009.
[67] Ruben Becker, Andreas Karrenbauer, Sebastian Krinninger, and Christoph Lenzen. Near-
optimal approximate shortest paths and transshipment in distributed and streaming models.
In Andr´ea W. Richa, editor, 31st International Symposium on Distributed Computing, DISC
2017, October 16-20, 2017, Vienna, Austria, volume 91 of LIPIcs, pages 7:1-7:16. Schloss
Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2017.
[68] Stephen Becker, J´erˆome Bobin, and Emmanuel J Cand`es. Nesta: A fast and accurate ﬁrst-
order method for sparse recovery. SIAM Journal on Imaging Sciences, 4(1):1-39, 2011.
[69] Soheil Behnezhad, Mahsa Derakhshan, Hossein Esfandiari, Elif Tan, and Hadi Yami. Brief
announcement: Graph matching in massive datasets. In Proceedings of the 29th ACM Sym-
posium on Parallelism in Algorithms and Architectures, SPAA 2017, Washington DC, USA,
July 24-26, 2017, pages 133-136, 2017.
[70] Pierre C Bellec. The noise barrier and the large signal bias of the lasso and other convex
estimators. arXiv preprint arXiv:1804.01230, 2018.
[71] Michele Benzi and Miroslav Tˆuma. A comparative study of sparse approximate inverse pre-
conditioners. Applied Numerical Mathematics, 30(2):305-340, 1999.
[72] Aaron Bernstein. Improved bounds for matching in random-order streams. In 47th Interna-
tional Colloquium on Automata, Languages, and Programming, ICALP 2020, July 8-11, 2020,
Saarbr¨ucken, Germany (Virtual Conference), pages 12:1-12:13, 2020.
[73] Aaron Bernstein, Maximilian Probst Gutenberg, and Thatchaphol Saranurak. Deterministic
decremental reachability, scc, and shortest paths via directed expanders and congestion bal-
ancing. In 61st IEEE Annual Symposium on Foundations of Computer Science, FOCS 2020,
Durham, NC, USA, November 16-19, 2020, pages 1123-1134, 2020.
[74] Espen Bernton. Langevin monte carlo and JKO splitting. In Conference On Learning Theory,
COLT 2018, Stockholm, Sweden, 6-9 July 2018, pages 1777-1798, 2018.
[75] Dimitris Bertsimas and Santosh S. Vempala. Solving convex programs by random walks. J.
ACM, 51(4):540-556, 2004.
[76] Julian Besag. Comments on "representations of knowledge in complex systems" by u. grenan-
der and mi miller. Journal of the Royal Statistical Society, Series B, 56:591-592, 1994.

BIBLIOGRAPHY
863
[77] Aditya Bhaskara, Aravinda Kanchana Ruwanpathirana, and Maheshakya Wijewardena. Prin-
cipal component regression with semirandom observations via matrix completion. In Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pages 2665-2673. PMLR, 2021.
[78] Vijay Bhattiprolu, Mrinalkanti Ghosh, Venkatesan Guruswami, Euiwoong Lee, and Madhur
Tulsiani. Approximability of p →q matrix norms: generalized Krivine rounding and hypercon-
tractive hardness. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 1358-1368. SIAM, 2019.
[79] Peter J Bickel, Ya'acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and
dantzig selector. The Annals of statistics, 37(4):1705-1732, 2009.
[80] Joris Bierkens, Paul Fearnhead, Gareth Roberts, et al. The zig-zag process and super-eﬃcient
sampling for bayesian analysis of big data. The Annals of Statistics, 47(3):1288-1320, 2019.
[81] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector
machines. In Proceedings of the 29th International Conference on Machine Learning, ICML
2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012, 2012.
[82] Jose H. Blanchet, Arun Jambulapati, Carson Kent, and Aaron Sidford.
Towards optimal
running times for optimal transport. CoRR, abs/1810.07717, 2018.
[83] Jose H. Blanchet and Yang Kang. Distributionally robust groupwise regularization estimator.
In Proceedings of The 9th Asian Conference on Machine Learning, ACML 2017, Seoul, Korea,
November 15-17, 2017., pages 97-112, 2017.
[84] Avrim Blum. Machine learning: My favorite results, directions, and open problems. In 44th
Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings., pages 2-2.
IEEE, 2003.
[85] Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. Journal
of Algorithms, 19(2):204-234, 1995.
[86] Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing.
Applied and computational harmonic analysis, 27(3):265-274, 2009.
[87] Thomas Blumensath and Mike E Davies. Normalized iterative hard thresholding: Guaranteed
stability and performance. IEEE Journal of selected topics in signal processing, 4(2):298-309,
2010.
[88] Sergey G Bobkov and Michel Ledoux.
Poincar´e's inequalities and talagrand's concentra-
tion phenomenon for the exponential distribution.
Probability Theory and Related Fields,
107(3):383-400, 1997.

BIBLIOGRAPHY
864
[89] Sergey G Bobkov and Michel Ledoux. From brunn-minkowski to brascamp-lieb and to loga-
rithmic sobolev inequalities. GAFA, Geometric and Functional Analysis, 10:1028-1052, 2000.
[90] Nicolas Bonneel, Michiel van de Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement
interpolation using lagrangian mass transport. ACM Trans. Graph., 30(6):158:1-158:12, 2011.
[91] Digvijay Boob, Saurabh Sawlani, and Di Wang. Faster width-dependent algorithm for mixed
packing and covering lps. In Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December
2019, Vancouver, BC, Canada, pages 15253-15262, 2019.
[92] L´eon Bottou, Frank E. Curtis, and Jorge Nocedal.
Optimization methods for large-scale
machine learning. SIAM Rev., 60(2):223-311, 2018.
[93] Nawaf Bou-Rabee and Andreas Eberle. Mixing time guarantees for unadjusted hamiltonian
monte carlo. CoRR, abs/2105.00887, 2021.
[94] Nawaf Bou-Rabee and Martin Hairer. Nonasymptotic mixing of the mala algorithm. IMA
Journal of Numerical Analysis, 33(1):80-110, 2012.
[95] Herm Jan Brascamp and Elliott H Lieb. On extensions of the brunn-minkowski and pr´ekopa-
leindler theorems, including inequalities for log concave functions, and with an application to
the diﬀusion equation. Journal of Functional Analysis, 22(4):366-389, 1976.
[96] Nicolas Brosse, Alain Durmus, Eric Moulines, and Marcelo Pereyra. Sampling from a log-
concave distribution with compact support with proximal langevin monte carlo. In Proceedings
of the 30th Conference on Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-10
July 2017, pages 319-342, 2017.
[97] S. Charles Brubaker and Santosh S. Vempala. Isotropic PCA and aﬃne-invariant clustering.
In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October
25-28, 2008, Philadelphia, PA, USA, pages 551-560, 2008.
[98] S´ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends
in Machine Learning, 8(3-4):231-357, 2015.
[99] S´ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.
[100] S´ebastien Bubeck, Michael B. Cohen, Yin Tat Lee, and Yuanzhi Li. An homotopy method
for lp regression provably beyond self-concordance and in input-sparsity time. In Proceedings
of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los
Angeles, CA, USA, June 25-29, 2018, pages 1130-1137, 2018.

BIBLIOGRAPHY
865
[101] S´ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution
with projected langevin monte carlo. Discret. Comput. Geom., 59(4):757-783, 2018.
[102] S´ebastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. Complexity of
highly parallel non-smooth convex optimization. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS
2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13900-13909, 2019.
[103] Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component
analysis? Journal of the ACM (JACM), 58(3):1-37, 2011.
[104] Emmanuel J Candes and Justin K Romberg. Signal recovery from random projections. In
Computational Imaging III, volume 5674, pages 76-86. International Society for Optics and
Photonics, 2005.
[105] Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from in-
complete and inaccurate measurements. Communications on Pure and Applied Mathematics:
A Journal Issued by the Courant Institute of Mathematical Sciences, 59(8):1207-1223, 2006.
[106] Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projections:
Universal encoding strategies? IEEE transactions on information theory, 52(12):5406-5425,
2006.
[107] Yu Cao, Jianfeng Lu, and Lihan Wang. Complexity of randomized algorithms for underdamped
langevin dynamics. CoRR, abs/2003.09906, 2020.
[108] Yair Carmon, John C. Duchi, Aaron Sidford, and Kevin Tian. A rank-1 sketch for matrix
multiplicative weights. In Conference on Learning Theory, COLT 2019, 25-28 June 2019,
Phoenix, AZ, USA, pages 589-623, 2019.
[109] Yair Carmon, Arun Jambulapati, Qijia Jiang, Yujia Jin, Yin Tat Lee, Aaron Sidford, and
Kevin Tian. Acceleration with a ball optimization oracle. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020.
[110] Yair Carmon, Arun Jambulapati, Yujia Jin, and Aaron Sidford. Thinking inside the ball:
Near-optimal minimization of the maximal loss. CoRR, abs/2105.01778, 2021.
[111] Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian. Variance reduction for matrix games.
In Advances in Neural Information Processing Systems 32 (NeurIPS), pages 11377-11388,
2019.

BIBLIOGRAPHY
866
[112] Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian.
Coordinate methods for matrix
games. In 61st Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages
283-293, 2020.
[113] Bob Carpenter, Andrew Gelman, Matthew D. Hoﬀman, Daniel Lee, Ben Goodrich, Michael
Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic
programming language. Journal of Statistical Software, 76(1), 2017.
[114] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning
algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.
[115] Deeparnab Chakrabarty and Sanjeev Khanna.
Better and simpler error analysis of the
sinkhorn-knopp algorithm for matrix scaling. In 1st Symposium on Simplicity in Algorithms,
SOSA 2018, January 7-10, 2018, New Orleans, LA, USA, pages 4:1-4:11, 2018.
[116] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for convex problems
with applications to imaging. Journal of mathematical imaging and vision, 40(1):120-145,
2011.
[117] Yi-Jun Chang, Martin Farach-Colton, Tsan-sheng Hsu, and Meng-Tsung Tsai.
Streaming
complexity of spanning tree computation. In Christophe Paul and Markus Bl¨aser, editors, 37th
International Symposium on Theoretical Aspects of Computer Science, STACS 2020, March
10-13, 2020, Montpellier, France, volume 154 of LIPIcs, pages 34:1-34:19. Schloss Dagstuhl -
Leibniz-Zentrum f¨ur Informatik, 2020.
[118] Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages
47-60, 2017.
[119] Niladri Chatterji, Nicolas Flammarion, Yian Ma, Peter Bartlett, and Michael Jordan. On the
theory of variance reduction for stochastic gradient monte carlo. In International Conference
on Machine Learning, pages 764-773, 2018.
[120] Niladri S. Chatterji, Peter L. Bartlett, and Philip M. Long. Oracle lower bounds for stochastic
gradient sampling algorithms. CoRR, abs/2002.00291, 2020.
[121] Tatjana Chavdarova, Gauthier Gidel, Fran¸cois Fleuret, and Simon Lacoste-Julien. Reducing
noise in GAN training with variance reduced extragradient. In Advances in Neural Information
Processing Systems 32 (NeurIPS), pages 391-401, 2019.
[122] JeﬀCheeger. A lower bound for the smallest eigenvalue of the laplacian. In Proceedings of the
Princeton conference in honor of Professor S. Bochner, pages 195-199, 1969.

BIBLIOGRAPHY
867
[123] Changyou Chen, Wenlin Wang, Yizhe Zhang, Qinliang Su, and Lawrence Carin. A convergence
analysis for a class of practical variance-reduction stochastic gradient mcmc. arXiv preprint
arXiv:1709.01180, 2017.
[124] Li Chen, Rasmus Kyng, Yang P Liu, Richard Peng, Maximilian Probst Gutenberg, and
Sushant Sachdeva. Maximum ﬂow and minimum-cost ﬂow in almost-linear time. arXiv preprint
arXiv:2203.00671, 2022.
[125] Lijie Chen, Gillat Kol, Dmitry Paramonov, Raghuvansh R. Saxena, Zhao Song, and Huacheng
Yu. Almost optimal super-constant-pass streaming lower bounds for reachability. In Samir
Khuller and Virginia Vassilevska Williams, editors, STOC '21: 53rd Annual ACM SIGACT
Symposium on Theory of Computing, Virtual Event, Italy, June 21-25, 2021, pages 570-583.
ACM, 2021.
[126] Sitan Chen, Frederic Koehler, Ankur Moitra, and Morris Yau. Classiﬁcation under misspeciﬁ-
cation: Halfspaces, generalized linear models, and connections to evolvability. arXiv preprint
arXiv:2006.04787, 2020.
[127] Yuansi Chen.
An almost constant lower bound of the isoperimetric coeﬃcient in the kls
conjecture. CoRR, abs/2011.13661, 2021.
[128] Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, and Bin Yu. Fast mixing of metropolized
hamiltonian monte carlo: Beneﬁts of multi-step gradients. CoRR, abs/1905.12247, 2019.
[129] Zongchen Chen and Santosh S. Vempala. Optimal convergence rate of hamiltonian monte carlo
for strongly logconcave distributions. In Approximation, Randomization, and Combinatorial
Optimization. Algorithms and Techniques, APPROX/RANDOM 2019, September 20-22, 2019,
Massachusetts Institute of Technology, Cambridge, MA, USA, pages 64:1-64:12, 2019.
[130] Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett, and Michael I. Jordan. Underdamped
langevin MCMC: A non-asymptotic analysis. In Conference On Learning Theory, COLT 2018,
Stockholm, Sweden, 6-9 July 2018, pages 300-323, 2018.
[131] Yu Cheng, Ilias Diakonikolas, and Rong Ge. High-dimensional robust mean estimation in
nearly-linear time. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages 2755-2771,
2019.
[132] Yu Cheng, Ilias Diakonikolas, Rong Ge, and David Woodruﬀ. Faster algorithms for high-
dimensional robust covariance estimation. arXiv preprint arXiv:1906.04661, 2019.
[133] Yu Cheng and Rong Ge. Non-convex matrix completion against a semi-random adversary. In
Conference On Learning Theory, pages 1362-1394. PMLR, 2018.

BIBLIOGRAPHY
868
[134] Yeshwanth Cherapanamjeri, Efe Aras, Nilesh Tripuraneni, Michael I. Jordan, Nicolas Flam-
marion, and Peter L. Bartlett. Optimal robust linear regression in nearly linear time. CoRR,
abs/2007.08137, 2020.
[135] Yeshwanth Cherapanamjeri, Nicolas Flammarion, and Peter L. Bartlett. Fast mean estimation
with sub-gaussian rates. In Conference on Learning Theory, COLT 2019, 25-28 June 2019,
Phoenix, AZ, USA, pages 786-806, 2019.
[136] Yeshwanth Cherapanamjeri, Sidhanth Mohanty, and Morris Yau. List decodable mean esti-
mation in nearly linear time. In 61st IEEE Annual Symposium on Foundations of Computer
Science, FOCS 2020, Durham, NC, USA, November 16-19, 2020, pages 141-148, 2020.
[137] Sinho Chewi, Chen Lu, Kwangjun Ahn, Xiang Cheng, Thibaut Le Gouic, and Philippe Rigol-
let. Optimal dimension dependence of the metropolis-adjusted langevin algorithm. CoRR,
abs/2012.12810, 2020.
[138] Rajesh Chitnis, Graham Cormode, Hossein Esfandiari, MohammadTaghi Hajiaghayi, Andrew
McGregor, Morteza Monemizadeh, and Sofya Vorotnikova. Kernelization via sampling with
applications to ﬁnding matchings and related problems in dynamic graph streams. In Proceed-
ings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA
2016, Arlington, VA, USA, January 10-12, 2016, pages 1326-1344, 2016.
[139] Paul Christiano, Jonathan A. Kelner, Aleksander Madry, Daniel A. Spielman, and Shang-
Hua Teng. Electrical ﬂows, laplacian systems, and faster approximation of maximum ﬂow
in undirected graphs. In Proceedings of the 43rd ACM Symposium on Theory of Computing,
STOC 2011, San Jose, CA, USA, 6-8 June 2011, pages 273-282, 2011.
[140] Kenneth L. Clarkson, Elad Hazan, and David P. Woodruﬀ. Sublinear optimization for machine
learning. In 51th Annual IEEE Symposium on Foundations of Computer Science, pages 449-
457, 2010.
[141] Kenneth L Clarkson and David P Woodruﬀ. Low rank approximation and regression in input
sparsity time. In Proceedings of the 45th annual ACM symposium on Symposium on theory of
computing, pages 81-90. ACM, 2013.
[142] Michael Cohen, Jelena Diakonikolas, and Lorenzo Orecchia.
On acceleration with noise-
corrupted gradients. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018, pages 1018-1027, 2018.
[143] Michael B. Cohen, Yin Tat Lee, Gary L. Miller, Jakub Pachocki, and Aaron Sidford. Geometric
median in nearly linear time. In Daniel Wichs and Yishay Mansour, editors, Proceedings of the
48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge,
MA, USA, June 18-21, 2016, pages 9-21. ACM, 2016.

BIBLIOGRAPHY
869
[144] Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix
multiplication time. J. ACM, 68(1):3:1-3:39, 2021.
[145] Michael B. Cohen, Aleksander Madry, Dimitris Tsipras, and Adrian Vladu. Matrix scaling
and balancing via box constrained newton's method and interior point methods. In 58th IEEE
Annual Symposium on Foundations of Computer Science, FOCS 2017, Berkeley, CA, USA,
October 15-17, 2017, pages 902-913, 2017.
[146] Michael B Cohen, Jelani Nelson, and David P Woodruﬀ. Optimal approximate matrix product
in terms of stable rank.
In 43rd International Colloquium on Automata, Languages, and
Programming (ICALP 2016). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.
[147] Michael B. Cohen, Aaron Sidford, and Kevin Tian. Relative lipschitzness in extragradient
methods and a direct recipe for acceleration. In 12th Conference on Innovations in Theoretical
Computer Science (ITCS), pages 62:1-62:18, 2021.
[148] Patrick L Combettes and Val´erie R Wajs.
Signal recovery by proximal forward-backward
splitting. Multiscale Modeling & Simulation, 4(4):1168-1200, 2005.
[149] Ben Cousins and Santosh S. Vempala. Gaussian cooling and o*(n3) algorithms for volume
and gaussian volume. SIAM J. Comput., 47(3):1237-1273, 2018.
[150] Benjamin Cousins and Santosh Vempala. Bypassing kls: Gaussian cooling and an oˆ*(n3)
volume algorithm. In Proceedings of the forty-seventh annual ACM symposium on Theory of
computing, pages 539-548, 2015.
[151] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances
in neural information processing systems, pages 2292-2300, 2013.
[152] Arnak S. Dalalyan.
Further and stronger analogy between sampling and optimization:
Langevin monte carlo and gradient descent. In Proceedings of the 30th Conference on Learning
Theory, COLT 2017, Amsterdam, The Netherlands, 7-10 July 2017, pages 678-689, 2017.
[153] Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-
concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
3(79):651-676, 2017.
[154] Arnak S Dalalyan, Mohamed Hebiri, and Johannes Lederer. On the prediction performance
of the lasso. Bernoulli, 23(1):552-581, 2017.
[155] Arnak S Dalalyan and Avetik Karagulyan. User-friendly guarantees for the langevin monte
carlo with inaccurate gradient. Stochastic Processes and their Applications, 129(12):5278-5311,
2019.

BIBLIOGRAPHY
870
[156] Arnak S. Dalalyan and Lionel Riou-Durand. On sampling from a log-concave density using
kinetic langevin diﬀusions. CoRR, abs/1807.09382, 2018.
[157] G. B. Dantzig. Linear Programming and Extensions. Princeton University Press, Princeton,
NJ, 1953.
[158] Jacques Dark and Christian Konrad. Optimal lower bounds for matching and vertex cover
in dynamic graph streams. In 35th Computational Complexity Conference, CCC 2020, July
28-31, 2020, Saarbr¨ucken, Germany (Virtual Conference), pages 30:1-30:14, 2020.
[159] Sanjoy Dasgupta. Learning mixtures of gaussians. In 40th Annual Symposium on Foundations
of Computer Science (Cat. No. 99CB37039), pages 634-644. IEEE, 1999.
[160] Sanjoy Dasgupta and Anupam Gupta.
An elementary proof of a theorem of johnson and
lindenstrauss. Random Struct. Algorithms, 22(1):60-65, 2003.
[161] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs
with optimism. In International Conference on Learning Representations, 2019.
[162] Alexandre d'Aspremont. Smooth optimization with approximate gradient. SIAM J. Optim.,
19(3):1171-1183, 2008.
[163] Alexandre d'Aspremont. Subsampling algorithms for semideﬁnite programming. Stochastic
Systems, 1(2):209-436, 2011.
[164] Ingrid Daubechies, Michel Defrise, and Christine De Mol.
An iterative thresholding algo-
rithm for linear inverse problems with a sparsity constraint. Communications on Pure and
Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences,
57(11):1413-1457, 2004.
[165] Mark A Davenport, Marco F Duarte, Yonina C Eldar, and Gitta Kutyniok. Introduction to
compressed sensing., 2012.
[166] Mark A Davenport, Deanna Needell, and Michael B Wakin. Signal space cosamp for sparse
recovery with redundant dictionaries. IEEE Transactions on Information Theory, 59(10):6820-
6829, 2013.
[167] Aaron Defazio.
A simple practical accelerated method for ﬁnite sums.
In Daniel D. Lee,
Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Ad-
vances in Neural Information Processing Systems 29 (NeurIPS), pages 676-684, 2016.
[168] Aaron Defazio, Francis R. Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing Systems 27 (NeurIPS), pages 1646-1654, 2014.

BIBLIOGRAPHY
871
[169] Erik Demaine. Lecture 19. Class notes, MIT 6.851: Advanced Data Structures, scribed by
Justin Holmgren, Jing Jian, Maksim Stepaneko, Mashhood Ishaque, 2012.
[170] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition, pages 248-255. Ieee, 2009.
[171] Jules Despersin and Guillaume Lecu´e. Robust subgaussian estimation of a mean vector in
nearly linear time. CoRR, abs/1906.03058, 2019.
[172] Olivier Devolder, Fran¸cois Glineur, and Yurii E. Nesterov. First-order methods of smooth
convex optimization with inexact oracle. Math. Program., 146(1-2):37-75, 2014.
[173] Ilias Diakonikolas, Themis Gouleakis, and Christos Tzamos. Distribution-independent pac
learning of halfspaces with massart noise. arXiv preprint arXiv:1906.10075, 2019.
[174] Ilias Diakonikolas, Russell Impagliazzo, Daniel Kane, Rex Lei, Jessica Sorrell, and Christos
Tzamos. Boosting in the presence of massart noise. arXiv preprint arXiv:2106.07779, 2021.
[175] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stew-
art. Robust estimators in high-dimensions without the computational intractability. SIAM J.
Comput., 48(2):742-864, 2019.
[176] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair
Stewart. Sever: A robust meta-algorithm for stochastic optimization. In Proceedings of the
36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, pages 1596-1606, 2019.
[177] Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair
Stewart.
Being robust (in high dimensions) can be practical.
In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pages 999-1008. JMLR. org, 2017.
[178] Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair
Stewart. Robustly learning a gaussian: Getting optimal error, eﬃciently. In Proceedings of
the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2683-2702.
SIAM, 2018.
[179] Ilias Diakonikolas, Daniel Kane, and Daniel Kongsgaard. List-decodable mean estimation via
iterative multi-ﬁltering. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020.

BIBLIOGRAPHY
872
[180] Ilias Diakonikolas, Daniel Kane, Daniel Kongsgaard, Jerry Li, and Kevin Tian. List-decodable
mean estimation in nearly-pca time. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N.
Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Infor-
mation Processing Systems 34: Annual Conference on Neural Information Processing Systems
2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 10195-10208, 2021.
[181] Ilias Diakonikolas, Daniel Kane, Daniel Kongsgaard, Jerry Li, and Kevin Tian. Clustering
mixture models in almost-linear time via list-decodable mean estimation. In Proceedings of
the 54th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2022, 2022.
[182] Ilias Diakonikolas and Daniel M Kane. Recent advances in algorithmic high-dimensional robust
statistics. arXiv preprint arXiv:1911.05911, 2019.
[183] Ilias Diakonikolas and Daniel M Kane. Hardness of learning halfspaces with massart noise.
arXiv preprint arXiv:2012.09720, 2020.
[184] Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos Zari-
ﬁs.
Threshold phenomena in learning halfspaces with massart noise.
arXiv preprint
arXiv:2108.08767, 2021.
[185] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower bounds for
robust estimation of high-dimensional gaussians and gaussian mixtures. In 2017 IEEE 58th
Annual Symposium on Foundations of Computer Science (FOCS), pages 73-84. IEEE, 2017.
[186] Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. List-decodable robust mean esti-
mation and learning mixtures of spherical gaussians. In Proceedings of the 50th Annual ACM
SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June
25-29, 2018, pages 1047-1060, 2018.
[187] Ilias Diakonikolas, Daniel M Kane, and Christos Tzamos. Forster decomposition and learning
halfspaces with noise. arXiv preprint arXiv:2107.05582, 2021.
[188] Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Eﬃcient algorithms and lower bounds
for robust linear regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages
2745-2754, 2019.
[189] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zariﬁs. Learning halfspaces
with massart noise under structured distributions. In Conference on Learning Theory, pages
1486-1513. PMLR, 2020.
[190] Ilias Diakonikolas, Jongho Park, and Christos Tzamos. Relu regression with massart noise.
arXiv preprint arXiv:2109.04623, 2021.

BIBLIOGRAPHY
873
[191] Jelena Diakonikolas, Maryam Fazel, and Lorenzo Orecchia. Width-independence beyond linear
objectives: Distributed fair packing and covering algorithms. CoRR, abs/1808.02517, 2018.
[192] Jelena Diakonikolas and Lorenzo Orecchia. Accelerated extra-gradient descent: A novel ac-
celerated ﬁrst-order method. In 9th Innovations in Theoretical Computer Science Conference,
ITCS 2018, January 11-14, 2018, Cambridge, MA, USA, pages 23:1-23:19, 2018.
[193] Jelena Diakonikolas and Lorenzo Orecchia. The approximate duality gap technique: A uniﬁed
theory of ﬁrst-order methods. SIAM Journal on Optimization, 29(1):660-689, 2019.
[194] Yihe Dong, Samuel Hopkins, and Jerry Li. Quantum entropy scoring for fast robust mean
estimation and improved outlier detection.
In Advances in Neural Information Processing
Systems, pages 6065-6075, 2019.
[195] David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-
1306, 2006.
[196] David L Donoho and Philip B Stark. Uncertainty principles and signal recovery. SIAM Journal
on Applied Mathematics, 49(3):906-931, 1989.
[197] Radu-Alexandru Dragomir, Adrien Taylor, Alexandre d'Aspremont, and J´erˆome Bolte.
Optimal complexity and certiﬁcation of bregman ﬁrst-order methods.
arXiv e-prints,
abs/1911.08510, 2019.
[198] Yoel Drori, Shoham Sabach, and Marc Teboulle. A simple algorithm for a class of nonsmooth
convex-concave saddle-point problems. Operations Research Letters, 43(2):209-214, 2015.
[199] Vladimir Druskin and Leonid Knizhnerman. Error bounds in the simple lanczos procedure for
computing functions of symmetric matrices and eigenvalues. U.S.S.R. Computational Mathe-
matics and Mathematical Physics, 31(7):970-983, 1991.
[200] Vladimir Druskin and Leonid Knizhnerman. Krylov subspace approximation of eigenpairs and
matrix functions in exact and computer arithmetic. Numerical Linear Algebra with Applica-
tions, 2(3):205-217, 1995.
[201] Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance re-
duction methods for policy evaluation. In 34th International Conference on Machine Learning
(ICML), pages 1049-1058, 2017.
[202] Kumar Avinava Dubey, Sashank J Reddi, Sinead A Williamson, Barnabas Poczos, Alexander J
Smola, and Eric P Xing. Variance reduction in stochastic gradient langevin dynamics. In
Advances in neural information processing systems, pages 1154-1162, 2016.

BIBLIOGRAPHY
874
[203] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Eﬃcient projections
onto the l 1-ball for learning in high dimensions.
In Proceedings of the 25th international
conference on Machine learning, pages 272-279. ACM, 2008.
[204] David Durfee, Yu Gao, Gramoz Goranci, and Richard Peng. Fully dynamic spectral vertex
sparsiﬁers and applications. In Moses Charikar and Edith Cohen, editors, Proceedings of the
51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ,
USA, June 23-26, 2019, pages 914-925. ACM, 2019.
[205] Alain Durmus, Szymon Majewski, and Blazej Miasojedow. Analysis of langevin monte carlo
via convex optimization. J. Mach. Learn. Res., 20:73:1-73:46, 2019.
[206] Alain Durmus and ´Eric Moulines. High-dimensional bayesian inference via the unadjusted
langevin algorithm. Bernoulli, 25(4A):2854-2882, 2019.
[207] Alain Durmus, Umut Simsekli, Eric Moulines, Roland Badeau, and Ga¨el Richard. Stochastic
gradient richardson-romberg markov chain monte carlo. In Advances in Neural Information
Processing Systems, pages 2047-2055, 2016.
[208] Pavel Dvurechensky, Alexander Gasnikov, and Alexey Kroshnin. Computational optimal trans-
port: Complexity by accelerated gradient descent is better than by sinkhorn's algorithm. In
Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stock-
holmsm¨assan, Stockholm, Sweden, July 10-15, 2018, pages 1366-1375, 2018.
[209] Pavel E. Dvurechensky and Alexander V. Gasnikov. Stochastic intermediate gradient method
for convex problems with stochastic inexact oracle. J. Optim. Theory Appl., 171(1):121-145,
2016.
[210] Raaz Dwivedi, Yuansi Chen, Martin J. Wainwright, and Bin Yu.
Log-concave sampling:
Metropolis-hastings algorithms are fast!
In Conference On Learning Theory, COLT 2018,
Stockholm, Sweden, 6-9 July 2018, pages 793-797, 2018.
[211] Cynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze gauss: optimal
bounds for privacy-preserving principal component analysis.
In David B. Shmoys, editor,
Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03,
2014, pages 11-20. ACM, 2014.
[212] Martin E. Dyer, Alan M. Frieze, and Ravi Kannan. A random polynomial time algorithm for
approximating the volume of convex bodies. J. ACM, 38(1):1-17, 1991.
[213] Sebastian Eggert, Lasse Kliemann, Peter Munstermann, and Anand Srivastav.
Bipartite
matching in the semi-streaming model. Algorithmica, 63(1-2):490-508, 2012.

BIBLIOGRAPHY
875
[214] Yonina C Eldar and Gitta Kutyniok. Compressed sensing: theory and applications. Cambridge
university press, 2012.
[215] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimiza-
tion using the wasserstein metric: performance guarantees and tractable reformulations. Math.
Program., 171(1-2):115-166, 2018.
[216] Hossein Esfandiari, Mohammad Taghi Hajiaghayi, Vahid Liaghat, Morteza Monemizadeh,
and Krzysztof Onak. Streaming algorithms for estimating the matching size in planar graphs
and beyond. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 1217-1233, 2015.
[217] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. SPIDER: near-optimal non-
convex optimization via stochastic path-integrated diﬀerential estimator. In Advances in Neu-
ral Information Processing Systems, 2018.
[218] Alireza Farhadi, Mohammad Taghi Hajiaghayi, Tung Mai, Anup Rao, and Ryan A. Rossi.
Approximate maximum matching in random streams. In Proceedings of the 2020 ACM-SIAM
Symposium on Discrete Algorithms, SODA 2020, Salt Lake City, UT, USA, January 5-8,
2020, pages 1773-1785, 2020.
[219] Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. Journal of Computer
and System Sciences, 63(4):639-671, 2001.
[220] Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semi-
random graph. Random Structures & Algorithms, 16(2):195-208, 2000.
[221] Joan Feigenbaum, Sampath Kannan, Andrew McGregor, Siddharth Suri, and Jian Zhang. On
graph problems in a semi-streaming model. Theor. Comput. Sci., 348(2-3):207-216, 2005.
[222] Olivier Fercoq and Peter Richt´arik. Accelerated, parallel, and proximal coordinate descent.
SIAM Journal on Optimization, 25(4):1997-2023, 2015.
[223] M´ario AT Figueiredo and Robert D Nowak. An em algorithm for wavelet-based image restora-
tion. IEEE Transactions on Image Processing, 12(8):906-916, 2003.
[224] Manuela Fischer, Slobodan Mitrovic, and Jara Uitto. Deterministic (1+ϵ)-approximate max-
imum matching with poly(1/ϵ) passes in the semi-streaming model. CoRR, abs/2106.04179,
2021.
[225] G. E. Forsythe and E. G. Straus. On best conditioned matrices. Proceedings of the American
Mathematical Society, 6(3):340-345, 1955.

BIBLIOGRAPHY
876
[226] Simon Foucart.
Hard thresholding pursuit: an algorithm for compressive sensing.
SIAM
Journal on Numerical Analysis, 49(6):2543-2563, 2011.
[227] Roy Frostig, Rong Ge, Sham M. Kakade, and Aaron Sidford. Un-regularizing: approximate
proximal point and faster stochastic algorithms for empirical risk minimization.
In 32nd
International Conference on Machine Learning (ICML), pages 2540-2548, 2015.
[228] Buddhima Gamlath, Sagar Kale, Slobodan Mitrovic, and Ola Svensson. Weighted matchings
via unweighted augmentations. In Proceedings of the 2019 ACM Symposium on Principles of
Distributed Computing, PODC 2019, Toronto, ON, Canada, July 29 - August 2, 2019, pages
491-500, 2019.
[229] Xuefeng Gao, Mert G¨urb¨uzbalaban, and Lingjiong Zhu.
Global convergence of stochastic
gradient hamiltonian monte carlo for non-convex stochastic optimization: Non-asymptotic
performance bounds and momentum-based acceleration.
arXiv preprint arXiv:1809.04618,
2018.
[230] Yu Gao, Yang P. Liu, and Richard Peng. Fully dynamic electrical ﬂows: Sparse maxﬂow faster
than goldberg-rao. In 62nd IEEE Annual Symposium on Foundations of Computer Science,
FOCS 2021, Denver, CO, USA, February 7-10, 2022, pages 516-527. IEEE, 2021.
[231] Dan Garber and Elad Hazan. Sublinear time algorithms for approximate semideﬁnite pro-
gramming. Math. Program., 158(1-2):329-361, 2016.
[232] Dan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In Proceedings of
the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July
2015, pages 560-568, 2015.
[233] Ankit Garg, Leonid Gurvits, Rafael Mendes de Oliveira, and Avi Wigderson. Operator scaling:
Theory and applications. Found. Comput. Math., 20(2):223-290, 2020.
[234] Ankit Garg and Rafael Oliveira.
Recent progress on scaling algorithms and applications.
Bulletin of EATCS, 2(125), 2018.
[235] Rong Ge, Holden Lee, and Jianfeng Lu. Estimating normalizing constants for log-concave
distributions: algorithms and lower bounds. In Proccedings of the 52nd Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2020, Chicago, IL, USA, June 22-26, 2020, pages
579-586, 2020.
[236] Aude Genevay, Marco Cuturi, Gabriel Peyr´e, and Francis R. Bach. Stochastic optimization
for large-scale optimal transport. In Advances in Neural Information Processing Systems 29:
Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain, pages 3432-3440, 2016.

BIBLIOGRAPHY
877
[237] Gauthier Gidel, Tony Jebara, and Simon Lacoste-Julien. Frank-Wolfe algorithms for saddle
point problems. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence
and Statistics, 2017.
[238] Andr´as Gily´en, Seth Lloyd, and Ewin Tang. Quantum-inspired low-rank stochastic regression
with logarithmic dependence on the dimension. arXiv preprint arXiv:1811.04909, 2018.
[239] Wayne Goddard. Part 2: Greedy algorithms, dynamic programming, graph algorithms. In-
troduction to Algorithms, 2004.
[240] Ashish Goel, Michael Kapralov, and Sanjeev Khanna. On the communication and streaming
complexity of maximum bipartite matching. In Proceedings of the Twenty-Third Annual ACM-
SIAM Symposium on Discrete Algorithms, SODA 2012, Kyoto, Japan, January 17-19, 2012,
pages 468-485, 2012.
[241] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for max-
imum cut and satisﬁability problems using semideﬁnite programming. J. ACM, 42(6):1115-
1145, 1995.
[242] Andrew V. Goldberg and Satish Rao.
Beyond the ﬂow decomposition barrier.
J. ACM,
45(5):783-797, 1998.
[243] Andrew V. Goldberg and Robert Endre Tarjan. Finding minimum-cost circulations by can-
celing negative cycles. J. ACM, 36(4):873-886, 1989.
[244] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. Commun.
ACM, 63(11):139-144, 2020.
[245] Gramoz Goranci. Dynamic graph algorithms and graph sparsiﬁcation: New techniques and
connections. CoRR, abs/1909.06413, 2019.
[246] Gramoz Goranci, Monika Henzinger, and Pan Peng. The power of vertex sparsiﬁers in dy-
namic graph algorithms. In Kirk Pruhs and Christian Sohler, editors, 25th Annual European
Symposium on Algorithms, ESA 2017, September 4-6, 2017, Vienna, Austria, volume 87 of
LIPIcs, pages 45:1-45:14. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2017.
[247] Gramoz Goranci, Monika Henzinger, and Pan Peng. Dynamic Eﬀective Resistances and Ap-
proximate Schur Complement on Separable Graphs. In Yossi Azar, Hannah Bast, and Grzegorz
Herman, editors, 26th Annual European Symposium on Algorithms (ESA 2018), volume 112
of Leibniz International Proceedings in Informatics (LIPIcs), pages 40:1-40:15, Dagstuhl, Ger-
many, 2018. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.

BIBLIOGRAPHY
878
[248] Gramoz Goranci, Harald R¨acke, Thatchaphol Saranurak, and Zihan Tan. The expander hier-
archy and its applications to dynamic graph algorithms. In D´aniel Marx, editor, Proceedings
of the 2021 ACM-SIAM Symposium on Discrete Algorithms, SODA 2021, Virtual Conference,
January 10 - 13, 2021, pages 2212-2228. SIAM, 2021.
[249] Rachel A. Gordon. Regression Analysis for the Social Sciences. Routledge, 2010.
[250] Fabrizio Grandoni, Stefano Leonardi, Piotr Sankowski, Chris Schwiegelshohn, and Shay
Solomon. (1+ϵ)-approximate incremental matching in constant deterministic amortized time.
In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA
2019, San Diego, California, USA, January 6-9, 2019, pages 1886-1898, 2019.
[251] A. Greenbaum and G. H. Rodrigue. Optimal preconditioners of a given sparsity pattern. BIT
Numerical Mathematics, 29(4):610-634, 1989.
[252] William H. Greene. Econometric analysis. Prentice Hall, 1990.
[253] Michael D. Grigoriadis and Leonid G. Khachiyan. A sublinear-time randomized approximation
algorithm for matrix games. Operation Research Letters, 18(2):53-58, 1995.
[254] Marcus J. Grote and Thomas Huckle. Parallel preconditioning with sparse approximate in-
verses. SIAM Journal on Scientiﬁc Computing, 18(3):838-853, 1997.
[255] Martin Gr¨otschel, L´aszl´o Lov´asz, and Alexander Schrijver. Geometric Algorithms and Com-
binatorial Optimization, volume 2 of Algorithms and Combinatorics. Springer, 1988.
[256] Ming Gu and Stanley C. Eisenstat. A divide-and-conquer algorithm for the symmetric trida-
gional eigenproblem. SIAM Journal on Matrix Analysis and Applications, 16(1):172-191, 1995.
[257] Osman G¨uler. New proximal point algorithms for convex minimization. SIAM J. Optim.,
2(4):649-664, 1992.
[258] Manoj Gupta. Maintaining approximate maximum matching in an incremental bipartite graph
in polylogarithmic update time. In 34th International Conference on Foundation of Software
Technology and Theoretical Computer Science, FSTTCS 2014, December 15-17, 2014, New
Delhi, India, pages 227-239, 2014.
[259] Manoj Gupta and Richard Peng.
Fully dynamic (1 + ϵ)-approximate matchings.
In 54th
Annual IEEE Symposium on Foundations of Computer Science, FOCS 2013, 26-29 October,
2013, Berkeley, CA, USA, pages 548-557, 2013.
[260] Neha Gupta and Aaron Sidford. Exploiting numerical sparsity for eﬃcient learning: faster
eigenvector computation and regression. In Advances in Neural Information Processing Sys-
tems, pages 5269-5278, 2018.

BIBLIOGRAPHY
879
[261] Venkatesan Guruswami and Krzysztof Onak. Superlinear lower bounds for multipass graph
processing. In Proceedings of the 28th Conference on Computational Complexity, CCC 2013,
K.lo Alto, California, USA, 5-7 June, 2013, pages 287-298, 2013.
[262] Maximilian Probst Gutenberg and Christian Wulﬀ-Nilsen. Fully-dynamic all-pairs shortest
paths: Improved worst-case time and space bounds. In Shuchi Chawla, editor, Proceedings of
the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA 2020, Salt Lake City, UT,
USA, January 5-8, 2020, pages 2562-2574. SIAM, 2020.
[263] Frank R. Hampel, Elvezio M. Ronchetti, Peter J. Rousseeuw, and Werner A. Stahel. Ro-
bust Statistics: the Approach based on Inﬂuence Functions. Wiley Series in Probability and
Mathematical Statistics, 1986.
[264] Kathrin Hanauer, Monika Henzinger, and Christian Schulz. Recent advances in fully dynamic
graph algorithms. CoRR, abs/2102.11169, 2021.
[265] Filip Hanzely, Peter Richtarik, and Lin Xiao. Accelerated bregman proximal gradient methods
for relatively smooth convex optimization. arXiv e-prints, abs/1808.03045, 2018.
[266] Moritz Hardt.
The zen of gradient descent.
http://blog.mrtz.org/2013/09/07/the-zen-of-
gradient-descent.html, 2013. Accessed: 2018-07-30.
[267] Gilles Harg´e. A convex/log-concave correlation inequality for gaussian measure and an appli-
cation to abstract wiener spaces. Probability theory and related ﬁelds, 130(3):415-440, 2004.
[268] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Opti-
mization, 2(3-4):157-325, 2016.
[269] Monika Henzinger, Sebastian Krinninger, and Danupon Nanongkai. A deterministic almost-
tight distributed algorithm for approximating single-source shortest paths. In Daniel Wichs
and Yishay Mansour, editors, Proceedings of the 48th Annual ACM SIGACT Symposium on
Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 489-498.
ACM, 2016.
[270] Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol Saranurak.
Unifying and strengthening hardness for dynamic problems via the online matrix-vector mul-
tiplication conjecture. In Proceedings of the Forty-Seventh Annual ACM on Symposium on
Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pages 21-30,
2015.
[271] J. Hiriart-Urruty and C. Lemar´echal.
Convex Analysis and Minimization Algorithms I.
Springer, New York, 1993.

BIBLIOGRAPHY
880
[272] Marlis Hochbruck and Alexander Ostermann. Exponential integrators. Acta Numerica, 19:209-
286, 2010.
[273] Matthew D. Hoﬀman and Andrew Gelman. The no-u-turn sampler: adaptively setting path
lengths in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):1593-1623, 2014.
[274] John E. Hopcroft and Richard M. Karp.
An n5/2 algorithm for maximum matchings in
bipartite graphs. SIAM J. Comput., 2(4):225-231, 1973.
[275] Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In
Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages
1021-1034, 2018.
[276] Ya-Ping Hsieh, Ali Kavis, Paul Rolland, and Volkan Cevher. Mirrored langevin dynamics.
In Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr´eal, Canada,
pages 2883-2892, 2018.
[277] Yu-Guan Hsieh, Franck Iutzeler, J´erˆome Malick, and Panayotis Mertikopoulos. On the con-
vergence of single-call stochastic extra-gradient methods. In Advances in Neural Information
Processing Systems 32 (NeurIPS), pages 6936-6946, 2019.
[278] Peter J Huber.
Robust estimation of a location parameter.
The Annals of Mathematical
Statistics, pages 73-101, 1964.
[279] Peter J Huber. Robust statistics, volume 523. John Wiley & Sons, 2004.
[280] Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods
for high-dimensional m-estimation. In NIPS, 2014.
[281] Rahul Jain, Zhengfeng Ji, Sarvagya Upadhyay, and John Watrous. QIP = PSPACE. J. ACM,
58(6):30:1-30:27, 2011.
[282] Rahul Jain and Penghui Yao. A parallel approximation algorithm for positive semideﬁnite
programming. In IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS
2011, Palm Springs, CA, USA, October 22-25, 2011, pages 463-471, 2011.
[283] Rahul Jain and Penghui Yao.
A parallel approximation algorithm for mixed packing and
covering semideﬁnite programs. CoRR, abs/1201.6090, 2012.
[284] Arun Jambulapati, Yujia Jin, Aaron Sidford, and Kevin Tian. Regularized box-simplex games
and dynamic decremental bipartite matching. In 49th International Colloquium on Automata,
Languages, and Programming (ICALP 2022), 2022.

BIBLIOGRAPHY
881
[285] Arun Jambulapati, Yin Tat Lee, Jerry Li, Swati Padmanabhan, and Kevin Tian. Positive
semideﬁnite programming: Mixed, parallel, and width-independent.
In Proceedings of the
52nd Annual ACM SIGACT Symposium on Theory of Computing, 2020.
[286] Arun Jambulapati, Yin Tat Lee, Jerry Li, Swati Padmanabhan, and Kevin Tian. Positive
semideﬁnite programming: Mixed, parallel, and width-independent. CoRR, abs/2002.04830v3,
2021.
[287] Arun Jambulapati, Jerry Li, Christopher Musco, Aaron Sidford, and Kevin Tian. Fast and
near-optimal diagonal preconditioning. CoRR, abs/2008.01722, 2021.
[288] Arun Jambulapati, Jerry Li, Tselil Schramm, and Kevin Tian. Robust regression revisited:
Acceleration and improved estimation rates. In Marc'Aurelio Ranzato, Alina Beygelzimer,
Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural
Information Processing Systems 34: Annual Conference on Neural Information Processing
Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 4475-4488, 2021.
[289] Arun Jambulapati, Jerry Li, and Kevin Tian. Robust sub-gaussian principal component anal-
ysis and width-independent schatten packing. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020.
[290] Arun Jambulapati, Yang P. Liu, and Aaron Sidford. Parallel reachability in almost linear
work and square root depth. In David Zuckerman, editor, 60th IEEE Annual Symposium on
Foundations of Computer Science, FOCS 2019, Baltimore, Maryland, USA, November 9-12,
2019, pages 1664-1686. IEEE Computer Society, 2019.
[291] Arun Jambulapati, Kirankumar Shiragur, and Aaron Sidford.
Eﬃcient structured matrix
recovery and nearly-linear time algorithms for solving inverse symmetric m-matrices. CoRR,
abs/1812.06295, 2018.
[292] Arun Jambulapati and Aaron Sidford. Ultrasparse ultrasparsiﬁers and faster laplacian system
solvers. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms, SODA
2021, Virtual Conference, January 10 - 13, 2021, pages 540-559, 2021.
[293] Arun Jambulapati, Aaron Sidford, and Kevin Tian. A direct tilde{O}(1/epsilon) iteration
parallel algorithm for optimal transport. In Advances in Neural Information Processing Sys-
tems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
8-14 December 2019, Vancouver, BC, Canada, pages 11355-11366, 2019.
[294] Mark Jerrum.
Large cliques elude the metropolis process.
Random Struct. Algorithms,
3(4):347-360, 1992.

BIBLIOGRAPHY
882
[295] He Jia, Aditi Laddha, Yin Tat Lee, and Santosh S. Vempala. Reducing isotropy and volume
to KLS: an o(n3ψ2) volume algorithm. CoRR, abs/2008.02146, 2020.
[296] Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting plane
method for convex optimization, convex-concave games, and its applications. In Konstantin
Makarychev, Yury Makarychev, Madhur Tulsiani, Gautam Kamath, and Julia Chuzhoy, ed-
itors, Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2020, Chicago, IL, USA, June 22-26, 2020, pages 944-953. ACM, 2020.
[297] Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Minmax optimization: Stable limit points
of gradient descent ascent are locally optimal. arXiv preprint arXiv:1902.00618, 2019.
[298] Yujia Jin and Aaron Sidford. Principal component projection and regression in nearly linear
time through asymmetric SVRG. In Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada, pages 3863-3873, 2019.
[299] Yujia Jin, Aaron Sidford, and Kevin Tian. Sharper rates for separable minimax and ﬁnite sum
optimization via primal-dual extragradient methods. CoRR, abs/2202.04640, 2022.
[300] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-
ance reduction. In Advances in Neural Information Processing Systems 26 (NeurIPS), pages
315-323, 2013.
[301] Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker-
planck equation. SIAM Journal on Mathematical Analysis, 29(1):1-17, 1998.
[302] Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with
stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17-58, 2011.
[303] Sham M. Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Applications of strong convexity-
strong smoothness duality to learning with matrices. arXiv e-prints, abs/0910.0610, 2009.
[304] Adam Tauman Kalai and Santosh S. Vempala. Eﬃcient algorithms for online decision prob-
lems. J. Comput. Syst. Sci., 71(3):291-307, 2005.
[305] Sagar Kale and Sumedh Tirodkar. Maximum matching in two, three, and a few more passes
over graph streams.
In Approximation, Randomization, and Combinatorial Optimization.
Algorithms and Techniques, APPROX/RANDOM 2017, August 16-18, 2017, Berkeley, CA,
USA, pages 15:1-15:21, 2017.
[306] Donggu Kang and James Payor. Flow rounding. CoRR, abs/1507.08139, 2015.

BIBLIOGRAPHY
883
[307] Ravindran Kannan, Hadi Salmasian, and Santosh S. Vempala. The spectral method for general
mixture models. SIAM J. Comput., 38(3):1141-1156, 2008.
[308] Michael Kapralov. Better bounds for matchings in the streaming model. In Proceedings of
the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2013, New
Orleans, Louisiana, USA, January 6-8, 2013, pages 1679-1697, 2013.
[309] Michael Kapralov. Space lower bounds for approximating maximum matching in the edge
arrival model. In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms,
SODA 2021, 2021.
[310] Michael Kapralov, Sanjeev Khanna, and Madhu Sudan. Approximating matching size from
random streams. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Dis-
crete Algorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 734-751,
2014.
[311] Michael Kapralov, Yin Tat Lee, Cameron Musco, Christopher Musco, and Aaron Sidford.
Single pass spectral sparsiﬁcation in dynamic streams. In 55th IEEE Annual Symposium on
Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014,
pages 561-570, 2014.
[312] Michael Kapralov, Slobodan Mitrovic, Ashkan Norouzi-Fard, and Jakab Tardos. Space eﬃcient
approximation to maximum matching size from uniform edge samples. In Proceedings of the
2020 ACM-SIAM Symposium on Discrete Algorithms, SODA 2020, Salt Lake City, UT, USA,
January 5-8, 2020, pages 1753-1772, 2020.
[313] David R. Karger. Better random sampling algorithms for ﬂows in undirected graphs. In Pro-
ceedings of the Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, 25-27 January
1998, San Francisco, California., pages 490-499, 1998.
[314] David R. Karger and Matthew S. Levine. Random sampling in residual graphs. In Proceed-
ings on 34th Annual ACM Symposium on Theory of Computing, May 19-21, 2002, Montr´eal,
Qu´ebec, Canada, pages 63-66, 2002.
[315] Sushrut Karmalkar, Adam R. Klivans, and Pravesh Kothari. List-decodable linear regression.
In Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, pages 7423-7432, 2019.
[316] Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceed-
ings of the sixteenth annual ACM symposium on Theory of computing, pages 302-311. ACM,
1984.

BIBLIOGRAPHY
884
[317] Jonathan Kelner, Frederic Koehler, Raghu Meka, and Dhruv Rohatgi. On the power of pre-
conditioning in sparse linear regression. arXiv preprint arXiv:2106.09207, 2021.
[318] Jonathan A. Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almost-linear-
time algorithm for approximate max ﬂow in undirected graphs, and its multicommodity gen-
eralizations. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 217-226, 2014.
[319] Jonathan A. Kelner, Jerry Li, Allen Liu, Aaron Sidford, and Kevin Tian. Semi-random sparse
recovery in nearly-linear time. CoRR, abs/2203.04002, 2022.
[320] Peter Kiss. Deterministic dynamic matching in worst-case update time. In Mark Braverman,
editor, 13th Innovations in Theoretical Computer Science Conference, ITCS 2022, January
31 - February 3, 2022, Berkeley, CA, USA, volume 215 of LIPIcs, pages 94:1-94:21. Schloss
Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2022.
[321] Adam R. Klivans, Pravesh K. Kothari, and Raghu Meka. Eﬃcient algorithms for outlier-
robust regression. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9
July 2018, pages 1420-1430, 2018.
[322] Philip A. Knight, Daniel Ruiz, and Bora U¸car. A symmetry preserving algorithm for matrix
scaling. SIAM Journal on Matrix Analysis and Applications, 35(3):931-955, 2014.
[323] O Kolossoski and Renato DC Monteiro.
An accelerated non-Euclidean hybrid proximal
extragradient-type algorithm for convex-concave saddle-point problems. Optimization Methods
and Software, 32(6):1244-1272, 2017.
[324] Vladimir Koltchinskii and Stanislav Minsker. l1-penalization in functional linear regression
with subgaussian design. Journal de l'Ecole polytechnique-Math´ematiques, 1:269-330, 2014.
[325] Christian Konrad. Maximum matching in turnstile streams. In Algorithms - ESA 2015 - 23rd
Annual European Symposium, Patras, Greece, September 14-16, 2015, Proceedings, pages 840-
852, 2015.
[326] Christian Konrad. A simple augmentation method for matchings with applications to stream-
ing algorithms. In 43rd International Symposium on Mathematical Foundations of Computer
Science, MFCS 2018, August 27-31, 2018, Liverpool, UK, pages 74:1-74:16, 2018.
[327] Christian Konrad, Fr´ed´eric Magniez, and Claire Mathieu.
Maximum matching in semi-
streaming with few passes. In Approximation, Randomization, and Combinatorial Optimiza-
tion. Algorithms and Techniques - 15th International Workshop, APPROX 2012, and 16th
International Workshop, RANDOM 2012, Cambridge, MA, USA, August 15-17, 2012. Pro-
ceedings, pages 231-242, 2012.

BIBLIOGRAPHY
885
[328] Tsvi Kopelowitz, Seth Pettie, and Ely Porat. Higher lower bounds from the 3sum conjecture.
In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms,
SODA 2016, Arlington, VA, USA, January 10-12, 2016, pages 1272-1287, 2016.
[329] G. M. Korpelevich. An extragradient method for ﬁnding saddle points and for other problems.
Ekonomika i Matematicheskie Metody, 12(4):747-756, 1976.
[330] Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and
improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT
Symposium on Theory of Computing, pages 1035-1046, 2018.
[331] Ioannis Koutis, Gary L. Miller, and Richard Peng. Approaching optimality for solving SDD
linear systems. In 51th Annual IEEE Symposium on Foundations of Computer Science, FOCS
2010, October 23-26, 2010, Las Vegas, Nevada, USA, pages 235-244, 2010.
[332] Dmitry Kovalev, Alexander V. Gasnikov, and Peter Richt´arik. Accelerated primal-dual gra-
dient method for smooth and convex-concave saddle-point problems with bilinear coupling.
arXiv e-prints, abs/2112.15199, 2021.
[333] Dmitry Kovalev, Adil Salim, and Peter Richt´arik. Optimal and practical algorithms for smooth
and strongly convex decentralized optimization. In Advances in Neural Information Processing
Systems 33 (NeurIPS), pages 18342-18352, 2020.
[334] Hendrik Anthony Kramers. Brownian motion in a ﬁeld of force and the diﬀusion model of
chemical reactions. Physica, 7(4):284-304, 1940.
[335] Gitta Kutyniok.
Theory and applications of compressed sensing.
GAMM-Mitteilungen,
36(1):79-101, 2013.
[336] Nathaniel Lahn, Deepika Mulchandani, and Sharath Raghvendra. A graph theoretic additive
approximation of optimal transport. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d'Alch´e-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13813-
13823, 2019.
[337] Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance.
In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages
665-674. IEEE, 2016.
[338] Guanghui Lan, Zhize Li, and Yi Zhou. A uniﬁed variance-reduced accelerated gradient method
for convex optimization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence

BIBLIOGRAPHY
886
d'Alch´e-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information
Processing Systems 32 (NeurIPS), pages 10462-10472, 2019.
[339] Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem of linear
diﬀerential and integral operators. Journal of Research of the National Bureau of Standards,
45(4), 1950.
[340] B´eatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model
selection. The Annals of Statistics, 28(5):1302-1338, 2000.
[341] Michel Ledoux. Concentration of measure and logarithmic Sobolev inequalities. Seminaire de
probabilities XXXIII, 1999.
[342] Yin Tat Lee. Lecture 8: Stochastic methods and applications. Class notes, UW CSE 599:
Interplay between Convex Optimization and Geometry, 2018.
[343] Yin Tat Lee, Satish Rao, and Nikhil Srivastava. A new approach to computing maximum
ﬂows using electrical ﬂows. In Symposium on Theory of Computing Conference, STOC'13,
Palo Alto, CA, USA, June 1-4, 2013, pages 755-764, 2013.
[344] Yin Tat Lee, Ruoqi Shen, and Kevin Tian. Logsmooth gradient concentration and tighter
runtimes for metropolized hamiltonian monte carlo. In Conference on Learning Theory, COLT
2020, 2020.
[345] Yin Tat Lee, Ruoqi Shen, and Kevin Tian. Lower bounds on metropolized sampling meth-
ods for well-conditioned distributions. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N.
Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Infor-
mation Processing Systems 34: Annual Conference on Neural Information Processing Systems
2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 18812-18824, 2021.
[346] Yin Tat Lee, Ruoqi Shen, and Kevin Tian. Structured logconcave sampling with a restricted
gaussian oracle.
In Mikhail Belkin and Samory Kpotufe, editors, Conference on Learning
Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA, volume 134 of Proceedings
of Machine Learning Research, pages 2993-3050. PMLR, 2021.
[347] Yin Tat Lee and Aaron Sidford. Eﬃcient accelerated coordinate descent methods and faster
algorithms for solving linear systems. In 54th Annual IEEE Symposium on Foundations of
Computer Science (FOCS), pages 147-156, 2013.
[348] Yin Tat Lee and Aaron Sidford.
Path ﬁnding methods for linear programming: Solving
linear programs in ˜o(vrank) iterations and faster algorithms for maximum ﬂow. In 55th IEEE
Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA,
October 18-21, 2014, pages 424-433, 2014.

BIBLIOGRAPHY
887
[349] Yin Tat Lee and Aaron Sidford. Eﬃcient inverse maintenance and faster algorithms for linear
programming. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS
2015, Berkeley, CA, USA, 17-20 October, 2015, pages 230-249, 2015.
[350] Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and
its implications for combinatorial and convex optimization. In Venkatesan Guruswami, editor,
IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley,
CA, USA, 17-20 October, 2015, pages 1049-1065. IEEE Computer Society, 2015.
[351] Yin Tat Lee, Zhao Song, and Santosh S. Vempala. Algorithmic theory of odes and sampling
from well-conditioned logconcave densities. CoRR, abs/1812.06243, 2018.
[352] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current
matrix multiplication time. In Conference on Learning Theory, COLT 2019, 25-28 June 2019,
Phoenix, AZ, USA, pages 2140-2157, 2019.
[353] Yin Tat Lee and He Sun. An sdp-based algorithm for linear-sized spectral sparsiﬁcation. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC
2017, Montreal, QC, Canada, June 19-23, 2017, pages 678-687, 2017.
[354] David Asher Levin, Yuval Peres, and Elizabeth Wilmer. Markov Chains and Mixing Times.
American Mathematical Society, 2009.
[355] Shlomo Levy and Peter K Fullagar. Reconstruction of a sparse spike train from a portion of
its spectrum and application to high-resolution deconvolution. Geophysics, 46(9):1235-1243,
1981.
[356] Adrian Lewis. Convex analysis on the hermitian matrices. SIAM Journal on Optimization,
6(0):164-177, 1996.
[357] Adrian Lewis and Hristo S. Sendov. Twice diﬀerentiable spectral functions. SIAM Journal on
Matrix Analysis and Applications, 23(0):368-386, 2001.
[358] Jason Li. Faster parallel algorithm for approximate shortest path. In Konstantin Makarychev,
Yury Makarychev, Madhur Tulsiani, Gautam Kamath, and Julia Chuzhoy, editors, Proccedings
of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, Chicago,
IL, USA, June 22-26, 2020, pages 308-321. ACM, 2020.
[359] Jerry Li and Guanghao Ye. Robust gaussian covariance estimation in nearly-matrix multipli-
cation time. In Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,
2020.

BIBLIOGRAPHY
888
[360] Jerry Zheng Li. Principled approaches to robust machine learning and beyond. PhD thesis,
Massachusetts Institute of Technology, 2018.
[361] Jun Z. Li, Devin M. Absher, Hua Tang, Audrey M. Southwick, Amanda M. Casto, Sohini
Ramachandran, Howard M. Cann, Gregory S. Barsh, Marcus Feldman, Luigi L. Cavalli-Sforza,
and Richard M. Myers. Worldwide human relationships inferred from genome-wide patterns
of variation. Science, 319:1100-1104, 2008.
[362] Mu Li, Gary L. Miller, and Richard Peng. Iterative row sampling. In 54th Annual IEEE
Symposium on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013, Berkeley,
CA, USA, pages 127-136, 2013.
[363] Henry Lin. Reducing directed max ﬂow to undirected max ﬂow. Unpublished Manuscript,
2009.
[364] Hongzhou Lin, Julien Mairal, and Za¨ıd Harchaoui. A universal catalyst for ﬁrst-order optimiza-
tion. In Advances in Neural Information Processing Systems 28 (NeurIPS), pages 3384-3392,
2015.
[365] Tianyi Lin, Nhat Ho, and Michael I Jordan. On the acceleration of the sinkhorn and greenkhorn
algorithms for optimal transport. arXiv preprint arXiv:1906.01437, 2019.
[366] Tianyi Lin, Chi Jin, and Michael I. Jordan. Near-optimal algorithms for minimax optimization.
In 33rd Annual Conference on Computational Learning Theory (COLT), pages 2738-2779,
2020.
[367] Nathan Linial, Alex Samorodnitsky, and Avi Wigderson. A deterministic strongly polynomial
algorithm for matrix scaling and approximate permanents. In Proceedings of the Thirtieth
Annual ACM Symposium on the Theory of Computing, Dallas, Texas, USA, May 23-26, 1998,
pages 644-652, 1998.
[368] S. CliﬀLiu, Zhao Song, and Hengjie Zhang. Breaking the n-pass barrier: A streaming algorithm
for maximum weight bipartite matching. CoRR, abs/2009.06106, 2020.
[369] Oren E. Livne and Gene H. Golub.
Scaling by binormalization.
Numerical Algorithms,
35(1):97-120, 2004.
[370] L´aszl´o Lov´asz and Santosh Vempala. Simulated annealing in convex bodies and an o*(n4)
volume algorithm. Journal of Computer and System Sciences, 72(2):392-417, 2006.
[371] L´aszl´o Lov´asz and Santosh S. Vempala. Fast algorithms for logconcave functions: Sampling,
rounding, integration and optimization. In 47th Annual IEEE Symposium on Foundations of
Computer Science (FOCS 2006), 21-24 October 2006, Berkeley, California, USA, Proceedings,
pages 57-68, 2006.

BIBLIOGRAPHY
889
[372] L´aszl´o Lov´asz and Santosh S. Vempala.
Hit-and-run from a corner.
SIAM J. Comput.,
35(4):985-1005, 2006.
[373] L´aszl´o Lov´asz and Santosh S. Vempala. The geometry of logconcave functions and sampling
algorithms. Random Struct. Algorithms, 30(3):307-358, 2007.
[374] Haihao Lu.
"relative-continuity" for non-lipschitz non-smooth convex optimization using
stochastic (or deterministic) mirror descent. INFORMS Journal on Optimization, pages 288-
303, 2019.
[375] Haihao Lu, Robert M. Freund, and Yurii E. Nesterov. Relatively smooth convex optimization
by ﬁrst-order methods, and applications. SIAM J. Optim., 28(1):333-354, 2018.
[376] Aleksander Madry.
Fast approximation algorithms for cut-based problems in undirected
graphs. In 51th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010,
October 23-26, 2010, Las Vegas, Nevada, USA, pages 245-254, 2010.
[377] Aleksander Madry. Navigating central path with electrical ﬂows: From ﬂows to matchings,
and back. In 54th Annual IEEE Symposium on Foundations of Computer Science, FOCS
2013, 26-29 October, 2013, Berkeley, CA, USA, pages 253-262, 2013.
[378] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International
Conference on Learning Representations (ICLR), 2018.
[379] Michael W. Mahoney, Satish Rao, Di Wang, and Peng Zhang. Approximating the solution to
mixed packing and covering lps in parallel o~(epsilonˆ{-3}) time. In 43rd International Col-
loquium on Automata, Languages, and Programming, ICALP 2016, July 11-15, 2016, Rome,
Italy, pages 52:1-52:14, 2016.
[380] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Approximation
algorithms for semi-random partitioning problems. In Proceedings of the forty-fourth annual
ACM symposium on Theory of computing, pages 367-384, 2012.
[381] Arian Maleki and David L Donoho. Optimally tuned iterative reconstruction algorithms for
compressed sensing. IEEE Journal of Selected Topics in Signal Processing, 4(2):330-341, 2010.
[382] St´ephane G Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries.
IEEE Transactions on signal processing, 41(12):3397-3415, 1993.
[383] Oren Mangoubi and Aaron Smith. Mixing of hamiltonian monte carlo on strongly log-concave
distributions 2: Numerical integrators. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages
586-595, 2019.

BIBLIOGRAPHY
890
[384] Oren Mangoubi and Nisheeth K. Vishnoi. Dimensionally tight bounds for second-order hamil-
tonian monte carlo. In Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montr´eal, Canada, pages 6030-6040, 2018.
[385] Jelena Marasevic, Cliﬀord Stein, and Gil Zussman. A fast distributed stateless algorithm for
alpha-fair packing problems. In 43rd International Colloquium on Automata, Languages, and
Programming, ICALP 2016, July 11-15, 2016, Rome, Italy, pages 54:1-54:15, 2016.
[386] Pascal Massart and ´Elodie N´ed´elec.
Risk bounds for statistical learning.
The Annals of
Statistics, 34(5):2326-2366, 2006.
[387] Andrew McGregor. Finding graph matchings in data streams. In Approximation, Randomiza-
tion and Combinatorial Optimization, Algorithms and Techniques, 8th International Workshop
on Approximation Algorithms for Combinatorial Optimization Problems, APPROX 2005 and
9th InternationalWorkshop on Randomization and Computation, RANDOM 2005, Berkeley,
CA, USA, August 22-24, 2005, Proceedings, pages 170-181, 2005.
[388] Andrew McGregor. Graph stream algorithms: a survey. SIGMOD Rec., 43(1):9-20, 2014.
[389] Andrew McGregor and Sofya Vorotnikova. A simple, space-eﬃcient, streaming algorithm for
matchings in low arboricity graphs. In 1st Symposium on Simplicity in Algorithms, SOSA
2018, January 7-10, 2018, New Orleans, LA, USA, pages 14:1-14:4, 2018.
[390] Michela Meister and Gregory Valiant. A data prism: Semi-veriﬁed learning in the small-alpha
regime. In Conference On Learning Theory, pages 1530-1546. PMLR, 2018.
[391] Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chan-
drasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra (gradient) mile. In 7th International Conference on Learning Representations, ICLR
2019, New Orleans, LA, USA, May 6-9, 2019, 2019.
[392] G´erard Meurant. The Lanczos and Conjugate Gradient Algorithms: From Theory to Finite
Precision Computations. Society for Industrial and Applied Mathematics, 2006.
[393] Marvin Minsky and Seymour Papert. Perceptrons—an introduction to computational geometry.
MIT Press, 1987.
[394] Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richt´arik, and Yura Malitsky.
Revisiting stochastic extragradient. arXiv preprint arXiv:1905.11373, 2019.

BIBLIOGRAPHY
891
[395] Hesameddin Mohammadi, Meisam Razaviyayn, and Mihailo R. Jovanovic. Performance of
noisy nesterov's accelerated method for strongly convex optimization problems. In 2019 Amer-
ican Control Conference, ACC 2019, Philadelphia, PA, USA, July 10-12, 2019, pages 3426-
3431, 2019.
[396] Ankur Moitra. What does robustness say about algorithms. ICML '17 Tutorial, 2017.
[397] Ankur Moitra, Amelia Perry, and Alexander S Wein. How robust are reconstruction thresholds
for community detection?
In Proceedings of the forty-eighth annual ACM symposium on
Theory of Computing, pages 828-841, 2016.
[398] Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A uniﬁed analysis of extra-gradient
and optimistic gradient methods for saddle point problems: Proximal point approach. arXiv
preprint arXiv:1901.08511, 2019.
[399] Cleve B. Moler and Charles Van Loan. Nineteen dubious ways to compute the exponential of
a matrix, twenty-ﬁve years later. SIAM Review, 45(1):3-49, 2003.
[400] Renato D. C. Monteiro and Benar Fux Svaiter. An accelerated hybrid proximal extragradient
method for convex optimization and its implications to second-order methods. SIAM J. Optim.,
23(2):1092-1125, 2013.
[401] Wenlong Mou, Nicolas Flammarion, Martin J. Wainwright, and Peter L. Bartlett. An eﬃcient
sampling algorithm for non-smooth composite potentials. CoRR, abs/1910.00551, 2019.
[402] Wenlong Mou, Yi-An Ma, Martin J. Wainwright, Peter L. Bartlett, and Michael I. Jordan.
High-order langevin diﬀusion yields an accelerated MCMC algorithm. CoRR, abs/1908.10859,
2019.
[403] Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
[404] Cameron Musco and Christopher Musco.
Randomized block krylov methods for stronger
and faster approximate singular value decomposition.
In Advances in Neural Information
Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015,
December 7-12, 2015, Montreal, Quebec, Canada, pages 1396-1404, 2015.
[405] Cameron Musco, Christopher Musco, and Aaron Sidford.
Stability of the lanczos method
for matrix function approximation. In Artur Czumaj, editor, Proceedings of the Twenty-Ninth
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2018, New Orleans, LA, USA,
January 7-10, 2018, pages 1605-1624. SIAM, 2018.
[406] Tigran Nagapetyan, Andrew B Duncan, Leonard Hasenclever, Sebastian J Vollmer, Lukasz
Szpruch, and Konstantinos Zygalakis. The true cost of stochastic gradient langevin dynamics.
arXiv preprint arXiv:1706.02692, 2017.

BIBLIOGRAPHY
892
[407] Hongseok Namkoong and John C Duchi.
Stochastic gradient methods for distributionally
robust optimization with f-divergences. In Advances in Neural Information Processing Systems,
pages 2208-2216, 2016.
[408] Danupon Nanongkai, Thatchaphol Saranurak, and Christian Wulﬀ-Nilsen. Dynamic minimum
spanning forest with subpolynomial worst-case update time. In Chris Umans, editor, 58th
IEEE Annual Symposium on Foundations of Computer Science, FOCS 2017, Berkeley, CA,
USA, October 15-17, 2017, pages 950-961. IEEE Computer Society, 2017.
[409] Radford M Neal. Mcmc using hamiltonian dynamics. Handbook of Markov chain Monte Carlo,
2(11):2, 2011.
[410] Deanna Needell and Joel A Tropp. Cosamp: Iterative signal recovery from incomplete and
inaccurate samples. Applied and computational harmonic analysis, 26(3):301-321, 2009.
[411] Deanna Needell and Roman Vershynin. Signal recovery from incomplete and inaccurate mea-
surements via regularized orthogonal matching pursuit. IEEE Journal of selected topics in
signal processing, 4(2):310-316, 2010.
[412] Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A uniﬁed frame-
work for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical
science, 27(4):538-557, 2012.
[413] Christopher Nemeth and Paul Fearnhead. Stochastic gradient markov chain monte carlo. arXiv
preprint arXiv:1907.06986, 2019.
[414] A. Nemirovski and D.˜B. Yudin. Problem Complexity and Method Eﬃciency in Optimization.
Wiley, 1983.
[415] Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequali-
ties with lipschitz continuous monotone operators and smooth convex-concave saddle point
problems. SIAM Journal on Optimization, 15(1):229-251, 2004.
[416] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochas-
tic approximation approach to stochastic programming.
SIAM Journal on optimization,
19(4):1574-1609, 2009.
[417] Yurii Nesterov. A method for solving a convex programming problem with convergence rate
O(1/k2). Doklady AN SSSR, 269:543-547, 1983.
[418] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume I.
Springer, 2003.

BIBLIOGRAPHY
893
[419] Yurii Nesterov. Smooth minimization of non-smooth functions. Math. Program., 103(1):127-
152, 2005.
[420] Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and
related problems. Math. Program., 109(2-3):319-344, 2007.
[421] Yurii Nesterov. Smoothing technique and its applications in semideﬁnite optimization. Math-
ematical Programming, Series A, 110:245-259, 2007.
[422] Yurii Nesterov.
Primal-dual subgradient methods for convex problems.
Math. Program.,
120(1):261-283, 2009.
[423] Yurii Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization problems.
SIAM Journal on Optimization, 22(2):341-362, 2012.
[424] Yurii Nesterov and Arkadi Nemirovski. Interior-Point Polynomial Algorithms in Convex Pro-
gramming. Society for Industrial and Applied Mathematics, 1994.
[425] Yurii E. Nesterov and Sebastian U. Stich. Eﬃciency of the accelerated coordinate descent
method on structured optimization problems. SIAM J. Optim., 27(1):110-123, 2017.
[426] John Von Neumann. Zur theorie der gesellschaftsspiele. Mathematische Annalen, 100:295-320,
1928.
[427] Jiazhong Nie, Wojciech Kotlowski, and Manfred K. Warmuth.
Online PCA with optimal
regrets. In Sanjay Jain, R´emi Munos, Frank Stephan, and Thomas Zeugmann, editors, Algo-
rithmic Learning Theory - 24th International Conference, ALT 2013, Singapore, October 6-9,
2013. Proceedings, volume 8139 of Lecture Notes in Computer Science, pages 98-112. Springer,
2013.
[428] F. W. J. Olver, A. B. Olde Daalhuis, D. W. Lozier, B. I. Schneider, R. F. Boisvert, C. W.
Clark, B. V. Saunders B. R. Mille and, H. S. Cohl, and eds. M. A. McClain. Nist digital library
of mathematical functions, 2020.
[429] Lorenzo Orecchia, Sushant Sachdeva, and Nisheeth K. Vishnoi.
Approximating the expo-
nential, the lanczos method and an ˜o(m)-time spectral algorithm for balanced separator. In
Proceedings of the 44th Symposium on Theory of Computing Conference, STOC 2012, New
York, NY, USA, May 19 - 22, 2012, pages 1141-1160, 2012.
[430] Felix Otto and C´edric Villani. Generalization of an inequality by talagrand and links with the
logarithmic sobolev inequality. Journal of Functional Analysis, 173(2):361-400, 2000.
[431] Yuyuan Ouyang and Yangyang Xu. Lower complexity bounds of ﬁrst-order methods for convex-
concave bilinear saddle-point problems. Mathematical Programming, 2019.

BIBLIOGRAPHY
894
[432] Balamurugan Palaniappan and Francis R. Bach. Stochastic variance reduction methods for
saddle-point problems. In Advances in Neural Information Processing Systems 29 (NeurIPS),
pages 1408-1416, 2016.
[433] Victor Y. Pan and Zhao Q. Chen. The complexity of the matrix eigenproblem. In Proceedings
of the 31st Annual ACM SIGACT Symposium on Theory of Computing, STOC 1999, 1999.
[434] Victor M. Panaretos and Yoav Zemel.
Amplitude and phase variation of point processes.
Annals of Statistics, 44(2):771-812, 2016.
[435] Neal Parikh and Stephen P. Boyd. Proximal algorithms. Found. Trends Optim., 1(3):127-239,
2014.
[436] Peristera Paschou, Jamey Lewis, Asif Javed, and Petros Drineas. Ancestry informative markers
for ﬁne-scale individual assignment to worldwide populations. J Med Genet., 47(12):835-847,
2010.
[437] Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krishnaprasad. Or-
thogonal matching pursuit: Recursive function approximation with applications to wavelet
decomposition. In Proceedings of 27th Asilomar conference on signals, systems and comput-
ers, pages 40-44. IEEE, 1993.
[438] Seth Patinkin. Method, apparatus, and system for clustering and classiﬁcation, August 30
2011. US Patent 8,010,466.
[439] Ami Paz and Gregory Schwartzman. A (2 + ϵ)-approximation for maximum weight matching
in the semi-streaming model. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Sym-
posium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January
16-19, pages 2153-2161, 2017.
[440] Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transac-
tions of the Royal Society of London, 185:71-110, 1894.
[441] Richard Peng. Approximate undirected maximum ﬂows in O(mpolylog(n)) time. In Proceed-
ings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA
2016, Arlington, VA, USA, January 10-12, 2016, pages 1862-1867, 2016.
[442] Richard Peng, Kanat Tangwongsan, and Peng Zhang. Faster and simpler width-independent
parallel algorithms for positive semideﬁnite programming. CoRR, abs/1201.5135, 2016.
[443] Marcelo Pereyra. Proximal markov chain monte carlo algorithms. Stat. Comput., 26(4):745-
760, 2016.

BIBLIOGRAPHY
895
[444] Marcelo Alejandro Pereyra, Philip Schniter, Emilie Chouzenoux, Jean-Christophe Pesquet,
Jean-Yves Tourneret, Alfred O. Hero, and Steve McLaughlin. A survey of stochastic simulation
and optimization methods in signal processing. IEEE Journal on Selected Topics in Signal
Processing, 10(2):224-241, 2015.
[445] Giorgio Pini and Giuseppe Gambolati. Is a simple diagonal scaling the best preconditioner for
conjugate gradients on supercomputers? Advances in Water Resources, 13(3):147-153, 1990.
[446] Luisa F Polania, Rafael E Carrillo, Manuel Blanco-Velasco, and Kenneth E Barner. Exploiting
prior knowledge in compressed sensing wireless ecg systems. IEEE journal of Biomedical and
Health Informatics, 19(2):508-519, 2014.
[447] Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust
estimation via robust gradient estimation. Journal of the Royal Statistical Society, Series B
(Methodological), 82(3):601-627, 2020.
[448] Zhaonan Qu, Yinyu Ye, and Zhengyuan Zhou. Diagonal preconditioning: Theory and algo-
rithms. arXiv:2003.07545, 2020.
[449] Zheng Qu and Peter Richt´arik. Coordinate descent with arbitrary sampling I: algorithms and
complexity. Optimization Methods and Software, 31(5):829-857, 2016.
[450] Kent Quanrud. Approximating optimal transport with linear programs. In 2nd Symposium
on Simplicity in Algorithms, SOSA@SODA 2019, January 8-9, 2019 - San Diego, CA, USA,
pages 6:1-6:9, 2019.
[451] Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv
e-prints, abs/1908.05659, 2019.
[452] Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable
sequences. In Advances in Neural Information Processing Systems 26: 27th Annual Conference
on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8,
2013, Lake Tahoe, Nevada, United States, pages 3066-3074, 2013.
[453] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for
correlated gaussian designs. The Journal of Machine Learning Research, 11:2241-2259, 2010.
[454] Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated gaussians.
In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages
85-96. IEEE, 2017.
[455] Peter Richt´arik and Martin Tak´aˇc. On optimal probabilities in stochastic coordinate descent
methods. Optimization Letters, 10(6):1233-1243, 2016.

BIBLIOGRAPHY
896
[456] Philippe Rigollet and Jan-Christian H¨utter. High-Dimensional Statistics. 2017.
[457] Gareth O Roberts and Richard L Tweedie. Exponential convergence of langevin distributions
and their discrete approximations. Bernoulli, 2(4):341-363, 1996.
[458] R.˜Tyrell Rockafellar. Convex Analysis. Princeton University Press, 1970.
[459] R Tyrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal
on control and optimization, 14(5):877-898, 1976.
[460] R Tyrrell Rockafellar.
Monotone operators associated with saddle-functions and minimax
problems. Nonlinear functional analysis, 18(part 1):397-407, 1970.
[461] Noah A. Rosenberg, Jonathan K. Pritchard, James L. Weber, Howard M. Cann, Kenneth K.
Kidd, Lev A. Zhivotovsky, and Marcus W. Feldman. Genetic structure of human populations.
Science, 298:2381-2385, 2002.
[462] Mark Rudelson and Roman Vershynin. Sparse reconstruction by convex relaxation: Fourier
and gaussian measurements. In 2006 40th Annual Conference on Information Sciences and
Systems, pages 207-212. IEEE, 2006.
[463] Mark Rudelson and Roman Vershynin. The smallest singular value of a random rectangular
matrix. Communications on Pure and Applied Mathematics, 62(12):1707-1739, 2009.
[464] Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial
on thompson sampling. Found. Trends Mach. Learn., 11(1):1-96, 2018.
[465] Yousef Saad.
Analysis of some krylov subspace approximations to the matrix exponential
operator. SIAM Journal on Numerical Analysis, 29(1):209-228, 1992.
[466] Sushant Sachdeva and Nisheeth K. Vishnoi.
Faster algorithms via approximation theory.
Foundations and Trends in Theoretical Computer Science, 9(2):125-210, 2014.
[467] Adil Salim, Dmitry Koralev, and Peter Richt´arik. Stochastic proximal langevin algorithm:
Potential splitting and nonasymptotic rates. In Advances in Neural Information Processing
Systems, pages 6653-6664, 2019.
[468] Piotr Sankowski. Maximum weight bipartite matching in matrix multiplication time. Theor.
Comput. Sci., 410(44):4480-4488, 2009.
[469] Fadil Santosa and William W Symes. Linear inversion of band-limited reﬂection seismograms.
SIAM Journal on Scientiﬁc and Statistical Computing, 7(4):1307-1330, 1986.
[470] Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpop-
ulation shift. arXiv preprint arXiv:2008.04859, 2020.

BIBLIOGRAPHY
897
[471] Ludwig Schmidt. Algorithms above the noise ﬂoor. PhD thesis, Massachusetts Institute of
Technology, 2018.
[472] Mark W. Schmidt, Nicolas Le Roux, and Francis R. Bach. Minimizing ﬁnite sums with the
stochastic average gradient. Math. Program., 162(1-2):83-112, 2017.
[473] Shai Shalev-Shwartz.
Online learning: Theory, algorithms, and applications.
PhD thesis,
Hebrew University, 2007.
[474] Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and
Trends in Machine Learning, 4(2):107-194, 2012.
[475] Shai Shalev-Shwartz and Ambuj Tewari. Stochastic methods for ℓ1-regularized loss minimiza-
tion. Journal of Machine Learning Research, 12:1865-1892, 2011.
[476] Shai Shalev-Shwartz and Yonatan Wexler. Minimizing the maximal loss: How and why. In
ICML, pages 793-801, 2016.
[477] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regular-
ized loss. J. Mach. Learn. Res., 14(1):567-599, 2013.
[478] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent
for regularized loss minimization. Math. Program., 155(1-2):105-145, 2016.
[479] R. Sharathkumar and Pankaj K. Agarwal. A near-linear time ϵ-approximation algorithm for
geometric bipartite matching. In Proceedings of the 44th Symposium on Theory of Computing
Conference, STOC 2012, New York, NY, USA, May 19 - 22, 2012, pages 385-394, 2012.
[480] Ruoqi Shen and Yin Tat Lee. The randomized midpoint method for log-concave sampling. In
Advances in Neural Information Processing Systems, pages 2100-2111, 2019.
[481] Ruoqi Shen, Kevin Tian, and Yin Tat Lee. Composite logconcave sampling with a restricted
gaussian oracle. CoRR, abs/2006.05976, 2020.
[482] Jonah Sherman. Nearly maximum ﬂows in nearly linear time. In 54th Annual IEEE Symposium
on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013, Berkeley, CA, USA,
pages 263-269, 2013.
[483] Jonah Sherman. Area-convexity, l∞regularization, and undirected multicommodity ﬂow. In
Hamed Hatami, Pierre McKenzie, and Valerie King, editors, 49th Annual ACM Symposium
on Theory of Computing (STOC), pages 452-460. ACM, 2017.
[484] Zhan Shi, Xinhua Zhang, and Yaoliang Yu. Bregman divergence for stochastic variance reduc-
tion: Saddle-point and adversarial prediction. In Advances in Neural Information Processing
Systems, 2017.

BIBLIOGRAPHY
898
[485] Ralph E. Showalter. Monotone operators in banach space and nonlinear partial diﬀerential
equations. Mathematical Surveys and Monographs, 49:162-163, 1997.
[486] Aaron Sidford and Kevin Tian. Coordinate methods for accelerating ℓ∞regression and faster
approximate maximum ﬂow. In 59th Annual IEEE Symposium on Foundations of Computer
Science, FOCS 2018, 7-9 October, 2018, Paris, France, 2018.
[487] Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and
faster algorithms for solving markov decision processes. In Proceedings of the Twenty-Ninth
Annual ACM-SIAM Symposium on Discrete Algorithms, pages 770-787. Society for Industrial
and Applied Mathematics, 2018.
[488] Daniel Dominic Sleator and Robert Endre Tarjan. A data structure for dynamic trees. J.
Comput. Syst. Sci., 26(3):362-391, 1983.
[489] Gary Smith. Essential Statistics, Regression, and Econometrics. Academic Press, 2012.
[490] Justin Solomon, Fernando de Goes, Gabriel Peyr´e, Marco Cuturi, Adrian Butscher, Andy
Nguyen, Tao Du, and Leonidas J. Guibas. Convolutional wasserstein distances: eﬃcient opti-
mal transportation on geometric domains. ACM Trans. Graph., 34(4):66:1-66:11, 2015.
[491] Daniel A Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partitioning,
graph sparsiﬁcation, and solving linear systems.
In Proceedings of the 36th Annual ACM
SIGACT Symposium on Theory of Computing, volume 4, 2004.
[492] Jacob Steinhardt. Robust Learning: Information Theory and Algorithms. PhD thesis, Stanford
University, 2018.
[493] Jacob Steinhardt, Moses Charikar, and Gregory Valiant. Resilience: A criterion for learning
in the presence of arbitrary outliers. arXiv preprint arXiv:1703.04940, 2017.
[494] Jacob Steinhardt, Pang Wei Koh, and Percy Liang.
Certiﬁed defenses for data poisoning
attacks. In Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,
pages 3517-3529, 2017.
[495] Jacob Steinhardt, Gregory Valiant, and Moses Charikar. Avoiding imposters and delinquents:
Adversarial crowdsourcing and peer prediction. In Advances in Neural Information Processing
Systems, pages 4439-4447, 2016.
[496] Fedor Stonyakina, Alexander Tyurin, Alexander Gasnikov, Pavel Dvurechensky, Artem Aga-
fonov, Darina Dvinskikh, Dmitry Pasechnyuk, Sergei Artamonov, and Victorya Piskunova.
Inexact relative smoothness and strong convexity for optimization and variational inequalities
by inexact model. arXiv e-prints, abs/2001.09013, 2020.

BIBLIOGRAPHY
899
[497] Thomas Strohmer and Roman Vershynin. A randomized solver for linear systems with ex-
ponential convergence. In Approximation, Randomization, and Combinatorial Optimization.
Algorithms and Techniques, 9th International Workshop on Approximation Algorithms for
Combinatorial Optimization Problems, APPROX 2006 and 10th International Workshop on
Randomization and Computation, RANDOM 2006, Barcelona, Spain, August 28-30 2006,
Proceedings, pages 499-507, 2006.
[498] Thomas Strohmer and Roman Vershynin. A randomized Kaczmarz algorithm with exponential
convergence. Journal of Fourier Analysis and Applications, 15(2):262, 2009.
[499] Conghui Tan, Tong Zhang, Shiqian Ma, and Ji Liu. Stochastic primal-dual method for empir-
ical risk minimization with o(1) per-iteration complexity. In Advances in Neural Information
Processing Systems, 2018.
[500] Robert Endre Tarjan.
Data structures and network algorithms, volume 44 of CBMS-NSF
regional conference series in applied mathematics. SIAM, 1983.
[501] Kiran Koshy Thekumparampil, Niao He, and Sewoong Oh. Lifted primal-dual method for bi-
linearly coupled smooth minimax optimization. In 25th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2022.
[502] Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Projec-
tion eﬃcient subgradient method and optimal nonsmooth frank-wolfe method. In Advances in
Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-
cessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
[503] Kevin Tian, Weihao Kong, and Gregory Valiant. Learning populations of parameters. In
Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.
Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Sys-
tems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA, pages 5778-5787, 2017.
[504] Kevin Tian, Teng Zhang, and James Zou. Cover: Learning covariate-speciﬁc vector representa-
tions with tensor decompositions. In Jennifer G. Dy and Andreas Krause, editors, Proceedings
of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan,
Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research,
pages 4933-4942. PMLR, 2018.
[505] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society. Series B (Methodological), 58(1):267-288, 1996.

BIBLIOGRAPHY
900
[506] Sumedh Tirodkar. Deterministic algorithms for maximum matching on general graphs in the
semi-streaming model. In 38th IARCS Annual Conference on Foundations of Software Tech-
nology and Theoretical Computer Science, FSTTCS 2018, December 11-13, 2018, Ahmedabad,
India, pages 39:1-39:16, 2018.
[507] Vladislav Tominin, Yaroslav Tominin, Ekaterina Borodich, Dmitry Kovalev, Alexander Gas-
nikov, and Pavel Dvurechensky. On accelerated saddle-point problems with composite struc-
ture. arXiv e-prints, abs/2103.09344v2, 2021.
[508] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In
Advances in Neural Information Processing Systems, pages 8000-8010, 2018.
[509] Lloyd N. Trefethen. Approximation Theory and Approximation Practice. Society for Industrial
and Applied Mathematics, USA, 2012.
[510] Joel A Tropp and Anna C Gilbert. Signal recovery from random measurements via orthogonal
matching pursuit. IEEE Transactions on information theory, 53(12):4655-4666, 2007.
[511] Koji Tsuda, Gunnar R¨atsch, and Manfred K. Warmuth. Matrix exponentiated gradient up-
dates for on-line learning and bregman projection. Journal of Machine Learning Research,
6:995-1018, 2005.
[512] John W Tukey.
A survey of sampling from contaminated distributions.
Contributions to
probability and statistics, pages 448-485, 1960.
[513] John W Tukey. Mathematics and the picturing of data. In Proceedings of the International
Congress of Mathematicians, Vancouver, 1975, volume 2, pages 523-531, 1975.
[514] Sara Van de Geer and Johannes Lederer.
The lasso, correlated design, and improved or-
acle inequalities. In From Probability to Statistics and Back: High-Dimensional Models and
Processes-A Festschrift in Honor of Jon A. Wellner, pages 303-316. Institute of Mathematical
Statistics, 2013.
[515] Jan van den Brand. A deterministic linear program solver in current matrix multiplication
time. In Shuchi Chawla, editor, Proceedings of the 2020 ACM-SIAM Symposium on Discrete
Algorithms, SODA 2020, Salt Lake City, UT, USA, January 5-8, 2020, pages 259-278. SIAM,
2020.
[516] Jan van den Brand, Yin Tat Lee, Yang P. Liu, Thatchaphol Saranurak, Aaron Sidford, Zhao
Song, and Di Wang.
Minimum cost ﬂows, mdps, and ℓ1-regression in nearly linear time
for dense instances. In STOC '21: 53rd Annual ACM SIGACT Symposium on Theory of
Computing, Virtual Event, Italy, June 21-25, 2021, pages 859-869, 2021.

BIBLIOGRAPHY
901
[517] Jan van den Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense linear pro-
grams in nearly linear time. In Konstantin Makarychev, Yury Makarychev, Madhur Tulsiani,
Gautam Kamath, and Julia Chuzhoy, editors, Proccedings of the 52nd Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2020, Chicago, IL, USA, June 22-26, 2020, pages
775-788. ACM, 2020.
[518] A. van der Sluis. Condition numbers and equilibration of matrices. Numerische Mathematik,
14(1):14-23, 1969.
[519] Santosh Vempala. Geometric random walks: A survey. MSRI Combinatorial and Computa-
tional Geometry, 52:573-612, 2005.
[520] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal
of Computer and System Sciences, 68(4):841-860, 2004.
[521] Santosh S. Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin al-
gorithm: Isoperimetry suﬃces. In Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 De-
cember 2019, Vancouver, BC, Canada, pages 8092-8104, 2019.
[522] Roman Vershynin. High-Dimensional Probability, An Introduction with Applications in Data
Science. 2016.
[523] Fran¸cois-Xavier Vialard. An elementary introduction to entropic regularization and proximal
methods for numerical optimal transport, 2019.
[524] Eric Vittinghoﬀ, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. Re-
gression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models.
Springer, 2005.
[525] John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior (com-
memorative edition). Princeton university press, 1944.
[526] Michael D. Vose. A linear algorithm for generating random numbers with a given distribution.
IEEE Transactions on software engineering, 17(9):972-975, 1991.
[527] David Wajc. Rounding dynamic matchings against an adaptive adversary. In Proccedings of
the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, Chicago,
IL, USA, June 22-26, 2020, pages 194-207, 2020.
[528] A. J. Walker.
An eﬃcient method for generating discrete random variables with general
distributions. ACM Transactions on Mathematical Software, 3(3):253-256, 1977.

BIBLIOGRAPHY
902
[529] Jialei Wang and Lin Xiao. Exploiting strong convexity from data with primal-dual ﬁrst-order
algorithms. In 34th International Conference on Machine Learning (ICML), pages 3694-3702,
2017.
[530] Jun-Kun Wang and Jacob D. Abernethy. Acceleration through optimistic no-regret dynamics.
In Advances in Neural Information Processing Systems 31 (NeurIPS), pages 3828-3838, 2018.
[531] Mengdi Wang. Primal-dual π learning: Sample complexity and sublinear run time for ergodic
markov decision problems. CoRR, abs/1710.06100, 2017.
[532] Mengdi Wang. Randomized linear programming solves the markov decision problem in nearly
linear (sometimes sublinear) time. Math. Oper. Res., 45(2):517-546, 2020.
[533] Yuanhao Wang and Jian Li. Improved algorithms for convex-concave minimax optimization.
In Advances in Neural Information Processing Systems 33 (NeurIPS), pages 4800-4810, 2020.
[534] Manfred K. Warmuth and Dima Kuzmin. Randomized PCA algorithms with regret bounds
that are logarithmic in the dimension. In Advances in Neural Information Processing Sys-
tems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing
Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pages 1481-1488, 2006.
[535] Manfred K. Warmuth and Dima Kuzmin. Online variance minimization. Machine Learning,
87(1):1-32, 2012.
[536] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics.
In Proceedings of the 28th international conference on machine learning (ICML-11), pages
681-688, 2011.
[537] Andre Wibisono. Proximal langevin algorithm: Rapid convergence under isoperimetry. CoRR,
abs/1911.01469, 2019.
[538] S lawomir T Wierzcho´n and Mieczys law A K lopotek. Modern algorithms of cluster analysis.
Springer, 2018.
[539] Blake E. Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite
objectives. In Advances in Neural Information Processing Systems 29 (NeurIPS), pages 3639-
3647, 2016.
[540] Stephen J Wright. Coordinate descent algorithms. Mathematical Programming, 151(1):3-34,
2015.
[541] Neal E. Young. Sequential and parallel algorithms for mixed packing and covering. In 42nd
Annual Symposium on Foundations of Computer Science, FOCS 2001, 14-17 October 2001,
Las Vegas, Nevada, USA, pages 538-546, 2001.

BIBLIOGRAPHY
903
[542] Yao-Liang Yu. The strong convexity of von neumann's entropy. http://www.cs.cmu.edu/
~yaoliang/mynotes/sc.pdf, 2013.
[543] A. Yurtsever, M. Udell, J. A. Tropp, and V. Cevher. Sketchy decisions: Convex low-rank matrix
optimization with optimal storage. In International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), pages 1188-1196, 2017.
[544] Chicheng Zhang and Yinan Li. Improved algorithms for eﬃcient active learning halfspaces
with massart and tsybakov noise. arXiv preprint arXiv:2102.05312, 2021.
[545] Junyu Zhang, Minyi Hong, and Shuzhong Zhang. On lower iteration complexity bounds for
the saddle point problems. arXiv e-prints, abs/1912.07481, 2019.
[546] Yuchen Zhang, Martin J Wainwright, and Michael I Jordan. Optimal prediction for sparse
linear models?
lower bounds for coordinate-separable m-estimators. Electronic Journal of
Statistics, 11(1):752-799, 2017.
[547] Yuchen Zhang and Lin Xiao. Stochastic primal-dual coordinate method for regularized em-
pirical risk minimization. J. Mach. Learn. Res., 18:84:1-84:42, 2017.
[548] Zhimin Zhang, Shoushui Wei, Dingwen Wei, Liping Li, Feng Liu, and Chengyu Liu. Compar-
ison of four recovery algorithms used in compressed sensing for ecg signal processing. In 2016
Computing in Cardiology Conference (CinC), pages 401-404. IEEE, 2016.
[549] Kaiwen Zhou, Qinghua Ding, Fanhua Shang, James Cheng, Danli Li, and Zhi-Quan Luo. Direct
acceleration of SAGA using sampled negative momentum. In 22nd International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS), pages 1602-1610. PMLR, 2019.
[550] Banghua Zhu, Jiantao Jiao, and Jacob Steinhardt. Robust estimation via generalized quasi-
gradients. CoRR, abs/2005.14073, 2020.
[551] Zeyuan Allen Zhu and Elad Hazan. Optimal black-box reductions between optimization objec-
tives. In Advances in Neural Information Processing Systems 29 (NeurIPS), pages 1606-1614,
2016.
[552] Zeyuan Allen Zhu, Yuanzhi Li, Rafael Mendes de Oliveira, and Avi Wigderson. Much faster
algorithms for matrix scaling. In 58th IEEE Annual Symposium on Foundations of Computer
Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017, pages 890-901, 2017.
[553] Zeyuan Allen Zhu, Zheng Qu, Peter Richt´arik, and Yang Yuan. Even faster accelerated co-
ordinate descent using non-uniform sampling. In 33rd International Conference on Machine
Learning (ICML), pages 1110-1119, 2016.

BIBLIOGRAPHY
904
[554] Difan Zou, Pan Xu, and Quanquan Gu.
Subsampled stochastic variance-reduced gradient
langevin dynamics. In International Conference on Uncertainty in Artiﬁcial Intelligence, 2018.
[555] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal
of the Royal Statistical Society. Series B (Statistical Methodology), 67(2):301-320, 2005.

